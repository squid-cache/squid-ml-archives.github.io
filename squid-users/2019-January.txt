From squid3 at treenet.co.nz  Wed Jan  2 10:01:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 Jan 2019 23:01:24 +1300
Subject: [squid-users] https debug
In-Reply-To: <b91ec8b7d7ae6339e12e7abbb63869cd@tiscali.it>
References: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>
 <ca313cd2-4344-d480-4563-82f9f1e43900@treenet.co.nz>
 <b91ec8b7d7ae6339e12e7abbb63869cd@tiscali.it>
Message-ID: <8537164e-6430-055e-0263-22d701c1215f@treenet.co.nz>

On 2/01/19 10:30 pm, Sampei wrote:
> About way to use https protocol I think I use connect tunnel, here

When a CONNECT tunnel is being used and not SSL-Bump'ed then all TLS
related issues are problems with one of the endpoint software. Not
related to the proxy at all. Squid is just blindly relaying the TLS
bytes as-is between the endpoints.

That said, some specific configs may encounter issues due to explicitly
telling Squid to do certain things which cannot be done to CONNECT
tunnels (eg. URL-rewrite, ACL checks of path strings), or to deny the
CONNECT which obviously would make the TLS not "work" at all.


I suspect that in your case some other port is involved which you do not
know about and are thus not letting through Squid. The access.log should
show what Squid is dealing with there.


> parttial of my squid.conf
> 
> 
> acl SSL_ports port 443????????? # https
> acl SSL_ports port 563????????? # snews
> ...
> acl Safe_ports port 80????????? # http
> acl Safe_ports port 21????????? # ftp
> acl Safe_ports port 443???????? # https
> ...
> http_access deny CONNECT !SSL_ports

Okay, but should be following the Safe_ports check. The default config
orders these checks by how common it is to encounter the attack types
they exist to prevent.

> http_access deny CONNECT !Safe_ports

The default config uses this instead:

 http_access deny !Safe_ports

The purpose of this Safe_ports ACL is to prevent the proxy handling
*any* traffic for protocols whose traffic syntax directly conflicts with
HTTP traffic syntax.

By limiting this check to only CONNECT messages, you are opening your
proxy to most of the attacks the Safe_port ACL was designed to prevent.



> acl test dstdomain example.com
> http_access allow test
> http_access allow CONNECT test

This latter is pointless. "test" was already allowed, so this line is
never reached by any traffic which it can match.


> I think to upgrade 4.x Squid but I'm looking for valid repository for
> Centos 7 which contains this pkg.

The official repositories for CentOS are detailed at
<https://wiki.squid-cache.org/KnowledgeBase/CentOS>

(I see that page needs an update Eliezer now has 4.4 in his main CentOS
repository <http://www1.ngtech.co.il/repo/centos/7/x86_64/>)


Amos


From johnrefwe at mail.com  Wed Jan  2 19:19:18 2019
From: johnrefwe at mail.com (johnr)
Date: Wed, 2 Jan 2019 13:19:18 -0600 (CST)
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
In-Reply-To: <cdd3e8bf-6a64-91d6-40b6-6e05133d851b@measurement-factory.com>
References: <1545946245329-0.post@n4.nabble.com>
 <cdd3e8bf-6a64-91d6-40b6-6e05133d851b@measurement-factory.com>
Message-ID: <1546456758259-0.post@n4.nabble.com>

I can open a bug if you think I should/can based on the backtrace here:

Core was generated by `(security_file_certgen) -s
/usr/local/squid/var/cache/squid/ssl_db -M 4MB'.
Program terminated with signal SIGSEGV, Segmentation fault.


Dec 28 22:15:20 vagrant-ubuntu-trusty-64 kernel: [ 4314.045153]
security_file_c[2876]: segfault at 100000000 ip 000000000040d5d8 sp
00007ffd810b4010 error 4 in security_file_certgen[400000+13000]

Program received signal SIGSEGV, Segmentation fault.
printX509Signature (out="", cert=...) at gadgets.cc:225
225   gadgets.cc: No such file or directory.
(gdb) bt
#0  printX509Signature (out="", cert=...) at gadgets.cc:225
#1  Ssl::OnDiskCertificateDbKey (properties=...) at gadgets.cc:238
#2  0x0000000000405f8d in processNewRequest (fs_block_size=4096,
max_db_size=4194304, 
    db_path="/var/lib/squid-pnr-proxy/ssl_db_v4", request_message=...) at
security_file_certgen.cc:195
#3  main (argc=<optimized out>, argv=0x7fffffffe608) at
security_file_certgen.cc:345



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From jimc at jfcarter.net  Wed Jan  2 22:01:53 2019
From: jimc at jfcarter.net (jimc)
Date: Wed, 02 Jan 2019 14:01:53 -0800
Subject: [squid-users] Caching mirrored origin server
Message-ID: <acf0ba4e5bed59057a4f56b3b29ed7ee@jfcarter.net>

I'm using squid-4.4-2.1.x86_64 from OpenSuSE Tumbleweed.  My goal is
that when doing periodic software updates. each host in my department
will contact my proxy to obtain the new metadata and packages (SuSE has
a syntax for this); the proxy will download each file only once.  This
sounds like pretty standard Squid operation, but there's a gross botfly
in the ointment: the origin servers return 302 Found, each time
redirecting to a different mirror, and with "normal" configuration this
result is passed back to the client which makes a new connection (via
the proxy) to that mirror, but the retrieved file will likely never be
accessed again from that mirror.

Is there anyone who really knows what Squid can do and who has an idea
how to get Squid to follow the redirects on a cache miss, and to respond
to the original URL with the cached copy, whichever mirror it came from?

https://ma.ttwagner.com/lazy-distro-mirrors-with-squid/ by Matt Wagner
(2014-04-03) has some suggestions along this line.  He fixates on one
mirror per origin server, and uses the virtual host feature to
effectively do an internal redirect to that mirror.  I also tried that
solution for a while, but mirrors come and go, and curating the mirror
selection turned out to be a reliability problem, so I gave it up.

Software package RPMs are often large, and in my application it's
important to raise the maximum_object_size; the largest item I've seen
is 158Mb and I've set the limit to 300Mb.  And of course enough disc
cache space has to be allowed to hold the expected set of RPMs to be
used, e.g. when installing the OS on several new machines.  My
cache_dir has 5000 Mb.

-- 
James F. Carter   Email: jimc at jfcarter.net
Web: http://www.math.ucla.edu/~jimc (q.v. for PGP key)

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190102/dbbc4e01/attachment.sig>

From stressedtux at hotmail.com  Thu Jan  3 14:37:55 2019
From: stressedtux at hotmail.com (stressedtux)
Date: Thu, 3 Jan 2019 08:37:55 -0600 (CST)
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for each
Message-ID: <1546526275247-0.post@n4.nabble.com>

Hi guys!

i need a hand to understand if it is possible to configure the proxy a
particular way. 

Im needing to configure the proxy to allow at the same time:

- a whitelist of sites that anyone that uses the proxy could use without
login
- and in addition to that i need to have specific ACLs for different
authenticated users. 

I need to control both http and https connections to external sites. I can
use sslbump but im having hard time configuring sslbump with proxy_auth, and
on top of that, i need different acl whitelists for different users.

Is this kind of configuration possible? Just trying to understand if im on a
dead road :D

Thanks in advanced!
Tux



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From m_zouhairy at skno.by  Thu Jan  3 14:41:08 2019
From: m_zouhairy at skno.by (Vacheslav)
Date: Thu, 3 Jan 2019 17:41:08 +0300
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
	each
In-Reply-To: <1546526275247-0.post@n4.nabble.com>
References: <1546526275247-0.post@n4.nabble.com>
Message-ID: <219501d4a372$5d016a00$17043e00$@skno.by>

Yeah,  with ufdbguard maybe there are other means ..

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of stressedtux
Sent: Thursday, January 3, 2019 5:38 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for each

>>
>i need a hand to understand if it is possible to configure the proxy a particular way. 

>Im needing to configure the proxy to allow at the same time:

>- a whitelist of sites that anyone that uses the proxy could use without login
- and in addition to that i need to have specific ACLs for different authenticated users. 

>I need to control both http and https connections to external sites. I can use sslbump but im having hard time configuring sslbump with proxy_auth, and on top of that, i need different acl whitelists for different users.

>Is this kind of configuration possible? Just trying to understand if im on a dead road :D

Thanks in advanced!
Tux



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From stressedtux at hotmail.com  Thu Jan  3 15:40:37 2019
From: stressedtux at hotmail.com (stressedtux)
Date: Thu, 3 Jan 2019 09:40:37 -0600 (CST)
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
	each
In-Reply-To: <219501d4a372$5d016a00$17043e00$@skno.by>
References: <1546526275247-0.post@n4.nabble.com>
 <219501d4a372$5d016a00$17043e00$@skno.by>
Message-ID: <1546530037095-0.post@n4.nabble.com>

With ufdbguard is possible to allow one user to have an acl and other user a
different acl? Im trying to completly block access to inet except for what i
should allow. 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From webmaster at squidblacklist.org  Thu Jan  3 15:45:05 2019
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Thu, 3 Jan 2019 09:45:05 -0600
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
 each
In-Reply-To: <1546530037095-0.post@n4.nabble.com>
References: <1546526275247-0.post@n4.nabble.com>
 <219501d4a372$5d016a00$17043e00$@skno.by>
 <1546530037095-0.post@n4.nabble.com>
Message-ID: <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>

Why are you asking support questions about a commercial product, on the 
squid proxy email users list?

On 1/3/2019 9:40 AM, stressedtux wrote:
> With ufdbguard is possible to allow one user to have an acl and other user a
> different acl? Im trying to completly block access to inet except for what i
> should allow.
>
>
>
> --
> Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Signed,

Benjamin E. Nichols
Founder & Chief Architect
1-(405)-301-9516
http://www.squidblacklist.org



From Antony.Stone at squid.open.source.it  Thu Jan  3 15:54:19 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 3 Jan 2019 16:54:19 +0100
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
	each
In-Reply-To: <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
References: <1546526275247-0.post@n4.nabble.com>
 <1546530037095-0.post@n4.nabble.com>
 <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
Message-ID: <201901031654.19170.Antony.Stone@squid.open.source.it>

On Thursday 03 January 2019 at 16:45:05, Benjamin E. Nichols wrote:

> Why are you asking support questions about a commercial product, on the
> squid proxy email users list?

Maybe because s/he's only just been introduced to ufdbguard by an asnwer from 
someone else on this list, and therefore doesn't yet realise there might be 
somewhere better to ask further questions about that?

Antony.

> On 1/3/2019 9:40 AM, stressedtux wrote:
> > With ufdbguard is possible to allow one user to have an acl and other
> > user a different acl? Im trying to completly block access to inet except
> > for what i should allow.

-- 
Angela Merkel arrives at Paris airport.
"Nationality?" asks the immigration officer.
"German," she replies.
"Occupation?"
"No, just here for a summit conference."

                                                   Please reply to the list;
                                                         please *don't* CC me.


From stressedtux at hotmail.com  Thu Jan  3 16:05:49 2019
From: stressedtux at hotmail.com (stressedtux)
Date: Thu, 3 Jan 2019 10:05:49 -0600 (CST)
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
	each
In-Reply-To: <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
References: <1546526275247-0.post@n4.nabble.com>
 <219501d4a372$5d016a00$17043e00$@skno.by>
 <1546530037095-0.post@n4.nabble.com>
 <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
Message-ID: <1546531549120-0.post@n4.nabble.com>

Sorry Guys, im not trying to start a witch hunt, Im just trying to understand
if squid alone or with squidguard or other plugin is able to do this:

- Blacklist all websites
- Allow a whitelist for "user1" 
- Allow a different whitelist for "user2" and so on (whitelist3 for user3,
whitelist4 for user4...)
- And have a whitelist for everyone, logged users and not logged ones. 
(i have to block all URLs, http and https)

Dont care about paid products... just trying to understand if im on the
correct path or trying to configure squid with these kind of rules is
imposible. Im new at squid and i been triying for 3 days already to
configure it this way with no success.

Thanks in advance
Tux



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Jan  3 16:12:46 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 3 Jan 2019 09:12:46 -0700
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
 each
In-Reply-To: <1546526275247-0.post@n4.nabble.com>
References: <1546526275247-0.post@n4.nabble.com>
Message-ID: <050eb81a-350d-5f2d-7ff7-1a360aa1fc6b@measurement-factory.com>

On 1/3/19 7:37 AM, stressedtux wrote:
> i need a hand to understand if it is possible to configure the proxy a
> particular way. 
> 
> Im needing to configure the proxy to allow at the same time:
> 
> - a whitelist of sites that anyone that uses the proxy could use without
> login
> - and in addition to that i need to have specific ACLs for different
> authenticated users. 
> 
> I need to control both http and https connections to external sites. I can
> use sslbump but im having hard time configuring sslbump with proxy_auth, and
> on top of that, i need different acl whitelists for different users.
> 
> Is this kind of configuration possible?

Yes, I believe that all of the above is possible in principle. If you
need help with specific configurations/ACLs, I suggest starting with the
simplest set of specific use cases and posting your best configuration
snippet that does not work, while explaining why you think it does not work.

You cannot authenticate HTTP inside bumped connections, but I do not
think you actually need that.

Alex.


From rousskov at measurement-factory.com  Thu Jan  3 16:34:34 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 3 Jan 2019 09:34:34 -0700
Subject: [squid-users] Caching mirrored origin server
In-Reply-To: <acf0ba4e5bed59057a4f56b3b29ed7ee@jfcarter.net>
References: <acf0ba4e5bed59057a4f56b3b29ed7ee@jfcarter.net>
Message-ID: <f9d0e25f-f6a2-4e16-a0e6-91d68e3620b6@measurement-factory.com>

On 1/2/19 3:01 PM, jimc wrote:
> I'm using squid-4.4-2.1.x86_64 from OpenSuSE Tumbleweed.? My goal is
> that when doing periodic software updates. each host in my department
> will contact my proxy to obtain the new metadata and packages (SuSE has
> a syntax for this); the proxy will download each file only once.? This
> sounds like pretty standard Squid operation, but there's a gross botfly
> in the ointment: the origin servers return 302 Found, each time
> redirecting to a different mirror, and with "normal" configuration this
> result is passed back to the client which makes a new connection (via
> the proxy) to that mirror, but the retrieved file will likely never be
> accessed again from that mirror.

The default solution for mapping many URLs to a single cache hit is the
store_id helper. That solution is only applicable to URLs that produce
the same content regardless of the URL from the set.

  * https://wiki.squid-cache.org/Features/StoreID
  * http://www.squid-cache.org/Doc/config/store_id_program/


> mirrors come and go, and curating the mirror
> selection turned out to be a reliability problem, so I gave it up.

If there is a common pattern to all mirrors for a given URL, then
store_id can help. You would still be responsible for curating the URL
mapping/patterns, of course, but it may be (more) manageable.

There is no built-in "follow redirect but cache under the original URL"
feature in Squid, probably because such a feature would result in
serving wrong responses in many typical cases. With store_id, the
decision to map URLs and the headaches/risks of doing so are all on your
side.

With some Squid development work, the missing feature can be implemented
on top of the existing adaptation interfaces and the core store_id
functionality, but nobody has done that so far IIRC.


HTH,

Alex.


From dweimer at dweimer.net  Thu Jan  3 16:48:19 2019
From: dweimer at dweimer.net (Dean E. Weimer)
Date: Thu, 03 Jan 2019 10:48:19 -0600
Subject: [squid-users] Web Socket Support For Reverse Proxy
Message-ID: <adb7a9ca0632ed7776eab41a2c9d88d6@dweimer.net>

I am running Squid as a reverse proxy for several internally hosted 
websites, some HTTP and some HTTPS with wild card cerrtificate. We have 
recently setup a Atlassian Confluence Server, and are unable to edit any 
documents through the reverse proxy. On inspection of client web console 
logs we are receiving the following error.

failed: Error during WebSocket handshake: Unexpected response code: 200

I have been searching the Squid documentation and can't find anything on 
web sockets. Is it not supported in reverse proxy mode?

Currently running Squid 4.3, the cache peer for the specific server is.

cache_peer 10.20.10.25 parent 8490 0 ssl no-query no-digest originserver 
name=confluence_parent sslcapath=/usr/local/share/certs 
sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on proxy-only
cache_peer_access confluence_parent allow CONFLUENCE SSL
cache_peer_access confluence_parent deny all

The confluence server is configured to use a proxy, and is aware that it 
is there. There instructions only discuss settings specific to Nginx and 
Apache, in both cases the Confluence connector is the same the 
difference is the settings for Nginx and Apache.

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From bruno.larini at riosoft.com.br  Thu Jan  3 17:00:50 2019
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Thu, 3 Jan 2019 15:00:50 -0200
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
 each
In-Reply-To: <1546526275247-0.post@n4.nabble.com>
References: <1546526275247-0.post@n4.nabble.com>
Message-ID: <dfe3a95d-5085-c18a-c709-f0d662e73d81@riosoft.com.br>

Em 03/01/2019 12:37, stressedtux escreveu:
> Hi guys!
>
> i need a hand to understand if it is possible to configure the proxy a
> particular way.
>
> Im needing to configure the proxy to allow at the same time:
>
> - a whitelist of sites that anyone that uses the proxy could use without
> login
> - and in addition to that i need to have specific ACLs for different
> authenticated users.
>
> I need to control both http and https connections to external sites. I can
> use sslbump but im having hard time configuring sslbump with proxy_auth, and
> on top of that, i need different acl whitelists for different users.
>
> Is this kind of configuration possible? Just trying to understand if im on a
> dead road :D
>
> Thanks in advanced!
> Tux

This link helped me a lot with ssl_bump: 
https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
To bump intercepted (implicit) https connections, you would need to add 
'https_port' with 'intercept' option to another REDIRECTed port, 
considering the example from the link. To 'bump' connections you need to 
add your self-signed certificate to the clients' trusted store, or else 
they will always receive certificate errors in their browsers.

Keep in mind that you don't need to use ssl_bump to block/allow https 
sites in most cases (in explicit mode, for example). Bumping is most 
useful if you're willing to audit the users' access in a deeper level or 
cache web content from https websites.
If setting up the clients is a problem to you, use 'splice' instead. It 
won't open the https traffic for you though.

The users and white-list part is a very common setup, there are lots of 
examples out there.

-Bruno



From marcus.kool at urlfilterdb.com  Thu Jan  3 17:01:45 2019
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 3 Jan 2019 15:01:45 -0200
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
 each
In-Reply-To: <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
References: <1546526275247-0.post@n4.nabble.com>
 <219501d4a372$5d016a00$17043e00$@skno.by>
 <1546530037095-0.post@n4.nabble.com>
 <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
Message-ID: <2c8c5b01-4b5a-9ca9-65d5-6e2c5187ce1b@urlfilterdb.com>

For those who do not know it yet: ufdbGuard is free.

ufdbGuard supports user-defined URL databases, 3rd party plain-text URL databases, and a commercial database from www.urlfilterdb.com.

Marcus


On 03/01/2019 13:45, Benjamin E. Nichols wrote:
> Why are you asking support questions about a commercial product, on the squid proxy email users list?
>
> On 1/3/2019 9:40 AM, stressedtux wrote:
>> With ufdbguard is possible to allow one user to have an acl and other user a
>> different acl? Im trying to completly block access to inet except for what i
>> should allow.
>>
>>
>>
>> -- 
>> Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>


From marcus.kool at urlfilterdb.com  Thu Jan  3 17:09:29 2019
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 3 Jan 2019 15:09:29 -0200
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
 each
In-Reply-To: <1546531549120-0.post@n4.nabble.com>
References: <1546526275247-0.post@n4.nabble.com>
 <219501d4a372$5d016a00$17043e00$@skno.by>
 <1546530037095-0.post@n4.nabble.com>
 <a38b61a8-be0e-f49d-6dd8-6abb0616b233@squidblacklist.org>
 <1546531549120-0.post@n4.nabble.com>
Message-ID: <e3dae87d-ee65-fe63-a565-e26b15944599@urlfilterdb.com>

ufdbGuard supports blacklists, whitelists, large numbers of whitelists, users and acls.

The configuration file is intuitive and if the Reference Manual does not explain everything, one can also write to the support desk of URLfilterDB or the ufdbguard mailing list.

Just for the record, I am biased since I am the author of ufdbGuard.

Marcus



On 03/01/2019 14:05, stressedtux wrote:
> Sorry Guys, im not trying to start a witch hunt, Im just trying to understand
> if squid alone or with squidguard or other plugin is able to do this:
>
> - Blacklist all websites
> - Allow a whitelist for "user1"
> - Allow a different whitelist for "user2" and so on (whitelist3 for user3,
> whitelist4 for user4...)
> - And have a whitelist for everyone, logged users and not logged ones.
> (i have to block all URLs, http and https)
>
> Dont care about paid products... just trying to understand if im on the
> correct path or trying to configure squid with these kind of rules is
> imposible. Im new at squid and i been triying for 3 days already to
> configure it this way with no success.
>
> Thanks in advance
> Tux
>
>
>
> --
> Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Thu Jan  3 17:19:51 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 3 Jan 2019 10:19:51 -0700
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
In-Reply-To: <1546456758259-0.post@n4.nabble.com>
References: <1545946245329-0.post@n4.nabble.com>
 <cdd3e8bf-6a64-91d6-40b6-6e05133d851b@measurement-factory.com>
 <1546456758259-0.post@n4.nabble.com>
Message-ID: <d52b3cab-eadb-d8d7-facf-60b131298c85@measurement-factory.com>

On 1/2/19 12:19 PM, johnr wrote:
> I can open a bug if you think I should/can based on the backtrace here:

I think that you should/can, but it is your call. If you decide to open
a bug report, please also post gdb output of the following commands
(sanitized if needed):

-------------
set print pretty
set print-static-members off

bt full

frame 0
print cert
print *cert.raw
print *sig
print s

frame 1
print properties

frame 2
print request_message
-------------


Thank you,

Alex.


> Core was generated by `(security_file_certgen) -s
> /usr/local/squid/var/cache/squid/ssl_db -M 4MB'.
> Program terminated with signal SIGSEGV, Segmentation fault.
> 
> 
> Dec 28 22:15:20 vagrant-ubuntu-trusty-64 kernel: [ 4314.045153]
> security_file_c[2876]: segfault at 100000000 ip 000000000040d5d8 sp
> 00007ffd810b4010 error 4 in security_file_certgen[400000+13000]
> 
> Program received signal SIGSEGV, Segmentation fault.
> printX509Signature (out="", cert=...) at gadgets.cc:225
> 225   gadgets.cc: No such file or directory.
> (gdb) bt
> #0  printX509Signature (out="", cert=...) at gadgets.cc:225
> #1  Ssl::OnDiskCertificateDbKey (properties=...) at gadgets.cc:238
> #2  0x0000000000405f8d in processNewRequest (fs_block_size=4096,
> max_db_size=4194304, 
>     db_path="/var/lib/squid-pnr-proxy/ssl_db_v4", request_message=...) at
> security_file_certgen.cc:195
> #3  main (argc=<optimized out>, argv=0x7fffffffe608) at
> security_file_certgen.cc:345


From jimc at jfcarter.net  Thu Jan  3 19:39:33 2019
From: jimc at jfcarter.net (jimc)
Date: Thu, 03 Jan 2019 11:39:33 -0800
Subject: [squid-users] Caching mirrored origin server
In-Reply-To: <f9d0e25f-f6a2-4e16-a0e6-91d68e3620b6@measurement-factory.com>
References: <acf0ba4e5bed59057a4f56b3b29ed7ee@jfcarter.net>
 <f9d0e25f-f6a2-4e16-a0e6-91d68e3620b6@measurement-factory.com>
Message-ID: <95a9f54378bf4ce3f53e28f7c331ec25@jfcarter.net>

On 2019-01-03 08:34, Alex Rousskov wrote:
> The default solution for mapping many URLs to a single cache hit is the
> store_id helper. That solution is only applicable to URLs that produce
> the same content regardless of the URL from the set.
> 
>   * https://wiki.squid-cache.org/Features/StoreID
>   * http://www.squid-cache.org/Doc/config/store_id_program/

@Alex, thanks for the pointer to the StoreID feature.  I'll try to adapt
one of the sample programs in the docs.  The database of distro mirror
patterns mentioned there will be very helpful.

-- 
James F. Carter   Email: jimc at jfcarter.net
Web: http://www.math.ucla.edu/~jimc (q.v. for PGP key)


From stressedtux at hotmail.com  Thu Jan  3 20:26:24 2019
From: stressedtux at hotmail.com (stressedtux)
Date: Thu, 3 Jan 2019 14:26:24 -0600 (CST)
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
	each
In-Reply-To: <dfe3a95d-5085-c18a-c709-f0d662e73d81@riosoft.com.br>
References: <1546526275247-0.post@n4.nabble.com>
 <dfe3a95d-5085-c18a-c709-f0d662e73d81@riosoft.com.br>
Message-ID: <1546547184049-0.post@n4.nabble.com>

Ty guys. I think i was finally able to solve it. 

For those who have the same problem, this was my solution:



#### Proxy Port
http_port 80 


################################
#### BEGIN  
################################

##  ACLs localnet
acl localnet src XXX.XXX.0.0/16	# My Network1
acl localnet src XXX.XXX.0.0/16	# My Network2

# ACLs Ports
acl http proto http
acl port_80 port 80
acl port_443 port 443
acl CONNECT method CONNECT


### 
auth_param basic program /usr/lib64/squid/basic_ncsa_auth
/etc/squid/passwords
auth_param basic realm proxy
acl authenticated proxy_auth REQUIRED

#########

#### Auth parameters
auth_param basic program /usr/lib64/squid/basic_ncsa_auth
/etc/squid/passwords
auth_param basic realm proxy
acl authenticated proxy_auth REQUIRED


##### Rules for global users, non-authenticated - "Global Whitelist"
acl global_whitelist dstdomain "/etc/squid/global_whitelist"
http_access allow http localnet port_80 global_whitelist
http_access allow CONNECT localnet port_443 global_whitelist


##### Rule for autenticated user stressedtux
acl login_stressedtux proxy_auth stressedtux
acl sites_stressedtux dstdomain "/etc/squid/sites_stressedtux.txt"
http_access allow http port_80 localnet sites_stressedtux login_stressedtux
http_access allow CONNECT port_443 localnet sites_stressedtux
login_stressedtux


##### Rules for autenticated users of "group" usrgrp1
acl login_usrgrp proxy_auth "/etc/squid/list_users_usrgrp1.txt"
acl sites_usrgrp dstdomain "/etc/squid/sites_usrgrp1.txt"
http_access allow http port_80 localnet sites_usrgrp login_usrgrp
http_access allow CONNECT port_443 localnet sites_usrgrp login_usrgrp

##### Block everything else
http_access deny all


################################
#### END
################################







--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From stressedtux at hotmail.com  Thu Jan  3 20:28:58 2019
From: stressedtux at hotmail.com (stressedtux)
Date: Thu, 3 Jan 2019 14:28:58 -0600 (CST)
Subject: [squid-users] Sslbump with multiple users and multiple ACLs for
	each
In-Reply-To: <1546547184049-0.post@n4.nabble.com>
References: <1546526275247-0.post@n4.nabble.com>
 <dfe3a95d-5085-c18a-c709-f0d662e73d81@riosoft.com.br>
 <1546547184049-0.post@n4.nabble.com>
Message-ID: <1546547338773-0.post@n4.nabble.com>

btw it was not necessary sslbump nor a plugin for my case. Ty all! 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Jan  4 07:39:13 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 Jan 2019 20:39:13 +1300
Subject: [squid-users] Web Socket Support For Reverse Proxy
In-Reply-To: <adb7a9ca0632ed7776eab41a2c9d88d6@dweimer.net>
References: <adb7a9ca0632ed7776eab41a2c9d88d6@dweimer.net>
Message-ID: <f47c0601-f70b-61c9-179a-6ce9364995d3@treenet.co.nz>

On 4/01/19 5:48 am, Dean E. Weimer wrote:
> I am running Squid as a reverse proxy for several internally hosted
> websites, some HTTP and some HTTPS with wild card cerrtificate. We have
> recently setup a Atlassian Confluence Server, and are unable to edit any
> documents through the reverse proxy. On inspection of client web console
> logs we are receiving the following error.
> 
> failed: Error during WebSocket handshake: Unexpected response code: 200
> 

The client software does not support the WebSockets fallback
mechanism(s) properly.


> I have been searching the Squid documentation and can't find anything on
> web sockets. Is it not supported in reverse proxy mode?

Indeed. 200 status is a "success" response from the origin. The data
requested is still being delivered, just with HTTP instead of WebSockets.


> 
> Currently running Squid 4.3, the cache peer for the specific server is.
> 
> cache_peer 10.20.10.25 parent 8490 0 ssl no-query no-digest originserver
> name=confluence_parent sslcapath=/usr/local/share/certs
> sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on proxy-only
> cache_peer_access confluence_parent allow CONFLUENCE SSL
> cache_peer_access confluence_parent deny all
> 
> The confluence server is configured to use a proxy, and is aware that it
> is there.

That could be the problem then. Why does the *server* need configuring
to *use* a proxy?

Clients use a proxy to fetch requests. Servers *receive* requests from a
proxy.


> There instructions only discuss settings specific to Nginx and
> Apache, in both cases the Confluence connector is the same the
> difference is the settings for Nginx and Apache.
> 


Amos


From info at schroeffu.ch  Fri Jan  4 10:38:59 2019
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Fri, 04 Jan 2019 10:38:59 +0000
Subject: [squid-users] Need help about ICAP scan timeout/max file size for
	big files
Message-ID: <7e8169f092321085e591faaa7642d589@schroeffu.ch>

Hi all,

i am trying to solve the problem, that SQUID is caching all the big files (for example 1GB) before sending them to the client, but the connected ICAP virus scanner is configured with max_file_size 2MB and scan_timeout 5 seconds. So all bigger files, or longer scanning times, should result in "clean" state from the icap virus scanner.

I am running antivirus FSIGK (F-Secure Internet GateKeeper) as an ICAP daemon connected to Squid with this configuration:

#ICAP
icap_enable on
acl domains_dont_icapscan url_regex -i "/etc/squid/ka/domains_dont_icapscan.acl"
acl audio rep_mime_type -i ^(audio/x-mpegurl|audio/mpeg|audio/ogg|audio/aac|audio/mp3)$

icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/request
adaptation_access service_req allow !domains_dont_icapscan
icap_service service_resp respmod_precache bypass=1 icap://127.0.0.1:1344/response
adaptation_access service_resp allow !domains_dont_icapscan !audio

Detecting viruses is working, but downloading large files is a huge problem. Squid is downloading them completely first into the servers memory and caching them, before sending them to the client. Its not stop scanning & caching after 2MB/5Seconds. When downloading big files (f.e. 1gb) the browser just does nothing but waiting a long time, because squid is downloading and caching 1gb before forward to client.

I tried change respmod_precache to respmod_postcache but it seems not to be implemented yet, with respmod_postcache fsigk icap log is empty , no virus detection works anymore.
I have a test-virus-file with 100MB (https://schroeffu.ch/100mbrandomvirus_begin.txt (https://schroeffu.ch/100mbrandomvirus_begin.txt) eicar+randomcontent) and the virus is detected by fsigk with settings max_scan_size=104400136 / scan_timeout=9000 , change them to max_scan_size=2147483 (2mb) and scan_timeout=5 (5Seconds) the virus is correctly not detected anymore, but, squid still does cache the 100mb before sending to the client.

How can I configure the ICAP Service to truly let bigger files/longer scan times through the icap service marked as "clean"?

Thanks for any help!
Schroeffu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190104/d495aa1e/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan  4 12:14:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Jan 2019 01:14:18 +1300
Subject: [squid-users] Need help about ICAP scan timeout/max file size
 for big files
In-Reply-To: <7e8169f092321085e591faaa7642d589@schroeffu.ch>
References: <7e8169f092321085e591faaa7642d589@schroeffu.ch>
Message-ID: <8dd230a5-b951-056b-372b-26f72698b64a@treenet.co.nz>

On 4/01/19 11:38 pm, info at schroeffu.ch wrote:
> 
> Hi all,
> 
> i am trying to solve the problem, that SQUID is caching all the big
> files (for example 1GB) before sending them to the client, but the
> connected ICAP virus scanner is configured with max_file_size 2MB and
> scan_timeout 5 seconds. So all bigger files, or longer scanning times,
> should result in "clean" state from the icap virus scanner.
> 
> I am running antivirus FSIGK (F-Secure Internet GateKeeper) as an ICAP
> daemon connected to Squid with this configuration:
> 
> #ICAP
> icap_enable on
> acl domains_dont_icapscan url_regex -i
> "/etc/squid/ka/domains_dont_icapscan.acl"
> acl audio rep_mime_type -i
> ^(audio\/x-mpegurl|audio\/mpeg|audio\/ogg|audio\/aac|audio/mp3)$
> 
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/request
> adaptation_access service_req allow !domains_dont_icapscan

> icap_service service_resp respmod_precache bypass=1
> icap://127.0.0.1:1344/response

> adaptation_access service_resp allow !domains_dont_icapscan !audio

The above line says that everything which is not *both* an audio file
and on your dont-scan list does get scanned.

In other words, you can only whitelist audio responses.


> 
> Detecting viruses is working, but downloading large files is a huge
> problem. Squid is downloading them completely first into the servers
> memory and caching them,> before sending them to the client. Its not stop
> scanning & caching after 2MB/5Seconds.

Squid is not scanning. The whole point of ICAP is that something *else*
is doing the content manipulation/scanning.

Squid is just a relaying the content octets blindly between the various
agents using it.


> When downloading big files (f.e.
> 1gb) the browser just does nothing but waiting a long time, because
> squid is downloading and caching 1gb before forward to client.

The amount of memory used will depend on other config settings which you
have not shown. Please provide all your config so we can analyze the
problem in full context of what is going on around these ICAP services.


> 
> I tried change respmod_precache to respmod_postcache but it seems not to
> be implemented yet, with respmod_postcache fsigk icap log is empty , no
> virus detection works anymore.

Correct. Post-cache ICAP hooks are not supported/implemented by Squid.

If scanning it once (pre-cache) is slow then scanning it per-fetch / N
times (aka post-cache) would be at least N times slower.



> I have a test-virus-file with 100MB
> (https://schroeffu.ch/100mbrandomvirus_begin.txt eicar+randomcontent)
> and the virus is detected by fsigk with settings max_scan_size=104400136
> / scan_timeout=9000 , change them to max_scan_size=2147483 (2mb) and
> scan_timeout=5 (5Seconds) the virus is correctly not detected anymore,

These sound like config setting for the scanning operation. None of that
has any relevance to Squid.

The file is a TXT file not an audio file, so as far as Squid can tell it
is always to be delivered to the scanner.


> but, squid still does cache the 100mb before sending to the client.
> 
> How can I configure the ICAP Service to truly let bigger files/longer
> scan times through the icap service marked as "clean"?


What you describe sounds like problems identified with ClamAV early on.
It turned out clam was storing the object to disk and waiting for it to
complete before scanning and providing any output to Squid.

Please check whether the scanner you are using does that type of behaviour.


Amos


From dweimer at dweimer.net  Fri Jan  4 13:41:27 2019
From: dweimer at dweimer.net (Dean E. Weimer)
Date: Fri, 04 Jan 2019 07:41:27 -0600
Subject: [squid-users] Web Socket Support For Reverse Proxy
In-Reply-To: <f47c0601-f70b-61c9-179a-6ce9364995d3@treenet.co.nz>
References: <adb7a9ca0632ed7776eab41a2c9d88d6@dweimer.net>
 <f47c0601-f70b-61c9-179a-6ce9364995d3@treenet.co.nz>
Message-ID: <1b15383eb84ea73bb10406cdb2ae986a@dweimer.net>

On 2019-01-04 1:39 am, Amos Jeffries wrote:
> On 4/01/19 5:48 am, Dean E. Weimer wrote:
>> I am running Squid as a reverse proxy for several internally hosted
>> websites, some HTTP and some HTTPS with wild card cerrtificate. We 
>> have
>> recently setup a Atlassian Confluence Server, and are unable to edit 
>> any
>> documents through the reverse proxy. On inspection of client web 
>> console
>> logs we are receiving the following error.
>> 
>> failed: Error during WebSocket handshake: Unexpected response code: 
>> 200
>> 
> 
> The client software does not support the WebSockets fallback
> mechanism(s) properly.

Searching for Fallback information led me to find part of the issue.


It appears that the Confluence web application is using synchrony to 
handle collaborative editing they wrote their own proxy to proxy the web 
socket requests to the synchrony app from within their app. That proxy 
has a known issue of not supporting the fallback feature. Apparently 
this was reported as a major bug in release notes of the beta version of 
the first 6.0 release and hasn't been addressed. The current 
documentation lists it as supported by default and tells you how to 
disable it if for some reason you don't want to allow fallback support.

<https://jira.atlassian.com/browse/CONFSERVER-44250>

XHR fallback has known issues when synchrony service is accessed via the 
synchrony-proxy web application.

Workaround is to ensure that browser traffic goes direct to synchrony 
instead of being routed via the confluence synchrony-proxy web 
application.


>> I have been searching the Squid documentation and can't find anything 
>> on
>> web sockets. Is it not supported in reverse proxy mode?
> 
> Indeed. 200 status is a "success" response from the origin. The data
> requested is still being delivered, just with HTTP instead of 
> WebSockets.
> 
> 
>> 
>> Currently running Squid 4.3, the cache peer for the specific server 
>> is.
>> 
>> cache_peer 10.20.10.25 parent 8490 0 ssl no-query no-digest 
>> originserver
>> name=confluence_parent sslcapath=/usr/local/share/certs
>> sslflags=DONT_VERIFY_PEER login=PASSTHRU front-end-https=on proxy-only
>> cache_peer_access confluence_parent allow CONFLUENCE SSL
>> cache_peer_access confluence_parent deny all
>> 
>> The confluence server is configured to use a proxy, and is aware that 
>> it
>> is there.
> 
> That could be the problem then. Why does the *server* need configuring
> to *use* a proxy?
> 
> Clients use a proxy to fetch requests. Servers *receive* requests from 
> a
> proxy.

The proxy configuration on the server tells it the hostname and port 
used on the proxy to properly rewrite the links.

> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Thanks,
    Dean E. Weimer
    http://www.dweimer.net/


From squid3 at treenet.co.nz  Fri Jan  4 14:37:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Jan 2019 03:37:47 +1300
Subject: [squid-users] [squid-announce] Squid-4.5 is available
Message-ID: <d6159d58-f75b-1af7-4690-5819cd465188@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.5 release!


This release is a security and bug fix release resolving several issues
found in the prior Squid releases.


The major changes to be aware of:

* Bug 4253: ssl_bump prevents access to some web contents

The SSL-Bump initial implementation was entangled with reverse-proxy
handling of decrypted HTTPS messages. This was a mistake we have been
reversing across the 3.5 and 4 cycles.

With this release SSL-Bump traffic handling is no longer tied to
reverse-proxy mode. As a result complications with ESI and
Surrogate-Control header handling have finally been resolved.


* Redesign forward_max_tries to count TCP connection attempts

This release includes an overhaul of the counting for HTTP message
forwarding and re-send attempts. This has an impact on how long it takes
Squid to detect and report connection errors to clients, persistent
connection overload recovery and detection of DEAD peer states.

The documentation for forward_max_tries and connect_retries has been
updated to more clearly specify the current expected behaviour.

Any users with systems tuned to optimize these behaviours should read
the updated squid.conf documentation and check their tuning after
upgrade to this release or any later.


* Fix client_connection_mark ACL handling of clientless transactions

This bug shows up as crashes when a client_connection_mark or
clientside_mark type ACL is used for access control. From this release
transactions without a client TCP connection will now produce a
non-match result when this ACL is tested.


* Multiple NetDB behaviour updates

NetDB state was not being recorded for connections to peers using TLS
nor for CONNECT tunnels. With the growth of HTTPS in recent times these
are increasingly important to optimize.

This release will now ping and record the latency information for these
connections to aid with optimizing connection setup of future transactions.


* The logformat code %>handshake is added

This code allows logging of initial bytes received for many protocols
to allow better debugging of unknown-protocol issues and external ACL
decision making.


* Use pkg-config for detecting libxml2

This release adds support for auto-detection of libxml2 location using
the pkg-config tools at build time. This may affect users of OS placing
libraries at a location outside the FHS layout. For example
cross-building or multi-architecture systems.

Note that support for custom PATH parameter is not yet implemented for
the --with-libxml2 build option. It is planned but did not make this
release. The pkg-config environment variables may be used for that if
necessary.



  All users of Squid-4 with SSL-Bump functionality are urged to upgrade
as soon as possible.

  All other users of Squid-4 are encouraged to upgrade as time permits.

  All users of Squid-3 are encouraged to upgrade where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From rousskov at measurement-factory.com  Fri Jan  4 19:43:32 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 4 Jan 2019 12:43:32 -0700
Subject: [squid-users] Need help about ICAP scan timeout/max file size
 for big files
In-Reply-To: <7e8169f092321085e591faaa7642d589@schroeffu.ch>
References: <7e8169f092321085e591faaa7642d589@schroeffu.ch>
Message-ID: <cd3c4ba9-7bf1-ca6c-f6e1-ff1e919142e3@measurement-factory.com>

On 1/4/19 3:38 AM, info at schroeffu.ch wrote:
> How can I configure the ICAP Service to truly let bigger files/longer
> scan times through the icap service marked as "clean"?

Which of the following questions are you asking?

1. How to configure Squid to never send huge files to your ICAP service?

2. How to configure your ICAP service to speed up huge-file decisions?

3. How to configure Squid to send huge files to your ICAP service
   without storing them in Squid memory or in Squid disk cache?

For all questions, do the huge files that you are dealing with have an
HTTP Content-Length response header?

And, if it is question #2, does your ICAP service support ICAP Preview
mode? Have you enabled ICAP previews in your Squid configuration?

Alex.


From bankarsomnath2011 at gmail.com  Tue Jan  8 04:54:27 2019
From: bankarsomnath2011 at gmail.com (bankar somnath)
Date: Tue, 8 Jan 2019 10:24:27 +0530
Subject: [squid-users] could not bind to binddn 'Can't contact LDAP server'
Message-ID: <CAL1SvTxddRb7ErmcjtM-0-fw-d6FNgGdzfCmU7O0+tYxYiTbcA@mail.gmail.com>

Hi Team,

I found below warning in /var/log/squid/cache.log file repeatedly. As
checked LDAP server is reachable from squid for 389 port. Also DNS entry is
correct and nslookup is working fine to DNS, please help to fix below
issue.

squid_ldap_auth: WARNING, could not bind to binddn 'Can't contact LDAP
server'
squid_ldap_auth: WARNING, could not bind to binddn 'Can't contact LDAP
server'
squid_ldap_auth: WARNING, could not bind to binddn 'Can't contact LDAP
server'
squid_ldap_auth: WARNING, could not bind to binddn 'Can't contact LDAP
server'

Regards,
Som.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190108/122b7bb0/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan  8 08:25:38 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Jan 2019 21:25:38 +1300
Subject: [squid-users] could not bind to binddn 'Can't contact LDAP
 server'
In-Reply-To: <CAL1SvTxddRb7ErmcjtM-0-fw-d6FNgGdzfCmU7O0+tYxYiTbcA@mail.gmail.com>
References: <CAL1SvTxddRb7ErmcjtM-0-fw-d6FNgGdzfCmU7O0+tYxYiTbcA@mail.gmail.com>
Message-ID: <3e49d068-4929-fdc1-a349-c5fdd9df1d88@treenet.co.nz>

On 8/01/19 5:54 pm, bankar somnath wrote:
> Hi Team,
> 
> I found below warning in /var/log/squid/cache.log file repeatedly. As
> checked LDAP server is reachable from squid for 389 port. Also DNS entry
> is correct and nslookup is working fine to DNS, please help to fix below
> issue.
> 
> squid_ldap_auth: WARNING, could not bind to binddn 'Can't contact LDAP
> server'


FYI: That helper name was deprecated since Squid-3.2. Please upgrade if
you can.

Back to your problem;
 What OS are you using?
 What Squid version are you using?
 What auth_param settings do you have in your squid.conf? (in order)

Amos


From eliezer at ngtech.co.il  Tue Jan  8 10:02:44 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Tue, 8 Jan 2019 12:02:44 +0200
Subject: [squid-users] Squid 4.1 Error negotiating SSL connection
In-Reply-To: <e973f4ab-3137-c391-1982-7dc412aad506@treenet.co.nz>
References: <007e01d4132a$e5741050$b05c30f0$@yahoo.com.ar>
 <e973f4ab-3137-c391-1982-7dc412aad506@treenet.co.nz>
Message-ID: <00bd01d4a739$4ebc6150$ec3523f0$@ngtech.co.il>

So with 4.5 we are still waiting for openssl to advance into TLS 1.3, right?
Can the thread writer add a list of these domains which can help others?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, July 4, 2018 07:21
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 4.1 Error negotiating SSL connection

On 04/07/18 12:06, Julian Perconti wrote:
> Hi all,
> 
>  
> 
> I have installed squid 4.1 on debian 9 with openssl 1.1.0f on
> transparent mode.
> 
>  
> 
> I need to know how to track this error: (debbuging options is almost
> impossible i mean examine the FD, etc.)
> 

The SSL-Bump activity is fairly complex at times and involves many
different layers and components. So an ALL,9 or ALL,7 debug log may be
necessary to trace the actions.

>  
> 
> kid1| Error negotiating SSL connection on FD 19:
> error:00000001:lib(0):func(0):reason(1) (1/-1)
> 
> 


Those annoyingly opaque error messages are produced by your OpenSSL library.

Other programs showing that same string apparently are negotiating
protocol version for the messaging layer or handshake format which are
incompatible with the choice of ciphers. eg SSLv2 message syntax with
TLS ciphers, or SSLv3 message syntax with  TLS/1.2-only ciphers.

Since you have done the cipher test, it may be the SSLv2 issue or some
TLS extension being attempted.


If cache.log is too obscure a packet trace with wireshark may be less
so. The clear-text part of TLS at the start should have better hints
about the issue, whatever it is.


 
> 
> There are a lot of them in cache.log when mobile devices uses
> (unsuccefully) apps like instagram/Pinterest/Facebook/twitter, etc.
> 
>  
> 
> Neither is a ?cipher-out? problem because I just tried:
> tls_outgoing_options cipher=ALL (only for testing)
> 

This test is mistaken.

"cipher=ALL" and "options=ALL" actually mean to actively *enable* lots
of things OpenSSL would normally disable. This still counts as
restriction, because only things compatible with the most obsolete or
broken cipher/option can be negotiated.

A correct test would be to _remove_ the cipher=* option entirely from
your config and see what changes.

With no manual restrictions the issues are then limited to natural
differences in OpenSSL version between client and Squid.


> 
> From any PC those sites works well. So there is not a certificate
> missing problem.
> 

When SSL-Bump is done crypto issues are the union of configured
capabilities at client (PC), proxy (Squid), server - plus the 3
particular crypto libraries on each of those uses. So 6 possible points
of failure, all affecting each other.

I find it is often a LOT easier (and more successful) to look at the TLS
handshake itself and see what is actually happening. Then figure out
from there what needs tuning to work around it.


> 
> Here a copy of most relevant config:
> 
>  
> 
> =================CFG==================
> 
>  
> 
> http_port 3128
> 
> http_port 3129 intercept
> 
> https_port 3130 intercept ssl-bump \
> 
>   cert=/etc/squid/ssl_cert/squid4ssl.pem \
> 
>   key=/etc/squid/ssl_cert/squid4ssl.pem \
> 
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
>  
> 
> sslcrtd_program /lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
> 
>  
> 
> tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt
> 
> tls_outgoing_options cafile=/etc/squid/ssl_cert/cabundle.pem
> 
> tls_outgoing_options options=NO_SSLv3
> 

This NO_SSLv3 may be part of issue. AFAIK when SSLv3 compatibility is no
longer required the latest OpenSSL is able to move to pure TLS message
syntax which has a few usually very minor differences which TLS/1.3 uses.

The services you mention are the ones IME most likely to be adopting
TLS/1.3 already when clients like your Squid accept it. Which is where
PC vs Squid library differences can lead to drastically different
visible outcomes.


> tls_outgoing_options
> cipher=ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
> 


HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From huaraz at moeller.plus.com  Tue Jan  8 11:55:42 2019
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Tue, 8 Jan 2019 11:55:42 -0000
Subject: [squid-users] [squid-announce] Squid-4.5 is available
In-Reply-To: <d6159d58-f75b-1af7-4690-5819cd465188__18406.7017086365$1546614300$gmane$org@treenet.co.nz>
References: <d6159d58-f75b-1af7-4690-5819cd465188__18406.7017086365$1546614300$gmane$org@treenet.co.nz>
Message-ID: <q122vt$nph$1@blaine.gmane.org>

Hi Amos,

  Is there any reason that kerberos_sid_group is not included in the tar ?

Thank you
Markus

"Amos Jeffries"  wrote in message 
news:d6159d58-f75b-1af7-4690-5819cd465188__18406.7017086365$1546614300$gmane$org at treenet.co.nz...

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.5 release!


This release is a security and bug fix release resolving several issues
found in the prior Squid releases.


The major changes to be aware of:

* Bug 4253: ssl_bump prevents access to some web contents

The SSL-Bump initial implementation was entangled with reverse-proxy
handling of decrypted HTTPS messages. This was a mistake we have been
reversing across the 3.5 and 4 cycles.

With this release SSL-Bump traffic handling is no longer tied to
reverse-proxy mode. As a result complications with ESI and
Surrogate-Control header handling have finally been resolved.


* Redesign forward_max_tries to count TCP connection attempts

This release includes an overhaul of the counting for HTTP message
forwarding and re-send attempts. This has an impact on how long it takes
Squid to detect and report connection errors to clients, persistent
connection overload recovery and detection of DEAD peer states.

The documentation for forward_max_tries and connect_retries has been
updated to more clearly specify the current expected behaviour.

Any users with systems tuned to optimize these behaviours should read
the updated squid.conf documentation and check their tuning after
upgrade to this release or any later.


* Fix client_connection_mark ACL handling of clientless transactions

This bug shows up as crashes when a client_connection_mark or
clientside_mark type ACL is used for access control. From this release
transactions without a client TCP connection will now produce a
non-match result when this ACL is tested.


* Multiple NetDB behaviour updates

NetDB state was not being recorded for connections to peers using TLS
nor for CONNECT tunnels. With the growth of HTTPS in recent times these
are increasingly important to optimize.

This release will now ping and record the latency information for these
connections to aid with optimizing connection setup of future transactions.


* The logformat code %>handshake is added

This code allows logging of initial bytes received for many protocols
to allow better debugging of unknown-protocol issues and external ACL
decision making.


* Use pkg-config for detecting libxml2

This release adds support for auto-detection of libxml2 location using
the pkg-config tools at build time. This may affect users of OS placing
libraries at a location outside the FHS layout. For example
cross-building or multi-architecture systems.

Note that support for custom PATH parameter is not yet implemented for
the --with-libxml2 build option. It is planned but did not make this
release. The pkg-config environment variables may be used for that if
necessary.



  All users of Squid-4 with SSL-Bump functionality are urged to upgrade
as soon as possible.

  All other users of Squid-4 are encouraged to upgrade as time permits.

  All users of Squid-3 are encouraged to upgrade where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From squid3 at treenet.co.nz  Tue Jan  8 14:36:38 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Jan 2019 03:36:38 +1300
Subject: [squid-users] [squid-announce] Squid-4.5 is available
In-Reply-To: <q122vt$nph$1@blaine.gmane.org>
References: <d6159d58-f75b-1af7-4690-5819cd465188__18406.7017086365$1546614300$gmane$org@treenet.co.nz>
 <q122vt$nph$1@blaine.gmane.org>
Message-ID: <00c95eaa-648f-885b-55f6-1f7fe93a80d0@treenet.co.nz>

On 9/01/19 12:55 am, Markus Moeller wrote:
> Hi Amos,
> 
> ?Is there any reason that kerberos_sid_group is not included in the tar ?
> 

That helper is in the queue for Squid-5 release, whenever that happens.

Amos


From info at schroeffu.ch  Tue Jan  8 16:46:13 2019
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Tue, 08 Jan 2019 16:46:13 +0000
Subject: [squid-users] Need help about ICAP scan timeout/max file size
 for big files
In-Reply-To: <cd3c4ba9-7bf1-ca6c-f6e1-ff1e919142e3@measurement-factory.com>
References: <cd3c4ba9-7bf1-ca6c-f6e1-ff1e919142e3@measurement-factory.com>
 <7e8169f092321085e591faaa7642d589@schroeffu.ch>
Message-ID: <5826e10ffcf1341dbc4d5e2a4e635321@schroeffu.ch>

Hi Alex (& hi Amos)

it depends on the ICAP Service. The one I am trying to use is F-Secure FSICAPD which is not working as expected.

So i compared with ClamAV C-ICAP: With ClamAV C-ICAP there is defined "MaxStreamSize 25M" as default, so after 25MB scanned by ICAP I can see with tcpdump on port 1344 "ICAP/1.0 200 OK" from ICAP to Squid which triggers the browser to start the download. Thats what i want also for F-Secure ICAP.

#ClamAV MaxStreamSize reached ICAP response:
ICAP/1.0 200 OK
Server: C-ICAP/0.4.4
Connection: keep-alive
ISTag: CI0001-1-squidclamav-10
Encapsulated: res-hdr=0, res-body=331

Unfortunately, the F-Secure ICAP is not sending this "ICAP/1.0 200 OK" after X MB or X Seconds. I am in touch with them if this is a bug, i dont know yet, they're checking that. So, if their ICAP really is not sending "ICAP/1.0 200 OK" after X Seconds/MB, can I configure SQUID with a workaround?

So, to your questions:

> 1. How to configure Squid to never send huge files to your ICAP service?

Yes, as a workaround, but how? Header of big files are usually not included.

> 2. How to configure your ICAP service to speed up huge-file decisions?

The header seems not include the file size. Here is an example of 100MB Virus File (EICAR Signature at the beginning) Header:

RESPMOD icap://127.0.0.1:1344/response ICAP/1.0
Host: 127.0.0.1:1344
Date: Fri, 04 Jan 2019 15:56:48 GMT
Encapsulated: req-hdr=0, res-hdr=434, res-body=676

GET https://schroeffu.ch/100mbrandomvirus_begin.txt HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: de,en-US;q=0.7,en;q=0.3
Accept-Encoding: gzip, deflate, br
Cookie: _pk_id.n/a.1636=5b8e9d8d8516ea65.1546604985.1.1546604985.1546604985.
Upgrade-Insecure-Requests: 1
Host: schroeffu.ch

HTTP/1.1 200 OK
Server: nginx
Date: Fri, 04 Jan 2019 15:56:48 GMT
Content-Type: text/plain
Last-Modified: Fri, 04 Jan 2019 15:31:19 GMT
Vary: Accept-Encoding
ETag: W/"5c2f7c47-61a8088"
X-Powered-By: PleskLin
Content-Encoding: gzip

The 200 OK reaches Squid after 100% of 100MB has been scanned by F-secure ICAP after 114 Seconds (!),  means, the browser is 114 Seconds doing nothing but watiting:

ICAP/1.0 200 OK
Server: F-Secure ICAP Server
ISTag: "FSAV-2019-01-02_04"
Connection: keep-alive
Expires: Fri, 04 Jan 2019 16:58:42 GMT
X-FSecure-Scan-Result: clean
X-FSecure-ORSP-FRS-Duration: 5.005693
X-FSecure-Transaction-Duration: 114.205939
X-FSecure-Versions: F-Secure Corporation Hydra/5.22 build 28/2018-12-28_01 F-Secure Corporation Aquarius/1.0 build 8/2019-01-02_04 fsavd/1.0/0148 fsicapd/1.1.277-263d28a
Encapsulated: res-hdr=0, res-body=242

> 3. How to configure Squid to send huge files to your ICAP service without storing them in Squid memory or in Squid disk cache?

No, this point we can forget.

I think best would be to configure squid, if ICAP is not able to scan the complete request in 10 seconds, skip (or mark as clean) and let browser download it. 10 seconds icap scan timeout seems to be the default in ESET Linux Gateway ICAP too. Can I configure that in Squid?


From squid at bloms.de  Tue Jan  8 16:52:23 2019
From: squid at bloms.de (Dieter Bloms)
Date: Tue, 8 Jan 2019 17:52:23 +0100
Subject: [squid-users] can't access https://www.finanzamt.bayern.de/ with
 sslbump (other sites works well)
Message-ID: <20190108165223.dcjkn443moruekfn@bloms.de>

Hello,

I've compiled squid 4.5 with openssl1.1 as shipped with debian9.
Sslbump works fine for all sides, but I can't access only one site
https://www.finanzamt.bayern.de/
and don't know the reason.
Ssllabs gives "A".
Here are the squid compile options:

--snip--
Squid Cache: Version 4.5
Service Name: squid

This binary uses OpenSSL 1.1.0j  20 Nov 2018. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-linux-gnu' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--libexecdir=${prefix}/lib/dv-squid4' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--prefix=/usr' '--sysconfdir=/etc/squid' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--localstatedir=/var' '--libexecdir=/usr/sbin' '--datadir=/usr/share/squid' '--mandir=/usr/share/man' '--with-default-user=squid' '--with-filedescriptors=65536' '--disable-auto-locale' '--disable-auth-negotiate' '--disable-auth-ntlm' '--disable-eui' '--disable-carp' '--disable-htcp' '--disable-ident-lookups' '--disable-loadable-modules' '--disable-translation' '--disable-wccp' '--disable-wccpv2' '--enable-async-io=128' '--enable-auth' '--enable-auth-basic=LDAP NCSA' '--enable-auth-digest=LDAP file' '--enable-epoll' '--enable-log-daemon-helpers=file' '--enable-icap-client' '--enable-inline' '--enable-snmp' '--enable-disk-io=AIO,DiskThreads,IpcIo,Blocking' '--enable-storeio=ufs,aufs,rock' '--enable-referer-log' '--enable-useragent-log' '--enable-large-cache-files' '--enable-removal-policies=lru,heap' '--enable-follow-x-forwarded-for' '--enable-ssl-crtd' '--with-openssl' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fdebug-prefix-map=/usr/src/packages/BUILD=. -fstack-protector-strong -Wformat -Werror=format-security' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fdebug-prefix-map=/usr/src/packages/BUILD=. -fstack-protector-strong -Wformat -Werror=format-security' --enable-ltdl-convenience
--snip--

The access.log looks like:

--snip--
1546962078.461   4726 x.x.x.x NONE/200 0 CONNECT www.finanzamt.bayern.de:443 - HIER_DIRECT/193.34.207.31 -
1546962078.472      0 x.x.x.x NONE/500 8495 GET https://www.finanzamt.bayern.de/ - HIER_NONE/- text/html
--snip--

no entries in cache.log

Can anybody try this site to see whether it is my local installation, or the webserver.

Thank you very much.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From Antony.Stone at squid.open.source.it  Tue Jan  8 17:13:31 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 8 Jan 2019 18:13:31 +0100
Subject: [squid-users] can't access https://www.finanzamt.bayern.de/
	with sslbump (other sites works well)
In-Reply-To: <20190108165223.dcjkn443moruekfn@bloms.de>
References: <20190108165223.dcjkn443moruekfn@bloms.de>
Message-ID: <201901081813.31217.Antony.Stone@squid.open.source.it>

On Tuesday 08 January 2019 at 17:52:23, Dieter Bloms wrote:

> Hello,
> 
> I've compiled squid 4.5 with openssl1.1 as shipped with debian9.
> Sslbump works fine for all sides, but I can't access only one site
> https://www.finanzamt.bayern.de/

Given who that is, I would not be at all surprised if they've used SSL pinning 
or similar to ensure that no form of MITM attack can be used to intercept data 
between clients and their website.

I can't test for this (I don't use SSL bump myself), but I wouldn't be 
surprised if the Bayern finance ministry is rather keen to avoid data 
interception.

No doubt others here can comment further, or advise where to look for positive 
confirmation of this theory.


Antony.

-- 
This email was created using 100% recycled electrons.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Tue Jan  8 18:09:22 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jan 2019 11:09:22 -0700
Subject: [squid-users] Need help about ICAP scan timeout/max file size
 for big files
In-Reply-To: <5826e10ffcf1341dbc4d5e2a4e635321@schroeffu.ch>
References: <cd3c4ba9-7bf1-ca6c-f6e1-ff1e919142e3@measurement-factory.com>
 <7e8169f092321085e591faaa7642d589@schroeffu.ch>
 <5826e10ffcf1341dbc4d5e2a4e635321@schroeffu.ch>
Message-ID: <90db7988-22dd-46ff-a2bf-b03315f2e8f1@measurement-factory.com>

On 1/8/19 9:46 AM, info at schroeffu.ch wrote:

> With ClamAV C-ICAP there is defined "MaxStreamSize 25M" as default,
> so after 25MB scanned by ICAP I can see with tcpdump on port 1344
> "ICAP/1.0 200 OK" from ICAP to Squid which triggers the browser to
> start the download. Thats what i want also for F-Secure ICAP.

The best solution would be for F-Secure to add support for (or enable in
your setup) "data trickling" or "patience pages". Any workarounds inside
Squid would be either nasty (e.g., timeouts, abandoned transactions,
etc.) or expensive (require Squid or eCAP/ICAP wrapper development).


> if their ICAP really is not sending "ICAP/1.0 200 OK" after X
> Seconds/MB, can I configure SQUID with a workaround?

You can try to specify a timeout via icap_io_timeout. Bugs
notwithstanding, Squid would terminate a connection to the ICAP service
that does not respond in X seconds. You may need to adjust
icap_service_failure_limit and/or icap_service_revival_delay to avoid
marking the affected ICAP service as "down" [too often]. Again, this is
not a proper solution and it may have negative side effects such as
memory leaks and unresponsive ICAP service. It may be worth trying while
you wait for F-Secure.

Unfortunately, the icap_io_timeout may not work if Squid is constantly
writing to the ICAP service (to deliver more virgin body bytes). Squid
should be treating each such write as an I/O, resetting the timeout.


You can also hack Squid to treat these cases specially. For example, you
could add adaptation_response_timeout or a similar directive that would
work like icap_io_timeout but ignore write activity. If you go down that
route, I suggest posting an RFC with new option description to squid-dev
as the first step.


You can even write an ICAP service (or eCAP adapter) that will add data
trickling or patience pages support to any ICAP service, but that is a
lot of development work!


> The header seems not include the file size. Here is an example of
> 100MB Virus File

Please note that you should test/analyze "real" transactions, not
requests for test files. If real transactions of interest usually lack
the Content-Length header, then timeout-based knobs are your best bet
(see above): There are no ACLs that can match accumulated response size
and, more importantly, there is no directive that would repeatedly
evaluate such ACLs as Squid accumulates the response body while waiting
for the ICAP response.


HTH,

Alex.


From antonino.sanacori at unibs.it  Wed Jan  9 09:34:41 2019
From: antonino.sanacori at unibs.it (Antonino Sanacori)
Date: Wed, 9 Jan 2019 10:34:41 +0100
Subject: [squid-users] Netdb. state too big
Message-ID: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>

 ?Hello.

My log/squid/netdb.state is 534MB, how can i reduce his size?

On my Debian 9 can I use logrotate to rotate the file?

Regards.

Antonino

-- 

______________________________________________

Antonino Sanacori



-- 


Informativa sulla Privacy: http://www.unibs.it/node/8155 
<http://www.unibs.it/node/8155>


From Ralf.Hildebrandt at charite.de  Wed Jan  9 09:38:58 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Jan 2019 10:38:58 +0100
Subject: [squid-users] [ext]  Netdb. state too big
In-Reply-To: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
Message-ID: <20190109093858.GC4403@charite.de>

* Antonino Sanacori <antonino.sanacori at unibs.it>:
> ?Hello.
> 
> My log/squid/netdb.state is 534MB, how can i reduce his size?
> 
> On my Debian 9 can I use logrotate to rotate the file?

You could disable it:
http://www.squid-cache.org/Doc/config/netdb_filename/

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Wed Jan  9 09:43:24 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Jan 2019 10:43:24 +0100
Subject: [squid-users] [ext]  Netdb. state too big
In-Reply-To: <20190109093858.GC4403@charite.de>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de>
Message-ID: <20190109094324.GD4403@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
> * Antonino Sanacori <antonino.sanacori at unibs.it>:
> > ?Hello.
> > 
> > My log/squid/netdb.state is 534MB, how can i reduce his size?
> > 
> > On my Debian 9 can I use logrotate to rotate the file?
> 
> You could disable it:
> http://www.squid-cache.org/Doc/config/netdb_filename/

http://etutorials.org/Server+Administration/Squid.+The+definitive+guide/Chapter+10.+Talking+to+Other+Squids/10.5+The+Network+Measurement+Database/
talks about "When the number of stored subnets reaches netdb_high,
Squid deletes the least recently used entries until the count is less
than netdb_low."

The question being: do you actually need this?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From antonino.sanacori at unibs.it  Wed Jan  9 10:33:42 2019
From: antonino.sanacori at unibs.it (Antonino Sanacori)
Date: Wed, 9 Jan 2019 11:33:42 +0100
Subject: [squid-users] [ext] Netdb. state too big
In-Reply-To: <20190109094324.GD4403@charite.de>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de> <20190109094324.GD4403@charite.de>
Message-ID: <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>

Hello Ralf.

I have not the typical record format visible in link 
http://etutorials.org... but many record as this:

x.y.z.a 8.0000 23.0000 1523875740 1523876930 ip server address

no subnet.

Il 09/01/2019 10:43, Ralf Hildebrandt ha scritto:
> * Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
>> * Antonino Sanacori <antonino.sanacori at unibs.it>:
>>>  ?Hello.
>>>
>>> My log/squid/netdb.state is 534MB, how can i reduce his size?
>>>
>>> On my Debian 9 can I use logrotate to rotate the file?
>> You could disable it:
>> http://www.squid-cache.org/Doc/config/netdb_filename/
> http://etutorials.org/Server+Administration/Squid.+The+definitive+guide/Chapter+10.+Talking+to+Other+Squids/10.5+The+Network+Measurement+Database/
> talks about "When the number of stored subnets reaches netdb_high,
> Squid deletes the least recently used entries until the count is less
> than netdb_low."
>
> The question being: do you actually need this?
>
-- 

______________________________________________

Antonino Sanacori
Servizi ICT
Universit? degli Studi di Brescia
Piazza del Mercato, 15 - 25121 Brescia
tel: 030 2988.325
email: antonino.sanacori at unibs.it


-- 


Informativa sulla Privacy: http://www.unibs.it/node/8155 
<http://www.unibs.it/node/8155>


From Ralf.Hildebrandt at charite.de  Wed Jan  9 10:38:25 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Jan 2019 11:38:25 +0100
Subject: [squid-users] [ext] Netdb. state too big
In-Reply-To: <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de>
 <20190109094324.GD4403@charite.de>
 <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>
Message-ID: <20190109103825.GF4403@charite.de>

* Antonino Sanacori <antonino.sanacori at unibs.it>:
> Hello Ralf.
> 
> I have not the typical record format visible in link
> http://etutorials.org... but many record as this:
> 
> x.y.z.a 8.0000 23.0000 1523875740 1523876930 ip server address
> 
> no subnet.

Yes, but do you need that at all?

http://www.squid-cache.org/mail-archive/squid-users/200007/0384.html
seems to imply that it's only useful in a parent-child setup (or cache
hierarchy).

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid3 at treenet.co.nz  Wed Jan  9 12:25:09 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Jan 2019 01:25:09 +1300
Subject: [squid-users] can't access https://www.finanzamt.bayern.de/
 with sslbump (other sites works well)
In-Reply-To: <20190108165223.dcjkn443moruekfn@bloms.de>
References: <20190108165223.dcjkn443moruekfn@bloms.de>
Message-ID: <54f2904d-28e5-3e46-c5ec-f7d66e1a16c9@treenet.co.nz>

On 9/01/19 5:52 am, Dieter Bloms wrote:
> Hello,
> 
> I've compiled squid 4.5 with openssl1.1 as shipped with debian9.
> Sslbump works fine for all sides, but I can't access only one site
> https://www.finanzamt.bayern.de/
> and don't know the reason.
> Ssllabs gives "A".

That means they are using "Good Practice" with their use of TLS. The
better they use TLS the less likely that SSL-Bump works.


...
> The access.log looks like:
> 
> --snip--
> 1546962078.461   4726 x.x.x.x NONE/200 0 CONNECT www.finanzamt.bayern.de:443 - HIER_DIRECT/193.34.207.31 -
> 1546962078.472      0 x.x.x.x NONE/500 8495 GET https://www.finanzamt.bayern.de/ - HIER_NONE/- text/html
> --snip--
> 
> no entries in cache.log
> 
> Can anybody try this site to see whether it is my local installation, or the webserver.
> 

Please check your cache.log and the 500-status error page message to
find out what the problem is. TLS is such a complicated system that it
is unlikely others will be able to see the reason your system is failing
with the very few details you have provided.


Amos


From squid3 at treenet.co.nz  Wed Jan  9 12:45:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Jan 2019 01:45:12 +1300
Subject: [squid-users] [ext] Netdb. state too big
In-Reply-To: <20190109103825.GF4403@charite.de>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de> <20190109094324.GD4403@charite.de>
 <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>
 <20190109103825.GF4403@charite.de>
Message-ID: <7f1a1e9a-6633-3495-6d2a-577a45efd263@treenet.co.nz>

On 9/01/19 11:38 pm, Ralf Hildebrandt wrote:
> * Antonino Sanacori:
>> Hello Ralf.
>>
>> I have not the typical record format visible in link
>> http://etutorials.org... but many record as this:
>>
>> x.y.z.a 8.0000 23.0000 1523875740 1523876930 ip server address
>>
>> no subnet.
> 
> Yes, but do you need that at all?
> 
> http://www.squid-cache.org/mail-archive/squid-users/200007/0384.html
> seems to imply that it's only useful in a parent-child setup (or cache
> hierarchy).
> 

It is most useful for peer connections where the peer selection
algorithm directly depends on the RTT times.

It is also used with persistent connection selection as a small
optimization to prioritize faster connections to be selected for re-use
rather than slower connections. This makes a big difference for servers
operating behind an IP load balancer.


This feature is always an optional one. In absence of NetDB RTT data
algorithms using it assume that all TCP connections are equally fast.

Amos


From Ralf.Hildebrandt at charite.de  Wed Jan  9 12:54:00 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 9 Jan 2019 13:54:00 +0100
Subject: [squid-users] [ext] Netdb. state too big
In-Reply-To: <7f1a1e9a-6633-3495-6d2a-577a45efd263@treenet.co.nz>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de>
 <20190109094324.GD4403@charite.de>
 <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>
 <20190109103825.GF4403@charite.de>
 <7f1a1e9a-6633-3495-6d2a-577a45efd263@treenet.co.nz>
Message-ID: <20190109125400.GG4403@charite.de>

> It is most useful for peer connections where the peer selection
> algorithm directly depends on the RTT times.

Like reverse proxies?
 
> It is also used with persistent connection selection as a small
> optimization to prioritize faster connections to be selected for re-use
> rather than slower connections. This makes a big difference for servers
> operating behind an IP load balancer.
> 
> This feature is always an optional one. In absence of NetDB RTT data
> algorithms using it assume that all TCP connections are equally fast.

In a "normal" proxy setup (users are behind 4 proxies which connect
them to the Internet), does use of Netdb make sense?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From rafael.akchurin at diladele.com  Wed Jan  9 13:01:54 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 9 Jan 2019 13:01:54 +0000
Subject: [squid-users] can't access https://www.finanzamt.bayern.de/
 with sslbump (other sites works well)
In-Reply-To: <54f2904d-28e5-3e46-c5ec-f7d66e1a16c9@treenet.co.nz>
References: <20190108165223.dcjkn443moruekfn@bloms.de>
 <54f2904d-28e5-3e46-c5ec-f7d66e1a16c9@treenet.co.nz>
Message-ID: <AM0PR04MB4753D3CF4D18371DEA4E9ABC8F8B0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Dieter,

Just for the record, I have no problems accessing that site using SSL bumping AD integrated Squid 4.4 (coupled with web safety ICAP filter but that should not matter really). Squid conf is more or less default with usual peek-and-splice (bump all) directives.

Best regards,
Rafael Akchurin
Diladele B.V.


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, 9 January 2019 13:25
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] can't access https://www.finanzamt.bayern.de/ with sslbump (other sites works well)

On 9/01/19 5:52 am, Dieter Bloms wrote:
> Hello,
> 
> I've compiled squid 4.5 with openssl1.1 as shipped with debian9.
> Sslbump works fine for all sides, but I can't access only one site 
> https://www.finanzamt.bayern.de/ and don't know the reason.
> Ssllabs gives "A".

That means they are using "Good Practice" with their use of TLS. The better they use TLS the less likely that SSL-Bump works.


...
> The access.log looks like:
> 
> --snip--
> 1546962078.461   4726 x.x.x.x NONE/200 0 CONNECT www.finanzamt.bayern.de:443 - HIER_DIRECT/193.34.207.31 -
> 1546962078.472      0 x.x.x.x NONE/500 8495 GET https://www.finanzamt.bayern.de/ - HIER_NONE/- text/html
> --snip--
> 
> no entries in cache.log
> 
> Can anybody try this site to see whether it is my local installation, or the webserver.
> 

Please check your cache.log and the 500-status error page message to find out what the problem is. TLS is such a complicated system that it is unlikely others will be able to see the reason your system is failing with the very few details you have provided.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Wed Jan  9 13:16:36 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Jan 2019 02:16:36 +1300
Subject: [squid-users] Netdb. state too big
In-Reply-To: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
Message-ID: <8fcad6ce-3a1b-f2a3-a55b-4c7f4aea06fd@treenet.co.nz>

On 9/01/19 10:34 pm, Antonino Sanacori wrote:
> ?Hello.
> 
> My log/squid/netdb.state is 534MB, how can i reduce his size?
> 

* You can safely delete Squid netdb.state journals at any time.

* You can disable NetDB by configuring "netdb_filename none" in squid.conf

However, that said ...

> On my Debian 9 can I use logrotate to rotate the file?
> 

... the Debian squid packages NetDB journal file is located at
/var/spool/squid/netdb.state unless you have configured netdb_filename
to place it elsewhere.


You should *not* treat netdb.state as a log. Performing log rotation,
backups, or even using its contents for analysis is pointless. Squid
will delete and rebuild it as an hourly snapshot during regular operation.


Amos


From squid3 at treenet.co.nz  Wed Jan  9 13:35:19 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Jan 2019 02:35:19 +1300
Subject: [squid-users] [ext] Netdb. state too big
In-Reply-To: <20190109125400.GG4403@charite.de>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de> <20190109094324.GD4403@charite.de>
 <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>
 <20190109103825.GF4403@charite.de>
 <7f1a1e9a-6633-3495-6d2a-577a45efd263@treenet.co.nz>
 <20190109125400.GG4403@charite.de>
Message-ID: <6c027922-bc33-50e1-f150-1bd4a064368d@treenet.co.nz>

On 10/01/19 1:54 am, Ralf Hildebrandt wrote:
>> It is most useful for peer connections where the peer selection
>> algorithm directly depends on the RTT times.
> 
> Like reverse proxies?
>  

Like weighted round-robin, ICP, HTCP.


>> It is also used with persistent connection selection as a small
>> optimization to prioritize faster connections to be selected for re-use
>> rather than slower connections. This makes a big difference for servers
>> operating behind an IP load balancer.
>>
>> This feature is always an optional one. In absence of NetDB RTT data
>> algorithms using it assume that all TCP connections are equally fast.
> 
> In a "normal" proxy setup (users are behind 4 proxies which connect
> them to the Internet), does use of Netdb make sense?
> 

Depends on how focused one is on latency and what RTT differences the
proxy is detecting. If you don't care about a few ms on some connections
or the RTT variation between connections to a server is normally under
1ms then it can be better to use the memory and CPU cycles for other
things like cache_mem.


Amos


From antonino.sanacori at unibs.it  Wed Jan  9 14:33:55 2019
From: antonino.sanacori at unibs.it (Antonino Sanacori)
Date: Wed, 9 Jan 2019 15:33:55 +0100
Subject: [squid-users] [ext] Netdb. state too big
In-Reply-To: <6c027922-bc33-50e1-f150-1bd4a064368d@treenet.co.nz>
References: <2346abd3-846b-ad30-2025-c2cf45f703b3@unibs.it>
 <20190109093858.GC4403@charite.de> <20190109094324.GD4403@charite.de>
 <18e986b9-d136-70be-5203-f61508989f2a@unibs.it>
 <20190109103825.GF4403@charite.de>
 <7f1a1e9a-6633-3495-6d2a-577a45efd263@treenet.co.nz>
 <20190109125400.GG4403@charite.de>
 <6c027922-bc33-50e1-f150-1bd4a064368d@treenet.co.nz>
Message-ID: <4954fe46-a06f-4c81-f73f-93bd7754c87e@unibs.it>

Many thanks Amos and Ralf,

i have a simple proxy with no proxy hierarchy, then i will set the 
parameter to "none".

Antonino

Il 09/01/2019 14:35, Amos Jeffries ha scritto:
> On 10/01/19 1:54 am, Ralf Hildebrandt wrote:
>>> It is most useful for peer connections where the peer selection
>>> algorithm directly depends on the RTT times.
>> Like reverse proxies?
>>   
> Like weighted round-robin, ICP, HTCP.
>
>
>>> It is also used with persistent connection selection as a small
>>> optimization to prioritize faster connections to be selected for re-use
>>> rather than slower connections. This makes a big difference for servers
>>> operating behind an IP load balancer.
>>>
>>> This feature is always an optional one. In absence of NetDB RTT data
>>> algorithms using it assume that all TCP connections are equally fast.
>> In a "normal" proxy setup (users are behind 4 proxies which connect
>> them to the Internet), does use of Netdb make sense?
>>
> Depends on how focused one is on latency and what RTT differences the
> proxy is detecting. If you don't care about a few ms on some connections
> or the RTT variation between connections to a server is normally under
> 1ms then it can be better to use the memory and CPU cycles for other
> things like cache_mem.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




-- 


Informativa sulla Privacy: http://www.unibs.it/node/8155 
<http://www.unibs.it/node/8155>


From russel_mcdonald at swbell.net  Wed Jan  9 16:39:39 2019
From: russel_mcdonald at swbell.net (Russel McDonald)
Date: Wed, 9 Jan 2019 16:39:39 +0000 (UTC)
Subject: [squid-users] Any way to cause 512 byte alignment of data passed to
	ECAP?
References: <451109100.8791325.1547051979356.ref@mail.yahoo.com>
Message-ID: <451109100.8791325.1547051979356@mail.yahoo.com>


I have 2 questions.

1.??????Is there any way to configure squid to align thedata passed to ECAP to occur on 512 byte boundaries, of course allowing for anyending partial size? I?m getting random unaligned lengths passed. If that isnot a current capability then is that an acceptable feature someone could joinand contribute?

2.??????When streaming a file, say playing a song (.m4afile), and scroll bar is jumped to the end of the song, is there any way thatthe ECAP callbacks specify where at in the file the stream restarted from?Dumping and looking at the chunk contents in method noteVbContentAvailable ??Idon?t see any HTML that indicates any offset/position/range. And in startenumerating the HTTP header entries I don?t see any such either (adapted->header().visitEach(*visitor);).

Russel McDonald
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190109/6cf1acb9/attachment.htm>

From rousskov at measurement-factory.com  Wed Jan  9 18:22:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Jan 2019 11:22:09 -0700
Subject: [squid-users] Any way to cause 512 byte alignment of data
 passed to ECAP?
In-Reply-To: <451109100.8791325.1547051979356@mail.yahoo.com>
References: <451109100.8791325.1547051979356.ref@mail.yahoo.com>
 <451109100.8791325.1547051979356@mail.yahoo.com>
Message-ID: <44d0b3a1-662e-ba56-ade9-401e46a52b5c@measurement-factory.com>

On 1/9/19 9:39 AM, Russel McDonald wrote:

> 1.?????? Is there any way to configure squid to align the data passed to
> ECAP to occur on 512 byte boundaries, of course allowing for any ending
> partial size? 

No, it is not possible to force Squid to align HTTP message data made
available to adaptation services. If you have to do something like that,
you might be able to shape the incoming TCP data into properly sized
packets before Squid reads them, but even if you are successful at that
difficult task, Squid may aggregate or split TCP payloads for various
reasons.


> If that is not a current capability then is that an acceptable
> feature someone could join and contribute?

I do not think so: In eCAP, it is the adapter that pulls message body
data from the host application! The host application (in your case
Squid) does not push data into the adapter. Thus, an adapter can pull as
much or as little as it wants, observing whatever "alignment" it needs.


> 2.?????? When streaming a file, say playing a song (.m4a file), and
> scroll bar is jumped to the end of the song, is there any way that the
> ECAP callbacks specify where at in the file the stream restarted from?

No, not directly: eCAP does not know anything about files, songs, and
scroll bars. It works at the lower HTTP level while you are asking about
higher level metadata that would be specific to the playback
protocol/method being used on top of HTTP.


> Dumping and looking at the chunk contents in method
> noteVbContentAvailable??I don?t see any HTML that indicates any
> offset/position/range. And in start enumerating the HTTP header entries
> I don?t see any such either (adapted->header().visitEach(*visitor);).

*If* the plugin/code in the client browser has requested something
special after the "scroll bar is jumped to the end of the song" event,
then that special request will be reflected in HTTP request headers
and/or body. It could be the request URL itself, the Range header field,
some custom request header, or even something inside a POST request body.

All those things _can_ be analyzed by an eCAP adapter code, using
existing eCAP APIs, of course. It is adapter responsibility to map those
HTTP concepts to playback position or any similar applicaction-specific
info.


HTH,

Alex.



From dm at belkam.com  Thu Jan 10 10:44:13 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Thu, 10 Jan 2019 14:44:13 +0400
Subject: [squid-users] squid 4.5 , ssl bump and c-icap on google sites
Message-ID: <0170dfda-875e-3c69-92e8-fb9851c41f8e@belkam.com>

Hello!


We are testing ssl-bump with squid 4.5.

Also we run c-icap with squid.


What is strange here -? ssl-bump works for google domains if icap is 
disabled,

but if it is on, then we get an error from c-icap:


You tried to upload/download a file that contains the virus: 
*uncompress: Uncompression Failure *
The Http location is: *https://www.youtube.com/
*

*
*

Although it works on most other sites.


Could you tell me why it does not work and is it possible to fix it?


Thank you!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190110/236b2821/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 10 17:44:15 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Jan 2019 06:44:15 +1300
Subject: [squid-users] squid 4.5 , ssl bump and c-icap on google sites
In-Reply-To: <0170dfda-875e-3c69-92e8-fb9851c41f8e@belkam.com>
References: <0170dfda-875e-3c69-92e8-fb9851c41f8e@belkam.com>
Message-ID: <d7ee6c24-d10e-a2f0-bb7a-ac361ee818fb@treenet.co.nz>

On 10/01/19 11:44 pm, Dmitry Melekhov wrote:
> Hello!
> 
> 
> We are testing ssl-bump with squid 4.5.
> 

With what settings?

Amos


From dm at belkam.com  Fri Jan 11 05:19:51 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 11 Jan 2019 09:19:51 +0400
Subject: [squid-users] squid 4.5 , ssl bump and c-icap on google sites
In-Reply-To: <0170dfda-875e-3c69-92e8-fb9851c41f8e@belkam.com>
References: <0170dfda-875e-3c69-92e8-fb9851c41f8e@belkam.com>
Message-ID: <e9bc29da-5327-223f-0973-f4cf627a2dfb@belkam.com>

Hello!

Problem? was on c-icap side, my build had no br support.

Thank you!

10.01.2019 14:44, Dmitry Melekhov ?????:
>
> Hello!
>
>
> We are testing ssl-bump with squid 4.5.
>
> Also we run c-icap with squid.
>
>
> What is strange here -? ssl-bump works for google domains if icap is 
> disabled,
>
> but if it is on, then we get an error from c-icap:
>
>
> You tried to upload/download a file that contains the virus: 
> *uncompress: Uncompression Failure *
> The Http location is: *https://www.youtube.com/
> *
>
> *
> *
>
> Although it works on most other sites.
>
>
> Could you tell me why it does not work and is it possible to fix it?
>
>
> Thank you!
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190111/db8e85d8/attachment.htm>

From eliezer at ngtech.co.il  Mon Jan 14 09:45:17 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Mon, 14 Jan 2019 11:45:17 +0200
Subject: [squid-users] Caching Vimeo Videos
In-Reply-To: <CAGycgFh+RZq83hsTvGX3Q8M7Mf02pp8jrBX2Re9U87EYqHNjEQ@mail.gmail.com>
References: <CAGycgFh+RZq83hsTvGX3Q8M7Mf02pp8jrBX2Re9U87EYqHNjEQ@mail.gmail.com>
Message-ID: <015e01d4abed$dcc90f30$965b2d90$@ngtech.co.il>

I wrote an example helper at:
http://gogs.ngtech.co.il/NgTech-LTD/storeid-helpers
 
which contains vimeo mp4 specific StoreID helper.
For their images there is no need for a StoreID helper they?. Want you to cache it.
 
Eliezer
 
----
 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il

 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Raju M K
Sent: Friday, November 30, 2018 12:06
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Caching Vimeo Videos
 
Need help on how to cache Vimeo videos under squid proxy.

-- 
Regards,
M K Raju.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190114/1bb5ffd2/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190114/1bb5ffd2/attachment.png>

From felix at b9d.de  Mon Jan 14 10:51:04 2019
From: felix at b9d.de (Felix Falk)
Date: Mon, 14 Jan 2019 11:51:04 +0100
Subject: [squid-users] Odd behavior on CNAME
Message-ID: <4c-5c3c6980-3-719f5a80@45883443>


Hi,
I have a linkchecker running over Squid as forwardproxy. One Site is not longer existing but DNS still has a CNAME Record except the CNAME is pointing nowhere:

> $ host de.sitestat.com
> de.sitestat.com is an alias for wildcard.sitestat.com.
> $ host wildcard.sitestat.com.
> $?

If called via curl:
> $ curl --proxy forwardproxy:3128 -v http://de.sitestat.com/
> * About to connect() to proxy forwardproxy.rz.babiel.com port 3128 (#0)
> * ? Trying 192.168.110.215...
> * Connected to forwardproxy.rz.babiel.com (192.168.110.215) port 3128 (#0)
> > GET http://de.sitestat.com/ HTTP/1.1
> > User-Agent: curl/7.29.0
> > Host: de.sitestat.com
> > Accept: */*
> > Proxy-Connection: Keep-Alive
> >?

The connection and this curl stays open for literally infinitely. (tested for 3 days).

In the Squid access.log nothing shows up. My cache.log has:
> 2019/01/14 11:30:31 kid1| ipcacheParse: No Address records in response to 'de.sitestat.com'

Any ideas how to solve this or is this simply a bug?

Felix
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190114/b29098dc/attachment.htm>

From eliezer at ngtech.co.il  Mon Jan 14 15:09:59 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Mon, 14 Jan 2019 17:09:59 +0200
Subject: [squid-users] Odd behavior on CNAME
In-Reply-To: <4c-5c3c6980-3-719f5a80@45883443>
References: <4c-5c3c6980-3-719f5a80@45883443>
Message-ID: <017701d4ac1b$381769e0$a8463da0$@ngtech.co.il>

It should return:
<!-- ERR_DNS_FAIL -->
? page.
And it takes 100 ms.
 
Can you re-test it?
Also make sure what happens if you run the same command from within the proxy host and not remotely.
 
Another option is that there is something else between you and the proxy but it sound odd.
 
Eliezer
 
----
 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il

 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Felix Falk
Sent: Monday, January 14, 2019 12:51
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Odd behavior on CNAME
 
Hi,
I have a linkchecker running over Squid as forwardproxy. One Site is not longer existing but DNS still has a CNAME Record except the CNAME is pointing nowhere:

> $ host de.sitestat.com
> de.sitestat.com is an alias for wildcard.sitestat.com.
> $ host wildcard.sitestat.com.
> $ 

If called via curl:
> $ curl --proxy forwardproxy:3128 -v http://de.sitestat.com/
> * About to connect() to proxy forwardproxy.rz.babiel.com port 3128 (#0)
> *   Trying 192.168.110.215...
> * Connected to forwardproxy.rz.babiel.com (192.168.110.215) port 3128 (#0)
> > GET http://de.sitestat.com/ HTTP/1.1
> > User-Agent: curl/7.29.0
> > Host: de.sitestat.com
> > Accept: */*
> > Proxy-Connection: Keep-Alive
> > 

The connection and this curl stays open for literally infinitely. (tested for 3 days).

In the Squid access.log nothing shows up. My cache.log has:
> 2019/01/14 11:30:31 kid1| ipcacheParse: No Address records in response to 'de.sitestat.com'

Any ideas how to solve this or is this simply a bug?

Felix
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190114/e93804b4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190114/e93804b4/attachment.png>

From felix at b9d.de  Mon Jan 14 15:13:47 2019
From: felix at b9d.de (Felix Falk)
Date: Mon, 14 Jan 2019 16:13:47 +0100
Subject: [squid-users] Odd behavior on CNAME
In-Reply-To: <017701d4ac1b$381769e0$a8463da0$@ngtech.co.il>
References: <4c-5c3c6980-3-719f5a80@45883443>
 <017701d4ac1b$381769e0$a8463da0$@ngtech.co.il>
Message-ID: <7140C77A-5AD4-465D-9D42-CA90AFA39A8E@b9d.de>

Hi,

I probably should mention:
- other working sites are doing fine over the proxy
- domains without any record are throwing the mentioned error properly

Felix

Am 14. Januar 2019 16:09:59 MEZ schrieb eliezer at ngtech.co.il:
>It should return:
><!-- ERR_DNS_FAIL -->
>? page.
>And it takes 100 ms.
> 
>Can you re-test it?
>Also make sure what happens if you run the same command from within the
>proxy host and not remotely.
> 
>Another option is that there is something else between you and the
>proxy but it sound odd.
> 
>Eliezer
> 
>----
> <http://ngtech.co.il/main-en/> Eliezer Croitoru
>Linux System Administrator
>Mobile: +972-5-28704261
>Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il
>
> 
>From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
>Of Felix Falk
>Sent: Monday, January 14, 2019 12:51
>To: squid-users at lists.squid-cache.org
>Subject: [squid-users] Odd behavior on CNAME
> 
>Hi,
>I have a linkchecker running over Squid as forwardproxy. One Site is
>not longer existing but DNS still has a CNAME Record except the CNAME
>is pointing nowhere:
>
>> $ host de.sitestat.com
>> de.sitestat.com is an alias for wildcard.sitestat.com.
>> $ host wildcard.sitestat.com.
>> $ 
>
>If called via curl:
>> $ curl --proxy forwardproxy:3128 -v http://de.sitestat.com/
>> * About to connect() to proxy forwardproxy.rz.babiel.com port 3128
>(#0)
>> *   Trying 192.168.110.215...
>> * Connected to forwardproxy.rz.babiel.com (192.168.110.215) port 3128
>(#0)
>> > GET http://de.sitestat.com/ HTTP/1.1
>> > User-Agent: curl/7.29.0
>> > Host: de.sitestat.com
>> > Accept: */*
>> > Proxy-Connection: Keep-Alive
>> > 
>
>The connection and this curl stays open for literally infinitely.
>(tested for 3 days).
>
>In the Squid access.log nothing shows up. My cache.log has:
>> 2019/01/14 11:30:31 kid1| ipcacheParse: No Address records in
>response to 'de.sitestat.com'
>
>Any ideas how to solve this or is this simply a bug?
>
>Felix

-- 
Felix Falk
@theftf
http://thef.tf/
+492022623399

OpenPGP Fingerprint: B8F6 590B F4B1 19F6 AE94 D24E D91B 960B 97E1 6444

Von meinem mobilen Plemplemplicator gesendet
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190114/accf5ae2/attachment.htm>

From rousskov at measurement-factory.com  Mon Jan 14 16:12:53 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Jan 2019 09:12:53 -0700
Subject: [squid-users] Odd behavior on CNAME
In-Reply-To: <4c-5c3c6980-3-719f5a80@45883443>
References: <4c-5c3c6980-3-719f5a80@45883443>
Message-ID: <52759a71-06be-1784-3b1c-8028dd4f10f7@measurement-factory.com>

On 1/14/19 3:51 AM, Felix Falk wrote:

> I have a linkchecker running over Squid as forwardproxy. One Site is not
> longer existing but DNS still has a CNAME Record except the CNAME is
> pointing nowhere:
> 
>> $ host de.sitestat.com
>> de.sitestat.com is an alias for wildcard.sitestat.com.
>> $ host wildcard.sitestat.com.
>> $?
> 
> If called via curl:
>> $ curl --proxy forwardproxy:3128 -v http://de.sitestat.com/
>> * About to connect() to proxy forwardproxy.rz.babiel.com port 3128 (#0)
>> * ? Trying 192.168.110.215...
>> * Connected to forwardproxy.rz.babiel.com (192.168.110.215) port 3128 (#0)
>> > GET http://de.sitestat.com/ HTTP/1.1
>> > User-Agent: curl/7.29.0
>> > Host: de.sitestat.com
>> > Accept: */*
>> > Proxy-Connection: Keep-Alive
>> >?
> 
> The connection and this curl stays open for literally infinitely.
> (tested for 3 days).
> 
> In the Squid access.log nothing shows up. My cache.log has:
>> 2019/01/14 11:30:31 kid1| ipcacheParse: No Address records in response
> to 'de.sitestat.com'
> 
> Any ideas how to solve this or is this simply a bug?

I suggest reporting a bug if you can reproduce with the latest v4
release. Posting a (link to) compressed ALL,9 cache.log to bugzilla may
speed up triage.

FWIW, using v5-based code, your test worked for me with an immediate
ERR_CANNOT_FORWARD error and the following level-1 cache.log entry:

2019/01/14 09:04:58| DNS error while resolving de.sitestat.com: No valid
address records

Alex.


From dm at belkam.com  Tue Jan 15 05:01:41 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Tue, 15 Jan 2019 09:01:41 +0400
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
Message-ID: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>

Hello!

According? to

https://wiki.squid-cache.org/Features/DynamicSslCert

recommended way to create certificate

openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout myCA.pem  -out myCA.pem

we can create certificate for longer time.

But sooner or later we'll have to renew it.

In this case, once we replaced certificate, it should be immediately replaced on user's computers,
not easy task, I don't sure it can be achieved in our environment.

We had the same issue with openvpn, fortunately it can check certificates from several ca's places in the same file,
so we had old and new certificates for some time.

I don't know is it possible to do something similar with squid and dynamic certificate generation,
I know it does not work now.

Could you share your experience? How do you replace certificates?

Thank you!


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/67fe61ef/attachment.htm>

From numsys at free.fr  Tue Jan 15 15:59:23 2019
From: numsys at free.fr (FredB)
Date: Tue, 15 Jan 2019 16:59:23 +0100
Subject: [squid-users] Squid 4.5 and intermediate CA
Message-ID: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>

Hi all,

I'm testing squid 4.5 and facing two issues with intermediate CA download

At first there is no source IP and I don't know how to allow this kind 
of requests with an identification acl

172.23.0.9 - user2 [15/Jan/2019:16:34:51 +0100] "CONNECT 
bugs.squid-cache.org:443 HTTP/1.1" 407 4442 447 TCP_DENIED:HIER_NONE 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" -
- - - [15/Jan/2019:16:34:51 +0100] "GET 
http://cert.int-x3.letsencrypt.org/ HTTP/1.1" 407 3536 0 
TCP_DENIED:HIER_NONE "-" -
172.23.0.9 - user2 [15/Jan/2019:16:34:51 +0100] "CONNECT 
bugs.squid-cache.org:443 HTTP/1.1" 200 0 447 NONE:HIER_DIRECT 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 
Firefox/64.0" bump

As you can see the request to letsencrypt is denied because a basic 
authentication is needed, how I can do a global ACL allow requests from 
squid ? I tested 127.0.0.1,local addresses but without any success

So for testing purpose I removed my identification rules

Now Squid can get the certificate

- - - [15/Jan/2019:16:33:43 +0100] "GET 
http://cert.int-x3.letsencrypt.org/ HTTP/1.1" 200 9737 0 NONE:HIER_NONE 
"-" -
172.23.0.9 - - [15/Jan/2019:16:33:43 +0100] "CONNECT 
bugs.squid-cache.org:443 HTTP/1.1" 200 0 447 NONE:HIER_DIRECT 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 
Firefox/64.0" bump
172.23.0.9 - - [15/Jan/2019:16:33:43 +0100] "GET 
https://bugs.squid-cache.org/ HTTP/1.1" 503 353 349 NONE:HIER_NONE 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" -

Cache.log

ssl3_get_server_certificate:certificate verify failed (1/-1/0)

I'm missing something?

Thanks

FredB


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/6436ec51/attachment.htm>

From eliezer at ngtech.co.il  Tue Jan 15 16:48:24 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Tue, 15 Jan 2019 18:48:24 +0200
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
Message-ID: <009a01d4acf2$21e20270$65a60750$@ngtech.co.il>

There should be a new acl names ?certificate-fetching?
So I assume you can use something like:
 
acl certfetch transaction_initiator certificate-fetching
http_access allow certfetch
 
Eliezer
 
----
 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il

 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of FredB
Sent: Tuesday, January 15, 2019 17:59
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid 4.5 and intermediate CA
 
Hi all,
I'm testing squid 4.5 and facing two issues with intermediate CA download 
At first there is no source IP and I don't know how to allow this kind of requests with an identification acl 
172.23.0.9 - user2 [15/Jan/2019:16:34:51 +0100] "CONNECT bugs.squid-cache.org:443 HTTP/1.1" 407 4442 447 TCP_DENIED:HIER_NONE "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" -
- - - [15/Jan/2019:16:34:51 +0100] "GET http://cert.int-x3.letsencrypt.org/ HTTP/1.1" 407 3536 0 TCP_DENIED:HIER_NONE "-" -
172.23.0.9 - user2 [15/Jan/2019:16:34:51 +0100] "CONNECT bugs.squid-cache.org:443 HTTP/1.1" 200 0 447 NONE:HIER_DIRECT "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" bump 
As you can see the request to letsencrypt is denied because a basic authentication is needed, how I can do a global ACL allow requests from squid ? I tested 127.0.0.1,local addresses but without any success 
So for testing purpose I removed my identification rules
Now Squid can get the certificate
- - - [15/Jan/2019:16:33:43 +0100] "GET http://cert.int-x3.letsencrypt.org/ HTTP/1.1" 200 9737 0 NONE:HIER_NONE "-" -
172.23.0.9 - - [15/Jan/2019:16:33:43 +0100] "CONNECT bugs.squid-cache.org:443 HTTP/1.1" 200 0 447 NONE:HIER_DIRECT "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" bump
172.23.0.9 - - [15/Jan/2019:16:33:43 +0100] "GET https://bugs.squid-cache.org/ HTTP/1.1" 503 353 349 NONE:HIER_NONE "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" -
Cache.log
ssl3_get_server_certificate:certificate verify failed (1/-1/0)
I'm missing something?
Thanks
FredB
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/335a0728/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/335a0728/attachment.png>

From eliezer at ngtech.co.il  Tue Jan 15 16:52:48 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Tue, 15 Jan 2019 18:52:48 +0200
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
Message-ID: <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>

With squid 4.x or even 3.5 you can use an intermediate CA.
So you will have the root key and certificate somewhere safe and renew the intermediate root CA every year or two.
 
The main root CA should be created at-least for a period of 5 years to allow this dynamicity you probably need.
 
Eliezer
 
*	I have seen security companies( AV ) that updates their root ca certificate using the AV or agent, if running an update file/service every startup is an option we can try to find a nice solution.
 
----
 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il

 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Dmitry Melekhov
Sent: Tuesday, January 15, 2019 07:02
To: squid-users at squid-cache.org
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
 
Hello!
According  to 
https://wiki.squid-cache.org/Features/DynamicSslCert
recommended way to create certificate 
openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout myCA.pem  -out myCA.pem
 
we can create certificate for longer time.
 
But sooner or later we'll have to renew it.
 
In this case, once we replaced certificate, it should be immediately replaced on user's computers,
not easy task, I don't sure it can be achieved in our environment.
 
We had the same issue with openvpn, fortunately it can check certificates from several ca's places in the same file,
so we had old and new certificates for some time.
 
I don't know is it possible to do something similar with squid and dynamic certificate generation,
I know it does not work now.
 
Could you share your experience? How do you replace certificates?
 
Thank you!
 
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/d3f98392/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/d3f98392/attachment.png>

From numsys at free.fr  Tue Jan 15 16:54:13 2019
From: numsys at free.fr (FredB)
Date: Tue, 15 Jan 2019 17:54:13 +0100
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <009a01d4acf2$21e20270$65a60750$@ngtech.co.il>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <009a01d4acf2$21e20270$65a60750$@ngtech.co.il>
Message-ID: <0d3f41a5-6ed2-dc94-86b2-60ecab4df98b@free.fr>

Hi Eliezer

It's just what I'm seeing and it works well, so with fetched_certificate 
rule the first point is now fixed





From eliezer at ngtech.co.il  Tue Jan 15 16:54:15 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Tue, 15 Jan 2019 18:54:15 +0200
Subject: [squid-users] Caching mirrored origin server
In-Reply-To: <95a9f54378bf4ce3f53e28f7c331ec25@jfcarter.net>
References: <acf0ba4e5bed59057a4f56b3b29ed7ee@jfcarter.net>
 <f9d0e25f-f6a2-4e16-a0e6-91d68e3620b6@measurement-factory.com>
 <95a9f54378bf4ce3f53e28f7c331ec25@jfcarter.net>
Message-ID: <00bd01d4acf2$f30262f0$d90728d0$@ngtech.co.il>

The DB of distro mirrors on the wiki is not up-to-date but it's a nice example.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of jimc
Sent: Thursday, January 3, 2019 21:40
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Caching mirrored origin server

On 2019-01-03 08:34, Alex Rousskov wrote:
> The default solution for mapping many URLs to a single cache hit is the
> store_id helper. That solution is only applicable to URLs that produce
> the same content regardless of the URL from the set.
> 
>   * https://wiki.squid-cache.org/Features/StoreID
>   * http://www.squid-cache.org/Doc/config/store_id_program/

@Alex, thanks for the pointer to the StoreID feature.  I'll try to adapt
one of the sample programs in the docs.  The database of distro mirror
patterns mentioned there will be very helpful.

-- 
James F. Carter   Email: jimc at jfcarter.net
Web: http://www.math.ucla.edu/~jimc (q.v. for PGP key)
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From dm at belkam.com  Tue Jan 15 17:01:46 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Tue, 15 Jan 2019 21:01:46 +0400
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
 <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
Message-ID: <1e082c8d-ae0d-d70c-974e-9bd70b480f87@belkam.com>


15.01.2019 20:52, eliezer at ngtech.co.il ?????:
>
> With squid 4.x or even 3.5 you can use an intermediate CA.
>
> So you will have the root key and certificate somewhere safe and renew 
> the intermediate root CA every year or two.
>
> The main root CA should be created at-least for a period of 5 years to 
> allow this dynamicity you probably need.
>
> Eliezer
>

5 years, really, not very long period of time, if I'll be sure to not 
work here in 5 years then I'll use this ;-) , unfortunately I'm not :-(

I don't need to replace certificate every year or so, but I need to have 
minimal service interruption for every user during certificate replacement,

and I'm sure that certificate will need replacement for some reason.


>   * I have seen security companies( AV ) that updates their root ca
>     certificate using the AV or agent, if running an update
>     file/service every startup is an option we can try to find a nice
>     solution.
>
Download certificate at every boot or user login....

This is good idea, thank you!



>  *
>
> ----
>
> Eliezer Croitoru <http://ngtech.co.il/main-en/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>
> cid:image001.png at 01D2675E.DCF360D0
>
> *From:*squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *Dmitry Melekhov
> *Sent:* Tuesday, January 15, 2019 07:02
> *To:* squid-users at squid-cache.org
> *Subject:* [squid-users] ssl bump, CA certificate renewal, how to?
>
> Hello!
>
> According? to
>
> https://wiki.squid-cache.org/Features/DynamicSslCert
>
> recommended way to create certificate
>
> openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 
> -extensions v3_ca -keyout myCA.pem-out myCA.pem
> we can create certificate for longer time.
> But sooner or later we'll have to renew it.
> In this case, once we replaced certificate, it should be immediately 
> replaced on user's computers,
> not easy task, I don't sure it can be achieved in our environment.
> We had the same issue with openvpn, fortunately it can check 
> certificates from several ca's places in the same file,
> so we had old and new certificates for some time.
> I don't know is it possible to do something similar with squid and 
> dynamic certificate generation,
> I know it does not work now.
> Could you share your experience? How do you replace certificates?
> Thank you!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/2b6dfb88/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190115/2b6dfb88/attachment.png>

From numsys at free.fr  Tue Jan 15 17:08:25 2019
From: numsys at free.fr (FredB)
Date: Tue, 15 Jan 2019 18:08:25 +0100
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
 <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
Message-ID: <0f8e3dde-59be-982a-3053-342c8f824f09@free.fr>

Now squid can get directly the intermediate CA as a browser does, it's a 
very interesting feature to me

Maybe I'm missing something, but I can see the request from squid now 
(with squid 4) it's a good point, my sslbump config is very basic, 
perhaps to basic cl step at_step SslBump1

ssl_bump peek step1 all

ssl_bump splice nobump -> just simple acl dstdomain

ssl_bump splice nobump





From numsys at free.fr  Tue Jan 15 17:09:01 2019
From: numsys at free.fr (FredB)
Date: Tue, 15 Jan 2019 18:09:01 +0100
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <0f8e3dde-59be-982a-3053-342c8f824f09@free.fr>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
 <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
 <0f8e3dde-59be-982a-3053-342c8f824f09@free.fr>
Message-ID: <d02e540f-3eae-e1ee-484f-9c8a2448d5c7@free.fr>

Sorry wrong topic

Le 15/01/2019 ? 18:08, FredB a ?crit?:
> Now squid can get directly the intermediate CA as a browser does, it's 
> a very interesting feature to me
>
> Maybe I'm missing something, but I can see the request from squid now 
> (with squid 4) it's a good point, my sslbump config is very basic, 
> perhaps to basic cl step at_step SslBump1
>
> ssl_bump peek step1 all
>
> ssl_bump splice nobump -> just simple acl dstdomain
>
> ssl_bump splice nobump
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From numsys at free.fr  Tue Jan 15 17:09:30 2019
From: numsys at free.fr (FredB)
Date: Tue, 15 Jan 2019 18:09:30 +0100
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <0d3f41a5-6ed2-dc94-86b2-60ecab4df98b@free.fr>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <009a01d4acf2$21e20270$65a60750$@ngtech.co.il>
 <0d3f41a5-6ed2-dc94-86b2-60ecab4df98b@free.fr>
Message-ID: <ab2fd1d1-d1c5-175d-f739-066678f11f5d@free.fr>

Now squid can get directly the intermediate CA as a browser does, it's a 
very interesting feature to me

Maybe I'm missing something, but I can see the request from squid now 
(with squid 4) it's a good point, my sslbump config is very basic, 
perhaps to basic cl step at_step SslBump1

ssl_bump peek step1 all

ssl_bump splice nobump -> just simple acl dstdomain

ssl_bump splice nobump




From bruno.larini at riosoft.com.br  Tue Jan 15 17:33:18 2019
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Tue, 15 Jan 2019 15:33:18 -0200
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <1e082c8d-ae0d-d70c-974e-9bd70b480f87@belkam.com>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
 <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
 <1e082c8d-ae0d-d70c-974e-9bd70b480f87@belkam.com>
Message-ID: <991f2244-962d-8aba-2aeb-6203e90ee563@riosoft.com.br>

Em 15/01/2019 15:01, Dmitry Melekhov escreveu:
>
> 5 years, really, not very long period of time, if I'll be sure to not 
> work here in 5 years then I'll use this ;-) , unfortunately I'm not :-(
>
> I don't need to replace certificate every year or so, but I need to 
> have minimal service interruption for every user during certificate 
> replacement,
>
> and I'm sure that certificate will need replacement for some reason.
>
If your clients are running Windows and are AD members, you could 
distribute the certificates very easily via GPO. If not I can only think 
of a scripted solution on client's side, as Eliezer suggested.
As for avoiding the downtime, try to add, not replace the new one in the 
clients' certificate store beforehand. When you're certain that all of 
the clients are updated, then switch the Squid's CA.

-Bruno


From rousskov at measurement-factory.com  Tue Jan 15 19:55:39 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Jan 2019 12:55:39 -0700
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
Message-ID: <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>

On 1/15/19 8:59 AM, FredB wrote:

> I'm testing squid 4.5 and facing two issues with intermediate CA download
> 
> At first there is no source IP and I don't know how to allow this kind
> of requests with an identification acl

How about using transaction_initiator ACL to identify requests generated
by Squid? This will not solve your
"ssl3_get_server_certificate:certificate verify failed (1/-1/0)"
problem, of course, but at least you would not have to disable
authentication.

Alex.


From eugene.elyashev at gmail.com  Wed Jan 16 02:10:51 2019
From: eugene.elyashev at gmail.com (eugene.elyashev at gmail.com)
Date: Tue, 15 Jan 2019 20:10:51 -0600 (CST)
Subject: [squid-users] FTP inspection configuration
Message-ID: <1547604651842-0.post@n4.nabble.com>

Hello,
I'm trying to configure squid 3.5.6 as an FTP proxy for native FTP uploads
to be inspected by an ICAP service.

Currently FileZilla fails to connect via proxy and also telnet on port 21
fails..

What is missing in the config and how to configure FileZilla connection?

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http

acl CONNECT method CONNECT

http_access allow localhost manager
http_access deny manager

http_access allow localnet
http_access allow localhost

http_access deny all

http_port 3128 ssl-bump
cert=/usr/local/squid-3.5.6/ssl_cert/squid356_https.pem
key=/usr/local/squid-3.5.6/ssl_cert/squid356_https.pem
always_direct allow all
ssl_bump server-first all
sslproxy_flags DONT_VERIFY_PEER
ftp_port 21

coredump_dir /usr/local/squid-3.5.6/var/cache/squid

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320


acl vontu_reqmod_http_upload method POST PUT
icap_service vontu_reqmod reqmod_precache 0 icap://<icap_server:1344>/reqmod
adaptation_service_set class_vontu_reqmod vontu_reqmod
adaptation_access class_vontu_reqmod allow vontu_reqmod_http_upload

icap_enable on
icap_io_timeout 70
icap_service_failure_limit 20
icap_service_revival_delay 30
icap_preview_enable on
icap_preview_size 0
icap_persistent_connections on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_client_username_encode on



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From dm at belkam.com  Wed Jan 16 05:56:36 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Wed, 16 Jan 2019 09:56:36 +0400
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <991f2244-962d-8aba-2aeb-6203e90ee563@riosoft.com.br>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
 <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
 <1e082c8d-ae0d-d70c-974e-9bd70b480f87@belkam.com>
 <991f2244-962d-8aba-2aeb-6203e90ee563@riosoft.com.br>
Message-ID: <cbe41cc1-7c7d-bd97-a84e-609a089b2421@belkam.com>

15.01.2019 21:33, Bruno de Paula Larini ?????:
> Em 15/01/2019 15:01, Dmitry Melekhov escreveu:
>>
>> 5 years, really, not very long period of time, if I'll be sure to not 
>> work here in 5 years then I'll use this ;-) , unfortunately I'm not :-(
>>
>> I don't need to replace certificate every year or so, but I need to 
>> have minimal service interruption for every user during certificate 
>> replacement,
>>
>> and I'm sure that certificate will need replacement for some reason.
>>
> If your clients are running Windows and are AD members, you could 
> distribute the certificates very easily via GPO. If not I can only 
> think of a scripted solution on client's side, as Eliezer suggested.

I guess we have not more 1/3 of computers in AD, and not all of them are 
windows , we also have linux and macos...


> As for avoiding the downtime, try to add, not replace the new one in 
> the clients' certificate store beforehand. When you're certain that 
> all of the clients are updated, then switch the Squid's CA.
>
> -Bruno 


Thank you very much, this simple and efficient :-)




From squid3 at treenet.co.nz  Wed Jan 16 03:33:55 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 Jan 2019 16:33:55 +1300
Subject: [squid-users] FTP inspection configuration
In-Reply-To: <1547604651842-0.post@n4.nabble.com>
References: <1547604651842-0.post@n4.nabble.com>
Message-ID: <e93fc433-d3d0-fe8b-cb23-bae6ff2c9d29@treenet.co.nz>

On 16/01/19 3:10 pm, eugene.elyashev wrote:
> Hello,
> I'm trying to configure squid 3.5.6 as an FTP proxy for native FTP uploads
> to be inspected by an ICAP service.

Please try an upgrade, there have been a lot of fixes in the 3+ years
since that release. Current production/stable release is v4.5.

For the FTP issues 3.5.28 would be enough of an upgrade. But ...

Since you are also using SSL-Bump you should be tracking the latest
Squid releases and upgrading frequently. TLS is a highly volatile
environment - almost every Squid release since v3.2 has had additions to
cope with that.


> 
> Currently FileZilla fails to connect via proxy and also telnet on port 21
> fails..
> 
> What is missing in the config and how to configure FileZilla connection?
> 

Your ICAP service is only processing PUT and POST transactions. IIRC, at
least some of the FTP native messaging occurs as GET.

...
> 
> http_port 3128 ssl-bump
> cert=/usr/local/squid-3.5.6/ssl_cert/squid356_https.pem
> key=/usr/local/squid-3.5.6/ssl_cert/squid356_https.pem
> always_direct allow all

The above is not necessary in v3.2+, it was only useful as a hack
workaround for a bug in a single v3.1.x point release.


> ssl_bump server-first all

This bumping mode is deprecated due to lack of ability to cope with
modern TLS extensions and behaviour (ie. TLS SNI). Use the v3.5+ actions
instead
 <https://wiki.squid-cache.org/Features/SslPeekAndSplice>


> sslproxy_flags DONT_VERIFY_PEER

Please do not do this, ever. It only prevents *you* from seeing problems
(eg to debug them), they still exist and affect the traffic.
 Remove the above line and then actually fix any problems that are then
visible.


> ftp_port 21
> 
> coredump_dir /usr/local/squid-3.5.6/var/cache/squid
> 
> refresh_pattern ^ftp:		1440	20%	10080
> refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> refresh_pattern .		0	20%	4320
> 
> 
> acl vontu_reqmod_http_upload method POST PUT
> icap_service vontu_reqmod reqmod_precache 0 icap://<icap_server:1344>/reqmod
> adaptation_service_set class_vontu_reqmod vontu_reqmod
> adaptation_access class_vontu_reqmod allow vontu_reqmod_http_upload
> 

The ACL above restricting the ICAP service to only seeing PUT and POT
requests is probably the cause of your problem.

Another possibility is one of the ICAP bugs which have been fixed in
later v3.5 releases.


Amos


From numsys at free.fr  Wed Jan 16 07:30:52 2019
From: numsys at free.fr (FredB)
Date: Wed, 16 Jan 2019 08:30:52 +0100
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>
Message-ID: <f5728fac-e4dd-4f71-4e2c-04ff4c760296@free.fr>

Yes it works, my first issue is now resolved

There is a 200 when automatic download occurs, so this part is good

Unfortunately still there is a code 503 at the third request, a specific 
bump configuration is needed ?

- - - [15/Jan/2019:16:33:43 +0100] "GET 
http://cert.int-x3.letsencrypt.org/ HTTP/1.1" 200 9737 0 NONE:HIER_NONE 
"-" -
172.23.0.9 - - [15/Jan/2019:16:33:43 +0100] "CONNECT 
bugs.squid-cache.org:443 HTTP/1.1" 200 0 447 NONE:HIER_DIRECT 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 
Firefox/64.0" bump
172.23.0.9 - - [15/Jan/2019:16:33:43 +0100] "GET 
https://bugs.squid-cache.org/ HTTP/1.1" 503 353 349 NONE:HIER_NONE 
"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0" -




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190116/b3a3a202/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 16 14:14:55 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Jan 2019 03:14:55 +1300
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <f5728fac-e4dd-4f71-4e2c-04ff4c760296@free.fr>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>
 <f5728fac-e4dd-4f71-4e2c-04ff4c760296@free.fr>
Message-ID: <c667defc-3108-87da-4548-ee15981b7456@treenet.co.nz>

On 16/01/19 8:30 pm, FredB wrote:
> Yes it works, my first issue is now resolved
> 
> There is a 200 when automatic download occurs, so this part is good
> 
> Unfortunately still there is a code 503 at the third request, a specific
> bump configuration is needed ??
> 

Have you double-checked that the certificate our server you are testing
against is presenting does actually validate using the CA chain
LetsEncrypt is currently presenting to your server at the D/L URL?

We have had a number of times when the update tools they provide fail to
set the server cert properly and it gets a non-validating one.

Amos


From numsys at free.fr  Wed Jan 16 15:47:49 2019
From: numsys at free.fr (FredB)
Date: Wed, 16 Jan 2019 16:47:49 +0100
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <c667defc-3108-87da-4548-ee15981b7456@treenet.co.nz>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>
 <f5728fac-e4dd-4f71-4e2c-04ff4c760296@free.fr>
 <c667defc-3108-87da-4548-ee15981b7456@treenet.co.nz>
Message-ID: <6df47621-a573-ffd4-2c7f-558c697b09e0@free.fr>

Hi Amos,

Yes it works, and I guess I found where the problem is, this is a 
pkix-cert mime type and I wonder, but maybe I'm wrong, that Squid can't 
use the file

openssl x509 -inform DER -in myfile shows the CA as text file, after 
that I can use the CA file with browser unable to download CA (wget for 
example)

Perhaps this is a "bug" because pkix-cert is used by browsers (or 
clients software) to automatically adds CA

https://www.iana.org/assignments/media-types/application/pkix-cert

FredB




From eliezer at ngtech.co.il  Wed Jan 16 21:14:33 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Wed, 16 Jan 2019 23:14:33 +0200
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <6df47621-a573-ffd4-2c7f-558c697b09e0@free.fr>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>
 <f5728fac-e4dd-4f71-4e2c-04ff4c760296@free.fr>
 <c667defc-3108-87da-4548-ee15981b7456@treenet.co.nz>
 <6df47621-a573-ffd4-2c7f-558c697b09e0@free.fr>
Message-ID: <00a901d4ade0$7a9658a0$6fc309e0$@ngtech.co.il>

There is no way to automatically add ROOT CA into browsers or software....
If a software does that it's only based on a pre-defined rules.
At my page:
http://ngtech.co.il/static/myCA/autoinstaller/

There are three examples and one of them is for linux (Ubuntu,Debian,CentOS).

You can see the right mime headers for the der(also cer) and pem formats.
(use curl...)

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of FredB
Sent: Wednesday, January 16, 2019 17:48
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 4.5 and intermediate CA

Hi Amos,

Yes it works, and I guess I found where the problem is, this is a 
pkix-cert mime type and I wonder, but maybe I'm wrong, that Squid can't 
use the file

openssl x509 -inform DER -in myfile shows the CA as text file, after 
that I can use the CA file with browser unable to download CA (wget for 
example)

Perhaps this is a "bug" because pkix-cert is used by browsers (or 
clients software) to automatically adds CA

https://www.iana.org/assignments/media-types/application/pkix-cert

FredB


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Jan 16 21:16:58 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Wed, 16 Jan 2019 23:16:58 +0200
Subject: [squid-users] ssl bump, CA certificate renewal, how to?
In-Reply-To: <991f2244-962d-8aba-2aeb-6203e90ee563@riosoft.com.br>
References: <6172b64f-8b7c-af2e-0d3d-dc33ef17ed60@belkam.com>
 <00ae01d4acf2$bf535680$3dfa0380$@ngtech.co.il>
 <1e082c8d-ae0d-d70c-974e-9bd70b480f87@belkam.com>
 <991f2244-962d-8aba-2aeb-6203e90ee563@riosoft.com.br>
Message-ID: <00ab01d4ade0$d0de82f0$729b88d0$@ngtech.co.il>

+1

If the certificate is still working do the updates step by step and when you have successfully distributed the certificate make the switch.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Bruno de Paula Larini
Sent: Tuesday, January 15, 2019 19:33
To: squid-users at squid-cache.org
Subject: Re: [squid-users] ssl bump, CA certificate renewal, how to?

Em 15/01/2019 15:01, Dmitry Melekhov escreveu:
>
> 5 years, really, not very long period of time, if I'll be sure to not 
> work here in 5 years then I'll use this ;-) , unfortunately I'm not :-(
>
> I don't need to replace certificate every year or so, but I need to 
> have minimal service interruption for every user during certificate 
> replacement,
>
> and I'm sure that certificate will need replacement for some reason.
>
If your clients are running Windows and are AD members, you could 
distribute the certificates very easily via GPO. If not I can only think 
of a scripted solution on client's side, as Eliezer suggested.
As for avoiding the downtime, try to add, not replace the new one in the 
clients' certificate store beforehand. When you're certain that all of 
the clients are updated, then switch the Squid's CA.

-Bruno
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From dm at belkam.com  Thu Jan 17 05:30:54 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Thu, 17 Jan 2019 09:30:54 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
Message-ID: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>

Hello!

While accessing site I can't access it through ssl bump.

See in cache log:

2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55: 
error:14090086:SSL routines:ssl3_get_server_certificate:certificate 
verify failed (1/-1/0)


In access log:

1547702300.945????? 0 192.168.22.229 NONE/503 329 GET 
https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/- text/html
1547702301.304???? 84 - TCP_MISS/404 162 GET 
http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0 
- HIER_DIRECT/91.199.212.52 text/h
tml


I can access site directly from browser.


Could you tell me why it doesn't work and how can I fix this?


Thank you!



From numsys at free.fr  Thu Jan 17 08:52:41 2019
From: numsys at free.fr (FredB)
Date: Thu, 17 Jan 2019 09:52:41 +0100
Subject: [squid-users] Squid 4.5 and intermediate CA
In-Reply-To: <00a901d4ade0$7a9658a0$6fc309e0$@ngtech.co.il>
References: <cd821371-4784-d4d6-b17a-629981c075ca@free.fr>
 <29505245-8719-3df8-1d37-57b5ae7a1190@measurement-factory.com>
 <f5728fac-e4dd-4f71-4e2c-04ff4c760296@free.fr>
 <c667defc-3108-87da-4548-ee15981b7456@treenet.co.nz>
 <6df47621-a573-ffd4-2c7f-558c697b09e0@free.fr>
 <00a901d4ade0$7a9658a0$6fc309e0$@ngtech.co.il>
Message-ID: <90036aaf-7d32-d327-8fef-6e95a0634f55@free.fr>

Hi,

I'm speaking about Intermediate CA (not root) with squid as client 
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-4-and-missing-intermediate-certs-td4684653.html

Not directly related, how you usually update your root CA for squid ? 
I'm just using ca-certificate directory from my system and it seems 
pretty outdated (Debian 9) there is a link somewhere, for example, 
using? the latest mozilla CA in Squid ?

FredB




From silvester at familielangen.de  Thu Jan 17 09:51:42 2019
From: silvester at familielangen.de (Silvester Langen)
Date: Thu, 17 Jan 2019 09:51:42 +0000
Subject: [squid-users] proxy ntlm-auth problems
Message-ID: <20190117095142.Horde.4pXMgOzZqm7QG9go6pIMZHL@webmail.familielangen.de>

Hello squid users.
  ?
  I have configured squid for ntlm authentication and it seems to work  
well. All needed browsers (ff, ie, chrome) work and programs like  
teamviewer or "heise register" do work too. But now I notice, that  
other programs like Sage HR, Dakota, Sfirm and Elster have problems  
with authentication.
  ?
  With wireshark I see the following:
  ?
  (Stage1) Browsers, Teamviewer, etc starting request to squid and  
squid returns "407 Proxy Authentication Required".?
  (Stage2) After that the client begins a new request for negotiation  
and sends the credentials. The connection works.
  ?
  But...
  ?
  (Stage1) Sage HR, Sfirm, etc. starts request to squid and squid  
returns "407 Proxy Authentication Required".?
  After that the client begins a new request but the same without  
credentials and negotiation. Of course, the proxy refuses the  
connection again.
  ?
  I have no idea why the client software doesn?t start stage2 and no  
idea to find out why.
  ?
  Here is my configuration for ntlm-auth:
  ?
  auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth  
-d --ntlm /usr/bin/ntlm_auth --diagnostics  
--helper-protocol=squid-2.5-ntlmssp --domain=mydomain --kerberos  
/usr/lib/squid3/squid_kerb_auth -d -s GSS_C_NO_NAME
  auth_param negotiate children 10
  auth_param negotiate keep_alive off
  acl auth proxy_auth REQUIRED
  http_access allow auth
  ?
  Thank you for helping me!#

Silvester
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190117/8c188441/attachment.htm>

From belle at bazuin.nl  Thu Jan 17 10:02:59 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 17 Jan 2019 11:02:59 +0100
Subject: [squid-users] proxy ntlm-auth problems
In-Reply-To: <20190117095142.Horde.4pXMgOzZqm7QG9go6pIMZHL@webmail.familielangen.de>
References: <20190117095142.Horde.4pXMgOzZqm7QG9go6pIMZHL@webmail.familielangen.de>
Message-ID: <vmime.5c4052d3.7bc1.51e611c557989c83@ms249-lin-003.rotterdam.bazuin.nl>

i suggest you try: 
?
auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \
??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s s GSS_C_NO_NAME \
??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=MYDOMAIN

Greetz, 
?
Louis
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Silvester Langen
Verzonden: donderdag 17 januari 2019 10:52
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] proxy ntlm-auth problems




Hello squid users.
?
I have configured squid for ntlm authentication and it seems to work well. All needed browsers (ff, ie, chrome) work and programs like teamviewer or "heise register" do work too. But now I notice, that other programs like Sage HR, Dakota, Sfirm and Elster have problems with authentication.
?
With wireshark I see the following:
?
(Stage1) Browsers, Teamviewer, etc starting request to squid and squid returns "407 Proxy Authentication Required".?
(Stage2) After that the client begins a new request for negotiation and sends the credentials. The connection works.
?
But...
?
(Stage1) Sage HR, Sfirm, etc. starts request to squid and squid returns "407 Proxy Authentication Required".?
After that the client begins a new request but the same without credentials and negotiation. Of course, the proxy refuses the connection again.
?
I have no idea why the client software doesn?t start stage2 and no idea to find out why.
?
Here is my configuration for ntlm-auth:
?
auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth -d --ntlm /usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp --domain=mydomain --kerberos /usr/lib/squid3/squid_kerb_auth -d -s GSS_C_NO_NAME
auth_param negotiate children 10
auth_param negotiate keep_alive off
acl auth proxy_auth REQUIRED
http_access allow auth
?
Thank you for helping me!#

Silvester
Silvester Langen
Fachinformatiker - Systemintegration
Auf dem Leuchtenberg 78

41517 Grevenbroich 
Mobil: 0170 69 66 580
Tel: 02181 21 555 01
Web: silvesterlangen.de

Zertifizierter MCSA, MCSE, LPIC-1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190117/c37c8b4c/attachment.htm>

From marko.cupac at mimar.rs  Thu Jan 17 10:41:05 2019
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Thu, 17 Jan 2019 11:41:05 +0100
Subject: [squid-users] Squid 4.5 crashes on FreeBSD
Message-ID: <20190117114105.18b59737@efreet.kappastar.com>

Hi,

I am trying to move to Squid 4.x from 3.x on FreeBSD 12.0-RELEASE, but
so far it is quite unstable for me.

Right now I am using Squid 4.5, and it crashes, giving the following in
cache.log:
assertion failed: stmem.cc:98: "lowestOffset () <= target_offset"

messages log shows squid restarts:
Jan 16 11:07:52 squid2 squid[64423]: Squid Parent: squid-1 process
86491 exited due to signal 6 with status 0
Jan 16 11:07:52 squid2squid[64423]: Squid Parent: (squid-1) process 86729 started
Jan 16 11:12:26 squid2 squid[64423]: Squid Parent: squid-1 process
86729 exited due to signal 6 with status 0
Jan 16 11:12:26 squid2 squid[64423]: Squid Parent: (squid-1) process
86955 started

Could this be a software bug, or can I avoid this with configuration
change?

Thank you in advance,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From arne-tobias.rak at stud.tu-darmstadt.de  Thu Jan 17 13:47:14 2019
From: arne-tobias.rak at stud.tu-darmstadt.de (Arne-Tobias Rak)
Date: Thu, 17 Jan 2019 14:47:14 +0100
Subject: [squid-users] Backing up squid cache and restoring it
Message-ID: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>

Hi,

my goal is to restore a previous cache state in squid 3.x running on 
Ubuntu 16.04.

So far I have tried to create a copy of the /var/spool/squid and 
/var/log/squid folders.
When restoring the cache, I first shutdown squid using
/sudo squid -k shutdown//
//sudo service squid stop -k
/and then restore the previously copied folder contents. I then start 
squid again using
/sudo service squid start./

Unfortunately, this does not restore the previous cache contents, as the 
spool/squid/swap.state
file is modified during squid startup.

Thanks in advance!
Arne

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190117/e07154c6/attachment.htm>

From alessio.troiano at leonardocompany.com  Thu Jan 17 15:28:28 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Thu, 17 Jan 2019 15:28:28 +0000
Subject: [squid-users] Squid does not send request to parent proxy
Message-ID: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>

Hello all,
I'm not able to configure squid for using a parent proxy only for some domain. All the rest should be fetched directly. I tried this configuration:
cache_peer 172.31.3.70 parent 8080 0 no-query default name=HUBATLDB
acl domainAT dstdomain voeazul.com.br
cache_peer_access HUBATLDB allow domainAT
never_direct allow domainAT

But the site www.voeazul.com.br is fetched direct. This is the access log:
%SQUID-4: 172.31.0.82 59719 [17/Jan/2019:22:55:36 +0800] "CONNECT www.voeazul.com.br:443 HTTP/1.1" www.voeazul.com.br - - "-" 200 - 816 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0" TCP_TUNNEL:HIER_DIRECT 23.77.9.57 443 53176

Can you help me?

Following the full conf:

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8# RFC1918 possible internal network
acl localnet src 172.16.0.0/12# RFC1918 possible internal network
acl localnet src 192.168.0.0/16# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
acl SOC_NET src 172.31.0.0/24# SOC Network
acl SMD src 10.30.0.47/32    # SMD Proxy
acl Proxy_HK src 172.31.2.64/27    # Proxy Hong Kong Network
ignore_expect_100 on
acl nocachesite dstdomain /etc/squid/nocachesite.acl

acl SSL_ports port 443
acl SSL_ports port 8443
acl SSL_ports port 2096         # INC000000012740
acl SSL_ports port 9091
acl SSL_ports port 9444         # INC000000013855
acl SSL_ports port 6082
acl Safe_ports port 80# http
acl Safe_ports port 21# ftp
acl Safe_ports port 443# https
acl Safe_ports port 70# gopher
acl Safe_ports port 210# wais
acl Safe_ports port 1025-65535# unregistered ports
acl Safe_ports port 280# http-mgmt
acl Safe_ports port 488# gss-http
acl Safe_ports port 591# filemaker
acl Safe_ports port 777# multiling http
acl CONNECT method CONNECT

forwarded_for delete
tcp_outgoing_address 172.31.2.71 SMD

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access allow manager SOC_NET
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user

cache_peer 172.31.3.70 parent 8080 0 no-query default name=HUBATLDB
acl domainAT dstdomain voeazul.com.br
cache_peer_access HUBATLDB allow domainAT
never_direct allow domainAT

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

acl PURGE method PURGE
http_access allow PURGE localhost
http_access deny PURGE

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 0.0.0.0:8080

# We recommend you to use at least the following line.
# migrated automatically by squid-migrate-conf, the original configuration was: hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
cache_effective_user squid
cache_effective_group squid
cache_dir diskd /home/squid 400000 64 512
cache_mem 4 GB
maximum_object_size_in_memory 2 MB
minimum_object_size 0 KB
maximum_object_size 100 MB
cache_swap_low 96
cache_swap_high 97
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
cache deny nocachesite
cache allow all
max_filedesc 8192

# Leave coredumps in the first cache dir
coredump_dir /home/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:144020%10080
refresh_pattern ^gopher:14400%1440
refresh_pattern -i (/cgi-bin/|\?) 00%0
refresh_pattern .020%4320

cache_mgr xxx at xxx.com

### BEGIN LOG FOR SIEM ###

#logformat siem  %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh %<a %>p
#access_log /var/log/squid/access.log siem
logformat custom_squid %%SQUID-4: %>a %>p [%tl] "%rm %ru HTTP/%rv" %<A %ui %un "%rp" %Hs %mt %<st "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh %<a %<p %<lp
access_log /var/log/squid/rsa/access.log custom_squid

### END LOG FOR SIEM ###
dns_v4_first on
log_icp_queries off
via off

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From lsavarino at olfeo.com  Thu Jan 17 16:13:55 2019
From: lsavarino at olfeo.com (Luca Savarino)
Date: Thu, 17 Jan 2019 17:13:55 +0100
Subject: [squid-users] External acl on delay_access directive
Message-ID: <391cc3f9-129b-ec93-f9f9-faaf16912799@olfeo.com>

Hello,


 ??? Having recently upgraded from squid 3.4.8 to squid 4.4, I stumbled 
into an issue that I tried to simplify the most I could through the 
attached configuration files and the explanation below.

 ??? I would like to use an external acl to set bandwidth limitations 
for my different users. So in squid 3.4.8, I would do something like 
(that's just a very simple example) :


 ??? ??? ??? delay_pools 1

 ??? ??? ??? delay_class 1 3

 ??? ??? ??? external_acl_type ip_user_helper %SRC 
/usr/lib/squid3/ext_file_userip_acl -f /etc/squid/ips.conf
 ??? ??? ??? acl ip_list external ip_user_helper test


 ??? ??? ??? delay_access 1 allow ip_list
 ??? ??? ??? delay_access 1 deny all

 ??? ??? ??? delay_parameters 1 80000/80000 80000/80000 80000/80000


 ??? with /tmp/ips.conf containing something like :


 ??? ??? ??? 10.1.0.55 ALL


 ??? If the ip I want to limit the bandwidth of is 10.1.0.55. In squid 
4.4 however, I can't get it to work properly : the user can access her 
page but she is not limited as expected and I get the following message 
multiple times in my cache.log file :


 ??? ??? ??? WARNING: ip_list ACL is used in context without an ALE 
state. Assuming mismatch.


 ??? I believe it is related but I am not sure (or maybe I just did 
something wrong). You can find a minimal configuration file attached to 
reproduce.


 ??? Thanks in advance for your help,


Regards,


Luca

-------------- next part --------------
acl localnet src 0.0.0.1-0.255.255.255	# RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8		# RFC 1918 local private network (LAN)
acl localnet src 100.64.0.0/10		# RFC 6598 shared address space (CGN)
acl localnet src 169.254.0.0/16 	# RFC 3927 link-local (directly plugged) machines
acl localnet src 172.16.0.0/12		# RFC 1918 local private network (LAN)
acl localnet src 192.168.0.0/16		# RFC 1918 local private network (LAN)
acl localnet src fc00::/7       	# RFC 4193 local private network range
acl localnet src fe80::/10      	# RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost

http_port 3128

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

delay_pools 1
delay_class 1 3

external_acl_type ip_user_helper %SRC /usr/lib/squid3/ext_file_userip_acl -f /etc/squid/ips.conf

acl ip_list external ip_user_helper test

http_access allow ip_list

delay_access 1 allow ip_list
delay_access 1 deny all

delay_parameters 1 80000/80000 80000/80000 80000/80000

From rousskov at measurement-factory.com  Thu Jan 17 16:39:40 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Jan 2019 09:39:40 -0700
Subject: [squid-users] External acl on delay_access directive
In-Reply-To: <391cc3f9-129b-ec93-f9f9-faaf16912799@olfeo.com>
References: <391cc3f9-129b-ec93-f9f9-faaf16912799@olfeo.com>
Message-ID: <24bf5ed4-ce6d-2bb6-abbd-bfc9c0052b47@measurement-factory.com>

On 1/17/19 9:13 AM, Luca Savarino wrote:

> WARNING: ip_list ACL is used in context without an ALE
> state. Assuming mismatch.

> delay_access 1 allow ip_list

Looks like a Squid bug to me -- Squid should supply ALE (a blob
containing various transaction details) to the delay_access code but
evidently does not.

If you are a developer or can hire a developer to fix this bug, a good
starting point could be the missing ACLFilledChecklist::al
initialization in DelayId::DelayClient().


HTH,

Alex.


From rousskov at measurement-factory.com  Thu Jan 17 16:43:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Jan 2019 09:43:09 -0700
Subject: [squid-users] Squid does not send request to parent proxy
In-Reply-To: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
References: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
Message-ID: <7997f7e2-6d63-6cd4-ee35-360905864aa0@measurement-factory.com>

On 1/17/19 8:28 AM, Troiano Alessio wrote:

> I'm not able to configure squid for using a parent proxy only for some domain. All the rest should be fetched directly. I tried this configuration:
> cache_peer 172.31.3.70 parent 8080 0 no-query default name=HUBATLDB
> acl domainAT dstdomain voeazul.com.br
> cache_peer_access HUBATLDB allow domainAT
> never_direct allow domainAT

Does turning nonhierarchical_direct off help?

Alex.


> But the site www.voeazul.com.br is fetched direct. This is the access log:
> %SQUID-4: 172.31.0.82 59719 [17/Jan/2019:22:55:36 +0800] "CONNECT www.voeazul.com.br:443 HTTP/1.1" www.voeazul.com.br - - "-" 200 - 816 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0" TCP_TUNNEL:HIER_DIRECT 23.77.9.57 443 53176
> 
> Can you help me?
> 
> Following the full conf:
> 
> #
> # Recommended minimum configuration:
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8# RFC1918 possible internal network
> acl localnet src 172.16.0.0/12# RFC1918 possible internal network
> acl localnet src 192.168.0.0/16# RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> acl SOC_NET src 172.31.0.0/24# SOC Network
> acl SMD src 10.30.0.47/32    # SMD Proxy
> acl Proxy_HK src 172.31.2.64/27    # Proxy Hong Kong Network
> ignore_expect_100 on
> acl nocachesite dstdomain /etc/squid/nocachesite.acl
> 
> acl SSL_ports port 443
> acl SSL_ports port 8443
> acl SSL_ports port 2096         # INC000000012740
> acl SSL_ports port 9091
> acl SSL_ports port 9444         # INC000000013855
> acl SSL_ports port 6082
> acl Safe_ports port 80# http
> acl Safe_ports port 21# ftp
> acl Safe_ports port 443# https
> acl Safe_ports port 70# gopher
> acl Safe_ports port 210# wais
> acl Safe_ports port 1025-65535# unregistered ports
> acl Safe_ports port 280# http-mgmt
> acl Safe_ports port 488# gss-http
> acl Safe_ports port 591# filemaker
> acl Safe_ports port 777# multiling http
> acl CONNECT method CONNECT
> 
> forwarded_for delete
> tcp_outgoing_address 172.31.2.71 SMD
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Only allow cachemgr access from localhost
> http_access allow manager localhost
> http_access allow manager SOC_NET
> http_access deny manager
> 
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> 
> cache_peer 172.31.3.70 parent 8080 0 no-query default name=HUBATLDB
> acl domainAT dstdomain voeazul.com.br
> cache_peer_access HUBATLDB allow domainAT
> never_direct allow domainAT
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> acl PURGE method PURGE
> http_access allow PURGE localhost
> http_access deny PURGE
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 0.0.0.0:8080
> 
> # We recommend you to use at least the following line.
> # migrated automatically by squid-migrate-conf, the original configuration was: hierarchy_stoplist cgi-bin ?
> 
> # Uncomment and adjust the following to add a disk cache directory.
> cache_effective_user squid
> cache_effective_group squid
> cache_dir diskd /home/squid 400000 64 512
> cache_mem 4 GB
> maximum_object_size_in_memory 2 MB
> minimum_object_size 0 KB
> maximum_object_size 100 MB
> cache_swap_low 96
> cache_swap_high 97
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> cache deny nocachesite
> cache allow all
> max_filedesc 8192
> 
> # Leave coredumps in the first cache dir
> coredump_dir /home/squid
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:144020%10080
> refresh_pattern ^gopher:14400%1440
> refresh_pattern -i (/cgi-bin/|\?) 00%0
> refresh_pattern .020%4320
> 
> cache_mgr xxx at xxx.com
> 
> ### BEGIN LOG FOR SIEM ###
> 
> #logformat siem  %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %Ss:%Sh %<a %>p
> #access_log /var/log/squid/access.log siem
> logformat custom_squid %%SQUID-4: %>a %>p [%tl] "%rm %ru HTTP/%rv" %<A %ui %un "%rp" %Hs %mt %<st "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh %<a %<p %<lp
> access_log /var/log/squid/rsa/access.log custom_squid
> 
> ### END LOG FOR SIEM ###
> dns_v4_first on
> log_icp_queries off
> via off



From rousskov at measurement-factory.com  Thu Jan 17 17:02:39 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Jan 2019 10:02:39 -0700
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
Message-ID: <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>

On 1/16/19 10:30 PM, Dmitry Melekhov wrote:

> 2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)


> In access log:

> 1547702300.945????? 0 192.168.22.229 NONE/503 329 GET https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/- text/html

> 1547702301.304???? 84 - TCP_MISS/404 162 GET http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0 - HIER_DIRECT/91.199.212.52 text/html

Your Squid (or some helper) appears to be adding an
"-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
resulting in a 404 response from that server. That suffix is not present
in the lkk-udm.esplus.ru certificate AFAICT:

> $ openssl x509 -in cert.pem -noout -text | fgrep http:
> URI:http://crl.comodoca.com/COMODORSADomainValidationSecureServerCA.crl
> CA Issuers - URI:http://crt.comodoca.com/COMODORSADomainValidationSecureServerCA.crt
> OCSP - URI:http://ocsp.comodoca.com

Alex.


From eliezer at ngtech.co.il  Thu Jan 17 17:03:12 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 17 Jan 2019 19:03:12 +0200
Subject: [squid-users] Digicert replacing couple root CA,
	why it wasn't mentioned here?
Message-ID: <04be01d4ae86$87ab53f0$9701fbd0$@ngtech.co.il>

I noticed that there was a change in the RootCA world:

https://www.digicert.com/replace-your-symantec-ssl-tls-certificates/

 

Anyone else knew about it?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190117/ab8c7bba/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11298 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190117/ab8c7bba/attachment.png>

From rousskov at measurement-factory.com  Thu Jan 17 17:11:50 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Jan 2019 10:11:50 -0700
Subject: [squid-users] Squid 4.5 crashes on FreeBSD
In-Reply-To: <20190117114105.18b59737@efreet.kappastar.com>
References: <20190117114105.18b59737@efreet.kappastar.com>
Message-ID: <b385e1e8-e920-0ff7-b035-4e51d308b983@measurement-factory.com>

On 1/17/19 3:41 AM, Marko Cupa? wrote:

> I am using Squid 4.5, and it crashes, giving the following in cache.log:
> assertion failed: stmem.cc:98: "lowestOffset () <= target_offset"

Probably bug #4823:
https://bugs.squid-cache.org/show_bug.cgi?id=4823

Alex.


From rousskov at measurement-factory.com  Thu Jan 17 17:22:46 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Jan 2019 10:22:46 -0700
Subject: [squid-users] Backing up squid cache and restoring it
In-Reply-To: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>
References: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>
Message-ID: <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>

On 1/17/19 6:47 AM, Arne-Tobias Rak wrote:

> my goal is to restore a previous cache state in squid 3.x running on
> Ubuntu 16.04.
> 
> So far I have tried to create a copy of the /var/spool/squid and
> /var/log/squid folders.
> When restoring the cache, I first shutdown squid using
> /sudo squid -k shutdown//
> //sudo service squid stop -k
> /and then restore the previously copied folder contents. I then start
> squid again using
> /sudo service squid start./

> Unfortunately, this does not restore the previous cache contents, as the
> spool/squid/swap.state file is modified during squid startup.

Modification of swap.state upon startup is not incompatible with cache
contents preservation.

In a clean shutdown context, the swap.state* files are essentially an
optimization. You may preserve/restore them if you want to speed up
building of the restored cache index OR you can delete them (and Squid
will slowly build a new cache index from scratch). Just do not leave the
newer swap.state* files when trying to restore the old cache.

If you preserve/restore the old swap.state* files, you may need to
preserve their timestamps as well.


If you need further help, please share cache.log entries related to
cache_dir loading and indicate why you think the old cache contents is
not preserved. Sharing your cache_dir configuration may also help.


Cheers,

Alex.


From alex at nanogherkin.com  Thu Jan 17 17:42:50 2019
From: alex at nanogherkin.com (Alex Crow)
Date: Thu, 17 Jan 2019 17:42:50 +0000
Subject: [squid-users] Digicert replacing couple root CA,
 why it wasn't mentioned here?
In-Reply-To: <04be01d4ae86$87ab53f0$9701fbd0$@ngtech.co.il>
References: <04be01d4ae86$87ab53f0$9701fbd0$@ngtech.co.il>
Message-ID: <53e7a306-cab2-819e-3aef-6fdef7b1bc26@nanogherkin.com>

It was all over the IT news sites I read (Register, Slashdot, etc). 
Changed all our Thawte certs from Symantec to Digicert a few months ago. 
Pretty painless actually.

Alex

On 17/01/2019 17:03, Eliezer Croitoru wrote:
>
> I noticed that there was a change in the RootCA world:
>
> https://www.digicert.com/replace-your-symantec-ssl-tls-certificates/
>
> Anyone else knew about it?
>
> Thanks,
>
> Eliezer
>
>

--
Insert pointless drivel here.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190117/5d7aecd3/attachment.htm>

From eliezer at ngtech.co.il  Fri Jan 18 07:52:46 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Fri, 18 Jan 2019 09:52:46 +0200
Subject: [squid-users] A nice StoreID helper I have seen: squid_dedup
Message-ID: <008b01d4af02$cd2ab2f0$678018d0$@ngtech.co.il>

I have seen that there is a very nice squid de-duplication helper at:
https://github.com/frispete/squid_dedup
 
I think it's worth adding into the squid-cache Related Software section.
 
Eliezer
 
----
 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190118/253d6093/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190118/253d6093/attachment.png>

From alessio.troiano at leonardocompany.com  Fri Jan 18 08:17:24 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 18 Jan 2019 08:17:24 +0000
Subject: [squid-users] R:  Squid does not send request to parent proxy
In-Reply-To: <7997f7e2-6d63-6cd4-ee35-360905864aa0@measurement-factory.com>
References: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
 <7997f7e2-6d63-6cd4-ee35-360905864aa0@measurement-factory.com>
Message-ID: <1bfca35e15184cc9a615f844c30aa956@ocgepvsw3101.ocr.priv>

Same result Alex:
%SQUID-4: 172.31.0.82 54345 [18/Jan/2019:16:16:10 +0800] "GET http://www.voeazul.com.br/ HTTP/1.1" www.voeazul.com.br - - "/" 403 text/html 726 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0" TCP_MISS:HIER_DIRECT 23.77.9.57 80 40266



Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From squid3 at treenet.co.nz  Fri Jan 18 08:38:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Jan 2019 21:38:50 +1300
Subject: [squid-users] A nice StoreID helper I have seen: squid_dedup
In-Reply-To: <008b01d4af02$cd2ab2f0$678018d0$@ngtech.co.il>
References: <008b01d4af02$cd2ab2f0$678018d0$@ngtech.co.il>
Message-ID: <a2bb22be-f468-679f-26fb-df88b9185d3f@treenet.co.nz>

On 18/01/19 8:52 pm, eliezer wrote:
> I have seen that there is a very nice squid de-duplication helper at:
> 
> https://github.com/frispete/squid_dedup
> 
> ?
> 
> I think it?s worth adding into the squid-cache Related Software section.
> 

Then please fill out the form at the end of the website page you think
it should be added to. Or better, contact the software author to ask
them to do it so we have their choice of contact address and blurb.


Cheers
Amos


From squid3 at treenet.co.nz  Fri Jan 18 09:07:36 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Jan 2019 22:07:36 +1300
Subject: [squid-users] Squid does not send request to parent proxy
In-Reply-To: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
References: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
Message-ID: <44619dfe-478b-900c-832e-41b7c38b8f3f@treenet.co.nz>

On 18/01/19 4:28 am, Troiano Alessio wrote:
> Hello all,
> I'm not able to configure squid for using a parent proxy only for some domain. All the rest should be fetched directly. I tried this configuration:
> cache_peer 172.31.3.70 parent 8080 0 no-query default name=HUBATLDB
> acl domainAT dstdomain voeazul.com.br
> cache_peer_access HUBATLDB allow domainAT
> never_direct allow domainAT

That is the correct design.  It does not work for you because you put
the wrong domain name in the domainAT ACL.


Look at the log carefully. See how the domain the client is asking for
is actually "www.voeazul.com.br". Even a single character difference
makes it an entirely different domain name - the "www." bit matters.


If you want domainAT to do exact-match then you need to add the www.*
sub-domain to the list. Like this:
 acl domainAT dstdomain voeazul.com.br www.voeazul.com.br


Or, you can use a wildcard (start with a '.') to match that domain and
all its sub-domains. Like this:

  acl domainAT dstdomain .voeazul.com.br



> 
> But the site www.voeazul.com.br is fetched direct. This is the access log:
> %SQUID-4: 172.31.0.82 59719 [17/Jan/2019:22:55:36 +0800] "CONNECT www.voeazul.com.br:443 HTTP/1.1" www.voeazul.com.br - - "-" 200 - 816 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0" TCP_TUNNEL:HIER_DIRECT 23.77.9.57 443 53176
> 
> Can you help me?
> 

What Squid version are you using?

I see config options which are only valid for Squid-3.1 your setup. If
you are using an old Squid please try an upgrade, or start planning to
do one. There are many security vulnerabilities which affect those very
old Squid-3 and some cannot be fixed there, so even versions with LTS
security support are vulnerable.


Cheers
Amos


From alessio.troiano at leonardocompany.com  Fri Jan 18 09:25:13 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 18 Jan 2019 09:25:13 +0000
Subject: [squid-users] R:  Squid does not send request to parent proxy
In-Reply-To: <44619dfe-478b-900c-832e-41b7c38b8f3f@treenet.co.nz>
References: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
 <44619dfe-478b-900c-832e-41b7c38b8f3f@treenet.co.nz>
Message-ID: <08c56c5f11bd41d0acdadda0719642a0@ocgepvsw3101.ocr.priv>

Thank you Amos in this way it works.
But that was an example, in my config I should use acl from file, and it doesn't work. So this is the relevant config:

acl parentproxyHUBAT dstdomain /etc/squid/hubatsite.acl
cache_peer 172.31.3.70 parent 8080 0 no-query default name=HUBATLDB
cache_peer_access HUBATLDB allow parentproxyHUBAT
never_direct allow parentproxyHUBAT

the file is:
[root at HUB-HK-PRX-03 squid]# ll hubatsite.acl
-rw-r--r-- 1 squid squid 168 Jan 17 23:18 hubatsite.acl
[root at HUB-HK-PRX-03 squid]# cat hubatsite.acl
.carefirst.com
employer.carefirst.com
cfsecuremail.carefirst.com
broker.carefirst.com
.wbmason.com
www.wbmason.com
images.wbmason.com
.voeazul.com.br
www.voeazul.com.br[root at HUB-HK-PRX-03 squid]#

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From squid3 at treenet.co.nz  Fri Jan 18 10:03:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 Jan 2019 23:03:47 +1300
Subject: [squid-users] R:  Squid does not send request to parent proxy
In-Reply-To: <08c56c5f11bd41d0acdadda0719642a0@ocgepvsw3101.ocr.priv>
References: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
 <44619dfe-478b-900c-832e-41b7c38b8f3f@treenet.co.nz>
 <08c56c5f11bd41d0acdadda0719642a0@ocgepvsw3101.ocr.priv>
Message-ID: <c8d51e86-8e05-b2e0-8799-53db2cee4797@treenet.co.nz>

On 18/01/19 10:25 pm, Troiano Alessio wrote:
> Thank you Amos in this way it works.
> But that was an example, in my config I should use acl from file, and it doesn't work. So this is the relevant config:
> 
> acl parentproxyHUBAT dstdomain /etc/squid/hubatsite.acl


Filenames need to be double-quoted.

   acl parentproxyHUBAT dstdomain "/etc/squid/hubatsite.acl"


Amos


From alessio.troiano at leonardocompany.com  Fri Jan 18 10:09:34 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 18 Jan 2019 10:09:34 +0000
Subject: [squid-users] R: R:  Squid does not send request to parent proxy
In-Reply-To: <c8d51e86-8e05-b2e0-8799-53db2cee4797@treenet.co.nz>
References: <a4653bc1976d4036a3a3f2df58a07664@ocgepvsw3101.ocr.priv>
 <44619dfe-478b-900c-832e-41b7c38b8f3f@treenet.co.nz>
 <08c56c5f11bd41d0acdadda0719642a0@ocgepvsw3101.ocr.priv>
 <c8d51e86-8e05-b2e0-8799-53db2cee4797@treenet.co.nz>
Message-ID: <5211af25f7e7485e89edebb43bea1069@ocgepvsw3101.ocr.priv>

Nice! Thank you so much Amos.

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From dm at belkam.com  Fri Jan 18 11:35:40 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Fri, 18 Jan 2019 15:35:40 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
Message-ID: <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>


17.01.2019 21:02, Alex Rousskov ?????:
> On 1/16/19 10:30 PM, Dmitry Melekhov wrote:
>
>> 2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
>
>> In access log:
>> 1547702300.945????? 0 192.168.22.229 NONE/503 329 GET https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/- text/html
>> 1547702301.304???? 84 - TCP_MISS/404 162 GET http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0 - HIER_DIRECT/91.199.212.52 text/html
> Your Squid (or some helper) appears to be adding an
> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
> resulting in a 404 response from that server. That suffix is not present
> in the lkk-udm.esplus.ru certificate AFAICT:


Yes, I suspected this, there is no helper which can add this, as far as 
I know, I'm out of office till Monday, I'll turn everything possible off 
on Monday, and retest,

but I don't th think is is helper...

Could you tell me -? can squid add this and , if yes, how can I turn 
this off?


Thank you!


>> $ openssl x509 -in cert.pem -noout -text | fgrep http:
>> URI:http://crl.comodoca.com/COMODORSADomainValidationSecureServerCA.crl
>> CA Issuers - URI:http://crt.comodoca.com/COMODORSADomainValidationSecureServerCA.crt
>> OCSP - URI:http://ocsp.comodoca.com


From rousskov at measurement-factory.com  Fri Jan 18 15:31:47 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Jan 2019 08:31:47 -0700
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
Message-ID: <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>

On 1/18/19 4:35 AM, Dmitry Melekhov wrote:
> 
> 17.01.2019 21:02, Alex Rousskov ?????:
>> On 1/16/19 10:30 PM, Dmitry Melekhov wrote:
>>
>>> 2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55:
>>> error:14090086:SSL routines:ssl3_get_server_certificate:certificate
>>> verify failed (1/-1/0)
>>
>>> In access log:
>>> 1547702300.945????? 0 192.168.22.229 NONE/503 329 GET
>>> https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/-
>>> text/html
>>> 1547702301.304???? 84 - TCP_MISS/404 162 GET
>>> http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0
>>> - HIER_DIRECT/91.199.212.52 text/html
>> Your Squid (or some helper) appears to be adding an
>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>> resulting in a 404 response from that server.

> Yes, I suspected this, there is no helper which can add this, as far as
> I know

> can squid add this

Squid itself does not add non-trivial paths to URLs. If your Squid does
not have a URL rewriter or an adaptation service, and the certificate
your Squid receives does not containt that weird URL, then this is
probably a Squid bug such as using an "unterminated c-string" when
forming the request URL. If you can reproduce, it may be fairly easy to
distinguish bugs from helpers from certificates as the source of this
problem using an ALL,9 cache.log.
 Alex.


From squid3 at treenet.co.nz  Fri Jan 18 17:08:52 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 19 Jan 2019 06:08:52 +1300
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
Message-ID: <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>

On 19/01/19 4:31 am, Alex Rousskov wrote:
> On 1/18/19 4:35 AM, Dmitry Melekhov wrote:
>>
>> 17.01.2019 21:02, Alex Rousskov ?????:
>>> On 1/16/19 10:30 PM, Dmitry Melekhov wrote:
>>>
>>>> 2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55:
>>>> error:14090086:SSL routines:ssl3_get_server_certificate:certificate
>>>> verify failed (1/-1/0)
>>>
>>>> In access log:
>>>> 1547702300.945????? 0 192.168.22.229 NONE/503 329 GET
>>>> https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/-
>>>> text/html
>>>> 1547702301.304???? 84 - TCP_MISS/404 162 GET
>>>> http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0
>>>> - HIER_DIRECT/91.199.212.52 text/html
>>> Your Squid (or some helper) appears to be adding an
>>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>>> resulting in a 404 response from that server.
> 
>> Yes, I suspected this, there is no helper which can add this, as far as
>> I know
> 


These mangled URLs are the expected result of a URL-rewrite/redirector
helper written to use the long ago deprecated Squid-1.x version of
helper protocol. Being used in a Squid configured to allow whitespace in
URLs.

When those two features are combined there is no way for Squid to
identify garbage after the end of URL in helper 1.0 syntax response,
from a v2.x syntax response with whitespace in the URL.

Squid-3.5 and later are only backward compatible to the Squid-2.0 helper
protocol. The older syntax is no longer supported at all.


Details of the Squid helper protocol can be found at
<https://wiki.squid-cache.org/Features/AddonHelpers#URL_manipulation>.


Amos


From eliezer at ngtech.co.il  Sun Jan 20 22:02:55 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Jan 2019 00:02:55 +0200
Subject: [squid-users] What's the best way to ban Let's encrypt based
	certificates? or whitelist a very narrow list of Root and
	Intermediates CA?
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>

OK so from the real world:

What's the best way to ban Let's encrypt based certificates? or whitelist a
very narrow list of Root and Intermediates CA?

 

I have a setup which one of the requirements is to restrict access to sites
which depends on Let's encrypt generated certificates.

The issue is that these sites are encrypted but do not offer any way of
assuring real ISO and couple other compatibilities of the ORG.

For a simple home user it's fine most of the time but for some it's not.

The most simple way is to block the specific domain but I need to know if
the site certificate is from Let's encrypt.


I was thinking about an external ACL helper that might check it for squid if
squid or openssl doesn't have currently an option to implement it.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190121/fc4cc0ee/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190121/fc4cc0ee/attachment.png>

From eliezer at ngtech.co.il  Sun Jan 20 22:04:40 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Jan 2019 00:04:40 +0200
Subject: [squid-users] Backing up squid cache and restoring it
In-Reply-To: <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>
References: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>
 <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>
Message-ID: <01e501d4b10c$2439d200$6cad7600$@ngtech.co.il>

Just to make sure:
This swap.state behavior is only for UFS/AUFS cache_dir or also to rock?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Thursday, January 17, 2019 19:23
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Backing up squid cache and restoring it

On 1/17/19 6:47 AM, Arne-Tobias Rak wrote:

> my goal is to restore a previous cache state in squid 3.x running on
> Ubuntu 16.04.
> 
> So far I have tried to create a copy of the /var/spool/squid and
> /var/log/squid folders.
> When restoring the cache, I first shutdown squid using
> /sudo squid -k shutdown//
> //sudo service squid stop -k
> /and then restore the previously copied folder contents. I then start
> squid again using
> /sudo service squid start./

> Unfortunately, this does not restore the previous cache contents, as the
> spool/squid/swap.state file is modified during squid startup.

Modification of swap.state upon startup is not incompatible with cache
contents preservation.

In a clean shutdown context, the swap.state* files are essentially an
optimization. You may preserve/restore them if you want to speed up
building of the restored cache index OR you can delete them (and Squid
will slowly build a new cache index from scratch). Just do not leave the
newer swap.state* files when trying to restore the old cache.

If you preserve/restore the old swap.state* files, you may need to
preserve their timestamps as well.


If you need further help, please share cache.log entries related to
cache_dir loading and indicate why you think the old cache contents is
not preserved. Sharing your cache_dir configuration may also help.


Cheers,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sun Jan 20 22:15:42 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 21 Jan 2019 00:15:42 +0200
Subject: [squid-users] https debug
In-Reply-To: <8537164e-6430-055e-0263-22d701c1215f@treenet.co.nz>
References: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>
 <ca313cd2-4344-d480-4563-82f9f1e43900@treenet.co.nz>
 <b91ec8b7d7ae6339e12e7abbb63869cd@tiscali.it>
 <8537164e-6430-055e-0263-22d701c1215f@treenet.co.nz>
Message-ID: <01e701d4b10d$af2389a0$0d6a9ce0$@ngtech.co.il>

You probably meant 4.5...
http://www1.ngtech.co.il/repo/centos/7/x86_64/squid-4.5-1.el7.x86_64.rpm

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, January 2, 2019 12:01
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] https debug

On 2/01/19 10:30 pm, Sampei wrote:
> About way to use https protocol I think I use connect tunnel, here

When a CONNECT tunnel is being used and not SSL-Bump'ed then all TLS
related issues are problems with one of the endpoint software. Not
related to the proxy at all. Squid is just blindly relaying the TLS
bytes as-is between the endpoints.

That said, some specific configs may encounter issues due to explicitly
telling Squid to do certain things which cannot be done to CONNECT
tunnels (eg. URL-rewrite, ACL checks of path strings), or to deny the
CONNECT which obviously would make the TLS not "work" at all.


I suspect that in your case some other port is involved which you do not
know about and are thus not letting through Squid. The access.log should
show what Squid is dealing with there.


> parttial of my squid.conf
> 
> 
> acl SSL_ports port 443          # https
> acl SSL_ports port 563          # snews
> ...
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> ...
> http_access deny CONNECT !SSL_ports

Okay, but should be following the Safe_ports check. The default config
orders these checks by how common it is to encounter the attack types
they exist to prevent.

> http_access deny CONNECT !Safe_ports

The default config uses this instead:

 http_access deny !Safe_ports

The purpose of this Safe_ports ACL is to prevent the proxy handling
*any* traffic for protocols whose traffic syntax directly conflicts with
HTTP traffic syntax.

By limiting this check to only CONNECT messages, you are opening your
proxy to most of the attacks the Safe_port ACL was designed to prevent.



> acl test dstdomain example.com
> http_access allow test
> http_access allow CONNECT test

This latter is pointless. "test" was already allowed, so this line is
never reached by any traffic which it can match.


> I think to upgrade 4.x Squid but I'm looking for valid repository for
> Centos 7 which contains this pkg.

The official repositories for CentOS are detailed at
<https://wiki.squid-cache.org/KnowledgeBase/CentOS>

(I see that page needs an update Eliezer now has 4.4 in his main CentOS
repository <http://www1.ngtech.co.il/repo/centos/7/x86_64/>)


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Sun Jan 20 22:20:33 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 20 Jan 2019 15:20:33 -0700
Subject: [squid-users] What's the best way to ban Let's encrypt based
 certificates? or whitelist a very narrow list of Root and Intermediates CA?
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
Message-ID: <902a2270-5905-f546-080d-2124f6001c1f@measurement-factory.com>

On 1/20/19 3:02 PM, Eliezer Croitoru wrote:

> What's the best way to ban Let's encrypt based certificates? or
> whitelist a very narrow list of Root and Intermediates CA?

A requirement to ban all Let's Encrypt sites sounds invalid to me, but
you can use certificate validator to do that. Same for whitelisting CAs.
The corresponding squid.conf directives are sslcrtvalidator_program and
sslcrtvalidator_children. For a rough description of the helper messages
format, please see "certificate validator" at

    https://wiki.squid-cache.org/Features/AddonHelpers

Squid distribution also includes a minimal certificate validation
helper: security_fake_certverify.pl


> I was thinking about an external ACL helper

Some use cases can be addressed using %ssl::<cert_issuer, but it would
be difficult to supply the right info the the external ACL helper in
general because Squid lacks logformat %codes that relay all intermediate
certificates.

Alex.


From rousskov at measurement-factory.com  Sun Jan 20 22:21:26 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 20 Jan 2019 15:21:26 -0700
Subject: [squid-users] Backing up squid cache and restoring it
In-Reply-To: <01e501d4b10c$2439d200$6cad7600$@ngtech.co.il>
References: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>
 <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>
 <01e501d4b10c$2439d200$6cad7600$@ngtech.co.il>
Message-ID: <c782703b-1377-446b-cb9b-5a29f65a82db@measurement-factory.com>

On 1/20/19 3:04 PM, Eliezer Croitoru wrote:

> This swap.state behavior is only for UFS/AUFS cache_dir or also to rock?

Rock cache_dirs do not use swap.state.

Alex.


> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Thursday, January 17, 2019 19:23
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Backing up squid cache and restoring it
> 
> On 1/17/19 6:47 AM, Arne-Tobias Rak wrote:
> 
>> my goal is to restore a previous cache state in squid 3.x running on
>> Ubuntu 16.04.
>>
>> So far I have tried to create a copy of the /var/spool/squid and
>> /var/log/squid folders.
>> When restoring the cache, I first shutdown squid using
>> /sudo squid -k shutdown//
>> //sudo service squid stop -k
>> /and then restore the previously copied folder contents. I then start
>> squid again using
>> /sudo service squid start./
> 
>> Unfortunately, this does not restore the previous cache contents, as the
>> spool/squid/swap.state file is modified during squid startup.
> 
> Modification of swap.state upon startup is not incompatible with cache
> contents preservation.
> 
> In a clean shutdown context, the swap.state* files are essentially an
> optimization. You may preserve/restore them if you want to speed up
> building of the restored cache index OR you can delete them (and Squid
> will slowly build a new cache index from scratch). Just do not leave the
> newer swap.state* files when trying to restore the old cache.
> 
> If you preserve/restore the old swap.state* files, you may need to
> preserve their timestamps as well.
> 
> 
> If you need further help, please share cache.log entries related to
> cache_dir loading and indicate why you think the old cache contents is
> not preserved. Sharing your cache_dir configuration may also help.
> 
> 
> Cheers,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From dm at belkam.com  Mon Jan 21 04:39:49 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Mon, 21 Jan 2019 08:39:49 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
Message-ID: <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>

18.01.2019 21:08, Amos Jeffries ?????:
> On 19/01/19 4:31 am, Alex Rousskov wrote:
>> On 1/18/19 4:35 AM, Dmitry Melekhov wrote:
>>> 17.01.2019 21:02, Alex Rousskov ?????:
>>>> On 1/16/19 10:30 PM, Dmitry Melekhov wrote:
>>>>
>>>>> 2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55:
>>>>> error:14090086:SSL routines:ssl3_get_server_certificate:certificate
>>>>> verify failed (1/-1/0)
>>>>> In access log:
>>>>> 1547702300.945????? 0 192.168.22.229 NONE/503 329 GET
>>>>> https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/-
>>>>> text/html
>>>>> 1547702301.304???? 84 - TCP_MISS/404 162 GET
>>>>> http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0
>>>>> - HIER_DIRECT/91.199.212.52 text/html
>>>> Your Squid (or some helper) appears to be adding an
>>>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>>>> resulting in a 404 response from that server.
>>> Yes, I suspected this, there is no helper which can add this, as far as
>>> I know
>
> These mangled URLs are the expected result of a URL-rewrite/redirector
> helper written to use the long ago deprecated Squid-1.x version of
> helper protocol. Being used in a Squid configured to allow whitespace in
> URLs.
>
> When those two features are combined there is no way for Squid to
> identify garbage after the end of URL in helper 1.0 syntax response,
> from a v2.x syntax response with whitespace in the URL.
>
> Squid-3.5 and later are only backward compatible to the Squid-2.0 helper
> protocol. The older syntax is no longer supported at all.
>
>
> Details of the Squid helper protocol can be found at
> <https://wiki.squid-cache.org/Features/AddonHelpers#URL_manipulation>.
>


Thank you!

You are absolutely right.

This is redirector, if I disable it, everything works.

Will contact redirector developer.



> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




From squid3 at treenet.co.nz  Mon Jan 21 08:09:56 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Jan 2019 21:09:56 +1300
Subject: [squid-users] https debug
In-Reply-To: <01e701d4b10d$af2389a0$0d6a9ce0$@ngtech.co.il>
References: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>
 <ca313cd2-4344-d480-4563-82f9f1e43900@treenet.co.nz>
 <b91ec8b7d7ae6339e12e7abbb63869cd@tiscali.it>
 <8537164e-6430-055e-0263-22d701c1215f@treenet.co.nz>
 <01e701d4b10d$af2389a0$0d6a9ce0$@ngtech.co.il>
Message-ID: <66724540-02b7-c715-0def-76d615e09391@treenet.co.nz>

On 21/01/19 11:15 am, Eliezer Croitoru wrote:
> You probably meant 4.5...
> http://www1.ngtech.co.il/repo/centos/7/x86_64/squid-4.5-1.el7.x86_64.rpm
> 

Time travel ...

> -----Original Message-----
> From: Amos Jeffries
> Sent: Wednesday, January 2, 2019 12:01

... back when 4.4 was all you had.

;-P

Amos


From ml at netfence.it  Mon Jan 21 08:51:12 2019
From: ml at netfence.it (Andrea Venturoli)
Date: Mon, 21 Jan 2019 09:51:12 +0100
Subject: [squid-users] What's the best way to ban Let's encrypt based
 certificates? or whitelist a very narrow list of Root and Intermediates CA?
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
Message-ID: <76a18c51-d9b6-9c65-8132-b9d5d799dd32@netfence.it>

On 1/20/19 11:02 PM, Eliezer Croitoru wrote:

> The issue is that these sites are encrypted but do not offer any way of 
> assuring real ISO and couple other compatibilities of the ORG.
> 
> For a simple home user it?s fine most of the time but for some it?s not.

Just out of curiosity, could you better explain this?
Pointer are enough if you prefer.

  bye & Thanks
	av.


From squid3 at treenet.co.nz  Mon Jan 21 09:46:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 21 Jan 2019 22:46:50 +1300
Subject: [squid-users] What's the best way to ban Let's encrypt based
 certificates? or whitelist a very narrow list of Root and Intermediates CA?
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
Message-ID: <c8d52170-f4a9-2739-c228-30012d39c4a6@treenet.co.nz>

On 21/01/19 11:02 am, Eliezer Croitoru wrote:
> OK so from the real world:
> 
> What's the best way to ban Let's encrypt based certificates? or
> whitelist a very narrow list of Root and Intermediates CA?
> 


Besides what Alex has answered to your first question. I think the
simpler approach would be the second, and probably more what you need
anyway...

 tls_outgoing_options default-ca=off cafile=X.pem cafile=Y.pem


That makes Squid outgoing connections *not* use the global Trusted CA
set. Then explicitly load the individual one(s) you *do* want to trust.

A whitelist - but only for the root / self-signed CA certs. Intermediary
CAs inherit their trust (or lack) from their root CA.

If intermediary CA trust matters to your situation then a custom
validator as mentioned by Alex would be necessary.

NP: You can list cafile=... as many times as you wish to load multiple
files and should be able to load multiple CA certs in any of the
file(s). But have not confirmed that latter.

cache_peer has matching options with "tls-" prefix.

Amos


From dm at belkam.com  Mon Jan 21 10:30:33 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Mon, 21 Jan 2019 14:30:33 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
Message-ID: <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>

21.01.2019 8:39, Dmitry Melekhov ?????:
> 18.01.2019 21:08, Amos Jeffries ?????:
>> On 19/01/19 4:31 am, Alex Rousskov wrote:
>>> On 1/18/19 4:35 AM, Dmitry Melekhov wrote:
>>>> 17.01.2019 21:02, Alex Rousskov ?????:
>>>>> On 1/16/19 10:30 PM, Dmitry Melekhov wrote:
>>>>>
>>>>>> 2019/01/17 09:18:21 kid1| ERROR: negotiating TLS on FD 55:
>>>>>> error:14090086:SSL routines:ssl3_get_server_certificate:certificate
>>>>>> verify failed (1/-1/0)
>>>>>> In access log:
>>>>>> 1547702300.945????? 0 192.168.22.229 NONE/503 329 GET
>>>>>> https://lkk-udm.esplus.ru/Services/Auth.asmx/Safe? dm HIER_NONE/-
>>>>>> text/html
>>>>>> 1547702301.304???? 84 - TCP_MISS/404 162 GET
>>>>>> http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt-/ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff-GETmyip=-myport=0 
>>>>>>
>>>>>> - HIER_DIRECT/91.199.212.52 text/html
>>>>> Your Squid (or some helper) appears to be adding an
>>>>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>>>>> resulting in a 404 response from that server.
>>>> Yes, I suspected this, there is no helper which can add this, as 
>>>> far as
>>>> I know
>>
>> These mangled URLs are the expected result of a URL-rewrite/redirector
>> helper written to use the long ago deprecated Squid-1.x version of
>> helper protocol. Being used in a Squid configured to allow whitespace in
>> URLs.
>>
>> When those two features are combined there is no way for Squid to
>> identify garbage after the end of URL in helper 1.0 syntax response,
>> from a v2.x syntax response with whitespace in the URL.
>>
>> Squid-3.5 and later are only backward compatible to the Squid-2.0 helper
>> protocol. The older syntax is no longer supported at all.
>>
>>
>> Details of the Squid helper protocol can be found at
>> <https://wiki.squid-cache.org/Features/AddonHelpers#URL_manipulation>.
>>
>
>
> Thank you!
>
> You are absolutely right.
>
> This is redirector, if I disable it, everything works.
>
> Will contact redirector developer.


There is? new rejik (rejik.ru) version, if somebody uses it, you can 
solve this problem by upgrade.


Thank you!





From numsys at free.fr  Mon Jan 21 10:35:55 2019
From: numsys at free.fr (FredB)
Date: Mon, 21 Jan 2019 11:35:55 +0100
Subject: [squid-users] ICAP and 403 Encapsulated answers (SSL denied domains)
Message-ID: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>

Hello all,

I'm playing with Squid4 and e2guardian as ICAP server.

I'm seeing something I misunderstand, when a SSL website is blocked 
e2guardian returns a encapsulated "HTTP/1.1 403 Forbidden" header this 
part seems good to me with an encrypted website a denied or redirection 
page can't be added

But unfortunately Squid adds a "Connection: keep-alive" header and if I 
just reload the page I'm waiting a timeout a long moment, (and there is 
no ICAP request between squid and e2) it's like the previous connection 
still opened.

So the first request is well denied, but the second is without answer

I tried to add "Connection: close" in encapsulated header from 
e2guardian without more success, but anyway "Connection: close" value is 
removed by squid

I'm doing something wrong ? This wastes connections and from user point 
of view the proxy is (very) slow, for example with ADS filtering some 
websites freezes

FI the request is well denied in squid and E2 logs

Maybe this is a bug, but I don't known if the issue is from Squid or E2 
? What is the correct response from an ICAP server with a denied SSL 
website request ?

Thank you

Fred





From lsavarino at olfeo.com  Mon Jan 21 12:22:05 2019
From: lsavarino at olfeo.com (Luca Savarino)
Date: Mon, 21 Jan 2019 13:22:05 +0100
Subject: [squid-users] External acl on delay_access directive
In-Reply-To: <24bf5ed4-ce6d-2bb6-abbd-bfc9c0052b47@measurement-factory.com>
References: <391cc3f9-129b-ec93-f9f9-faaf16912799@olfeo.com>
 <24bf5ed4-ce6d-2bb6-abbd-bfc9c0052b47@measurement-factory.com>
Message-ID: <0a1dc717-202c-b58f-00ca-fa6f487b30ef@olfeo.com>

Hello Alex,


 ??? ??? Thank you for your help. Attached is a patch which seems to fix 
the issue. Does it seem correct to you ?


Regards,


Luca

On 1/17/19 5:39 PM, Alex Rousskov wrote:
> On 1/17/19 9:13 AM, Luca Savarino wrote:
>
>> WARNING: ip_list ACL is used in context without an ALE
>> state. Assuming mismatch.
>> delay_access 1 allow ip_list
> Looks like a Squid bug to me -- Squid should supply ALE (a blob
> containing various transaction details) to the delay_access code but
> evidently does not.
>
> If you are a developer or can hire a developer to fix this bug, a good
> starting point could be the missing ACLFilledChecklist::al
> initialization in DelayId::DelayClient().
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0001-Initialize-ALE-for-delay_pools.patch
Type: text/x-patch
Size: 835 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190121/746fb0d0/attachment.bin>

From dm at belkam.com  Mon Jan 21 16:08:58 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Mon, 21 Jan 2019 20:08:58 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
Message-ID: <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>


21.01.2019 14:30, Dmitry Melekhov ?????:
> Your Squid (or some helper) appears to be adding an
>>>>>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>>>>>> resulting in a 404 response from that server.
>>>>> Yes, I suspected this, there is no helper which can add this, as 
>>>>> far as
>>>>> I know
>>>
>>> These mangled URLs are the expected result of a URL-rewrite/redirector
>>> helper written to use the long ago deprecated Squid-1.x version of
>>> helper protocol. Being used in a Squid configured to allow 
>>> whitespace in
>>> URLs.
>>>
>>> When those two features are combined there is no way for Squid to
>>> identify garbage after the end of URL in helper 1.0 syntax response,
>>> from a v2.x syntax response with whitespace in the URL.
>>>
>>> Squid-3.5 and later are only backward compatible to the Squid-2.0 
>>> helper
>>> protocol. The older syntax is no longer supported at all.
>>>
>>>
>>> Details of the Squid helper protocol can be found at
>>> <https://wiki.squid-cache.org/Features/AddonHelpers#URL_manipulation>.
>>>
>>
>>
>> Thank you!
>>
>> You are absolutely right.
>>
>> This is redirector, if I disable it, everything works.
>>
>> Will contact redirector developer.
>
>
> There is? new rejik (rejik.ru) version, if somebody uses it, you can 
> solve this problem by upgrade.
>

btw, according to redirector developer problem is in lack of ipv6 support.

Is there any reasons squid sends ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff 
to redirector?


Thank you!


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190121/b16a8525/attachment.htm>

From rousskov at measurement-factory.com  Mon Jan 21 17:28:10 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Jan 2019 10:28:10 -0700
Subject: [squid-users] ICAP and 403 Encapsulated answers (SSL denied
 domains)
In-Reply-To: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>
References: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>
Message-ID: <8cfb73a6-6cbb-4907-f304-a43487ae3856@measurement-factory.com>

On 1/21/19 3:35 AM, FredB wrote:

> I'm playing with Squid4 and e2guardian as ICAP server.
> 
> I'm seeing something I misunderstand, when a SSL website is blocked
> e2guardian returns a encapsulated "HTTP/1.1 403 Forbidden" header this
> part seems good to me with an encrypted website a denied or redirection
> page can't be added

> But unfortunately Squid adds a "Connection: keep-alive" header

It is not clear _why_ you consider that header "unfortunate" and the
connection "wasted". That header may or may not be wrong, and the
connection may or may not be reusable, depending on many factors (that
you have not shared).


> and if I
> just reload the page I'm waiting a timeout a long moment, (and there is
> no ICAP request between squid and e2) it's like the previous connection
> still opened.
> 
> So the first request is well denied, but the second is without answer

Can the browser reuse the connection after receiving the HTTP 403
(Forbidden) response? Does it? If you provide a sample of client-Squid
request and response headers (including CONNECT messages, if any), and
specify whether they were all sharing the same TCP connection, then we
may be able to assign the blame for the "timeout".

If (some of) the messages are encrypted, providing ALL,2 cache.log may
work. Otherwise, a packet capture (in pcap format) is probably the
easiest sharing method.


> I tried to add "Connection: close" in encapsulated header from
> e2guardian without more success, but anyway "Connection: close" value is
> removed by squid

Yes, by ICAP design, an ICAP service does not have direct control over
HTTP connections maintained by the host application (e.g., Squid).

Alex.


From rousskov at measurement-factory.com  Mon Jan 21 17:46:42 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Jan 2019 10:46:42 -0700
Subject: [squid-users] External acl on delay_access directive
In-Reply-To: <0a1dc717-202c-b58f-00ca-fa6f487b30ef@olfeo.com>
References: <391cc3f9-129b-ec93-f9f9-faaf16912799@olfeo.com>
 <24bf5ed4-ce6d-2bb6-abbd-bfc9c0052b47@measurement-factory.com>
 <0a1dc717-202c-b58f-00ca-fa6f487b30ef@olfeo.com>
Message-ID: <84b87722-5c0b-75c7-b516-923e1e83865e@measurement-factory.com>

On 1/21/19 5:22 AM, Luca Savarino wrote:

> Attached is a patch which seems to fix the issue.

Glad you have a fix that works for you, but this mailing list is not the
right place for patch reviews. If you want to submit your changes to the
Squid project, I suggest creating a GitHub pull request. The procedure
is outlined at https://wiki.squid-cache.org/MergeProcedure


> Does it seem correct to you ?

The patch is buggy: Setting AccessLogEntry::reply field like that may
lead to memory leaks and/or crashes. Ideally, the reply field should be
set when the reply becomes known. I do not know whether that is already
done (elsewhere in the code). If it is done, than the reply setting line
in the patch can be removed. You can answer that question for your
particular use case by checking (e.g., in a debugger or by adding a
debugs() message) whether http->al->reply is nil before the assignment.


HTH,

Alex.


> On 1/17/19 5:39 PM, Alex Rousskov wrote:
>> On 1/17/19 9:13 AM, Luca Savarino wrote:
>>
>>> WARNING: ip_list ACL is used in context without an ALE
>>> state. Assuming mismatch.
>>> delay_access 1 allow ip_list
>> Looks like a Squid bug to me -- Squid should supply ALE (a blob
>> containing various transaction details) to the delay_access code but
>> evidently does not.
>>
>> If you are a developer or can hire a developer to fix this bug, a good
>> starting point could be the missing ACLFilledChecklist::al
>> initialization in DelayId::DelayClient().
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Mon Jan 21 18:29:52 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Jan 2019 11:29:52 -0700
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
 <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
Message-ID: <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>

On 1/21/19 9:08 AM, Dmitry Melekhov wrote:

>> Your Squid (or some helper) appears to be adding an
>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>> resulting in a 404 response from that server.

> Is there any reasons squid sends ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff
> to redirector?

What Squid logformat %code or url_rewrite_extras %code does that address
come from? Should the corresponding request have that address? For
example, internally-generated requests do not have HTTP client addresses.

Will the redirector work if that address is sent as a "-" instead of
"ff...fff"?


Cheers,

Alex.


From michael at hendrie.id.au  Tue Jan 22 05:47:54 2019
From: michael at hendrie.id.au (Michael Hendrie)
Date: Tue, 22 Jan 2019 16:17:54 +1030
Subject: [squid-users] TCP_TUNNEL and ecap
Message-ID: <5F38A577-1BB5-4940-8C25-7A7019B55C51@hendrie.id.au>

Hi All,

I have an ecap adapter that amongst other things tracks response size.  This works fine for HTTP and ssl-bump'd HTTPS but not for TCP_TUNNEL responses as they are not seen by the ecap adapter.

I understand that in most cases adaptation of a tunnelled HTTPS response is pointless as it would result message corruption but wondering if it is at all possible to get the TCP_TUNNEL response seen by ecap, I cant see a config option for it in 3.5 or 4.5.

Thanks, 

Michael

From dm at belkam.com  Tue Jan 22 05:52:30 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Tue, 22 Jan 2019 09:52:30 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
 <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
 <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>
Message-ID: <50aef43a-ce10-96aa-44fb-021c27423792@belkam.com>

21.01.2019 22:29, Alex Rousskov ?????:
> On 1/21/19 9:08 AM, Dmitry Melekhov wrote:
>
>>> Your Squid (or some helper) appears to be adding an
>>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>>> resulting in a 404 response from that server.
>> Is there any reasons squid sends ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff
>> to redirector?
> What Squid logformat %code or url_rewrite_extras %code does that address
> come from?


default on my case

>   Should the corresponding request have that address? For
> example, internally-generated requests do not have HTTP client addresses.
>
> Will the redirector work if that address is sent as a "-" instead of
> "ff...fff"?

rejik redirector developer thinks its better to use 127.0.0.1 as squid 
address,

but said that this is squid preference...

if can read discussion here 
https://rejik.ru/bb_rus/viewtopic.php?f=1&t=1979 in russian.

Thank you!





From xplod at xplod.de  Tue Jan 22 08:19:19 2019
From: xplod at xplod.de (=?utf-8?Q?XploD?=)
Date: Tue, 22 Jan 2019 08:19:19 +0000
Subject: [squid-users] Squid 4.5 Transparent Proxy,
 StrongSwan VPN - Working in Browser but not in any android apps
Message-ID: <kcis.A04D06E233A54DD98710C10575C244B3@v22018042993464812.supersrv.de>

Hi.



I've got a strange problem, and I don't know if you can help me:

To secure my mobile phone, I have set up a VPN using Strongswan which is used anytime I use an open WiFi hotspot. This works fine.



But to get rid of all the trackers applied to websites and android apps, I want to use a proxy to filter any unwanted communication:

So I have set up squid to intercept both port 80 and 443, with SSL_BUMP, Self-Signed Certificates, ...



In firefox mobile, I had to download the CA-certificate in PEM format, so that firefox asked if I wanted to install the certificate. After doing so, the proxy works just fine, and any website shows the Squid Authority as CA.?



For Chrome, I had to download the CA-Certificate as .crt file. I installed that in Android, so that it is displayed in the user section of the Trusted-CA page. Afte that, Chrome accessed any website without complains, stating that every site was signed by the Squid Authority.



But now my problem:

Any android app I try wants to open an SSL connection to some servers, but none of them does work. Every app either says it has no connection, or shows a certifate mismatch...

Can anybody tell me what I have to do so that every android app accepts the intercepted connection?



Best regards,

Dirk



BTW: If any squid developer is reading this: Squid is awesome work! Thank you very much for such beauty!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190122/58b4e634/attachment.htm>

From numsys at free.fr  Tue Jan 22 08:22:58 2019
From: numsys at free.fr (FredB)
Date: Tue, 22 Jan 2019 09:22:58 +0100
Subject: [squid-users] ICAP and 403 Encapsulated answers (SSL denied
 domains)
In-Reply-To: <8cfb73a6-6cbb-4907-f304-a43487ae3856@measurement-factory.com>
References: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>
 <8cfb73a6-6cbb-4907-f304-a43487ae3856@measurement-factory.com>
Message-ID: <cc051bb8-e180-1be8-51b3-b0653c6ae2cf@free.fr>

Hello Alex


>> But unfortunately Squid adds a "Connection: keep-alive" header
> It is not clear _why_ you consider that header "unfortunate" and the
> connection "wasted". That header may or may not be wrong, and the
> connection may or may not be reusable, depending on many factors (that
> you have not shared).
>
Your are right, it's not clear for me too, the only thing I'm seeing 
it's that a keep-alive is not present in my answer from ICAP but well 
added in header to client, after that if there is a refresh the browser 
waits for the page a long time

But perhaps this is not related to my issue


>
> work. Otherwise, a packet capture (in pcap format) is probably the
> easiest sharing method.
>

Here a short tcpdump trace 
https://nas.traceroot.fr:8081/owncloud/index.php/s/egrcXnU3lxiU0mi

 ? 1 - I'm surfing to the website https://www.toto.fr

 ? 2 - I receive a 403 (blank page)

 ? 3 - I refresh the page, and I wait a long time before timeout

A real issue is filtering ADS, surf to www.aaa.com and block www.bbb.com 
(ads), there are multiple links to bbb in aaa, in this case www.aaa.com 
never appears completely (or after a long time) the browser freeze and 
still waiting bbb? (the name appears in bottom: waiting for bbb)


>
> Yes, by ICAP design, an ICAP service does not have direct control over
> HTTP connections maintained by the host application (e.g., Squid).

Yes it's what I saw and read in the rfc

Thank you

Fred



From eliezer at ngtech.co.il  Tue Jan 22 12:32:15 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 22 Jan 2019 14:32:15 +0200
Subject: [squid-users] https debug
In-Reply-To: <66724540-02b7-c715-0def-76d615e09391@treenet.co.nz>
References: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>
 <ca313cd2-4344-d480-4563-82f9f1e43900@treenet.co.nz>
 <b91ec8b7d7ae6339e12e7abbb63869cd@tiscali.it>
 <8537164e-6430-055e-0263-22d701c1215f@treenet.co.nz>
 <01e701d4b10d$af2389a0$0d6a9ce0$@ngtech.co.il>
 <66724540-02b7-c715-0def-76d615e09391@treenet.co.nz>
Message-ID: <004d01d4b24e$81bdf490$8539ddb0$@ngtech.co.il>

I didn't knew it's such a known repo.
It's weird when someone in the street recognized me and identified me as the Squid-Cache RPM repo .

Or in japanse " Hazukash? ".

:D

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: Monday, January 21, 2019 10:10
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] https debug

On 21/01/19 11:15 am, Eliezer Croitoru wrote:
> You probably meant 4.5...
> http://www1.ngtech.co.il/repo/centos/7/x86_64/squid-4.5-1.el7.x86_64.rpm
> 

Time travel ...

> -----Original Message-----
> From: Amos Jeffries
> Sent: Wednesday, January 2, 2019 12:01

... back when 4.4 was all you had.

;-P

Amos



From grafhuy at yahoo.fr  Tue Jan 22 13:21:42 2019
From: grafhuy at yahoo.fr (graf huy)
Date: Tue, 22 Jan 2019 13:21:42 +0000 (UTC)
Subject: [squid-users] using clang to compile squid 4-5
References: <99079882.2816932.1548163302767.ref@mail.yahoo.com>
Message-ID: <99079882.2816932.1548163302767@mail.yahoo.com>

Hi,
The purpose is to compile squid to get HTTPS or SSL with bump support, on Debian 10 (Buster).
After trying to compile squid-4-5? with clang (clang version 7.0.1-4 (tags/RELEASE_701/final)), it doesn't work. 

Target: x86_64-pc-linux-gnu
Thread model: posix
InstalledDir: /usr/bin
Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/6
Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/6.5.0
Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/7
Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/7.4.0
Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/8
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/6
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/6.5.0
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/7
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/7.4.0
Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/8
Selected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/8
Candidate multilib: .;@m64
Candidate multilib: 32;@m32
Candidate multilib: x32;@mx32
Selected multilib: .;@m64

The command ./configure is followed by the options:
--enable-ssl --enable-ssl-crtd --localstatedir=/var --datadir=/usr/share/squid --sysconfdir=/etc/squid --libexecdir=/usr/lib/squid --mandir=/usr/share/man --enable-inline --disable-arch-native --enable-async-io=8? --enable-storeio="ufs,aufs,diskd,rock" --enable-removal-policies="lru,heap" --enable-delay-pools --enable-cache-digests --enable-icap-client --enable-follow-x-forwarded-for --enable-auth-basic="DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB" --enable-auth-digest="file,LDAP" --enable-auth-negotiate="kerberos,wrapper" --enable-auth-ntlm="fake,SMB_LM" --enable-external-acl-helpers="file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group" --enable-security-cert-validators="fake" --enable-storeid-rewrite-helpers="file" --enable-url-rewrite-helpers="fake" --enable-eui? --enable-esi --enable-icmp --enable-zph-qos --enable-ecap --disable-translation --with-swapdir=/var/spool/squid --with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid --with-filedescriptors=65536 --with-large-files --with-default-user=proxy --with-gnutls --with-openssl

The option --with-openssl is added because the output of ./configure ...? says to add it for ssl support.

The Makefile is modified so each line with gcc is replaced with clang and each line of g++ replaced with clang++. But gcc is still used.
Some of these:

CC = clang

CCDEPMODE =depmode=gcc3

CFLAGS = -m64 -Wall-g -O2

CGIEXT = .cgi

CHMOD = /bin/chmod

CPP = clang -E

CPPFLAGS = -I/usr/include/libxml2

CRYPTLIB = -lcrypt

CXX = clang++

CXXCPP = clang++ -E

CXXDEPMODE =depmode=gcc3

CXXFLAGS = -m64 -I/usr/include/p11-kit-1 -g -O2




I'm doing something wrong, but don't see where.
Thanks in advance for any help.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190122/4481e0c4/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan 22 15:13:24 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jan 2019 08:13:24 -0700
Subject: [squid-users] TCP_TUNNEL and ecap
In-Reply-To: <5F38A577-1BB5-4940-8C25-7A7019B55C51@hendrie.id.au>
References: <5F38A577-1BB5-4940-8C25-7A7019B55C51@hendrie.id.au>
Message-ID: <adbeabce-e375-9c16-8fe5-d6a9ebac8688@measurement-factory.com>

On 1/21/19 10:47 PM, Michael Hendrie wrote:

> I understand that in most cases adaptation of a tunnelled HTTPS 
> response is pointless as it would result message corruption but 
> wondering if it is at all possible to get the TCP_TUNNEL response 
> seen by ecap

It would be possible (and, in some cases, useful) to send tunneled
traffic through adaptation services, but doing so requires a significant
development effort: HTTP already provides what ICAP and eCAP services
need to be able to see (and even modify) tunnels, but Squid does not
implement such adaptations (yet?).

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


From rousskov at measurement-factory.com  Tue Jan 22 15:51:07 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jan 2019 08:51:07 -0700
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <50aef43a-ce10-96aa-44fb-021c27423792@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
 <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
 <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>
 <50aef43a-ce10-96aa-44fb-021c27423792@belkam.com>
Message-ID: <2937e57f-6c52-668c-8845-a0cd3fe91ea1@measurement-factory.com>

On 1/21/19 10:52 PM, Dmitry Melekhov wrote:
> 21.01.2019 22:29, Alex Rousskov ?????:
>>>> Your Squid (or some helper) appears to be adding an
>>>> "-/ffff...GETmyip=-myport=0" suffix to the crt.sectigo.com URL,
>>>> resulting in a 404 response from that server.

>>> Is there any reasons squid sends ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff
>>> to redirector?

>> What Squid logformat %code or url_rewrite_extras %code does that address
>> come from?

> default on my case

>> Should the corresponding request have that address? For
>> example, internally-generated requests do not have HTTP client addresses.

>> Will the redirector work if that address is sent as a "-" instead of
>> "ff...fff"?


> rejik redirector developer thinks its better to use 127.0.0.1 as squid
> address,

It sounds like you misunderstood my questions. I will detail them below.

I suspect that fff...fff comes from %>A (whether that %code comes from
the default url_rewrite_extras in your configuration is unimportant).

%>A is documented to to be a client FQDN. I am not sure, and this is not
documented, but perhaps when the client IP address does not point back
to a domain name, %>A should be a client IP address.

For intermediate certificate downloading transactions, Squid does not
have a client address because those transactions are not initiated by a
client connection to Squid. They are generated internally by Squid. In
such cases, Squid should be sending a dash (-), not 127.0.0.1, not
fff...fff, not localhost, and not anything else that might be
misinterpreted as a client IP address or domain name.

I have not investigated why Squid does not send a dash, or what it would
take to fix Squid, but it is likely that this will be eventually fixed
because lying about client address is a bug. To plan the deployment of
that future fix, it may be useful to know whether the redirector you use
handles a dash value for %>A correctly. You may be able to test that by
configuring url_rewrite_extras explicitly and replacing %>A with a dash.

Alex.


From rousskov at measurement-factory.com  Tue Jan 22 16:17:06 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jan 2019 09:17:06 -0700
Subject: [squid-users] using clang to compile squid 4-5
In-Reply-To: <99079882.2816932.1548163302767@mail.yahoo.com>
References: <99079882.2816932.1548163302767.ref@mail.yahoo.com>
 <99079882.2816932.1548163302767@mail.yahoo.com>
Message-ID: <b059395b-3301-1e0a-abca-950e5379d8f0@measurement-factory.com>

On 1/22/19 6:21 AM, graf huy wrote:

> The Makefile is modified so each line with gcc is replaced with clang
> and each line of g++ replaced with clang++. But gcc is still used.

I am not sure you are doing that, but, just in case, you should not be
modifying Makefiles (or any other files generated by ./configure)
manually. You should convince ./configure to select clang instead.

I suggest using update-alternatives(1) to switch between gcc and clang
compilers (or between gcc versions). It may be a bit difficult to get
working initially, but works well once you figure out the right
commands. Please note that you should be switching both C and C++
compilers at the same time (i.e., both should come from either GCC or
clang). Search the web for more detailed instructions -- your "how to
switch from gcc to clang on Debian 10" question/problem is not specific
to Squid.

If you are using update-alternatives already, you may want to share your
commands, their output, and the corresponding ./configure output.

Alex.


From rousskov at measurement-factory.com  Tue Jan 22 17:33:29 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jan 2019 10:33:29 -0700
Subject: [squid-users] ICAP and 403 Encapsulated answers (SSL denied
 domains)
In-Reply-To: <cc051bb8-e180-1be8-51b3-b0653c6ae2cf@free.fr>
References: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>
 <8cfb73a6-6cbb-4907-f304-a43487ae3856@measurement-factory.com>
 <cc051bb8-e180-1be8-51b3-b0653c6ae2cf@free.fr>
Message-ID: <6b798ccd-8653-4106-30a7-d576e4166156@measurement-factory.com>

On 1/22/19 1:22 AM, FredB wrote:

> Here a short tcpdump trace
> https://nas.traceroot.fr:8081/owncloud/index.php/s/egrcXnU3lxiU0mi
> 
> ? 1 - I'm surfing to the website https://www.toto.fr

Yes (tcp.stream eq 30).


> ? 2 - I receive a 403 (blank page)

> HTTP/1.1 403 Forbidden
> Server: e2guardian
> Date: Mon, 21 Jan 2019 10:06:54 GMT
> X-Cache: MISS from proxyorion_test
> X-Cache-Lookup: NONE from proxyorion_test:3128
> Transfer-Encoding: chunked
> Via: 1.1 proxyorion_test (squid/4.5)
> Connection: keep-alive
> 
> 0

Agreed. Frame 99 contains a well-formed HTTP 403 response with an empty
body. IIRC, popular browsers refuse to display 403 responses to CONNECT
requests. There is also nothing to display in your specific case because
the 403 response body is empty, but that is irrelevant.


> 3 - I refresh the page, and I wait a long time before timeout

The trace you posted does not seem to show this part AFAICT. Perhaps
your "refresh" was not forceful enough for the browser to open a new
connection. I do not know whether that part is important.


> A real issue is filtering ADS

Please note that it is your responsibility to reproduce the
real/relevant problem. Your current test case may be sufficient -- I do
not know -- but if it is _not_ sufficient, we may not be able to tell
that it is insufficient or irrelevant, and will chase ghosts.


In summary, the trace you posted does not seem to indicate a Squid
problem. That does not mean there is no problem. It only means this use
case does not seem to expose that problem from Squid point of view.

If you believe that the browser is waiting for Squid to send something
after those HTTP 403 response bytes, then it sounds like there is a
browser bug -- Squid sent a full/complete response AFAICT. You may be
able to learn more about browser needs by debugging the browser.


As a workaround, you can try disabling client-to-Squid persistent
connections (client_persistent_connections off) or changing your ICAP
service to produce a response with a non-empty 403 body.


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Jan 22 19:56:31 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Jan 2019 08:56:31 +1300
Subject: [squid-users] Squid 4.5 Transparent Proxy,
 StrongSwan VPN - Working in Browser but not in any android apps
In-Reply-To: <kcis.A04D06E233A54DD98710C10575C244B3@v22018042993464812.supersrv.de>
References: <kcis.A04D06E233A54DD98710C10575C244B3@v22018042993464812.supersrv.de>
Message-ID: <869f7007-d5fd-4cdd-2241-310dc608b0f3@treenet.co.nz>

On 22/01/19 9:19 pm, XploD wrote:
> 
> Can anybody tell me what I have to do so that every android app accepts
> the intercepted connection?
> 

IIRC there is also a phone CA certificate store where it can be added.
Though I do not recall exactly where it is right now.

Even with that setup some apps (from eg Youtube and Facebook) use
certificate pinning. They bundle the domains CA cert hard-coded into the
app it self and only trusts that exact CA. Or use a client certificate
similarly bundled with each app to authenticate against the server.

When either of those TLS features are used SSL-Bump cannot do the 'bump'
action - only the peek, splice or terminate work. That is still enough
to identify the destination domain, but no deep inspection.


> 
> BTW: If any squid developer is reading this: Squid is awesome work!
> Thank you very much for such beauty!
> 

On behalf of the team: thank you.

Amos


From squid3 at treenet.co.nz  Wed Jan 23 04:24:34 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Jan 2019 17:24:34 +1300
Subject: [squid-users] using clang to compile squid 4-5
In-Reply-To: <b059395b-3301-1e0a-abca-950e5379d8f0@measurement-factory.com>
References: <99079882.2816932.1548163302767.ref@mail.yahoo.com>
 <99079882.2816932.1548163302767@mail.yahoo.com>
 <b059395b-3301-1e0a-abca-950e5379d8f0@measurement-factory.com>
Message-ID: <e5bad83a-336e-e0d7-1aeb-d3fba03a195f@treenet.co.nz>

On 23/01/19 5:17 am, Alex Rousskov wrote:
> On 1/22/19 6:21 AM, graf huy wrote:
> 
>> The Makefile is modified so each line with gcc is replaced with clang
>> and each line of g++ replaced with clang++. But gcc is still used.
> 
> I am not sure you are doing that,

Seconded. With both my Squid Project and Debian pkg-squid Team hat's on
I'd like to know why you feel any need to force the compiler?


Amos


From dm at belkam.com  Wed Jan 23 04:40:10 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Wed, 23 Jan 2019 08:40:10 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <2937e57f-6c52-668c-8845-a0cd3fe91ea1@measurement-factory.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
 <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
 <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>
 <50aef43a-ce10-96aa-44fb-021c27423792@belkam.com>
 <2937e57f-6c52-668c-8845-a0cd3fe91ea1@measurement-factory.com>
Message-ID: <839cc4c3-a88f-b629-21a0-e133f159b2b5@belkam.com>

22.01.2019 19:51, Alex Rousskov ?????:
>
> It sounds like you misunderstood my questions. I will detail them below.
>
> I suspect that fff...fff comes from %>A (whether that %code comes from
> the default url_rewrite_extras in your configuration is unimportant).
>
> %>A is documented to to be a client FQDN. I am not sure, and this is not
> documented, but perhaps when the client IP address does not point back
> to a domain name, %>A should be a client IP address.
>
> For intermediate certificate downloading transactions, Squid does not
> have a client address because those transactions are not initiated by a
> client connection to Squid. They are generated internally by Squid. In
> such cases, Squid should be sending a dash (-), not 127.0.0.1, not
> fff...fff, not localhost, and not anything else that might be
> misinterpreted as a client IP address or domain name.
>
> I have not investigated why Squid does not send a dash, or what it would
> take to fix Squid, but it is likely that this will be eventually fixed
> because lying about client address is a bug. To plan the deployment of
> that future fix, it may be useful to know whether the redirector you use
> handles a dash value for %>A correctly. You may be able to test that by
> configuring url_rewrite_extras explicitly and replacing %>A with a dash.


Thank you for explanation, it is easier for me to contact rejik 
developer and ask him to pass traffic if client address is "-" as he 
already did for

fff...fff.  So, I'll inform him that such change is planned and he will be ready :-)

Thank you!




From squid3 at treenet.co.nz  Wed Jan 23 04:53:30 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Jan 2019 17:53:30 +1300
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <839cc4c3-a88f-b629-21a0-e133f159b2b5@belkam.com>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
 <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
 <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>
 <50aef43a-ce10-96aa-44fb-021c27423792@belkam.com>
 <2937e57f-6c52-668c-8845-a0cd3fe91ea1@measurement-factory.com>
 <839cc4c3-a88f-b629-21a0-e133f159b2b5@belkam.com>
Message-ID: <7a0558f3-bac0-d12d-47f8-6ef2d4366029@treenet.co.nz>

On 23/01/19 5:40 pm, Dmitry Melekhov wrote:
> 
> Thank you for explanation, it is easier for me to contact rejik
> developer and ask him to pass traffic if client address is "-" as he
> already did for
> 
> fff...fff.? So, I'll inform him that such change is planned and he will
> be ready :-)


Um, to be more prescriptive ...

The (%>a) part *before* the '/' is the actual "client IP address".

If that is '-' (like your logs show it already is) then the reverse-DNS
FQDN part *after* the '/' cannot be relied upon at all so should
generally be ignored.


Whether or not we change the FQDN parts display, it could still have an
IPv6 address when a real IPv6 client arrives - and the IP part before
the '/' would then also still have an IPv6 address too. So IPv6 support
is needed regardless.


Amos


From dm at belkam.com  Wed Jan 23 05:06:38 2019
From: dm at belkam.com (Dmitry Melekhov)
Date: Wed, 23 Jan 2019 09:06:38 +0400
Subject: [squid-users] squid 4.5, can't download certificate?
In-Reply-To: <7a0558f3-bac0-d12d-47f8-6ef2d4366029@treenet.co.nz>
References: <f036e69d-8e70-40ae-962f-c20463e14da8@belkam.com>
 <33436780-190f-db1c-0d97-2223287ba3dc@measurement-factory.com>
 <7b7a1b36-c832-8749-5c9a-8073dbca094d@belkam.com>
 <12b7851d-d93c-992e-56b6-70f942eea528@measurement-factory.com>
 <99ca5193-8868-1c93-1064-1ede4e0edeef@treenet.co.nz>
 <d0114f35-e16a-6318-7500-1762ca1e226c@belkam.com>
 <d8cc4b3a-bb76-130d-06a7-7d8c12c073cc@belkam.com>
 <ce72c97e-05d7-835f-8491-942403fe0d8b@belkam.com>
 <5bc8b5d3-7907-f329-88f6-e1c60e470092@measurement-factory.com>
 <50aef43a-ce10-96aa-44fb-021c27423792@belkam.com>
 <2937e57f-6c52-668c-8845-a0cd3fe91ea1@measurement-factory.com>
 <839cc4c3-a88f-b629-21a0-e133f159b2b5@belkam.com>
 <7a0558f3-bac0-d12d-47f8-6ef2d4366029@treenet.co.nz>
Message-ID: <f448d4d8-c232-20c7-9742-ba85152aeb37@belkam.com>

23.01.2019 8:53, Amos Jeffries ?????:
> On 23/01/19 5:40 pm, Dmitry Melekhov wrote:
>> Thank you for explanation, it is easier for me to contact rejik
>> developer and ask him to pass traffic if client address is "-" as he
>> already did for
>>
>> fff...fff.? So, I'll inform him that such change is planned and he will
>> be ready :-)
>
> Um, to be more prescriptive ...
>
> The (%>a) part *before* the '/' is the actual "client IP address".
>
> If that is '-' (like your logs show it already is) then the reverse-DNS
> FQDN part *after* the '/' cannot be relied upon at all so should
> generally be ignored.


Thank you!

Looks like it's better to wait until it will be fixed, because rejik 
developer is very responsive and , I guess, will provide fix very soon.

>
> Whether or not we change the FQDN parts display, it could still have an
> IPv6 address when a real IPv6 client arrives - and the IP part before
> the '/' would then also still have an IPv6 address too. So IPv6 support
> is needed regardless.
>
Yes, sure, but , really, here we have no ISP which provides ipv6,

so it is not problem for next several years.

Thank you!





From eliezer at ngtech.co.il  Wed Jan 23 06:59:57 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Jan 2019 08:59:57 +0200
Subject: [squid-users] What's the best way to ban Let's encrypt based
	certificates? or whitelist a very narrow list of Root and
	Intermediates CA?
In-Reply-To: <76a18c51-d9b6-9c65-8132-b9d5d799dd32@netfence.it>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
 <76a18c51-d9b6-9c65-8132-b9d5d799dd32@netfence.it>
Message-ID: <02b601d4b2e9$407bdb50$c17391f0$@ngtech.co.il>

OK so,

Every Root CA have differ level of certification.
For example there are Root CA's which are allowed to sign only for encryption
...and basic domain ownership validation which can be verified against a Domain Regristrar.
Compared to this there are couple other level's of Certificates like what is name "EV" (the one of banks and such critical ORG's).
Let's encrypt brings to domain ownership the ability to being verified as the domain owner or it's proxy.

The Root CA that the bank of America uses has the license to offer not only encryption but also:
* Ensures the identity of a remote computer
* Proves your identity to a remote computer
* Protects e-mail messages
* Ensures software came from software publisher
* Protects software from alteration after publication
* Allows data to be signed with the current time

Compared to Let's encrypt that is an intermediate CA with the next license:
* Protects e-mail messages
* Ensures the identity of a remote computer
* Proves your identity to a remote computer
* Allows data to be signed with the current time
* Allows data on disk to be encrypted
* 2.23.140.1.2.1
* 1.3.6.1.4.1.44947.1.1.1
* Document Signing

Which doesn't includes:
* Ensures software came from software publisher

Which is critical for ISO bounded web services.

In another words:
If the certificate is not EV ie the name of the corporation or business it means that it's not ISO compliance regarding
paying using a credit/visa/other card.

So if you are going to pay to someone over the Internet only pay if you know and validated the identity of the owner and\or orginzation.
This concept was introduced to prevent phishing and other things.
One of the exception I have seen is Paypal main site which does have EV named license/certificate but the name is not embedded into the certificate so I prefer not to buy in this specific site but buy locally.

All The Bests,
Eliezer

* For others paypal might be good enough... 

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Andrea Venturoli
Sent: Monday, January 21, 2019 10:51
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] What's the best way to ban Let's encrypt based certificates? or whitelist a very narrow list of Root and Intermediates CA?

On 1/20/19 11:02 PM, Eliezer Croitoru wrote:

> The issue is that these sites are encrypted but do not offer any way 
> of assuring real ISO and couple other compatibilities of the ORG.
> 
> For a simple home user it?s fine most of the time but for some it?s not.

Just out of curiosity, could you better explain this?
Pointer are enough if you prefer.

  bye & Thanks
	av.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From augustus_meyer at gmx.net  Wed Jan 23 08:22:01 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 23 Jan 2019 02:22:01 -0600 (CST)
Subject: [squid-users] squid on openwrt: Possible to get rid of "...
 SECURITY ALERT: Host header forgery detected ..." msgs ?
Message-ID: <1548231721587-0.post@n4.nabble.com>

Running squid 4.4 on very limited device, unfortunately quite a lot of
messages: "... SECURITY ALERT: Host header forgery detected ... "  show up. 
Unable to eliminate real cause of this issue (even using iptables to redir
all DNS requests to one dnsmasq does not help), these annoying messages tend
to fill up cache.log, which is kept in precious RAM. 
Is there an "official" method to suppress these messages ?
Or can you please give a hint, where to apply a (hopefully) simple patch ?





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From numsys at free.fr  Wed Jan 23 10:17:27 2019
From: numsys at free.fr (FredB)
Date: Wed, 23 Jan 2019 11:17:27 +0100
Subject: [squid-users] ICAP and 403 Encapsulated answers (SSL denied
 domains)
In-Reply-To: <6b798ccd-8653-4106-30a7-d576e4166156@measurement-factory.com>
References: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>
 <8cfb73a6-6cbb-4907-f304-a43487ae3856@measurement-factory.com>
 <cc051bb8-e180-1be8-51b3-b0653c6ae2cf@free.fr>
 <6b798ccd-8653-4106-30a7-d576e4166156@measurement-factory.com>
Message-ID: <b877c1a4-094d-1f43-904b-1e2d7a6deaff@free.fr>


>
> As a workaround, you can try disabling client-to-Squid persistent
> connections (client_persistent_connections off) or changing your ICAP
> service to produce a response with a non-empty 403 body.


You are right this is a browser bug (firefox at least recent versions) 
and this issue can be resolved by client_persistent_connections off 
unfortunately non-empty body is not enough

I will post a bug report to firefox

I found nothing in documentation about client_persistent_connections off 
impact, do you think this can be problematic with high load ?

Fred


From squid3 at treenet.co.nz  Wed Jan 23 10:57:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Jan 2019 23:57:24 +1300
Subject: [squid-users] What's the best way to ban Let's encrypt based
 certificates? or whitelist a very narrow list of Root and Intermediates CA?
In-Reply-To: <02b601d4b2e9$407bdb50$c17391f0$@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
 <76a18c51-d9b6-9c65-8132-b9d5d799dd32@netfence.it>
 <02b601d4b2e9$407bdb50$c17391f0$@ngtech.co.il>
Message-ID: <b8a63746-949d-ad67-4d89-8e5d6e572933@treenet.co.nz>



On 23/01/19 7:59 pm, Eliezer Croitoru wrote:
> OK so,
> 
> Every Root CA have differ level of certification.
> For example there are Root CA's which are allowed to sign only for encryption
> ...and basic domain ownership validation which can be verified against a Domain Regristrar.
> Compared to this there are couple other level's of Certificates like what is name "EV" (the one of banks and such critical ORG's).
> Let's encrypt brings to domain ownership the ability to being verified as the domain owner or it's proxy.
> 
> The Root CA that the bank of America uses has the license to offer not only encryption but also:
> * Ensures the identity of a remote computer
> * Proves your identity to a remote computer
> * Protects e-mail messages
> * Ensures software came from software publisher
> * Protects software from alteration after publication
> * Allows data to be signed with the current time
> 
> Compared to Let's encrypt that is an intermediate CA with the next license:
> * Protects e-mail messages
> * Ensures the identity of a remote computer
> * Proves your identity to a remote computer
> * Allows data to be signed with the current time
> * Allows data on disk to be encrypted
> * 2.23.140.1.2.1
> * 1.3.6.1.4.1.44947.1.1.1
> * Document Signing
> 

Those listed things above sound like the X.509 certificate 'use'
properties are what you actually need to be checking. Am I right?

> Which doesn't includes:
> * Ensures software came from software publisher
> 
> Which is critical for ISO bounded web services.
> 
> In another words:
> If the certificate is not EV ie the name of the corporation or business it means that it's not ISO compliance regarding
> paying using a credit/visa/other card.
> 
> So if you are going to pay to someone over the Internet only pay if you know and validated the identity of the owner and\or orginzation.
> This concept was introduced to prevent phishing and other things.
> One of the exception I have seen is Paypal main site which does have EV named license/certificate but the name is not embedded into the certificate so I prefer not to buy in this specific site but buy locally.
> 

A validator which checks for existence or non-existence of certain X.509
permissions would be the better approach instead of a curated whitelist
of CA names. That way;
 * you are not limited to whitelisting and its inherent "human error" or
incompleteness component biasing for/against any particular CAs,
 * you can publish the required criteria for transparency,
 * CAs can choose for themselves whether they adjust certs permissions
to be blocked or un-blocked without involving any tricky politics to
lower your required standard of proof.


Some CAs might for example have a special root CA with restricted
policies to comply with the ISO requirement, and another for their wider
use.


Amos


From squid3 at treenet.co.nz  Wed Jan 23 11:02:05 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Jan 2019 00:02:05 +1300
Subject: [squid-users] squid on openwrt: Possible to get rid of "...
 SECURITY ALERT: Host header forgery detected ..." msgs ?
In-Reply-To: <1548231721587-0.post@n4.nabble.com>
References: <1548231721587-0.post@n4.nabble.com>
Message-ID: <a1a6a99b-970d-4dcd-1476-6a4371b849a9@treenet.co.nz>

On 23/01/19 9:22 pm, reinerotto wrote:
> Running squid 4.4 on very limited device, unfortunately quite a lot of
> messages: "... SECURITY ALERT: Host header forgery detected ... "  show up. 
> Unable to eliminate real cause of this issue (even using iptables to redir
> all DNS requests to one dnsmasq does not help), these annoying messages tend
> to fill up cache.log, which is kept in precious RAM. 
> Is there an "official" method to suppress these messages ?
> Or can you please give a hint, where to apply a (hopefully) simple patch ?
> 

See <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>

FYI: There is still active malware out there searching for proxies that
are vulnerable and utilizing them for nefarious uses.


The last person to ask this questio turned out to have a network
infected with that malware.
I thought last year that a decade of fixed Squid being used was long
enough for things to die down and let us loosen up a bit. Then was
informed about yet another ISP being attacked through those methods. So
no, an official patch removing them is not on the book yet.

Almost all the ways we are able to reduce the side effects have been
done and are included in that Squid-4.4

Amos


From augustus_meyer at gmx.net  Wed Jan 23 13:55:56 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 23 Jan 2019 07:55:56 -0600 (CST)
Subject: [squid-users] squid on openwrt: Possible to get rid of "...
 SECURITY ALERT: Host header forgery detected ..." msgs ?
In-Reply-To: <a1a6a99b-970d-4dcd-1476-6a4371b849a9@treenet.co.nz>
References: <1548231721587-0.post@n4.nabble.com>
 <a1a6a99b-970d-4dcd-1476-6a4371b849a9@treenet.co.nz>
Message-ID: <1548251756880-0.post@n4.nabble.com>

I suspect, these messages, for example, are not caused by any malware, but
somehow by skype:

2019/01/23 13:38:18 kid1| SECURITY ALERT: on URL:
mobile.pipe.aria.microsoft.com:443
2019/01/23 13:38:18 kid1| SECURITY ALERT: Host header forgery detected on
local=52.114.76.35:443 remote=192.168.182.10:59312 FD 31 flags=33 (local IP
does not match any domain IP)
2019/01/23 13:38:18 kid1| SECURITY ALERT: on URL:
mobile.pipe.aria.microsoft.com:443
2019/01/23 13:39:03 kid1| SECURITY ALERT: Host header forgery detected on
local=52.114.74.44:443 remote=192.168.182.10:59378 FD 37 flags=33 (local IP
does not match any domain IP)
2019/01/23 13:39:03 kid1| SECURITY ALERT: on URL:
mobile.pipe.aria.microsoft.com:443


May be,  some inconsistency of cached DNS in the client and the
openwrt-device, running squid.
There are some "rumours", that not all browsers correctly honor TTL for
cached DNS.


Anyway, even, in case malware would trigger these messages, then this opens
the gate to attack resource limited squid-installations, like mine on
openwrt, by flooding cache.log, kept in RAM, and possibly forcing an
OOM-crash.
Simple fix would be to disable cache.log, but I am hesitating to do so, not
to drop more valuable messages.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eliezer at ngtech.co.il  Wed Jan 23 19:47:08 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 23 Jan 2019 21:47:08 +0200
Subject: [squid-users] What's the best way to ban Let's encrypt based
	certificates? or whitelist a very narrow list of Root and
	Intermediates CA?
In-Reply-To: <b8a63746-949d-ad67-4d89-8e5d6e572933@treenet.co.nz>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACYcxf5AA1DRIxwngDTQmDMAQAAAAA=@ngtech.co.il>
 <76a18c51-d9b6-9c65-8132-b9d5d799dd32@netfence.it>
 <02b601d4b2e9$407bdb50$c17391f0$@ngtech.co.il>
 <b8a63746-949d-ad67-4d89-8e5d6e572933@treenet.co.nz>
Message-ID: <04c101d4b354$6cd8a870$4689f950$@ngtech.co.il>

Amos,

Thanks for the feedback.

Now that we write about the subject in clear text it's making things a bit clear.
I wasn't sure about the  purpose of the helpers to begin with.

As you wrote before, for specific use cases these X.509 properties are what this specific organization need to verify.
>From my point of view most users(non-technical) are yet to understand these properties good enough and these are things
that us and our kids needs to learn about the Internet: "not every Encrypted traffic means secure!".

Specifically Let's encrypt makes sure that the world of encryption will be more popular and there for more secure.
So +*2 for them but still the point stays with then that encryption might not always be the right way.
SFTP ,MS and OpenSSL + others proved that encryption is a must in our world but not always the right answer.

..Also DNSSec and/or EDNS makes this picture much clear.

Eliezer

* If I missed someone it's because there are too many to say thank you to/for.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, January 23, 2019 12:57
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] What's the best way to ban Let's encrypt based certificates? or whitelist a very narrow list of Root and Intermediates CA?



On 23/01/19 7:59 pm, Eliezer Croitoru wrote:
> OK so,
> 
> Every Root CA have differ level of certification.
> For example there are Root CA's which are allowed to sign only for encryption
> ...and basic domain ownership validation which can be verified against a Domain Regristrar.
> Compared to this there are couple other level's of Certificates like what is name "EV" (the one of banks and such critical ORG's).
> Let's encrypt brings to domain ownership the ability to being verified as the domain owner or it's proxy.
> 
> The Root CA that the bank of America uses has the license to offer not only encryption but also:
> * Ensures the identity of a remote computer
> * Proves your identity to a remote computer
> * Protects e-mail messages
> * Ensures software came from software publisher
> * Protects software from alteration after publication
> * Allows data to be signed with the current time
> 
> Compared to Let's encrypt that is an intermediate CA with the next license:
> * Protects e-mail messages
> * Ensures the identity of a remote computer
> * Proves your identity to a remote computer
> * Allows data to be signed with the current time
> * Allows data on disk to be encrypted
> * 2.23.140.1.2.1
> * 1.3.6.1.4.1.44947.1.1.1
> * Document Signing
> 

Those listed things above sound like the X.509 certificate 'use'
properties are what you actually need to be checking. Am I right?

> Which doesn't includes:
> * Ensures software came from software publisher
> 
> Which is critical for ISO bounded web services.
> 
> In another words:
> If the certificate is not EV ie the name of the corporation or business it means that it's not ISO compliance regarding
> paying using a credit/visa/other card.
> 
> So if you are going to pay to someone over the Internet only pay if you know and validated the identity of the owner and\or orginzation.
> This concept was introduced to prevent phishing and other things.
> One of the exception I have seen is Paypal main site which does have EV named license/certificate but the name is not embedded into the certificate so I prefer not to buy in this specific site but buy locally.
> 

A validator which checks for existence or non-existence of certain X.509
permissions would be the better approach instead of a curated whitelist
of CA names. That way;
 * you are not limited to whitelisting and its inherent "human error" or
incompleteness component biasing for/against any particular CAs,
 * you can publish the required criteria for transparency,
 * CAs can choose for themselves whether they adjust certs permissions
to be blocked or un-blocked without involving any tricky politics to
lower your required standard of proof.


Some CAs might for example have a special root CA with restricted
policies to comply with the ISO requirement, and another for their wider
use.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jan 24 01:44:00 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Jan 2019 14:44:00 +1300
Subject: [squid-users] squid on openwrt: Possible to get rid of "...
 SECURITY ALERT: Host header forgery detected ..." msgs ?
In-Reply-To: <1548251756880-0.post@n4.nabble.com>
References: <1548231721587-0.post@n4.nabble.com>
 <a1a6a99b-970d-4dcd-1476-6a4371b849a9@treenet.co.nz>
 <1548251756880-0.post@n4.nabble.com>
Message-ID: <5b9085e4-7d5b-6f7a-15c4-5e2056d20563@treenet.co.nz>

On 24/01/19 2:55 am, reinerotto wrote:
> I suspect, these messages, for example, are not caused by any malware, but
> somehow by skype:
> 
> 2019/01/23 13:38:18 kid1| SECURITY ALERT: on URL:
> mobile.pipe.aria.microsoft.com:443
> 2019/01/23 13:38:18 kid1| SECURITY ALERT: Host header forgery detected on
> local=52.114.76.35:443 remote=192.168.182.10:59312 FD 31 flags=33 (local IP
> does not match any domain IP)
> 2019/01/23 13:38:18 kid1| SECURITY ALERT: on URL:
> mobile.pipe.aria.microsoft.com:443
> 2019/01/23 13:39:03 kid1| SECURITY ALERT: Host header forgery detected on
> local=52.114.74.44:443 remote=192.168.182.10:59378 FD 37 flags=33 (local IP
> does not match any domain IP)
> 2019/01/23 13:39:03 kid1| SECURITY ALERT: on URL:
> mobile.pipe.aria.microsoft.com:443
> 
> 
> May be,  some inconsistency of cached DNS in the client and the
> openwrt-device, running squid.


I have checked those domains and their DNS results. Their IPs change
every 30sec to another random IP in a /16 block belonging to a company
which is *not* Microsoft. This is just one /16 out of the 10s of
millions of IPs MS have allocated.

If you can track down any inconsistency in DNS caching that would help
reduce the frequency of false-positive tests and generally improve the
behaviour of all software using that DNS cache.


Meanwhile directly addressing those warnings would be reducing or
removing the use of HTTP persistence on client connections.

 <http://www.squid-cache.org/Doc/config/client_lifetime/>
 <http://www.squid-cache.org/Doc/config/client_persistent_connections/>



> There are some "rumours", that not all browsers correctly honor TTL for
> cached DNS.

Um, I suspect you don't understand what that use of double-quote means:
rumours about rumours existing. Either way that does not matter.


What Browsers do is use persistent connections. Part of HTTP design, and
used in reasonable ways. It's just that DNS TTL may have different
duration - case in point being these Skype connections where the IP is
forced to change every 30sec, persistence is indefinite but usually
several minutes.
Consider having a Skype chat where you close and re-open the app every
30sec versus only doing that once and hour, or once a day. DNS Best
Practice is/was to use 24hr TTLs - the mega corps do their own thing, so
one guess why your logos are so annoying?


With short DNS TTL by the time a second (or third, or Nth) HTTP request
is sent in the connection the origin has all the appearance of having
moved elsewhere and become indistinguishable from an attacker diverting
traffic to get themselves a trivial tunnel into your network.


For now all we can do is take the warnings seriously and find ways to
prevent the network behaviours that cause them. The security issues this
detection prevents are so nasty we consider the pain (monetary costs,
latency and bandwidth - not just log sizes) worth the price of avoiding
those outcomes.


> 
> Anyway, even, in case malware would trigger these messages, then this opens
> the gate to attack resource limited squid-installations, like mine on
> openwrt, by flooding cache.log, kept in RAM, and possibly forcing an
> OOM-crash.
> Simple fix would be to disable cache.log, but I am hesitating to do so, not
> to drop more valuable messages.

That OpenWRT case is exactly what squid.conf "debug_options rotate=N"
option was designed for. Set the N to the number of cache.log files you
want to retain. A log monitor to trigger "squid -k rotate" when the logs
get too large completes the picture for complete control over how much
memory is spent on these logs.
 An older solution is to place a Unix pipe at the cache.log filesystem
path. Sending the log lines either directly to a processor or another
device entirely.

Amos


From arne-tobias.rak at stud.tu-darmstadt.de  Thu Jan 24 09:50:07 2019
From: arne-tobias.rak at stud.tu-darmstadt.de (Arne-Tobias Rak)
Date: Thu, 24 Jan 2019 10:50:07 +0100
Subject: [squid-users] Backing up squid cache and restoring it
In-Reply-To: <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>
References: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>
 <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>
Message-ID: <00ef9f21-60ae-f3d5-a7ee-f25b22cabd6a@stud.tu-darmstadt.de>

Thank you for your help. I eventually managed to get it working. The 
problem was related to stopping the squid service using the -k argument.

Closing squid using

sudo service squid stop

allows me to restore previous cache contents without any issues.


Am 17.01.2019 um 18:22 schrieb Alex Rousskov:
> On 1/17/19 6:47 AM, Arne-Tobias Rak wrote:
>
>> my goal is to restore a previous cache state in squid 3.x running on
>> Ubuntu 16.04.
>>
>> So far I have tried to create a copy of the /var/spool/squid and
>> /var/log/squid folders.
>> When restoring the cache, I first shutdown squid using
>> /sudo squid -k shutdown//
>> //sudo service squid stop -k
>> /and then restore the previously copied folder contents. I then start
>> squid again using
>> /sudo service squid start./
>> Unfortunately, this does not restore the previous cache contents, as the
>> spool/squid/swap.state file is modified during squid startup.
> Modification of swap.state upon startup is not incompatible with cache
> contents preservation.
>
> In a clean shutdown context, the swap.state* files are essentially an
> optimization. You may preserve/restore them if you want to speed up
> building of the restored cache index OR you can delete them (and Squid
> will slowly build a new cache index from scratch). Just do not leave the
> newer swap.state* files when trying to restore the old cache.
>
> If you preserve/restore the old swap.state* files, you may need to
> preserve their timestamps as well.
>
>
> If you need further help, please share cache.log entries related to
> cache_dir loading and indicate why you think the old cache contents is
> not preserved. Sharing your cache_dir configuration may also help.
>
>
> Cheers,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From gaardiolor at gmail.com  Thu Jan 24 12:24:18 2019
From: gaardiolor at gmail.com (Marc)
Date: Thu, 24 Jan 2019 13:24:18 +0100
Subject: [squid-users] squid hanging in 100% steal
Message-ID: <CAPxJK5Dem9Xt6fwOyg2HjZJn+Zh8raUPEZyceGAhOmWsXWZ5zA@mail.gmail.com>

Hi,

For some reason my squid sometimes hangs (after weeks of running
smoothly) in 100% steal, until I kill the proces and restart it, after
which the proces will again run stable for weeks.

It's running on a AWS EC2 instance, squid version:
squid-3.5.20-10.34.amzn1.x86_64 , see below for some debugging info.
Any idea what could be the problem here ? Thanks!

top:
[11:56:49][root at ip-172-31-9-138 ~]# top
top - 11:57:11 up 218 days, 17:36,  1 user,  load average: 1.06, 1.17, 1.09
Tasks:  81 total,   2 running,  79 sleeping,   0 stopped,   0 zombie
Cpu(s):  4.5%us,  0.3%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si, 95.2%st
Mem:    501220k total,   405748k used,    95472k free,    65512k buffers
Swap:        0k total,        0k used,        0k free,    88948k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
29963 squid     20   0  290m 171m 7472 R 99.9 35.1 672:59.73 squid
    1 root      20   0 19648 2480 2148 S  0.0  0.5   0:02.05 init
<snip>

vmstat:
[11:57:39][root at ip-172-31-9-138 ~]# vmstat 1
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  1      0  95408  65536  89052    0    0     0     4    1    1  0  0 99  0  0
 1  0      0  95408  65536  89040    0    0     0     4   56   36  5  0  0  0 95
 2  0      0  95408  65536  89040    0    0     0     0   54   18  5  0  0  0 95
 1  0      0  95408  65536  89040    0    0     0     0   57   30  5  0  0  0 95
 1  0      0  95408  65536  89040    0    0     0     4   52   25  5  0  0  0 95
 3  0      0  95408  65536  89040    0    0     0     0   52   14  6  0  0  0 94
 1  0      0  95408  65536  89040    0    0     0     0   50   26  4  0  0  0 96
 2  0      0  95408  65536  89040    0    0     0     0   53   21  6  0  0  0 94
 1  0      0  95408  65540  89036    0    0     0    12   62   38  5  0  0  0 95
 2  0      0  95408  65540  89040    0    0     0    36   55   14  5  0  0  0 95
 1  0      0  95408  65540  89040    0    0     0     0   51   34  5  0  0  0 95

gdb:
[11:55:07][root at ip-172-31-9-138 ~]# sudo gdb -n -batch -ex backtrace -pid 29963
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib64/libthread_db.so.1".
0x00000000007bca52 in
CbcPointer<Comm::TcpAcceptor>::operator=(CbcPointer<Comm::TcpAcceptor>
const&) ()
#0  0x00000000007bca52 in
CbcPointer<Comm::TcpAcceptor>::operator=(CbcPointer<Comm::TcpAcceptor>
const&) ()
#1  0x00000000007bc3d4 in Comm::AcceptLimiter::kick() ()
#2  0x0000000000721867 in AsyncCall::make() ()
#3  0x00000000007259e2 in AsyncCallQueue::fireNext() ()
#4  0x0000000000725e20 in AsyncCallQueue::fire() ()
#5  0x00000000005b0089 in EventLoop::runOnce() ()
#6  0x00000000005b0178 in EventLoop::run() ()
#7  0x00000000006192cc in SquidMain(int, char**) ()
#8  0x0000000000514b3b in main ()

strace:
[11:52:51][root at ip-172-31-9-138 ~]# strace -t -s 8192 -f -p 29963
Process 29963 attached
11:53:00 accept(10, {sa_family=AF_INET6, sin6_port=htons(45756),
inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0,
sin6_scope_id=0}, [28]) = 16
11:53:00 getsockname(16, {sa_family=AF_INET6, sin6_port=htons(3128),
inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0,
sin6_scope_id=0}, [28]) = 0
11:53:00 fcntl(16, F_GETFD)             = 0
11:53:00 fcntl(16, F_SETFD, FD_CLOEXEC) = 0
11:53:00 fcntl(16, F_GETFL)             = 0x2 (flags O_RDWR)
11:53:00 fcntl(16, F_SETFL, O_RDWR|O_NONBLOCK) = 0
11:53:00 socket(PF_INET, SOCK_STREAM, IPPROTO_IP) = 23
11:53:00 ioctl(23, SIOCGARP, 0x7ffd21abeaa0) = -1 ENODEV (No such device)
11:53:00 ioctl(23, SIOCGIFCONF, {120, {{"lo", {AF_INET,
inet_addr("127.0.0.1")}}, {"eth0", {AF_INET, inet_addr("<snip>")}},
{"eth1", {AF_INET, inet_addr("<snip>")}}}}) = 0
11:53:00 ioctl(23, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such
device or address)
11:53:00 ioctl(23, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such
device or address)
11:53:00 close(23)                      = 0
11:53:00 epoll_ctl(5, EPOLL_CTL_DEL, 27, {0, {u32=27, u64=4294967323}}) = 0
11:53:00 close(27)                      = 0
11:53:03 accept(10, {sa_family=AF_INET6, sin6_port=htons(50050),
inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0,
sin6_scope_id=0}, [28]) = 23
11:53:03 getsockname(23, {sa_family=AF_INET6, sin6_port=htons(3128),
inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0,
sin6_scope_id=0}, [28]) = 0
11:53:03 fcntl(23, F_GETFD)             = 0
11:53:03 fcntl(23, F_SETFD, FD_CLOEXEC) = 0
11:53:03 fcntl(23, F_GETFL)             = 0x2 (flags O_RDWR)
11:53:03 fcntl(23, F_SETFL, O_RDWR|O_NONBLOCK) = 0
11:53:03 socket(PF_INET, SOCK_STREAM, IPPROTO_IP) = 25
11:53:03 ioctl(25, SIOCGARP, 0x7ffd21abeaa0) = -1 ENODEV (No such device)
11:53:03 ioctl(25, SIOCGIFCONF, {120, {{"lo", {AF_INET,
inet_addr("127.0.0.1")}}, {"eth0", {AF_INET, inet_addr("<snip>")}},
{"eth1", {AF_INET, inet_addr("<snip>")}}}}) = 0
11:53:03 ioctl(25, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such
device or address)
11:53:03 ioctl(25, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such
device or address)
11:53:03 close(25)                      = 0
11:53:03 stat("/etc/localtime", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0
11:53:03 write(9, "<snip> <snip> <snip> - - [24/Jan/2019:11:52:44
+0000] \"CONNECT <snip>  HTTP/1.1\" 200 0 \"-\" \"Mozilla/5.0 (Windows
NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\"
TCP_TUNNEL:HIER_DIRECT\n", 223) = 223
11:53:03 epoll_ctl(5, EPOLL_CTL_DEL, 15, {0, {u32=15, u64=4294967311}}) = 0
11:53:03 close(15)                      = 0
^CProcess 29963 detached


From squid3 at treenet.co.nz  Thu Jan 24 13:23:03 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Jan 2019 02:23:03 +1300
Subject: [squid-users] squid hanging in 100% steal
In-Reply-To: <CAPxJK5Dem9Xt6fwOyg2HjZJn+Zh8raUPEZyceGAhOmWsXWZ5zA@mail.gmail.com>
References: <CAPxJK5Dem9Xt6fwOyg2HjZJn+Zh8raUPEZyceGAhOmWsXWZ5zA@mail.gmail.com>
Message-ID: <22a7ce79-3ca8-5f01-8d4d-3e2784c4db2a@treenet.co.nz>

On 25/01/19 1:24 am, Marc wrote:
> Hi,
> 
> For some reason my squid sometimes hangs (after weeks of running
> smoothly) in 100% steal, until I kill the proces and restart it, after
> which the proces will again run stable for weeks.

What does "100% steal" mean?

> 
> It's running on a AWS EC2 instance, squid version:
> squid-3.5.20-10.34.amzn1.x86_64 , see below for some debugging info.
> Any idea what could be the problem here ? Thanks!
> 
> top:
> [11:56:49][root at ip-172-31-9-138 ~]# top
> top - 11:57:11 up 218 days, 17:36,  1 user,  load average: 1.06, 1.17, 1.09
> Tasks:  81 total,   2 running,  79 sleeping,   0 stopped,   0 zombie
> Cpu(s):  4.5%us,  0.3%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si, 95.2%st
> Mem:    501220k total,   405748k used,    95472k free,    65512k buffers
> Swap:        0k total,        0k used,        0k free,    88948k cached
> 
>   PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
> 29963 squid     20   0  290m 171m 7472 R 99.9 35.1 672:59.73 squid
>     1 root      20   0 19648 2480 2148 S  0.0  0.5   0:02.05 init
> <snip>
> 
> vmstat:
> [11:57:39][root at ip-172-31-9-138 ~]# vmstat 1
> procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
>  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
>  1  1      0  95408  65536  89052    0    0     0     4    1    1  0  0 99  0  0
>  1  0      0  95408  65536  89040    0    0     0     4   56   36  5  0  0  0 95
>  2  0      0  95408  65536  89040    0    0     0     0   54   18  5  0  0  0 95
>  1  0      0  95408  65536  89040    0    0     0     0   57   30  5  0  0  0 95
>  1  0      0  95408  65536  89040    0    0     0     4   52   25  5  0  0  0 95
>  3  0      0  95408  65536  89040    0    0     0     0   52   14  6  0  0  0 94
>  1  0      0  95408  65536  89040    0    0     0     0   50   26  4  0  0  0 96
>  2  0      0  95408  65536  89040    0    0     0     0   53   21  6  0  0  0 94
>  1  0      0  95408  65540  89036    0    0     0    12   62   38  5  0  0  0 95
>  2  0      0  95408  65540  89040    0    0     0    36   55   14  5  0  0  0 95
>  1  0      0  95408  65540  89040    0    0     0     0   51   34  5  0  0  0 95
> 
> gdb:
> [11:55:07][root at ip-172-31-9-138 ~]# sudo gdb -n -batch -ex backtrace -pid 29963
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib64/libthread_db.so.1".
> 0x00000000007bca52 in
> CbcPointer<Comm::TcpAcceptor>::operator=(CbcPointer<Comm::TcpAcceptor>
> const&) ()
> #0  0x00000000007bca52 in
> CbcPointer<Comm::TcpAcceptor>::operator=(CbcPointer<Comm::TcpAcceptor>
> const&) ()
> #1  0x00000000007bc3d4 in Comm::AcceptLimiter::kick() ()
> #2  0x0000000000721867 in AsyncCall::make() ()
> #3  0x00000000007259e2 in AsyncCallQueue::fireNext() ()
> #4  0x0000000000725e20 in AsyncCallQueue::fire() ()
> #5  0x00000000005b0089 in EventLoop::runOnce() ()
> #6  0x00000000005b0178 in EventLoop::run() ()
> #7  0x00000000006192cc in SquidMain(int, char**) ()
> #8  0x0000000000514b3b in main ()
> 

This looks like it may be one of the symptoms of
<https://bugs.squid-cache.org/show_bug.cgi?id=4885> which was fixed in
Squid-4.3 release.

Please try the current Squid-4 release to see if the issue is already
resolved. v3.5 is no longer supported, so if it is a bug we will need
traces and replication using the current Squid (v4 or v5) version to
have a realistic chance of anyone being able to fix it.

Amos


From leolistas at solutti.com.br  Thu Jan 24 13:56:22 2019
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Thu, 24 Jan 2019 11:56:22 -0200
Subject: [squid-users] squid on openwrt: Possible to get rid of "...
 SECURITY ALERT: Host header forgery detected ..." msgs ?
In-Reply-To: <1548231721587-0.post@n4.nabble.com>
References: <1548231721587-0.post@n4.nabble.com>
Message-ID: <26634aaf-1dd6-7098-a0fa-6e0578733bef@solutti.com.br>

Em 23/01/2019 06:22, reinerotto escreveu:
> Running squid 4.4 on very limited device, unfortunately quite a lot of
> messages: "... SECURITY ALERT: Host header forgery detected ... "  show up.
> Unable to eliminate real cause of this issue (even using iptables to redir
> all DNS requests to one dnsmasq does not help), these annoying messages tend
> to fill up cache.log, which is kept in precious RAM.
> Is there an "official" method to suppress these messages ?
> Or can you please give a hint, where to apply a (hopefully) simple patch ?
>
>
>

 ??? I have some OpenWRT boxes running squid 3.5 and cache_log simply 
goes null ... i do have access log enabled, with scripts to rotate, 
export to another server (where log analyzis are done) and keep just a 
minimum on the box itself, as storage is a big problem on these boxes.



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From russel_mcdonald at swbell.net  Thu Jan 24 15:38:56 2019
From: russel_mcdonald at swbell.net (Russel McDonald)
Date: Thu, 24 Jan 2019 15:38:56 +0000 (UTC)
Subject: [squid-users] Windows ECAP Success! And question about upload
References: <192996571.419021.1548344336603.ref@mail.yahoo.com>
Message-ID: <192996571.419021.1548344336603@mail.yahoo.com>

Hi, I now have Squid running on Windows with ECAP passing both HTTP and HTTPS stream decrypted to my adapter. But I'm only seeing the download stream. How do I configure Squid to see the upload stream? For example, I'd like to go to https:\\www.twitter.com, log in, tweet "Ulysses 12345" and have my adapter change that to "Grant 12345" so that "Grant 12345" is what gets posted. A squid.conf change? Or instead do I need reverse proxy as well.
Russel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190124/97a1944e/attachment.htm>

From eliezer at ngtech.co.il  Thu Jan 24 17:33:45 2019
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 24 Jan 2019 19:33:45 +0200
Subject: [squid-users] squid hanging in 100% steal
In-Reply-To: <CAPxJK5Dem9Xt6fwOyg2HjZJn+Zh8raUPEZyceGAhOmWsXWZ5zA@mail.gmail.com>
References: <CAPxJK5Dem9Xt6fwOyg2HjZJn+Zh8raUPEZyceGAhOmWsXWZ5zA@mail.gmail.com>
Message-ID: <085a01d4b40a$f5594d10$e00be730$@ngtech.co.il>

You can try the latest squid with my repo at:
http://ngtech.co.il/repo/amzn/1/

http://ngtech.co.il/repo/amzn/1/x86_64/squid-4.5-1.amzn1.x86_64.rpm
http://ngtech.co.il/repo/amzn/1/x86_64/squid-helpers-4.5-1.amzn1.x86_64.rpm

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marc
Sent: Thursday, January 24, 2019 14:24
To: squid-users at lists.squid-cache.org
Subject: [squid-users] squid hanging in 100% steal

Hi,

For some reason my squid sometimes hangs (after weeks of running
smoothly) in 100% steal, until I kill the proces and restart it, after which the proces will again run stable for weeks.

It's running on a AWS EC2 instance, squid version:
squid-3.5.20-10.34.amzn1.x86_64 , see below for some debugging info.
Any idea what could be the problem here ? Thanks!

top:
[11:56:49][root at ip-172-31-9-138 ~]# top
top - 11:57:11 up 218 days, 17:36,  1 user,  load average: 1.06, 1.17, 1.09
Tasks:  81 total,   2 running,  79 sleeping,   0 stopped,   0 zombie
Cpu(s):  4.5%us,  0.3%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si, 95.2%st
Mem:    501220k total,   405748k used,    95472k free,    65512k buffers
Swap:        0k total,        0k used,        0k free,    88948k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
29963 squid     20   0  290m 171m 7472 R 99.9 35.1 672:59.73 squid
    1 root      20   0 19648 2480 2148 S  0.0  0.5   0:02.05 init
<snip>

vmstat:
[11:57:39][root at ip-172-31-9-138 ~]# vmstat 1 procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  1      0  95408  65536  89052    0    0     0     4    1    1  0  0 99  0  0
 1  0      0  95408  65536  89040    0    0     0     4   56   36  5  0  0  0 95
 2  0      0  95408  65536  89040    0    0     0     0   54   18  5  0  0  0 95
 1  0      0  95408  65536  89040    0    0     0     0   57   30  5  0  0  0 95
 1  0      0  95408  65536  89040    0    0     0     4   52   25  5  0  0  0 95
 3  0      0  95408  65536  89040    0    0     0     0   52   14  6  0  0  0 94
 1  0      0  95408  65536  89040    0    0     0     0   50   26  4  0  0  0 96
 2  0      0  95408  65536  89040    0    0     0     0   53   21  6  0  0  0 94
 1  0      0  95408  65540  89036    0    0     0    12   62   38  5  0  0  0 95
 2  0      0  95408  65540  89040    0    0     0    36   55   14  5  0  0  0 95
 1  0      0  95408  65540  89040    0    0     0     0   51   34  5  0  0  0 95

gdb:
[11:55:07][root at ip-172-31-9-138 ~]# sudo gdb -n -batch -ex backtrace -pid 29963 [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1".
0x00000000007bca52 in
CbcPointer<Comm::TcpAcceptor>::operator=(CbcPointer<Comm::TcpAcceptor>
const&) ()
#0  0x00000000007bca52 in
CbcPointer<Comm::TcpAcceptor>::operator=(CbcPointer<Comm::TcpAcceptor>
const&) ()
#1  0x00000000007bc3d4 in Comm::AcceptLimiter::kick() ()
#2  0x0000000000721867 in AsyncCall::make() ()
#3  0x00000000007259e2 in AsyncCallQueue::fireNext() ()
#4  0x0000000000725e20 in AsyncCallQueue::fire() ()
#5  0x00000000005b0089 in EventLoop::runOnce() ()
#6  0x00000000005b0178 in EventLoop::run() ()
#7  0x00000000006192cc in SquidMain(int, char**) ()
#8  0x0000000000514b3b in main ()

strace:
[11:52:51][root at ip-172-31-9-138 ~]# strace -t -s 8192 -f -p 29963 Process 29963 attached
11:53:00 accept(10, {sa_family=AF_INET6, sin6_port=htons(45756), inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, [28]) = 16
11:53:00 getsockname(16, {sa_family=AF_INET6, sin6_port=htons(3128), inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, [28]) = 0
11:53:00 fcntl(16, F_GETFD)             = 0
11:53:00 fcntl(16, F_SETFD, FD_CLOEXEC) = 0
11:53:00 fcntl(16, F_GETFL)             = 0x2 (flags O_RDWR)
11:53:00 fcntl(16, F_SETFL, O_RDWR|O_NONBLOCK) = 0
11:53:00 socket(PF_INET, SOCK_STREAM, IPPROTO_IP) = 23
11:53:00 ioctl(23, SIOCGARP, 0x7ffd21abeaa0) = -1 ENODEV (No such device)
11:53:00 ioctl(23, SIOCGIFCONF, {120, {{"lo", {AF_INET, inet_addr("127.0.0.1")}}, {"eth0", {AF_INET, inet_addr("<snip>")}}, {"eth1", {AF_INET, inet_addr("<snip>")}}}}) = 0
11:53:00 ioctl(23, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such device or address)
11:53:00 ioctl(23, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such device or address)
11:53:00 close(23)                      = 0
11:53:00 epoll_ctl(5, EPOLL_CTL_DEL, 27, {0, {u32=27, u64=4294967323}}) = 0
11:53:00 close(27)                      = 0
11:53:03 accept(10, {sa_family=AF_INET6, sin6_port=htons(50050), inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, [28]) = 23
11:53:03 getsockname(23, {sa_family=AF_INET6, sin6_port=htons(3128), inet_pton(AF_INET6, "::ffff:<snip>", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, [28]) = 0
11:53:03 fcntl(23, F_GETFD)             = 0
11:53:03 fcntl(23, F_SETFD, FD_CLOEXEC) = 0
11:53:03 fcntl(23, F_GETFL)             = 0x2 (flags O_RDWR)
11:53:03 fcntl(23, F_SETFL, O_RDWR|O_NONBLOCK) = 0
11:53:03 socket(PF_INET, SOCK_STREAM, IPPROTO_IP) = 25
11:53:03 ioctl(25, SIOCGARP, 0x7ffd21abeaa0) = -1 ENODEV (No such device)
11:53:03 ioctl(25, SIOCGIFCONF, {120, {{"lo", {AF_INET, inet_addr("127.0.0.1")}}, {"eth0", {AF_INET, inet_addr("<snip>")}}, {"eth1", {AF_INET, inet_addr("<snip>")}}}}) = 0
11:53:03 ioctl(25, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such device or address)
11:53:03 ioctl(25, SIOCGARP, 0x7ffd21abeaa0) = -1 ENXIO (No such device or address)
11:53:03 close(25)                      = 0
11:53:03 stat("/etc/localtime", {st_mode=S_IFREG|0644, st_size=118, ...}) = 0
11:53:03 write(9, "<snip> <snip> <snip> - - [24/Jan/2019:11:52:44
+0000] \"CONNECT <snip>  HTTP/1.1\" 200 0 \"-\" \"Mozilla/5.0 (Windows
NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\"
TCP_TUNNEL:HIER_DIRECT\n", 223) = 223
11:53:03 epoll_ctl(5, EPOLL_CTL_DEL, 15, {0, {u32=15, u64=4294967311}}) = 0
11:53:03 close(15)                      = 0
^CProcess 29963 detached
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Jan 24 22:27:56 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Jan 2019 15:27:56 -0700
Subject: [squid-users] ICAP and 403 Encapsulated answers (SSL denied
 domains)
In-Reply-To: <b877c1a4-094d-1f43-904b-1e2d7a6deaff@free.fr>
References: <0d99f322-62a8-9b7a-86cc-d0894b9a5537@free.fr>
 <8cfb73a6-6cbb-4907-f304-a43487ae3856@measurement-factory.com>
 <cc051bb8-e180-1be8-51b3-b0653c6ae2cf@free.fr>
 <6b798ccd-8653-4106-30a7-d576e4166156@measurement-factory.com>
 <b877c1a4-094d-1f43-904b-1e2d7a6deaff@free.fr>
Message-ID: <27b779bd-6dca-dfc0-abeb-b31668236536@measurement-factory.com>

On 1/23/19 3:17 AM, FredB wrote:

> I found nothing in documentation about client_persistent_connections off
> impact, do you think this can be problematic with high load ?

Yes, disabling client-to-Squid persistent connections can increase load
on the proxy server. In SslBump environments that bump many connections,
such an increase can be drastic. IIRC, it may also break some
authenticaiton mechanisms that rely on connection persistency.

Alex.




From rousskov at measurement-factory.com  Thu Jan 24 22:47:43 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Jan 2019 15:47:43 -0700
Subject: [squid-users] squid on openwrt: Possible to get rid of "...
 SECURITY ALERT: Host header forgery detected ..." msgs ?
In-Reply-To: <5b9085e4-7d5b-6f7a-15c4-5e2056d20563@treenet.co.nz>
References: <1548231721587-0.post@n4.nabble.com>
 <a1a6a99b-970d-4dcd-1476-6a4371b849a9@treenet.co.nz>
 <1548251756880-0.post@n4.nabble.com>
 <5b9085e4-7d5b-6f7a-15c4-5e2056d20563@treenet.co.nz>
Message-ID: <ce1d2e27-6d26-d6b3-3a65-00efcef7f67e@measurement-factory.com>

On 1/23/19 6:44 PM, Amos Jeffries wrote:
> For now all we can do is take the warnings seriously and find ways to
> prevent the network behaviours that cause them. 

For the record, the above is an opinion rather than a fact or consensus.
There are, of course, other (and far more realistic/useful) things we
could do. For example, we could give the admin the choice of which
"forgeries" should be classified as false positives and treated
accordingly, and we could improve reporting of the "forgeries" so that
the reporting itself does not become a problem.


> The security issues this detection prevents are so nasty we consider
> the pain worth the price of avoiding those outcomes.

In many cases, it would be possible for admins to suffer virtually no
"pain" while still "preventing security issues" and following best
practices (such as keeping logging enabled).

Alex.


From rousskov at measurement-factory.com  Thu Jan 24 22:55:21 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Jan 2019 15:55:21 -0700
Subject: [squid-users] Windows ECAP Success! And question about upload
In-Reply-To: <192996571.419021.1548344336603@mail.yahoo.com>
References: <192996571.419021.1548344336603.ref@mail.yahoo.com>
 <192996571.419021.1548344336603@mail.yahoo.com>
Message-ID: <15c63835-aa19-1c7e-9ca4-31ff8d02904b@measurement-factory.com>

On 1/24/19 8:38 AM, Russel McDonald wrote:
> Hi, I now have Squid running on Windows with ECAP passing both HTTP and
> HTTPS stream decrypted to my adapter. But I'm only seeing the download
> stream. How do I configure Squid to see the upload stream?

To see requests, you need a REQMOD eCAP service (reqmod_precache). A
REQMOD service is similar to a RESPMOD service you apparently have
working, but it is used for HTTP requests instead of HTTP responses. The
same set of squid.conf directives is used for both REQMOD and RESPMOD
vectoring points. The service itself will need to be written to handle
HTTP request messages instead of HTTP response messages, but the
protocol-level differences are minor.

Alex.


From squid3 at treenet.co.nz  Fri Jan 25 02:56:59 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Jan 2019 15:56:59 +1300
Subject: [squid-users] Backing up squid cache and restoring it
In-Reply-To: <00ef9f21-60ae-f3d5-a7ee-f25b22cabd6a@stud.tu-darmstadt.de>
References: <3bb9c2be-4f7a-0a2e-ca1d-96a24c1e6b3c@stud.tu-darmstadt.de>
 <d97d597d-d66f-1c26-cd7a-6fe2afd1af2a@measurement-factory.com>
 <00ef9f21-60ae-f3d5-a7ee-f25b22cabd6a@stud.tu-darmstadt.de>
Message-ID: <cd547ff9-ae2a-768b-8aec-0e226b06a0dc@treenet.co.nz>

On 24/01/19 10:50 pm, Arne-Tobias Rak wrote:
> Thank you for your help. I eventually managed to get it working. The
> problem was related to stopping the squid service using the -k argument.
> 
> Closing squid using
> 
> sudo service squid stop
> 
> allows me to restore previous cache contents without any issues.
> 

Hmm, that does not make sense to me.

Squid-3.x are not compatible with systemd. They only way to use Squid-3
with a .service setup is for that to be linked up to use the "-k
shutdown" commands internally. Even then systemd's default SIGKILL
behaviour and confusion over what PID it is supposed to be monitoring
can result in problems anyway.

Please switch to Squid-4 if you want to use systemd controls.

Amos


From berezin008 at ya.ru  Fri Jan 25 08:15:35 2019
From: berezin008 at ya.ru (=?utf-8?B?0JDQu9C10LrRgdCw0L3QtNGAINCQ0LvQtdC60YHQsNC90LTRgNC+0LLQuNGHINCR0LXRgNC10LfQuNC9?=)
Date: Fri, 25 Jan 2019 13:15:35 +0500
Subject: [squid-users] HELP! Ssl_bump - acl , dstdomain ,
	denied by fqdn need ip
In-Reply-To: <70191851548399562@sas1-87f9feb8d943.qloud-c.yandex.net>
Message-ID: <57763001548404135@sas1-512a5b2b1037.qloud-c.yandex.net>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/400d0d48/attachment.htm>

From alessio.troiano at leonardocompany.com  Fri Jan 25 10:29:46 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 25 Jan 2019 10:29:46 +0000
Subject: [squid-users] How to definitively disable IPv6
Message-ID: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>

Hello,
I need to definitively solve the ipv6 (un)reachbility issue.
I state I read this topic: http://squid-web-proxy-cache.1019090.n4.nabble.com/dns-v4-first-on-ignored-td4658427.html but not found a solution. Amos wrote ?Squid tests for IPv6 ability automatically by opening a socket on a private IP address, if that works the socket options are noted and used.?
Anyway I disable IPv6 on my Red Hat 7.4 with the following:
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.bond0.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
Used the ?dns_v4_first on? and also ?tcp_outgoing_address 172.31.1.x all? on squid conf to force the use of IPv4.
Anyway squid try to connect to the IPv6 address instead of IPv4 and I?m not able to reach it:
C:\Users\atroiano>nslookup download.pdfforge.org
Server:  espevmdxxxx.xxxx.prv
Address:  172.x.x.x

Risposta da un server non autorevole:
Nome:    download.pdfforge.org
Addresses:  2001:4860:4802:38::15
          2001:4860:4802:34::15
          2001:4860:4802:32::15
          2001:4860:4802:36::15
          216.239.32.21
          216.239.38.21
          216.239.36.21
          216.239.34.21
[root at HUB-RM-PRX-03 ~]# tail -f /var/log/squid/rsa/access.log | grep pdfforge.org
%SQUID-4: 172.31.x.x 49444 [25/Jan/2019:11:02:58 +0100] "GET http://download.pdfforge.org/download/pdfcreator/PDFCreator-stable HTTP/1.1" download.pdfforge.org - - "/download/pdfcreator/PDFCreator-stable" 503 text/html 4545 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0" TCP_MISS:HIER_DIRECT 2001:4860:4802:38::15 80 0
Squid doesn?t try to connect to IPv4 addresses for this site and for many others.

What can I do?
My ISP is IPv4 only.

Thank you, Regards.

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.
________________________________
The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/985ee26d/attachment.htm>

From bruno.larini at riosoft.com.br  Fri Jan 25 12:07:45 2019
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Fri, 25 Jan 2019 10:07:45 -0200
Subject: [squid-users] How to definitively disable IPv6
In-Reply-To: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
References: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
Message-ID: <e0bd9b08-0431-5d0f-a41b-1c179bd11575@riosoft.com.br>

Em 25/01/2019 08:29, Troiano Alessio escreveu:
> What can I do?
>
> My ISP is IPv4 only.
>
I'm not completely sure but it looks more like a DNS issue than the IP 
binding on Squid server. But check if your 'ifcfg-ethX' has IPV6INIT=no. 
Also, is Squid listening on all local IPs? If yes, then try binding it 
on a single local IPv4 with "http_port" instead.
(this may be out of the scope of the list but...) If your DNS server on 
172.x.x.x is running named, check if it has OPTIONS="-4" on 
sysconfig/named or if 'listen-on-v6 port 53 { ::1; };' on named.conf is 
commented out. If it is caching, clean it too just to be sure.

-Bruno
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/b2b7c075/attachment.htm>

From alessio.troiano at leonardocompany.com  Fri Jan 25 13:15:53 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 25 Jan 2019 13:15:53 +0000
Subject: [squid-users] R:  How to definitively disable IPv6
In-Reply-To: <e0bd9b08-0431-5d0f-a41b-1c179bd11575@riosoft.com.br>
References: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
 <e0bd9b08-0431-5d0f-a41b-1c179bd11575@riosoft.com.br>
Message-ID: <5f90a51fac5f4d748714d4a678894d8f@ocgepvsw3101.ocr.priv>

Thank you Bruno for the answer.
The DNS returns both IPv6 and IPv4 addresses, but it depends on the request (A or AAAA). Squid should do both and prefer in order the IPv4 answer.
I added the IPV6INIT=no on my interface and http_port 172.31.1.68:8080, restarted squid, but same behavior.

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.
________________________________
The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/54cb8e7d/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 25 15:10:30 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Jan 2019 04:10:30 +1300
Subject: [squid-users] HELP! Ssl_bump - acl , dstdomain ,
 denied by fqdn need ip
In-Reply-To: <57763001548404135@sas1-512a5b2b1037.qloud-c.yandex.net>
References: <57763001548404135@sas1-512a5b2b1037.qloud-c.yandex.net>
Message-ID: <3833ea72-0c76-5172-dda3-98a4aa966212@treenet.co.nz>

On 25/01/19 9:15 pm, ????????? ????????????? ??????? wrote:
> Please HELP!
> ?
> Hello dear members of the community
> excuse me for disturbing me, but I could not find an answer to the
> question, so I speak to you, sorry again
> ?
> i have
> ?
...
> 
> in /etc/squid.conf
> 
> .......
> 
> acl test dstdomain partner.steam-api.com
> ?
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ?
> ssl_bump peek step1 all

NP: That 'all' has no purpose here.

> ssl_bump splice test

The ssl_bump rules when checked for intercepted traffic are run *before*
anything gets decrypted. Thus there is no HTTP(S) request to get a URL
from, so no URL domain (dstdomain).

Use ssl::server_name ACL type instead. It can match TLS SNI domain (if
any) retrieved by the step1 peek action.


> ssl_bump bump
> ?
> ?
> http_port 192.168.50.1:3128 intercept
> https_port 192.168.50.1:3129 intercept ssl-bump
> options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off
> cert=/etc/squid/ssl_cert/squidCA.pem
> ?
> ?
> ?
> when I am trying to access the site from a browser from a local network
> partner.steam-api.com
> ?
> access.log
> ?
> [Fri Jan 25 06:50:10 2019].514 ? ? ?0 192.168.50.10 TCP_DENIED/200 0
> CONNECT 208.64.202.87:443 - HIER_NONE/- -

Traffic arriving is immediately being denied access into the proxy. The
other log entries and errors are resulting from that fact.

> ?
> but the address at the end partner.steam-api.com ?can be dynamic and
> constantly changing, so I need a connection by name
> tell me what is my mistake?

Two mistakes. First is the dstdomain vs ssl::server_name ACL types
mentioned above.

Second mistake is http_access rules deny'ing CONNECT messages generated
by Squid to represent the TCP SYN packet for SSL-Bump step1. At that
point all Squid has access to is the raw-IP:port details. SNI where the
server name is received requires the initial CONNECT to be allowed into
the proxy before the TLS inspection can begin.


Amos


From rousskov at measurement-factory.com  Fri Jan 25 15:16:52 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Jan 2019 08:16:52 -0700
Subject: [squid-users] HELP! Ssl_bump - acl , dstdomain ,
 denied by fqdn need ip
In-Reply-To: <57763001548404135@sas1-512a5b2b1037.qloud-c.yandex.net>
References: <57763001548404135@sas1-512a5b2b1037.qloud-c.yandex.net>
Message-ID: <5224291a-8399-1c6b-357d-a7c749a6c7d4@measurement-factory.com>

On 1/25/19 1:15 AM, ????????? ????????????? ??????? wrote:

> 0 192.168.50.10 TCP_DENIED/200 0 CONNECT 208.64.202.87:443 - HIER_NONE/- -

Looks like your http_access rules deny some (or all) CONNECT requests,
probably during SslBump step1. This is not related to your ssl_bump
rules. Examine those rules and adjust them to allow CONNECT requests you
want to allow (and deny all other CONNECT requests).


> acl test dstdomain partner.steam-api.com

I doubt this causes TCP_DENIED errors, but you may want to use an
ssl::server_name ACL instead of dstdomain.


HTH,

Alex.


> [Fri Jan 25 06:50:10 2019].516 ? ? ?0 192.168.50.10 TCP_DENIED/200 0
> CONNECT 208.64.202.87:443 - HIER_NONE/- -
> [Fri Jan 25 06:50:10 2019].530 ? ? ?0 192.168.50.10 TCP_DENIED/200 0
> CONNECT 208.64.202.87:443 - HIER_NONE/- -
> [Fri Jan 25 06:50:10 2019].537 ? ? ?0 192.168.50.10 TAG_NONE/403 3806
> GET https://partner.steam-api.com/ - HIER_NONE/- text/html
> [Fri Jan 25 06:50:10 2019].568 ? ? ?0 192.168.50.10 TCP_DENIED/200 0
> CONNECT 208.64.202.87:443 - HIER_NONE/- -
> [Fri Jan 25 06:50:10 2019].576 ? ? ?0 192.168.50.10 TCP_DENIED/200 0
> CONNECT 208.64.202.87:443 - HIER_NONE/- -
> [Fri Jan 25 06:50:10 2019].583 ? ? ?0 192.168.50.10 TAG_NONE/403 3806
> GET http://berezin:0/squid-internal-static/icons/SN.png - HIER_NONE/-
> text/html
> ?
> in browser i have are error
> ?
> squid error the requested url could not be retrieved
> the following error was encountered while trying to retrieve the url
> https://208.64.202.87 <https://208.64.202.87/>
> ?
> if i add 208.64.202.87 <https://208.64.202.87/> in acl test dstdomain
> everything is good and I connect to partner.steam-api.com
> ?
> ?
> but the address at the end partner.steam-api.com ?can be dynamic and
> constantly changing, so I need a connection by name
> tell me what is my mistake?
> ?
> --?
> ? ?????????,
> ????????? ????????????? ???????
> ?
> With respect,
> Alexander Alexandrovich Berezin
> ?
> ?
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Fri Jan 25 16:00:07 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Jan 2019 09:00:07 -0700
Subject: [squid-users] How to definitively disable IPv6
In-Reply-To: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
References: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
Message-ID: <7c5b0413-e913-a908-49d3-23c98a33d7d6@measurement-factory.com>

On 1/25/19 3:29 AM, Troiano Alessio wrote:

> I need to definitively solve the ipv6 (un)reachbility issue.

You can

* build Squid with --disable-ipv6

* try an experimental (and unofficial) Squid branch that implements the
TCP part of the Happy Eyeballs algorithm:
https://github.com/measurement-factory/squid/pull/3

Alex.



> I state I read this topic:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/dns-v4-first-on-ignored-td4658427.html
> but not found a solution. Amos wrote ?Squid tests for IPv6 ability
> automatically by opening a socket on a private IP address, if that works
> the socket options are noted and used.?
> 
> Anyway I disable IPv6 on my Red Hat 7.4 with the following:
> 
> net.ipv6.conf.all.disable_ipv6 = 1
> 
> net.ipv6.conf.default.disable_ipv6 = 1
> 
> net.ipv6.conf.bond0.disable_ipv6 = 1
> 
> net.ipv6.conf.lo.disable_ipv6 = 1
> 
> Used the ?dns_v4_first on? and also ?tcp_outgoing_address 172.31.1.x
> all? on squid conf to force the use of IPv4.
> 
> Anyway squid try to connect to the IPv6 address instead of IPv4 and I?m
> not able to reach it:
> 
> C:\Users\atroiano>nslookup download.pdfforge.org
> 
> Server:? espevmdxxxx.xxxx.prv
> 
> Address:? 172.x.x.x
> 
> ?
> 
> Risposta da un server non autorevole:
> 
> Nome:??? download.pdfforge.org
> 
> Addresses:? 2001:4860:4802:38::15
> 
> ????????? 2001:4860:4802:34::15
> 
> ????????? 2001:4860:4802:32::15
> 
> ????????? 2001:4860:4802:36::15
> 
> ????????? 216.239.32.21
> 
> ????????? 216.239.38.21
> 
> ????????? 216.239.36.21
> 
> ????????? 216.239.34.21
> 
> [root at HUB-RM-PRX-03 ~]# tail -f /var/log/squid/rsa/access.log | grep
> pdfforge.org
> 
> %SQUID-4: 172.31.x.x 49444 [25/Jan/2019:11:02:58 +0100] "GET
> http://download.pdfforge.org/download/pdfcreator/PDFCreator-stable
> HTTP/1.1" download.pdfforge.org - -
> "/download/pdfcreator/PDFCreator-stable" 503 text/html 4545 "-"
> "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101
> Firefox/64.0" TCP_MISS:HIER_DIRECT 2001:4860:4802:38::15 80 0
> 
> Squid doesn?t try to connect to IPv4 addresses for this site and for
> many others.
> 
> ?
> 
> What can I do?
> 
> My ISP is IPv4 only.
> 
> ?
> 
> Thank you, Regards.
> 
> 
> Il presente messaggio e-mail e ogni suo allegato devono intendersi
> indirizzati esclusivamente al destinatario indicato e considerarsi dal
> contenuto strettamente riservato e confidenziale. Se non siete
> l'effettivo destinatario o avete ricevuto il messaggio e-mail per
> errore, siete pregati di avvertire immediatamente il mittente e di
> cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema
> informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del
> presente messaggio da parte di chi non ne ? il destinatario ?
> strettamente proibito e pu? dar luogo a responsabilit? di carattere
> civile e penale punibili ai sensi di legge.
> Questa e-mail ha valore legale solo se firmata digitalmente ai sensi
> della normativa vigente.
> ------------------------------------------------------------------------
> The contents of this email message and any attachments are intended
> solely for the addressee(s) and contain confidential and/or privileged
> information.
> If you are not the intended recipient of this message, or if this
> message has been addressed to you in error, please immediately notify
> the sender and then delete this message and any attachments from your
> system. If you are not the intended recipient, you are hereby notified
> that any use, dissemination, copying, or storage of this message or its
> attachments is strictly prohibited. Unauthorized disclosure and/or use
> of information contained in this email message may result in civil and
> criminal liability. ?
> This e-mail has legal value according to the applicable laws only if it
> is digitally signed by the sender
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Fri Jan 25 16:00:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Jan 2019 05:00:18 +1300
Subject: [squid-users] How to definitively disable IPv6
In-Reply-To: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
References: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
Message-ID: <21317797-8c47-6d29-ee33-e1a28a3150da@treenet.co.nz>

On 25/01/19 11:29 pm, Troiano Alessio wrote:
> Hello,
> 
> I need to definitively solve the ipv6 (un)reachbility issue.
> 
> I state I read this topic:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/dns-v4-first-on-ignored-td4658427.html
> but not found a solution. Amos wrote ?Squid tests for IPv6 ability
> automatically by opening a socket on a private IP address, if that works
> the socket options are noted and used.?
> 
> Anyway I disable IPv6 on my Red Hat 7.4 with the following:
> 
> net.ipv6.conf.all.disable_ipv6 = 1
> 
> net.ipv6.conf.default.disable_ipv6 = 1
> 
> net.ipv6.conf.bond0.disable_ipv6 = 1
> 
> net.ipv6.conf.lo.disable_ipv6 = 1
> 

IIRC there are boot options necessary so the machine kernel starts with
its IPv6 TCP stack disabled.


> Used the ?dns_v4_first on? and also ?tcp_outgoing_address 172.31.1.x
> all? on squid conf to force the use of IPv4.

Neither of which forces anything.

 dns_v4_first influences the sorting order of DNS results provided to
Squids server selection logic. Services which are IPv6-only or whose
IPv4 are not working _will_ attempt to use IPv6.


  NP: Please be aware that error pages only mention the *last* error to
be encountered. With dns_v4_first you will see an IPv6 address being
mentioned as not contactable. Because all the IPv4 failed (first) then
all the IPv6 failed (last).


 tcp_outgoing_address only applies on protocols for which that address
is valid. Meaning the above only sets a particular address on IPv4
connections - it has no effect on IPv6 connections.


The only way to completely disable IPv6 is to build Squid with
--disable-ipv6.


> 
> Anyway squid try to connect to the IPv6 address instead of IPv4 and I?m
> not able to reach it:
> 
> C:\Users\atroiano>nslookup download.pdfforge.org
> 
> Server:? espevmdxxxx.xxxx.prv
> 
> Address:? 172.x.x.x
> 
> ?
> 
> Risposta da un server non autorevole:
> 
> Nome:??? download.pdfforge.org
> 
> Addresses:? 2001:4860:4802:38::15
> 
> ????????? 2001:4860:4802:34::15
> 
> ????????? 2001:4860:4802:32::15
> 
> ????????? 2001:4860:4802:36::15
> 
> ????????? 216.239.32.21
> 
> ????????? 216.239.38.21
> 
> ????????? 216.239.36.21
> 
> ????????? 216.239.34.21
> 

Are any of those IPv4 addresses able to be connected to and fetched from
by processes on the Squid machine?

The squidclient tool can be used to probe individual server/IP for
issues fetching requests.



> [root at HUB-RM-PRX-03 ~]# tail -f /var/log/squid/rsa/access.log | grep
> pdfforge.org
> 
> %SQUID-4: 172.31.x.x 49444 [25/Jan/2019:11:02:58 +0100] "GET
> http://download.pdfforge.org/download/pdfcreator/PDFCreator-stable
> HTTP/1.1" download.pdfforge.org - -
> "/download/pdfcreator/PDFCreator-stable" 503 text/html 4545 "-"
> "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101
> Firefox/64.0" TCP_MISS:HIER_DIRECT 2001:4860:4802:38::15 80 0
> 
> Squid doesn?t try to connect to IPv4 addresses for this site and for
> many others.
> 

I suspect Squid actually is, but not telling you everything it does to
retry different destination servers / IPs before it gets to the final
failure point.

Please check the mgr:ipcache log to see what IPs Squid has known for
that domain and which ones are flagged 'B' for broken/bad/failing.

Amos


From bernsen at gmail.com  Fri Jan 25 16:51:34 2019
From: bernsen at gmail.com (Bill Bernsen)
Date: Fri, 25 Jan 2019 11:51:34 -0500
Subject: [squid-users] Using a static wildcard certificate with ssl-bump in
 explicit forward proxy mode
Message-ID: <CAEPopkVwgFj=xbX4Pz9ohAiK8APq9K4imrVN-Ba7J9FaF6tUMA@mail.gmail.com>

Hi,

I have squid running as an explicit forward proxy on the host
example.com controlling
access to all hosts in *.example.com. All the hosts in *.example.com have
self-signed certificates that I want to appear as trusted to user browsers.
I don't have the option of obtaining a trusted CA. I do, however, have a
trusted wildcard certificate for *.example.com available. Is there a way
that I can tell squid to present this static wildcard certificate to
clients in lieu of all upstream server certificates?

Thank you,
Bill
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/c5bf5641/attachment.htm>

From bandeep2000 at gmail.com  Fri Jan 25 17:18:51 2019
From: bandeep2000 at gmail.com (bandeep2000)
Date: Fri, 25 Jan 2019 09:18:51 -0800
Subject: [squid-users] Fwd: Https blocked sites getting ssl error ,
 with connection abruptly ending - Peek and splice feature
In-Reply-To: <CAL0CaX8GWhm4TKg892Gt8tf+gMr3-_5edecg=hShrKPJqoC8kA@mail.gmail.com>
References: <CAL0CaX8GWhm4TKg892Gt8tf+gMr3-_5edecg=hShrKPJqoC8kA@mail.gmail.com>
Message-ID: <CAL0CaX_O3C=1JJ_OHSEm_-M0CQrEx7RALcDov-6eYQKQKnwskg@mail.gmail.com>

Hi Everyone,

Have configured squid proxy with https whitelisted sites using ssl bump,
peek and splice feature
in transparent mode.
Although non whitelisted site are getting blocked, but it is not graceful,
with 'ssl connect error'  and  no 403 message(using curl). For http, it is
working fine with Access denied with 403 http error code.

Using ssl bump 'terminate all' seem to abruptly stop the connection, this
might cause issues in our application.

Is there a way to terminate the connection with access denied message
gracefully(with 403 error code)  just like it does for Http.

*Non Whitelisted site error:*

curl -I https://nba.com

*curl: (35) SSL connect error*

*http non whitelisted site:*

c5278791 at ban-squid-client22 ~]$ curl -I http://nba.com

HTTP/1.1 403 Forbidden

Server: squid/3.5.28

Mime-Version: 1.0

Date: Fri, 25 Jan 2019 17:01:38 GMT

Content-Type: text/html;charset=utf-8

Content-Length: 3574

X-Squid-Error: ERR_ACCESS_DENIED 0

Vary: Accept-Language

Content-Language: en

X-Cache: MISS from squid

Via: 1.1 squid (squid/3.5.28)

Connection: keep-alive

*https whitelisted site works fine:*

curl -I https://cnn.com

HTTP/1.1 301 Moved Permanently

Server: Varnish

Retry-After: 0

Content-Length: 0

Cache-Control: public, max-age=600

Location: https://www.cnn.com/

Accept-Ranges: bytes

Date: Fri, 25 Jan 2019 17:00:08 GMT

Via: 1.1 varnish

Connection: close

Set-Cookie: countryCode=US; Domain=.cnn.com; Path=/

Set-Cookie: geoData=mountain view|CA|94043|US|NA; Domain=.cnn.com; Path=/

X-Served-By: cache-sea1038-SEA

X-Cache: HIT

X-Cache-Hits: 0





*Squid.conf Details:*

visible_hostname squid



cache deny all

#Handling HTTP requests

http_port 3128 intercept

acl allowed_http_sites dstdomain .amazonaws.com .bbc.com

#acl allowed_http_sites dstdomain [you can add other domains to permit]

http_access allow allowed_http_sites



#Handling HTTPS requests

https_port 3130 cert=/etc/pki/tls/certs/squidCA.pem ssl-bump intercept

acl SSL_port port 443

http_access allow SSL_port

acl allowed_https_sites ssl::server_name .amazonaws.com .cnn.com .yahoo.com
.bbc.com

#acl allowed_https_sites ssl::server_name [you can add other domains to
permit]

acl step1 at_step SslBump1

acl step2 at_step SslBump2

acl step3 at_step SslBump3

ssl_bump peek step1 all

ssl_bump splice allowed_https_sites

#ssl_bump peek step2 all

ssl_bump terminate  all



http_access deny all


*Squid version:*

squid -v

Squid Cache: Version *3.5.28*

Service Name: squid


This binary uses OpenSSL 1.0.1e-fips 11 Feb 2013. For legal restrictions on
distribution see https://www.openssl.org/source/license.html


configure options:  '--prefix=/usr' '--includedir=/usr/include'
'--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--localstatedir=/var' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-openssl' '--enable-ssl-crtd'
--enable-ltdl-convenien


*OS version:*

cat /etc/redhat-release

CentOS release 6.10 (Final)

Thanks,

-Bandeep
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/32f83fa5/attachment.htm>

From musheghdavtyan at gmail.com  Fri Jan 25 17:30:11 2019
From: musheghdavtyan at gmail.com (=?utf-8?B?1YTVuNaC1bfVpdWyINS01aHVvtWp1bXVodW2?=)
Date: Fri, 25 Jan 2019 21:30:11 +0400
Subject: [squid-users] YouTube cache
In-Reply-To: <mailman.0.1548437230.20968.squid-users@lists.squid-cache.org>
References: <mailman.0.1548437230.20968.squid-users@lists.squid-cache.org>
Message-ID: <5c4b47a1.1c69fb81.9d9a6.55f3@mx.google.com>

Hi dear squid users. Somebody can help me to make youtube online videos cache? Anybody have experience in 2019? Thanks a lot
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190125/7a6f98ba/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 25 18:19:33 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Jan 2019 11:19:33 -0700
Subject: [squid-users] Fwd: Https blocked sites getting ssl error ,
 with connection abruptly ending - Peek and splice feature
In-Reply-To: <CAL0CaX_O3C=1JJ_OHSEm_-M0CQrEx7RALcDov-6eYQKQKnwskg@mail.gmail.com>
References: <CAL0CaX8GWhm4TKg892Gt8tf+gMr3-_5edecg=hShrKPJqoC8kA@mail.gmail.com>
 <CAL0CaX_O3C=1JJ_OHSEm_-M0CQrEx7RALcDov-6eYQKQKnwskg@mail.gmail.com>
Message-ID: <d1e7b5f6-4390-a050-e049-4a030a1d3c3b@measurement-factory.com>

On 1/25/19 10:18 AM, bandeep2000 wrote:

> Have configured squid proxy with https whitelisted sites using ssl bump,
> peek and splice feature in transparent mode.


> Is there a way to terminate the connection with access denied message
> gracefully(with 403 error code)

Yes, there is, but it comes at a price: If you want to serve an HTTP
response to the TLS client, you must bump the client connection.
Actually, Squid will bump on errors automatically for you if you do
_not_ tell it to terminate the TLS connection in ssl_bump rules and rely
on http_access for access control instead. Here is an incomplete and
untested sketch to illustrate the idea:

  ssl_bump peek all
  ssl_bump splice all

  ... add rules to allow step1 CONNECT requests here ...
  http_access allow allowed_http_sites
  http_access deny all

Alex.


From squid3 at treenet.co.nz  Sat Jan 26 02:42:29 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Jan 2019 15:42:29 +1300
Subject: [squid-users] Using a static wildcard certificate with ssl-bump
 in explicit forward proxy mode
In-Reply-To: <CAEPopkVwgFj=xbX4Pz9ohAiK8APq9K4imrVN-Ba7J9FaF6tUMA@mail.gmail.com>
References: <CAEPopkVwgFj=xbX4Pz9ohAiK8APq9K4imrVN-Ba7J9FaF6tUMA@mail.gmail.com>
Message-ID: <884b51e1-2876-d642-1420-4ef0b87b09bf@treenet.co.nz>

On 26/01/19 5:51 am, Bill Bernsen wrote:
> Hi,
> 
> I have squid running as an explicit forward proxy on the
> host?example.com <http://example.com/>?controlling access to all hosts
> in *.example.com <http://example.com/>. All the hosts in *.example.com
> <http://example.com/>?have self-signed certificates that I want to
> appear as trusted to user browsers. I don't have the option of obtaining
> a trusted CA. I do, however, have a trusted wildcard certificate for
> *.example.com <http://example.com/>?available. Is there a way that I can
> tell squid to present this static wildcard certificate to clients in
> lieu of all upstream server certificates?


As a forward proxy clients are *not* connecting to any of the
*.example.com domains. They are connecting to your proxy hostname - and
telling it to take care of the origin connections. So all clients need
is trust for the CA which signed the proxy's certificate.

The proxy is the only agent in the path which needs to trust the
wildcard *.example.com certificate.


Amos


From schokobecher at gmail.com  Sat Jan 26 03:19:03 2019
From: schokobecher at gmail.com (Schokobecher)
Date: Sat, 26 Jan 2019 10:19:03 +0700
Subject: [squid-users] Only allow specific Users per Port
Message-ID: <CANXaQcrGt1QrBGjE2x1ip79hqr3pPbSNMT3F_8Pw4k3hHH9ySg@mail.gmail.com>

Hello,

I'm struggling quite a bit with transitioning from basic_ncsa_auth to
basic_db_auth.
I have some ports where only certain users (sometimes just one) is allowed
to connect/pass the ACL check.

I'm running Squid 3.28 on Ubuntu

I have lines like this:
acl userA proxy_auth_regex -i userA

Which reads the htpasswd file and matches the user based on the regex.
Port config looks like this:

http_port 3201 name=3201
acl userA3201 myportname 3201
cache_peer example.com parent 3300 0 no-query no-digest proxy-only
standby=60 name=up01
cache_peer_access  up01  allow userA3201
never_direct allow userA3201
http_access allow  userA3201 userA

And that for multiple Ports.

I now want to transition to basic_db_auth and got it up and running, but
the problem is that the above does not work anymore. All authed users can
now connect to every port.

UserA can use Port 3201,3202,3206 for connecting to the proxy
UserB can't use these and only can use 3315

What is the best/cleanest way to regain the above functionality?

Thank you in advance!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190126/002715d5/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 26 03:30:35 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Jan 2019 16:30:35 +1300
Subject: [squid-users] YouTube cache
In-Reply-To: <5c4b47a1.1c69fb81.9d9a6.55f3@mx.google.com>
References: <mailman.0.1548437230.20968.squid-users@lists.squid-cache.org>
 <5c4b47a1.1c69fb81.9d9a6.55f3@mx.google.com>
Message-ID: <7d123d23-c5b4-c511-1938-790f70bd4ae2@treenet.co.nz>

On 26/01/19 6:30 am, ?????? ??????? wrote:
> Hi dear squid users. Somebody can help me to make youtube online videos
> cache? Anybody have experience in 2019? Thanks a lot
> 

The situation with YouTube cacheability has not changed in several
years. The many discussions and tools mentioned on this mailing list and
others about it since ~2014 still apply.

* The videos themselves are fully cacheable already. The issue is just
that there are a very large number of encoding variants for each video.

So...

* The tangled sequence of web requests leading to a particular video has
as many or more variations of HTTP transactions. Improving HIT ratios
needs to account for all this stateful complication.


Eliezer and some others have made tools to untangle that maze and
identify which video URIs are equivalent and can be de-duplicated in the
cache. I will leave it to one of them to post the current details of
their products.


Amos


From squid3 at treenet.co.nz  Sat Jan 26 04:21:04 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Jan 2019 17:21:04 +1300
Subject: [squid-users] Only allow specific Users per Port
In-Reply-To: <CANXaQcrGt1QrBGjE2x1ip79hqr3pPbSNMT3F_8Pw4k3hHH9ySg@mail.gmail.com>
References: <CANXaQcrGt1QrBGjE2x1ip79hqr3pPbSNMT3F_8Pw4k3hHH9ySg@mail.gmail.com>
Message-ID: <0a2fbd4e-2f92-a8e4-5a28-b60c6cb6472a@treenet.co.nz>

On 26/01/19 4:19 pm, Schokobecher wrote:
> Hello,
> 
> I'm struggling quite a bit with transitioning from?basic_ncsa_auth to
> basic_db_auth.
> I have some ports where only certain users (sometimes just one) is
> allowed to connect/pass the ACL check.
> 
> I'm running Squid 3.28 on Ubuntu??
> 
> I have lines like this:
> acl?userA proxy_auth_regex -i userA
> 
> Which reads the htpasswd file and matches the user based on the regex.

That is technically wrong. When figuring out this type of problem the
details matter.

That is an ACL which reads the HTTP request message for details and
matches true if it finds "usera" or any case-insensitive variation of that.

It has a prerequisite that the auth system has already authenticated
those credentials as valid. But the ACL itself does not do any of that.

As a result of that seemingly minor detail that ACL will happily
non-match when it should match if the access control using it is a
'fast' category control. Correlated with that it may also wrongly match
if the ACL is configured in a '!' modifier.



> Port config looks like this:
> 
> http_port 3201 name=3201
> acl userA3201 myportname 3201
> cache_peer example.com parent 3300 0 no-query
> no-digest proxy-only standby=60 name=up01
> cache_peer_access? up01? allow userA3201
> never_direct allow userA3201
> http_access allow? userA3201 userA

So "usera" is allowed when they use port 3201.

What else have you configured? This line *cannot* be the one allowing
other users to that port, nor this user to other ports. Some other line
or combination of lines is doing that.


> 
> And that for multiple Ports.
> 
> I now want to transition to basic_db_auth and got it up and running, but
> the problem is that the above does not work anymore. All authed users
> can now connect to every port.
> 

That implies something in your access controls changed. The few you have
mentioned do not show anything related to the problem.

OR, maybe you set the DB helper to return OK for users unrelated to the
actual HTTP request client. You have omitted those details too.


> UserA can use Port 3201,3202,3206 for connecting to the proxy
> UserB can't use these and only can use 3315
> 
> What is the best/cleanest way to regain the above functionality?

Cleanest way is to:

 1) revert to the old config file. check that it still works.

 2) check that the new SQL DB contents match the NCSA htpasswd entries.

 3) change only the auth_param "program" line setting which helper is
used. Nothing else, not even other auth_param lines should be touched (yet).

 4) check that the proxy behaviour has not changed in regards to who is
getting to what.
  - if there is a change then your parameters to the DB helper need fixing.
 - otherwise problem stated above is solved and you can move on to other
changes.


Amos


From squid3 at treenet.co.nz  Sat Jan 26 07:21:44 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Jan 2019 20:21:44 +1300
Subject: [squid-users] How to definitively disable IPv6
In-Reply-To: <21317797-8c47-6d29-ee33-e1a28a3150da@treenet.co.nz>
References: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
 <21317797-8c47-6d29-ee33-e1a28a3150da@treenet.co.nz>
Message-ID: <47813f24-e625-218f-df56-cd75abbdb54e@treenet.co.nz>

On 26/01/19 5:00 am, Amos Jeffries wrote:
> On 25/01/19 11:29 pm, Troiano Alessio wrote:
>> Hello,
>>
>> I need to definitively solve the ipv6 (un)reachbility issue.
>>
>> I state I read this topic:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/dns-v4-first-on-ignored-td4658427.html
>> but not found a solution. Amos wrote ?Squid tests for IPv6 ability
>> automatically by opening a socket on a private IP address, if that works
>> the socket options are noted and used.?
>>
>> Anyway I disable IPv6 on my Red Hat 7.4 with the following:
>>
>> net.ipv6.conf.all.disable_ipv6 = 1
>>
>> net.ipv6.conf.default.disable_ipv6 = 1
>>
>> net.ipv6.conf.bond0.disable_ipv6 = 1
>>
>> net.ipv6.conf.lo.disable_ipv6 = 1
>>
> 
> IIRC there are boot options necessary so the machine kernel starts with
> its IPv6 TCP stack disabled.
> 


FWIW; the patch I made to detect and disable IPv6 inside Squid when the
::1 cannot be bound has now been merged and should be in the upcoming
v4.6 release.

If you wish to try it out before then it can be found at
<http://www.squid-cache.org/Versions/v5/changesets/squid-5-4685e7ba556dd81facf98e6a8e5503211cff3f1a.patch>


Amos


From matthias.weigel at maweos.de  Sun Jan 27 10:42:05 2019
From: matthias.weigel at maweos.de (Matthias Weigel)
Date: Sun, 27 Jan 2019 11:42:05 +0100
Subject: [squid-users] Big HTTP-POST file uploads not working
Message-ID: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>


Hi,

i am using Squid as a reverse proxy.
Squid tested is 3.5.20 and 3.5.28 (same result). OS of Squid box is
Centos7 latest.

The backend is some IIS-server with a software "Esko Webcenter" (used in
printing industry).

Everything is working, except large HTTP-POST uploads stall and time
out. Small HTTP-POST uploads work fine.
The amount of "large" or "small" varies and seems to be somehow
dependent on connection speed. For some tests with fast connections, the
limit seems to be around 800 MB, for a slower connection speed more like
50MB. The exact amount of data transfered is not consistent, each test
shows a different value. The software wants to upload up to 3GB.

The upload client is some Javascript/Ajax bloat, that shows a progress
bar. That progess bar stalls at some point and then the Javascript
client shows some generic error message.

Everything works fine without Squid reverse proxy in between.
Everything works fine with Apache as reverse proxy!

In all tests, a simple firewall is in the path, that shows no error. No
IPS, DPI or similar is present. No Selinux on Squid box.

When the upload stalls, i am seeing the receive queue grow on the Squid
box OS:
netstat -anp
Proto Recv-Q Send-Q Local Address           Foreign Address
State       PID/Program name
[...]
tcp6  1443336      0 217.xx.xx.18:2443        77.7.88.123:43252
ESTABLISHED 10512/(squid-1)
[...]

At this point wireshark shows the TCP window at the client shrink to
zero and after some time a tcp teardown.

So squid seems to not read any more from the client tcp stream! Why?


In cache.log i get no error.
In access.log i get the following (each line one test):
1548315375.580 147346 77.7.88.123 TCP_MISS/502 5200 POST
https://webcenter.example.com/WebCenter/asyncupload.wcr?action=DO_UPLOAD
- FIRSTUP_PARENT/10.yy.yy.21 text/html
1548315515.773 140151 77.7.88.123 TCP_MISS/502 5200 POST
https://webcenter.example.com/WebCenter/asyncupload.wcr?action=DO_UPLOAD
- FIRSTUP_PARENT/10.yy.yy.21 text/html
1548316276.926 154505 77.7.88.123 TCP_MISS/502 5200 POST
https://webcenter.example.com/WebCenter/asyncupload.wcr?action=DO_UPLOAD
- FIRSTUP_PARENT/10.yy.yy.21 text/html
1548318410.172 137555 77.7.88.123 TCP_MISS/502 5200 POST
https://webcenter.example.com/WebCenter/asyncupload.wcr?action=DO_UPLOAD
- FIRSTUP_PARENT/10.yy.yy.21 text/html
1548318550.384 140082 77.7.88.123 TCP_MISS/502 5200 POST
https://webcenter.example.com/WebCenter/asyncupload.wcr?action=DO_UPLOAD
- FIRSTUP_PARENT/10.yy.yy.21 text/html
1548352767.439 141549 77.7.88.123 TCP_MISS/502 5122 POST
https://webcenter.example.com/WebCenter/asyncupload.wcr?action=DO_UPLOAD
- FIRSTUP_PARENT/10.yy.yy.21 text/html


Using some of Squids debug function, the HTTP-POST request of a testfile
upload with size 268435456 bytes seems to look like this:
POST /WebCenter/asyncupload.wcr?action=DO_UPLOAD HTTP/1.1^M
Host: webcenter.example.com^M
Connection: keep-alive^M
Content-Length: 268436133^M
Accept: application/json, text/javascript, */*; q=0.01^M
Origin: https://webcenter.example.com^M
X-Requested-With: XMLHttpRequest^M
User-Agent: Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36^M
DNT: 1^M
Content-Type: multipart/form-data;
boundary=----WebKitFormBoundaryoYIJ0e8lVMdsNIh4^M
Referer:
https://webcenter.example.com/WebCenter/projdetailsdocsadddocapplet.jsp?projectID=00002_0000028952&folderID=00002_0000363216&menu_file=addnewdocument^M
Accept-Encoding: gzip, deflate, br^M
Accept-Language: de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7^M
Cookie:
JSESSIONID=3B3B60B9E2EF7F74700AF4877C17F6A9C43E76DEF8F219839EEC3779B310124C15EEE4CF57CB8437E6FD5FFE8D76EF5BE79D2304D1023C4F6A539D0F619A245D;
/WebCenter/projdetails.jsp=; /WebCenter/projdetailsdocs.jsp=;
ASPSESSIONIDQERDBCDC=OPHNNBAADGNBKMBPGGKAFIOI^M


Squid forwards it to the backend server like this:
POST /WebCenter/asyncupload.wcr?action=DO_UPLOAD HTTP/1.1^M
Content-Length: 268436133^M
Accept: application/json, text/javascript, */*; q=0.01^M
Origin: https://webcenter.example.com^M
X-Requested-With: XMLHttpRequest^M
User-Agent: Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36^M
DNT: 1^M
Content-Type: multipart/form-data;
boundary=----WebKitFormBoundaryoYIJ0e8lVMdsNIh4^M
Referer:
https://webcenter.example.com/WebCenter/projdetailsdocsadddocapplet.jsp?projectID=00002_0000028952&folderID=00002_0000363216&menu_file=addnewdocument^M
Accept-Encoding: gzip, deflate, br^M
Accept-Language: de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7^M
Cookie:
JSESSIONID=3B3B60B9E2EF7F74700AF4877C17F6A9C43E76DEF8F219839EEC3779B310124C15EEE4CF57CB8437E6FD5FFE8D76EF5BE79D2304D1023C4F6A539D0F619A245D;
/WebCenter/projdetails.jsp=; /WebCenter/projdetailsdocs.jsp=;
ASPSESSIONIDQERDBCDC=OPHNNBAADGNBKMBPGGKAFIOI^M
Host: webcenter.example.com^M
Via: 1.1 HFKGN062.example.com (squid/3.5.28)^M
Surrogate-Capability: HFKGN062.example.com="Surrogate/1.0 ESI/1.0"^M
X-Forwarded-For: 77.7.88.123^M
Cache-Control: max-age=259200^M
Connection: keep-alive^M
Front-End-Https: On^M
^M

In the debug logs i find the following error message:
X-Squid-Error : ERR_READ_ERROR 104

I tried to change various kernel parameters like tcp_rmem, rmem_max,
etc. Results in no change to the problem.

Any ideas what is causing this?
How can i further track down the problem?
Is anybody on this list able to do HTTP-POST with gigabyte sized files
through squid?

TIA

Matthias


P.S. here is my squid.conf:
acl SSL_ports port 443
acl CONNECT method CONNECT
acl trace method TRACE
acl webcenter dstdomain webcenter.example.com
acl my443 myport 443
acl my2443 myport 2443
http_access deny trace
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow webcenter
http_access allow localhost
http_access deny all
icp_access allow all
http_port 80 accel
https_port  2443 accel vport=443 cert=/etc/squid/certs/webcenter.crt.pem
key=/etc/squid/certs/webcenter.key.pem options=NO_SSLv2,NO_SSLv3
ssl_unclean_shutdown on
cache_peer 10.yy.yy.21 parent 443 0 originserver no-digest
front-end-https=on no-query ssl sslflags=DONT_VERIFY_PEER
ssloptions=NO_SSLv2,NO_SSLv3 login=PASS
cache_peer_access 10.yy.yy.21 allow webcenter
cache_peer_access 10.yy.yy.21 deny all
cache_mem 250 MB
maximum_object_size_in_memory 1 MB
cache_dir ufs /var/spool/squid 10000 48 256
maximum_object_size 40960 KB
access_log /var/log/squid/access.log squid
cache_store_log none
debug_options ALL,1
strip_query_terms off
redirect_program /etc/squid/urlmapper.pl
redirect_children 10
redirect_rewrites_host_header on
acl QUERY urlpath_regex cgi-bin \?
cache deny QUERY
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern .               0       20%     4320
request_header_max_size 200 KB
reply_header_max_size 200 KB
request_body_max_size 3 GB
peer_connect_timeout 10 seconds
request_timeout 5 minutes
persistent_request_timeout 5 minutes
half_closed_clients on
pconn_timeout 10 minute
shutdown_lifetime 60 seconds
cache_mgr hotline at example.com
detect_broken_pconn on
deny_info TCP_RESET trace
always_direct deny all
never_direct allow all
hosts_file /etc/hosts
coredump_dir /var/spool/squid


From rousskov at measurement-factory.com  Sun Jan 27 17:19:54 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 27 Jan 2019 10:19:54 -0700
Subject: [squid-users] Big HTTP-POST file uploads not working
In-Reply-To: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
References: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
Message-ID: <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>

On 1/27/19 3:42 AM, Matthias Weigel wrote:

> large HTTP-POST uploads stall and time out.

> squid seems to not read any more from the client tcp stream! Why?


A Squid developer probably can find the answer to that question by
examining cache.log with debug_options set to ALL,9.


> In the debug logs i find the following error message:
> X-Squid-Error : ERR_READ_ERROR 104

Does ERR_READ_ERROR first appear in cache.log before or after the client
times out?


> How can i further track down the problem?

Post (a link to) compressed cache.log with debug_options set to ALL,9
and reproducing the problem with a single transaction without any other
traffic going through the Squid box.

Alex.


From alessio.troiano at leonardocompany.com  Mon Jan 28 08:19:20 2019
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Mon, 28 Jan 2019 08:19:20 +0000
Subject: [squid-users] R:  How to definitively disable IPv6
In-Reply-To: <21317797-8c47-6d29-ee33-e1a28a3150da@treenet.co.nz>
References: <f4c2f26f7bf045e38a1d973198e7ba2b@ocgepvsw3101.ocr.priv>
 <21317797-8c47-6d29-ee33-e1a28a3150da@treenet.co.nz>
Message-ID: <1dbf2290d8af4590b9dd6431823b0ea9@ocgepvsw3101.ocr.priv>

> > Anyway squid try to connect to the IPv6 address instead of IPv4 and
> > I?m not able to reach it:
> >
> > C:\Users\atroiano>nslookup download.pdfforge.org
> >
> > Server:  espevmdxxxx.xxxx.prv
> >
> > Address:  172.x.x.x
> >
> >
> >
> > Risposta da un server non autorevole:
> >
> > Nome:    download.pdfforge.org
> >
> > Addresses:  2001:4860:4802:38::15
> >
> >           2001:4860:4802:34::15
> >
> >           2001:4860:4802:32::15
> >
> >           2001:4860:4802:36::15
> >
> >           216.239.32.21
> >
> >           216.239.38.21
> >
> >           216.239.36.21
> >
> >           216.239.34.21
> >

> Are any of those IPv4 addresses able to be connected to and fetched from by processes on the Squid machine?

> The squidclient tool can be used to probe individual server/IP for issues fetching requests.


Finally Amos you are right! The IPv4 addresses are blocked by the firewall. It is difficult to understand that, if these connections are not logged. I'll use squidclient and mgr:ipcache for future debugging.
Thank you so much!

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From matthias.weigel at maweos.de  Mon Jan 28 19:50:55 2019
From: matthias.weigel at maweos.de (Matthias Weigel)
Date: Mon, 28 Jan 2019 20:50:55 +0100
Subject: [squid-users] Big HTTP-POST file uploads not working
In-Reply-To: <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>
References: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
 <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>
Message-ID: <244143e6-7bc7-958f-7781-bb610f58bacd@maweos.de>

Hi Alex,

this problem just got a lot weirder:
- Upload works fine with debug_options ALL,7 ALL,8 or ALL,9.
- Upload fails with debug_options ALL,6 and lower.

I created a debug_options ALL,6 cache.log.
http://94.16.117.186/squid/cache.log.3b.gz

During this test, the following happened at the client:

Stall at 20:10:38
Recover at 20:12:47
Stall at 20:14:11
Failure at 20:16:17

Any ideas?

Best Regards

Matthias

Am 27.01.2019 um 18:19 schrieb Alex Rousskov:
> On 1/27/19 3:42 AM, Matthias Weigel wrote:
> 
>> large HTTP-POST uploads stall and time out.
> 
>> squid seems to not read any more from the client tcp stream! Why?
> 
> 
> A Squid developer probably can find the answer to that question by
> examining cache.log with debug_options set to ALL,9.
> 
> 
>> In the debug logs i find the following error message:
>> X-Squid-Error : ERR_READ_ERROR 104
> 
> Does ERR_READ_ERROR first appear in cache.log before or after the client
> times out?
> 
> 
>> How can i further track down the problem?
> 
> Post (a link to) compressed cache.log with debug_options set to ALL,9
> and reproducing the problem with a single transaction without any other
> traffic going through the Squid box.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jan 29 04:01:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Jan 2019 17:01:12 +1300
Subject: [squid-users] Big HTTP-POST file uploads not working
In-Reply-To: <244143e6-7bc7-958f-7781-bb610f58bacd@maweos.de>
References: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
 <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>
 <244143e6-7bc7-958f-7781-bb610f58bacd@maweos.de>
Message-ID: <318ba4ba-7fef-5de3-555f-69375f553151@treenet.co.nz>

On 29/01/19 8:50 am, Matthias Weigel wrote:
> Hi Alex,
> 
> this problem just got a lot weirder:
> - Upload works fine with debug_options ALL,7 ALL,8 or ALL,9.
> - Upload fails with debug_options ALL,6 and lower.
> 
> I created a debug_options ALL,6 cache.log.
> http://94.16.117.186/squid/cache.log.3b.gz
> 
> During this test, the following happened at the client:
> 
> Stall at 20:10:38
> Recover at 20:12:47
> Stall at 20:14:11
> Failure at 20:16:17
> 
> Any ideas?
> 

The FD 18 client->Squid I/O stops happening at 20:10:35.923 with input
buffer full and more data apparently waiting to arrive.

But the FD 20 Squid->server I/O halted earlier at 20:10:35.437 with a
4KB packet being sent and still waiting for the signal to proceed
sending more.

At 20:12:47.068 a signal is received finally, but it indicates the
server connection has gone away with no reason given. Squid generates a
502 error response and delivers that to the client.

The client opens a new connection, does all its TLS handshakes again and
re-sends the same very large POST request. Which encounters the same
sequence of server stops responding, client buffer fills and "stall"
until the server FD is closed. Then again the 502 from Squid but the
client does not resume this time.


So whatever is going on it looks to me like it is the origin server
being a problem. Though you may need to look at the actual TCP packet
flow to confirm. I expect you will find that a packet from Squid to the
server does not get ACK'd, or a network timeout occurs (NAT or TCP
lifetimes are common), ...

 Or maybe the server is itself dealing with a similar backlog from
unresponsive internal agent/component leaving its input buffer to
overflow just like Squids one from the client does.


Amos


From matthias.weigel at maweos.de  Tue Jan 29 07:05:24 2019
From: matthias.weigel at maweos.de (Matthias Weigel)
Date: Tue, 29 Jan 2019 08:05:24 +0100
Subject: [squid-users] Big HTTP-POST file uploads not working
In-Reply-To: <318ba4ba-7fef-5de3-555f-69375f553151@treenet.co.nz>
References: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
 <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>
 <244143e6-7bc7-958f-7781-bb610f58bacd@maweos.de>
 <318ba4ba-7fef-5de3-555f-69375f553151@treenet.co.nz>
Message-ID: <099e6e9c-4813-fd23-c2a7-7fd8487196da@maweos.de>

Hi Amos,

thanks for your analysing!

Any idea, why the IIS server behaves differently with Apache as the
Reverse Proxy (anything else being the same)?

Does Squid do anything special with TCP streams?

Best Regards

Matthias

Am 29.01.2019 um 05:01 schrieb Amos Jeffries:
> On 29/01/19 8:50 am, Matthias Weigel wrote:
>> Hi Alex,
>>
>> this problem just got a lot weirder:
>> - Upload works fine with debug_options ALL,7 ALL,8 or ALL,9.
>> - Upload fails with debug_options ALL,6 and lower.
>>
>> I created a debug_options ALL,6 cache.log.
>> http://94.16.117.186/squid/cache.log.3b.gz
>>
>> During this test, the following happened at the client:
>>
>> Stall at 20:10:38
>> Recover at 20:12:47
>> Stall at 20:14:11
>> Failure at 20:16:17
>>
>> Any ideas?
>>
> 
> The FD 18 client->Squid I/O stops happening at 20:10:35.923 with input
> buffer full and more data apparently waiting to arrive.
> 
> But the FD 20 Squid->server I/O halted earlier at 20:10:35.437 with a
> 4KB packet being sent and still waiting for the signal to proceed
> sending more.
> 
> At 20:12:47.068 a signal is received finally, but it indicates the
> server connection has gone away with no reason given. Squid generates a
> 502 error response and delivers that to the client.
> 
> The client opens a new connection, does all its TLS handshakes again and
> re-sends the same very large POST request. Which encounters the same
> sequence of server stops responding, client buffer fills and "stall"
> until the server FD is closed. Then again the 502 from Squid but the
> client does not resume this time.
> 
> 
> So whatever is going on it looks to me like it is the origin server
> being a problem. Though you may need to look at the actual TCP packet
> flow to confirm. I expect you will find that a packet from Squid to the
> server does not get ACK'd, or a network timeout occurs (NAT or TCP
> lifetimes are common), ...
> 
>  Or maybe the server is itself dealing with a similar backlog from
> unresponsive internal agent/component leaving its input buffer to
> overflow just like Squids one from the client does.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jan 29 10:46:30 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 Jan 2019 23:46:30 +1300
Subject: [squid-users] Big HTTP-POST file uploads not working
In-Reply-To: <099e6e9c-4813-fd23-c2a7-7fd8487196da@maweos.de>
References: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
 <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>
 <244143e6-7bc7-958f-7781-bb610f58bacd@maweos.de>
 <318ba4ba-7fef-5de3-555f-69375f553151@treenet.co.nz>
 <099e6e9c-4813-fd23-c2a7-7fd8487196da@maweos.de>
Message-ID: <3f18517e-b704-aabd-0e66-1186e3883560@treenet.co.nz>

On 29/01/19 8:05 pm, Matthias Weigel wrote:
> Hi Amos,
> 
> thanks for your analysing!
> 
> Any idea, why the IIS server behaves differently with Apache as the
> Reverse Proxy (anything else being the same)?
> 

No ideas, sorry.

> Does Squid do anything special with TCP streams?
> 

Nothing that would really stand out AFAIK. The data is re-packaged into
4096 byte chunks to the server, 4170 bytes after encoding on those
particular streams. Maybe that, but unlikely.

The trace shows a fairly regular flow except at the stall points where
the send-more-data I/O signals just stop arriving.

Amos


From rentorbuy at yahoo.com  Tue Jan 29 10:56:19 2019
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 29 Jan 2019 10:56:19 +0000 (UTC)
Subject: [squid-users] installing Squid: /run dir creation
References: <790823684.173235.1548759379684.ref@mail.yahoo.com>
Message-ID: <790823684.173235.1548759379684@mail.yahoo.com>

Hi,

My Linux distro warns me that when trying to install Squid an attempt is made to write to a "volatile" dir.

The Makefile in the src subdir contains:

??? $(mkinstalldirs) $(DESTDIR)`dirname $(DEFAULT_PID_FILE)`

The default PID file being /run/squid.pid, the above tries to make the /run dir.

Is it necessary to keep this in the Makefile?

Shouldn't the /run/* files be created at runtime anyway?

The /run dir is also created by the OS.

Thanks,

Vieri


From squid3 at treenet.co.nz  Tue Jan 29 12:05:52 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 30 Jan 2019 01:05:52 +1300
Subject: [squid-users] installing Squid: /run dir creation
In-Reply-To: <790823684.173235.1548759379684@mail.yahoo.com>
References: <790823684.173235.1548759379684.ref@mail.yahoo.com>
 <790823684.173235.1548759379684@mail.yahoo.com>
Message-ID: <3fb803b6-b405-a39a-bfc9-a36ae23c7a33@treenet.co.nz>

On 29/01/19 11:56 pm, Vieri wrote:
> Hi,
> 
> My Linux distro warns me that when trying to install Squid an attempt is made to write to a "volatile" dir.
> 
> The Makefile in the src subdir contains:
> 
> ??? $(mkinstalldirs) $(DESTDIR)`dirname $(DEFAULT_PID_FILE)`
> 
> The default PID file being /run/squid.pid, the above tries to make the /run dir.
> 
> Is it necessary to keep this in the Makefile?
> 

Yes. The path is configurable with --with-pidfile=PATH, so it can be
absolutely anywhere.


> Shouldn't the /run/* files be created at runtime anyway?
> 
> The /run dir is also created by the OS.

The Squid default path for this file is $localstatedir/run/squid.pid as
specified by older versions of FHS. It is not clear whether your local
state dir is '/' or the above option is being used to place it exactly.
Normally these would expand to /var/run/squid.pid.

It would help to have a hint about what OS you are using and what
./configure parameters you used.


Please be aware that Makefile are auto-generated. Only a small portion
of their contents is anything we wrote and attempts at patching can be
erased midway through a build by the toolchain detecting files as out of
sync with the actual source.


Amos


From rentorbuy at yahoo.com  Tue Jan 29 12:22:54 2019
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 29 Jan 2019 12:22:54 +0000 (UTC)
Subject: [squid-users] installing Squid: /run dir creation
In-Reply-To: <3fb803b6-b405-a39a-bfc9-a36ae23c7a33@treenet.co.nz>
References: <790823684.173235.1548759379684.ref@mail.yahoo.com>
 <790823684.173235.1548759379684@mail.yahoo.com>
 <3fb803b6-b405-a39a-bfc9-a36ae23c7a33@treenet.co.nz>
Message-ID: <1476053375.198812.1548764574251@mail.yahoo.com>


On Tuesday, January 29, 2019, 1:06:22 PM GMT+1, Amos Jeffries <squid3 at treenet.co.nz> wrote: 
>>
>> Is it necessary to keep this in the Makefile?
>> 
>
> Yes. The path is configurable with --with-pidfile=PATH, so it can be
> absolutely anywhere.
>
> It would help to have a hint about what OS you are using and what
> /configure parameters you used.

I'm using Gentoo and the ebuild (package manager) hardcodes the PID file name when calling the configure script:

--with-pidfile=/run/squid.pid

So if this is the case then maybe it would make sense to remove that mkinstalldirs line in the Makefile, at least only downstream by the Gentoo devs as a patch before configuring/compiling. 
Makefiles might change in the future, but that would be up to the Gentoo devs to update. 

I don't know for sure yet if this is why Gentoo "warns" me that the Squid installation is trying to write to /run, or if there are other parts of the installation code that might do so too.

I'll make a few tests first, but correct me if I'm wrog when I say that if one *always* passes the same PID file path to the configure script then that mkinstalldirs can be safely removed from the Makefile.

Thanks,

Vieri


From rentorbuy at yahoo.com  Tue Jan 29 12:44:49 2019
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 29 Jan 2019 12:44:49 +0000 (UTC)
Subject: [squid-users] installing Squid: /run dir creation
In-Reply-To: <3fb803b6-b405-a39a-bfc9-a36ae23c7a33@treenet.co.nz>
References: <790823684.173235.1548759379684.ref@mail.yahoo.com>
 <790823684.173235.1548759379684@mail.yahoo.com>
 <3fb803b6-b405-a39a-bfc9-a36ae23c7a33@treenet.co.nz>
Message-ID: <50307464.208913.1548765889907@mail.yahoo.com>

I can add the following info to my previous e-mail. Here's the configure command (the pid file name is always the same -- other options may vary according to user preferences or system deps):

$ ./configure --prefix=/usr --build=x86_64-pc-linux-gnu --host=x86_64-pc-linux-gnu --mandir=/usr/share/man --infodir=/usr/share/info --datadir=/usr/share --sysconfdir=/etc --localstatedir=/var/lib --disable-dependency-tracking --disable-silent-rules --docdir=/usr/share/doc/squid-4.5 --htmldir=/usr/share/doc/squid-4.5/html --with-sysroot=/ --libdir=/usr/lib64 --sysconfdir=/etc/squid --libexecdir=/usr/libexec/squid --localstatedir=/var --with-pidfile=/run/squid.pid --datadir=/usr/share/squid --with-logdir=/var/log/squid --with-default-user=squid --enable-removal-policies=lru,heap --enable-storeio=aufs,diskd,rock,ufs --enable-disk-io --enable-auth-basic=NCSA,POP3,getpwnam,SMB,SMB_LM,LDAP,PAM,RADIUS --enable-auth-digest=file,LDAP,eDirectory --enable-auth-ntlm=SMB_LM --enable-auth-negotiate=kerberos,wrapper --enable-external-acl-helpers=file_userip,session,unix_group,delayer,time_quota,wbinfo_group,LDAP_group,eDirectory_userip,kerberos_ldap_group --enable-log-daemon-helpers --enable-url-rewrite-helpers --enable-cache-digests --enable-delay-pools --enable-eui --enable-icmp --enable-follow-x-forwarded-for --with-large-files --with-build-environment=default --disable-strict-error-checking --disable-arch-native --with-included-ltdl=/usr/include --with-ltdl-libdir=/usr/lib64 --with-libcap --enable-ipv6 --disable-snmp --with-openssl --with-nettle --with-gnutls --enable-ssl-crtd --disable-ecap --disable-esi --enable-htcp --enable-wccp --enable-wccpv2 --enable-linux-netfilter --enable-zph-qos --with-netfilter-conntrack --with-mit-krb5 --without-heimdal-krb5



From rousskov at measurement-factory.com  Tue Jan 29 18:11:01 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 29 Jan 2019 11:11:01 -0700
Subject: [squid-users] Big HTTP-POST file uploads not working
In-Reply-To: <099e6e9c-4813-fd23-c2a7-7fd8487196da@maweos.de>
References: <05907edd-92b2-cfca-1b8c-51cbc43afd91@maweos.de>
 <7e754791-dc8a-90c7-ee94-74d46f3f59bc@measurement-factory.com>
 <244143e6-7bc7-958f-7781-bb610f58bacd@maweos.de>
 <318ba4ba-7fef-5de3-555f-69375f553151@treenet.co.nz>
 <099e6e9c-4813-fd23-c2a7-7fd8487196da@maweos.de>
Message-ID: <fa38ccc7-e136-1e31-f020-7d3c8e830181@measurement-factory.com>

On 1/29/19 12:05 AM, Matthias Weigel wrote:

> Any idea, why the IIS server behaves differently with Apache as the
> Reverse Proxy (anything else being the same)?

Given Amos' analysis, if you want to get to the bottom of this problem,
you would need to figure out why the origin server drops the connection.
This may take a while, but is usually possible by enabling origin server
debugging, analyzing packet dumps, comparing working cases with
non-working ones, and/or, in the extreme cases, playing with how Squid
sends data. It may take a pro a few hours or a few days, but it is
usually doable.

One interesting experiment would be to test Squid-Apache-IIS chain.

The analysis can either confirm a Squid bug (something we can fix),
detect a hardware problem (something you can fix), or provide enough
information to develop a workaround for the IIS bug (other than enabling
ALL,9 debugging in Squid :-).

Since ALL,9 debugging has an effect, it is possible that rate-limiting
Squid-IIS and/or client-Squid connection would help work around the problem.

Alex.


> Am 29.01.2019 um 05:01 schrieb Amos Jeffries:
>> On 29/01/19 8:50 am, Matthias Weigel wrote:
>>> Hi Alex,
>>>
>>> this problem just got a lot weirder:
>>> - Upload works fine with debug_options ALL,7 ALL,8 or ALL,9.
>>> - Upload fails with debug_options ALL,6 and lower.
>>>
>>> I created a debug_options ALL,6 cache.log.
>>> http://94.16.117.186/squid/cache.log.3b.gz
>>>
>>> During this test, the following happened at the client:
>>>
>>> Stall at 20:10:38
>>> Recover at 20:12:47
>>> Stall at 20:14:11
>>> Failure at 20:16:17
>>>
>>> Any ideas?
>>>
>>
>> The FD 18 client->Squid I/O stops happening at 20:10:35.923 with input
>> buffer full and more data apparently waiting to arrive.
>>
>> But the FD 20 Squid->server I/O halted earlier at 20:10:35.437 with a
>> 4KB packet being sent and still waiting for the signal to proceed
>> sending more.
>>
>> At 20:12:47.068 a signal is received finally, but it indicates the
>> server connection has gone away with no reason given. Squid generates a
>> 502 error response and delivers that to the client.
>>
>> The client opens a new connection, does all its TLS handshakes again and
>> re-sends the same very large POST request. Which encounters the same
>> sequence of server stops responding, client buffer fills and "stall"
>> until the server FD is closed. Then again the 502 from Squid but the
>> client does not resume this time.
>>
>>
>> So whatever is going on it looks to me like it is the origin server
>> being a problem. Though you may need to look at the actual TCP packet
>> flow to confirm. I expect you will find that a packet from Squid to the
>> server does not get ACK'd, or a network timeout occurs (NAT or TCP
>> lifetimes are common), ...
>>
>>  Or maybe the server is itself dealing with a similar backlog from
>> unresponsive internal agent/component leaving its input buffer to
>> overflow just like Squids one from the client does.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From bandeep2000 at gmail.com  Wed Jan 30 07:11:08 2019
From: bandeep2000 at gmail.com (bandeep2000)
Date: Tue, 29 Jan 2019 23:11:08 -0800
Subject: [squid-users] Squid not coming up with dynamic host certificate on
	ssl bum
Message-ID: <CAL0CaX_aZ+QSZV6OO9kUwFY8cKsBRL5ESUbbyFTCpO=t5W5gNw@mail.gmail.com>

Have squid in transparent, want to ssl bump all the connections which are
not whitelisted, but when given *generate-host-certificates=on , *squid
keeps crashing when trying to bring it up after service restart.



*/var/log/messages*

Jan 30 07:05:52 ban-squid-proxy22 squid[23323]: Squid Parent: (squid-1)
process 23441 started

Jan 30 07:05:52 ban-squid-proxy22 (squid-1): The ssl_crtd helpers are
crashing too rapidly, need help!

Jan 30 07:05:52 ban-squid-proxy22 squid[23323]: Squid Parent: (squid-1)
process 23441 exited with status 1

Jan 30 07:05:52 ban-squid-proxy22 squid[23397]: Squid Parent: (squid-1)
process 23449 started

Jan 30 07:05:52 ban-squid-proxy22 (squid-1): The ssl_crtd helpers are
crashing too rapidly, need help!

Jan 30 07:05:52 ban-squid-proxy22 squid[23397]: Squid Parent: (squid-1)
process 23449 exited with status 1


*squid.conf details:*

visible_hostname squid


cache deny all

#Handling HTTP requests

http_port 3128 intercept

acl allowed_http_sites dstdomain .amazonaws.com .bbc.com

acl blacklist url_regex -i /.(.*?)

#acl allowed_http_sites dstdomain [you can add other domains to permit]

http_access allow allowed_http_sites

http_access deny blacklist


#Handling HTTPS requests

#https_port 3130 cert=/etc/pki/tls/certs/squidCA.pem ssl-bump intercept

#/root/openssl/squid.crt  squid.csr  /root/openssl/squid.key

*https_port 3130 cert=/root/openssl/squid.crt key=/root/openssl/squid.key
ssl-bump intercept generate-host-certificates=on version=1
options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE*

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB


acl SSL_port port 443

http_access allow SSL_port

acl allowed_https_sites ssl::server_name .amazonaws.com .cnn.com .yahoo.com
.bbc.com


acl step1 at_step SslBump1

acl step2 at_step SslBump2

acl step3 at_step SslBump3


ssl_bump peek step1 all

#ssl_bump peek all

ssl_bump splice step2 allowed_https_sites

ssl_bump splice step3 allowed_https_sites

ssl_bump bump step2 all




http_access deny all


coredump_dir /var/cache/squid



*Command to generate SSL certificate:*

sudo openssl genrsa -out squid.key 2048
sudo openssl req -new -key squid.key -out squid.csr -subj
"/C=XX/ST=XX/L=squid/O=squid/CN=squid"
sudo openssl x509 -req -days 3650 -in squid.csr -signkey squid.key -out
squid.crt

*Squid and OS version:*

squid -v

Squid Cache: Version 3.5.28

Service Name: squid


This binary uses OpenSSL 1.0.1e-fips 11 Feb 2013. For legal restrictions on
distribution see https://www.openssl.org/source/license.html


configure options:  '--prefix=/usr' '--includedir=/usr/include'
'--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--localstatedir=/var' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-openssl' '--enable-ssl-crtd'
--enable-ltdl-convenience

[c5278791 at ban-squid-proxy22 ~]$ cat /etc/redhat-release

CentOS release 6.10 (Final)

[c5278791 at ban-squid-proxy22 ~]$


Please let me know.

Thanks!
-Bandeep
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190129/239c646c/attachment.htm>

From grafhuy at yahoo.fr  Wed Jan 30 08:38:23 2019
From: grafhuy at yahoo.fr (graf huy)
Date: Wed, 30 Jan 2019 08:38:23 +0000 (UTC)
Subject: [squid-users] using clang to compile squid 4-5
In-Reply-To: <e5bad83a-336e-e0d7-1aeb-d3fba03a195f@treenet.co.nz>
References: <99079882.2816932.1548163302767.ref@mail.yahoo.com>
 <99079882.2816932.1548163302767@mail.yahoo.com>
 <b059395b-3301-1e0a-abca-950e5379d8f0@measurement-factory.com>
 <e5bad83a-336e-e0d7-1aeb-d3fba03a195f@treenet.co.nz>
Message-ID: <289180401.107999.1548837503168@mail.yahoo.com>

 Hi,
Using update-alternatives to set clang++ was a success, and export CC=/usr/bin/clang does the job (trying two differents methods). Debian project recommended to use export (CC,CCX) for versioning and only update-alternatives for alternative as it says.Anyway compilation with clang was as quick as gcc can do.
/usr/local/squid/sbin# ./squid -v
Squid Cache: Version 4.5
Service Name: squid

This binary uses OpenSSL 1.1.1a? 20 Nov 2018. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:? '--enable-ssl' '--enable-ssl-crtd' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,SMB_LM' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-security-cert-validators=fake' '--enable-storeid-rewrite-helpers=file' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-gnutls' '--with-openssl' 'CC=/usr/bin/clang'



>Seconded. With both my Squid Project and Debian pkg-squid Team hat's on
>I'd like to know why you feel any need to force the compiler?
There were errors to patch with memcpy because of the gcc version? 8.2.0-14 Debian wich considers some warnings as errors, besides some targets are BSD's.
smblib-util.c: In function ?SMB_Negotiate?:
smblib-util.c:404:9: error: ?strncpy? output may be truncated copying 79 bytes from a string of length 79 [-Werror=stringop-truncation]
???????? strncpy(p, Con_Handle -> Svr_PDom, sizeof(Con_Handle -> Svr_PDom) - 1);
???????? ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
smblib-util.c:427:9: error: ?strncpy? output may be truncated copying 79 bytes from a string of length 79 [-Werror=stringop-truncation]
???????? strncpy(p, Con_Handle -> Svr_PDom, sizeof(Con_Handle -> Svr_PDom) - 1);
???????? ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
smblib-util.c: In function ?SMB_TreeConnect?:
smblib-util.c:541:5: error: ?strncpy? specified bound 129 equals destination size [-Werror=stringop-truncation]
???? strncpy(tree -> path, path, sizeof(tree -> path));
???? ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
smblib-util.c:542:5: error: ?strncpy? specified bound 20 equals destination size [-Werror=stringop-truncation]
???? strncpy(tree -> device_type, device, sizeof(tree -> device_type));
???? ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Although my heart bleeds to use a compiler not compliant with GPL on Debian.
Great to known you are on this package, I thought there was only Luigi Gangitano (luigi at debian.org) for ages...

Best regards.


    Le mercredi 23 janvier 2019 ? 05:25:04 UTC+1, Amos Jeffries <squid3 at treenet.co.nz> a ?crit :  
 
 On 23/01/19 5:17 am, Alex Rousskov wrote:
> On 1/22/19 6:21 AM, graf huy wrote:
> 
>> The Makefile is modified so each line with gcc is replaced with clang
>> and each line of g++ replaced with clang++. But gcc is still used.
> 
> I am not sure you are doing that,

Seconded. With both my Squid Project and Debian pkg-squid Team hat's on
I'd like to know why you feel any need to force the compiler?


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190130/832d0ef8/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 30 13:04:52 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Jan 2019 02:04:52 +1300
Subject: [squid-users] Squid not coming up with dynamic host certificate
 on ssl bum
In-Reply-To: <CAL0CaX_aZ+QSZV6OO9kUwFY8cKsBRL5ESUbbyFTCpO=t5W5gNw@mail.gmail.com>
References: <CAL0CaX_aZ+QSZV6OO9kUwFY8cKsBRL5ESUbbyFTCpO=t5W5gNw@mail.gmail.com>
Message-ID: <2733a0ec-c7c4-ac6d-51a9-bd072a9e72ff@treenet.co.nz>

On 30/01/19 8:11 pm, bandeep2000 wrote:
> Have squid in transparent, want to ssl bump all the connections which
> are not whitelisted, but when given?*generate-host-certificates=on ,
> *squid keeps crashing when trying to bring it up after service restart.
> 
> 
> 
> */var/log/messages*
> 
> Jan 30 07:05:52 ban-squid-proxy22 squid[23323]: Squid Parent: (squid-1)
> process 23441 started
> 
> Jan 30 07:05:52 ban-squid-proxy22 (squid-1): The ssl_crtd helpers are
> crashing too rapidly, need help!
> 

There is the error. cache.log should contain more details and possibly
instructions about the error.

Probably you did not initialize the certificate database or it needs to
be purged and reinitialized.




> 
> *squid.conf details:*
> *
> *
> 
> visible_hostname squid
> 


This name will clash with any other proxy calling itself "squid" and the
traffic may/will be rejected as forwarding loop.

Using the full hostname or FQDN is best to ensure the URLs of objects
provided direct from Squid to clients can be fetched by those clients.


> 
> cache deny all
> 
> #Handling HTTP requests
> 
> http_port 3128 intercept
> 
> acl allowed_http_sites dstdomain ...
> acl blacklist url_regex -i /.(.*?)

The above is equivalent to:

  acl blacklist url_regex /.

Meaning "blacklist" matches any URI containing a '/' character followed
by one other character...

> 
> 
> http_access allow allowed_http_sites
> 
> http_access deny blacklist
> 


... all URLs start with "scheme://" therefore the first '/' always
exists and is always followed by the second '/'.

... So any traffic with a URL is blacklisted.

The only traffic allowed is that on the whitelist or with URI or URN -
the latter two do not require the '//' scheme delimiters. So they
usually will not match, but may do so.


> 
> #Handling HTTPS requests
> 
> #https_port 3130 cert=/etc/pki/tls/certs/squidCA.pem ssl-bump intercept
> 
> #/root/openssl/squid.crt? squid.csr? /root/openssl/squid.key
> 
> *https_port 3130 cert=/root/openssl/squid.crt
> key=/root/openssl/squid.key ssl-bump intercept
> generate-host-certificates=on version=1
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE*
> 

Please put the traffic mode ("intercept" for these) as the first option
after the port number. The Squid "-k parse" checks can only verify
correct operation and help instructions if the mode is known when the
other options are interpreted.



> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> 
> 
> acl SSL_port port 443
> 
> http_access allow SSL_port
> 

So any attempt to open opaque tunnels (uses a URI not a URL) to port 443
to any domain is allowed by any client who can get TCP connections to
reach your proxy port 3128.
 Also to any server in the allowed_https_sites whitelist regardless of
whether the client is your LAN or an external attacker.

(NP: there are good reasons we recommend the default !Safe_ports and
"CONNECT !SSL_ports" ACL checks as to be used firs and your rules
second. Mostly it is about protecting your network from abusers.)


> acl allowed_https_sites ssl::server_name ...
> 
> 
> acl step1 at_step SslBump1
> 
> acl step2 at_step SslBump2
> 
> acl step3 at_step SslBump3
> 
> 
> ssl_bump peek step1 all
> 
> #ssl_bump peek all
> 
> ssl_bump splice step2 allowed_https_sites
> 
> ssl_bump splice step3 allowed_https_sites
> 

No traffic should ever reach the step3. Since step2 always finishes with
the above splice or the below bump actions. There are no other
possibilities at step2 which would ever lead to step3 (server
credentials) being checked.


> ssl_bump bump step2 all
> 

Note: the "all" ACL is always pointless on ssl_bump lines and seems to
often confuse people into thinking that a line matches all traffic (it
does nothing). I recommend removing those and re-checking the rules
against your understanding of what your policy needs to make happen.


Amos


From squid3 at treenet.co.nz  Wed Jan 30 13:07:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Jan 2019 02:07:43 +1300
Subject: [squid-users] using clang to compile squid 4-5
In-Reply-To: <289180401.107999.1548837503168@mail.yahoo.com>
References: <99079882.2816932.1548163302767.ref@mail.yahoo.com>
 <99079882.2816932.1548163302767@mail.yahoo.com>
 <b059395b-3301-1e0a-abca-950e5379d8f0@measurement-factory.com>
 <e5bad83a-336e-e0d7-1aeb-d3fba03a195f@treenet.co.nz>
 <289180401.107999.1548837503168@mail.yahoo.com>
Message-ID: <810c689d-7d83-f916-085d-0f881cc4d6c7@treenet.co.nz>

On 30/01/19 9:38 pm, graf huy wrote:
> Hi,
> 
> Using update-alternatives to set clang++ was a success, and export
> CC=/usr/bin/clang does the job (trying two differents methods). Debian
> project recommended to use export (CC,CCX) for versioning and only
> update-alternatives for alternative as it says.
> Anyway compilation with clang was as quick as gcc can do.
> 

Great to know. Nothing to fix on that front for us then.

> 
>>Seconded. With both my Squid Project and Debian pkg-squid Team hat's on
>>I'd like to know why you feel any need to force the compiler?
> 
> There were errors to patch with memcpy because of the gcc version?
> 8.2.0-14 Debian wich considers some warnings as errors, besides some
> targets are BSD's.
> 

I'm not sure I follow that. Are you building with patched GCC? or a
patched libc? or something else?

I build with the Debian Sid GCC version myself and have not seen these
in any test builds with that or similar GCC versions. Am trying your
build options now just to be sure.


On the matter of BSD's, since they provide clang as the default compiler
they (FreeBSD at least) build with clan/clang++ without any need for
CC/CXX setting. We have Jenkins tests for that.


> 
> Great to known you are on this package, I thought there was only Luigi
> Gangitano (luigi at debian.org) for ages...
> 

Welcome. There are several of us, with Luigi as team lead.

Amos


From rentorbuy at yahoo.com  Wed Jan 30 14:10:44 2019
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 30 Jan 2019 14:10:44 +0000 (UTC)
Subject: [squid-users] daily releases
References: <1996981566.161146.1548857444728.ref@mail.yahoo.com>
Message-ID: <1996981566.161146.1548857444728@mail.yahoo.com>

Hi,

Does anyone know of a convenient one-liner to get the latest daily release tarball, eg. http://www.squid-cache.org/Versions/v4/squid-4.5-20190128-r568e66b7c.tar.gz, without having to search for it manually on the web?

Either that or a symlink that would always point to the "latest daily".

Thanks,

Vieri

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190130/3b6412e3/attachment.htm>

From belle at bazuin.nl  Wed Jan 30 14:22:41 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 30 Jan 2019 15:22:41 +0100
Subject: [squid-users] daily releases
In-Reply-To: <1996981566.161146.1548857444728@mail.yahoo.com>
References: <1996981566.161146.1548857444728@mail.yahoo.com>
Message-ID: <vmime.5c51b331.56dc.23f5005729e94fc1@ms249-lin-003.rotterdam.bazuin.nl>

Hai,?
?
I?use this for the latest 4.xx release
mkdir squid && curl -q -L $(lynx -listonly -nonumbers -dump http://www.squid-cache.org/Versions/v4/? | grep squid-4.5 | grep ".tar.gz") | tar -xz -C squid? --strip-components 1 -f -? 
?
and this one for the daily
mkdir squid-$(date +%F) && curl -q -L $(lynx -listonly -nonumbers -dump http://www.squid-cache.org/Versions/v4/? | grep "squid-4.5-" | grep ".tar.gz") | tar -xz -C squid-$(date +%F)? --strip-components 1 -f -
?
maybe is possible in another way but this works for me. 
?
greetz, 
?
Louis
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Vieri
Verzonden: woensdag 30 januari 2019 15:11
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] daily releases



Hi,

Does anyone know of a convenient one-liner to get the latest daily release tarball, eg. http://www.squid-cache.org/Versions/v4/squid-4.5-20190128-r568e66b7c.tar.gz, without having to search for it manually on the web?

Either that or a symlink that would always point to the "latest daily".

Thanks,

Vieri










-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190130/dc3ddc00/attachment.htm>

From belle at bazuin.nl  Wed Jan 30 16:12:14 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 30 Jan 2019 17:12:14 +0100
Subject: [squid-users] using clang to compile squid 4-5
In-Reply-To: <810c689d-7d83-f916-085d-0f881cc4d6c7@treenet.co.nz>
References: <289180401.107999.1548837503168@mail.yahoo.com>
Message-ID: <vmime.5c51ccde.3cc6.66e612638aa7be@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

Good to hear there are more then Luigi :-) 

I builded debian packages yesterday for squid 4.5 
Which was pretty simple and worked fine in the end. 

Get the source of 4.4  ( apt-get source -t unstable squid  )
Copy the debian folder from 4.4 into the 4.5 folder.

And changed in the changelog the squid version, builded fine. 
Test build failed, my change was. 

diff squid-4.4/debian/rules squid-4.5/debian/rules
22c22
< DEB_INSTALL_DOCS_squid-common := debian/copyright CONTRIBUTORS CREDITS QUICKSTART RELEASENOTES.html SPONSORS
---
> DEB_INSTALL_DOCS_squid-common := debian/copyright CONTRIBUTORS CREDITS QUICKSTART SPONSORS

4.5 was missing : RELEASENOTES.html 
Uhm must say, i builded the "squid-4.5-20190128-r568e66b7c" version. 

Working now on a backport for debian stretch. 

Thank for all the good work Amos. ( and Luigi ) ! 

@Amos, any chance to upload 4.5 to unstable or experimental? 
I was hoping that 4.5 would get into buster, im guessing thats not going to happen? 


Greetz, 

Louis





> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Amos Jeffries
> Verzonden: woensdag 30 januari 2019 14:08
> Aan: graf huy; squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] using clang to compile squid 4-5
> 
> On 30/01/19 9:38 pm, graf huy wrote:
> > Hi,
> > 
> > Using update-alternatives to set clang++ was a success, and export
> > CC=/usr/bin/clang does the job (trying two differents 
> methods). Debian
> > project recommended to use export (CC,CCX) for versioning and only
> > update-alternatives for alternative as it says.
> > Anyway compilation with clang was as quick as gcc can do.
> > 
> 
> Great to know. Nothing to fix on that front for us then.
> 
> > 
> >>Seconded. With both my Squid Project and Debian pkg-squid 
> Team hat's on
> >>I'd like to know why you feel any need to force the compiler?
> > 
> > There were errors to patch with memcpy because of the gcc version?
> > 8.2.0-14 Debian wich considers some warnings as errors, besides some
> > targets are BSD's.
> > 
> 
> I'm not sure I follow that. Are you building with patched GCC? or a
> patched libc? or something else?
> 
> I build with the Debian Sid GCC version myself and have not seen these
> in any test builds with that or similar GCC versions. Am trying your
> build options now just to be sure.
> 
> 
> On the matter of BSD's, since they provide clang as the 
> default compiler
> they (FreeBSD at least) build with clan/clang++ without any need for
> CC/CXX setting. We have Jenkins tests for that.
> 
> 
> > 
> > Great to known you are on this package, I thought there was 
> only Luigi
> > Gangitano (luigi at debian.org) for ages...
> > 
> 
> Welcome. There are several of us, with Luigi as team lead.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From bernsen at gmail.com  Wed Jan 30 18:47:10 2019
From: bernsen at gmail.com (Bill Bernsen)
Date: Wed, 30 Jan 2019 13:47:10 -0500
Subject: [squid-users] Using a static wildcard certificate with ssl-bump
 in explicit forward proxy mode
In-Reply-To: <884b51e1-2876-d642-1420-4ef0b87b09bf@treenet.co.nz>
References: <CAEPopkVwgFj=xbX4Pz9ohAiK8APq9K4imrVN-Ba7J9FaF6tUMA@mail.gmail.com>
 <884b51e1-2876-d642-1420-4ef0b87b09bf@treenet.co.nz>
Message-ID: <CAEPopkVVsRtcCNnHgGvSYTTbOX6=AxQsNpie4pMR6xoc-q8feg@mail.gmail.com>

Amos, thank you for the quick response. My original question could use an
example to clarify.

client ------> example.com (HTTPS squid proxy) ------> instance.example.com
(HTTPS server)

The HTTPS squid proxy on example.com has a trusted wildcard certificate for
*.example.com
The HTTPS server on instance.example.com has an untrusted certificate for
instance.example.com

So without MITM, the client issues a CONNECT to squid running on example.com
which does its TLS, authenticates, connects to upstream then goes into
tunneling mode. The client does the TLS handshake with instance.example.com,
receives its untrusted certificate, and isn't happy.

I'm looking for a MITM mode that, instead of requiring a CA that can
dynamically create trusted certs on the fly, will return a wildcard
certificate for all requests (or even better, for any requests matching
hosts in its subdomain). Is that something that exists?

I hacked up my own version of ssl_crtd to serve a static cert and ran into
another wrinkle. Is there a version of squid that supports ssl-bump with
https_port?

On Fri, Jan 25, 2019 at 9:42 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 26/01/19 5:51 am, Bill Bernsen wrote:
> > Hi,
> >
> > I have squid running as an explicit forward proxy on the
> > host example.com <http://example.com/> controlling access to all hosts
> > in *.example.com <http://example.com/>. All the hosts in *.example.com
> > <http://example.com/> have self-signed certificates that I want to
> > appear as trusted to user browsers. I don't have the option of obtaining
> > a trusted CA. I do, however, have a trusted wildcard certificate for
> > *.example.com <http://example.com/> available. Is there a way that I can
> > tell squid to present this static wildcard certificate to clients in
> > lieu of all upstream server certificates?
>
>
> As a forward proxy clients are *not* connecting to any of the
> *.example.com domains. They are connecting to your proxy hostname - and
> telling it to take care of the origin connections. So all clients need
> is trust for the CA which signed the proxy's certificate.
>
> The proxy is the only agent in the path which needs to trust the
> wildcard *.example.com certificate.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190130/634b9ad1/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 30 20:02:59 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Jan 2019 09:02:59 +1300
Subject: [squid-users] daily releases
In-Reply-To: <1996981566.161146.1548857444728@mail.yahoo.com>
References: <1996981566.161146.1548857444728.ref@mail.yahoo.com>
 <1996981566.161146.1548857444728@mail.yahoo.com>
Message-ID: <53ed2fb4-3803-f4eb-29cb-ce671d743ce4@treenet.co.nz>

On 31/01/19 3:10 am, Vieri wrote:
> Hi,
> 
> Does anyone know of a convenient one-liner to get the latest daily
> release tarball, eg.
> http://www.squid-cache.org/Versions/v4/squid-4.5-20190128-r568e66b7c.tar.gz,
> without having to search for it manually on the web?
> 

The contents of the tarball are provided by rsync to optimize update
bandwidth:

<https://wiki.squid-cache.org/DeveloperResources#Bootstrapped_sources_via_rsync>


Amos


From squid3 at treenet.co.nz  Wed Jan 30 20:22:41 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Jan 2019 09:22:41 +1300
Subject: [squid-users] using clang to compile squid 4-5
In-Reply-To: <vmime.5c51ccde.3cc6.66e612638aa7be@ms249-lin-003.rotterdam.bazuin.nl>
References: <289180401.107999.1548837503168@mail.yahoo.com>
 <vmime.5c51ccde.3cc6.66e612638aa7be@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <532bd24f-f92e-a3ad-2aee-c628c2a0c894@treenet.co.nz>

On 31/01/19 5:12 am, L.P.H. van Belle wrote:
> Hai, 
> 
> Good to hear there are more then Luigi :-) 
> 
> I builded debian packages yesterday for squid 4.5 
> Which was pretty simple and worked fine in the end. 
> 
> Get the source of 4.4  ( apt-get source -t unstable squid  )
> Copy the debian folder from 4.4 into the 4.5 folder.
> 
> And changed in the changelog the squid version, builded fine. 
> Test build failed, my change was. 
> 
> diff squid-4.4/debian/rules squid-4.5/debian/rules
> 22c22
> < DEB_INSTALL_DOCS_squid-common := debian/copyright CONTRIBUTORS CREDITS QUICKSTART RELEASENOTES.html SPONSORS
> ---
>> DEB_INSTALL_DOCS_squid-common := debian/copyright CONTRIBUTORS CREDITS QUICKSTART SPONSORS
> 
> 4.5 was missing : RELEASENOTES.html 
> Uhm must say, i builded the "squid-4.5-20190128-r568e66b7c" version. 

Aye, looking into that is on my worklist for this weekend while waiting
for the release prep testing.


> 
> Working now on a backport for debian stretch. 
> 
> Thank for all the good work Amos. ( and Luigi ) ! 
> 
> @Amos, any chance to upload 4.5 to unstable or experimental? 
> I was hoping that 4.5 would get into buster, im guessing thats not going to happen? 
> 

Um, that will depend on Luigi's time between now and then. We have until
the 10th to roll and test it.

I have updated the version in the team repository (pushed just now) but
the changes are relatively minor and 4.6 is due this weekend. If I can
replicate this strncpy issue in the next day a temporary fix for should
also be added before next upload.

Amos


From squid3 at treenet.co.nz  Thu Jan 31 01:14:16 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Jan 2019 14:14:16 +1300
Subject: [squid-users] Using a static wildcard certificate with ssl-bump
 in explicit forward proxy mode
In-Reply-To: <CAEPopkVVsRtcCNnHgGvSYTTbOX6=AxQsNpie4pMR6xoc-q8feg@mail.gmail.com>
References: <CAEPopkVwgFj=xbX4Pz9ohAiK8APq9K4imrVN-Ba7J9FaF6tUMA@mail.gmail.com>
 <884b51e1-2876-d642-1420-4ef0b87b09bf@treenet.co.nz>
 <CAEPopkVVsRtcCNnHgGvSYTTbOX6=AxQsNpie4pMR6xoc-q8feg@mail.gmail.com>
Message-ID: <a42f71f6-2d59-5cf4-aeed-59923fc135cc@treenet.co.nz>

On 31/01/19 7:47 am, Bill Bernsen wrote:
> Amos, thank you for the quick response. My original question could use
> an example to clarify.
> 
> client ------> example.com (HTTPS squid proxy)
> ------> instance.example.com (HTTPS server)
> 

Sadly this does not by itself actually clarify the issue. Are those
domains the machine hostnames or the HTTP(S) message URL domains?
 (The description later does that, so this comment is more an FYI.)


> The HTTPS squid proxy on example.com has a trusted

... definition of that "on" and thus its implications for traffic syntax
and the resulting behaviour limitations is the key point(s) I am trying
to understand here.


> wildcard certificate for *.example.com
> The HTTPS server on instance.example.com 
> has an untrusted certificate for instance.example.com
> 
> So without MITM, the client issues a CONNECT to squid running on
> example.com <http://example.com> which does its TLS, authenticates,
> connects to upstream then goes into tunneling mode. The client does the
> TLS handshake with instance.example.com <http://instance.example.com>,
> receives its untrusted certificate, and isn't happy.


In this case the client is fully aware that the proxy exists.

=> The proxy did *not* get a request to contact instance.example.com -
therefore it did not connect to instance.example.com.

==> The proxy was asked for a tunnel to "example.com" and all security
validation done by the proxy will be comparing the *exact* FQDN
"example.com" against values in that traffic.

The client *separately* (inside the tunneled opaque bytes) contacts the
server and negotiates use of the "instance.example.com" virtual host.

=> The proxy has zero involvement and zero knowledge of this.

==> all security validation done by the client itself and will be
comparing the negotiated FQDN "instance.example.com" against values in
that traffic.

PS. Note there are no wildcards other than the cert field(s). The things
tested against that wildcard has always an exact actual value.


> 
> I'm looking for a MITM mode that, instead of requiring a CA that can
> dynamically create trusted certs on the fly, will return a wildcard
> certificate for all requests (or even better, for any requests matching
> hosts in its subdomain). Is that something that exists?

MITM mode the client does not connect to the proxy. The client connects
to an origin.

 The network NAT or whatever doing the *intercept* part is delivering
the traffic to the proxy.

 Squid is _itself_ generating the CONNECT which shows up (to simplify
processing and failure handling). What data is available determines what
can be done:

SSL-Bump step-1 : only TCP SYN packet details are available.
   ==> raw-IP:port and nothing else.

SSL-Bump step-2 : TLS SNI (maybe)
   ==> exact FQDN value "instance.example.com"

Notice that at no point yet is the MITM ever aware that "example.com"
plays a part and no reason to even suspect a wildcard existence.

At step-3 things get a little bit easier since the server X.509
certificate is available. That cert is the only place the wildcard can
come from, and is in a certificate field where free-form text is allowed
and *often* used. One sub-domain may also sit on a separate server not
using this same cert with wildcard. So care needs to be taken to avoid
issues from that data source.


> 
> I hacked up my own version of ssl_crtd to serve a static cert and ran

Fine. Though you should be able to use generate-host-certificates=off
and place your static cert into the cert= parameter. If that is not
working it is a Squid bug to be fixed.

I have PRs already underway to make the generate option take the CA cert
and leave cert= for the static certs like your case needs. No ETA on
when that will be completed though.


> into another wrinkle. Is there a version of squid that supports ssl-bump
> with https_port?

All versions of Squid-3.2 and later support SSL-Bump on https_port.

Though since forward-proxy and MITM have mutually exclusive X.509
certificate requirements currently the only way to do SSL-Bump on
https_port is to use "intercept" or "tproxy" traffic modes - leaving
explicit TLS proxy with the forward-proxy mode.


Amos

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190131/384c4d85/attachment.sig>

From Ralf.Hildebrandt at charite.de  Thu Jan 31 09:21:23 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 31 Jan 2019 10:21:23 +0100
Subject: [squid-users] [ext] Re:  squid hanging in 100% steal
In-Reply-To: <22a7ce79-3ca8-5f01-8d4d-3e2784c4db2a@treenet.co.nz>
References: <CAPxJK5Dem9Xt6fwOyg2HjZJn+Zh8raUPEZyceGAhOmWsXWZ5zA@mail.gmail.com>
 <22a7ce79-3ca8-5f01-8d4d-3e2784c4db2a@treenet.co.nz>
Message-ID: <20190131092121.gxnihcl6iz4jucyq@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:
> On 25/01/19 1:24 am, Marc wrote:
> > Hi,
> > 
> > For some reason my squid sometimes hangs (after weeks of running
> > smoothly) in 100% steal, until I kill the proces and restart it, after
> > which the proces will again run stable for weeks.
> 
> What does "100% steal" mean?

http://blog.scoutapp.com/articles/2013/07/25/understanding-cpu-steal-time-when-should-you-be-worried

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From rentorbuy at yahoo.com  Thu Jan 31 12:20:24 2019
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 31 Jan 2019 12:20:24 +0000 (UTC)
Subject: [squid-users] daily releases
In-Reply-To: <53ed2fb4-3803-f4eb-29cb-ce671d743ce4@treenet.co.nz>
References: <1996981566.161146.1548857444728.ref@mail.yahoo.com>
 <1996981566.161146.1548857444728@mail.yahoo.com>
 <53ed2fb4-3803-f4eb-29cb-ce671d743ce4@treenet.co.nz>
Message-ID: <2105370186.832747.1548937224021@mail.yahoo.com>

 
On Wednesday, January 30, 2019, 9:12:51 PM GMT+1, Amos Jeffries <squid3 at treenet.co.nz> wrote: 
>> Does anyone know of a convenient one-liner to get the latest daily
>> release tarball, eg.
>> http://www.squid-cache.org/Versions/v4/squid-4.5-20190128-r568e66b7c.tar.gz,
>> without having to search for it manually on the web?
>
> The contents of the tarball are provided by rsync to optimize update
> bandwidth:
> 
> <https://wiki.squid-cache.org/DeveloperResources#Bootstrapped_sources_via_rsync>

rsync allows to sync the latest source for a particular main version (eg. Squid 4 or Squid 5).
However, it does not allow to pull in squid v. 4's source code published on Jan 28th 2019 just like I would get by downloading squid-4.5-20190128-r568e66b7c.tar.gz.
Furthermore, I'm guessing that the "daily" tarballs that are published on the web site's download page are hand-picked because they are known to solve bugs, and are considered to be somewhat "stable". For instance, if I were to rsync today would I get the same code as that of the above mentioned tarball?

Another simple solution would be to be able to list the files in the /Versions/v4/ directory, but it is not allowed by the server.

Vieri


From genesta.sebastien at gmail.com  Thu Jan 31 17:13:38 2019
From: genesta.sebastien at gmail.com (=?UTF-8?Q?S=C3=A9bastien_Genesta?=)
Date: Thu, 31 Jan 2019 18:13:38 +0100
Subject: [squid-users] Squid - SSL-tunnel-error in Google Chrome
Message-ID: <CAEbB6eG-4QH05vuNO0qZ4fDxF_YzhdMRtK5j+Rckrtvneg3Kug@mail.gmail.com>

Hi,

We are encountering strange behaviour using squid proxy has a
non-transparent proxy.

We're randomly encountering "ERR_TUNNEL_CONNECTION_FAILED".

When this error occurs, just wait 1 minutes and the site we tried to reach
become accessible.

I've read that this issue is often due to a misconfiguration of the proxy
server.

Do you see what kind of option could lead to this behaviour?

Thanks for your help.

Seb.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190131/9a968f09/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Jan 31 17:20:52 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 31 Jan 2019 18:20:52 +0100
Subject: [squid-users] Squid - SSL-tunnel-error in Google Chrome
In-Reply-To: <CAEbB6eG-4QH05vuNO0qZ4fDxF_YzhdMRtK5j+Rckrtvneg3Kug@mail.gmail.com>
References: <CAEbB6eG-4QH05vuNO0qZ4fDxF_YzhdMRtK5j+Rckrtvneg3Kug@mail.gmail.com>
Message-ID: <201901311820.52309.Antony.Stone@squid.open.source.it>

On Thursday 31 January 2019 at 18:13:38, S?bastien Genesta wrote:

> Hi,
> 
> We are encountering strange behaviour using squid proxy has a
> non-transparent proxy.
> 
> We're randomly encountering "ERR_TUNNEL_CONNECTION_FAILED".
> 
> When this error occurs, just wait 1 minutes and the site we tried to reach
> become accessible.

Give us some examples of which site/s this happens with?

> I've read

Where?

> that this issue is often due to a misconfiguration of the proxy server.
> 
> Do you see what kind of option could lead to this behaviour?

Please - at least give us a clue - show us what your existing configuration is.


Antony.

-- 
"Good health" is merely the slowest rate at which you can die.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Thu Jan 31 19:18:20 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 31 Jan 2019 12:18:20 -0700
Subject: [squid-users] daily releases
In-Reply-To: <2105370186.832747.1548937224021@mail.yahoo.com>
References: <1996981566.161146.1548857444728.ref@mail.yahoo.com>
 <1996981566.161146.1548857444728@mail.yahoo.com>
 <53ed2fb4-3803-f4eb-29cb-ce671d743ce4@treenet.co.nz>
 <2105370186.832747.1548937224021@mail.yahoo.com>
Message-ID: <179fff93-5d97-7d56-e9ba-42344d301b9b@measurement-factory.com>

On 1/31/19 5:20 AM, Vieri wrote:

> I'm guessing that the "daily" tarballs that are published on the web
> site's download page are hand-picked because they are known to solve
> bugs, and are considered to be somewhat "stable".

AFAIK, daily tarballs are published automatically. If the script can
build a tarball today, the script publishes it. Exceptional situations
aside, there is no human involved. And that is how it is supposed to be.

Automated tests and pull request reviews offer some protection, but they
cannot guarantee stability.

Alex.


From robertocarna36 at gmail.com  Thu Jan 31 19:48:02 2019
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Thu, 31 Jan 2019 16:48:02 -0300
Subject: [squid-users] Squid doesn't execute url_rewrite_program
	/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
Message-ID: <CAG2Qp6uNz-sKVMqbjH_OHpLZK=bALLT5gSyyjhcfi2NyEpUkBg@mail.gmail.com>

Dear, I have Squid 3.5.23 and I use Squidguard for URL and domain filtering.

In squid.conf I have this line:

url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

but in this proxy server, the line is not executed by Squid, so Squidguard
doesn't work at all.

Same configuration in another proxy server works OK.

Please can you tell me how I can force the execution of url_rewrite_program
line ???

Thanks a lot !!!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190131/d723f264/attachment.htm>

