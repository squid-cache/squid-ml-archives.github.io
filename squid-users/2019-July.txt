From mjguiao at gmail.com  Mon Jul  1 02:04:07 2019
From: mjguiao at gmail.com (Mike Golf)
Date: Mon, 1 Jul 2019 12:04:07 +1000
Subject: [squid-users] squid-users Digest, Vol 58, Issue 31
In-Reply-To: <mailman.1.1561896001.6309.squid-users@lists.squid-cache.org>
References: <mailman.1.1561896001.6309.squid-users@lists.squid-cache.org>
Message-ID: <CAApifbbQLD-__WdqO1Qd4HjL9gD7ZjP75O__Gmmyv_xCxM6dkQ@mail.gmail.com>

I'm looking for help modifying the stock squid config file, within the
GUI I can bypass the proxy completely (HTTP + HTTPS) for certain LAN
IP's; however this will also stop them from accessing the cached HTTP
data. I don't want this rather I want the IP addresses in the range of
192.168.1.2 - 192.168.1.200 to be excluded from HTTPS caching but
still being able to access/cache with the HTTP proxy. I don't know how
to modify the standard configuration files to allow this, PFSense will
bypass(HTTP + HTTPS) any IP I add to "Bypass Proxy for These Source
IPs".

I specified these IP's as DHCP just for a bit of context since my
personal devices 192.168.1.200-192.168.1.254 are statically assigned
devices which I was going to deploy the CA's on, I wanted to avoid
having to deploy CA's to every single device which makes up my DHCP
range. It won't be fun having to install CA's on someones device every
time a guest asks me for my WiFi password. Regarding SSL I made a
mistake on this I just offhandedly generalized all HTTPS stuff as
"SSL" since I'm just used to people saying TLS/SSL when they refer to
HTTPS.

I'm running the HTTP proxy in transparent mode and I've included the
current configuration I'm using for reference, could you walk me
through how I would go about modifying the configuration file. I'm not
to familiar with squid terminology so could you please explain it to
me like I'm 5 (ELI5). I don't know how to structure the directives and
ACL's to allow this since the GUI menu uses a a "blanket"
configuration for whatever you input, I need help with specifying the
custom options.



# This file is automatically generated by pfSense
# Do not edit manually !

http_port 192.168.1.1:3128
http_port 127.0.0.1:3128 intercept
icp_port 0
digest_generation off
dns_v4_first off
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons
visible_hostname localhost
cache_mgr admin at localhost
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 1
debug_options rotate=1
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  192.168.1.0/24
forwarded_for delete
via off
httpd_suppress_version_string on
uri_whitespace strip


cache_mem 2048 MB
maximum_object_size_in_memory 20480 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 256 MB
cache_dir aufs /var/squid/cache 36864 16 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
cache allow all
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320


#Remote proxies


# Setup some default acls
# ACLs all, manager, localhost, and to_localhost are predefined.
acl allsrc src all
acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  3128
3129 1025-65535
acl sslports port 443 563

acl purge method PURGE
acl connect method CONNECT

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS
http_access allow manager localhost

http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !safeports
http_access deny CONNECT !sslports

# Always allow localhost connections
http_access allow localhost

request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
delay_access 1 allow allsrc

# Reverse Proxy settings


# Custom options before auth


# Setup allowed ACLs
# Allow local network(s) on interface(s)
http_access allow localnet
# Default block all to be sure
http_access deny allsrc


> Send squid-users mailing list submissions to
>  squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>  http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>  squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>  squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>  1. Re: Bypassing SSL Man In the Middle Filtering For Certain LAN
>  IP's (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sun, 30 Jun 2019 18:36:19 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Bypassing SSL Man In the Middle Filtering
>  For Certain LAN IP's
> Message-ID: <6e721121-1569-4b6c-21f0-6429d763c5ae at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 30/06/19 2:32 pm, Mike Golf wrote:
> > Hi All,
> >
> > I've setup a squid proxy server on my PFSense router, is there any way
> > of bypassing HTTPS/SSL filtering for certain LAN IP's.
>
> HTTPS is not normally filtered at all. So for that to be happening
> something must be forcing it - all you have to do is *not* force the
> filtering or MITM to happen.
>
> * remove any rules in your NAT or routes directing port 443 to the proxy.
>
> * remove any https_port in the proxy for receiving that intercepted traffic
>
> * remove any SSL-Bump config for handling intercepted port 443 traffic
> or decrypting CONNECT tunnels.
>
> With that all done you will at most be left with clients using the proxy
> in forward-proxy capacity to open CONNECT tunnels.
>
>
> > I have IP
> > addresses 192.168.1.0-192.168.1.200 allocated through DHCP and I want
> > these devices to bypass SSL interception but not the standard HTTP proxy.
>
> Consider how are those clients using the proxy in the first place? Their
> method of IP assignment has nothing to do with it.
>
>
> >
> > Since most modern sites use HTTPS by default HTTP caching isn't that
> > effective anymore,
>
> That is a deceptive statement, more false than most think. But
> irrelevant since what you are wanting will prevent HTTPS caching entirely.
>
>
> > however I want my personal devices to use the SSL
> > proxy
>
> Note that SSL protocols both v2 and v3 are obsolete.
>
> Are you asking for:
>  a) a TLS explicit proxy, or
>  b) a TLS interception proxy, or
>  c) a forward-proxy for relaying HTTPS ?
>
>
> >so I can get the fastest possible browsing experience without
> > having to install certificate authorities on my guests devices which use
> > the DHCP range.
> >
>
> A proxy is not going to do anything in regards to speed for those clients.
>
> The only way which you can improve speed with a proxy is by caching of
> HTTPS content - by avoiding all the re-encrypt delays on every request
> that can be made a HIT. But that requires those cert installations you
> are trying to avoid.
>
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 58, Issue 31
> *******************************************
>


From squid3 at treenet.co.nz  Mon Jul  1 06:28:33 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 1 Jul 2019 18:28:33 +1200
Subject: [squid-users] squid-users Digest, Vol 58, Issue 31
In-Reply-To: <CAApifbbQLD-__WdqO1Qd4HjL9gD7ZjP75O__Gmmyv_xCxM6dkQ@mail.gmail.com>
References: <mailman.1.1561896001.6309.squid-users@lists.squid-cache.org>
 <CAApifbbQLD-__WdqO1Qd4HjL9gD7ZjP75O__Gmmyv_xCxM6dkQ@mail.gmail.com>
Message-ID: <ae64dcd1-be2c-aef3-7fcd-83403a7f9d69@treenet.co.nz>

>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of squid-users digest..."
>>
>>
>> Today's Topics:
>>
>>  1. Re: Bypassing SSL Man In the Middle Filtering For Certain LAN
>>  IP's (Amos Jeffries)
>>


On 1/07/19 2:04 pm, Mike Golf wrote:
> I'm looking for help modifying the stock squid config file, within the
> GUI I can bypass the proxy completely (HTTP + HTTPS) for certain LAN
> IP's; however this will also stop them from accessing the cached HTTP
> data. I don't want this rather I want the IP addresses in the range of
> 192.168.1.2 - 192.168.1.200 to be excluded from HTTPS caching but
> still being able to access/cache with the HTTP proxy. I don't know how
> to modify the standard configuration files to allow this, PFSense will
> bypass(HTTP + HTTPS) any IP I add to "Bypass Proxy for These Source
> IPs".

>
> I'm running the HTTP proxy in transparent mode and I've included the
> current configuration I'm using for reference, could you walk me
> through how I would go about modifying the configuration file. I'm not
> to familiar with squid terminology so could you please explain it to
> me like I'm 5 (ELI5). I don't know how to structure the directives and
> ACL's to allow this since the GUI menu uses a a "blanket"
> configuration for whatever you input, I need help with specifying the
> custom options.
>
> # This file is automatically generated by pfSense
> # Do not edit manually !

Unfortunately I'm not familiar enough with the pfSense GUI to provide
simple instructions for how to use it.

That said ...

>
> http_port 192.168.1.1:3128
> http_port 127.0.0.1:3128 intercept


... there is no https_port here to receive HTTPS or TLS/SSL traffic.

Which means the HTTPS traffic is cannot be cached by this proxy. You
should not have to do anything - what you are asking for is the existing
behaviour of the config file you showed.

Are you seeing https:// URLs in your access.log file? If not, then don't
worry.
 If you are, then that client is an HTTP-only client requesting that
Squid handle the HTTPS parts on its behalf.


Amos


From squid3 at treenet.co.nz  Mon Jul  1 08:14:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 1 Jul 2019 20:14:43 +1200
Subject: [squid-users] delay_pools does not work in squid 4.x
In-Reply-To: <1561911494387-0.post@n4.nabble.com>
References: <1561904256839-0.post@n4.nabble.com>
 <2ef1d897-965c-e09c-b90c-af14d0f26ce4@treenet.co.nz>
 <1561911494387-0.post@n4.nabble.com>
Message-ID: <a8a8b2ed-87f6-cf1e-24b5-c7c0b4b8c2e7@treenet.co.nz>

On 1/07/19 4:18 am, sargen wrote:
>>> acl deny_blocked proxy_auth -i "/usr/local/etc/squid/blocked_users.acl"
>>> delay_pools 1
>>> delay_class 1 4
>>> delay_access 1 allow deny_blocked
>>> delay_access 1 deny all
>>> delay_parameters 1 -1 / -1 -1 / -1 -1 / -1 8000/16000
>>>
>> If the above is an exact copy-paste of your cofig file then the syntax
>> is invalid. Squid-3 may have accepted it but Squid-4 is a bit more
>> pedantic about correct syntax.
> 
> Ok, may be.
> 
>>> After upgrading squid from version 3.5.28 to 4.5, delay_pools stopped
>>> working.
>> How did it stop?
> 
> Sorry, I don't know English well enough.

Your English seems fine. The query was because you did not provide any
details about what the "not working" looked like. There are quite a few
known bugs with delay pools doing odd things - they are an extremely old
feature that pre-dates most QoS functionality people tend to be more
familiar with.


> On version 3.5.28 with such settings, users from the deny_blocked list used
> a bandwidth of 64kbps.
> On version 4.x with the same settings, users from the deny_blocked list used
> the entire bandwidth without restrictions.
> 

Okay.

>> More to the point, why was the above working at all?
> 
> I dont know.
> I just used delay pool class 4
> (http://www.squid-cache.org/Versions/v4/cfgman/delay_class.html) with the
> appropriate delay_parameters as described in the
> http://www.squid-cache.org/Versions/v4/cfgman/delay_parameters.html
> documentation, and this worked exactly as I wanted.
> 
>> *If* Alex's guess is correct, then the bug will be fixed in Squid-5. You
>> can try that cutting-edge code. Whether that is actually your problem or
>> not, nobody knows.
> 
> Ok.
> It was important for me to make sure that the developers were aware of the
> problem. Thanks!
> 

Well, yes and no. The small detail that nobody is sure the problem you
have is the one Alex was talking about still applies. Your problem may
be one none of us are aware of yet. We will not know for sure until you
run the v5 code and see whether it starts working.

Amos


From sargen1907 at mail.ru  Mon Jul  1 15:26:43 2019
From: sargen1907 at mail.ru (sargen)
Date: Mon, 1 Jul 2019 10:26:43 -0500 (CDT)
Subject: [squid-users] delay_pools does not work in squid 4.x
In-Reply-To: <a8a8b2ed-87f6-cf1e-24b5-c7c0b4b8c2e7@treenet.co.nz>
References: <1561904256839-0.post@n4.nabble.com>
 <2ef1d897-965c-e09c-b90c-af14d0f26ce4@treenet.co.nz>
 <1561911494387-0.post@n4.nabble.com>
 <a8a8b2ed-87f6-cf1e-24b5-c7c0b4b8c2e7@treenet.co.nz>
Message-ID: <1561994803592-0.post@n4.nabble.com>

> Your English seems fine. The query was because you did not provide any
> details about what the "not working" looked like. There are quite a few
> known bugs with delay pools doing odd things - they are an extremely old
> feature that pre-dates most QoS functionality people tend to be more
> familiar with.

Now i'm understand this. Possibly, it would be worth providing the contents
of the cache.log file with enabled debug_options All,1 77,9. I will try to
collect this information.

> Well, yes and no. The small detail that nobody is sure the problem you
> have is the one Alex was talking about still applies. Your problem may
> be one none of us are aware of yet. We will not know for sure until you
> run the v5 code and see whether it starts working.

I'm going to try squid v5 on this week. If I can run it, I will surely write
you about the results.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From codycushing at gmail.com  Tue Jul  2 21:05:27 2019
From: codycushing at gmail.com (Cody Cushing)
Date: Tue, 2 Jul 2019 14:05:27 -0700
Subject: [squid-users] Squid + OpenSSL w/FIPS
Message-ID: <CABM+cAyOf9eTHw+wEq3aBKOdtf-OTtoLRbaiG-ezvhf+GToHUw@mail.gmail.com>

Hello, I would like to use Squid as a forward proxy to ensure traffic
leaving my VM is using a TLS connection negotiated through a client using
FIPS certified encryption. I have OpenSSL w/FIPS configured on my VM, and
Squid properly configured as a forward proxy.

What I do not know is:
? is this sufficient (does Squid use any available OpenSSL crypto on the
system)
? or do I need to compile a custom Squid build referencing the OpenSSL fips
"modules" (two C libraries)
? or does Squid reference completely different crypto libraries and neither
of the above two considerations are even valid

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190702/1b5452e3/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jul  2 21:30:36 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 2 Jul 2019 23:30:36 +0200
Subject: [squid-users] Squid + OpenSSL w/FIPS
In-Reply-To: <CABM+cAyOf9eTHw+wEq3aBKOdtf-OTtoLRbaiG-ezvhf+GToHUw@mail.gmail.com>
References: <CABM+cAyOf9eTHw+wEq3aBKOdtf-OTtoLRbaiG-ezvhf+GToHUw@mail.gmail.com>
Message-ID: <201907022330.36331.Antony.Stone@squid.open.source.it>

On Tuesday 02 July 2019 at 23:05:27, Cody Cushing wrote:

> Hello, I would like to use Squid as a forward proxy to ensure traffic
> leaving my VM is using a TLS connection negotiated through a client using
> FIPS certified encryption. I have OpenSSL w/FIPS configured on my VM, and
> Squid properly configured as a forward proxy.

So, surely all you need is a firewall to block any direct traffic which attempts 
to bypass the TLS client?

> What I do not know is:
> ? is this sufficient (does Squid use any available OpenSSL crypto on the
> system)
> ? or do I need to compile a custom Squid build referencing the OpenSSL fips
> "modules" (two C libraries)
> ? or does Squid reference completely different crypto libraries and neither
> of the above two considerations are even valid

You say you want to use "a TLS connection negotiated through a client using 
FIPS certified encryption".  What's at the other end of that connection (ie: 
what is your VM talking to to create this link)?

Are you saying that you want HTTPS connections from your VM to go only to 
remote servers which support this FIPS-certified TLS method, and no other 
websites should be accessible?

Or, are you trying to tunnel HTTP and HTTPS traffic from your VM to some trusted 
endpoint - if so, what happens to it from there?

Basically, given a connection from your VM to some random website, what part 
of the connection are you trying to encrypt in this specific way?


Regards,


Antony.

-- 
"Life is just a lot better if you feel you're having 10 [small] wins a day 
rather than a [big] win every 10 years or so."

 - Chris Hadfield, former skiing (and ski racing) instructor

                                                   Please reply to the list;
                                                         please *don't* CC me.


From sargen1907 at mail.ru  Wed Jul  3 06:23:41 2019
From: sargen1907 at mail.ru (sargen)
Date: Wed, 3 Jul 2019 01:23:41 -0500 (CDT)
Subject: [squid-users] delay_pools does not work in squid 4.x
In-Reply-To: <1561994803592-0.post@n4.nabble.com>
References: <1561904256839-0.post@n4.nabble.com>
 <2ef1d897-965c-e09c-b90c-af14d0f26ce4@treenet.co.nz>
 <1561911494387-0.post@n4.nabble.com>
 <a8a8b2ed-87f6-cf1e-24b5-c7c0b4b8c2e7@treenet.co.nz>
 <1561994803592-0.post@n4.nabble.com>
Message-ID: <1562135021331-0.post@n4.nabble.com>

Ok, this morning I downloaded the latest available version of squid5 and
worked a little with it. I managed to build squid with the following
parameters:

root at testproxy:/usr/local/squid/sbin # ./squid -v
Squid Cache: Version 5.0.0-20190630-rdb8510369
Service Name: squid
configure options: '--prefix = /usr/local/squid' '--enable-delay-pools'
--disable-ipv6' '--disable-external-acl-helpers' --enable-ltdl-convenience

Then I added the following lines to the end of the default configuration
file /usr/local/squid/etc/squid.conf:

# sargen configuration start here

visible_hostname testproxy.svgc.local
cache_effective_user squid
delay_pools 1
delay_class 1 1
delay_access 1 allow localnet
delay_access 1 deny all
delay_parameters 1 16000/16000 # 128 Kbps

I did not test the work of delay_class 4, because the above simple
configuration with delay_class 1 also did not limit the bandwidth on squid
version 4.7

Next, I run squid, and checked the bandwidth limit on several
internet-speedmeter's sites. Everything worked as needed - the bandwidth was
limited to 128Kbps

P.S. Unfortunately, on my test system i cant't managed to buld Squid5
without --disable-external-acl-helpers options: an error occurred during
compilation, which was described by the following link
http://squid-web-proxy-cache.1019090.n4.nabble.com/Latest-squid-5-compile-error-td4686907.html

root at testproxy:/usr/local/squid/sbin # uname -a
FreeBSD testproxy.svgc.local 11.2-RELEASE-p10 FreeBSD 11.2-RELEASE-p10 #0:
Mon May 13 21:20:50 UTC 2019    
root at amd64-builder.daemonology.net:/usr/obj/usr/src/sys/GENERIC  amd64

Thanks for your support!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eperez at quadrianweb.com  Fri Jul  5 01:23:04 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Thu, 4 Jul 2019 20:23:04 -0500
Subject: [squid-users] Squid 3.1.23 and microsoft yammer configuration
Message-ID: <CACXMG+t2neqiyycPhwdDsdPNc35aO7QJjnP6VC+_w9-pqV7dCw@mail.gmail.com>

Hi,
I am using Squid version 3.1.23
And I wonder if anyone has come with problems trying to use microsoft
yammer (www.yammer.com).

Apart from allowing the usual domains, are there any special
configurations such as ssl_bump or something? Perhaps on the
browser-side?

Funny thing is that on access.log I see no attempt of accesing the
site. It seems the GET request never "leaves" the firefox/chrome/IE
browser on the cliet computer.

Comments are welcomed.

compiled with:
Squid Cache: Version 3.1.23
configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu'
'--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr'
'--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc'
'--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--enable-internal-dns'
'--disable-strict-error-checking' '--exec_prefix=/usr'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=$(localstatedir)/log/squid'
'--with-pidfile=$(localstatedir)/run/squid.pid'
'--disable-dependency-tracking' '--enable-arp-acl'
'--enable-follow-x-forwarded-for'
'--enable-auth=basic,digest,ntlm,negotiate'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SMB,YP,getpwnam,multi-domain-NTLM,SASL,DB,POP3,squid_radius_auth'
'--enable-ntlm-auth-helpers=smb_lm,no_check,fakeauth'
'--enable-digest-auth-helpers=password,ldap,eDirectory'
'--enable-negotiate-auth-helpers=squid_kerb_auth'
'--enable-external-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-referer-log' '--enable-removal-policies=heap,lru'
'--enable-snmp' '--enable-ssl' '--enable-storeio=aufs,diskd,ufs'
'--enable-useragent-log' '--enable-wccpv2' '--enable-esi'
'--enable-http-violations' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=16384' '--with-dl' '--with-openssl'
'--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu'
'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
--param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'LDFLAGS=-pie'
'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
--with-squid=/builddir/build/BUILD/squid-3.1.23


thanks,

---------------------
Erick Perez
---------------------


From eperez at quadrianweb.com  Fri Jul  5 03:50:21 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Thu, 4 Jul 2019 22:50:21 -0500
Subject: [squid-users] Squid 3.1.23 and microsoft yammer configuration
In-Reply-To: <CACXMG+t2neqiyycPhwdDsdPNc35aO7QJjnP6VC+_w9-pqV7dCw@mail.gmail.com>
References: <CACXMG+t2neqiyycPhwdDsdPNc35aO7QJjnP6VC+_w9-pqV7dCw@mail.gmail.com>
Message-ID: <CACXMG+srGo+-UnzT0dmWGHDA3tgTGeNiHYojApM1C67yYF8hHA@mail.gmail.com>

Please disregard my question as the "culprit" was the sophos endpoint agent.

In case  someone faces the same problem, please see this:

https://community.sophos.com/kb/en-us/26095
Sophos Enterprise Console: How to authorize a blocked application

Thanks and good night.

---------------------
Erick Perez
---------------------

On Thu, Jul 4, 2019 at 8:23 PM Erick Perez - Quadrian Enterprises
<eperez at quadrianweb.com> wrote:
>
> Hi,
> I am using Squid version 3.1.23
> And I wonder if anyone has come with problems trying to use microsoft
> yammer (www.yammer.com).
>
> Apart from allowing the usual domains, are there any special
> configurations such as ssl_bump or something? Perhaps on the
> browser-side?
>
> Funny thing is that on access.log I see no attempt of accesing the
> site. It seems the GET request never "leaves" the firefox/chrome/IE
> browser on the cliet computer.
>
> Comments are welcomed.
>
> compiled with:
> Squid Cache: Version 3.1.23
> configure options:  '--build=x86_64-redhat-linux-gnu'
> '--host=x86_64-redhat-linux-gnu' '--target=x86_64-redhat-linux-gnu'
> '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr'
> '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc'
> '--datadir=/usr/share' '--includedir=/usr/include'
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--enable-internal-dns'
> '--disable-strict-error-checking' '--exec_prefix=/usr'
> '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=$(localstatedir)/log/squid'
> '--with-pidfile=$(localstatedir)/run/squid.pid'
> '--disable-dependency-tracking' '--enable-arp-acl'
> '--enable-follow-x-forwarded-for'
> '--enable-auth=basic,digest,ntlm,negotiate'
> '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SMB,YP,getpwnam,multi-domain-NTLM,SASL,DB,POP3,squid_radius_auth'
> '--enable-ntlm-auth-helpers=smb_lm,no_check,fakeauth'
> '--enable-digest-auth-helpers=password,ldap,eDirectory'
> '--enable-negotiate-auth-helpers=squid_kerb_auth'
> '--enable-external-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter'
> '--enable-referer-log' '--enable-removal-policies=heap,lru'
> '--enable-snmp' '--enable-ssl' '--enable-storeio=aufs,diskd,ufs'
> '--enable-useragent-log' '--enable-wccpv2' '--enable-esi'
> '--enable-http-violations' '--with-aio' '--with-default-user=squid'
> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
> '--with-pthreads' 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu'
> 'target_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector
> --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'LDFLAGS=-pie'
> 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie'
> --with-squid=/builddir/build/BUILD/squid-3.1.23
>
>
> thanks,
>
> ---------------------
> Erick Perez
> ---------------------


From herinirina.razaka at gmail.com  Sat Jul  6 10:27:33 2019
From: herinirina.razaka at gmail.com (Hery Razakarimanana)
Date: Sat, 6 Jul 2019 13:27:33 +0300
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CAGiUhtM__K9udv64jThszjEmiX07SXjXqea1z4uBEFAe3o7KuA@mail.gmail.com>

hello
I want to confirm my address email to the mailing lit

Best Regards,
Heri
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190706/59527693/attachment.htm>

From herinirina.razaka at gmail.com  Sat Jul  6 10:38:15 2019
From: herinirina.razaka at gmail.com (Hery Razakarimanana)
Date: Sat, 6 Jul 2019 13:38:15 +0300
Subject: [squid-users] squid 3.5.27 issue 407 Proxy Authentication Required
Message-ID: <CAGiUhtOfJ8XJOQqz3wbi6BWoAUV7TY9GvHWEF=Sp3ah714s2rg@mail.gmail.com>

Hi,


I'm trying to implement proxy cache (3.5.27)

*Config*
auth_param negotiate program /usr/lib64/squid/negotiate_wrapper_auth --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--domain=MYDOMAIN --kerberos /usr/lib64/squid/negotiate_kerberos_auth -d -s
GSS_C_NO_NAME
#auth_param negotiate program /usr/lib64/squid/negotiate_wrapper_auth --ntlm
/usr/bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp
--domain=MYDOMAIN --kerberos /usr/lib64/squid/negotiate_kerberos_auth -s
HTTP/sv-proxy
auth_param negotiate children 30
auth_param negotiate keep_alive on

auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp --domain=MYDOMAIN
auth_param ntlm children 30
auth_param ntlm keep_alive off

#acl kerb_auth proxy_auth REQUIRED
acl authenticated proxy_auth REQUIRED
#Definition du r?seau qui pourra acc?der au Proxy
acl localnet src 192.168.0.0/16

*I run :*
export http_proxy=http://MYDOMAIN <http://mydomain/>'\'mysuser:pass at sv-proxy
:3128
export https_proxy=http://MYDOMAIN <http://mydomain/>'\'
mysuser:pass at sv-proxy:3128

curl -v https://www.google.com
* About to connect() to proxy sv-proxy port 3128 (#0)
*   Trying 192.168.101.7...
* Connected to sv-infra-proxy (192.168.101.7) port 3128 (#0)
* Establish HTTP proxy tunnel to www.google.com:443
* Proxy auth using Basic with user 'MYDOMAIN\myuser'
> CONNECT www.google.com:443 HTTP/1.1
> Host: www.google.com:443
> Proxy-Authorization: Basic
> TlVNRU5cc3ZjLXNxdWlkLWJhdGNoOmJiMEE5UXlrZ3ZvTGxYa3F2bjhH
> User-Agent: curl/7.29.0
> Proxy-Connection: Keep-Alive
>
< HTTP/1.1 407 Proxy Authentication Required
< Server: squid/3.5.27
< Mime-Version: 1.0
< Date: Sat, 06 Jul 2019 09:56:41 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 3665
< X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
< Vary: Accept-Language
< Content-Language: en
< Proxy-Authenticate: Negotiate
< Proxy-Authenticate: NTLM
< X-Cache: MISS from sv-proxy
< X-Cache-Lookup: NONE from sv-proxy:3128
< Via: 1.1 sv-proxy (squid/3.5.27)
< Connection: close
<
* Ignore 3665 bytes of response-body
* Received HTTP code 407 from proxy after CONNECT
* Connection #0 to host sv-proxy left intact
curl: (56) Received HTTP code 407 from proxy after CONNECT

*in cahe log I got :*
WARNING: Unsupported or unconfigured/inactive proxy-auth scheme, 'Basic
TlVNRU5cc3ZjLXNxdWlkLWJhdGNoOmJiMEE5UXlrZ3ZvTGxYa3F2bjhH'

Please what is the issue?

Best regards,
Heri
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190706/44b5f41f/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul  6 11:24:33 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 6 Jul 2019 23:24:33 +1200
Subject: [squid-users] squid 3.5.27 issue 407 Proxy Authentication
 Required
In-Reply-To: <CAGiUhtOfJ8XJOQqz3wbi6BWoAUV7TY9GvHWEF=Sp3ah714s2rg@mail.gmail.com>
References: <CAGiUhtOfJ8XJOQqz3wbi6BWoAUV7TY9GvHWEF=Sp3ah714s2rg@mail.gmail.com>
Message-ID: <4b0f5bad-ccd9-5037-0d06-b48c48475320@treenet.co.nz>

On 6/07/19 10:38 pm, Hery Razakarimanana wrote:
> 
> Please what is the issue?
> 

Your proxy only accepts Negotiate/Kerberos, Negotiate/NTLM or NTLM
authentication.


But you have curl using environment variables containing Basic auth
credentials.

To solve you will need to either;

* add Basic auth to the proxy squid.conf

OR,

* remove the environment variables and configure the agents like curl to
use NTLM or Negotiate instead.


Amos


From leomessi983 at yahoo.com  Sat Jul  6 11:51:36 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Sat, 6 Jul 2019 11:51:36 +0000 (UTC)
Subject: [squid-users] ipsec and squid https intercept
References: <536304004.3605116.1562413896401.ref@mail.yahoo.com>
Message-ID: <536304004.3605116.1562413896401@mail.yahoo.com>

HiI use 2 server that connected to each other with IPsec tunnel. 

client >>>> Server1 ======ipsec tunnel======Server2>>>>Internat
I configured Nat in Server2 toward internet and I use squid with tproxy and ssl bump configuration to intercept https requests!without ipsec tunnel my squid server work fine and also when I disable squid in server2 and only use IPsec tunnel everythig is going fine but when I enable squid with IPsec tunnel my client in their browsers get SSL_ERROR_RX_RECORD_TOO_LONG error and squid cache.log show this errors:
"Jul? 6 15:44:59 ParsGateVM800 squid[27066] [daemon:info:1e]: 2019/07/06 15:44:59| SECURITY ALERT: on URL: mobile.pipe.aria.microsoft.com:443Jul? 6 15:44:59 ParsGateVM800 squid[27066] [daemon:info:1e]: 2019/07/06 15:44:59| SECURITY ALERT: Host header forgery detected on local=52.114.128.8:443 remote=10.0.0.110:60270 FD 12 flags=17 (local IP does not match any domain IP)"
I checked my DNS configuration in clients and squid server and they are both same and are 8.8.8.8!
what should i do?! 
what is wrong with my configurations?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190706/d8edf86e/attachment.htm>

From fratesi_guido at hotmail.com  Sat Jul  6 15:12:45 2019
From: fratesi_guido at hotmail.com (Guido Fratesi)
Date: Sat, 6 Jul 2019 15:12:45 +0000
Subject: [squid-users] Gmail issues windows 10
Message-ID: <DB8PR02MB5706BAFD642185B935E81A189EF40@DB8PR02MB5706.eurprd02.prod.outlook.com>

Good morning
I am trying to access Gmail on a windows 10 computer with Squid  installed on a Windows server Essentail acting as a  Domain controller.
I am able to open the GMAIL page, however after I enter the user, Gmail does not move forward asking the password.

Please help

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190706/dbb366c4/attachment.htm>

From fratesi_guido at hotmail.com  Sat Jul  6 15:15:13 2019
From: fratesi_guido at hotmail.com (Guido Fratesi)
Date: Sat, 6 Jul 2019 15:15:13 +0000
Subject: [squid-users] Gmail issues windows 10
Message-ID: <DB8PR02MB570694B37DF6F9BCFD0BC2959EF40@DB8PR02MB5706.eurprd02.prod.outlook.com>

Good morning
I am trying to access Gmail on a windows 10 computer with Squid  installed on a Windows server Essentail acting as a  Domain controller.
I am able to open the GMAIL page, however after I enter the user, Gmail does not move forward asking the password.

Please help



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190706/d141703b/attachment.htm>

From fratesi_guido at hotmail.com  Sat Jul  6 15:23:41 2019
From: fratesi_guido at hotmail.com (Guido Fratesi)
Date: Sat, 6 Jul 2019 15:23:41 +0000
Subject: [squid-users] Gmail issues Windows 10
Message-ID: <DB8PR02MB5706D2843AEAEF29158AE9499EF40@DB8PR02MB5706.eurprd02.prod.outlook.com>

Good morning
I am trying to access Gmail on a windows 10 computer with Squid  installed on a Windows server Essentail acting as a  Domain controller.
I am able to open the GMAIL page, however after I enter the user, Gmail does not move forward asking the password.

Please help

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190706/27bc9469/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul  6 15:27:02 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Jul 2019 03:27:02 +1200
Subject: [squid-users] ipsec and squid https intercept
In-Reply-To: <536304004.3605116.1562413896401@mail.yahoo.com>
References: <536304004.3605116.1562413896401.ref@mail.yahoo.com>
 <536304004.3605116.1562413896401@mail.yahoo.com>
Message-ID: <3ce55080-9211-05d8-2630-6408642b6051@treenet.co.nz>

On 6/07/19 11:51 pm, leomessi983 wrote:
> Hi
> I use 2 server that connected to each other with IPsec tunnel.
> 
> client >>>> Server1 ======ipsec tunnel======Server2>>>>Internat
> 
> I configured Nat in Server2 toward internet and I use squid with tproxy
> and ssl bump configuration to intercept https requests!
> without ipsec tunnel my squid server work fine and also when I disable
> squid in server2 and only use IPsec tunnel everythig is going fine but
> when I enable squid with IPsec tunnel my client in their browsers get
> SSL_ERROR_RX_RECORD_TOO_LONG error and squid cache.log show this errors:
> 
> "Jul? 6 15:44:59 ParsGateVM800 squid[27066] [daemon:info:1e]: 2019/07/06
> 15:44:59| SECURITY ALERT: on URL: mobile.pipe.aria.microsoft.com:443
> Jul? 6 15:44:59 ParsGateVM800 squid[27066] [daemon:info:1e]: 2019/07/06
> 15:44:59| SECURITY ALERT: Host header forgery detected on
> local=52.114.128.8:443 remote=10.0.0.110:60270 FD 12 flags=17 (local IP
> does not match any domain IP)"
> 
> I checked my DNS configuration in clients and squid server and they are
> both same and are 8.8.8.8!
> 

Each query to the 8.8.8.8 servers produces different results. Which
defeats the purpose of having the DNS resolver set to the same thing.

You need to have a local resolver which the two share. That local
resolver can be forwarding to 8.8.8.8 if you really want to.


Which version of Squid are you running? that RX_RECORD error usually
means the other endpoint is not sending TLS. Older versions of Squid
might be sending out a plain-text HTTP response.

Amos


From herinirina.razaka at gmail.com  Sun Jul  7 09:25:50 2019
From: herinirina.razaka at gmail.com (Hery Razakarimanana)
Date: Sun, 7 Jul 2019 12:25:50 +0300
Subject: [squid-users] squid 3.5.27 issue 407 Proxy Authentication
	Required
In-Reply-To: <CAGiUhtNUWLFZQaEm1teZOkbXUEjbh9ANm+jxB4sgGVEH_5OsHw@mail.gmail.com>
References: <CAGiUhtOfJ8XJOQqz3wbi6BWoAUV7TY9GvHWEF=Sp3ah714s2rg@mail.gmail.com>
 <4b0f5bad-ccd9-5037-0d06-b48c48475320@treenet.co.nz>
 <CAGiUhtMC+FKchhdRQhVLfENTD+jC=tv1UUF7xYy6es58ZR7d4Q@mail.gmail.com>
 <CAGiUhtNUWLFZQaEm1teZOkbXUEjbh9ANm+jxB4sgGVEH_5OsHw@mail.gmail.com>
Message-ID: <CAGiUhtMQA-5QaMAbVbf-_bCpfJoD4aRh7bbr1n1X9Jv+SvyXsg@mail.gmail.com>

Amos any news?
I have look into old reports on *TAG_NONE_ABORTED/XXX *according to splice
connection but i don't really understand
Thanks
Heri

On Sun, Jul 7, 2019 at 3:17 AM Hery Razakarimanana <
herinirina.razaka at gmail.com> wrote:

> Hello,
>
> It was working fine.
> I have made comments in my squid.conf then restart and meet new issue.
>
>
>
>
>
> * wget http://perdu.com <http://perdu.com>--2019-07-07 03:13:03--
>  http://perdu.com/ <http://perdu.com/>Resolving sv-proxy (sv-proxy)...
> 127.0.0.1, 192.168.101.7Connecting to sv-proxyg (sv-infra-proxy.numen.mg
> <http://sv-infra-proxy.numen.mg>)|127.0.0.1|:3128... connected.Proxy
> request sent, awaiting response..*.
>
> And it is waiting without response
> In /var/log/squid/access.log I got :
>
>
> *1562458068.026 495171 127.0.0.1 TAG_NONE_ABORTED/000 0 GET
> http://perdu.com/ <http://perdu.com/> - HIER_NONE/- -1562458084.867   8927
> 127.0.0.1 TAG_NONE_ABORTED/000 0 GET http://perdu.com/ <http://perdu.com/>
> - HIER_NONE/- -*
>
> Could you help please?
> Best regards
> Heri
>
>
> On Sat, Jul 6, 2019 at 8:28 PM Hery Razakarimanana <
> herinirina.razaka at gmail.com> wrote:
>
>> I have implemented basic authentication
>>
>> auth_param basic program /usr/lib64/squid/basic_ldap_auth -b
>> "dc=MYDOMAIN,dc=COM" -f "uid=%s" -h ldap.MYDOMAIN.COM
>> auth_param basic children 10
>>
>> After runing curl i got :
>>
>> curl -v https://www.google.com
>> * About to connect() to proxy sv-proxy port 3128 (#0)
>> *   Trying 127.0.0.1...
>> * Connected to sv-proxy (127.0.0.1) port 3128 (#0)
>> * Establish HTTP proxy tunnel to www.google.com:443
>> * Proxy auth using Basic with user 'MYDOMAIN\svc-squid-batch'
>> > CONNECT www.google.com:443 HTTP/1.1
>> > Host: www.google.com:443
>> > Proxy-Authorization: Basic
>> TlVNRU5cc3ZjLXNxdWlkLWJhdGNoOmJiMEE5UXlrZ3TGxYa3F2bjhH
>> > User-Agent: curl/7.29.0
>> > Proxy-Connection: Keep-Alive
>> >
>> < HTTP/1.1 407 Proxy Authentication Required
>> < Server: squid/3.5.27
>> < Mime-Version: 1.0
>> < Date: Sat, 06 Jul 2019 17:18:41 GMT
>> < Content-Type: text/html;charset=utf-8
>> < Content-Length: 3692
>> < X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
>> < Vary: Accept-Language
>> < Content-Language: en
>> < Proxy-Authenticate: Negotiate
>> < Proxy-Authenticate: NTLM
>>
>> *< Proxy-Authenticate: Basic realm="Squid proxy-caching web server"*
>> Authentication problem. Ignoring this.*
>> < X-Cache: MISS from sv-proxy
>> < X-Cache-Lookup: NONE from sv-proxy:3128
>> < Via: 1.1 sv-proxy (squid/3.5.27)
>> < Connection: close
>> <
>> * Received HTTP code 407 from proxy after CONNECT
>> * Connection #0 to host sv-proxy left intact
>> curl: (56) Received HTTP code 407 from proxy after CONNECT
>>
>> What is the issue?  realm in parameter?
>> I have give proxy server before but got the same issue
>>
>> Thanks Ramos
>>
>>
>>
>> On Sat, Jul 6, 2019 at 2:24 PM Amos Jeffries <squid3 at treenet.co.nz>
>> wrote:
>>
>>> On 6/07/19 10:38 pm, Hery Razakarimanana wrote:
>>> >
>>> > Please what is the issue?
>>> >
>>>
>>> Your proxy only accepts Negotiate/Kerberos, Negotiate/NTLM or NTLM
>>> authentication.
>>>
>>>
>>> But you have curl using environment variables containing Basic auth
>>> credentials.
>>>
>>> To solve you will need to either;
>>>
>>> * add Basic auth to the proxy squid.conf
>>>
>>> OR,
>>>
>>> * remove the environment variables and configure the agents like curl to
>>> use NTLM or Negotiate instead.
>>>
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190707/18c377a8/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul  7 10:12:09 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Jul 2019 22:12:09 +1200
Subject: [squid-users] squid 3.5.27 issue 407 Proxy Authentication
 Required
In-Reply-To: <CAGiUhtMQA-5QaMAbVbf-_bCpfJoD4aRh7bbr1n1X9Jv+SvyXsg@mail.gmail.com>
References: <CAGiUhtOfJ8XJOQqz3wbi6BWoAUV7TY9GvHWEF=Sp3ah714s2rg@mail.gmail.com>
 <4b0f5bad-ccd9-5037-0d06-b48c48475320@treenet.co.nz>
 <CAGiUhtMC+FKchhdRQhVLfENTD+jC=tv1UUF7xYy6es58ZR7d4Q@mail.gmail.com>
 <CAGiUhtNUWLFZQaEm1teZOkbXUEjbh9ANm+jxB4sgGVEH_5OsHw@mail.gmail.com>
 <CAGiUhtMQA-5QaMAbVbf-_bCpfJoD4aRh7bbr1n1X9Jv+SvyXsg@mail.gmail.com>
Message-ID: <f09cb1fb-ce1f-99cb-df57-b38e88620a7f@treenet.co.nz>

[ For free help please keep messages on-list. ]


On 7/07/19 9:25 pm, Hery Razakarimanana wrote:
> 
> Amos any news?
> I have look into old reports on *TAG_NONE_ABORTED/XXX *according to
> splice connection but i don't really understand
> Thanks
> Heri
> 
> On Sun, Jul 7, 2019 at 3:17 AM Hery Razakarimanana wrote:
> 
>     Hello,
> 
>     It was working fine.
>     I have made comments in my squid.conf then restart and meet new issue.
> 
>     *?wget http://perdu.com
>     --2019-07-07 03:13:03-- ?http://perdu.com/
>     Resolving sv-proxy (sv-proxy)... 127.0.0.1, 192.168.101.7
>     Connecting to sv-proxyg (sv-infra-proxy.numen.mg
>     <http://sv-infra-proxy.numen.mg>)|127.0.0.1|:3128... connected.
>     Proxy request sent, awaiting response..*.
> 
>     And it is waiting without response
>     In /var/log/squid/access.log I got :
> 
>     *1562458068.026 495171 127.0.0.1 TAG_NONE_ABORTED/000 0 GET
>     http://perdu.com/ - HIER_NONE/- -
>     1562458084.867 ? 8927 127.0.0.1 TAG_NONE_ABORTED/000 0 GET
>     http://perdu.com/ - HIER_NONE/- -*
> 
>     Could you help please?


The client is aborting these transactions. The first takes 495sec the
second only 9sec.

No server was contacted. So either DNS lookups take forever, or the TCP
connection time exceeds 495 seconds.


Amos


From christian.tardif at bell.ca  Tue Jul  9 13:10:21 2019
From: christian.tardif at bell.ca (Tardif, Christian)
Date: Tue, 9 Jul 2019 13:10:21 +0000
Subject: [squid-users] Non-standard proxy setup
Message-ID: <adf806f395d24d45a575a0ee772759d3@DG2MBX03-WYN.bell.corp.bce.ca>

Hi,

I'm trying to figure out how to make the following setup work:

I have a node on which there's an application which isn't proxy aware so basically, the only remaining option would be to use a transparent proxy. But my corporate proxy isn't a transparent proxy. So I have to build this in two layers. My solution would be to:


1)     Have a squid proxy on the node's router host configured as a transparent proxy for both HTTP and HTTPS

2)     Have this squid proxy configured to talk to the parent host, which would be my corporate proxy

3)     Have this squid proxy able to decide if a particular flow should go to the corporate proxy or connect "directly" with the destination host

I've been successful at tasks #2 and #3 (well, in fact, I did it with tinyproxy but stopped because of task #1

I've partly succedded at task #1. In fact, it worked for HTTP. I haven't figured out how to do it for HTTPS. My questions are:


1)     I do not understand how the client would be able to perform a CONNECT to reach squid in HTTPS. So I'm assuming that there's some other magic.

2)     The second thing I don't understand is the certificates management. Let's say my node tries to reach https://www.google.com but does not know anything about the proxy. I assume that the client will get the certificate from squid in some way, but would probably expect to receive a certificate from Google. How would that work?

Can someone help me?   I'm running out of options...

Thanks,

Christian Tardif
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190709/fe8a972b/attachment.htm>

From rousskov at measurement-factory.com  Tue Jul  9 13:54:25 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Jul 2019 09:54:25 -0400
Subject: [squid-users] Non-standard proxy setup
In-Reply-To: <adf806f395d24d45a575a0ee772759d3@DG2MBX03-WYN.bell.corp.bce.ca>
References: <adf806f395d24d45a575a0ee772759d3@DG2MBX03-WYN.bell.corp.bce.ca>
Message-ID: <bf4b6e33-5075-ef84-dea9-c42ef68ac46f@measurement-factory.com>

On 7/9/19 9:10 AM, Tardif, Christian wrote:

> I have a node on which there?s an application which isn?t proxy aware so
> basically, the only remaining option would be to use a transparent
> proxy. But my corporate proxy isn?t a transparent proxy. So I have to
> build this in two layers. My solution would be to:
> 
> ?
> 
> 1)???? Have a squid proxy on the node?s router host configured as a
> transparent proxy for both HTTP and HTTPS
> 
> 2)???? Have this squid proxy configured to talk to the parent host,
> which would be my corporate proxy
> 
> 3)???? Have this squid proxy able to decide if a particular flow should
> go to the corporate proxy or connect ?directly? with the destination host
> 
> ?
> 
> I?ve been successful at tasks #2 and #3 (well, in fact, I did it with
> tinyproxy but stopped because of task #1
> 
> ?
> 
> I?ve partly succedded at task #1. In fact, it worked for HTTP. I haven?t
> figured out how to do it for HTTPS. My questions are:
> 
> ?
> 
> 1)???? I do not understand how the client would be able to perform a
> CONNECT to reach squid in HTTPS. So I?m assuming that there?s some other
> magic.

The client will attempt to open a TLS/TCP connection to the origin
server. Your router (or some such) will redirect client TLS/TCP bytes to
your Squid's https_port. If configured correctly, Squid will accept that
TCP connection and wrap/forward it into/inside an HTTP CONNECT tunnel
through the corporate proxy.


> 2)???? The second thing I don?t understand is the certificates
> management. Let?s say my node tries to reach https://www.google.com but
> does not know anything about the proxy. I assume that the client will
> get the certificate from squid in some way, but would probably expect to
> receive a certificate from Google. How would that work?

* If you do not want your Squid to look inside the connection to
google.com, then your Squid will work at TCP level. Same for the
corporate proxy. Both proxies will forward Google certificate to the
unsuspecting client and everything will work fine most[XXX] of the time.

* Otherwise, you will need to use SslBump functionality and impersonate
the origin server, including faking its certificate. If you add your
proxy CA certificate to the client, this bumping will work for some
sites and will break others.

[XXX] The only HTTPS-related problem you may have in a tunneling-only
Squid is with TCP-level error reporting to the client (e.g., when Squid
cannot connect to the corporate proxy). By default, Squid may want to
bump the client connection (to report those errors to the client),
causing bumping problems mentioned in the second bullet above. For Squid
configurations that are not supposed to bump traffic at all, this
implicit bumping on errors is a bug/misfeature.


HTH,

Alex.


From arunabha.saha at gmail.com  Wed Jul 10 23:44:32 2019
From: arunabha.saha at gmail.com (Arunabha Saha)
Date: Wed, 10 Jul 2019 16:44:32 -0700
Subject: [squid-users] squid-users Digest, Vol 59, Issue 10
In-Reply-To: <mailman.1.1562760001.11370.squid-users@lists.squid-cache.org>
References: <mailman.1.1562760001.11370.squid-users@lists.squid-cache.org>
Message-ID: <CABCok=+LboOP_fPFxqHU15MTVOb_9HONj+a77e6BtAEKQkzEpQ@mail.gmail.com>

>The client will attempt to open a TLS/TCP connection to the origin
>server. Your router (or some such) will redirect client TLS/TCP bytes to
>your Squid's https_port. If configured correctly, Squid will accept that
>TCP connection and wrap/forward it into/inside an HTTP CONNECT tunnel
>through the corporate proxy.

   I'm trying to accomplish something similiar but i don't see squid
wrap the connection to parent proxy in a HTTP CONNECT tunnel.
   User ----->Squid(Transparent Proxy)--------->Parent Proxy------>Internet.
   I need to see a CONNECT tunnel between Squid(Transparent Proxy)
and Parent Proxy but I don't.   Based on another thread, Is this
something that works only starting squid 4.X.   My version is squid
3.5.25.


On Wed, Jul 10, 2019 at 5:02 AM
<squid-users-request at lists.squid-cache.org> wrote:
>
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Non-standard proxy setup (Tardif, Christian)
>    2. Re: Non-standard proxy setup (Alex Rousskov)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 9 Jul 2019 13:10:21 +0000
> From: "Tardif, Christian" <christian.tardif at bell.ca>
> To: "squid-users at lists.squid-cache.org"
>         <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Non-standard proxy setup
> Message-ID:
>         <adf806f395d24d45a575a0ee772759d3 at DG2MBX03-WYN.bell.corp.bce.ca>
> Content-Type: text/plain; charset="utf-8"
>
> Hi,
>
> I'm trying to figure out how to make the following setup work:
>
> I have a node on which there's an application which isn't proxy aware so basically, the only remaining option would be to use a transparent proxy. But my corporate proxy isn't a transparent proxy. So I have to build this in two layers. My solution would be to:
>
>
> 1)     Have a squid proxy on the node's router host configured as a transparent proxy for both HTTP and HTTPS
>
> 2)     Have this squid proxy configured to talk to the parent host, which would be my corporate proxy
>
> 3)     Have this squid proxy able to decide if a particular flow should go to the corporate proxy or connect "directly" with the destination host
>
> I've been successful at tasks #2 and #3 (well, in fact, I did it with tinyproxy but stopped because of task #1
>
> I've partly succedded at task #1. In fact, it worked for HTTP. I haven't figured out how to do it for HTTPS. My questions are:
>
>
> 1)     I do not understand how the client would be able to perform a CONNECT to reach squid in HTTPS. So I'm assuming that there's some other magic.
>
> 2)     The second thing I don't understand is the certificates management. Let's say my node tries to reach https://www.google.com but does not know anything about the proxy. I assume that the client will get the certificate from squid in some way, but would probably expect to receive a certificate from Google. How would that work?
>
> Can someone help me?   I'm running out of options...
>
> Thanks,
>
> Christian Tardif
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190709/fe8a972b/attachment-0001.html>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 9 Jul 2019 09:54:25 -0400
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Non-standard proxy setup
> Message-ID:
>         <bf4b6e33-5075-ef84-dea9-c42ef68ac46f at measurement-factory.com>
> Content-Type: text/plain; charset=windows-1252
>
> On 7/9/19 9:10 AM, Tardif, Christian wrote:
>
> > I have a node on which there?s an application which isn?t proxy aware so
> > basically, the only remaining option would be to use a transparent
> > proxy. But my corporate proxy isn?t a transparent proxy. So I have to
> > build this in two layers. My solution would be to:
> >
> >
> >
> > 1)     Have a squid proxy on the node?s router host configured as a
> > transparent proxy for both HTTP and HTTPS
> >
> > 2)     Have this squid proxy configured to talk to the parent host,
> > which would be my corporate proxy
> >
> > 3)     Have this squid proxy able to decide if a particular flow should
> > go to the corporate proxy or connect ?directly? with the destination host
> >
> >
> >
> > I?ve been successful at tasks #2 and #3 (well, in fact, I did it with
> > tinyproxy but stopped because of task #1
> >
> >
> >
> > I?ve partly succedded at task #1. In fact, it worked for HTTP. I haven?t
> > figured out how to do it for HTTPS. My questions are:
> >
> >
> >
> > 1)     I do not understand how the client would be able to perform a
> > CONNECT to reach squid in HTTPS. So I?m assuming that there?s some other
> > magic.
>
> The client will attempt to open a TLS/TCP connection to the origin
> server. Your router (or some such) will redirect client TLS/TCP bytes to
> your Squid's https_port. If configured correctly, Squid will accept that
> TCP connection and wrap/forward it into/inside an HTTP CONNECT tunnel
> through the corporate proxy.
>
>
> > 2)     The second thing I don?t understand is the certificates
> > management. Let?s say my node tries to reach https://www.google.com but
> > does not know anything about the proxy. I assume that the client will
> > get the certificate from squid in some way, but would probably expect to
> > receive a certificate from Google. How would that work?
>
> * If you do not want your Squid to look inside the connection to
> google.com, then your Squid will work at TCP level. Same for the
> corporate proxy. Both proxies will forward Google certificate to the
> unsuspecting client and everything will work fine most[XXX] of the time.
>
> * Otherwise, you will need to use SslBump functionality and impersonate
> the origin server, including faking its certificate. If you add your
> proxy CA certificate to the client, this bumping will work for some
> sites and will break others.
>
> [XXX] The only HTTPS-related problem you may have in a tunneling-only
> Squid is with TCP-level error reporting to the client (e.g., when Squid
> cannot connect to the corporate proxy). By default, Squid may want to
> bump the client connection (to report those errors to the client),
> causing bumping problems mentioned in the second bullet above. For Squid
> configurations that are not supposed to bump traffic at all, this
> implicit bumping on errors is a bug/misfeature.
>
>
> HTH,
>
> Alex.
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 59, Issue 10
> *******************************************



-- 
regards,
Arun


From rousskov at measurement-factory.com  Thu Jul 11 13:31:02 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Jul 2019 09:31:02 -0400
Subject: [squid-users] Non-standard proxy setup
In-Reply-To: <CABCok=+LboOP_fPFxqHU15MTVOb_9HONj+a77e6BtAEKQkzEpQ@mail.gmail.com>
References: <mailman.1.1562760001.11370.squid-users@lists.squid-cache.org>
 <CABCok=+LboOP_fPFxqHU15MTVOb_9HONj+a77e6BtAEKQkzEpQ@mail.gmail.com>
Message-ID: <42a9f4e2-8ca2-eb2b-88e9-751d4af7558a@measurement-factory.com>

On 7/10/19 7:44 PM, Arunabha Saha wrote:
>> The client will attempt to open a TLS/TCP connection to the origin
>> server. Your router (or some such) will redirect client TLS/TCP bytes to
>> your Squid's https_port. If configured correctly, Squid will accept that
>> TCP connection and wrap/forward it into/inside an HTTP CONNECT tunnel
>> through the corporate proxy.

> i don't see squid
> wrap the connection to parent proxy in a HTTP CONNECT tunnel.
>    User ----->Squid(Transparent Proxy)--------->Parent Proxy------>Internet.
>    I need to see a CONNECT tunnel between Squid(Transparent Proxy)
> and Parent Proxy but I don't.   Based on another thread, Is this
> something that works only starting squid 4.X.

I do not remember for sure, but you may need a development version of
Squid (future v5) or an unofficial patch to forward intercepted tunnels
to a cache peer. If SslBump-related peering support is indeed required
to support such forwarding, then please see this seemingly unrelated bug
report for more details and options:

  https://bugs.squid-cache.org/show_bug.cgi?id=4968

Alex.


From ahmed.zaeem at netstream.ps  Sat Jul 13 09:20:53 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 13 Jul 2019 12:20:53 +0300
Subject: [squid-users] allowing headers per ip and block headers on others
Message-ID: <46DAFC20-4BD3-46BF-AC8A-FF2CDF7772E8@netstream.ps>

hello folks .

say i have a set of rules to block some certain types of headers  as below :

header_access Pragma deny all
header_access Keep-Alive deny all


but i want those above two headers allowed when accessing ip 1.2.3.4

is my config below is correct ? i tested but didn?t work 

acl allowip src 1.2.3.4
##############################
header_access Pragma allow allowip all
header_access Keep-Alive allow allowip all
header_access Pragma deny all
header_access Keep-Alive deny all
#######################


any idea ?


Thanks 

From uhlar at fantomas.sk  Sat Jul 13 09:36:28 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 13 Jul 2019 11:36:28 +0200
Subject: [squid-users] allowing headers per ip and block headers on
	others
In-Reply-To: <46DAFC20-4BD3-46BF-AC8A-FF2CDF7772E8@netstream.ps>
References: <46DAFC20-4BD3-46BF-AC8A-FF2CDF7772E8@netstream.ps>
Message-ID: <20190713093628.GA14626@fantomas.sk>

On 13.07.19 12:20, --Ahmad-- wrote:
>say i have a set of rules to block some certain types of headers  as below :
>
>header_access Pragma deny all
>header_access Keep-Alive deny all
>
>
>but i want those above two headers allowed when accessing ip 1.2.3.4
>
>is my config below is correct ? i tested but didn?t work
>
>acl allowip src 1.2.3.4

when accessing 1.2.3.4 or whn your client is 1.2.3.4?

>##############################
>header_access Pragma allow allowip all
>header_access Keep-Alive allow allowip all

the "all" is superflous above.

>header_access Pragma deny all
>header_access Keep-Alive deny all
>#######################

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I'm not interested in your website anymore.
If you need cookies, bake them yourself.


From ahmed.zaeem at netstream.ps  Sat Jul 13 09:55:05 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 13 Jul 2019 12:55:05 +0300
Subject: [squid-users] allowing headers per ip and block headers on
	others
In-Reply-To: <20190713093628.GA14626@fantomas.sk>
References: <46DAFC20-4BD3-46BF-AC8A-FF2CDF7772E8@netstream.ps>
 <20190713093628.GA14626@fantomas.sk>
Message-ID: <F6A37B7C-8191-4B09-8FBC-58D8046FB596@netstream.ps>



i want it when squid access / contact with 1.2.3.4


Thanks 



> On 13 Jul 2019, at 12:36, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:
> 
> when accessing 1.2.3.4 or whn your client is 1.2.3.4?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190713/49eb948d/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul 13 11:37:44 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jul 2019 23:37:44 +1200
Subject: [squid-users] allowing headers per ip and block headers on
	others
In-Reply-To: <F6A37B7C-8191-4B09-8FBC-58D8046FB596@netstream.ps>
References: <46DAFC20-4BD3-46BF-AC8A-FF2CDF7772E8@netstream.ps>
 <20190713093628.GA14626@fantomas.sk>
 <F6A37B7C-8191-4B09-8FBC-58D8046FB596@netstream.ps>
Message-ID: <8a361974-de04-fad6-a8bc-8fcec3d39b15@treenet.co.nz>

On 13/07/19 9:55 pm, --Ahmad-- wrote:
> 
> 
> i want it when squid access / contact with 1.2.3.4
> 
> 

Use "dst" ACL for destination IP's.

However, requests are generated before sending. Squid does not know
which IP will *in future* be used to deliver the request. So this is
unlikely to work properly even if you get the ACL rules correct.



On 13/07/19 9:20 pm, --Ahmad-- wrote:
> hello folks .
>
> say i have a set of rules to block some certain types of headers  as
below :
>
> header_access Pragma deny all
> header_access Keep-Alive deny all
>

"header_access" is an old Squid-2 directive.

"Keep-Alive" is a HTTP/1.0-only Hop-By-Hop header, Squid does *not* use
it in any way, nor send it to servers.
 => The rules you have for "Keep-Alive" do nothing.


"Pragma" is likewise an old HTTP/1.0 header, *but* has some mandatory
handling requirements when seen in HTTP/1.1. Removing it will break HTTP
cache responses.


Amos


From rousskov at measurement-factory.com  Sat Jul 13 12:10:33 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 13 Jul 2019 08:10:33 -0400
Subject: [squid-users] allowing headers per ip and block headers on
	others
In-Reply-To: <8a361974-de04-fad6-a8bc-8fcec3d39b15@treenet.co.nz>
References: <46DAFC20-4BD3-46BF-AC8A-FF2CDF7772E8@netstream.ps>
 <20190713093628.GA14626@fantomas.sk>
 <F6A37B7C-8191-4B09-8FBC-58D8046FB596@netstream.ps>
 <8a361974-de04-fad6-a8bc-8fcec3d39b15@treenet.co.nz>
Message-ID: <fe80997e-80da-dc21-1ce6-cc9d03ce2957@measurement-factory.com>

On 7/13/19 7:37 AM, Amos Jeffries wrote:
> On 13/07/19 9:55 pm, --Ahmad-- wrote:
>> i want it when squid access / contact with 1.2.3.4

> Use "dst" ACL for destination IP's.

> However, requests are generated before sending. Squid does not know
> which IP will *in future* be used to deliver the request.

request_header_access is applied _after_ the connection to the origin
server or peer is established. Squid does know the destination IP when
applying request_header_access rules.

Alex.


From squid3 at treenet.co.nz  Sat Jul 13 13:31:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 01:31:10 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:1 Denial of
	Service issue in	cachemgr.cgi
Message-ID: <bac263e7-2b31-da04-0a47-d3d7a787f7c3@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:1
__________________________________________________________________

Advisory ID:        SQUID-2019:1
Date:               July 12, 2019
Summary:            Denial of Service issue
                    in cachemgr.cgi
Affected versions:  Squid 4.x -> 4.7
Fixed in version:   Squid 4.8
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_1.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12854
__________________________________________________________________

Problem Description:

 Due to incorrect string termination the cachemgr.cgi may access
 unallocated memory.

 On systems with memory access protections this can result in
 the CGI process terminating unexpectedly. Resulting in a
 denial of service for all clients using it.

__________________________________________________________________

Severity:

 This problem allows a remote attacker with access to the Squid
 manager API to perform a denial of service on other clients.

 This problem is limited to the cachemgr CGI binary.

 Web servers which run per-client instances of CGI tools are
 affected by the issue, but the denial of service is not able to
 affect other clients.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.8.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 http://www.squid-cache.org/Versions/v4/changesets/squid-4-2981a957716c61ff7e21eee1d7d6eb5a237e466d.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All cachemgr.cgi 3.x and older versions are not vulnerable.

 All cachemgr.cgi 4.x versions up to and including 4.7 are
 vulnerable.

 All Squid-4.7 and older versions accessed via the http:// URL
 manager interface are not vulnerable.

To determine the version and interface, look at the footer of
manager reports for the "Generated by" string.

__________________________________________________________________

Workarounds:

Either;

 Convert to exclusively using the HTTP manager interface until
 cachemgr.cgi can be upgraded to a fixed build.

Or;

 Deny all access with 'manager' ACL in squid.conf.

 This completely removes the vulnerability at cost of reduced
 management and monitoring capabilities.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Alex Rousskov of The
 Measurement Factory.

 Fixed by Amos Jeffries from Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-04-10 21:13:50 UTC Initial Report
 2019-05-18 09:43:41 UTC Patch Released
 2019-06-16 10:52:51 UTC CVE Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sat Jul 13 13:32:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 01:32:10 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:2 Denial of
 Service in HTTP Basic Authentication processing
Message-ID: <5785a67e-062a-3394-dbf4-32c973bec71a@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:2
__________________________________________________________________

Advisory ID:        SQUID-2019:2
Date:               July 12, 2019
Summary:            Denial of Service issue
                    in HTTP Basic Authentication processing.
Affected versions:  Squid 2.x -> 2.7.STABLE9
                    Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.7
Fixed in version:   Squid 4.8
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_2.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12529
__________________________________________________________________

Problem Description:

 Due to incorrect buffer management Squid is vulnerable to a
 denial of service attack when processing HTTP Basic
 Authentication credentials.

__________________________________________________________________

Severity:

 Due to incorrect string termination the Basic authentication
 credentials decoder may access memory outside the decode buffer.

 On systems with memory access protections this can result in
 the Squid process being terminated unexpectedly. Resulting in a
 denial of service for all clients using the proxy.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.8.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-dd46b5417809647f561d8a5e0e74c3aacd235258.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x up to and including 2.7.0STABLE9 being used for
 Basic Authentication are vulnerable.

 All Squid-3.x up to and including 3.5.28 being used for Basic
 Authentication are vulnerable.

 All Squid-4.x up to and including 4.7 being used for Basic
 Authentication are vulnerable.


To determine whether auth_param is configured for Basic
authentication in Squid-3.2 and later use the command:

 squid -k parse | grep auth_param


To determine whether auth_param is configured for Basic
authentication in Squid-3.1 and older use the command:

 grep auth_param /etc/squid/squid.conf

__________________________________________________________________

Workarounds:

Either;

 Remove 'auth_param basic ...' configuration settings from
 squid.conf.

Or,

 Build Squid-3.2.14 and later versions with --disable-auth-basic


__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-05-21 21:31:31 UTC Patches Released
 2019-06-05 15:52:17 UTC CVE Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sat Jul 13 13:33:00 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 01:33:00 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:3 Denial of
 Service in HTTP Digest Authentication processing
Message-ID: <241fa895-1623-0885-a1ea-e15c85706f99@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:3
__________________________________________________________________

Advisory ID:        SQUID-2019:3
Date:               July 12, 2019
Summary:            Denial of Service issue
                    in HTTP Digest Authentication processing.
Affected versions:  Squid 3.3.9 -> 3.5.28
                    Squid 4.x -> 4.7
Fixed in version:   Squid 4.8
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_3.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12525
__________________________________________________________________

Problem Description:

 Due to incorrect buffer management Squid is vulnerable to a
 denial of service attack when processing HTTP Digest
 Authentication credentials.

__________________________________________________________________

Severity:

 Due to incorrect input validation the HTTP Request header
 parser for Digest authentication may access memory outside the
 allocated memory buffer.

 On systems with memory access protections this can result in
 the Squid process being terminated unexpectedly. Resulting in a
 denial of service for all clients using the proxy.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.8.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 3.5:
 <http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-ec0d0f39cf28da14eead0ba5e777e95855bc2f67.patch>

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-409956536647b3a05ee1e367424a24ae6b8f13fd.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x are not vulnerable.

 All Squid-3.x up to and including 3.3.8 are not vulnerable.

 All Squid-3.3.9 up to and including 3.3.14 being used for Digest
 authentication are vulnerable.

 All Squid-3.4 versions up to and including 3.4.14 being used for
 Digest authentication are vulnerable.

 All Squid-3.5 versions up to and including 3.5.28 being used for
 Digest authentication are vulnerable.

 All Squid-4.x up to and including 4.7 being used for Digest
 Authentication are vulnerable.


To determine whether auth_param is configured for Digest
authentication use the command:

 squid -k parse | grep auth_param

__________________________________________________________________

Workarounds:

Either;

 Remove 'auth_param digest ...' configuration settings from
 squid.conf.

Or,

 Build Squid with --disable-auth-digest


__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-06-05 15:52:17 UTC CVE Assignment
 2019-06-08 21:09:23 UTC Patches Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sat Jul 13 13:33:33 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 01:33:33 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:5 Heap
 Overflow issue in HTTP Basic Authentication processing
Message-ID: <19af35c9-26f7-5710-719f-61a6971aaca4@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:5
__________________________________________________________________

Advisory ID:        SQUID-2019:5
Date:               July 12, 2019
Summary:            Heap Overflow issue
                    in HTTP Basic Authentication processing.
Affected versions:  Squid 4.0.23 -> 4.7
Fixed in version:   Squid 4.8
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_5.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12527
__________________________________________________________________

Problem Description:

 Due to incorrect buffer management Squid is vulnerable to a
 heap overflow and possible remote code execution attack when
 processing HTTP Authentication credentials.

__________________________________________________________________

Severity:

 This allows a malicious client to write a substantial amount of
 arbitrary data to the heap. Potentially gaining ability to
 execute arbitrary code.

 On systems with memory access protections this can result in
 the Squid process being terminated unexpectedly. Resulting in a
 denial of service for all clients using the proxy.

 This issue is limited to traffic accessing the Squid Cache
 Manager reports or using the FTP protocol gateway.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.8.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-7f73e9c5d17664b882ed32590e6af310c247f320.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x are not vulnerable.

 All Squid-3.x are not vulnerable.

 All Squid-4.x up to and including 4.0.22 are not vulnerable.

 All Squid-4.0.23 up to and including 4.7 built with Basic
 Authentication features are vulnerable.

__________________________________________________________________

Workarounds:

Either;

 Deny ftp:// protocol URLs being proxied and Cache Manager report
 access to all clients:

    acl FTP proto FTP
    http_access deny FTP
    http_access deny manager

Or,

 Build Squid with --disable-auth-basic

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-06-05 15:52:17 UTC CVE Assignment
 2019-06-19 05:58:36 UTC Patches Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sat Jul 13 13:33:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 01:33:47 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:6 Multiple
 Cross-Site Scripting issues in cachemgr.cgi
Message-ID: <ff25630d-7efb-0845-b171-90557a160b35@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:6
__________________________________________________________________

Advisory ID:        SQUID-2019:6
Date:               July 12, 2019
Summary:            Multiple Cross-Site Scripting issues
                    in cachemgr.cgi.
Affected versions:  Squid 2.x all releases
                    Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.7
Fixed in version:   Squid 4.8
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_6.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-13345
__________________________________________________________________

Problem Description:

 Due to incorrect input handling Squid cachemgr.cgi tool is
 vulnerable to multiple cross-site scripting attacks.

__________________________________________________________________

Severity:

 This allows a malicious server to embed URLs in its content such
 that user credentials and other information can be extracted from
 a client or administrator with access to the Squid cachemgr.cgi
 tool URL.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.8.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 3.x:
 <http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-5730c2b5cb56e7639dc423dd62651c8736a54e35.patch>

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-be1dc8614e7514103ba84d4067ed6fd15ab8f82e.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 Squid proxy is not vulnerable. The problem is isolated to the
 manager CGI interface tool.

 cachemgr.cgi tool displays its version number in the HTML page
 footer:

  All 2.x versions up to and including 2.7.STABLE9 are vulnerable.

  All 3.x versions up to and including 3.5.28 are vulnerable.

  All 4.x versions up to and including 4.7 are vulnerable.

  If your cachemgr.cgi does not display a version it is likely
  to be one of the older vulnerable versions.

__________________________________________________________________

Workarounds:

Either;

 Remove use of the cachemgr.cgi tool. It is only necessary for
 older proxy management. Modern Squid proxies management reports
 can be accessed directly.

Or,

 Add CORS protection to the web server running the CGI tool such
 that remote requests to the cachemgr.cgi tool cannot use
 query-string parameters.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Anil Pazvant.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-27 13:38:06 UTC Initial Report
 2019-06-05 15:52:17 UTC CVE Assignment
 2019-07-04 01:17:48 UTC Patches Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sat Jul 13 13:28:48 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 01:28:48 +1200
Subject: [squid-users] [squid-announce] Squid 4.8 is available
Message-ID: <4c0364ff-689e-e955-88ed-0ed3a0e75bdb@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.8 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:


 * SQUID-2019:1 Denial of Service issue in cachemgr.cgi
   (CVE-2019-12854)

This issue security vulnerability is in the cachemgr.cgi tool (not the
squid proxy). With certain requests the cachemgr.cgi may access
unallocated memory.

On systems with memory access protections this can result in the CGI
process terminating unexpectedly. Resulting in a denial of service for
all clients using it.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_1.txt>


 * SQUID-2019:2 Denial of Service in HTTP Basic Authentication
   (CVE-2019-12529)

The Basic authentication credentials decoder may access memory outside
the decode buffer.

On systems with memory access protections this can result in the Squid
process being terminated unexpectedly. Resulting in a denial of service
for all clients using the proxy.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_2.txt>


 * SQUID-2019:3 Denial of Service in HTTP Digest Authentication
   (CVE-2019-12525)

The HTTP Request header parser for Digest authentication may access
memory outside the allocated memory buffer.

On systems with memory access protections this can result in the Squid
process being terminated unexpectedly. Resulting in a denial of service
for all clients using the proxy.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_3.txt>


 * SQUID-2019:5 Heap Overflow issue in HTTP Basic Authentication
   (CVE-2019-12527)

This allows a malicious client to write a substantial amount of
arbitrary data to the heap. Potentially gaining ability to execute
arbitrary code.

On systems with memory access protections this can result in the Squid
process being terminated unexpectedly. Resulting in a denial of service
for all clients using the proxy.

This issue is limited to traffic accessing the Squid Cache Manager
reports or using the FTP protocol gateway.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_5.txt>


 * SQUID-2019:6 Multiple Cross-Site Scripting issues in cachemgr.cgi
   (CVE-2019-13345)

This allows a malicious server to embed URLs in its content such that
user credentials and other information can be extracted from a client or
administrator with access to the Squid cachemgr.cgi tool URL.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_6.txt>


 * Regression: Fix tls-min-version= being ignored

Squid-4 has been allowing negotiation of TLS versions prohibited by this
option. Also, for some configurations the tls-options= also does not
work as intended. This release fixes both these options to work as
documented.


 * Add the NO_TLSv1_3 option to available tls-options values

This release brings the ability to prohibit OpenSSL from negotiating
TLS/1.3 with clients, peers or servers.


 * Bug 4953: to_localhost does not include ::

Some OS treat unspecified destination address as an implicit
localhost connection attempt. This was a well-known issue with IPv4
which was supposed to be prohibited with IPv6 traffic. However, once
again OS have appeared which treat IPv6 :: as an alias of localhost.

To make matters worse some domains return :: explicitly as their IP
address to DNS AAAA queries.

We have added ::/128 to the pre-defined to_localhost ACL. Any users of
that ACL not able to update are advised to add this to their squid.conf
immediately:

  acl to_localhost dst ::/128


 * Bug 4889: Ignore ECONNABORTED in accept(2)

This shows up as a large number of hung socket connections and/or
cache.log entries like:
  oldAccept ...: (53) Software caused connection abort

It primarily occurs on OpenBSD 6.5 and later, but may be seen on other
systems as well.



  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From tarotapprentice at yahoo.com  Sat Jul 13 23:04:46 2019
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 14 Jul 2019 09:04:46 +1000
Subject: [squid-users] Squid security advisories
Message-ID: <611CF840-C760-4577-94E7-1C478908A86C@yahoo.com>

On the Squid-Announce list there were advisories 2019:1, 2, 3, 5 and 6. Was there a 2019:4 that was missed?

MarkJ


From squid3 at treenet.co.nz  Sun Jul 14 02:24:48 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 14:24:48 +1200
Subject: [squid-users] Squid security advisories
In-Reply-To: <611CF840-C760-4577-94E7-1C478908A86C@yahoo.com>
References: <611CF840-C760-4577-94E7-1C478908A86C@yahoo.com>
Message-ID: <ceccfea5-400f-6346-11de-f78d0a579361@treenet.co.nz>

On 14/07/19 11:04 am, TarotApprentice wrote:
> On the Squid-Announce list there were advisories 2019:1, 2, 3, 5 and 6. Was there a 2019:4 that was missed?
> 

Yes and no.  There is a :4 issue. But the fix turned out to be
incomplete so did not make it into this release.

Amos


From tarotapprentice at yahoo.com  Sun Jul 14 03:17:53 2019
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 14 Jul 2019 13:17:53 +1000
Subject: [squid-users] Squid security advisories
In-Reply-To: <ceccfea5-400f-6346-11de-f78d0a579361@treenet.co.nz>
References: <611CF840-C760-4577-94E7-1C478908A86C@yahoo.com>
 <ceccfea5-400f-6346-11de-f78d0a579361@treenet.co.nz>
Message-ID: <F3FDB91C-34FF-403D-8EED-2D0B3218E7CE@yahoo.com>

Thanks. Debian still have 4.6. I?ll raise a bug for Debian in the hope they will get them into Buster and Stretch via their backports repos. They seem to ignore emails, even to the maintainer group.


> On 14 Jul 2019, at 12:24 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 14/07/19 11:04 am, TarotApprentice wrote:
>> On the Squid-Announce list there were advisories 2019:1, 2, 3, 5 and 6. Was there a 2019:4 that was missed?
>> 
> 
> Yes and no.  There is a :4 issue. But the fix turned out to be
> incomplete so did not make it into this release.
> 
> Amos



From squid3 at treenet.co.nz  Sun Jul 14 04:14:58 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 16:14:58 +1200
Subject: [squid-users] Squid security advisories
In-Reply-To: <F3FDB91C-34FF-403D-8EED-2D0B3218E7CE@yahoo.com>
References: <611CF840-C760-4577-94E7-1C478908A86C@yahoo.com>
 <ceccfea5-400f-6346-11de-f78d0a579361@treenet.co.nz>
 <F3FDB91C-34FF-403D-8EED-2D0B3218E7CE@yahoo.com>
Message-ID: <2182bac3-72f3-b526-9c8d-8082dd333d4a@treenet.co.nz>

On 14/07/19 3:17 pm, TarotApprentice wrote:
> Thanks. Debian still have 4.6. I?ll raise a bug for Debian in the hope they will get them into Buster and Stretch via their backports repos. They seem to ignore emails, even to the maintainer group.
> 

Already packaged and awaiting Luigis' test and upload :-P

Amos

> 
>> On 14 Jul 2019, at 12:24 pm, Amos Jeffries wrote:
>>
>>> On 14/07/19 11:04 am, TarotApprentice wrote:
>>> On the Squid-Announce list there were advisories 2019:1, 2, 3, 5 and 6. Was there a 2019:4 that was missed?
>>>
>>
>> Yes and no.  There is a :4 issue. But the fix turned out to be
>> incomplete so did not make it into this release.
>>
>> Amos
> 


From mikio.kishi at gmail.com  Sun Jul 14 05:33:42 2019
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Sun, 14 Jul 2019 14:33:42 +0900
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
Message-ID: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>

Hi all,

 https://www.spinics.net/lists/squid/msg90523.html

As mentioned in the above URL, I would like to use "SSL Bump with HTTP
Cache Peer Parent" as well.
However, still seems not be supported like the following.

- FwdState.cc (in squid-4.8 which is currect stable version)
   825  FwdState::connectStart()
   826  {
   827      assert(serverDestinations.size() > 0);
   828
   829      debugs(17, 3, "fwdConnectStart: " << entry->url());
   830
   831      request->hier.startPeerClock();
   832
   833      // Do not fowrward bumped connections to parent proxy unless it
is an
   834      // origin server
   835      if (serverDestinations[0]->getPeer() &&
!serverDestinations[0]->getPeer()->options.originserver &&
request->flags.sslBumped) {
   836          debugs(50, 4, "fwdConnectStart: Ssl bumped connections
through parent proxy are not allowed");
   837          ErrorState *anErr = new ErrorState(ERR_CANNOT_FORWARD,
Http::scServiceUnavailable, request);
   838          fail(anErr);
   839          self = NULL; // refcounted
   840          return;
   841      }

Do you have any plan to support that? or Are there any solutions for that ?

Regards,
--
Mikio Kishi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190714/22f83d7b/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 14 07:35:28 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jul 2019 19:35:28 +1200
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>
References: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>
Message-ID: <76aa980c-5c43-ee12-6399-102072fa91c5@treenet.co.nz>

On 14/07/19 5:33 pm, mikio.kishi wrote:
> Hi all,
> 
> ?https://www.spinics.net/lists/squid/msg90523.html
> 
> As mentioned in the above URL, I would like to use "SSL Bump with HTTP
> Cache Peer Parent" as well.
> However, still seems not be supported like the following.
> 
...
> 
> Do you have any plan to support that? or Are there any solutions for that ?
> 

The feature being discussed in that thread last year is available in
Squid-5. Feel free to use that version, though do test before putting
into production as bugs are still expected.

Amos


From rousskov at measurement-factory.com  Sun Jul 14 12:58:08 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 14 Jul 2019 08:58:08 -0400
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <76aa980c-5c43-ee12-6399-102072fa91c5@treenet.co.nz>
References: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>
 <76aa980c-5c43-ee12-6399-102072fa91c5@treenet.co.nz>
Message-ID: <ed2efd45-295f-2c98-8370-348cc0d7a680@measurement-factory.com>

On 7/14/19 3:35 AM, Amos Jeffries wrote:
> On 14/07/19 5:33 pm, mikio.kishi wrote:
>> Hi all,
>>
>> ?https://www.spinics.net/lists/squid/msg90523.html
>>
>> As mentioned in the above URL, I would like to use "SSL Bump with HTTP
>> Cache Peer Parent" as well.
>> However, still seems not be supported like the following.
>>
> ...
>>
>> Do you have any plan to support that? or Are there any solutions for that ?
>>
> 
> The feature being discussed in that thread last year is available in
> Squid-5. Feel free to use that version, though do test before putting
> into production as bugs are still expected.

In addition to what Amos has said, you may be interested in the v4 patch
described at https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1

Alex.


From mikio.kishi at gmail.com  Sun Jul 14 14:51:56 2019
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Sun, 14 Jul 2019 23:51:56 +0900
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <ed2efd45-295f-2c98-8370-348cc0d7a680@measurement-factory.com>
References: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>
 <76aa980c-5c43-ee12-6399-102072fa91c5@treenet.co.nz>
 <ed2efd45-295f-2c98-8370-348cc0d7a680@measurement-factory.com>
Message-ID: <CAMUMefYYUz6iSbL96HB=AV3-n0SwjMZAVDbiRuUO_PjpJ6_dMA@mail.gmail.com>

Alex,

Thank you for your reply.

>In addition to what Amos has said, you may be interested in the v4 patch
>described at https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1

Do you have plan to support above officially ?

Regards,
--
Mikio Kishi

On Sun, Jul 14, 2019 at 9:58 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 7/14/19 3:35 AM, Amos Jeffries wrote:
> > On 14/07/19 5:33 pm, mikio.kishi wrote:
> >> Hi all,
> >>
> >>  https://www.spinics.net/lists/squid/msg90523.html
> >>
> >> As mentioned in the above URL, I would like to use "SSL Bump with HTTP
> >> Cache Peer Parent" as well.
> >> However, still seems not be supported like the following.
> >>
> > ...
> >>
> >> Do you have any plan to support that? or Are there any solutions for
> that ?
> >>
> >
> > The feature being discussed in that thread last year is available in
> > Squid-5. Feel free to use that version, though do test before putting
> > into production as bugs are still expected.
>
> In addition to what Amos has said, you may be interested in the v4 patch
> described at https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1
>  Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190714/a8244122/attachment.htm>

From rousskov at measurement-factory.com  Sun Jul 14 15:42:34 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 14 Jul 2019 11:42:34 -0400
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <CAMUMefYYUz6iSbL96HB=AV3-n0SwjMZAVDbiRuUO_PjpJ6_dMA@mail.gmail.com>
References: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>
 <76aa980c-5c43-ee12-6399-102072fa91c5@treenet.co.nz>
 <ed2efd45-295f-2c98-8370-348cc0d7a680@measurement-factory.com>
 <CAMUMefYYUz6iSbL96HB=AV3-n0SwjMZAVDbiRuUO_PjpJ6_dMA@mail.gmail.com>
Message-ID: <adba4d3a-7aae-3402-a795-ef030504df48@measurement-factory.com>

On 7/14/19 10:51 AM, mikio.kishi at gmail.com wrote:

>>In addition to what Amos has said, you may be interested in the v4 patch
>>described at?https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1

> Do you have plan to support above officially ?

The feature has already been rejected from the official v4 inclusion
because the underlying changes are too big/risky for that branch.

Alex.


> On Sun, Jul 14, 2019 at 9:58 PM Alex Rousskov wrote:
> 
>     On 7/14/19 3:35 AM, Amos Jeffries wrote:
>     > On 14/07/19 5:33 pm, mikio.kishi wrote:
>     >> Hi all,
>     >>
>     >> ?https://www.spinics.net/lists/squid/msg90523.html
>     >>
>     >> As mentioned in the above URL, I would like to use "SSL Bump with
>     HTTP
>     >> Cache Peer Parent" as well.
>     >> However, still seems not be supported like the following.
>     >>
>     > ...
>     >>
>     >> Do you have any plan to support that? or Are there any solutions
>     for that ?
>     >>
>     >
>     > The feature being discussed in that thread last year is available in
>     > Squid-5. Feel free to use that version, though do test before putting
>     > into production as bugs are still expected.
> 
>     In addition to what Amos has said, you may be interested in the v4 patch
>     described at?https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1
>     ?Alex.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 



From nurdiyana.mdali at gmail.com  Mon Jul 15 06:28:56 2019
From: nurdiyana.mdali at gmail.com (Nurdiyana Ali)
Date: Mon, 15 Jul 2019 14:28:56 +0800
Subject: [squid-users] Allowing access to Whatsapp Web
Message-ID: <CALH6zOvP7KRSJXi_mtvm_BE0zuhamb43FCFo=1K09NdbuMxKwQ@mail.gmail.com>

Hello,

We have Squid Cache: Version 3.5.20 deployment in our environment. Our
users require access to web.whatapp.com. I whitelisted this site on the
proxy configuration, however, it's giving out error:
Can?t connect securely to this page
This might be because the site uses outdated or unsafe TLS security
settings. If this keeps happening, try contacting the website?s owner.

I have the following sites in the whitelist (as per forum guide):
web.whatsapp.com
dyn.web.whatsapp.com
w0.web.whatsapp.com
w1.web.whatsapp.com
w2.web.whatsapp.com
w3.web.whatsapp.com
w4.web.whatsapp.com
w5.web.whatsapp.com
w6.web.whatsapp.com
w7.web.whatsapp.com
w8.web.whatsapp.com
w9.web.whatsapp.com
pps.whatsapp.net
mms.whatsapp.net
mmg-fna.whatsapp.net

Still no luck.

How do I allow whatsapp web on Squid Proxy?

Sincerely,
Nurdiyana Md Ali.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190715/6df4c3b9/attachment.htm>

From uhlar at fantomas.sk  Mon Jul 15 07:56:49 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 15 Jul 2019 09:56:49 +0200
Subject: [squid-users] Allowing access to Whatsapp Web
In-Reply-To: <CALH6zOvP7KRSJXi_mtvm_BE0zuhamb43FCFo=1K09NdbuMxKwQ@mail.gmail.com>
References: <CALH6zOvP7KRSJXi_mtvm_BE0zuhamb43FCFo=1K09NdbuMxKwQ@mail.gmail.com>
Message-ID: <20190715075649.GA30826@fantomas.sk>

On 15.07.19 14:28, Nurdiyana Ali wrote:
>We have Squid Cache: Version 3.5.20 deployment in our environment. Our
>users require access to web.whatapp.com. I whitelisted this site on the
>proxy configuration

how? can you show us the directive you have used to allow access to
whatsapp?

>, however, it's giving out error:
>Can?t connect securely to this page
>This might be because the site uses outdated or unsafe TLS security
>settings. If this keeps happening, try contacting the website?s owner.
>
>I have the following sites in the whitelist (as per forum guide):
>web.whatsapp.com
>dyn.web.whatsapp.com
>w0.web.whatsapp.com
>w1.web.whatsapp.com
>w2.web.whatsapp.com
>w3.web.whatsapp.com
>w4.web.whatsapp.com
>w5.web.whatsapp.com
>w6.web.whatsapp.com
>w7.web.whatsapp.com
>w8.web.whatsapp.com
>w9.web.whatsapp.com
>pps.whatsapp.net
>mms.whatsapp.net
>mmg-fna.whatsapp.net
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"To Boot or not to Boot, that's the question." [WD1270 Caviar]


From mikio.kishi at gmail.com  Mon Jul 15 12:39:37 2019
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Mon, 15 Jul 2019 21:39:37 +0900
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <adba4d3a-7aae-3402-a795-ef030504df48@measurement-factory.com>
References: <CAMUMefbVL4_DVQsE04do57p19whUFEhbxpJRtQHOePs4kc_haA@mail.gmail.com>
 <76aa980c-5c43-ee12-6399-102072fa91c5@treenet.co.nz>
 <ed2efd45-295f-2c98-8370-348cc0d7a680@measurement-factory.com>
 <CAMUMefYYUz6iSbL96HB=AV3-n0SwjMZAVDbiRuUO_PjpJ6_dMA@mail.gmail.com>
 <adba4d3a-7aae-3402-a795-ef030504df48@measurement-factory.com>
Message-ID: <CAMUMefZNbw4ELrn2E6diw9bJLXKXTGuMJPjk=7YWzwSpUAd7xA@mail.gmail.com>

Alex,

>The feature has already been rejected from the official v4 inclusion
>because the underlying changes are too big/risky for that branch.

I see. I understood that the v4 won't be able to support it.
Anyway, when will you release v5 officially ?

Regards,
--
Mikio Kishi

On Mon, Jul 15, 2019 at 12:42 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 7/14/19 10:51 AM, mikio.kishi at gmail.com wrote:
>
> >>In addition to what Amos has said, you may be interested in the v4 patch
> >>described at https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1
>
> > Do you have plan to support above officially ?
>
> The feature has already been rejected from the official v4 inclusion
> because the underlying changes are too big/risky for that branch.
>  Alex.
>
>
> > On Sun, Jul 14, 2019 at 9:58 PM Alex Rousskov wrote:
> >
> >     On 7/14/19 3:35 AM, Amos Jeffries wrote:
> >     > On 14/07/19 5:33 pm, mikio.kishi wrote:
> >     >> Hi all,
> >     >>
> >     >>  https://www.spinics.net/lists/squid/msg90523.html
> >     >>
> >     >> As mentioned in the above URL, I would like to use "SSL Bump with
> >     HTTP
> >     >> Cache Peer Parent" as well.
> >     >> However, still seems not be supported like the following.
> >     >>
> >     > ...
> >     >>
> >     >> Do you have any plan to support that? or Are there any solutions
> >     for that ?
> >     >>
> >     >
> >     > The feature being discussed in that thread last year is available
> in
> >     > Squid-5. Feel free to use that version, though do test before
> putting
> >     > into production as bugs are still expected.
> >
> >     In addition to what Amos has said, you may be interested in the v4
> patch
> >     described at https://bugs.squid-cache.org/show_bug.cgi?id=4968#c1
> >      Alex.
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190715/e6a66546/attachment.htm>

From ahmed.zaeem at netstream.ps  Mon Jul 15 14:33:47 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 15 Jul 2019 17:33:47 +0300
Subject: [squid-users] tcp_outgoing_address acl based on - incoming header
	Flag
Message-ID: <562F2312-BF17-4AB3-8E93-BDE3088D0552@netstream.ps>

Hello Team .

i want to ask how can i adapt external ip address in squid based on incoming requests .

say i have squid with ips :
1.1.1.1
2.2.2.2
3.3.3.3

##################

a client with src ip 192.168.1.200 will connect to squid port 3128 .
client will initiate a header called start with value = 1.1.1.1

how can i let squid make tcp_outgoing_address with the value in the incoming header  ?

can i put the value of incoming header as acl  variable and get it tcp_outoging_address as variable ?


note that incoming requests only numeric values IPV4 string .


Thanks 



From rousskov at measurement-factory.com  Mon Jul 15 19:00:23 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jul 2019 15:00:23 -0400
Subject: [squid-users] tcp_outgoing_address acl based on - incoming
 header Flag
In-Reply-To: <562F2312-BF17-4AB3-8E93-BDE3088D0552@netstream.ps>
References: <562F2312-BF17-4AB3-8E93-BDE3088D0552@netstream.ps>
Message-ID: <29d31e21-5a60-997b-6ce6-9382f8af5d2d@measurement-factory.com>

On 7/15/19 10:33 AM, --Ahmad-- wrote:

> i want to ask how can i adapt external ip address in squid based on incoming requests .
> 
> say i have squid with ips :
> 1.1.1.1
> 2.2.2.2
> 3.3.3.3

> client will initiate a header called start with value = 1.1.1.1

> how can i let squid make tcp_outgoing_address with the value in the incoming header  ?

Have you tried using req_header ACLs to direct requests to their
corresponding outgoing addresses?

  acl requestsWithStartEqual1p1p1p1 req_header Start ^1[.]1[.]1[.]1$
  ...
  tcp_outgoing_address 1.1.1.1 requestsWithStartEqual1p1p1p1
  tcp_outgoing_address 2.2.2.2 requestsWithStartEqual2p2p2p2
  ...

Alex.


From James.Zuelow at juneau.org  Mon Jul 15 19:11:07 2019
From: James.Zuelow at juneau.org (James Zuelow)
Date: Mon, 15 Jul 2019 19:11:07 +0000
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64,
 "Too few negotiateauthenticator processes are running"
Message-ID: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>

We have a pair of Squid proxies, running as a failover pair with ucarp.

Both of these proxies are domain joined with Samba, and we've been using Kerberos authentication for several years.

After Debian Buster was released, we upgraded the failover unit and did some basic testing.  Everything seemed to go correctly.  Unfortunately when we tested, we didn't put the failover under a serious load - we merely made sure each component was working the way we expected it to.

We waited a week, and then updated the primary.

As soon as the primary was updated and assumed a real load, users started seeing proxy authentication prompts and the proxy started operating very slowly - to the point where sessions would time out.  We quickly rolled to the failover, but the problem remained.

Since this was a major version upgrade, everything on the server had changed so I had lots of places to look for errors.  I did in fact find that my file descriptor settings in limits.conf had reverted back to the default of 1024, but even after fixing this the proxy was slow.

I see in the logs many occurrences of "Too few negotiateauthenticator processes are running" - the negotiate authenticators look like they're crashing every 15-45 seconds when the proxy is busy (between 80-100 requests per second at my site).

Doing a quick Google, I found this:  https://github.com/diladele/websafety-issues/issues/1141
Which refers to this:  https://bugs.squid-cache.org/show_bug.cgi?id=4936

The fix referred to in bug 4936 appears to be about a month old.

https://tracker.debian.org/pkg/squid implies that the version of squid in Buster is older than that, last merged into testing (now stable) in February.

Before I file a Debian bug report, how could I go about confirming the presence of bug 4936 in the current Debian stable version of Squid?  Are the dates good enough?

Thank you!

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190715/43582319/attachment.htm>

From ahmed.zaeem at netstream.ps  Mon Jul 15 20:50:54 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 15 Jul 2019 23:50:54 +0300
Subject: [squid-users] tcp_outgoing_address acl based on - incoming
 header Flag
In-Reply-To: <29d31e21-5a60-997b-6ce6-9382f8af5d2d@measurement-factory.com>
References: <562F2312-BF17-4AB3-8E93-BDE3088D0552@netstream.ps>
 <29d31e21-5a60-997b-6ce6-9382f8af5d2d@measurement-factory.com>
Message-ID: <F1ECD66C-8CB9-4AE4-8C7C-FCF05ABC8D89@netstream.ps>

Hi Alex Thank you very much .


i ask is it possible we have it as variable ?

and tcp_outgoing_address to match acl as variable header from incoming packs ?



> On 15 Jul 2019, at 22:00, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> acl requestsWithStartEqual1p1p1p1 req_header Start ^1[.]1[.]1[.]1$
>  ...
>  tcp_outgoing_address 1.1.1.1 requestsWithStartEqual1p1p1p1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190715/0792091a/attachment.htm>

From rousskov at measurement-factory.com  Mon Jul 15 21:54:27 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jul 2019 17:54:27 -0400
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64,
 "Too few negotiateauthenticator processes are running"
In-Reply-To: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
References: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
Message-ID: <def2693c-a325-7f77-c1f3-e877bca01200@measurement-factory.com>

On 7/15/19 3:11 PM, James Zuelow wrote:

> how could I go about confirming the presence of bug 4936 in the
> current Debian stable version of Squid?

If you want to be sure, get the source code version of the Debian Squid
package (with all the Debian patches applied) and check whether
src/auth/negotiate/wrapper/negotiate_wrapper.cc still contains the buggy
code. See the Squid bug report description[1] for the pattern to look
for (or post a link to the source file here so that others can check).

[1] https://bugs.squid-cache.org/show_bug.cgi?id=4936#c0

Alex.


From rousskov at measurement-factory.com  Mon Jul 15 21:56:28 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jul 2019 17:56:28 -0400
Subject: [squid-users] tcp_outgoing_address acl based on - incoming
 header Flag
In-Reply-To: <F1ECD66C-8CB9-4AE4-8C7C-FCF05ABC8D89@netstream.ps>
References: <562F2312-BF17-4AB3-8E93-BDE3088D0552@netstream.ps>
 <29d31e21-5a60-997b-6ce6-9382f8af5d2d@measurement-factory.com>
 <F1ECD66C-8CB9-4AE4-8C7C-FCF05ABC8D89@netstream.ps>
Message-ID: <dbf0b254-f7c0-ff47-e93f-05ccfa910cf8@measurement-factory.com>

On 7/15/19 4:50 PM, --Ahmad-- wrote:

> i ask is it possible we have it as variable ?
> 
> and tcp_outgoing_address to match acl as variable header from incoming
> packs ?

Sorry, I do not understand what "as variable" means in this context.
Please give an example or explain why the proposed solution does not
address your problem.

Alex.


>> On 15 Jul 2019, at 22:00, Alex Rousskov wrote:
>>
>> acl requestsWithStartEqual1p1p1p1 req_header Start ^1[.]1[.]1[.]1$
>> ?...
>> ?tcp_outgoing_address 1.1.1.1 requestsWithStartEqual1p1p1p1
> 



From leiwen14 at gmail.com  Mon Jul 15 23:46:31 2019
From: leiwen14 at gmail.com (Lei Wen)
Date: Mon, 15 Jul 2019 16:46:31 -0700
Subject: [squid-users] Does request_header_replace support calling into
	another file
Message-ID: <CAPu9cN44caD-+3RtkxOCQGsRZ1=oSoDv60BK539A+LkiA4UruQ@mail.gmail.com>

I am using request_header_replace to modify out going HTTP headers, mainly
the basic/bearer token. Does request_header_replace support calling another
file, the content in that file would be "Basic xxxx...".

Thanks,
Lei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190715/df3b040a/attachment.htm>

From James.Zuelow at juneau.org  Mon Jul 15 23:54:30 2019
From: James.Zuelow at juneau.org (James Zuelow)
Date: Mon, 15 Jul 2019 23:54:30 +0000
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64,
 "Too few negotiateauthenticator processes are running"
In-Reply-To: <def2693c-a325-7f77-c1f3-e877bca01200@measurement-factory.com>
References: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
 <def2693c-a325-7f77-c1f3-e877bca01200@measurement-factory.com>
Message-ID: <139cca5c3ff84067a537807f6f89728e@City-Exch-DB2.cbj.local>

> On 7/15/19 3:11 PM, James Zuelow wrote:
> 
> > how could I go about confirming the presence of bug 4936 in the
> > current Debian stable version of Squid?
> 
> If you want to be sure, get the source code version of the Debian Squid
> package (with all the Debian patches applied) and check whether
> src/auth/negotiate/wrapper/negotiate_wrapper.cc still contains the buggy
> code. See the Squid bug report description[1] for the pattern to look for (or
> post a link to the source file here so that others can check).
> 
> [1] https://bugs.squid-cache.org/show_bug.cgi?id=4936#c0
> 
> Alex.

Thank you!

I believe the Debian source code is here:  http://deb.debian.org/debian/pool/main/s/squid/squid_4.6.orig.tar.gz
With the patches included in this file here:  http://deb.debian.org/debian/pool/main/s/squid/squid_4.6-1.debian.tar.xz 

None of the patches appear to modify negotiate_wrapper.cc.

When I glance through negotiate_wrapper.cc I find something slightly different from either form of the bug report.  The Debian code has a buf+3 construct.

Debian:

(line 182)
length = BASE64_DECODE_LENGTH(strlen(buf+3));

and 

(line 196)
        if (!base64_decode_update(&ctx, &dstLen, token, strlen(buf+3), buf+3) ||
                !base64_decode_final(&ctx)) {
            if (debug_enabled)
                fprintf(stderr, "%s| %s: Invalid base64 token [%s]\n", LogTime(), PROGRAM, buf+3);
            fprintf(stdout, "BH Invalid negotiate request token\n");
            continue;
        }

(I only have a very basic knowledge of C++, so I could be looking at the wrong things completely.)

Thanks,

James


From rousskov at measurement-factory.com  Tue Jul 16 03:50:16 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jul 2019 23:50:16 -0400
Subject: [squid-users] Does request_header_replace support calling into
 another file
In-Reply-To: <CAPu9cN44caD-+3RtkxOCQGsRZ1=oSoDv60BK539A+LkiA4UruQ@mail.gmail.com>
References: <CAPu9cN44caD-+3RtkxOCQGsRZ1=oSoDv60BK539A+LkiA4UruQ@mail.gmail.com>
Message-ID: <914cd9b9-39a9-4609-917d-8494b799ed35@measurement-factory.com>

On 7/15/19 7:46 PM, Lei Wen wrote:
> Does?request_header_replace support calling another file

No, AFACIT, request_header_replace's replacement value does not support
dynamic evaluation of any kind (such as logformat %code expansion). One
should be able to use an eCAP service, an ICAP service, and possibly
request_header_add (with an external ACL that loads the value of the
header from a file as a transaction annotation) instead.

Alex.


From rousskov at measurement-factory.com  Tue Jul 16 03:58:56 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jul 2019 23:58:56 -0400
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64,
 "Too few negotiateauthenticator processes are running"
In-Reply-To: <139cca5c3ff84067a537807f6f89728e@City-Exch-DB2.cbj.local>
References: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
 <def2693c-a325-7f77-c1f3-e877bca01200@measurement-factory.com>
 <139cca5c3ff84067a537807f6f89728e@City-Exch-DB2.cbj.local>
Message-ID: <38369a62-aaae-ac9b-b768-22ba7aa3e464@measurement-factory.com>

On 7/15/19 7:54 PM, James Zuelow wrote:
>> On 7/15/19 3:11 PM, James Zuelow wrote:
>>
>>> how could I go about confirming the presence of bug 4936 in the
>>> current Debian stable version of Squid?
>>
>> If you want to be sure, get the source code version of the Debian Squid
>> package (with all the Debian patches applied) and check whether
>> src/auth/negotiate/wrapper/negotiate_wrapper.cc still contains the buggy
>> code. See the Squid bug report description[1] for the pattern to look for (or
>> post a link to the source file here so that others can check).
>>
>> [1] https://bugs.squid-cache.org/show_bug.cgi?id=4936#c0

> I believe the Debian source code is here:  http://deb.debian.org/debian/pool/main/s/squid/squid_4.6.orig.tar.gz

AFAICT, that source code has Bug 4936.


> When I glance through negotiate_wrapper.cc I find something slightly
> different from either form of the bug report.  The Debian code has a
> buf+3 construct.

Yes, I removed +3 (i.e. skipping "BH " characters) from the sketch in
the bug report to minimize noise.

> Debian:


> length = BASE64_DECODE_LENGTH(strlen(buf+3));

...

        if (!(token = static_cast<uint8_t *>(xmalloc(length)))) {

...

>         if (!base64_decode_update(&ctx, &dstLen, token, strlen(buf+3), buf+3) ||

...

        token[dstLen] = '\0';

Alex.


From rafael.akchurin at diladele.com  Tue Jul 16 05:53:00 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 16 Jul 2019 05:53:00 +0000
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64,
 "Too few negotiateauthenticator processes are running"
In-Reply-To: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
References: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
Message-ID: <AM0PR04MB4753A1CA5F7C8F5FF9E6C85B8FCE0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello James,

Here is to confirm that after applying this patch, rebuilding Squid 4.6 and deploying it into production of about 700 proxy connected clients using mostly Kerberos authentication followed by NTLM and Basic LDAP the mentioned issue with negotiate wrapper went away. No more pop us from client browsers.

Best regards,
Rafael Akchurin
Diladele B.V.

--
Need easy to manage DNS filter? See our new project at https://dnssafety.io/

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of James Zuelow
Sent: Monday, July 15, 2019 9:11 PM
To: 'squid-users at lists.squid-cache.org'
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64, "Too few negotiateauthenticator processes are running"

We have a pair of Squid proxies, running as a failover pair with ucarp.

Both of these proxies are domain joined with Samba, and we've been using Kerberos authentication for several years.

After Debian Buster was released, we upgraded the failover unit and did some basic testing.  Everything seemed to go correctly.  Unfortunately when we tested, we didn't put the failover under a serious load - we merely made sure each component was working the way we expected it to.

We waited a week, and then updated the primary.

As soon as the primary was updated and assumed a real load, users started seeing proxy authentication prompts and the proxy started operating very slowly - to the point where sessions would time out.  We quickly rolled to the failover, but the problem remained.

Since this was a major version upgrade, everything on the server had changed so I had lots of places to look for errors.  I did in fact find that my file descriptor settings in limits.conf had reverted back to the default of 1024, but even after fixing this the proxy was slow.

I see in the logs many occurrences of "Too few negotiateauthenticator processes are running" - the negotiate authenticators look like they're crashing every 15-45 seconds when the proxy is busy (between 80-100 requests per second at my site).

Doing a quick Google, I found this:  https://github.com/diladele/websafety-issues/issues/1141
Which refers to this:  https://bugs.squid-cache.org/show_bug.cgi?id=4936

The fix referred to in bug 4936 appears to be about a month old.

https://tracker.debian.org/pkg/squid implies that the version of squid in Buster is older than that, last merged into testing (now stable) in February.

Before I file a Debian bug report, how could I go about confirming the presence of bug 4936 in the current Debian stable version of Squid?  Are the dates good enough?

Thank you!

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190716/faec0fbd/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 16 06:23:26 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jul 2019 18:23:26 +1200
Subject: [squid-users] Does request_header_replace support calling into
 another file
In-Reply-To: <CAPu9cN44caD-+3RtkxOCQGsRZ1=oSoDv60BK539A+LkiA4UruQ@mail.gmail.com>
References: <CAPu9cN44caD-+3RtkxOCQGsRZ1=oSoDv60BK539A+LkiA4UruQ@mail.gmail.com>
Message-ID: <d285b69c-c2a6-d659-6f86-0ec4797d1a39@treenet.co.nz>

On 16/07/19 11:46 am, Lei Wen wrote:
> I am using?request_header_replace to modify out going HTTP headers,
> mainly the basic/bearer token. Does?request_header_replace support
> calling another file, the content in that file would be "Basic xxxx...".
> 

What are you trying to achieve here?
 Your example implies that you need to do something other than just
replacing headers. Such as actual auth logic.


Amos


From aland at burngreave.net  Tue Jul 16 12:34:12 2019
From: aland at burngreave.net (Kate Dawson)
Date: Tue, 16 Jul 2019 13:34:12 +0100
Subject: [squid-users] sending certificate chain from squid reverse proxy
Message-ID: <20190716123412.5mimbz5vozdnjcik@apple.rat.burntout.org>

Hi, 

Is it possible to send a certificate chain from squid when it's used in
reverse proxy (accel) mode and compiled with gnutls ?  

I am running Debian Buster, and the packaged squid https://packages.debian.org/buster/squid is 4.6-1 

squid -v reports that it is compiled  --with-gnutls

I have the following line (for squid proxy in front of Microsoft Exchange 2016).

https_port 443 accel tls-cert=fullchain.crt tls-key=privkey.pem defaultsite=webmail.example.com vhost  connection-auth=off tls-dh=dh2048.pem

Where fullchain.crt is a concatenation of the public certificate and an
intermediate CA. 

From the http://www.squid-cache.org/Versions/v4/cfgman/http_port.html
page it says regarding the tls-cert option

tls-cert=	Path to file containing an X.509 certificate (PEM format)
			to be used in the TLS handshake ServerHello.
		
			...

			When OpenSSL is used this file may also contain a
			chain of intermediate CA certificates to send in the
			TLS handshake.

			When GnuTLS is used this option (and any paired
			tls-key= option) may be repeated to load multiple
			certificates for different domains.

is it possible to send an intermediate certificate when build with GnuTLS, and if so, what is the options ? 


Thanks in advance, 

Kate Dawson

-- 
"The introduction of a coordinate system to geometry is an act of violence"
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190716/52cab04d/attachment.sig>

From James.Zuelow at juneau.org  Tue Jul 16 16:07:04 2019
From: James.Zuelow at juneau.org (James Zuelow)
Date: Tue, 16 Jul 2019 16:07:04 +0000
Subject: [squid-users] Debian Buster, Squid 4.6-1 amd64,
 "Too few negotiateauthenticator processes are running"
In-Reply-To: <38369a62-aaae-ac9b-b768-22ba7aa3e464@measurement-factory.com>
References: <f641a3cf0b6e497d9835765dc6bd1527@City-Exch-DB2.cbj.local>
 <def2693c-a325-7f77-c1f3-e877bca01200@measurement-factory.com>
 <139cca5c3ff84067a537807f6f89728e@City-Exch-DB2.cbj.local>
 <38369a62-aaae-ac9b-b768-22ba7aa3e464@measurement-factory.com>
Message-ID: <d9e946f10d694b6cb77568b4b2175fe3@City-Exch-DB2.cbj.local>


> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Alex Rousskov

---8<---
> > I believe the Debian source code is here:
> > http://deb.debian.org/debian/pool/main/s/squid/squid_4.6.orig.tar.gz
> 
> AFAICT, that source code has Bug 4936.
> 
> 
---8<---

Thank you Alex!


James

From arunabha.saha at gmail.com  Tue Jul 16 17:51:27 2019
From: arunabha.saha at gmail.com (Arunabha Saha)
Date: Tue, 16 Jul 2019 10:51:27 -0700
Subject: [squid-users] squid-users Digest, Vol 59, Issue 12
In-Reply-To: <mailman.1.1562932801.20074.squid-users@lists.squid-cache.org>
References: <mailman.1.1562932801.20074.squid-users@lists.squid-cache.org>
Message-ID: <CABCok=JtD-KueEGDdoD5kr-mUZX9d2uRX_U31a6xxpJOZbXM+A@mail.gmail.com>

Thanks.   i did get it working with the latest 5.0.0 (unreleased) code
in github.    The configuration has to be  "ssl-bump client-first .."
for this to work.
Does that sound right?

On Fri, Jul 12, 2019 at 5:02 AM
<squid-users-request at lists.squid-cache.org> wrote:
>
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Non-standard proxy setup (Alex Rousskov)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Thu, 11 Jul 2019 09:31:02 -0400
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Non-standard proxy setup
> Message-ID:
>         <42a9f4e2-8ca2-eb2b-88e9-751d4af7558a at measurement-factory.com>
> Content-Type: text/plain; charset=utf-8
>
> On 7/10/19 7:44 PM, Arunabha Saha wrote:
> >> The client will attempt to open a TLS/TCP connection to the origin
> >> server. Your router (or some such) will redirect client TLS/TCP bytes to
> >> your Squid's https_port. If configured correctly, Squid will accept that
> >> TCP connection and wrap/forward it into/inside an HTTP CONNECT tunnel
> >> through the corporate proxy.
>
> > i don't see squid
> > wrap the connection to parent proxy in a HTTP CONNECT tunnel.
> >    User ----->Squid(Transparent Proxy)--------->Parent Proxy------>Internet.
> >    I need to see a CONNECT tunnel between Squid(Transparent Proxy)
> > and Parent Proxy but I don't.   Based on another thread, Is this
> > something that works only starting squid 4.X.
>
> I do not remember for sure, but you may need a development version of
> Squid (future v5) or an unofficial patch to forward intercepted tunnels
> to a cache peer. If SslBump-related peering support is indeed required
> to support such forwarding, then please see this seemingly unrelated bug
> report for more details and options:
>
>   https://bugs.squid-cache.org/show_bug.cgi?id=4968
>
> Alex.
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 59, Issue 12
> *******************************************



-- 
regards,
Arun


From rousskov at measurement-factory.com  Tue Jul 16 18:24:38 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jul 2019 14:24:38 -0400
Subject: [squid-users] Non-standard proxy setup
In-Reply-To: <CABCok=JtD-KueEGDdoD5kr-mUZX9d2uRX_U31a6xxpJOZbXM+A@mail.gmail.com>
References: <mailman.1.1562932801.20074.squid-users@lists.squid-cache.org>
 <CABCok=JtD-KueEGDdoD5kr-mUZX9d2uRX_U31a6xxpJOZbXM+A@mail.gmail.com>
Message-ID: <10042704-0095-8a59-d146-b202ff09394e@measurement-factory.com>

On 7/16/19 1:51 PM, Arunabha Saha wrote:
> i did get it working with the latest 5.0.0 (unreleased) code
> in github.    The configuration has to be  "ssl-bump client-first .."
> for this to work. Does that sound right?


No, it does not, both because the deprecated "client-first" action
should not be used in moderns Squids, and because supported SslBump
actions should work through peers IIRC (which action is the right one
for you depends on your exact needs -- not every action will work for
any given use case, of course).

Alex.

>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of squid-users digest..."


>> On 7/10/19 7:44 PM, Arunabha Saha wrote:
>>>> The client will attempt to open a TLS/TCP connection to the origin
>>>> server. Your router (or some such) will redirect client TLS/TCP bytes to
>>>> your Squid's https_port. If configured correctly, Squid will accept that
>>>> TCP connection and wrap/forward it into/inside an HTTP CONNECT tunnel
>>>> through the corporate proxy.
>>
>>> i don't see squid
>>> wrap the connection to parent proxy in a HTTP CONNECT tunnel.
>>>    User ----->Squid(Transparent Proxy)--------->Parent Proxy------>Internet.
>>>    I need to see a CONNECT tunnel between Squid(Transparent Proxy)
>>> and Parent Proxy but I don't.   Based on another thread, Is this
>>> something that works only starting squid 4.X.
>>
>> I do not remember for sure, but you may need a development version of
>> Squid (future v5) or an unofficial patch to forward intercepted tunnels
>> to a cache peer. If SslBump-related peering support is indeed required
>> to support such forwarding, then please see this seemingly unrelated bug
>> report for more details and options:
>>
>>   https://bugs.squid-cache.org/show_bug.cgi?id=4968
>>
>> Alex.


From ahmed.zaeem at netstream.ps  Tue Jul 16 22:11:06 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 17 Jul 2019 01:11:06 +0300
Subject: [squid-users] Possible to user reply_header_add directive with acl
 random access list ?
Message-ID: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>

Hello folks ,
want to ask .
Possible to user reply_header_add directive with acl random access list ?

i read that reply_header_add only need fast acl and im not sure if random acl is fast/slow based on below :

http://www.squid-cache.org/Doc/config/reply_header_add/
and
https://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs

so indeed i would like i can match reply_header_add with some random acls .

i tried some samples and i got an unexpected/Wrong results .

let me know your thoughts for that issue .

kind regards 



From rousskov at measurement-factory.com  Wed Jul 17 04:10:51 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jul 2019 00:10:51 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
Message-ID: <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>

On 7/16/19 6:11 PM, --Ahmad-- wrote:

> Possible to user reply_header_add directive with acl random access list?

Yes, it is possible.


> i read that reply_header_add only need fast acl and im not sure if random acl is fast/slow

The random ACL is fast. GitHub pull requests that add that missing info
to the random ACL documentation in src/cf.data.pre are welcomed.
https://wiki.squid-cache.org/MergeProcedure

Alex.


From ahmed.zaeem at netstream.ps  Wed Jul 17 09:41:58 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 17 Jul 2019 12:41:58 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
Message-ID: <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>

Hi Alex, 
acl half10000 random 1/10
acl half10001 random 1/9
acl half10002 random 1/8
acl half10003 random 1/7
acl half10004 random 1/6
acl half10005 random 1/5
acl half10006 random 1/4
acl half10007 random 1/3
acl half10008 random 1/2
acl half10009 random 1/1
########################################
reply_header_add start "A" half10000
reply_header_add start "B" half10001
reply_header_add start "C" half10002
reply_header_add start "D" half10003
reply_header_add start "E" half10004
reply_header_add start "F" half10005
reply_header_add start "G" half10006
reply_header_add start "H" half10007
reply_header_add start "I" half10008
reply_header_add start "J" half10009
##############################
tcp_outgoing_address 12.13.100.1 half10000
tcp_outgoing_address 12.13.100.2 half10001
tcp_outgoing_address 12.13.100.3 half10002
tcp_outgoing_address 12.13.100.4 half10003
tcp_outgoing_address 12.13.100.5 half10004
tcp_outgoing_address 12.13.100.6 half10005
tcp_outgoing_address 12.13.100.7 half10006
tcp_outgoing_address 12.13.100.8 half10007
tcp_outgoing_address 12.13.100.9 half10008
tcp_outgoing_address 12.13.100.10 half10009



 curl -x 12.13.100.250:2000    -U hi:hi  ifconfig.io  -v

* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.100.250...
* TCP_NODELAY set
* Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
* Proxy auth using Basic with user 'hi'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Wed, 17 Jul 2019 09:34:57 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: G
< start: F
< start: E
< start: E
< 
12.13.100.2 
* Connection #0 to host 12.13.100.250 left intact



another Hit :


 curl -x 12.13.100.250:2000    -U hi:hi  ifconfig.io  -v

* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.100.250...
* TCP_NODELAY set
* Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
* Proxy auth using Basic with user 'hi'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Wed, 17 Jul 2019 09:34:57 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: F
< start: A
< start: J
< start: I
< 
12.13.100.6



so as you see above , i have multiple replied headers not single one .
and the replied header even are wrong .
so wrong multiple results i do recieve .


my questions is :

1- why mutiple replies do we recieve not single reply ?
2- why the recieved replies are wrong , i expect single reply based on my random acls we setup . ?

do we need other stuff with random acl to have it work with header directive ?




Thank You 


> On 17 Jul 2019, at 7:10, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 7/16/19 6:11 PM, --Ahmad-- wrote:
> 
>> Possible to user reply_header_add directive with acl random access list?
> 
> Yes, it is possible.
> 
> 
>> i read that reply_header_add only need fast acl and im not sure if random acl is fast/slow
> 
> The random ACL is fast. GitHub pull requests that add that missing info
> to the random ACL documentation in src/cf.data.pre are welcomed.
> https://wiki.squid-cache.org/MergeProcedure
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jul 17 10:43:33 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jul 2019 22:43:33 +1200
Subject: [squid-users] sending certificate chain from squid reverse proxy
In-Reply-To: <20190716123412.5mimbz5vozdnjcik@apple.rat.burntout.org>
References: <20190716123412.5mimbz5vozdnjcik@apple.rat.burntout.org>
Message-ID: <58dfb81a-18cd-edad-224c-7bc41d19c265@treenet.co.nz>

On 17/07/19 12:34 am, Kate Dawson wrote:
> Hi, 
> 
> Is it possible to send a certificate chain from squid when it's used in
> reverse proxy (accel) mode and compiled with gnutls ?  
> 

That has not been implemented yet. Sorry.

> 
> is it possible to send an intermediate certificate when build with GnuTLS, and if so, what is the options ? 
> 

The intention is for the chain loading (when supported) to be configured
with tls-cert= the same as one would do for OpenSSL now.

Amos


From squid3 at treenet.co.nz  Wed Jul 17 11:42:25 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jul 2019 23:42:25 +1200
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
Message-ID: <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>

On 17/07/19 9:41 pm, --Ahmad-- wrote:
> Hi Alex, 
> acl half10000 random 1/10
> acl half10001 random 1/9
> acl half10002 random 1/8
> acl half10003 random 1/7
> acl half10004 random 1/6
> acl half10005 random 1/5
> acl half10006 random 1/4
> acl half10007 random 1/3
> acl half10008 random 1/2
> acl half10009 random 1/1
> ########################################
> reply_header_add start "A" half10000
> reply_header_add start "B" half10001
> reply_header_add start "C" half10002
> reply_header_add start "D" half10003
> reply_header_add start "E" half10004
> reply_header_add start "F" half10005
> reply_header_add start "G" half10006
> reply_header_add start "H" half10007
> reply_header_add start "I" half10008
> reply_header_add start "J" half10009
> ##############################
> tcp_outgoing_address 12.13.100.1 half10000
> tcp_outgoing_address 12.13.100.2 half10001
> tcp_outgoing_address 12.13.100.3 half10002
> tcp_outgoing_address 12.13.100.4 half10003
> tcp_outgoing_address 12.13.100.5 half10004
> tcp_outgoing_address 12.13.100.6 half10005
> tcp_outgoing_address 12.13.100.7 half10006
> tcp_outgoing_address 12.13.100.8 half10007
> tcp_outgoing_address 12.13.100.9 half10008
> tcp_outgoing_address 12.13.100.10 half10009
> 
> 
> 
>  curl -x 12.13.100.250:2000    -U hi:hi  ifconfig.io  -v
> 
> * Rebuilt URL to: ifconfig.io/
> *   Trying 12.13.100.250...
> * TCP_NODELAY set
> * Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
> * Proxy auth using Basic with user 'hi'
>> GET http://ifconfig.io/ HTTP/1.1
>> Host: ifconfig.io
>> Proxy-Authorization: Basic YmVuOmJlbg==
>> User-Agent: curl/7.54.0
>> Accept: */*
>> Proxy-Connection: Keep-Alive
>>
> < HTTP/1.1 200 OK
> < Date: Wed, 17 Jul 2019 09:34:57 GMT
> < Content-Type: text/plain; charset=utf-8
> < Content-Length: 40
> < Connection: keep-alive
> < start: G
> < start: F
> < start: E
> < start: E
> < 
> 12.13.100.2 
> * Connection #0 to host 12.13.100.250 left intact
> 

That reply does look strange. "E" should only occur once, and "J" is
missing.


> 
> another Hit :
> 
> 
>  curl -x 12.13.100.250:2000    -U hi:hi  ifconfig.io  -v
> 
> * Rebuilt URL to: ifconfig.io/
> *   Trying 12.13.100.250...
> * TCP_NODELAY set
> * Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
> * Proxy auth using Basic with user 'hi'
>> GET http://ifconfig.io/ HTTP/1.1
>> Host: ifconfig.io
>> Proxy-Authorization: Basic YmVuOmJlbg==
>> User-Agent: curl/7.54.0
>> Accept: */*
>> Proxy-Connection: Keep-Alive
>>
> < HTTP/1.1 200 OK
> < Date: Wed, 17 Jul 2019 09:34:57 GMT
> < Content-Type: text/plain; charset=utf-8
> < Content-Length: 40
> < Connection: keep-alive
> < start: F
> < start: A
> < start: J
> < start: I
> < 
> 12.13.100.6
> 
> 
> 
> so as you see above , i have multiple replied headers not single one .
> and the replied header even are wrong .
> so wrong multiple results i do recieve .
> 

reply_header_add does not stop with the first matching line like
http_access. Each is checked to see if that value is to be added.

So naturally each letter has a random chance of being added.

In other words;
 You have configured Squid to add the header "start" between 0 and 10
times, with a selection of letters.


The tcp_outgoing_address check for which IP address to use is
independent of what headers are added. That directive *does* stop on
first matching line.


> 
> my questions is :
> 
> 1- why mutiple replies do we recieve not single reply ?

What do you mean by "multiple replies" ?


> 2- why the recieved replies are wrong , i expect single reply based on my random acls we setup . ?
> 

Every time a "random" type ACL is tested a new random number is selected
and checked against the match:non-match ratio you configure.



> do we need other stuff with random acl to have it work with header directive ?
> 

The ACL works as designed. You appear to have missed the fact that each
check/test of the ACL uses a different randomly selected number.


Amos


From ahmed.zaeem at netstream.ps  Wed Jul 17 11:55:13 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 17 Jul 2019 14:55:13 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
Message-ID: <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>

Hi Amos , Thank you for you info .

indeed i read about reply header ACL That :

##############################################
One or more Squid ACLs may be specified to restrict header
	injection to matching responses. As always in squid.conf, all
	ACLs in the ACL list must be satisfied for the insertion to
	happen. The reply_header_add option supports fast ACLs only.

	See also: request_header_add.
#################################################

im not sure what do i need to let the output single value and not multiple values .

about your Question :
> 1- why mutiple replies do we recieve not single reply ?

What do you mean by "multiple replies" ?
????> i mean i would like the result to be as below :

* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.100.250...
* TCP_NODELAY set
* Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
* Proxy auth using Basic with user 'hi'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Wed, 17 Jul 2019 09:34:57 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: A
< 
12.13.100.1
* Connection #0 to host 12.13.100.250 left intact



* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.100.250...
* TCP_NODELAY set
* Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
* Proxy auth using Basic with user 'hi'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Wed, 17 Jul 2019 09:34:57 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: B
< 
12.13.100.2
* Connection #0 to host 12.13.100.250 left intact




* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.100.250...
* TCP_NODELAY set
* Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
* Proxy auth using Basic with user 'hi'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Wed, 17 Jul 2019 09:34:57 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: C
< 
12.13.100.3
* Connection #0 to host 12.13.100.250 left intact


* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.100.250...
* TCP_NODELAY set
* Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
* Proxy auth using Basic with user 'hi'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Wed, 17 Jul 2019 09:34:57 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: D
< 
12.13.100.4
* Connection #0 to host 12.13.100.250 left intact



###############################################


Check the 4 tests above ? those i want the result to be .
if i have external ip 12.13.100.4 , the Header should  be single and = < start: D
if i go external 12.13.100.3 ,the Header should  be single and = < start: C
if i go external 12.13.100.2 ,the Header should  be single and = < start: B
if i go external 12.13.100.1 ,the Header should  be single and = < start: B


SO basically i want 1 answer matching the acl :

acl half10000 random 1/10
acl half10001 random 1/9
acl half10002 random 1/8
acl half10003 random 1/7
acl half10004 random 1/6
acl half10005 random 1/5
acl half10006 random 1/4
acl half10007 random 1/3
acl half10008 random 1/2
acl half10009 random 1/1



as  you see above the ACLS above should be matching single values not multiple values .

and when i get multiple headers replies it doesnt satisfying my needs .


what do you think amos ?


Thanks agian 


> On 17 Jul 2019, at 14:42, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 17/07/19 9:41 pm, --Ahmad-- wrote:
>> Hi Alex, 
>> acl half10000 random 1/10
>> acl half10001 random 1/9
>> acl half10002 random 1/8
>> acl half10003 random 1/7
>> acl half10004 random 1/6
>> acl half10005 random 1/5
>> acl half10006 random 1/4
>> acl half10007 random 1/3
>> acl half10008 random 1/2
>> acl half10009 random 1/1
>> ########################################
>> reply_header_add start "A" half10000
>> reply_header_add start "B" half10001
>> reply_header_add start "C" half10002
>> reply_header_add start "D" half10003
>> reply_header_add start "E" half10004
>> reply_header_add start "F" half10005
>> reply_header_add start "G" half10006
>> reply_header_add start "H" half10007
>> reply_header_add start "I" half10008
>> reply_header_add start "J" half10009
>> ##############################
>> tcp_outgoing_address 12.13.100.1 half10000
>> tcp_outgoing_address 12.13.100.2 half10001
>> tcp_outgoing_address 12.13.100.3 half10002
>> tcp_outgoing_address 12.13.100.4 half10003
>> tcp_outgoing_address 12.13.100.5 half10004
>> tcp_outgoing_address 12.13.100.6 half10005
>> tcp_outgoing_address 12.13.100.7 half10006
>> tcp_outgoing_address 12.13.100.8 half10007
>> tcp_outgoing_address 12.13.100.9 half10008
>> tcp_outgoing_address 12.13.100.10 half10009
>> 
>> 
>> 
>> curl -x 12.13.100.250:2000    -U hi:hi  ifconfig.io  -v
>> 
>> * Rebuilt URL to: ifconfig.io/
>> *   Trying 12.13.100.250...
>> * TCP_NODELAY set
>> * Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
>> * Proxy auth using Basic with user 'hi'
>>> GET http://ifconfig.io/ HTTP/1.1
>>> Host: ifconfig.io
>>> Proxy-Authorization: Basic YmVuOmJlbg==
>>> User-Agent: curl/7.54.0
>>> Accept: */*
>>> Proxy-Connection: Keep-Alive
>>> 
>> < HTTP/1.1 200 OK
>> < Date: Wed, 17 Jul 2019 09:34:57 GMT
>> < Content-Type: text/plain; charset=utf-8
>> < Content-Length: 40
>> < Connection: keep-alive
>> < start: G
>> < start: F
>> < start: E
>> < start: E
>> < 
>> 12.13.100.2 
>> * Connection #0 to host 12.13.100.250 left intact
>> 
> 
> That reply does look strange. "E" should only occur once, and "J" is
> missing.
> 
> 
>> 
>> another Hit :
>> 
>> 
>> curl -x 12.13.100.250:2000    -U hi:hi  ifconfig.io  -v
>> 
>> * Rebuilt URL to: ifconfig.io/
>> *   Trying 12.13.100.250...
>> * TCP_NODELAY set
>> * Connected to 12.13.100.250 (12.13.100.250) port 2000 (#0)
>> * Proxy auth using Basic with user 'hi'
>>> GET http://ifconfig.io/ HTTP/1.1
>>> Host: ifconfig.io
>>> Proxy-Authorization: Basic YmVuOmJlbg==
>>> User-Agent: curl/7.54.0
>>> Accept: */*
>>> Proxy-Connection: Keep-Alive
>>> 
>> < HTTP/1.1 200 OK
>> < Date: Wed, 17 Jul 2019 09:34:57 GMT
>> < Content-Type: text/plain; charset=utf-8
>> < Content-Length: 40
>> < Connection: keep-alive
>> < start: F
>> < start: A
>> < start: J
>> < start: I
>> < 
>> 12.13.100.6
>> 
>> 
>> 
>> so as you see above , i have multiple replied headers not single one .
>> and the replied header even are wrong .
>> so wrong multiple results i do recieve .
>> 
> 
> reply_header_add does not stop with the first matching line like
> http_access. Each is checked to see if that value is to be added.
> 
> So naturally each letter has a random chance of being added.
> 
> In other words;
> You have configured Squid to add the header "start" between 0 and 10
> times, with a selection of letters.
> 
> 
> The tcp_outgoing_address check for which IP address to use is
> independent of what headers are added. That directive *does* stop on
> first matching line.
> 
> 
>> 
>> my questions is :
>> 
>> 1- why mutiple replies do we recieve not single reply ?
> 
> What do you mean by "multiple replies" ?
> 
> 
>> 2- why the recieved replies are wrong , i expect single reply based on my random acls we setup . ?
>> 
> 
> Every time a "random" type ACL is tested a new random number is selected
> and checked against the match:non-match ratio you configure.
> 
> 
> 
>> do we need other stuff with random acl to have it work with header directive ?
>> 
> 
> The ACL works as designed. You appear to have missed the fact that each
> check/test of the ACL uses a different randomly selected number.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190717/c9c5952c/attachment.htm>

From rousskov at measurement-factory.com  Wed Jul 17 13:05:43 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jul 2019 09:05:43 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
Message-ID: <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>

On 7/17/19 7:55 AM, --Ahmad-- wrote:

> indeed i read about reply header ACL That :

> all ACLs in
> the ACL list must be satisfied for the insertion to happen.

Amos is right, but the documentation you quote has nothing to do with
the fact that each reply_header_add rule is checked. That arguably
non-obvious behavior should be documented IMO. Quality pull requests
that enhance Squid documentation are welcomed on GitHub[1].


> what do i need to let the output single value and not
> multiple values

You are pushing against ACL limits, but it _is_ possible to restrict
further reply_header_add matches using modern Squid ACLs alone:

  acl markProcessed annotate_client processed=yes
  acl markedProcessed note processed yes

  acl p1in10 random 1/10
  acl p1in9  random 1/9
  ...

  reply_header_add Start "A" !markedProcessed p1in10 markProcessed
  reply_header_add Start "B" !markedProcessed p1in9  markProcessed
  ...

If you have a large number of possible Start values, then adding a Start
header using an eCAP adapter may be faster than checking so many ACLs. I
do not know what "large" means here, but I would not worry if you have
fewer than 100 values.


N.B. Please do not misinterpret my responses as an implication that what
you are doing overall is a good idea, or that there are no better ways
to accomplish the same goal. I am just answering specific questions in
case those answers would be useful for other use cases.


Cheers,

Alex.
[1] https://wiki.squid-cache.org/MergeProcedure


From squid at borrill.org.uk  Wed Jul 17 14:20:51 2019
From: squid at borrill.org.uk (Stephen Borrill)
Date: Wed, 17 Jul 2019 15:20:51 +0100
Subject: [squid-users] SOLVED - SECURITY ALERT: Host header forgery
	detected
Message-ID: <ef9b8067-8793-316e-1d61-f674df11e737@borrill.org.uk>

(old thread revived)

A common problem is with sites that have very short TTLs.

For instance login.live.com sometimes has a TTL of 60 seconds. The squid
server is using BIND as a recursive DNS resolver and clients are using
the same BIND instance too. All clients (iOS, Windows, Android)
sometimes use an old IP address and so you hit the Host header forgery
detected problem.

I can't see how to mitigate this problem.

On 16/05/18 12:05, Eliezer Croitoru wrote:
> Amos, 
> 
> And this issue is kind of big\mega corp services or CDN services. 
> Now I am really not sure I understand what this security host forgery is about. 
> There are couple cases: 
> - Simple forward proxy with ssl-bump which no header forgery should ever happen when the client requests for a specific domain and no IP 
> - Intercept proxy  with ssl-bump enabled that has no SNI host 
> - Intercept proxy with ssl-bump enabled that has SNI and squid passes the clients SNI host 
> 
> Which one of the above is this specific case? 
> And if there are other cases it's good to list them and I will try to wiki these details. 
> 
> Thanks, 
> Eliezer 
> 
> ---- 
> Eliezer Croitoru 
> Linux System Administrator 
> Mobile: +972-5-28704261 
> Email: [hidden email] 
> 
> 
> 
> -----Original Message----- 
> From: squid-users <[hidden email]> On Behalf Of Amos Jeffries 
> Sent: Tuesday, May 15, 2018 21:28 
> To: [hidden email] 
> Subject: Re: [squid-users] SOLVED - SECURITY ALERT: Host header forgery detected 
> 
> On 16/05/18 02:02, Eliezer Croitoru wrote:
> 
>> Hey Martin, 
>> 
>> Technically there should be a way to inform Squid-Cache about multiple addresses for the same destination. 
>> If Squid doesn't know that it's a real IP of the domains a partial solution is to use the same DNS service but it can also be something else. 
>> For example there should be a way\option for squid to decide if this address of the client or server is secured. 
>> 
>> Amos what do you think? 
>> Can a Host header forgery detection override acl be added? Should it be added? 
>> I believe that  if there are some properties to the remote certificate we can flag the service as "Secure" 
>> IE if the OS runs a "openssl s_client -host www.ubuntnu.com -connect 91.189.89.118:443 
>>  And the certificate is fine then... it's there is no place for any SECURITY ALERT.
> 
> A malicious actor would simply forward the TLS handshake to the real 
> server they are spoofing. Same way Squid does for SSL-Bump. 
> 
> The counter argument of not sending SNI to that suspicious server will 
> have failures with these exact same mega-corp services. Think 
> foo.example.com hosted on Google hosting where the generic server cert 
> is "foo.1e1.net" not "foo.example.com", nor even google.com". 
> 
> 
> The "problem" that needs to be resolved is simply that the genuine 
> servers do not have a reliable match between their IP and client 
> presented domain name(s). 
> 
> Amos 



From ahmed.zaeem at netstream.ps  Wed Jul 17 14:40:30 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 17 Jul 2019 17:40:30 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
Message-ID: <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>

Thanks Alex , i tried your acl not recognised !

2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'


do i need to recompile squid to enable this kind of ACLS ?




> On 17 Jul 2019, at 16:05, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> markProcessed

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190717/1bc5c3b2/attachment.htm>

From rousskov at measurement-factory.com  Wed Jul 17 15:27:26 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jul 2019 11:27:26 -0400
Subject: [squid-users] SOLVED - SECURITY ALERT: Host header forgery
 detected
In-Reply-To: <ef9b8067-8793-316e-1d61-f674df11e737@borrill.org.uk>
References: <ef9b8067-8793-316e-1d61-f674df11e737@borrill.org.uk>
Message-ID: <526a3ab6-460d-a6e3-dd0e-9f5ac57336bd@measurement-factory.com>

On 7/17/19 10:20 AM, Stephen Borrill wrote:

> A common problem is with sites that have very short TTLs.
> 
> For instance login.live.com sometimes has a TTL of 60 seconds. The squid
> server is using BIND as a recursive DNS resolver and clients are using
> the same BIND instance too. All clients (iOS, Windows, Android)
> sometimes use an old IP address and so you hit the Host header forgery
> detected problem.
> 
> I can't see how to mitigate this problem.

This problem can be mitigated by focusing not on stopping malicious
actors but on minimizing their negative effects. The following two steps
could help AFAICT:

1. When a host header forgery is suspected, allow the transaction
through but under a quarantine regime -- the transaction cannot write to
any cache and cannot read any non-public info. Squid could still warn
about its suspicions, and the admin can be given control over the
frequency of these warnings. Perhaps these warnings can be made
more/less prominent depending on the lack/presence of the confirmation
in #2 below.

2. If (and only if) Squid can validate the server as matching the
client-specified domain name (via the server certificate validation),
the quarantine regime in #1 can be lifted. This is similar to the
validation a client would have to do, of course. However, the client has
more info so sometimes Squid validation will work, and sometimes it will
fail.


Squid already implements portions of #1. No #2 aspects are supported IIRC.


Or we can just change Squid to give the admin control over the frequency
of these warnings but always muddle through with forwarding the
transaction despite known grave risks. We all know that, given a chance,
the vast majority of admins will simply disable warnings.

Alex.


>> -----Original Message----- 
>> From: squid-users <[hidden email]> On Behalf Of Amos Jeffries 
>> Sent: Tuesday, May 15, 2018 21:28 
>> To: [hidden email] 
>> Subject: Re: [squid-users] SOLVED - SECURITY ALERT: Host header forgery detected 

>> The "problem" that needs to be resolved is simply that the genuine 
>> servers do not have a reliable match between their IP and client 
>> presented domain name(s). 
>>
>> Amos 


From rousskov at measurement-factory.com  Wed Jul 17 15:36:49 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jul 2019 11:36:49 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
Message-ID: <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>

On 7/17/19 10:40 AM, --Ahmad-- wrote:

> 2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'

> do i need to recompile squid to enable this kind of ACLS ?

These ACLs are only supported in the development version of Squid
(future v5): https://github.com/squid-cache/squid/commit/63e82d8

Alex.


From ahmed.zaeem at netstream.ps  Wed Jul 17 16:05:49 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 17 Jul 2019 19:05:49 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
Message-ID: <B5E72BC4-D013-4447-A772-2894C5B90578@netstream.ps>

Hi Alex thanks for info .

well have compiled squid 5 
and i made exact steps as you mentioned .

now i have delayed responce with single header info .

but its wrong value ????.  not correct reply header !!!

so instead of  getting START A i see START B or E and so on .





> On 17 Jul 2019, at 18:36, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 7/17/19 10:40 AM, --Ahmad-- wrote:
> 
>> 2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'
> 
>> do i need to recompile squid to enable this kind of ACLS ?
> 
> These ACLs are only supported in the development version of Squid
> (future v5): https://github.com/squid-cache/squid/commit/63e82d8
> 
> Alex.



From ashleyshih at softnext.com.tw  Thu Jul 18 07:52:41 2019
From: ashleyshih at softnext.com.tw (Ashley)
Date: Thu, 18 Jul 2019 02:52:41 -0500 (CDT)
Subject: [squid-users] network problems with squid ssl-bump
Message-ID: <1563436361314-0.post@n4.nabble.com>

Hi all,

I have a couple of questions about squid 3.5. My company has set up a squid
proxy with sslbump functionality. There are more than 300 people in my
company and we are all intensive users of internet.

After we implemented our squid web proxy (with multiple instances) , we had
problems with our network. The internet becomes very slow and some of the
web pages are time out. We checked the cache.log and some error logs are
shown as follows.

2019/07/18 14:27:48 kid4| ERROR: Disconnecting from a helper that overflowed
32768-byte Squid input buffer: ssl_crtd #Hlpr33
019/07/18 14:59:34 kid6| Error negotiating SSL on FD 146: error:14077410:SSL
routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure (1/-1/0)
2019/07/18 14:59:36 kid3| Error negotiating SSL on FD 153:
error:00000000:lib(0):func(0):reason(0) (5/0/0)
2019/07/18 14:59:41 kid5| fqdncacheParse: No PTR record for '50.57.251.150'
2019/07/18 14:59:41 kid5| fqdncacheParse: No PTR record for '50.57.251.150'
2019/07/18 14:59:41 kid1| Failed to connect to nameserver 10.0.0.254 using
TCP.
2019/07/18 14:59:41 kid5| Error negotiating SSL on FD 82: error:14077438:SSL
routines:SSL23_GET_SERVER_HELLO:tlsv1 alert internal error (1/-1/0)
2019/07/18 14:59:43 kid5| Error negotiating SSL on FD 94: error:14077410:SSL
routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure (1/-1/0)
2019/07/18 14:59:43 kid6| fqdncacheParse: No PTR record for
'207.246.147.249'
2019/07/18 14:59:43 kid6| fqdncacheParse: No PTR record for
'207.246.147.249'
2019/07/18 14:59:49 kid2| Error negotiating SSL on FD 64:
error:00000000:lib(0):func(0):reason(0) (5/-1/54)


Our squid.conf is as follows.

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

cache_effective_user www

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

/usr/local/eaclhelper/CSQR_MyPortFWHelper.php
external_acl_type myPortFWFilter children-max=6 ttl=1 %SRC
/usr/local/eaclhelper/CSQR_MyPortFWHelper
acl myPortFW external myPortFWFilter
deny_info info_myPortFW myPortFW
http_access deny myPortFW

external_acl_type myAclFilter children-idle=3 children-startup=6
children-max=64 %SRC %SRCPORT %DST %PORT %URI
/usr/local/eaclhelper/CSQR_MyAclHelper
acl myAcl external myAclFilter
deny_info info_myAcl myAcl
http_access deny myAcl

http_access allow localnet
http_access allow localhost

http_access deny all

workers 6
sslproxy_session_cache_size 0

if ${process_number} = 7

http_port 127.0.0.1:3127 

else

http_port 127.0.0.${process_number}:3128
https_port 127.0.0.${process_number}:3129 intercept ssl-bump
cert=/usr/local/squid/ssl_cert/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB

if ${process_number} = 2


endif # endif for proc 2

if ${process_number} = 3

# For Debug, change /var/log/squid to your own log directory
# access_log /var/squid/logs/backend${process_number}.access.log
# cache_log /var/squid/logs/backend${process_number}.cache.log

endif # endif for proc 3


if ${process_number} = 4

# For Debug, change /var/log/squid to your own log directory
# access_log /var/squid/logs/backend${process_number}.access.log
# cache_log /var/squid/logs/backend${process_number}.cache.log

endif # endif for proc 4

endif # endif for squid coordinator

# ssl_bump ACL
acl broken_dstdom dstdomain
"/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_dstdom.default"
acl broken_dst dst
"/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_dst.default"
acl bump_dstdom dstdomain
"/home/www/htdocs/snims/lurk/sslTP_default_conf/sslbump_dstdom.default"
acl bump_dst dst
"/home/www/htdocs/snims/lurk/sslTP_default_conf/sslbump_dst.default"

always_direct allow all

# the following two options are unsafe and not always necessary:
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

# sslcrtd
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/squid/ssl_db -M
4MB
sslcrtd_children 32 startup=5 idle=3

# ecap_adapter
ecap_enable off
adaptation_send_client_ip on
loadable_modules /usr/local/ecap_adapter/lib/ecap_adapter_passthru.so
ecap_service eReqmod reqmod_precache 0
ecap://e-cap.org/ecap/services/sample/passthru 
ecap_service eRespmod respmod_precache 0
ecap://e-cap.org/ecap/services/sample/passthru 

adaptation_service_set reqFilter eReqmod
adaptation_service_set respFilter eRespmod
adaptation_access respFilter allow all
adaptation_access reqFilter allow all

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/squid/cache 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/squid/cache

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

## for squid.sh rotate , ??re-open
logfile_rotate 0

## sslBump ssl server name ACL
acl monitorSites ssl::server_name
"/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_sslserver.default"
acl monitorSites_define ssl::server_name
"/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_sslserver.define"
acl monitorSites_security ssl::server_name
"/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_sslserver.security"
acl monitorSites_security_IP dst
"/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_dst.security"
acl brokenSites ssl::server_name
"/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_sslserver.default"
acl brokenSites_define ssl::server_name
"/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_sslserver.define"

## sslBump acl setting for squid 3.5 
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all

ssl_bump splice broken_dstdom
ssl_bump splice broken_dst
ssl_bump splice brokenSites_define

# ssl_bump bump !brokenSites !brokenSites_define
ssl_bump bump monitorSites !brokenSites !brokenSites_define
ssl_bump bump monitorSites_define !brokenSites !brokenSites_define
ssl_bump bump monitorSites_security !brokenSites !brokenSites_define
ssl_bump bump monitorSites_security_IP !brokenSites !brokenSites_define

ssl_bump splice all

## sslproxy cert setting for squid 3.5
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslproxy_cert_adapt setValidAfter all

Can anyone advise what?s wrong with our settings? Or is there any limitation
for sslbump in terms of concurrent connections? Any suggestions and advice
will be highly appreciated.

Ashley






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From avo.andrinantenaina at gmail.com  Thu Jul 18 13:57:17 2019
From: avo.andrinantenaina at gmail.com (ANDRINANTENAINA Avo)
Date: Thu, 18 Jul 2019 16:57:17 +0300
Subject: [squid-users] squid time out
Message-ID: <CAEuwzCEJM4Z1HXqYpaqNSdOF1sJhF3QvA7Nn5293e8t1yZd4gQ@mail.gmail.com>

Dear all,



I have the following setup :

*# ../sbin/squid
-v
/usr/local/squid/etc*

*Squid Cache: Version 5.0.0-VCS*

*Service Name: squid*

*configure options:  '--with-logdir=/var/log/squid'
'--enable-auth-basic=LDAP,PAM,SMB,RADIUS'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-digest=LDAP,eDirectory' '--with-default-user=proxy'*

*#
/usr/local/squid/etc*



I have a huge range in terms of network, but awkwardly, the
authentication/ACL and everything works well in one given subnet but not on
the others. The users in the other subnets are not able to surf the
internet, and this without any specific logs from the proxy side ( the most
significant part of the config could be seen below). Any request from these
users just times out.



*#debug_options 29,9*

*#dns_nameservers 192.168.0.9 192.168.0.4*

*#connect_timeout 1  minute*

*debug_options ALL,9 11,3 20,3*

*### negotiate kerberos and ntlm authentication*

*auth_param negotiate program
/usr/local/squid/libexec/negotiate_wrapper_auth   -d --ntlm
/usr/local/samba/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp  --domain=BCM --kerberos
/usr/local/squid/libexec/ext_kerberos_sid_group_acl -d -s GSS_C_NO_NAME*

*auth_param negotiate children 60*

*auth_param negotiate keep_alive off*



*### pure ntlm authentication*

*auth_param ntlm program /usr/local/samba/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp  --domain=KATANA*

*auth_param ntlm children 60*

*auth_param ntlm keep_alive off*





*# warning: basic authentication sends passwords plaintext*

*# a network sniffer can and will discover passwords*

*auth_param basic program /usr/local/samba/bin/ntlm_auth
--helper-protocol=squid-2.5-basic*

*auth_param basic children 60*

*auth_param basic credentialsttl 4 hours*



*##*

*auth_param basic program /usr/local/squid/libexec/basic_ldap_auth  -R -b
"dc=KATANA,dc=LOCAL" -D simpleuser at katana.local -W
/usr/local/squid/etc/pass.txt -f sAMAccountName=%s -h 192.168.111.4*

*auth_param basic children 60*

*auth_param basic realm Banky Foibe*

*auth_param basic credentialsttl 1 minute*





*acl local0  dst  172.16.0.0/12 <http://172.16.0.0/12>*

*acl local1  dst  192.168.0.0/16 <http://192.168.0.0/16>*

*http_access allow local0 all*

*http_access allow local1 all*

*cache deny local1*

*cache deny local0*

*redirector_access deny local0*

*redirector_access deny local1*



*http_access deny !auth*

*http_access allow auth*

*#http_access deny all*

*http_port 8080*



I can?t really understand the issue, from the affected networks:

-          The user is able to ping the proxy and access its port 8080
(through telnet / netcat)

-          The request is able to reach the proxy but the in the access_log
the *?user? *is missing

*1563455060.396      1 192.168.230.195 TCP_DENIED/407 4714 GET
http://api.bing.com/qsml.aspx <http://api.bing.com/qsml.aspx>? -
HIER_NONE/- text/html*

-          TCP_DENIED/407, requesting the user to go through the
authentication phase is presented by the proxy to the user?s browser but
nothing happens. I thought that if the timer set to Kerberos, NTLM expires,
a pop up should appear but nothing (from wireshark)

*GET http://www.bing.com/favicon.ico <http://www.bing.com/favicon.ico>
HTTP/1.1*

*Accept: */**

*UA-CPU: AMD64*

*Accept-Encoding: gzip, deflate*

*User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0)
like Gecko*

*Host: www.bing.com <http://www.bing.com>*

*Proxy-Connection: Keep-Alive*



*HTTP/1.1 407 Proxy Authentication Required*

*Server: squid/5.0.0-VCS*

*Mime-Version: 1.0*

*Date: Thu, 18 Jul 2019 10:01:53 GMT*

*Content-Type: text/html;charset=utf-8*

*Content-Length: 3733*

*X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0*

*Vary: Accept-Language*

*Content-Language: en*

*Proxy-Authenticate: Negotiate*

*Proxy-Authenticate: NTLM*

*Proxy-Authenticate: Basic realm="KATANA - PERIMETER"*

*X-Cache: MISS from katana_proxy*

*Via: 1.1 lichtquanta (squid/5.0.0-VCS)*

*Connection: close*



-          On cache.log there is nothing that could mean something, just a
bunch of ARP error. Tried to debug the section 29 for authentication ? but
nothing. Checked the IE internet options, just in case the windows
authentication profile is no ticked ? but it is there.

I am lost so any help would really be appreciated.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190718/c9c20c9f/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul 18 14:59:13 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jul 2019 02:59:13 +1200
Subject: [squid-users] squid time out
In-Reply-To: <CAEuwzCEJM4Z1HXqYpaqNSdOF1sJhF3QvA7Nn5293e8t1yZd4gQ@mail.gmail.com>
References: <CAEuwzCEJM4Z1HXqYpaqNSdOF1sJhF3QvA7Nn5293e8t1yZd4gQ@mail.gmail.com>
Message-ID: <9b813ff3-23b3-c35a-8b40-403ee67053a5@treenet.co.nz>

On 19/07/19 1:57 am, ANDRINANTENAINA Avo wrote:
> 
> I have a huge range in terms of network, but awkwardly, the
> authentication/ACL and everything works well in one given subnet but not
> on the others. The users in the other subnets are not able to surf the
> internet, and this without any specific logs from the proxy side ( the
> most significant part of the config could be seen below). Any request
> from these users just times out. ?____
> 
...

> __?__
> 
> I can?t really understand the issue, from the affected networks:____
> 
> __-????????? __The user is able to ping the proxy and access its port
> 8080 (through telnet / netcat) ____
> 
> __-????????? __The request is able to reach the proxy but the in the
> access_log the /?user? /is missing ____
> 
> /1563455060.396????? 1 192.168.230.195 TCP_DENIED/407 4714 GET
> http://api.bing.com/qsml.aspx? - HIER_NONE/- text/html____/
> 
> __-????????? __TCP_DENIED/407, requesting the user to go through the
> authentication phase is presented by the proxy to the user?s browser but
> nothing happens. I thought that if the timer set to Kerberos, NTLM
> expires, a pop up should appear but nothing (from wireshark)____
> 

Er. Not sure what you mean by a timer.

The log entry is a reasonable first-request from any client. No sane
client will broadcast user credentials until it knows the receiving
agent needs them - and in what form they are needed.
 That is why your log entry has no username, and the purpose of the 407
status.

Once that 407 is delivered to the Browser that HTTP transaction is over.
If nothing happens afterwards that is a Browser or network layer
problem, nothing to do with Squid. (There are exceptions, but I see no
sign of those being relevant in your config).

Browser popup is what happens if the Browser is _unable_ to find
appropriate user credentials to send the proxy or web server needing
login. If it is able to find any Kerberors, NTLM or Basic auth
credentials to use (in that order of priority) - it will start a new
HTTP transaction using those. Which will be logged as a separate HTTP
transaction.
 But, if those credentials are not able to validate there may not be any
resulting username to log. Your wireshark trace shows no
Proxy-Authorization header in the request, so of course there will be no
username on that transactions log entry.


Setting the timeouts on credentials usability between the DC and the
Browsers will only cause credential tokens to become invalid before they
arrive at the proxy. That can lead to loops of transactions with 407 and
no username logged, especially with NTLM credentials.

Setting any of the auth related TTL or timeouts in squid.conf to short
values will only cause extra work for the auth validation process.
Slowing everything down. It has no effect on whether credentials are
valid, nor what the Browser does.

Despite the PR and marketing MS have done about single-sign-on being a
NTLM thing, it is actually a regular part of all HTTP authentication.
Seeing the popup is a *bad* sign, something is going wrong with the
Browsers auth setup if it has to be bothering the user for details.
 On Windows particularly the Browser should have access to the users
machine login or Kerberos keytab and so use one of those to access the
proxy without bothering or even being noticed by the user at all.

> 
> -????????? On cache.log there is nothing that could mean something, just
> a bunch of ARP error. Tried to debug the section 29 for authentication ?
> but nothing. Checked the IE internet options, just in case the windows
> authentication profile is no ticked ? but it is there.
> 

ARP errors may be nothing, or it could be a sign that your routing needs
something fixed.
 A routing problem might be affecting background connectivity for NTLM
and Kerberos processes the Browser has to do to allocate auth tokens
with DC.
 It might also effect the proxy verifying those tokens, but that would
have a different more obvious error logged.


If the above does not help your troubleshooting, please consider posting
your whole squid.conf.  (Without the #comment lines, and obfuscate
anything like cachemgr_passwd which should not be made public - but in a
way which ensures we can still tell eg that two IPs are different numbers).

Amos


From squid3 at treenet.co.nz  Thu Jul 18 15:29:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jul 2019 03:29:18 +1200
Subject: [squid-users] network problems with squid ssl-bump
In-Reply-To: <1563436361314-0.post@n4.nabble.com>
References: <1563436361314-0.post@n4.nabble.com>
Message-ID: <039788b5-8f00-6724-91f6-0353cf6e3728@treenet.co.nz>

On 18/07/19 7:52 pm, Ashley wrote:
> Hi all,
> 
> I have a couple of questions about squid 3.5. My company has set up a squid
> proxy with sslbump functionality. There are more than 300 people in my
> company and we are all intensive users of internet.
> 

The TLS environment is a very volatile place this past decade. Almost
every Squid release has some significant changes you will need for
SSL-Bump to work nicely or sometimes to work at all.

Due to this our official advice is to follow the latest release if you
need SSL-Bump. Especially if you encounter any problems, try the latest
and see if the problem disappears. Today that means Squid-4 (current
stable), or possibly even Squid-5 (cutting-edge, test well before use).

Squid-3.5.28 as bundled by some OS distributors can be reasonable *if*
it meets your needs. Which apparently it does not, and if you are
building your own Squid definitely build the latest version out.



> After we implemented our squid web proxy (with multiple instances) , we had
> problems with our network. The internet becomes very slow and some of the
> web pages are time out. We checked the cache.log and some error logs are
> shown as follows.
> 
> 2019/07/18 14:27:48 kid4| ERROR: Disconnecting from a helper that overflowed
> 32768-byte Squid input buffer: ssl_crtd #Hlpr33

That is a problem. The SSL-Bump cert generator cannot deliver the
intended fake cert back to Squid for delivery to a client.

Take a close look at the certs which are being mimic'd by this helper
and ensure they are less than 30KB in size. The 32KB buffer needs to
hold the cert plus a bunch of helper protocol values used by Squid.

If you have not done something that would make the cert be huge (eg a
server with lots of domain in its SubjectName). Then it may be a bug in
that helper, try newer Squid version.


The below look like side effects of the above. When no cert is able to
generate, negotiation will break.


> 019/07/18 14:59:34 kid6| Error negotiating SSL on FD 146: error:14077410:SSL
> routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure (1/-1/0)
> 2019/07/18 14:59:36 kid3| Error negotiating SSL on FD 153:
> error:00000000:lib(0):func(0):reason(0) (5/0/0)
> 2019/07/18 14:59:41 kid5| fqdncacheParse: No PTR record for '50.57.251.150'
> 2019/07/18 14:59:41 kid5| fqdncacheParse: No PTR record for '50.57.251.150'
> 2019/07/18 14:59:41 kid1| Failed to connect to nameserver 10.0.0.254 using
> TCP.
> 2019/07/18 14:59:41 kid5| Error negotiating SSL on FD 82: error:14077438:SSL
> routines:SSL23_GET_SERVER_HELLO:tlsv1 alert internal error (1/-1/0)
> 2019/07/18 14:59:43 kid5| Error negotiating SSL on FD 94: error:14077410:SSL
> routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure (1/-1/0)
> 2019/07/18 14:59:43 kid6| fqdncacheParse: No PTR record for
> '207.246.147.249'
> 2019/07/18 14:59:43 kid6| fqdncacheParse: No PTR record for
> '207.246.147.249'
> 2019/07/18 14:59:49 kid2| Error negotiating SSL on FD 64:
> error:00000000:lib(0):func(0):reason(0) (5/-1/54)
> 
> 
> Our squid.conf is as follows.
> 

...
> 
> cache_effective_user www
> 

Be Very Careful. That user account is a standard service name for web
servers. Squid is not a web server and should not be given access to
web-server related system resources. Likewise web servers should not be
given access to Squid internal state data (eg the SSL-Bump generated
security keys, cache contents, and shared memory sockets).

...
> 
> /usr/local/eaclhelper/CSQR_MyPortFWHelper.php
> external_acl_type myPortFWFilter children-max=6 ttl=1 %SRC
> /usr/local/eaclhelper/CSQR_MyPortFWHelper
> acl myPortFW external myPortFWFilter
> deny_info info_myPortFW myPortFW
> http_access deny myPortFW
> 
> external_acl_type myAclFilter children-idle=3 children-startup=6
> children-max=64 %SRC %SRCPORT %DST %PORT %URI
> /usr/local/eaclhelper/CSQR_MyAclHelper
> acl myAcl external myAclFilter
> deny_info info_myAcl myAcl
> http_access deny myAcl
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all
> 
> workers 6
> sslproxy_session_cache_size 0
> 
> if ${process_number} = 7
> 
> http_port 127.0.0.1:3127 
> 
> else
> 
> http_port 127.0.0.${process_number}:3128
> https_port 127.0.0.${process_number}:3129 intercept ssl-bump
> cert=/usr/local/squid/ssl_cert/myCA.pem generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> 
... <snip unnecessary config> ...

> 
> endif # endif for squid coordinator
> 
> # ssl_bump ACL
> acl broken_dstdom dstdomain
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_dstdom.default"
> acl broken_dst dst
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_dst.default"
> acl bump_dstdom dstdomain
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/sslbump_dstdom.default"
> acl bump_dst dst
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/sslbump_dst.default"
> 
> always_direct allow all

Please remove. You do not have cache_peer. The above was only ever
needed to workaround a very short-lived bug back in Squid-3.1.



> # the following two options are unsafe and not always necessary:
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> 

Please do not use DONT_VERIFY_PEER. All it does is prevent you (the
admin) from seeing what problems are happening.

The "allow all" for errors is also a bad idea. I suggest removing both
of these, then solving the error(s) that show up. yes you will see
errors happening, the idea is to look into them and see whether it is an
attack or network issue to fix before they become massive problems like
you are having now.


> # sslcrtd
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/squid/ssl_db -M
> 4MB
> sslcrtd_children 32 startup=5 idle=3
> 
> # ecap_adapter
> ecap_enable off
> adaptation_send_client_ip on
> loadable_modules /usr/local/ecap_adapter/lib/ecap_adapter_passthru.so
> ecap_service eReqmod reqmod_precache 0
> ecap://e-cap.org/ecap/services/sample/passthru 
> ecap_service eRespmod respmod_precache 0
> ecap://e-cap.org/ecap/services/sample/passthru 
> 
> adaptation_service_set reqFilter eReqmod
> adaptation_service_set respFilter eRespmod
> adaptation_access respFilter allow all
> adaptation_access reqFilter allow all
> 
...
> 
> ## sslBump ssl server name ACL
> acl monitorSites ssl::server_name
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_sslserver.default"
> acl monitorSites_define ssl::server_name
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_sslserver.define"
> acl monitorSites_security ssl::server_name
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_sslserver.security"
> acl monitorSites_security_IP dst
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/monitor_dst.security"
> acl brokenSites ssl::server_name
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_sslserver.default"
> acl brokenSites_define ssl::server_name
> "/home/www/htdocs/snims/lurk/sslTP_default_conf/no_sslbump_sslserver.define"
> 
> ## sslBump acl setting for squid 3.5 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3

NP: you are not using the step2 or step3 ACLs. They can be removed.

> ssl_bump peek step1 all

The "all" on the above line is pointless.

> 
> ssl_bump splice broken_dstdom
> ssl_bump splice broken_dst
> ssl_bump splice brokenSites_define
> 

After the above line anything matching brokenSites_define will be
spliced. OR is a server which _cannot_ be spliced.

So you can remove the redundant "!brokenSites_define" from the below lines.

Since you are following these below lines with a "splice all" line you
can greatly simplify the ACL processing time by adding this line right here:
 ssl_bump splice brokenSites

.. then removing all the "!brokenSites" below.


> ssl_bump bump monitorSites !brokenSites !brokenSites_define
> ssl_bump bump monitorSites_define !brokenSites !brokenSites_define
> ssl_bump bump monitorSites_security !brokenSites !brokenSites_define
> ssl_bump bump monitorSites_security_IP !brokenSites !brokenSites_define
> 
> ssl_bump splice all
> 
> ## sslproxy cert setting for squid 3.5
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslproxy_cert_adapt setValidAfter all
> 
> Can anyone advise what?s wrong with our settings? Or is there any limitation
> for sslbump in terms of concurrent connections? Any suggestions and advice
> will be highly appreciated.

Squid-3 SSL-Bump is limited to OpenSSL 1.0 capabilities. The bump action
is limited to the union of client capabilities, server capabilities,
Squid OpenSSL 1.0 capabilities, and the configured sslproxy_* restrictions.


Amos



From chip_pop at hotmail.com  Thu Jul 18 17:09:12 2019
From: chip_pop at hotmail.com (joseph)
Date: Thu, 18 Jul 2019 12:09:12 -0500 (CDT)
Subject: [squid-users] PartialResponsesCaching
Message-ID: <1563469752561-0.post@n4.nabble.com>

we AR in future and less caching by site move to https 
the only left mostly partial object or large file get disconnected half way
so caching range will help gain more HIT
any attempt in near future to make squid cache range file? 



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Jul 18 17:32:59 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jul 2019 13:32:59 -0400
Subject: [squid-users] PartialResponsesCaching
In-Reply-To: <1563469752561-0.post@n4.nabble.com>
References: <1563469752561-0.post@n4.nabble.com>
Message-ID: <c6e59e08-e477-8d18-3770-4291b5dc5eed@measurement-factory.com>

On 7/18/19 1:09 PM, joseph wrote:

> any attempt in near future to make squid cache range file? 

FWIW, I am not aware of any such attempts. Most likely, reliable range
caching should come on top of important caching code "infrastructure"
improvements that we are focusing on right now.

Alex.


From ahmed.zaeem at netstream.ps  Thu Jul 18 19:48:17 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 18 Jul 2019 22:48:17 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
Message-ID: <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>

Any recommendation alex ?

im sure 100 % i have made same as you asked but still i get wrong results .

i can see 1 result , but its wrong .


Thanks 


> On 17 Jul 2019, at 18:36, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 7/17/19 10:40 AM, --Ahmad-- wrote:
> 
>> 2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'
> 
>> do i need to recompile squid to enable this kind of ACLS ?
> 
> These ACLs are only supported in the development version of Squid
> (future v5): https://github.com/squid-cache/squid/commit/63e82d8
> 
> Alex.



From rousskov at measurement-factory.com  Thu Jul 18 20:08:49 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jul 2019 16:08:49 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
Message-ID: <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>

On 7/18/19 3:48 PM, --Ahmad-- wrote:
> Any recommendation alex ?

I recommend isolating the problem to the minimum number of transactions
(probably one or two in your case) and then posting your Squid
configuration, actual transaction headers, and an explanation why those
actual headers are wrong (and what headers you expected to see).

Alex.


>> On 17 Jul 2019, at 18:36, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>>
>> On 7/17/19 10:40 AM, --Ahmad-- wrote:
>>
>>> 2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'
>>
>>> do i need to recompile squid to enable this kind of ACLS ?
>>
>> These ACLs are only supported in the development version of Squid
>> (future v5): https://github.com/squid-cache/squid/commit/63e82d8
>>
>> Alex.



From ahmed.zaeem at netstream.ps  Thu Jul 18 22:15:24 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 19 Jul 2019 01:15:24 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
Message-ID: <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>

Ok , here we Go :

###########################################
dns_nameservers 1.0.0.1
acl markProcessed annotate_client processed=yes
acl markedProcessed note processed yes
#########################################
acl half10000 random 1/5
acl half10001 random 1/4
acl half10002 random 1/3
acl half10003 random 1/2
acl half10004 random 1/1
########################################
reply_header_add start "a" !markedProcessed half10000 markProcessed
reply_header_add start "B" !markedProcessed half10001 markProcessed
reply_header_add start "C" !markedProcessed half10002 markProcessed
reply_header_add start "D" !markedProcessed half10003 markProcessed
reply_header_add start "E" !markedProcessed half10004 markProcessed
#####################################################################
tcp_outgoing_address 12.13.200.10 half10000
tcp_outgoing_address 12.13.200.11 half10001
tcp_outgoing_address 12.13.200.12 half10002
tcp_outgoing_address 12.13.200.13 half10003
tcp_outgoing_address 12.13.200.14 half10004
#####################################################################






Curl Testing :


root:~ user$ curl -x 12.13.200.250:2000    -U testx:testx  ifconfig.io  -v
* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.200.250...
* TCP_NODELAY set
* Connected to 12.13.200.250 (12.13.200.250) port 2000 (#0)
* Proxy auth using Basic with user 'testx'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Thu, 18 Jul 2019 22:04:11 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: E
< 
12.13.200.12
* Connection #0 to host 12.13.200.250 left intact




root:~ user$ curl -x 12.13.200.250:2000    -U testx:testx  ifconfig.io  -v
* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.200.250...
* TCP_NODELAY set
* Connected to 12.13.200.250 (12.13.200.250) port 2000 (#0)
* Proxy auth using Basic with user 'testx'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Thu, 18 Jul 2019 22:04:12 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 40
< Connection: keep-alive
< start: B
< 
12.13.200.13
* Connection #0 to host 12.13.200.250 left intact




root:~ user$ curl -x 12.13.200.250:2000    -U testx:testx  ifconfig.io  -v
* Rebuilt URL to: ifconfig.io/
*   Trying 12.13.200.250...
* TCP_NODELAY set
* Connected to 12.13.200.250 (12.13.200.250) port 2000 (#0)
* Proxy auth using Basic with user 'testx'
> GET http://ifconfig.io/ HTTP/1.1
> Host: ifconfig.io
> Proxy-Authorization: Basic YmVuOmJlbg==
> User-Agent: curl/7.54.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
< HTTP/1.1 200 OK
< Date: Thu, 18 Jul 2019 22:04:13 GMT
< Content-Type: text/plain; charset=utf-8
< Content-Length: 38
< Connection: keep-alive
< start: a
< 
12.13.200.14
* Connection #0 to host 12.13.200.250 left intact
root:~ user$ 




Look @ results above i made 3 tests .


12.13.200.13 --> B
12.13.200.14 --> a
12.13.200.12 ---> E

And those are wrong ?.


above are wrong reply values , the correct should be as below based on the Acls we configured .



 12.13.200.13 --->D
 12.13.200.12 ---->C
 12.13.200.14  ---->E


i hope its clear now :)

Thanks and looking forward to hear from you .





> On 18 Jul 2019, at 23:08, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 7/18/19 3:48 PM, --Ahmad-- wrote:
>> Any recommendation alex ?
> 
> I recommend isolating the problem to the minimum number of transactions
> (probably one or two in your case) and then posting your Squid
> configuration, actual transaction headers, and an explanation why those
> actual headers are wrong (and what headers you expected to see).
> 
> Alex.
> 
> 
>>> On 17 Jul 2019, at 18:36, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>>> 
>>> On 7/17/19 10:40 AM, --Ahmad-- wrote:
>>> 
>>>> 2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'
>>> 
>>>> do i need to recompile squid to enable this kind of ACLS ?
>>> 
>>> These ACLs are only supported in the development version of Squid
>>> (future v5): https://github.com/squid-cache/squid/commit/63e82d8
>>> 
>>> Alex.
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190719/87a8c9f8/attachment.htm>

From anwer-ali at live.com  Fri Jul 19 01:35:52 2019
From: anwer-ali at live.com (M. Anwer Ali)
Date: Fri, 19 Jul 2019 01:35:52 +0000
Subject: [squid-users] Squid + ShadowSocks
In-Reply-To: <MN2PR15MB3502FB3929CE1F1E03D3180B85CB0@MN2PR15MB3502.namprd15.prod.outlook.com>
References: <MN2PR15MB3502FB3929CE1F1E03D3180B85CB0@MN2PR15MB3502.namprd15.prod.outlook.com>
Message-ID: <MN2PR15MB3502E1E1944B388EB74A989485CB0@MN2PR15MB3502.namprd15.prod.outlook.com>

Hi,

I have attached current setup of squid in the attachment. All the HTTP traffic is passing through squid. We are mostly using is for Web Filtering and its working fine.
Now we have a new addition to this setup, where we have installed Shadow Socks server at a remote location. Now we want squid to be working as it is working now, but if the end-user is opening a specific website lets say (abc.com), it should be forwarded to Shadow Socks server and from there to internet. If the end-user is directly connecting to ShadowSocks server (through SS client application) everything is working fine but then squid in not involved in it. So i want all the traffic to first go to squid (as in current scenario), then squid should see if the requested Website in abc.com, then it should forward it to ShadowSocks first and then shadow socks will send it to internet, and it takes same path on its way back(SS>Squid>Enduser)...

How can I achieve this...
Attachement Link:http://prntscr.com/oh2jch

Thanks...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190719/cd98ae85/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 19 02:44:06 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jul 2019 22:44:06 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
	acl random access list ?
In-Reply-To: <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
Message-ID: <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>

On July 18, 2019 18:15:30 --Ahmad--  wrote:


> reply_header_add start "a" !markedProcessed half10000 markProcessed

...

I would replace 1/1 random ACL with "all", but OK.


> tcp_outgoing_address 12.13.200.10 half10000

Strange: Your outgoing address decisions appear to be random, completely 
independent from your Start values. Is that what you want?


> 12.13.200.13 --> B
> 12.13.200.14 --> a
> 12.13.200.12 ---> E
>
>
> And those are wrong ?.
>
>
>
>
> above are wrong reply values , the correct should be as below based on the 
> Acls we configured .
>
>
>
>
>
>
> 12.13.200.13 --->D
> 12.13.200.12 ---->C
> 12.13.200.14  ---->E

I see nothing in your configuration that would tie outgoing address to 
Start values. Where did you configure Squid to use "D" for .13 or vice versa?

Alex.



>
>
>
>> On 18 Jul 2019, at 23:08, Alex Rousskov wrote:
>>
>> On 7/18/19 3:48 PM, --Ahmad-- wrote:
>>
>>> Any recommendation alex ?
>>
>> I recommend isolating the problem to the minimum number of transactions
>> (probably one or two in your case) and then posting your Squid
>> configuration, actual transaction headers, and an explanation why those
>> actual headers are wrong (and what headers you expected to see).
>>
>> Alex.
>>
>>
>>
>>>> On 17 Jul 2019, at 18:36, Alex Rousskov <rousskov at measurement-factory.com> 
>>>> wrote:
>>>>
>>>> On 7/17/19 10:40 AM, --Ahmad-- wrote:
>>>>
>>>>
>>>>> 2019/07/17 09:21:42| FATAL: Invalid ACL type ?annotate_client'
>>>>
>>>>
>>>>> do i need to recompile squid to enable this kind of ACLS ?
>>>>
>>>> These ACLs are only supported in the development version of Squid
>>>> (future v5): https://github.com/squid-cache/squid/commit/63e82d8
>>>>
>>>> Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190718/208894e6/attachment.htm>

From avo.andrinantenaina at gmail.com  Fri Jul 19 05:30:22 2019
From: avo.andrinantenaina at gmail.com (ANDRINANTENAINA Avo)
Date: Fri, 19 Jul 2019 08:30:22 +0300
Subject: [squid-users] squid time out
In-Reply-To: <mailman.3802.1563479175.3120.squid-users@lists.squid-cache.org>
References: <mailman.3802.1563479175.3120.squid-users@lists.squid-cache.org>
Message-ID: <CAEuwzCGf48jn7W-Lr+6+_mc+eKhT_1Dozb3BfjPe1spgOuFHBA@mail.gmail.com>

Hi Amos,

Thank you for your prompt reply.

As you said, the first request is hitting the proxy with the "user" field
empty, but there is no second request. And I was wrong about the "timer".
Please find below the config

*auth_param negotiate program
/usr/local/squid/libexec/negotiate_wrapper_auth   -d --ntlm
/usr/local/samba/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp  --domain=KATANA --kerberos
/usr/local/squid/libexec/ext_kerberos_sid_group_acl -d -s GSS_C_NO_NAME*

*auth_param negotiate children 60*

*auth_param negotiate keep_alive off*



*auth_param ntlm program /usr/local/samba/bin/ntlm_auth --diagnostics
--helper-protocol=squid-2.5-ntlmssp  --domain=KATANA*

*auth_param ntlm children 60*

*auth_param ntlm keep_alive off*



*auth_param basic program /usr/local/samba/bin/ntlm_auth
--helper-protocol=squid-2.5-basic*

*auth_param basic children 60*

*auth_param basic credentialsttl 4 hours*



*auth_param basic program /usr/local/squid/libexec/basic_ldap_auth  -R -b
"dc=KATANA,dc=LOCAL" -D katanauser at KATANA.LOCAL -W
/usr/local/squid/etc/pass.txt -f sAMAccountName=%s -h 192.168.111.40*

*auth_param basic children 60*

*auth_param basic realm Katana Local*

*auth_param basic credentialsttl 1 minute*



*acl auth proxy_auth REQUIRED*



*tcp_outgoing_address 0.0.0.0 all*

*dns_v4_first    on*



*acl mimeblock rep_mime_type ^application/x-shockwave-flash$*

*http_reply_access deny mimeblock*

*acl deny_rep_mime_flashvideo rep_mime_type video/flv*

*http_reply_access deny deny_rep_mime_flashvideo*



*acl local0  dst  172.16.0.0/12 <http://172.16.0.0/12>*

*acl local1  dst  192.168.0.0/16 <http://192.168.0.0/16>*

*http_access allow local0 all*

*http_access allow local1 all*

*cache deny local1*

*cache deny local0*

*redirector_access deny local0*

*redirector_access deny local1*



*http_access deny !auth*

*http_access allow auth*

*#http_access deny all*

*http_port 8080*



*debug_options 29,9*

*cache_swap_low 94*

*cache_swap_high 95*

*logfile_rotate 150*



*cache_dir aufs /media/STORAGE/cache 7000 16 256*

*cache_log  /media/STORAGE/ACCESS/cache.log*

*access_log /media/STORAGE/ACCESS/access.log*



*refresh_pattern ^ftp:    1440  20%  10080*

*refresh_pattern ^gopher:  1440  0%  1440*

*refresh_pattern -i (/cgi-bin/|\?) 0  0%  0*

*refresh_pattern .    0  20%  4320*



*acl allsrc src all*

*acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901  8080 3129
1025-65535*

*acl sslports port 443 563*



*acl purge method PURGE*

*acl connect method CONNECT*



*acl HTTP proto HTTP*

*acl HTTPS proto HTTPS*

*acl allowed_subnets src 192.168.0.0/16 <http://192.168.0.0/16>*

*http_access allow allowed_subnets*

*http_access allow manager localhost*



*http_access deny manager*

*http_access allow purge localhost*

*http_access deny purge*

*http_access deny !safeports*

*http_access deny CONNECT !sslports*



*http_access allow localhost*



*request_body_max_size 0 KB*

*delay_pools 1*

*delay_class 1 2*

*delay_parameters 1 -1/-1 -1/-1*

*delay_initial_bucket_level 100*

*delay_access 1 allow allsrc*



*http_access deny allsrc*



*acl max_user_ip_conn max_user_ip -s 1*

*http_access deny max_user_ip_conn*

*deny_info https://192.168.111.111/index3.html
<https://192.168.111.111/index3.html>  max_user_ip_conn*



*acl Java browser Java/1.4 Java/1.5 Java/1.6 Java/1.7 Java/1.8*

*http_access allow Java*



*url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -l /var/log/squid*

*url_rewrite_children 64 startup=16 idle=4 concurrency=0*

*debug_options 28,9*

*url_rewrite_children 10*





*icap_enable on*

*icap_send_client_ip on*

*icap_send_client_username on*

*icap_client_username_encode off*

*icap_client_username_header X-Authenticated-User*

*icap_preview_enable on*

*icap_preview_size 1024*

*icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1345/squidclamav <http://127.0.0.1:1345/squidclamav>*

*adaptation_access service_req allow all*

*icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1345/squidclamav <http://127.0.0.1:1345/squidclamav>*

*adaptation_access service_resp allow all*



Thank you




>
>
> Message: 1
> Date: Fri, 19 Jul 2019 02:59:13 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid time out
> Message-ID: <9b813ff3-23b3-c35a-8b40-403ee67053a5 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 19/07/19 1:57 am, ANDRINANTENAINA Avo wrote:
> >
> > I have a huge range in terms of network, but awkwardly, the
> > authentication/ACL and everything works well in one given subnet but not
> > on the others. The users in the other subnets are not able to surf the
> > internet, and this without any specific logs from the proxy side ( the
> > most significant part of the config could be seen below). Any request
> > from these users just times out.  ____
> >
> ...
>
> > __ __
> >
> > I can?t really understand the issue, from the affected networks:____
> >
> > __-          __The user is able to ping the proxy and access its port
> > 8080 (through telnet / netcat) ____
> >
> > __-          __The request is able to reach the proxy but the in the
> > access_log the /?user? /is missing ____
> >
> > /1563455060.396      1 192.168.230.195 TCP_DENIED/407 4714 GET
> > http://api.bing.com/qsml.aspx? - HIER_NONE/- text/html____/
> >
> > __-          __TCP_DENIED/407, requesting the user to go through the
> > authentication phase is presented by the proxy to the user?s browser but
> > nothing happens. I thought that if the timer set to Kerberos, NTLM
> > expires, a pop up should appear but nothing (from wireshark)____
> >
>
> Er. Not sure what you mean by a timer.
>
> The log entry is a reasonable first-request from any client. No sane
> client will broadcast user credentials until it knows the receiving
> agent needs them - and in what form they are needed.
>  That is why your log entry has no username, and the purpose of the 407
> status.
>
> Once that 407 is delivered to the Browser that HTTP transaction is over.
> If nothing happens afterwards that is a Browser or network layer
> problem, nothing to do with Squid. (There are exceptions, but I see no
> sign of those being relevant in your config).
>
> Browser popup is what happens if the Browser is _unable_ to find
> appropriate user credentials to send the proxy or web server needing
> login. If it is able to find any Kerberors, NTLM or Basic auth
> credentials to use (in that order of priority) - it will start a new
> HTTP transaction using those. Which will be logged as a separate HTTP
> transaction.
>  But, if those credentials are not able to validate there may not be any
> resulting username to log. Your wireshark trace shows no
> Proxy-Authorization header in the request, so of course there will be no
> username on that transactions log entry.
>
>
> Setting the timeouts on credentials usability between the DC and the
> Browsers will only cause credential tokens to become invalid before they
> arrive at the proxy. That can lead to loops of transactions with 407 and
> no username logged, especially with NTLM credentials.
>
> Setting any of the auth related TTL or timeouts in squid.conf to short
> values will only cause extra work for the auth validation process.
> Slowing everything down. It has no effect on whether credentials are
> valid, nor what the Browser does.
>
> Despite the PR and marketing MS have done about single-sign-on being a
> NTLM thing, it is actually a regular part of all HTTP authentication.
> Seeing the popup is a *bad* sign, something is going wrong with the
> Browsers auth setup if it has to be bothering the user for details.
>  On Windows particularly the Browser should have access to the users
> machine login or Kerberos keytab and so use one of those to access the
> proxy without bothering or even being noticed by the user at all.
>
> >
> > -          On cache.log there is nothing that could mean something, just
> > a bunch of ARP error. Tried to debug the section 29 for authentication ?
> > but nothing. Checked the IE internet options, just in case the windows
> > authentication profile is no ticked ? but it is there.
> >
>
> ARP errors may be nothing, or it could be a sign that your routing needs
> something fixed.
>  A routing problem might be affecting background connectivity for NTLM
> and Kerberos processes the Browser has to do to allocate auth tokens
> with DC.
>  It might also effect the proxy verifying those tokens, but that would
> have a different more obvious error logged.
>
>
> If the above does not help your troubleshooting, please consider posting
> your whole squid.conf.  (Without the #comment lines, and obfuscate
> anything like cachemgr_passwd which should not be made public - but in a
> way which ensures we can still tell eg that two IPs are different numbers).
>
> Amos
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190719/8fd8c023/attachment.htm>

From ahmed.zaeem at netstream.ps  Fri Jul 19 06:49:47 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 19 Jul 2019 09:49:47 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
Message-ID: <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>

Hi Alex .

Strange: Your outgoing address decisions appear to be random, completely independent from your Start values. Is that what you want?
yes , it suppose to have header as i configured the acls .


>  12.13.200.13 --->D
>  12.13.200.12 ---->C
>  12.13.200.14  ??>E

Not 

> 12.13.200.13 --> B
> 12.13.200.14 --> a
> 12.13.200.12 ---> E


I see nothing in your configuration that would tie outgoing address to Start values. Where did you configure Squid to use "D" for .13 or vice versa?
May im wrong in config , i thought that my config above like :


###########################################
dns_nameservers 1.0.0.1
acl markProcessed annotate_client processed=yes
acl markedProcessed note processed yes
#########################################
acl half10000 random 1/5

reply_header_add start "a" !markedProcessed half10000 markProcessed

tcp_outgoing_address 12.13.200.10 half10000


But may be im wrong with config and im open now to any suggestions to change the config to get it working as i mentioned above with headers .


Thanks 




> On 19 Jul 2019, at 5:44, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> Strange: Your outgoing address decisions appear to be random, completely independent from your Start values. Is that what you want?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190719/dd7f6213/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 19 10:04:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jul 2019 22:04:50 +1200
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
Message-ID: <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>

On 19/07/19 6:49 pm, --Ahmad-- wrote:

> 
> But may be im wrong with config and im open now to any suggestions to
> change the config to get it working as i mentioned above with headers .
> 

As I said at the end of my earlier mail:

"
You appear to have missed the fact that each
check/test of the ACL uses a different randomly selected number.
"


These:

>
>  reply_header_add start "a" !markedProcessed half10000 markProcessed
>
>  tcp_outgoing_address 12.13.200.10 half10000
>

... contain two different check/test of the ACL called half10000.

One for reply_header_add, another one for tcp_outgoing_address.

-> a random 1/5 of requests will have "Start: a" header added.

-> a random 1/5 of requests will try to send from 12.13.200.10 IP address.

The two sets likely do not overlap. Though since this is truly random -
there is a 2.5% chance that any request might *look* like what you are
seeking.


To make the IP based on the "a" existence you have to ... base it on the
"a" - not on some random number.


Amos


From ahmed.zaeem at netstream.ps  Fri Jul 19 12:53:56 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 19 Jul 2019 15:53:56 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
 <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>
Message-ID: <5D87B89D-BDC9-4188-8C21-42373CB42EB7@netstream.ps>

Hi Guys , Thank you a lot for your cooperation .

is there any way can i let header acl stop on the 1st MATCH ?

do you have any other thing can we do to achieve what im looking for based on my config below ?


Thanks 





> On 19 Jul 2019, at 13:04, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> To make the IP based on the "a" existence you have to ... base it on the
> "a" - not on some random number.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190719/837ccaf7/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 19 13:08:30 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Jul 2019 09:08:30 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <5D87B89D-BDC9-4188-8C21-42373CB42EB7@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
 <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>
 <5D87B89D-BDC9-4188-8C21-42373CB42EB7@netstream.ps>
Message-ID: <4f1f1d41-df11-03e5-caac-a63d47f4e268@measurement-factory.com>

On 7/19/19 8:53 AM, --Ahmad-- wrote:

> is there any way can i let header acl stop on the 1st MATCH ?

Yes, your reply_header_add ACLs effectively stop on the first match,
using the annotation trick. That part of your configuration is probably
working. The primary problem is elsewhere.


> do you have any other thing can we do to achieve what im looking for
> based on my config below ?

FWIW, I do not know what you are looking for. I even checked earlier
emails on this thread and could not find that information. Can you
(re)state your goals using the following template?

"When Squid receives a client request with HTTP header X, I want Squid
to forward that request using outgoing TCP address Y, and then add HTTP
header Z to the response that Squid sends to the client."

Replace X, Y, and Z with your actual requirements. Adjust as needed,
including removing any unnecessary parts.

Alex.


From ahmed.zaeem at netstream.ps  Fri Jul 19 18:54:25 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 19 Jul 2019 21:54:25 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <4f1f1d41-df11-03e5-caac-a63d47f4e268@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
 <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>
 <5D87B89D-BDC9-4188-8C21-42373CB42EB7@netstream.ps>
 <4f1f1d41-df11-03e5-caac-a63d47f4e268@measurement-factory.com>
Message-ID: <6F46B4C9-A083-494B-BF0E-286230E2B5F8@netstream.ps>

Alex .. indeed i asked many questions and you already solved me old issues . i do apologise for that Drop .
here is what we are going to achieve .


in simple :

i want to have external random addressees from list of addresses .
and in the same time i want a header like ?start? header  who can be sent from squid to Host with tag.

say i have 10 ips 
i want random external over them .
and i want single  on each those 10 ips be sent back to Host.
if external was ip1 , then ?start header? should be A
if external was ip2 , then ?start header? should be b
if external was ip3 , then ?start header? should be c
if external was ip4 , then ?start header? should be d

and so on .


Thanks and again Guys you have been much helpful .


Thanks 


> On 19 Jul 2019, at 16:08, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 7/19/19 8:53 AM, --Ahmad-- wrote:
> 
>> is there any way can i let header acl stop on the 1st MATCH ?
> 
> Yes, your reply_header_add ACLs effectively stop on the first match,
> using the annotation trick. That part of your configuration is probably
> working. The primary problem is elsewhere.
> 
> 
>> do you have any other thing can we do to achieve what im looking for
>> based on my config below ?
> 
> FWIW, I do not know what you are looking for. I even checked earlier
> emails on this thread and could not find that information. Can you
> (re)state your goals using the following template?
> 
> "When Squid receives a client request with HTTP header X, I want Squid
> to forward that request using outgoing TCP address Y, and then add HTTP
> header Z to the response that Squid sends to the client."
> 
> Replace X, Y, and Z with your actual requirements. Adjust as needed,
> including removing any unnecessary parts.
> 
> Alex.



From rafael.akchurin at diladele.com  Fri Jul 19 19:35:07 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 19 Jul 2019 19:35:07 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.8 (rebuilt with
 sslbump support from sources in Debian unstable)
Message-ID: <AM0PR04MB475327EF6DC2EBAEE0A2031A8FCB0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Greeting all,

The online repository with latest Squid 4.8 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid48.diladele.com.
Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation (look for feature-squid-4.8-1 branch).
Scripts for Ubuntu 16 will be updated in the near future.

Hope you will find this helpful. Note that older repo of squid46.diladele.com will be taken down in two years.

Best regards,
Rafael Akchurin
Diladele B.V.

P.S. Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid48.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid48.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient



--
Please take a look at another our project - DNS Safety filtering server.
Sort of Web Safety implemented as DNS Server.
Might be interesting in deployments where HTTPS decryption is not possible.
https://dnssafety.io/

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190719/d7608f6e/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 19 20:03:28 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Jul 2019 16:03:28 -0400
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <6F46B4C9-A083-494B-BF0E-286230E2B5F8@netstream.ps>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
 <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>
 <5D87B89D-BDC9-4188-8C21-42373CB42EB7@netstream.ps>
 <4f1f1d41-df11-03e5-caac-a63d47f4e268@measurement-factory.com>
 <6F46B4C9-A083-494B-BF0E-286230E2B5F8@netstream.ps>
Message-ID: <09b4dbd7-817d-3762-5e1c-0f2709a9059a@measurement-factory.com>

On 7/19/19 2:54 PM, --Ahmad-- wrote:

> say i have 10 ips 
> i want random external over them .

> if external was ip1 , then ?start header? should be A
> if external was ip2 , then ?start header? should be b
> if external was ip3 , then ?start header? should be c
> if external was ip4 , then ?start header? should be d

I can suggest two options. The first one is a little simpler, but it
uses actual IP addresses (e.g., "1.1.1.1") instead of IP address
labels/pseudonyms (e.g. "A") for Start header values:

  # select one of ten IPs using a uniform random distribution
  tcp_outgoing_address 12.13.200.10 p1in10
  tcp_outgoing_address 12.13.200.11 p1in9
  ...
  tcp_outgoing_address 12.13.200.19 all

  # tell the client what IP our to-server connection originated from
  reply_header_add Start "%<la"


If you do not really want to send the actual IP values in Start headers,
and are not worried that Squid may not actually use the selected
outgoing IP address for some reason, then you can use annotations to
mark specific tcp_outgoing_address decisions:

  acl markDecisionA annotate_client decision=A
  acl markDecisionB annotate_client decision=B
  ...
  acl markDecisionJ annotate_client decision=J

  # select one of ten IPs using a uniform random distribution
  # and remember our decision as a transaction annotation
  tcp_outgoing_address 12.13.200.10 p1in10 markDecisionA
  tcp_outgoing_address 12.13.200.11 p1in9 markDecisionB
  ...
  tcp_outgoing_address 12.13.200.19 markDecisionJ

  # relay our tcp_outgoing_address decision to the client
  reply_header_add Start "%note{decision}"


HTH,

Alex.


From ahmed.zaeem at netstream.ps  Fri Jul 19 21:01:33 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 20 Jul 2019 00:01:33 +0300
Subject: [squid-users] Possible to user reply_header_add directive with
 acl random access list ?
In-Reply-To: <09b4dbd7-817d-3762-5e1c-0f2709a9059a@measurement-factory.com>
References: <1366B0A4-770D-4855-A891-B1B213DE8ABD@netstream.ps>
 <ad0aa8c1-6d0b-66cc-5d54-27d156cdf53d@measurement-factory.com>
 <CA294BAF-83A0-4C14-9F0E-A219367640FB@netstream.ps>
 <2df28a3e-5b74-3a6c-6b6c-d4ace9e9c3d4@treenet.co.nz>
 <69E6FFF8-CDA3-4BA3-9759-5B0E7FDE2CDE@netstream.ps>
 <85dca8b3-9999-deb7-1278-1b14f600ecbf@measurement-factory.com>
 <CB6CF539-3F29-4EDA-B34A-DFDC98F4F30D@netstream.ps>
 <7fa3ca84-4a91-42ce-bc65-8f469e1d194b@measurement-factory.com>
 <BAF4F90D-E79A-4A06-907A-D3C6F8B894DA@netstream.ps>
 <4654a52f-d00e-4061-6bd5-ceb262c365f1@measurement-factory.com>
 <FCC8E7F9-09F9-4454-B041-11F1B8C50D01@netstream.ps>
 <16c081cfff0.277d.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <1F1951D5-B8F7-4293-A158-A33D3ACFDC08@netstream.ps>
 <ac6ffc1e-b15a-5c31-da63-cec653ba788e@treenet.co.nz>
 <5D87B89D-BDC9-4188-8C21-42373CB42EB7@netstream.ps>
 <4f1f1d41-df11-03e5-caac-a63d47f4e268@measurement-factory.com>
 <6F46B4C9-A083-494B-BF0E-286230E2B5F8@netstream.ps>
 <09b4dbd7-817d-3762-5e1c-0f2709a9059a@measurement-factory.com>
Message-ID: <FEA80E78-6455-4545-B693-774C6246839F@netstream.ps>

Alex you have been helpful a-lot .

i would appreciate your help & Amos for what you provided .

Thanks for your kind support .

you have simplified all what i need .


Kind regards 


> On 19 Jul 2019, at 23:03, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> reply_header_add Start "%<la"

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190720/1e0540f9/attachment.htm>

From tarotapprentice at yahoo.com  Sat Jul 20 05:19:11 2019
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 20 Jul 2019 05:19:11 +0000 (UTC)
Subject: [squid-users] caching apt package lists/Raspbian
References: <1454313392.2447317.1563599951391.ref@mail.yahoo.com>
Message-ID: <1454313392.2447317.1563599951391@mail.yahoo.com>

Recently upgraded to Raspbian Buster and squid 4.6. Since then I am unable to cache the Packages.xz that apt uses. The various other Pis using this proxy all end up downloading the 30MB Packages.xz every time. Does anyone have any suggestions on how to get it to cache?

Cheers
MarkJ


squid -v
Squid Cache: Version 4.6
Service Name: squid
Raspbian linux


access.log

1563597855.786??? 605 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 15306 GET http://raspbian.raspberrypi.org/raspbian/dists/buster/InRelease - HIER_DIRECT/93.93.128.193 -

1563597855.811??? 620 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 25429 GET http://archive.raspberrypi.org/debian/dists/buster/InRelease - HIER_DIRECT/93.93.128.133 -

1563597857.486??? 620 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 205801 GET http://archive.raspberrypi.org/debian/dists/buster/main/binary-armhf/Packages.gz - HIER_DIRECT/93.93.128.133 application/x-gzip

1563597936.436? 80026 192.168.1.73 TCP_MISS_ABORTED/200 2641974 GET http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-armhf/Packages.xz - HIER_DIRECT/93.93.128.193 application/x-xz


config file

acl localnet src 192.168.1.0/24 # internal network
acl localnet src fc00::/7?????? # RFC 4193 local private network range
acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged) machines
acl l500-020b src 192.168.1.20
acl SSL_ports port 443
acl Safe_ports port 80????????? # http
acl Safe_ports port 21????????? # ftp
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher
acl Safe_ports port 210???????? # wais
acl Safe_ports port 1025-65535? # unregistered ports
acl Safe_ports port 280???????? # http-mgmt
acl Safe_ports port 488???????? # gss-http
acl Safe_ports port 591???????? # filemaker
acl Safe_ports port 777???????? # multiling http
acl CONNECT method CONNECT
acl ads dstdomain .ad1.pamedia.com.au
acl ads dstdomain .ad3.pamedia.com.au
acl ads dstdomain .adevents.com.au
acl ads dstdomain .adinfinity.com.au
acl ads dstdomain .ads.excitehome.net.au
acl ads dstdomain .ads.fairfax.com.au
acl ads dstdomain .ads.godaddy.com
acl ads dstdomain .ads.google.com
acl ads dstdomain .ads.ninemsn.com.au
acl ads dstdomain .ads.optusnet.com.au
acl ads dstdomain .ads.property.com.au
acl ads dstdomain .ads.youtube.com
acl ads dstdomain .adserver.news.com.au
acl ads dstdomain .au.adserver.yahoo.com
acl ads dstdomain .doubleclick.net
acl ads dstdomain .googleadservices.com
acl ads dstdomain .zoomdirect.com.au
acl malware dstdomain am10.ru
acl malware dstdomain deepspacer.com
acl malware dstdomain trafficconverter.biz
acl malware dstdomain .eu.interia.pl
acl malware dstdomain .expo9.exponential.com
acl malware dstdomain .flashtalking.com
acl malware dstdomain .funad.co.kr
acl malware dstdomain .luckytime.co.kr
acl malware dstdomain .trafficholder.com
acl malware2 dst 96.43.128.194
acl hiddenwasp dst 103.206.122.245
acl hiddenwasp dst 103.206.123.13
acl hiddenwasp2 dstdomain http://103.206.123.13
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny ads
http_access deny malware
http_access deny malware2
http_access deny hiddenwasp
http_access deny hiddenwasp2
http_access allow l500-020b manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128
cache_mem 448 MB
maximum_object_size 320 MB
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
cache_dir aufs /var/spool/squid 18432 32 256
quick_abort_min -1 KB
client_request_buffer_max_size 128 KB
coredump_dir /var/spool/squid
access_log /var/log/squid/access.log squid
cache_log /var/log/squid/cache.log
logfile_rotate 7
netdb_filename none
refresh_pattern (\.deb|\.udeb)$ 1440??? 80%???? 10080
refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
refresh_pattern .?????????????? 0?????? 20%???? 4320
host_verify_strict on
max_filedescriptors 1200
dns_v4_first on
pinger_enable off
shutdown_lifetime 5 seconds


From ahmed.zaeem at netstream.ps  Sat Jul 20 10:49:09 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 20 Jul 2019 13:49:09 +0300
Subject: [squid-users] squid external address to be sequential from list of
	addresses
Message-ID: <760B0CD6-D09C-4F5B-94AA-ADDC7263FDD9@netstream.ps>

Hello Folks ,
wondering  ?. i need squid to have sequential outgoing addresses over specified list .

say i want an ip:port for that connection .
and have pool of 10 address .
is it possible with squid to match 1st ip as external for 1st request .
2nd ip for 2nd request .

3rd ip for 3rd request 

until reach end , then the cycle span again ?

im thinking of how can do it   with marking acl or so ?

Thanks  

From squid3 at treenet.co.nz  Sat Jul 20 14:36:05 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jul 2019 02:36:05 +1200
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <1454313392.2447317.1563599951391@mail.yahoo.com>
References: <1454313392.2447317.1563599951391.ref@mail.yahoo.com>
 <1454313392.2447317.1563599951391@mail.yahoo.com>
Message-ID: <89d5147f-0b4f-c609-6ca2-3837e9ec697b@treenet.co.nz>

On 20/07/19 5:19 pm, TarotApprentice wrote:
> Recently upgraded to Raspbian Buster and squid 4.6. Since then I am
unable to cache the Packages.xz that apt uses. The various other Pis
using this proxy all end up downloading the 30MB Packages.xz every time.
Does anyone have any suggestions on how to get it to cache?
> 
> Cheers MarkJ
> 

According to both Redbot and my manual check the object is only 12MB,
not 30MB. If you are getting 30MB somebody is interfering with that
download.


It should be caching by default. The redbot tool shows the site is
providing all the required cache headers and working perfectly for
revalidation. The REFRESH_UNMODIFIED log entries show that too.

The TCP_MISS_ABORTED indicates that for that log entry there was nothing
in cache (yet) for that URL, and the client aborted the transfer with
only 2.6MB fetched.



Can you try having just one Pi do its update and seeing if the .xz
object is cached afterwards?

Alternatively try the command:
  squidclient
http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-armhf/Packages.xz

It the object is cacheable, but your environment tends to have the Pi's
all fetching at the same time (eg before the first finishes), then you
may find collapsed_forwarding feature of use. That helps with caching
parallel fetches of objects.

Amos


> squid -v
> Squid Cache: Version 4.6
> Service Name: squid
> Raspbian linux
> 
> 
> access.log
> 
> 1563597855.786??? 605 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 15306 GET http://raspbian.raspberrypi.org/raspbian/dists/buster/InRelease - HIER_DIRECT/93.93.128.193 -
> 
> 1563597855.811??? 620 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 25429 GET http://archive.raspberrypi.org/debian/dists/buster/InRelease - HIER_DIRECT/93.93.128.133 -
> 
> 1563597857.486??? 620 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 205801 GET http://archive.raspberrypi.org/debian/dists/buster/main/binary-armhf/Packages.gz - HIER_DIRECT/93.93.128.133 application/x-gzip
> 
> 1563597936.436? 80026 192.168.1.73 TCP_MISS_ABORTED/200 2641974 GET http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-armhf/Packages.xz - HIER_DIRECT/93.93.128.193 application/x-xz
> 
> 
> config file
> 
...
> acl hiddenwasp2 dstdomain http://103.206.123.13

The above "http://" is not a valid domain name.

> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny ads
> http_access deny malware
> http_access deny malware2
> http_access deny hiddenwasp
> http_access deny hiddenwasp2
> http_access allow l500-020b manager
> http_access deny manager


'dst' ACL is quite slow and resource intensive. You should put these
manager rules above the "malware2" denial to protect against DoS better.

...
> http_port 3128
> cache_mem 448 MB
> maximum_object_size 320 MB
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> cache_dir aufs /var/spool/squid 18432 32 256
> quick_abort_min -1 KB
> client_request_buffer_max_size 128 KB

...

> refresh_pattern (\.deb|\.udeb)$ 1440??? 80%???? 10080
> refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
> refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
> refresh_pattern .?????????????? 0?????? 20%???? 4320


Amos


From squid3 at treenet.co.nz  Sat Jul 20 14:39:25 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jul 2019 02:39:25 +1200
Subject: [squid-users] Squid + ShadowSocks
In-Reply-To: <MN2PR15MB3502E1E1944B388EB74A989485CB0@MN2PR15MB3502.namprd15.prod.outlook.com>
References: <MN2PR15MB3502FB3929CE1F1E03D3180B85CB0@MN2PR15MB3502.namprd15.prod.outlook.com>
 <MN2PR15MB3502E1E1944B388EB74A989485CB0@MN2PR15MB3502.namprd15.prod.outlook.com>
Message-ID: <c26c0e01-d44f-6bdf-6ef3-22df5413c7bc@treenet.co.nz>

On 19/07/19 1:35 pm, M. Anwer Ali wrote:
> Hi,
> 
> I have attached current setup of squid in the attachment. All the HTTP
> traffic is passing through squid. We are mostly using is for Web
> Filtering and its working fine.
> Now we have a new addition to this setup, where we have installed Shadow
> Socks server at a remote location. Now we want squid to be working as it
> is working now, but if the end-user is opening a specific website lets
> say (abc.com), it should be forwarded to Shadow Socks server and from
> there to internet. If the end-user is directly connecting to ShadowSocks
> server (through SS client application) everything is working fine but
> then squid in not involved in it. So i want all the traffic to first go
> to squid (as in current scenario), then squid should see if the
> requested Website in abc.com, then it should forward it to ShadowSocks
> first and then shadow socks will send it to internet, and it takes same
> path on its way back(SS>Squid>Enduser)...
> 
> How can I achieve this...


To communicate with SOCKS servers Squid needs to be custom built with
SOCKS support.

See the 'existing state' section of
<https://wiki.squid-cache.org/Features/Socks>


Amos



From leomessi983 at yahoo.com  Sat Jul 20 15:07:56 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Sat, 20 Jul 2019 15:07:56 +0000 (UTC)
Subject: [squid-users] squdi access.log
References: <182198942.4739354.1563635276460.ref@mail.yahoo.com>
Message-ID: <182198942.4739354.1563635276460@mail.yahoo.com>

..HiWhy do I see multiple different lines in access.log file?Is every line a separate request?I used ssl-bump , peek at_step sslbump1 and then based on my ACL,I bump them or splice them!my squid.conf for log:logformat squid2?? %ts %{%Y %b %d %H:%M:%S}tl %>a %<a %<A %ru %>Hs %<Hs %ssl::bump_mode
 For example for google.com I see multiple lines in access.log:1563634658 2019 Jul 20 19:27:38? 40.0.0.40 - - 216.58.208.67:443 200 - splice
1563634658 2019 Jul 20 19:27:38? 40.0.0.40 - - 216.58.208.67:443 200 - splice
1563634659 2019 Jul 20 19:27:39? 40.0.0.40 - - 172.217.18.130:443 200 - splice
1563634659 2019 Jul 20 19:27:39? 40.0.0.40 - - 216.58.208.78:443 200 - splice
1563634659 2019 Jul 20 19:27:39? 40.0.0.40 - - 172.217.18.130:443 200 - splice
where is https:// google.com in the this log?

If i denied google , access.log shows:1563634748 2019 Jul 20 19:29:08? 40.0.0.40 172.217.18.130 googleads.g.doubleclick.net googleads.g.doubleclick.net:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 216.58.208.78 apis.google.com apis.google.com:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 172.217.18.130 adservice.google.com adservice.google.com:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 216.58.208.67 ssl.gstatic.com ssl.gstatic.com:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 216.58.208.67 www.gstatic.com www.gstatic.com:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 172.217.18.132 www.google.com www.google.com:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 172.217.18.132 www.google.com www.google.com:443 200 - splice
1563634748 2019 Jul 20 19:29:08? 40.0.0.40 172.217.18.132 172.217.18.132 172.217.18.132:443 200 - bump
1563634749 2019 Jul 20 19:29:09? 40.0.0.40 134.0.216.195 detectportal.firefox.com http://detectportal.firefox.com/success.txt 200 200 -
1563634749 2019 Jul 20 19:29:09? 40.0.0.40 - - 99.86.163.28:443 200 - splice


Thank you





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190720/218069e5/attachment.htm>

From rousskov at measurement-factory.com  Sat Jul 20 19:47:53 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 20 Jul 2019 15:47:53 -0400
Subject: [squid-users] squdi access.log
In-Reply-To: <182198942.4739354.1563635276460@mail.yahoo.com>
References: <182198942.4739354.1563635276460.ref@mail.yahoo.com>
 <182198942.4739354.1563635276460@mail.yahoo.com>
Message-ID: <68874060-9d9d-6fe7-7a10-8770f7066d10@measurement-factory.com>

On 7/20/19 11:07 AM, leomessi983 at yahoo.com wrote:

> Why do I see multiple different lines in access.log file?

I believe the following wiki page answers that question. Search for the
word "log" in the Processing Steps section.

  https://wiki.squid-cache.org/Features/SslPeekAndSplice

> Is every line a separate request?

The answer depends what you consider a "request" to be in this context.
Please see above URL for logging details.


> I used ssl-bump , peek at_step sslbump1 and then based on my ACL,I bump
> them or splice them! my squid.conf for log:
> logformat squid2?? %ts %{%Y %b %d %H:%M:%S}tl %>a %<a %<A %ru %>Hs %<Hs
> %ssl::bump_mode
> 
> For example for google.com I see multiple lines in access.log:
> 1563634658 2019 Jul 20 19:27:38? 40.0.0.40 - - 216.58.208.67:443 200 - splice
> 1563634658 2019 Jul 20 19:27:38? 40.0.0.40 - - 216.58.208.67:443 200 - splice
> 1563634659 2019 Jul 20 19:27:39? 40.0.0.40 - - 172.217.18.130:443 200 - splice
> 1563634659 2019 Jul 20 19:27:39? 40.0.0.40 - - 216.58.208.78:443 200 - splice
> 1563634659 2019 Jul 20 19:27:39? 40.0.0.40 - - 172.217.18.130:443 200 - splice
> 
> where is https:// google.com in the this log?

At step1, Squid cannot see the URLs you expect. And Squid does not see
the HTTP request if you tell it to splice during step2. You can try
logging %ssl::>sni and %ssl::<cert_subject. See their documentation in
squid.conf.documented.

To see the HTTP request, Squid has to bump the connection.


> If i denied google , access.log shows:

If you deny access, Squid bumps the client connection and, if that
bumping is successful, receives the HTTP request.

Alex.


From rousskov at measurement-factory.com  Sat Jul 20 20:09:33 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 20 Jul 2019 16:09:33 -0400
Subject: [squid-users] squid external address to be sequential from list
	of addresses
In-Reply-To: <760B0CD6-D09C-4F5B-94AA-ADDC7263FDD9@netstream.ps>
References: <760B0CD6-D09C-4F5B-94AA-ADDC7263FDD9@netstream.ps>
Message-ID: <c69bcf50-9c61-e965-c8f9-9f347ca1a7b3@measurement-factory.com>

On 7/20/19 6:49 AM, --Ahmad-- wrote:

> i need squid to have sequential outgoing addresses over specified list .
> 
> say i want an ip:port for that connection .
> and have pool of 10 address .
> is it possible with squid to match 1st ip as external for 1st request .
> 2nd ip for 2nd request . 
> 3rd ip for 3rd request 
> 
> until reach end , then the cycle span again ?
> 
> im thinking of how can do it   with marking acl or so ?

If you need to round-robin outgoing addresses for requests on the same
persistent connection, then a long list of annotation ACLs may be
sufficient (but awkward!) way of doing that. If you understand how
annotations helped avoid multiple matches in your random-based
configuration, you may be able to build the corresponding configuration
for this use case as well. If not, please post your failed attempts with
analysis why you think they should have worked and ask questions -- this
will avoid the impression of abusing the mailing list for doing your own
legwork.

If you need to round-robin outgoing address across all cache misses,
then you will probably have to either write an external ACL (that sets
the right annotation) or add a new "round robin" ACL type to Squid
sources. The latter requires development, of course. I recommend
starting with the external ACL if you need this functionality (and later
optimize with a new built-in ACL type if really needed).

In all of these use cases, please note that you are configuring which
outgoing address Squid should use. The order of those
decisions/assignments may not match the order in which Squid opens new
connections and/or sends requests. Connection opening delays and other
factors may change the order, deviating from the strict round-robin rules.


HTH,

Alex.


From tarotapprentice at yahoo.com  Sun Jul 21 04:20:47 2019
From: tarotapprentice at yahoo.com (Mark James)
Date: Sun, 21 Jul 2019 14:20:47 +1000
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <89d5147f-0b4f-c609-6ca2-3837e9ec697b@treenet.co.nz>
References: <1454313392.2447317.1563599951391.ref@mail.yahoo.com>
 <1454313392.2447317.1563599951391@mail.yahoo.com>
 <89d5147f-0b4f-c609-6ca2-3837e9ec697b@treenet.co.nz>
Message-ID: <45004701-F8F9-47F1-9D45-007ECBE81D6A@yahoo.com>

Doing an ?apt update? on the squid machine got another TCP_MISS_ABORTED for ::1 and then subsequent IPv4 requests from other Pis get the TCP_REQUEST_UNMODIFIED.

Packages.xz was 13MB.


> On 21 Jul 2019, at 12:36 am, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 20/07/19 5:19 pm, TarotApprentice wrote:
>> Recently upgraded to Raspbian Buster and squid 4.6. Since then I am
> unable to cache the Packages.xz that apt uses. The various other Pis
> using this proxy all end up downloading the 30MB Packages.xz every time.
> Does anyone have any suggestions on how to get it to cache?
>> 
>> Cheers MarkJ
>> 
> 
> According to both Redbot and my manual check the object is only 12MB,
> not 30MB. If you are getting 30MB somebody is interfering with that
> download.
> 
> 
> It should be caching by default. The redbot tool shows the site is
> providing all the required cache headers and working perfectly for
> revalidation. The REFRESH_UNMODIFIED log entries show that too.
> 
> The TCP_MISS_ABORTED indicates that for that log entry there was nothing
> in cache (yet) for that URL, and the client aborted the transfer with
> only 2.6MB fetched.
> 
> 
> 
> Can you try having just one Pi do its update and seeing if the .xz
> object is cached afterwards?
> 
> Alternatively try the command:
>  squidclient
> http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-armhf/Packages.xz
> 
> It the object is cacheable, but your environment tends to have the Pi's
> all fetching at the same time (eg before the first finishes), then you
> may find collapsed_forwarding feature of use. That helps with caching
> parallel fetches of objects.
> 
> Amos
> 
> 
>> squid -v
>> Squid Cache: Version 4.6
>> Service Name: squid
>> Raspbian linux
>> 
>> 
>> access.log
>> 
>> 1563597855.786    605 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 15306 GET http://raspbian.raspberrypi.org/raspbian/dists/buster/InRelease - HIER_DIRECT/93.93.128.193 -
>> 
>> 1563597855.811    620 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 25429 GET http://archive.raspberrypi.org/debian/dists/buster/InRelease - HIER_DIRECT/93.93.128.133 -
>> 
>> 1563597857.486    620 192.168.1.73 TCP_REFRESH_UNMODIFIED/200 205801 GET http://archive.raspberrypi.org/debian/dists/buster/main/binary-armhf/Packages.gz - HIER_DIRECT/93.93.128.133 application/x-gzip
>> 
>> 1563597936.436  80026 192.168.1.73 TCP_MISS_ABORTED/200 2641974 GET http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-armhf/Packages.xz - HIER_DIRECT/93.93.128.193 application/x-xz
>> 
>> 
>> config file
>> 
> ...
>> acl hiddenwasp2 dstdomain http://103.206.123.13
> 
> The above "http://" is not a valid domain name.
> 
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access deny ads
>> http_access deny malware
>> http_access deny malware2
>> http_access deny hiddenwasp
>> http_access deny hiddenwasp2
>> http_access allow l500-020b manager
>> http_access deny manager
> 
> 
> 'dst' ACL is quite slow and resource intensive. You should put these
> manager rules above the "malware2" denial to protect against DoS better.
> 
> ...
>> http_port 3128
>> cache_mem 448 MB
>> maximum_object_size 320 MB
>> memory_replacement_policy lru
>> cache_replacement_policy heap LFUDA
>> cache_dir aufs /var/spool/squid 18432 32 256
>> quick_abort_min -1 KB
>> client_request_buffer_max_size 128 KB
> 
> ...
> 
>> refresh_pattern (\.deb|\.udeb)$ 1440    80%     10080
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Jul 21 04:42:56 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jul 2019 16:42:56 +1200
Subject: [squid-users] squid time out
In-Reply-To: <CAEuwzCGf48jn7W-Lr+6+_mc+eKhT_1Dozb3BfjPe1spgOuFHBA@mail.gmail.com>
References: <mailman.3802.1563479175.3120.squid-users@lists.squid-cache.org>
 <CAEuwzCGf48jn7W-Lr+6+_mc+eKhT_1Dozb3BfjPe1spgOuFHBA@mail.gmail.com>
Message-ID: <d3f19f48-26db-a1af-7ee6-4fc510758837@treenet.co.nz>

On 19/07/19 5:30 pm, ANDRINANTENAINA Avo wrote:
> Hi Amos,?
> 
> Thank you for your prompt reply.
> 
> As you said, the first request is hitting the proxy with the "user"
> field empty, but there is no second request. And I was wrong about the
> "timer".?
> Please find below the config 

I'm not seeing anything obvious. Though its very hard to read black text
on very dark grey background.

Perhapse it is related to the delays necessary for clamav scanning to
occur or ufdbguard rules.

Amos


From squid3 at treenet.co.nz  Sun Jul 21 04:39:01 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jul 2019 16:39:01 +1200
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <45004701-F8F9-47F1-9D45-007ECBE81D6A@yahoo.com>
References: <1454313392.2447317.1563599951391.ref@mail.yahoo.com>
 <1454313392.2447317.1563599951391@mail.yahoo.com>
 <89d5147f-0b4f-c609-6ca2-3837e9ec697b@treenet.co.nz>
 <45004701-F8F9-47F1-9D45-007ECBE81D6A@yahoo.com>
Message-ID: <cc298ed6-d0ff-ad85-c04a-bd488eb1238a@treenet.co.nz>

On 21/07/19 4:20 pm, Mark James wrote:
> Doing an ?apt update? on the squid machine got another TCP_MISS_ABORTED for ::1 and then subsequent IPv4 requests from other Pis get the TCP_REQUEST_UNMODIFIED.
> 

That hints that there is something broken in your local network IPv6
connectivity. Perhapse ICMPv6 is not working properly?

Amos


From tarotapprentice at yahoo.com  Sun Jul 21 07:08:04 2019
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 21 Jul 2019 07:08:04 +0000 (UTC)
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <cc298ed6-d0ff-ad85-c04a-bd488eb1238a@treenet.co.nz>
References: <1454313392.2447317.1563599951391.ref@mail.yahoo.com>
 <1454313392.2447317.1563599951391@mail.yahoo.com>
 <89d5147f-0b4f-c609-6ca2-3837e9ec697b@treenet.co.nz>
 <45004701-F8F9-47F1-9D45-007ECBE81D6A@yahoo.com>
 <cc298ed6-d0ff-ad85-c04a-bd488eb1238a@treenet.co.nz>
Message-ID: <488909992.2609923.1563692884646@mail.yahoo.com>

It whatever Raspbian and the router do by default, although I do use an iptables firewall. I normally don't see any IPv6 from the other Pis, so maybe something to do with localhost and the loopback interface.

Cheers






On Sunday, 21 July 2019, 2:45:59 pm AEST, Amos Jeffries <squid3 at treenet.co.nz> wrote: 





On 21/07/19 4:20 pm, Mark James wrote:
> Doing an ?apt update? on the squid machine got another TCP_MISS_ABORTED for ::1 and then subsequent IPv4 requests from other Pis get the TCP_REQUEST_UNMODIFIED.
> 

That hints that there is something broken in your local network IPv6
connectivity. Perhapse ICMPv6 is not working properly?


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From service.mv at gmail.com  Mon Jul 22 14:37:09 2019
From: service.mv at gmail.com (Service MV)
Date: Mon, 22 Jul 2019 11:37:09 -0300
Subject: [squid-users] HAProxy + Squid
Message-ID: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>

Hello everyone, I would like to know if the configuration I want to do is
viable:
1 Load Balancer HAProxy configured in TCP mode.
2 Squid servers 4.7.2 with negotiate kerberos authentication and LDAP group
authorizations.
The idea is that the web clients of my lan point to the IP/Name of the Load
Balancer and that this distributes the load between the proxy servers.
Attached is a link to a configuration diagram.
https://cloudcraft.co/view/00ccd7cb-861c-4e70-a38e-980fdd6cfad3?key=iEa-Gyp8R0ZSh-fxDNi58A
Thank you very much in advance for your comments.
Best regards

Gabriel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190722/0c6ba915/attachment.htm>

From rafael.akchurin at diladele.com  Mon Jul 22 18:27:54 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 22 Jul 2019 18:27:54 +0000
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
Message-ID: <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Gabriel,

We do exactly that in our lab, see docs at https://docs.diladele.com/administrator_guide_7_0/active_directory_extra/redundancy/haproxy_proxy_protocol.html
It works perfectly.

Best regards,
Rafael Akchurin
Diladele B.V.



From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Service MV
Sent: Monday, July 22, 2019 4:37 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] HAProxy + Squid

Hello everyone, I would like to know if the configuration I want to do is viable:
1 Load Balancer HAProxy configured in TCP mode.
2 Squid servers 4.7.2 with negotiate kerberos authentication and LDAP group authorizations.
The idea is that the web clients of my lan point to the IP/Name of the Load Balancer and that this distributes the load between the proxy servers.
Attached is a link to a configuration diagram.
https://cloudcraft.co/view/00ccd7cb-861c-4e70-a38e-980fdd6cfad3?key=iEa-Gyp8R0ZSh-fxDNi58A
Thank you very much in advance for your comments.
Best regards

Gabriel

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190722/36e32a0c/attachment.htm>

From zby at post.cz  Mon Jul 22 19:53:38 2019
From: zby at post.cz (zby at post.cz)
Date: Mon, 22 Jul 2019 21:53:38 +0200 (CEST)
Subject: [squid-users] squid 4 fails to authenticate using NTLM
Message-ID: <F3P.gEvA.Jlcjmo2mJw.1TDXH2@seznam.cz>


My problem:? my browser keeps on prompting for authentication.

Facts:




Debian 10 x86_64


squid-4.6 + samba-4.9



joined AD using "net ads join -U ...". OK.

wbinfo -t : OK

wbinfo -P or -p : OK

wbinfo -i userXYZ : returns data (OK)

wbinfo -g (well, fails to "deliver", too many users?)

smbclient -U userXYZ //host/share : works, logs me in




wbinfo -a domain\\user%pass:

plaintext password authentication succeeded
challenge/response password authentication failed




sqadmin at host13:~$ ntlm_auth --helper-protocol=squid-2.5-ntlmssp --domain=ad
001
userw01 Passwd001
SPNEGO request [userw01 Passwd001] invalid prefix
BH SPNEGO request invalid prefix




squid/cache.log:

.....

2019/07/22 17:39:31.252 kid1| 11,2| client_side.cc(1323) parseHttpRequest: 
HTTP Client REQUEST:
---------
CONNECT www.bing.com:443 HTTP/1.0
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like 
Gecko
Host: www.bing.com
Proxy-Authorization: NTLM TlRMTVNTUAADAAAAGAAYAIwAAABOAU4
BpAAAAAoACgBYAAAAEAAQAGIAAAAa....
DNT: 1
Proxy-Connection: Keep-Alive
Pragma: no-cache
Content-Length: 0


----------
2019/07/22 17:39:31.253 kid1| 29,9| UserRequest.cc(57) valid: Validating 
Auth::UserRequest '0x55eb35131d80'.
2019/07/22 17:39:31.253 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x55eb35131d80'.
2019/07/22 17:39:31.253 kid1| 29,9| UserRequest.cc(65) authenticated: user 
not fully authenticated.
2019/07/22 17:39:31.253 kid1| 29,9| UserRequest.cc(332) authenticate: header
NTLM TlRMTVNTUAADAAAAGAAYAIwAAABOAU4.....

...

2019/07/22 17:39:31.256 kid1| 29,9| UserRequest.cc(254) authenticate: auth 
state ntlm failed. NTLM TlRMTVNTUAADAAAAGAA....







Please advise.

Thank you.

Zbynek




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190722/0e44c709/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 23 09:01:06 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Jul 2019 21:01:06 +1200
Subject: [squid-users] squid 4 fails to authenticate using NTLM
In-Reply-To: <F3P.gEvA.Jlcjmo2mJw.1TDXH2@seznam.cz>
References: <F3P.gEvA.Jlcjmo2mJw.1TDXH2@seznam.cz>
Message-ID: <3c892a5d-ac09-1eab-0772-e074dc462095@treenet.co.nz>

On 23/07/19 7:53 am, zby wrote:
> My problem:? my browser keeps on prompting for authentication.
> Facts:
> 
> Debian 10 x86_64
> squid-4.6 + samba-4.9
> joined AD using "net ads join -U ...". OK.
> wbinfo -t : OK
> wbinfo -P or -p : OK
> wbinfo -i userXYZ : returns data (OK)
> wbinfo -g (well, fails to "deliver", too many users?)
> smbclient -U userXYZ //host/share : works, logs me in

This is irrelevant to Squid. It only tells that the user account has
filesystem access privileges. Nothing about web access privileges, or
whether the *Squid* user account has access to authenticate user logins.


> 
> wbinfo -a domain\\user%pass:
> plaintext password authentication succeeded

 "plaintext" means Basic authentication.

> challenge/response password authentication failed
> 

Challenge/Response could mean anything auth related.


> sqadmin at host13:~$ ntlm_auth --helper-protocol=squid-2.5-ntlmssp
> --domain=ad001
> userw01 Passwd001
> SPNEGO request [userw01 Passwd001] invalid prefix
> BH SPNEGO request invalid prefix
> 

"userw01 Passwd001" is not a SPNEGO token.

see
<https://wiki.squid-cache.org/Features/AddonHelpers#Negotiate_and_NTLM_Scheme>

Pass the helper the "KK" request command and the token you see in the
HTTP headers. For example:

KK TlRMTVNTUAADAAAAGAAYAIwAAABOAU4BpAAAAAoACgBYAAAAEAAQAGIAAAAa...



Amos


From ahmed.zaeem at netstream.ps  Tue Jul 23 14:16:05 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 23 Jul 2019 17:16:05 +0300
Subject: [squid-users] error:transaction-end-before-headers on squid 5.x
Message-ID: <A770B108-CBCE-4C31-8009-D4042DD87F05@netstream.ps>

Hello folks .

recently i moved to squid 5 to get some features .

but i have new errors such as :

error:transaction-end-before-headers

in access log file .


is it related to bug ?


Thanks 


From zby at post.cz  Tue Jul 23 14:34:16 2019
From: zby at post.cz (zby at post.cz)
Date: Tue, 23 Jul 2019 16:34:16 +0200 (CEST)
Subject: [squid-users] squid 4 fails to authenticate using NTLM
References: <F3P.gEvA.Jlcjmo2mJw.1TDXH2@seznam.cz>
 <3c892a5d-ac09-1eab-0772-e074dc462095@treenet.co.nz>
Message-ID: <G5z.gEwW.1RwgBOkkTAn.1TDnhe@seznam.cz>


echo "KK TlRMTVNTUAADAAAAGAAYAIwA....." | ntlm_auth --helper-protocol=squid-
2.5-ntlmssp --domain=DOM1


NA NT_STATUS_INVALID_PARAMETER




---------------------------------------


squid.conf snippet:

...


## Authentication of NTLM:
auth_param ntlm program /usr/bin/ntlm_auth --diagnostics --helper-protocol=
squid-2.5-ntlmssp --domain=DOM1
auth_param ntlm children 100 startup=10

auth_param ntlm keep_alive off

external_acl_type ad_group ttl=600 children-max=200 %LOGIN /usr/lib/squid/
ext_wbinfo_group_acl

...

##No other auth scheme.

----------------------------------------------


## /var/lib/samba:

drwxr-x---? 2 root winbindd_priv?? 4096 Jul 23 15:30 winbindd_privileged






Zbynek








---------- P?vodn? e-mail ----------
Od: Amos Jeffries <squid3 at treenet.co.nz>
Komu: squid-users at lists.squid-cache.org
Datum: 23. 7. 2019 11:03:37
P?edm?t: Re: [squid-users] squid 4 fails to authenticate using NTLM 
"On 23/07/19 7:53 am, zby wrote:
> My problem:? my browser keeps on prompting for authentication.
> Facts:
> 
> Debian 10 x86_64
> squid-4.6 + samba-4.9
> joined AD using "net ads join -U ...". OK.
> wbinfo -t : OK
> wbinfo -P or -p : OK
> wbinfo -i userXYZ : returns data (OK)
> wbinfo -g (well, fails to "deliver", too many users?)
> smbclient -U userXYZ //host/share : works, logs me in

This is irrelevant to Squid. It only tells that the user account has
filesystem access privileges. Nothing about web access privileges, or
whether the *Squid* user account has access to authenticate user logins.


> 
> wbinfo -a domain\\user%pass:
> plaintext password authentication succeeded

"plaintext" means Basic authentication.

> challenge/response password authentication failed
> 

Challenge/Response could mean anything auth related.


> sqadmin at host13:~$ ntlm_auth --helper-protocol=squid-2.5-ntlmssp
> --domain=ad001
> userw01 Passwd001
> SPNEGO request [userw01 Passwd001] invalid prefix
> BH SPNEGO request invalid prefix
> 

"userw01 Passwd001" is not a SPNEGO token.

see
<https://wiki.squid-cache.org/Features/AddonHelpers#Negotiate_and_NTLM_
Scheme>

Pass the helper the "KK" request command and the token you see in the
HTTP headers. For example:

KK TlRMTVNTUAADAAAAGAAYAIwAAABOAU4BpAAAAAoACgBYAAAAEAAQAGIAAAAa...



Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190723/86421503/attachment.htm>

From rousskov at measurement-factory.com  Tue Jul 23 14:41:21 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Jul 2019 10:41:21 -0400
Subject: [squid-users] error:transaction-end-before-headers on squid 5.x
In-Reply-To: <A770B108-CBCE-4C31-8009-D4042DD87F05@netstream.ps>
References: <A770B108-CBCE-4C31-8009-D4042DD87F05@netstream.ps>
Message-ID: <117419e2-2177-b5b7-0f79-15e019c23ecc@measurement-factory.com>

On 7/23/19 10:16 AM, --Ahmad-- wrote:
> Hello folks .
> 
> recently i moved to squid 5 to get some features .
> 
> but i have new errors such as :
> 
> error:transaction-end-before-headers
> 
> in access log file .
> 
> 
> is it related to bug ?
> 
> 
> Thanks 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Jul 23 14:46:31 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Jul 2019 10:46:31 -0400
Subject: [squid-users] error:transaction-end-before-headers on squid 5.x
In-Reply-To: <A770B108-CBCE-4C31-8009-D4042DD87F05@netstream.ps>
References: <A770B108-CBCE-4C31-8009-D4042DD87F05@netstream.ps>
Message-ID: <a029045d-5afd-abb0-3ccb-d8d54724bc44@measurement-factory.com>

On 7/23/19 10:16 AM, --Ahmad-- wrote:
> 
> recently i moved to squid 5 to get some features .
> 
> but i have new errors such as :
> 
> error:transaction-end-before-headers
> 
> in access log file .
> 
> 
> is it related to bug ?

>From [1]: Unused connections received in http_port or https_port or
transactions terminated before reading[parsing] request headers are
logged with URI error:transaction-end-before-headers.

  [1] http://www.squid-cache.org/Doc/config/access_log/

Whether those unused (from HTTP point of view) connections are a bug
somewhere depends on your environment. Usually, they are just benign
noise (rather than a true/serious bug or a DoS attack) but YMMV.

This log records appeared in Squid v4.


HTH,

Alex.
P.S. Sorry for my previous response being empty.


From zby at post.cz  Tue Jul 23 16:20:54 2019
From: zby at post.cz (zby at post.cz)
Date: Tue, 23 Jul 2019 18:20:54 +0200 (CEST)
Subject: [squid-users] squid 4 fails to authenticate using NTLM
References: <F3P.gEvA.Jlcjmo2mJw.1TDXH2@seznam.cz>
 <3c892a5d-ac09-1eab-0772-e074dc462095@treenet.co.nz>
Message-ID: <GCs.gEwk.1sJBAe1aZ0y.1TDpFc@seznam.cz>


I found one more thing in the cache.log:

Got user=[user1] domain=[DOM1] workstation=[machine1] len1=24 len2=334
Login for user [DOM1\[user1]@[machine1 failed due to [Reading winbind reply 
failed!]
ntlmssp_server_auth_send: Checking NTLMSSP password for DOM1\user1 failed: 
NT_STATUS_UNSUCCESSFUL
gensec_update_done: ntlmssp[0x55713e452900]: NT_STATUS_UNSUCCESSFUL
GENSEC login failed: NT_STATUS_UNSUCCESSFUL




Why failed?

/var/lib/samba:


drwxr-x---? 2 root winbindd_priv?? 4096 Jul 23 18:09 winbindd_privileged

/var/run/samba:

drwxr-xr-x 2 root root???? 60 Jul 23 18:09 winbindd




If I chmod to anything else than expected winbindd fails to start 
complaining about an unexpected dir mode.

The dir modes remain the same as "defined" in the debian package.

ntlm_auth --username=user1 run as a regular user results in: "NT_STATUS_OK: 
The operation completed successfully. (0x0)"

It should fail if not allowed to read from winbind, I suppose.




Thanks.

Zb









---------- P?vodn? e-mail ----------
Od: Amos Jeffries <squid3 at treenet.co.nz>
Komu: squid-users at lists.squid-cache.org
Datum: 23. 7. 2019 11:03:37
P?edm?t: Re: [squid-users] squid 4 fails to authenticate using NTLM 
"On 23/07/19 7:53 am, zby wrote:
> My problem:? my browser keeps on prompting for authentication.
> Facts:
> 
> Debian 10 x86_64
> squid-4.6 + samba-4.9
> joined AD using "net ads join -U ...". OK.
> wbinfo -t : OK
> wbinfo -P or -p : OK
> wbinfo -i userXYZ : returns data (OK)
> wbinfo -g (well, fails to "deliver", too many users?)
> smbclient -U userXYZ //host/share : works, logs me in

This is irrelevant to Squid. It only tells that the user account has
filesystem access privileges. Nothing about web access privileges, or
whether the *Squid* user account has access to authenticate user logins.


> 
> wbinfo -a domain\\user%pass:
> plaintext password authentication succeeded

"plaintext" means Basic authentication.

> challenge/response password authentication failed
> 

Challenge/Response could mean anything auth related.


> sqadmin at host13:~$ ntlm_auth --helper-protocol=squid-2.5-ntlmssp
> --domain=ad001
> userw01 Passwd001
> SPNEGO request [userw01 Passwd001] invalid prefix
> BH SPNEGO request invalid prefix
> 

"userw01 Passwd001" is not a SPNEGO token.

see
<https://wiki.squid-cache.org/Features/AddonHelpers#Negotiate_and_NTLM_
Scheme>

Pass the helper the "KK" request command and the token you see in the
HTTP headers. For example:

KK TlRMTVNTUAADAAAAGAAYAIwAAABOAU4BpAAAAAoACgBYAAAAEAAQAGIAAAAa...



Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190723/b67eedfb/attachment.htm>

From zby at post.cz  Wed Jul 24 07:02:34 2019
From: zby at post.cz (zby at post.cz)
Date: Wed, 24 Jul 2019 09:02:34 +0200 (CEST)
Subject: [squid-users] squid 4 fails to authenticate using NTLM
References: <F3P.gEvA.Jlcjmo2mJw.1TDXH2@seznam.cz>
 <3c892a5d-ac09-1eab-0772-e074dc462095@treenet.co.nz>
 <GCs.gEwk.1sJBAe1aZ0y.1TDpFc@seznam.cz>
Message-ID: <GrR.gEwg.26io2fXxHCh.1TE0AA@seznam.cz>


Good morning (CEST).

Solved for NTLM.

I added the squid user to the group winbindd_priv as described in "man ntml_
auth". Well, I just overlooked it.

Thanks all for reading/thinking/help.

Zbynek










---------- P?vodn? e-mail ----------
Od: zby at post.cz
Komu: Amos Jeffries <squid3 at treenet.co.nz>
Datum: 23. 7. 2019 18:24:13
P?edm?t: Re: [squid-users] squid 4 fails to authenticate using NTLM 
"
I found one more thing in the cache.log:

Got user=[user1] domain=[DOM1] workstation=[machine1] len1=24 len2=334
Login for user [DOM1\[user1]@[machine1 failed due to [Reading winbind reply 
failed!]
ntlmssp_server_auth_send: Checking NTLMSSP password for DOM1\user1 failed: 
NT_STATUS_UNSUCCESSFUL
gensec_update_done: ntlmssp[0x55713e452900]: NT_STATUS_UNSUCCESSFUL
GENSEC login failed: NT_STATUS_UNSUCCESSFUL




Why failed?

/var/lib/samba:


drwxr-x---? 2 root winbindd_priv?? 4096 Jul 23 18:09 winbindd_privileged

/var/run/samba:

drwxr-xr-x 2 root root???? 60 Jul 23 18:09 winbindd




If I chmod to anything else than expected winbindd fails to start 
complaining about an unexpected dir mode.

The dir modes remain the same as "defined" in the debian package.

ntlm_auth --username=user1 run as a regular user results in: "NT_STATUS_OK: 
The operation completed successfully. (0x0)"

It should fail if not allowed to read from winbind, I suppose.




Thanks.

Zb









---------- P?vodn? e-mail ----------
Od: Amos Jeffries <squid3 at treenet.co.nz>
Komu: squid-users at lists.squid-cache.org
Datum: 23. 7. 2019 11:03:37
P?edm?t: Re: [squid-users] squid 4 fails to authenticate using NTLM 
"On 23/07/19 7:53 am, zby wrote:
> My problem:? my browser keeps on prompting for authentication.
> Facts:
> 
> Debian 10 x86_64
> squid-4.6 + samba-4.9
> joined AD using "net ads join -U ...". OK.
> wbinfo -t : OK
> wbinfo -P or -p : OK
> wbinfo -i userXYZ : returns data (OK)
> wbinfo -g (well, fails to "deliver", too many users?)
> smbclient -U userXYZ //host/share : works, logs me in

This is irrelevant to Squid. It only tells that the user account has
filesystem access privileges. Nothing about web access privileges, or
whether the *Squid* user account has access to authenticate user logins.


> 
> wbinfo -a domain\\user%pass:
> plaintext password authentication succeeded

"plaintext" means Basic authentication.

> challenge/response password authentication failed
> 

Challenge/Response could mean anything auth related.


> sqadmin at host13:~$ ntlm_auth --helper-protocol=squid-2.5-ntlmssp
> --domain=ad001
> userw01 Passwd001
> SPNEGO request [userw01 Passwd001] invalid prefix
> BH SPNEGO request invalid prefix
> 

"userw01 Passwd001" is not a SPNEGO token.

see
<https://wiki.squid-cache.org/Features/AddonHelpers#Negotiate_and_NTLM_
Scheme>

Pass the helper the "KK" request command and the token you see in the
HTTP headers. For example:

KK TlRMTVNTUAADAAAAGAAYAIwAAABOAU4BpAAAAAoACgBYAAAAEAAQAGIAAAAa...



Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
"_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
"
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190724/b47b3166/attachment.htm>

From devilindisguisedevlin at gmail.com  Thu Jul 25 05:41:25 2019
From: devilindisguisedevlin at gmail.com (Devilindisguise)
Date: Thu, 25 Jul 2019 00:41:25 -0500 (CDT)
Subject: [squid-users] Squid Cache Problem
Message-ID: <1564033285452-0.post@n4.nabble.com>

Hello all

Let me preface this by stating I am far from being a Squid expert so please
bear with me.

We have what is probably an easy one. Some Windows servers use a locally
installed Squid proxy instance for all outbound traffic. These servers also
make use of some F5 GTM (DNS) servers to provide a resilient inter-DC DNS
topology.

Essentially what should happen is under steady state conditions any DNS
request should be given IP address a.a.a.a, then under failure be given
b.b.b.b. The GTM DNS TTL is 30 seconds.

What we?re finding is that even after 5 mins of failure any HTTP request
from IE (configured with the Squid proxy) still targets a.a.a.a and traffic
is dropped. During this period if we remove the Squid proxy from the IE
settings, it works as now we target b.b.b.b. 

So clearly some sort of caching, possibly DNS, is being done on the Squid. 

Where is a good place to start on Squid to troubleshoot this,

Thank you 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Thu Jul 25 06:55:06 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 25 Jul 2019 08:55:06 +0200
Subject: [squid-users] Squid Cache Problem
In-Reply-To: <1564033285452-0.post@n4.nabble.com>
References: <1564033285452-0.post@n4.nabble.com>
Message-ID: <20190725065506.GA14953@fantomas.sk>

On 25.07.19 00:41, Devilindisguise wrote:
>We have what is probably an easy one. Some Windows servers use a locally
>installed Squid proxy instance for all outbound traffic. These servers also
>make use of some F5 GTM (DNS) servers to provide a resilient inter-DC DNS
>topology.
>
>Essentially what should happen is under steady state conditions any DNS
>request should be given IP address a.a.a.a, then under failure be given
>b.b.b.b. The GTM DNS TTL is 30 seconds.
>
>What we?re finding is that even after 5 mins of failure any HTTP request
>from IE (configured with the Squid proxy) still targets a.a.a.a and traffic
>is dropped. During this period if we remove the Squid proxy from the IE
>settings, it works as now we target b.b.b.b.
>
>So clearly some sort of caching, possibly DNS, is being done on the Squid.

One of main points of DNS design is to be cacheable.
That is why DNS is not suited for load balancing and failover switching.

however, you should be able to look at content of DNS cache in squid using
cachemgr.cgi to see what's wrong there.

also, you can sniff the DNS traffic to see if only proper responses are
going to squid.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
REALITY.SYS corrupted. Press any key to reboot Universe.


From sebastiaan.wijker at sbdinc.com  Thu Jul 25 10:00:16 2019
From: sebastiaan.wijker at sbdinc.com (gswijker)
Date: Thu, 25 Jul 2019 05:00:16 -0500 (CDT)
Subject: [squid-users] VoIP Software trouble
Message-ID: <1564048816044-0.post@n4.nabble.com>

Hello,
 
I have trouble with a VoIP software. It can't connect to the VoIP server.
Log of working software:
09:14:56.3	Telephony device	Loading
09:14:56.5	Telephony device	License: CRM
09:15:01.0	Telephony device	Connecting
09:15:01.1	Telephony device	Method: XSI-HTTPS
09:15:01.1	Telephony device	Connecting
09:15:01.1	Telephony device	Verified: CN=*.interact.mtel.eu, OU=PositiveSSL
Wildcard, OU=Domain Control Validated
09:15:01.2	Telephony device	Version: 21.0
09:15:01.5	Telephony device	Channel started
09:15:01.5	Telephony device	Fetching directory: Extensions
09:15:01.9	Telephony device	2 common records
09:15:02.0	Telephony device	0 personal records
09:15:02.0	Telephony device	127 records downloaded, 0 seconds
09:15:02.6	Telephony device	Service pack found: OBT CONNECTOR CRM
09:15:02.6	Telephony device	Service pack: CRM
09:15:02.6	Telephony device	License: CRM

Log of not working software:
10:13:44.5	Telephony device	Loading
10:13:44.6	Telephony device	License: CRM
10:13:49.1	Telephony device	Connecting
10:13:49.2	Telephony device	Method: XSI-HTTPS
10:13:49.2	Telephony device	Connecting

OS: Debian Linux 9 v19.36
Squid Proxy Server v3.5

tail -f /var/log/squid/access.log:
1564047457.829  65109 10.1.10.224 TAG_NONE/503 0 CONNECT
clients.interact.mtel.eu:443 - HIER_NONE/- -

/etc/squid/squid.conf:
acl all src all
acl localhost src 127.0.0.1/32
acl localnet src 10.1.9.0/24
acl localnet src 10.1.10.0/24
acl localnet src 172.19.142.0/24

icp_port 3130
icp_access allow all

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 2208 # Evolve OBT
acl Safe_ports port 2209 # Evolve OBT
acl CONNECT method CONNECT
acl HTTPS proto HTTPS

http_access allow all
http_access allow localnet
http_access allow localhost
http_access deny all

http_port 3128
http_port 3130

dns_v4_first on

ssl_bump peek all
ssl_bump splice all

http_access allow localhost manager
http_access deny manager

http_port 3128
cache_mem 1048 MB
cache_dir ufs /var/spool/squid 100 16 256

access_log /var/log/squid/combined.log combined
access_log /var/log/squid/access.log squid
cache_log /var/log/squid/debug.log
coredump_dir /var/spool/squid
strip_query_terms off

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

cache_effective_user proxy
check_hostnames off
cache_effective_group proxy

I have search for months, but I can't find the solution.
Can someone help me? I'm a linux novice, so do it step by step, please.

Best regards,
gswijker 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From devilindisguisedevlin at gmail.com  Thu Jul 25 10:40:11 2019
From: devilindisguisedevlin at gmail.com (Devilindisguise)
Date: Thu, 25 Jul 2019 05:40:11 -0500 (CDT)
Subject: [squid-users] Squid Cache Problem
In-Reply-To: <20190725065506.GA14953@fantomas.sk>
References: <1564033285452-0.post@n4.nabble.com>
 <20190725065506.GA14953@fantomas.sk>
Message-ID: <1564051211129-0.post@n4.nabble.com>

Great, thank you.

We'll take a look at the DNS cache and see what we find.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Jul 25 13:33:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Jul 2019 09:33:09 -0400
Subject: [squid-users] VoIP Software trouble
In-Reply-To: <1564048816044-0.post@n4.nabble.com>
References: <1564048816044-0.post@n4.nabble.com>
Message-ID: <b8ba9feb-58c5-5c22-516d-5df668b140a8@measurement-factory.com>

On 7/25/19 6:00 AM, gswijker wrote:

> Squid Proxy Server v3.5
...
> ssl_bump peek all
> ssl_bump splice all

Please upgrade to Squid v4 (at least) or stop using SslBump features,
depending on whether you actually need SslBump features. And if you do
need SslBump features, then you must configure http_port(s) accordingly.


> tail -f /var/log/squid/access.log:
> 1564047457.829  65109 10.1.10.224 TAG_NONE/503 0 CONNECT
> clients.interact.mtel.eu:443 - HIER_NONE/- -

The primary question is why is your Squid responding with a 503 error to
the CONNECT request? Perhaps Squid cannot resolve
clients.interact.mtel.eu domain name? You can see Squid error response
(that may have more details) in a packet capture (or, probably, in
cache.log after setting debug_options to ALL,2).


> http_access allow all
> http_access allow localnet
> http_access allow localhost
> http_access deny all
...
> http_access allow localhost manager
> http_access deny manager


This combination does not make sense. The very first rule is the only
one that will work, potentially turning your Squid into an open proxy.
However, this is not the reason for those 503 errors.


> http_port 3128
> http_port 3130
...
> http_port 3128

One http_port directive per port/address, please. Perhaps you are not
looking at cache.log errors/warnings? They are often useful.


> I'm a linux novice, so do it step by step, please.

Sorry, the above is all I had time for. If you need more detailed
instructions, then hopefully somebody on the list can give them to you.


HTH,

Alex.


From kakoimiku at gmail.com  Mon Jul 29 02:46:34 2019
From: kakoimiku at gmail.com (kmiku7)
Date: Sun, 28 Jul 2019 21:46:34 -0500 (CDT)
Subject: [squid-users] Why `Storage Mem capacity` has a value larger than
	100%.
Message-ID: <1564368394287-0.post@n4.nabble.com>

Hello
My squid consume too much memory at startup time grow.
>From manager cgi /squid-internal-mgr/info, I saw the `Storage Mem capacity`
in Cache information for squid is 2429.7%, larger than 100%. In my opinion,
this value should be less or equal to 100%. I have searched but find nothing
about it.
Can someone tell me why? And how to limit squid memory usage, to avoid
memory grow as startup time grow and handle more and more request.
Thanks~.
The value of `cache_mem` option is 256M.

The following is outputs:

curl http://127.0.0.1:155/squid-internal-mgr/info
Squid Object Cache: Version 4.4
Build Info:
Service Name: squidHot
Start Time:	Thu, 13 Feb 2019 06:37:48 GMT
Current Time:	Wed, 17 Jul 2019 10:46:26 GMT
Connection information for squid:
	Number of clients accessing cache:	(client_db off)
	Number of HTTP requests received:	562562
	Number of ICP messages received:	11385
	Number of ICP messages sent:	11421
	Number of queued ICP replies:	0
	Number of HTCP messages received:	0
	Number of HTCP messages sent:	0
	Request failure ratio:	 0.00
	Average HTTP requests per minute since start:	2.6
	Average ICP messages per minute since start:	0.1
	Select loop called: 2360855132 times, 5.604 ms avg
Cache information for squid:
	Hits as % of all requests:	5min: 100.0%, 60min: 98.9%
	Hits as % of bytes sent:	5min: -0.0%, 60min: 90.6%
	Memory hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Disk hits as % of hit requests:	5min: 100.0%, 60min: 83.3%
	Storage Swap size:	62529876 KB
	Storage Swap capacity:	 1.5% used, 98.5% free
*	Storage Mem size:	6369400 KB
	Storage Mem capacity:	2429.7% used, -2329.7% free*
	Mean Object Size:	68338.66 KB
	Requests given to unlinkd:	3



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sebastiaan.wijker at sbdinc.com  Mon Jul 29 07:56:20 2019
From: sebastiaan.wijker at sbdinc.com (gswijker)
Date: Mon, 29 Jul 2019 02:56:20 -0500 (CDT)
Subject: [squid-users] VoIP Software trouble
In-Reply-To: <b8ba9feb-58c5-5c22-516d-5df668b140a8@measurement-factory.com>
References: <1564048816044-0.post@n4.nabble.com>
 <b8ba9feb-58c5-5c22-516d-5df668b140a8@measurement-factory.com>
Message-ID: <1564386980689-0.post@n4.nabble.com>

Hello Alex,

I have upgrade the OS to Debian Linux 10 with Squid version 4.6.

The /etc/squid/squid.conf is now basic with:
http_access allow all (for testing)
dns_v4_first on
debug_option All,2

tail -f /var/log/squid/cache.log:
---------
CONNECT clients.interact.mtel.eu:443 HTTP/1.1
User-Agent: Connect 3.1.0.18300
Host: clients.interact.mtel.eu


----------
2019/07/29 09:50:52.762 kid1| 85,2| client_side_request.cc(752)
clientAccessCheckDone: The request CONNECT clients.interact.mtel.eu:443 is
ALLOWED; last ACL checked: all
2019/07/29 09:50:52.762 kid1| 85,2| client_side_request.cc(728)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2019/07/29 09:50:52.762 kid1| 85,2| client_side_request.cc(752)
clientAccessCheckDone: The request CONNECT clients.interact.mtel.eu:443 is
ALLOWED; last ACL checked: all
2019/07/29 09:50:52.762 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
Find IP destination for: clients.interact.mtel.eu:443' via
clients.interact.mtel.eu
2019/07/29 09:50:52.762 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths:
Found sources for 'clients.interact.mtel.eu:443'
2019/07/29 09:50:52.762 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:  
always_direct = DENIED
2019/07/29 09:50:52.762 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:   
never_direct = DENIED
2019/07/29 09:50:52.762 kid1| 44,2| peer_select.cc(308) peerSelectDnsPaths:         
DIRECT = local=0.0.0.0 remote=91.220.147.34:443 flags=1
2019/07/29 09:50:52.762 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:       
timedout = 0
2019/07/29 09:51:52.793 kid1| 4,2| errorpage.cc(1259) BuildContent: No
existing error page language negotiated for ERR_CONNECT_FAIL. Using default
error file.
2019/07/29 09:51:52.793 kid1| 33,2| client_side.cc(582) swanSong:
local=10.1.9.55:3128 remote=10.1.10.131:50756 flags=1
2019/07/29 09:51:58.822 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
connection on FD 14
2019/07/29 09:51:58.822 kid1| 5,2| TcpAcceptor.cc(317) acceptNext:
connection on local=[::]:3128 remote=[::] FD 14 flags=9
2019/07/29 09:51:58.822 kid1| 17,2| QosConfig.cc(126)
getNfmarkFromConnection: QOS: Failed to retrieve connection mark: (-1) (1)
Operation not permitted (Destination 10.1.9.55:3128, source
10.1.10.131:50767)
2019/07/29 09:51:58.823 kid1| 11,2| client_side.cc(1319) parseHttpRequest:
HTTP Client local=10.1.9.55:3128 remote=10.1.10.131:50767 FD 9 flags=1
2019/07/29 09:51:58.823 kid1| 11,2| client_side.cc(1323) parseHttpRequest:
HTTP Client REQUEST:

Best regards,
Sebastiaan



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Mon Jul 29 13:25:32 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Jul 2019 09:25:32 -0400
Subject: [squid-users] VoIP Software trouble
In-Reply-To: <1564386980689-0.post@n4.nabble.com>
References: <1564048816044-0.post@n4.nabble.com>
 <b8ba9feb-58c5-5c22-516d-5df668b140a8@measurement-factory.com>
 <1564386980689-0.post@n4.nabble.com>
Message-ID: <e70e5122-dd08-4a3b-a775-275b8f7804c8@measurement-factory.com>

On 7/29/19 3:56 AM, gswijker wrote:
> tail -f /var/log/squid/cache.log:
> ---------
> CONNECT clients.interact.mtel.eu:443 HTTP/1.1
> User-Agent: Connect 3.1.0.18300
> Host: clients.interact.mtel.eu
> 
> 
> ----------
> 2019/07/29 09:50:52.762 kid1| ... DIRECT remote=91.220.147.34:443
> 2019/07/29 09:51:52.793 kid1| ... ERR_CONNECT_FAIL.

Your Squid cannot establish a TCP connection to 91.220.147.34 port 443.
Judging by the timestamps, there was a 60 second timeout somewhere. I do
not know why the connection attempt timed out, but the answer probably
lies outside of Squid itself. FWIW, my Squid can connect to that address.

Alex.


From service.mv at gmail.com  Mon Jul 29 19:58:50 2019
From: service.mv at gmail.com (Service MV)
Date: Mon, 29 Jul 2019 16:58:50 -0300
Subject: [squid-users] Unable to limit bandwidth (squid 4.7.2 )
Message-ID: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>

Hello everyone!
I have a 100/100 Mbit/s internet link and I am trying unsuccessfully to
limit downloads to a maximum of 15Mb/s of any IP on my network. Some
downloads consume the entire link.
I copy my settings to help me see where I'm going wrong. Thank you very
much!
Gabriel

PS.: squid -v '--enable-delay-pools'

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
#acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN)
acl localnet src 10.10.8.0/22 # (My LAN)
#acl largefiledown src 10.10.8.0/22 # Limitar bajada y subida a 10Mbps
#acl localnet src 10.0.0.0/8 # RFC 1918 local private network (LAN)
#acl localnet src 100.64.0.0/10 # RFC 6598 shared address space (CGN)
#acl localnet src 169.254.0.0/16 # RFC 3927 link-local (directly plugged)
machines
#acl localnet src 172.16.0.0/12 # RFC 1918 local private network (LAN)
#acl localnet src 192.168.0.0/16 # RFC 1918 local private network (LAN)
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10       # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

acl LS_whitedomains dstdomain "/etc/squid/acl/whitedomains.txt"
acl LS_blackdomains dstdomain "/etc/squid/acl/blackdomains.txt"
acl LS_malicius dstdomain "/etc/squid/acl/malicius.txt"
acl LS_ads-tracking dstdomain "/etc/squid/acl/ads-tracking.txt"

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

delay_pools 1
delay_class 1 2
delay_parameters 1 103809024/103809024 15728640/15728640 # (98/98 megabytes
in bytes and 15/15 megabytes in bytes)
delay_access 1 allow localnet

http_access deny LS_blackdomains
http_access allow LS_whitedomains
http_access deny LS_malicius
http_access deny LS_ads-tracking


# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed


http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

# Other settings
quick_abort_min 0 KB
quick_abort_max 0 KB
read_timeout 5 minutes
request_timeout 3 minutes
shutdown_lifetime 0 seconds
ipcache_size 2048
fqdncache_size 4096
forwarded_for off
visible_hostname s-px02
httpd_suppress_version_string on
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190729/1966e219/attachment.htm>

From alex at nanogherkin.com  Mon Jul 29 21:12:30 2019
From: alex at nanogherkin.com (alex at nanogherkin.com)
Date: Mon, 29 Jul 2019 22:12:30 +0100
Subject: [squid-users] Unable to limit bandwidth (squid 4.7.2 )
References: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>
Message-ID: <-lgt1t-fqyq9pok5sjbmluyjmqd7fqbw5so8u-ibe0podf229z-y2v7l8y33bh6v51il1aco3q4-x0nqve-l99ar3lj4ins-18ftfu8yltz-idz5bcw8g3be776k54a17jb0-ops1psrm8qto-bob5gi.1564434750391@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190729/d153c7f5/attachment.htm>

From ahmed.zaeem at netstream.ps  Mon Jul 29 21:37:16 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 30 Jul 2019 00:37:16 +0300
Subject: [squid-users] dns_v4_first off for squid Squid Cache: Version
 5.0.0-20190715-rd3527ec67
Message-ID: <26AC8B3C-51B7-41B9-B032-B7FDB50DECE9@netstream.ps>

Hello Folks .

i have a problem with IPV6 when i moved to squid Squid Cache: Version 5.0.0-20190715-rd3527ec67.

in squid 3.5 when i put :
dns_v4_first off 
i have all resolution of domains for ipv6 as 1st priority then ipv4 .

but ?
when i have squid 5.x.x

seems this directive not effective bec i keep have all domains to be ipv4 1st .


can you check for me if its config or squid version ?


Kind regards 




From squid3 at treenet.co.nz  Mon Jul 29 22:14:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jul 2019 10:14:50 +1200
Subject: [squid-users] dns_v4_first off for squid Squid Cache: Version
 5.0.0-20190715-rd3527ec67
In-Reply-To: <26AC8B3C-51B7-41B9-B032-B7FDB50DECE9@netstream.ps>
References: <26AC8B3C-51B7-41B9-B032-B7FDB50DECE9@netstream.ps>
Message-ID: <e03f946759ac5082bbe6c0a383c65e68@treenet.co.nz>

On 2019-07-30 09:37, --Ahmad-- wrote:
> Hello Folks .
> 
> i have a problem with IPV6 when i moved to squid Squid Cache: Version
> 5.0.0-20190715-rd3527ec67.
> 
> in squid 3.5 when i put :
> dns_v4_first off
> i have all resolution of domains for ipv6 as 1st priority then ipv4 .
> 
> but ?
> when i have squid 5.x.x
> seems this directive not effective bec i keep have all domains to be 
> ipv4 1st .
> 

Squid-5 has the remaining parts of Happy Eyeballs algorithm for DNS 
resolving which were missing from earlier Squid. IPs are used as soon as 
they are delivered, and both types of TCP connection are attempted in 
parallel - with first TCP connection type to open being used. That makes 
sorting the way that directive used to do no longer reliable.

The ideal setup is to have working ICMP and ICMPv6 to inform Squid 
quickly of connectivity issues so it can skip past any non-working 
server IPs.

Alternatively; to force IPv4 use first configure the DNS resolver used 
by Squid to send the IPs in your desired order, or reject AAAA queries. 
That will ensure software other than Squid uses your preferences too.

<http://www.squid-cache.org/Versions/v5/squid-5.0.0-20190719-re10182dab-RELEASENOTES.html#ss2.2>

Amos



From rousskov at measurement-factory.com  Mon Jul 29 22:39:04 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Jul 2019 18:39:04 -0400
Subject: [squid-users] dns_v4_first off for squid Squid Cache: Version
 5.0.0-20190715-rd3527ec67
In-Reply-To: <26AC8B3C-51B7-41B9-B032-B7FDB50DECE9@netstream.ps>
References: <26AC8B3C-51B7-41B9-B032-B7FDB50DECE9@netstream.ps>
Message-ID: <1fab9888-2111-1b63-e1e9-fabf02ff7629@measurement-factory.com>

On 7/29/19 5:37 PM, --Ahmad-- wrote:
> Hello Folks .
> 
> i have a problem with IPV6 when i moved to squid Squid Cache: Version 5.0.0-20190715-rd3527ec67.
> 
> in squid 3.5 when i put :
> dns_v4_first off 
> i have all resolution of domains for ipv6 as 1st priority then ipv4 .
> 
> but ?
> when i have squid 5.x.x
> 
> seems this directive not effective bec i keep have all domains to be ipv4 1st .

Yes, Squid v5 ignores dns_v4_first. That option is supposed to be
completely removed IIRC.


> can you check for me if its config or squid version ?

Please see the Happy Eyeballs Update section in future v5 release
notes[1a, 1b] while keeping in mind another big Happy Eyeballs change[2]
that is currently waiting for the reviewer who has been stalling it for
3+ months.

[1a]
https://github.com/squid-cache/squid/blob/master/doc/release-notes/release-5.sgml#L65

[1b]
https://github.com/squid-cache/squid/blob/master/doc/release-notes/release-5.sgml#L201

[2] https://github.com/squid-cache/squid/pull/386

Alex.


From squid3 at treenet.co.nz  Tue Jul 30 08:29:44 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jul 2019 20:29:44 +1200
Subject: [squid-users] Why `Storage Mem capacity` has a value larger
 than 100%.
In-Reply-To: <1564368394287-0.post@n4.nabble.com>
References: <1564368394287-0.post@n4.nabble.com>
Message-ID: <5e730ec3-29ca-e633-984b-06908bc41c4b@treenet.co.nz>

On 29/07/19 2:46 pm, kmiku7 wrote:
> Hello
> My squid consume too much memory at startup time grow.
> From manager cgi /squid-internal-mgr/info, I saw the `Storage Mem capacity`
> in Cache information for squid is 2429.7%, larger than 100%. In my opinion,
> this value should be less or equal to 100%. I have searched but find nothing
> about it.
> Can someone tell me why? And how to limit squid memory usage, to avoid
> memory grow as startup time grow and handle more and more request.

The numbers in these reports can get a bit weird if you are running a
32-bit build of Squid on a 64-bit system, or just with very large sizes
for some things like cache.

When the value seems absurd like this use an external tool such as 'top'
to see if Squid is actually using as much memory as claimed.

If you have SMP workers configured then shared memory not working
properly could result in excessive memory usage across all of them.


Also, your proxy is apparently trying to fit objects with an *average*
size exceeding 70MB into that 256MB of cache. The bit of the report you
elided shows how many it is trying to fit in there.


Amos


From service.mv at gmail.com  Tue Jul 30 12:57:21 2019
From: service.mv at gmail.com (Service MV)
Date: Tue, 30 Jul 2019 09:57:21 -0300
Subject: [squid-users] Unable to limit bandwidth (squid 4.7.2 )
In-Reply-To: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>
References: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>
Message-ID: <CA+d==oFidSkJEvgy3VODaLdpgDrKEq6=_wO3GAoG3tZoiWhYbA@mail.gmail.com>

Thanks for patience.

I modify the line:
#                                 All net setting
                                                          Individual client
setting
#                                 first 15MB of file download full speed,
then continue at 10MB/s            first 10MB of file download full speed,
then continue at 7MB/s
delay_parameters 1      1310720/1966080
                                                           917504/1310720

In this way I can make the Delay Pool work.
But I'm still not sure if I'm using my symmetrical 100Mb/s bandwidth
correctly.

Any comments on that?


El lun., 29 de jul. de 2019 a la(s) 16:58, Service MV (service.mv at gmail.com)
escribi?:

> Hello everyone!
> I have a 100/100 Mbit/s internet link and I am trying unsuccessfully to
> limit downloads to a maximum of 15Mb/s of any IP on my network. Some
> downloads consume the entire link.
> I copy my settings to help me see where I'm going wrong. Thank you very
> much!
> Gabriel
>
> PS.: squid -v '--enable-delay-pools'
>
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> #acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN)
> acl localnet src 10.10.8.0/22 # (My LAN)
> #acl largefiledown src 10.10.8.0/22 # Limitar bajada y subida a 10Mbps
> #acl localnet src 10.0.0.0/8 # RFC 1918 local private network (LAN)
> #acl localnet src 100.64.0.0/10 # RFC 6598 shared address space (CGN)
> #acl localnet src 169.254.0.0/16 # RFC 3927 link-local (directly plugged)
> machines
> #acl localnet src 172.16.0.0/12 # RFC 1918 local private network (LAN)
> #acl localnet src 192.168.0.0/16 # RFC 1918 local private network (LAN)
> #acl localnet src fc00::/7       # RFC 4193 local private network range
> #acl localnet src fe80::/10       # RFC 4291 link-local (directly plugged)
> machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
>
> acl LS_whitedomains dstdomain "/etc/squid/acl/whitedomains.txt"
> acl LS_blackdomains dstdomain "/etc/squid/acl/blackdomains.txt"
> acl LS_malicius dstdomain "/etc/squid/acl/malicius.txt"
> acl LS_ads-tracking dstdomain "/etc/squid/acl/ads-tracking.txt"
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> delay_pools 1
> delay_class 1 2
> delay_parameters 1 103809024/103809024 15728640/15728640 # (98/98
> megabytes in bytes and 15/15 megabytes in bytes)
> delay_access 1 allow localnet
>
> http_access deny LS_blackdomains
> http_access allow LS_whitedomains
> http_access deny LS_malicius
> http_access deny LS_ads-tracking
>
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
>
>
> http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> http_port 3128
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
>
> # Other settings
> quick_abort_min 0 KB
> quick_abort_max 0 KB
> read_timeout 5 minutes
> request_timeout 3 minutes
> shutdown_lifetime 0 seconds
> ipcache_size 2048
> fqdncache_size 4096
> forwarded_for off
> visible_hostname s-px02
> httpd_suppress_version_string on
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190730/732b4723/attachment.htm>

From service.mv at gmail.com  Tue Jul 30 13:22:18 2019
From: service.mv at gmail.com (Service MV)
Date: Tue, 30 Jul 2019 10:22:18 -0300
Subject: [squid-users] Unable to limit bandwidth (squid 4.7.2 )
In-Reply-To: <CA+d==oFidSkJEvgy3VODaLdpgDrKEq6=_wO3GAoG3tZoiWhYbA@mail.gmail.com>
References: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>
 <CA+d==oFidSkJEvgy3VODaLdpgDrKEq6=_wO3GAoG3tZoiWhYbA@mail.gmail.com>
Message-ID: <CA+d==oEeORkzpGb5x+uDG4Ca5zK3WhAfwmM6ms+G4E1SDr8Mww@mail.gmail.com>

Just to explain clearly, my goal is that no user of my LAN can download
more than 15 megabits/s, because some downloads consume me 100 magabits/s
leaving the rest of the users offline.
Since squid calculates in bytes, it would be: 1966080 bytes the limit that
I want to establish for any user of my LAN
Thank you very much for your help.

El mar., 30 de jul. de 2019 a la(s) 09:57, Service MV (service.mv at gmail.com)
escribi?:

> Thanks for patience.
>
> I modify the line:
> #                                 All net setting
>                                                           Individual client
> setting
> #                                 first 15MB of file download full speed,
> then continue at 10MB/s            first 10MB of file download full speed,
> then continue at 7MB/s
> delay_parameters 1      1310720/1966080
>                                                            917504/1310720
>
> In this way I can make the Delay Pool work.
> But I'm still not sure if I'm using my symmetrical 100Mb/s bandwidth
> correctly.
>
> Any comments on that?
>
>
> El lun., 29 de jul. de 2019 a la(s) 16:58, Service MV (
> service.mv at gmail.com) escribi?:
>
>> Hello everyone!
>> I have a 100/100 Mbit/s internet link and I am trying unsuccessfully to
>> limit downloads to a maximum of 15Mb/s of any IP on my network. Some
>> downloads consume the entire link.
>> I copy my settings to help me see where I'm going wrong. Thank you very
>> much!
>> Gabriel
>>
>> PS.: squid -v '--enable-delay-pools'
>>
>> #
>> # Recommended minimum configuration:
>> #
>>
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>> #acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN)
>> acl localnet src 10.10.8.0/22 # (My LAN)
>> #acl largefiledown src 10.10.8.0/22 # Limitar bajada y subida a 10Mbps
>> #acl localnet src 10.0.0.0/8 # RFC 1918 local private network (LAN)
>> #acl localnet src 100.64.0.0/10 # RFC 6598 shared address space (CGN)
>> #acl localnet src 169.254.0.0/16 # RFC 3927 link-local (directly
>> plugged) machines
>> #acl localnet src 172.16.0.0/12 # RFC 1918 local private network (LAN)
>> #acl localnet src 192.168.0.0/16 # RFC 1918 local private network (LAN)
>> #acl localnet src fc00::/7       # RFC 4193 local private network range
>> #acl localnet src fe80::/10       # RFC 4291 link-local (directly
>> plugged) machines
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 21 # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70 # gopher
>> acl Safe_ports port 210 # wais
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl Safe_ports port 280 # http-mgmt
>> acl Safe_ports port 488 # gss-http
>> acl Safe_ports port 591 # filemaker
>> acl Safe_ports port 777 # multiling http
>> acl CONNECT method CONNECT
>>
>> acl LS_whitedomains dstdomain "/etc/squid/acl/whitedomains.txt"
>> acl LS_blackdomains dstdomain "/etc/squid/acl/blackdomains.txt"
>> acl LS_malicius dstdomain "/etc/squid/acl/malicius.txt"
>> acl LS_ads-tracking dstdomain "/etc/squid/acl/ads-tracking.txt"
>>
>> #
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> http_access deny !Safe_ports
>>
>> # Deny CONNECT to other than secure SSL ports
>> http_access deny CONNECT !SSL_ports
>>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> #http_access deny to_localhost
>>
>> #
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>>
>> delay_pools 1
>> delay_class 1 2
>> delay_parameters 1 103809024/103809024 15728640/15728640 # (98/98
>> megabytes in bytes and 15/15 megabytes in bytes)
>> delay_access 1 allow localnet
>>
>> http_access deny LS_blackdomains
>> http_access allow LS_whitedomains
>> http_access deny LS_malicius
>> http_access deny LS_ads-tracking
>>
>>
>> # Example rule allowing access from your local networks.
>> # Adapt localnet in the ACL section to list your (internal) IP networks
>> # from where browsing should be allowed
>>
>>
>> http_access allow localnet
>> http_access allow localhost
>>
>> # And finally deny all other access to this proxy
>> http_access deny all
>>
>> # Squid normally listens to port 3128
>> http_port 3128
>>
>> # Uncomment and adjust the following to add a disk cache directory.
>> #cache_dir ufs /var/spool/squid 100 16 256
>>
>> # Leave coredumps in the first cache dir
>> coredump_dir /var/spool/squid
>>
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>> refresh_pattern . 0 20% 4320
>>
>> # Other settings
>> quick_abort_min 0 KB
>> quick_abort_max 0 KB
>> read_timeout 5 minutes
>> request_timeout 3 minutes
>> shutdown_lifetime 0 seconds
>> ipcache_size 2048
>> fqdncache_size 4096
>> forwarded_for off
>> visible_hostname s-px02
>> httpd_suppress_version_string on
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190730/ddf88efa/attachment.htm>

From kakoimiku at gmail.com  Wed Jul 31 03:23:06 2019
From: kakoimiku at gmail.com (kmiku7)
Date: Tue, 30 Jul 2019 22:23:06 -0500 (CDT)
Subject: [squid-users] Why `Storage Mem capacity` has a value larger
	than 100%.
In-Reply-To: <5e730ec3-29ca-e633-984b-06908bc41c4b@treenet.co.nz>
References: <1564368394287-0.post@n4.nabble.com>
 <5e730ec3-29ca-e633-984b-06908bc41c4b@treenet.co.nz>
Message-ID: <1564543386225-0.post@n4.nabble.com>

Amos Jeffries wrote
> On 29/07/19 2:46 pm, kmiku7 wrote:
>> Hello
>> My squid consume too much memory at startup time grow.
>> From manager cgi /squid-internal-mgr/info, I saw the `Storage Mem
>> capacity`
>> in Cache information for squid is 2429.7%, larger than 100%. In my
>> opinion,
>> this value should be less or equal to 100%. I have searched but find
>> nothing
>> about it.
>> Can someone tell me why? And how to limit squid memory usage, to avoid
>> memory grow as startup time grow and handle more and more request.
> 
> The numbers in these reports can get a bit weird if you are running a
> 32-bit build of Squid on a 64-bit system, or just with very large sizes
> for some things like cache.
> 
> When the value seems absurd like this use an external tool such as 'top'
> to see if Squid is actually using as much memory as claimed.
> 
> If you have SMP workers configured then shared memory not working
> properly could result in excessive memory usage across all of them.
> 
> 
> Also, your proxy is apparently trying to fit objects with an *average*
> size exceeding 70MB into that 256MB of cache. The bit of the report you
> elided shows how many it is trying to fit in there.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Thanks your reply.
I am running 64bit build of squid on 64bit system. The output of top/ps
shows that squid is using as much memory as claimed in report.
I configure cache directory with size of 4T: cache_dir ufs PATH 4194304 128
256.

There are many child-process start, following is output of `ps`:
USER    20401  0.0  0.0  71020  2728 ?        Ss   Feb14   0:00
/PATH/TO/squid -f /PATH/TO/CONFIG/FILE -n squidHot
USER    20405  0.5 11.8 8258832 7771408 ?     S    Feb14 1298:30 (squid-1)
--kid squid-1 -f /PATH/TO/CONFIG/FILE -n squidHot
USER    20440  0.0  0.0  29468  1444 ?        S    Feb14   0:16
(logfile-daemon) /PATH/TO/access.log
USER    20441  0.0  0.0  29460  1256 ?        S    Feb14   0:00 (unlinkd)
USER    20444  0.0  0.0  29468  1252 ?        S    Feb14   0:00
(logfile-daemon) /PATH/TO/store.log

Process 20405 cost maximum memory.

Other part of report also make me puzzled:
	Internal Data Structures:
		  1185 StoreEntries
		  1184 StoreEntries with MemObjects
		     8 Hot Object Cache Items
		     9 on-disk objects
`9 on-disk objects` means only 9 entries of 1185 are stored on disk, and
others are stored in memory?



Amos Jeffries wrote
> Also, your proxy is apparently trying to fit objects with an *average*
> size exceeding 70MB into that 256MB of cache. The bit of the report you
> elided shows how many it is trying to fit in there.

Yes, we have many file larger than 256MB. But what problem will this lead
to? And why?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From augustus_meyer at gmx.net  Wed Jul 31 09:42:38 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 31 Jul 2019 04:42:38 -0500 (CDT)
Subject: [squid-users] Unable to limit bandwidth (squid 4.7.2 )
In-Reply-To: <CA+d==oEeORkzpGb5x+uDG4Ca5zK3WhAfwmM6ms+G4E1SDr8Mww@mail.gmail.com>
References: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>
 <CA+d==oFidSkJEvgy3VODaLdpgDrKEq6=_wO3GAoG3tZoiWhYbA@mail.gmail.com>
 <CA+d==oEeORkzpGb5x+uDG4Ca5zK3WhAfwmM6ms+G4E1SDr8Mww@mail.gmail.com>
Message-ID: <1564566158985-0.post@n4.nabble.com>

Delay pools are broken in squid 4.x for https. Work for http only.
"Known" bug, said to be fixed in squid 5.x only.
You might consider using squid 3.x instead. 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From marcus.kool at urlfilterdb.com  Wed Jul 31 10:19:29 2019
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 31 Jul 2019 07:19:29 -0300
Subject: [squid-users] Unable to limit bandwidth (squid 4.7.2 )
In-Reply-To: <CA+d==oEeORkzpGb5x+uDG4Ca5zK3WhAfwmM6ms+G4E1SDr8Mww@mail.gmail.com>
References: <CA+d==oG2UZDTsfO610kr91vDSQ03m_EZkgBxVC0K=UrWmgMdCw@mail.gmail.com>
 <CA+d==oFidSkJEvgy3VODaLdpgDrKEq6=_wO3GAoG3tZoiWhYbA@mail.gmail.com>
 <CA+d==oEeORkzpGb5x+uDG4Ca5zK3WhAfwmM6ms+G4E1SDr8Mww@mail.gmail.com>
Message-ID: <f5eeb87c-5455-2e2f-7f9e-94f827727f97@urlfilterdb.com>

On Linux you can use iptables to do qos and make sure that a single connection does not consume all bandwidth.

Marcus


On 30/07/2019 10:22, Service MV wrote:
> Just to explain clearly, my goal is that no user of my LAN can download more than 15 megabits/s, because some downloads consume me 100 magabits/s leaving the rest of the users offline.
> Since squid calculates in bytes, it would be: 1966080 bytes the limit that I want to establish for any user of my LAN
> Thank you very much for your help.
>
> El mar., 30 de jul. de 2019 a la(s) 09:57, Service MV (service.mv at gmail.com <mailto:service.mv at gmail.com>) escribi?:
>
>     Thanks for patience.
>
>     I modify the line:
>     #???????????????????????????????? All net setting Individual client setting
>     # ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? first 15MB of file download full speed, then continue at 10MB/s? ????????? first 10MB of file download full speed, then continue at 7MB/s
>     delay_parameters 1????? 1310720/1966080 917504/1310720
>
>     In this way I can make the Delay Pool work.
>     But I'm still not sure if I'm using my symmetrical 100Mb/s bandwidth correctly.
>
>     Any comments on that?
>
>
>     El lun., 29 de jul. de 2019 a la(s) 16:58, Service MV (service.mv at gmail.com <mailto:service.mv at gmail.com>) escribi?:
>
>         Hello everyone!
>         I have a 100/100 Mbit/s internet link and I am trying unsuccessfully to limit downloads to a maximum of 15Mb/s of any IP on my network. Some downloads consume the entire link.
>         I copy my settings to help me see where I'm going wrong. Thank you very much!
>         Gabriel
>
>         PS.: squid -v '--enable-delay-pools'
>
>         #
>         # Recommended minimum configuration:
>         #
>
>         # Example rule allowing access from your local networks.
>         # Adapt to list your (internal) IP networks from where browsing
>         # should be allowed
>         #acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN)
>         acl localnet src 10.10.8.0/22 <http://10.10.8.0/22> # (My LAN)
>         #acl largefiledown src 10.10.8.0/22 <http://10.10.8.0/22> # Limitar bajada y subida a 10Mbps
>         #acl localnet src 10.0.0.0/8 <http://10.0.0.0/8> # RFC 1918 local private network (LAN)
>         #acl localnet src 100.64.0.0/10 <http://100.64.0.0/10> # RFC 6598 shared address space (CGN)
>         #acl localnet src 169.254.0.0/16 <http://169.254.0.0/16> # RFC 3927 link-local (directly plugged) machines
>         #acl localnet src 172.16.0.0/12 <http://172.16.0.0/12> # RFC 1918 local private network (LAN)
>         #acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> # RFC 1918 local private network (LAN)
>         #acl localnet src fc00::/7 ? ? ? # RFC 4193 local private network range
>         #acl localnet src fe80::/10 ? ? ? # RFC 4291 link-local (directly plugged) machines
>
>         acl SSL_ports port 443
>         acl Safe_ports port 80 # http
>         acl Safe_ports port 21 # ftp
>         acl Safe_ports port 443 # https
>         acl Safe_ports port 70 # gopher
>         acl Safe_ports port 210 # wais
>         acl Safe_ports port 1025-65535 # unregistered ports
>         acl Safe_ports port 280 # http-mgmt
>         acl Safe_ports port 488 # gss-http
>         acl Safe_ports port 591 # filemaker
>         acl Safe_ports port 777 # multiling http
>         acl CONNECT method CONNECT
>
>         acl LS_whitedomains dstdomain "/etc/squid/acl/whitedomains.txt"
>         acl LS_blackdomains dstdomain "/etc/squid/acl/blackdomains.txt"
>         acl LS_malicius dstdomain "/etc/squid/acl/malicius.txt"
>         acl LS_ads-tracking dstdomain "/etc/squid/acl/ads-tracking.txt"
>
>         #
>         # Recommended minimum Access Permission configuration:
>         #
>         # Deny requests to certain unsafe ports
>         http_access deny !Safe_ports
>
>         # Deny CONNECT to other than secure SSL ports
>         http_access deny CONNECT !SSL_ports
>
>         # Only allow cachemgr access from localhost
>         http_access allow localhost manager
>         http_access deny manager
>
>         # We strongly recommend the following be uncommented to protect innocent
>         # web applications running on the proxy server who think the only
>         # one who can access services on "localhost" is a local user
>         #http_access deny to_localhost
>
>         #
>         # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>         #
>
>         delay_pools 1
>         delay_class 1 2
>         delay_parameters 1 103809024/103809024 15728640/15728640 # (98/98 megabytes in bytes and 15/15 megabytes in bytes)
>         delay_access 1 allow localnet
>
>         http_access deny LS_blackdomains
>         http_access allow LS_whitedomains
>         http_access deny LS_malicius
>         http_access deny LS_ads-tracking
>
>
>         # Example rule allowing access from your local networks.
>         # Adapt localnet in the ACL section to list your (internal) IP networks
>         # from where browsing should be allowed
>
>
>         http_access allow localnet
>         http_access allow localhost
>
>         # And finally deny all other access to this proxy
>         http_access deny all
>
>         # Squid normally listens to port 3128
>         http_port 3128
>
>         # Uncomment and adjust the following to add a disk cache directory.
>         #cache_dir ufs /var/spool/squid 100 16 256
>
>         # Leave coredumps in the first cache dir
>         coredump_dir /var/spool/squid
>
>         #
>         # Add any of your own refresh_pattern entries above these.
>         #
>         refresh_pattern ^ftp: 1440 20% 10080
>         refresh_pattern ^gopher: 1440 0% 1440
>         refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>         refresh_pattern . 0 20% 4320
>
>         # Other settings
>         quick_abort_min 0 KB
>         quick_abort_max 0 KB
>         read_timeout 5 minutes
>         request_timeout 3 minutes
>         shutdown_lifetime 0 seconds
>         ipcache_size 2048
>         fqdncache_size 4096
>         forwarded_for off
>         visible_hostname s-px02
>         httpd_suppress_version_string on
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190731/48263dcf/attachment.htm>

From nick.srg at yandex.ru  Wed Jul 31 10:47:58 2019
From: nick.srg at yandex.ru (Nikita Seregin)
Date: Wed, 31 Jul 2019 13:47:58 +0300
Subject: [squid-users] Using few ICAP services at one time
Message-ID: <20834791564570078@myt6-7f11d54e2b12.qloud-c.yandex.net>

Hi All.

I need to use Squid with few ICAP services, to be able to check files for viruses with few antivirus services.

But if i use the chain, the first service modifies the response and the object isn't being sent to the second service. Instead, the modified content (blocking page) is being sent to the second service.

Is it possible to send the same original request/response to few ICAP services separately?

Thanks in advance.

Nick.


From rousskov at measurement-factory.com  Wed Jul 31 13:50:30 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 Jul 2019 09:50:30 -0400
Subject: [squid-users] Using few ICAP services at one time
In-Reply-To: <20834791564570078@myt6-7f11d54e2b12.qloud-c.yandex.net>
References: <20834791564570078@myt6-7f11d54e2b12.qloud-c.yandex.net>
Message-ID: <0fc9684b-04b5-88bc-0fbb-399467a21722@measurement-factory.com>

On 7/31/19 6:47 AM, Nikita Seregin wrote:

> if i use the chain, the first service modifies the response and the
> object isn't being sent to the second service. Instead, the modified
> content (blocking page) is being sent to the second service.

If you can modify/configure the first ICAP service to _annotate_ the
transaction (via ICAP and/or HTTP headers that would mark the response
as virus free/infected) instead of generating a brand new HTTP response,
then you can still chain your ICAP services and feed the (mostly or
100%) virgin response to the second service.


> Is it possible to send the same original request/response to few ICAP
> services separately?

Yes, but only using a custom "broadcasting" ICAP or eCAP service. You
would have to write such a service yourself (or purchase it). Squid does
not come with one. From Squid point of view, there will be just one
service to deal with in this setup.


Alex.


From squid3 at treenet.co.nz  Wed Jul 31 14:39:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 1 Aug 2019 02:39:50 +1200
Subject: [squid-users] Why `Storage Mem capacity` has a value larger
 than 100%.
In-Reply-To: <1564543386225-0.post@n4.nabble.com>
References: <1564368394287-0.post@n4.nabble.com>
 <5e730ec3-29ca-e633-984b-06908bc41c4b@treenet.co.nz>
 <1564543386225-0.post@n4.nabble.com>
Message-ID: <ca793032-31b7-1219-4c2b-ec27d8bf01d5@treenet.co.nz>

On 31/07/19 3:23 pm, kmiku7 wrote:
> 
> Thanks your reply.
> I am running 64bit build of squid on 64bit system. The output of top/ps
> shows that squid is using as much memory as claimed in report.

Okay. That means the negative values are just an artifact of 32-bit
types used in the report display, not actually overflow bugs in the
store code.

> I configure cache directory with size of 4T: cache_dir ufs PATH 4194304 128
> 256.

FYI: On 64-bit systems each 1GB of disk storage needs approximately 15MB
of RAM for the index and metadata.

Does your machine actually have 60GB of RAM available for the proxy to
use for this large cache_dir index?


Those numbers are relative to the avg object size. So I advise tuning
the min-size= parameter so only the many-MB objects get stored there.
That should cut the index RAM requirement by a few orders of magnitude.


> 
> There are many child-process start, following is output of `ps`:
> USER    20401  0.0  0.0  71020  2728 ?        Ss   Feb14   0:00
> /PATH/TO/squid -f /PATH/TO/CONFIG/FILE -n squidHot
> USER    20405  0.5 11.8 8258832 7771408 ?     S    Feb14 1298:30 (squid-1)
> --kid squid-1 -f /PATH/TO/CONFIG/FILE -n squidHot
> USER    20440  0.0  0.0  29468  1444 ?        S    Feb14   0:16
> (logfile-daemon) /PATH/TO/access.log
> USER    20441  0.0  0.0  29460  1256 ?        S    Feb14   0:00 (unlinkd)
> USER    20444  0.0  0.0  29468  1252 ?        S    Feb14   0:00
> (logfile-daemon) /PATH/TO/store.log
> 
> Process 20405 cost maximum memory.
> 
> Other part of report also make me puzzled:
> 	Internal Data Structures:
> 		  1185 StoreEntries

ie. Total number of objects being cached by this proxy.


> 		  1184 StoreEntries with MemObjects

ie. Total number of objects which have at least some portion stored in
RAM for fast access.

This includes:
 * all objects in cache_mem
 * all cacheable objects currently being received from a server
 * all cacheable objects currently being delivered to a client
(though objects may match multiple of those criteria, each is only
counted once).

The difference between this and total objects (1185 - 1184 = 1) is the
number of objects *only* stored in a cache_dir.


> 		     8 Hot Object Cache Items

ie. Total count of items in cache_mem area of RAM.

> 		     9 on-disk objects

ie. Total count of objects stored in all configured cache_dir.


> `9 on-disk objects` means only 9 entries of 1185 are stored on disk, and
> others are stored in memory?
> 

Essentially, yes.


> 
> 
> Amos Jeffries wrote
>> Also, your proxy is apparently trying to fit objects with an *average*
>> size exceeding 70MB into that 256MB of cache. The bit of the report you
>> elided shows how many it is trying to fit in there.
> 
> Yes, we have many file larger than 256MB. But what problem will this lead
> to? And why?
> 

That will lead to disk I/O capacity being a major limiting factor in
delivery speed for all those objects. Since their data has to be saved
to or read from disk in order to be used.

Modulo bugs, the store is only supposed to be keeping a small portion of
them in memory awaiting delivery (on sending them) or waiting for
swapout to disk (on receiving). Though maximum_object_size_in_memory
place a role there, objects under that limit *may* be loaded fully into
cache_mem.

Check your object size limits:
<http://www.squid-cache.org/Doc/config/minimum_object_size/>
<http://www.squid-cache.org/Doc/config/maximum_object_size/>
<http://www.squid-cache.org/Doc/config/maximum_object_size_in_memory/>


PS. I also advise upgrading to the latest v4 release to avoid the
security issues and a memory leak that have been fixed since v4.4.

Amos


From creditu at eml.cc  Wed Jul 31 21:41:24 2019
From: creditu at eml.cc (creditu at eml.cc)
Date: Wed, 31 Jul 2019 15:41:24 -0600
Subject: [squid-users] Reverse Proxy Detected
Message-ID: <d5e9f059-cf8f-4a87-a9de-dc0782d1f165@www.fastmail.com>

We have been using several squid servers in accelerator mode for a number of years mainly for load balancing to send public requests to backend servers.  The requests to the squids typically come via a well known commercial  caching service.   The squids don't do any caching, they just forward requests to the backend. 

Recently the vulnerability scanner that we use changed a plugin from Info level to Moderate for reverse proxy detection.  We need to mitigate this so the vulnerability scanner doesn't flag for the reverse proxy detection. 

On a non-production server I added the following.  This seems to mitigate the vulnerability in the eyes of the scanner.  (I may be able to get away with not including the X-Cache-Lookup  line and still fix the issue.)  

via off
reply_header_access X-Cache deny all
reply_header_access X-Cache-Lookup deny all

This removes the headers for both the outgoing traffic to the Internet and the backend traffic to the webservers.  I have not seen any operational impact of doing this, but wanted get some feedback on if there is a better way to fix this issue and if I am missing any possible implications.

Also, does the following have the same effect as "via off"?
reply_header_access Via deny all

 


