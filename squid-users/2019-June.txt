From tel13471167744 at gmail.com  Sun Jun  2 23:35:54 2019
From: tel13471167744 at gmail.com (afkout)
Date: Mon, 3 Jun 2019 07:35:54 +0800
Subject: [squid-users] Reverse Proxy
Message-ID: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>

im using caddy for reverse proxy,  but caddy not support multiple local
ip.
here is my caddy configure

https://steamcommunity.com {
    tls steamcommunity.crt steamcommunity.key  #self signed
    proxy / https://steamuserimages-a.akamaihd.net
https://steamcdn-a.akamaihd.net {
    policy random
    fail_timeout 5s
    timeout 30s
    transparent
    header_upstream Host steamcommunity.com
    }
}

i try ssl_bump and cache_peer  but not working.how should i do
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190603/bee36791/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun  3 06:23:27 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 3 Jun 2019 18:23:27 +1200
Subject: [squid-users] Reverse Proxy
In-Reply-To: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>
References: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>
Message-ID: <752241de-5c86-e8c4-f8b8-df03925fabdc@treenet.co.nz>

On 3/06/19 11:35 am, afkout wrote:
> im using caddy for reverse proxy,? but caddy not support multiple local
> ip.

AFAICT 'caddy' is not Squid. So this does not appear to be a Squid
related question. Have you tried the caddy support forum?


Amos


From steven.niu at nokia-sbell.com  Mon Jun  3 09:20:47 2019
From: steven.niu at nokia-sbell.com (Niu, Steven (NSB - CN/Qingdao))
Date: Mon, 3 Jun 2019 09:20:47 +0000
Subject: [squid-users] parameter forward_max_retries
Message-ID: <f1be2e4a38b24bc784c3c1583d2ed2e3@nokia-sbell.com>

Hello,  

I'm sorry for any bother to you. But we are evaluating our solutions and need author's confirmation about the parameter forward_max_retries. 
Could you give me a yes or no if you have clear answer? I appreciate you response very much! Thank you!

If the configuration parameter of forward_max_retries is 3, for the following cases, is the assumption true or false? 
Case 1:  FQDN has 4 IPs from DNS result: ip1, ip2, ip3, ip4. 
	If ip1, ip2, ip3 are failed, Squid would not try ip4 since forward_max_retries is 3. 
Case 2:  FQDN has 2 IPs from DNS result: ip1, ip2. 
	If ip1, ip2 are failed, Squid would give final response since no more IP can try. 

Best Regards,
Steven



From squid3 at treenet.co.nz  Mon Jun  3 11:05:15 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 3 Jun 2019 23:05:15 +1200
Subject: [squid-users] parameter forward_max_retries
In-Reply-To: <f1be2e4a38b24bc784c3c1583d2ed2e3@nokia-sbell.com>
References: <f1be2e4a38b24bc784c3c1583d2ed2e3@nokia-sbell.com>
Message-ID: <cd7f887f-0f76-b567-2075-ed8ed60c1951@treenet.co.nz>

On 3/06/19 9:20 pm, Niu, Steven (NSB - CN/Qingdao) wrote:
> Hello,  
> 
> I'm sorry for any bother to you. But we are evaluating our solutions and need author's confirmation about the parameter forward_max_retries. 


Then you had best contact the author. This is a mailing list for *users*
of Squid to discuss and learn about the software from each other, and
from those developers like myself who participate here.

AFAIK neither the developer who designed the feature, nor the developer
who re-designed it to operate differently participate in this mailing list.


> Could you give me a yes or no if you have clear answer? I appreciate you response very much! Thank you!
> 
> If the configuration parameter of forward_max_retries is 3, for the following cases, is the assumption true or false? 
> Case 1:  FQDN has 4 IPs from DNS result: ip1, ip2, ip3, ip4. 
> 	If ip1, ip2, ip3 are failed, Squid would not try ip4 since forward_max_retries is 3. 
> Case 2:  FQDN has 2 IPs from DNS result: ip1, ip2. 
> 	If ip1, ip2 are failed, Squid would give final response since no more IP can try. 
> 

The answer depends on which version of Squid you are using, what those
IPs are, what type of HTTP message is being attempted, and how long the
process is taking.


In regards to Squid version:

* The initial design used (n_tries > forward_max_tries). Squid up to
v4.0 use this.

* The redesigned code uses (n_tries >= forward_max_tries). Squid v4 and
later use this.

=> Both versions prevent destinations being contacted when their
expression produces the "true" boolean result.


In regards to IPs:

* Squid which are built with IPv6 disabled completely ignore IPv6
results from DNS. They are not considered in the retries list.

* If the failure occurs at the TCP connect(2) level, then Squid-3.1 and
older do not count these as tries and will attempt all IPs
forward_max_tries times. Squid-3.2 and later will count them as tries.


Please also be aware that many popular websites have dozens of IPs in
their DNS responses. Setting forward_max_tries less than 20 can result
in these very popular websites not being contactable.


In regards to HTTP message type:

* HTTP standards specify some request methods as non-idempotent.
Retrying these requests is forbidden.

* HTTP standards define some response status codes as final and fatal.
These cannot ever be retried by an intermediary.

* HTTP transactions which involve certain stateful application layer
features which break HTTP requirements about statelessness cannot be
retried.
 ==> These features impact retry behaviour of the TCP connection itself.
So the non-retry outcome can be inherited from some past HTTP
transaction(s) activity.

* Any HTTP request with a payload that has begun delivery cannot be retried.

* Any HTTP response which began but did not completely arrive cannot be
retried.


In regards to timing:

* timeouts for TCP connect action, TCP lifetimes, HTTP forwarding, and
I/O actions can terminate any given tries early and cause later ones to
not happen.

* TCP error handling is subject to ICMP (both v4 and v6) being enabled
and working on your network and on *every* network between your proxy
and the origin serves (both outbound and inbound routes).
 -> without ICMP the TCP errors result in 'hung' connections which
usually hit timeout errors before the forwarding re-tries succeed.


Overall;

  Squid always give a final response when that final response is known.
Where that response comes from is far more complicated than your stated
cases consider.


Have you tried testing for yourself the particular sites you are worried
about?

Amos


From tel13471167744 at gmail.com  Mon Jun  3 13:58:25 2019
From: tel13471167744 at gmail.com (afkout)
Date: Mon, 3 Jun 2019 08:58:25 -0500 (CDT)
Subject: [squid-users] Reverse Proxy
In-Reply-To: <752241de-5c86-e8c4-f8b8-df03925fabdc@treenet.co.nz>
References: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>
 <752241de-5c86-e8c4-f8b8-df03925fabdc@treenet.co.nz>
Message-ID: <1559570305583-0.post@n4.nabble.com>

sorry i dont explain clearly.
i want to implement same function in squid like caddy dose.
a.com is blocked for some reason. but b.com and a.com use the same cdn
network.
when caddy get a a.com request , send it to cdn network node and set the SNI
domain a.com.
can squid implement that?






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Jun  3 14:05:31 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Jun 2019 02:05:31 +1200
Subject: [squid-users] Reverse Proxy
In-Reply-To: <1559570305583-0.post@n4.nabble.com>
References: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>
 <752241de-5c86-e8c4-f8b8-df03925fabdc@treenet.co.nz>
 <1559570305583-0.post@n4.nabble.com>
Message-ID: <7857d36b-ab89-51f9-ee46-1d286db02791@treenet.co.nz>

On 4/06/19 1:58 am, afkout wrote:
> sorry i dont explain clearly.
> i want to implement same function in squid like caddy dose.
> a.com is blocked for some reason. but b.com and a.com use the same cdn
> network.
> when caddy get a a.com request , send it to cdn network node and set the SNI
> domain a.com.
> can squid implement that?
> 

Proxying traffic is what Squid does, yes.

Amos


From adilias3 at gmx.com  Mon Jun  3 23:27:42 2019
From: adilias3 at gmx.com (Ilias Clifton)
Date: Tue, 4 Jun 2019 01:27:42 +0200
Subject: [squid-users] LDAP authentication from android and iphones
In-Reply-To: <trinity-28772a7a-c296-4980-b1ff-14cf1219136a-1559255290231@3c-app-mailcom-bs01>
References: <20190530043615.4B797E029C@lists.squid-cache.org>
 <trinity-28772a7a-c296-4980-b1ff-14cf1219136a-1559255290231@3c-app-mailcom-bs01>
Message-ID: <trinity-2b0685a2-2139-44b0-b40e-f415505d0ef6-1559604462843@3c-app-mailcom-bs12>

> > > >> Sent: Wednesday, May 29 2019 6:42
> > > >> From: Ilias Clifton
> > > >>
> > > >> I have Squid 3.5.27 running on Ubuntu 18.04.2, and have been unsuccesfull in being able to authenticate users via ldap (kerberos is working well)
> > > >>
> > > >> What else can I do for troubleshooting?
> > > >>
> >
> > What I do is take one of the access.log lines and read through the squid.conf (whole thing) to see what squid would do with that transaction. Most 40* status problems are with http_access ordering, so quickly spotted.
> >
> > If you can provide those details in full im happy to do so for you. Or someone experienced with a similar config may spot the issue.
> >
> 
> Sent:?Friday, May 31, 2019 at 8:28 AM
> From:?"Ilias Clifton" <adilias3 at gmx.com>
> 
> See squid.conf below.. Any other config files you need to see?

Would upgrading to a new version of squid help, like 4.6?


From steven.niu at nokia-sbell.com  Tue Jun  4 01:40:45 2019
From: steven.niu at nokia-sbell.com (Niu, Steven (NSB - CN/Qingdao))
Date: Tue, 4 Jun 2019 01:40:45 +0000
Subject: [squid-users] parameter forward_max_retries
In-Reply-To: <cd7f887f-0f76-b567-2075-ed8ed60c1951@treenet.co.nz>
References: <f1be2e4a38b24bc784c3c1583d2ed2e3@nokia-sbell.com>
 <cd7f887f-0f76-b567-2075-ed8ed60c1951@treenet.co.nz>
Message-ID: <ada53323360545ca918637dde7f0b4a1@nokia-sbell.com>

Amos, 

Thanks for your so detailed explanation. ? . I used wrong email address. 
Yes, we plan to have a test. But before our lab gets ready, it would be good to know the answer. 

Thanks again, 
Steven

-----Original Message-----
From: Amos Jeffries <squid3 at treenet.co.nz> 
Sent: Monday, June 3, 2019 7:05 PM
To: Niu, Steven (NSB - CN/Qingdao) <steven.niu at nokia-sbell.com>; squid-users at lists.squid-cache.org
Cc: Liu, Hank (NSB - CN/Qingdao) <hank.liu at nokia-sbell.com>
Subject: Re: [squid-users] parameter forward_max_retries

On 3/06/19 9:20 pm, Niu, Steven (NSB - CN/Qingdao) wrote:
> Hello,
> 
> I'm sorry for any bother to you. But we are evaluating our solutions and need author's confirmation about the parameter forward_max_retries. 


Then you had best contact the author. This is a mailing list for *users* of Squid to discuss and learn about the software from each other, and from those developers like myself who participate here.

AFAIK neither the developer who designed the feature, nor the developer who re-designed it to operate differently participate in this mailing list.


> Could you give me a yes or no if you have clear answer? I appreciate you response very much! Thank you!
> 
> If the configuration parameter of forward_max_retries is 3, for the following cases, is the assumption true or false? 
> Case 1:  FQDN has 4 IPs from DNS result: ip1, ip2, ip3, ip4. 
> 	If ip1, ip2, ip3 are failed, Squid would not try ip4 since forward_max_retries is 3. 
> Case 2:  FQDN has 2 IPs from DNS result: ip1, ip2. 
> 	If ip1, ip2 are failed, Squid would give final response since no more IP can try. 
> 

The answer depends on which version of Squid you are using, what those IPs are, what type of HTTP message is being attempted, and how long the process is taking.


In regards to Squid version:

* The initial design used (n_tries > forward_max_tries). Squid up to
v4.0 use this.

* The redesigned code uses (n_tries >= forward_max_tries). Squid v4 and later use this.

=> Both versions prevent destinations being contacted when their expression produces the "true" boolean result.


In regards to IPs:

* Squid which are built with IPv6 disabled completely ignore IPv6 results from DNS. They are not considered in the retries list.

* If the failure occurs at the TCP connect(2) level, then Squid-3.1 and older do not count these as tries and will attempt all IPs forward_max_tries times. Squid-3.2 and later will count them as tries.


Please also be aware that many popular websites have dozens of IPs in their DNS responses. Setting forward_max_tries less than 20 can result in these very popular websites not being contactable.


In regards to HTTP message type:

* HTTP standards specify some request methods as non-idempotent.
Retrying these requests is forbidden.

* HTTP standards define some response status codes as final and fatal.
These cannot ever be retried by an intermediary.

* HTTP transactions which involve certain stateful application layer features which break HTTP requirements about statelessness cannot be retried.
 ==> These features impact retry behaviour of the TCP connection itself.
So the non-retry outcome can be inherited from some past HTTP
transaction(s) activity.

* Any HTTP request with a payload that has begun delivery cannot be retried.

* Any HTTP response which began but did not completely arrive cannot be retried.


In regards to timing:

* timeouts for TCP connect action, TCP lifetimes, HTTP forwarding, and I/O actions can terminate any given tries early and cause later ones to not happen.

* TCP error handling is subject to ICMP (both v4 and v6) being enabled and working on your network and on *every* network between your proxy and the origin serves (both outbound and inbound routes).
 -> without ICMP the TCP errors result in 'hung' connections which usually hit timeout errors before the forwarding re-tries succeed.


Overall;

  Squid always give a final response when that final response is known.
Where that response comes from is far more complicated than your stated cases consider.


Have you tried testing for yourself the particular sites you are worried about?

Amos

From squid3 at treenet.co.nz  Tue Jun  4 05:13:49 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Jun 2019 17:13:49 +1200
Subject: [squid-users] LDAP authentication from android and iphones
In-Reply-To: <trinity-2b0685a2-2139-44b0-b40e-f415505d0ef6-1559604462843@3c-app-mailcom-bs12>
References: <20190530043615.4B797E029C@lists.squid-cache.org>
 <trinity-28772a7a-c296-4980-b1ff-14cf1219136a-1559255290231@3c-app-mailcom-bs01>
 <trinity-2b0685a2-2139-44b0-b40e-f415505d0ef6-1559604462843@3c-app-mailcom-bs12>
Message-ID: <6e332956-a0fa-f051-8a6e-53a6e841f17e@treenet.co.nz>

On 4/06/19 11:27 am, Ilias Clifton wrote:
>>>>>> Sent: Wednesday, May 29 2019 6:42
>>>>>> From: Ilias Clifton
>>>>>>
>>>>>> I have Squid 3.5.27 running on Ubuntu 18.04.2, and have been unsuccesfull in being able to authenticate users via ldap (kerberos is working well)
>>>>>>
>>>>>> What else can I do for troubleshooting?
>>>>>>
>>>
>>> What I do is take one of the access.log lines and read through the squid.conf (whole thing) to see what squid would do with that transaction. Most 40* status problems are with http_access ordering, so quickly spotted.
>>>
>>> If you can provide those details in full im happy to do so for you. Or someone experienced with a similar config may spot the issue.
>>>
>>
>> Sent:?Friday, May 31, 2019 at 8:28 AM
>> From:?"Ilias Clifton"
>>
>> See squid.conf below.. Any other config files you need to see?
> 
> Would upgrading to a new version of squid help, like 4.6?

That is unlikely to make any difference. But since the problem cause is
still not properly known I cannot answer that definitively.


Did you see my response about your config? It is most probably that
whitespace on the auth_param line, or possibly the sequence of auth
related ACLs in http_access.


Amos


From messpam at free.fr  Wed Jun  5 16:27:09 2019
From: messpam at free.fr (papaJustify)
Date: Wed, 5 Jun 2019 11:27:09 -0500 (CDT)
Subject: [squid-users] Squid to authenticate non domain computers to a
	corporate proxy
Message-ID: <1559752029913-0.post@n4.nabble.com>

Hi all. I try to find how to use squid a a middle proxy between non domain
computers (Pc, raspberrys etc ..) and a domain proxy.

The way I want to setup is : 

Raspberry --> Squid --> Corporate proxy --> Internet

the corporate proxy accepts NT Users with their domain (eg : user =
EMEA\USER1, pass = myPass)

I would like to setup the raspberry to use squid as a proxy and squid to
auth as the NT user to my corporate proxy.
I have a dedicated domain user to use for this.
can someone help please ?
thanx in advance




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From adilias3 at gmx.com  Wed Jun  5 22:42:23 2019
From: adilias3 at gmx.com (Ilias Clifton)
Date: Thu, 6 Jun 2019 00:42:23 +0200
Subject: [squid-users] LDAP authentication from android and iphones
In-Reply-To: <6e332956-a0fa-f051-8a6e-53a6e841f17e@treenet.co.nz>
References: <20190530043615.4B797E029C@lists.squid-cache.org>
 <trinity-28772a7a-c296-4980-b1ff-14cf1219136a-1559255290231@3c-app-mailcom-bs01>
 <trinity-2b0685a2-2139-44b0-b40e-f415505d0ef6-1559604462843@3c-app-mailcom-bs12>
 <6e332956-a0fa-f051-8a6e-53a6e841f17e@treenet.co.nz>
Message-ID: <trinity-5242e289-a138-465c-aba2-877563c52368-1559774543621@3c-app-mailcom-bs02>

>>>>>>> Sent: Wednesday, May 29 2019 6:42
>>>>>>> From: Ilias Clifton
>>>>>>>
>>>>>>> I have Squid 3.5.27 running on Ubuntu 18.04.2, and have been unsuccesfull in being able to authenticate users via ldap (kerberos is working well)
>>>>>>>
>>>>>>> What else can I do for troubleshooting?
>>>>>>>
>>>>
>>>> What I do is take one of the access.log lines and read through the squid.conf (whole thing) to see what squid would do with that transaction. Most 40* status problems are with http_access ordering, so quickly spotted.
>>>>
>>>> If you can provide those details in full im happy to do so for you. Or someone experienced with a similar config may spot the issue.
>>>>
>>>
>>> Sent:?Friday, May 31, 2019 at 8:28 AM
>>> From:?"Ilias Clifton"
>>>
>>> See squid.conf below.. Any other config files you need to see?
>>
>> Would upgrading to a new version of squid help, like 4.6?
>
>That is unlikely to make any difference. But since the problem cause is
>still not properly known I cannot answer that definitively.
>
>
>Did you see my response about your config? It is most probably that
>whitespace on the auth_param line, or possibly the sequence of auth
>related ACLs in http_access.
>

I had missed your other reply somehow - I will make your recommended changes and reply to that message.

Thanks.


From squid3 at treenet.co.nz  Thu Jun  6 07:41:16 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Jun 2019 19:41:16 +1200
Subject: [squid-users] Squid to authenticate non domain computers to a
 corporate proxy
In-Reply-To: <1559752029913-0.post@n4.nabble.com>
References: <1559752029913-0.post@n4.nabble.com>
Message-ID: <b4e5464e-0615-f7b5-b358-f4858020f951@treenet.co.nz>

On 6/06/19 4:27 am, papaJustify wrote:
> Hi all. I try to find how to use squid a a middle proxy between non domain
> computers (Pc, raspberrys etc ..) and a domain proxy.
> 
> The way I want to setup is : 
> 
> Raspberry --> Squid --> Corporate proxy --> Internet
> 
> the corporate proxy accepts NT Users with their domain (eg : user =
> EMEA\USER1, pass = myPass)
> 
> I would like to setup the raspberry to use squid as a proxy and squid to
> auth as the NT user to my corporate proxy.
> I have a dedicated domain user to use for this.
> can someone help please ?


Squid does not support authenticating against an HTTP server/peer with
NTLM (which was deprecated in 2006).

Squid does support Negotiate/Kerberos auth with peers. See the
cache_peer "login=NEGOTIATE" option.
<http://www.squid-cache.org/Doc/config/cache_peer/>


Amos


From JOfficer at istreamfs.com  Thu Jun  6 14:38:43 2019
From: JOfficer at istreamfs.com (Joey Officer)
Date: Thu, 6 Jun 2019 14:38:43 +0000
Subject: [squid-users] Attempting to use follow_x_forwarded_for in ACL
Message-ID: <DM5PR19MB1579C8AE57CE893D803F7C9ECD170@DM5PR19MB1579.namprd19.prod.outlook.com>

Greetings all,

squid.conf references the ability to use the x-forwarded-for header in ACLs by using the follow_x_forwarded_for in ACL, referenced here: http://www.squid-cache.org/Doc/config/follow_x_forwarded_for/ and here http://www.squid-cache.org/Doc/config/acl_uses_indirect_client/

There appear to be three pre-reqs, which I've met:

squid.conf: acl_uses_indirect_client on
squid built with --enable-follow-x-forwarded-for (confirmed)
and the appropriate ACL entries (see below)

In my scenario, I have a pair of squid hosts (squid ver 3.5.6) sitting behind a pair of haproxy/keepalived hosts which provide balancing and redundancy/availability.  Haproxy is configured to add an x-forwarded-for header (if one doesn't already exist) and I can see the x-forwarded-for header in the request if I run packet capture on the squid hosts.

For this scenario, I have a box sitting on the 192.168.4.0/24 network, which has access to three IPs on 192.168.2.0/24 network (2.30, 2.31, and 2.32 which are haproxy1, haproxy2, and keepalived vIP respectively).  Hosts wanting internet access must using the haproxy-vip as a proxy IP, which is then forwarded to the real squid backends.  To sum up:

haproxy1 - 192.168.2.30
haproxy2 - 192.168.2.31
haproxy-vip - 192.168.2.32
squid1 - 192.168.2.128
squid2 - 192.168.2.129
zone1 - 192.168.3.0/24 with hosts having a proxy configured as 192.168.2.32:3128
client1 - 192.168.4.31 with a proxy configured as 192.168.2.32:3128

Squid will see the real-ip of the client connection as the haproxy-vip endpoint and not the real-client IP.  If I understand the documentation correctly, I should be able to perform something like the following in an ACL:

# create acl source references
acl zone1 src 192.168.3.0/24
acl client1 src 192.168.4.31/32

# acl to test x-forwarded-for matching header
acl testing_proxy_dst dstdomain .google.com
follow_x_forwarded_for allow zone1 testing_proxy_dst
follow_x_forwarded_for allow client1 testing_proxy_dst

When I attempt to perform a wget (testing) to www.google.com<http://www.google.com> from my client1, I'm getting a permission denied:

# wget www.google.com
--2019-06-06 08:20:30--  http://www.google.com/
Connecting to 192.168.2.32:3128... connected.
Proxy request sent, awaiting response... 403 Forbidden
2019-06-06 08:20:30 ERROR 403: Forbidden.

If I change the proxy (and corresponding relevant http_access acl ) so that the client goes direct to the squid host, the client is allowed.

Any help would be greatly appreciated.

Joey


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190606/5ebe3646/attachment.htm>

From techchavez at gmail.com  Fri Jun  7 06:43:41 2019
From: techchavez at gmail.com (Techie)
Date: Thu, 6 Jun 2019 23:43:41 -0700
Subject: [squid-users] client delay pools
Message-ID: <CAEUA182hRBdrJaJFHHaES5mxCPqXqpKfAhnYUC20mw4aVfXojA@mail.gmail.com>

Hello,

Running Squid 3.5 on Centos

I am trying to limit uploads to AWS S3 buckets using squid to prevent
saturation by end users.

I have tried utilizing client delay pools but they seem to have no effect.
delay pools for limiting downloads work great but I am trying to limit up
not down.

In the below config servera is correctly limited on download but not upload.
Anyone get client delay pools working properly? Can you spot any error in
my config?

Squid was compiled with
--enable-delay-pools


delay_pools 1
client_delay_pools 1
delay_class 1 1
delay_parameters 1 250000/250000
client_delay_parameters 1 250000 250000
delay_access 1 allow servera
client_delay_access 1 allow servera

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190606/65eeb2cd/attachment.htm>

From techchavez at gmail.com  Fri Jun  7 06:50:42 2019
From: techchavez at gmail.com (Techie)
Date: Thu, 6 Jun 2019 23:50:42 -0700
Subject: [squid-users] Squid 3.5 Disable CONNECT TUNNEL
Message-ID: <CAEUA18325yHbqJLaAzd-AR1sMAbPvFPTPiHAgy+C3DG7e1FPOA@mail.gmail.com>

Hello,

Previously running squid 3.1 on Centos 6, recently went to Centos7 with
squid 3.5.
Since the upgrade I have been receiving SSL errors connecting to https
sites.

I notice in the log for squid 3.1I have entries like this when hitting
https sites
 172.16.80.25 TCP_MISS/200 6086 CONNECT www.securesite.com:443 -
DIRECT/x.x.x.x

Now they look to be utilizing TCP_TUNNEL as seen below with squid 3.5
192.168.2.10 TCP_TUNNEL/200 4371 CONNECT www.securesite.com:443 -
HIER_DIRECT/x.x.x.x

Is there a way to disable the TCP_TUNNEL feature?

A bit new to squid.


Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190606/240e5361/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun  7 08:44:28 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Jun 2019 20:44:28 +1200
Subject: [squid-users] Squid 3.5 Disable CONNECT TUNNEL
In-Reply-To: <CAEUA18325yHbqJLaAzd-AR1sMAbPvFPTPiHAgy+C3DG7e1FPOA@mail.gmail.com>
References: <CAEUA18325yHbqJLaAzd-AR1sMAbPvFPTPiHAgy+C3DG7e1FPOA@mail.gmail.com>
Message-ID: <646bb9c7-a17b-50f4-e4fd-e8b3f22d99ac@treenet.co.nz>

On 7/06/19 6:50 pm, Techie wrote:
> Hello,
> 
> Previously running squid 3.1 on Centos 6, recently went to Centos7 with
> squid 3.5.
> Since the upgrade I have been receiving SSL errors connecting to https
> sites.
> 
> I notice in the log for squid 3.1I have entries like this when hitting
> https sites
> ?172.16.80.25 TCP_MISS/200 6086 CONNECT www.securesite.com:443
> <http://www.securesite.com:443> - DIRECT/x.x.x.x
> 
> Now they look to be utilizing TCP_TUNNEL as seen below with squid 3.5
> 192.168.2.10 TCP_TUNNEL/200 4371 CONNECT www.securesite.com:443
> <http://www.securesite.com:443> - HIER_DIRECT/x.x.x.x?
> 
> Is there a way to disable the TCP_TUNNEL feature?


The "MISS" earlier was always a lie, implying that the cache had some
involvement. These transactions are simply not involving cache in any
way. The old version log entries that had CONNECT method with "TCP_MISS"
are identical to what the newer versions log as CONNECT with "TCP_TUNNEL"

If you are seeing "TUNNEL" logged, then Squid is not touching that
traffic at all. Any TLS/SSL problems are an issue between the client and
server directly talking that protocol to each other - Squid is
irrelevant to traffic problems.

If you are okay telling us what HTTPS errors exactly are showing up
perhaps someone may be able to help with or at least identify where the
problem actually is.


Amos


From srikanth.raju at affirm.com  Fri Jun  7 09:30:33 2019
From: srikanth.raju at affirm.com (Srikanth Raju)
Date: Fri, 7 Jun 2019 02:30:33 -0700
Subject: [squid-users] Questions about connection pooling to origins when
 using squid as a HTTPS forward egress proxy
Message-ID: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>

Hello!
We are planning to use squid as a forward egress proxy to whitelist
domains. In general, we configured it to whitelist/blacklist domains based
on the examples in the site and this seems to work with peek and splice on
our preliminary tests as a transparent egress proxy. We're doing this with
an AWS VPC using the standard techniques documented in their blogs

I had a few questions about the setup for some additional features

   - We want to have the ability to connection pool certain HTTPS calls
   going externally within from within squid. This would be specifically for
   some external partners that we know have slow connection setup time and or
   in case of misbehaving libraries.
   - WIth "peek and splice" method for HTTPS, this doesn't make sense,
   since it's a TCP tunnel basically. There shouldn't be a way to replay the
   handshake, hence it's impossible to pool at the squid layer.
   - We need to consider 'bump' for some use cases along with our own
   intermediate CA, which we're ok with, since we can choose the domains to
   'splice' and domains to 'bump'/
   - The biggest reason we care about TLS termination with bump is because
   we think it might give us performance benefits along some critical code
   paths *due to connection pooling to some slow upstreams within squid.*
   - Does squid automatically do this or does it need some extra config. I
   was looking at 'server_connections' config var. [Currently we roughly
   follow the config in the AWS Guide]
   <https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/>


Another thing we cared about , with a much lower priority, was HTTP/2
translation. We would like to reap the benefits of HTTP/2 on external
services that do support it and we connect to, but our application does not
yet have any production-safe http2 clients(python).
Is there any roadmap for when that will land on Squid master?

Thanks y'all
- Srikanth

Platform and Online Frameworks. Affirm Inc.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190607/95b0179b/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun  7 09:34:00 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Jun 2019 21:34:00 +1200
Subject: [squid-users] Attempting to use follow_x_forwarded_for in ACL
In-Reply-To: <DM5PR19MB1579C8AE57CE893D803F7C9ECD170@DM5PR19MB1579.namprd19.prod.outlook.com>
References: <DM5PR19MB1579C8AE57CE893D803F7C9ECD170@DM5PR19MB1579.namprd19.prod.outlook.com>
Message-ID: <a375833f-952f-1130-6891-aeb5ff905442@treenet.co.nz>

On 7/06/19 2:38 am, Joey Officer wrote:
> Greetings all,
> 
> ?
> 
> squid.conf references the ability to use the x-forwarded-for header in
> ACLs by using the follow_x_forwarded_for in ACL, referenced here:
> http://www.squid-cache.org/Doc/config/follow_x_forwarded_for/ and here
> http://www.squid-cache.org/Doc/config/acl_uses_indirect_client/
> 
> ?
> 
> There appear to be three pre-reqs, which I?ve met:
> 
> ?
> 
> squid.conf: acl_uses_indirect_client on
> 
> squid built with --enable-follow-x-forwarded-for (confirmed)
> 
> and the appropriate ACL entries (see below)
> 


More correctly; configuration to tell Squid which clients to trust
X-Forwarded-For headers from.


> ?
> 
> In my scenario, I have a pair of squid hosts (squid ver 3.5.6) sitting
> behind a pair of haproxy/keepalived hosts which provide balancing and
> redundancy/availability.? Haproxy is configured to add an
> x-forwarded-for header (if one doesn?t already exist) and I can see the
> x-forwarded-for header in the request if I run packet capture on the
> squid hosts.
> 
> ?
> 
> For this scenario, I have a box sitting on the 192.168.4.0/24 network,
> which has access to three IPs on 192.168.2.0/24 network (2.30, 2.31, and
> 2.32 which are haproxy1, haproxy2, and keepalived vIP respectively).?
> Hosts wanting internet access must using the haproxy-vip as a proxy IP,
> which is then forwarded to the real squid backends.? To sum up:
> 
> ?
> 
> haproxy1 ? 192.168.2.30
> 
> haproxy2 ? 192.168.2.31
> 
> haproxy-vip ? 192.168.2.32
> 
> squid1 ? 192.168.2.128
> 
> squid2 ? 192.168.2.129
> 
> zone1 ? 192.168.3.0/24 with hosts having a proxy configured as
> 192.168.2.32:3128
> 
> client1 ? 192.168.4.31 with a proxy configured as 192.168.2.32:3128
> 
> ?
> 
> Squid will see the real-ip of the client connection as the haproxy-vip
> endpoint and not the real-client IP.? If I understand the documentation
> correctly, I should be able to perform something like the following in
> an ACL:
> 


"client" is the initiating end of a single TCP connection. It is a
relative term. The client on traffic arriving to Squid really is the
haproxy-vip IP.

There may be X-Forwarded-For header chains passed in by the client(s) on
the other side of the HAProxy. Or other clients that bypass the HAProxy
to connect directly with Squid. None of the IP's are necessarily what
you consider the "real" client.

Understanding that is important to understanding what
follow_x_Forwarded_for does and how to use it safely.


follow_x_forwarded_for tells Squid which clients it can trust to receive
X-Forwarded-For values from. In other words your HAProxy machines *only*
are trusted to deliver true XFF headers:

  acl haproxy src 192.168.2.30-192.168.2.32
  follow_x_forwarded_for allow haproxy
  follow_x_forwarded_for deny all


So whatever IPs connected to the HAProxy will be treated as Squid
client-IP instead of the HAProxy themselves. Any clients bypassing the
haproxy and connecting to Squid will remain being recorded properly as
the "real" client. And any fake values a "real" client tries to inject
will be ignored.


> 
> # create acl source references
> 
> acl zone1 src 192.168.3.0/24
> 
> acl client1 src 192.168.4.31/32
> 
> ?
> 
> # acl to test x-forwarded-for matching header
> 
> acl testing_proxy_dst dstdomain .google.com
> 
> follow_x_forwarded_for allow zone1 testing_proxy_dst
> 
> follow_x_forwarded_for allow client1 testing_proxy_dst
> 

What you are doing here is telling Squid to trust any values / lies the
zone1 or cleint1 machines send in their XFF headers - but *not* to trust
the HAProxy frontends.

I hope that makes it clear what is going on.


Amos


From squid3 at treenet.co.nz  Fri Jun  7 09:40:22 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Jun 2019 21:40:22 +1200
Subject: [squid-users] client delay pools
In-Reply-To: <CAEUA182hRBdrJaJFHHaES5mxCPqXqpKfAhnYUC20mw4aVfXojA@mail.gmail.com>
References: <CAEUA182hRBdrJaJFHHaES5mxCPqXqpKfAhnYUC20mw4aVfXojA@mail.gmail.com>
Message-ID: <9b04ed74-cd20-91c3-e242-fdaac6ae1e3e@treenet.co.nz>

On 7/06/19 6:43 pm, Techie wrote:
> Hello,
> 
> Running Squid 3.5 on Centos
> 
> I am trying to limit uploads to AWS S3 buckets using squid to prevent
> saturation by end users.
> 
> I have tried utilizing client delay pools but they seem to have no effect.
> delay pools for limiting downloads work great but I am trying to limit
> up not down.
> 
> In the below config servera is correctly limited on download but not upload.
> Anyone get client delay pools working properly? Can you spot any error
> in my config?


At a guess I would suggest that your "servera" ACL is trying to test
values that are not available at the time the client_delay_access is
evaluated.


Amos


From squid3 at treenet.co.nz  Fri Jun  7 10:41:00 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Jun 2019 22:41:00 +1200
Subject: [squid-users] Questions about connection pooling to origins
 when using squid as a HTTPS forward egress proxy
In-Reply-To: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
References: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
Message-ID: <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>

On 7/06/19 9:30 pm, Srikanth Raju wrote:
> Hello!
> We are planning to use squid as a forward egress proxy to whitelist
> domains. In general, we configured it to whitelist/blacklist domains
> based on the examples in the site and this seems to work with peek and
> splice on our preliminary tests as a transparent egress proxy. We're
> doing this with an AWS VPC using the standard techniques documented in
> their blogs
> 
> I had a few questions about the setup for some additional features
> 
>   * We want to have the ability to connection pool certain HTTPS calls
>     going externally within from within squid. This would be
>     specifically for some external partners that we know have slow
>     connection setup time and or in case of misbehaving libraries.
>   * WIth "peek and splice" method for HTTPS, this doesn't make sense,
>     since it's a TCP tunnel basically. There shouldn't be a way to
>     replay the handshake, hence it's impossible to pool at the squid layer.

Your conclusion is correct. But the true reason is that splice sets up
an end-to-end relationship for the client and server. Squid has no part
of anything happening inside the tunnel. There is *literally* no way to
coordinate N clients to share a tunnel.


>   * We need to consider 'bump' for some use cases along with our own
>     intermediate CA, which we're ok with, since we can choose the
>     domains to 'splice' and domains to 'bump'/

Correct.

>   * The biggest reason we care about TLS termination with bump is
>     because we think it might give us performance benefits along some
>     critical code paths *due to connection pooling to some slow
>     upstreams within squid.*
>   * Does squid automatically do this or does it need some extra config.
>     I was looking at 'server_connections' config var.


HTTPS connections cannot be pooled due to protocol ties at the transport
level between clients and servers. Once details of the TLS handshake are
delivered they are pinned together.

Instead Squid delivers what https:// responses it can from cache, which
is the next best thing.


> [Currently we
>     roughly follow the config in the AWS Guide]
>     <https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/>
> 

Please be aware that config is unsafe. It effectively makes an
open-proxy setup. Any client anywhere in the world can abuse the proxy
as a relay to reach any AWS hosted site.


> 
> Another thing we cared about , with a much lower priority, was HTTP/2
> translation. We would like to reap the benefits of HTTP/2 on external
> services that do support it and we connect to, but our application does
> not yet have any production-safe http2 clients(python).
> Is there any roadmap for when that will land on Squid master?
> 

HTTP/2 is not being directly worked on at present. I go back to it
occasionally, but most of my time has been taken up with improving the
HTTPS support.


Amos


From srikanth.raju at affirm.com  Fri Jun  7 11:24:53 2019
From: srikanth.raju at affirm.com (Srikanth Raju)
Date: Fri, 7 Jun 2019 04:24:53 -0700
Subject: [squid-users] Questions about connection pooling to origins
 when using squid as a HTTPS forward egress proxy
In-Reply-To: <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>
References: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
 <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>
Message-ID: <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>

>   * The biggest reason we care about TLS termination with bump is
> >     because we think it might give us performance benefits along some
> >     critical code paths *due to connection pooling to some slow
> >     upstreams within squid.*
> >   * Does squid automatically do this or does it need some extra config.
> >     I was looking at 'server_connections' config var.
>
> HTTPS connections cannot be pooled due to protocol ties at the transport
> level between clients and servers. Once details of the TLS handshake are
> delivered they are pinned together.
>
> Well, what I meant was, that if we use "bump" directive, it is effectively
terminating the  TLS connection from client at squid. And then squid
initiates a separate TLS connection to the server. with it's own shared
secret. Those connections to the servers/backends can be pooled. This means
there's a decryption/reencryption step in between. Is not that what happens
with squid?



> Instead Squid delivers what https:// responses it can from cache, which
> is the next best thing.
>
>
> > [Currently we
> >     roughly follow the config in the AWS Guide]
> >     <
> https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/
> >
> >
>
> Please be aware that config is unsafe. It effectively makes an
> open-proxy setup. Any client anywhere in the world can abuse the proxy
> as a relay to reach any AWS hosted site.
>

Ah, interesting, thank you for pointing out that detail. We're just testing
and playing around with it right now, so we're safe luckily :)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190607/042fedce/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun  7 12:58:30 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Jun 2019 00:58:30 +1200
Subject: [squid-users] Questions about connection pooling to origins
 when using squid as a HTTPS forward egress proxy
In-Reply-To: <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>
References: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
 <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>
 <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>
Message-ID: <5ba23b94-c37e-31ef-badc-94b52714fc85@treenet.co.nz>

On 7/06/19 11:24 pm, Srikanth Raju wrote:
> 
> 
>     >? ?* The biggest reason we care about TLS termination with bump is
>     >? ? ?because we think it might give us performance benefits along some
>     >? ? ?critical code paths *due to connection pooling to some slow
>     >? ? ?upstreams within squid.*
>     >? ?* Does squid automatically do this or does it need some extra
>     config.
>     >? ? ?I was looking at 'server_connections' config var.
> 
>     HTTPS connections cannot be pooled due to protocol ties at the transport
>     level between clients and servers. Once details of the TLS handshake are
>     delivered they are pinned together.
> 
> Well, what I meant was, that if we use "bump" directive, it is
> effectively terminating the? TLS connection from client at squid. And
> then squid initiates a separate TLS connection to the server. with it's
> own shared secret. Those connections to the servers/backends can be
> pooled. This means there's a decryption/reencryption step in between. Is
> not that what happens with squid?


Not to the degree needed for pooling. There are still many properties
from termination status, to token binding which require a 1:1 binding
between them.

It could potentially be done one day. But is not present yet and TLS is
in an arms race situation which makes it harder all the time to even do
SSL-Bump transparently.


It is really only possible for CDN operators to do pooling to their
origin servers. That is because they / reverse-proxies do not have to
use SSL-Bump at all.

Amos


From JOfficer at istreamfs.com  Fri Jun  7 14:03:12 2019
From: JOfficer at istreamfs.com (Joey Officer)
Date: Fri, 7 Jun 2019 14:03:12 +0000
Subject: [squid-users] Attempting to use follow_x_forwarded_for in ACL
In-Reply-To: <a375833f-952f-1130-6891-aeb5ff905442@treenet.co.nz>
References: <DM5PR19MB1579C8AE57CE893D803F7C9ECD170@DM5PR19MB1579.namprd19.prod.outlook.com>
 <a375833f-952f-1130-6891-aeb5ff905442@treenet.co.nz>
Message-ID: <DM5PR19MB1579088FD14378C793AC7EA7CD100@DM5PR19MB1579.namprd19.prod.outlook.com>

Thank you for the reply Amos.  I went back and read the documentation yesterday with a fresh set of eyes and realized what I was doing wrong.  Your summary at the end of your reply is exactly where I ended up once I understood what the follow_x_forwarded_for directive was doing.  To be fair, the documentation is straight-forward enough, I simply mis-understood what it was really doing (might have been lack of sleep).  I ended up turning on debug and reading through cache.log to see the rule and it's lack of matching (that's when I went back and re-read the docs - having my light bulb moment).

I modified my squid.conf almost exactly as you referenced (I didn't know I could put in a range of IPs like that for the haproxy hosts, so thanks!) and re-tested.  The end result is exactly what I was expecting / hoping for.  Hopefully this thread can serve as a clearer example for future generations ;)

Cheers,
Joey


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Friday, June 7, 2019 4:34 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Attempting to use follow_x_forwarded_for in ACL

On 7/06/19 2:38 am, Joey Officer wrote:
> Greetings all,
> 
> ?
> 
> squid.conf references the ability to use the x-forwarded-for header in 
> ACLs by using the follow_x_forwarded_for in ACL, referenced here:
> http://www.squid-cache.org/Doc/config/follow_x_forwarded_for/ and here 
> http://www.squid-cache.org/Doc/config/acl_uses_indirect_client/
> 
> ?
> 
> There appear to be three pre-reqs, which I?ve met:
> 
> ?
> 
> squid.conf: acl_uses_indirect_client on
> 
> squid built with --enable-follow-x-forwarded-for (confirmed)
> 
> and the appropriate ACL entries (see below)
> 


More correctly; configuration to tell Squid which clients to trust X-Forwarded-For headers from.


> ?
> 
> In my scenario, I have a pair of squid hosts (squid ver 3.5.6) sitting 
> behind a pair of haproxy/keepalived hosts which provide balancing and 
> redundancy/availability.? Haproxy is configured to add an 
> x-forwarded-for header (if one doesn?t already exist) and I can see 
> the x-forwarded-for header in the request if I run packet capture on 
> the squid hosts.
> 
> ?
> 
> For this scenario, I have a box sitting on the 192.168.4.0/24 network, 
> which has access to three IPs on 192.168.2.0/24 network (2.30, 2.31, 
> and
> 2.32 which are haproxy1, haproxy2, and keepalived vIP respectively). 
> Hosts wanting internet access must using the haproxy-vip as a proxy 
> IP, which is then forwarded to the real squid backends.? To sum up:
> 
> ?
> 
> haproxy1 ? 192.168.2.30
> 
> haproxy2 ? 192.168.2.31
> 
> haproxy-vip ? 192.168.2.32
> 
> squid1 ? 192.168.2.128
> 
> squid2 ? 192.168.2.129
> 
> zone1 ? 192.168.3.0/24 with hosts having a proxy configured as
> 192.168.2.32:3128
> 
> client1 ? 192.168.4.31 with a proxy configured as 192.168.2.32:3128
> 
> ?
> 
> Squid will see the real-ip of the client connection as the haproxy-vip 
> endpoint and not the real-client IP.? If I understand the 
> documentation correctly, I should be able to perform something like 
> the following in an ACL:
> 


"client" is the initiating end of a single TCP connection. It is a relative term. The client on traffic arriving to Squid really is the haproxy-vip IP.

There may be X-Forwarded-For header chains passed in by the client(s) on the other side of the HAProxy. Or other clients that bypass the HAProxy to connect directly with Squid. None of the IP's are necessarily what you consider the "real" client.

Understanding that is important to understanding what follow_x_Forwarded_for does and how to use it safely.


follow_x_forwarded_for tells Squid which clients it can trust to receive X-Forwarded-For values from. In other words your HAProxy machines *only* are trusted to deliver true XFF headers:

  acl haproxy src 192.168.2.30-192.168.2.32
  follow_x_forwarded_for allow haproxy
  follow_x_forwarded_for deny all


So whatever IPs connected to the HAProxy will be treated as Squid client-IP instead of the HAProxy themselves. Any clients bypassing the haproxy and connecting to Squid will remain being recorded properly as the "real" client. And any fake values a "real" client tries to inject will be ignored.


> 
> # create acl source references
> 
> acl zone1 src 192.168.3.0/24
> 
> acl client1 src 192.168.4.31/32
> 
> ?
> 
> # acl to test x-forwarded-for matching header
> 
> acl testing_proxy_dst dstdomain .google.com
> 
> follow_x_forwarded_for allow zone1 testing_proxy_dst
> 
> follow_x_forwarded_for allow client1 testing_proxy_dst
> 

What you are doing here is telling Squid to trust any values / lies the
zone1 or cleint1 machines send in their XFF headers - but *not* to trust the HAProxy frontends.

I hope that makes it clear what is going on.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Fri Jun  7 16:04:02 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 7 Jun 2019 10:04:02 -0600
Subject: [squid-users] Questions about connection pooling to origins
 when using squid as a HTTPS forward egress proxy
In-Reply-To: <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>
References: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
 <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>
 <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>
Message-ID: <4ec949f2-2794-5f9a-40ee-d7d182b73316@measurement-factory.com>

On 6/7/19 5:24 AM, Srikanth Raju wrote:
> 
> 
>     >? ?* The biggest reason we care about TLS termination with bump is
>     >? ? ?because we think it might give us performance benefits along some
>     >? ? ?critical code paths *due to connection pooling to some slow
>     >? ? ?upstreams within squid.*
>     >? ?* Does squid automatically do this or does it need some extra
>     config.
>     >? ? ?I was looking at 'server_connections' config var.
> 
>     HTTPS connections cannot be pooled due to protocol ties at the transport
>     level between clients and servers. Once details of the TLS handshake are
>     delivered they are pinned together.
> 
> Well, what I meant was, that if we use "bump" directive, it is
> effectively terminating the? TLS connection from client at squid.

If you bump at step 1 (a.k.a. client-first bumping), then yes, TLS
connections to server can be pooled. I do not know whether Squid
supports that today, but there is no fundamental reason why it cannot.
Unfortunately, in most cases, you cannot successfully bump at step 1.

If you bump at step 2+, then the resulting client-Squid and Squid-server
connection pairs are based on the client handshake. Yes, Squid
establishes its own TLS connection to the origin server, but it does so
while mimicking TLS client properties. Reusing that server connection
for another client would be risky -- the second client can request some
TLS security features that the first client did not care about, and
Squid would have to essentially ignore the second client requirements
when talking to the origin server using a reused to-server connection
established on behalf of the first TLS client. It would be possible to
add an option that enables such risky reuse AFAICT, but that reuse
should not be done by default.


> And
> then squid initiates a separate TLS connection to the server. with it's
> own shared secret. Those connections to the servers/backends can be
> pooled. This means there's a decryption/reencryption step in between. Is
> not that what happens with squid?

Yes, it is, but pooling is disabled (i.e. the to-server connection is
"pinned" to the from-client connection) due to security concerns
discussed above.

Alex.


From srikanth.raju at affirm.com  Tue Jun 11 03:09:31 2019
From: srikanth.raju at affirm.com (Srikanth Raju)
Date: Mon, 10 Jun 2019 20:09:31 -0700
Subject: [squid-users] Questions about connection pooling to origins
 when using squid as a HTTPS forward egress proxy
In-Reply-To: <4ec949f2-2794-5f9a-40ee-d7d182b73316@measurement-factory.com>
References: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
 <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>
 <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>
 <4ec949f2-2794-5f9a-40ee-d7d182b73316@measurement-factory.com>
Message-ID: <CALnQT_5mvpQdQrypoOmuYqMBqPe3Od_N3E=fB+aBwtCDRnLDqQ@mail.gmail.com>

I see, thank you for this explanation and clarification Alex!

Would you like me to file a bug report for this and is this work you think
that might be of interest to others who use squid?

On Fri, Jun 7, 2019 at 9:04 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 6/7/19 5:24 AM, Srikanth Raju wrote:
> >
> >
> >     >   * The biggest reason we care about TLS termination with bump is
> >     >     because we think it might give us performance benefits along
> some
> >     >     critical code paths *due to connection pooling to some slow
> >     >     upstreams within squid.*
> >     >   * Does squid automatically do this or does it need some extra
> >     config.
> >     >     I was looking at 'server_connections' config var.
> >
> >     HTTPS connections cannot be pooled due to protocol ties at the
> transport
> >     level between clients and servers. Once details of the TLS handshake
> are
> >     delivered they are pinned together.
> >
> > Well, what I meant was, that if we use "bump" directive, it is
> > effectively terminating the  TLS connection from client at squid.
>
> If you bump at step 1 (a.k.a. client-first bumping), then yes, TLS
> connections to server can be pooled. I do not know whether Squid
> supports that today, but there is no fundamental reason why it cannot.
> Unfortunately, in most cases, you cannot successfully bump at step 1.
>
> If you bump at step 2+, then the resulting client-Squid and Squid-server
> connection pairs are based on the client handshake. Yes, Squid
> establishes its own TLS connection to the origin server, but it does so
> while mimicking TLS client properties. Reusing that server connection
> for another client would be risky -- the second client can request some
> TLS security features that the first client did not care about, and
> Squid would have to essentially ignore the second client requirements
> when talking to the origin server using a reused to-server connection
> established on behalf of the first TLS client. It would be possible to
> add an option that enables such risky reuse AFAICT, but that reuse
> should not be done by default.
>
>
> > And
> > then squid initiates a separate TLS connection to the server. with it's
> > own shared secret. Those connections to the servers/backends can be
> > pooled. This means there's a decryption/reencryption step in between. Is
> > not that what happens with squid?
>
> Yes, it is, but pooling is disabled (i.e. the to-server connection is
> "pinned" to the from-client connection) due to security concerns
> discussed above.
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190610/ffae7a57/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 11 03:53:51 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 10 Jun 2019 21:53:51 -0600
Subject: [squid-users] Questions about connection pooling to origins
 when using squid as a HTTPS forward egress proxy
In-Reply-To: <CALnQT_5mvpQdQrypoOmuYqMBqPe3Od_N3E=fB+aBwtCDRnLDqQ@mail.gmail.com>
References: <CALnQT_4tm9xZDGb3feQHbznW-mU_--_hQNL9J5gP0aF99+hMPA@mail.gmail.com>
 <8e3c7ff6-ef54-165b-0b72-bb2b6fb60010@treenet.co.nz>
 <CALnQT_6tS+=56AKZNvQ2w4isFb4yQ+TQtf84AA-PQD_4-KNZoA@mail.gmail.com>
 <4ec949f2-2794-5f9a-40ee-d7d182b73316@measurement-factory.com>
 <CALnQT_5mvpQdQrypoOmuYqMBqPe3Od_N3E=fB+aBwtCDRnLDqQ@mail.gmail.com>
Message-ID: <ee8ee875-f80a-15cc-f222-cfd21f8211bf@measurement-factory.com>

On 6/10/19 9:09 PM, Srikanth Raju wrote:

> Would you like me to file a bug report for this and is this work you
> think that might be of interest to others who use squid?

You can certainly submit a feature request for optional pooling of
connections bumped at step 2+, but I do not know whether others would
support an official addition of that feature. The answer may depend on
the quality of your arguments and the commonality of your use case.

Alex.


> On Fri, Jun 7, 2019 at 9:04 AM Alex Rousskov wrote:
> 
>     On 6/7/19 5:24 AM, Srikanth Raju wrote:
>     >
>     >
>     >? ? ?>? ?* The biggest reason we care about TLS termination with
>     bump is
>     >? ? ?>? ? ?because we think it might give us performance benefits
>     along some
>     >? ? ?>? ? ?critical code paths *due to connection pooling to some slow
>     >? ? ?>? ? ?upstreams within squid.*
>     >? ? ?>? ?* Does squid automatically do this or does it need some extra
>     >? ? ?config.
>     >? ? ?>? ? ?I was looking at 'server_connections' config var.
>     >
>     >? ? ?HTTPS connections cannot be pooled due to protocol ties at the
>     transport
>     >? ? ?level between clients and servers. Once details of the TLS
>     handshake are
>     >? ? ?delivered they are pinned together.
>     >
>     > Well, what I meant was, that if we use "bump" directive, it is
>     > effectively terminating the? TLS connection from client at squid.
> 
>     If you bump at step 1 (a.k.a. client-first bumping), then yes, TLS
>     connections to server can be pooled. I do not know whether Squid
>     supports that today, but there is no fundamental reason why it cannot.
>     Unfortunately, in most cases, you cannot successfully bump at step 1.
> 
>     If you bump at step 2+, then the resulting client-Squid and Squid-server
>     connection pairs are based on the client handshake. Yes, Squid
>     establishes its own TLS connection to the origin server, but it does so
>     while mimicking TLS client properties. Reusing that server connection
>     for another client would be risky -- the second client can request some
>     TLS security features that the first client did not care about, and
>     Squid would have to essentially ignore the second client requirements
>     when talking to the origin server using a reused to-server connection
>     established on behalf of the first TLS client. It would be possible to
>     add an option that enables such risky reuse AFAICT, but that reuse
>     should not be done by default.
> 
> 
>     > And
>     > then squid initiates a separate TLS connection to the server. with
>     it's
>     > own shared secret. Those connections to the servers/backends can be
>     > pooled. This means there's a decryption/reencryption step in
>     between. Is
>     > not that what happens with squid?
> 
>     Yes, it is, but pooling is disabled (i.e. the to-server connection is
>     "pinned" to the from-client connection) due to security concerns
>     discussed above.
> 
>     Alex.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 



From tel13471167744 at gmail.com  Tue Jun 11 06:24:02 2019
From: tel13471167744 at gmail.com (afkout)
Date: Tue, 11 Jun 2019 01:24:02 -0500 (CDT)
Subject: [squid-users] Reverse Proxy
In-Reply-To: <7857d36b-ab89-51f9-ee46-1d286db02791@treenet.co.nz>
References: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>
 <752241de-5c86-e8c4-f8b8-df03925fabdc@treenet.co.nz>
 <1559570305583-0.post@n4.nabble.com>
 <7857d36b-ab89-51f9-ee46-1d286db02791@treenet.co.nz>
Message-ID: <1560234242830-0.post@n4.nabble.com>

i try ssl_bump and header_replace , not working
TCP_MISS/503 5452 GET https://steamcommunity.com/ cjm
ROUNDROBIN_PARENT/72.246.190.104 text/html

config
acl sc ssl::server_name steamcommunity.com
cache_peer steamuserimages-a.akamaihd.net parent 443 7 no-query proxy-only
round-robin name=ak1 tls tls-min-version=1.2 originserer
cache_peer_access ak1 allow sc
http_port 3128  ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/cacert.pem
key=/etc/squid/cakey.pem






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Jun 11 09:47:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Jun 2019 21:47:50 +1200
Subject: [squid-users] Reverse Proxy
In-Reply-To: <1560234242830-0.post@n4.nabble.com>
References: <CALhmUbx8iuz4vf8K11=KwYqYPhQY0h=y-KPKVunbMuvh4_a1DA@mail.gmail.com>
 <752241de-5c86-e8c4-f8b8-df03925fabdc@treenet.co.nz>
 <1559570305583-0.post@n4.nabble.com>
 <7857d36b-ab89-51f9-ee46-1d286db02791@treenet.co.nz>
 <1560234242830-0.post@n4.nabble.com>
Message-ID: <a2222ba8-df5a-74f8-cceb-b1fe57dcd6e6@treenet.co.nz>

On 11/06/19 6:24 pm, afkout wrote:
> i try ssl_bump and header_replace , not working
> TCP_MISS/503 5452 GET https://steamcommunity.com/ cjm
> ROUNDROBIN_PARENT/72.246.190.104 text/html
> 
> config
> acl sc ssl::server_name steamcommunity.com
> cache_peer steamuserimages-a.akamaihd.net parent 443 7 no-query proxy-only
> round-robin name=ak1 tls tls-min-version=1.2 originserer
> cache_peer_access ak1 allow sc
> http_port 3128  ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/cacert.pem
> key=/etc/squid/cakey.pem
> 

Ah. This is not a reverse-proxy config. This is a regular forward-proxy
that decrypts / MITM's any HTTPS going through it.


This would be a reverse-proxy for the steamcommunity.com domain:

  https_port 443 accel \
    cert=/etc/squid/cacert.pem \
    key=/etc/squid/cakey.pem

  cache_peer steamuserimages-a.akamaihd.net parent 443 0 name=ak1 \
    originserver no-query proxy-only tls-min-version=1.2

  acl sc dstdomain .steamcommunity.com
  cache_peer_access ak1 allow sc
  http_access allow sc


For multiple domains in that same https_port (note the 's') your
cert/key need to cover all of them. That could be a wildcard cert, or
one with multiple domains in its CN SubjectAltName.

Alternatively you can have multiple https_port lines with different IPs
for different certs.

If you are using the latest Squid-4, then GnuTLS builds (not OpenSSL)
can also support multiple certs. Bundle the key+cert (plus any relevant
CA chain) into one PEM file for each domain and load each file with
tls-cert= option. Like so:

  https_port 443 accel \
    tls-cert=/etc/squid/example.com.pem \
    tls-cert=/etc/squid/example.net.pem \
    tls-cert=/etc/squid/example.org.pem


Amos


From nick.srg at yandex.ru  Tue Jun 11 11:36:22 2019
From: nick.srg at yandex.ru (=?utf-8?B?0J3QuNC60LjRgtCwINCh0LXRgNGR0LPQuNC9?=)
Date: Tue, 11 Jun 2019 14:36:22 +0300
Subject: [squid-users] Empty ACL technical risks
Message-ID: <14626031560252982@myt5-a323eb993ef7.qloud-c.yandex.net>

Hi All,

If there is an empty acl in squid.conf, squid gives us warning message during restart/reconfigure.

We wonder if these warnings are just notifications for administrator, or there are some really technical risks.

Like here for example: https://bugs.launchpad.net/ubuntu/+source/squid-deb-proxy/+bug/1659567
Amos Jeffries wrote: "The check is a generic validity check used for all ACLs. Whether it is 'harmless' depends on future events at the time of checking. So just silencing or ignoring would leave a lot of nasty misconfigurations quietly accepted"

Could these "nasty misconfigurations" be made only by administrator, or is it about squid possible wrong behavior?

Are there any strong technical reasons to avoid using of empty ACLs in production environment?

And are there any news about explicit flag to indicate whether an ACL is allowed to be empty or not?

Thank you.

Nikita Seregin


From squid3 at treenet.co.nz  Wed Jun 12 05:58:19 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Jun 2019 17:58:19 +1200
Subject: [squid-users] Empty ACL technical risks
In-Reply-To: <14626031560252982@myt5-a323eb993ef7.qloud-c.yandex.net>
References: <14626031560252982@myt5-a323eb993ef7.qloud-c.yandex.net>
Message-ID: <d3680ffd-49d9-d8c7-eac5-61134c5c71be@treenet.co.nz>

On 11/06/19 11:36 pm, ?????? ??????? wrote:
> Hi All,
> 
> If there is an empty acl in squid.conf, squid gives us warning message during restart/reconfigure.
> 
> We wonder if these warnings are just notifications for administrator, or there are some really technical risks.
> 
> Like here for example: https://bugs.launchpad.net/ubuntu/+source/squid-deb-proxy/+bug/1659567
> Amos Jeffries wrote: "The check is a generic validity check used for all ACLs. Whether it is 'harmless' depends on future events at the time of checking. So just silencing or ignoring would leave a lot of nasty misconfigurations quietly accepted"
> 
> Could these "nasty misconfigurations" be made only by administrator, or is it about squid possible wrong behavior?
> 

The Ubuntu bug report you referenced is a good example why. The file
which is initially empty is explicitly being added to by non-admin
entities. Who then have an automated action to trigger reconfigure of
the running proxy.

The risk there is that those entities are not necessarily knowing what
valid ACL data is. Nor in a position to fix the resulting DoS if they
get it wrong and make Squid exit on the reconfigure.
 That breaking reconfigure may be a long time after the config change
was made.



> Are there any strong technical reasons to avoid using of empty ACLs in production environment?
> 

The main reason is that risk of DoS-ing the proxy and everyone using it
for an indeterminate amount of time until the admin can be summoned and
track down why the proxy is not running.


Another reason is every transaction handled by Squid has to spend CPU
cycles setting up access checklists, fetching the data to be tested,
then calling the processing code - even if the ACL is empty and thus
immediately returns its DUNNO result.


Which brings us to DUNNO being the third match state. So things like:

 acl foo src "/some/empty.file"
 http_access allow foo
 http_access allow !foo

... results in the surprise *access denied*.


> And are there any news about explicit flag to indicate whether an ACL is allowed to be empty or not?
> 

Nobody has submitted anything towards one.

As you noted at the start it is a *warning* message. Squid should
continue to run "fine". Provided your definition of "fine" accounts for
the above technical issues and odd behaviour.

Cheers,
Amos


From nick.srg at yandex.ru  Fri Jun 14 12:27:45 2019
From: nick.srg at yandex.ru (Nikita Seregin)
Date: Fri, 14 Jun 2019 15:27:45 +0300
Subject: [squid-users] Empty ACL technical risks
In-Reply-To: <d3680ffd-49d9-d8c7-eac5-61134c5c71be@treenet.co.nz>
References: <14626031560252982@myt5-a323eb993ef7.qloud-c.yandex.net>
 <d3680ffd-49d9-d8c7-eac5-61134c5c71be@treenet.co.nz>
Message-ID: <6201441560515265@myt2-dc4bba9bb23c.qloud-c.yandex.net>

Hi again.

Thank you for the answer.

So, as i understood, the empty acl files isn't the best option.


And may i include config files, which sometimes can become empty?


For example:

I put into /etc/squid/squid.conf next string:

       include /etc/squid/certificates.conf

And in the /etc/squid/certificates.conf i put:
       acl TRUSTED_FINGERPRINTS server_cert_fingerprint 7A:29:27:9A:DF:C4:4E:18:4D:94:E1:BB:2A:D9:09:3A:70:B1:AB:16
       acl TRUSTED_FINGERPRINTS server_cert_fingerprint 70:B1:AB:16:7A:29:27:9A:DF:C4:4E:18:4D:94:E1:BB:2A:D9:09:3A
       sslproxy_cert_sign signTrusted TRUSTED_FINGERPRINTS

Will it be OK, if i will just clear the /etc/squid/certificates.conf file in case if i don't have any fingerprints to put in, and keep the include /etc/squid/certificates.conf directive in squid.conf untouched? So in fact it will include the empty file.
Are there any technical risks?


12.06.2019, 08:58, "Amos Jeffries" <squid3 at treenet.co.nz>:
> On 11/06/19 11:36 pm, ?????? ??????? wrote:
>> ?Hi All,
>>
>> ?If there is an empty acl in squid.conf, squid gives us warning message during restart/reconfigure.
>>
>> ?We wonder if these warnings are just notifications for administrator, or there are some really technical risks.
>>
>> ?Like here for example: https://bugs.launchpad.net/ubuntu/+source/squid-deb-proxy/+bug/1659567
>> ?Amos Jeffries wrote: "The check is a generic validity check used for all ACLs. Whether it is 'harmless' depends on future events at the time of checking. So just silencing or ignoring would leave a lot of nasty misconfigurations quietly accepted"
>>
>> ?Could these "nasty misconfigurations" be made only by administrator, or is it about squid possible wrong behavior?
>
> The Ubuntu bug report you referenced is a good example why. The file
> which is initially empty is explicitly being added to by non-admin
> entities. Who then have an automated action to trigger reconfigure of
> the running proxy.
>
> The risk there is that those entities are not necessarily knowing what
> valid ACL data is. Nor in a position to fix the resulting DoS if they
> get it wrong and make Squid exit on the reconfigure.
> ?That breaking reconfigure may be a long time after the config change
> was made.
>
>> ?Are there any strong technical reasons to avoid using of empty ACLs in production environment?
>
> The main reason is that risk of DoS-ing the proxy and everyone using it
> for an indeterminate amount of time until the admin can be summoned and
> track down why the proxy is not running.
>
> Another reason is every transaction handled by Squid has to spend CPU
> cycles setting up access checklists, fetching the data to be tested,
> then calling the processing code - even if the ACL is empty and thus
> immediately returns its DUNNO result.
>
> Which brings us to DUNNO being the third match state. So things like:
>
> ?acl foo src "/some/empty.file"
> ?http_access allow foo
> ?http_access allow !foo
>
> ... results in the surprise *access denied*.
>
>> ?And are there any news about explicit flag to indicate whether an ACL is allowed to be empty or not?
>
> Nobody has submitted anything towards one.
>
> As you noted at the start it is a *warning* message. Squid should
> continue to run "fine". Provided your definition of "fine" accounts for
> the above technical issues and odd behaviour.
>
> Cheers,
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ahmed.zaeem at netstream.ps  Sat Jun 15 08:57:44 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 15 Jun 2019 11:57:44 +0300
Subject: [squid-users] Useragent request/reply headers with squid .
Message-ID: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>

Hello Folks ,

im trying to disable user agent info to be leaked out of squid using :

request_header_access User-Agent deny all
reply _header_access User-Agent deny all

squid very 3.5.x


but when i test sending the user agent info via curl info it seems squid is not removing it and passing it to the server 

curl -x    x.x.x.x:19000    -U pass:pass  -X POST https://uploadbeta.com/api/parse-user-agent/ -d "s=nUser-Agent:%20Mozilla/4.0%20(compatible;%20MSIE%207.0;%20linux%20NT%206.1)?

result ??>>>>>>>>> {"platform":"linux","browser":"MSIE","version":"7.0?}


as you see above i tried with squid to disable useragent , but in curl it seems squid leaked it 

any idea why squid leaking useragent ?



Thanks 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190615/a83f6f00/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Jun 15 09:08:55 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 15 Jun 2019 11:08:55 +0200
Subject: [squid-users] Useragent request/reply headers with squid .
In-Reply-To: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
References: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
Message-ID: <201906151108.55480.Antony.Stone@squid.open.source.it>

On Saturday 15 June 2019 at 10:57:44, --Ahmad-- wrote:

> Hello Folks ,
> 
> im trying to disable user agent info to be leaked out of squid

Just out of interest, why?

> using :
> 
> request_header_access User-Agent deny all
> reply _header_access User-Agent deny all
> 
> squid very 3.5.x

I'm not sure I understand that.

> but when i test sending the user agent info via curl info it seems squid is
> not removing it and passing it to the server
> 
> curl -x    x.x.x.x:19000    -U pass:pass  -X POST
> https://uploadbeta.com/api/parse-user-agent/ -d
> "s=nUser-Agent:%20Mozilla/4.0%20(compatible;%20MSIE%207.0;%20linux%20NT%20
> 6.1)?
> 
> result ??>>>>>>>>> {"platform":"linux","browser":"MSIE","version":"7.0?}

What happens if you try an HTTP request instead of HTTPS?

> as you see above i tried with squid to disable useragent , but in curl it
> seems squid leaked it
> 
> any idea why squid leaking useragent ?

Is this an intercept proxy, or have you set environment variables so that curl 
knows to send its request via Squid?


Antony.

-- 
"If I've told you once, I've told you a million times - stop exaggerating!"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Walter.H at mathemainzel.info  Sat Jun 15 09:10:33 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 15 Jun 2019 11:10:33 +0200
Subject: [squid-users] Useragent request/reply headers with squid .
In-Reply-To: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
References: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
Message-ID: <5D04B609.1000701@mathemainzel.info>

On 15.06.2019 10:57, --Ahmad-- wrote:
> Hello Folks ,
>
> im trying to disable user agent info to be leaked out of squid using :
>
> request_header_access User-Agent deny all
> reply _header_access User-Agent deny all
>
> squid very 3.5.x
>
the reply_header_access is sensless, remove it
and add the following

request_header_replace User-Agent Mozilla/5.0

but be aware the mass of website admins might rely on this, and you 
would not get a reply anyway ...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190615/2453e654/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190615/2453e654/attachment.bin>

From ahmed.zaeem at netstream.ps  Sat Jun 15 09:37:29 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 15 Jun 2019 12:37:29 +0300
Subject: [squid-users] Useragent request/reply headers with squid .
In-Reply-To: <5D04B609.1000701@mathemainzel.info>
References: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
 <5D04B609.1000701@mathemainzel.info>
Message-ID: <E65B5ABD-C927-4F64-9876-AB973BE55297@netstream.ps>

Guys im just trying to understand HTTP protocol and squid as GW for internet .

i just want to know how can squid deal with headers .

i just want to know how can squid prevent useragent from browser being sent to website 

Thanks 




> On 15 Jun 2019, at 12:10, Walter H. <Walter.H at mathemainzel.info> wrote:
> 
> On 15.06.2019 10:57, --Ahmad-- wrote:
>> 
>> Hello Folks ,
>> 
>> im trying to disable user agent info to be leaked out of squid using :
>> 
>> request_header_access User-Agent deny all
>> reply _header_access User-Agent deny all
>> 
>> squid very 3.5.x
>> 
> the reply_header_access is sensless, remove it
> and add the following
> 
> request_header_replace User-Agent Mozilla/5.0
> 
> but be aware the mass of website admins might rely on this, and you would not get a reply anyway ...
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat Jun 15 09:37:17 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Jun 2019 21:37:17 +1200
Subject: [squid-users] Useragent request/reply headers with squid .
In-Reply-To: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
References: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
Message-ID: <db7545a3-4ae3-d130-aee5-f179d71e1201@treenet.co.nz>

On 15/06/19 8:57 pm, --Ahmad-- wrote:
> Hello Folks ,
> 
> im trying to disable user agent info to be leaked out of squid using :
> 
> request_header_access User-Agent deny all
> reply _header_access User-Agent deny all
> 
> squid very 3.5.x
> 
> 
> but when i test sending the?user agent info via curl info it seems squid
> is not removing it and?passing it to the server?
> 
> curl -x? ? x.x.x.x:19000? ? -U pass:pass? -X POST
> https://uploadbeta.com/api/parse-user-agent/ -d
> "s=nUser-Agent:%20Mozilla/4.0%20(compatible;%20MSIE%207.0;%20linux%20NT%206.1)?
> 
> result???>>>>>>>>> {"platform":"linux","browser":"MSIE","version":"7.0?}
> 
> 
> as you see above i tried with squid to disable useragent , but in curl
> it seems squid leaked it?
> 
> any idea why squid leaking useragent ?


Besides what the others have already pointed out; you are also sending a
U-A string as message data. Not in a header.

So there is no way to tell from your test:
 * whether the HTTP message available to Squid has a U-A header at all, and
 * whether the header is in a form Squid has access to remove
(decrypted), and
 * whether the form processor is using the form data or the MIME data
(header)

In short. This test is so incorrect as to not produce even useful side
effects.

I suggest you use cache.log and "debug_options 11,2" to see what
messages and headers are entering and leaving Squid.


Amos


From Antony.Stone at squid.open.source.it  Sat Jun 15 09:41:37 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 15 Jun 2019 11:41:37 +0200
Subject: [squid-users] Useragent request/reply headers with squid .
In-Reply-To: <E65B5ABD-C927-4F64-9876-AB973BE55297@netstream.ps>
References: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
 <5D04B609.1000701@mathemainzel.info>
 <E65B5ABD-C927-4F64-9876-AB973BE55297@netstream.ps>
Message-ID: <201906151141.38079.Antony.Stone@squid.open.source.it>

On Saturday 15 June 2019 at 11:37:29, --Ahmad-- wrote:

> Guys im just trying to understand HTTP protocol and squid as GW for
> internet .

Hm, "understand" or "break" :) ?

> i just want to know how can squid deal with headers .

You *have* read the warning / advice at
http://www.squid-cache.org/Doc/config/request_header_access/
"Doing this VIOLATES the HTTP standard.  Enabling this feature could make you 
liable for problems which it causes." ?

> i just want to know how can squid prevent useragent from browser being sent
> to website

Why?  What is your purpose for this?


Antony.

-- 
I still maintain the point that designing a monolithic kernel in 1991 is a 
fundamental error.  Be thankful you are not my student.  You would not get a 
high grade for such a design :-)
 - Andrew Tanenbaum to Linus Torvalds

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Sat Jun 15 09:53:30 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 15 Jun 2019 12:53:30 +0300
Subject: [squid-users] Useragent request/reply headers with squid .
In-Reply-To: <201906151141.38079.Antony.Stone@squid.open.source.it>
References: <881A7179-BCCC-494F-A073-BA2BA10BB987@netstream.ps>
 <5D04B609.1000701@mathemainzel.info>
 <E65B5ABD-C927-4F64-9876-AB973BE55297@netstream.ps>
 <201906151141.38079.Antony.Stone@squid.open.source.it>
Message-ID: <0F704413-BD75-4604-B5D4-88AA914E8D81@netstream.ps>

ok if my question is not suitable or dangerous  ?.  i do apologise .

i will try to google it myself  .


Thank you for your time .






> On 15 Jun 2019, at 12:41, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> On Saturday 15 June 2019 at 11:37:29, --Ahmad-- wrote:
> 
>> Guys im just trying to understand HTTP protocol and squid as GW for
>> internet .
> 
> Hm, "understand" or "break" :) ?
> 
>> i just want to know how can squid deal with headers .
> 
> You *have* read the warning / advice at
> http://www.squid-cache.org/Doc/config/request_header_access/
> "Doing this VIOLATES the HTTP standard.  Enabling this feature could make you 
> liable for problems which it causes." ?
> 
>> i just want to know how can squid prevent useragent from browser being sent
>> to website
> 
> Why?  What is your purpose for this?
> 
> 
> Antony.
> 
> -- 
> I still maintain the point that designing a monolithic kernel in 1991 is a 
> fundamental error.  Be thankful you are not my student.  You would not get a 
> high grade for such a design :-)
> - Andrew Tanenbaum to Linus Torvalds
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From email+squid at charlie.is  Mon Jun 17 11:46:12 2019
From: email+squid at charlie.is (Charlie Orford)
Date: Mon, 17 Jun 2019 12:46:12 +0100
Subject: [squid-users] Prepending a string to cache_peer username
Message-ID: <b1952ab5-5b68-b392-1fe4-ca815560d165@charlie.is>

Annoyingly, one of our upstream cache_peers requires a fixed string to 
be prepended to client usernames.

I'm aware the login= option for cache_peer allows substituting * with 
client provided username and appending a fixed string to this.

Is there a way to achieve something similar if we need to prepend rather 
than append a string (perhaps a clever ACL/external_auth trick we could 
use to modify client usernames destined for this peer)?

Thanks for any suggestions.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190617/0b4855a5/attachment.htm>

From johnrefwe at mail.com  Mon Jun 17 20:33:18 2019
From: johnrefwe at mail.com (johnr)
Date: Mon, 17 Jun 2019 15:33:18 -0500 (CDT)
Subject: [squid-users] Squid Listening on many ports
Message-ID: <1560803598510-0.post@n4.nabble.com>

Hi,

I am wanting to run squid listening on many ports (~100-200). From prior
mailing list questions
(http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-multiple-ips-is-listenting-to-some-ips-with-port-and-not-all-of-ips-td4668784.html),
I see how this is possible. But, I was curious about an answer in that
thread...



A brief glance through the squid code seems as if there is support for
epoll... In that case, shouldn't listening on multiple ports not have the
performance cost mentioned above? 

Thank you,

John



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ngtech1ltd at gmail.com  Tue Jun 18 05:01:53 2019
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Tue, 18 Jun 2019 08:01:53 +0300
Subject: [squid-users] Prepending a string to cache_peer username
In-Reply-To: <b1952ab5-5b68-b392-1fe4-ca815560d165@charlie.is>
References: <b1952ab5-5b68-b392-1fe4-ca815560d165@charlie.is>
Message-ID: <006c01d52592$f2fff010$d8ffd030$@gmail.com>

I believe that eCAP or ICAP can do the trick for you.

However I am not sure if it?s a good thing to pass usernames and password in WWW Http requests.

 

Eliezer

 

----

 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com



 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Charlie Orford
Sent: Monday, June 17, 2019 2:46 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Prepending a string to cache_peer username

 

Annoyingly, one of our upstream cache_peers requires a fixed string to be prepended to client usernames.

I'm aware the login= option for cache_peer allows substituting * with client provided username and appending a fixed string to this.

Is there a way to achieve something similar if we need to prepend rather than append a string (perhaps a clever ACL/external_auth trick we could use to modify client usernames destined for this peer)?

Thanks for any suggestions.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190618/fc5e7a3e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190618/fc5e7a3e/attachment.png>

From squid3 at treenet.co.nz  Tue Jun 18 05:56:40 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Jun 2019 17:56:40 +1200
Subject: [squid-users] Squid Listening on many ports
In-Reply-To: <1560803598510-0.post@n4.nabble.com>
References: <1560803598510-0.post@n4.nabble.com>
Message-ID: <5df0bfab-88f4-1957-5191-bee7211ebecc@treenet.co.nz>

On 18/06/19 8:33 am, johnr wrote:
> Hi,
> 
> I am wanting to run squid listening on many ports (~100-200). From prior
> mailing list questions
> (http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-with-multiple-ips-is-listenting-to-some-ips-with-port-and-not-all-of-ips-td4668784.html),
> I see how this is possible. But, I was curious about an answer in that
> thread...
> 
> 
> 
> A brief glance through the squid code seems as if there is support for
> epoll... In that case, shouldn't listening on multiple ports not have the
> performance cost mentioned above? 
> 

No. The problem is that each "port with an I/O" needs to be looked up in
the set of LISTEN sockets.

It does not matter whether the I/O event handler produces a small set of
I/O-only sockets (epoll, kqueue), or a large set with a small sub-set
marked as I/O pending (select, poll). Only the ones with I/O pending
have the listener lookup. When the listener set is large they both have
the same CPU loading side effect(s).


PS. please be aware (as you can see above) that quoting with HTML via
the Nabble forum does not make it to this plain-text mailing list.


Cheers
Amos


From email+squid at charlie.is  Tue Jun 18 11:34:11 2019
From: email+squid at charlie.is (Charlie Orford)
Date: Tue, 18 Jun 2019 12:34:11 +0100
Subject: [squid-users] Prepending a string to cache_peer username
In-Reply-To: <006c01d52592$f2fff010$d8ffd030$@gmail.com>
References: <b1952ab5-5b68-b392-1fe4-ca815560d165@charlie.is>
 <006c01d52592$f2fff010$d8ffd030$@gmail.com>
Message-ID: <d00a1a67-8904-490c-9925-328455fe787d@charlie.is>

On 18/06/2019 06:01, ngtech1ltd at gmail.com wrote:
>
> I believe that eCAP or ICAP can do the trick for you.
>
> However I am not sure if it?s a good thing to pass usernames and 
> password in WWW Http requests.
>
> Eliezer
>
> *From:*squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *Charlie Orford
> *Sent:* Monday, June 17, 2019 2:46 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Prepending a string to cache_peer username
>
> Annoyingly, one of our upstream cache_peers requires a fixed string to 
> be prepended to client usernames.
>
> I'm aware the login= option for cache_peer allows substituting * with 
> client provided username and appending a fixed string to this.
>
> Is there a way to achieve something similar if we need to prepend 
> rather than append a string (perhaps a clever ACL/external_auth trick 
> we could use to modify client usernames destined for this peer)?
>
> Thanks for any suggestions.
>


Thanks Eliezer. ICAP looks interesting. Do you know for sure if Squid's 
cache_peer code would indeed use the username from a modified 
Proxy-Authorization header rather than the initial authenticated 
username provided when the client originally connected to Squid (made 
available to ICAP via adaptation_send_username)?




From squid3 at treenet.co.nz  Tue Jun 18 12:15:55 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Jun 2019 00:15:55 +1200
Subject: [squid-users] Prepending a string to cache_peer username
In-Reply-To: <006c01d52592$f2fff010$d8ffd030$@gmail.com>
References: <b1952ab5-5b68-b392-1fe4-ca815560d165@charlie.is>
 <006c01d52592$f2fff010$d8ffd030$@gmail.com>
Message-ID: <78cca6e1-db9d-cb31-9e0f-624143647909@treenet.co.nz>

On 18/06/19 5:01 pm, ngtech1ltd wrote:
> I believe that eCAP or ICAP can do the trick for you.
> 
> However I am not sure if it?s a good thing to pass usernames and
> password in WWW Http requests.
> 

Only if there are no other peers, and no traffic going direct either.

Otherwise you end up broadcasting the users credentials everywhere.
Unfortunately that applies to all the header editing by Squid itself as
well.


On 17/06/19 11:46 pm, Charlie Orford wrote:
> Annoyingly, one of our upstream cache_peers requires a fixed string to
> be prepended to client usernames.

What type of fixed string exactly?


>
> I'm aware the login= option for cache_peer allows substituting * with
> client provided username and appending a fixed string to this.
>

That is wrong. There is no change to the username at all.

"cache_peer ... login=*:foo " takes the username from the client and the
password "foo" to login to that peer. The fact that Basic is the only
type of login is incidental, and the 'append' an illusion from the Basic
auth credential syntax being username then password. Other types may be
added in future that use different credential encoding.


> Is there a way to achieve something similar if we need to prepend rather
> than append a string (perhaps a clever ACL/external_auth trick we could
> use to modify client usernames destined for this peer)?
>

All the ways which come to mind impose unreasonable restrictions. Like
Eliezers idea 100% of traffic has to go to that peer or not being able
to authenticate the users in your own proxy etc.


Amos


From Joseph.Garbacik at netapp.com  Tue Jun 18 13:18:13 2019
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Tue, 18 Jun 2019 13:18:13 +0000
Subject: [squid-users] Squid Process
Message-ID: <93C63E59-272C-4105-978A-9F712CFF8D91@contoso.com>

Is there a chart or diagram somewhere to depict what key elements are processed before others in squid? For instance, is cache_peer traffic handed off to the other host before SSL decrypt, ICAP, or url writes occurs on the first host? I would assume so for performance reasons but was hoping to see something definitive as a reference.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190618/d2d0bdc8/attachment.htm>

From Ralf.Hildebrandt at charite.de  Tue Jun 18 13:37:46 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 18 Jun 2019 15:37:46 +0200
Subject: [squid-users] Log resolved IP somehow?
Message-ID: <20190618133744.tve7r7mammfnlp2g@charite.de>

>From my log:
============

Mon Jun 17 07:28:47 2019     36 10.39.68.232 TCP_DENIED/302 390 CONNECT trx.adscale.de:443 - HIER_NONE/- text/html accessRule=ensiloip -

Now I tried find out why  trx.adscale.de is being denied. I'm using squid-5 with annotate_transaction:

acl markensiloip annotate_transaction accessRule=ensiloip
acl ensiloip dst "/etc/squid5/manual-ensilo-ipblocklist.acl"
http_access deny ensiloip markensiloip


So I *DO* know that /etc/squid5/manual-ensilo-ipblocklist.acl must be
the reason for the refusal -- so I resolved trx.adscale.de and got:

# host trx.adscale.de
trx.adscale.de is an alias for san.adscale.de.edgekey.net.
san.adscale.de.edgekey.net is an alias for e9040.g.akamaiedge.net.
e9040.g.akamaiedge.net has address 95.100.198.56


So a CDN is being used. And alas:


# fgrep -c 95.100.198.56 /etc/squid5/manual-ensilo-ipblocklist.acl
0
# fgrep -c 95.100.198 /etc/squid5/manual-ensilo-ipblocklist.acl
0
# fgrep -c 95.100 /etc/squid5/manual-ensilo-ipblocklist.acl
0

So, I guss the IP must have change between to time "trx.adscale.de" was
blocked and now. 

How can I log the IP "trx.adscale.de" resolved to when the rejection happened?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid3 at treenet.co.nz  Tue Jun 18 13:58:45 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Jun 2019 01:58:45 +1200
Subject: [squid-users] Squid Process
In-Reply-To: <93C63E59-272C-4105-978A-9F712CFF8D91@contoso.com>
References: <93C63E59-272C-4105-978A-9F712CFF8D91@contoso.com>
Message-ID: <948db19b-6e3c-8d8a-43fe-ca487702888d@treenet.co.nz>

On 19/06/19 1:18 am, Garbacik, Joe wrote:
> Is there a chart or diagram somewhere to depict what key elements are
> processed before others in squid?

The architecture document (what we have) can be found at
<https://wiki.squid-cache.org/ProgrammingGuide/Architecture>

Any particular reason you want that info?


> For instance, is cache_peer traffic
> handed off to the other host before SSL decrypt, ICAP, or url writes
> occurs on the first host? I would assume so for performance reasons but
> was hoping to see something definitive as a reference. ?
> 

What exactly do you think "cache_peer traffic" is?


Amos


From squid3 at treenet.co.nz  Tue Jun 18 14:13:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Jun 2019 02:13:08 +1200
Subject: [squid-users] Log resolved IP somehow?
In-Reply-To: <20190618133744.tve7r7mammfnlp2g@charite.de>
References: <20190618133744.tve7r7mammfnlp2g@charite.de>
Message-ID: <566a2c2f-eaa5-4f9d-4473-2a7577246d83@treenet.co.nz>

On 19/06/19 1:37 am, Ralf Hildebrandt wrote:
> From my log:
> ============
> 
> Mon Jun 17 07:28:47 2019     36 10.39.68.232 TCP_DENIED/302 390 CONNECT trx.adscale.de:443 - HIER_NONE/- text/html accessRule=ensiloip -
> 
> Now I tried find out why  trx.adscale.de is being denied. I'm using squid-5 with annotate_transaction:
> 
> acl markensiloip annotate_transaction accessRule=ensiloip
> acl ensiloip dst "/etc/squid5/manual-ensilo-ipblocklist.acl"
> http_access deny ensiloip markensiloip
> 
> 
> So I *DO* know that /etc/squid5/manual-ensilo-ipblocklist.acl must be
> the reason for the refusal -- so I resolved trx.adscale.de and got:
> 
> # host trx.adscale.de
> trx.adscale.de is an alias for san.adscale.de.edgekey.net.
> san.adscale.de.edgekey.net is an alias for e9040.g.akamaiedge.net.
> e9040.g.akamaiedge.net has address 95.100.198.56
> 
> 
> So a CDN is being used. And alas:
> 
> 
> # fgrep -c 95.100.198.56 /etc/squid5/manual-ensilo-ipblocklist.acl
> 0
> # fgrep -c 95.100.198 /etc/squid5/manual-ensilo-ipblocklist.acl
> 0
> # fgrep -c 95.100 /etc/squid5/manual-ensilo-ipblocklist.acl
> 0
> 
> So, I guss the IP must have change between to time "trx.adscale.de" was
> blocked and now. 

Or,
 its IPv6 is listed.

Or,
 your test was done from a different machine than the one running Squid.

Or,
 the DNS query packet arrived at Akamai via a different DNS recursive
resolver this time.

Or,
 the Internet route between your network and Akamai DNS changed slightly.

(Don't we all love query-dependent DNS responses.)

> 
> How can I log the IP "trx.adscale.de" resolved to when the rejection happened?
> 

Your DNS resolver logs should contain that info.

If the check is close to the transaction time, then your Squid ipcache
manager report should list all the IPs that domain has.

Other than that, your best bet would be the debug trace of what ACLs are
matching. "debug_options 28,4" should do it.


Amos


From Ralf.Hildebrandt at charite.de  Tue Jun 18 14:20:20 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 18 Jun 2019 16:20:20 +0200
Subject: [squid-users] [ext] Re:  Log resolved IP somehow?
In-Reply-To: <566a2c2f-eaa5-4f9d-4473-2a7577246d83@treenet.co.nz>
References: <20190618133744.tve7r7mammfnlp2g@charite.de>
 <566a2c2f-eaa5-4f9d-4473-2a7577246d83@treenet.co.nz>
Message-ID: <20190618142018.ptr7ovlgtaxahbcy@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:

> Or,
>  its IPv6 is listed.

No ipv6 here.
 
> Or,
>  your test was done from a different machine than the one running Squid.

Nope.
 
> Or,
>  the DNS query packet arrived at Akamai via a different DNS recursive
> resolver this time.

Could be
 
> Or,
>  the Internet route between your network and Akamai DNS changed slightly.

Unlikely, but possible.
 
> (Don't we all love query-dependent DNS responses.)

:D

> > How can I log the IP "trx.adscale.de" resolved to when the rejection happened?
> > 
> 
> Your DNS resolver logs should contain that info.

I could indeed log that. Don't know if I'm doing that right now, but
probably not.
 
> If the check is close to the transaction time, then your Squid ipcache
> manager report should list all the IPs that domain has.
> 
> Other than that, your best bet would be the debug trace of what ACLs are
> matching. "debug_options 28,4" should do it.


Well, I do know which ACL is matching, just not which line.

I'll go for the query log.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Ralf.Hildebrandt at charite.de  Tue Jun 18 14:32:27 2019
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 18 Jun 2019 16:32:27 +0200
Subject: [squid-users] [ext] Re:  Log resolved IP somehow?
In-Reply-To: <20190618142018.ptr7ovlgtaxahbcy@charite.de>
References: <20190618133744.tve7r7mammfnlp2g@charite.de>
 <566a2c2f-eaa5-4f9d-4473-2a7577246d83@treenet.co.nz>
 <20190618142018.ptr7ovlgtaxahbcy@charite.de>
Message-ID: <20190618143225.oncvtupe7sy655an@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:

> I'll go for the query log.

Jun 18 16:29:08 proxy-cvk-1 unbound[42287]: [1560868148] unbound[42287:1] info: response for ih.adscale.de. A IN
Jun 18 16:29:08 proxy-cvk-1 unbound[42287]: [1560868148] unbound[42287:1] info: reply from <adscale.de.> 193.108.91.117#53

...

Jun 18 16:30:08 proxy-cvk-1 unbound[42287]: [1560868208] unbound[42287:1] info: response for ih.adscale.de. A IN
Jun 18 16:30:08 proxy-cvk-1 unbound[42287]: [1560868208] unbound[42287:1] info: reply from <adscale.de.> 184.85.248.65#53

...

Jun 18 16:30:08 proxy-cvk-1 unbound[42287]: [1560868208] unbound[42287:1] info: response for ih.adscale.de. A IN
Jun 18 16:30:08 proxy-cvk-1 unbound[42287]: [1560868208] unbound[42287:1] info: reply from <g.akamaiedge.net.> 2.16.186.150#53

The fun never stops

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From email+squid at charlie.is  Tue Jun 18 14:58:15 2019
From: email+squid at charlie.is (Charlie Orford)
Date: Tue, 18 Jun 2019 15:58:15 +0100
Subject: [squid-users] Prepending a string to cache_peer username
In-Reply-To: <78cca6e1-db9d-cb31-9e0f-624143647909@treenet.co.nz>
References: <b1952ab5-5b68-b392-1fe4-ca815560d165@charlie.is>
 <006c01d52592$f2fff010$d8ffd030$@gmail.com>
 <78cca6e1-db9d-cb31-9e0f-624143647909@treenet.co.nz>
Message-ID: <bb0e89b4-ee8a-2dda-6d6d-6646aecc5a62@charlie.is>


On 18/06/2019 13:15, Amos Jeffries wrote:
> On 17/06/19 11:46 pm, Charlie Orford wrote:
>> Annoyingly, one of our upstream cache_peers requires a fixed string to
>> be prepended to client usernames.
> What type of fixed string exactly?

It's a special meaning string to control how that peer itself further 
routes? traffic.

>> I'm aware the login= option for cache_peer allows substituting * with
>> client provided username and appending a fixed string to this.
>>
> That is wrong. There is no change to the username at all.
>
> "cache_peer ... login=*:foo " takes the username from the client and the
> password "foo" to login to that peer. The fact that Basic is the only
> type of login is incidental, and the 'append' an illusion from the Basic
> auth credential syntax being username then password. Other types may be
> added in future that use different credential encoding.

Sorry, I was being brief in my initial email. When I said substituting 
"*" I meant the star is substituted with the client username plus any 
additional text that might be specified to the right of the star. As per 
squid docs: "The star can optionally be followed by some extra 
information which is added to the username."

So if I configure the cache_peer with login=*-foo:bar; and a client 
connects to squid with username "charlie" then the peer will receive 
"charlie-foo" as the username.

>
>> Is there a way to achieve something similar if we need to prepend rather
>> than append a string (perhaps a clever ACL/external_auth trick we could
>> use to modify client usernames destined for this peer)?
>>
> All the ways which come to mind impose unreasonable restrictions. Like
> Eliezers idea 100% of traffic has to go to that peer or not being able
> to authenticate the users in your own proxy etc.

Actually, in our case this may be acceptable. The user credentials are 
not sensitive and no direct traffic leaves Squid (everything one way or 
another goes out via a peer). We randomly generate them client side and 
have a custom authenticator (using auth_param) that blindly auths 
everything (actual access to squid is enforced by IP address). This 
approach combined with the "userhash" peer selection method allows us 
evenly distribute client traffic across a number of upstream peers but 
gives a client a way to stay with a specific peer if they want to 
(assuming that peer is "up"). We further control the set of upstream 
peers a particular client has access to by using myportname ACLs in 
combination with cache_peer_access ACLs.

It's a strange setup but suits our needs well.






From shekhar.satyanarayana at hpe.com  Wed Jun 19 04:13:35 2019
From: shekhar.satyanarayana at hpe.com (Satyanarayana, Shekhar)
Date: Wed, 19 Jun 2019 04:13:35 +0000
Subject: [squid-users] Squid4 forward proxy to upgrade from ws to wss
In-Reply-To: <6B4F4813-51FC-463B-9960-1009DF06697E@hpe.com>
References: <6B4F4813-51FC-463B-9960-1009DF06697E@hpe.com>
Message-ID: <FA0ED5E2-8B58-4E22-B9CD-8F5CB61066A7@hpe.com>

Hi Squid Community,

I am relatively new to Squid and I am facing the following issue, would truly appreciate if you could help.

Squid4.6 is used as a forward proxy to convert all traffic to secure traffic.
The configuration of squid is very simple, it allows all traffic and uses urlrewrite.pl to replace "http" to "https".(SSL-BUMP is NOT used) Squid proxy has tls_outgoing_options set, so the following works:

  client(http) -----> Squid ------> Server(https)


Now, I am trying to replicate the same with websockets.
There are 3 test cases,
1.
   client(ws)------> Squid -----> Server(ws)

2.
   client(wss) ------> Squid -----> Server(wss)

3
   client(ws) ------> Squid -----> Server(wss)


The first two cases work with squid, but the third one does not work. And I only need the third option.
I have given debug logs for urlrewrite.pl to show the exact request received for a websocket connection, and the following is the log:
Here port 8080: is server and port 3128: is squid

DEBUG:root:localhost:8080 127.0.0.1/localhost - CONNECT myip=127.0.0.1 myport=3128


Even wireshark shows the same,
1. CONNECT HTTP 1.1
2. GET
3. upgrade protocol.

Question:
1.Is there any way to upgrade a websocket connection to secure websocket using squid4.6?
2.Or say I use wss-client (without certificate) and a wss-server(with certificates), is there a way to inform squid to use its own certificates even mentioned in "tls_outgoing_options" to establish the connection?

REQUIRED:
Client will always send a unsecure traffic HTTP/WS
and Squid should upgrade it to HTTPS/WSS.
In our application setup, we use our own openssl libraries to create certificates - which cannot be included in the (client.go) go-tls package, so we use squid proxy to use the certificates generated by our own openssl libraries.
Client and Forward-Proxy (Squid) are both in our specific environment, so squid.conf is very simple and allows all traffic.
And we need mutual cert authentication.

SQUID CONF CODE

                #
                # Recommended minimum configuration:
                #

                # Example rule allowing access from your local networks.
                # Adapt to list your (internal) IP networks from where browsing
                # should be allowed
                acl localhost src 127.0.0.1

                acl SSL_ports port 443
                acl Safe_ports port 443 # https
                acl Safe_ports port 80  # http
                acl CONNECT method CONNECT

                http_access deny !Safe_ports
                http_access deny CONNECT !SSL_ports
                http_access allow localhost
                http_access deny all

                # Squid normally listens to port 3128
                http_port 3128

                url_rewrite_program /etc/squid/urlrewrite.pl
                url_rewrite_access allow  all
                tls_outgoing_options cert=/etc/squid/proxy.crt
                tls_outgoing_options key=/etc/squid/proxy.key
                tls_outgoing_options cafile=/etc/squid/serverauth.crt


urlrewrite CODE

                #!/usr/bin/perl
                select(STDOUT);
                $| = 1;
                while (<>) {
                                #print STDOUT "OK rewrite-url=\"https://google.com\"\n";

                                if (/^(|\d+\s+)((\w+):\/+)([^\/:]+)(|:(\d+))(|\/\S*)(|\s.*)$/) {
                                                my $channel = $1;
                                                my $protocolClean = $3;
                                                my $domain = $4;
                                                my $port = $5;
                                                my $portClean = $6;
                                                my $urlPath = $7;

                                if ($protocolClean eq 'http' ){#&& ($port eq '' || $portClean eq '80')) {
                                                   print STDOUT "${channel}OK rewrite-url=\"https://${domain}${port}${urlPath}\"\n";
                                   #print STDOUT "${channel}OK rewrite-url=\"https://google.com\"\n";
                                } else {
                                                   print STDOUT "${channel}ERR\n";
                                }
                                }
                }

Thank you,
Shekhar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190619/05621921/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 19 06:31:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Jun 2019 18:31:12 +1200
Subject: [squid-users] Squid4 forward proxy to upgrade from ws to wss
In-Reply-To: <FA0ED5E2-8B58-4E22-B9CD-8F5CB61066A7@hpe.com>
References: <6B4F4813-51FC-463B-9960-1009DF06697E@hpe.com>
 <FA0ED5E2-8B58-4E22-B9CD-8F5CB61066A7@hpe.com>
Message-ID: <b3731828-8005-ebda-9cab-47eacc74505e@treenet.co.nz>

On 19/06/19 4:13 pm, Satyanarayana, Shekhar wrote:
> Hi Squid Community,
> 
> I am relatively new to Squid and I am facing the following issue, would
> truly appreciate if you could help.
> 
> Squid4.6 is used as a forward proxy to convert all traffic to secure
> traffic.
> 
> The configuration of squid is very simple, it allows all traffic and
> uses urlrewrite.pl to replace "http" to "https".

What you are doing is actually the opposite to secure. Letting the
server think the traffic is secure so it passes on confidential or
privacy sensitive information - then exposing all that within clear-text
HTTP and again within the client itself.


> 
> Question:
> 
> 1.Is there any way to upgrade a websocket connection to secure websocket
> using squid4.6?
> 

No. Squid does not support WebSockets natively.


> 2.Or say I use wss-client (without certificate) and a wss-server(with
> certificates), is there a way to inform squid to use its own
> certificates even mentioned in "tls_outgoing_options" to establish the
> connection?
> 

What Squid does is enact the CONNECT or GET request of the HTTP messages
you see with wireshark - excluding the Upgrade HTTP feature you may see
being attempted.

For the CONNECT WebSockets happens inside the tunnel. With no
interference by Squid.

For the GET either the server accepts the fallback to HTTP response. Or
rejects it and the client is expected to fallback itself to another
method of communication. eg WebSockets native port or a CONNECT tunnel.

You cannot simply turn a GET request onto a bi-directional binary
tunnel. Nor a bi-directional tunnel into a GET response. They are
entirely different syntax and incompatible concepts / semantics.

Amos


From roman.nersisyan at hpe.com  Wed Jun 19 13:12:50 2019
From: roman.nersisyan at hpe.com (Nersisyan, Roman)
Date: Wed, 19 Jun 2019 13:12:50 +0000
Subject: [squid-users] Squid4 forward proxy to upgrade from ws to wss
In-Reply-To: <b3731828-8005-ebda-9cab-47eacc74505e@treenet.co.nz>
References: <6B4F4813-51FC-463B-9960-1009DF06697E@hpe.com>
 <FA0ED5E2-8B58-4E22-B9CD-8F5CB61066A7@hpe.com>
 <b3731828-8005-ebda-9cab-47eacc74505e@treenet.co.nz>
Message-ID: <08D74A92-90D1-469F-B053-6FB7485C6BD5@hpe.com>

Thank you, Amos,

In scenario that Shekhar described it is acceptable to have unsecure clinet<->squid communication. There would be actually only one client and and the difficulty we are facing is that our websocket client implementation is unable to interact with specific certificates using openssl. Ultimate goal is that client (via squid) should represent to server certificates that openssl have access. Thus, we are considering squid. 

Based on your answer I assume it is not possible to configure squid in such a way, please correct me if I'm wrong.
Seems our scenario is similar to the option #3 described here: http://lists.squid-cache.org/pipermail/squid-users/2017-January/013953.html


Thanks,
Roman
 

?On 6/18/19, 11:31 PM, "squid-users on behalf of Amos Jeffries" <squid-users-bounces at lists.squid-cache.org on behalf of squid3 at treenet.co.nz> wrote:

    On 19/06/19 4:13 pm, Satyanarayana, Shekhar wrote:
    > Hi Squid Community,
    > 
    > I am relatively new to Squid and I am facing the following issue, would
    > truly appreciate if you could help.
    > 
    > Squid4.6 is used as a forward proxy to convert all traffic to secure
    > traffic.
    > 
    > The configuration of squid is very simple, it allows all traffic and
    > uses urlrewrite.pl to replace "http" to "https".
    
    What you are doing is actually the opposite to secure. Letting the
    server think the traffic is secure so it passes on confidential or
    privacy sensitive information - then exposing all that within clear-text
    HTTP and again within the client itself.
    
    
    > 
    > Question:
    > 
    > 1.Is there any way to upgrade a websocket connection to secure websocket
    > using squid4.6?
    > 
    
    No. Squid does not support WebSockets natively.
    
    
    > 2.Or say I use wss-client (without certificate) and a wss-server(with
    > certificates), is there a way to inform squid to use its own
    > certificates even mentioned in "tls_outgoing_options" to establish the
    > connection?
    > 
    
    What Squid does is enact the CONNECT or GET request of the HTTP messages
    you see with wireshark - excluding the Upgrade HTTP feature you may see
    being attempted.
    
    For the CONNECT WebSockets happens inside the tunnel. With no
    interference by Squid.
    
    For the GET either the server accepts the fallback to HTTP response. Or
    rejects it and the client is expected to fallback itself to another
    method of communication. eg WebSockets native port or a CONNECT tunnel.
    
    You cannot simply turn a GET request onto a bi-directional binary
    tunnel. Nor a bi-directional tunnel into a GET response. They are
    entirely different syntax and incompatible concepts / semantics.
    
    Amos
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.squid-2Dcache.org_listinfo_squid-2Dusers&d=DwIGaQ&c=C5b8zRQO1miGmBeVZ2LFWg&r=3q1cou6mQpySVdgvrT4UXuR-8zDVO5Th0-ypQm3MaSE&m=DXanVFfv4gim1_IFY24RenJExFIumvXojFN06ovaVpA&s=53LUnq1oGInZT4hR-9DXBQYqjI_OSYRktzTEhBOzQ1U&e= 
    
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 5103 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190619/e94b4c14/attachment.bin>

From rousskov at measurement-factory.com  Thu Jun 20 21:07:36 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Jun 2019 15:07:36 -0600
Subject: [squid-users] Log resolved IP somehow?
In-Reply-To: <20190618133744.tve7r7mammfnlp2g@charite.de>
References: <20190618133744.tve7r7mammfnlp2g@charite.de>
Message-ID: <26d4338c-bf70-f9c6-6214-c19c6f94e297@measurement-factory.com>

On 6/18/19 7:37 AM, Ralf Hildebrandt wrote:

> Mon Jun 17 07:28:47 2019     36 10.39.68.232 TCP_DENIED/302 390 CONNECT trx.adscale.de:443 - HIER_NONE/- text/html accessRule=ensiloip -

> Now I tried find out why  trx.adscale.de is being denied. I'm using squid-5 with annotate_transaction:

> acl markensiloip annotate_transaction accessRule=ensiloip
> acl ensiloip dst "/etc/squid5/manual-ensilo-ipblocklist.acl"
> http_access deny ensiloip markensiloip


> So I *DO* know that /etc/squid5/manual-ensilo-ipblocklist.acl must be
> the reason for the refusal

> How can I log the IP "trx.adscale.de" resolved to when the rejection happened?

You can annotate each rule in /etc/squid5/manual-ensilo-ipblocklist.acl
in addition to annotating their cumulative result. This is not a direct
answer to your question, but the trick works well for some ACL lists.

Alternatively, one could enhance Squid to optionally record (and later
log) which resolved address was used by "dst" and similar DNS-related
ACLs. This will require some non-trivial work, including getting the
configuration design right, but I think that "label the address used by
this ACL as address Foo" and "log previously labeled address Foo" could
be generally useful features.


HTH,

Alex.


From summaiya at amazon.co.uk  Fri Jun 21 10:45:06 2019
From: summaiya at amazon.co.uk (summaiya)
Date: Fri, 21 Jun 2019 05:45:06 -0500 (CDT)
Subject: [squid-users] How to enable proxy protocol v2 on squid version
	4.6.1, and NLB
Message-ID: <1561113906270-0.post@n4.nabble.com>

Hi All, 

I have deployed EC2 Egress URL Filtering Squid Proxy solution, I have used
AWS PrivateLink to centralize web filtering in explicit mode. Squid proxy
farm is implemented by a Network Load Balancer which distributes TCP
requests across multiple Target Squid proxy instances, running in separate
Availability Zones

My setup is similar to that mentioned in this blog :-
https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-use-aws-privatelink-to-secure-and-scale-web-filtering-using-explicit-proxy/

I have installed Squid version 4.6.1, but the access log do not show the
client ip address, even though I added the below rules:- 
http_port 3128 require-proxy-header
http_port 3128
proxy_protocol_access allow localnet

The proxy settings at the client are below :-
[root at ip-172-16-1-99 ~]# export | grep proxy
declare -x
http_proxy="http://vpce-05a51748abb0bfd68-4e77o32h.vpce-svc-070d1304cc7cc5b5f.eu-west-2.vpce.amazonaws.com:3128"
declare -x
https_proxy="http://vpce-05a51748abb0bfd68-4e77o32h.vpce-svc-070d1304cc7cc5b5f.eu-west-2.vpce.amazonaws.com:3128"
declare -x no_proxy="169.254.169.254

But still the access logs do not show the client ip address, am I missing
something in the solution.Do I have to enable the proxy protocol v2 at NLB
level as welll, will it break the application? 
I checked most of the similar blogs, but I did not find any proper solution.

Squid Access logs :- showing ip address of NLB not client ip address 

[root at ip-10-0-0-193 squid]# cat access.log
1560426278.960      0 10.0.0.17 TAG_NONE/400 4546 NONE error:invalid-request
- HIER_NONE/- text/html
1560426279.647      0 10.0.0.17 TAG_NONE/400 4546 NONE error:invalid-request
- HIER_NONE/- text/html

Kindly provide some steps which I need to take care at squid servers conf
file and at client instance.

Regards
Summaiya 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Jun 21 12:13:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Jun 2019 00:13:24 +1200
Subject: [squid-users] How to enable proxy protocol v2 on squid version
 4.6.1, and NLB
In-Reply-To: <1561113906270-0.post@n4.nabble.com>
References: <1561113906270-0.post@n4.nabble.com>
Message-ID: <aee01b57-5b5c-689e-09df-9f3d836c3c66@treenet.co.nz>

On 21/06/19 10:45 pm, summaiya wrote:
> Hi All, 
> 
> I have deployed EC2 Egress URL Filtering Squid Proxy solution, I have used
> AWS PrivateLink to centralize web filtering in explicit mode. Squid proxy
> farm is implemented by a Network Load Balancer which distributes TCP
> requests across multiple Target Squid proxy instances, running in separate
> Availability Zones
> 
> My setup is similar to that mentioned in this blog :-
> https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-use-aws-privatelink-to-secure-and-scale-web-filtering-using-explicit-proxy/
> 
> I have installed Squid version 4.6.1, but the access log do not show the
> client ip address, even though I added the below rules:- 
> http_port 3128 require-proxy-header
> http_port 3128

You cannot have two identical listening ports. Remove the second.

> proxy_protocol_access allow localnet
> 

This access control is supposed to *only* match true for the specific
machines who are allowed to send PROXY protocol traffic to your Squid
(aka the where A/B machinery).

WARNING: If you open it to a whole network like AWS you are effectively
allowing anyone else with AWS hosted services to use your proxy and
worse, to control what information shows up in your log files - so you
cannot see who the abuser/attacker is. Use these features with extreme
care, they actively hide attacks from your regular logging view(s).


> The proxy settings at the client are below :-
> [root at ip-172-16-1-99 ~]# export | grep proxy
> declare -x
> http_proxy="http://vpce-05a51748abb0bfd68-4e77o32h.vpce-svc-070d1304cc7cc5b5f.eu-west-2.vpce.amazonaws.com:3128"
> declare -x
> https_proxy="http://vpce-05a51748abb0bfd68-4e77o32h.vpce-svc-070d1304cc7cc5b5f.eu-west-2.vpce.amazonaws.com:3128"
> declare -x no_proxy="169.254.169.254
> 
> But still the access logs do not show the client ip address, am I missing
> something in the solution.Do I have to enable the proxy protocol v2 at NLB
> level as welll, will it break the application? 
> I checked most of the similar blogs, but I did not find any proper solution.
> 

Since you do not already know the answer to that question I suspect you
may be misunderstanding what PROXY protocol is.

PROXY is a wrapper protocol for use between two intermediaries. Such
that the frontend one can inform the backend about details of TCP
connections it is relaying.


>From your log below it looks like the NLB is the frontend and Squid the
backend. But I am not completely clear on your full HTTP route design,
so there may be other middleware agents to take into account.

Hopefully the above details can help you answer the question for
yourself about where to enable PROXY and whether its actually usable in
your topology. Keep in mind that others using it for their designs does
not mean yours is able to.


> Squid Access logs :- showing ip address of NLB not client ip address 
> 
> [root at ip-10-0-0-193 squid]# cat access.log
> 1560426278.960      0 10.0.0.17 TAG_NONE/400 4546 NONE error:invalid-request
> - HIER_NONE/- text/html
> 1560426279.647      0 10.0.0.17 TAG_NONE/400 4546 NONE error:invalid-request
> - HIER_NONE/- text/html
> 
> Kindly provide some steps which I need to take care at squid servers conf
> file and at client instance.

10.0.0.17 is connecting to your Squid and sending something which is not
a PROXY protocol header.

So yes at very least *if* that is a middleware machine; then it needs to
support sending PROXY protocol (and to have it enabled).

Amos


From julien412 at yahoo.fr  Sun Jun 23 17:04:10 2019
From: julien412 at yahoo.fr (julien412 at yahoo.fr)
Date: Sun, 23 Jun 2019 17:04:10 +0000 (UTC)
Subject: [squid-users] splash page: redirection loop
References: <48746430.1055188.1561309450503.ref@mail.yahoo.com>
Message-ID: <48746430.1055188.1561309450503@mail.yahoo.com>

Hello,
I'm trying to use Squid with Splash page and followed https://wiki.squid-cache.org/ConfigExamples/Portal/Splash but I've got an issue with a redirection loop.Connecting to any web site redirects to splash page but splash page is redirected to itself in infinite loop until squid breaks it.
This happens on Centos7/Squid 3.5.20, Ubuntu bionic/3.5.27 or 4.4 source build.

Installed with ansible role with travis testing failing on redirectionhttps://travis-ci.org/juju4/ansible-squid/jobs/549377518
This part should not happen????????????? +< HTTP/1.1 302 Found
????????????? +< Server: squid/3.5.27
????????????? +< Mime-Version: 1.0
????????????? +< Date: Sun, 23 Jun 2019 15:04:22 GMT
????????????? +< Content-Type: text/html;charset=utf-8
????????????? +< Content-Length: 0
????????????? +< Location: http://localhost/splash.php?url=http%3A%2F%2Flocalhost%2Fsplash.php%3Furl%3Dhttp%253A%252F%252Fwww.google.com%252F
????????????? +< X-Squid-Error: 403 Access Denied
????????????? +< X-Cache: MISS from default-splash-ubuntu-1804-1561301803
????????????? +< X-Cache-Lookup: NONE from default-splash-ubuntu-1804-1561301803:3128
????????????? +< Connection: keep-alive
Config extract????? external_acl_type splash_page ttl=60 concurrency=100 %SRC /usr/lib/squid/ext_session_acl -t 7200 -b /var/lib/squid/session.db
?????? acl existing_users external splash_page
?????? deny_info http://localhost/splash.php?url=%s existing_users?????? 
?????? http_access deny !existing_users
Any advices?
Using splash page standalone works as expected as well as manual call to ext_session_acl in cli.

Thanks!
Julien

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190623/146d4536/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 24 13:05:39 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Jun 2019 01:05:39 +1200
Subject: [squid-users] splash page: redirection loop
In-Reply-To: <48746430.1055188.1561309450503@mail.yahoo.com>
References: <48746430.1055188.1561309450503.ref@mail.yahoo.com>
 <48746430.1055188.1561309450503@mail.yahoo.com>
Message-ID: <ebf5d968-bb73-0024-8853-63e42e799eb9@treenet.co.nz>

On 24/06/19 5:04 am, julien412 at yahoo.fr wrote:
> Hello,
> 
> I'm trying to use Squid with Splash page and followed
> https://wiki.squid-cache.org/ConfigExamples/Portal/Splash but I've got
> an issue with a redirection loop.
> Connecting to any web site redirects to splash page but splash page is
> redirected to itself in infinite loop until squid breaks it.
> 
> This happens on Centos7/Squid 3.5.20, Ubuntu bionic/3.5.27 or 4.4 source
> build.
> 
> Installed with ansible role with travis testing failing on redirection
> https://travis-ci.org/juju4/ansible-squid/jobs/549377518
> 
> This part should not happen
> ????????????? +< HTTP/1.1 302 Found
> ????????????? +< Server: squid/3.5.27
> ????????????? +< Mime-Version: 1.0
> ????????????? +< Date: Sun, 23 Jun 2019 15:04:22 GMT
> ????????????? +< Content-Type: text/html;charset=utf-8
> ????????????? +< Content-Length: 0
> ????????????? +< Location:
> http://localhost/splash.php?url=http%3A%2F%2Flocalhost%2Fsplash.php%3Furl%3Dhttp%253A%252F%252Fwww.google.com%252F
> ????????????? +< X-Squid-Error: 403 Access Denied
> ????????????? +< X-Cache: MISS from default-splash-ubuntu-1804-1561301803
> ????????????? +< X-Cache-Lookup: NONE from
> default-splash-ubuntu-1804-1561301803:3128
> ????????????? +< Connection: keep-alive
> 
> Config extract
> ????? external_acl_type splash_page ttl=60 concurrency=100 %SRC
> /usr/lib/squid/ext_session_acl -t 7200 -b /var/lib/squid/session.db
> ?????? acl existing_users external splash_page
> ?????? deny_info http://localhost/splash.php?url=%s existing_users??????
> ?????? http_access deny !existing_users
> 
> Any advices?


Couple of things:

* this is a localhost URL. The client is expected to contact *its*
localhost, not use the proxy for the followup request. But that is not
related to the loop here.


* you need to check access.log to see whether the client src-IP is
changing between requests. If it does that is the cause of the loop.

 - the test is broken: it configures Squid to send data to
access_custom.log *instead* of access.log, then tries to use the empty
access.log as the test log output.


* please add "-d" to the session helper command line options. That
should show what the helper is doing to declare "no session" when the
client feeds back the splash URL to the proxy.



PS. If I am reading the PR which is being tested - it looks like it
changes the check from one which checks;
 - the Location URL being redirected to the splash page => OK
 - the Location URL looping => BAD
 - all non-splash URLs => BAD
to;
 - the Location has *any* URL which includes the splash page => OK
   (corollary: - the Location URL looping => OK !!)
 - all non-splash URLs => BAD

Squid is only failing this test because v3.5 eventually rejects one of
the 8KB+ long URLs generated by the loop.


There are problems with the underlying helper tests too:

   describe command('echo 10.0.0.1 concurrency=100 | ...

==>  "10.0.0.1" is an invalid concurrency channel number. Channel IDs
are integer values in current helpers. If your squid.conf contains
"concurrency=100" the channel-ID delivered to the helper will be an
integer between 0 and 99 (inclusive).

==>  "concurrency=100" is the name of the session you just asked the
helper to create.

==> combined the above problems mean your helper test is not testing the
same type of sessions as your other tests are trying to use.

Luckily the session helper does not actually care what the session name
values are, they are just opaque strings - hashed and stored for "-t N"
seconds. So this still tests that the helper works, just not in the way
apparently intended.


Amos


From sklarkin at gmail.com  Mon Jun 24 16:22:34 2019
From: sklarkin at gmail.com (oleg palukhin)
Date: Mon, 24 Jun 2019 19:22:34 +0300
Subject: [squid-users] Compile error from port on FreeBSD 11.2-RELEASE
	r342572
Message-ID: <20190624192234.2a7b7652@palukhin.co.em>

Hi list.	
Trying update to squid3-3.5.28_2 from squid3-3.5.28_1 (port on  FreeBSD
11.2-RELEASE):
"--- support.lo ---
support.cc:2203:9: error: no matching function for call to
'SSL_CTX_sess_set_get_cb' SSL_CTX_sess_set_get_cb(ctx, get_session_cb);
        ^~~~~~~~~~~~~~~~~~~~~~~
/usr/local/include/openssl/ssl.h:737:6: note: candidate function not
viable: no known conversion from 'SSL_SESSION *(SSL *, unsigned char *,
int, int *)' (aka 'ssl_session_st *(ssl_st *, unsigned char *, int, int
*)') to 'SSL_SESSION *(*)(struct ssl_st *, const unsigned char *, int,
int *)' (aka 'ssl_session_st *(*)(ssl_st *, const unsigned char *, int,
int *)') for 2nd argument void SSL_CTX_sess_set_get_cb(SSL_CTX *ctx, ^
1 error generated."

My DEFAULT_VERSIONS+=ssl=libressl, may be it`s break point? All previos
updating compilations were clean until now.
Any kick in right direction, please.

-- 
________________
Regards,
Oleg Palukhin


From julien412 at yahoo.fr  Mon Jun 24 22:59:03 2019
From: julien412 at yahoo.fr (julien412 at yahoo.fr)
Date: Mon, 24 Jun 2019 22:59:03 +0000 (UTC)
Subject: [squid-users] splash page: redirection loop
In-Reply-To: <ebf5d968-bb73-0024-8853-63e42e799eb9@treenet.co.nz>
References: <48746430.1055188.1561309450503.ref@mail.yahoo.com>
 <48746430.1055188.1561309450503@mail.yahoo.com>
 <ebf5d968-bb73-0024-8853-63e42e799eb9@treenet.co.nz>
Message-ID: <348892700.2133374.1561417143753@mail.yahoo.com>

 Thanks Amos!
Comments inline
    On Monday, June 24, 2019, 9:06:41 a.m. EDT, Amos Jeffries <squid3 at treenet.co.nz> wrote:  
 
 On 24/06/19 5:04 am, julien412 at yahoo.fr wrote:
> Hello,
> 
> I'm trying to use Squid with Splash page and followed
> https://wiki.squid-cache.org/ConfigExamples/Portal/Splash but I've got
> an issue with a redirection loop.
> Connecting to any web site redirects to splash page but splash page is
> redirected to itself in infinite loop until squid breaks it.
> 
> This happens on Centos7/Squid 3.5.20, Ubuntu bionic/3.5.27 or 4.4 source
> build.
> 
> Installed with ansible role with travis testing failing on redirection
> https://travis-ci.org/juju4/ansible-squid/jobs/549377518
> 
> This part should not happen
> ????????????? +< HTTP/1.1 302 Found
> ????????????? +< Server: squid/3.5.27
> ????????????? +< Mime-Version: 1.0
> ????????????? +< Date: Sun, 23 Jun 2019 15:04:22 GMT
> ????????????? +< Content-Type: text/html;charset=utf-8
> ????????????? +< Content-Length: 0
> ????????????? +< Location:
> http://localhost/splash.php?url=http%3A%2F%2Flocalhost%2Fsplash.php%3Furl%3Dhttp%253A%252F%252Fwww.google.com%252F
> ????????????? +< X-Squid-Error: 403 Access Denied
> ????????????? +< X-Cache: MISS from default-splash-ubuntu-1804-1561301803
> ????????????? +< X-Cache-Lookup: NONE from
> default-splash-ubuntu-1804-1561301803:3128
> ????????????? +< Connection: keep-alive
> 
> Config extract
> ????? external_acl_type splash_page ttl=60 concurrency=100 %SRC
> /usr/lib/squid/ext_session_acl -t 7200 -b /var/lib/squid/session.db
> ?????? acl existing_users external splash_page
> ?????? deny_info http://localhost/splash.php?url=%s existing_users??????
> ?????? http_access deny !existing_users
> 
> Any advices?


Couple of things:

* this is a localhost URL. The client is expected to contact *its*
localhost, not use the proxy for the followup request. But that is not
related to the loop here.

[J] Agreed. this line is commented to allow testing from localhost with splash page: `http_access allow localhost`

* you need to check access.log to see whether the client src-IP is
changing between requests. If it does that is the cause of the loop.
[J] It does not. I tested on private environment with different systems and src-IP stays the same as expected.

 - the test is broken: it configures Squid to send data to
access_custom.log *instead* of access.log, then tries to use the empty
access.log as the test log output.
[J] I don't see any test on access.log. travis after_script includes access.log for convenience and missing access_custom.log which is the one relevant here. no serverspec tests


* please add "-d" to the session helper command line options. That
should show what the helper is doing to declare "no session" when the
client feeds back the splash URL to the proxy.

[J] tried both line below but does not seem to work`external_acl_type splash_page ttl=60 concurrency=100 %SRC /usr/lib/squid/ext_session_acl -t 7200 -b /var/lib/squid/session.db -d``external_acl_type splash_page ttl=60 concurrency=100 %SRC /usr/lib/squid/ext_session_acl -d -t 7200 -b /var/lib/squid/session.db`=> from cache.log
2019/06/24 14:03:01 kid1| helperOpenServers: Starting 1/5 'ext_session_acl' processes
(ext_session_acl): invalid option -- 'd'
Usage: (ext_session_acl) [-t|-T session_timeout] [-b dbpath] [-a]
??????? -t sessiontimeout?????? Idle timeout after which sessions will be forgotten (user activity will reset)
??????? -T sessiontimeout?????? Fixed timeout after which sessions will be forgotten (regardless of user activity)
??????? -b dbpath?????????????? Path where persistent session database will be kept
??????? -a????????????????????? Active mode requiring LOGIN argument to start a session


PS. If I am reading the PR which is being tested - it looks like it
changes the check from one which checks;
 - the Location URL being redirected to the splash page => OK
 - the Location URL looping => BAD
 - all non-splash URLs => BAD
to;
 - the Location has *any* URL which includes the splash page => OK
? (corollary: - the Location URL looping => OK !!)
 - all non-splash URLs => BAD

Squid is only failing this test because v3.5 eventually rejects one of
the 8KB+ long URLs generated by the loop.


There are problems with the underlying helper tests too:

? describe command('echo 10.0.0.1 concurrency=100 | ...

==>? "10.0.0.1" is an invalid concurrency channel number. Channel IDs
are integer values in current helpers. If your squid.conf contains
"concurrency=100" the channel-ID delivered to the helper will be an
integer between 0 and 99 (inclusive).

==>? "concurrency=100" is the name of the session you just asked the
helper to create.

==> combined the above problems mean your helper test is not testing the
same type of sessions as your other tests are trying to use.

Luckily the session helper does not actually care what the session name
values are, they are just opaque strings - hashed and stored for "-t N"
seconds. So this still tests that the helper works, just not in the way
apparently intended.
[J] so, should be just an integer like that?
? describe command('echo 42 concurrency=100 | ...
[J] concurrency was just to avoid this# echo 10 | /usr/lib/squid/ext_session_acl -t 7200 -b /var/lib/squid/session.db
FATAL: /usr/lib/squid/ext_session_acl is concurrent and requires the concurrency option to be specified.[J] so the right test would be? both seems to work.
? describe command('echo 42 test concurrency=100 | ...or
? describe command('echo 42 test | ...

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190624/ad6fddc7/attachment.htm>

From jared.fox at practiv.com  Tue Jun 25 01:24:28 2019
From: jared.fox at practiv.com (Jared Fox)
Date: Tue, 25 Jun 2019 13:24:28 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid 4.4
Message-ID: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>

Hi Squid-Users

I need your help!

So i have had been using Squid 3.5.20 (installed on Amazon Linux 2)
and its acting as a transparent ssl proxy with whitelist of allowed
addresses. I want to avoid running a mitm proxy and having to add CA
certs to all services/containers etc. Traffic is routed to the squid
instance via a route-table to Interface.

" Issue 1 - upgrade from 3.5.20 to 4.4.4 (squid-4.4-4.amzn2.0.4.x86_64) "

- So my working config below does not work with 4.x but it kind of
does for 3.5.x and its appears that i require the squid-helper package
which doesn't exist for Amazon linux.
- When starting squid it tries to create an ssl database via
security_file_certgen, but this shouldnt be needed as i'm providing a
self-signed certs that doesnt get used in transparent mode but is a
hard dependency in 3.5.

" Errors produced: "

(security_file_certgen)2019/06/25 00:37:57 kid1| ERROR: No
forward-proxy ports configured.
2019/06/25 00:37:57 kid1| ERROR: No forward-proxy ports configured.
2019/06/25 00:37:57 kid1| storeDirWriteCleanLogs: Starting...
: Uninitialized SSL certificate database directory:
/var/spool/squid/ssl_db. To initialize, run "security_file_certgen -c
-s /var/spool/squid/ssl_db".
2019/06/25 00:37:57 kid1|   Finished.  Wrote 0 entries.
2019/06/25 00:37:57 kid1|   Took 0.00 seconds (  0.00 entries/sec).
2019/06/25 00:37:57 kid1| FATAL: mimeLoadIcon: cannot parse internal
URL: http://ip-10-0-60-70.ec2.internal:0/squid-internal-static/icons/silk/image.png
2019/06/25 00:37:57 kid1| Squid Cache (Version 4.4): Terminated abnormally.

" Squid config file contains: "

=======================
acl localnet src 10.0.0.0/8   # Kubernetes VPC CIDR range
acl SSL_ports port 443        # HTTPS
acl Safe_ports port 80        # HTTP
acl Safe_ports port 443       # HTTPS
acl CONNECT method CONNECT    # Traffic restriction
acl step1 at_step SslBump1    # Needed by ssl-bump

# -------------------------------
# Whitelist the following Domains
# -------------------------------

# Shorten whitelist - just for this email / Edited config here
acl domainIsWhitelisted ssl::server_name googleapis.l.google.com
acl domainIsWhitelisted ssl::server_name logging.googleapis.com
acl domainIsWhitelisted ssl::server_name cloudtrace.googleapis.com

# ----------------------------------------------

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Passively Intercepted HTTPS Traffic
https_port 9091 cert=/etc/squid/example.com.cert
key=/etc/squid/example.com.private ssl-bump intercept
acl step1 at_step SslBump1
ssl_bump peek all
ssl_bump splice domainIsWhitelisted
ssl_bump terminate all

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Logging
logformat custom1 %tg %6tr %>a %<A %Ss/%03>Hs %<st %rm %ru %ssl::>sni HTTP/%rv
access_log daemon:/var/log/squid/access_custom1.log custom1
access_log udp://127.0.0.1:5140
=======================

" Issue 2 "
- So the reason for the upgrade is that some TLS 1.2 are being blocked
when they should be whitelisted and it depends on the clients used, eg
Curl vs Netty, i believe this maybe due to unsupported tls extensions
but i can prove this as differences via tcpdump are minor.

It this because my configuration above it incorrect.

Kind regards

Jared Fox
DevOps Architect - Practiv


From squid3 at treenet.co.nz  Tue Jun 25 09:04:32 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Jun 2019 21:04:32 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid
 4.4
In-Reply-To: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
References: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
Message-ID: <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>

On 25/06/19 1:24 pm, Jared Fox wrote:
> Hi Squid-Users
> 
> I need your help!
> 
> So i have had been using Squid 3.5.20 (installed on Amazon Linux 2)
> and its acting as a transparent ssl proxy with whitelist of allowed
> addresses. I want to avoid running a mitm proxy and having to add CA
> certs to all services/containers etc. Traffic is routed to the squid
> instance via a route-table to Interface.
> 
> " Issue 1 - upgrade from 3.5.20 to 4.4.4 (squid-4.4-4.amzn2.0.4.x86_64) "
> 
> - So my working config below does not work with 4.x but it kind of
> does for 3.5.x and its appears that i require the squid-helper package
> which doesn't exist for Amazon linux.

You will have to contact whoever created the package for that.

You should be able to run the v3.5 helpers with a later Squid - but will
of course not gain any improvements that have been made in the later
version helpers.


> - When starting squid it tries to create an ssl database via
> security_file_certgen, but this shouldnt be needed as i'm providing a
> self-signed certs that doesnt get used in transparent mode but is a
> hard dependency in 3.5.

That is a bug, side effect of the helper being started even when not
needed. As a workaround it should be sufficient to create the DB for the
helper and leave it not being used.

> 
> " Errors produced: "
> 
> (security_file_certgen)2019/06/25 00:37:57 kid1| ERROR: No
> forward-proxy ports configured.
> 2019/06/25 00:37:57 kid1| ERROR: No forward-proxy ports configured.

That is correct. You only have one port (9091) - which is an intercept port.

At least one forward-proxy port is needed for a fully functional proxy.
3128 is the official one for that.


> 2019/06/25 00:37:57 kid1| storeDirWriteCleanLogs: Starting...
> : Uninitialized SSL certificate database directory:
> /var/spool/squid/ssl_db. To initialize, run "security_file_certgen -c
> -s /var/spool/squid/ssl_db".
> 2019/06/25 00:37:57 kid1|   Finished.  Wrote 0 entries.
> 2019/06/25 00:37:57 kid1|   Took 0.00 seconds (  0.00 entries/sec).
> 2019/06/25 00:37:57 kid1| FATAL: mimeLoadIcon: cannot parse internal
> URL: http://ip-10-0-60-70.ec2.internal:0/squid-internal-static/icons/silk/image.png

Side effect of not having a forward-proxy port is that all URLs for
things clients require fetching from Squid are invalid.

Amos


From squid3 at treenet.co.nz  Tue Jun 25 09:09:03 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Jun 2019 21:09:03 +1200
Subject: [squid-users] Compile error from port on FreeBSD 11.2-RELEASE
 r342572
In-Reply-To: <20190624192234.2a7b7652@palukhin.co.em>
References: <20190624192234.2a7b7652@palukhin.co.em>
Message-ID: <543495d7-3777-5f3c-1806-2c5b9943c754@treenet.co.nz>



On 25/06/19 4:22 am, oleg palukhin wrote:
> Hi list.	
> Trying update to squid3-3.5.28_2 from squid3-3.5.28_1 (port on  FreeBSD
> 11.2-RELEASE):
> "--- support.lo ---
> support.cc:2203:9: error: no matching function for call to
> 'SSL_CTX_sess_set_get_cb' SSL_CTX_sess_set_get_cb(ctx, get_session_cb);
>         ^~~~~~~~~~~~~~~~~~~~~~~
> /usr/local/include/openssl/ssl.h:737:6: note: candidate function not
> viable: no known conversion from 'SSL_SESSION *(SSL *, unsigned char *,
> int, int *)' (aka 'ssl_session_st *(ssl_st *, unsigned char *, int, int
> *)') to 'SSL_SESSION *(*)(struct ssl_st *, const unsigned char *, int,
> int *)' (aka 'ssl_session_st *(*)(ssl_st *, const unsigned char *, int,
> int *)') for 2nd argument void SSL_CTX_sess_set_get_cb(SSL_CTX *ctx, ^
> 1 error generated."
> 
> My DEFAULT_VERSIONS+=ssl=libressl, may be it`s break point? All previos
> updating compilations were clean until now.
> Any kick in right direction, please.
> 


libressl claims to be "OpenSSL version 2.0".

Please try the current stable / production release of Squid. Which
today is v4.

PS. If you are buildling your own Squid and using TLS/SSL functionality
please follow the latest release version. TLS is a very volatile
environment these past few years and almost every Squid release has
improvements for things like this.

Amos


From sklarkin at gmail.com  Tue Jun 25 09:53:57 2019
From: sklarkin at gmail.com (oleg palukhin)
Date: Tue, 25 Jun 2019 12:53:57 +0300
Subject: [squid-users] Compile error from port on FreeBSD 11.2-RELEASE
 r342572
In-Reply-To: <543495d7-3777-5f3c-1806-2c5b9943c754@treenet.co.nz>
References: <20190624192234.2a7b7652@palukhin.co.em>
 <543495d7-3777-5f3c-1806-2c5b9943c754@treenet.co.nz>
Message-ID: <20190625125357.2c416659@palukhin.co.em>


> On 25/06/19 4:22 am, oleg palukhin wrote:
> > Hi list.	
> > Trying update to squid3-3.5.28_2 from squid3-3.5.28_1 (port on
> > FreeBSD 11.2-RELEASE):
> > "--- support.lo ---
> > support.cc:2203:9: error: no matching function for call to
> > 'SSL_CTX_sess_set_get_cb' SSL_CTX_sess_set_get_cb(ctx,
> > get_session_cb); ^~~~~~~~~~~~~~~~~~~~~~~
> > /usr/local/include/openssl/ssl.h:737:6: note: candidate function not
> > viable: no known conversion from 'SSL_SESSION *(SSL *, unsigned
> > char *, int, int *)' (aka 'ssl_session_st *(ssl_st *, unsigned char
> > *, int, int *)') to 'SSL_SESSION *(*)(struct ssl_st *, const
> > unsigned char *, int, int *)' (aka 'ssl_session_st *(*)(ssl_st *,
> > const unsigned char *, int, int *)') for 2nd argument void
> > SSL_CTX_sess_set_get_cb(SSL_CTX *ctx, ^ 1 error generated."
> > 
> > My DEFAULT_VERSIONS+=ssl=libressl, may be it`s break point? All
> > previos updating compilations were clean until now.
> > Any kick in right direction, please.
> >   
> 
> 
> libressl claims to be "OpenSSL version 2.0".
> 
> Please try the current stable / production release of Squid. Which
> today is v4.
> 
> PS. If you are buildling your own Squid and using TLS/SSL
> functionality please follow the latest release version. TLS is a very
> volatile environment these past few years and almost every Squid
> release has improvements for things like this.
> 
> Amos

So, time to move to v4.

Thank you, Amos.

-- 
________________
Regards,
Oleg Palukhin


From reiniervegar at gmail.com  Tue Jun 25 17:54:10 2019
From: reiniervegar at gmail.com (Reynier Vega Reyes)
Date: Tue, 25 Jun 2019 13:54:10 -0400
Subject: [squid-users] squid basic
Message-ID: <CA+fQBbSui2Kn_Lnd9tbVJ3D4K=tRaiJ1TDcA4Uhz16nCFVRS6g@mail.gmail.com>

i need help to change basic authentication in squid 3.5. i am using auth_param
basic program /usr/lib/squid/squid_ldap_auth but i want to use auth_param
digest program /usr/lib/digest_ldap_auth  against openldap. can anyone help
me.

-- 
No dejes que el miedo al fracaso te impida jugar el juego.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190625/dd238acf/attachment.htm>

From jared.fox at practiv.com  Tue Jun 25 20:55:07 2019
From: jared.fox at practiv.com (Jared Fox)
Date: Wed, 26 Jun 2019 08:55:07 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid
	4.4
In-Reply-To: <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>
References: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
 <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>
Message-ID: <CAOR_B-fdBfA14O+keNTzmoLswAipEE1uxBQ=XX5mGNY8ivDHDg@mail.gmail.com>

Thank you Amos

I will update the Squid config and give Squid-helpers 3.5 a go today
and let you know.

Do you have any idea why only some tls 1.2 connections would work with
the whitelisting.?

Thanks
Jared
DevOps Architect - Practiv

On Tue, Jun 25, 2019 at 9:04 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 25/06/19 1:24 pm, Jared Fox wrote:
> > Hi Squid-Users
> >
> > I need your help!
> >
> > So i have had been using Squid 3.5.20 (installed on Amazon Linux 2)
> > and its acting as a transparent ssl proxy with whitelist of allowed
> > addresses. I want to avoid running a mitm proxy and having to add CA
> > certs to all services/containers etc. Traffic is routed to the squid
> > instance via a route-table to Interface.
> >
> > " Issue 1 - upgrade from 3.5.20 to 4.4.4 (squid-4.4-4.amzn2.0.4.x86_64) "
> >
> > - So my working config below does not work with 4.x but it kind of
> > does for 3.5.x and its appears that i require the squid-helper package
> > which doesn't exist for Amazon linux.
>
> You will have to contact whoever created the package for that.
>
> You should be able to run the v3.5 helpers with a later Squid - but will
> of course not gain any improvements that have been made in the later
> version helpers.
>
>
> > - When starting squid it tries to create an ssl database via
> > security_file_certgen, but this shouldnt be needed as i'm providing a
> > self-signed certs that doesnt get used in transparent mode but is a
> > hard dependency in 3.5.
>
> That is a bug, side effect of the helper being started even when not
> needed. As a workaround it should be sufficient to create the DB for the
> helper and leave it not being used.
>
> >
> > " Errors produced: "
> >
> > (security_file_certgen)2019/06/25 00:37:57 kid1| ERROR: No
> > forward-proxy ports configured.
> > 2019/06/25 00:37:57 kid1| ERROR: No forward-proxy ports configured.
>
> That is correct. You only have one port (9091) - which is an intercept port.
>
> At least one forward-proxy port is needed for a fully functional proxy.
> 3128 is the official one for that.
>
>
> > 2019/06/25 00:37:57 kid1| storeDirWriteCleanLogs: Starting...
> > : Uninitialized SSL certificate database directory:
> > /var/spool/squid/ssl_db. To initialize, run "security_file_certgen -c
> > -s /var/spool/squid/ssl_db".
> > 2019/06/25 00:37:57 kid1|   Finished.  Wrote 0 entries.
> > 2019/06/25 00:37:57 kid1|   Took 0.00 seconds (  0.00 entries/sec).
> > 2019/06/25 00:37:57 kid1| FATAL: mimeLoadIcon: cannot parse internal
> > URL: http://ip-10-0-60-70.ec2.internal:0/squid-internal-static/icons/silk/image.png
>
> Side effect of not having a forward-proxy port is that all URLs for
> things clients require fetching from Squid are invalid.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From jared.fox at practiv.com  Wed Jun 26 02:45:25 2019
From: jared.fox at practiv.com (Jared Fox)
Date: Wed, 26 Jun 2019 14:45:25 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid
	4.4
In-Reply-To: <CAOR_B-fdBfA14O+keNTzmoLswAipEE1uxBQ=XX5mGNY8ivDHDg@mail.gmail.com>
References: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
 <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>
 <CAOR_B-fdBfA14O+keNTzmoLswAipEE1uxBQ=XX5mGNY8ivDHDg@mail.gmail.com>
Message-ID: <CAOR_B-dDaActqjLuPupGHsZMfr+4YhcmsUChZPsHsVaqNWKfXA@mail.gmail.com>

Hi Amos / Squid-Users

So some good news and bad news and i'm still blocked.

== Good news ==
I have managed to get Squid 4.7 running on Centos 7.6.1810, with the
squid & squid-helpers binary rpms from
`http://www1.ngtech.co.il/repo/centos/$releasever/$basearch/`.

FYI: The squid-helpers rpm does not work in Amazon Linux 2 due to
incomplete dependencies. out of scope of this help request, as i'm not
concerned by this at the moment. It's a 3rd party rpm anyway.

The squid-helpers security_file_certgen, required a symlink to work as
the security_file_certgen is not in the default path. Symlink was
quicker than just updating PATH. `ln -s
/usr/lib64/squid/security_file_certgen
/usr/local/sbin/security_file_certgen`

Only squid.conf change (from what was previously listed) was to add:
http_port 3128

== Bad news / Major Blocker ==
https connections to cloud tracing is still being blocked, these are
TLS 1.2 and uses SNI as seen via tcpdump.

    26/Jun/2019:02:23:13    956 Kube-Node-Zone-B-IP 162.247.242.26
TCP_TUNNEL/200 3059 CONNECT 162.247.242.26:443
collector-001.newrelic.com HTTP/1.1
    26/Jun/2019:02:23:14    978 Kube-Node-Zone-B-IP 162.247.242.26
TCP_TUNNEL/200 3059 CONNECT 162.247.242.26:443
collector-001.newrelic.com HTTP/1.1
    26/Jun/2019:02:23:16     95 Kube-Node-Zone-B-IP 216.58.199.74
NONE/200 0 CONNECT 216.58.199.74:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:16     96 Kube-Node-Zone-B-IP 216.58.199.42
NONE/200 0 CONNECT 216.58.199.42:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:16     94 Kube-Node-Zone-B-IP 172.217.167.106
NONE/200 0 CONNECT 172.217.167.106:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:16     95 Kube-Node-Zone-B-IP 172.217.167.74
NONE/200 0 CONNECT 172.217.167.74:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:16     94 Kube-Node-Zone-B-IP 172.217.25.170
NONE/200 0 CONNECT 172.217.25.170:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:16     96 Kube-Node-Zone-B-IP 172.217.25.138
NONE/200 0 CONNECT 172.217.25.138:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:17     94 Kube-Node-Zone-B-IP 216.58.203.106
NONE/200 0 CONNECT 216.58.203.106:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:17     96 Kube-Node-Zone-B-IP 216.58.200.106
NONE/200 0 CONNECT 216.58.200.106:443 cloudtrace.googleapis.com
HTTP/1.1
    26/Jun/2019:02:23:17    848 Kube-Node-Zone-B-IP 162.247.242.27
TCP_TUNNEL/200 3112 CONNECT 162.247.242.27:443
collector-001.newrelic.com HTTP/1.1
    26/Jun/2019:02:23:18    994 Kube-Node-Zone-B-IP 162.247.242.27
TCP_TUNNEL/200 3059 CONNECT 162.247.242.27:443
collector-001.newrelic.com HTTP/1.1
    26/Jun/2019:02:23:19    833 Kube-Node-Zone-B-IP 162.247.242.27
TCP_TUNNEL/200 3059 CONNECT 162.247.242.27:443
collector-001.newrelic.com HTTP/1.1
    26/Jun/2019:02:23:20   1192 Kube-Node-Zone-B-IP 162.247.242.27
TCP_TUNNEL/200 3059 CONNECT 162.247.242.27:443
collector-001.newrelic.com HTTP/1.1

I really need to get Google Stackdriver Cloud Tracing working with
squid so am open to any advice / recommendations.

Kind regards

Jared Fox

DevOps Architect - Practiv


From squid3 at treenet.co.nz  Wed Jun 26 11:28:36 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Jun 2019 23:28:36 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid
 4.4
In-Reply-To: <CAOR_B-dDaActqjLuPupGHsZMfr+4YhcmsUChZPsHsVaqNWKfXA@mail.gmail.com>
References: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
 <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>
 <CAOR_B-fdBfA14O+keNTzmoLswAipEE1uxBQ=XX5mGNY8ivDHDg@mail.gmail.com>
 <CAOR_B-dDaActqjLuPupGHsZMfr+4YhcmsUChZPsHsVaqNWKfXA@mail.gmail.com>
Message-ID: <13662bfa-c96d-3419-37f4-f030107ca3c3@treenet.co.nz>

On 26/06/19 2:45 pm, Jared Fox wrote:>
> == Bad news / Major Blocker ==
> https connections to cloud tracing is still being blocked, these are
> TLS 1.2 and uses SNI as seen via tcpdump.
>
Okay, now that you have the v4 capabilities:

* Please add %ssl::bump_mode to your log so we can see easily which
SSL-Bump step each transaction is representing. The
"cloudtrace.googleapis.com" ones all say 200 (success) so it is not
clear whether that is a successful peek, or successful terminate action.


Please be aware that in your config the ssl::server_name ACL is *not*
matching the SNI in your config.
- Your ssl_bump rules say "peek all" - so peek happens on the two Hello
messages. When the serverHello has been peek'd the real server name is
available from the servers own certificate.

 So that server cert name is what the ssl_server_name matches against
when checking the "splice domainIsWhitelisted" rule.

 The dozens of servers at cloudtrace.googleapis.com call themselves
"edgecert.googleapis.com" and have a long list of sub-domains for
googleapis.com.

 ==> I suggest changing domainIsWhitelisted to match just the
".googleapis.com" part of the domain.


Alternatively you can add the new "--client-requested" flag to the ACL,
which will force it to use the SNI even after more reliable info is
available. Like so:
  acl domainIsWhitelisted ssl::server_name \
     --client-requested cloudtrace.googleapis.com


If those do not work, then someone will need to dig down into the
cache.log debug trace of what the ssl_bump ACLs are matching against.


Amos


From andersonrosario2 at gmail.com  Wed Jun 26 13:29:21 2019
From: andersonrosario2 at gmail.com (Anderson Rosario)
Date: Wed, 26 Jun 2019 09:29:21 -0400
Subject: [squid-users] Help with HTTPS SQUID 3.1.23
Message-ID: <CACtB8=AXp23Zi15EY-+5JGG=R+RyP-DFB-pmrQRdCZ7ETVffDw@mail.gmail.com>

I can not access to HTTPS sites, 3 weeks ago was working fine, without
doing any change in the topology update or config stopped and it is not
working with HTTPS sites. it keeps loading and I recieve a message from
navegators The connection to the server was reset while the page was
loading.

*here my squid config:*

#
# Recommended minimum configuration:


#
visible_hostname proxy.local.local

acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
#acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/23 # RFC1918 possible internal network
acl localnet src 192.168.0.0/23
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 85 # puerto agregado
acl Safe_ports port 883 # puerto agregado
acl Safe_ports port 5222 # puerto agregado
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_port 3128

######### AD AUTH ###########
auth_param basic program /usr/lib/squid/squid_ldap_auth -R -b
"dc=local,dc=LOCAL" -D "cn=squid,ou=proxy,dc=local,dc=LOCAL" -w "123456" -f
sAMAccountName=%s -h 192.168.0.213

auth_param basic children 5
auth_param basic realm Inserte su usuario de Windows para navegar
auth_param basic credentialsttl 1 hour

external_acl_type ldap_group %LOGIN /usr/lib/squid/squid_ldap_group -R -b
"dc=local,dc=LOCAL" -D "cn=squid,ou=proxy,dc=local,dc=LOCAL" -w "123456" -f
"(&(objectclass=person)
(sAMAccountName=%v)(memberof=cn=%a,ou=proxy,dc=local,dc=LOCAL))" -h
192.168.0.213
##############################

###### ALCs que definen los grupos ######
acl nivel0 external ldap_group nivel0
acl nivel1 external ldap_group nivel1
acl nivel2 external ldap_group nivel2
acl nivel3 external ldap_group nivel3
acl nivel4 external ldap_group nivel4
acl nivel5 external ldap_group nivel5
acl nivel6 external ldap_group nivel6

#########################################

###### Custom ACLs ######
acl rule1 url_regex -i ars humano senasa universal arsuniversal google.com
google.com.do universal.com.do .tss.gov.do tss tss.gov.do banreservas
banreservas.com universal.com arshumano arshumano.com consultascuentas
consultascuentas.arshumano.com banreservas.com.do \.jpg$

acl rule2 dstdomain .facebook.com .youtube.com .rdmusica.com .
listindiario.com .diariolibre.com .hotmail.com .outlook.com .yahoo.com .
mlb.com .espn.com .bleacherreport.com .lamega.com .espn.go.com .
espndeportes.com mail.google.com .twitter.com .hi5.com .freakshare.com .
bitshare.com .seriespepito.com .seriales.com .cuevana.tv .rapidshare.com .
supercarros.com .chatango.com .blogger.com .videobb.com .gmail.com

acl rule3 dstdomain .youtube.com .mlb.com .espn.com .bleacherreport.com .
lamega.com .espn.go.com .espndeportes.com       seriespepito.com .
seriales.com .cuevana.tv .rapidshare.com .supercarros.com .chatango.com .
blogger.com .videobb.com .sex.com .xxx.com .facebook.com

acl desc1 url_regex -i \.avi$ \.mov$ \.rar$ \.qt$ \.mpe$ \.mpeg$ \.mpg$
\.ief$ \.wav$ \.mp3$ \.mp4$ \.tar$ \.rpm$ \.zip$ \.gtar$ \.exe$ \.movie$
\.midi$ \.mid$ \.kar$ \.java$ \.dir$ sex lesbian porn porno xxx

acl rule7 dstdomain .facebook.com .hotmail.com mail.google.com .gmail.com .
yahoo.com .yahoo.es accounts.google.com

acl desc7 url_regex -i accounts gmail mail accounts.google.com

acl desc2 url_regex -i \.avi$ \.mov$ \.rar$ \.qt$ \.mpe$ \.mpeg$ \.mpg$
\.jpe$ \.jpg$ \.jpeg$ \.ief$ \.bmp$ \.wav$ \.mp3$ \.mp4$ \.tar$ \.rpm$
\.zip$ \.gtar$ \.exe$ \.movie$ \.midi$ \.mid$ \.kar$ \.dir$ \.png$ sex
lesbian porn porno

acl desc3 url_regex -i \.avi$ \.mov$ \.qt$ \.ief$  \.wav$ \.mp3$ \.mp4$
\.tar$ \.rpm$ \.gtar$ \.exe$ \.movie$ \.midi$ \.mid$ \.kar$  \.dir$ \.bmp$
\.java$ \.png$ \.mpe$ \.mpeg$ \.mpg$  lesbian porn porno xxx

acl desc4 url_regex -i \.avi$ \.png$  \.java$ \.mpe$ \.mpeg$ \.mpg$ \.mov$
\.qt$  \.rpm$\.gtar$ \.exe$ \.movie$ \.dir$ \.rar$ sex lesbian porn porno
#########################

###### Reglas de acceso ######

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports


#
http_access allow nivel6
http_access allow nivel5
http_access allow nivel4
http_access allow nivel3 !rule3 !desc3 !rule7 !desc7
http_access allow nivel2 !rule2 !desc2 !rule7 !desc7
http_access deny nivel1 !rule1
http_access allow nivel1 !desc1 !rule7 !desc7
http_access deny nivel0
http_access deny all
##############################


# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# We recommend you to use at least the following line.
hierarchy_stoplist cgi-bin ?

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190626/56dd8e31/attachment.htm>

From chirayu.patel at truecomtelesoft.com  Wed Jun 26 15:12:04 2019
From: chirayu.patel at truecomtelesoft.com (Chirayu Patel)
Date: Wed, 26 Jun 2019 20:42:04 +0530
Subject: [squid-users] Running squid in intercept mode breaks whatsapp
Message-ID: <CAOhxsyzB+9Fday-EWFbERaEOOsnQyfO7XFTYiJOV=1zjOxQZwA@mail.gmail.com>

Following is my squid config :

http_port 3129 intercept
https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
## For Captive Portal
http_port 3132 intercept
https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=1MB

#sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
#sslcrtd_children 5

# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

# TLS/SSL bumping steps
ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data
ssl_bump splice all # splice: no active bumping

pinger_enable off
digest_generation off
netdb_filename none
ipcache_size 128
fqdncache_size 128
via off
forwarded_for transparent
httpd_suppress_version_string on
cache deny all
cache_mem 0 MB
memory_pools off
shutdown_lifetime 3 seconds

#logfile_daemon /dev/null
access_log none

#acl good_url dstdomain .yahoo.com
http_access allow all

url_rewrite_program /tmp/squid/urlcat
url_rewrite_children 3 startup=1 idle=1 concurrency=10
#url_rewrite_access allow all
#url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_extras "%>a %lp %ssl::>sni"

----------------------------------------------------------------------------------------------

I am redirecting port 80 and port 443 traffic to squid..

I went through some blogs and forums which suggest that Whatsapp seems to
send non SSL traffic on port 443 (
https://developers.facebook.com/docs/whatsapp/guides/network-requirements/)

And since I am running in intercept mode, it seems to terminate non-SSL
traffic as well..

Could someone kindly suggest how to bypass this..
--
Thank You
Chirayu Patel
Truecom Telesoft
+91 8758484287
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190626/0c017d41/attachment.htm>

From jared.fox at practiv.com  Wed Jun 26 23:39:58 2019
From: jared.fox at practiv.com (Jared Fox)
Date: Thu, 27 Jun 2019 11:39:58 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid
	4.4
In-Reply-To: <13662bfa-c96d-3419-37f4-f030107ca3c3@treenet.co.nz>
References: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
 <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>
 <CAOR_B-fdBfA14O+keNTzmoLswAipEE1uxBQ=XX5mGNY8ivDHDg@mail.gmail.com>
 <CAOR_B-dDaActqjLuPupGHsZMfr+4YhcmsUChZPsHsVaqNWKfXA@mail.gmail.com>
 <13662bfa-c96d-3419-37f4-f030107ca3c3@treenet.co.nz>
Message-ID: <CAOR_B-ffSjPTXowhRQqsFWGm20_gUA2Ps1LxA3mOy1ma_WdUBg@mail.gmail.com>

Hi Amos

So i have tried the following based on your suggestions, but it is
still failing and have errors below:

1. Switched to a wildcard whitelist instead of single domain
2. Updated the logformat to provide more information, see below:
3. Add in `--client-requested`, but this made no difference.
   3a. Add to single ACL, acl domainIsWhitelisted ssl::server_name
--client-requested cloudtrace.googleapis.com
   3b. Commented out single record, switched to wildcard
   3c. Add to wildcard

Error messages and Logs:

Access Log:     26/Jun/2019:23:18:38     96 REDACTED 216.58.200.106
NONE/200 0 CONNECT 216.58.200.106:443 HTTP/1.1 SSL:
cloudtrace.googleapis.com peek Client(Subject/Tx/Neg/Sup/Cip): -
TLS/1.0 - TLS/1.2 - Server(Subject/Rx/Neg/Sup/Cip): - TLS/1.2 -
TLS/1.2 -

Cache Log:     2019/06/26 23:18:38 kid1| ERROR: negotiating TLS on FD
11: error:140920F8:SSL routines:ssl3_get_server_hello:unknown cipher
returned (1/-1/0)

Can you please explain what you mean? What should this changed to so
that it does work.

> Please be aware that in your config the ssl::server_name ACL is *not* matching the SNI in your config.
> - Your ssl_bump rules say "peek all" - so peek happens on the two Hello
> messages. When the serverHello has been peek'd the real server name is
> available from the servers own certificate.

Updated Squid.conf.

# ===========================
# Squid 4.7 Config - Work in Progress
# ===========================

acl localnet src 10.0.0.0/8                 # Kubernetes VPC CIDR range
acl SSL_ports port 443                      # HTTPS
acl Safe_ports port 80                       # HTTP
acl Safe_ports port 443                     # HTTPS
acl CONNECT method CONNECT   # Traffic restriction
acl step1 at_step SslBump1              # Needed by ssl-bump

# -------------------------------
# Whitelist the following Domains
# -------------------------------
# FQDN - Try to use FQDN
acl domainIsWhitelisted ssl::server_name accounts.google.com

# ----------------------------------------------
# Wildcard
acl domainIsWhitelisted ssl::server_name --client-requested .googleapis.com
acl domainIsWhitelisted ssl::server_name --client-requested
.googleapis.l.google.com
# -------------------------------

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Passively Intercepted HTTPS Traffic
https_port 9091 cert=/etc/squid/example.com.cert
key=/etc/squid/example.com.private ssl-bump intercept
acl step1 at_step SslBump1
ssl_bump peek all
ssl_bump splice domainIsWhitelisted
ssl_bump terminate all

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Logging
logformat custom1 %tg %6tr %>a %<A %Ss/%03>Hs %<st %rm %ru HTTP/%rv
SSL: %ssl::>sni %ssl::bump_mode Client(Subject/Tx/Neg/Sup/Cip):
%ssl::>cert_subject %ssl::>received_hello_version
%ssl::>negotiated_version %ssl::>received_supported_version
%ssl::>negotiated_cipher Server(Subject/Rx/Neg/Sup/Cip):
%ssl::<cert_subject %ssl::<received_hello_version
%ssl::<negotiated_version %ssl::<received_supported_version
%ssl::<negotiated_cipher
access_log daemon:/var/log/squid/access_custom1.log custom1

# Listen on port 3128 for HTTP Connet method - unused and firewalled off.
http_port 3128
# End of File

Kind regards
Jared


From squid3 at treenet.co.nz  Thu Jun 27 10:31:53 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Jun 2019 22:31:53 +1200
Subject: [squid-users] Help with transparent whitelisting proxy on Squid
 4.4
In-Reply-To: <CAOR_B-ffSjPTXowhRQqsFWGm20_gUA2Ps1LxA3mOy1ma_WdUBg@mail.gmail.com>
References: <CAOR_B-fHdCTcNoWvS20cVtC5_cFQeTY7_vyv2sD=oJUbJj2aug@mail.gmail.com>
 <b4a2afa6-65e8-881c-f706-51a1d44912df@treenet.co.nz>
 <CAOR_B-fdBfA14O+keNTzmoLswAipEE1uxBQ=XX5mGNY8ivDHDg@mail.gmail.com>
 <CAOR_B-dDaActqjLuPupGHsZMfr+4YhcmsUChZPsHsVaqNWKfXA@mail.gmail.com>
 <13662bfa-c96d-3419-37f4-f030107ca3c3@treenet.co.nz>
 <CAOR_B-ffSjPTXowhRQqsFWGm20_gUA2Ps1LxA3mOy1ma_WdUBg@mail.gmail.com>
Message-ID: <7c67e5a8-a92e-46eb-556c-3eff73f4e114@treenet.co.nz>

On 27/06/19 11:39 am, Jared Fox wrote:
> Hi Amos
> 
> So i have tried the following based on your suggestions, but it is
> still failing and have errors below:
> 
> 1. Switched to a wildcard whitelist instead of single domain
> 2. Updated the logformat to provide more information, see below:
> 3. Add in `--client-requested`, but this made no difference.
>    3a. Add to single ACL, acl domainIsWhitelisted ssl::server_name
> --client-requested cloudtrace.googleapis.com
>    3b. Commented out single record, switched to wildcard
>    3c. Add to wildcard
> 
> Error messages and Logs:
> 
> Access Log:     26/Jun/2019:23:18:38     96 REDACTED 216.58.200.106
> NONE/200 0 CONNECT 216.58.200.106:443 HTTP/1.1 SSL:
> cloudtrace.googleapis.com peek Client(Subject/Tx/Neg/Sup/Cip): -
> TLS/1.0 - TLS/1.2 - Server(Subject/Rx/Neg/Sup/Cip): - TLS/1.2 -
> TLS/1.2 -
> 
> Cache Log:     2019/06/26 23:18:38 kid1| ERROR: negotiating TLS on FD
> 11: error:140920F8:SSL routines:ssl3_get_server_hello:unknown cipher
> returned (1/-1/0)
> 

This means the OpenSSL library being used by Squid does not contain any
support for the cipher(s) the server chose to use for this transaction.

They only way I am aware of to avoid it is to upgrade the OpenSSL
library Squid is built against.


> Can you please explain what you mean? What should this changed to so
> that it does work.
> 
>> Please be aware that in your config the ssl::server_name ACL is *not* matching the SNI in your config.
>> - Your ssl_bump rules say "peek all" - so peek happens on the two Hello
>> messages. When the serverHello has been peek'd the real server name is
>> available from the servers own certificate.
> 

To quote the ssl::server_name documentation:

"
# The ACL computes server name(s) using such information sources as
# CONNECT request URI, TLS client SNI, and TLS server certificate
# subject (CN and SubjectAltName). The computed server name(s) usually
# change with each SslBump step, as more info becomes available:
# * SNI is used as the server name instead of the request URI,
# * subject name(s) from the server certificate (CN and
#   SubjectAltName) are used as the server names instead of SNI.
"

That last bullet point is what is/was happening with your original proxy
config.

The "--client-requested" flag overrides that and causes the SNI to be
used in the match even when server cert is known.


> Updated Squid.conf.
> 
> # ===========================
> # Squid 4.7 Config - Work in Progress
> # ===========================
> 
> acl localnet src 10.0.0.0/8                 # Kubernetes VPC CIDR range
> acl SSL_ports port 443                      # HTTPS
> acl Safe_ports port 80                       # HTTP
> acl Safe_ports port 443                     # HTTPS
> acl CONNECT method CONNECT   # Traffic restriction
> acl step1 at_step SslBump1              # Needed by ssl-bump
> 
> # -------------------------------
> # Whitelist the following Domains
> # -------------------------------
> # FQDN - Try to use FQDN
> acl domainIsWhitelisted ssl::server_name accounts.google.com
> 
> # ----------------------------------------------
> # Wildcard
> acl domainIsWhitelisted ssl::server_name --client-requested .googleapis.com
> acl domainIsWhitelisted ssl::server_name --client-requested
> .googleapis.l.google.com
> # -------------------------------
> 
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Passively Intercepted HTTPS Traffic
> https_port 9091 cert=/etc/squid/example.com.cert
> key=/etc/squid/example.com.private ssl-bump intercept
> acl step1 at_step SslBump1
> ssl_bump peek all
> ssl_bump splice domainIsWhitelisted
> ssl_bump terminate all
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> # Logging
> logformat custom1 %tg %6tr %>a %<A %Ss/%03>Hs %<st %rm %ru HTTP/%rv
> SSL: %ssl::>sni %ssl::bump_mode Client(Subject/Tx/Neg/Sup/Cip):
> %ssl::>cert_subject %ssl::>received_hello_version
> %ssl::>negotiated_version %ssl::>received_supported_version
> %ssl::>negotiated_cipher Server(Subject/Rx/Neg/Sup/Cip):
> %ssl::<cert_subject %ssl::<received_hello_version
> %ssl::<negotiated_version %ssl::<received_supported_version
> %ssl::<negotiated_cipher
> access_log daemon:/var/log/squid/access_custom1.log custom1
> 
> # Listen on port 3128 for HTTP Connet method - unused and firewalled off.
> http_port 3128


NP: this is not about CONNECT method. It is about serving up error
pages, FTP listings, and all the icons/scripts/stylesheets etc embedded
in those.

Amos


From squid3 at treenet.co.nz  Thu Jun 27 10:59:49 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Jun 2019 22:59:49 +1200
Subject: [squid-users] Help with HTTPS SQUID 3.1.23
In-Reply-To: <CACtB8=AXp23Zi15EY-+5JGG=R+RyP-DFB-pmrQRdCZ7ETVffDw@mail.gmail.com>
References: <CACtB8=AXp23Zi15EY-+5JGG=R+RyP-DFB-pmrQRdCZ7ETVffDw@mail.gmail.com>
Message-ID: <73b43b98-c819-45e8-57b7-988eaae91eb1@treenet.co.nz>

On 27/06/19 1:29 am, Anderson Rosario wrote:
> I can not access to HTTPS sites, 3 weeks ago was working fine, without
> doing any change in the topology update or config stopped and it is not
> working with HTTPS sites. it keeps loading and I recieve a message from
> navegators The connection to the server was reset while the page was
> loading.
> 

Your Squid is not doing anything with HTTPS at all. It lets CONNECT
tunnels through - provided the server name meets your required ACLs.

It may be related to Browser changes in how they handle non-200
responses to CONNECT since your access controls all require a login to
take place.


> *here my squid config:*
> 
> #
> visible_hostname proxy.local.local

Really .local.local ?


...> http_port 3128
> 
> ######### AD AUTH ###########
> auth_param basic program /usr/lib/squid/squid_ldap_auth -R -b
> "dc=local,dc=LOCAL" -D "cn=squid,ou=proxy,dc=local,dc=LOCAL" -w "123456"
> -f sAMAccountName=%s -h 192.168.0.213
> 
> auth_param basic children 5
> auth_param basic realm Inserte su usuario de Windows para navegar
> auth_param basic credentialsttl 1 hour
> 
> external_acl_type ldap_group %LOGIN /usr/lib/squid/squid_ldap_group -R
> -b "dc=local,dc=LOCAL" -D "cn=squid,ou=proxy,dc=local,dc=LOCAL" -w
> "123456" -f "(&(objectclass=person)
> (sAMAccountName=%v)(memberof=cn=%a,ou=proxy,dc=local,dc=LOCAL))" -h
> 192.168.0.213
> ##############################
> 
> ###### ALCs que definen los grupos ######
> acl nivel0 external ldap_group nivel0
> acl nivel1 external ldap_group nivel1
> acl nivel2 external ldap_group nivel2
> acl nivel3 external ldap_group nivel3
> acl nivel4 external ldap_group nivel4
> acl nivel5 external ldap_group nivel5
> acl nivel6 external ldap_group nivel6
> 
> #########################################
> 
...
> #########################
> 
> ###### Reglas de acceso ######
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> 
> #
> http_access allow nivel6
> http_access allow nivel5
> http_access allow nivel4
> http_access allow nivel3 !rule3 !desc3 !rule7 !desc7
> http_access allow nivel2 !rule2 !desc2 !rule7 !desc7
> http_access deny nivel1 !rule1
> http_access allow nivel1 !desc1 !rule7 !desc7
> http_access deny nivel0
> http_access deny all
> ##############################
> 
> 

Due to the "deny all" being above the http_access lines below do anything.

What this means is that external parties *are* allowed to access the
proxy management reports and potentially private info about other clients.

 ... not only is the below recommended *minimum* config. It is supposed
to be listed early like the Safe_ports and SSL_ports rules in order to
protect your network from attacks.


> # Recommended minimum Access Permission configuration:
> #
> # Only allow cachemgr access from localhost
> http_access allow manager localhost
> http_access deny manager
> 

HTH
Amos



From squid3 at treenet.co.nz  Thu Jun 27 11:33:40 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Jun 2019 23:33:40 +1200
Subject: [squid-users] Running squid in intercept mode breaks whatsapp
In-Reply-To: <CAOhxsyzB+9Fday-EWFbERaEOOsnQyfO7XFTYiJOV=1zjOxQZwA@mail.gmail.com>
References: <CAOhxsyzB+9Fday-EWFbERaEOOsnQyfO7XFTYiJOV=1zjOxQZwA@mail.gmail.com>
Message-ID: <f6ec6a4a-03f5-de89-e5cd-a4e19ee953dd@treenet.co.nz>

On 27/06/19 3:12 am, Chirayu Patel wrote:
> 
> I am redirecting port 80 and port 443 traffic to squid..
> 
> I went through some blogs and forums which suggest that Whatsapp seems
> to send non SSL traffic on port 443
> (https://developers.facebook.com/docs/whatsapp/guides/network-requirements/)
> 
> And since I am running in intercept mode, it seems to terminate non-SSL
> traffic as well..
> 
> Could someone kindly suggest how to bypass this..

<http://www.squid-cache.org/Doc/config/on_unsupported_protocol/>

Note the required Squid version.

Amos


From Walter.H at mathemainzel.info  Fri Jun 28 14:20:37 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Fri, 28 Jun 2019 16:20:37 +0200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
Message-ID: <5D162235.201@mathemainzel.info>

Hello,

at some specific hosts
this is shown in cache.log
2019/06/28 16:11:12 kid1| Error negotiating SSL on FD 17: 
error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message (1/-1/0)

and this is the error page I get

Failed to establish a secure connection to .../

  (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)/
  Handshake with SSL server failed: error:1408E0F4:SSL 
routines:SSL3_GET_MESSAGE:unexpected message

what is causing this?

in case some want to try:   https://www.3bg.at/
(when disabling SSL-bump no problem)

Thanks,
Walter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190628/2e14e2ba/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190628/2e14e2ba/attachment.bin>

From belle at bazuin.nl  Fri Jun 28 14:34:43 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 28 Jun 2019 16:34:43 +0200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
In-Reply-To: <5D162235.201@mathemainzel.info>
References: <5D162235.201@mathemainzel.info>
Message-ID: <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>

the SSL3_GET_MESSAGE ? 
?
Maybe because the only support TLSv1.2 ? 
Its long ago i seen a site good configured for ones with its TLS settings. 
?
So most probely, your downgrading the connection within the proxy settings to sslv3 
?
And sharing you config might help to see that. 
?
Greetz, 
?
Louis
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Walter H.
Verzonden: vrijdag 28 juni 2019 16:21
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] SQUID_ERR_SSL_HANDSHAKE



Hello,

at some specific hosts
this is shown in cache.log
2019/06/28 16:11:12 kid1| Error negotiating SSL on FD 17: error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message (1/-1/0)

and this is the error page I get

Failed to establish a secure connection to ...

?(71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
?Handshake with SSL server failed: error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message

what is causing this?

in case some want to try:?? https://www.3bg.at/
(when disabling SSL-bump no problem)

Thanks,
Walter


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190628/459aa7a5/attachment.htm>

From Walter.H at mathemainzel.info  Fri Jun 28 15:03:33 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Fri, 28 Jun 2019 17:03:33 +0200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
In-Reply-To: <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>
References: <5D162235.201@mathemainzel.info>
 <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <5D162C45.20104@mathemainzel.info>

this is in my squid.conf


acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl nobumpsites ssl::server_name "/etc/squid/sslnobumpsites-acl.squid" 
<-- e.g. www.google.com

ssl_bump stare step1 all
ssl_bump splice nobumpsites
ssl_bump bump all

acl brokenButTrusted dstdomain 
"/etc/squid/brokenbuttrustedsites-acl.squid" <-- contains e.g.  
download.microsoft.com

acl certSelfSigned ssl_error X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
...
acl squidSslHandshake ssl_error SQUID_ERR_SSL_HANDSHAKE

sslproxy_cert_sign_hash sha256

sslproxy_cert_error allow brokenButTrusted
sslproxy_cert_error deny all

sslproxy_cafile /etc/squid/ca-bundle.trust.crt
sslproxy_cipher 
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
sslproxy_options NO_SSLv2 NO_SSLv3 TLSv1 TLSv1_1 TLSv1_2

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/local/squid/ssl_db -M 16MB
sslcrtd_children 8




On 28.06.2019 16:34, L.P.H. van Belle wrote:
> the SSL3_GET_MESSAGE?
> Maybe because the only support TLSv1.2 ?
> Its long ago i seen a site good configured for ones with its TLS 
> settings.
> So most probely, your downgrading the connection within the proxy 
> settings to sslv3
> And sharing you config might help to see that.
> Greetz,
> Louis
>
>     *Van:* squid-users
>     [mailto:squid-users-bounces at lists.squid-cache.org] *Namens *Walter H.
>     *Verzonden:* vrijdag 28 juni 2019 16:21
>     *Aan:* squid-users at lists.squid-cache.org
>     *Onderwerp:* [squid-users] SQUID_ERR_SSL_HANDSHAKE
>
>     Hello,
>
>     at some specific hosts
>     this is shown in cache.log
>     2019/06/28 16:11:12 kid1| Error negotiating SSL on FD 17:
>     error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message
>     (1/-1/0)
>
>     and this is the error page I get
>
>     Failed to establish a secure connection to .../
>
>      (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)/
>      Handshake with SSL server failed: error:1408E0F4:SSL
>     routines:SSL3_GET_MESSAGE:unexpected message
>
>     what is causing this?
>
>     in case some want to try: https://www.3bg.at/
>     (when disabling SSL-bump no problem)
>
>     Thanks,
>     Walter
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190628/b561c7b0/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190628/b561c7b0/attachment.bin>

From squid3 at treenet.co.nz  Sat Jun 29 08:17:11 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Jun 2019 20:17:11 +1200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
In-Reply-To: <5D162C45.20104@mathemainzel.info>
References: <5D162235.201@mathemainzel.info>
 <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>
 <5D162C45.20104@mathemainzel.info>
Message-ID: <2607d2aa-b73a-71fd-629f-9d42e71dd76d@treenet.co.nz>

On 29/06/19 3:03 am, Walter H. wrote:
>
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
> sslproxy_options NO_SSLv2 NO_SSLv3 TLSv1 TLSv1_1 TLSv1_2
> 

I do not see the tls-dh setting necessary for the elliptic curves to
work in your displayed config. So that would make the above cipher
directive essentially disable everything except SSLv3 with MEDIUM/HIGH
level non-RSA ciphers.

The value of sslproxy_options directive is colon (:) or comma (,)
delimited. When multiple values like the above are configured only the
first in the list is used. Which forces only TLS/1.2

It is not clear what OpenSSL will do when those conflicting options are
handed to it. But it looks like it is down-grading to SSLv3 as L.P.H.
said then breaking when something else arrives back.



I would start by removing the sslproxy_cipher directive entirely and
fixing sslproxy_options to be:

 sslproxy_options NO_SSLv2:NO_SSLv3

then re-adding the cipher *restrictions* (the ones with '!' prefix)
gradually to see what you actually need and where the problem starts.
You should not need the additions as secure ciphers are always enabled
by default.

If you can watch the TLS negotiations with wireshark as you experiment
it can help understand which options actually do anything. There are
overlaps like "!LOW" should be including the removals that "!RC4" does
and such overlaps.


Amos


From Walter.H at mathemainzel.info  Sat Jun 29 10:34:12 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 29 Jun 2019 12:34:12 +0200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
In-Reply-To: <2607d2aa-b73a-71fd-629f-9d42e71dd76d@treenet.co.nz>
References: <5D162235.201@mathemainzel.info>
 <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>
 <5D162C45.20104@mathemainzel.info>
 <2607d2aa-b73a-71fd-629f-9d42e71dd76d@treenet.co.nz>
Message-ID: <5D173EA4.7060100@mathemainzel.info>

On 29.06.2019 10:17, Amos Jeffries wrote:
> On 29/06/19 3:03 am, Walter H. wrote:
>> sslproxy_cipher
>> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
>> sslproxy_options NO_SSLv2 NO_SSLv3 TLSv1 TLSv1_1 TLSv1_2
>>
> I do not see the tls-dh setting necessary for the elliptic curves to
> work in your displayed config.
do you mean the dhparams= at the http_port here?

http_port 3128 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=16MB cert=/etc/squid/cert/squidCA.pem 
options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE 
dhparams=/etc/squid/cert/dhparam.pem

>   So that would make the above cipher
> directive essentially disable everything except SSLv3 with MEDIUM/HIGH
> level non-RSA ciphers.
even with this:

sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
and the sslproxy_cipher commented out,

this site doesn't work;

sslcrtvalidator_program cache=8192 ttl=240 
/usr/lib64/squid/ssl_crtvalid/main.sh
sslcrtvalidator_children 12 startup=5 idle=1 concurrency=1

this validator isn't called at all with the site  https://www.3bg.at
e.g. with  https://wiki.squid-cache.org   this validator-script is 
caled, and
there is the following traced

0 cert_validate 5324 host=wiki.squid-cache.org
proto_version=TLSv1.2
cipher=ECDHE-RSA-AES256-GCM-SHA384
...


>
> The value of sslproxy_options directive is colon (:) or comma (,)
> delimited. When multiple values like the above are configured only the
> first in the list is used. Which forces only TLS/1.2
I changed this to

sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE

> It is not clear what OpenSSL will do when those conflicting options are
> handed to it. But it looks like it is down-grading to SSLv3 as L.P.H.
> said then breaking when something else arrives back.
quite strange only a few sites don't work, https://www.3bg.at is an 
example of such;
many others work as expected;


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190629/65280c55/attachment.bin>

From squid3 at treenet.co.nz  Sat Jun 29 12:13:39 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Jun 2019 00:13:39 +1200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
In-Reply-To: <5D173EA4.7060100@mathemainzel.info>
References: <5D162235.201@mathemainzel.info>
 <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>
 <5D162C45.20104@mathemainzel.info>
 <2607d2aa-b73a-71fd-629f-9d42e71dd76d@treenet.co.nz>
 <5D173EA4.7060100@mathemainzel.info>
Message-ID: <94206d52-19b2-f616-623b-1b4ba9af917a@treenet.co.nz>

On 29/06/19 10:34 pm, Walter H. wrote:
> On 29.06.2019 10:17, Amos Jeffries wrote:
>> On 29/06/19 3:03 am, Walter H. wrote:
>>> sslproxy_cipher
>>> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
>>>
>>> sslproxy_options NO_SSLv2 NO_SSLv3 TLSv1 TLSv1_1 TLSv1_2
>>>
>> I do not see the tls-dh setting necessary for the elliptic curves to
>> work in your displayed config.
> do you mean the dhparams= at the http_port here?

Sorry, I got that wrong. There is no params for Squid->server
connections. The flags on sslproxy_options you found for the next part
below were what you need.

> 
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/cert/squidCA.pem
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
> dhparams=/etc/squid/cert/dhparam.pem
> 
>> ? So that would make the above cipher
>> directive essentially disable everything except SSLv3 with MEDIUM/HIGH
>> level non-RSA ciphers.
> even with this:
> 
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
> and the sslproxy_cipher commented out,
> 
> this site doesn't work;
> 
> sslcrtvalidator_program cache=8192 ttl=240
> /usr/lib64/squid/ssl_crtvalid/main.sh
> sslcrtvalidator_children 12 startup=5 idle=1 concurrency=1
> 
> this validator isn't called at all with the site? https://www.3bg.at
> e.g. with? https://wiki.squid-cache.org?? this validator-script is
> caled, and
> there is the following traced
> 
> 0 cert_validate 5324 host=wiki.squid-cache.org
> proto_version=TLSv1.2
> cipher=ECDHE-RSA-AES256-GCM-SHA384
> ...
> 

That is a good sign. That exact combo is in the set supported by the
breaking server so it is unlikely your Squid or its OpenSSL is
contributing to this particular problem.


> 
>>
>> The value of sslproxy_options directive is colon (:) or comma (,)
>> delimited. When multiple values like the above are configured only the
>> first in the list is used. Which forces only TLS/1.2
> I changed this to
> 
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
> 
>> It is not clear what OpenSSL will do when those conflicting options are
>> handed to it. But it looks like it is down-grading to SSLv3 as L.P.H.
>> said then breaking when something else arrives back.
> quite strange only a few sites don't work, https://www.3bg.at is an
> example of such;
> many others work as expected;
> 

That is a bit odd. Though looking at the SSL Labs report for this
www.3bg.at site their restricting to only TLS/1.2 and there are many
clients for which the encryption handshake does not work.

<https://www.ssllabs.com/ssltest/analyze.html?d=www.3bg.at> look to the
list of failures under "Handshake Simulation" and the whole list of "Not
simulated clients" for comparison with UA of any of your clients having
trouble connecting there.


Squid SSL-Bump is limited to negotiating use of TLS versions and
features which are supported by both itself and the client when offering
things to the server. So the problem of some clients agents not
supporting TLS/1.2 or the ciphers the server wants to use can make the
site fail even if your Squid outbound settings support them.


PS. At the technical level that exact error from OpenSSL means that some
data arrived from the server at a time when only TLS alert messages were
supposed to be happening.  I suspect it could be a sign that the
Internet between your proxy and that server is being MITM'd by an agent
that corrupts the protocol for some reason. eg someone elses proxy
rejecting the connection but getting its error response syntax wrong.


Amos


From Walter.H at mathemainzel.info  Sat Jun 29 14:03:49 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 29 Jun 2019 16:03:49 +0200
Subject: [squid-users] SQUID_ERR_SSL_HANDSHAKE
In-Reply-To: <94206d52-19b2-f616-623b-1b4ba9af917a@treenet.co.nz>
References: <5D162235.201@mathemainzel.info>
 <vmime.5d162583.676d.f7ab49540e5a233@ms249-lin-003.rotterdam.bazuin.nl>
 <5D162C45.20104@mathemainzel.info>
 <2607d2aa-b73a-71fd-629f-9d42e71dd76d@treenet.co.nz>
 <5D173EA4.7060100@mathemainzel.info>
 <94206d52-19b2-f616-623b-1b4ba9af917a@treenet.co.nz>
Message-ID: <5D176FC5.1010307@mathemainzel.info>

Hello Amos,

On 29.06.2019 14:13, Amos Jeffries wrote:
>
> That is a good sign. That exact combo is in the set supported by the
> breaking server so it is unlikely your Squid or its OpenSSL is
> contributing to this particular problem.
>
>> quite strange only a few sites don't work, https://www.3bg.at is an 
>> example of such; many others work as expected; 
> That is a bit odd. Though looking at the SSL Labs report for this
> www.3bg.at site their restricting to only TLS/1.2 and there are many
> clients for which the encryption handshake does not work.
>
> <https://www.ssllabs.com/ssltest/analyze.html?d=www.3bg.at>  look to the
> list of failures under "Handshake Simulation" and the whole list of "Not
> simulated clients" for comparison with UA of any of your clients having
> trouble connecting there.
I have my own website and there I did something similar - disabling 
TLSv1 and TLSv1.1,
thus only allowing TLSv1.2
here
https://www.ssllabs.com/ssltest/analyze.html?d=ssl.mathemainzel.info
shows the same; many failures under "Handshake Simulation"
but the weird thing, this works with my Squid :-)
>
>
> Squid SSL-Bump is limited to negotiating use of TLS versions and
> features which are supported by both itself and the client when offering
> things to the server. So the problem of some clients agents not
> supporting TLS/1.2 or the ciphers the server wants to use can make the
> site fail even if your Squid outbound settings support them.
>
>
> PS. At the technical level that exact error from OpenSSL means that some
> data arrived from the server at a time when only TLS alert messages were
> supposed to be happening.
there is also something different;   when doing the following:

openssl s_client -connect  HOST:PORT -servername HOST

this lasts about 1 or 2 minutes until a certificate is shown with  
www.3bg.at
but with my site this goes quickly withing seconds;

> I suspect it could be a sign that the
> Internet between your proxy and that server is being MITM'd by an agent
> that corrupts the protocol for some reason. eg someone elses proxy
> rejecting the connection but getting its error response syntax wrong.
could this be a proxy on the server side?
but the strange:  without SSL bump or direct without squid this site works;
(even my browser uses an uncommon UA string and is not the original Firefox)

what strange thing is doing this bad on some sites?

Thanks,
Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190629/f0994ca5/attachment.bin>

From julien412 at yahoo.fr  Sat Jun 29 13:58:37 2019
From: julien412 at yahoo.fr (julien412 at yahoo.fr)
Date: Sat, 29 Jun 2019 13:58:37 +0000 (UTC)
Subject: [squid-users] splash page: redirection loop
In-Reply-To: <348892700.2133374.1561417143753@mail.yahoo.com>
References: <48746430.1055188.1561309450503.ref@mail.yahoo.com>
 <48746430.1055188.1561309450503@mail.yahoo.com>
 <ebf5d968-bb73-0024-8853-63e42e799eb9@treenet.co.nz>
 <348892700.2133374.1561417143753@mail.yahoo.com>
Message-ID: <960088814.207011.1561816717990@mail.yahoo.com>

with more debug_options, I got 

https://travis-ci.org/juju4/ansible-squid/jobs/552143024
access_custom.log:
?????? 29/Jun/2019:12:34:32 +0000 127.0.0.1 "GET http://www.google.com/ HTTP/1.1" 302 418 "-" "curl/7.58.0" TCP_DENIED:Sh
?????? 29/Jun/2019:12:34:32 +0000 127.0.0.1 "GET http://localhost/splash.php?url=http%3A%2F%2Fwww.google.com%2F HTTP/1.1" 302 470 "-" "curl/7.58.0" TCP_DENIED:Sh
?????? 29/Jun/2019:12:34:32 +0000 127.0.0.1 "GET http://localhost/splash.php?url=http%3A%2F%2Flocalhost%2Fsplash.php%3Furl%3Dhttp%253A%252F%252Fwww.google.com%252F HTTP/1.1" 302 534 "-" "curl/7.58.0" TCP_DENIED:Sh
cache.log?????? 2019/06/29 12:34:32.415 kid1| 82,9| external_acl.cc(742) aclMatchExternal: acl="splash_page"
?????? 2019/06/29 12:34:32.415 kid1| 82,9| external_acl.cc(771) aclMatchExternal: No helper entry available
?????? 2019/06/29 12:34:32.416 kid1| 82,4| external_acl.cc(833) aclMatchExternal: entry = { date=1561811672, result=DENIED tag= log= }
?????? 2019/06/29 12:34:32.416 kid1| 82,4| external_acl.cc(835) aclMatchExternal: entry user=
?????? 2019/06/29 12:34:32.416 kid1| 82,2| external_acl.cc(841) aclMatchExternal: splash_page = DENIED
which seems to repeat for each request, even to splash page
any other debug options?currently usingdebug_options ALL,1 33,2 28,9 82,9

Thanks!
J

    
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190629/61aa37b2/attachment.htm>

From mjguiao at gmail.com  Sun Jun 30 02:32:17 2019
From: mjguiao at gmail.com (Mike Golf)
Date: Sun, 30 Jun 2019 12:32:17 +1000
Subject: [squid-users] Bypassing SSL Man In the Middle Filtering For Certain
	LAN IP's
Message-ID: <CAApifbZF8uFmh25m0=zdemUCHatceShAO3aaK78W2x1=HVzFAQ@mail.gmail.com>

Hi All,

I've setup a squid proxy server on my PFSense router, is there any way of
bypassing HTTPS/SSL filtering for certain LAN IP's. I have IP addresses
192.168.1.0-192.168.1.200 allocated through DHCP and I want these devices
to bypass SSL interception but not the standard HTTP proxy.

Since most modern sites use HTTPS by default HTTP caching isn't that
effective anymore, however I want my personal devices to use the SSL proxy
so I can get the fastest possible browsing experience without having to
install certificate authorities on my guests devices which use the DHCP
range.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190630/2b7044a1/attachment.htm>

From squid3 at treenet.co.nz  Sun Jun 30 06:36:19 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Jun 2019 18:36:19 +1200
Subject: [squid-users] Bypassing SSL Man In the Middle Filtering For
 Certain LAN IP's
In-Reply-To: <CAApifbZF8uFmh25m0=zdemUCHatceShAO3aaK78W2x1=HVzFAQ@mail.gmail.com>
References: <CAApifbZF8uFmh25m0=zdemUCHatceShAO3aaK78W2x1=HVzFAQ@mail.gmail.com>
Message-ID: <6e721121-1569-4b6c-21f0-6429d763c5ae@treenet.co.nz>

On 30/06/19 2:32 pm, Mike Golf wrote:
> Hi All,
> 
> I've setup a squid proxy server on my PFSense router, is there any way
> of bypassing HTTPS/SSL filtering for certain LAN IP's.

HTTPS is not normally filtered at all. So for that to be happening
something must be forcing it - all you have to do is *not* force the
filtering or MITM to happen.

* remove any rules in your NAT or routes directing port 443 to the proxy.

* remove any https_port in the proxy for receiving that intercepted traffic

* remove any SSL-Bump config for handling intercepted port 443 traffic
or decrypting CONNECT tunnels.

With that all done you will at most be left with clients using the proxy
in forward-proxy capacity to open CONNECT tunnels.


> I have IP
> addresses 192.168.1.0-192.168.1.200 allocated through DHCP and I want
> these devices to bypass SSL interception but not the standard HTTP proxy.

Consider how are those clients using the proxy in the first place? Their
method of IP assignment has nothing to do with it.


> 
> Since most modern sites use HTTPS by default HTTP caching isn't that
> effective anymore,

That is a deceptive statement, more false than most think. But
irrelevant since what you are wanting will prevent HTTPS caching entirely.


> however I want my personal devices to use the SSL
> proxy 

Note that SSL protocols both v2 and v3 are obsolete.

Are you asking for:
 a) a TLS explicit proxy, or
 b) a TLS interception proxy, or
 c) a forward-proxy for relaying HTTPS ?


>so I can get the fastest possible browsing experience without
> having to install certificate authorities on my guests devices which use
> the DHCP range.
> 

A proxy is not going to do anything in regards to speed for those clients.

The only way which you can improve speed with a proxy is by caching of
HTTPS content - by avoiding all the re-encrypt delays on every request
that can be made a HIT. But that requires those cert installations you
are trying to avoid.


Amos


From sargen1907 at mail.ru  Sun Jun 30 14:17:36 2019
From: sargen1907 at mail.ru (sargen)
Date: Sun, 30 Jun 2019 09:17:36 -0500 (CDT)
Subject: [squid-users] delay_pools does not work in squid 4.x
Message-ID: <1561904256839-0.post@n4.nabble.com>

Hello.
My system is FreeBSD 11.2. I am using squid 3.5.28 compiled with support for
delay_pools in the following configuration

acl deny_blocked proxy_auth -i "/usr/local/etc/squid/blocked_users.acl"
delay_pools 1
delay_class 1 4
delay_access 1 allow deny_blocked
delay_access 1 deny all
delay_parameters 1 -1 / -1 -1 / -1 -1 / -1 8000/16000

After upgrading squid from version 3.5.28 to 4.5, delay_pools stopped
working.
I found this thread -
http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-pools-in-squid4-not-working-with-https-td4685837.html,
but the discussion seems to be it ended in nothing, and I had to go back to
version 3.5.28.
Two days ago, I encountered this problem again, and found that in squid 4.7
version, delay_pools are still not working.
This time, FreeBSD 12.0 was installed on the server, and I could not install
squid 3.5.25, since this port was marked as broken in it due to conflicts
with openssl.

When can we expect a solution to the issue of delay_pools?
Thanks.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Jun 30 15:20:23 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 1 Jul 2019 03:20:23 +1200
Subject: [squid-users] delay_pools does not work in squid 4.x
In-Reply-To: <1561904256839-0.post@n4.nabble.com>
References: <1561904256839-0.post@n4.nabble.com>
Message-ID: <2ef1d897-965c-e09c-b90c-af14d0f26ce4@treenet.co.nz>

On 1/07/19 2:17 am, sargen wrote:
> Hello.
> My system is FreeBSD 11.2. I am using squid 3.5.28 compiled with support for
> delay_pools in the following configuration
> 
> acl deny_blocked proxy_auth -i "/usr/local/etc/squid/blocked_users.acl"
> delay_pools 1
> delay_class 1 4
> delay_access 1 allow deny_blocked
> delay_access 1 deny all
> delay_parameters 1 -1 / -1 -1 / -1 -1 / -1 8000/16000
> 

If the above is an exact copy-paste of your cofig file then the syntax
is invalid. Squid-3 may have accepted it but Squid-4 is a bit more
pedantic about correct syntax.

Also please use the "none" keywords instead of "-1/-1" values (note the
absence of whitespaces around those numbers).



> After upgrading squid from version 3.5.28 to 4.5, delay_pools stopped
> working.

How did it stop?

More to the point, why was the above working at all?
 * proxy_auth requires remote helper lookups to authenticate.
 * delay pools cannot perform that type of lookup.


> I found this thread -
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Delay-pools-in-squid4-not-working-with-https-td4685837.html,
> but the discussion seems to be it ended in nothing, and I had to go back to
> version 3.5.28.

The discussion ends with a reference to more details in another thread.

..
> When can we expect a solution to the issue of delay_pools?

*If* Alex's guess is correct, then the bug will be fixed in Squid-5. You
can try that cutting-edge code. Whether that is actually your problem or
not, nobody knows.


Amos


From sargen1907 at mail.ru  Sun Jun 30 16:18:14 2019
From: sargen1907 at mail.ru (sargen)
Date: Sun, 30 Jun 2019 11:18:14 -0500 (CDT)
Subject: [squid-users] delay_pools does not work in squid 4.x
In-Reply-To: <2ef1d897-965c-e09c-b90c-af14d0f26ce4@treenet.co.nz>
References: <1561904256839-0.post@n4.nabble.com>
 <2ef1d897-965c-e09c-b90c-af14d0f26ce4@treenet.co.nz>
Message-ID: <1561911494387-0.post@n4.nabble.com>

>> acl deny_blocked proxy_auth -i "/usr/local/etc/squid/blocked_users.acl"
>> delay_pools 1
>> delay_class 1 4
>> delay_access 1 allow deny_blocked
>> delay_access 1 deny all
>> delay_parameters 1 -1 / -1 -1 / -1 -1 / -1 8000/16000
>> 
> If the above is an exact copy-paste of your cofig file then the syntax
> is invalid. Squid-3 may have accepted it but Squid-4 is a bit more
> pedantic about correct syntax.

Ok, may be.

>> After upgrading squid from version 3.5.28 to 4.5, delay_pools stopped
>> working.
> How did it stop?

Sorry, I don't know English well enough.
On version 3.5.28 with such settings, users from the deny_blocked list used
a bandwidth of 64kbps.
On version 4.x with the same settings, users from the deny_blocked list used
the entire bandwidth without restrictions.

> More to the point, why was the above working at all?

I dont know.
I just used delay pool class 4
(http://www.squid-cache.org/Versions/v4/cfgman/delay_class.html) with the
appropriate delay_parameters as described in the
http://www.squid-cache.org/Versions/v4/cfgman/delay_parameters.html
documentation, and this worked exactly as I wanted.

>*If* Alex's guess is correct, then the bug will be fixed in Squid-5. You
>can try that cutting-edge code. Whether that is actually your problem or
>not, nobody knows.

Ok.
It was important for me to make sure that the developers were aware of the
problem. Thanks!




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


