From rousskov at measurement-factory.com  Fri Mar  1 00:09:58 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Feb 2019 17:09:58 -0700
Subject: [squid-users] HTTP2
In-Reply-To: <CAFSk7pSC_eMmJOTv-WO=f=e83GLR_PWtzM-_LE5rTf-yuQopcg@mail.gmail.com>
References: <CAFSk7pS8rAi2=y6r=aqCkrk8S2015dNBuNBQ+N8aLQXiLxGpVw@mail.gmail.com>
 <53069a68-981d-0f40-bcab-fa42bbe9920c@measurement-factory.com>
 <CAFSk7pSC_eMmJOTv-WO=f=e83GLR_PWtzM-_LE5rTf-yuQopcg@mail.gmail.com>
Message-ID: <db7e9501-bcd3-6e4e-3159-8361c8c4500c@measurement-factory.com>

On 2/28/19 3:35 PM, Andrej van der Zee wrote:

> It sounds like its still a long way to get HTTP/2 support released and
> contributing therefore is not an option in company time.?

With enough will and resources, we can promptly overcome the obstacles I
have outlined. They are not insurmountable! Whether investing in that is
better than the alternatives depends on your (company) circumstances. I
just wanted to warn that "more code" is kind of the opposite of what we
need right now as far as HTTP/2 support is concerned.

Alex.


> On Thu, Feb 28, 2019 at 6:28 PM Alex Rousskov wrote:
> 
>     On 2/27/19 10:30 AM, Andrej van der Zee wrote:
> 
>     > I understood that http2 is work in progress.
>     > Is there anything to say about when this might be released??
> 
>     IMO, given the way the Squid Project operates right now, the correct
>     answer to that question is close to "hopefully not in the foreseeable
>     future": We cannot add quality HTTP/2 support right now, and adding some
>     hacky version of it would be disastrous for Squid stability, support,
>     and development. Combined with where the popular clients and origin
>     servers are going, it may be better to fantasize about HTTP/3 support
>     instead.
> 
>     Based on Factory experience with adding HTTP/2 support to Web Polygraph,
>     I consider the following (partially overlapping) preconditions as
>     necessary for serious HTTP/2 (or HTTP/3) work in Squid:
> 
>     ? 1. Proper QA infrastructure.
> 
>     ? 2. Elimination of technical debt that prevents proper restructuring
>     ? ? ?of HTTP/2-sensitive code.
> 
>     ? 3. An agreement regarding overall HTTP/2 code architecture.
> 
>     ? 4. An efficient way to accept huge code changes.
> 
>     ? 5. A project lead capable, willing, trusted, and funded
>     ? ? ?to orchestrate such a big change from beginning to end.
> 
>     Right now, *none* of the above preconditions are satisfied.
> 
>     There is slow but steady progress with #1 and areas of #2.
> 
>     The situation with #3 and #4 is worse than it was a few years ago -- we
>     are wasting insane amounts of time on getting much simpler code changes
>     reviewed and accepted. Many changes require a rewrite before they should
>     be accepted (and some are indeed rewritten). Nobody can afford to
>     rewrite a pull request with initial HTTP/2 support!
> 
>     We have nobody who can satisfy #5 criteria right now.
> 
> 
>     On 2/27/19 7:27 PM, Amos Jeffries wrote:
> 
>     > If anyone wants to jump in and lend a hand my HTTP/2 work is up on
>     > github. IMO the best tasks to collaborate on would be designing
>     > cppunit tests
>     Creating more unofficial code is a bad idea at this time IMO.
> 
>     Alex.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> -- 
> Andrej van der Zee
> Oranje-Vrijstaatkade 49
> 1093KS Amsterdam
> +31-(0)6-8133-9388
> https://www.linkedin.com/in/andrejvanderzee/



From michael at hendrie.id.au  Fri Mar  1 09:47:43 2019
From: michael at hendrie.id.au (Michael Hendrie)
Date: Fri, 1 Mar 2019 20:17:43 +1030
Subject: [squid-users] Squid-3.5.28 slowdown
Message-ID: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>

Hi Guys,

I have a squid-3.5.28 installation that is deployed to do transparent ssl-bump of HTTPS traffic (linux bridge, tproxy).  The server is not overly busy, CPU and RAM usage is low + no swap being used yet regularly the squid service is choking HTTPS traffic to a point where websites are timing out.  Any other traffic flowing through the bridge is unaffected and continues to operate at normal expected speeds.

I have checked all obvious things, CPU/RAM usage, network interface errors, conntrack table and TCP resource exhaustion yet all look fine. There is no caching taking place and disk IO is not a problem.

During the times where squid is slow, even using squidclient to query squid state is extremely slow to respond, as you can see below snip from the access.log the mgr:coutners and mgr:5min requests are taking up to 30 seconds to complete when usually the response time is 0:

1551397216.978     31 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397220.633      0 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397233.385      2 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397237.431     14 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397262.074      2 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397280.644     17 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397314.764      1 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397330.455      5 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397377.265      4 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397385.727      0 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397396.161     17 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/5min - HIER_NONE/- - -
1551397432.974     11 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397462.897     11 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397492.759      7 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397522.611      9 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397552.521     12 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397582.484     17 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -
1551397612.446     10 127.0.0.1 TCP_MISS/000 0 GET cache_object://localhost/counters - HIER_NONE/- - -

I have a number of these severs deployed, all running same hardware/config/squid versions and only this one is experiencing an issue.....looking for suggestions on what could be occurring and how to debug further?  

Thanks,

Michael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190301/b493006e/attachment.htm>

From flashdown at data-core.org  Fri Mar  1 11:04:35 2019
From: flashdown at data-core.org (Enrico Heine)
Date: Fri, 01 Mar 2019 12:04:35 +0100
Subject: [squid-users] Squid-3.5.28 slowdown
In-Reply-To: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>
References: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>
Message-ID: <E2D7AB5B-0C8F-4E91-B43E-F5FCDDB2CD1E@data-core.org>

Hey Michael, 

>>just a shot into the dark<<, is it possible that you use the adaption service for ICAP?

If so, fast test, this should return 0 if u are not affected by this, if higher than 0 check the link below:
netstat -pa | grep CLOSE_WAIT | wc -l 

also have a look into /var/log/kern.log 

if so please checkout this bug https://bugs.squid-cache.org/show_bug.cgi?id=4526

So if it is your issue, please note that it seems that this issue may also be present in V4, but the bugreport for V4 has been closed as duplicate..

Best regards,
Flashdown

Am 1. M?rz 2019 10:47:43 MEZ schrieb Michael Hendrie <michael at hendrie.id.au>:
>Hi Guys,
>
>I have a squid-3.5.28 installation that is deployed to do transparent
>ssl-bump of HTTPS traffic (linux bridge, tproxy).  The server is not
>overly busy, CPU and RAM usage is low + no swap being used yet
>regularly the squid service is choking HTTPS traffic to a point where
>websites are timing out.  Any other traffic flowing through the bridge
>is unaffected and continues to operate at normal expected speeds.
>
>I have checked all obvious things, CPU/RAM usage, network interface
>errors, conntrack table and TCP resource exhaustion yet all look fine.
>There is no caching taking place and disk IO is not a problem.
>
>During the times where squid is slow, even using squidclient to query
>squid state is extremely slow to respond, as you can see below snip
>from the access.log the mgr:coutners and mgr:5min requests are taking
>up to 30 seconds to complete when usually the response time is 0:
>
>1551397216.978     31 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397220.633      0 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397233.385      2 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397237.431     14 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397262.074      2 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397280.644     17 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397314.764      1 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397330.455      5 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397377.265      4 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397385.727      0 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397396.161     17 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/5min - HIER_NONE/- - -
>1551397432.974     11 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397462.897     11 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397492.759      7 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397522.611      9 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397552.521     12 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397582.484     17 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>1551397612.446     10 127.0.0.1 TCP_MISS/000 0 GET
>cache_object://localhost/counters - HIER_NONE/- - -
>
>I have a number of these severs deployed, all running same
>hardware/config/squid versions and only this one is experiencing an
>issue.....looking for suggestions on what could be occurring and how to
>debug further?  
>
>Thanks,
>
>Michael

-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190301/dbe5120a/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar  1 12:21:37 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Mar 2019 01:21:37 +1300
Subject: [squid-users] Squid-3.5.28 slowdown
In-Reply-To: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>
References: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>
Message-ID: <c8c06023-877c-4de1-52f8-2b43821652cf@treenet.co.nz>

On 1/03/19 10:47 pm, Michael Hendrie wrote:
> Hi Guys,
> 
> I have a squid-3.5.28 installation that is deployed to do transparent
> ssl-bump of HTTPS traffic (linux bridge, tproxy). ?The server is not
> overly busy, CPU and RAM usage is low + no swap being used yet regularly
> the squid service is choking HTTPS traffic to a point where websites are
> timing out. ?Any other traffic flowing through the bridge is unaffected
> and continues to operate at normal expected speeds.
> 
> I have checked all obvious things, CPU/RAM usage, network interface
> errors, conntrack table and TCP resource exhaustion yet all look fine.
> There is no caching taking place and disk IO is not a problem.
> 
> During the times where squid is slow, even using squidclient to query
> squid state is extremely slow to respond, as you can see below snip from
> the access.log the mgr:coutners and mgr:5min requests are taking up to
> 30 seconds to complete when usually the response time is 0:
> 
> 1551397216.978?? ??31 127.0.0.1 TCP_MISS/000 0 GET
> cache_object://localhost/5min - HIER_NONE/- - -

If that '31' is what you are meaning by "30 seconds" then please be
aware the duration column numbers are in *milli*-seconds.

So while in absolute terms this 31ms is significantly slower than
under-1ms (aka '0'). It is not humanly visible.

Any visible delays you may be having are not being shown by these log
entries. Which hints that they are somewhere in the delays between HTTP
transactions - either the I/O on traffic not yet logged, or in the TLS
handshake timings.

Amos


From egoitz at sarenet.es  Fri Mar  1 12:59:16 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Fri, 01 Mar 2019 13:59:16 +0100
Subject: [squid-users] Squid and url modifying
Message-ID: <11c17c2320f135905d81d3112bbd1339@sarenet.es>

Good afternoon, 

Is it possible for Squid to do something like : 

- Receive request :
https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88 

and 

to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
[1] 

I mean not to redirect users with url redirection. Just act as a proxy
but having Squid the proper acknoledge internally for being able to make
the proper request to the destination?. Is it possible without
redirecting url, to return for instance a 403 error to the source web
browser in order to not be able to access to the site if some kind of
circumstances are given?. 

If the last config, was not possible... perhaps I needed to just to
redirect forcibly?. I have read for that purpose you can use URL
redirectors.... so I assume the concept is : 

- Receive request :
https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88 

and 

to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
[1] 

If all conditions for allowing to see the content are OK, return the web
browser a 301 redirect answer with the
https://oooeeee.eeee.ttt/u?ii=99&j=88 [1] URL. Else, just return a 403
or redirect you to a Forbidden page... I think this could be implemented
with URL redirectors...but... the fact is... which kind of conditions or
env situations can you use for validating the content inside the url
redirector?. 

Thanks a lot for your time :) 

Cheers! 

-- 

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [2] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

Links:
------
[1] https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&amp;j=88
[2] http://www.sarenet.es
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190301/c92d7545/attachment.htm>

From the.tuxster at gmail.com  Fri Mar  1 13:33:03 2019
From: the.tuxster at gmail.com (Joseph Jones)
Date: Fri, 1 Mar 2019 13:33:03 +0000
Subject: [squid-users] SslBump Not working for transparent proxy
Message-ID: <CAEjE5erqL-x=NFhJBG_LyeZRDsbHO9Uxa4ufhTRfZrS__g=wiQ@mail.gmail.com>

I've been trying to get SslBump work for whitelist purposes and so far have
been failing.

It's my understanding in order for SslBump to do whitelist it will need to
do a splice at step2 or step3.

Looking at my logs I see step1 matching but I never see step2. I believe
it's because of what I found in the docs.

> Rules with actions that are impossible at the current step are ignored.

What I believe I'm failing to understand is the the order SslBump steps are
happening and when I can actually use the whitelist acl to compare to the
SNI provided.

watching the logs I see the http_access request happens in the order they
appear in the file. but SslBump step1 seems to happen before any
http_access.

Since I never see step2 happen in my logs I'm not sure where it happens.
ultimately the request is rejected because of my final deny all at line 57.

I've also observed that using a non-transparent proxy. SSL and non SSL
request get evaluated at line 48 which is where I allow from my whitelist
and localnet. This seems to make sense. So he only thing I need to
understand I believe is the SSLbump steps. the order things happen in

is the final deny I have wrong? Or is my ssl_bump simply wrong?

you'll notice in my config I've commented a bunch of http_access out. I was
hoping if I made the file simpler it would be easier to troubleshoot. I
intend to put them back when I figure out my problem.

cache.log: https://pastebin.com/uZVn6f4Q
squid.conf: https://pastebin.com/D49H5rYS
squid -k parse: https://pastebin.com/F0U2SvUm

-- 
Joseph M Jones
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190301/3889a9cd/attachment.htm>

From mzgmedia at gmail.com  Fri Mar  1 13:41:13 2019
From: mzgmedia at gmail.com (mzgmedia)
Date: Fri, 1 Mar 2019 07:41:13 -0600 (CST)
Subject: [squid-users] ipv4 + ipv6
Message-ID: <1551447673452-0.post@n4.nabble.com>

hello

we a squid server with both IPv4 and IPv6

now if a user will connect to Ipv4, it will also get an Ipv6 from squid. We
want to prevent that, if the user will connect to IPv4, to have just Ipv4
and if connects to Ipv6 to have just Ipv6 and it seems that we can't manage
to do this.

Can you please provide some hits?

thank you



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From belle at bazuin.nl  Fri Mar  1 14:36:08 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 1 Mar 2019 15:36:08 +0100
Subject: [squid-users] compiling squid 4
In-Reply-To: <6b3ee0ff-fa7d-87e2-9c46-8fbfe063f463@dvm.esines.cu>
References: <6b3ee0ff-fa7d-87e2-9c46-8fbfe063f463@dvm.esines.cu>
Message-ID: <vmime.5c794358.50c0.2392c76879617cd8@ms249-lin-003.rotterdam.bazuin.nl>

Its pretty simple.. 

Enable the debian sid source in your ubuntu 18
apt install -y software-properties-common debian-archive-keyring dirmngr
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 8B48AD6246925553
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 7638D0442B90D010

add-apt-repository "deb-src http://ftp.nl.debian.org/debian sid main contrib non-free"
apt-get update

I dont enable the deb line from sid because that prevents possible unwanted upgrades/replacements of files. 

# get build-depends
apt-get build-dep squid3

# get source and build squid
apt-get source squid3 -B

And wait. 
Thats what im doing now ;-) .. 15 min later done. 

 dpkg-genbuildinfo --build=binary
 dpkg-genchanges --build=binary >../squid_4.6-1_amd64.changes
dpkg-genchanges: info: binary-only upload (no source code included)
 dpkg-source --after-build squid-4.6
dpkg-buildpackage: info: binary-only upload (no source included)

This results in a some debs, setup a file/web repo to use them so you can apt-get install them. 
Move all .deb files in a folder 

mkdir amd64
mv *.deb amd64/
apt-ftparchive packages amd64/ > amd64/Packages

And add a line like this in your /etc/apt/sources.list.d/squid-local.list
echo "deb [trusted=yes] file:/home/build/squid/ amd64/" > /etc/apt/sources.list.d/squid-local.list
# NOTE , dont forget to change the path here^^ 
You can move this to any place as long amd64/ exitst. 
You can change this to a webserver, then it looks like : #deb [trusted=yes] http://localhost/ amd64/"
And document root and webserver setup, if you move amd64 to /var/www/html/ 
Then it should work directly after you install a webserver. 

Results. 
apt-get update
apt-cache policy squid
squid:
  Installed: (none)
  Candidate: 4.6-1
  Version table:
     4.6-1 500
        500 file:/root amd64/ Packages
     3.5.27-1ubuntu1.1 500
        500 http://nl.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages
     3.5.27-1ubuntu1 500
        500 http://nl.archive.ubuntu.com/ubuntu bionic/main amd64 Packages


apt-get install squid
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  libdbi-perl squid-common squid-langpack
Suggested packages:
  libmldbm-perl libnet-daemon-perl libsql-statement-perl squidclient squid-cgi squid-purge resolvconf smbclient ufw
The following NEW packages will be installed:
  libdbi-perl squid squid-common squid-langpack
0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.
Need to get 861 kB/3,752 kB of archives.
After this operation, 13.4 MB of additional disk space will be used.
Do you want to continue? [Y/n]
Get:1 file:/root amd64/ squid-common 4.6-1 [309 kB]
Get:2 file:/root amd64/ squid 4.6-1 [2,582 kB]
Get:3 http://nl.archive.ubuntu.com/ubuntu bionic/main amd64 squid-langpack all 20170901-1 [137 kB]
Get:4 http://nl.archive.ubuntu.com/ubuntu bionic/main amd64 libdbi-perl amd64 1.640-1 [724 kB]


Now you ;-) 

And why above works... 
Because of the fantastic work of the squid devs! 


Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Alex Guti?rrez Mart?nez
> Verzonden: donderdag 28 februari 2019 22:03
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: [squid-users] compiling squid 4
> 
> Hello again community, I still have problems compiling squid 
> 4. This is 
> what i did this time
> 
> Soporte b?sico necesario:
> 
> # apt install arj bzip2 xz-utils cabextract cpio file lzma lhasa lzop 
> rpm2cpio gzip nomarch pax lzop rar unrar unzip zoo unace razor pyzor 
> tnef ripole zip p7zip-full mc multitail ccze libcppunit-dev 
> libsasl2-dev 
> libxml2-dev libkrb5-dev libdb-dev libnetfilter-conntrack-dev 
> libexpat1-dev libcap2-dev libldap2-dev libpam0g-dev libgnutls28-dev 
> libssl-dev libdbi-perl libecap3 libecap3-dev libntlm0-dev 
> libkf5kiontlm5 
> samba-dev ldap-utils
> 
> Instalar binarios necesarios para compilar Squid4, the system 
> retunr a 
> error whe in start the service:
> 
> sudo apt install logrotate acl attr autoconf bison nettle-dev 
> build-essential libacl1-dev libaio-dev libattr1-dev libblkid-dev 
> libbsd-dev libcap2-dev libcppunit-dev libldap2-dev pkg-config 
> libxml2-dev libdb-dev libgnutls28-dev openssl devscripts fakeroot 
> libdbi-perl libssl1.0-dev libcppunit-dev libecap3-dev libkrb5-dev 
> comerr-dev libnetfilter-conntrack-dev libpam0g-dev libsasl2-dev
> 
>  ?sudo groupadd -g 13 proxy
>  ?sudo mkdir -p /var/spool/squid
>  ?sudo mkdir -p /var/log/squid
>  ?sudo useradd --system -g proxy -u 13 -d /var/spool/squid -M -s 
> /usr/sbin/nologin proxy
>  ?sudo chown proxy:proxy /var/spool/squid
>  ?sudo chown proxy:proxy /var/log/squid
> 
> cd /opt
> wget -c http://www.squid-cache.org/Versions/v4/squid-4.6.tar.xz
>  ?tar xfv squid-4.6.tar.xz
> 
> Configuramos las opciones b?sicas que podamos necesitar:
> 
> ./configure --srcdir=. --prefix=/usr --localstatedir=/var/lib/squid 
> --libexecdir=/usr/lib/squid --datadir=/usr/share/squid 
> --sysconfdir=/etc/squid --with-default-user=proxy 
> --with-logdir=/var/log/squid --with-open-ssl=/etc/ssl/openssl.cnf 
> --with-openssl --enable-ssl --enable-ssl-crtd 
> --build=x86_64-linux-gnu 
> --with-pidfile=/var/run/squid.pid --enable-removal-policies=lru,heap 
> --enable-delay-pools --enable-cache-digests --enable-icap-client 
> --enable-ecap --enable-follow-x-forwarded-for --with-large-files 
> --with-filedescriptors=65536 
> --enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RA
DIUS,SASL,SMB 
> --enable-auth-digest=file,LDAP 
> --enable-auth-negotiate=kerberos,wrapper 
> --enable-auth-ntlm=fake --enable-linux-netfilter 
> --with-swapdir=/var/spool/squid --enable-useragent-log --enable-htpc 
> --infodir=/usr/share/info --mandir=/usr/share/man 
> --includedir=/usr/include --disable-maintainer-mode 
> --disable-dependency-tracking --disable-silent-rules --enable-inline 
> --enable-async-io --enable-storeio=ufs,aufs,diskd,rock --enable-eui 
> --enable-esi --enable-icmp --enable-zph-qos 
> --enable-external-acl-helpers=file_userip,kerberos_ldap_group,
time_quota,LDAP_group,session,SQL_session,unix_group,wbinfo_group 
> --enable-url-rewrite-helpers=fake --enable-translation --enable-epoll 
> --enable-snmp --enable-wccpv2 --with-aio --with-pthreads
> 
> Compiling with? 4 cores:
> 
> make -j 4
> 
> Installing:
> 
> sudo? make install
> 
> create initi script:
> 
> sudo nano /etc/init.d/squid
> 
> initi content:
> 
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> # squid4???? Startup script for the SQUID HTTP proxy-cache.
> #
> # Version:?? @(#)squid4 init script? 1.0? 20-Feb-2019 
> leslie84 at nauta.cu
> ##############################################################
> ##########
> 
> ### BEGIN INIT INFO
> # Provides:????????? squid
> # Required-Start:??? $network $remote_fs $syslog
> # Required-Stop:???? $network $remote_fs $syslog
> # Should-Start:????? $named
> # Should-Stop:?????? $named
> # Default-Start:???? 2 3 4 5
> # Default-Stop:????? 0 1 6
> # Short-Description: Squid HTTP Proxy version 4.x
> ### END INIT INFO
> 
> NAME=squid
> DESC="Squid HTTP Proxy 4.x"
> DAEMON=/usr/sbin/squid
> PIDFILE=/var/run/$NAME.pid
> CONFIG=/etc/squid/squid.conf
> SQUID_ARGS="-YC -f $CONFIG"
> 
> [ ! -f /etc/default/squid ] || . /etc/default/squid
> 
> . /lib/lsb/init-functions
> 
> PATH=/bin:/usr/bin:/sbin:/usr/sbin
> 
> [ -x $DAEMON ] || exit 0
> 
> ulimit -n 65535
> 
> find_cache_dir () {
>  ??????? w="???? " # space tab
>  ??????? res=`sed -ne '
> s/^'$1'['"$w"']\+[^'"$w"']\+['"$w"']\+\([^'"$w"']\+\).*$/\1/p;
>  ??????????????? t end;
>  ??????????????? d;
>  ??????????????? :end q' < $CONFIG`
>  ??????? [ -n "$res" ] || res=$2
>  ??????? echo "$res"
> }
> 
> find_cache_type () {
>  ??????? w="???? " # space tab
>  ??????? res=`sed -ne '
>  ??????????????? s/^'$1'['"$w"']\+\([^'"$w"']\+\).*$/\1/p;
>  ??????????????? t end;
>  ??????????????? d;
>  ??????????????? :end q' < $CONFIG`
>  ??????? [ -n "$res" ] || res=$2
>  ??????? echo "$res"
> }
> 
> start () {
>  ??????? cache_dir=`find_cache_dir cache_dir`
>  ??????? cache_type=`find_cache_type cache_dir`
> 
>  ??????? #
>  ??????? # Create spool dirs if they don't exist.
>  ??????? #
>  ??????? if [ "$cache_type" = "coss" -a -d "$cache_dir" -a ! -f 
> "$cache_dir/stripe" ] || [ "$cache_type" != "coss" -a -d 
> "$cache_dir" -a 
> ! -d "$cache_dir/00" ]
>  ??????? then
>  ??????????????? log_warning_msg "Creating $DESC cache structure"
>  ??????????????? $DAEMON -z -f $CONFIG
>  ??????? fi
> 
>  ??????? umask 027
>  ??????? ulimit -n 65535
>  ??????? cd $cache_dir
>  ??????? start-stop-daemon --quiet --start \
>  ??????????????? --pidfile $PIDFILE \
>  ??????????????? --exec $DAEMON -- $SQUID_ARGS < /dev/null
>  ??????? return $?
> }
> 
> stop () {
>  ??????? PID=`cat $PIDFILE 2>/dev/null`
>  ??????? start-stop-daemon --stop --quiet --pidfile $PIDFILE 
> --exec $DAEMON
>  ??????? #
>  ??????? #?????? Now we have to wait until squid has _really_ stopped.
>  ??????? #
>  ??????? sleep 2
>  ??????? if test -n "$PID" && kill -0 $PID 2>/dev/null
>  ??????? then
>  ??????????????? log_action_begin_msg " Waiting"
>  ??????????????? cnt=0
>  ??????????????? while kill -0 $PID 2>/dev/null
>  ??????????????? do
>  ??????????????????????? cnt=`expr $cnt + 1`
>  ??????????????????????? if [ $cnt -gt 24 ]
>  ??????????????????????? then
>  ??????????????????????????????? log_action_end_msg 1
>  ??????????????????????????????? return 1
>  ??????????????????????? fi
>  ??????????????????????? sleep 5
>  ??????????????????????? log_action_cont_msg ""
>  ??????????????? done
>  ??????????????? log_action_end_msg 0
>  ??????????????? return 0
>  ??????? else
>  ??????????????? return 0
>  ??????? fi
> }
> 
> case "$1" in
>  ??? start)
>  ??????? log_daemon_msg "Starting $DESC" "$NAME"
>  ??????? if start ; then
>  ??????????????? log_end_msg $?
>  ??????? else
>  ??????????????? log_end_msg $?
>  ??????? fi
>  ??????? ;;
>  ??? stop)
>  ??????? log_daemon_msg "Stopping $DESC" "$NAME"
>  ??????? if stop ; then
>  ??????????????? log_end_msg $?
>  ??????? else
>  ??????????????? log_end_msg $?
>  ??????? fi
>  ??????? ;;
>  ??? reload|force-reload)
>  ??????? log_action_msg "Reloading $DESC configuration files"
>  ??????? start-stop-daemon --stop --signal 1 \
>  ??????????????? --pidfile $PIDFILE --quiet --exec $DAEMON
>  ??????? log_action_end_msg 0
>  ??????? ;;
>  ??? restart)
>  ??????? log_daemon_msg "Restarting $DESC" "$NAME"
>  ??????? stop
>  ??????? if start ; then
>  ??????????????? log_end_msg $?
>  ??????? else
>  ??????????????? log_end_msg $?
>  ??????? fi
>  ??????? ;;
>  ??? status)
>  ??????? status_of_proc -p $PIDFILE $DAEMON $NAME && exit 0 || exit 3
>  ??????? ;;
>  ??? *)
>  ??????? echo "Usage: /etc/init.d/$NAME 
> {start|stop|reload|force-reload|restart|status}"
>  ??????? exit 3
>  ??????? ;;
> esac
> 
> exit 0
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> 
> set permissions:
> 
> cambiar permisos al archivo
>  ???? sudo chmod +x /etc/init.d/squid
> 
> activar servicio de squid
>  ???? sudo update-rc.d squid defaults
> 
> 
> 
> After all this when i start the service this is the response 
> from the system
> 
> 
>  ?systemctl status squid.service
> ? squid.service
>  ?? Loaded: loaded (/etc/init.d/squid; generated)
>  ?? Active: failed (Result: exit-code) since Thu 2019-02-28 15:52:15 
> CST; 5s ago
>  ???? Docs: man:systemd-sysv-generator(8)
>  ? Process: 18732 ExecStart=/etc/init.d/squid start (code=exited, 
> status=203/EXEC)
> 
> feb 28 15:52:15 sq4 systemd[1]: Starting squid.service...
> feb 28 15:52:15 sq4 systemd[18732]: squid.service: Failed to execute 
> command: Exec format error
> feb 28 15:52:15 sq4 systemd[18732]: squid.service: Failed at 
> step EXEC 
> spawning /etc/init.d/squid: Exec format error
> feb 28 15:52:15 sq4 systemd[1]: squid.service: Control 
> process exited, 
> code=exited status=203
> feb 28 15:52:15 sq4 systemd[1]: squid.service: Failed with result 
> 'exit-code'.
> feb 28 15:52:15 sq4 systemd[1]: Failed to start squid.service.
> 
> 
> Im using Ubuntu 18.04.2, thanks in advance.
> 
> 
> PD: Thanks for your answer? Rafael Akchurin but unforntunately the 
> article that you suggest won?t work for me.
> 
> -- 
> Saludos Cordiales
> 
> Lic. Alex Guti?rrez Mart?nez
> 
> Tel. +53 7 2710327
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From alex at dvm.esines.cu  Fri Mar  1 13:12:34 2019
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 1 Mar 2019 08:12:34 -0500
Subject: [squid-users] compiling squid 4
In-Reply-To: <6b3ee0ff-fa7d-87e2-9c46-8fbfe063f463@dvm.esines.cu>
References: <6b3ee0ff-fa7d-87e2-9c46-8fbfe063f463@dvm.esines.cu>
Message-ID: <baaf3cd5-2553-63ee-39bd-eda6248f458e@dvm.esines.cu>

Hello again community, I still have problems compiling squid 4. This is 
what i did this time:


Basic support:

sudo apt install logrotate acl attr autoconf bison nettle-dev 
build-essential libacl1-dev libaio-dev libattr1-dev libblkid-dev 
libbsd-dev libcap2-dev libcppunit-dev libldap2-dev pkg-config 
libxml2-dev libdb-dev libgnutls28-dev openssl devscripts fakeroot 
libdbi-perl libssl1.0-dev libcppunit-dev libecap3-dev libkrb5-dev 
comerr-dev libnetfilter-conntrack-dev libpam0g-dev libsasl2-dev rj bzip2 
xz-utils cabextract cpio file lzma lhasa lzop rpm2cpio gzip nomarch pax 
lzop rar unrar unzip zoo unace razor pyzor tnef ripole zip p7zip-full mc 
multitail ccze libcppunit-dev libsasl2-dev libxml2-dev libkrb5-dev 
libdb-dev libnetfilter-conntrack-dev libexpat1-dev libcap2-dev 
libldap2-dev libpam0g-dev libgnutls28-dev libssl-dev libdbi-perl 
libecap3 libecap3-dev libntlm0-dev libkf5kiontlm5 samba-dev ldap-utils

 ?sudo groupadd -g 13 proxy
 ?sudo mkdir -p /var/spool/squid
 ?sudo mkdir -p /var/log/squid
 ?sudo useradd --system -g proxy -u 13 -d /var/spool/squid -M -s 
/usr/sbin/nologin proxy
 ?sudo chown proxy:proxy /var/spool/squid
 ?sudo chown proxy:proxy /var/log/squid

cd /opt
wget -c http://www.squid-cache.org/Versions/v4/squid-4.6.tar.xz
 ?tar xfv squid-4.6.tar.xz

Configuramos las opciones b?sicas que podamos necesitar:

./configure --srcdir=. --prefix=/usr --localstatedir=/var/lib/squid 
--libexecdir=/usr/lib/squid --datadir=/usr/share/squid 
--sysconfdir=/etc/squid --with-default-user=proxy 
--with-logdir=/var/log/squid --with-open-ssl=/etc/ssl/openssl.cnf 
--with-openssl --enable-ssl --enable-ssl-crtd --build=x86_64-linux-gnu 
--with-pidfile=/var/run/squid.pid --enable-removal-policies=lru,heap 
--enable-delay-pools --enable-cache-digests --enable-icap-client 
--enable-ecap --enable-follow-x-forwarded-for --with-large-files 
--with-filedescriptors=65536 
--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB 
--enable-auth-digest=file,LDAP --enable-auth-negotiate=kerberos,wrapper 
--enable-auth-ntlm=fake --enable-linux-netfilter 
--with-swapdir=/var/spool/squid --enable-useragent-log --enable-htpc 
--infodir=/usr/share/info --mandir=/usr/share/man 
--includedir=/usr/include --disable-maintainer-mode 
--disable-dependency-tracking --disable-silent-rules --enable-inline 
--enable-async-io --enable-storeio=ufs,aufs,diskd,rock --enable-eui 
--enable-esi --enable-icmp --enable-zph-qos 
--enable-external-acl-helpers=file_userip,kerberos_ldap_group,time_quota,LDAP_group,session,SQL_session,unix_group,wbinfo_group 
--enable-url-rewrite-helpers=fake --enable-translation --enable-epoll 
--enable-snmp --enable-wccpv2 --with-aio --with-pthreads

Compiling with? 4 cores:

make -j 4

Installing:

sudo? make install

create initi script:

sudo nano /etc/init.d/squid

initi content:

######################################################################################################
######################################################################################################
######################################################################################################
# squid4???? Startup script for the SQUID HTTP proxy-cache.
#
# Version:?? @(#)squid4 init script? 1.0? 20-Feb-2019 leslie84 at nauta.cu
########################################################################

### BEGIN INIT INFO
# Provides:????????? squid
# Required-Start:??? $network $remote_fs $syslog
# Required-Stop:???? $network $remote_fs $syslog
# Should-Start:????? $named
# Should-Stop:?????? $named
# Default-Start:???? 2 3 4 5
# Default-Stop:????? 0 1 6
# Short-Description: Squid HTTP Proxy version 4.x
### END INIT INFO

NAME=squid
DESC="Squid HTTP Proxy 4.x"
DAEMON=/usr/sbin/squid
PIDFILE=/var/run/$NAME.pid
CONFIG=/etc/squid/squid.conf
SQUID_ARGS="-YC -f $CONFIG"

[ ! -f /etc/default/squid ] || . /etc/default/squid

. /lib/lsb/init-functions

PATH=/bin:/usr/bin:/sbin:/usr/sbin

[ -x $DAEMON ] || exit 0

ulimit -n 65535

find_cache_dir () {
 ??????? w="???? " # space tab
 ??????? res=`sed -ne '
s/^'$1'['"$w"']\+[^'"$w"']\+['"$w"']\+\([^'"$w"']\+\).*$/\1/p;
 ??????????????? t end;
 ??????????????? d;
 ??????????????? :end q' < $CONFIG`
 ??????? [ -n "$res" ] || res=$2
 ??????? echo "$res"
}

find_cache_type () {
 ??????? w="???? " # space tab
 ??????? res=`sed -ne '
 ??????????????? s/^'$1'['"$w"']\+\([^'"$w"']\+\).*$/\1/p;
 ??????????????? t end;
 ??????????????? d;
 ??????????????? :end q' < $CONFIG`
 ??????? [ -n "$res" ] || res=$2
 ??????? echo "$res"
}

start () {
 ??????? cache_dir=`find_cache_dir cache_dir`
 ??????? cache_type=`find_cache_type cache_dir`

 ??????? #
 ??????? # Create spool dirs if they don't exist.
 ??????? #
 ??????? if [ "$cache_type" = "coss" -a -d "$cache_dir" -a ! -f 
"$cache_dir/stripe" ] || [ "$cache_type" != "coss" -a -d "$cache_dir" -a 
! -d "$cache_dir/00" ]
 ??????? then
 ??????????????? log_warning_msg "Creating $DESC cache structure"
 ??????????????? $DAEMON -z -f $CONFIG
 ??????? fi

 ??????? umask 027
 ??????? ulimit -n 65535
 ??????? cd $cache_dir
 ??????? start-stop-daemon --quiet --start \
 ??????????????? --pidfile $PIDFILE \
 ??????????????? --exec $DAEMON -- $SQUID_ARGS < /dev/null
 ??????? return $?
}

stop () {
 ??????? PID=`cat $PIDFILE 2>/dev/null`
 ??????? start-stop-daemon --stop --quiet --pidfile $PIDFILE --exec $DAEMON
 ??????? #
 ??????? #?????? Now we have to wait until squid has _really_ stopped.
 ??????? #
 ??????? sleep 2
 ??????? if test -n "$PID" && kill -0 $PID 2>/dev/null
 ??????? then
 ??????????????? log_action_begin_msg " Waiting"
 ??????????????? cnt=0
 ??????????????? while kill -0 $PID 2>/dev/null
 ??????????????? do
 ??????????????????????? cnt=`expr $cnt + 1`
 ??????????????????????? if [ $cnt -gt 24 ]
 ??????????????????????? then
 ??????????????????????????????? log_action_end_msg 1
 ??????????????????????????????? return 1
 ??????????????????????? fi
 ??????????????????????? sleep 5
 ??????????????????????? log_action_cont_msg ""
 ??????????????? done
 ??????????????? log_action_end_msg 0
 ??????????????? return 0
 ??????? else
 ??????????????? return 0
 ??????? fi
}

case "$1" in
 ??? start)
 ??????? log_daemon_msg "Starting $DESC" "$NAME"
 ??????? if start ; then
 ??????????????? log_end_msg $?
 ??????? else
 ??????????????? log_end_msg $?
 ??????? fi
 ??????? ;;
 ??? stop)
 ??????? log_daemon_msg "Stopping $DESC" "$NAME"
 ??????? if stop ; then
 ??????????????? log_end_msg $?
 ??????? else
 ??????????????? log_end_msg $?
 ??????? fi
 ??????? ;;
 ??? reload|force-reload)
 ??????? log_action_msg "Reloading $DESC configuration files"
 ??????? start-stop-daemon --stop --signal 1 \
 ??????????????? --pidfile $PIDFILE --quiet --exec $DAEMON
 ??????? log_action_end_msg 0
 ??????? ;;
 ??? restart)
 ??????? log_daemon_msg "Restarting $DESC" "$NAME"
 ??????? stop
 ??????? if start ; then
 ??????????????? log_end_msg $?
 ??????? else
 ??????????????? log_end_msg $?
 ??????? fi
 ??????? ;;
 ??? status)
 ??????? status_of_proc -p $PIDFILE $DAEMON $NAME && exit 0 || exit 3
 ??????? ;;
 ??? *)
 ??????? echo "Usage: /etc/init.d/$NAME 
{start|stop|reload|force-reload|restart|status}"
 ??????? exit 3
 ??????? ;;
esac

exit 0
######################################################################################################
######################################################################################################
######################################################################################################

set permissions:

cambiar permisos al archivo
 ???? sudo chmod +x /etc/init.d/squid

activar servicio de squid
 ???? sudo update-rc.d squid defaults



After all this when i start the service this is the response from the system


 ?systemctl status squid.service
? squid.service
 ?? Loaded: loaded (/etc/init.d/squid; generated)
 ?? Active: failed (Result: exit-code) since Thu 2019-02-28 15:52:15 
CST; 5s ago
 ???? Docs: man:systemd-sysv-generator(8)
 ? Process: 18732 ExecStart=/etc/init.d/squid start (code=exited, 
status=203/EXEC)

feb 28 15:52:15 sq4 systemd[1]: Starting squid.service...
feb 28 15:52:15 sq4 systemd[18732]: squid.service: Failed to execute 
command: Exec format error
feb 28 15:52:15 sq4 systemd[18732]: squid.service: Failed at step EXEC 
spawning /etc/init.d/squid: Exec format error
feb 28 15:52:15 sq4 systemd[1]: squid.service: Control process exited, 
code=exited status=203
feb 28 15:52:15 sq4 systemd[1]: squid.service: Failed with result 
'exit-code'.
feb 28 15:52:15 sq4 systemd[1]: Failed to start squid.service.


Im using Ubuntu 18.04.2, thanks in advance.


PD: Thanks for your answer? Rafael Akchurin but unforntunately the 
article that you suggest won?t work for me.

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327





From belle at bazuin.nl  Fri Mar  1 15:55:04 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 1 Mar 2019 16:55:04 +0100
Subject: [squid-users] compiling squid 4
In-Reply-To: <baaf3cd5-2553-63ee-39bd-eda6248f458e@dvm.esines.cu>
References: <6b3ee0ff-fa7d-87e2-9c46-8fbfe063f463@dvm.esines.cu>
Message-ID: <vmime.5c7955d8.65bf.77c448e8778e7758@ms249-lin-003.rotterdam.bazuin.nl>

Hai Alex, 

Ahh.. You wanted with ssl, sorry missed that. 
Here you go. 

apt-get source squid

cd squid-4.6/debian/

Edit rules, after the line, --with-gnutls
Add these: --enable-ssl --enable-ssl-crtd --with-openssl
Save.

Edit changelog
Change the version 4.6-1 to 4.6-1ssl
Save 

Install libgnutls28-dev openssl 

cd ../..

apt-get source squid3 -b

And resulting in squid 4.6 with ssl. 

This is the most simple way to compile squid and get it working. 
Its not the best way, for that setup a builder environment. 

This works! Try it. 

Greetz, 

Louis

> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Alex Guti?rrez Mart?nez
> Verzonden: vrijdag 1 maart 2019 14:13
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: [squid-users] compiling squid 4
> 
> Hello again community, I still have problems compiling squid 
> 4. This is 
> what i did this time:
> 
> 
> Basic support:
> 
> sudo apt install logrotate acl attr autoconf bison nettle-dev 
> build-essential libacl1-dev libaio-dev libattr1-dev libblkid-dev 
> libbsd-dev libcap2-dev libcppunit-dev libldap2-dev pkg-config 
> libxml2-dev libdb-dev libgnutls28-dev openssl devscripts fakeroot 
> libdbi-perl libssl1.0-dev libcppunit-dev libecap3-dev libkrb5-dev 
> comerr-dev libnetfilter-conntrack-dev libpam0g-dev 
> libsasl2-dev rj bzip2 
> xz-utils cabextract cpio file lzma lhasa lzop rpm2cpio gzip 
> nomarch pax 
> lzop rar unrar unzip zoo unace razor pyzor tnef ripole zip 
> p7zip-full mc 
> multitail ccze libcppunit-dev libsasl2-dev libxml2-dev libkrb5-dev 
> libdb-dev libnetfilter-conntrack-dev libexpat1-dev libcap2-dev 
> libldap2-dev libpam0g-dev libgnutls28-dev libssl-dev libdbi-perl 
> libecap3 libecap3-dev libntlm0-dev libkf5kiontlm5 samba-dev ldap-utils
> 
>  ?sudo groupadd -g 13 proxy
>  ?sudo mkdir -p /var/spool/squid
>  ?sudo mkdir -p /var/log/squid
>  ?sudo useradd --system -g proxy -u 13 -d /var/spool/squid -M -s 
> /usr/sbin/nologin proxy
>  ?sudo chown proxy:proxy /var/spool/squid
>  ?sudo chown proxy:proxy /var/log/squid
> 
> cd /opt
> wget -c http://www.squid-cache.org/Versions/v4/squid-4.6.tar.xz
>  ?tar xfv squid-4.6.tar.xz
> 
> Configuramos las opciones b?sicas que podamos necesitar:
> 
> ./configure --srcdir=. --prefix=/usr --localstatedir=/var/lib/squid 
> --libexecdir=/usr/lib/squid --datadir=/usr/share/squid 
> --sysconfdir=/etc/squid --with-default-user=proxy 
> --with-logdir=/var/log/squid --with-open-ssl=/etc/ssl/openssl.cnf 
> --with-openssl --enable-ssl --enable-ssl-crtd 
> --build=x86_64-linux-gnu 
> --with-pidfile=/var/run/squid.pid --enable-removal-policies=lru,heap 
> --enable-delay-pools --enable-cache-digests --enable-icap-client 
> --enable-ecap --enable-follow-x-forwarded-for --with-large-files 
> --with-filedescriptors=65536 
> --enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RA
DIUS,SASL,SMB 
> --enable-auth-digest=file,LDAP 
> --enable-auth-negotiate=kerberos,wrapper 
> --enable-auth-ntlm=fake --enable-linux-netfilter 
> --with-swapdir=/var/spool/squid --enable-useragent-log --enable-htpc 
> --infodir=/usr/share/info --mandir=/usr/share/man 
> --includedir=/usr/include --disable-maintainer-mode 
> --disable-dependency-tracking --disable-silent-rules --enable-inline 
> --enable-async-io --enable-storeio=ufs,aufs,diskd,rock --enable-eui 
> --enable-esi --enable-icmp --enable-zph-qos 
> --enable-external-acl-helpers=file_userip,kerberos_ldap_group,
time_quota,LDAP_group,session,SQL_session,unix_group,wbinfo_group 
> --enable-url-rewrite-helpers=fake --enable-translation --enable-epoll 
> --enable-snmp --enable-wccpv2 --with-aio --with-pthreads
> 
> Compiling with? 4 cores:
> 
> make -j 4
> 
> Installing:
> 
> sudo? make install
> 
> create initi script:
> 
> sudo nano /etc/init.d/squid
> 
> initi content:
> 
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> # squid4???? Startup script for the SQUID HTTP proxy-cache.
> #
> # Version:?? @(#)squid4 init script? 1.0? 20-Feb-2019 
> leslie84 at nauta.cu
> ##############################################################
> ##########
> 
> ### BEGIN INIT INFO
> # Provides:????????? squid
> # Required-Start:??? $network $remote_fs $syslog
> # Required-Stop:???? $network $remote_fs $syslog
> # Should-Start:????? $named
> # Should-Stop:?????? $named
> # Default-Start:???? 2 3 4 5
> # Default-Stop:????? 0 1 6
> # Short-Description: Squid HTTP Proxy version 4.x
> ### END INIT INFO
> 
> NAME=squid
> DESC="Squid HTTP Proxy 4.x"
> DAEMON=/usr/sbin/squid
> PIDFILE=/var/run/$NAME.pid
> CONFIG=/etc/squid/squid.conf
> SQUID_ARGS="-YC -f $CONFIG"
> 
> [ ! -f /etc/default/squid ] || . /etc/default/squid
> 
> . /lib/lsb/init-functions
> 
> PATH=/bin:/usr/bin:/sbin:/usr/sbin
> 
> [ -x $DAEMON ] || exit 0
> 
> ulimit -n 65535
> 
> find_cache_dir () {
>  ??????? w="???? " # space tab
>  ??????? res=`sed -ne '
> s/^'$1'['"$w"']\+[^'"$w"']\+['"$w"']\+\([^'"$w"']\+\).*$/\1/p;
>  ??????????????? t end;
>  ??????????????? d;
>  ??????????????? :end q' < $CONFIG`
>  ??????? [ -n "$res" ] || res=$2
>  ??????? echo "$res"
> }
> 
> find_cache_type () {
>  ??????? w="???? " # space tab
>  ??????? res=`sed -ne '
>  ??????????????? s/^'$1'['"$w"']\+\([^'"$w"']\+\).*$/\1/p;
>  ??????????????? t end;
>  ??????????????? d;
>  ??????????????? :end q' < $CONFIG`
>  ??????? [ -n "$res" ] || res=$2
>  ??????? echo "$res"
> }
> 
> start () {
>  ??????? cache_dir=`find_cache_dir cache_dir`
>  ??????? cache_type=`find_cache_type cache_dir`
> 
>  ??????? #
>  ??????? # Create spool dirs if they don't exist.
>  ??????? #
>  ??????? if [ "$cache_type" = "coss" -a -d "$cache_dir" -a ! -f 
> "$cache_dir/stripe" ] || [ "$cache_type" != "coss" -a -d 
> "$cache_dir" -a 
> ! -d "$cache_dir/00" ]
>  ??????? then
>  ??????????????? log_warning_msg "Creating $DESC cache structure"
>  ??????????????? $DAEMON -z -f $CONFIG
>  ??????? fi
> 
>  ??????? umask 027
>  ??????? ulimit -n 65535
>  ??????? cd $cache_dir
>  ??????? start-stop-daemon --quiet --start \
>  ??????????????? --pidfile $PIDFILE \
>  ??????????????? --exec $DAEMON -- $SQUID_ARGS < /dev/null
>  ??????? return $?
> }
> 
> stop () {
>  ??????? PID=`cat $PIDFILE 2>/dev/null`
>  ??????? start-stop-daemon --stop --quiet --pidfile $PIDFILE 
> --exec $DAEMON
>  ??????? #
>  ??????? #?????? Now we have to wait until squid has _really_ stopped.
>  ??????? #
>  ??????? sleep 2
>  ??????? if test -n "$PID" && kill -0 $PID 2>/dev/null
>  ??????? then
>  ??????????????? log_action_begin_msg " Waiting"
>  ??????????????? cnt=0
>  ??????????????? while kill -0 $PID 2>/dev/null
>  ??????????????? do
>  ??????????????????????? cnt=`expr $cnt + 1`
>  ??????????????????????? if [ $cnt -gt 24 ]
>  ??????????????????????? then
>  ??????????????????????????????? log_action_end_msg 1
>  ??????????????????????????????? return 1
>  ??????????????????????? fi
>  ??????????????????????? sleep 5
>  ??????????????????????? log_action_cont_msg ""
>  ??????????????? done
>  ??????????????? log_action_end_msg 0
>  ??????????????? return 0
>  ??????? else
>  ??????????????? return 0
>  ??????? fi
> }
> 
> case "$1" in
>  ??? start)
>  ??????? log_daemon_msg "Starting $DESC" "$NAME"
>  ??????? if start ; then
>  ??????????????? log_end_msg $?
>  ??????? else
>  ??????????????? log_end_msg $?
>  ??????? fi
>  ??????? ;;
>  ??? stop)
>  ??????? log_daemon_msg "Stopping $DESC" "$NAME"
>  ??????? if stop ; then
>  ??????????????? log_end_msg $?
>  ??????? else
>  ??????????????? log_end_msg $?
>  ??????? fi
>  ??????? ;;
>  ??? reload|force-reload)
>  ??????? log_action_msg "Reloading $DESC configuration files"
>  ??????? start-stop-daemon --stop --signal 1 \
>  ??????????????? --pidfile $PIDFILE --quiet --exec $DAEMON
>  ??????? log_action_end_msg 0
>  ??????? ;;
>  ??? restart)
>  ??????? log_daemon_msg "Restarting $DESC" "$NAME"
>  ??????? stop
>  ??????? if start ; then
>  ??????????????? log_end_msg $?
>  ??????? else
>  ??????????????? log_end_msg $?
>  ??????? fi
>  ??????? ;;
>  ??? status)
>  ??????? status_of_proc -p $PIDFILE $DAEMON $NAME && exit 0 || exit 3
>  ??????? ;;
>  ??? *)
>  ??????? echo "Usage: /etc/init.d/$NAME 
> {start|stop|reload|force-reload|restart|status}"
>  ??????? exit 3
>  ??????? ;;
> esac
> 
> exit 0
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> ##############################################################
> ########################################
> 
> set permissions:
> 
> cambiar permisos al archivo
>  ???? sudo chmod +x /etc/init.d/squid
> 
> activar servicio de squid
>  ???? sudo update-rc.d squid defaults
> 
> 
> 
> After all this when i start the service this is the response 
> from the system
> 
> 
>  ?systemctl status squid.service
> ? squid.service
>  ?? Loaded: loaded (/etc/init.d/squid; generated)
>  ?? Active: failed (Result: exit-code) since Thu 2019-02-28 15:52:15 
> CST; 5s ago
>  ???? Docs: man:systemd-sysv-generator(8)
>  ? Process: 18732 ExecStart=/etc/init.d/squid start (code=exited, 
> status=203/EXEC)
> 
> feb 28 15:52:15 sq4 systemd[1]: Starting squid.service...
> feb 28 15:52:15 sq4 systemd[18732]: squid.service: Failed to execute 
> command: Exec format error
> feb 28 15:52:15 sq4 systemd[18732]: squid.service: Failed at 
> step EXEC 
> spawning /etc/init.d/squid: Exec format error
> feb 28 15:52:15 sq4 systemd[1]: squid.service: Control 
> process exited, 
> code=exited status=203
> feb 28 15:52:15 sq4 systemd[1]: squid.service: Failed with result 
> 'exit-code'.
> feb 28 15:52:15 sq4 systemd[1]: Failed to start squid.service.
> 
> 
> Im using Ubuntu 18.04.2, thanks in advance.
> 
> 
> PD: Thanks for your answer? Rafael Akchurin but unforntunately the 
> article that you suggest won?t work for me.
> 
> -- 
> Saludos Cordiales
> 
> Lic. Alex Guti?rrez Mart?nez
> 
> Tel. +53 7 2710327
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Fri Mar  1 19:01:38 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Mar 2019 08:01:38 +1300
Subject: [squid-users] ipv4 + ipv6
In-Reply-To: <1551447673452-0.post@n4.nabble.com>
References: <1551447673452-0.post@n4.nabble.com>
Message-ID: <5fc6004d-0dde-d60b-cc29-27cbffc3b4cd@treenet.co.nz>

On 2/03/19 2:41 am, mzgmedia wrote:
> hello
> 
> we a squid server with both IPv4 and IPv6
> 
> now if a user will connect to Ipv4, it will also get an Ipv6 from squid. We
> want to prevent that, if the user will connect to IPv4, to have just Ipv4
> and if connects to Ipv6 to have just Ipv6 and it seems that we can't manage
> to do this.

Sounds like you have a regular forward/explicit proxy setup right?

In those setups the client is connecting only to Squid. It delegates
*all* server operations to the proxy. IP address(es) of the next server
in the HTTP chain has nothing to do with the TC/IP connections between
client and proxy.

Amos


From squid3 at treenet.co.nz  Fri Mar  1 19:09:42 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Mar 2019 08:09:42 +1300
Subject: [squid-users] compiling squid 4
In-Reply-To: <vmime.5c7955d8.65bf.77c448e8778e7758@ms249-lin-003.rotterdam.bazuin.nl>
References: <6b3ee0ff-fa7d-87e2-9c46-8fbfe063f463@dvm.esines.cu>
 <vmime.5c7955d8.65bf.77c448e8778e7758@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <c0bd0216-3a28-356e-fcd3-d10bc154a727@treenet.co.nz>

On 2/03/19 4:55 am, L.P.H. van Belle wrote:
> Hai Alex, 
> 
> Ahh.. You wanted with ssl, sorry missed that. 
> Here you go. 
> 
> apt-get source squid
> 
> cd squid-4.6/debian/
> 
> Edit rules, after the line, --with-gnutls
> Add these: --enable-ssl --enable-ssl-crtd --with-openssl
> Save.
> 
> Edit changelog
> Change the version 4.6-1 to 4.6-1ssl
> Save 
> 
> Install libgnutls28-dev openssl 


Make that:
  apt-get install libgnutls28-dev libssl-dev openssl


Amos


From the.tuxster at gmail.com  Fri Mar  1 19:54:29 2019
From: the.tuxster at gmail.com (Joseph Jones)
Date: Fri, 1 Mar 2019 19:54:29 +0000
Subject: [squid-users] Sslbump Not working for transparent proxy
Message-ID: <CAEjE5eq3PKCvRWK2HyaL+5BzYjc2yazxFRmZBYyuRMxt8YTTxQ@mail.gmail.com>

I've been trying to get SslBump work for whitelist purposes and so far
have been failing.

It's my understanding in order for SslBump to do whitelist it will
need to do a splice at step2 or step3.

Looking at my logs I see step1 matching but I never see step2. I
believe it's because of what I found in the docs.

> Rules with actions that are impossible at the current step are ignored.

What I believe I'm failing to understand is the the order SslBump
steps are happening and when I can actually use the whitelist acl to
compare to the SNI provided.

watching the logs I see the http_access request happens in the order
they appear in the file, but SslBump step1 seems to happen before any
http_access.

Since I never see step2 happen in my logs I'm not sure where it
happens. ultimately the request is rejected because of my final deny
all at line 57.

I've also observed that using a non-transparent proxy. SSL and non SSL
request get evaluated at line 48 which is where I allow from my
whitelist and localnet. This seems to make sense. So the only thing I
need to understand I believe is the SSLbump steps.

is the final deny I have wrong? Or is my ssl_bump simply wrong? when
does the first step to happen?

you'll notice in my config I've commented a bunch of http_access out.
I was hoping if I made the file simpler it would be easier to
troubleshoot. I intend to put them back when I figure out my problem.

cache.log: https://pastebin.com/uZVn6f4Q
squid.conf: https://pastebin.com/D49H5rYS
squid -k parse: https://pastebin.com/F0U2SvUm

-- 
Joseph M Jones


From amlgp at mftsl.xyz  Fri Mar  1 20:47:46 2019
From: amlgp at mftsl.xyz (amlgp)
Date: Fri, 1 Mar 2019 14:47:46 -0600 (CST)
Subject: [squid-users] Squid proxy 100% CPU 3.5.28 -Cache off
Message-ID: <1551473266758-0.post@n4.nabble.com>

I am running on a i7-7700k with 32GB of ram on Centos 6. 

Squid 3.5.28 is the latest version Centos 6 can run.

I have about 250 proxies/clients accessing squid and my cache deny all is on
the first line in my squid.conf. No errors in my logs and "squid -k parse"
shows no "WARNING" messages.

Clients/proxies are experiencing lag and when checking my Cpu usage is 100%.
I tried to restart and its smooth for a few minutes at 1-2% cpu, but then
jumps to 100% again.

I also tried squid -k debug but nothing shows up in my
/var/log/squid/cache.log which is where the log is supposed to go to(file
doesn't exist). Do I need to manually create the log file first?

I've also looked in squid.out and there hasn't been any error messages since
last month when I tried to install squid.

Any idea what I can do to debug the problem or figure out how to stop it
from happening? Thank you in advance!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Mar  1 21:23:56 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 1 Mar 2019 14:23:56 -0700
Subject: [squid-users] ipv4 + ipv6
In-Reply-To: <1551447673452-0.post@n4.nabble.com>
References: <1551447673452-0.post@n4.nabble.com>
Message-ID: <b9b05f9d-b138-7e22-373d-42595634f419@measurement-factory.com>

On 3/1/19 6:41 AM, mzgmedia wrote:

> we a squid server with both IPv4 and IPv6

> now if a user will connect to Ipv4, it will also get an Ipv6 from squid. We
> want to prevent that, if the user will connect to IPv4, to have just Ipv4
> and if connects to Ipv6 to have just Ipv6 and it seems that we can't manage
> to do this.

> Can you please provide some hits?

Amos is right, so please make sure that you truly _need_ what you are
asking about. For a more direct answer to your question, see [1].
TLDR: Squid currently lacks configuration options to enforce IP version
for outgoing connections.

[1]
http://lists.squid-cache.org/pipermail/squid-users/2017-October/016769.html

Alex.


From rousskov at measurement-factory.com  Fri Mar  1 21:39:27 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 1 Mar 2019 14:39:27 -0700
Subject: [squid-users] Squid proxy 100% CPU 3.5.28 -Cache off
In-Reply-To: <1551473266758-0.post@n4.nabble.com>
References: <1551473266758-0.post@n4.nabble.com>
Message-ID: <e029b648-8052-4ac3-66d4-0fd2bc49eb04@measurement-factory.com>

On 3/1/19 1:47 PM, amlgp wrote:

> I have about 250 proxies/clients accessing squid and my cache deny all is on
> the first line in my squid.conf.

FYI: The position of the set of "cache" directives does not affect how
Squid works. Configuration is "compiled" at start time. At runtime,
Squid uses the "compiled" version of all the directives.


> Clients/proxies are experiencing lag and when checking my Cpu usage is 100%.
> I tried to restart and its smooth for a few minutes at 1-2% cpu, but then
> jumps to 100% again.

You might be suffering from Bug 4885:
https://bugs.squid-cache.org/show_bug.cgi?id=4885


> I also tried squid -k debug but nothing shows up in my
> /var/log/squid/cache.log which is where the log is supposed to go to(file
> doesn't exist).

If that file does not exist then your Squid is logging elsewhere or
Squid tries to log somewhere it cannot. I would expect the latter to be
a fatal startup error, but I have not checked that.


> Do I need to manually create the log file first?

No, you do not.


> Any idea what I can do to debug the problem

You should get cache.log working before you proceed any further. That
log may contain critical information that is not available in system
logs. I would start by checking cache_log option in your squid.conf. If
it is not there, see squid.conf.documented for your default location.

Also please make sure that debug_options is either absent or set to
something like ALL,1.

You can also collect stack traces from a running Squid process, but I
would get cache.log working first.

Alex.


From michael at hendrie.id.au  Fri Mar  1 22:43:49 2019
From: michael at hendrie.id.au (Michael Hendrie)
Date: Sat, 2 Mar 2019 09:13:49 +1030
Subject: [squid-users] Squid-3.5.28 slowdown
In-Reply-To: <E2D7AB5B-0C8F-4E91-B43E-F5FCDDB2CD1E@data-core.org>
References: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>
 <E2D7AB5B-0C8F-4E91-B43E-F5FCDDB2CD1E@data-core.org>
Message-ID: <996B1404-3E7D-4070-A90C-AFC0999AFAC0@hendrie.id.au>


> On 1 Mar 2019, at 9:34 pm, Enrico Heine <flashdown at data-core.org> wrote:
> 
> >>just a shot into the dark<<, is it possible that you use the adaption service for ICAP?

There is an eCAP adaptation service but not ICAP, would eCAP be effected by the same condition reported the bug report you linked to?  
Early in the investigation I did change 'ecap enable off' and do 'squid -k reconfigure' while the condition was present but it didn't restore speed, a full squid restart was required.

> If so, fast test, this should return 0 if u are not affected by this, if higher than 0 check the link below:
> netstat -pa | grep CLOSE_WAIT | wc -l 
> 
> also have a look into /var/log/kern.log 

I will check these out next time the condition occurs

Thanks,

Michael





From squid3 at treenet.co.nz  Sat Mar  2 03:16:38 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Mar 2019 16:16:38 +1300
Subject: [squid-users] Sslbump Not working for transparent proxy
In-Reply-To: <CAEjE5eq3PKCvRWK2HyaL+5BzYjc2yazxFRmZBYyuRMxt8YTTxQ@mail.gmail.com>
References: <CAEjE5eq3PKCvRWK2HyaL+5BzYjc2yazxFRmZBYyuRMxt8YTTxQ@mail.gmail.com>
Message-ID: <ff886ef3-46b0-aa48-ff07-da5e8a9a7642@treenet.co.nz>

On 2/03/19 8:54 am, Joseph Jones wrote:
> I've been trying to get SslBump work for whitelist purposes and so far
> have been failing.
> 
> It's my understanding in order for SslBump to do whitelist it will
> need to do a splice at step2 or step3.

Not quite. For intercepted traffic you do need a peek at step1 to get
the TLS SNI details. Before that Squid only has raw-IP.


But your problem is earlier than even step1. Before bumping starts Squid
synthesizes a CONNECT message to check if the client is allowed to even
make requests of the proxy. This uses the TCP SYN packet src-IP as
message URI.


Your http_access permissions being *only* these:

 http_access allow localnet http_whitelist
 http_access deny all


... the raw-IP URI will not match true for the whitelist check. Leaving
the deny to reject the client.


Then we get to the SSL-Bump. Since the decision has already been made to
reject this client all Squid does is the peek and client-first bump
actions. These happen in order to deliver that denial page in a form
that Browsers will most likely display (no guarantee though).


What you need to avoid this too-early denial is allow CONNECT which
happen on the interception port. Add these lines above yoru deny all:

  acl port3129 myportname 3129
  http_access allow CONNECT port3129



PS. I also advise to leave the Safe_ports and SSL_Ports checks from the
default config as they were. They have no effect on any of the SSL-Bump
activity and protect your proxy against several types of DoS and other
nasty attacks.


Amos


From squid3 at treenet.co.nz  Sat Mar  2 05:28:48 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Mar 2019 18:28:48 +1300
Subject: [squid-users] Squid and url modifying
In-Reply-To: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
Message-ID: <5df95510-b780-f922-84e5-e4f4cdd5a18d@treenet.co.nz>

On 2/03/19 1:59 am, Egoitz Aurrekoetxea wrote:
> Good afternoon,
> 
> 
> Is it possible for Squid to do something like :
> 
> 
> - Receive request : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
> 
> 
> and
> 
> 
> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
> 

Possible? Yes.

Good Idea? No.

This is a typical URL-rewrite. Except that in order to perform the
SSL-Bump to see that URL in the first place Squid has probably had to
contact the server and now has TLS bound to that particular server. So
you better hope the server oooeeee.eeee.ttt.thesquidserver.org knows how
to answer requests for the https://oooeeee.eeee.ttt/* URLs.

Avoiding that problem is only possible with bumping at step2 (aka.
client-first bumping). Which opens your proxy to a large number of
possible attacks and nasty side effects from incompatible TLS features
being negotiated on client and server connections.
 YMMV on whether that is even usable for your situation.


> 
> I mean not to redirect users with url redirection. Just act as a proxy
> but having Squid the proper acknoledge internally for being able to make
> the proper request to the destination?. Is it possible without
> redirecting url, to return for instance a 403 error to the source web
> browser in order to not be able to access to the site if some kind of
> circumstances are given?

Of course.

The issue people tend to have is that Browsers do not show proxy error
messages in a lot of circumstances. They still show *an* error though.
So if your goal is just to block, yes that much is relatively easy.


> 
> If all conditions for allowing to see the content are OK, return the web
> browser a 301 redirect answer with the
> https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> URL. Else,
> just return a 403 or redirect you to a Forbidden page... I think this
> could be implemented with URL redirectors...but... the fact is... which
> kind of conditions or env situations can you use for validating the
> content inside the url redirector?.

302 is better since this is not a permanent state. Your policy may
change any time.

Anyhow, 30x redirection is the *best* way to do it.

The helper is contacted by Squid after the TLS has been decrypted and
the HTTP(S) messages area arriving. Before that point there is no URL to
rewrite or redirect.

Helpers can do anything they like. Squid can pass them details of the
client TCP connection, the TLS state, or the HTTP request the client has
delivered. And some other details (eg rDNS, IDENT, ASN or such) that
Squid can lookup. Anything else the helper can lookup by itself can also
be used.

That extreme flexibility is the point of the helper. If you just need a
somewhat static mapping deny_info can perform redirection faster.


Amos


From rousskov at measurement-factory.com  Sat Mar  2 22:21:05 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 2 Mar 2019 15:21:05 -0700
Subject: [squid-users] Squid and url modifying
In-Reply-To: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
Message-ID: <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>

On 3/1/19 5:59 AM, Egoitz Aurrekoetxea wrote:

> Is it possible for Squid to do something like :

> - Receive request : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88

> and

> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88

How does your Squid receive the former request? Amos' answer probably
assumes that your Squid is _not_ oooeeee.eeee.ttt.thesquidserver.org,
but the name you have chosen for your example may imply that it is.

* If your Squid is _intercepting_ traffic destined for the real
oooeeee.eeee.ttt.thesquidserver.org, then see Amos' answer.

* If your Squid is representing oooeeee.eeee.ttt.thesquidserver.org,
then your Squid is a reverse proxy that ought to have the certificate
key for that domain, and none of the SslBump problems that Amos
mentioned apply.

Please clarify what your use case is.

Alex.



> I mean not to redirect users with url redirection. Just act as a proxy
> but having Squid the proper acknoledge internally for being able to make
> the proper request to the destination?. Is it possible without
> redirecting url, to return for instance a 403 error to the source web
> browser in order to not be able to access to the site if some kind of
> circumstances are given?.
> 
> 
> If the last config, was not possible... perhaps I needed to just to
> redirect forcibly?. I have read for that purpose you can use URL
> redirectors.... so I assume the concept is :
> 
> 
> - Receive request : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
> 
> 
> and
> 
> 
> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
> 
> 
> If all conditions for allowing to see the content are OK, return the web
> browser a 301 redirect answer with the
> https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> URL. Else,
> just return a 403 or redirect you to a Forbidden page... I think this
> could be implemented with URL redirectors...but... the fact is... which
> kind of conditions or env situations can you use for validating the
> content inside the url redirector?.
> 
> 
> 
> Thanks a lot for your time :)
> 
> 
> Cheers!
> 
> 
> 
> 
> -- 
> sarenet
> *Egoitz Aurrekoetxea*
> Dpto. de sistemas
> 944 209 470
> Parque Tecnol?gico. Edificio 103
> 48170 Zamudio (Bizkaia)
> egoitz at sarenet.es <mailto:egoitz at sarenet.es>
> www.sarenet.es <http://www.sarenet.es>
> 
> Antes de imprimir este correo electr?nico piense si es necesario hacerlo.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From commercials24 at yahoo.de  Sun Mar  3 01:29:02 2019
From: commercials24 at yahoo.de (steven)
Date: Sun, 3 Mar 2019 02:29:02 +0100
Subject: [squid-users] icap not answering
Message-ID: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>

Hi,


i would like todo modifications on https connections and therefore 
enabled ssl bump in squid 4.4, now i would like to see the real traffic 
and icap looks like a way to watch and change that traffic.

but squid is not answering to icap://127.0.0.1:1344 when using pyicap or 
telnet.

the telnet error is:

telnet 127.0.0.1 1344
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused

which is imho good because it tells me that something is answering on 
that port after all.

did i misconfigure something?



config:

debug_options 28,9
#icap
icap_enable on
icap_service service_req reqmod_precache bypass=1 
icap://127.0.0.1:1344/reqmod
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0 
icap://127.0.0.1:1344/respmod
adaptation_access service_resp allow all
acl localnet src 127.0.0.1/32 192.168.10.0/24
http_access allow localnet
acl SSL_ports port 443
acl CONNECT method CONNECT
#http_access deny !Safe_ports
#http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
include /etc/squid/conf.d/*
http_access allow localhost
coredump_dir /var/spool/squid
refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
refresh_pattern ^gopher:??? 1440??? 0%??? 1440
refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
refresh_pattern .??? ??? 0??? 20%??? 4320
# default end
# my config
http_port 3128 accel ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
https_port 3129 ssl-bump intercept generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db 
-M 4MB
acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all



From marcus.kool at urlfilterdb.com  Sun Mar  3 10:11:07 2019
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sun, 3 Mar 2019 07:11:07 -0300
Subject: [squid-users] icap not answering
In-Reply-To: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>
References: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>
Message-ID: <f132eb9f-e448-ebef-e1dc-b27359a23d6c@urlfilterdb.com>

Squid is an ICAP client, not an ICAP server!, and does not repond on port 1344.
Marcus


On 02/03/2019 22:29, steven wrote:
> Hi,
>
>
> i would like todo modifications on https connections and therefore enabled ssl bump in squid 4.4, now i would like to see the real traffic and icap looks like a way to watch and change that traffic.
>
> but squid is not answering to icap://127.0.0.1:1344 when using pyicap or telnet.
>
> the telnet error is:
>
> telnet 127.0.0.1 1344
> Trying 127.0.0.1...
> telnet: Unable to connect to remote host: Connection refused
>
> which is imho good because it tells me that something is answering on that port after all.
>
> did i misconfigure something?
>
>
>
> config:
>
> debug_options 28,9
> #icap
> icap_enable on
> icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/reqmod
> adaptation_access service_req allow all
> icap_service service_resp respmod_precache bypass=0 icap://127.0.0.1:1344/respmod
> adaptation_access service_resp allow all
> acl localnet src 127.0.0.1/32 192.168.10.0/24
> http_access allow localnet
> acl SSL_ports port 443
> acl CONNECT method CONNECT
> #http_access deny !Safe_ports
> #http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> include /etc/squid/conf.d/*
> http_access allow localhost
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
> refresh_pattern .??? ??? 0??? 20%??? 4320
> # default end
> # my config
> http_port 3128 accel ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
> https_port 3129 ssl-bump intercept generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
> acl step1 at_step SslBump1
>
> ssl_bump peek step1
> ssl_bump bump all
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From georgexsh at gmail.com  Mon Mar  4 04:39:51 2019
From: georgexsh at gmail.com (George Xie)
Date: Mon, 4 Mar 2019 12:39:51 +0800
Subject: [squid-users] squid in container aborted on low memory server
Message-ID: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>

hi all:

Squid version: 3.5.23-5+deb9u1
Docker version 18.09.3, build 774a1f4
Linux instance-4 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64
GNU/Linux

I have the following squid config:


http_port 127.0.0.1:3128
cache deny all
access_log none


runs in a container with following Dockerfile:

FROM debian:9
RUN apt update && \
apt install --yes squid


the total memory of the host server is very low, only 592m, about 370m free
memory.
if I start squid in the container, squid will abort immediately.

error messages in /var/log/squid/cache.log:


FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!

Squid Cache (Version 3.5.23): Terminated abnormally.
CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
Maximum Resident Size: 47168 KB


error message captured with strace -f -e trace=memory:

[pid   920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)


it appears that squid (or glibc) tries to allocate 392m memory, which is
larger than host free memory 370m.
but I guess squid don't need that much memory, I have another running squid
instance, which only uses < 200m memory.
the oddest thing is if I run squid on the host (also Debian 9) directly,
not in the container, squid could start and run as normal.

am I doing something wrong thing here?

Xie Shi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/cef1c73c/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar  4 05:44:38 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Mar 2019 18:44:38 +1300
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
References: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
Message-ID: <1898faaa-4799-3bca-d118-51b8d01f3374@treenet.co.nz>

On 4/03/19 5:39 pm, George Xie wrote:
> hi all:
> 
> Squid version: 3.5.23-5+deb9u1
> Docker version 18.09.3, build 774a1f4
> Linux instance-4 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27)
> x86_64 GNU/Linux
> 
> I have the following squid config:
> 
> 
>     http_port 127.0.0.1:3128
>     cache deny all
>     access_log none
> 

What is it exactly that you think this is doing in regards to Squid
memory needs?


> 
> runs in a container with following Dockerfile:
> 
>     FROM debian:9
>     RUN apt update && \
>     apt install --yes squid
> 
> 
> the total memory of the host?server is very low, only 592m, about 370m
> free memory.
> if I start squid in the container, squid will abort immediately.?
> 
> error messages in /var/log/squid/cache.log:
> 
> 
>     FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!
> 
>     Squid Cache (Version 3.5.23): Terminated abnormally.
>     CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
>     Maximum Resident Size: 47168 KB
> 
> 
> error message captured with?strace -f -e trace=memory:
> 
>     [pid? ?920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
>     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
> 
> 
> it appears that squid (or glibc) tries to allocate 392m memory, which is
> larger than host free memory?370m.
> but I guess squid don't need that much memory, I have another running
> squid instance, which only uses < 200m memory.

No doubt it is configured to use less memory. For example by reducing
the default memory cache size.


> the oddest thing is if I run squid on the host (also Debian?9) directly,
> not in the container, squid could start and run as normal.
> 

Linux typically allows RAM over-allocation. Which works okay so long as
there is sufficient swap space and there is time between memory usage to
do the swap in/out process.

Amos


From leomessi983 at yahoo.com  Mon Mar  4 07:18:20 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Mon, 4 Mar 2019 07:18:20 +0000 (UTC)
Subject: [squid-users] sslcrtd_program db
References: <988283535.826940.1551683900857.ref@mail.yahoo.com>
Message-ID: <988283535.826940.1551683900857@mail.yahoo.com>

.
....Hi My problem is when i disable generate-host-certificates
sslcrtd_program
I cant redirect HTTPS requests to block err page!!I don't really understand what this configuration do!What does actually this configurations "generate-host-certificates and dynamic-cert-mem-cach-size" do? generate cert for squid to communicate to server or client? why when i disable it squid does not show block err page for HTTPS requests? i don't want to use? sslcrtd_program db in my host and i want to block HTTPS requests based of my acl and splice all? of other requests and also show block err page to clients!Which certificates stored in the sslcrtd_program directory? If i enable generate-host-certificates haw meny prossec do i need to handate more than 1000? requests?
Is that possible or? i have to use sslcrtd_program db?
Tank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/b487072e/attachment.htm>

From egoitz at sarenet.es  Mon Mar  4 07:53:47 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Mon, 04 Mar 2019 08:53:47 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
Message-ID: <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>

Good morning, 

Thanks a lot mates for your reading time. Alex, Amos speciall? to you
that answered too to this mail. 

My idea is simple. I wanted specific url, to be filtered through the
proxy. How can I manage this URL to be checked by the proxy?. I assumed,
I could modify the real and original content where urls appeared by
setting for instance : 

- Being the real url : https://oooeeee.eeee.ttt/u?ii=99&j=88 [2] 

- I would rewrite in the own content the URL so that  the new URL is now
: https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88 

The domain thesquidserver.org [2] will be used for doing wilcards. For
instance : *.thesquidserver.org *.*.thesquidserver.org etc... will
resolve to the ip of the Squid server. But I don't want any url being
asked as whatever.thesquidserver.org to be checked... just those ones I
have wrote in some place... 

So I was trying to write some content managing script, which should
check if that URL is needed to be checked and in case it should, check
it against an icap service. If Icap service gives you all to be right,
redirect you to the real site (just removing the thesquidserver.org for
the URL for instance). If that URL contains malware for instance, give
you an error page. 

This is all what I was trying to do... Some time ago, I used Squid with
Dansguardian for this kind of purposes, but now I wanted to do something
slightly different. I wanted to pass a request (if should be passed) to
an icap service and later depeding in the result of that ICAP service
(which I don't really know how could I check with an script) redirect to
the real site or give an error page. 

For this purpose is perhaps the reason because url redirector programs
exist?. I'm trying to see the entire puzzle :) 

Any help would be very appreciatted :) 

Thank you so much,

---

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [1] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

El 2019-03-02 23:21, Alex Rousskov escribi?:

> On 3/1/19 5:59 AM, Egoitz Aurrekoetxea wrote:
> 
>> Is it possible for Squid to do something like :
> 
>> - Receive request : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
> 
>> and
> 
>> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
> 
> How does your Squid receive the former request? Amos' answer probably
> assumes that your Squid is _not_ oooeeee.eeee.ttt.thesquidserver.org,
> but the name you have chosen for your example may imply that it is.
> 
> * If your Squid is _intercepting_ traffic destined for the real
> oooeeee.eeee.ttt.thesquidserver.org, then see Amos' answer.
> 
> * If your Squid is representing oooeeee.eeee.ttt.thesquidserver.org,
> then your Squid is a reverse proxy that ought to have the certificate
> key for that domain, and none of the SslBump problems that Amos
> mentioned apply.
> 
> Please clarify what your use case is.
> 
> Alex.
> 
>> I mean not to redirect users with url redirection. Just act as a proxy
>> but having Squid the proper acknoledge internally for being able to make
>> the proper request to the destination?. Is it possible without
>> redirecting url, to return for instance a 403 error to the source web
>> browser in order to not be able to access to the site if some kind of
>> circumstances are given?.
>> 
>> If the last config, was not possible... perhaps I needed to just to
>> redirect forcibly?. I have read for that purpose you can use URL
>> redirectors.... so I assume the concept is :
>> 
>> - Receive request : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
>> 
>> and
>> 
>> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
>> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
>> 
>> If all conditions for allowing to see the content are OK, return the web
>> browser a 301 redirect answer with the
>> https://oooeeee.eeee.ttt/u?ii=99&j=88
>> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> URL. Else,
>> just return a 403 or redirect you to a Forbidden page... I think this
>> could be implemented with URL redirectors...but... the fact is... which
>> kind of conditions or env situations can you use for validating the
>> content inside the url redirector?.
>> 
>> Thanks a lot for your time :)
>> 
>> Cheers!
>> 
>> -- 
>> sarenet
>> *Egoitz Aurrekoetxea*
>> Dpto. de sistemas
>> 944 209 470
>> Parque Tecnol?gico. Edificio 103
>> 48170 Zamudio (Bizkaia)
>> egoitz at sarenet.es <mailto:egoitz at sarenet.es>
>> www.sarenet.es [1] <http://www.sarenet.es>
>> 
>> Antes de imprimir este correo electr?nico piense si es necesario hacerlo.
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 

Links:
------
[1] http://www.sarenet.es
[2] https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&amp;j=88
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/b64f94bb/attachment.htm>

From georgexsh at gmail.com  Mon Mar  4 08:45:14 2019
From: georgexsh at gmail.com (George Xie)
Date: Mon, 4 Mar 2019 16:45:14 +0800
Subject: [squid-users] squid in container aborted on low memory server
Message-ID: <CAHsiY=d4eNA81EJ0NBctKTCGV0vUhz=j5hacFkbkGXCshLx1Jw@mail.gmail.com>

>
> > On 4/03/19 5:39 pm, George Xie wrote:
> > > hi all:
> > >
> > > Squid version: 3.5.23-5+deb9u1
> > > Docker version 18.09.3, build 774a1f4
> > > Linux instance-4 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27)
> > > x86_64 GNU/Linux
> > >
> > > I have the following squid config:
> > >
> > >
> > >     http_port 127.0.0.1:3128
> > >     cache deny all
> > >     access_log none
> > >
> > What is it exactly that you think this is doing in regards to Squid
> > memory needs?
> >
>

sorry, I don't get your quest.


> > >
> > > runs in a container with following Dockerfile:
> > >
> > >     FROM debian:9
> > >     RUN apt update && \
> > >     apt install --yes squid
> > >
> > >
> > > the total memory of the host server is very low, only 592m, about 370m
> > > free memory.
> > > if I start squid in the container, squid will abort immediately.
> > >
> > > error messages in /var/log/squid/cache.log:
> > >
> > >
> > >     FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!
> > >
> > >     Squid Cache (Version 3.5.23): Terminated abnormally.
> > >     CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
> > >     Maximum Resident Size: 47168 KB
> > >
> > >
> > > error message captured with strace -f -e trace=memory:
> > >
> > >     [pid   920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
> > >     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate
> memory)
> > >
> > >
> > > it appears that squid (or glibc) tries to allocate 392m memory, which
> is
> > > larger than host free memory 370m.
> > > but I guess squid don't need that much memory, I have another running
> > > squid instance, which only uses < 200m memory.
> > No doubt it is configured to use less memory. For example by reducing
> > the default memory cache size.
> >
>

that running squid instance has the same config.


> > > the oddest thing is if I run squid on the host (also Debian 9)
> directly,
> > > not in the container, squid could start and run as normal.
> > >
> > Linux typically allows RAM over-allocation. Which works okay so long as
> > there is sufficient swap space and there is time between memory usage to
> > do the swap in/out process.
> > Amos
>

swap is disabled in the host server, so do in the container.

after all, I wonder why squid would try to claim 392m memory if don't need
that much.

XieShi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/ee0ebb7a/attachment.htm>

From flashdown at data-core.org  Mon Mar  4 09:42:50 2019
From: flashdown at data-core.org (Enrico Heine)
Date: Mon, 04 Mar 2019 10:42:50 +0100
Subject: [squid-users] Squid-3.5.28 slowdown
In-Reply-To: <996B1404-3E7D-4070-A90C-AFC0999AFAC0@hendrie.id.au>
References: <7C9A9A04-3257-4C1A-9197-55B73D74085F@hendrie.id.au>
 <E2D7AB5B-0C8F-4E91-B43E-F5FCDDB2CD1E@data-core.org>
 <996B1404-3E7D-4070-A90C-AFC0999AFAC0@hendrie.id.au>
Message-ID: <17F6D645-13A8-4B9A-8B83-2CDEF2CF7842@data-core.org>

Hm, I do at least "believe" that it is very likely to be the same with ecap, but I don't know this protocol in anyway, so I can't give a qualified answer on that. 

Anyway, if it is your issue then you can use the test command provided anytime and see the issue slowly emerging until it reaches an amount where tcp_mem is getting to big and a net rate limit is triggered by the kernel which then finally results in slow networking performance, which can be only resolved with a squid restart  . Also check /var/log/kern.log for the point in time where you had the slowness issue, you should see some lines there which you can provide here.

Anyway, if it is your bug, please share this info with us.

Br, Flashdown

Am 1. M?rz 2019 23:43:49 MEZ schrieb Michael Hendrie <michael at hendrie.id.au>:
>
>> On 1 Mar 2019, at 9:34 pm, Enrico Heine <flashdown at data-core.org>
>wrote:
>> 
>> >>just a shot into the dark<<, is it possible that you use the
>adaption service for ICAP?
>
>There is an eCAP adaptation service but not ICAP, would eCAP be
>effected by the same condition reported the bug report you linked to?  
>Early in the investigation I did change 'ecap enable off' and do 'squid
>-k reconfigure' while the condition was present but it didn't restore
>speed, a full squid restart was required.
>
>> If so, fast test, this should return 0 if u are not affected by this,
>if higher than 0 check the link below:
>> netstat -pa | grep CLOSE_WAIT | wc -l 
>> 
>> also have a look into /var/log/kern.log 
>
>I will check these out next time the condition occurs
>
>Thanks,
>
>Michael

-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/53477315/attachment.htm>

From rousskov at measurement-factory.com  Mon Mar  4 16:23:49 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Mar 2019 09:23:49 -0700
Subject: [squid-users] Squid and url modifying
In-Reply-To: <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
Message-ID: <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>

On 3/4/19 12:53 AM, Egoitz Aurrekoetxea wrote:

> My idea is simple. I wanted specific url, to be filtered through the
> proxy. How can I manage this URL to be checked by the proxy?.

To answer your questions correctly, we need to translate the vague
description above into one of the many Squid configurations that may
match that vague description. In hope to do that, I am asking these two
basic questions:

1. Do clients/browsers request
https://oooeeee.eeee.ttt.thesquidserver.org/... URLs? Or do they request
https://oooeeee.eeee.ttt/... URLs?

For the purpose of the next question, lets assume that the answer to the
above question is: "Clients request https://publicDomain/... URLs"
(where "publicDomain" is one of the two domains mentioned in that
question). Let's further assume that when clients do a DNS lookup for
publicDomain they get a publicIp IP address back.

2. Does your Squid listen on port 443 of publicIp?

Alex.



> I assumed,
> I could modify the real and original content where urls appeared by
> setting for instance :
> 
> 
> - Being the real url : https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
> 
> - I would rewrite in the own content the URL so that? the new URL is now
> : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
> 
> The domain thesquidserver.org
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> will be used
> for doing wilcards. For instance : *.thesquidserver.org
> *.*.thesquidserver.org etc... will resolve to the ip of the Squid
> server. But I don't want any url being asked as
> whatever.thesquidserver.org to be checked... just those ones I have
> wrote in some place...
> 
> 
> So I was trying to write some content managing script, which should
> check if that URL is needed to be checked and in case it should, check
> it against an icap service. If Icap service gives you all to be right,
> redirect you to the real site (just removing the thesquidserver.org for
> the URL for instance). If that URL contains malware for instance, give
> you an error page.
> 
> 
> This is all what I was trying to do... Some time ago, I used Squid with
> Dansguardian for this kind of purposes, but now I wanted to do something
> slightly different. I wanted to pass a request (if should be passed) to
> an icap service and later depeding in the result of that ICAP service
> (which I don't really know how could I check with an script) redirect to
> the real site or give an error page.
> 
> 
> For this purpose is perhaps the reason because url redirector programs
> exist?. I'm trying to see the entire puzzle :)


> El 2019-03-02 23:21, Alex Rousskov escribi?:
> 
>> On 3/1/19 5:59 AM, Egoitz Aurrekoetxea wrote:
>>
>>> Is it possible for Squid to do something like :
>>
>>> - Receive request :
>>> https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
>>
>>> and
>>
>>> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
>>
>> How does your Squid receive the former request? Amos' answer probably
>> assumes that your Squid is _not_ oooeeee.eeee.ttt.thesquidserver.org,
>> but the name you have chosen for your example may imply that it is.
>>
>> * If your Squid is _intercepting_ traffic destined for the real
>> oooeeee.eeee.ttt.thesquidserver.org, then see Amos' answer.
>>
>> * If your Squid is representing oooeeee.eeee.ttt.thesquidserver.org,
>> then your Squid is a reverse proxy that ought to have the certificate
>> key for that domain, and none of the SslBump problems that Amos
>> mentioned apply.
>>
>> Please clarify what your use case is.
>>
>> Alex.
>>
>>
>>
>>> I mean not to redirect users with url redirection. Just act as a proxy
>>> but having Squid the proper acknoledge internally for being able to make
>>> the proper request to the destination?. Is it possible without
>>> redirecting url, to return for instance a 403 error to the source web
>>> browser in order to not be able to access to the site if some kind of
>>> circumstances are given?.
>>>
>>>
>>> If the last config, was not possible... perhaps I needed to just to
>>> redirect forcibly?. I have read for that purpose you can use URL
>>> redirectors.... so I assume the concept is :
>>>
>>>
>>> - Receive request :
>>> https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
>>>
>>>
>>> and
>>>
>>>
>>> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
>>> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
>>>
>>>
>>> If all conditions for allowing to see the content are OK, return the web
>>> browser a 301 redirect answer with the
>>> https://oooeeee.eeee.ttt/u?ii=99&j=88
>>> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> URL. Else,
>>> just return a 403 or redirect you to a Forbidden page... I think this
>>> could be implemented with URL redirectors...but... the fact is... which
>>> kind of conditions or env situations can you use for validating the
>>> content inside the url redirector?.
>>>
>>>
>>>
>>> Thanks a lot for your time :)
>>>
>>>
>>> Cheers!
>>>
>>>
>>>
>>>
>>> --?
>>> sarenet
>>> *Egoitz Aurrekoetxea*
>>> Dpto. de sistemas
>>> 944 209 470
>>> Parque Tecnol?gico. Edificio 103
>>> 48170 Zamudio (Bizkaia)
>>> egoitz at sarenet.es <mailto:egoitz at sarenet.es>
>>> <mailto:egoitz at sarenet.es <mailto:egoitz at sarenet.es>>
>>> www.sarenet.es <http://www.sarenet.es> <http://www.sarenet.es>
>>>
>>> Antes de imprimir este correo electr?nico piense si es necesario hacerlo.
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> <mailto:squid-users at lists.squid-cache.org>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Mon Mar  4 16:34:49 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Mar 2019 09:34:49 -0700
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
References: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
Message-ID: <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>

On 3/3/19 9:39 PM, George Xie wrote:

> Squid version: 3.5.23-5+deb9u1

>     http_port 127.0.0.1:3128
>     cache deny all
>     access_log none

Unfortunately, this configuration wastes RAM: Squid is not yet smart
enough to understand that you do not want any caching and may allocate
256+ MB of memory cache plus supporting indexes. To correct that default
behavior, add this:

      cache_mem 0

Furthermore, older Squids, possibly including your no-longer-supported
version, may allocate shared memory indexes where none are needed. That
might explain why you see your Squid allocating a 392 MB table.

If you want to know what is going on for sure, then configure malloc to
dump core on allocation failures and post a stack trace leading to that
allocation failure so that we know _what_ Squid was trying to allocate
when it ran out of RAM.


HTH,

Alex.


> runs in a container with following Dockerfile:
> 
>     FROM debian:9
>     RUN apt update && \
>     apt install --yes squid
> 
> 
> the total memory of the host?server is very low, only 592m, about 370m
> free memory.
> if I start squid in the container, squid will abort immediately.?
> 
> error messages in /var/log/squid/cache.log:
> 
> 
>     FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!
> 
>     Squid Cache (Version 3.5.23): Terminated abnormally.
>     CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
>     Maximum Resident Size: 47168 KB
> 
> 
> error message captured with?strace -f -e trace=memory:
> 
>     [pid? ?920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
>     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
> 
> 
> it appears that squid (or glibc) tries to allocate 392m memory, which is
> larger than host free memory?370m.
> but I guess squid don't need that much memory, I have another running
> squid instance, which only uses < 200m memory.
> the oddest thing is if I run squid on the host (also Debian?9) directly,
> not in the container, squid could start and run as normal.
> 
> am I doing something wrong thing here?
> 
> Xie Shi
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From egoitz at sarenet.es  Mon Mar  4 17:20:28 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Mon, 04 Mar 2019 18:20:28 +0100
Subject: [squid-users] Issues setting up a proxy for malware scanning
Message-ID: <e34b942f1befaa0952d29f01459c0c25@sarenet.es>

Hi mates! 

I was trying to setup a Squid server for the following matter. I wanted
to have some modified url pointing to my Squid proxy, so that Squid to
be able to connect to destination, scan the content and if all is ok,
return a 3xx to the real URL. For that purpose I use the following
configuration https://pastebin.com/raw/mP73fame . The url redirector in
that config is  https://pastebin.com/p6Usmq75 

I'm facing the two following problems, probably due to not having a
large experience in Squid : 

- I needed the Sophos ICAP service to scan content and see there's no
malware there, before returning a 30X redirect to the real url. 

- https content is not being redirected... I get the following error : 

curl -vv
https://2016.eicar.org.cloud-protection.sarenet.es/download/eicarcom2.zip
*   Trying 172.16.8.41...
* TCP_NODELAY set
* Connected to 2016.eicar.org.cloud-protection.sarenet.es (172.16.8.41)
port 443 (#0)
* ALPN, offering http/1.1
* Cipher selection:
ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
* Closing connection 0
curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
protocol 

Could anyone give us a clue for fixing this two issues?. Is it a
possible configuration?. 

Best regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/4bac0924/attachment.htm>

From sihirlinet at hotmail.com  Mon Mar  4 17:50:02 2019
From: sihirlinet at hotmail.com (ronin1907)
Date: Mon, 4 Mar 2019 11:50:02 -0600 (CST)
Subject: [squid-users] Squid fallback
Message-ID: <1551721802471-0.post@n4.nabble.com>

Hello,

I m installating squid its working fine and when I want to check from
http://ipv6-test.com/ fallback is running fine. My question is this;
How can I close this option ?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From egoitz at sarenet.es  Mon Mar  4 18:20:25 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Mon, 04 Mar 2019 19:20:25 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
Message-ID: <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>

Hi Alex, 

I'm so sorry... have tried explaining the best I could... sorry.... 

Clients, will ask : 

https://oooeeee.eeee.ttt.thesquidserver.org/ 

but redirector if site is virus free (checked with an icap daemon)
should return a 302 to https://oooeeee.eeee.ttt/ [2] 

For the second question, I say I have DNAT rules, for being able to
redirect tcp/80 and tcp/443 to squid's port silently. So the answer I
assume should be yes. 

I'll try to say again, what I'm trying to do. 

I wanted to setup a proxy machine which I wanted to be able to receive
url like : 

- www.iou.net.theproxy.com/hj.php?ui=9 [3] 

If this site returns clean content (scanned by Icap server) the url
redirector should return : 

- www.iou.net/hj.php?ui=9 [4] (the real url) as URL. 

I'm using this config https://pastebin.com/raw/mP73fame and this
redirector code https://pastebin.com/p6Usmq75 

So I would say my questions are : 

- Is it possible with Squid to achieve my goal?. With Squid, a
redirector, and a Icap daemon which performs virus scanning... 

- For plain http the config and the URL seem to be working BUT the virus
are not being scanned. Could the config be adjusted for that?. 

Cheers! 

---

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [1] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

El 2019-03-04 17:23, Alex Rousskov escribi?:

> On 3/4/19 12:53 AM, Egoitz Aurrekoetxea wrote:
> 
>> My idea is simple. I wanted specific url, to be filtered through the
>> proxy. How can I manage this URL to be checked by the proxy?.
> 
> To answer your questions correctly, we need to translate the vague
> description above into one of the many Squid configurations that may
> match that vague description. In hope to do that, I am asking these two
> basic questions:
> 
> 1. Do clients/browsers request
> https://oooeeee.eeee.ttt.thesquidserver.org/... URLs? Or do they request
> https://oooeeee.eeee.ttt/... URLs?
> 
> For the purpose of the next question, lets assume that the answer to the
> above question is: "Clients request https://publicDomain/... URLs"
> (where "publicDomain" is one of the two domains mentioned in that
> question). Let's further assume that when clients do a DNS lookup for
> publicDomain they get a publicIp IP address back.
> 
> 2. Does your Squid listen on port 443 of publicIp?
> 
> Alex.
> 
>> I assumed,
>> I could modify the real and original content where urls appeared by
>> setting for instance :
>> 
>> - Being the real url : https://oooeeee.eeee.ttt/u?ii=99&j=88
>> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
>> 
>> - I would rewrite in the own content the URL so that  the new URL is now
>> : https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
>> 
>> The domain thesquidserver.org
>> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> will be used
>> for doing wilcards. For instance : *.thesquidserver.org
>> *.*.thesquidserver.org etc... will resolve to the ip of the Squid
>> server. But I don't want any url being asked as
>> whatever.thesquidserver.org to be checked... just those ones I have
>> wrote in some place...
>> 
>> So I was trying to write some content managing script, which should
>> check if that URL is needed to be checked and in case it should, check
>> it against an icap service. If Icap service gives you all to be right,
>> redirect you to the real site (just removing the thesquidserver.org for
>> the URL for instance). If that URL contains malware for instance, give
>> you an error page.
>> 
>> This is all what I was trying to do... Some time ago, I used Squid with
>> Dansguardian for this kind of purposes, but now I wanted to do something
>> slightly different. I wanted to pass a request (if should be passed) to
>> an icap service and later depeding in the result of that ICAP service
>> (which I don't really know how could I check with an script) redirect to
>> the real site or give an error page.
>> 
>> For this purpose is perhaps the reason because url redirector programs
>> exist?. I'm trying to see the entire puzzle :)
> 
> El 2019-03-02 23:21, Alex Rousskov escribi?:
> 
> On 3/1/19 5:59 AM, Egoitz Aurrekoetxea wrote:
> 
> Is it possible for Squid to do something like : 
> - Receive request :
> https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88 
> and 
> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88 
> How does your Squid receive the former request? Amos' answer probably
> assumes that your Squid is _not_ oooeeee.eeee.ttt.thesquidserver.org,
> but the name you have chosen for your example may imply that it is.
> 
> * If your Squid is _intercepting_ traffic destined for the real
> oooeeee.eeee.ttt.thesquidserver.org, then see Amos' answer.
> 
> * If your Squid is representing oooeeee.eeee.ttt.thesquidserver.org,
> then your Squid is a reverse proxy that ought to have the certificate
> key for that domain, and none of the SslBump problems that Amos
> mentioned apply.
> 
> Please clarify what your use case is.
> 
> Alex.
> 
> I mean not to redirect users with url redirection. Just act as a proxy
> but having Squid the proper acknoledge internally for being able to make
> the proper request to the destination?. Is it possible without
> redirecting url, to return for instance a 403 error to the source web
> browser in order to not be able to access to the site if some kind of
> circumstances are given?.
> 
> If the last config, was not possible... perhaps I needed to just to
> redirect forcibly?. I have read for that purpose you can use URL
> redirectors.... so I assume the concept is :
> 
> - Receive request :
> https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88
> 
> and
> 
> to really perform a request as : https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88>
> 
> If all conditions for allowing to see the content are OK, return the web
> browser a 301 redirect answer with the
> https://oooeeee.eeee.ttt/u?ii=99&j=88
> <https://oooeeee.eeee.ttt.thesquidserver.org/u?ii=99&j=88> URL. Else,
> just return a 403 or redirect you to a Forbidden page... I think this
> could be implemented with URL redirectors...but... the fact is... which
> kind of conditions or env situations can you use for validating the
> content inside the url redirector?.
> 
> Thanks a lot for your time :)
> 
> Cheers!
> 
> -- 
> sarenet
> *Egoitz Aurrekoetxea*
> Dpto. de sistemas
> 944 209 470
> Parque Tecnol?gico. Edificio 103
> 48170 Zamudio (Bizkaia)
> egoitz at sarenet.es <mailto:egoitz at sarenet.es>
> <mailto:egoitz at sarenet.es <mailto:egoitz at sarenet.es>>
> www.sarenet.es [1] <http://www.sarenet.es> <http://www.sarenet.es>
> 
> Antes de imprimir este correo electr?nico piense si es necesario hacerlo.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 

Links:
------
[1] http://www.sarenet.es
[2] https://oooeeee.eeee.ttt.thesquidserver.org/
[3] http://www.iou.net.theproxy.com/hj.php?ui=9
[4] http://www.iou.net/hj.php?ui=9
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/f1a1387e/attachment.htm>

From uhlar at fantomas.sk  Mon Mar  4 18:25:38 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 4 Mar 2019 19:25:38 +0100
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
References: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
 <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
Message-ID: <20190304182538.GA14525@fantomas.sk>

>On 3/3/19 9:39 PM, George Xie wrote:
>> Squid version: 3.5.23-5+deb9u1

debian 9, currently stable, soon to be replaced by debian 10, containing
squid-4.4

>>     http_port 127.0.0.1:3128
>>     cache deny all
>>     access_log none

On 04.03.19 09:34, Alex Rousskov wrote:
>Unfortunately, this configuration wastes RAM: Squid is not yet smart
>enough to understand that you do not want any caching and may allocate
>256+ MB of memory cache plus supporting indexes. To correct that default
>behavior, add this:
>
>      cache_mem 0

this should help most.

>Furthermore, older Squids, possibly including your no-longer-supported
>version

its supported, just not by squid developers. There are many SW distributions
that try to support software for longer than just a few weeks/months, e.g
during whole few-year release cycle.

>might explain why you see your Squid allocating a 392 MB table.
>
>If you want to know what is going on for sure, then configure malloc to
>dump core on allocation failures and post a stack trace leading to that
>allocation failure so that we know _what_ Squid was trying to allocate
>when it ran out of RAM.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese. 


From felipeapolanco at gmail.com  Mon Mar  4 22:27:56 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Mon, 4 Mar 2019 18:27:56 -0400
Subject: [squid-users] Need help blocking an specific HTTPS website
Message-ID: <CADcj3=5VZf8EOCwpAu7BHp7BY_HfCAYus2MpepSGRD-=P6wSyA@mail.gmail.com>

Hi,

I have been trying to block https://web.whatsapp.com/ from squid and I have
been unable to.

So far I have this:

I can block other HTTPS websites fine
I can block www.whatsapp.com fine
I cannot block web.whatsapp.com

I have HTTPS transparent interception enabled and I am bumping all TCP
connections, but still this one doesn't appear to get blocked by squid.

This is part of my configuration:
===========================
acl blockwa1 url_regex whatsapp\.com$
acl blockwa2 dstdomain .whatsapp.com
acl blockwa3 ssl::server_name .whatsapp.com
acl step1 at_step SslBump1

http_access deny blockwa1
http_access deny blockwa2
http_access deny blockwa3

ssl_bump peek step1
ssl_bump bump all
============================

Can anyone advise me here?

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/1f27817b/attachment.htm>

From commercials24 at yahoo.de  Mon Mar  4 23:10:46 2019
From: commercials24 at yahoo.de (steven)
Date: Tue, 5 Mar 2019 00:10:46 +0100
Subject: [squid-users] icap not answering
In-Reply-To: <f132eb9f-e448-ebef-e1dc-b27359a23d6c@urlfilterdb.com>
References: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>
 <f132eb9f-e448-ebef-e1dc-b27359a23d6c@urlfilterdb.com>
Message-ID: <12476d4e-9f68-959b-dce0-647cfb9b1704@yahoo.de>

Ah thank you for that clarification, the python icap servers i tested so 
far are not very promissing but at least theres a connection now.

sadly squid does not allow http access at all, only https access.



access.log


1551740163.106????? 0 192.168.10.116 TCP_MISS/500 4776 GET 
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-to-listen-to-HTTPS-td4682393.html 
- HIER_NONE/- text/html
1551740163.173????? 0 192.168.10.116 TCP_IMS_HIT/304 294 GET 
http://backup:3128/squid-internal-static/icons/SN.png - HIER_NONE/- 
image/png

backup is the host where squid is running on


the webpage shown in the browser says: *Unable to forward this request 
at this time.*


cache.log

2019/03/05 00:08:30.319 kid1| 28,4| Eui48.cc(179) lookup: 
id=0x5559d1923114 query ARP table
2019/03/05 00:08:30.319 kid1| 28,4| Eui48.cc(224) lookup: 
id=0x5559d1923114 query ARP on each interface (160 found)
2019/03/05 00:08:30.319 kid1| 28,4| Eui48.cc(230) lookup: 
id=0x5559d1923114 found interface lo
2019/03/05 00:08:30.319 kid1| 28,4| Eui48.cc(230) lookup: 
id=0x5559d1923114 found interface eth0
2019/03/05 00:08:30.319 kid1| 28,4| Eui48.cc(239) lookup: 
id=0x5559d1923114 looking up ARP address for 192.168.10.116 on eth0
2019/03/05 00:08:30.319 kid1| 28,4| Eui48.cc(275) lookup: 
id=0x5559d1923114 got address a4:34:d9:ea:b3:34 on eth0
2019/03/05 00:08:30.319 kid1| 28,3| Checklist.cc(70) preCheck: 
0x5559d14e2f78 checking slow rules
2019/03/05 00:08:30.319 kid1| 28,5| Acl.cc(124) matches: checking 
(ssl_bump rules)
2019/03/05 00:08:30.320 kid1| 28,5| Checklist.cc(397) bannedAction: 
Action 'ALLOWED/3' is not banned
2019/03/05 00:08:30.320 kid1| 28,5| Acl.cc(124) matches: checking 
(ssl_bump rule)
2019/03/05 00:08:30.320 kid1| 28,5| Acl.cc(124) matches: checking step1
2019/03/05 00:08:30.320 kid1| 28,3| Acl.cc(151) matches: checked: step1 = 1
2019/03/05 00:08:30.320 kid1| 28,3| Acl.cc(151) matches: checked: 
(ssl_bump rule) = 1
2019/03/05 00:08:30.320 kid1| 28,3| Acl.cc(151) matches: checked: 
(ssl_bump rules) = 1
2019/03/05 00:08:30.320 kid1| 28,3| Checklist.cc(63) markFinished: 
0x5559d14e2f78 answer ALLOWED for match
2019/03/05 00:08:30.320 kid1| 28,3| Checklist.cc(163) checkCallback: 
ACLChecklist::checkCallback: 0x5559d14e2f78 answer=ALLOWED
2019/03/05 00:08:30.320 kid1| 28,3| Checklist.cc(70) preCheck: 
0x5559d19279a8 checking slow rules
2019/03/05 00:08:30.320 kid1| 28,5| Acl.cc(124) matches: checking 
http_access
2019/03/05 00:08:30.320 kid1| 28,5| Checklist.cc(397) bannedAction: 
Action 'ALLOWED/0' is not banned
2019/03/05 00:08:30.320 kid1| 28,5| Acl.cc(124) matches: checking 
http_access#1
2019/03/05 00:08:30.320 kid1| 28,5| Acl.cc(124) matches: checking localnet
2019/03/05 00:08:30.320 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare: 
aclIpAddrNetworkCompare: compare: 
192.168.10.116:45900/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ff00] 
(192.168.10.0:45900)? vs 
192.168.10.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ff00]
2019/03/05 00:08:30.320 kid1| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'192.168.10.116:45900' found
2019/03/05 00:08:30.320 kid1| 28,3| Acl.cc(151) matches: checked: 
localnet = 1
2019/03/05 00:08:30.320 kid1| 28,3| Acl.cc(151) matches: checked: 
http_access#1 = 1
2019/03/05 00:08:30.320 kid1| 28,3| Acl.cc(151) matches: checked: 
http_access = 1
2019/03/05 00:08:30.320 kid1| 28,3| Checklist.cc(63) markFinished: 
0x5559d19279a8 answer ALLOWED for match
2019/03/05 00:08:30.320 kid1| 28,3| Checklist.cc(163) checkCallback: 
ACLChecklist::checkCallback: 0x5559d19279a8 answer=ALLOWED
2019/03/05 00:08:30.320 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff85d5a130
2019/03/05 00:08:30.320 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7fff85d5a130
2019/03/05 00:08:30.320 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff85d5a130
2019/03/05 00:08:30.320 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7fff85d5a130
2019/03/05 00:08:30.320 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x5559d19279a8
2019/03/05 00:08:30.320 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x5559d19279a8
2019/03/05 00:08:30.320 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x5559d14e2f78
2019/03/05 00:08:30.320 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x5559d14e2f78




current squid config:

#icap
icap_enable off
icap_preview_enable off
icap_send_client_ip on
icap_send_client_username on
icap_service service_req reqmod_precache bypass=1 
icap://127.0.0.1:1344/request
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0 
icap://127.0.0.1:1344/response
adaptation_access service_resp allow all
acl localnet src 192.168.10.0/24
acl CONNECT method CONNECT
http_access allow localnet
coredump_dir /var/spool/squid
refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
refresh_pattern ^gopher:??? 1440??? 0%??? 1440
refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
refresh_pattern .??? ??? 0??? 20%??? 4320
http_port 3128 accel ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
https_port 3129 ssl-bump intercept generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db 
-M 4MB
acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all

forwarded_for transparent


any ideas whats wrong?



On 03.03.19 11:11, Marcus Kool wrote:
> Squid is an ICAP client, not an ICAP server!, and does not repond on 
> port 1344.
> Marcus
>
>
> On 02/03/2019 22:29, steven wrote:
>> Hi,
>>
>>
>> i would like todo modifications on https connections and therefore 
>> enabled ssl bump in squid 4.4, now i would like to see the real 
>> traffic and icap looks like a way to watch and change that traffic.
>>
>> but squid is not answering to icap://127.0.0.1:1344 when using pyicap 
>> or telnet.
>>
>> the telnet error is:
>>
>> telnet 127.0.0.1 1344
>> Trying 127.0.0.1...
>> telnet: Unable to connect to remote host: Connection refused
>>
>> which is imho good because it tells me that something is answering on 
>> that port after all.
>>
>> did i misconfigure something?
>>
>>
>>
>> config:
>>
>> debug_options 28,9
>> #icap
>> icap_enable on
>> icap_service service_req reqmod_precache bypass=1 
>> icap://127.0.0.1:1344/reqmod
>> adaptation_access service_req allow all
>> icap_service service_resp respmod_precache bypass=0 
>> icap://127.0.0.1:1344/respmod
>> adaptation_access service_resp allow all
>> acl localnet src 127.0.0.1/32 192.168.10.0/24
>> http_access allow localnet
>> acl SSL_ports port 443
>> acl CONNECT method CONNECT
>> #http_access deny !Safe_ports
>> #http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>> include /etc/squid/conf.d/*
>> http_access allow localhost
>> coredump_dir /var/spool/squid
>> refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
>> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
>> refresh_pattern .??? ??? 0??? 20%??? 4320
>> # default end
>> # my config
>> http_port 3128 accel ssl-bump generate-host-certificates=on 
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
>> https_port 3129 ssl-bump intercept generate-host-certificates=on 
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
>> sslcrtd_program /usr/lib/squid/security_file_certgen -s 
>> /var/lib/ssl_db -M 4MB
>> acl step1 at_step SslBump1
>>
>> ssl_bump peek step1
>> ssl_bump bump all
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190305/1c066a23/attachment.htm>

From leolistas at solutti.com.br  Mon Mar  4 23:21:38 2019
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 4 Mar 2019 20:21:38 -0300
Subject: [squid-users] Need help blocking an specific HTTPS website
In-Reply-To: <CADcj3=5VZf8EOCwpAu7BHp7BY_HfCAYus2MpepSGRD-=P6wSyA@mail.gmail.com>
References: <CADcj3=5VZf8EOCwpAu7BHp7BY_HfCAYus2MpepSGRD-=P6wSyA@mail.gmail.com>
Message-ID: <8cc474f3-34df-e056-c634-e2b048d41314@solutti.com.br>

Em 04/03/2019 19:27, Felipe Arturo Polanco escreveu:
> Hi,
>
> I have been trying to block https://web.whatsapp.com/ from squid and I 
> have been unable to.
>
> So far I have this:
>
> I can block other HTTPS websites fine
> I can block www.whatsapp.com <http://www.whatsapp.com> fine
> I cannot block web.whatsapp.com <http://web.whatsapp.com>
>
> I have HTTPS transparent interception enabled and I am bumping all TCP 
> connections, but still this one doesn't appear to get blocked by squid.
>
> This is part of my configuration:
> ===========================
> acl blockwa1 url_regex whatsapp\.com$
> acl blockwa2 dstdomain .whatsapp.com <http://whatsapp.com>
> acl blockwa3 ssl::server_name .whatsapp.com <http://whatsapp.com>
> acl step1 at_step SslBump1
>

 ??? blockwa1 and blockwa2 should definitely block web.whatsapp.com .. 
your rules seems right.

 ??? Can you confirm the web.whatsapp.com access are getting through 
squid ? Are these accesses on your access.log with something different 
than DENIED status ?



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190304/c6bd28c2/attachment.htm>

From vineshthakur at gmail.com  Tue Mar  5 03:24:19 2019
From: vineshthakur at gmail.com (chia123)
Date: Mon, 4 Mar 2019 21:24:19 -0600 (CST)
Subject: [squid-users] SSL Accel Connection Reset
In-Reply-To: <20171121103653.GA27366@mail.ephemeric.online>
References: <20171120132445.GC26545@mail.ephemeric.online>
 <d9d30f5c-59b7-7462-a47b-3e310f0f8ef5@treenet.co.nz>
 <20171121103653.GA27366@mail.ephemeric.online>
Message-ID: <1551756259743-0.post@n4.nabble.com>

Hi Robert,
How did you resolve this issue?
>From what I read curl doesn't support https proxy till version 7.52.0
I'm running into similar problem where my machine is sending plaintext
CONNECT to the https proxy instead of starting a TLS handshake.
I'm using python urlib2 but I also used curl pre 7.52.0 version and it has
the same issue.
Any help will be greatly appreciated.

Thanks



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From georgexsh at gmail.com  Tue Mar  5 04:22:22 2019
From: georgexsh at gmail.com (George Xie)
Date: Tue, 5 Mar 2019 12:22:22 +0800
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
References: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
 <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
Message-ID: <CAHsiY=e6QOY45bc1oWccaoJ84VA7F+g5YUUnbL2H3Dw-RPyqeg@mail.gmail.com>

> To correct that default
> behavior, add this:
>   cache_mem 0

thanks for your advice, but actually, I have tried this option before,
found no difference. besides, and I have tried `memory_pools off`.

> Furthermore, older Squids, possibly including your no-longer-supported
> version, may allocate shared memory indexes where none are needed. That
> might explain why you see your Squid allocating a 392 MB table.

that's fair, I will give squid 4.4 a try later.

> If you want to know what is going on for sure, then configure malloc to
> dump core on allocation failures and post a stack trace leading to that
> allocation failure so that we know _what_ Squid was trying to allocate
> when it ran out of RAM.

hope following backtrace is helpful:

(gdb) bt
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007ffff562e42a in __GI_abort () at abort.c:89
#2  0x0000555555728eb5 in fatal_dump (
    message=0x555555e764e0 <xcalloc::msg> "xcalloc: Unable to allocate
1048576 blocks of 392 bytes!\n") at fatal.cc:113
#3  0x0000555555a09837 in xcalloc (n=1048576, sz=sz at entry=392) at xalloc.cc:90
#4  0x00005555558a3d0a in comm_init () at comm.cc:1206
#5  0x0000555555789104 in SquidMain (argc=<optimized out>, argv=0x7fffffffed48)
    at main.cc:1481
#6  0x000055555568a48b in SquidMainSafe (argv=<optimized out>,
argc=<optimized out>)
    at main.cc:1261
#7  main (argc=<optimized out>, argv=<optimized out>) at main.cc:1254


Xie Shi


On Tue, Mar 5, 2019 at 12:34 AM Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>
> On 3/3/19 9:39 PM, George Xie wrote:
>
> > Squid version: 3.5.23-5+deb9u1
>
> >     http_port 127.0.0.1:3128
> >     cache deny all
> >     access_log none
>
> Unfortunately, this configuration wastes RAM: Squid is not yet smart
> enough to understand that you do not want any caching and may allocate
> 256+ MB of memory cache plus supporting indexes. To correct that default
> behavior, add this:
>
>       cache_mem 0
>
> Furthermore, older Squids, possibly including your no-longer-supported
> version, may allocate shared memory indexes where none are needed. That
> might explain why you see your Squid allocating a 392 MB table.
>
> If you want to know what is going on for sure, then configure malloc to
> dump core on allocation failures and post a stack trace leading to that
> allocation failure so that we know _what_ Squid was trying to allocate
> when it ran out of RAM.
>
>
> HTH,
>
> Alex.
>
>
> > runs in a container with following Dockerfile:
> >
> >     FROM debian:9
> >     RUN apt update && \
> >     apt install --yes squid
> >
> >
> > the total memory of the host server is very low, only 592m, about 370m
> > free memory.
> > if I start squid in the container, squid will abort immediately.
> >
> > error messages in /var/log/squid/cache.log:
> >
> >
> >     FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!
> >
> >     Squid Cache (Version 3.5.23): Terminated abnormally.
> >     CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
> >     Maximum Resident Size: 47168 KB
> >
> >
> > error message captured with strace -f -e trace=memory:
> >
> >     [pid   920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
> >     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
> >
> >
> > it appears that squid (or glibc) tries to allocate 392m memory, which is
> > larger than host free memory 370m.
> > but I guess squid don't need that much memory, I have another running
> > squid instance, which only uses < 200m memory.
> > the oddest thing is if I run squid on the host (also Debian 9) directly,
> > not in the container, squid could start and run as normal.
> >
> > am I doing something wrong thing here?
> >
> > Xie Shi
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Mar  5 05:13:31 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Mar 2019 18:13:31 +1300
Subject: [squid-users] icap not answering
In-Reply-To: <12476d4e-9f68-959b-dce0-647cfb9b1704@yahoo.de>
References: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>
 <f132eb9f-e448-ebef-e1dc-b27359a23d6c@urlfilterdb.com>
 <12476d4e-9f68-959b-dce0-647cfb9b1704@yahoo.de>
Message-ID: <d093dc56-8064-f94e-a867-7ad6626e4570@treenet.co.nz>

On 5/03/19 12:10 pm, steven wrote:
> Ah thank you for that clarification, the python icap servers i tested so
> far are not very promissing but at least theres a connection now.
> 
> sadly squid does not allow http access at all, only https access.
> 

Er, that would be because the only http_port you have is configured with
'accl' - making it a reverse-proxy port. But you do not have any
cache_peer configured to handle that type of traffic.


So, is there any particular reason you have that port receiving 'accel'
/ reverse-proxy mode traffic?
 If not remove that mode flag and things should all work for HTTP too.


> 
> access.log
> 
> 
> 1551740163.106????? 0 192.168.10.116 TCP_MISS/500 4776 GET
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-to-listen-to-HTTPS-td4682393.html
> - HIER_NONE/- text/html

> 1551740163.173????? 0 192.168.10.116 TCP_IMS_HIT/304 294 GET
> http://backup:3128/squid-internal-static/icons/SN.png - HIER_NONE/-
> image/png
> 

That is part of the 500 error page being delivered.

Since you are running a reverse-proxy, the Squid visible host name
really should be a FQDN so visitors can resolve the URLs of content
provided by Squid.


> backup is the host where squid is running on
> 
> 
> the webpage shown in the browser says: *Unable to forward this request
> at this time.*
> 
> 
> cache.log
> 

The log section provided shows only the first http_access and ssl_bump
rules deciding to allow the client to contact the proxy so it can peek
at the TLS client handshake.


> current squid config:
> 
> #icap
> icap_enable off
> icap_preview_enable off
> icap_send_client_ip on
> icap_send_client_username on
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/request
> adaptation_access service_req allow all
> icap_service service_resp respmod_precache bypass=0
> icap://127.0.0.1:1344/response
> adaptation_access service_resp allow all
> acl localnet src 192.168.10.0/24
> acl CONNECT method CONNECT

NP: the CONNECT ACL should be a built-in now. No need for the line above :-)


> http_access allow localnet
...
> http_port 3128 accel ssl-bump generate-host-certificates=on \
>     dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem



HTH
Amos


From rousskov at measurement-factory.com  Tue Mar  5 07:13:36 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Mar 2019 00:13:36 -0700
Subject: [squid-users] Squid and url modifying
In-Reply-To: <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
Message-ID: <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>

On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:

> Clients, will ask :
> 
> https://oooeeee.eeee.ttt.thesquidserver.org/

> So the answer [to the second question] I assume should be yes.

If I am interpreting your answers correctly, then your setup looks like
a reverse proxy to me. In that case, you do not need SslBump and
interception. You do need an web server certificate for the
oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
Do you already have that?


> I have DNAT rules, for being able to
> redirect tcp/80 and tcp/443 to squid's port silently.

Please note that your current Squid configuration is not a reverse proxy
configuration. It is an interception configuration. It also lacks
https_port for handling port 443 traffic. There are probably some
documents on Squid wiki (and/or elsewhere) explaining how to configure
Squid to become a reverse proxy. Follow them.


> I wanted to setup a proxy machine which I wanted to be able to receive
> url like :
> 
> - www.iou.net.theproxy.com/hj.php?ui=9
> 
> If this site returns clean content (scanned by Icap server) the url
> redirector should return :
> 
> - www.iou.net/hj.php?ui=9 <http://www.iou.net/hj.php?ui=9> (the real
> url) as URL.

OK.


> - Is it possible with Squid to achieve my goal?. With Squid, a
> redirector, and a Icap daemon which performs virus scanning...

A redirector seems out of scope here -- it works on requests while you
want to rewrite (scanned by ICAP) responses.

It is probably possible to use deny_info to respond with a redirect
message. To trigger a deny_info action, you would have to configure your
Squid to block virus-free responses, which is rather strange!


> - For plain http the config and the URL seem to be working BUT the virus
> are not being scanned. Could the config be adjusted for that?.


I would start by removing the redirector, "intercept", SslBump, and
disabling ICAP. Configure your Squid as a reverse proxy without any
virus scanning. Then add ICAP. Get the virus scanning working without
any URL manipulation. Once that is done, you can adjust Squid to block
virus-free responses (via http_reply_access) and trigger a deny_info
response containing an HTTP redirect.


Please note that once the browser gets a redirect to another site, that
browser is not going to revisit your reverse proxy for any content
related to that other site -- all requests for that other site will go
from the browser to that other site. Your proxy will not be in the loop
anymore. If that is not what you want, then you cannot use redirects at
all -- you would have to accelerate that other site for all requests
instead and make sure that other site does not contain absolute URLs
pointing the browser away from your reverse proxy.


Disclaimer: I have not tested the above ideas and, again, I may be
misinterpreting what you really want to achieve.

Alex.


From egoitz at sarenet.es  Tue Mar  5 07:30:30 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Tue, 05 Mar 2019 08:30:30 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
Message-ID: <d8e73061dfaf44a2bc7b5daee45aa839@sarenet.es>

Good morning Alex, 

Thank you so much for your time. Your interpretations I would say are
almost exact. I say almost, because I wanted to be a reverse proxy of
multiple sites. Not just for the sites you host or similar... And yes I
wanted, let's say if all is OK "block" the request by giving a 301 to
directly the site. Yes I know I won't traverse the proxy any more after
that BUT I will only go direct if content is clean. If it is not I will
receive an error response from the ICAP so all is fine then... 

I'll deeply check your comments and will tell here something :) 

Thank you so much!

---

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [3] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

El 2019-03-05 08:13, Alex Rousskov escribi?:

> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
> 
>> Clients, will ask :
>> 
>> https://oooeeee.eeee.ttt.thesquidserver.org/
> 
>> So the answer [to the second question] I assume should be yes.
> 
> If I am interpreting your answers correctly, then your setup looks like
> a reverse proxy to me. In that case, you do not need SslBump and
> interception. You do need an web server certificate for the
> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
> Do you already have that?
> 
>> I have DNAT rules, for being able to
>> redirect tcp/80 and tcp/443 to squid's port silently.
> 
> Please note that your current Squid configuration is not a reverse proxy
> configuration. It is an interception configuration. It also lacks
> https_port for handling port 443 traffic. There are probably some
> documents on Squid wiki (and/or elsewhere) explaining how to configure
> Squid to become a reverse proxy. Follow them.
> 
>> I wanted to setup a proxy machine which I wanted to be able to receive
>> url like :
>> 
>> - www.iou.net.theproxy.com/hj.php?ui=9 [1]
>> 
>> If this site returns clean content (scanned by Icap server) the url
>> redirector should return :
>> 
>> - www.iou.net/hj.php?ui=9 [2] <http://www.iou.net/hj.php?ui=9> (the real
>> url) as URL.
> 
> OK.
> 
>> - Is it possible with Squid to achieve my goal?. With Squid, a
>> redirector, and a Icap daemon which performs virus scanning...
> 
> A redirector seems out of scope here -- it works on requests while you
> want to rewrite (scanned by ICAP) responses.
> 
> It is probably possible to use deny_info to respond with a redirect
> message. To trigger a deny_info action, you would have to configure your
> Squid to block virus-free responses, which is rather strange!
> 
>> - For plain http the config and the URL seem to be working BUT the virus
>> are not being scanned. Could the config be adjusted for that?.
> 
> I would start by removing the redirector, "intercept", SslBump, and
> disabling ICAP. Configure your Squid as a reverse proxy without any
> virus scanning. Then add ICAP. Get the virus scanning working without
> any URL manipulation. Once that is done, you can adjust Squid to block
> virus-free responses (via http_reply_access) and trigger a deny_info
> response containing an HTTP redirect.
> 
> Please note that once the browser gets a redirect to another site, that
> browser is not going to revisit your reverse proxy for any content
> related to that other site -- all requests for that other site will go
> from the browser to that other site. Your proxy will not be in the loop
> anymore. If that is not what you want, then you cannot use redirects at
> all -- you would have to accelerate that other site for all requests
> instead and make sure that other site does not contain absolute URLs
> pointing the browser away from your reverse proxy.
> 
> Disclaimer: I have not tested the above ideas and, again, I may be
> misinterpreting what you really want to achieve.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 

Links:
------
[1] http://www.iou.net.theproxy.com/hj.php?ui=9
[2] http://www.iou.net/hj.php?ui=9
[3] http://www.sarenet.es
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190305/274cf4fe/attachment.htm>

From georgexsh at gmail.com  Tue Mar  5 08:11:36 2019
From: georgexsh at gmail.com (George Xie)
Date: Tue, 5 Mar 2019 16:11:36 +0800
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
References: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
 <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
Message-ID: <CAHsiY=eQ=wtpN8hgtR1qCR2ahBHzuieCiY66KeFeTX-PXZe3_w@mail.gmail.com>

more detail of the backtrace:

(gdb) up
#4  0x00005555558a3d0a in comm_init () at comm.cc:1206
1206        fd_table =(fde *) xcalloc(Squid_MaxFD, sizeof(fde));
(gdb) p Squid_MaxFD
$1 = 1048576
(gdb) p sizeof(fde)
$2 = 392

It seems Squid_MaxFD is way too large, and its value is directly from ulimit:

# ulimit -n
1048576

therefore, I try to add this option:

max_filedesc 4096

now squid works and only takes ~50m memory.
thanks very much for your help!

Xie Shi

Xie Shi

On Tue, Mar 5, 2019 at 12:34 AM Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>
> On 3/3/19 9:39 PM, George Xie wrote:
>
> > Squid version: 3.5.23-5+deb9u1
>
> >     http_port 127.0.0.1:3128
> >     cache deny all
> >     access_log none
>
> Unfortunately, this configuration wastes RAM: Squid is not yet smart
> enough to understand that you do not want any caching and may allocate
> 256+ MB of memory cache plus supporting indexes. To correct that default
> behavior, add this:
>
>       cache_mem 0
>
> Furthermore, older Squids, possibly including your no-longer-supported
> version, may allocate shared memory indexes where none are needed. That
> might explain why you see your Squid allocating a 392 MB table.
>
> If you want to know what is going on for sure, then configure malloc to
> dump core on allocation failures and post a stack trace leading to that
> allocation failure so that we know _what_ Squid was trying to allocate
> when it ran out of RAM.
>
>
> HTH,
>
> Alex.
>
>
> > runs in a container with following Dockerfile:
> >
> >     FROM debian:9
> >     RUN apt update && \
> >     apt install --yes squid
> >
> >
> > the total memory of the host server is very low, only 592m, about 370m
> > free memory.
> > if I start squid in the container, squid will abort immediately.
> >
> > error messages in /var/log/squid/cache.log:
> >
> >
> >     FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!
> >
> >     Squid Cache (Version 3.5.23): Terminated abnormally.
> >     CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
> >     Maximum Resident Size: 47168 KB
> >
> >
> > error message captured with strace -f -e trace=memory:
> >
> >     [pid   920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
> >     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
> >
> >
> > it appears that squid (or glibc) tries to allocate 392m memory, which is
> > larger than host free memory 370m.
> > but I guess squid don't need that much memory, I have another running
> > squid instance, which only uses < 200m memory.
> > the oddest thing is if I run squid on the host (also Debian 9) directly,
> > not in the container, squid could start and run as normal.
> >
> > am I doing something wrong thing here?
> >
> > Xie Shi
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Mar  5 08:13:36 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Mar 2019 21:13:36 +1300
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <CAHsiY=d4eNA81EJ0NBctKTCGV0vUhz=j5hacFkbkGXCshLx1Jw@mail.gmail.com>
References: <CAHsiY=d4eNA81EJ0NBctKTCGV0vUhz=j5hacFkbkGXCshLx1Jw@mail.gmail.com>
Message-ID: <f620fd2a-e132-76f9-ea46-e2353c9d077e@treenet.co.nz>

On 4/03/19 9:45 pm, George Xie wrote:
>     > On 4/03/19 5:39 pm, George Xie wrote:
>     > > hi all:
>     > >
>     > > Squid version: 3.5.23-5+deb9u1
>     > > Docker version 18.09.3, build 774a1f4
>     > > Linux instance-4 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27)
>     > > x86_64 GNU/Linux
>     > >
>     > > I have the following squid config:
>     > >
>     > >
>     > > ? ? http_port 127.0.0.1:3128 <http://127.0.0.1:3128>
>     > > ? ? cache deny all
>     > > ? ? access_log none
>     > >
>     > What is it exactly that you think this is doing in regards to Squid
>     > memory needs?
>     >
> 
> 
> sorry, I don't get your quest.
> ?

I was asking to see what you were thinking was going on with those settings.

As Alex already pointed out the "cache deny all" does not reduce memory
needs of Squid in any way. It just makes 256MB of that RAM become
pointless allocating.

So, if you actually do not want the proxy caching anything, then
disabling the cache_mem (set it to 0 as per Alex response) would be the
best choice of action before you go any further.

Or if you *do* want caching, and were trying to disable it for testing
the memory issue. Then your test was wrong, and produces incorrect
conclusion. Just reducing cache_mem would be best for this case - set it
to a value that should reasonably fit this container and see if the
proxy runs okay.


...
>     > >
>     > > it appears that squid (or glibc) tries to allocate 392m memory,
>     which is
>     > > larger than host free memory 370m.
>     > > but I guess squid don't need that much memory, I have another
>     running
>     > > squid instance, which only uses < 200m memory.
>     > No doubt it is configured to use less memory. For example by reducing
>     > the default memory cache size.
>     >
> 
> 
> that running squid instance has the same config.
> ?

Then something odd is going on between the two. They should indeed have
had the same behaviour (either work or same error).

Whatever the issue is it is being triggered by the large blocks of RAM
allocated by a default Squid. The easiest to modify is the cache_mem.


> 
>     > > the oddest thing is if I run squid on the host (also Debian 9)
>     directly,
>     > > not in the container, squid could start and run as normal.
>     > >
>     > Linux typically allows RAM over-allocation. Which works okay so
>     long as
>     > there is sufficient swap space and there is time between memory
>     usage to
>     > do the swap in/out process.
>     > Amos
> 
> 
> swap is disabled in the host server, so do in the container.?
> 
> after all, I wonder why squid would try to claim?392m memory if don't
> need that much.
> 

Squid thinks it does. All client traffic is denied being cached by that
"deny all". BUT ... there are internally generated items which also use
cache. So there is 256MB default RAM cache allocated and only those few
small things being put in it.

You could set it to '0' or to some small value and the allocation size
should go down accordingly.


That said, every bit of client traffic headed towards the proxy uses
memory of volatile amount and at peak times it may need to allocate
large blocks.

So disabling swap entirely on the server is not a great idea. It just
moves the error and shutdown to happen at peak traffic times when it is
least wanted.


Amos


From egoitz at sarenet.es  Tue Mar  5 08:29:43 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Tue, 05 Mar 2019 09:29:43 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
Message-ID: <71570f80d464732933dee8355882422c@sarenet.es>

Hi Alex, 

What you told about http_reply_access could work for me... but I have a
problem... 

Can http_reply_access and some for of... url_regexp dstdom_regex or
similar.... cause a redirect by using matching content?. 

I mean : 

https://a.b.c.cloud.aaa.bbb 

to be redirected to : 

https://a.b.c 

Let's say matching all but cloud.aaa.bbb ? 

Cheers!!

---

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [3] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

El 2019-03-05 08:13, Alex Rousskov escribi?:

> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
> 
>> Clients, will ask :
>> 
>> https://oooeeee.eeee.ttt.thesquidserver.org/
> 
>> So the answer [to the second question] I assume should be yes.
> 
> If I am interpreting your answers correctly, then your setup looks like
> a reverse proxy to me. In that case, you do not need SslBump and
> interception. You do need an web server certificate for the
> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
> Do you already have that?
> 
>> I have DNAT rules, for being able to
>> redirect tcp/80 and tcp/443 to squid's port silently.
> 
> Please note that your current Squid configuration is not a reverse proxy
> configuration. It is an interception configuration. It also lacks
> https_port for handling port 443 traffic. There are probably some
> documents on Squid wiki (and/or elsewhere) explaining how to configure
> Squid to become a reverse proxy. Follow them.
> 
>> I wanted to setup a proxy machine which I wanted to be able to receive
>> url like :
>> 
>> - www.iou.net.theproxy.com/hj.php?ui=9 [1]
>> 
>> If this site returns clean content (scanned by Icap server) the url
>> redirector should return :
>> 
>> - www.iou.net/hj.php?ui=9 [2] <http://www.iou.net/hj.php?ui=9> (the real
>> url) as URL.
> 
> OK.
> 
>> - Is it possible with Squid to achieve my goal?. With Squid, a
>> redirector, and a Icap daemon which performs virus scanning...
> 
> A redirector seems out of scope here -- it works on requests while you
> want to rewrite (scanned by ICAP) responses.
> 
> It is probably possible to use deny_info to respond with a redirect
> message. To trigger a deny_info action, you would have to configure your
> Squid to block virus-free responses, which is rather strange!
> 
>> - For plain http the config and the URL seem to be working BUT the virus
>> are not being scanned. Could the config be adjusted for that?.
> 
> I would start by removing the redirector, "intercept", SslBump, and
> disabling ICAP. Configure your Squid as a reverse proxy without any
> virus scanning. Then add ICAP. Get the virus scanning working without
> any URL manipulation. Once that is done, you can adjust Squid to block
> virus-free responses (via http_reply_access) and trigger a deny_info
> response containing an HTTP redirect.
> 
> Please note that once the browser gets a redirect to another site, that
> browser is not going to revisit your reverse proxy for any content
> related to that other site -- all requests for that other site will go
> from the browser to that other site. Your proxy will not be in the loop
> anymore. If that is not what you want, then you cannot use redirects at
> all -- you would have to accelerate that other site for all requests
> instead and make sure that other site does not contain absolute URLs
> pointing the browser away from your reverse proxy.
> 
> Disclaimer: I have not tested the above ideas and, again, I may be
> misinterpreting what you really want to achieve.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 

Links:
------
[1] http://www.iou.net.theproxy.com/hj.php?ui=9
[2] http://www.iou.net/hj.php?ui=9
[3] http://www.sarenet.es
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190305/5d2aeb07/attachment.htm>

From egoitz at sarenet.es  Tue Mar  5 08:57:56 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Tue, 05 Mar 2019 09:57:56 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
Message-ID: <4c6e883e3c7f8a8a9b5643290e62ea44@sarenet.es>

Hi!, 

I have Squid configured with the virus scanning software using ICAP and
working. But, when I do : 

acl matchear_todo url_regex [-i] ^.*$
http_reply_access deny matchear_todo
deny_info   http://172.16.8.61/redirigir.php?url=%s matchear_todo 

it's always redirecting me without passing the own ICAP system... I
wanted the redirection to be done only when content is clean... this is
doing it always... have I missed something? 

Cheers! 

---

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [3] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

El 2019-03-05 08:13, Alex Rousskov escribi?:

> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
> 
>> Clients, will ask :
>> 
>> https://oooeeee.eeee.ttt.thesquidserver.org/
> 
>> So the answer [to the second question] I assume should be yes.
> 
> If I am interpreting your answers correctly, then your setup looks like
> a reverse proxy to me. In that case, you do not need SslBump and
> interception. You do need an web server certificate for the
> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
> Do you already have that?
> 
>> I have DNAT rules, for being able to
>> redirect tcp/80 and tcp/443 to squid's port silently.
> 
> Please note that your current Squid configuration is not a reverse proxy
> configuration. It is an interception configuration. It also lacks
> https_port for handling port 443 traffic. There are probably some
> documents on Squid wiki (and/or elsewhere) explaining how to configure
> Squid to become a reverse proxy. Follow them.
> 
>> I wanted to setup a proxy machine which I wanted to be able to receive
>> url like :
>> 
>> - www.iou.net.theproxy.com/hj.php?ui=9 [1]
>> 
>> If this site returns clean content (scanned by Icap server) the url
>> redirector should return :
>> 
>> - www.iou.net/hj.php?ui=9 [2] <http://www.iou.net/hj.php?ui=9> (the real
>> url) as URL.
> 
> OK.
> 
>> - Is it possible with Squid to achieve my goal?. With Squid, a
>> redirector, and a Icap daemon which performs virus scanning...
> 
> A redirector seems out of scope here -- it works on requests while you
> want to rewrite (scanned by ICAP) responses.
> 
> It is probably possible to use deny_info to respond with a redirect
> message. To trigger a deny_info action, you would have to configure your
> Squid to block virus-free responses, which is rather strange!
> 
>> - For plain http the config and the URL seem to be working BUT the virus
>> are not being scanned. Could the config be adjusted for that?.
> 
> I would start by removing the redirector, "intercept", SslBump, and
> disabling ICAP. Configure your Squid as a reverse proxy without any
> virus scanning. Then add ICAP. Get the virus scanning working without
> any URL manipulation. Once that is done, you can adjust Squid to block
> virus-free responses (via http_reply_access) and trigger a deny_info
> response containing an HTTP redirect.
> 
> Please note that once the browser gets a redirect to another site, that
> browser is not going to revisit your reverse proxy for any content
> related to that other site -- all requests for that other site will go
> from the browser to that other site. Your proxy will not be in the loop
> anymore. If that is not what you want, then you cannot use redirects at
> all -- you would have to accelerate that other site for all requests
> instead and make sure that other site does not contain absolute URLs
> pointing the browser away from your reverse proxy.
> 
> Disclaimer: I have not tested the above ideas and, again, I may be
> misinterpreting what you really want to achieve.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
 

Links:
------
[1] http://www.iou.net.theproxy.com/hj.php?ui=9
[2] http://www.iou.net/hj.php?ui=9
[3] http://www.sarenet.es
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190305/5cded39b/attachment.htm>

From b4uwallet1 at gmail.com  Tue Mar  5 10:52:59 2019
From: b4uwallet1 at gmail.com (b4uwallet)
Date: Tue, 5 Mar 2019 04:52:59 -0600 (CST)
Subject: [squid-users] How to secure your cryptocurrencies
Message-ID: <1551783179911-0.post@n4.nabble.com>

The craze of dealing in cryptocurrencies is leading to its pitch right now.
If we gather upon the research that is made from the sensational sectors of
working so it is concluded that there are more than 2 million clients those
are dealing with blockchain. It not enough thousands of them are performing
selling and buying transactions daily. 
It is versatile truth the dealing in cryptocurrency may lead to a profitable
state. So, millions of people are enriched with blockchain.
Despite all these profitable statements, the thing that worthy matters are
is your money secure ?? is it safe?
This kind of pop-ups really occurs even at once in our mind. So, there is a
very secure arrangement by B4U  Wallet and Exchange <http://b4uwallet.com> 
. Today, we will tell you how you can secure your cryptocurrencies with us.
Protecting your software?
It's our foremost duty to secure your amount so the first attempt that's
made by our team ( B4U wallet <http://b4uwallet.com>  ) is we going to
secure all your leading software?s. We will protect that software of your
wallet. It is same as to have a backup of your application on a personal
computer. It proves itself very helpful in the case of hacking.
After this protection your every click will be protected approximately!! You
don't need to worry whether you are clicking or viewing the attachments.
Mobile application safety
If you have downloaded the wallet application. So, it first secured by your
lock. (nobody can even view it without your pin-code). Somehow, if anyone
knows your pin. You don't need to worry even then!! You will be provided (2
Factor authentication). 
No transfer-able sector needed!!
While using the wallet application you can transfer all your funds without
and helping account like PayPal etc. Each transaction will be performed
protectively, and you don't need to risk yourself. We are working as a
responsible team for our clients.
Backups in hardware
It's quite often to make a backup for your general information. It's quite
good !! but what if you lost your pc? or your data is removed by the
software. Here comes the most troubling situation. Moreover, if it?s a
desire for somebody to hack your account. So, he or she will definitely
remove all your backup. Stay calm !! you don't need to worry about it. We
will back up your data in the hardware sector of our record. Doesn't matter
if the pc is broken up or restored you I'll definitely get your data back in
a very promising way.
Access keys 
You will get access keys with the interacting of B4U wallet. We will provide
you access to 
Your phone number,
Email,
Fax or security question.
This access will lead you to have your hands on your account whether it's
lost. It good news!!
Inscription
Within our services, you will be provided with an encrypting sector. Even
though your phone is lost. Your whole devices will be given authority to
change your password as well as to take out your money secure. One thing you
should keep in mind is you one is in safe hand and you will not lose them.
You will be given each solution but the thing that matter is you should be
the rightful owner of the amount.
Keylogger
When you will create a successful account on B4U  wallet and exchange
<http://b4uwallet.com>  . After the perfect evaluation of your account you
will be provided with a key logger. A hacker can have access to your
password but not towards your key log so its 100% safer optimizer to make
your  cryptocurrency <http://b4uwallet.com>   save.
Two step verifications
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377684/B4U_Cryptocurrency_Wallet_and_Exchange_%2826%29.gif> 
Despite all these services, a service of two-step verification will be
provided you will set up your account, open it, and set a two-step
verification code without knowing this code nobody can even perform a single
transaction from your account. It makes your wallet much safe!!
Cold wallet service.
You can easily, safe all your data with cold wallet it is an offline sector
that helps you to keep bitcoin offline in a drive. It is used for the
every-day transaction. With this service, no one else can get towards your
funds. This wallet is also used for viewing your account details and
savings.
These all kinds of services are provided just to satisfy you guys. So, trust
and we will surely secure your each and every transaction.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Mar  5 11:35:04 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Mar 2019 00:35:04 +1300
Subject: [squid-users] Squid fallback
In-Reply-To: <1551721802471-0.post@n4.nabble.com>
References: <1551721802471-0.post@n4.nabble.com>
Message-ID: <9824df08-1f7c-8930-a2f8-33f2942ae1f0@treenet.co.nz>

On 5/03/19 6:50 am, ronin1907 wrote:
> Hello,
> 
> I m installating squid its working fine and when I want to check from
> http://ipv6-test.com/ fallback is running fine. My question is this;
> How can I close this option ?
> 

Er, why?


Amos


From squid3 at treenet.co.nz  Tue Mar  5 11:46:57 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Mar 2019 00:46:57 +1300
Subject: [squid-users] Issues setting up a proxy for malware scanning
In-Reply-To: <e34b942f1befaa0952d29f01459c0c25@sarenet.es>
References: <e34b942f1befaa0952d29f01459c0c25@sarenet.es>
Message-ID: <63a8841a-3935-a00d-1258-f19b0fe9a756@treenet.co.nz>

On 5/03/19 6:20 am, Egoitz Aurrekoetxea wrote:
> Hi mates!
> 
> 
> I was trying to setup a Squid server for the following matter. I wanted
> to have some modified url pointing to my Squid proxy, so that Squid to
> be able to connect to destination, scan the content and if all is ok,
> return a 3xx to the real URL. For that purpose I use the following
> configuration https://pastebin.com/raw/mP73fame . The url redirector in
> that config is? https://pastebin.com/p6Usmq75
> 
> 
> I'm facing the two following problems, probably due to not having a
> large experience in Squid :


It seems not have much experience with HTTP either.

You would be far better off forgetting this whole fake-domains and URL
redirection thing. It is almost the hardest possible way to do what you
say you are wanting.

Just divert the client traffic to the proxy and scan (or not) as it goes
through. No problems with changing HTTP response objects just to get the
traffic to a state where the scanner receives it. No confusing the
scanner which may at times be checking the domain naming pattern as part
of the signature.


HTH
Amos


From rousskov at measurement-factory.com  Tue Mar  5 14:36:10 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Mar 2019 07:36:10 -0700
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <CAHsiY=cxgt5mEzjscCHdhFrva8mV_vfnqeJma0dsOC_MBmrRWQ@mail.gmail.com>
References: <CAHsiY=cM2oCEuFQ60TLAvJ1n0SFcJD_G1Jv6WDg9RVYGj5499Q@mail.gmail.com>
 <be44d8c5-aa6a-1bcb-6867-92b569587ac8@measurement-factory.com>
 <CAHsiY=e6QOY45bc1oWccaoJ84VA7F+g5YUUnbL2H3Dw-RPyqeg@mail.gmail.com>
 <CAHsiY=cxgt5mEzjscCHdhFrva8mV_vfnqeJma0dsOC_MBmrRWQ@mail.gmail.com>
Message-ID: <99ddcbdd-26ac-11b6-868f-6a3ffc106fcb@measurement-factory.com>

On 3/4/19 9:45 PM, George Xie wrote:

> #4  0x00005555558a3d0a in comm_init () at comm.cc:1206
> 1206        fd_table =(fde *) xcalloc(Squid_MaxFD, sizeof(fde));
> (gdb) p Squid_MaxFD
> $1 = 1048576
> (gdb) p sizeof(fde)
> $2 = 392
> 
> It seems Squid_MaxFD is way too large, and its value is directly from ulimit:
> 
> # ulimit -n
> 1048576
> 
> therefore, I try to add this option:
> 
> max_filedesc 4096
> 
> now squid works and only takes ~50m memory.
> thanks very much for your help!

Glad you figured it out!

Alex.


> Xie Shi
> On Tue, Mar 5, 2019 at 12:22 PM George Xie <georgexsh at gmail.com> wrote:
>>
>>> To correct that default
>>> behavior, add this:
>>>   cache_mem 0
>>
>> thanks for your advice, but actually, I have tried this option before,
>> found no difference. besides, and I have tried `memory_pools off`.
>>
>>> Furthermore, older Squids, possibly including your no-longer-supported
>>> version, may allocate shared memory indexes where none are needed. That
>>> might explain why you see your Squid allocating a 392 MB table.
>>
>> that's fair, I will give squid 4.4 a try later.
>>
>>> If you want to know what is going on for sure, then configure malloc to
>>> dump core on allocation failures and post a stack trace leading to that
>>> allocation failure so that we know _what_ Squid was trying to allocate
>>> when it ran out of RAM.
>>
>> hope following backtrace is helpful:
>>
>> (gdb) bt
>> #0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
>> #1  0x00007ffff562e42a in __GI_abort () at abort.c:89
>> #2  0x0000555555728eb5 in fatal_dump (
>>     message=0x555555e764e0 <xcalloc::msg> "xcalloc: Unable to allocate
>> 1048576 blocks of 392 bytes!\n") at fatal.cc:113
>> #3  0x0000555555a09837 in xcalloc (n=1048576, sz=sz at entry=392) at xalloc.cc:90
>> #4  0x00005555558a3d0a in comm_init () at comm.cc:1206
>> #5  0x0000555555789104 in SquidMain (argc=<optimized out>, argv=0x7fffffffed48)
>>     at main.cc:1481
>> #6  0x000055555568a48b in SquidMainSafe (argv=<optimized out>,
>> argc=<optimized out>)
>>     at main.cc:1261
>> #7  main (argc=<optimized out>, argv=<optimized out>) at main.cc:1254
>>
>>
>> Xie Shi
>>
>>
>> On Tue, Mar 5, 2019 at 12:34 AM Alex Rousskov
>> <rousskov at measurement-factory.com> wrote:
>>>
>>> On 3/3/19 9:39 PM, George Xie wrote:
>>>
>>>> Squid version: 3.5.23-5+deb9u1
>>>
>>>>     http_port 127.0.0.1:3128
>>>>     cache deny all
>>>>     access_log none
>>>
>>> Unfortunately, this configuration wastes RAM: Squid is not yet smart
>>> enough to understand that you do not want any caching and may allocate
>>> 256+ MB of memory cache plus supporting indexes. To correct that default
>>> behavior, add this:
>>>
>>>       cache_mem 0
>>>
>>> Furthermore, older Squids, possibly including your no-longer-supported
>>> version, may allocate shared memory indexes where none are needed. That
>>> might explain why you see your Squid allocating a 392 MB table.
>>>
>>> If you want to know what is going on for sure, then configure malloc to
>>> dump core on allocation failures and post a stack trace leading to that
>>> allocation failure so that we know _what_ Squid was trying to allocate
>>> when it ran out of RAM.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>> runs in a container with following Dockerfile:
>>>>
>>>>     FROM debian:9
>>>>     RUN apt update && \
>>>>     apt install --yes squid
>>>>
>>>>
>>>> the total memory of the host server is very low, only 592m, about 370m
>>>> free memory.
>>>> if I start squid in the container, squid will abort immediately.
>>>>
>>>> error messages in /var/log/squid/cache.log:
>>>>
>>>>
>>>>     FATAL: xcalloc: Unable to allocate 1048576 blocks of 392 bytes!
>>>>
>>>>     Squid Cache (Version 3.5.23): Terminated abnormally.
>>>>     CPU Usage: 0.012 seconds = 0.004 user + 0.008 sys
>>>>     Maximum Resident Size: 47168 KB
>>>>
>>>>
>>>> error message captured with strace -f -e trace=memory:
>>>>
>>>>     [pid   920] mmap(NULL, 411176960, PROT_READ|PROT_WRITE,
>>>>     MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = -1 ENOMEM (Cannot allocate memory)
>>>>
>>>>
>>>> it appears that squid (or glibc) tries to allocate 392m memory, which is
>>>> larger than host free memory 370m.
>>>> but I guess squid don't need that much memory, I have another running
>>>> squid instance, which only uses < 200m memory.
>>>> the oddest thing is if I run squid on the host (also Debian 9) directly,
>>>> not in the container, squid could start and run as normal.
>>>>
>>>> am I doing something wrong thing here?
>>>>
>>>> Xie Shi
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users



From felipeapolanco at gmail.com  Tue Mar  5 16:11:26 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 5 Mar 2019 12:11:26 -0400
Subject: [squid-users] Need help blocking an specific HTTPS website
In-Reply-To: <8cc474f3-34df-e056-c634-e2b048d41314@solutti.com.br>
References: <CADcj3=5VZf8EOCwpAu7BHp7BY_HfCAYus2MpepSGRD-=P6wSyA@mail.gmail.com>
 <8cc474f3-34df-e056-c634-e2b048d41314@solutti.com.br>
Message-ID: <CADcj3=6S4e1gn2zkQ1q6QX4Q9j2QhRkSa+BJDyJ0xLM0icUvbA@mail.gmail.com>

I confirm that, I can see TCP_DENIED requests on the access.log to
web.whatsapp.com but still the websites loads.

1551192823.356     47 192.168.112.144 TCP_DENIED/403 4453 GET
https://web.whatsapp.com/ws - HIER_NONE/- text/html

On Mon, Mar 4, 2019 at 7:21 PM Leonardo Rodrigues <leolistas at solutti.com.br>
wrote:

> Em 04/03/2019 19:27, Felipe Arturo Polanco escreveu:
>
> Hi,
>
> I have been trying to block https://web.whatsapp.com/ from squid and I
> have been unable to.
>
> So far I have this:
>
> I can block other HTTPS websites fine
> I can block www.whatsapp.com fine
> I cannot block web.whatsapp.com
>
> I have HTTPS transparent interception enabled and I am bumping all TCP
> connections, but still this one doesn't appear to get blocked by squid.
>
> This is part of my configuration:
> ===========================
> acl blockwa1 url_regex whatsapp\.com$
> acl blockwa2 dstdomain .whatsapp.com
> acl blockwa3 ssl::server_name .whatsapp.com
> acl step1 at_step SslBump1
>
>
>     blockwa1 and blockwa2 should definitely block web.whatsapp.com ..
> your rules seems right.
>
>     Can you confirm the web.whatsapp.com access are getting through squid
> ? Are these accesses on your access.log with something different than
> DENIED status ?
>
>
>
> --
>
>
> 	Atenciosamente / Sincerily,
> 	Leonardo Rodrigues
> 	Solutti Tecnologia
> 	http://www.solutti.com.br
>
> 	Minha armadilha de SPAM, N?O mandem email
> 	gertrudes at solutti.com.br
> 	My SPAMTRAP, do not email it
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190305/fea01d9f/attachment.htm>

From rousskov at measurement-factory.com  Tue Mar  5 16:45:43 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Mar 2019 09:45:43 -0700
Subject: [squid-users] Squid and url modifying
In-Reply-To: <4c6e883e3c7f8a8a9b5643290e62ea44@sarenet.es>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
 <4c6e883e3c7f8a8a9b5643290e62ea44@sarenet.es>
Message-ID: <5dcdcf16-ac33-0ae2-a6ee-83ef0c5f21ca@measurement-factory.com>

On 3/5/19 1:57 AM, Egoitz Aurrekoetxea wrote:

> I have Squid configured with the virus scanning software using ICAP and
> working. But, when I do :
> 
> acl matchear_todo url_regex [-i] ^.*$

FYI: "[-i]" is documentation syntax that means an optional flag called
"-i". If you want to use that "-i" flag, then type

  acl matchear_todo url_regex -i ^.*$

... but keep in mind that "-i" makes no sense when you regular
expression does not contain small or capital characters. Adding "-i"
would not change what URLs such a regular expression would match.


> http_reply_access deny matchear_todo
> deny_info?? http://172.16.8.61/redirigir.php?url=%s matchear_todo

Why are you blocking based on URL instead of blocking based on the ICAP
scan result? In your earlier specifications, you wanted to
block/redirect only those transactions that were certified virus-free by
your ICAP client. The above matchear_todo ACL does not do that.


> it's always redirecting me without passing the own ICAP system...

Looking at the Squid code, what you describe overall seems impossible --
Squid checks http_reply_access _after_ the RESPMOD transaction, not
before it. Adding http_reply_access cannot disable ICAP scans AFAICT!
Are you sure it has that effect in your use case?


> I
> wanted the redirection to be done only when content is clean... this is
> doing it always... have I missed something?

Your ACL says nothing about "clean". It says "always". How does your
ICAP service mark "clean" (or "dirty") HTTP responses? Your ACL needs to
match that marking (or the absence of that marking).

Alex.


> El 2019-03-05 08:13, Alex Rousskov escribi?:
> 
>> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
>>
>>> Clients, will ask :
>>>
>>> https://oooeeee.eeee.ttt.thesquidserver.org/
>>
>>> So the answer [to the second question] I assume should be yes.
>>
>> If I am interpreting your answers correctly, then your setup looks like
>> a reverse proxy to me. In that case, you do not need SslBump and
>> interception. You do need an web server certificate for the
>> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
>> Do you already have that?
>>
>>
>>> I have DNAT rules, for being able to
>>> redirect tcp/80 and tcp/443 to squid's port silently.
>>
>> Please note that your current Squid configuration is not a reverse proxy
>> configuration. It is an interception configuration. It also lacks
>> https_port for handling port 443 traffic. There are probably some
>> documents on Squid wiki (and/or elsewhere) explaining how to configure
>> Squid to become a reverse proxy. Follow them.
>>
>>
>>> I wanted to setup a proxy machine which I wanted to be able to receive
>>> url like :
>>>
>>> - www.iou.net.theproxy.com/hj.php?ui=9
>>> <http://www.iou.net.theproxy.com/hj.php?ui=9>
>>>
>>> If this site returns clean content (scanned by Icap server) the url
>>> redirector should return :
>>>
>>> - www.iou.net/hj.php?ui=9 <http://www.iou.net/hj.php?ui=9>
>>> <http://www.iou.net/hj.php?ui=9> (the real
>>> url) as URL.
>>
>> OK.
>>
>>
>>> - Is it possible with Squid to achieve my goal?. With Squid, a
>>> redirector, and a Icap daemon which performs virus scanning...
>>
>> A redirector seems out of scope here -- it works on requests while you
>> want to rewrite (scanned by ICAP) responses.
>>
>> It is probably possible to use deny_info to respond with a redirect
>> message. To trigger a deny_info action, you would have to configure your
>> Squid to block virus-free responses, which is rather strange!
>>
>>
>>> - For plain http the config and the URL seem to be working BUT the virus
>>> are not being scanned. Could the config be adjusted for that?.
>>
>>
>> I would start by removing the redirector, "intercept", SslBump, and
>> disabling ICAP. Configure your Squid as a reverse proxy without any
>> virus scanning. Then add ICAP. Get the virus scanning working without
>> any URL manipulation. Once that is done, you can adjust Squid to block
>> virus-free responses (via http_reply_access) and trigger a deny_info
>> response containing an HTTP redirect.
>>
>>
>> Please note that once the browser gets a redirect to another site, that
>> browser is not going to revisit your reverse proxy for any content
>> related to that other site -- all requests for that other site will go
>> from the browser to that other site. Your proxy will not be in the loop
>> anymore. If that is not what you want, then you cannot use redirects at
>> all -- you would have to accelerate that other site for all requests
>> instead and make sure that other site does not contain absolute URLs
>> pointing the browser away from your reverse proxy.
>>
>>
>> Disclaimer: I have not tested the above ideas and, again, I may be
>> misinterpreting what you really want to achieve.
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From egoitz at sarenet.es  Tue Mar  5 16:59:10 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Tue, 05 Mar 2019 17:59:10 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <5dcdcf16-ac33-0ae2-a6ee-83ef0c5f21ca@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
 <4c6e883e3c7f8a8a9b5643290e62ea44@sarenet.es>
 <5dcdcf16-ac33-0ae2-a6ee-83ef0c5f21ca@measurement-factory.com>
Message-ID: <0f561bacfd7506c96a4bcf15422c5e4b@sarenet.es>

Hi Alex!! 

I do answer below!! Many many thanks in advance...

---

EGOITZ AURREKOETXEA 
Dpto. de sistemas 
944 209 470
Parque Tecnol?gico. Edificio 103
48170 Zamudio (Bizkaia) 
egoitz at sarenet.es 
www.sarenet.es [3] 
Antes de imprimir este correo electr?nico piense si es necesario
hacerlo. 

El 2019-03-05 17:45, Alex Rousskov escribi?:

> On 3/5/19 1:57 AM, Egoitz Aurrekoetxea wrote:
> 
>> I have Squid configured with the virus scanning software using ICAP and
>> working. But, when I do :
>> 
>> acl matchear_todo url_regex [-i] ^.*$
> 
> FYI: "[-i]" is documentation syntax that means an optional flag called
> "-i". If you want to use that "-i" flag, then type
> 
> acl matchear_todo url_regex -i ^.*$
> 
> ... but keep in mind that "-i" makes no sense when you regular
> expression does not contain small or capital characters. Adding "-i"
> would not change what URLs such a regular expression would match. 
> 
> I SEE... I THOUGH IT WAS FOR MATCHING CASE INSENSITIVELY... SOME SORT OF I/______/ 
> 
>> http_reply_access deny matchear_todo
>> deny_info   http://172.16.8.61/redirigir.php?url=%s matchear_todo
> 
> Why are you blocking based on URL instead of blocking based on the ICAP
> scan result? In your earlier specifications, you wanted to
> block/redirect only those transactions that were certified virus-free by
> your ICAP client. The above matchear_todo ACL does not do that. 
> 
> THAT WAS AN ATTEMPT OF ACHIEVING MY GOAL. REDIRECT REQUESTS TO A PHP WHICH DOES THE REQUEST TO A "NEXT SQUID" AND THEN RETURN ONE THING OR ANOTHER.... 
> 
> SORRY, THAT'S WRONG. I HAVE DONE TONS OF TESTS... AT PRESENT... I DON'T REALLY KNOW HOW TO DO THAT... I WOULD BE VERY THANKFUL IF YOU COULD GUIDE ME ON HOW COULD I DO IT... IS IT POSSIBLE TO BE DONE FROM SQUID SIDE?. OR DOES THE OWN ICAP IMPLEMENTATION DIRECTLY RETURN A 3XX ANSWER?. 
> 
>> it's always redirecting me without passing the own ICAP system...
> 
> Looking at the Squid code, what you describe overall seems impossible --
> Squid checks http_reply_access _after_ the RESPMOD transaction, not
> before it. Adding http_reply_access cannot disable ICAP scans AFAICT!
> Are you sure it has that effect in your use case? 
> 
> IT SEEMED TO DO SO YES.... I'LL TRY IT AGAIN.... 
> 
>> I
>> wanted the redirection to be done only when content is clean... this is
>> doing it always... have I missed something?
> 
> Your ACL says nothing about "clean". It says "always". How does your
> ICAP service mark "clean" (or "dirty") HTTP responses? Your ACL needs to
> match that marking (or the absence of that marking). 
> 
> COULD YOU GIVE ME A CLUE OF HOW COULD I DO IT?. 
> 
> Alex. 
> 
> THANKS ALEX!!!! 
> 
> El 2019-03-05 08:13, Alex Rousskov escribi?:
> 
> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
> 
> Clients, will ask :
> 
> https://oooeeee.eeee.ttt.thesquidserver.org/ 
> So the answer [to the second question] I assume should be yes. 
> If I am interpreting your answers correctly, then your setup looks like
> a reverse proxy to me. In that case, you do not need SslBump and
> interception. You do need an web server certificate for the
> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
> Do you already have that?
> 
> I have DNAT rules, for being able to
> redirect tcp/80 and tcp/443 to squid's port silently. 
> Please note that your current Squid configuration is not a reverse proxy
> configuration. It is an interception configuration. It also lacks
> https_port for handling port 443 traffic. There are probably some
> documents on Squid wiki (and/or elsewhere) explaining how to configure
> Squid to become a reverse proxy. Follow them.
> 
> I wanted to setup a proxy machine which I wanted to be able to receive
> url like :
> 
> - www.iou.net.theproxy.com/hj.php?ui=9 [1]
> <http://www.iou.net.theproxy.com/hj.php?ui=9>
> 
> If this site returns clean content (scanned by Icap server) the url
> redirector should return :
> 
> - www.iou.net/hj.php?ui=9 [2] <http://www.iou.net/hj.php?ui=9>
> <http://www.iou.net/hj.php?ui=9> (the real
> url) as URL. 
> OK.
> 
> - Is it possible with Squid to achieve my goal?. With Squid, a
> redirector, and a Icap daemon which performs virus scanning... 
> A redirector seems out of scope here -- it works on requests while you
> want to rewrite (scanned by ICAP) responses.
> 
> It is probably possible to use deny_info to respond with a redirect
> message. To trigger a deny_info action, you would have to configure your
> Squid to block virus-free responses, which is rather strange!
> 
> - For plain http the config and the URL seem to be working BUT the virus
> are not being scanned. Could the config be adjusted for that?. 
> 
> I would start by removing the redirector, "intercept", SslBump, and
> disabling ICAP. Configure your Squid as a reverse proxy without any
> virus scanning. Then add ICAP. Get the virus scanning working without
> any URL manipulation. Once that is done, you can adjust Squid to block
> virus-free responses (via http_reply_access) and trigger a deny_info
> response containing an HTTP redirect.
> 
> Please note that once the browser gets a redirect to another site, that
> browser is not going to revisit your reverse proxy for any content
> related to that other site -- all requests for that other site will go
> from the browser to that other site. Your proxy will not be in the loop
> anymore. If that is not what you want, then you cannot use redirects at
> all -- you would have to accelerate that other site for all requests
> instead and make sure that other site does not contain absolute URLs
> pointing the browser away from your reverse proxy.
> 
> Disclaimer: I have not tested the above ideas and, again, I may be
> misinterpreting what you really want to achieve.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 

Links:
------
[1] http://www.iou.net.theproxy.com/hj.php?ui=9
[2] http://www.iou.net/hj.php?ui=9
[3] http://www.sarenet.es
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190305/8822be54/attachment.htm>

From rousskov at measurement-factory.com  Tue Mar  5 17:48:52 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Mar 2019 10:48:52 -0700
Subject: [squid-users] Squid and url modifying
In-Reply-To: <0f561bacfd7506c96a4bcf15422c5e4b@sarenet.es>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
 <4c6e883e3c7f8a8a9b5643290e62ea44@sarenet.es>
 <5dcdcf16-ac33-0ae2-a6ee-83ef0c5f21ca@measurement-factory.com>
 <0f561bacfd7506c96a4bcf15422c5e4b@sarenet.es>
Message-ID: <b64b57df-5f0a-a25c-4a6e-d44b94fc491d@measurement-factory.com>

On 3/5/19 9:59 AM, Egoitz Aurrekoetxea wrote:

> El 2019-03-05 17:45, Alex Rousskov escribi?:
>> On 3/5/19 1:57 AM, Egoitz Aurrekoetxea wrote:
>>
>>> I have Squid configured with the virus scanning software using ICAP and
>>> working. But, when I do :
>>>
>>> acl matchear_todo url_regex [-i] ^.*$

>> FYI: "[-i]" is documentation syntax that means an optional flag called
>> "-i". If you want to use that "-i" flag, then type
>>
>> ??acl matchear_todo url_regex -i ^.*$
>>
>> ... but keep in mind that "-i" makes no sense when you regular
>> expression does not contain small or capital characters. Adding "-i"
>> would not change what URLs such a regular expression would match.
 ?
> I see... I though it was for matching case insensitively...

You thought correctly. The -i flag enables case insensitive matches
indeed, but you are specifying that flag incorrectly (extra square
brackets), and it makes no sense to specify it at all for your specific
regular expression!


>>> http_reply_access deny matchear_todo
>>> deny_info?? http://172.16.8.61/redirigir.php?url=%s matchear_todo

>> Why are you blocking based on URL instead of blocking based on the ICAP
>> scan result? In your earlier specifications, you wanted to
>> block/redirect only those transactions that were certified virus-free by
>> your ICAP client. The above matchear_todo ACL does not do that.
 ?
>> *That was an attempt of achieving my goal. Redirect requests to a php
>> which does the request to a "next Squid" and then return one thing or
>> another....*

Sounds like you are asking about one thing and then testing/discussing
another. Doing so makes helping you more difficult. Focus on making the
simplest use case working first.


>> Is it possible to be done from Squid side?

Probably (as long as your ICAP service can signal clean/dirty status in
a way Squid ACLs can detect). Since you appear to change the
problem/goal, I am not sure what the answer to this question is.


> Or does the own ICAP implementation directly return a 3xx answer?

That works as well. In that case, you do not need deny_info tricks.

>> Your ACL says nothing about "clean". It says "always". How does your
>> ICAP service mark "clean" (or "dirty") HTTP responses? Your ACL needs to
>> match that marking (or the absence of that marking).
 ?
> Could you give me a clue of how could I do it?

I cannot because I do not know what your ICAP service is capable of (and
do not have the time to research that). For example, if your ICAP
service can add an HTTP header to dirty HTTP responses, then you can use
the corresponding Squid ACL to detect the presence of that header in the
adapted response.

Alex.
 ?

>>> El 2019-03-05 08:13, Alex Rousskov escribi?:
>>>
>>>> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
>>>>
>>>>> Clients, will ask :
>>>>>
>>>>> https://oooeeee.eeee.ttt.thesquidserver.org/
>>>>
>>>>> So the answer [to the second question] I assume should be yes.
>>>>
>>>> If I am interpreting your answers correctly, then your setup looks like
>>>> a reverse proxy to me. In that case, you do not need SslBump and
>>>> interception. You do need an web server certificate for the
>>>> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
>>>> Do you already have that?
>>>>
>>>>
>>>>> I have DNAT rules, for being able to
>>>>> redirect tcp/80 and tcp/443 to squid's port silently.
>>>>
>>>> Please note that your current Squid configuration is not a reverse proxy
>>>> configuration. It is an interception configuration. It also lacks
>>>> https_port for handling port 443 traffic. There are probably some
>>>> documents on Squid wiki (and/or elsewhere) explaining how to configure
>>>> Squid to become a reverse proxy. Follow them.
>>>>
>>>>
>>>>> I wanted to setup a proxy machine which I wanted to be able to receive
>>>>> url like :
>>>>>
>>>>> - www.iou.net.theproxy.com/hj.php?ui=9
>>>>> <http://www.iou.net.theproxy.com/hj.php?ui=9>
>>>>> <http://www.iou.net.theproxy.com/hj.php?ui=9>
>>>>>
>>>>> If this site returns clean content (scanned by Icap server) the url
>>>>> redirector should return :
>>>>>
>>>>> - www.iou.net/hj.php?ui=9 <http://www.iou.net/hj.php?ui=9>
>>>>> <http://www.iou.net/hj.php?ui=9>
>>>>> <http://www.iou.net/hj.php?ui=9> (the real
>>>>> url) as URL.
>>>>
>>>> OK.
>>>>
>>>>
>>>>> - Is it possible with Squid to achieve my goal?. With Squid, a
>>>>> redirector, and a Icap daemon which performs virus scanning...
>>>>
>>>> A redirector seems out of scope here -- it works on requests while you
>>>> want to rewrite (scanned by ICAP) responses.
>>>>
>>>> It is probably possible to use deny_info to respond with a redirect
>>>> message. To trigger a deny_info action, you would have to configure your
>>>> Squid to block virus-free responses, which is rather strange!
>>>>
>>>>
>>>>> - For plain http the config and the URL seem to be working BUT the
>>>>> virus
>>>>> are not being scanned. Could the config be adjusted for that?.
>>>>
>>>>
>>>> I would start by removing the redirector, "intercept", SslBump, and
>>>> disabling ICAP. Configure your Squid as a reverse proxy without any
>>>> virus scanning. Then add ICAP. Get the virus scanning working without
>>>> any URL manipulation. Once that is done, you can adjust Squid to block
>>>> virus-free responses (via http_reply_access) and trigger a deny_info
>>>> response containing an HTTP redirect.
>>>>
>>>>
>>>> Please note that once the browser gets a redirect to another site, that
>>>> browser is not going to revisit your reverse proxy for any content
>>>> related to that other site -- all requests for that other site will go
>>>> from the browser to that other site. Your proxy will not be in the loop
>>>> anymore. If that is not what you want, then you cannot use redirects at
>>>> all -- you would have to accelerate that other site for all requests
>>>> instead and make sure that other site does not contain absolute URLs
>>>> pointing the browser away from your reverse proxy.
>>>>
>>>>
>>>> Disclaimer: I have not tested the above ideas and, again, I may be
>>>> misinterpreting what you really want to achieve.
>>>>
>>>> Alex.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> <mailto:squid-users at lists.squid-cache.org>
>>>> <mailto:squid-users at lists.squid-cache.org
>>>> <mailto:squid-users at lists.squid-cache.org>>
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> <mailto:squid-users at lists.squid-cache.org>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Mar  6 04:04:42 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Mar 2019 17:04:42 +1300
Subject: [squid-users] Need help blocking an specific HTTPS website
In-Reply-To: <CADcj3=6S4e1gn2zkQ1q6QX4Q9j2QhRkSa+BJDyJ0xLM0icUvbA@mail.gmail.com>
References: <CADcj3=5VZf8EOCwpAu7BHp7BY_HfCAYus2MpepSGRD-=P6wSyA@mail.gmail.com>
 <8cc474f3-34df-e056-c634-e2b048d41314@solutti.com.br>
 <CADcj3=6S4e1gn2zkQ1q6QX4Q9j2QhRkSa+BJDyJ0xLM0icUvbA@mail.gmail.com>
Message-ID: <c7217e02-f198-ac2e-dcaf-0e538f2695f5@treenet.co.nz>

On 6/03/19 5:11 am, Felipe Arturo Polanco wrote:
> I confirm that, I can see TCP_DENIED requests on the access.log to
> web.whatsapp.com <http://web.whatsapp.com> but still the websites loads.
> 
> 1551192823.356? ? ?47 192.168.112.144 TCP_DENIED/403 4453 GET
> https://web.whatsapp.com/ws - HIER_NONE/- text/html
> 


Perhapse WhatsApp uses other protocols to get through when denied by the
proxy.

Have you tried blocking UDP port 80 and 443 (QUIC protocol) in your
firewall?

And of course ports 4244, 5222, 5223, 5228 and 5242.


Amos


From egoitz at sarenet.es  Wed Mar  6 07:50:22 2019
From: egoitz at sarenet.es (Egoitz Aurrekoetxea)
Date: Wed, 06 Mar 2019 08:50:22 +0100
Subject: [squid-users] Squid and url modifying
In-Reply-To: <b64b57df-5f0a-a25c-4a6e-d44b94fc491d@measurement-factory.com>
References: <11c17c2320f135905d81d3112bbd1339@sarenet.es>
 <e4584f74-bbce-ddc6-2820-7dce42b73449@measurement-factory.com>
 <5bc0a8f654f93be42c90ebbe5a99e0ec@sarenet.es>
 <3d94875a-ae2b-77e7-1581-1880bb99bdc5@measurement-factory.com>
 <7b64f26e818a118fa8fbe2dfe2530616@sarenet.es>
 <093a8eef-d155-5638-815b-f74bf727795e@measurement-factory.com>
 <4c6e883e3c7f8a8a9b5643290e62ea44@sarenet.es>
 <5dcdcf16-ac33-0ae2-a6ee-83ef0c5f21ca@measurement-factory.com>
 <0f561bacfd7506c96a4bcf15422c5e4b@sarenet.es>
 <b64b57df-5f0a-a25c-4a6e-d44b94fc491d@measurement-factory.com>
Message-ID: <a56e9966c969dd1057ac56a91c85d988@sarenet.es>

Hi!, 

Thank you so much for all your effort. We have finally got it done by
using a mixed solution. A script plus the Squid actual configured mode
:) 

I wanted to thank really all your time because it has been like gold for
me :) :) 

Bye mates! 

El 2019-03-05 18:48, Alex Rousskov escribi?:

> On 3/5/19 9:59 AM, Egoitz Aurrekoetxea wrote:
> 
> El 2019-03-05 17:45, Alex Rousskov escribi?: On 3/5/19 1:57 AM, Egoitz Aurrekoetxea wrote:
> 
> I have Squid configured with the virus scanning software using ICAP and
> working. But, when I do :
> 
> acl matchear_todo url_regex [-i] ^.*$

>> FYI: "[-i]" is documentation syntax that means an optional flag called
>> "-i". If you want to use that "-i" flag, then type
>> 
>> acl matchear_todo url_regex -i ^.*$
>> 
>> ... but keep in mind that "-i" makes no sense when you regular
>> expression does not contain small or capital characters. Adding "-i"
>> would not change what URLs such a regular expression would match.

> I see... I though it was for matching case insensitively...

You thought correctly. The -i flag enables case insensitive matches
indeed, but you are specifying that flag incorrectly (extra square
brackets), and it makes no sense to specify it at all for your specific
regular expression!

> http_reply_access deny matchear_todo
> deny_info   http://172.16.8.61/redirigir.php?url=%s matchear_todo

>> Why are you blocking based on URL instead of blocking based on the ICAP
>> scan result? In your earlier specifications, you wanted to
>> block/redirect only those transactions that were certified virus-free by
>> your ICAP client. The above matchear_todo ACL does not do that.

>> *That was an attempt of achieving my goal. Redirect requests to a php
>> which does the request to a "next Squid" and then return one thing or
>> another....*

Sounds like you are asking about one thing and then testing/discussing
another. Doing so makes helping you more difficult. Focus on making the
simplest use case working first.

>> Is it possible to be done from Squid side?

Probably (as long as your ICAP service can signal clean/dirty status in
a way Squid ACLs can detect). Since you appear to change the
problem/goal, I am not sure what the answer to this question is.

> Or does the own ICAP implementation directly return a 3xx answer?

That works as well. In that case, you do not need deny_info tricks.

>> Your ACL says nothing about "clean". It says "always". How does your
>> ICAP service mark "clean" (or "dirty") HTTP responses? Your ACL needs to
>> match that marking (or the absence of that marking).

> Could you give me a clue of how could I do it?

I cannot because I do not know what your ICAP service is capable of (and
do not have the time to research that). For example, if your ICAP
service can add an HTTP header to dirty HTTP responses, then you can use
the corresponding Squid ACL to detect the presence of that header in the
adapted response.

Alex.

> El 2019-03-05 08:13, Alex Rousskov escribi?:
> 
> On 3/4/19 11:20 AM, Egoitz Aurrekoetxea wrote:
> 
> Clients, will ask :
> 
> https://oooeeee.eeee.ttt.thesquidserver.org/ 
> So the answer [to the second question] I assume should be yes. 
> If I am interpreting your answers correctly, then your setup looks like
> a reverse proxy to me. In that case, you do not need SslBump and
> interception. You do need an web server certificate for the
> oooeeee.eeee.ttt.thesquidserver.org domain, issued by a well-trusted CA.
> Do you already have that?
> 
> I have DNAT rules, for being able to
> redirect tcp/80 and tcp/443 to squid's port silently. 
> Please note that your current Squid configuration is not a reverse proxy
> configuration. It is an interception configuration. It also lacks
> https_port for handling port 443 traffic. There are probably some
> documents on Squid wiki (and/or elsewhere) explaining how to configure
> Squid to become a reverse proxy. Follow them.
> 
> I wanted to setup a proxy machine which I wanted to be able to receive
> url like :
> 
> - www.iou.net.theproxy.com/hj.php?ui=9 [1]
> <http://www.iou.net.theproxy.com/hj.php?ui=9>
> <http://www.iou.net.theproxy.com/hj.php?ui=9>
> 
> If this site returns clean content (scanned by Icap server) the url
> redirector should return :
> 
> - www.iou.net/hj.php?ui=9 [2] <http://www.iou.net/hj.php?ui=9>
> <http://www.iou.net/hj.php?ui=9>
> <http://www.iou.net/hj.php?ui=9> (the real
> url) as URL. 
> OK.
> 
> - Is it possible with Squid to achieve my goal?. With Squid, a
> redirector, and a Icap daemon which performs virus scanning... 
> A redirector seems out of scope here -- it works on requests while you
> want to rewrite (scanned by ICAP) responses.
> 
> It is probably possible to use deny_info to respond with a redirect
> message. To trigger a deny_info action, you would have to configure your
> Squid to block virus-free responses, which is rather strange!
> 
> - For plain http the config and the URL seem to be working BUT the
> virus
> are not being scanned. Could the config be adjusted for that?. 
> 
> I would start by removing the redirector, "intercept", SslBump, and
> disabling ICAP. Configure your Squid as a reverse proxy without any
> virus scanning. Then add ICAP. Get the virus scanning working without
> any URL manipulation. Once that is done, you can adjust Squid to block
> virus-free responses (via http_reply_access) and trigger a deny_info
> response containing an HTTP redirect.
> 
> Please note that once the browser gets a redirect to another site, that
> browser is not going to revisit your reverse proxy for any content
> related to that other site -- all requests for that other site will go
> from the browser to that other site. Your proxy will not be in the loop
> anymore. If that is not what you want, then you cannot use redirects at
> all -- you would have to accelerate that other site for all requests
> instead and make sure that other site does not contain absolute URLs
> pointing the browser away from your reverse proxy.
> 
> Disclaimer: I have not tested the above ideas and, again, I may be
> misinterpreting what you really want to achieve.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>
> <mailto:squid-users at lists.squid-cache.org
> <mailto:squid-users at lists.squid-cache.org>>
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users 

Links:
------
[1] http://www.iou.net.theproxy.com/hj.php?ui=9
[2] http://www.iou.net/hj.php?ui=9
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190306/a75de30a/attachment.htm>

From georgexsh at gmail.com  Wed Mar  6 18:25:21 2019
From: georgexsh at gmail.com (George Xie)
Date: Thu, 7 Mar 2019 02:25:21 +0800
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <f620fd2a-e132-76f9-ea46-e2353c9d077e@treenet.co.nz>
References: <CAHsiY=d4eNA81EJ0NBctKTCGV0vUhz=j5hacFkbkGXCshLx1Jw@mail.gmail.com>
 <f620fd2a-e132-76f9-ea46-e2353c9d077e@treenet.co.nz>
Message-ID: <CAHsiY=cK=wvfukGug_z+E08ThPJHngy=DbuKeETZopKFDtOJdg@mail.gmail.com>

thanks for your detailed reply, Amos.

now I have found out the culprit is large file descriptor number limit.
Docker CE for Debian package has set this limit (RLIMIT_NOFILE) in the
container to 1048576, thus squid set Squid_MaxFD to 1048576, then
allocates 1048576*392 = 392m memory for `fd_table`.
in the host, RLIMIT_NOFILE is only 1024.

> So, if you actually do not want the proxy caching anything, then
> disabling the cache_mem (set it to 0 as per Alex response) would be the
> best choice of action before you go any further.

in fact, I have tried this option before but failed.
it is very nice to learn this advice, I will add it to my proxy only
squid config.

> So disabling swap entirely on the server is not a great idea. It just
> moves the error and shutdown to happen at peak traffic times when it is
> least wanted.

but I guess this is common in the "cloud" era, eg: Google Compute Engine.


Xie Shi
On Tue, Mar 5, 2019 at 4:13 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 4/03/19 9:45 pm, George Xie wrote:
> >     > On 4/03/19 5:39 pm, George Xie wrote:
> >     > > hi all:
> >     > >
> >     > > Squid version: 3.5.23-5+deb9u1
> >     > > Docker version 18.09.3, build 774a1f4
> >     > > Linux instance-4 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27)
> >     > > x86_64 GNU/Linux
> >     > >
> >     > > I have the following squid config:
> >     > >
> >     > >
> >     > >     http_port 127.0.0.1:3128 <http://127.0.0.1:3128>
> >     > >     cache deny all
> >     > >     access_log none
> >     > >
> >     > What is it exactly that you think this is doing in regards to Squid
> >     > memory needs?
> >     >
> >
> >
> > sorry, I don't get your quest.
> >
>
> I was asking to see what you were thinking was going on with those settings.
>
> As Alex already pointed out the "cache deny all" does not reduce memory
> needs of Squid in any way. It just makes 256MB of that RAM become
> pointless allocating.
>
> So, if you actually do not want the proxy caching anything, then
> disabling the cache_mem (set it to 0 as per Alex response) would be the
> best choice of action before you go any further.
>
> Or if you *do* want caching, and were trying to disable it for testing
> the memory issue. Then your test was wrong, and produces incorrect
> conclusion. Just reducing cache_mem would be best for this case - set it
> to a value that should reasonably fit this container and see if the
> proxy runs okay.
>
>
> ...
> >     > >
> >     > > it appears that squid (or glibc) tries to allocate 392m memory,
> >     which is
> >     > > larger than host free memory 370m.
> >     > > but I guess squid don't need that much memory, I have another
> >     running
> >     > > squid instance, which only uses < 200m memory.
> >     > No doubt it is configured to use less memory. For example by reducing
> >     > the default memory cache size.
> >     >
> >
> >
> > that running squid instance has the same config.
> >
>
> Then something odd is going on between the two. They should indeed have
> had the same behaviour (either work or same error).
>
> Whatever the issue is it is being triggered by the large blocks of RAM
> allocated by a default Squid. The easiest to modify is the cache_mem.
>
>
> >
> >     > > the oddest thing is if I run squid on the host (also Debian 9)
> >     directly,
> >     > > not in the container, squid could start and run as normal.
> >     > >
> >     > Linux typically allows RAM over-allocation. Which works okay so
> >     long as
> >     > there is sufficient swap space and there is time between memory
> >     usage to
> >     > do the swap in/out process.
> >     > Amos
> >
> >
> > swap is disabled in the host server, so do in the container.
> >
> > after all, I wonder why squid would try to claim 392m memory if don't
> > need that much.
> >
>
> Squid thinks it does. All client traffic is denied being cached by that
> "deny all". BUT ... there are internally generated items which also use
> cache. So there is 256MB default RAM cache allocated and only those few
> small things being put in it.
>
> You could set it to '0' or to some small value and the allocation size
> should go down accordingly.
>
>
> That said, every bit of client traffic headed towards the proxy uses
> memory of volatile amount and at peak times it may need to allocate
> large blocks.
>
> So disabling swap entirely on the server is not a great idea. It just
> moves the error and shutdown to happen at peak traffic times when it is
> least wanted.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Wed Mar  6 19:51:24 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Mar 2019 12:51:24 -0700
Subject: [squid-users] squid in container aborted on low memory server
In-Reply-To: <CAHsiY=cK=wvfukGug_z+E08ThPJHngy=DbuKeETZopKFDtOJdg@mail.gmail.com>
References: <CAHsiY=d4eNA81EJ0NBctKTCGV0vUhz=j5hacFkbkGXCshLx1Jw@mail.gmail.com>
 <f620fd2a-e132-76f9-ea46-e2353c9d077e@treenet.co.nz>
 <CAHsiY=cK=wvfukGug_z+E08ThPJHngy=DbuKeETZopKFDtOJdg@mail.gmail.com>
Message-ID: <0a262777-98a5-7baf-71ca-d1a8b67e557d@measurement-factory.com>

On 3/6/19 11:25 AM, George Xie wrote:

>> So disabling swap entirely on the server is not a great idea. It just
>> moves the error and shutdown to happen at peak traffic times when it is
>> least wanted.

> but I guess this is common in the "cloud" era, eg: Google Compute Engine.

Moreover, in many production environments "swapping during peak traffic
times" is worse than "shutting down during peak traffic times". YMMV.

Alex.


> Xie Shi
> On Tue, Mar 5, 2019 at 4:13 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>
>> On 4/03/19 9:45 pm, George Xie wrote:
>>>     > On 4/03/19 5:39 pm, George Xie wrote:
>>>     > > hi all:
>>>     > >
>>>     > > Squid version: 3.5.23-5+deb9u1
>>>     > > Docker version 18.09.3, build 774a1f4
>>>     > > Linux instance-4 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27)
>>>     > > x86_64 GNU/Linux
>>>     > >
>>>     > > I have the following squid config:
>>>     > >
>>>     > >
>>>     > >     http_port 127.0.0.1:3128 <http://127.0.0.1:3128>
>>>     > >     cache deny all
>>>     > >     access_log none
>>>     > >
>>>     > What is it exactly that you think this is doing in regards to Squid
>>>     > memory needs?
>>>     >
>>>
>>>
>>> sorry, I don't get your quest.
>>>
>>
>> I was asking to see what you were thinking was going on with those settings.
>>
>> As Alex already pointed out the "cache deny all" does not reduce memory
>> needs of Squid in any way. It just makes 256MB of that RAM become
>> pointless allocating.
>>
>> So, if you actually do not want the proxy caching anything, then
>> disabling the cache_mem (set it to 0 as per Alex response) would be the
>> best choice of action before you go any further.
>>
>> Or if you *do* want caching, and were trying to disable it for testing
>> the memory issue. Then your test was wrong, and produces incorrect
>> conclusion. Just reducing cache_mem would be best for this case - set it
>> to a value that should reasonably fit this container and see if the
>> proxy runs okay.
>>
>>
>> ...
>>>     > >
>>>     > > it appears that squid (or glibc) tries to allocate 392m memory,
>>>     which is
>>>     > > larger than host free memory 370m.
>>>     > > but I guess squid don't need that much memory, I have another
>>>     running
>>>     > > squid instance, which only uses < 200m memory.
>>>     > No doubt it is configured to use less memory. For example by reducing
>>>     > the default memory cache size.
>>>     >
>>>
>>>
>>> that running squid instance has the same config.
>>>
>>
>> Then something odd is going on between the two. They should indeed have
>> had the same behaviour (either work or same error).
>>
>> Whatever the issue is it is being triggered by the large blocks of RAM
>> allocated by a default Squid. The easiest to modify is the cache_mem.
>>
>>
>>>
>>>     > > the oddest thing is if I run squid on the host (also Debian 9)
>>>     directly,
>>>     > > not in the container, squid could start and run as normal.
>>>     > >
>>>     > Linux typically allows RAM over-allocation. Which works okay so
>>>     long as
>>>     > there is sufficient swap space and there is time between memory
>>>     usage to
>>>     > do the swap in/out process.
>>>     > Amos
>>>
>>>
>>> swap is disabled in the host server, so do in the container.
>>>
>>> after all, I wonder why squid would try to claim 392m memory if don't
>>> need that much.
>>>
>>
>> Squid thinks it does. All client traffic is denied being cached by that
>> "deny all". BUT ... there are internally generated items which also use
>> cache. So there is 256MB default RAM cache allocated and only those few
>> small things being put in it.
>>
>> You could set it to '0' or to some small value and the allocation size
>> should go down accordingly.
>>
>>
>> That said, every bit of client traffic headed towards the proxy uses
>> memory of volatile amount and at peak times it may need to allocate
>> large blocks.
>>
>> So disabling swap entirely on the server is not a great idea. It just
>> moves the error and shutdown to happen at peak traffic times when it is
>> least wanted.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From russel_mcdonald at swbell.net  Wed Mar  6 20:00:21 2019
From: russel_mcdonald at swbell.net (Russel McDonald)
Date: Wed, 6 Mar 2019 20:00:21 +0000 (UTC)
Subject: [squid-users] Not Implemented error returned I believe due to
	chunking
References: <154569489.1199303.1551902421332.ref@mail.yahoo.com>
Message-ID: <154569489.1199303.1551902421332@mail.yahoo.com>

Hi,
On Windows I'm getting an error returned when attempting to upload a file to an Amazon AWS S3 bucket, using either the WinSCP app or just using the S3 upload interface via browser, pointing to my squid proxy.I can download just fine, but not upload. ECAP is enabled for both the request and the response.
Here is the error:A header you provided implies functionality that is not implementedExtra Details: Header: Transfer-Encoding, RequestId: 3ED7C576056435A0, HostId: SU/9a+qN5rL79STErKgziygc2A34jLhWJ110qbUB1fele67wIZ5thMmgtNnKczBFVxuOy810HB4=
If I instead only specify to pass the response to my adapter via ECAP then the issue goes away.
So I captured the HTTP header for the success case and the failure case and narrowed it down the the failure case not specifying the length and specifying ",chunking" at the end. I assume if the request is passed to the adapter then the header has to be constructed at a time when the length of the file contents is not known? Hence has to use chunking.
I've indeed read a few forum links about just this issue, but confused as to whether chunking is supposed to finally work for 3.5.28 that I have or not. I compared to 4.x source and it looked basically the same.
I'll be needing to upload large files, too large to fit in memory, hence pretty sure chunking will be a requirement.
Is this is known issue? Any thoughts on solving it?
Thanks! I'm a Windows kernel filter driver developer so working open source is new to me. But with this forum's help I've had quite a lot of success. So thanks!
Russel McDonald
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190306/ad1e98c3/attachment.htm>

From rousskov at measurement-factory.com  Wed Mar  6 20:21:47 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Mar 2019 13:21:47 -0700
Subject: [squid-users] Not Implemented error returned I believe due to
 chunking
In-Reply-To: <154569489.1199303.1551902421332@mail.yahoo.com>
References: <154569489.1199303.1551902421332.ref@mail.yahoo.com>
 <154569489.1199303.1551902421332@mail.yahoo.com>
Message-ID: <3de2ca31-bc99-0a8b-c1b3-f9f89827948f@measurement-factory.com>

On 3/6/19 1:00 PM, Russel McDonald wrote:

> I've indeed read a few forum links about just this issue, but confused
> as to whether chunking is supposed to finally work for 3.5.28

Chunked requests should be supported in all modern Squid versions,
including v3.5.28. There are some corner cases where dechunking code
fails, but they are very unlikely to affect you. I do not know what
forum links you have read, but I do not recall any general long-standing
problems with chunking support in modern Squids.


> If I instead only specify to pass the response to my adapter via
> ECAP then the issue goes away. So I captured the HTTP header for the 
> success case and the failure case and narrowed it down the the 
> failure case not specifying the length and specifying ",chunking" at
> the end.

Many adapters strip Content-Length header and effectively switch to
chunked encoding because it is easier and cheaper to adapt message
bodies that way. Does your eCAP adapter remove the Content-Length header
(possibly replacing it with Transfer-Encoding:chunked)? Such actions are
OK from HTTP point of view, but they may expose lack of chunked request
support at the _origin server_.


> I assume if the request is passed to the adapter then the header has
> to be constructed at a time when the length of the file contents is
> not known? Hence has to use chunking.

Squid does not "construct" that request header from scratch -- it
essentially forwards the request header sent by the client. If I
interpret your description correctly, the client sends a
Content-Length:N header, so the size of the request body (i.e. N bytes)
is known to Squid and, hence, to the REQMOD eCAP adapter in advance.

It is possible that your adapter strips that Content-Length header, and
then the origin server complains because it cannot handle chunked
requests. If that is what's happening, then you will need to seek
support from the adapter makers.

Alex.


From commercials24 at yahoo.de  Wed Mar  6 22:33:20 2019
From: commercials24 at yahoo.de (steven)
Date: Wed, 6 Mar 2019 23:33:20 +0100
Subject: [squid-users] icap example server not working
In-Reply-To: <3de2ca31-bc99-0a8b-c1b3-f9f89827948f@measurement-factory.com>
References: <154569489.1199303.1551902421332.ref@mail.yahoo.com>
 <154569489.1199303.1551902421332@mail.yahoo.com>
 <3de2ca31-bc99-0a8b-c1b3-f9f89827948f@measurement-factory.com>
Message-ID: <6a5b3605-60aa-5dfc-7acd-821c026c783f@yahoo.de>

hi,


after going trough rfc3507 (icap) im not really sure how a icap server 
is supposed to work together with squid, for example the python icap 
servers that are available via github have method names like:


example_OPTIONS, example_REQMOD, example_RESPMOD

yara_OPTIONS_, yara_....



or the example on this page: https://github.com/netom/pyicap

echo_OPTIONS, echo_RESPMOD


with:

icap_service service_resp respmod_precache bypass=0 
icap://127.0.0.1:1344/respmod
adaptation_access service_resp allow all

127.0.0.1 - - [06/b'Mar'/2019 23:22:58] respmod_OPTIONS not found
127.0.0.1 - - [06/b'Mar'/2019 23:22:58] code 404, message b'Not Found'


with:

icap_service service_resp respmod_precache bypass=0 
icap://127.0.0.1:1344/echo
adaptation_access service_resp allow all

----------------------------------------
Exception happened during processing of request from ('127.0.0.1', 46458)
Traceback (most recent call last):
 ? File "/usr/lib/python3.7/socketserver.py", line 650, in 
process_request_thread
 ??? self.finish_request(request, client_address)
 ? File "/usr/lib/python3.7/socketserver.py", line 360, in finish_request
 ??? self.RequestHandlerClass(request, client_address, self)
 ? File "/usr/lib/python3.7/socketserver.py", line 720, in __init__
 ??? self.handle()
 ? File 
"/home/julius/code/python/icap/venv/lib/python3.7/site-packages/pyicap.py", 
line 443, in handle
 ??? self.handle_one_request()
 ? File 
"/home/julius/code/python/icap/venv/lib/python3.7/site-packages/pyicap.py", 
line 494, in handle_one_request
 ??? method()
 ? File "icaptest.py", line 22, in echo_OPTIONS
 ??? self.send_headers(False)
 ? File 
"/home/julius/code/python/icap/venv/lib/python3.7/site-packages/pyicap.py", 
line 326, in send_headers
 ??? icap_header_str += k + b': ' + v + b'\r\n'
TypeError: can only concatenate str (not "bytes") to str
----------------------------------------




squid config:

icap_enable on

icap_preview_enable off
icap_send_client_ip on
icap_send_client_username on
icap_service service_req reqmod_precache bypass=1 
icap://127.0.0.1:1344/respmod
adaptation_access service_req allow all
#icap_service service_resp respmod_precache bypass=0 
icap://127.0.0.1:1344/response
#adaptation_access service_resp allow all




why is this simple example not working?



From rousskov at measurement-factory.com  Thu Mar  7 03:59:03 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Mar 2019 20:59:03 -0700
Subject: [squid-users] icap example server not working
In-Reply-To: <6a5b3605-60aa-5dfc-7acd-821c026c783f@yahoo.de>
References: <154569489.1199303.1551902421332.ref@mail.yahoo.com>
 <154569489.1199303.1551902421332@mail.yahoo.com>
 <3de2ca31-bc99-0a8b-c1b3-f9f89827948f@measurement-factory.com>
 <6a5b3605-60aa-5dfc-7acd-821c026c783f@yahoo.de>
Message-ID: <4881dbc1-46ba-229a-8835-765fb75013b5@measurement-factory.com>

On 3/6/19 3:33 PM, steven wrote:

> after going trough rfc3507 (icap) im not really sure how a icap server
> is supposed to work together with squid,

At the protocol level, an ICAP server works pretty much how RFC 3507
describes it. At the ICAP server software/application level, each server
"works" in its own way, irrelevant to Squid.


> the example on this page: https://github.com/netom/pyicap

> 127.0.0.1 - - [06/b'Mar'/2019 23:22:58] respmod_OPTIONS not found
> 127.0.0.1 - - [06/b'Mar'/2019 23:22:58] code 404, message b'Not Found'

I know nothing about that ICAP framework, but you probably need to
follow its README directions and define the respmod_OPTIONS method
(among other things):

> You can use a framework by importing stuff from the pyicap package,
> extending the protocol handler class and starting the server, passing
> your handler to it: ...

In other words, that specific ICAP code is not a ready-to-use ICAP
server but a _framework_ for building an ICAP server (that does what
_you_ want it to do).

If you need an ICAP server to do X, then you should either find an
existing ICAP server that already does X or build a new ICAP server that
does X. The latter should be accomplished either by using an existing
ICAP library/framework (like the one you found above) or by writing a
loadable module/plugin/adapter (that does X) for an existing ICAP server
that accepts such adapters.

Squid wiki has a bit more info about ICAP:

* https://wiki.squid-cache.org/SquidFaq/ContentAdaptation#ICAP
* https://wiki.squid-cache.org/Features/ICAP

The first page also discusses (and compares) various ICAP alternatives.


HTH,

Alex.


> icap_service service_resp respmod_precache bypass=0
> icap://127.0.0.1:1344/echo
> adaptation_access service_resp allow all
> 
> ----------------------------------------
> Exception happened during processing of request from ('127.0.0.1', 46458)
> Traceback (most recent call last):
> ? File "/usr/lib/python3.7/socketserver.py", line 650, in
> process_request_thread
> ??? self.finish_request(request, client_address)
> ? File "/usr/lib/python3.7/socketserver.py", line 360, in finish_request
> ??? self.RequestHandlerClass(request, client_address, self)
> ? File "/usr/lib/python3.7/socketserver.py", line 720, in __init__
> ??? self.handle()
> ? File
> "/home/julius/code/python/icap/venv/lib/python3.7/site-packages/pyicap.py",
> line 443, in handle
> ??? self.handle_one_request()
> ? File
> "/home/julius/code/python/icap/venv/lib/python3.7/site-packages/pyicap.py",
> line 494, in handle_one_request
> ??? method()
> ? File "icaptest.py", line 22, in echo_OPTIONS
> ??? self.send_headers(False)
> ? File
> "/home/julius/code/python/icap/venv/lib/python3.7/site-packages/pyicap.py",
> line 326, in send_headers
> ??? icap_header_str += k + b': ' + v + b'\r\n'
> TypeError: can only concatenate str (not "bytes") to str
> ----------------------------------------
> 
> 
> 
> 
> squid config:
> 
> icap_enable on
> 
> icap_preview_enable off
> icap_send_client_ip on
> icap_send_client_username on
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/respmod
> adaptation_access service_req allow all
> #icap_service service_resp respmod_precache bypass=0
> icap://127.0.0.1:1344/response
> #adaptation_access service_resp allow all
> 
> 
> 
> 
> why is this simple example not working?
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dkanejs at gmail.com  Thu Mar  7 12:17:18 2019
From: dkanejs at gmail.com (dkanejs)
Date: Thu, 7 Mar 2019 06:17:18 -0600 (CST)
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
Message-ID: <1551961038560-0.post@n4.nabble.com>

Foreword

I'm by no means an expert so please bear with me...

I have seen many questions about this but they are all for Squid 3 and none
of the configurations work for Squid 4.

I have also tried using (with adjustment for Squid 4) answers from questions
on here and serverfault, etc but without success.
What I'm trying to do

I'm trying to create a transparent (requires no client configuration) Squid
proxy for HTTP and HTTPS.

In short, I want to whitelist specific domains on both HTTP and HTTPS.
Details

- Building / Running in AWS
- Ubuntu 18.04
- Instance has Source/Destination check disabled
- Private subnets route table points 0.0.0.0/0 to EC2 Instance
- Squid 4.6 compiled with SSL support options: 

Current Configuration


iptables


Results

Using the proxy via a host in the private subnet for HTTP:


Using the proxy via a host in the private subnet for HTTPS:


Happy to share more information / configuration if I've missed something
important.

If anyone can help shed some light on what I'm doing wrong, (or share a
working configuration to put me out of my misery!), it would be much
appreciated thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Thu Mar  7 12:21:54 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 7 Mar 2019 13:21:54 +0100
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
In-Reply-To: <1551961038560-0.post@n4.nabble.com>
References: <1551961038560-0.post@n4.nabble.com>
Message-ID: <201903071321.54569.Antony.Stone@squid.open.source.it>

On Thursday 07 March 2019 at 13:17:18, dkanejs wrote:

> I'm trying to create a transparent (requires no client configuration) Squid
> proxy for HTTP and HTTPS.
> 
> In short, I want to whitelist specific domains on both HTTP and HTTPS.
> Details
> 
> - Building / Running in AWS
> - Ubuntu 18.04
> - Instance has Source/Destination check disabled
> - Private subnets route table points 0.0.0.0/0 to EC2 Instance
> - Squid 4.6 compiled with SSL support options:

I might be good to tell us exactly what options you have used to compile Squid 
for yourself - this can be important, especially with SSL stuff.

> Current Configuration
> 
> iptables

What?

> Results
> 
> Using the proxy via a host in the private subnet for HTTP:
> 
> Using the proxy via a host in the private subnet for HTTPS:

Well, what are the results?  What waorks, what doesn't?

> Happy to share more information / configuration if I've missed something
> important.

Er, tell us what you're trying to do and what isn't working, and what you see 
in the Squid log files for those requests.

Also, helpful to post your squid.conf without comments or blank lines, so we 
can see how you're trying to set this up.


Regards,


Antony.

-- 
Bill Gates has personally assured the Spanish Academy that he will never allow 
the upside-down question mark to disappear from Microsoft word-processing 
programs, which must be reassuring for millions of Spanish-speaking people, 
though just a piddling afterthought as far as he's concerned.

 - Lynne Truss, "Eats, Shoots and Leaves"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From dkanejs at gmail.com  Thu Mar  7 12:35:12 2019
From: dkanejs at gmail.com (dkanejs)
Date: Thu, 7 Mar 2019 06:35:12 -0600 (CST)
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
In-Reply-To: <201903071321.54569.Antony.Stone@squid.open.source.it>
References: <1551961038560-0.post@n4.nabble.com>
 <201903071321.54569.Antony.Stone@squid.open.source.it>
Message-ID: <1551962112557-0.post@n4.nabble.com>

Thanks for the reply and apologies my post didn't include the HTML fragments:

Configuration:

./configure \
    --enable-ssl \
    --enable-ssl-crtd \
    --with-openssl \
    --disable-arch-native \
    --prefix=/usr \
    --localstatedir=/var \
    --sysconfdir=/etc/squid \
    --libexecdir=/usr/lib/squid \
    --datadir=/usr/share/squid \
    --with-default-user=proxy \
    --with-logdir=/var/log/squid \
    --with-pidfile=/var/run/squid.pid

Squid configuration:

visible_hostname squid
http_port 3128
acl whitelist dstdomain .example.com
http_access allow whitelist
https_port 3129 cert=/etc/squid/squid.pem
options=NO_SSLv2,NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
ssl-bump intercept
acl SSL_port port 443
http_access allow SSL_port
acl CONNECT method CONNECT
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump peek step2 whitelist
ssl_bump splice step3 whitelist
ssl_bump terminate step2 all
http_access deny all
coredump_dir /var/cache/squid/

iptables:

iptables -t nat -I PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3128
iptables -t nat -I PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3129

Access log:

1551954200.914     54 10.0.1.166 NONE_ABORTED/200 0 CONNECT
93.184.216.34:443 - HIER_NONE/- -
1551954214.370      0 10.0.1.166 NONE/400 3810 GET / - HIER_NONE/- text/html
1551954217.223      0 10.0.1.166 NONE/400 3810 GET / - HIER_NONE/- text/html
1551954256.558      0 10.0.1.166 NONE/400 3810 GET / - HIER_NONE/- text/html
1551954261.638      0 10.0.1.166 NONE/400 3810 GET / - HIER_NONE/- text/html
1551954273.516    215 10.0.1.166 NONE_ABORTED/200 0 CONNECT
93.184.216.34:443 - HIER_NONE/- -
1551954391.304      1 185.59.221.44 NONE_ABORTED/200 0 CONNECT
10.0.0.151:443 - HIER_NONE/- -
1551954395.346      0 185.59.221.44 NONE_ABORTED/200 0 CONNECT
10.0.0.151:443 - HIER_NONE/- -
1551954398.938      0 185.59.221.44 NONE_ABORTED/200 0 CONNECT
10.0.0.151:443 - HIER_NONE/- -

Thanks again,
David



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Mar  7 13:34:14 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Mar 2019 02:34:14 +1300
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
In-Reply-To: <1551962112557-0.post@n4.nabble.com>
References: <1551961038560-0.post@n4.nabble.com>
 <201903071321.54569.Antony.Stone@squid.open.source.it>
 <1551962112557-0.post@n4.nabble.com>
Message-ID: <a949d229-40ae-c97b-9c81-7fb30883285f@treenet.co.nz>

On 8/03/19 1:35 am, dkanejs wrote:
> Thanks for the reply and apologies my post didn't include the HTML fragments:
> 
> Configuration:
> 
> ./configure \
>     --enable-ssl \
>     --enable-ssl-crtd \
>     --with-openssl \
>     --disable-arch-native \
>     --prefix=/usr \
>     --localstatedir=/var \
>     --sysconfdir=/etc/squid \
>     --libexecdir=/usr/lib/squid \
>     --datadir=/usr/share/squid \
>     --with-default-user=proxy \
>     --with-logdir=/var/log/squid \
>     --with-pidfile=/var/run/squid.pid
> 
> Squid configuration:
> 
> visible_hostname squid
> http_port 3128
> acl whitelist dstdomain .example.com
> http_access allow whitelist

You are missing the default security protections against DoS and some
other attack types. Please leave those Safe_ports and SSL_ports access
lines and place your custom rules after them.


> https_port 3129 cert=/etc/squid/squid.pem
> options=NO_SSLv2,NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET 
> cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> ssl-bump intercept

The NAT 'intercept' mode flag needs to be second, right after the port
number.

The 'ssl-bump' flag should go before the cert= option so the right types
of cert are loaded. Without this ordering Squid-4 cannot warn you about
cert type errors (if any).

The "NO_SSLv2" is invalid. As of Squid-4 all options relating to SSLv2
are no longer supported in any way.


> acl SSL_port port 443
> http_access allow SSL_port
> acl CONNECT method CONNECT

"CONNECT" ACL is now a built-in. You do not have to define it.

> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1 all
> ssl_bump peek step2 whitelist
> ssl_bump splice step3 whitelist
> ssl_bump terminate step2 all

The use of "all" ACL in the above lines does nothing but confuse people.

Please also be aware the 'whitelist' ACL will not match reliably in TLS
handshake because TLS does not have HTTP message URLs - thus no URL
domain name.

That means you should expect to see only terminated TLS handshakes with
this config. Anything actually being accepted and responded to would be
the anomaly.

I think what you are needing is probably this:

 ssl_bump peek all
 acl tls_whitelist ssl::server_name .example.com
 ssl_bump splice step3 tls_whitelist
 ssl_bump terminate all



Amos


From dkanejs at gmail.com  Thu Mar  7 16:10:17 2019
From: dkanejs at gmail.com (dkanejs)
Date: Thu, 7 Mar 2019 10:10:17 -0600 (CST)
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
In-Reply-To: <a949d229-40ae-c97b-9c81-7fb30883285f@treenet.co.nz>
References: <1551961038560-0.post@n4.nabble.com>
 <201903071321.54569.Antony.Stone@squid.open.source.it>
 <1551962112557-0.post@n4.nabble.com>
 <a949d229-40ae-c97b-9c81-7fb30883285f@treenet.co.nz>
Message-ID: <1551975017916-0.post@n4.nabble.com>

Thanks for your analysis Amos.

As you can tell I'm still figuring this stuff out.

HTTPS is working now but HTTP is not, not sure what I'm doing wrong.

I tried my best to understand your comments and now have the following
config:


squid.conf


visible_hostname squid

acl localnet src 10.0.0.0/8

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

http_access deny !Safe_ports

http_access allow localhost manager
http_access deny manager

acl whitelist dstdomain .example.com
http_access allow whitelist

http_access allow localnet
http_access allow localhost
http_access deny all

http_port 3128
https_port 3129 intercept ssl-bump cert=/etc/squid/squid.pem

ssl_bump peek all
acl tls_whitelist ssl::server_name .example.com
ssl_bump splice tls_whitelist
ssl_bump terminate all

coredump_dir /var/cache/squid


Thanks and let me know if i have misunderstood your amendments.

Thanks again,
David



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Thu Mar  7 18:56:44 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 7 Mar 2019 19:56:44 +0100
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
In-Reply-To: <1551975017916-0.post@n4.nabble.com>
References: <1551961038560-0.post@n4.nabble.com>
 <201903071321.54569.Antony.Stone@squid.open.source.it>
 <1551962112557-0.post@n4.nabble.com>
 <a949d229-40ae-c97b-9c81-7fb30883285f@treenet.co.nz>
 <1551975017916-0.post@n4.nabble.com>
Message-ID: <20190307185644.GA15959@fantomas.sk>

[B
>Thanks for your analysis Amos.
>
>As you can tell I'm still figuring this stuff out.
>
>HTTPS is working now but HTTP is not, not sure what I'm doing wrong.
>
>I tried my best to understand your comments and now have the following
>config:

>http_port 3128
>https_port 3129 intercept ssl-bump cert=/etc/squid/squid.pem

if you intercept/redirect http to port 3128, it must be configured as
"intercept" too.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
They that can give up essential liberty to obtain a little temporary
safety deserve neither liberty nor safety. -- Benjamin Franklin, 1759


From dkanejs at gmail.com  Fri Mar  8 02:49:57 2019
From: dkanejs at gmail.com (dkanejs)
Date: Thu, 7 Mar 2019 20:49:57 -0600 (CST)
Subject: [squid-users] Squid 4.6 Transparent HTTP & HTTPS Proxy
In-Reply-To: <20190307185644.GA15959@fantomas.sk>
References: <1551961038560-0.post@n4.nabble.com>
 <201903071321.54569.Antony.Stone@squid.open.source.it>
 <1551962112557-0.post@n4.nabble.com>
 <a949d229-40ae-c97b-9c81-7fb30883285f@treenet.co.nz>
 <1551975017916-0.post@n4.nabble.com> <20190307185644.GA15959@fantomas.sk>
Message-ID: <1552013397331-0.post@n4.nabble.com>

>if you intercept/redirect http to port 3128, it must be configured as
"intercept" too.

Spot on cheers!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From commercials24 at yahoo.de  Sun Mar 10 19:26:13 2019
From: commercials24 at yahoo.de (steven)
Date: Sun, 10 Mar 2019 20:26:13 +0100
Subject: [squid-users] icap not answering
In-Reply-To: <d093dc56-8064-f94e-a867-7ad6626e4570@treenet.co.nz>
References: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>
 <f132eb9f-e448-ebef-e1dc-b27359a23d6c@urlfilterdb.com>
 <12476d4e-9f68-959b-dce0-647cfb9b1704@yahoo.de>
 <d093dc56-8064-f94e-a867-7ad6626e4570@treenet.co.nz>
Message-ID: <b4990be1-cff3-b8d1-f322-970efb903228@yahoo.de>


On 05.03.19 06:13, Amos Jeffries wrote:
> On 5/03/19 12:10 pm, steven wrote:
>> Ah thank you for that clarification, the python icap servers i tested so
>> far are not very promissing but at least theres a connection now.
>>
>> sadly squid does not allow http access at all, only https access.
>>
> Er, that would be because the only http_port you have is configured with
> 'accl' - making it a reverse-proxy port. But you do not have any
> cache_peer configured to handle that type of traffic.
>
>
> So, is there any particular reason you have that port receiving 'accel'
> / reverse-proxy mode traffic?
>   If not remove that mode flag and things should all work for HTTP too.
>

removed the accel mode but still no luck with http, when opening the adress:

http://squid-web-proxy-cache.1019090.n4.nabble.com/http-port-with-quot-transparent-quot-or-quot-intercept-quot-td4677133.html


The following error was encountered while trying to retrieve the URL: 
/http-port-with-quot-transparent-quot-or-quot-intercept-quot-td4677133.html 
<http://squid-web-proxy-cache.1019090.n4.nabble.com/http-port-with-quot-transparent-quot-or-quot-intercept-quot-td4677133.html>


invalid url




in this tutorial:

https://www.reddit.com/r/sysadmin/comments/a67hly/squid_proxy_a_short_guide_forward_transparent/


the guy uses two ports for http like this:

|http_port 3128 # Listen on this HTTP port, intercepting requests 
http_port 3129 intercept and then with iptables he redirects 80 to port 
3129 which does not work here :( export 
http_proxy=http://192.168.10.215:3140 && wget google.de # im using 3140 
as intercept port. config at the end. --2019-03-10 20:20:56-- 
http://google.de/ Connecting to 192.168.10.215:3140... connected. Proxy 
request sent, awaiting response... 403 Forbidden 2019-03-10 20:20:56 
ERROR 403: Forbidden. |

cache.log entry:

2019/03/10 20:16:20 kid1| WARNING: Forwarding loop detected for:
GET / HTTP/1.1
User-Agent: Wget/1.19.4 (linux-gnu)
Accept: */*
Accept-Encoding: identity
Via: 1.1 backup (squid/4.4)
Cache-Control: max-age=259200
Connection: keep-alive
Host: google.de



and with:

export http_proxy=http://192.168.10.215:3129 && wget google.de

no cache .log entry, wget output:

--2019-03-10 20:22:42--? (try: 2)? http://google.de/
Connecting to 192.168.10.215:3129... connected.
Proxy request sent, awaiting response... No data received.
Retrying.


why does my client get a 403?






grep -v '#' squid.conf


icap_enable off
icap_preview_enable off
icap_send_client_ip on
icap_send_client_username on
icap_service service_req reqmod_precache bypass=1 
icap://127.0.0.1:1344/request
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0 
icap://127.0.0.1:1344/response
adaptation_access service_resp allow all
acl localnet src 192.168.10.0/24
http_access allow localnet
coredump_dir /var/spool/squid
refresh_pattern ^ftp:??????? 1440??? 20%??? 10080
refresh_pattern ^gopher:??? 1440??? 0%??? 1440
refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
refresh_pattern .??????? 0??? 20%??? 4320
http_port 3128
http_port 3140 intercept
https_port 3129 ssl-bump intercept generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db 
-M 4MB
acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190310/a6106f66/attachment.htm>

From squid3 at treenet.co.nz  Mon Mar 11 10:40:05 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Mar 2019 23:40:05 +1300
Subject: [squid-users] icap not answering
In-Reply-To: <b4990be1-cff3-b8d1-f322-970efb903228@yahoo.de>
References: <314fe334-d317-351f-062a-6caab4e8fc03@yahoo.de>
 <f132eb9f-e448-ebef-e1dc-b27359a23d6c@urlfilterdb.com>
 <12476d4e-9f68-959b-dce0-647cfb9b1704@yahoo.de>
 <d093dc56-8064-f94e-a867-7ad6626e4570@treenet.co.nz>
 <b4990be1-cff3-b8d1-f322-970efb903228@yahoo.de>
Message-ID: <6495d662-c132-dd07-8859-01defae41691@treenet.co.nz>

On 11/03/19 8:26 am, steven wrote:
> 
> On 05.03.19 06:13, Amos Jeffries wrote:
>> On 5/03/19 12:10 pm, steven wrote:
>>> Ah thank you for that clarification, the python icap servers i tested so
>>> far are not very promissing but at least theres a connection now.
>>>
>>> sadly squid does not allow http access at all, only https access.
>>>
>> Er, that would be because the only http_port you have is configured with
>> 'accl' - making it a reverse-proxy port. But you do not have any
>> cache_peer configured to handle that type of traffic.
>>
>>
>> So, is there any particular reason you have that port receiving 'accel'
>> / reverse-proxy mode traffic?
>>  If not remove that mode flag and things should all work for HTTP too.
>>
> 
> removed the accel mode but still no luck with http, when opening the adress:
> 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/http-port-with-quot-transparent-quot-or-quot-intercept-quot-td4677133.html
> 
> 
> The following error was encountered while trying to retrieve the URL:
> /http-port-with-quot-transparent-quot-or-quot-intercept-quot-td4677133.html
> 

Ah, that is an origin-form URL.


> 
> in this tutorial:
> 
> https://www.reddit.com/r/sysadmin/comments/a67hly/squid_proxy_a_short_guide_forward_transparent/
> 
> 
> the guy uses two ports for http like this:
> 
> |http_port 3128 # Listen on this HTTP port, intercepting requests
> http_port 3129 intercept and then with iptables he redirects 80 to port
> 3129 which does not work here :( export

Which should work fine ... provided the right type of traffic is passed
to each port.


> http_proxy=http://192.168.10.215:3140 && wget google.de # im using 3140
> as intercept port. config at the end. --2019-03-10 20:20:56--
> http://google.de/ Connecting to 192.168.10.215:3140... connected. Proxy
> request sent, awaiting response... 403 Forbidden 2019-03-10 20:20:56
> ERROR 403: Forbidden. |
> 

Hmm. You keep mixing port modes and traffic types.


Port 3128, 80 and 443 all have different traffic syntax and handling
requirements. The mode flags tell Squid which syntax is expected and
valid arriving at that port. Default mode is forward/explicit-proxy so
there is no flag for that mode/syntax.


Ports with 'intercept' flags must *only* have traffic passed to them
from the OS NAT subsystem.

Clients should be connecting directly to the domain origin on port 80.
Do not configure them with any details about the proxy. Eg passing the
http_proxy environment variable is configuring wget to use an
explicit-proxy port.

Your wget test should be using port 3128 in that http_proxy= setting. Or
not using that setting at all for tests of the port 80 and port 443
traffic (which should be getting intercepted by NAT).


> 
> grep -v '#' squid.conf
> 
...

NP: You are missing the default security rules to protect against DoS
and other nasty attacks.


> http_access allow localnet
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:??????? 1440??? 20%??? 10080
> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
> refresh_pattern .??????? 0??? 20%??? 4320


> http_port 3128

Above port is for forward-proxy / explicit-proxy traffic. Clients need
to be explicitly configured to send traffic here, or instructed to by
response URLs generated by this proxy.

You do not have 'ssl-bump' so any TLS/SSL/HTTPS traffic from these
clients will go through in CONNECT tunnels without inspection.


> http_port 3140 intercept

Above port is for NAT intercepted port 80 traffic. Clients are
contacting HTTP origin servers directly.

There is no TLS/SSL/HTTPS traffic on this port. Attempts by the client
to Upgrade to non-HTTP protocols (including HTTPS) will be ignored.


> https_port 3129 ssl-bump intercept generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/myCA.pem


Above port is for NAT intercepted port 443 traffic, with SSL-Bump'ing.
Clients are contacting HTTPS origin servers directly.

There is no plain-text HTTP traffic on this port. Attempts by the client
to Upgrade to non-HTTPS protocols (including HTTP) will be ignored.

on_unsupported_protocol determines what happens to non-TLS traffic
arriving at this port. Internet requirements are that traffic is
rejected, though abuse of port 443 for sneaking other things through
this port is so popular it may not always be possible.



> sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db
> -M 4MB
> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> ssl_bump bump all
> 

NP: SSL-Bump'ing operations are performed on all traffic without
knowledge of the server X.509 certificate details. This introduces
TLS/SSL errors and several classes of security vulnerability.

Amos


From felipeapolanco at gmail.com  Mon Mar 11 19:53:13 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Mon, 11 Mar 2019 15:53:13 -0400
Subject: [squid-users] How to extract decrypted traffic for further analysis
	using Snort?
Message-ID: <CADcj3=4XrjAB+EBybMDohA8DvnCT_eUeDtceBP9noZ4LgOf-mA@mail.gmail.com>

Hi,

I'm trying to find a way to get the HTTP traffic analysed after being
decrypted, by using Snort.

Does someone know how to do this? I can redirect IP traffic with regular
HTTP into Snort but I haven't found a way inside squid to do the same.

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190311/08aae281/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Mar 11 19:58:52 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 11 Mar 2019 20:58:52 +0100
Subject: [squid-users] How to extract decrypted traffic for further
	analysis using Snort?
In-Reply-To: <CADcj3=4XrjAB+EBybMDohA8DvnCT_eUeDtceBP9noZ4LgOf-mA@mail.gmail.com>
References: <CADcj3=4XrjAB+EBybMDohA8DvnCT_eUeDtceBP9noZ4LgOf-mA@mail.gmail.com>
Message-ID: <201903112058.52216.Antony.Stone@squid.open.source.it>

On Monday 11 March 2019 at 20:53:13, Felipe Arturo Polanco wrote:

> Hi,
> 
> I'm trying to find a way to get the HTTP traffic analysed after being
> decrypted, by using Snort.
> 
> Does someone know how to do this? I can redirect IP traffic with regular
> HTTP into Snort but I haven't found a way inside squid to do the same.

How about https://wiki.squid-cache.org/Features/ICAP ?


Antony.

-- 
Please apologise my errors, since I have a very small device.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ceo at teo-en-ming-corp.com  Mon Mar 11 22:48:12 2019
From: ceo at teo-en-ming-corp.com (Turritopsis Dohrnii Teo En Ming)
Date: Mon, 11 Mar 2019 22:48:12 +0000
Subject: [squid-users] How Do I Implement Reverse Proxy Using Squid Version
	4.6?
Message-ID: <SG2PR01MB264687A637B385DABA2748CF87480@SG2PR01MB2646.apcprd01.prod.exchangelabs.com>

Good morning from Singapore,

How do I implement reverse proxy using Squid version 4.6?

Thank you.

===BEGIN EMAIL SIGNATURE===

The Gospel for all Targeted Individuals (TIs):

[The New York Times] Microwave Weapons Are Prime Suspect in Ills of U.S. Embassy Workers

Link: https://www.nytimes.com/2018/09/01/science/sonic-attack-cuba-microwave.html

********************************************************************************************

Singaporean Mr. Turritopsis Dohrnii Teo En Ming's Academic Qualifications as at 14 Feb 2019

[1] https://tdtemcerts.wordpress.com/

[2] https://tdtemcerts.blogspot.sg/

[3] https://www.scribd.com/user/270125049/Teo-En-Ming

===END EMAIL SIGNATURE===
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190311/49994f4f/attachment.htm>

From rousskov at measurement-factory.com  Mon Mar 11 23:53:52 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 11 Mar 2019 17:53:52 -0600
Subject: [squid-users] How to extract decrypted traffic for further
 analysis using Snort?
In-Reply-To: <CADcj3=4XrjAB+EBybMDohA8DvnCT_eUeDtceBP9noZ4LgOf-mA@mail.gmail.com>
References: <CADcj3=4XrjAB+EBybMDohA8DvnCT_eUeDtceBP9noZ4LgOf-mA@mail.gmail.com>
Message-ID: <466758d1-5ff4-785c-3731-cae726d4c324@measurement-factory.com>

On 3/11/19 1:53 PM, Felipe Arturo Polanco wrote:

> I'm trying to find a way to get the HTTP traffic analysed?after being
> decrypted, by using Snort.
> 
> Does someone know how to do this? I can redirect IP traffic with regular
> HTTP into Snort but I haven't found a way inside squid to do the same.

I believe a similar question has been answered a few years ago, and that
answer is still valid. I will quote that exchange below for your
convenience, but the source is at
http://lists.squid-cache.org/pipermail/squid-users/2016-September/012689.html

Item 3 includes an ICAP option that Antony suggested on this thread, and
I know there are eCAP adapters that implement raw HTTP traffic emulation
mentioned there.

Alex.

On 09/26/2016, Alex Rousskov wrote:

> On 09/26/2016 05:41 AM, James Lay wrote:
>> So I'm going to try and get some visibility into tls traffic.  Not
>> concerned with the sslbumping of the traffic, but what I DON'T know what
>> to do is what to do with the traffic once it's decrypted.  This squid
>> machine runs IDS software as well, so my hope was to have the IDS
>> software listen to traffic that'd decrypted, but for the life of me I'm
>> not sure where to start.  Does squid pipe out a stream?  Or does the IDS
>> listen to a different "interface"?  Is this where ICAP comes in? 

> Squid-IDS integration is mostly independent from SslBump issues -- you
> integrate traffic analysis of plain and secure traffic similarly. Your
> options depend on IDS interfaces:
> 
> 1. If IDS is content with passively looking at something Squid can log
> (after the transaction is completed), then give IDS the logs (see
> access_log and logformat directives). This is what Amos recommended in
> his response. It is the best option if your IDS can use it.
> 
> 2. If IDS is content with reacting to something Squid can log while
> processing a message, then write or purchase a custom external ACL
> script. External ACL input can be customized just like the access log.
> 
> 3. If IDS needs access to message bodies, then use an ICAP or eCAP
> service to give IDS whole messages. You may have to write or purchase
> that service. How that service is going to give messages to IDS depends
> on IDS interfaces. Some IDSes have APIs while others listen to raw
> traffic (that a service can emulate and emit).


From squid3 at treenet.co.nz  Tue Mar 12 03:15:54 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Mar 2019 16:15:54 +1300
Subject: [squid-users] How Do I Implement Reverse Proxy Using Squid
 Version 4.6?
In-Reply-To: <SG2PR01MB264687A637B385DABA2748CF87480@SG2PR01MB2646.apcprd01.prod.exchangelabs.com>
References: <SG2PR01MB264687A637B385DABA2748CF87480@SG2PR01MB2646.apcprd01.prod.exchangelabs.com>
Message-ID: <47d43006-ec25-053c-94f0-ea68df5119de@treenet.co.nz>

On 12/03/19 11:48 am, Turritopsis Dohrnii Teo En Ming wrote:
> Good morning from Singapore,
> 
> How do I implement reverse proxy using Squid version 4.6?
> 


The same way it has been implemented since Squid-2.6.

<https://wiki.squid-cache.org/SquidFaq/ReverseProxy>


Amos


From eliezer at ngtech.co.il  Tue Mar 12 06:40:53 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Tue, 12 Mar 2019 08:40:53 +0200
Subject: [squid-users] How to extract decrypted traffic for further
	analysis using Snort?
In-Reply-To: <466758d1-5ff4-785c-3731-cae726d4c324@measurement-factory.com>
References: <CADcj3=4XrjAB+EBybMDohA8DvnCT_eUeDtceBP9noZ4LgOf-mA@mail.gmail.com>
 <466758d1-5ff4-785c-3731-cae726d4c324@measurement-factory.com>
Message-ID: <03fb01d4d89e$8a856f60$9f904e20$@ngtech.co.il>

+1

The main issue is websockets.
Since Squid doesn't have websockets related code implemented in a public code
the Squid instance would break more then one connection.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Tuesday, March 12, 2019 01:54
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] How to extract decrypted traffic for further analysis using Snort?

On 3/11/19 1:53 PM, Felipe Arturo Polanco wrote:

> I'm trying to find a way to get the HTTP traffic analysed after being
> decrypted, by using Snort.
> 
> Does someone know how to do this? I can redirect IP traffic with regular
> HTTP into Snort but I haven't found a way inside squid to do the same.

I believe a similar question has been answered a few years ago, and that
answer is still valid. I will quote that exchange below for your
convenience, but the source is at
http://lists.squid-cache.org/pipermail/squid-users/2016-September/012689.html

Item 3 includes an ICAP option that Antony suggested on this thread, and
I know there are eCAP adapters that implement raw HTTP traffic emulation
mentioned there.

Alex.

On 09/26/2016, Alex Rousskov wrote:

> On 09/26/2016 05:41 AM, James Lay wrote:
>> So I'm going to try and get some visibility into tls traffic.  Not
>> concerned with the sslbumping of the traffic, but what I DON'T know what
>> to do is what to do with the traffic once it's decrypted.  This squid
>> machine runs IDS software as well, so my hope was to have the IDS
>> software listen to traffic that'd decrypted, but for the life of me I'm
>> not sure where to start.  Does squid pipe out a stream?  Or does the IDS
>> listen to a different "interface"?  Is this where ICAP comes in? 

> Squid-IDS integration is mostly independent from SslBump issues -- you
> integrate traffic analysis of plain and secure traffic similarly. Your
> options depend on IDS interfaces:
> 
> 1. If IDS is content with passively looking at something Squid can log
> (after the transaction is completed), then give IDS the logs (see
> access_log and logformat directives). This is what Amos recommended in
> his response. It is the best option if your IDS can use it.
> 
> 2. If IDS is content with reacting to something Squid can log while
> processing a message, then write or purchase a custom external ACL
> script. External ACL input can be customized just like the access log.
> 
> 3. If IDS needs access to message bodies, then use an ICAP or eCAP
> service to give IDS whole messages. You may have to write or purchase
> that service. How that service is going to give messages to IDS depends
> on IDS interfaces. Some IDSes have APIs while others listen to raw
> traffic (that a service can emulate and emit).
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From leomessi983 at yahoo.com  Tue Mar 12 14:57:27 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Tue, 12 Mar 2019 14:57:27 +0000 (UTC)
Subject: [squid-users] reply_header_access for Strict-Transport-Security
	doesn't work
References: <947947536.4137185.1552402647368.ref@mail.yahoo.com>
Message-ID: <947947536.4137185.1552402647368@mail.yahoo.com>

HiI compiled squid with this options:
./configure \
--with-openssl \
--enable-ssl-crtd \
--prefix=/usr \
--enable-linux-netfilter \
--with-netfilter-conntrack \
--exec-prefix=/usr \
--includedir=/usr/include \
--datadir=/usr/share/squid \
--libdir=/usr/lib64 \
--libexecdir=/usr/lib64/squid \
--localstatedir=/var \
--sysconfdir=/etc/squid/ \
--sharedstatedir=/var/lib/ \
--with-logdir=/var/log/squid/ \
--enable-ltdl-convenience \
--enable-http-violationsbut when i use "request_header_access Strict-Transport-Security deny all" in my squid.conf its doesnt work?
What is wrong?I want to block https request and show block page for them,but for some sites like bing.com or google.com i got "HSTS Error" in my client!!What can i do?
?
Tanx all

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190312/4a23f4be/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar 12 16:45:54 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Mar 2019 05:45:54 +1300
Subject: [squid-users] reply_header_access for Strict-Transport-Security
 doesn't work
In-Reply-To: <947947536.4137185.1552402647368@mail.yahoo.com>
References: <947947536.4137185.1552402647368.ref@mail.yahoo.com>
 <947947536.4137185.1552402647368@mail.yahoo.com>
Message-ID: <769b32cc-4b7f-a5ff-f524-899e62270c23@treenet.co.nz>

On 13/03/19 3:57 am, leomessi983 wrote:
> Hi
> I compiled squid with this options:
> 
> 
> ./configure \
> --with-openssl \
> --enable-ssl-crtd \
> --prefix=/usr \
> --enable-linux-netfilter \
> --with-netfilter-conntrack \
> --exec-prefix=/usr \
> --includedir=/usr/include \
> --datadir=/usr/share/squid \
> --libdir=/usr/lib64 \
> --libexecdir=/usr/lib64/squid \
> --localstatedir=/var \
> --sysconfdir=/etc/squid/ \
> --sharedstatedir=/var/lib/ \
> --with-logdir=/var/log/squid/ \
> --enable-ltdl-convenience \
> --enable-http-violations
> 
> but when i use "request_header_access Strict-Transport-Security deny
> all" in my squid.conf its doesnt work?
> 
> What is wrong?


Strict-Transport-Security is an instruction from the server to the
client. That makes it a *response* header.


> I want to block https request and show block page for them,but for some
> sites like bing.com or google.com i got "HSTS Error" in my client!!
> What can i do?
> ?

Current Squid automatically erase that header to prevent HSTS breaking
web traffic. Where possible try to get clients to upgrade to Browsers
which have also dropped use of the feature.

Please be aware the HSTS header have a time period associated. Once a
client has received the header via *any* connection (including non-HTTP
connections). It will complaining about HSTS errors until the period
expires, maybe a bit longer.

That means you need to test it with clients that can erase their HSTS
information cache explicitly. Find a site with short HSTS timeout to
test with. OR wait up to 7 days between each test request.

Amos


From leomessi983 at yahoo.com  Tue Mar 12 19:10:41 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Tue, 12 Mar 2019 19:10:41 +0000 (UTC)
Subject: [squid-users] reply_header_access for Strict-Transport-Security
	doesn't work
References: <1537370603.4339880.1552417841401.ref@mail.yahoo.com>
Message-ID: <1537370603.4339880.1552417841401@mail.yahoo.com>

Hi Amos,tank you for your reply!
> Current Squid automatically erase that header to prevent HSTS breaking
> web traffic. Where possible try to get clients to upgrade to Browsers
> which have also dropped use of the feature.My clients have last Firefox browser but when i use squid and bumb sites for my block sites my client get HSTS error and squid doesn't automatically erase that header!Should i enable some feature or something else in squid configuration?
How can i know that squid does that automatically?!Is certificate and certificate type is important?!

Tank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190312/abf7c96f/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar 12 22:41:42 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Mar 2019 11:41:42 +1300
Subject: [squid-users] reply_header_access for Strict-Transport-Security
 doesn't work
In-Reply-To: <1537370603.4339880.1552417841401@mail.yahoo.com>
References: <1537370603.4339880.1552417841401.ref@mail.yahoo.com>
 <1537370603.4339880.1552417841401@mail.yahoo.com>
Message-ID: <b51c7887-85e8-401c-e6f8-1952f53b52be@treenet.co.nz>

On 13/03/19 8:10 am, leomessi983 wrote:
> Hi Amos,tank you for your reply!
> 
>> Current Squid automatically erase that header to prevent HSTS breaking
>> web traffic. Where possible try to get clients to upgrade to Browsers
>> which have also dropped use of the feature.
> 
> My clients have last Firefox browser but when i use squid and bumb sites
> for my block sites my client get HSTS error and squid doesn't
> automatically erase that header!
> Should i enable some feature or something else in squid configuration?


If your Squid is not removing it then add config like you had earlier.

BUT use the reply header directive instead of the request one:

 reply_header_access Strict-Transport-Security deny all



> How can i know that squid does that automatically?!

The header named "Strict-Transport-Security" should not exist in HTTP
response messages delivered by Squid to any client.

Note that HTTPS messages delivered inside CONNECT tunnels are not
touched by Squid in any way unless SSL-Bump features are being used.

Also that modern web traffic can be using any one of 5 protocols for
messaging (SPDY, HTTP/2, QUIC, HTTP/3 and WebSockets) over which Squid
has no control. If any of those are in use they act as side-channels
where the HSTS header can be delivered to the client despite anything
Squid does.


> Is certificate and certificate type is important?!
> 

Not as far as I know. This HSTS is just a requirement to use secure
connections in place of plain=text HTTP connections.

Amos


From JOfficer at istreamfs.com  Wed Mar 13 13:09:11 2019
From: JOfficer at istreamfs.com (Joey Officer)
Date: Wed, 13 Mar 2019 13:09:11 +0000
Subject: [squid-users] attempting to disable (or mute) logs
Message-ID: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>

I'm running a squid instance in AWS behind a network load balancer.  As part of the health checks, at least that's what I believe, we're seeing this log entry spamming which is hiding the rest of the relevant log data.  Sample log entry (repeating countless times)

1552419269.039 0 172.34.33.137 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -

I've added the following:

acl dontLog http_status 000             # tcp_denied (due to auth)

cache_store_log none
cache_log /dev/null
access_log stdio:/var/log/squid/access.log !dontLog

Any help on hiding that log entry so I can get back to useful data would be great.

Thanks,
Joey


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190313/ee78ab24/attachment.htm>

From marcus.kool at urlfilterdb.com  Wed Mar 13 14:05:33 2019
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 13 Mar 2019 11:05:33 -0300
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
Message-ID: <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>

I think you are suffering from this bug: https://bugs.squid-cache.org/show_bug.cgi?id=4906

Marcus


On 13/03/2019 10:09, Joey Officer wrote:
>
> I?m running a squid instance in AWS behind a network load balancer.? As part of the health checks, at least that?s what I believe, we?re seeing this log entry spamming which is hiding the rest of 
> the relevant log data. Sample log entry (repeating countless times)
>
> 1552419269.039 0 172.34.33.137 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
>
> I?ve added the following:
>
> acl dontLog http_status 000???????????? # tcp_denied (due to auth)
>
> cache_store_log none
>
> cache_log /dev/null
>
> access_log stdio:/var/log/squid/access.log !dontLog
>
> Any help on hiding that log entry so I can get back to useful data would be great.
>
> Thanks,
>
> Joey
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190313/8ec49a46/attachment.htm>

From felipeapolanco at gmail.com  Fri Mar 15 15:38:16 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Fri, 15 Mar 2019 11:38:16 -0400
Subject: [squid-users] ACL inside ClamAV?
Message-ID: <CADcj3=5snZR9ps3EwSDvxbOicNrFFOv+t1LUFT5P0MUUYW5Jzg@mail.gmail.com>

Hi,

Is it possible to use SQUID ACL inside ClamAV or any other ICAP server?

The idea is to have a list of file types to be denied for some users and
allowed for some others.

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190315/4825c924/attachment.htm>

From rafael.akchurin at diladele.com  Fri Mar 15 15:54:23 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 15 Mar 2019 15:54:23 +0000
Subject: [squid-users] ACL inside ClamAV?
In-Reply-To: <CADcj3=5snZR9ps3EwSDvxbOicNrFFOv+t1LUFT5P0MUUYW5Jzg@mail.gmail.com>
References: <CADcj3=5snZR9ps3EwSDvxbOicNrFFOv+t1LUFT5P0MUUYW5Jzg@mail.gmail.com>
Message-ID: <AM0PR04MB4753086198E4A2536B1B70218F440@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Felipe,

We have something like this in our ICAP server.
See https://docs.diladele.com/administrator_guide_7_0/web_filter/policies/blocking_file_downloads.html

Best regards,
Rafael Akchurin
Diladele B.V.

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Felipe Arturo Polanco
Sent: Friday, 15 March 2019 16:38
To: squid-users at lists.squid-cache.org
Subject: [squid-users] ACL inside ClamAV?

Hi,

Is it possible to use SQUID ACL inside ClamAV or any other ICAP server?

The idea is to have a list of file types to be denied for some users and allowed for some others.

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190315/906cff69/attachment.htm>

From JOfficer at istreamfs.com  Fri Mar 15 16:44:43 2019
From: JOfficer at istreamfs.com (Joey Officer)
Date: Fri, 15 Mar 2019 16:44:43 +0000
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
Message-ID: <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>

Thanks Marcus, that does appear to be the same thing I?m seeing.

Appreciate you providing a link to the bug, I?ll be following it.

Joey


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Wednesday, March 13, 2019 9:06 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] attempting to disable (or mute) logs


I think you are suffering from this bug: https://bugs.squid-cache.org/show_bug.cgi?id=4906

Marcus


On 13/03/2019 10:09, Joey Officer wrote:
I?m running a squid instance in AWS behind a network load balancer.  As part of the health checks, at least that?s what I believe, we?re seeing this log entry spamming which is hiding the rest of the relevant log data.  Sample log entry (repeating countless times)

1552419269.039 0 172.34.33.137 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -

I?ve added the following:

acl dontLog http_status 000             # tcp_denied (due to auth)

cache_store_log none
cache_log /dev/null
access_log stdio:/var/log/squid/access.log !dontLog

Any help on hiding that log entry so I can get back to useful data would be great.

Thanks,
Joey





_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190315/9d8f4d51/attachment.htm>

From rousskov at measurement-factory.com  Fri Mar 15 18:22:14 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 15 Mar 2019 12:22:14 -0600
Subject: [squid-users] ACL inside ClamAV?
In-Reply-To: <CADcj3=5snZR9ps3EwSDvxbOicNrFFOv+t1LUFT5P0MUUYW5Jzg@mail.gmail.com>
References: <CADcj3=5snZR9ps3EwSDvxbOicNrFFOv+t1LUFT5P0MUUYW5Jzg@mail.gmail.com>
Message-ID: <c7d531fc-062d-22c8-691d-c77b80a6f39d@measurement-factory.com>

On 3/15/19 9:38 AM, Felipe Arturo Polanco wrote:

> Is it possible to use SQUID ACL inside ClamAV or any other ICAP server?

Not exactly -- Squid ACLs do not cross the adaptation boundary -- but it
is possible for the ICAP or eCAP service to take Squid ACL _results_
into account. It is also possible for Squid itself to apply ACLs based
on the ICAP or eCAP service decisions.

The specifics would heavily depend on what exactly you are trying to do,
on Squid version, and on your ICAP/eCAP service capabilities. Thus,
start by detailing your use case.


> The idea is to have a list of file types to be denied for some users and
> allowed for some others.

There are many ways to interpret this description. For example, you can
use Squid ACLs to _not_ send responses (of a certain content type
requested by certain users) to an ICAP service. Or, with some ICAP or
eCAP services, you can take the service's "this file is of certain type"
decision and use that info in a Squid ACL to decide whether to block or
forward the scanned response.

Alex.


From squid3 at treenet.co.nz  Sat Mar 16 15:43:37 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Mar 2019 04:43:37 +1300
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
Message-ID: <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>

On 16/03/19 5:44 am, Joey Officer wrote:
> Thanks Marcus, that does appear to be the same thing I?m seeing.
> 
> ?
> 
> Appreciate you providing a link to the bug, I?ll be following it.
> 
> ?

I'm not so sure that is right.

Your attempted ACL check was based on HTTP response status code when
this transaction has no response at all (thus no status). The "000" is a
log display artifact meaning "none", not an actual value. So the match
result of 'no-response' is correct.

I think you are needing the "has" ACL instead (new in Squid-4):

 acl hasRequest has request
 access_log stdio:/var/log/squid/access.log !hasRequest


I have not tried this myself, but it is supposed to be the way to do
what you are wanting. If you are using an older Squid-3.x version please
consider this a reason to upgrade to current.

Amos


From itai at hysolate.com  Sun Mar 17 07:22:56 2019
From: itai at hysolate.com (Itai Tieger)
Date: Sun, 17 Mar 2019 02:22:56 -0500 (CDT)
Subject: [squid-users] Got [No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
Message-ID: <1552807376811-0.post@n4.nabble.com>

Hey, 

I'm using squid 4.4 compiled with openssl 1.1.0. 
Sometimes when I try to access a site, I get this error: 
 The following error was encountered while trying to retrieve the URL:
https://175.41.13.121/* Failed to establish a secure connection to
175.41.13.121 The system returned: [No Error] (TLS code:
SQUID_ERR_SSL_HANDSHAKE) Handshake with SSL server failed: [No Error] This
proxy and the remote host failed to negotiate a mutually acceptable security
settings for handling your request. It is possible that the remote host does
not support secure connections, or the proxy is not satisfied with the host
security credentials. Your cache administrator is webmaster. 
This is the capture file: 
sniff.cap
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377689/sniff.cap>  
I can't seem to understand what is the problem, what exactly is missing and
how can I debug it myself? 

I also get many 
 32	2019/02/25 00:09:19 kid1| ERROR: negotiating TLS on FD 43:
error:1416F086:SSL routines:tls_process_server_certificate:certificate
verify failed (1/-1/0) 
in the log, might be related... ?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Mar 17 07:45:26 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Mar 2019 20:45:26 +1300
Subject: [squid-users] Got [No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
In-Reply-To: <1552807376811-0.post@n4.nabble.com>
References: <1552807376811-0.post@n4.nabble.com>
Message-ID: <45adf994-ea59-cc01-c703-b0c23f0e365d@treenet.co.nz>

On 17/03/19 8:22 pm, Itai Tieger wrote:
> This is the capture file: 
> sniff.cap
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377689/sniff.cap>  
> I can't seem to understand what is the problem, what exactly is missing and
> how can I debug it myself? 

Your trace shows TCP stream #35 as the only one to that server IP
address. The handshake is successful in that stream.


> 
> I also get many 
>  32	2019/02/25 00:09:19 kid1| ERROR: negotiating TLS on FD 43:
> error:1416F086:SSL routines:tls_process_server_certificate:certificate
> verify failed (1/-1/0) 
> in the log, might be related... ?
> 

Possibly. The server certificate being invalid or otherwise unable to
verify would certainly be a handshake failure.

Amos


From itai at hysolate.com  Sun Mar 17 08:10:21 2019
From: itai at hysolate.com (Itai Tieger)
Date: Sun, 17 Mar 2019 03:10:21 -0500 (CDT)
Subject: [squid-users] Got [No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
In-Reply-To: <45adf994-ea59-cc01-c703-b0c23f0e365d@treenet.co.nz>
References: <1552807376811-0.post@n4.nabble.com>
 <45adf994-ea59-cc01-c703-b0c23f0e365d@treenet.co.nz>
Message-ID: <1552810221114-0.post@n4.nabble.com>

Amos, Thanks for the quick response! 

The error happened on all streams (or at least, on all refresh tries).
I too didn't see any error in the captures so I can't say what's the issue.
I doubt there's a server certificate problem - it worked before and it
worked after...
What other reasons could lead to that?

Attaching all the other captures. This time the upstream one (the last one
was the downstream).
sniff0.cap
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377689/sniff0.cap>  
sniff1.cap
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377689/sniff1.cap>  
sniff3.cap
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377689/sniff3.cap>  

I don't see anything special here (maybe you'll see), but evidently, there's
a problem.
What can I do more to better understand the problem?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From info at microlinux.fr  Mon Mar 18 09:39:00 2019
From: info at microlinux.fr (Nicolas Kovacs)
Date: Mon, 18 Mar 2019 10:39:00 +0100
Subject: [squid-users] Replace SquidGuard with ufdbguard : configuration
	examples ?
Message-ID: <1165ea78-247f-14cf-913c-5da51c7801b5@microlinux.fr>

Hi,

I've been running the Squid + SquidGuard combination for quite some time
in our local school. I'm also filtering HTTPS connections using the
Squid SSL Bump functionality.

I'd like to test ufdbguard, since SquidGuard doesn't seem to be
maintained anymore, and it's also quite RAM-consuming.

I've read the PDF manual of ufdbguard, but before going any further, I'd
like to ask. Do any of you guys here use the Squid + ufdbguard
combination ? And if this is the case, can you eventually send me a few
working configuration files ? I'm currently fiddling with a local
sandbox installation, and I have some trouble putting the pieces together.

Cheers from the sunny South of France,

Niki Kovacs
-- 
Microlinux - Solutions informatiques durables
7, place de l'?glise - 30730 Montpezat
Site : https://www.microlinux.fr
Blog : https://blog.microlinux.fr
Mail : info at microlinux.fr
T?l. : 04 66 63 10 32


From marcus.kool at urlfilterdb.com  Mon Mar 18 10:07:40 2019
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 18 Mar 2019 07:07:40 -0300
Subject: [squid-users] Replace SquidGuard with ufdbguard : configuration
 examples ?
In-Reply-To: <1165ea78-247f-14cf-913c-5da51c7801b5@microlinux.fr>
References: <1165ea78-247f-14cf-913c-5da51c7801b5@microlinux.fr>
Message-ID: <e08adb6c-d79c-65c4-ee82-f6f8d5e60a99@urlfilterdb.com>

The ufdbGuard source files and packages have an example config file.

If you have a ufdbGuard-specific issue I suggest to use the list of ufdbGuard or go directly to the support desk of URLfilterDB.

Marcus


On 18/03/2019 06:39, Nicolas Kovacs wrote:
> Hi,
>
> I've been running the Squid + SquidGuard combination for quite some time
> in our local school. I'm also filtering HTTPS connections using the
> Squid SSL Bump functionality.
>
> I'd like to test ufdbguard, since SquidGuard doesn't seem to be
> maintained anymore, and it's also quite RAM-consuming.
>
> I've read the PDF manual of ufdbguard, but before going any further, I'd
> like to ask. Do any of you guys here use the Squid + ufdbguard
> combination ? And if this is the case, can you eventually send me a few
> working configuration files ? I'm currently fiddling with a local
> sandbox installation, and I have some trouble putting the pieces together.
>
> Cheers from the sunny South of France,
>
> Niki Kovacs


From leomessi983 at yahoo.com  Mon Mar 18 13:26:42 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Mon, 18 Mar 2019 13:26:42 +0000 (UTC)
Subject: [squid-users] security_file_certgen problem
References: <6874242.7198247.1552915602896.ref@mail.yahoo.com>
Message-ID: <6874242.7198247.1552915602896@mail.yahoo.com>

Hi allI compiled squid 4.6 with this options:./configure \
--with-openssl \
--enable-ssl-crtd \
--prefix=/usr \
--enable-linux-netfilter \
--with-netfilter-conntrack \
--exec-prefix=/usr \
--includedir=/usr/include \
--datadir=/usr/share/squid \
--libdir=/usr/lib64 \
--libexecdir=/usr/lib64/squid \
--localstatedir=/var \
--sysconfdir=/etc/squid/ \
--sharedstatedir=/var/lib/ \
--with-logdir=/var/log/squid/ \
--enable-ltdl-convenience \
--enable-http-violationsAnd my configurations is:acl Blk ssl::server_name "/var/squid/blk.list"
ssl_bump bump Blk
acl urlBlk dstdomain " /var/squid/blk.list"
reply_header_access Strict-Transport-Security deny all

http_access deny urlBlkhttp_access allow all
http_port 0.0.0.0:3128
http_port 0.0.0.0:3129 tproxy
https_port 3130 tproxy ssl-bump \
??????? tls-cert=/etc/squid/ssl/myca.pem \
??????? generate-host-certificates=on dynamic_cert_mem_cache_size=20MB
sslcrtd_program /usr/lib64/squid/security_file_certgen -s /var/lib/ssl_db -M 20MB
sslcrtd_children 10 startup=5 idle=1
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice all
shutdown_lifetime 5 seconds
cache deny all
cache_mem 0


After that i use squid to block https requests, when i try to get blocked https site i get this error in my cache.log:
2019/03/18 16:46:11| WARNING: /usr/lib64/squid/security_file_certgen -s /var/lib/ssl_db -M 20MB #Hlpr1 exited2019/03/18 16:46:11| Too few /usr/lib64/squid/security_file_certgen -s /var/lib/ssl_db -M 20MB processes are running (need 1/10)
2019/03/18 16:46:11| Starting new helpers
2019/03/18 16:46:11| helperOpenServers: Starting 1/10 'security_file_certgen' processes
2019/03/18 16:46:11| "ssl_crtd" helper returned <NULL> reply.


What is wrong? what am i do?!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190318/f65125e5/attachment.htm>

From rousskov at measurement-factory.com  Mon Mar 18 14:03:56 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Mar 2019 08:03:56 -0600
Subject: [squid-users] Got [No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
In-Reply-To: <1552807376811-0.post@n4.nabble.com>
References: <1552807376811-0.post@n4.nabble.com>
Message-ID: <f8e39805-7f59-82d4-1cfb-5f5244462b0a@measurement-factory.com>

On 3/17/19 1:22 AM, Itai Tieger wrote:

> I'm using squid 4.4 compiled with openssl 1.1.0. 
> Sometimes when I try to access a site, I get this error: 

> (TLS code: SQUID_ERR_SSL_HANDSHAKE) Handshake with SSL server failed: [No Error] 


> how can I debug it myself? 

Since the error is probably detected inside OpenSSL SSL_connect(), I
would start by extracting the corresponding server certificate from the
packet capture and asking OpenSSL library on the Squid box to validate it.


> I also get many 
>  32	2019/02/25 00:09:19 kid1| ERROR: negotiating TLS on FD 43:
> error:1416F086:SSL routines:tls_process_server_certificate:certificate
> verify failed (1/-1/0) 
> in the log, might be related... ?

It is -- SQUID_ERR_SSL_HANDSHAKE is only returned after printing the
above level-1 message AFAICT.


BTW, if Squid does not relay the above OpenSSL error details to the
error page, it is a Squid bug or deficiency.


Alex.


From itai at hysolate.com  Mon Mar 18 16:21:15 2019
From: itai at hysolate.com (Itai Tieger)
Date: Mon, 18 Mar 2019 11:21:15 -0500 (CDT)
Subject: [squid-users] Got [No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
In-Reply-To: <f8e39805-7f59-82d4-1cfb-5f5244462b0a@measurement-factory.com>
References: <1552807376811-0.post@n4.nabble.com>
 <f8e39805-7f59-82d4-1cfb-5f5244462b0a@measurement-factory.com>
Message-ID: <1552926075666-0.post@n4.nabble.com>

Thanks for the answer!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From anon.amish at gmail.com  Tue Mar 19 09:45:52 2019
From: anon.amish at gmail.com (Amish)
Date: Tue, 19 Mar 2019 15:15:52 +0530
Subject: [squid-users] url_rewrite_program,
	sslbump and CONNECT = broken redirect page?
Message-ID: <21a90d00-83e9-51be-11d6-7f419d20327e@gmail.com>

Hello,

I have perfectly working SSL bump setup (via proxy CONNECT requests) 
except when a site is blocked.

I have a rewrite program which blocks say foo.com.

Roughly it does this:

HTTP - non secure
STDIN: GET http://foo.com
STDOUT: rewrite-url="http://127.0.0.1/blocked"

Above works fine as expected, the page is fetched and shown.

But now if it is a CONNECT (https) request:

STDIN: CONNECT foo.com:443
STDOUT: rewrite-url="http://127.0.0.1/blocked"

Then instead of fetching the above page it tries to fetch "CONNECT 
http:443" and returns ERR_DNS_FAIL page.

The problem code begins here: (client_side_request.cc)
https://github.com/squid-cache/squid/blob/master/src/client_side_request.cc#L1261

which leads to: (AnyP:;Uri parse() function)
https://github.com/squid-cache/squid/blob/master/src/anyp/Uri.cc#L211

which treats CONNECT request differently then what is documented.

It finds domain as something colon number. And looks like ignores 
urlpath completely.

So in my case it becomes http:443.

And hence redirection breaks.

How do I convert CONNECT requests over ssl bump to GET 
http://127.0.0.1/blocked

This exact issue was reported earlier too in 2015 but the person who 
reported it probably couldn't locate the exact reason and bug went 
unnoticed.

http://lists.squid-cache.org/pipermail/squid-users/2015-August/005170.html

Regards

Amish.


From squid3 at treenet.co.nz  Tue Mar 19 11:32:14 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Mar 2019 00:32:14 +1300
Subject: [squid-users] security_file_certgen problem
In-Reply-To: <6874242.7198247.1552915602896@mail.yahoo.com>
References: <6874242.7198247.1552915602896.ref@mail.yahoo.com>
 <6874242.7198247.1552915602896@mail.yahoo.com>
Message-ID: <51d9eefb-b1d8-1e4f-3363-e51662694f50@treenet.co.nz>

On 19/03/19 2:26 am, leomessi983 wrote:
> Hi all
> I compiled squid 4.6 with this options:
> 
...
> 
> And my configurations is:
...
> sslcrtd_program /usr/lib64/squid/security_file_certgen -s
> /var/lib/ssl_db -M 20MB

Have you initialized the /var/lib/ssl_db directory using the
low-privilege account Squid operates as?


> 
> After that i use squid to block https requests, when i try to get
> blocked https site i get this error in my cache.log:
> 
> 2019/03/18 16:46:11| WARNING: /usr/lib64/squid/security_file_certgen -s
> /var/lib/ssl_db -M 20MB #Hlpr1 exited

The helper should have output a message before it shutdown. If that
managed to get written it would occur somewhere before this line in your
cache.log.


> 2019/03/18 16:46:11| Too few /usr/lib64/squid/security_file_certgen -s
> /var/lib/ssl_db -M 20MB processes are running (need 1/10)
> 2019/03/18 16:46:11| Starting new helpers
> 2019/03/18 16:46:11| helperOpenServers: Starting 1/10
> 'security_file_certgen' processes
> 2019/03/18 16:46:11| "ssl_crtd" helper returned <NULL> reply.
> 
> 
> What is wrong? what am i do?!
> 

Usually run the helper manually with the -c option to initialize the
OpenSSL certificate storage before using it. Make sure this is done with
the same user account Squid will be using when it runs.
 Also make sure that anything like AppArmor or SELinux that does
advanced filesystem permissions is updated to permit access to Squid.



Amos


From squid3 at treenet.co.nz  Tue Mar 19 11:25:49 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Mar 2019 00:25:49 +1300
Subject: [squid-users] url_rewrite_program,
 sslbump and CONNECT = broken redirect page?
In-Reply-To: <21a90d00-83e9-51be-11d6-7f419d20327e@gmail.com>
References: <21a90d00-83e9-51be-11d6-7f419d20327e@gmail.com>
Message-ID: <631212ab-0323-3146-87b4-4c3a1f055a3b@treenet.co.nz>

On 19/03/19 10:45 pm, Amish wrote:
> Hello,
> 
> I have perfectly working SSL bump setup (via proxy CONNECT requests)
> except when a site is blocked.
> 
> I have a rewrite program which blocks say foo.com.
> 

Please remember there is a difference between "rewrite" (what you are
actually doing) and "redirect" (per your subject).

* Redirect is a standardized HTTP mechanism. It works wherever the
client support that basic HTTP mechanism.

* Rewrite is an abuse of the proxy place in the protocol. It more often
causes problems than works.


> Roughly it does this:
> 
> HTTP - non secure
> STDIN: GET http://foo.com
> STDOUT: rewrite-url="http://127.0.0.1/blocked"
> 
> Above works fine as expected, the page is fetched and shown.
> 
> But now if it is a CONNECT (https) request:
> 
> STDIN: CONNECT foo.com:443
> STDOUT: rewrite-url="http://127.0.0.1/blocked"
> 
> Then instead of fetching the above page it tries to fetch "CONNECT
> http:443" and returns ERR_DNS_FAIL page.
> 
> The problem code begins here: (client_side_request.cc)
> https://github.com/squid-cache/squid/blob/master/src/client_side_request.cc#L1261
> 

No. The problem starts with the helper ignoring the method and original
URI form/structure Squid passed to it.


> 
> which leads to: (AnyP:;Uri parse() function)
> https://github.com/squid-cache/squid/blob/master/src/anyp/Uri.cc#L211
> 
> which treats CONNECT request differently then what is documented.
> 

The authoritative documentation is the code comment starting at that
files #L212, and the RFC it references. Other documentation which does
not align is outdated and/or incorrect.


Specifically be aware that URI and URL are different things. Your helper
is responsible for delivering whichever URI syntax is valid as
request-target for the message being re-written. Squid passes the method
name to the helper for exactly this case.


> It finds domain as something colon number. And looks like ignores
> urlpath completely.

authority-form URI have no such thing as a path. They are a hostname (or
raw-IP) a colon, and a port number. Any remainder is garbage.


> 
> So in my case it becomes http:443.
> 
> And hence redirection breaks.

Redirection would not work regardless of what Squid does with your
helpers output. The client is sending native TLS on the connection - the
redirected-to URL is HTML or something like it.

At best you can hope for is the client to spontaneously abort with a TCP
RST.

At worst the client can enter an infinite series of attempts to
re-connect ... until your proxy machine consumes all available TCP ports
and *all* TCP traffic through the machine halts - even traffic by other
software or the OS itself.


> 
> How do I convert CONNECT requests over ssl bump to GET
> http://127.0.0.1/blocked
> 

You cannot deliver protocol X to a client which is expecting
incompatible protocol Y and have anything good happen.

In this case probably delivering HTML text (or similar) to a client
expecting a TLS serverHello message.


> This exact issue was reported earlier too in 2015 but the person who
> reported it probably couldn't locate the exact reason and bug went
> unnoticed.

It was not unnoticed. Has been reported many times over the years. The
bug is in the helper.


You should fix the helper (if possible) to cope with CONNECT and other
unusual types of URI input it may receive. There may be protocols other
than "http://", path-only URLs, URN, or even just an asterisk ('*') on
some methods.
 <https://tools.ietf.org/html/rfc7230#section-5.3>
 <https://tools.ietf.org/html/rfc3986>

To re-write the helper output must not only be a valid URI, but also
compatible with the operations the original URI was going to perform.


If that is not possible, you can workaround this particular case by
adding this to your squid.conf to skip processing of the CONNECT
messages entirely:

 url_rewrite_access deny CONNECT


Cheers,
Amos


From JOfficer at istreamfs.com  Tue Mar 19 13:34:02 2019
From: JOfficer at istreamfs.com (Joey Officer)
Date: Tue, 19 Mar 2019 13:34:02 +0000
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
Message-ID: <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>

Confirmed I'm on Squid v4

# squid --version
Squid Cache: Version 4.4
Service Name: squid

I added the above and restarted my container, but the logged output has no change.  I also wanted to add that I may not have provided enough information.  The spam log entries are stdout - which is where Cloudwatch (in my case) is logging the events.  Squid container is being started with the following command:

squid -f /etc/squid/squid.conf -NYCd 1

which is the following options:
-N        Master process runs in foreground and is a worker. No kids.
-Y        Only return UDP_HIT or UDP_MISS_NOFETCH during fast reload.
-C        Do not catch fatal signals.
-d level  Write debugging to stderr also.

I don't have access to the access.log itself, so I'll have to assume the above entry is working for that file.  With that in mind, how would I apply the same filter to the stdout of the squid process?

From anon.amish at gmail.com  Wed Mar 20 12:03:30 2019
From: anon.amish at gmail.com (Amish)
Date: Wed, 20 Mar 2019 17:33:30 +0530
Subject: [squid-users] url_rewrite_program,
 sslbump and CONNECT = broken redirect page?
In-Reply-To: <631212ab-0323-3146-87b4-4c3a1f055a3b@treenet.co.nz>
References: <21a90d00-83e9-51be-11d6-7f419d20327e@gmail.com>
 <631212ab-0323-3146-87b4-4c3a1f055a3b@treenet.co.nz>
Message-ID: <838c8213-1d7d-a291-dc37-2223f82be416@gmail.com>



On 19/03/19 4:55 pm, Amos Jeffries wrote:
>
> You should fix the helper (if possible) to cope with CONNECT and other
> unusual types of URI input it may receive. There may be protocols other
> than "http://", path-only URLs, URN, or even just an asterisk ('*') on
> some methods.
>   <https://tools.ietf.org/html/rfc7230#section-5.3>
>   <https://tools.ietf.org/html/rfc3986>
>
> To re-write the helper output must not only be a valid URI, but also
> compatible with the operations the original URI was going to perform.
>
> If that is not possible, you can workaround this particular case by
> adding this to your squid.conf to skip processing of the CONNECT
> messages entirely:
>
>   url_rewrite_access deny CONNECT

Thanks for the elaborate reply. Now I have fixed my helper to reply with 
"OK"? for CONNECT requests.

But I have came across another issue related to "notes" and SSL bumped 
connection.

I will compose another e-mail with new subject to explain the issue, soon.

Regards

Amish


From squid3 at treenet.co.nz  Wed Mar 20 12:40:39 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Mar 2019 01:40:39 +1300
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
Message-ID: <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>

On 20/03/19 2:34 am, Joey Officer wrote:
> Confirmed I'm on Squid v4
> 
> # squid --version
> Squid Cache: Version 4.4
> Service Name: squid
> 
> I added the above and restarted my container, but the logged output has no change.  I also wanted to add that I may not have provided enough information.  The spam log entries are stdout - which is where Cloudwatch (in my case) is logging the events.  Squid container is being started with the following command:
> 
> squid -f /etc/squid/squid.conf -NYCd 1
> 
> which is the following options:
> -N        Master process runs in foreground and is a worker. No kids.
> -Y        Only return UDP_HIT or UDP_MISS_NOFETCH during fast reload.
> -C        Do not catch fatal signals.
> -d level  Write debugging to stderr also.
> 
> I don't have access to the access.log itself, so I'll have to assume the above entry is working for that file.  With that in mind, how would I apply the same filter to the stdout of the squid process?
> 

There is no Squid output on stdout. Your _Cloudwatch_ thing may be
sending log data there, but that has nothing directly to do with Squid.


At most Squid prints some initial messages to stderr on startup which
Unix based OS typically write into a kernel log called
'/var/log/messages' (but YMMV). Once the cache.log *file* is initialized
critical and important notices, along with all helper stderr outputs go
to that cache.log file.

 ***** Please note that setting cache.log to /dev/null is highly dangerous.

  Squid log rotation, restart etc may *replace* the filesystem special
inode for /dev/null with a regular inode for a cache.log file. Squid has
root privileges on startup (required for other device access and
actions) which allow that inode replacement to be possible.


Amos


From augustus_meyer at gmx.net  Wed Mar 20 15:55:57 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 20 Mar 2019 10:55:57 -0500 (CDT)
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
 <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
Message-ID: <1553097357921-0.post@n4.nabble.com>

>***** Please note that setting cache.log to /dev/null is highly dangerous. <

Interesting. As this is standard when running squid on openwrt.
Is there any _safe_ method to disable output to cache.log ?
 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From leomessi983 at yahoo.com  Wed Mar 20 21:04:36 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Wed, 20 Mar 2019 21:04:36 +0000 (UTC)
Subject: [squid-users] security_file_certgen problem
References: <1184497381.8828258.1553115876872.ref@mail.yahoo.com>
Message-ID: <1184497381.8828258.1553115876872@mail.yahoo.com>

> Have you initialized the /var/lib/ssl_db directory using the
> low-privilege account Squid operates as?
Yes i use -c option and set permissions for nobody and nogroup user which squid use!
> The helper should have output a message before it shutdown. If that
> managed to get written it would occur somewhere before this line in your
> cache.log.
After squid showed that warning  fatal error accrued and some termination errors!

I used security_file_certgen helper from squid4.3 source files and then i created a .deb package from my squid 4.6 compiled files and former helpers!
then squid runs perfectly!!
I think security_file_certgen helper in squid 4.6 is the problem!!
Also when i copy cecutity_file_certgen helper from older squid 4.3 to my new machine whit squid 4.6 it is OK !!!!!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190320/c8af631d/attachment.htm>

From ygreenfield at kewsystems.com  Wed Mar 20 21:23:06 2019
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Wed, 20 Mar 2019 17:23:06 -0400
Subject: [squid-users] Non-transparent proxy with cache_peer and ssl_bump
Message-ID: <D3EDBD3399484070B6AFAE9296461DC5@OhrSomayach>

Hello all,

I'm pretty sure this has been asked and answered more than once, but I've
been over the emails and the docs, and I still can't figure out how to make
this work. Might one of you be able to guide me here?

We have the following setup in our network:
Client ---- Squid1 ---- Squid2 ---- Internet

Squid1 is running Squid 3.5.28.
Squid2 is running Squid 3.5.23.

We do not do transparent proxying. Each user sets their proxy server in
their browser to use Squid1.

The browser authenticates to Squid1. Squid1 passes the authenticated request
to Squid2. Squid2 rewrites the request based on the authenticated user. 

It all works except for peered https traffic. Both squid1 and squid2 work
separately for https traffic. Cache_peer works if I remove SSL from it. It's
just the peered https that does not.

Here are the relevant lines in Squid1 squid.conf:

# START SQUID1 CONF
http_port 3128
http_port 3129 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=6MB cert=/etc/squid/ssl/myCA.pem name=bumped
options=ALL

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl NoBump ssl::server_name  "/etc/squid/nobump/domains"

ssl_bump peek step1 all
ssl_bump peek step2 NoBump
ssl_bump splice step3 NoBump
ssl_bump stare step2
ssl_bump bump step3

sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

request_header_access Surrogate-Capability deny all
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

cache_peer proxy2.ourserver.com parent 3130 0 no-query no-digest
login=PASSTHRU default ssl  sslcert=/etc/squid/ssl/myCA.pem
never_direct allow all

# END SQUID1 CONF


Here are the relevant lines in Squid2 squid.conf:

# START SQUID2 CONF
http_port 3128 name=non-bumped
http_port 3130 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=6MB cert=/etc/squid/ssl/myCA.pem name=bumped
options=ALL

acl non-bumped myportname non-bumped
acl bumped myportname bumped

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl NoBump ssl::server_name  "/etc/squid/nobump/domains"

ssl_bump peek step1 all
ssl_bump peek step2 NoBump
ssl_bump splice step3 NoBump
ssl_bump stare step2
ssl_bump bump step3

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

# END SQUID2 CONF


Squid1 cache.log shows lots of this:

2019/03/20 16:22:14 kid1| TCP connection to proxy2.ourserver.com/3130 failed
2019/03/20 16:22:14 kid1| Error negotiating SSL on FD 14: error:140770FC:SSL
routines:SSL23_GET_SERVER_HELLO:unknown protocol


Here are my questions:

1. Does squid 3.5 even allow sending https between peers? I've read
conflicting emails, but I'm pretty sure it does. (Do I have to comment out
some source code?)

2. What file goes into the cache_peer directive sslcert? I'm using the same
PEM file for cahe_peer on Squid1 and http_port on Squid2. Is that a mistake?

3. What else am I doing wrong?


Thanks for your time and help!



From dave.mulford at gmail.com  Wed Mar 20 21:46:35 2019
From: dave.mulford at gmail.com (Dave Mulford)
Date: Wed, 20 Mar 2019 16:46:35 -0500
Subject: [squid-users] Squid HIER_NONE state in access logs
Message-ID: <fc086abd-0392-ca66-d56e-ac4bbeab5d3d@gmail.com>

Hi everyone,

I'm trying to determine what would cause Squid proxy to log a 
TCP_MISS/200 with a HIER_NONE state in its access logs.  A sample line 
looks like this (I've masked some of the values):

1549073041.578 8 1.2.3.4 TCP_MISS/200 123456 GET 
https://www.example.com:8443/mysite.js - HIER_NONE/-
+application/javascript ...

Looking in the forums I found an answer [1] that says this state can be 
logged if collapsed_forwarding or iCAP/eCAP are used.  I'm using neither.

What other conditions could cause a TCP_MISS/200 with a HIER_NONE?

Thanks,
Dave

[1] 
http://squid-web-proxy-cache.1019090.n4.nabble.com/HIER-NONE-on-TCP-MISS-tp4682907p4682920.html


From rousskov at measurement-factory.com  Wed Mar 20 21:58:17 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Mar 2019 15:58:17 -0600
Subject: [squid-users] Non-transparent proxy with cache_peer and ssl_bump
In-Reply-To: <D3EDBD3399484070B6AFAE9296461DC5@OhrSomayach>
References: <D3EDBD3399484070B6AFAE9296461DC5@OhrSomayach>
Message-ID: <527c1b32-fd70-ca91-ead9-f9c085390e2a@measurement-factory.com>

On 3/20/19 3:23 PM, Yosi Greenfield wrote:

> ssl_bump splice step3 NoBump
> ssl_bump bump step3

> cache_peer proxy2.ourserver.com ... ssl

Forwarding most SslBump-related connections to cache_peers is still
unsupported by official Squids, including Squid v3 and v4. Measurement
Factory code that implements this feature is being officially reviewed
at https://github.com/squid-cache/squid/pull/380/

If you can test the above-referenced code, please do.

However, even if the above-referenced changes are officially accepted
(into v5), they will not allow you to do "TLS inside TLS" -- you will
not be able to forward most SslBump-related connections to HTTPS proxies
(i.e. your "cache_peer ssl").

Fortunately, forwarding to HTTPS proxies is not critical in most use
cases -- one layer of TLS encryption is often enough. Unfortunately, you
will expose CONNECT requests between Squid1 and Squid2 until we add that
support or perhaps [controversially] allow bumped traffic to be sent to
HTTPS proxies without additional encryption. I am not aware of anybody
working on either right now.


> 1. Does squid 3.5 even allow sending https between peers? 

Squid allows sending plain HTTP traffic to an HTTPS peer. That is not
what you are configuring your squid1 to do though: You are telling
squid1 to send bumped HTTPS traffic to an HTTPS peer. The latter is not
supported.


> 2. What file goes into the cache_peer directive sslcert?

Let's assume that the TCP connection between squid1 and squid2 is
encrypted with TLS (i.e., your configuration with "cache_peer ssl"). TLS
supports certificate-based client authentication: A TLS client (i.e.,
squid1 in your case) sends its SSL certificate to the TLS server (i.e.,
squid2 in your case). The TLS server (i.e. squid2) validates that
certificate against some mutually trusted CA and allows (or denies) the
connection.

"cache_peer sslcert" names the file containing the (client) SSL
certificate that squid1 sends and squid2 expects/validates.


> I'm using the same
> PEM file for cahe_peer on Squid1 and http_port on Squid2. Is that a mistake?

It is a mistake in most (possibly all) use cases. The former is a
(client) SSL certificate that squid1 sends and squid2 validates. The
latter is an SSL CA certificate to generate fake (server) SSL
certificates. Squid2 sends those generated certificates. A bumping
Squid1 validates them.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Mar 20 22:11:29 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Mar 2019 16:11:29 -0600
Subject: [squid-users] Squid HIER_NONE state in access logs
In-Reply-To: <fc086abd-0392-ca66-d56e-ac4bbeab5d3d@gmail.com>
References: <fc086abd-0392-ca66-d56e-ac4bbeab5d3d@gmail.com>
Message-ID: <eb455b46-1607-3d02-d4b4-c55c64009f97@measurement-factory.com>

On 3/20/19 3:46 PM, Dave Mulford wrote:

> I'm trying to determine what would cause Squid proxy to log a
> TCP_MISS/200 with a HIER_NONE state in its access logs.? A sample line
> looks like this (I've masked some of the values):

> 1549073041.578 8 1.2.3.4 TCP_MISS/200 123456 GET
> https://www.example.com:8443/mysite.js - HIER_NONE/-

> Looking in the forums I found an answer [1] that says this state can be
> logged if collapsed_forwarding or iCAP/eCAP are used.? I'm using neither.

> What other conditions could cause a TCP_MISS/200 with a HIER_NONE?

1. Unknown(?) Squid bugs that do not relay origin server details to
access.log.

2. Delay pools are enabled (at Squid build time) and the response has
not been fully received/stored by Squid when this client came for it.
This is essentially a bug that was deliberately introduced to quickly
fix another bug. For background, see:

* https://bugs.squid-cache.org/show_bug.cgi?id=2096
* https://bugs.squid-cache.org/show_bug.cgi?id=1001

A better fix for bug 1001 is wanted. I am not aware of anybody working
on or sponsoring it right now.


HTH,

Alex.


From squid3 at treenet.co.nz  Thu Mar 21 02:24:51 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Mar 2019 15:24:51 +1300
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <1553097357921-0.post@n4.nabble.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
 <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
 <1553097357921-0.post@n4.nabble.com>
Message-ID: <8dc16f6c-ee81-1073-072d-0d68066337d4@treenet.co.nz>

On 21/03/19 4:55 am, reinerotto wrote:
>> ***** Please note that setting cache.log to /dev/null is highly dangerous. <
> 
> Interesting. As this is standard when running squid on openwrt.
> Is there any _safe_ method to disable output to cache.log ?
>  

Not to disable, this is the log which receives notices of *critical*
events - such as things which are crashing Squid. If your Squid
cache.log has much content at all there is a major problem to fix rather
than just silencing the log about it.


Placing "debug_options rotate=N" can prevent old cache.log being kept
past log rotation. N being the count of old cache.log to keep.

Your use of "-d 1" command line parameter makes Squid log important as
well as critical events. That may increase the log size a little for
normal use. Again if there is much content being logged that would be a
sign of problems needing to be fixed.

Some important level issues may be outside Squid. So after fixing the
issues you can, simply remove the use of "-d 1".

Amos


From squid3 at treenet.co.nz  Thu Mar 21 03:22:21 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Mar 2019 16:22:21 +1300
Subject: [squid-users] Non-transparent proxy with cache_peer and ssl_bump
In-Reply-To: <527c1b32-fd70-ca91-ead9-f9c085390e2a@measurement-factory.com>
References: <D3EDBD3399484070B6AFAE9296461DC5@OhrSomayach>
 <527c1b32-fd70-ca91-ead9-f9c085390e2a@measurement-factory.com>
Message-ID: <fab68442-7a7d-ef01-be52-8326a74a885f@treenet.co.nz>

On 21/03/19 10:58 am, Alex Rousskov wrote:
> On 3/20/19 3:23 PM, Yosi Greenfield wrote:
> 
>> ssl_bump splice step3 NoBump
>> ssl_bump bump step3
> 
>> cache_peer proxy2.ourserver.com ... ssl
> 
> Forwarding most SslBump-related connections to cache_peers is still
> unsupported by official Squids, including Squid v3 and v4. Measurement
> Factory code that implements this feature is being officially reviewed
> at https://github.com/squid-cache/squid/pull/380/
> 
> If you can test the above-referenced code, please do.
> 
> However, even if the above-referenced changes are officially accepted
> (into v5), they will not allow you to do "TLS inside TLS" -- you will
> not be able to forward most SslBump-related connections to HTTPS proxies
> (i.e. your "cache_peer ssl").
> 
> Fortunately, forwarding to HTTPS proxies is not critical in most use
> cases -- one layer of TLS encryption is often enough. Unfortunately, you
> will expose CONNECT requests between Squid1 and Squid2 until we add that
> support or perhaps [controversially] allow bumped traffic to be sent to
> HTTPS proxies without additional encryption. I am not aware of anybody
> working on either right now.
> 


Er, sending the bumped https:// requests to a cache_peer with 'ssl' does
work, though not advised. It is equivalent to the client-first type
SSL-Bumping which has a lot of nasty security side effects.


What is broken here is that squid2 config has http_port receiving the
TLS connections from squid1. So when the plain-text response arrives
from squid2 instead of a TLS server handshake squid1 produces that
negotiate error.



Yosi:

The quick-fix is to add an https_port line to you squid2 and make the
squid1 cache_peer line point at that port instead of 3130.
 Be aware the cert= parameter on squid2 https_port will be used by
squid1 to bump *all* traffic you send through that cache_peer link -
thus causing those nasty client-first problems.

OR, to avoid the client-first effects do not use cache_peer at all. Let
the squid1 traffic go 'direct' and use interception to divert that port
443 traffic into squid2. YMMV on whether this works.

OR, use the experimental PR code Alex referenced for your squid1 and
remove the 'ssl' options from the cache_peer.



Also, please remove DONT_VERIFY_PEER. It hides important problems from
*you* while sill letting them cause major issues to clients. It is not
even useful for debugging because of that hiding.

Also, "sslproxy_cert_error allow all" is a similarly bad idea. There are
*some* errors which you may need to allow, but not everything. Use this
directive with great care. Look into every error that shows up. Properly
fix as many as you can first. Only after that hide the ones which are
still necessary to ignore.


Amos


From rousskov at measurement-factory.com  Thu Mar 21 03:36:55 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Mar 2019 21:36:55 -0600
Subject: [squid-users] Non-transparent proxy with cache_peer and ssl_bump
In-Reply-To: <fab68442-7a7d-ef01-be52-8326a74a885f@treenet.co.nz>
References: <D3EDBD3399484070B6AFAE9296461DC5@OhrSomayach>
 <527c1b32-fd70-ca91-ead9-f9c085390e2a@measurement-factory.com>
 <fab68442-7a7d-ef01-be52-8326a74a885f@treenet.co.nz>
Message-ID: <54a00e0c-6013-c2d7-b94c-c2570f0c2c62@measurement-factory.com>

On 3/20/19 9:22 PM, Amos Jeffries wrote:
> On 21/03/19 10:58 am, Alex Rousskov wrote:
>> On 3/20/19 3:23 PM, Yosi Greenfield wrote:
>>
>>> ssl_bump splice step3 NoBump
>>> ssl_bump bump step3
>>
>>> cache_peer proxy2.ourserver.com ... ssl


>> Forwarding most SslBump-related connections to cache_peers is still
>> unsupported by official Squids, including Squid v3 and v4.

> Er, sending the bumped https:// requests to a cache_peer with 'ssl' does
> work, though not advised.

Sending requests bumped at step2 or step3 does not work[1]. Requests
bumped at step1 are a rare and irrelevant-to-the-config-in-question
exception. That is why I said "most".

Alex.
[1] https://github.com/squid-cache/squid/blob/master/src/FwdState.cc#L883


From augustus_meyer at gmx.net  Thu Mar 21 07:15:47 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Thu, 21 Mar 2019 02:15:47 -0500 (CDT)
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <8dc16f6c-ee81-1073-072d-0d68066337d4@treenet.co.nz>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
 <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
 <1553097357921-0.post@n4.nabble.com>
 <8dc16f6c-ee81-1073-072d-0d68066337d4@treenet.co.nz>
Message-ID: <1553152547692-0.post@n4.nabble.com>

In short words, there is _no_ safe method to disable cache.log

Reason to disable cache.log most of all is because of this kind of messages:

2019/03/20 22:41:43 kid1| SECURITY ALERT: Host header forgery detected on
local=31.13.93.35:443 remote=10.1.0.202:51283 FD 194 flags=33 (local IP does
not match any domain IP)
2019/03/20 22:41:43 kid1| SECURITY ALERT: on URL: www.facebook.com:443

Which is a rather old issue, I have found no fix for until now.
A logrotate every 8h keeps the logsize at about 0.5MB, but even that is
precious RAM on an embedded system.
Unfortunately, AFAIK, there is no filter just to drop these messages.









--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Mar 21 09:53:06 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Mar 2019 22:53:06 +1300
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <1553152547692-0.post@n4.nabble.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
 <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
 <1553097357921-0.post@n4.nabble.com>
 <8dc16f6c-ee81-1073-072d-0d68066337d4@treenet.co.nz>
 <1553152547692-0.post@n4.nabble.com>
Message-ID: <d8079cbf-c494-af3b-32a3-319ce0f70e4e@treenet.co.nz>

On 21/03/19 8:15 pm, reinerotto wrote:
> In short words, there is _no_ safe method to disable cache.log
> 
> Reason to disable cache.log most of all is because of this kind of messages:
> 
> 2019/03/20 22:41:43 kid1| SECURITY ALERT: Host header forgery detected on
> local=31.13.93.35:443 remote=10.1.0.202:51283 FD 194 flags=33 (local IP does
> not match any domain IP)
> 2019/03/20 22:41:43 kid1| SECURITY ALERT: on URL: www.facebook.com:443
> 
> Which is a rather old issue, I have found no fix for until now.

Those are showing up because of your "-d 1" command line option. Remove
that option or set it to 0 instead of 1.

> A logrotate every 8h keeps the logsize at about 0.5MB, but even that is
> precious RAM on an embedded system.
> Unfortunately, AFAIK, there is no filter just to drop these messages.
> 

The filter on cache.log is debug level and/or section:

 0 - critical
 1 - important
 2 - protocol messages
 3-8 - various debug info
 9 - data dumps

The default level is 0.

Amos


From squid3 at treenet.co.nz  Thu Mar 21 11:28:53 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Mar 2019 00:28:53 +1300
Subject: [squid-users] security_file_certgen problem
In-Reply-To: <1184497381.8828258.1553115876872@mail.yahoo.com>
References: <1184497381.8828258.1553115876872.ref@mail.yahoo.com>
 <1184497381.8828258.1553115876872@mail.yahoo.com>
Message-ID: <3edae3c7-70e5-91c7-35e6-65df63760a8f@treenet.co.nz>

On 21/03/19 10:04 am, leomessi983 wrote:
>> Have you initialized the /var/lib/ssl_db directory using the
>> low-privilege account Squid operates as?
> Yes i use -c option and set permissions for nobody and nogroup user which squid use!
> 

If you are using Debian packages, or packages based on the Debian
official .deb the user account is 'proxy'. That could be the problem.


>> The helper should have output a message before it shutdown. If that
>> managed to get written it would occur somewhere before this line in your
>> cache.log.
> After squid showed that warning  fatal error accrued and some termination errors!
> 

Yes that is clear. That warning is the "last straw" at the *end* as
Squid gives up on the helper. The helper will already have aborted maybe
10 times leading up to that and should have printed its reason for the
abort. So I asked what was going on *before* the warning ?


> I usedsecurity_file_certgen helper from squid4.3 source files and then i
> created a .deb package from my squid 4.6 compiled files and former helpers!
> then squid runs perfectly!!
> I think security_file_certgen helper in squid 4.6 is the problem!!
> 
> Also when i copy cecutity_file_certgen helper from older squid 4.3 to my new machine whit squid 4.6 it is OK !!!!!
> 

That is odd. The only changes related to that helper between those
version is improved support for OpenSSL 1.0.* libraries and deprecated API.


Amos


From secinfo.pcim at arz.at  Thu Mar 21 17:07:38 2019
From: secinfo.pcim at arz.at (Secinfo)
Date: Thu, 21 Mar 2019 18:07:38 +0100
Subject: [squid-users] Squid 3.5.28 crash regulary when many requests occur
Message-ID: <OF2C18C8B5.71089D87-ONC12583C4.005DD713-C12583C4.005E1562@arz.at>

Hi we are using Squid as a Caching proxy for out patch deployment.

Since we moved to version 3.5.28 on windows Server ,we have problems in 
our
main spots.

typical cache.log entry?s are
2019/03/12 14:12:00 kid1| Using Least Load store dir selection
2019/03/12 14:12:00 kid1| Set Current Directory to
C:\smart\Squid\var\cache\squid
2019/03/12 14:12:00 kid1| Finished loading MIME types and icons.
2019/03/12 14:12:00 kid1| HTCP Disabled.
2019/03/12 14:12:00 kid1| Squid plugin modules loaded: 0
2019/03/12 14:12:00 kid1| Adaptation support is off.
2019/03/12 14:12:00 kid1| Accepting HTTP Socket connections at 
local=[::]:81
remote=[::] FD 12 flags=9
2019/03/12 14:12:00 kid1| Done reading F:\LEMSS-Content swaplog (743
entries)
2019/03/12 14:12:00 kid1| Finished rebuilding storage from disk.
2019/03/12 14:12:00 kid1|       482 Entries scanned
2019/03/12 14:12:00 kid1|         0 Invalid entries.
2019/03/12 14:12:00 kid1|         0 With invalid flags.
2019/03/12 14:12:00 kid1|       223 Objects loaded.
2019/03/12 14:12:00 kid1|         0 Objects expired.
2019/03/12 14:12:00 kid1|         0 Objects cancelled.
2019/03/12 14:12:00 kid1|       119 Duplicate URLs purged.
2019/03/12 14:12:00 kid1|       140 Swapfile clashes avoided.
2019/03/12 14:12:00 kid1|   Took 0.14 seconds (1588.31 objects/sec).
2019/03/12 14:12:00 kid1| Beginning Validation Procedure
2019/03/12 14:12:00 kid1|   Completed Validation Procedure
2019/03/12 14:12:00 kid1|   Validated 223 Entries
2019/03/12 14:12:00 kid1|   store_swap_size = 6227808.00 KB
2019/03/12 14:12:01 kid1| storeLateRelease: released 0 objects
2019/03/12 14:12:03 kid1| DiskThreadsDiskFile::openDone: (2) No such file 
or
directory
2019/03/12 14:12:03 kid1|                F:\LEMSS-Content/00/00/00000025
2019/03/12 14:12:03 kid1| WARNING: 1 swapin MD5 mismatches
2019/03/12 14:12:03 kid1| DiskThreadsDiskFile::openDone: (2) No such file 
or
directory
2019/03/12 14:12:03 kid1|                F:\LEMSS-Content/00/00/00000025
2019/03/12 14:12:03 kid1| WARNING: 1 swapin MD5 mismatches
2019/03/12 14:12:03 kid1| DiskThreadsDiskFile::openDone: (2) No such file 
or
directory
2019/03/12 14:12:03 kid1|                F:\LEMSS-Content/00/00/0000002D
2019/03/12 14:12:24 kid1| DiskThreadsDiskFile::openDone: (16) Device or
resource busy
2019/03/12 14:12:24 kid1|                F:\LEMSS-Content/00/00/000000CC
2019/03/12 14:12:24 kid1| DiskThreadsDiskFile::openDone: (2) No such file 
or
directory
2019/03/12 14:12:24 kid1|                F:\LEMSS-Content/00/00/00000079
2019/03/12 14:12:31 kid1| DiskThreadsDiskFile::openDone: (2) No such file 
or
directory
2019/03/12 14:12:31 kid1|                F:\LEMSS-Content/00/00/00000008
2019/03/12 14:12:59 kid1| DiskThreadsDiskFile::openDone: (13) Permission
denied


after that squid service is running but does not keep connections ore
reastablish connections.
Squid service has to be restarted.

Depending on the location we have 100 Mbit to 10 Gbit.

our squid.conf at the main branches is like
#Local Settings
acl all src all
acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl to_localhost dst 127.0.0.0/8

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed

acl localnet src 10.0.0.0/8              # RFC1918 possible internal 
network
acl localnet src 172.16.0.0/12           # RFC1918 possible internal 
network
acl localnet src 192.168.0.0/16          # RFC1918 possible internal 
network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl Windowsupdate dstdomain lemss.m287.local

acl SSL_ports port 443
acl Safe_ports port 80                           # http
acl Safe_ports port 21                           # ftp
acl Safe_ports port 443                          # https
acl Safe_ports port 70                           # gopher
acl Safe_ports port 210                          # wais
acl Safe_ports port 1025-65535           # unregistered ports
acl Safe_ports port 280                          # http-mgmt
acl Safe_ports port 488                          # gss-http
acl Safe_ports port 591                          # filemaker
acl Safe_ports port 777                          # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#

# Only allow cachemgr access from localhost
http_access allow localhost manager
# http_access allow windowsudpate localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost
http_access allow Windowsupdate

# And finally deny all other access to this proxy
http_access deny all

# Limit number of days to keep logs
# logfile_rotate 2

# Squid normally listens to port 3128
http_port 81

# Delay parameters
delay_pools 1
delay_class 1 1
delay_parameters 1 none -1/-1
delay_access 1 allow localnet


# Uncomment the line below to enable disk caching - path format is
/cygdrive/<full path to cache folder>, i.e.
cache_dir aufs F:\LEMSS-Content 10000 16 256
cache_mem 2048 MB

# Leave coredumps in the first cache dir
coredump_dir C:\smart\Squid\var\cache\squid

#debug_options All,1 33,2 28,9

# Add any of your own refresh_pattern entries above these.
# refresh_pattern -i
lemss.m287.local/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 
43200
80% 129600 reload-into-ims
refresh_pattern -i
lemss.m287.local/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 
43200
80% 129600 reload-into-ims
refresh_pattern ^ftp:                            1440            20%  
10080
refresh_pattern ^gopher:                 1440            0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0              0%              0
refresh_pattern .                                0               20%  4320

dns_v4_first on

dns_nameservers 10.1.19.203 10.1.19.204

# pipeline_prefetch 2

max_filedescriptors 3200

range_offset_limit 2000 MB windowsupdate

maximum_object_size 2000 MB
collapsed_forwarding on

quick_abort_min -1

thats our squid.conf
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190321/4a4cbf0e/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar 22 12:24:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 Mar 2019 01:24:24 +1300
Subject: [squid-users] Squid 3.5.28 crash regulary when many requests
 occur
In-Reply-To: <OF2C18C8B5.71089D87-ONC12583C4.005DD713-C12583C4.005E1562@arz.at>
References: <OF2C18C8B5.71089D87-ONC12583C4.005DD713-C12583C4.005E1562@arz.at>
Message-ID: <6252279e-838d-4711-fbab-45ab232b43ed@treenet.co.nz>

On 22/03/19 6:07 am, Secinfo wrote:
> Hi we are using Squid as a Caching proxy for out patch deployment.
> 
> Since we moved to version 3.5.28 on windows Server ,we have problems in our
> main spots.
> 
> typical cache.log entry?s are

... snip a lot of normal startup messages ...


These following are side effects of crashing. That leaves any partially
received and open files with broken content. Squid has detected and
resolved these issues automatically.

> 2019/03/12 14:12:03 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
> directory
> 2019/03/12 14:12:03 kid1| ? ? ? ? ? ? ? ? ?F:\LEMSS-Content/00/00/00000025
> 2019/03/12 14:12:03 kid1| WARNING: 1 swapin MD5 mismatches
> 2019/03/12 14:12:03 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
> directory
...

When you have an actual crash situation (as opposed to a FATAL event,
exception, or assertion - where Squid shuts itself down cleanly) we need
a backtrace from the crash to identify what is going on.

I'm not sure exactly how one does that for Windows servers these days,
cygwin should in theory have the same or similar tools as Unix. Details
of what we need can be found at:
<https://wiki.squid-cache.org/SquidFaq/BugReporting#crashes_and_core_dumps>

Once you have a backtrace you may find existing bugs in our Bugzilla
(<https://bugs.squid-cache.org/>) about this issue. Remember to search
closed bugs as it may only be fixed in the current stable series. If
there is nothing already on record please report it.

Amos


From augustus_meyer at gmx.net  Fri Mar 22 18:19:39 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Fri, 22 Mar 2019 13:19:39 -0500 (CDT)
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <d8079cbf-c494-af3b-32a3-319ce0f70e4e@treenet.co.nz>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
 <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
 <1553097357921-0.post@n4.nabble.com>
 <8dc16f6c-ee81-1073-072d-0d68066337d4@treenet.co.nz>
 <1553152547692-0.post@n4.nabble.com>
 <d8079cbf-c494-af3b-32a3-319ce0f70e4e@treenet.co.nz>
Message-ID: <1553278779475-0.post@n4.nabble.com>

Are you shure, default level=0 ?

I have squid 4.4; either started simply using "squid" or "squid -d 0".
squid.conf does not contain any line
debug_options .....


however, in all cases messages like
2019/03/22 18:06:04 kid1| SECURITY ALERT: on URL: edge-mqtt.facebook.com:443
2019/03/22 18:06:04 kid1| SECURITY ALERT: Host header forgery detected on
local=31.13.85.2:443 remote=192.168.182.8:5623                0 FD 35
flags=33 (local IP does not match any domain IP)

fill up cache.log.





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Mar 24 14:32:34 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Mar 2019 03:32:34 +1300
Subject: [squid-users] attempting to disable (or mute) logs
In-Reply-To: <1553278779475-0.post@n4.nabble.com>
References: <DM5PR19MB1579773FC5F93258C4B994E8CD4A0@DM5PR19MB1579.namprd19.prod.outlook.com>
 <3a342eb7-002c-d5ba-95ce-64efc8e35bdd@urlfilterdb.com>
 <DM5PR19MB157911A075179448D8547414CD440@DM5PR19MB1579.namprd19.prod.outlook.com>
 <72431cad-ece8-6e24-add1-33c0bda5bbb1@treenet.co.nz>
 <DM5PR19MB1579E6EF2D883E592E2EF66ECD400@DM5PR19MB1579.namprd19.prod.outlook.com>
 <277e7aac-8a80-a9f0-db39-c24f8277ad72@treenet.co.nz>
 <1553097357921-0.post@n4.nabble.com>
 <8dc16f6c-ee81-1073-072d-0d68066337d4@treenet.co.nz>
 <1553152547692-0.post@n4.nabble.com>
 <d8079cbf-c494-af3b-32a3-319ce0f70e4e@treenet.co.nz>
 <1553278779475-0.post@n4.nabble.com>
Message-ID: <0e17c0aa-81ec-4c5a-cf1c-153313611541@treenet.co.nz>

On 23/03/19 7:19 am, reinerotto wrote:
> Are you shure, default level=0 ?
> 

Sorry, yes. Checking it I see ALL,1 is default.

So to silence add this to squid.conf:

 debug_options ALL,0


Amos


From heiler.bemerguy at cinbesa.com.br  Mon Mar 25 19:15:12 2019
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 25 Mar 2019 16:15:12 -0300
Subject: [squid-users] How to catch a big spender ?
Message-ID: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>

Hail,

We've seen some high upload bandwidth usage on our router graphs and 
we'd like to know what was happening at that time...

Any tools or tricks to know that? I bet most of you have had this 
"curiosity" already too lol


-- 
Best Regards,

Heiler Bemerguy - CINBESA
Analista de Redes, Wi-Fi,
Virtualiza??o e Servi?os Internet
(55) 91 98151-4894



From bruno.larini at riosoft.com.br  Mon Mar 25 20:03:26 2019
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Mon, 25 Mar 2019 17:03:26 -0300
Subject: [squid-users] How to catch a big spender ?
In-Reply-To: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
References: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
Message-ID: <64729425-0b82-998f-0b1a-4ed7ff05f641@riosoft.com.br>

Em 25/03/2019 16:15, Heiler Bemerguy escreveu:
> Hail,
>
> We've seen some high upload bandwidth usage on our router graphs and 
> we'd like to know what was happening at that time...
>
> Any tools or tricks to know that? I bet most of you have had this 
> "curiosity" already too lol
>
>
Search for "sqstat". The tool is very simple, but it works for me.


From info at microlinux.fr  Mon Mar 25 20:10:53 2019
From: info at microlinux.fr (Nicolas Kovacs)
Date: Mon, 25 Mar 2019 21:10:53 +0100
Subject: [squid-users] How to catch a big spender ?
In-Reply-To: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
References: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
Message-ID: <1f8e9e06-dec9-dae9-3d6a-b5e86a95efd9@microlinux.fr>

Le 25/03/2019 ? 20:15, Heiler Bemerguy a ?crit?:
> We've seen some high upload bandwidth usage on our router graphs and
> we'd like to know what was happening at that time...
> 
> Any tools or tricks to know that? I bet most of you have had this
> "curiosity" already too lol

Here's what I use to catch bandwidth hogs in our local network:

https://www.microlinux.fr/squidanalyzer-centos-7/

Cheers,

Niki

-- 
Microlinux - Solutions informatiques durables
7, place de l'?glise - 30730 Montpezat
Site : https://www.microlinux.fr
Mail : info at microlinux.fr
T?l. : 04 66 63 10 32


From squid3 at treenet.co.nz  Tue Mar 26 08:01:27 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Mar 2019 21:01:27 +1300
Subject: [squid-users] How to catch a big spender ?
In-Reply-To: <1f8e9e06-dec9-dae9-3d6a-b5e86a95efd9@microlinux.fr>
References: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
 <1f8e9e06-dec9-dae9-3d6a-b5e86a95efd9@microlinux.fr>
Message-ID: <40e03bba-9863-ef57-2de2-76f80de92353@treenet.co.nz>

On 26/03/19 9:10 am, Nicolas Kovacs wrote:
> Le 25/03/2019 ? 20:15, Heiler Bemerguy a ?crit?:
>> We've seen some high upload bandwidth usage on our router graphs and
>> we'd like to know what was happening at that time...
>>
>> Any tools or tricks to know that? I bet most of you have had this
>> "curiosity" already too lol
> 
> Here's what I use to catch bandwidth hogs in our local network:
> 
> https://www.microlinux.fr/squidanalyzer-centos-7/
> 

Please be aware that tools based on log files only include completed
transactions. They do not include information about still-active
transactions, such as CONNECT tunnels for HTTPS traffic.


To identify events where an ongoing transaction or tunnel is using
excess bandwidth one needs to use the cache manager or SNMP interfaces
of Squid. The earlier mentioned sqstat and similar tools do that.

Amos


From heiler.bemerguy at cinbesa.com.br  Tue Mar 26 18:29:57 2019
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Tue, 26 Mar 2019 15:29:57 -0300
Subject: [squid-users] How to catch a big spender ?
In-Reply-To: <40e03bba-9863-ef57-2de2-76f80de92353@treenet.co.nz>
References: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
 <1f8e9e06-dec9-dae9-3d6a-b5e86a95efd9@microlinux.fr>
 <40e03bba-9863-ef57-2de2-76f80de92353@treenet.co.nz>
Message-ID: <6efb33fa-4618-d3a8-dbb5-cddd82d4e409@cinbesa.com.br>

Thank you all for the replies but we actually need to get information 
about a transfer (generally UPLOADS) done in the past hours or days .. 
they're not active anymore.

Because we have some bandwidth usage graphs and sometimes we see a peak 
on upload bandwidth and my manager asks "who did that ???"......

We already have "squid analyzer" running but it doesn't seem to be 
useful for uploads or to get something using TIME as reference ... like 
"who used that much bandwidth at 3 AM ?"


Best Regards,

Heiler Bemerguy - CINBESA
Analista de Redes, Wi-Fi,
Virtualiza??o e Servi?os Internet
(55) 91 98151-4894

Em 26/03/2019 05:01, Amos Jeffries escreveu:
> On 26/03/19 9:10 am, Nicolas Kovacs wrote:
>> Le 25/03/2019 ? 20:15, Heiler Bemerguy a ?crit?:
>>> We've seen some high upload bandwidth usage on our router graphs and
>>> we'd like to know what was happening at that time...
>>>
>>> Any tools or tricks to know that? I bet most of you have had this
>>> "curiosity" already too lol
>> Here's what I use to catch bandwidth hogs in our local network:
>>
>> https://www.microlinux.fr/squidanalyzer-centos-7/
>>
> Please be aware that tools based on log files only include completed
> transactions. They do not include information about still-active
> transactions, such as CONNECT tunnels for HTTPS traffic.
>
>
> To identify events where an ongoing transaction or tunnel is using
> excess bandwidth one needs to use the cache manager or SNMP interfaces
> of Squid. The earlier mentioned sqstat and similar tools do that.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From eliezer at ngtech.co.il  Wed Mar 27 09:47:40 2019
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Wed, 27 Mar 2019 11:47:40 +0200
Subject: [squid-users] How to catch a big spender ?
In-Reply-To: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
References: <1955e0f9-4e10-02f1-a8ba-c50888915a63@cinbesa.com.br>
Message-ID: <00a501d4e482$1e5ba170$5b12e450$@ngtech.co.il>

What about some sort of netflow service?
In the ISP world the radius server does accounting and it's a bit different
ntop have a network probe that might help you:
https://www.ntop.org/

I am using routers to do this kind of a task instead of the proxy.
The other option is to use QOS between the squid machine.
You probably can add some rules dynamically with some kind of CAP and to mark the clients connection with iptables.
I don't remember if iptables have quota or not.
And you can use some kind of ipset that can be updated dynamically by an external software.
The client will be added to the "too much user" mark and then he can download and upload with a speed CAP/QOS.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Heiler Bemerguy
Sent: Monday, March 25, 2019 9:15 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] How to catch a big spender ?

Hail,

We've seen some high upload bandwidth usage on our router graphs and we'd like to know what was happening at that time...

Any tools or tricks to know that? I bet most of you have had this "curiosity" already too lol


-- 
Best Regards,

Heiler Bemerguy - CINBESA
Analista de Redes, Wi-Fi,
Virtualiza??o e Servi?os Internet
(55) 91 98151-4894

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid at buglecreek.com  Wed Mar 27 21:17:19 2019
From: squid at buglecreek.com (squid at buglecreek.com)
Date: Wed, 27 Mar 2019 17:17:19 -0400
Subject: [squid-users] Deny_Info TCP_RESET
Message-ID: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>

Operating in reverse proxy mode.   I'm trying to send a TCP reset in response to the acl below:

acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
deny_info TCP_RESET example_url
http_access deny example_url

Looking at the packets I see the following response:

HTTP/1.0 403 Forbidden
Server: squid
Mime-Version: 1.0
Date: Wed, 27 Mar 2019 20:36:20 GMT
Content-Type: text/html
Content-Length: 5
X-Squid-Error: TCP_RESET 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from www.example.com
X-Cache-Lookup: NONE from www.example.com:80
Via: 1.0 www.example.com (squid)
Connection: keep-alive

reset

Squid sends the headers and the word reset.  Then future requests seem to work as expected.  No headers are sent, the word reset isn't sent and squid ultimately sends a RST and ACK.

Then after some time  or squid gets reloaded the headers are sent again, then things seem to work as I would expect.

I'm not sure if it will help, but wanted to try the following to see if that will get rid of that initial header being sent. 

acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
deny_info TCP_RESET example_url
http_reply_access deny example_url

Do I still need the http_access deny example_url in addition to the http_reply_access deny example_url statement, or does the http_reply_access take the place of http_access statement:


acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
deny_info TCP_RESET example_url
http_reply_access deny example_url
http_access deny example_url


From rousskov at measurement-factory.com  Wed Mar 27 22:23:28 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Mar 2019 16:23:28 -0600
Subject: [squid-users] Deny_Info TCP_RESET
In-Reply-To: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>
References: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>
Message-ID: <fc4a7de9-b0d6-fb86-12af-71ad4076b8d5@measurement-factory.com>

On 3/27/19 3:17 PM, squid at buglecreek.com wrote:
> Operating in reverse proxy mode.   I'm trying to send a TCP reset in response to the acl below:
> 
> acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
> deny_info TCP_RESET example_url
> http_access deny example_url
> 
> Looking at the packets I see the following response:
> 
> HTTP/1.0 403 Forbidden
> Server: squid
> Mime-Version: 1.0
> Date: Wed, 27 Mar 2019 20:36:20 GMT
> Content-Type: text/html
> Content-Length: 5
> X-Squid-Error: TCP_RESET 0
> Vary: Accept-Language
> Content-Language: en
> X-Cache: MISS from www.example.com
> X-Cache-Lookup: NONE from www.example.com:80
> Via: 1.0 www.example.com (squid)
> Connection: keep-alive
> 
> reset
> 
> Squid sends the headers and the word reset.  

FWIW, I cannot reproduce this problem in simple Squid v5 tests. I get
the expected behavior (a connection reset) and the expected cache.log
entry (with debug_options set to ALL,9):

2019/03/27 16:05:49.365| 4,2| errorpage.cc(714) errorAppendEntry: RSTing
this reply

2019/03/27 16:05:49.372| 33,3| client_side.cc(1596)
clientProcessRequestFinished: Sending TCP RST on local=127.0.0.1:3128
remote=127.0.0.1:46922 FD 13 flags=1

If you can reproduce with Squid v4 or later, I suggest sharing your
(compressed) ALL,9 cache.log while reproducing the problem with a single
denied transaction.

Are you using ICAP or eCAP REQMOD adaptation?

Can you think of any ACL-driven directives in your squid.conf that might
be accessed _after_ an "http_access deny" rule matches? Due to an old
configuration design bug, deny_info may "forget" that access was denied
by the example_url ACL if Squid has to evaluate other ACLs before
deny_info is checked. A debugging log requested above would expose such
problems.


> I'm not sure if it will help, but wanted to try the following to see
> if that will get rid of that initial header being sent.

> acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
> deny_info TCP_RESET example_url
> http_reply_access deny example_url

> Do I still need the http_access deny example_url in addition to the
> http_reply_access deny example_url statement, or does the
> http_reply_access take the place of http_access statement:

IIRC, http_reply_access does not apply to transactions denied by
http_access -- Squid does not check its own access denial response. Bugs
notwithstanding, your earlier/simpler configuration should work.

Alex.


From yanier at eleccav.une.cu  Wed Mar 27 23:29:08 2019
From: yanier at eleccav.une.cu (Yanier Salazar Sanchez)
Date: Wed, 27 Mar 2019 19:29:08 -0400
Subject: [squid-users] delay_pools don't work on squid 4.4
Message-ID: <019301d4e4f4$e07c4260$a174c720$@eleccav.une.cu>

Good afternoon:
I have squid4.4 installed in linux on ubuntu 18.04.1 and I have the
following problem with the delay_pools aren't working properly, the squid
identify them with their number but do not comply with the speed limits
They are functioning as if they didn't have speed limits
Someone has some idea why this happens.
Previously I had squid 3.5 and they worked perfectly. I don't compile this
version of squid, but I download a precompiled version .deb.

A clarification I have kerberos authentication.

Thanks for your time

 

I'm so sorry for my bad English

 

 

/etc/squid# squid -v

Squid Cache: Version 4.4

Service Name: squid

Debian linux

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid' '--srcdir=.' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g
-O2 -fdebug-prefix-map=/build/squid-4.4=. -fstack-protector-strong -Wformat
-Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-z,relro
-Wl,-z,now -Wl,--as-needed -latomic' '--enable-build-info=Debian linux'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB
' '--enable-auth-digest=file,LDAP'
'--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,SMB_LM'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,se
ssion,SQL_session,time_quota,unix_group,wbinfo_group'
'--enable-security-cert-validators=fake'
'--enable-storeid-rewrite-helpers=file' '--enable-url-rewrite-helpers=fake'
'--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos'
'--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-filedescriptors=65536' '--with-large-files'
'--with-default-user=proxy' '--with-gnutls' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2
-fdebug-prefix-map=/build/squid-4.4=. -fstack-protector-strong -Wformat
-Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now
-Wl,--as-needed -latomic' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
'CXXFLAGS=-g -O2 -fdebug-prefix-map=/build/squid-4.4=.
-fstack-protector-strong -Wformat -Werror=format-security'

 

squidstat shows me this

remote=10.100.1.150:49791

local=10.100.2.5:3128

uri=r4---sn-xuxaxasda234-i58e.googlevideo.com:443

bytes=3429753

seconds=20

username=juan

delay_pool=4

connection=0x55fd2432a158

 

squid.conf 

acl youtube url_regex .youtube.com .youtube.com:443 .googlevideo.com
.googlevideo.com:443

delay_initial_bucket_level 90

 

delay_class 4 2

delay_access 4 allow youtube

delay_parameters 4 18000/19000 10000/11000

delay_access 4 deny all

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190327/9ee65ace/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar 28 03:17:36 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Mar 2019 21:17:36 -0600
Subject: [squid-users] delay_pools don't work on squid 4.4
In-Reply-To: <019301d4e4f4$e07c4260$a174c720$@eleccav.une.cu>
References: <019301d4e4f4$e07c4260$a174c720$@eleccav.une.cu>
Message-ID: <acf244ab-0e20-2848-2b3f-e7529013f51e@measurement-factory.com>

On 3/27/19 5:29 PM, Yanier Salazar Sanchez wrote:

> I have squid4.4 installed in linux on ubuntu 18.04.1 and [...]
> delay_pools are functioning as if they didn?t have speed limits

While testing peering support for SslBump transactions[1], Factory has
discovered that delay pools are broken in Squid v4 and v5 as far as
tunneled traffic is concerned[2]. I do not have a stand-alone fix for
that bug -- too many changes surrounding the fixed code in that pull
request, but it is likely that the bug will be fixed in v5 if our pull
request is accepted. The proposed code is available for testing[3].

[1] https://github.com/squid-cache/squid/pull/380
[2] https://github.com/squid-cache/squid/pull/380/commits/679645f
[3] https://github.com/squid-cache/squid/pull/380.patch


HTH,

Alex.


> /etc/squid# squid -v
> 
> Squid Cache: Version 4.4
> 
> Service Name: squid
> 
> Debian linux
> 
> configure options:? '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid' '--srcdir=.'
> '--disable-maintainer-mode' '--disable-dependency-tracking'
> '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2
> -fdebug-prefix-map=/build/squid-4.4=. -fstack-protector-strong -Wformat
> -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-z,relro
> -Wl,-z,now -Wl,--as-needed -latomic' '--enable-build-info=Debian linux'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man'
> '--enable-inline' '--disable-arch-native' '--enable-async-io=8'
> '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,SMB_LM'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
> '--enable-security-cert-validators=fake'
> '--enable-storeid-rewrite-helpers=file'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-icmp' '--enable-zph-qos' '--enable-ecap'
> '--disable-translation' '--with-swapdir=/var/spool/squid'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--with-filedescriptors=65536' '--with-large-files'
> '--with-default-user=proxy' '--with-gnutls' '--enable-linux-netfilter'
> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2
> -fdebug-prefix-map=/build/squid-4.4=. -fstack-protector-strong -Wformat
> -Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now
> -Wl,--as-needed -latomic' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
> 'CXXFLAGS=-g -O2 -fdebug-prefix-map=/build/squid-4.4=.
> -fstack-protector-strong -Wformat -Werror=format-security'
> 
> ?
> 
> squidstat shows me this
> 
> remote=10.100.1.150:49791
> 
> local=10.100.2.5:3128
> 
> uri=r4---sn-xuxaxasda234-i58e.googlevideo.com:443
> 
> bytes=3429753
> 
> seconds=20
> 
> username=juan
> 
> delay_pool=4
> 
> connection=0x55fd2432a158
> 
> ?
> 
> squid.conf
> 
> acl youtube url_regex .youtube.com .youtube.com:443 .googlevideo.com
> .googlevideo.com:443
> 
> delay_initial_bucket_level 90
> 
> ?
> 
> delay_class 4 2
> 
> delay_access 4 allow youtube
> 
> delay_parameters 4 18000/19000 10000/11000
> 
> delay_access 4 deny all
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From creditu at eml.cc  Thu Mar 28 14:13:59 2019
From: creditu at eml.cc (creditu at eml.cc)
Date: Thu, 28 Mar 2019 10:13:59 -0400
Subject: [squid-users] Deny_Info TCP_RESET
In-Reply-To: <fc4a7de9-b0d6-fb86-12af-71ad4076b8d5@measurement-factory.com>
References: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>
 <fc4a7de9-b0d6-fb86-12af-71ad4076b8d5@measurement-factory.com>
Message-ID: <10c50af1-7775-444a-b6f1-f794c8b1eea3@www.fastmail.com>



On Wed, Mar 27, 2019, at 4:23 PM, Alex Rousskov wrote:
> On 3/27/19 3:17 PM, squid at buglecreek.com wrote:
> > Operating in reverse proxy mode.   I'm trying to send a TCP reset in response to the acl below:
> > 
> > acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
> > deny_info TCP_RESET example_url
> > http_access deny example_url
> > 
> > Looking at the packets I see the following response:
> > 
> > HTTP/1.0 403 Forbidden
> > Server: squid
> > Mime-Version: 1.0
> > Date: Wed, 27 Mar 2019 20:36:20 GMT
> > Content-Type: text/html
> > Content-Length: 5
> > X-Squid-Error: TCP_RESET 0
> > Vary: Accept-Language
> > Content-Language: en
> > X-Cache: MISS from www.example.com
> > X-Cache-Lookup: NONE from www.example.com:80
> > Via: 1.0 www.example.com (squid)
> > Connection: keep-alive
> > 
> > reset
> > 
> > Squid sends the headers and the word reset.  
> 
> FWIW, I cannot reproduce this problem in simple Squid v5 tests. I get
> the expected behavior (a connection reset) and the expected cache.log
> entry (with debug_options set to ALL,9):
> 
> 2019/03/27 16:05:49.365| 4,2| errorpage.cc(714) errorAppendEntry: RSTing
> this reply
> 
> 2019/03/27 16:05:49.372| 33,3| client_side.cc(1596)
> clientProcessRequestFinished: Sending TCP RST on local=127.0.0.1:3128
> remote=127.0.0.1:46922 FD 13 flags=1
> 
> If you can reproduce with Squid v4 or later, I suggest sharing your
> (compressed) ALL,9 cache.log while reproducing the problem with a single
> denied transaction.
> 
> Are you using ICAP or eCAP REQMOD adaptation?
> 
> Can you think of any ACL-driven directives in your squid.conf that might
> be accessed _after_ an "http_access deny" rule matches? Due to an old
> configuration design bug, deny_info may "forget" that access was denied
> by the example_url ACL if Squid has to evaluate other ACLs before
> deny_info is checked. A debugging log requested above would expose such
> problems.
> 
> 
> > I'm not sure if it will help, but wanted to try the following to see
> > if that will get rid of that initial header being sent.
> 
> > acl example_url url_regex -i [^:]+://[^0-9]*.example.com.*
> > deny_info TCP_RESET example_url
> > http_reply_access deny example_url
> 
> > Do I still need the http_access deny example_url in addition to the
> > http_reply_access deny example_url statement, or does the
> > http_reply_access take the place of http_access statement:
> 
> IIRC, http_reply_access does not apply to transactions denied by
> http_access -- Squid does not check its own access denial response. Bugs
> notwithstanding, your earlier/simpler configuration should work.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
Thank you for the reply and I should have mentioned that we are on a pre 4 version at the moment with plans to upgrade.  I was looking at a way to fix this behavior in the meantime.   Not using ICAP or eCAP REQMODl.  I don't see anything that would come after a http_access_deny rule matches but will check further.

Is using the    http_reply_access deny a viable option if all else fails to correct the issue until we can upgrade?  I'm still a bit confused on how to implement it to give it try.  Do I need to use  http_access  with http_reply_access or can http_reply_access be used by itself with deny_info?


From yanier at eleccav.une.cu  Thu Mar 28 15:30:39 2019
From: yanier at eleccav.une.cu (Yanier Salazar Sanchez)
Date: Thu, 28 Mar 2019 11:30:39 -0400
Subject: [squid-users] delay_pools don't work on squid 4.4
In-Reply-To: <acf244ab-0e20-2848-2b3f-e7529013f51e@measurement-factory.com>
References: <019301d4e4f4$e07c4260$a174c720$@eleccav.une.cu>
 <acf244ab-0e20-2848-2b3f-e7529013f51e@measurement-factory.com>
Message-ID: <02c601d4e57b$329264e0$97b72ea0$@eleccav.une.cu>

Thanks for reply, I had already read that version 4 doesn't work and I thought that bug was already fixed. I use squid without using SSlBump or splice. I will try to download that code and compile it for ubuntu 18.04

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Wednesday, March 27, 2019 11:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] delay_pools don't work on squid 4.4

On 3/27/19 5:29 PM, Yanier Salazar Sanchez wrote:

> I have squid4.4 installed in linux on ubuntu 18.04.1 and [...] 
> delay_pools are functioning as if they didn?t have speed limits

While testing peering support for SslBump transactions[1], Factory has discovered that delay pools are broken in Squid v4 and v5 as far as tunneled traffic is concerned[2]. I do not have a stand-alone fix for that bug -- too many changes surrounding the fixed code in that pull request, but it is likely that the bug will be fixed in v5 if our pull request is accepted. The proposed code is available for testing[3].

[1] https://github.com/squid-cache/squid/pull/380
[2] https://github.com/squid-cache/squid/pull/380/commits/679645f
[3] https://github.com/squid-cache/squid/pull/380.patch


HTH,

Alex.


> /etc/squid# squid -v
> 
> Squid Cache: Version 4.4
> 
> Service Name: squid
> 
> Debian linux
> 
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid' '--srcdir=.'
> '--disable-maintainer-mode' '--disable-dependency-tracking'
> '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 
> -fdebug-prefix-map=/build/squid-4.4=. -fstack-protector-strong 
> -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 
> -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' '--enable-build-info=Debian linux'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man'
> '--enable-inline' '--disable-arch-native' '--enable-async-io=8'
> '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,SMB_LM'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
> '--enable-security-cert-validators=fake'
> '--enable-storeid-rewrite-helpers=file'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-icmp' '--enable-zph-qos' '--enable-ecap'
> '--disable-translation' '--with-swapdir=/var/spool/squid'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--with-filedescriptors=65536' '--with-large-files'
> '--with-default-user=proxy' '--with-gnutls' '--enable-linux-netfilter'
> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 
> -fdebug-prefix-map=/build/squid-4.4=. -fstack-protector-strong 
> -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro 
> -Wl,-z,now -Wl,--as-needed -latomic' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
> 'CXXFLAGS=-g -O2 -fdebug-prefix-map=/build/squid-4.4=.
> -fstack-protector-strong -Wformat -Werror=format-security'
> 
>  
> 
> squidstat shows me this
> 
> remote=10.100.1.150:49791
> 
> local=10.100.2.5:3128
> 
> uri=r4---sn-xuxaxasda234-i58e.googlevideo.com:443
> 
> bytes=3429753
> 
> seconds=20
> 
> username=juan
> 
> delay_pool=4
> 
> connection=0x55fd2432a158
> 
>  
> 
> squid.conf
> 
> acl youtube url_regex .youtube.com .youtube.com:443 .googlevideo.com
> .googlevideo.com:443
> 
> delay_initial_bucket_level 90
> 
>  
> 
> delay_class 4 2
> 
> delay_access 4 allow youtube
> 
> delay_parameters 4 18000/19000 10000/11000
> 
> delay_access 4 deny all
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From rousskov at measurement-factory.com  Thu Mar 28 23:36:59 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Mar 2019 17:36:59 -0600
Subject: [squid-users] Deny_Info TCP_RESET
In-Reply-To: <10c50af1-7775-444a-b6f1-f794c8b1eea3@www.fastmail.com>
References: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>
 <fc4a7de9-b0d6-fb86-12af-71ad4076b8d5@measurement-factory.com>
 <10c50af1-7775-444a-b6f1-f794c8b1eea3@www.fastmail.com>
Message-ID: <817aa8e9-9cfc-6438-0df5-facbddaf2e8a@measurement-factory.com>

On 3/28/19 8:13 AM, creditu at eml.cc wrote:

> Is using the http_reply_access deny a viable option if all else fails
> to correct the issue until we can upgrade?

Probably it is not: I am not sure, but based on my quick reading of the
code and a basic test, http_reply_access does not support the "deny_info
TCP_RESET" feature at all. Only http_access (and possibly
adapted_http_access, but I did not check) supports that feature (bugs
notwithstanding).

In the ideal world, "deny_info TCP_RESET" would work regardless of which
directive (http_access, adapted_http_access/http_access2, or
http_reply_access) is denying access. I may have assumed that ideal in
my earlier response, but this is not what was hacked into Squid AFAICT.


> I'm still a bit confused on how to implement it to give it try.

There is pretty much no relationship between the three access checking
directives mentioned above. They are checked in their natural order, and
the first denial stops further transaction progress, with the denial
page delivered to the client. If you understand how http_access works,
you can apply that understanding to the other two directives.


> Do I need to use http_access with http_reply_access 

No, you do not:

* http_access checks whether HTTP client request should be forwarded (to
the server.

* http_reply_access checks whether HTTP server response should be
forwarded to the client. Naturally, there is no server response to speak
of if the request was previously denied using http_access.


> can http_reply_access be used by itself with deny_info?

You can, but http_reply_access does not support the TCP_RESET feature.
Fixes and fix sponsorship welcomed.


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Mar 29 01:14:31 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Mar 2019 19:14:31 -0600
Subject: [squid-users] Deny_Info TCP_RESET
In-Reply-To: <817aa8e9-9cfc-6438-0df5-facbddaf2e8a@measurement-factory.com>
References: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>
 <fc4a7de9-b0d6-fb86-12af-71ad4076b8d5@measurement-factory.com>
 <10c50af1-7775-444a-b6f1-f794c8b1eea3@www.fastmail.com>
 <817aa8e9-9cfc-6438-0df5-facbddaf2e8a@measurement-factory.com>
Message-ID: <4e42d66b-c5d5-56f0-6e2e-c2278364a492@measurement-factory.com>

On 3/28/19 5:36 PM, Alex Rousskov wrote:
> On 3/28/19 8:13 AM, creditu at eml.cc wrote:
>> Is using the http_reply_access deny a viable option if all else fails
>> to correct the issue until we can upgrade?

> Probably it is not: I am not sure, but based on my quick reading of the
> code and a basic test, http_reply_access does not support the "deny_info
> TCP_RESET" feature at all. Only http_access (and possibly
> adapted_http_access, but I did not check) supports that feature (bugs
> notwithstanding).

Also see Squid Bug 4585:
https://bugs.squid-cache.org/show_bug.cgi?id=4585

Alex.


From alex at dvm.esines.cu  Fri Mar 29 14:30:24 2019
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 29 Mar 2019 10:30:24 -0400
Subject: [squid-users] kerberos
Message-ID: <d1d9e732-4c32-1615-40ed-d4263b91d5ac@dvm.esines.cu>

Hello Community, I just compiled my squid 4. Everything works fine 
except integration to the Kerberos authentication server.

I have already managed to integrate my ubuntu with the kerberos and the 
tickets are created correctly. Here i leave my configuration of the auth 
in the squid
###############################################################################################
auth_param negotiate program / usr / lib / squid / 
negotiate_kerberos_auth -d -s HTTP / proxy.empresa.cu
auth_param negotiate children 1000
auth_param negotiate keep_alive on

external_acl_type kerberos_group ttl = 3600 negative_ttl = 3600% LOGIN / 
usr / lib / squid / ext_kerberos_ldap_group_acl -a -g Internet_access -D 
EMPRESA.CU

###############################################################################################
in this case my domain is EMPRESA.CU

but i been unable to log in

this is the cache log

2019/03/28 09:46:47 kid1| helperOpenServers: Starting 0/1000 
'negotiate_kerberos_auth' processes
2019/03/28 09:46:47 kid1| helperStatefulOpenServers: No 
'negotiate_kerberos_auth' processes needed.
2019/03/28 09:46:47 kid1| helperOpenServers: Starting 0/5 
'ext_kerberos_ldap_group_acl' processes
2019/03/28 09:46:47 kid1| helperOpenServers: No 
'ext_kerberos_ldap_group_acl' processes needed.
2019/03/28 09:46:47 kid1| helperOpenServers: Starting 0/5 
'ext_kerberos_ldap_group_acl' processes
2019/03/28 09:46:47 kid1| helperOpenServers: No 
'ext_kerberos_ldap_group_acl' processes needed.

Any advice to make the login funtional?

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190329/959243ca/attachment.htm>

From creditu at eml.cc  Fri Mar 29 15:12:55 2019
From: creditu at eml.cc (creditu at eml.cc)
Date: Fri, 29 Mar 2019 11:12:55 -0400
Subject: [squid-users] Deny_Info TCP_RESET
In-Reply-To: <4e42d66b-c5d5-56f0-6e2e-c2278364a492@measurement-factory.com>
References: <f444dab1-d621-4439-8375-9c7b12551cbc@www.fastmail.com>
 <fc4a7de9-b0d6-fb86-12af-71ad4076b8d5@measurement-factory.com>
 <10c50af1-7775-444a-b6f1-f794c8b1eea3@www.fastmail.com>
 <817aa8e9-9cfc-6438-0df5-facbddaf2e8a@measurement-factory.com>
 <4e42d66b-c5d5-56f0-6e2e-c2278364a492@measurement-factory.com>
Message-ID: <7c8d78b5-9e61-4aa2-8e7f-5869c8b149c1@www.fastmail.com>



On Thu, Mar 28, 2019, at 7:14 PM, Alex Rousskov wrote:
> On 3/28/19 5:36 PM, Alex Rousskov wrote:
> > On 3/28/19 8:13 AM, creditu at eml.cc wrote:
> >> Is using the http_reply_access deny a viable option if all else fails
> >> to correct the issue until we can upgrade?
> 
> > Probably it is not: I am not sure, but based on my quick reading of the
> > code and a basic test, http_reply_access does not support the "deny_info
> > TCP_RESET" feature at all. Only http_access (and possibly
> > adapted_http_access, but I did not check) supports that feature (bugs
> > notwithstanding).
> 
> Also see Squid Bug 4585:
> https://bugs.squid-cache.org/show_bug.cgi?id=4585
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

Thanks for all the information on this and the bug link.  We'll have to see if there is some way we can work around this as compliance policy checks see this as providing a page that is not compliant, at least some of the time.  Sometimes the page is not on the bad list because the RST and ACK gets sent back. Thanks again.


From david at articatech.com  Sat Mar 30 09:22:56 2019
From: david at articatech.com (David Touzeau)
Date: Sat, 30 Mar 2019 10:22:56 +0100
Subject: [squid-users] Why Squid on CentOS is faster than Debian ?
Message-ID: <066e5172-ab3e-3095-98e0-59891b3397b0@articatech.com>

Hi all,

Did you have perform squid stress on Debian against CentOS ?

I have installed:

  * Debian 9 net install + Squid compiled
  * CentOS 7 minimal? + Squid compiled

Same version, same compilation parameters, same Squid settings.

It seems that Squid on CentOS is 10 times faster than squid on Debian

What are kernel differences that made this huge performance changes?


Best regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190330/efc67ee4/attachment.htm>

From uhlar at fantomas.sk  Sat Mar 30 12:05:46 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 30 Mar 2019 13:05:46 +0100
Subject: [squid-users] Why Squid on CentOS is faster than Debian ?
In-Reply-To: <066e5172-ab3e-3095-98e0-59891b3397b0@articatech.com>
References: <066e5172-ab3e-3095-98e0-59891b3397b0@articatech.com>
Message-ID: <20190330120546.GA29173@fantomas.sk>

On 30.03.19 10:22, David Touzeau wrote:
>Did you have perform squid stress on Debian against CentOS ?
>
>I have installed:
>
> * Debian 9 net install + Squid compiled
> * CentOS 7 minimal? + Squid compiled
>
>Same version, same compilation parameters, same Squid settings.

>It seems that Squid on CentOS is 10 times faster than squid on Debian

faster in what? Response time? number of parallel connections?
single or multiple connection data transfers?
HTTP or HTTPS?

>What are kernel differences that made this huge performance changes?

no kernel differences should cause 10x speed difference.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I'm not interested in your website anymore.
If you need cookies, bake them yourself.


From david at articatech.com  Sat Mar 30 14:41:16 2019
From: david at articatech.com (David Touzeau)
Date: Sat, 30 Mar 2019 15:41:16 +0100
Subject: [squid-users] Why Squid on CentOS is faster than Debian ?
In-Reply-To: <20190330120546.GA29173@fantomas.sk>
References: <066e5172-ab3e-3095-98e0-59891b3397b0@articatech.com>
 <20190330120546.GA29173@fantomas.sk>
Message-ID: <594ce52d-78c0-6593-cd51-7edf719978d4@articatech.com>

On 30.03.19 10:22, David Touzeau wrote:

>> Did you have perform squid stress on Debian against CentOS ?
>>
>> I have installed:
>>
>> * Debian 9 net install + Squid compiled
>> * CentOS 7 minimal? + Squid compiled
>>
>> Same version, same compilation parameters, same Squid settings.
>
>> It seems that Squid on CentOS is 10 times faster than squid on Debian
>
> faster in what? Response time? number of parallel connections?
> single or multiple connection data transfers?
> HTTP or HTTPS?
>
>> What are kernel differences that made this huge performance changes?
>
> no kernel differences should cause 10x speed difference.
>
>
Faster in what? Response time?

1. response time, MISS and HIT are faster

Example:

on Centos MEM_HIT are about? 0-1 msec against Debian about 3-4 msec

Number of parallel connections? single or multiple connection data 
transfers?

2. multiple

HTTP or HTTPS?

3. HTTP



From squid3 at treenet.co.nz  Sun Mar 31 03:50:07 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 31 Mar 2019 16:50:07 +1300
Subject: [squid-users] Why Squid on CentOS is faster than Debian ?
In-Reply-To: <594ce52d-78c0-6593-cd51-7edf719978d4@articatech.com>
References: <066e5172-ab3e-3095-98e0-59891b3397b0@articatech.com>
 <20190330120546.GA29173@fantomas.sk>
 <594ce52d-78c0-6593-cd51-7edf719978d4@articatech.com>
Message-ID: <5d6ef032-46ee-d4fa-a734-710a6341788c@treenet.co.nz>

On 31/03/19 3:41 am, David Touzeau wrote:
> On 30.03.19 10:22, David Touzeau wrote:
> 
>>> Did you have perform squid stress on Debian against CentOS ?
>>>
>>> I have installed:
>>>
>>> * Debian 9 net install + Squid compiled
>>> * CentOS 7 minimal? + Squid compiled
>>>
>>> Same version, same compilation parameters, same Squid settings.
>>
>>> It seems that Squid on CentOS is 10 times faster than squid on Debian
>>
>> faster in what? Response time? number of parallel connections?
>> single or multiple connection data transfers?
>> HTTP or HTTPS?
>>
>>> What are kernel differences that made this huge performance changes?
>>
>> no kernel differences should cause 10x speed difference.
>>

If you still have the config.log files from the build you may be able to
track down something being detected (or not) in one of the builds.

The -march=native or -O level options for compile would be the first
place I look for a major difference like that. Either on Squid or on one
of the system libraries it uses. The *FLAGS summary at the end of the
build can be a good starting point for comparison.

Compiler version can also have an effect as newer compilers use more
performance related tricks than older ones (YMMV on which tricks are
actually better).


>>
> Faster in what? Response time?
> 
> 1. response time, MISS and HIT are faster
> 
> Example:
> 
> on Centos MEM_HIT are about? 0-1 msec against Debian about 3-4 msec
> 

On the same test traffic?


Amos


From squid3 at treenet.co.nz  Sun Mar 31 04:22:19 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 31 Mar 2019 17:22:19 +1300
Subject: [squid-users] kerberos
In-Reply-To: <d1d9e732-4c32-1615-40ed-d4263b91d5ac@dvm.esines.cu>
References: <d1d9e732-4c32-1615-40ed-d4263b91d5ac@dvm.esines.cu>
Message-ID: <583a4b76-f9e4-f482-7527-1a52252fccb5@treenet.co.nz>

On 30/03/19 3:30 am, Alex Guti?rrez Mart?nez wrote:
> Hello Community, I just compiled my squid 4. Everything works fine
> except integration to the Kerberos authentication server.
> 
> I have already managed to integrate my ubuntu with the kerberos and the
> tickets are created correctly. Here i leave my configuration of the auth
> in the squid
> ###############################################################################################
> auth_param negotiate program / usr / lib / squid /
> negotiate_kerberos_auth -d -s HTTP / proxy.empresa.cu
> auth_param negotiate children 1000

Why 1000? This looks to me like a number one would be forced to use for
NTLM auth due to how painfully slow NTLM is.

Kerberos can easily handle several orders of magnitude more traffic per
helper than NTLM. So you can possibly cut that down to 10 or 100
depending on how many TCP connections are being handled per-second in
production traffic.



> auth_param negotiate keep_alive on
> 
> external_acl_type kerberos_group ttl = 3600 negative_ttl = 3600% LOGIN /
> usr / lib / squid / ext_kerberos_ldap_group_acl -a -g Internet_access -D
> EMPRESA.CU
> 

Missing details of squid.conf acl and access control directives.
http_access in particular, but also any others using the auth ACLs.


> ###############################################################################################
> in this case my domain is EMPRESA.CU
> 
> but i been unable to log in
> 
> this is the cache log
> 
> 2019/03/28 09:46:47 kid1| helperOpenServers: Starting 0/1000
> 'negotiate_kerberos_auth' processes
> 2019/03/28 09:46:47 kid1| helperStatefulOpenServers: No
> 'negotiate_kerberos_auth' processes needed.
> 2019/03/28 09:46:47 kid1| helperOpenServers: Starting 0/5
> 'ext_kerberos_ldap_group_acl' processes
> 2019/03/28 09:46:47 kid1| helperOpenServers: No
> 'ext_kerberos_ldap_group_acl' processes needed.
> 2019/03/28 09:46:47 kid1| helperOpenServers: Starting 0/5
> 'ext_kerberos_ldap_group_acl' processes
> 2019/03/28 09:46:47 kid1| helperOpenServers: No
> 'ext_kerberos_ldap_group_acl' processes needed.

Notice the word *needed*.


Current Squid default only start helpers when traffic actually needs them.

If you have a proxy with a very large memory footprint when running,
that default may be sub-optimal. The 'startup' and 'idle' parameters are
for tuning that.
 <http://www.squid-cache.org/Doc/config/auth_param/>
 <http://www.squid-cache.org/Doc/config/external_acl_type/>


Amos


From david at articatech.com  Sun Mar 31 22:23:04 2019
From: david at articatech.com (David Touzeau)
Date: Mon, 1 Apr 2019 00:23:04 +0200
Subject: [squid-users] Why Squid on CentOS is faster than Debian ?
In-Reply-To: <5d6ef032-46ee-d4fa-a734-710a6341788c@treenet.co.nz>
References: <066e5172-ab3e-3095-98e0-59891b3397b0@articatech.com>
 <20190330120546.GA29173@fantomas.sk>
 <594ce52d-78c0-6593-cd51-7edf719978d4@articatech.com>
 <5d6ef032-46ee-d4fa-a734-710a6341788c@treenet.co.nz>
Message-ID: <11154c61-dd09-99d3-e07e-a505d72ba81b@articatech.com>


Le 31/03/2019 ? 05:50, Amos Jeffries a ?crit?:
> On 31/03/19 3:41 am, David Touzeau wrote:
>> On 30.03.19 10:22, David Touzeau wrote:
>>
>>>> Did you have perform squid stress on Debian against CentOS ?
>>>>
>>>> I have installed:
>>>>
>>>> * Debian 9 net install + Squid compiled
>>>> * CentOS 7 minimal? + Squid compiled
>>>>
>>>> Same version, same compilation parameters, same Squid settings.
>>>> It seems that Squid on CentOS is 10 times faster than squid on Debian
>>> faster in what? Response time? number of parallel connections?
>>> single or multiple connection data transfers?
>>> HTTP or HTTPS?
>>>
>>>> What are kernel differences that made this huge performance changes?
>>> no kernel differences should cause 10x speed difference.
>>>
> If you still have the config.log files from the build you may be able to
> track down something being detected (or not) in one of the builds.
>
> The -march=native or -O level options for compile would be the first
> place I look for a major difference like that. Either on Squid or on one
> of the system libraries it uses. The *FLAGS summary at the end of the
> build can be a good starting point for comparison.
>
> Compiler version can also have an effect as newer compilers use more
> performance related tricks than older ones (YMMV on which tricks are
> actually better).
>
>
>> Faster in what? Response time?
>>
>> 1. response time, MISS and HIT are faster
>>
>> Example:
>>
>> on Centos MEM_HIT are about? 0-1 msec against Debian about 3-4 msec
>>
> On the same test traffic?
>
>
> Amos

Thanks Amos, we will take care during the compilation.

But to be sure of tests:

Using same settings, same cache, same hardware and same destination 
websites.






