From wx499258179 at gmail.com  Fri Nov  1 05:23:51 2019
From: wx499258179 at gmail.com (499258179)
Date: Fri, 1 Nov 2019 00:23:51 -0500 (CDT)
Subject: [squid-users] helperHandleRead: unexpected read
Message-ID: <1572585831047-0.post@n4.nabble.com>

I just start learn squid, and i find a 
2019/11/01 00:06:14 kid1| helperHandleRead: unexpected read from timecheck
#Hlpr8, 3 bytes 'OK
'
2019/11/01 00:06:20 kid1| helperHandleRead: unexpected read from timecheck
#Hlpr6, 3 bytes 'OK
in my cache log, timecheck is my php auth for squid
I have follow questions,
1. what is helperHandleRead: unexpected read
2. how to fix this issue




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Nov  1 06:01:58 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Nov 2019 19:01:58 +1300
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
In-Reply-To: <fbd9fefd24994631b233a8872612f865@ecritel.net>
References: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>
 <201910301739.28311.Antony.Stone@squid.open.source.it>
 <fbd9fefd24994631b233a8872612f865@ecritel.net>
Message-ID: <8d65149e-3209-a043-cb31-520193854f61@treenet.co.nz>

On 1/11/19 5:53 am, FOUTREL S?bastien wrote:
> ------------------------------------------------------------------------
> *De :* Antony Stone
> *Envoy? :* mercredi 30 octobre 2019 17:39
> ?
> On Wednesday 30 October 2019 at 17:11:29, FOUTREL S?bastien wrote:
> 
>> Hello, I would like to use squid as a transparent proxy for my users.
> 
>> "Clients" are behind a Debian "Router" which MASQUERADE them (as they use
>> RFC 1918 ips).
>> 
>> I have a Squid 4.6 from Debian Buster packages installed on a "Proxy"
>> server which is outside my network.
>> 
>> I read a lot of tutorials and examples from squid site...
> 
> Did that include the links I've given below?
> 
> Yes I read almost all examples config?from?wiki.squid-cache.org
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>And I was
> mislead by the fact that there is a DNAT config and a REDIRECT config..
> DNAT is completely useless if Squid only support to be on the router.
> Wasn't it possible to dnat to a different server with older versions (my
> memory is faulty) ?
> http://tldp.org/HOWTO/TransparentProxy-6.html?for example.


Squid-2 used to ignore all NAT errors and just go where the client HTTP
headers were claiming to be going. This proved to be a major security
vulnerability with a pile of nasty related issues and side effects.
CVE-2009-0801 for reference.

DNAT is a tiny amount faster and less CPU cycles on the kernel NAT side
of interception, and can be used in config tricks to get more than 64K
entries in the NAT tables. So it is kept around for extremely
high-traffic proxies.

REDIRECT is better for zero-conf installations or ones with a dynamic IP
address on the proxy machine (eg IPv6 auto-conf and privacy addressing).


> 
> I read the "fw mark and route policy" method as an alternative not the
> only way to go. My mistake.
> 

Easily made if you are reading *every* example config. Policy Routing
_is_ an alternative ... to WCCP.

There are so many different types of routers with different config
requirements, and also numerous NAT systems. Our formal Intercept
examples are laid out as separate router config example and NAT config
example. Pick one from each category as appropriate to the software your
network uses for each machine.



Cheers
Amos


From squid3 at treenet.co.nz  Fri Nov  1 06:05:33 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Nov 2019 19:05:33 +1300
Subject: [squid-users] helperHandleRead: unexpected read
In-Reply-To: <1572585831047-0.post@n4.nabble.com>
References: <1572585831047-0.post@n4.nabble.com>
Message-ID: <a182f3cc-09ea-a3dc-3a7e-3f925e9e28ec@treenet.co.nz>

On 1/11/19 6:23 pm, 499258179 wrote:
> I just start learn squid, and i find a 
> 2019/11/01 00:06:14 kid1| helperHandleRead: unexpected read from timecheck
> #Hlpr8, 3 bytes 'OK
> '
> 2019/11/01 00:06:20 kid1| helperHandleRead: unexpected read from timecheck
> #Hlpr6, 3 bytes 'OK
> in my cache log, timecheck is my php auth for squid
> I have follow questions,
> 1. what is helperHandleRead: unexpected read

Your helper is sending a line of data to Squid when there has been no
matching query sent to it.


> 2. how to fix this issue
> 

Find out why the helper is not delivering only and exactly ONE line of
output (response/result) to each single line of input (query/lookup).

Probably incorrect number of newlines being printed to stdout.

<https://wiki.squid-cache.org/Features/AddonHelpers#What_language_are_helper_meant_to_be_written_in.3F>


Amos


From wx499258179 at gmail.com  Fri Nov  1 06:15:29 2019
From: wx499258179 at gmail.com (499258179)
Date: Fri, 1 Nov 2019 01:15:29 -0500 (CDT)
Subject: [squid-users] helperHandleRead: unexpected read
In-Reply-To: <a182f3cc-09ea-a3dc-3a7e-3f925e9e28ec@treenet.co.nz>
References: <1572585831047-0.post@n4.nabble.com>
 <a182f3cc-09ea-a3dc-3a7e-3f925e9e28ec@treenet.co.nz>
Message-ID: <1572588929166-0.post@n4.nabble.com>

Following is my php script, can u check for me, it is looks like  only one
line stdout
<?php

$STDIN=fopen("php://stdin", "r");
$STDOUT=fopen("php://stdout", "w");

$servername = "localhost";
$username = "test";
$password = "test";
$dbname = "test";
$socket = "/var/lib/mysql/mysql.sock";
$conn = mysqli_connect($servername, $username, $password,
$dbname,0,$socket);

if (mysqli_connect_errno()) {
    echo "Failed to connect to MySQL: " . mysqli_connect_error();
        exit();
}

date_default_timezone_set('US/Eastern');

while (!feof($STDIN)) {
$input = trim(fgets($STDIN));
$data = explode(' ',$input);
$username = rawurldecode($data[0]);

$result = mysqli_query($conn,"SELECT * FROM MyGuests WHERE username =
'$username'");
$row = mysqli_fetch_array($result);
$Time_Now = date("Y-m-d H:i:s");
$Start_Date = $row['reg_date'];
$Expire_Date = date("Y-m-d h:i:s",strtotime("$Start_Date +720 hours"));

if($Start_Date < $Time_Now)
{
        if($Time_Now < $Expire_Date)
        {
                fwrite($STDOUT, "OK\n");
                mysqli_close($conn);
        }
        else
        {
                fwrite($STDOUT, "ERR\n");
                mysqli_close($conn);
        }
}
else
{
fwrite($STDOUT, "ERR\n");
mysqli_close($conn);
}
}

?>



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Nov  1 06:32:32 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Nov 2019 19:32:32 +1300
Subject: [squid-users] optional verification of clients?
In-Reply-To: <e35417dc-192c-d7ca-cb1c-1e4b2498d2a8@spawn.link>
References: <e35417dc-192c-d7ca-cb1c-1e4b2498d2a8@spawn.link>
Message-ID: <bdef7112-a855-3be9-9883-ec3a560c8784@treenet.co.nz>

On 1/11/19 9:19 am, Antonio SJ Musumeci wrote:
> Is there a way to do something similar to NGINX's "ssl_verify_client
> optional;"?


Set sslflags=DELAYED_AUTH on the http(s)_port line.

Though why you would want to slow every TLS connection setup with KBs of
certificates pushed in both directions then "dropped on the floor" is
beyond me.


Amos


From rafael.akchurin at diladele.com  Fri Nov  1 06:35:22 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 1 Nov 2019 06:35:22 +0000
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
In-Reply-To: <8d65149e-3209-a043-cb31-520193854f61@treenet.co.nz>
References: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>
 <201910301739.28311.Antony.Stone@squid.open.source.it>
 <fbd9fefd24994631b233a8872612f865@ecritel.net>
 <8d65149e-3209-a043-cb31-520193854f61@treenet.co.nz>
Message-ID: <AM0PR04MB47539841AADD2AFC4CCC53A28F620@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Sebastian, 

If you decide to go policy routing way as Amos suggested - please see the tutorial at https://docs.diladele.com/tutorials/policy_based_routing_squid/index.html
Or https://docs.diladele.com/tutorials/web_filter_https_squid_cisco_wccp/index.html for WCCP.

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Friday, 1 November 2019 07:02
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Unsuccessful at using Squid v4 with intercept

On 1/11/19 5:53 am, FOUTREL S?bastien wrote:
> ----------------------------------------------------------------------
> --
> *De :* Antony Stone
> *Envoy? :* mercredi 30 octobre 2019 17:39
> ?
> On Wednesday 30 October 2019 at 17:11:29, FOUTREL S?bastien wrote:
> 
>> Hello, I would like to use squid as a transparent proxy for my users.
> 
>> "Clients" are behind a Debian "Router" which MASQUERADE them (as they 
>> use RFC 1918 ips).
>> 
>> I have a Squid 4.6 from Debian Buster packages installed on a "Proxy"
>> server which is outside my network.
>> 
>> I read a lot of tutorials and examples from squid site...
> 
> Did that include the links I've given below?
> 
> Yes I read almost all examples config?from?wiki.squid-cache.org 
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>And I was 
> mislead by the fact that there is a DNAT config and a REDIRECT config..
> DNAT is completely useless if Squid only support to be on the router.
> Wasn't it possible to dnat to a different server with older versions 
> (my memory is faulty) ?
> http://tldp.org/HOWTO/TransparentProxy-6.html?for example.


Squid-2 used to ignore all NAT errors and just go where the client HTTP headers were claiming to be going. This proved to be a major security vulnerability with a pile of nasty related issues and side effects.
CVE-2009-0801 for reference.

DNAT is a tiny amount faster and less CPU cycles on the kernel NAT side of interception, and can be used in config tricks to get more than 64K entries in the NAT tables. So it is kept around for extremely high-traffic proxies.

REDIRECT is better for zero-conf installations or ones with a dynamic IP address on the proxy machine (eg IPv6 auto-conf and privacy addressing).


> 
> I read the "fw mark and route policy" method as an alternative not the 
> only way to go. My mistake.
> 

Easily made if you are reading *every* example config. Policy Routing _is_ an alternative ... to WCCP.

There are so many different types of routers with different config requirements, and also numerous NAT systems. Our formal Intercept examples are laid out as separate router config example and NAT config example. Pick one from each category as appropriate to the software your network uses for each machine.



Cheers
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Fri Nov  1 07:23:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Nov 2019 20:23:12 +1300
Subject: [squid-users] helperHandleRead: unexpected read
In-Reply-To: <1572588929166-0.post@n4.nabble.com>
References: <1572585831047-0.post@n4.nabble.com>
 <a182f3cc-09ea-a3dc-3a7e-3f925e9e28ec@treenet.co.nz>
 <1572588929166-0.post@n4.nabble.com>
Message-ID: <6c978212-0a27-d5b4-cb9d-f9faa253ecc7@treenet.co.nz>

On 1/11/19 7:15 pm, 499258179 wrote:
> Following is my php script, can u check for me, it is looks like  only one
> line stdout

How have you tested it?

Your explicit fwrite calls look okay for the expected outputs. But are
they actually the only things producing output?

With PHP any of the functions you call can produce things that the
engine puts on stdout (eg errors, but also debug info) if any one of the
many background config file(s) says to.


FWIW: Have you considered using the DB session helper bundled with Squid?

Something like the below should do the same as your current helper.
Double check the --cond syntax, it has been a while since I dealt with
MySQL timestamp math.


external_acl_type guests %un /usr/lib/squid/ext_sql_session_acl \
  --dsn DBI:mysql:database=test:localhost:3306 --persist \
  --user test --password test \
  --table MyGuests --uidcol username \
  --cond "NOW() < DATETIME_ADD(reg_date, INTERVAL 720 HOUR)"


Amos


From edi.weissmann at gmail.com  Fri Nov  1 08:22:02 2019
From: edi.weissmann at gmail.com (Eduard Weissmann)
Date: Fri, 1 Nov 2019 09:22:02 +0100
Subject: [squid-users] reply_body_max_size not always enforced
Message-ID: <CABr3N7j-iPta-2ma1BftjeH0pJ3vtUkVLq_CZ=EUh-_zva7fWw@mail.gmail.com>

Hi,

I've configured Squid to block large resources:

reply_body_max_size 50 MB all

Blocking works for some urls, (HTTP/1.1):
http://download.thinkbroadband.com/1GB.zip

But it does not work for others (HTTP/2):
https://upload.wikimedia.org/wikipedia/commons/0/0b/Sandro_Botticelli_-_La_nascita_di_Venere_-_Google_Art_Project_-_edited.jpg

I'm wondering: why is the second URL not blocked? Is it because the
response is HTTP/2?

I've read in the docs about how the response size is checked twice and how
that all works, but in the case of both URLs the response has a
content-length header defined.

Using curl for tests:
curl --proxy 127.0.0.1:3128 -v -s "
https://upload.wikimedia.org/wikipedia/commons/0/0b/Sandro_Botticelli_-_La_nascita_di_Venere_-_Google_Art_Project_-_edited.jpg"
1> /dev/null

Thank you

Best regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191101/2fc7224e/attachment.htm>

From uhlar at fantomas.sk  Fri Nov  1 11:22:27 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 1 Nov 2019 12:22:27 +0100
Subject: [squid-users] reply_body_max_size not always enforced
In-Reply-To: <CABr3N7j-iPta-2ma1BftjeH0pJ3vtUkVLq_CZ=EUh-_zva7fWw@mail.gmail.com>
References: <CABr3N7j-iPta-2ma1BftjeH0pJ3vtUkVLq_CZ=EUh-_zva7fWw@mail.gmail.com>
Message-ID: <20191101112226.GA20007@fantomas.sk>

On 01.11.19 09:22, Eduard Weissmann wrote:
>I've configured Squid to block large resources:
>
>reply_body_max_size 50 MB all
>
>Blocking works for some urls, (HTTP/1.1):
>http://download.thinkbroadband.com/1GB.zip
>
>But it does not work for others (HTTP/2):
>https://upload.wikimedia.org/wikipedia/commons/0/0b/Sandro_Botticelli_-_La_nascita_di_Venere_-_Google_Art_Project_-_edited.jpg
>
>I'm wondering: why is the second URL not blocked? Is it because the
>response is HTTP/2?

I assume it's not blocked because it's https, thus ('s' meas secure)
encrypted and squid only sees TCP tunnel made through it, not any requests
and responses, so it can't block either.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I just got lost in thought. It was unfamiliar territory.


From trapexit at spawn.link  Fri Nov  1 12:17:37 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Fri, 1 Nov 2019 08:17:37 -0400
Subject: [squid-users] optional verification of clients?
In-Reply-To: <bdef7112-a855-3be9-9883-ec3a560c8784@treenet.co.nz>
References: <e35417dc-192c-d7ca-cb1c-1e4b2498d2a8@spawn.link>
 <bdef7112-a855-3be9-9883-ec3a560c8784@treenet.co.nz>
Message-ID: <b165a209-dacf-dd6c-57b6-fd9f466eca2b@spawn.link>

On 11/1/2019 2:32 AM, Amos Jeffries wrote:
> On 1/11/19 9:19 am, Antonio SJ Musumeci wrote:
>> Is there a way to do something similar to NGINX's "ssl_verify_client
>> optional;"?
> 
> 
> Set sslflags=DELAYED_AUTH on the http(s)_port line.
> 
> Though why you would want to slow every TLS connection setup with KBs of
> certificates pushed in both directions then "dropped on the floor" is
> beyond me.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

The docs indicated that DELAYED_AUTH isn't implemented and doesn't seem 
to work on 4.8. If I enable it it acts as if no certs are passed and the 
http_access user_cert acl I setup which works fine when not using 
DELAYED_AUTH does not seem to trigger the verification.

Regardless, the point is to create an "anonymous" setup. Not all clients 
have, need, or can provide certs. With NGINX setting verify to optional 
I can verify iff they are provided allowing me to convert no certs into 
a generic guest / anonymous account and entitle separately.

If I understand DELAYED_AUTH's behavior this isn't going to get me that. 
I need to be able to tell if the cert was provided. If verification is 
just delayed till when the acl is processed that doesn't help unless 
there is an acl I'm missing that indicates a cert was provided. The 
ssl_error acl values all imply existence.


From edi.weissmann at gmail.com  Fri Nov  1 13:42:36 2019
From: edi.weissmann at gmail.com (Eduard Weissmann)
Date: Fri, 1 Nov 2019 14:42:36 +0100
Subject: [squid-users] reply_body_max_size not always enforced
In-Reply-To: <20191101112226.GA20007@fantomas.sk>
References: <CABr3N7j-iPta-2ma1BftjeH0pJ3vtUkVLq_CZ=EUh-_zva7fWw@mail.gmail.com>
 <20191101112226.GA20007@fantomas.sk>
Message-ID: <CABr3N7iEoQjs9rLiScLa-X3Egskj8TNBvLb09CNbs=CWOq42xQ@mail.gmail.com>

Oh.. of course! That was so silly of me.
Thank you.

On Fri, Nov 1, 2019 at 12:22 PM Matus UHLAR - fantomas <uhlar at fantomas.sk>
wrote:

> On 01.11.19 09:22, Eduard Weissmann wrote:
> >I've configured Squid to block large resources:
> >
> >reply_body_max_size 50 MB all
> >
> >Blocking works for some urls, (HTTP/1.1):
> >http://download.thinkbroadband.com/1GB.zip
> >
> >But it does not work for others (HTTP/2):
> >
> https://upload.wikimedia.org/wikipedia/commons/0/0b/Sandro_Botticelli_-_La_nascita_di_Venere_-_Google_Art_Project_-_edited.jpg
> >
> >I'm wondering: why is the second URL not blocked? Is it because the
> >response is HTTP/2?
>
> I assume it's not blocked because it's https, thus ('s' meas secure)
> encrypted and squid only sees TCP tunnel made through it, not any requests
> and responses, so it can't block either.
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> I just got lost in thought. It was unfamiliar territory.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191101/cf3ad717/attachment.htm>

From wx499258179 at gmail.com  Fri Nov  1 22:58:50 2019
From: wx499258179 at gmail.com (499258179)
Date: Fri, 1 Nov 2019 17:58:50 -0500 (CDT)
Subject: [squid-users] helperHandleRead: unexpected read
In-Reply-To: <6c978212-0a27-d5b4-cb9d-f9faa253ecc7@treenet.co.nz>
References: <1572585831047-0.post@n4.nabble.com>
 <a182f3cc-09ea-a3dc-3a7e-3f925e9e28ec@treenet.co.nz>
 <1572588929166-0.post@n4.nabble.com>
 <6c978212-0a27-d5b4-cb9d-f9faa253ecc7@treenet.co.nz>
Message-ID: <1572649130693-0.post@n4.nabble.com>

I just use php function call my pho script, such as php /etc/test.php.
When I type test1, stdout only have OK



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sat Nov  2 00:37:38 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Nov 2019 13:37:38 +1300
Subject: [squid-users] optional verification of clients?
In-Reply-To: <b165a209-dacf-dd6c-57b6-fd9f466eca2b@spawn.link>
References: <e35417dc-192c-d7ca-cb1c-1e4b2498d2a8@spawn.link>
 <bdef7112-a855-3be9-9883-ec3a560c8784@treenet.co.nz>
 <b165a209-dacf-dd6c-57b6-fd9f466eca2b@spawn.link>
Message-ID: <97180954-5943-6bb3-4ccb-0d1024cac911@treenet.co.nz>

On 2/11/19 1:17 am, Antonio SJ Musumeci wrote:
> On 11/1/2019 2:32 AM, Amos Jeffries wrote:
>> On 1/11/19 9:19 am, Antonio SJ Musumeci wrote:
>>> Is there a way to do something similar to NGINX's "ssl_verify_client
>>> optional;"?
>>
>>
>> Set sslflags=DELAYED_AUTH on the http(s)_port line.
>>
>> Though why you would want to slow every TLS connection setup with KBs of
>> certificates pushed in both directions then "dropped on the floor" is
>> beyond me.
>>
> 
> The docs indicated that DELAYED_AUTH isn't implemented and doesn't seem
> to work on 4.8. If I enable it it acts as if no certs are passed and the
> http_access user_cert acl I setup which works fine when not using
> DELAYED_AUTH does not seem to trigger the verification.
> 

Oh well. That was the closest Squid has. I was hoping the library would
sent cert request but not verify the clients response. So the details
would be available for logging etc as handshake parameters.

If that client cert request/delivery is not working then the only
alternative would be two proxy ports, one with client certificates
required and one without. Which does not match what you are trying to
achieve.


If this is of particular importance patch/PR are welcome. I will keep it
in mind for future TLS improvements, but there is no guarantees that way.
<https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F>
<https://wiki.squid-cache.org/DeveloperResources>

Amos


From squid3 at treenet.co.nz  Sat Nov  2 01:16:41 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Nov 2019 14:16:41 +1300
Subject: [squid-users] helperHandleRead: unexpected read
In-Reply-To: <1572649130693-0.post@n4.nabble.com>
References: <1572585831047-0.post@n4.nabble.com>
 <a182f3cc-09ea-a3dc-3a7e-3f925e9e28ec@treenet.co.nz>
 <1572588929166-0.post@n4.nabble.com>
 <6c978212-0a27-d5b4-cb9d-f9faa253ecc7@treenet.co.nz>
 <1572649130693-0.post@n4.nabble.com>
Message-ID: <45f25189-30be-ace6-5581-6708d2c211eb@treenet.co.nz>

On 2/11/19 11:58 am, 499258179 wrote:
> I just use php function call my pho script, such as php /etc/test.php.
> When I type test1, stdout only have OK
> 

No empty lines?

Is your squid.conf set to use concurrency for this helper? (this script
cannot handle that).

Amos


From mgresko8 at gmail.com  Sun Nov  3 21:24:27 2019
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Sun, 3 Nov 2019 22:24:27 +0100
Subject: [squid-users] ssl bump intermediate certificate
In-Reply-To: <64c612ec-9f72-2565-b0dd-aabe2aaf2f69@treenet.co.nz>
References: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>
 <5DB953DC.1020306@mathemainzel.info> <20191030094206.GA32145@fantomas.sk>
 <CAChjPdTMwPFN8eNacz+UYsEB2DzcmM=V9gg6neK5J6M22nUTSQ@mail.gmail.com>
 <64c612ec-9f72-2565-b0dd-aabe2aaf2f69@treenet.co.nz>
Message-ID: <CAChjPdQAA3jja1hX9+b77jVSPq718rMJrG15Z7eCc0bmAvovRA@mail.gmail.com>

Hello,

I already tried adding root ca to the pem file int the cert= option.
But it had no effect.

the squid -k parse seems good point.

I got: Ignoring non-issuer CA from /etc/squid/bump-CA/bump-ca.crt

If I add the root ca, that one is reported to be added, but still
ignoring the bump ca. Why is it ignoring my CA?

The reported purposeof the certificate is:
Certificate purposes:
SSL client : Yes
SSL client CA : No
SSL server : Yes
SSL server CA : No
Netscape SSL server : Yes
Netscape SSL server CA : No
S/MIME signing : Yes
S/MIME signing CA : No
S/MIME encryption : Yes
S/MIME encryption CA : No
CRL signing : Yes
CRL signing CA : No
Any Purpose : Yes
Any Purpose CA : Yes
OCSP helper : Yes
OCSP helper CA : No
Time Stamp signing : No
Time Stamp signing CA : No

What am I doing wrong?

Thanks

Marek

2019-10-31 8:38 GMT+01:00, Amos Jeffries <squid3 at treenet.co.nz>:
> On 31/10/19 9:49 am, Marek Gre?ko wrote:
>> Hello,
>>
>> Matus, I also found the document. It should be sending the chain, but
>> is not. When I specify cafile option it responds I shoud use
>> tls-cafile. But in either case it is not sending.
>>
>> Walter, if squid has such requirement, then it is unfinished. Every
>> other proxy is able to run its CA as an intermediate and clients
>> install only root CA. The proxy should be responsible to hold the
>> chain. The url Matus sent is the correct way how to do it, but is is
>> not working. At least not in 4.8 vesion.
>>
>
> "
> cafile=
>   File containing additional CA certificates to use
>   when verifying client certificates.
> "
>
> Note that last line. Squid-4 is more strict about its configured inputs
> being used for what they are documented as.
>
> The best place to put the chain is actually in the PEM file used in the
> cert= parameter. It should contain as much of the chain as you want
> Squid to send, starting with the proxies signing CA cert and going up
> the chained intermediate CA certs towards the root CA.
>
>
> Squid-4 will validate all certificates actually are a chain with correct
> sequence, ignoring any which are incorrect or out of sequence. Running
> "squid -k parse" will reports any errors loading the chain.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Sun Nov  3 23:28:05 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 Nov 2019 12:28:05 +1300
Subject: [squid-users] ssl bump intermediate certificate
Message-ID: <mailman.6.1736411443.1101126.squid-users@lists.squid-cache.org>

All of the "CA" entries in that purposes list say "No". So this is not a CA certificate, it is an origin server certificate.

It can only be used to receive explicit TLS proxy or HTTPS origin server traffic.

Amos

Sent from my alcatel U5

From uhlar at fantomas.sk  Mon Nov  4 12:30:23 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 4 Nov 2019 13:30:23 +0100
Subject: [squid-users] comments in ACL files
Message-ID: <20191104123023.GA6769@fantomas.sk>

Hello,

1. is it possible to use comments in ACL files?

http://www.squid-cache.org/Doc/config/acl/
says:

   acl aclname acltype "file" ...

   When using "file", the file should contain one item per line.

but anything about possible comments, although they seem to be used on the
net according to web search...

#comment
1.2.3.4

2. how does squid behave with more items in one line, does it use or ignore
them?

1.2.3.4 # comment

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"Where do you want to go to die?" [Microsoft]


From sfoutrel at ecritel.net  Mon Nov  4 12:57:23 2019
From: sfoutrel at ecritel.net (=?Windows-1252?Q?FOUTREL_S=E9bastien?=)
Date: Mon, 4 Nov 2019 12:57:23 +0000
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
In-Reply-To: <AM0PR04MB47539841AADD2AFC4CCC53A28F620@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>
 <201910301739.28311.Antony.Stone@squid.open.source.it>
 <fbd9fefd24994631b233a8872612f865@ecritel.net>
 <8d65149e-3209-a043-cb31-520193854f61@treenet.co.nz>,
 <AM0PR04MB47539841AADD2AFC4CCC53A28F620@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <2c5c409409f6460485395a13f16f5bbe@ecritel.net>


Hello,
With your comments and help I succesfully did the fwmark/routing intercept squid.

>From what i read from the other examples, the only method to use if squid is not on the router is always using routes. Which means my squid MUST always be directly connected to the router.
Am I wrong ?

The only way that seems to be able to work if not directly connected is the wccp one which seems to use gre.

Thanks.

________________________________
De : squid-users <squid-users-bounces at lists.squid-cache.org> de la part de Rafael Akchurin <rafael.akchurin at diladele.com>
Envoy? : vendredi 1 novembre 2019 07:35
? : Amos Jeffries; squid-users at lists.squid-cache.org
Objet : Re: [squid-users] Unsuccessful at using Squid v4 with intercept

Hello Sebastian,

If you decide to go policy routing way as Amos suggested - please see the tutorial at https://docs.diladele.com/tutorials/policy_based_routing_squid/index.html
Transparently filtering HTTPS with Squid and Policy Based Routing ? Web Filter for Your Network<https://docs.diladele.com/tutorials/policy_based_routing_squid/index.html>
docs.diladele.com
Detailed tutorial for setting up policy based routing intercept-style HTTPS filtering on Ubuntu 16 using Squid, iptables and Web Safety.



Or https://docs.diladele.com/tutorials/web_filter_https_squid_cisco_wccp/index.html for WCCP.
Transparent HTTPS Web Filter with Squid, Cisco ASA and ICAP ? Web Filter for Your Network<https://docs.diladele.com/tutorials/web_filter_https_squid_cisco_wccp/index.html>
docs.diladele.com
Step-by-step tutorial for enabling transparent HTTP and HTTPS filtering with Squid, Cisco ASA and Diladele Web Safety.




Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Friday, 1 November 2019 07:02
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Unsuccessful at using Squid v4 with intercept

On 1/11/19 5:53 am, FOUTREL S?bastien wrote:
> ----------------------------------------------------------------------
> --
> *De :* Antony Stone
> *Envoy? :* mercredi 30 octobre 2019 17:39
>
> On Wednesday 30 October 2019 at 17:11:29, FOUTREL S?bastien wrote:
>
>> Hello, I would like to use squid as a transparent proxy for my users.
>
>> "Clients" are behind a Debian "Router" which MASQUERADE them (as they
>> use RFC 1918 ips).
>>
>> I have a Squid 4.6 from Debian Buster packages installed on a "Proxy"
>> server which is outside my network.
>>
>> I read a lot of tutorials and examples from squid site...
>
> Did that include the links I've given below?
>
> Yes I read almost all examples config from wiki.squid-cache.org
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>And I was
> mislead by the fact that there is a DNAT config and a REDIRECT config..
> DNAT is completely useless if Squid only support to be on the router.
> Wasn't it possible to dnat to a different server with older versions
> (my memory is faulty) ?
> http://tldp.org/HOWTO/TransparentProxy-6.html for example.


Squid-2 used to ignore all NAT errors and just go where the client HTTP headers were claiming to be going. This proved to be a major security vulnerability with a pile of nasty related issues and side effects.
CVE-2009-0801 for reference.

DNAT is a tiny amount faster and less CPU cycles on the kernel NAT side of interception, and can be used in config tricks to get more than 64K entries in the NAT tables. So it is kept around for extremely high-traffic proxies.

REDIRECT is better for zero-conf installations or ones with a dynamic IP address on the proxy machine (eg IPv6 auto-conf and privacy addressing).


>
> I read the "fw mark and route policy" method as an alternative not the
> only way to go. My mistake.
>

Easily made if you are reading *every* example config. Policy Routing _is_ an alternative ... to WCCP.

There are so many different types of routers with different config requirements, and also numerous NAT systems. Our formal Intercept examples are laid out as separate router config example and NAT config example. Pick one from each category as appropriate to the software your network uses for each machine.



Cheers
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191104/b48c5552/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov  4 13:03:05 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Nov 2019 02:03:05 +1300
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
In-Reply-To: <2c5c409409f6460485395a13f16f5bbe@ecritel.net>
References: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>
 <201910301739.28311.Antony.Stone@squid.open.source.it>
 <fbd9fefd24994631b233a8872612f865@ecritel.net>
 <8d65149e-3209-a043-cb31-520193854f61@treenet.co.nz>
 <AM0PR04MB47539841AADD2AFC4CCC53A28F620@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <2c5c409409f6460485395a13f16f5bbe@ecritel.net>
Message-ID: <3d7d9a20-4fb8-7a55-7aed-856f1d5afc00@treenet.co.nz>

On 5/11/19 1:57 am, FOUTREL S?bastien wrote:
> 
> Hello,
> With your comments and help I succesfully did the fwmark/routing
> intercept squid.
> 
> From what i read from the other examples, the only method to use if
> squid is not on the router is always using routes. Which means my squid
> MUST always be directly connected to the router.
> Am I wrong ?
> 

You can route over a tunnel between the router and Squid ...



> The only way that seems to be able to work if not directly connected is
> the wccp one which seems to use gre.
> 

... GRE is one type of tunnel.


Amos


From squid3 at treenet.co.nz  Mon Nov  4 13:16:34 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Nov 2019 02:16:34 +1300
Subject: [squid-users] comments in ACL files
In-Reply-To: <20191104123023.GA6769@fantomas.sk>
References: <20191104123023.GA6769@fantomas.sk>
Message-ID: <06eecb94-f24c-e223-df2b-466d391cbcba@treenet.co.nz>

On 5/11/19 1:30 am, Matus UHLAR - fantomas wrote:
> Hello,
> 
> 1. is it possible to use comments in ACL files?
> 
> http://www.squid-cache.org/Doc/config/acl/
> says:
> 
> ? acl aclname acltype "file" ...
> 
> ? When using "file", the file should contain one item per line.
> 
> but anything about possible comments, although they seem to be used on the
> net according to web search...
> 
> #comment
> 1.2.3.4

It depends on the ACL type. Some have special handling for files with
values, some do not support loading from files.

The rest support files with:
 * one ACL value per line, and
 * ignoring whitespace prefix and suffix, and
 * '#' as comment prefix.


> 
> 2. how does squid behave with more items in one line, does it use or ignore
> them?
> 
> 1.2.3.4 # comment
> 

All of the line (excluding the whitesace prefix/suffix) is one value.

Amos


From uhlar at fantomas.sk  Mon Nov  4 16:21:21 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 4 Nov 2019 17:21:21 +0100
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
 <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
Message-ID: <20191104162121.GA14172@fantomas.sk>

>> On 02.10.19 16:22, Matus UHLAR - fantomas wrote:
>>> http://bugs.squid-cache.org/show_bug.cgi?id=4977

>On 10/2/19 10:50 AM, Matus UHLAR - fantomas wrote:
>> OK, now I've got 1.7G pcap,? 2.8M access.log (22k requests) and cache.log
>> saying when did the problem appear.
>> Any idea what to search for?

On 02.10.19 13:48, Alex Rousskov wrote:
>cache.log with debug_options set to just "ALL,1" (or default), right?

I have raqised it to ALL,2

>I cannot give you specific instructions, but you can try to find the
>transaction that caused the crash by correlating packet capture with the
>timing of the crash. If you are lucky, the transaction would not be
>encrypted and will have few other transactions nearby. In that case, we
>may be able to learn something from the transaction bytes received and
>sent by Squid.

I have new cache.log with debugging, new pcap file, and core (didn't have
before) backed up (/var/tmp should last for some time):

the last request:

CONNECT www.google.com:443 HTTP/1.1
Host: www.google.com


----------
2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(752) clientAccessCheckDone: The request CONNECT www.google.com:443 is ALLOWED; last ACL checked: work_time1
2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(728) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(752) clientAccessCheckDone: The request CONNECT www.google.com:443 is ALLOWED; last ACL checked: work_time1
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths: Find IP destination for: www.google.com:443' via www.google.com
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths: Found sources for 'www.google.com:443'
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:   always_direct = DENIED
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:    never_direct = DENIED
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(308) peerSelectDnsPaths:          DIRECT = local=[::] remote=[2a00:1450:4014:801::2004]:443 flags=1
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(308) peerSelectDnsPaths:          DIRECT = local=0.0.0.0 remote=216.58.201.100:443 flags=1
2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:        timedout = 0
2019/11/04 16:59:09.725 kid1| 20,2| store.cc(980) checkCachable: StoreEntry::checkCachable: NO: not cachable
2019/11/04 16:59:09.727 kid1| 33,2| client_side.cc(582) swanSong: local=192.168.251.230:3128 remote=192.168.169.31:62822 flags=1
2019/11/04 16:59:09.730 kid1| 20,2| store.cc(980) checkCachable: StoreEntry::checkCachable: NO: not cachable
2019/11/04 16:59:09.730 kid1| assertion failed: stmem.cc:98: "lowestOffset () <= target_offset"


and the gdb backtrace:

Core was generated by `(squid-1) --kid squid-1 -YC -f /etc/squid/squid.conf'.
Program terminated with signal SIGABRT, Aborted.
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
50      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
[Current thread is 1 (Thread 0x7f939ea82e80 (LWP 13179))]
(gdb) backtrace
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007f93a15b2535 in __GI_abort () at abort.c:79
#2  0x00005654ebcee5ad in ?? ()
#3  0x00005654ebe6c170 in mem_hdr::freeDataUpto(long) ()
#4  0x00005654ebe27440 in MemObject::trimSwappable() ()
#5  0x00005654ebe74bf5 in StoreEntry::trimMemory(bool) ()
#6  0x00005654ec100a80 in Store::Controller::memoryOut(StoreEntry&, bool) ()
#7  0x00005654ebe84305 in StoreEntry::swapOut() ()
#8  0x00005654ebe7b3fd in StoreEntry::invokeHandlers() ()
#9  0x00005654ebe736ac in StoreEntry::write(StoreIOBuffer) ()
#10 0x00005654ebf0a590 in Client::storeReplyBody(char const*, long) ()
#11 0x00005654ebdeca37 in HttpStateData::writeReplyBody() ()
#12 0x00005654ebdf1bd5 in HttpStateData::processReplyBody() ()
#13 0x00005654ebdf382a in HttpStateData::processReply() ()
#14 0x00005654ebdf4da8 in HttpStateData::readReply(CommIoCbParams const&) ()
#15 0x00005654ebdf8d6b in JobDialer<HttpStateData>::dial(AsyncCall&) ()
#16 0x00005654ebf5be51 in AsyncCall::make() ()
#17 0x00005654ebf5d3a4 in AsyncCallQueue::fireNext() ()
#18 0x00005654ebf5d6e9 in AsyncCallQueue::fire() ()
#19 0x00005654ebdb98ea in EventLoop::runOnce() ()
#20 0x00005654ebdb99d8 in EventLoop::run() ()
#21 0x00005654ebe22ab9 in SquidMain(int, char**) ()
#22 0x00005654ebd13901 in main ()
(gdb)

I don't know where do those raise.c and raise.c belong (libthread? glibc?)

squid is stripped, i could probably compile it without stripping and re-try

Any ideas how to debug further? (disabling multi-cpu?)

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
How does cat play with mouse? cat /dev/mouse


From rousskov at measurement-factory.com  Mon Nov  4 17:13:06 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Nov 2019 12:13:06 -0500
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <20191104162121.GA14172@fantomas.sk>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
 <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
 <20191104162121.GA14172@fantomas.sk>
Message-ID: <b2e38136-dda2-239b-3a5b-3b518917bdde@measurement-factory.com>

On 11/4/19 11:21 AM, Matus UHLAR - fantomas wrote:
> the last request:
> 
> CONNECT www.google.com:443 HTTP/1.1
> Host: www.google.com


That request does not seem to match the stack trace: The stack trace
points to a non-CONNECT transaction AFAICT. Perhaps you are bumping that
tunnel? Please note that you are not looking for the last request, but
for the request that triggered the assertion. It is very difficult to do
that with just ALL,2 and stripped executable, especially when you have
lots of concurrent transactions flying by.


Alex.



> ----------
> 2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(752)
> clientAccessCheckDone: The request CONNECT www.google.com:443 is
> ALLOWED; last ACL checked: work_time1
> 2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(728)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(752)
> clientAccessCheckDone: The request CONNECT www.google.com:443 is
> ALLOWED; last ACL checked: work_time1
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(281)
> peerSelectDnsPaths: Find IP destination for: www.google.com:443' via
> www.google.com
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(302)
> peerSelectDnsPaths: Found sources for 'www.google.com:443'
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(303)
> peerSelectDnsPaths:?? always_direct = DENIED
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(304)
> peerSelectDnsPaths:??? never_direct = DENIED
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(308)
> peerSelectDnsPaths:????????? DIRECT = local=[::]
> remote=[2a00:1450:4014:801::2004]:443 flags=1
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(308)
> peerSelectDnsPaths:????????? DIRECT = local=0.0.0.0
> remote=216.58.201.100:443 flags=1
> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(317)
> peerSelectDnsPaths:??????? timedout = 0
> 2019/11/04 16:59:09.725 kid1| 20,2| store.cc(980) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2019/11/04 16:59:09.727 kid1| 33,2| client_side.cc(582) swanSong:
> local=192.168.251.230:3128 remote=192.168.169.31:62822 flags=1
> 2019/11/04 16:59:09.730 kid1| 20,2| store.cc(980) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2019/11/04 16:59:09.730 kid1| assertion failed: stmem.cc:98:
> "lowestOffset () <= target_offset"
> 
> 
> and the gdb backtrace:
> 
> Core was generated by `(squid-1) --kid squid-1 -YC -f
> /etc/squid/squid.conf'.
> Program terminated with signal SIGABRT, Aborted.
> #0? __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
> 50????? ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
> [Current thread is 1 (Thread 0x7f939ea82e80 (LWP 13179))]
> (gdb) backtrace
> #0? __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
> #1? 0x00007f93a15b2535 in __GI_abort () at abort.c:79
> #2? 0x00005654ebcee5ad in ?? ()
> #3? 0x00005654ebe6c170 in mem_hdr::freeDataUpto(long) ()
> #4? 0x00005654ebe27440 in MemObject::trimSwappable() ()
> #5? 0x00005654ebe74bf5 in StoreEntry::trimMemory(bool) ()
> #6? 0x00005654ec100a80 in Store::Controller::memoryOut(StoreEntry&,
> bool) ()
> #7? 0x00005654ebe84305 in StoreEntry::swapOut() ()
> #8? 0x00005654ebe7b3fd in StoreEntry::invokeHandlers() ()
> #9? 0x00005654ebe736ac in StoreEntry::write(StoreIOBuffer) ()
> #10 0x00005654ebf0a590 in Client::storeReplyBody(char const*, long) ()
> #11 0x00005654ebdeca37 in HttpStateData::writeReplyBody() ()
> #12 0x00005654ebdf1bd5 in HttpStateData::processReplyBody() ()
> #13 0x00005654ebdf382a in HttpStateData::processReply() ()
> #14 0x00005654ebdf4da8 in HttpStateData::readReply(CommIoCbParams
> const&) ()
> #15 0x00005654ebdf8d6b in JobDialer<HttpStateData>::dial(AsyncCall&) ()
> #16 0x00005654ebf5be51 in AsyncCall::make() ()
> #17 0x00005654ebf5d3a4 in AsyncCallQueue::fireNext() ()
> #18 0x00005654ebf5d6e9 in AsyncCallQueue::fire() ()
> #19 0x00005654ebdb98ea in EventLoop::runOnce() ()
> #20 0x00005654ebdb99d8 in EventLoop::run() ()
> #21 0x00005654ebe22ab9 in SquidMain(int, char**) ()
> #22 0x00005654ebd13901 in main ()
> (gdb)
> 
> I don't know where do those raise.c and raise.c belong (libthread? glibc?)
> 
> squid is stripped, i could probably compile it without stripping and re-try
> 
> Any ideas how to debug further? (disabling multi-cpu?)
> 



From uhlar at fantomas.sk  Mon Nov  4 17:59:03 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 4 Nov 2019 18:59:03 +0100
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <b2e38136-dda2-239b-3a5b-3b518917bdde@measurement-factory.com>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
 <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
 <20191104162121.GA14172@fantomas.sk>
 <b2e38136-dda2-239b-3a5b-3b518917bdde@measurement-factory.com>
Message-ID: <20191104175903.GA16466@fantomas.sk>

>On 11/4/19 11:21 AM, Matus UHLAR - fantomas wrote:
>> the last request:
>>
>> CONNECT www.google.com:443 HTTP/1.1
>> Host: www.google.com

On 04.11.19 12:13, Alex Rousskov wrote:
>That request does not seem to match the stack trace: The stack trace
>points to a non-CONNECT transaction AFAICT.

Oh, yes, may be caused by other GET request fetching from cache or storing
to cache (doesn't happen wich cache disabled).

> Perhaps you are bumping that
>tunnel? Please note that you are not looking for the last request, but
>for the request that triggered the assertion. It is very difficult to do
>that with just ALL,2 and stripped executable, especially when you have
>lots of concurrent transactions flying by.

No bumping. I can create unstripped executable (great if it works with
current core, but I may create new one).


>> ----------
>> 2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(752)
>> clientAccessCheckDone: The request CONNECT www.google.com:443 is
>> ALLOWED; last ACL checked: work_time1
>> 2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(728)
>> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
>> 2019/11/04 16:59:09.721 kid1| 85,2| client_side_request.cc(752)
>> clientAccessCheckDone: The request CONNECT www.google.com:443 is
>> ALLOWED; last ACL checked: work_time1
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(281)
>> peerSelectDnsPaths: Find IP destination for: www.google.com:443' via
>> www.google.com
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(302)
>> peerSelectDnsPaths: Found sources for 'www.google.com:443'
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(303)
>> peerSelectDnsPaths:?? always_direct = DENIED
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(304)
>> peerSelectDnsPaths:??? never_direct = DENIED
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(308)
>> peerSelectDnsPaths:????????? DIRECT = local=[::]
>> remote=[2a00:1450:4014:801::2004]:443 flags=1
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(308)
>> peerSelectDnsPaths:????????? DIRECT = local=0.0.0.0
>> remote=216.58.201.100:443 flags=1
>> 2019/11/04 16:59:09.721 kid1| 44,2| peer_select.cc(317)
>> peerSelectDnsPaths:??????? timedout = 0
>> 2019/11/04 16:59:09.725 kid1| 20,2| store.cc(980) checkCachable:
>> StoreEntry::checkCachable: NO: not cachable
>> 2019/11/04 16:59:09.727 kid1| 33,2| client_side.cc(582) swanSong:
>> local=192.168.251.230:3128 remote=192.168.169.31:62822 flags=1
>> 2019/11/04 16:59:09.730 kid1| 20,2| store.cc(980) checkCachable:
>> StoreEntry::checkCachable: NO: not cachable
>> 2019/11/04 16:59:09.730 kid1| assertion failed: stmem.cc:98:
>> "lowestOffset () <= target_offset"
>>
>>
>> and the gdb backtrace:
>>
>> Core was generated by `(squid-1) --kid squid-1 -YC -f
>> /etc/squid/squid.conf'.
>> Program terminated with signal SIGABRT, Aborted.
>> #0? __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
>> 50????? ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
>> [Current thread is 1 (Thread 0x7f939ea82e80 (LWP 13179))]
>> (gdb) backtrace
>> #0? __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
>> #1? 0x00007f93a15b2535 in __GI_abort () at abort.c:79
>> #2? 0x00005654ebcee5ad in ?? ()
>> #3? 0x00005654ebe6c170 in mem_hdr::freeDataUpto(long) ()
>> #4? 0x00005654ebe27440 in MemObject::trimSwappable() ()
>> #5? 0x00005654ebe74bf5 in StoreEntry::trimMemory(bool) ()
>> #6? 0x00005654ec100a80 in Store::Controller::memoryOut(StoreEntry&,
>> bool) ()
>> #7? 0x00005654ebe84305 in StoreEntry::swapOut() ()
>> #8? 0x00005654ebe7b3fd in StoreEntry::invokeHandlers() ()
>> #9? 0x00005654ebe736ac in StoreEntry::write(StoreIOBuffer) ()
>> #10 0x00005654ebf0a590 in Client::storeReplyBody(char const*, long) ()
>> #11 0x00005654ebdeca37 in HttpStateData::writeReplyBody() ()
>> #12 0x00005654ebdf1bd5 in HttpStateData::processReplyBody() ()
>> #13 0x00005654ebdf382a in HttpStateData::processReply() ()
>> #14 0x00005654ebdf4da8 in HttpStateData::readReply(CommIoCbParams
>> const&) ()
>> #15 0x00005654ebdf8d6b in JobDialer<HttpStateData>::dial(AsyncCall&) ()
>> #16 0x00005654ebf5be51 in AsyncCall::make() ()
>> #17 0x00005654ebf5d3a4 in AsyncCallQueue::fireNext() ()
>> #18 0x00005654ebf5d6e9 in AsyncCallQueue::fire() ()
>> #19 0x00005654ebdb98ea in EventLoop::runOnce() ()
>> #20 0x00005654ebdb99d8 in EventLoop::run() ()
>> #21 0x00005654ebe22ab9 in SquidMain(int, char**) ()
>> #22 0x00005654ebd13901 in main ()
>> (gdb)
>>
>> I don't know where do those raise.c and raise.c belong (libthread? glibc?)
>>
>> squid is stripped, i could probably compile it without stripping and re-try
>>
>> Any ideas how to debug further? (disabling multi-cpu?)

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"Where do you want to go to die?" [Microsoft]


From jlowry at gmail.com  Tue Nov  5 06:26:11 2019
From: jlowry at gmail.com (John Lowry)
Date: Mon, 4 Nov 2019 22:26:11 -0800
Subject: [squid-users] Cannot configure squid 4.6 to splice without bumping
Message-ID: <CAOWEjnCwaB-s80pvH9S-vKoW6EvGN-WH3Tq+7u1cS5zMa2qjfg@mail.gmail.com>

I've been banging my head on this one for a while. I am setting up parental
controls on my network using squidguard. I have a raspberry pi running
squid 4.6 and the router has a policy that sends all web traffic from my
children's computers to squid.

Everything works correctly for HTTP connections but I cannot get HTTPS to
stop bumping. I want to splice all HTTPS connections in order to filter
with squidguard but I do not want to ever bump (because it causes browser
errors in chrome for a lot of sites).

I've tried many, many different settings and I always get traffic bumped.
Here is an example:

http_port 3128 intercept

https_port 3129 intercept tls-cert=/etc/squid/ssl_cert/myCA.pem
tls-key=/etc/squid/ssl_cert/myCA.pem

...

ssl_bump peek step1

ssl_bump peek step2

ssl_bump splice step2

I've tried setting debug_options to 9 but cannot see anything useful in the
logs to indicate why it is not splicing. I always just see the full set of
request headers in the logs for HTTPS connections, indicating that the
connection is bumped.

One thing I did notice is that the ssl logformat options do not work. I get
errors like this on restart:

FATAL: Can't parse configuration token: '%ssl::>sni'
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191104/64e4bea9/attachment.htm>

From uhlar at fantomas.sk  Tue Nov  5 09:34:13 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 5 Nov 2019 10:34:13 +0100
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <20191104175903.GA16466@fantomas.sk>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
 <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
 <20191104162121.GA14172@fantomas.sk>
 <b2e38136-dda2-239b-3a5b-3b518917bdde@measurement-factory.com>
 <20191104175903.GA16466@fantomas.sk>
Message-ID: <20191105093413.GB5031@fantomas.sk>

>On 04.11.19 12:13, Alex Rousskov wrote:
>>That request does not seem to match the stack trace: The stack trace
>>points to a non-CONNECT transaction AFAICT.
>
>Oh, yes, may be caused by other GET request fetching from cache or storing
>to cache (doesn't happen wich cache disabled).
>
>>Perhaps you are bumping that
>>tunnel? Please note that you are not looking for the last request, but
>>for the request that triggered the assertion. It is very difficult to do
>>that with just ALL,2 and stripped executable, especially when you have
>>lots of concurrent transactions flying by.

On 04.11.19 18:59, Matus UHLAR - fantomas wrote:
>No bumping. I can create unstripped executable (great if it works with
>current core, but I may create new one).

I didn't know about debian archive for debug symbols. Now I have full
backtrace, hopefully:

(gdb) backtrace
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  0x00007f93a15b2535 in __GI_abort () at abort.c:79
#2  0x00005654ebcee5ad in xassert (msg=msg at entry=0x5654ec133290 "lowestOffset () <= target_offset", file=file at entry=0x5654ec133043 "stmem.cc", line=line at entry=98) at debug.cc:618
#3  0x00005654ebe6c170 in mem_hdr::freeDataUpto (this=this at entry=0x5654f6c11988, target_offset=target_offset at entry=0) at stmem.cc:39
#4  0x00005654ebe27440 in MemObject::trimSwappable (this=0x5654f6c11950) at MemObject.cc:400
#5  0x00005654ebe74bf5 in StoreEntry::trimMemory (this=0x5654edd697b0, preserveSwappable=<optimized out>) at store.cc:1902
#6  0x00005654ec100a80 in Store::Controller::memoryOut (this=<optimized out>, e=..., preserveSwappable=<optimized out>) at Controller.cc:557
#7  0x00005654ebe84305 in StoreEntry::swapOut (this=0x5654edd697b0) at store_swapout.cc:185
#8  0x00005654ebe7b3fd in StoreEntry::invokeHandlers (this=this at entry=0x5654edd697b0) at store_client.cc:719
#9  0x00005654ebe736ac in StoreEntry::write (this=0x5654edd697b0, writeBuffer=...) at store.cc:836
#10 0x00005654ebf0a590 in Client::storeReplyBody (this=this at entry=0x5654f15e4d88,
    data=0x5654f5a21f70 "A\336{\t,8K\246\353\257=\"r\r\210\375u\254\061\016\301\001\t\255\265\206\030h\350`a\300Kd\002\222k\354\247\016 at 3m\211v\204\227rdL\030\006\026j\024\002XXM\240R)\343k?\017\222\261\367X\033\234:\204\242\315\032\205\020\032*Zm?\273\272\361\060\310j\276\217o\364m\377O\375\006&?", len=len at entry=1400) at ../../src/StoreIOBuffer.h:23
#11 0x00005654ebf0a5d3 in Client::addVirginReplyBody (this=this at entry=0x5654f15e4d88, data=<optimized out>, len=len at entry=1400) at Client.cc:1005
#12 0x00005654ebdeca37 in HttpStateData::writeReplyBody (this=0x5654f15e4d88) at http.cc:1359
#13 0x00005654ebdf1bd5 in HttpStateData::processReplyBody (this=0x5654f15e4d88) at http.cc:1424
#14 0x00005654ebdf382a in HttpStateData::processReply (this=0x5654f15e4d88) at http.cc:1251
#15 0x00005654ebdf4da8 in HttpStateData::readReply (this=0x5654f15e4d88, io=...) at http.cc:1223
#16 0x00005654ebdf8d6b in JobDialer<HttpStateData>::dial (this=0x5654f281c0f0, call=...) at base/AsyncJobCalls.h:169
#17 0x00005654ebf5be51 in AsyncCall::make (this=this at entry=0x5654f281c0c0) at AsyncCall.cc:40
#18 0x00005654ebf5d3a4 in AsyncCallQueue::fireNext (this=<optimized out>) at AsyncCallQueue.cc:56
#19 0x00005654ebf5d6e9 in AsyncCallQueue::fire (this=0x5654ed4cdc40) at AsyncCallQueue.cc:42
#20 0x00005654ebdb98ea in EventLoop::dispatchCalls (this=0x7ffcde3afba0) at EventLoop.cc:144
#21 EventLoop::runOnce (this=this at entry=0x7ffcde3afba0) at EventLoop.cc:121
#22 0x00005654ebdb99d8 in EventLoop::run (this=this at entry=0x7ffcde3afba0) at EventLoop.cc:83
#23 0x00005654ebe22ab9 in SquidMain (argc=<optimized out>, argv=<optimized out>) at main.cc:1707
#24 0x00005654ebd13901 in SquidMainSafe (argv=0x7ffcde3b0008, argc=6) at main.cc:1415
#25 main (argc=6, argv=0x7ffcde3b0008) at main.cc:1403

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
You have the right to remain silent. Anything you say will be misquoted,
then used against you.


From nick at howitts.co.uk  Tue Nov  5 09:40:43 2019
From: nick at howitts.co.uk (Nick Howitt)
Date: Tue, 5 Nov 2019 09:40:43 +0000
Subject: [squid-users] Another "Forwarding loop detected" issue
Message-ID: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>

I am trying to help someone who is running squid-3.5.20-12 on a 
standalone server with the dansguardian content filter and suddenly 
recently has been getting a lot of messages like:

    2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
    HEAD / HTTP/1.0
    Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
    Cache-Control: max-age=259200
    Connection: keep-alive
    X-Forwarded-For: 10.10.1.2
    Host: 10.10.1.2:8080


The access log looks something like:

    1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
    http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
    1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
    http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
    1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
    http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -

(but these are for different transactions - they are all the same apart 
from the timestamps)

The content filter listens on port 8080 and squid on 3128. The machine 
is on 10.10.1.2

All the other posts I've seen seem to be for transparent mode or where 
there is a User Agent string. I have found nothing to cover this 
scenario. How can I troubleshoot to fix it and what information do you 
need from me to help diagnose?

The only thing I have thought of is to put in a firewall rule blocking 
traffic from 10.10.1.2 to 10.10.1.2:8080 but I fear shooting myself in 
the foot.

Any help would be greatly appreciated.

Nick



From squid3 at treenet.co.nz  Tue Nov  5 10:44:07 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Nov 2019 23:44:07 +1300
Subject: [squid-users] Another "Forwarding loop detected" issue
In-Reply-To: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
References: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
Message-ID: <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>

On 5/11/19 10:40 pm, Nick Howitt wrote:
> I am trying to help someone who is running squid-3.5.20-12 on a
> standalone server with the dansguardian content filter and suddenly
> recently has been getting a lot of messages like:
> 
> ?? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
> ?? HEAD / HTTP/1.0
> ?? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
> ?? Cache-Control: max-age=259200
> ?? Connection: keep-alive
> ?? X-Forwarded-For: 10.10.1.2
> ?? Host: 10.10.1.2:8080
> 
> 
> The access log looks something like:
> 
> ?? 1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
> ?? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
> ?? 1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
> ?? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
> ?? 1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
> ?? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
> 
> (but these are for different transactions - they are all the same apart
> from the timestamps)
> 

That is what a forwarding loop looks like in the access.log.


> The content filter listens on port 8080 and squid on 3128. The machine
> is on 10.10.1.2
> 
> All the other posts I've seen seem to be for transparent mode or where
> there is a User Agent string. I have found nothing to cover this
> scenario. How can I troubleshoot to fix it and what information do you
> need from me to help diagnose?
> 

Something is telling Squid the origin server being contacted exists at
10.10.1.2:8080. You can see that in the Host header of the message.

I would trace the traffic flow from the client to Squid.

Amos


From nick at howitts.co.uk  Tue Nov  5 11:07:32 2019
From: nick at howitts.co.uk (Nick Howitt)
Date: Tue, 5 Nov 2019 11:07:32 +0000
Subject: [squid-users] Another "Forwarding loop detected" issue
In-Reply-To: <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>
References: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
 <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>
Message-ID: <8b62367c-7021-b392-5b87-76393d405457@howitts.co.uk>



On 05/11/2019 10:44, Amos Jeffries wrote:
> On 5/11/19 10:40 pm, Nick Howitt wrote:
>> I am trying to help someone who is running squid-3.5.20-12 on a
>> standalone server with the dansguardian content filter and suddenly
>> recently has been getting a lot of messages like:
>>
>>  ?? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
>>  ?? HEAD / HTTP/1.0
>>  ?? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
>>  ?? Cache-Control: max-age=259200
>>  ?? Connection: keep-alive
>>  ?? X-Forwarded-For: 10.10.1.2
>>  ?? Host: 10.10.1.2:8080
>>
>>
>> The access log looks something like:
>>
>>  ?? 1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>  ?? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>  ?? 1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>  ?? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>  ?? 1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>  ?? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>
>> (but these are for different transactions - they are all the same apart
>> from the timestamps)
>>
> That is what a forwarding loop looks like in the access.log.
>
>
>> The content filter listens on port 8080 and squid on 3128. The machine
>> is on 10.10.1.2
>>
>> All the other posts I've seen seem to be for transparent mode or where
>> there is a User Agent string. I have found nothing to cover this
>> scenario. How can I troubleshoot to fix it and what information do you
>> need from me to help diagnose?
>>
> Something is telling Squid the origin server being contacted exists at
> 10.10.1.2:8080. You can see that in the Host header of the message.
>
> I would trace the traffic flow from the client to Squid.
>
But isn't everything coming to 8080 as that is the proxy you'd set up in 
the browser? I'm afraid I don't understand how proxying works at the 
packet level. I see nothing before these messages to indicate the 
packets are coming from elsewhere. A cut down startup log looks like:

    <snip>
    2019/10/31 13:47:40 kid1| helperOpenServers: Starting 5/5
    'ext_unix_group_acl' processes
    2019/10/31 13:47:40 kid1| HTCP Disabled.
    2019/10/31 13:47:40 kid1| Finished loading MIME types and icons.
    2019/10/31 13:47:40 kid1| Accepting HTTP Socket connections at
    local=[::1]:3128 remote=[::] FD 2021 flags=9
    2019/10/31 13:47:40 kid1| Accepting HTTP Socket connections at
    local=127.0.0.1:3128 remote=[::] FD 2022 flags=9
    2019/10/31 13:47:40 kid1| Accepting HTTP Socket connections at
    local=10.10.1.2:3128 remote=[::] FD 2023 flags=9
    2019/10/31 13:48:12 kid1| WARNING: Forwarding loop detected for:
    HEAD / HTTP/1.0
    Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
    Cache-Control: max-age=259200
    Connection: keep-alive
    X-Forwarded-For: 10.10.1.2
    Host: 10.10.1.2:8080


    2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
    HEAD / HTTP/1.0
    Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
    Cache-Control: max-age=259200
    Connection: keep-alive
    X-Forwarded-For: 10.10.1.2
    Host: 10.10.1.2:8080


Is there anything I can look for in my logs or do I need to do some sort 
of tcpdump with some filters?

Thanks,

Nick




From nick at howitts.co.uk  Tue Nov  5 12:57:31 2019
From: nick at howitts.co.uk (Nick Howitt)
Date: Tue, 5 Nov 2019 12:57:31 +0000
Subject: [squid-users] Another "Forwarding loop detected" issue
In-Reply-To: <8b62367c-7021-b392-5b87-76393d405457@howitts.co.uk>
References: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
 <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>
 <8b62367c-7021-b392-5b87-76393d405457@howitts.co.uk>
Message-ID: <e11f5b05-84be-ec74-e06d-92ac689ccb45@howitts.co.uk>

On 05/11/2019 11:07, Nick Howitt wrote:
>
>
> On 05/11/2019 10:44, Amos Jeffries wrote:
>> On 5/11/19 10:40 pm, Nick Howitt wrote:
>>> I am trying to help someone who is running squid-3.5.20-12 on a
>>> standalone server with the dansguardian content filter and suddenly
>>> recently has been getting a lot of messages like:
>>>
>>> ??? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
>>> ??? HEAD / HTTP/1.0
>>> ??? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
>>> ??? Cache-Control: max-age=259200
>>> ??? Connection: keep-alive
>>> ??? X-Forwarded-For: 10.10.1.2
>>> ??? Host: 10.10.1.2:8080
>>>
>>>
>>> The access log looks something like:
>>>
>>> ??? 1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>> ??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>> ??? 1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>> ??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>> ??? 1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>> ??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>
>>> (but these are for different transactions - they are all the same apart
>>> from the timestamps)
>>>
>> That is what a forwarding loop looks like in the access.log.
>>
>>
>>> The content filter listens on port 8080 and squid on 3128. The machine
>>> is on 10.10.1.2
>>>
>>> All the other posts I've seen seem to be for transparent mode or where
>>> there is a User Agent string. I have found nothing to cover this
>>> scenario. How can I troubleshoot to fix it and what information do you
>>> need from me to help diagnose?
>>>
>> Something is telling Squid the origin server being contacted exists at
>> 10.10.1.2:8080. You can see that in the Host header of the message.
>>
>> I would trace the traffic flow from the client to Squid.
>>
> But isn't everything coming to 8080 as that is the proxy you'd set up 
> in the browser? I'm afraid I don't understand how proxying works at 
> the packet level. I see nothing before these messages to indicate the 
> packets are coming from elsewhere. A cut down startup log looks like:
>
> ?? <snip>
> ?? 2019/10/31 13:47:40 kid1| helperOpenServers: Starting 5/5
> ?? 'ext_unix_group_acl' processes
> ?? 2019/10/31 13:47:40 kid1| HTCP Disabled.
> ?? 2019/10/31 13:47:40 kid1| Finished loading MIME types and icons.
> ?? 2019/10/31 13:47:40 kid1| Accepting HTTP Socket connections at
> ?? local=[::1]:3128 remote=[::] FD 2021 flags=9
> ?? 2019/10/31 13:47:40 kid1| Accepting HTTP Socket connections at
> ?? local=127.0.0.1:3128 remote=[::] FD 2022 flags=9
> ?? 2019/10/31 13:47:40 kid1| Accepting HTTP Socket connections at
> ?? local=10.10.1.2:3128 remote=[::] FD 2023 flags=9
> ?? 2019/10/31 13:48:12 kid1| WARNING: Forwarding loop detected for:
> ?? HEAD / HTTP/1.0
> ?? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
> ?? Cache-Control: max-age=259200
> ?? Connection: keep-alive
> ?? X-Forwarded-For: 10.10.1.2
> ?? Host: 10.10.1.2:8080
>
>
> ?? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
> ?? HEAD / HTTP/1.0
> ?? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
> ?? Cache-Control: max-age=259200
> ?? Connection: keep-alive
> ?? X-Forwarded-For: 10.10.1.2
> ?? Host: 10.10.1.2:8080
>
>
> Is there anything I can look for in my logs or do I need to do some 
> sort of tcpdump with some filters?
>
> Thanks,
>
> Nick
At the moment the wpad file is not pointing to the proxy server so no 
machines should be using it. I have tried a:

    tcpdump -vvvnnn -A -i eth0 port 8080 -s 1500


This gives me bursts of:

    07:50:47.569305 IP (tos 0x0, ttl 128, id 56718, offset 0, flags
    [DF], proto TCP (6), length 52)
     ??? 10.10.11.215.64857 > 10.10.1.2.8080: Flags [S], cksum 0x389b
    (correct), seq 625662051, win 64240, options [mss 1460,nop,wscale
    8,nop,nop,sackOK], length 0
    E..4.. at ....H

    ..

    ...Y..%J.c........8...............
    07:50:47.569419 IP (tos 0x0, ttl 64, id 7161, offset 0, flags [DF],
    proto TCP (6), length 40)
     ??? 10.10.1.2.8080 > 10.10.11.215.64857: Flags [R.], cksum 0x744b
    (correct), seq 0, ack 1, win 0, length 0
    E..(.. at .@...

    ..

    .....Y....%J.dP...tK..


 From what I've researched so far there are no http headers in these 
packets. The proxy is 10.10.1.2. Does this mean 10.10.11.215 could be 
the offending machine if no other machines should be using the proxy? Or 
do I need to do something cleverer with my tcpdump?



From rousskov at measurement-factory.com  Tue Nov  5 14:02:07 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Nov 2019 09:02:07 -0500
Subject: [squid-users] Cannot configure squid 4.6 to splice without
 bumping
In-Reply-To: <CAOWEjnCwaB-s80pvH9S-vKoW6EvGN-WH3Tq+7u1cS5zMa2qjfg@mail.gmail.com>
References: <CAOWEjnCwaB-s80pvH9S-vKoW6EvGN-WH3Tq+7u1cS5zMa2qjfg@mail.gmail.com>
Message-ID: <07da2884-cba9-7b89-1db8-4c30af9f3cac@measurement-factory.com>

On 11/5/19 1:26 AM, John Lowry wrote:

> I've tried many, many different settings and I always get traffic
> bumped. Here is an example:

> http_port 3128 intercept?
> 
> https_port 3129 intercept tls-cert=/etc/squid/ssl_cert/myCA.pem
> tls-key=/etc/squid/ssl_cert/myCA.pem

The above configuration does not enable SslBump features.


> ssl_bump peek step1
> ssl_bump peek step2
> ssl_bump splice step2

These rules are poorly written (the last one will never match), but they
are unused because the port directives do not enable SslBump.

If an SSL connection is bumped (or even peeked at!) with the above
configuration, then there is a Squid bug somewhere. However, I do not
think your TLS connections are actually bumped. Please see below.


> I've tried setting debug_options to 9 but cannot see anything useful in
> the logs to indicate why it is not splicing. I always just see the full
> set of request headers in the logs for HTTPS connections, indicating
> that the connection is bumped.?

I suspect your Squid is acting as an intercepting HTTPS proxy: It
terminates all intercepted SSL connections as if they were directed at
the Squid instance itself. The end result will look similar to bumping
from "I can see the headers" point of view.

You may be able to tell the difference by looking at certificate
details: With an HTTPS proxy, all connections will have the same leaf
myCA.pem certificate as opposed to mimicked origin server certificate
signed by myCA.pem. There may be other, more obvious signs like the
details of the "Accepting..." lines that Squid reports at startup.


> One thing I did notice is that the ssl logformat options do not work. I
> get errors like this on restart:

> FATAL: Can't parse configuration token: '%ssl::>sni'

Was your Squid built with OpenSSL support? The details are
version-specific, but you can find them (and the configuration result)
using the following commands:

  ./configure --help | fgrep -5i ssl
  squid -v

Alex.


From rousskov at measurement-factory.com  Tue Nov  5 14:10:50 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Nov 2019 09:10:50 -0500
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <20191105093413.GB5031@fantomas.sk>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
 <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
 <20191104162121.GA14172@fantomas.sk>
 <b2e38136-dda2-239b-3a5b-3b518917bdde@measurement-factory.com>
 <20191104175903.GA16466@fantomas.sk> <20191105093413.GB5031@fantomas.sk>
Message-ID: <b9d7e95d-0f72-655c-5ded-ff8f67783dd6@measurement-factory.com>

On 11/5/19 4:34 AM, Matus UHLAR - fantomas wrote:

> Now I have full backtrace, hopefully:

You are making progress, but the backtrace itself is not sufficient. One
has to examine the core dump to find out transaction details (e.g., the
URL) _and_ log enough of those details into cache.log (usually at levels
much higher than ALL,2) to restore Squid state and figure out what is
going on or build a reliable reproducer. Sorry if I was not clear about
that from the start.

It does take time and some expertise to figure out what is going on.
Unfortunately, I cannot personally guide you through this painful
process right now.

Alex.


> (gdb) backtrace
> #0? __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
> #1? 0x00007f93a15b2535 in __GI_abort () at abort.c:79
> #2? 0x00005654ebcee5ad in xassert (msg=msg at entry=0x5654ec133290
> "lowestOffset () <= target_offset", file=file at entry=0x5654ec133043
> "stmem.cc", line=line at entry=98) at debug.cc:618
> #3? 0x00005654ebe6c170 in mem_hdr::freeDataUpto
> (this=this at entry=0x5654f6c11988, target_offset=target_offset at entry=0) at
> stmem.cc:39
> #4? 0x00005654ebe27440 in MemObject::trimSwappable (this=0x5654f6c11950)
> at MemObject.cc:400
> #5? 0x00005654ebe74bf5 in StoreEntry::trimMemory (this=0x5654edd697b0,
> preserveSwappable=<optimized out>) at store.cc:1902
> #6? 0x00005654ec100a80 in Store::Controller::memoryOut (this=<optimized
> out>, e=..., preserveSwappable=<optimized out>) at Controller.cc:557
> #7? 0x00005654ebe84305 in StoreEntry::swapOut (this=0x5654edd697b0) at
> store_swapout.cc:185
> #8? 0x00005654ebe7b3fd in StoreEntry::invokeHandlers
> (this=this at entry=0x5654edd697b0) at store_client.cc:719
> #9? 0x00005654ebe736ac in StoreEntry::write (this=0x5654edd697b0,
> writeBuffer=...) at store.cc:836
> #10 0x00005654ebf0a590 in Client::storeReplyBody
> (this=this at entry=0x5654f15e4d88,
> ?? data=0x5654f5a21f70 "A\336{\t,8K\246\353\257=\"r\r\210\375u\254\061
> \016\301\001\t\255\265\206\030h\350`a\300Kd\002\222k\354\247\016 at 3m\211v
> \204\227rdL\030\006\026j\024\002XXM\240R)\343k?\017\222\261\367X\033
> \234:\204\242\315\032\205\020\032*Zm?\273\272\361\060\310j\276\217o
> \364m\377O\375\006&?", len=len at entry=1400) at ../../src/StoreIOBuffer.h:23
> #11 0x00005654ebf0a5d3 in Client::addVirginReplyBody
> (this=this at entry=0x5654f15e4d88, data=<optimized out>,
> len=len at entry=1400) at Client.cc:1005
> #12 0x00005654ebdeca37 in HttpStateData::writeReplyBody
> (this=0x5654f15e4d88) at http.cc:1359
> #13 0x00005654ebdf1bd5 in HttpStateData::processReplyBody
> (this=0x5654f15e4d88) at http.cc:1424
> #14 0x00005654ebdf382a in HttpStateData::processReply
> (this=0x5654f15e4d88) at http.cc:1251
> #15 0x00005654ebdf4da8 in HttpStateData::readReply (this=0x5654f15e4d88,
> io=...) at http.cc:1223
> #16 0x00005654ebdf8d6b in JobDialer<HttpStateData>::dial
> (this=0x5654f281c0f0, call=...) at base/AsyncJobCalls.h:169
> #17 0x00005654ebf5be51 in AsyncCall::make
> (this=this at entry=0x5654f281c0c0) at AsyncCall.cc:40
> #18 0x00005654ebf5d3a4 in AsyncCallQueue::fireNext (this=<optimized
> out>) at AsyncCallQueue.cc:56
> #19 0x00005654ebf5d6e9 in AsyncCallQueue::fire (this=0x5654ed4cdc40) at
> AsyncCallQueue.cc:42
> #20 0x00005654ebdb98ea in EventLoop::dispatchCalls (this=0x7ffcde3afba0)
> at EventLoop.cc:144
> #21 EventLoop::runOnce (this=this at entry=0x7ffcde3afba0) at EventLoop.cc:121
> #22 0x00005654ebdb99d8 in EventLoop::run
> (this=this at entry=0x7ffcde3afba0) at EventLoop.cc:83
> #23 0x00005654ebe22ab9 in SquidMain (argc=<optimized out>,
> argv=<optimized out>) at main.cc:1707
> #24 0x00005654ebd13901 in SquidMainSafe (argv=0x7ffcde3b0008, argc=6) at
> main.cc:1415
> #25 main (argc=6, argv=0x7ffcde3b0008) at main.cc:1403
> 



From trapexit at spawn.link  Tue Nov  5 16:05:55 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Tue, 5 Nov 2019 11:05:55 -0500
Subject: [squid-users] optional verification of clients?
In-Reply-To: <97180954-5943-6bb3-4ccb-0d1024cac911@treenet.co.nz>
References: <e35417dc-192c-d7ca-cb1c-1e4b2498d2a8@spawn.link>
 <bdef7112-a855-3be9-9883-ec3a560c8784@treenet.co.nz>
 <b165a209-dacf-dd6c-57b6-fd9f466eca2b@spawn.link>
 <97180954-5943-6bb3-4ccb-0d1024cac911@treenet.co.nz>
Message-ID: <e44260fa-0c39-168b-2fd3-e69c49119e10@spawn.link>

On 11/1/2019 8:37 PM, Amos Jeffries wrote:
> Oh well. That was the closest Squid has. I was hoping the library would
> sent cert request but not verify the clients response. So the details
> would be available for logging etc as handshake parameters.
> 
> If that client cert request/delivery is not working then the only
> alternative would be two proxy ports, one with client certificates
> required and one without. Which does not match what you are trying to
> achieve.
> 
> 
> If this is of particular importance patch/PR are welcome. I will keep it
> in mind for future TLS improvements, but there is no guarantees that way.
> <https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F>
> <https://wiki.squid-cache.org/DeveloperResources>

I've done a quick hack to remove SSL_VERIFY_FAIL_IF_NO_PEER_CERT from 
Ssl::SetupVerifyCallback in ssl/support.cc. It *appears* that this 
accomplishes what I want. I'm seeing client cert info when provided and 
not when I don't (in acl user_cert, logging, external_acl_handler, etc.) 
Anyone know if there may be some gotchas that I could be missing? Some 
data structures or behavior expecting the VERIFY_FAIL_IF_NO_PEER_CERT 
behavior? If it sounds safe I'll look into turning this into a proper 
sslflags option.


From felipeapolanco at gmail.com  Tue Nov  5 20:06:03 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 5 Nov 2019 16:06:03 -0400
Subject: [squid-users] How to use http_status acl?
Message-ID: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>

Hi,

I have been trying to match http_status acl in my squid.conf file but it
has no effect.

My goal is to add a given header to specific HTTP return codes.

eg:
This works:
acl user1 src 192.168.0.6/32
reply_header_add Cache-Control "no-store" user1

This doesn't work:
acl 307_redirect http_status 307
reply_header_add Cache-Control "no-store" 307_redirect

Any ideas on what could I be missing here?

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191105/25d4d8ed/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov  5 20:43:45 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Nov 2019 15:43:45 -0500
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
Message-ID: <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>

On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:

> I have been trying to match http_status acl in my squid.conf file but it
> has no effect.
> 
> My goal is to add a given header to specific HTTP return?codes.
> 
> eg:
> This works:
> acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
> reply_header_add Cache-Control "no-store" user1
> 
> This doesn't work:
> acl 307_redirect http_status 307
> reply_header_add Cache-Control "no-store" 307_redirect
> 
> Any ideas on what could I be missing here?

Does that 307 response come from a server (including cache_peers) or is
it generated by Squid itself?

Alex.


From felipeapolanco at gmail.com  Tue Nov  5 21:23:53 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 5 Nov 2019 17:23:53 -0400
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
Message-ID: <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>

It comes from an Icap server but I tried 200 status code from the webserver
directly and doesn't work either.

On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
>
> > I have been trying to match http_status acl in my squid.conf file but it
> > has no effect.
> >
> > My goal is to add a given header to specific HTTP return codes.
> >
> > eg:
> > This works:
> > acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
> > reply_header_add Cache-Control "no-store" user1
> >
> > This doesn't work:
> > acl 307_redirect http_status 307
> > reply_header_add Cache-Control "no-store" 307_redirect
> >
> > Any ideas on what could I be missing here?
>
> Does that 307 response come from a server (including cache_peers) or is
> it generated by Squid itself?
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191105/d4e71cee/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov  5 22:01:00 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Nov 2019 17:01:00 -0500
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
 <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
Message-ID: <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>

On 11/5/19 4:23 PM, Felipe Arturo Polanco wrote:
> I tried 200 status code from the
> webserver directly and doesn't work either.

Sounds like a Squid bug to me then. If you can reproduce with Squid v4
or later, please consider filing a bug report in Squid bugzilla. Quality
fixes welcomed.

Alex.


> On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov wrote:
> 
>     On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
> 
>     > I have been trying to match http_status acl in my squid.conf file
>     but it
>     > has no effect.
>     >
>     > My goal is to add a given header to specific HTTP return?codes.
>     >
>     > eg:
>     > This works:
>     > acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
>     <http://192.168.0.6/32>
>     > reply_header_add Cache-Control "no-store" user1
>     >
>     > This doesn't work:
>     > acl 307_redirect http_status 307
>     > reply_header_add Cache-Control "no-store" 307_redirect
>     >
>     > Any ideas on what could I be missing here?
> 
>     Does that 307 response come from a server (including cache_peers) or is
>     it generated by Squid itself?
> 
>     Alex.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 



From uhlar at fantomas.sk  Wed Nov  6 09:39:59 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 6 Nov 2019 10:39:59 +0100
Subject: [squid-users] Another "Forwarding loop detected" issue
In-Reply-To: <e11f5b05-84be-ec74-e06d-92ac689ccb45@howitts.co.uk>
References: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
 <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>
 <8b62367c-7021-b392-5b87-76393d405457@howitts.co.uk>
 <e11f5b05-84be-ec74-e06d-92ac689ccb45@howitts.co.uk>
Message-ID: <20191106093959.GD5845@fantomas.sk>

>>>On 5/11/19 10:40 pm, Nick Howitt wrote:
>>>>I am trying to help someone who is running squid-3.5.20-12 on a
>>>>standalone server with the dansguardian content filter and suddenly
>>>>recently has been getting a lot of messages like:
>>>>
>>>>??? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
>>>>??? HEAD / HTTP/1.0
>>>>??? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
>>>>??? Cache-Control: max-age=259200
>>>>??? Connection: keep-alive
>>>>??? X-Forwarded-For: 10.10.1.2
>>>>??? Host: 10.10.1.2:8080
>>>>
>>>>
>>>>The access log looks something like:
>>>>
>>>>??? 1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>??? 1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>??? 1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>
>>>>(but these are for different transactions - they are all the same apart
>>>>from the timestamps)


>>On 05/11/2019 10:44, Amos Jeffries wrote:
>>>That is what a forwarding loop looks like in the access.log.

>>>>The content filter listens on port 8080 and squid on 3128. The machine
>>>>is on 10.10.1.2

\On 05.11.19 12:57, Nick Howitt wrote:
>At the moment the wpad file is not pointing to the proxy server so no 
>machines should be using it. I have tried a:
>
>   tcpdump -vvvnnn -A -i eth0 port 8080 -s 1500
>
>
>This gives me bursts of:
>
>   07:50:47.569305 IP (tos 0x0, ttl 128, id 56718, offset 0, flags
>   [DF], proto TCP (6), length 52)
>    ??? 10.10.11.215.64857 > 10.10.1.2.8080: Flags [S], cksum 0x389b

>From what I've researched so far there are no http headers in these 
>packets. The proxy is 10.10.1.2. Does this mean 10.10.11.215 could be 
>the offending machine if no other machines should be using the proxy? 
>Or do I need to do something cleverer with my tcpdump?

I don't think so.

How does your schema look like?
How does your content filter work?

The logs above show that someone from local machins (content-filter) is
using squid to access local machine port 8080, which should be your content
filter. 

That looks much like a loop, connections from squid or content filter that
are going back to content filter via squid



-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Chernobyl was an Windows 95 beta test site.


From nick at howitts.co.uk  Wed Nov  6 09:54:31 2019
From: nick at howitts.co.uk (Nick Howitt)
Date: Wed, 6 Nov 2019 09:54:31 +0000
Subject: [squid-users] Another "Forwarding loop detected" issue
In-Reply-To: <20191106093959.GD5845@fantomas.sk>
References: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
 <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>
 <8b62367c-7021-b392-5b87-76393d405457@howitts.co.uk>
 <e11f5b05-84be-ec74-e06d-92ac689ccb45@howitts.co.uk>
 <20191106093959.GD5845@fantomas.sk>
Message-ID: <dab4e715-ce67-5ac6-c716-da41d93c53ad@howitts.co.uk>



On 06/11/2019 09:39, Matus UHLAR - fantomas wrote:
>>>> On 5/11/19 10:40 pm, Nick Howitt wrote:
>>>>> I am trying to help someone who is running squid-3.5.20-12 on a
>>>>> standalone server with the dansguardian content filter and suddenly
>>>>> recently has been getting a lot of messages like:
>>>>>
>>>>> ??? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
>>>>> ??? HEAD / HTTP/1.0
>>>>> ??? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
>>>>> ??? Cache-Control: max-age=259200
>>>>> ??? Connection: keep-alive
>>>>> ??? X-Forwarded-For: 10.10.1.2
>>>>> ??? Host: 10.10.1.2:8080
>>>>>
>>>>>
>>>>> The access log looks something like:
>>>>>
>>>>> ??? 1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>> ??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>> ??? 1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>> ??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>> ??? 1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>> ??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>>
>>>>> (but these are for different transactions - they are all the same 
>>>>> apart
>>>>> from the timestamps)
>
>
>>> On 05/11/2019 10:44, Amos Jeffries wrote:
>>>> That is what a forwarding loop looks like in the access.log.
>
>>>>> The content filter listens on port 8080 and squid on 3128. The 
>>>>> machine
>>>>> is on 10.10.1.2
>
> \On 05.11.19 12:57, Nick Howitt wrote:
>> At the moment the wpad file is not pointing to the proxy server so no 
>> machines should be using it. I have tried a:
>>
>> ? tcpdump -vvvnnn -A -i eth0 port 8080 -s 1500
>>
>>
>> This gives me bursts of:
>>
>> ? 07:50:47.569305 IP (tos 0x0, ttl 128, id 56718, offset 0, flags
>> ? [DF], proto TCP (6), length 52)
>> ?? ??? 10.10.11.215.64857 > 10.10.1.2.8080: Flags [S], cksum 0x389b
>
>> From what I've researched so far there are no http headers in these 
>> packets. The proxy is 10.10.1.2. Does this mean 10.10.11.215 could be 
>> the offending machine if no other machines should be using the proxy? 
>> Or do I need to do something cleverer with my tcpdump?
>
> I don't think so.
>
> How does your schema look like?
> How does your content filter work?
>
> The logs above show that someone from local machins (content-filter) is
> using squid to access local machine port 8080, which should be your 
> content
> filter.
> That looks much like a loop, connections from squid or content filter 
> that
> are going back to content filter via squid
>
>
>
The set up is eth0 (10.10.1.2:8080) -> Content filter (dansguardian) -> 
Squid (port 3128) -> eth0 -> gateway

If what you are saying is right then a firewall rule blocking source 
10.10.1.2 to 10.10.1.2:8080 may work. I am not sure if it would be in 
the FORWARD or INPUT chain and I don't know if it would cause collateral 
damage. It also does not explain why only recently it started going 
wrong. The machine has been rebuilt now and I am waiting for it to 
trigger again, upgrading from ClearOS6.x (a Centos derivative) to 
ClearOS 7.6 (which will soon update to 7.7).



From uhlar at fantomas.sk  Wed Nov  6 10:59:03 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 6 Nov 2019 11:59:03 +0100
Subject: [squid-users] Another "Forwarding loop detected" issue
In-Reply-To: <dab4e715-ce67-5ac6-c716-da41d93c53ad@howitts.co.uk>
References: <97c9d063-4a2c-70cc-1643-73cedc953ab8@howitts.co.uk>
 <c8ad2aab-da8b-54a6-1b69-b6775b518955@treenet.co.nz>
 <8b62367c-7021-b392-5b87-76393d405457@howitts.co.uk>
 <e11f5b05-84be-ec74-e06d-92ac689ccb45@howitts.co.uk>
 <20191106093959.GD5845@fantomas.sk>
 <dab4e715-ce67-5ac6-c716-da41d93c53ad@howitts.co.uk>
Message-ID: <20191106105903.GE5845@fantomas.sk>

>On 06/11/2019 09:39, Matus UHLAR - fantomas wrote:
>>>>>On 5/11/19 10:40 pm, Nick Howitt wrote:
>>>>>>I am trying to help someone who is running squid-3.5.20-12 on a
>>>>>>standalone server with the dansguardian content filter and suddenly
>>>>>>recently has been getting a lot of messages like:
>>>>>>
>>>>>>??? 2019/10/31 13:48:14 kid1| WARNING: Forwarding loop detected for:
>>>>>>??? HEAD / HTTP/1.0
>>>>>>??? Via: 1.0 HSFilterHyperos7.haftr.local (squid/3.5.20)
>>>>>>??? Cache-Control: max-age=259200
>>>>>>??? Connection: keep-alive
>>>>>>??? X-Forwarded-For: 10.10.1.2
>>>>>>??? Host: 10.10.1.2:8080
>>>>>>
>>>>>>
>>>>>>The access log looks something like:
>>>>>>
>>>>>>??? 1572545946.383 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>>>??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>>>??? 1572545946.477 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>>>??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>>>??? 1572545946.493 120000 10.10.1.2 TCP_MISS_ABORTED/000 0 HEAD
>>>>>>??? http://10.10.1.2:8080/ - HIER_DIRECT/10.10.1.2 -
>>>>>>
>>>>>>(but these are for different transactions - they are all the 
>>>>>>same apart
>>>>>>from the timestamps)
>>
>>
>>>>On 05/11/2019 10:44, Amos Jeffries wrote:
>>>>>That is what a forwarding loop looks like in the access.log.
>>
>>>>>>The content filter listens on port 8080 and squid on 3128. 
>>>>>>The machine
>>>>>>is on 10.10.1.2

>>How does your schema look like?
>>How does your content filter work?
>>
>>The logs above show that someone from local machins (content-filter) is
>>using squid to access local machine port 8080, which should be your 
>>content
>>filter.
>>That looks much like a loop, connections from squid or content 
>>filter that
>>are going back to content filter via squid

On 06.11.19 09:54, Nick Howitt wrote:
>The set up is eth0 (10.10.1.2:8080) -> Content filter (dansguardian) 
>-> Squid (port 3128) -> eth0 -> gateway

I understand this as:

client
->
10.10.1.2:8080 aka Content filter (dansguardian)
->
10.10.1.2:3128 aka squid 
->
the net.


>If what you are saying is right then a firewall rule blocking source 
>10.10.1.2 to 10.10.1.2:8080 may work

apparently, but I don't understand why would anyone from 10.10.1.2 to
10.10.1.2:8080.
Is it any HTTP client running on 10.10.1.2 ? Then it's ok.

Is it squid or dansguardian ?  Then something is broken in your setup, or,
any client is requesting 10.10.1.2:8080 which should apparently be disabled
in squid config.

> I am not sure if it would be in 
>the FORWARD or INPUT chain

INPUT chain, since it's connection from to local IP, unless it's redirected
connection.

But IIRC you have said your clients have proxy configured.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Emacs is a complicated operating system without good text editor.


From felipeapolanco at gmail.com  Wed Nov  6 13:49:33 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Wed, 6 Nov 2019 09:49:33 -0400
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
 <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
 <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>
Message-ID: <CADcj3=497cy=0ZSHyWnOLBQvzeqv_xGG_zQrL4sJH1ODBfBodw@mail.gmail.com>

I have this warning in the logs:

WARNING: 307_redirect ACL is used in context without an HTTP response.
Assuming mismatch.
Acl.cc(151) matches: checked: 307_redirect = 0

I also tested using rep_header ACL and that causes the same warning and
defaulting to 0.

Do I need anything else to make reply access lists to work?

On Tue, Nov 5, 2019 at 6:01 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/5/19 4:23 PM, Felipe Arturo Polanco wrote:
> > I tried 200 status code from the
> > webserver directly and doesn't work either.
>
> Sounds like a Squid bug to me then. If you can reproduce with Squid v4
> or later, please consider filing a bug report in Squid bugzilla. Quality
> fixes welcomed.
>
> Alex.
>
>
> > On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov wrote:
> >
> >     On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
> >
> >     > I have been trying to match http_status acl in my squid.conf file
> >     but it
> >     > has no effect.
> >     >
> >     > My goal is to add a given header to specific HTTP return codes.
> >     >
> >     > eg:
> >     > This works:
> >     > acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
> >     <http://192.168.0.6/32>
> >     > reply_header_add Cache-Control "no-store" user1
> >     >
> >     > This doesn't work:
> >     > acl 307_redirect http_status 307
> >     > reply_header_add Cache-Control "no-store" 307_redirect
> >     >
> >     > Any ideas on what could I be missing here?
> >
> >     Does that 307 response come from a server (including cache_peers) or
> is
> >     it generated by Squid itself?
> >
> >     Alex.
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191106/86bda64a/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov  6 16:47:12 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Nov 2019 11:47:12 -0500
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <CADcj3=497cy=0ZSHyWnOLBQvzeqv_xGG_zQrL4sJH1ODBfBodw@mail.gmail.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
 <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
 <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>
 <CADcj3=497cy=0ZSHyWnOLBQvzeqv_xGG_zQrL4sJH1ODBfBodw@mail.gmail.com>
Message-ID: <857709d1-b972-e50e-997d-51dc34610719@measurement-factory.com>

On 11/6/19 8:49 AM, Felipe Arturo Polanco wrote:
> I have this warning in the logs:
> 
> WARNING: 307_redirect ACL is used in context without an HTTP response.
> Assuming mismatch.
> Acl.cc(151) matches: checked: 307_redirect = 0
> 
> I also tested using?rep_header ACL and that causes the same warning and
> defaulting to 0.
> 
> Do I need anything else to make reply access lists to work?

What is your Squid version?

Alex.


> On Tue, Nov 5, 2019 at 6:01 PM Alex Rousskov wrote:
> 
>     On 11/5/19 4:23 PM, Felipe Arturo Polanco wrote:
>     > I tried 200 status code from the
>     > webserver directly and doesn't work either.
> 
>     Sounds like a Squid bug to me then. If you can reproduce with Squid v4
>     or later, please consider filing a bug report in Squid bugzilla. Quality
>     fixes welcomed.
> 
>     Alex.
> 
> 
>     > On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov wrote:
>     >
>     >? ? ?On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
>     >
>     >? ? ?> I have been trying to match http_status acl in my squid.conf
>     file
>     >? ? ?but it
>     >? ? ?> has no effect.
>     >? ? ?>
>     >? ? ?> My goal is to add a given header to specific HTTP return?codes.
>     >? ? ?>
>     >? ? ?> eg:
>     >? ? ?> This works:
>     >? ? ?> acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
>     <http://192.168.0.6/32>
>     >? ? ?<http://192.168.0.6/32>
>     >? ? ?> reply_header_add Cache-Control "no-store" user1
>     >? ? ?>
>     >? ? ?> This doesn't work:
>     >? ? ?> acl 307_redirect http_status 307
>     >? ? ?> reply_header_add Cache-Control "no-store" 307_redirect
>     >? ? ?>
>     >? ? ?> Any ideas on what could I be missing here?
>     >
>     >? ? ?Does that 307 response come from a server (including
>     cache_peers) or is
>     >? ? ?it generated by Squid itself?
>     >
>     >? ? ?Alex.
>     >? ? ?_______________________________________________
>     >? ? ?squid-users mailing list
>     >? ? ?squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>     >? ? ?http://lists.squid-cache.org/listinfo/squid-users
>     >
> 



From felipeapolanco at gmail.com  Wed Nov  6 18:03:37 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Wed, 6 Nov 2019 14:03:37 -0400
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <857709d1-b972-e50e-997d-51dc34610719@measurement-factory.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
 <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
 <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>
 <CADcj3=497cy=0ZSHyWnOLBQvzeqv_xGG_zQrL4sJH1ODBfBodw@mail.gmail.com>
 <857709d1-b972-e50e-997d-51dc34610719@measurement-factory.com>
Message-ID: <CADcj3=7Z7607rMoFwP5MXmsLAUmKqQw9kRBTRp8LRUv-aReTbQ@mail.gmail.com>

4.7 from this branch:
https://github.com/measurement-factory/squid/tree/SQUID-323-WebSocket-support


On Wed, Nov 6, 2019 at 12:47 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/6/19 8:49 AM, Felipe Arturo Polanco wrote:
> > I have this warning in the logs:
> >
> > WARNING: 307_redirect ACL is used in context without an HTTP response.
> > Assuming mismatch.
> > Acl.cc(151) matches: checked: 307_redirect = 0
> >
> > I also tested using rep_header ACL and that causes the same warning and
> > defaulting to 0.
> >
> > Do I need anything else to make reply access lists to work?
>
> What is your Squid version?
>
> Alex.
>
>
> > On Tue, Nov 5, 2019 at 6:01 PM Alex Rousskov wrote:
> >
> >     On 11/5/19 4:23 PM, Felipe Arturo Polanco wrote:
> >     > I tried 200 status code from the
> >     > webserver directly and doesn't work either.
> >
> >     Sounds like a Squid bug to me then. If you can reproduce with Squid
> v4
> >     or later, please consider filing a bug report in Squid bugzilla.
> Quality
> >     fixes welcomed.
> >
> >     Alex.
> >
> >
> >     > On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov wrote:
> >     >
> >     >     On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
> >     >
> >     >     > I have been trying to match http_status acl in my squid.conf
> >     file
> >     >     but it
> >     >     > has no effect.
> >     >     >
> >     >     > My goal is to add a given header to specific HTTP
> return codes.
> >     >     >
> >     >     > eg:
> >     >     > This works:
> >     >     > acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
> >     <http://192.168.0.6/32>
> >     >     <http://192.168.0.6/32>
> >     >     > reply_header_add Cache-Control "no-store" user1
> >     >     >
> >     >     > This doesn't work:
> >     >     > acl 307_redirect http_status 307
> >     >     > reply_header_add Cache-Control "no-store" 307_redirect
> >     >     >
> >     >     > Any ideas on what could I be missing here?
> >     >
> >     >     Does that 307 response come from a server (including
> >     cache_peers) or is
> >     >     it generated by Squid itself?
> >     >
> >     >     Alex.
> >     >     _______________________________________________
> >     >     squid-users mailing list
> >     >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>
> >     >     http://lists.squid-cache.org/listinfo/squid-users
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191106/fe3baf65/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov  6 18:24:08 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Nov 2019 13:24:08 -0500
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <CADcj3=7Z7607rMoFwP5MXmsLAUmKqQw9kRBTRp8LRUv-aReTbQ@mail.gmail.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
 <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
 <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>
 <CADcj3=497cy=0ZSHyWnOLBQvzeqv_xGG_zQrL4sJH1ODBfBodw@mail.gmail.com>
 <857709d1-b972-e50e-997d-51dc34610719@measurement-factory.com>
 <CADcj3=7Z7607rMoFwP5MXmsLAUmKqQw9kRBTRp8LRUv-aReTbQ@mail.gmail.com>
Message-ID: <02a01c3a-eab1-a811-d87a-b76c318ed5a4@measurement-factory.com>

On 11/6/19 1:03 PM, Felipe Arturo Polanco wrote:
> 4.7 from this branch:
> https://github.com/measurement-factory/squid/tree/SQUID-323-WebSocket-support??

It looks like you are hitting a bug that has not been fixed yet:
reply_header_add/httpHdrAdd() does not supply essential transaction info
to ACL checks. There has been significant progress in fixing similar
bugs recently, but this one was somehow missed AFAICT.

Alex.


> On Wed, Nov 6, 2019 at 12:47 PM Alex Rousskov wrote:
> 
>     On 11/6/19 8:49 AM, Felipe Arturo Polanco wrote:
>     > I have this warning in the logs:
>     >
>     > WARNING: 307_redirect ACL is used in context without an HTTP response.
>     > Assuming mismatch.
>     > Acl.cc(151) matches: checked: 307_redirect = 0
>     >
>     > I also tested using?rep_header ACL and that causes the same
>     warning and
>     > defaulting to 0.
>     >
>     > Do I need anything else to make reply access lists to work?
> 
>     What is your Squid version?
> 
>     Alex.
> 
> 
>     > On Tue, Nov 5, 2019 at 6:01 PM Alex Rousskov wrote:
>     >
>     >? ? ?On 11/5/19 4:23 PM, Felipe Arturo Polanco wrote:
>     >? ? ?> I tried 200 status code from the
>     >? ? ?> webserver directly and doesn't work either.
>     >
>     >? ? ?Sounds like a Squid bug to me then. If you can reproduce with
>     Squid v4
>     >? ? ?or later, please consider filing a bug report in Squid
>     bugzilla. Quality
>     >? ? ?fixes welcomed.
>     >
>     >? ? ?Alex.
>     >
>     >
>     >? ? ?> On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov wrote:
>     >? ? ?>
>     >? ? ?>? ? ?On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
>     >? ? ?>
>     >? ? ?>? ? ?> I have been trying to match http_status acl in my
>     squid.conf
>     >? ? ?file
>     >? ? ?>? ? ?but it
>     >? ? ?>? ? ?> has no effect.
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?> My goal is to add a given header to specific HTTP
>     return?codes.
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?> eg:
>     >? ? ?>? ? ?> This works:
>     >? ? ?>? ? ?> acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
>     <http://192.168.0.6/32>
>     >? ? ?<http://192.168.0.6/32>
>     >? ? ?>? ? ?<http://192.168.0.6/32>
>     >? ? ?>? ? ?> reply_header_add Cache-Control "no-store" user1
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?> This doesn't work:
>     >? ? ?>? ? ?> acl 307_redirect http_status 307
>     >? ? ?>? ? ?> reply_header_add Cache-Control "no-store" 307_redirect
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?> Any ideas on what could I be missing here?
>     >? ? ?>
>     >? ? ?>? ? ?Does that 307 response come from a server (including
>     >? ? ?cache_peers) or is
>     >? ? ?>? ? ?it generated by Squid itself?
>     >? ? ?>
>     >? ? ?>? ? ?Alex.
>     >? ? ?>? ? ?_______________________________________________
>     >? ? ?>? ? ?squid-users mailing list
>     >? ? ?>? ? ?squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>     >? ? ?>? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>>
>     >? ? ?>? ? ?http://lists.squid-cache.org/listinfo/squid-users
>     >? ? ?>
>     >
> 



From felipeapolanco at gmail.com  Wed Nov  6 18:33:48 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Wed, 6 Nov 2019 14:33:48 -0400
Subject: [squid-users] How to use http_status acl?
In-Reply-To: <02a01c3a-eab1-a811-d87a-b76c318ed5a4@measurement-factory.com>
References: <CADcj3=4JADGDuuLVXF+Qp2HS6hqtbZ-D9wrz=At2BAapAF9zbw@mail.gmail.com>
 <d8581c52-99c6-e17b-cf09-b0913f00227d@measurement-factory.com>
 <CADcj3=53dMFXQu+HeY6ZhY1OtDihVsTPV2jfr+P0tn_-4UpACg@mail.gmail.com>
 <fae922d5-2df9-4cfd-34f1-9d94fa0777ea@measurement-factory.com>
 <CADcj3=497cy=0ZSHyWnOLBQvzeqv_xGG_zQrL4sJH1ODBfBodw@mail.gmail.com>
 <857709d1-b972-e50e-997d-51dc34610719@measurement-factory.com>
 <CADcj3=7Z7607rMoFwP5MXmsLAUmKqQw9kRBTRp8LRUv-aReTbQ@mail.gmail.com>
 <02a01c3a-eab1-a811-d87a-b76c318ed5a4@measurement-factory.com>
Message-ID: <CADcj3=5bAxKJZYczZ4+njN2E28ZhP_X25S8kXkY2P5GYfnihzg@mail.gmail.com>

Thanks for the information Alex, we will handle this at ICAP level then.

Regards,

On Wed, Nov 6, 2019 at 2:24 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/6/19 1:03 PM, Felipe Arturo Polanco wrote:
> > 4.7 from this branch:
> >
> https://github.com/measurement-factory/squid/tree/SQUID-323-WebSocket-support
>
>
> It looks like you are hitting a bug that has not been fixed yet:
> reply_header_add/httpHdrAdd() does not supply essential transaction info
> to ACL checks. There has been significant progress in fixing similar
> bugs recently, but this one was somehow missed AFAICT.
>
> Alex.
>
>
> > On Wed, Nov 6, 2019 at 12:47 PM Alex Rousskov wrote:
> >
> >     On 11/6/19 8:49 AM, Felipe Arturo Polanco wrote:
> >     > I have this warning in the logs:
> >     >
> >     > WARNING: 307_redirect ACL is used in context without an HTTP
> response.
> >     > Assuming mismatch.
> >     > Acl.cc(151) matches: checked: 307_redirect = 0
> >     >
> >     > I also tested using rep_header ACL and that causes the same
> >     warning and
> >     > defaulting to 0.
> >     >
> >     > Do I need anything else to make reply access lists to work?
> >
> >     What is your Squid version?
> >
> >     Alex.
> >
> >
> >     > On Tue, Nov 5, 2019 at 6:01 PM Alex Rousskov wrote:
> >     >
> >     >     On 11/5/19 4:23 PM, Felipe Arturo Polanco wrote:
> >     >     > I tried 200 status code from the
> >     >     > webserver directly and doesn't work either.
> >     >
> >     >     Sounds like a Squid bug to me then. If you can reproduce with
> >     Squid v4
> >     >     or later, please consider filing a bug report in Squid
> >     bugzilla. Quality
> >     >     fixes welcomed.
> >     >
> >     >     Alex.
> >     >
> >     >
> >     >     > On Tue, Nov 5, 2019 at 4:43 PM Alex Rousskov wrote:
> >     >     >
> >     >     >     On 11/5/19 3:06 PM, Felipe Arturo Polanco wrote:
> >     >     >
> >     >     >     > I have been trying to match http_status acl in my
> >     squid.conf
> >     >     file
> >     >     >     but it
> >     >     >     > has no effect.
> >     >     >     >
> >     >     >     > My goal is to add a given header to specific HTTP
> >     return codes.
> >     >     >     >
> >     >     >     > eg:
> >     >     >     > This works:
> >     >     >     > acl user1 src 192.168.0.6/32 <http://192.168.0.6/32>
> >     <http://192.168.0.6/32>
> >     >     <http://192.168.0.6/32>
> >     >     >     <http://192.168.0.6/32>
> >     >     >     > reply_header_add Cache-Control "no-store" user1
> >     >     >     >
> >     >     >     > This doesn't work:
> >     >     >     > acl 307_redirect http_status 307
> >     >     >     > reply_header_add Cache-Control "no-store" 307_redirect
> >     >     >     >
> >     >     >     > Any ideas on what could I be missing here?
> >     >     >
> >     >     >     Does that 307 response come from a server (including
> >     >     cache_peers) or is
> >     >     >     it generated by Squid itself?
> >     >     >
> >     >     >     Alex.
> >     >     >     _______________________________________________
> >     >     >     squid-users mailing list
> >     >     >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>
> >     >     >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>>
> >     >     >     http://lists.squid-cache.org/listinfo/squid-users
> >     >     >
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191106/d2a68380/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov  8 14:48:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 03:48:47 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:8 Multiple
	issues in URI	processing
Message-ID: <60954f6b-228d-d7a2-3939-5d65af9ba782@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:8
__________________________________________________________________

Advisory ID:        SQUID-2019:8
Date:               November 05, 2019
Summary:            Multiple issues in URI processing.
Affected versions:  Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.8
Fixed in version:   Squid 4.9
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_8.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12523
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18676
__________________________________________________________________

Problem Description:

 Due to improper input validation Squid is vulnerable to security
 bypass attacks. Attacker can gain access to restricted HTTP
 servers.

 Due to incorrect input validation Squid is vulnerable to a buffer
 overflow which can result in Denial of Service to all clients
 using the proxy.

__________________________________________________________________

Severity:

 Any remote client may access resources which should be restricted
 and not available to them. Such as those protected behind client
 IP ACLs. Attacker could also gain access to manager services when
 Via header is turned off.

 Any remote client can perform a Denial of Service on all other
 clients using the proxy.

__________________________________________________________________

Updated Packages:

 These bugs are fixed by Squid version 4.9.

 In addition, a patch addressing this problem for stable releases
 can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-fbbdf75efd7a5cc244b4886a9d42ea458c5a3a73.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

Use the command 'squid -v' to view version and build details of
your proxy;

 All Squid 2.x have not been checked.

 All Squid-3.x up to and including 3.5.28 are vulnerable.

 All Squid-4.x up to and including 4.8 are vulnerable.

__________________________________________________________________

Workaround:

 Access to manager services can be prevented by enabling the Via
 header:
   via on

 There are no reliable workarounds to prevent access to restricted
 upstream servers.

 There are no workarounds for the Denial of Service issue.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the squid-users at squid-cache.org mailing list is your
 primary support point. For subscription details see
 http://www.squid-cache.org/Support/mailing-lists.html.

 For reporting of non-security bugs in the latest release
 the squid bugzilla database should be used
 http://bugs.squid-cache.org/.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at squid-cache.org mailing list. It is a closed list
 (though anyone can post) and security related bug reports are
 treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 The security bypass vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 The Denial of Service vulnerability was discovered by Kristoffer
 Danielsson.

 Fixed by Amos Jeffries, Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-06-05 15:52:17 UTC CVE-2019-12523 Assignment
 2019-07-03 01:07:41 UTC Additional Report
 2019-11-04 13:43:22 UTC CVE-2019-18676 Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Nov  8 14:54:41 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 03:54:41 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:9 Cross-Site
 Request Forgery issue in HTTP Request processing
Message-ID: <b1919f7f-9adf-93bf-35aa-99752c2c3102@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:9
__________________________________________________________________

Advisory ID:        SQUID-2019:9
Date:               November 05, 2019
Summary:            Cross-Site Request Forgery issue
                    in HTTP Request processing.
Affected versions:  Squid 2.x -> 2.7.STABLE9
                    Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.8
Fixed in version:   Squid 4.9
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_9.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18677
__________________________________________________________________

Problem Description:

 Due to incorrect message processing Squid configured with
 append_domain can inappropriately redirect traffic to origins it
 should not be delivered to.

__________________________________________________________________

Severity:

 This issue allows attackers to hide origin servers for phishing
 attacks or malware download URLs.

 This issue is restricted to proxies with append_domain
 configured. It is relatively easy for attackers to probe and
 determine whether a target network proxy has this directive
 along with its value.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.9.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 3.5:
 <http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-e5f1813a674848dde570f7920873e1071f96e0b4.patch>

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-36492033ea4097821a4f7ff3ddcb971fbd1e8ba0.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid without append_domain configured are not vulnerable.

 All Squid-2.x up to and including 2.7.STABLE9 with append_domain
 configured are vulnerable.

 All Squid-3.x up to and including 3.5.28 with append_domain
 configured are vulnerable.

 All Squid-4.x up to and including 4.8 with append_domain
 configured are vulnerable.


To determine whether append_domain is configured use the command:

 squid -k parse | grep append_domain

__________________________________________________________________

Workarounds:

 Remove append_domain configuration settings from squid.conf.

 The append_domain feature is redundant when /etc/resolv.conf
 is used to determine hostnames. However, please note that use
 of /etc/resolv.conf may require removal of dns_nameservers and
 other redundant DNS directives.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Kristoffer Danielsson.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-06-26 21:43:49 UTC Initial Report
 2019-07-12 03:08:00 UTC Patches Released
 2019-11-04 13:43:22 UTC CVE-2019-18677 Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Nov  8 14:47:31 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 03:47:31 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:7 Heap
	Overflow issue in URN	processing
Message-ID: <8852d5c4-caa6-f265-3fd0-2a008a6cfcc2@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:7
__________________________________________________________________

Advisory ID:        SQUID-2019:7
Date:               November 5, 2019
Summary:            Heap Overflow issue
                    in URN processing.
Affected versions:  Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.8
Fixed in version:   Squid 4.9
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_7.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12526
__________________________________________________________________

Problem Description:

 Due to incorrect buffer management Squid is vulnerable to a
 heap overflow and possible remote code execution attack when
 processing URN.

__________________________________________________________________

Severity:

 This allows a malicious client to write a substantial amount of
 arbitrary data to the heap. Potentially gaining ability to
 execute arbitrary code.

 On systems with memory access protections this can result in
 the Squid process being terminated unexpectedly. Resulting in a
 denial of service for all clients using the proxy.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.9.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-7aa0184a720fd216191474e079f4fe87de7c4f5a.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x are not vulnerable.

 All Squid-3.x up to and including 3.5.28 are vulnerable.

 All Squid-4.x up to and including 4.8 are vulnerable.

__________________________________________________________________

Workarounds:

 Deny urn: protocol URI being proxied to all clients:

    acl URN proto URN
    http_access deny URN

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 Fixed by Eduard Bagdasaryan of The Measurement Factory.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-06-05 15:52:17 UTC CVE Assignment
 2019-09-15 15:32:30 UTC Patches Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Nov  8 14:47:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 03:47:10 +1300
Subject: [squid-users] [squid-announce] Squid 4.9 is available
Message-ID: <45332302-88fd-0747-a9c1-c21100c76e15@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.9 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:


 * SQUID-2019:6 Multiple Cross-Site Scripting issues in cachemgr.cgi
   (CVE-2019-13345)

The previous fix for this issues turned out to be incomplete. An
additional parameter has been identified as containing the same set of
XSS issues.

See the advisory for updated patches:
 <http://www.squid-cache.org/Advisories/SQUID-2019_6.txt>


Please note that cachemgr.cgi tool is deprecated. All users of this tool
are advised to plan migration to the HTTP manager API provided by
current Squid proxies.


 * SQUID-2019:7 Heap Overflow in URN processing
   (CVE-2019-12526)

This allows a malicious client to write a substantial amount of
arbitrary data to the heap. Potentially gaining ability to
execute arbitrary code.

On systems with memory access protections this can result in
the Squid process being terminated unexpectedly. Resulting in a
denial of service for all clients using the proxy.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_7.txt>


 * SQUID-2019:8 Multiple Issues in URI processing
   (CVE-2019-12523, CVE-2019-18676)

Any remote client may access resources which should be restricted
and not available to them. Such as those protected behind client
IP ACLs. Attacker could also gain access to manager services when
Via header is turned off.

Any remote client can perform a Denial of Service on all other
clients using the proxy.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_8.txt>


 * SQUID-2019:9 Cross-Site Request Forgery in HTTP Request processing
   (CVE-2019-18677)

This issue allows attackers to hide origin servers for phishing
attacks or malware download URLs.

This issue is restricted to proxies with append_domain
configured. It is relatively easy for attackers to probe and
determine whether a target network proxy has this directive
along with its value.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_9.txt>


 * SQUID-2019:10 HTTP Request Splitting in HTTP message processing
   (CVE-2019-18678)

This issue allows attackers to smuggle HTTP requests through
frontend software to a Squid which splits the HTTP Request
pipeline differently. The resulting Response messages corrupt
caches between client and Squid with attacker controlled content
at arbitrary URLs..

Effects are isolated to software between the attacker client and
Squid. There are no effects on Squid itself, nor any upstream
servers.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_10.txt>


 * SQUID-2019:11 Information Disclosure in HTTP Digest Authentication
   (CVE-2019-18679)

Nonce tokens contain the raw byte value of a pointer which sits
within heap memory allocation. This information reduces ASLR
protections and may aid attackers isolating memory areas to
target for remote code execution attacks.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2019_11.txt>


 * Bug 4966: Lower cache_peer hostname

This shows up as a DNS failure to resolve the peer name if it was
configured with any upper case characters.

The change to always lower-case peer names may affect configurations
relying on mixed case instead of the name= parameter to allow multiple
entries for a peer name and port.

It may also affect configurations using mixed or upper-case peer names
with the peername or peername_regex ACL type. Admin using these
configurations should take extra care when upgrading as the ACL may not
provide any warnings before starting to non-match for a peer.


 * TLS: Multiple SSL-Bump fixes

This release brings multiple important fixes to how Squid SSL-Bump
features parse TLS traffic and interacts with the certificate validation
helper(s).

The issues solved show up as TLS protocol failures with no indication
from TLS traffic trace of any invalid data; or sometimes connection
timeouts. Unfortunately those same effects may come from many other
causes as well which may not be fixed yet.

This version of Squid should now be considered the minimum supported for
debugging TLS protocol weirdness when using SSL-Bump or related features.


 * TLS: Fix expiration of self-signed generated certs to be 3 years

The certificate generator previously was generating certificates
slightly short of 3 years expiry timestamp. This is perfectly valid, but
may be surprising for systems expecting a multiple of years.

This release generates new certificates with the updated time period.
Old certificates will continue to be used with the old period until they
expire, or are discarded from the certificate cache.


 * TLS: Fix on_unsupported_protocol tunnel action

Instead of tunneling traffic, a matching on_unsupported_protocol
"tunnel" action resulted in a Squid error response sent to the client
(or, where an error response was not possible, in a connection closure).


 * Fix several rock cache_dir corruption issues

Previous design of the rock storage system means that rock caches may
become littered with incomplete objects, or objects with incorrect final
chunk. Data protection measures will normally catch these and report
metadata mismatches. However there is a possibility some responses may
be delivered.

It is recommended that users with cache_dir rock configured perform a
cache erase and rebuild procedure during or shortly after upgrading.
 <https://wiki.squid-cache.org/SquidFaq/ClearingTheCache>



  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries




_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Nov  8 14:56:57 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 03:56:57 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:10 HTTP
 Request Splitting issue in HTTP message processing
Message-ID: <8d9b16ec-cec9-57dc-7f95-971dc6fc3888@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:10
__________________________________________________________________

Advisory ID:        SQUID-2019:10
Date:               November 05, 2019
Summary:            HTTP Request Splitting issue
                    in HTTP message processing.
Affected versions:  Squid 3.0 -> 3.5.28
                    Squid 4.x -> 4.8
Fixed in version:   Squid 4.9
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_10.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18678
__________________________________________________________________

Problem Description:

 Due to incorrect message parsing Squid is vulnerable to an HTTP
 request splitting issue.

__________________________________________________________________

Severity:

 This issue allows attackers to smuggle HTTP requests through
 frontend software to a Squid which splits the HTTP Request
 pipeline differently. The resulting Response messages corrupt
 caches between client and Squid with attacker controlled content
 at arbitrary URLs..

 Effects are isolated to software between the attacker client and
 Squid. There are no effects on Squid itself, nor any upstream
 servers.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.9.

 In addition, a patch addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-671ba97abe929156dc4c717ee52ad22fba0f7443.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x have not been checked.

 All Squid-3.x up to and including 3.5.28 are vulnerable.

 All Squid-4.x up to and including 4.8 are vulnerable.

__________________________________________________________________

Workarounds:

 There are no workarounds for this vulnerability.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by by R??gis Leroy (regilero
 from Makina Corpus).

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-07-24 11:52:51 UTC Initial Report
 2019-09-11 02:52:52 UTC Patches Released
 2019-11-04 13:43:22 UTC CVE Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Nov  8 15:01:46 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 04:01:46 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:11 Information
 Disclosure issue in HTTP Digest Authentication
Message-ID: <6957cd43-ae02-b641-78e4-7130ee95e4b9@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:11
__________________________________________________________________

Advisory ID:        SQUID-2019:11
Date:               November 05, 2019
Summary:            Information Disclosure issue
                    in HTTP Digest Authentication.
Affected versions:  Squid 2.x -> 2.7.STABLE9
                    Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.8
Fixed in version:   Squid 4.9
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_11.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18679
__________________________________________________________________

Problem Description:

 Due to incorrect data management Squid is vulnerable to a
 information disclosure when processing HTTP Digest
 Authentication.

__________________________________________________________________

Severity:

 Nonce tokens contain the raw byte value of a pointer which sits
 within heap memory allocation. This information reduces ASLR
 protections and may aid attackers isolating memory areas to
 target for remote code execution attacks.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.9.

 In addition, a patch addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-671ba97abe929156dc4c717ee52ad22fba0f7443.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x up to and including 2.7.STABLE9 are vulnerable.

 All Squid-3.x up to and including 3.5.28 are vulnerable.

 All Squid-4.x up to and including 4.8 are vulnerable.

__________________________________________________________________

Workarounds:

Either;

 Remove 'auth_param digest ...' configuration settings from
 squid.conf.

Or,

 Build Squid with --disable-auth-digest

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered and fixed by David Fifield.

__________________________________________________________________

Revision history:

 2019-08-05 06:15:36 UTC Initial Report
 2019-10-20 18:59:08 UTC Patches Released
 2019-11-04 13:43:22 UTC CVE Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From darren at ksn-systems.com  Fri Nov  8 17:53:41 2019
From: darren at ksn-systems.com (Darren Breeze)
Date: Sat, 09 Nov 2019 06:53:41 +1300
Subject: [squid-users] Header forgery detected
Message-ID: <7311932e-2438-4733-a505-06b1cca19b93@www.fastmail.com>


Hi All

I am trying to set up squid 3.5 (have to stick with this version) to intercept and https bump / splice, it's all working OK with the exception of some elements of a https site failing to load (the browser just shows "failed"). matched with the failures, I see this type of message in the cache log. 

2019/11/08 17:39:46 kid1| SECURITY ALERT: Host header forgery detected on local=23.213.186.14:443 remote=172.16.3.250:57041 FD 28 flags=33 (local IP does not match any domain IP)
2019/11/08 17:39:46 kid1| SECURITY ALERT: on URL: static1.squarespace.com:443

172.16.3.250 is the clients PC address. 

doing a lookup on the hostname returns

root at cbuild:~/build/ksn-boot/cmake-build-debug/bin# nslookup
> server 127.0.0.1
Default server: 127.0.0.1
Address: 127.0.0.1#53
> static1.squarespace.com
Server: 127.0.0.1
Address: 127.0.0.1#53

Non-authoritative answer:
static1.squarespace.com canonical name = prod.squarespace.map.fastly.net.
Name: prod.squarespace.map.fastly.net
Address: 151.101.0.238
Name: prod.squarespace.map.fastly.net
Address: 151.101.64.238
Name: prod.squarespace.map.fastly.net
Address: 151.101.128.238
Name: prod.squarespace.map.fastly.net
Address: 151.101.192.238

so the address is different and points to a CDN endpoint 

14.186.213.23.in-addr.arpa name = a23-213-186-14.deploy.static.akamaitechnologies.com.


The host is ubuntu 18.04 and both squid and the client are using the DNS on the squid box. 

Can anyone please point me where I need to start looking

thanks in advance

Darren B.


This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the system manager. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. If you are not the intended recipient you are notified that disclosing, copying, distributing or taking any action in reliance on the contents of this information is strictly prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191109/664b4d01/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov  9 00:39:23 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Nov 2019 13:39:23 +1300
Subject: [squid-users] Header forgery detected
In-Reply-To: <7311932e-2438-4733-a505-06b1cca19b93@www.fastmail.com>
References: <7311932e-2438-4733-a505-06b1cca19b93@www.fastmail.com>
Message-ID: <7f1cb2c7-5303-fe2a-5757-70003eaf0fa5@treenet.co.nz>

On 9/11/19 6:53 am, Darren Breeze wrote:
> 
> 
> The host is ubuntu 18.04 and both squid and the client are using the DNS
> on the squid box. 

Well, apparently not. Because the client thinks the domain is hosted
somewhere (Akamai) contrary to what the DNS records say (Fastly).

> 
> Can anyone please point me where I need to start looking
> 

Next step for me would be to start at the client end of things. Very
carefully to see where it is actually getting the IP address for that
domain from.


Amos


From uhlar at fantomas.sk  Sat Nov  9 07:00:34 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 9 Nov 2019 08:00:34 +0100
Subject: [squid-users] Header forgery detected
In-Reply-To: <7311932e-2438-4733-a505-06b1cca19b93@www.fastmail.com>
References: <7311932e-2438-4733-a505-06b1cca19b93@www.fastmail.com>
Message-ID: <20191109070033.GA8641@fantomas.sk>

On 09.11.19 06:53, Darren Breeze wrote:
>I am trying to set up squid 3.5 (have to stick with this version)
why?

> to intercept and https bump / splice,
squid 3 has problems with bumping/splicing that are fixed in squid4...

> it's all working OK with the exception of some elements of a https site failing to load (the browser just shows "failed"). matched with the failures, I see this type of message in the cache log.
>
>2019/11/08 17:39:46 kid1| SECURITY ALERT: Host header forgery detected on local=23.213.186.14:443 remote=172.16.3.250:57041 FD 28 flags=33 (local IP does not match any domain IP)

seems you are trying to intercept by doing DNAT on remote machine, which
causes this problem.

https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery

you must use ip policy routing or WCCP when interceptin outside of squid
machine:

https://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Requirements_and_methods_for_Interception_Caching

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"One World. One Web. One Program." - Microsoft promotional advertisement
"Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler


From uhlar at fantomas.sk  Sat Nov  9 12:26:38 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 9 Nov 2019 13:26:38 +0100
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <b9d7e95d-0f72-655c-5ded-ff8f67783dd6@measurement-factory.com>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
 <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>
 <20191104162121.GA14172@fantomas.sk>
 <b2e38136-dda2-239b-3a5b-3b518917bdde@measurement-factory.com>
 <20191104175903.GA16466@fantomas.sk>
 <20191105093413.GB5031@fantomas.sk>
 <b9d7e95d-0f72-655c-5ded-ff8f67783dd6@measurement-factory.com>
Message-ID: <20191109122638.GA15347@fantomas.sk>

>On 11/5/19 4:34 AM, Matus UHLAR - fantomas wrote:
>> Now I have full backtrace, hopefully:

On 05.11.19 09:10, Alex Rousskov wrote:
>You are making progress, but the backtrace itself is not sufficient. One
>has to examine the core dump to find out transaction details (e.g., the
>URL) _and_ log enough of those details into cache.log (usually at levels
>much higher than ALL,2) to restore Squid state and figure out what is
>going on or build a reliable reproducer. Sorry if I was not clear about
>that from the start.
>
>It does take time and some expertise to figure out what is going on.
>Unfortunately, I cannot personally guide you through this painful
>process right now.

I understand.

Luckily, the problem turned out to be because of squid bug #4823 reported in
debian bug #943692 (when searching month ago, debian bug didn't exist and I
haven't found squid bug that time).

hopefully the mentioed patch is safe in squid 4.6 (I have applied it and it
seems so).

hopefully we can close this thread now.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I wonder how much deeper the ocean would be without sponges.


From darren at ksn-systems.com  Sat Nov  9 19:40:03 2019
From: darren at ksn-systems.com (Darren Breeze)
Date: Sun, 10 Nov 2019 08:40:03 +1300
Subject: [squid-users] Header forgery detected
In-Reply-To: <7f1cb2c7-5303-fe2a-5757-70003eaf0fa5@treenet.co.nz>
References: <7311932e-2438-4733-a505-06b1cca19b93@www.fastmail.com>
 <7f1cb2c7-5303-fe2a-5757-70003eaf0fa5@treenet.co.nz>
Message-ID: <17a67fe4-a313-4ac1-821e-635638b574a7@www.fastmail.com>


Thank You Amos

There was a second DHCP service on that network handing out conflicting DNS data. Once that was stopped, everything worked as expected.



Darren B.

This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the system manager. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. If you are not the intended recipient you are notified that disclosing, copying, distributing or taking any action in reliance on the contents of this information is strictly prohibited.



From ahmed.zaeem at netstream.ps  Sun Nov 10 10:07:17 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 10 Nov 2019 13:07:17 +0300
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
Message-ID: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>

Hello Folks ,

i have a severs who run java and we need to access it from IDRAC console .

squid is 4.8 not able to get it work .
always i have error of java prompt , Unable to launch application .

if i use without proxy it work , if i use with squid it don?t work .

tried to add the directive below :

#####################
acl Java browser Java/1.4 Java/1.5 Java/1.6  Java/1.7  Java/1.8  Java/1.9
http_access allow Java
############################



Let me know Guys if there is a way to get it work or its not possible .

Thanks 

From suxikang1 at hisilicon.com  Sun Nov 10 12:13:09 2019
From: suxikang1 at hisilicon.com (hindsight1)
Date: Sun, 10 Nov 2019 06:13:09 -0600 (CST)
Subject: [squid-users] Squid crash - 3.5.21
In-Reply-To: <aff9b1ab-5233-5e28-af18-5b867dad89df@measurement-factory.com>
References: <1475491833.8486.41.camel@shoprite.co.za>
 <aff9b1ab-5233-5e28-af18-5b867dad89df@measurement-factory.com>
Message-ID: <1573387989765-0.post@n4.nabble.com>


Hi,Alex
When running Squid4.8 on the arm using SMP. And setting 4 or 7,9  worker
,the same error occurred in the log.
>> Received Bus Error...dying, 
Debugging the core file with gdb found that the address was not aligned. 

Does it consider the case where only address-aligned access is supported on
arm

The question is, why is the type of Item in the PageStack class using
uint32_t, can it be replaced with uint64_t or size_t? When I change to
uint64_t, the problem disappears.
I tried another way to ensure that the theCapacity  parameter passed in to
an even number also solves this problem.

Kind Regards,
hindsight
Alex Rousskov wrote
> On 10/03/2016 04:50 AM, Jasper Van Der Westhuizen wrote:
>> This morning I had some problems with some of our proxies. 2 Proxies in
>> cluster A crashed with the below errors. The shortly afterwards 4 in
>> cluster B did the same. Both clusters are configured to run their cache
>> in memory with SMP and 4 workers configured.
>> 
>> FATAL: Received Bus Error...dying.
> 
> 
> There are at least two possible reasons:
> 
>   1. A bug in Squid and
>   2. Memory overallocation by the OS kernel.
> 
> To fix the former, the developers will need a stack trace (at least). I
> recommend filing a bug report after getting that trace and excluding
> reason #2. Squid wiki and various system administration guides explain
> how to make Squid dump core files.
> 
> To check for memory overallocation, you can temporary start Squid v4.0
> with "shared_memory_locking on". Unfortunately, that squid.conf
> directive is not available in Squid v3. You may be able to emulate it
> using some OS-specific sysctl or environment variables, but doing so may
> be far from trivial, and I do not have instructions.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Sun Nov 10 19:55:07 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 10 Nov 2019 20:55:07 +0100
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
Message-ID: <20191110195506.GC20636@fantomas.sk>

On 10.11.19 13:07, --Ahmad-- wrote:
>i have a severs who run java and we need to access it from IDRAC console .
>
>squid is 4.8 not able to get it work .
>always i have error of java prompt , Unable to launch application .
>
>if i use without proxy it work , if i use with squid it don?t work .

do you mean, if you configure proxy in java?
(java has own proxy settings)

If so, you should check squid logs first so see what requests have been
denied to your client IP.

probably your idrac console port is not alowed in squid, shouldbe
listed in ssl_ports probably.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"Two words: Windows survives." - Craig Mundie, Microsoft senior strategist
"So does syphillis. Good thing we have penicillin." - Matthew Alton


From uhlar at fantomas.sk  Sun Nov 10 19:56:21 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 10 Nov 2019 20:56:21 +0100
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
Message-ID: <20191110195621.GD20636@fantomas.sk>

On 10.11.19 13:07, --Ahmad-- wrote:
tried to add the directive below :
>
>#####################
>acl Java browser Java/1.4 Java/1.5 Java/1.6  Java/1.7  Java/1.8  Java/1.9
>http_access allow Java
>############################

never do this, you will open your proxy to attacks where it could help
or you won't get any difference when it won't.

see my previous reply.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux is like a teepee: no Windows, no Gates and an apache inside...


From rousskov at measurement-factory.com  Mon Nov 11 16:19:16 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 11 Nov 2019 11:19:16 -0500
Subject: [squid-users] Squid crash - 3.5.21
In-Reply-To: <1573387989765-0.post@n4.nabble.com>
References: <1475491833.8486.41.camel@shoprite.co.za>
 <aff9b1ab-5233-5e28-af18-5b867dad89df@measurement-factory.com>
 <1573387989765-0.post@n4.nabble.com>
Message-ID: <31926e6a-d867-c8ad-4ad7-c1402867ab59@measurement-factory.com>

Context: hindsight1 was replying to a 2016 email archived at
http://lists.squid-cache.org/pipermail/squid-users/2016-October/012881.html


On 11/10/19 7:13 AM, hindsight1 wrote:
> When running Squid4.8 on the arm using SMP. And setting 4 or 7,9  worker
> ,the same error occurred in the log.

>>> Received Bus Error...dying, 

> Debugging the core file with gdb found that the address was not aligned. 
> 
> Does it consider the case where only address-aligned access is supported on
> arm

I am not sure whether "it" in your sentence refers to gdb or Squid, but
if Squid dereferences an unaligned data field in shared memory, then it
is most likely a Squid bug.


> The question is, why is the type of Item in the PageStack class using
> uint32_t,

Probably to avoid wasting space: Squid Store does not yet support caches
with more than 16777215 entries (a signed 25-bit integer) and SMP-aware
caches do not support storing responses with more pages than that limit.
Even though response size and the number of responses are conceptually
different limits, the underlying code/structures tie the two together IIRC.


> can it be replaced with uint64_t or size_t?

I have not checked whether any other adjustments would be required, but
I suspect that the answer is "yes". Needless to say, this replacement
will waste RAM.


> When I change to uint64_t, the problem disappears.

If there is an unaligned access bug, we should fix that bug directly
rather than relying on side effects of field sizes or, at least, we
should confirm and document that those side effects are the right fix.

To fix this properly, a stack trace would be very helpful. Can you post
that?


Alex.


> Alex Rousskov wrote
>> On 10/03/2016 04:50 AM, Jasper Van Der Westhuizen wrote:
>>> This morning I had some problems with some of our proxies. 2 Proxies in
>>> cluster A crashed with the below errors. The shortly afterwards 4 in
>>> cluster B did the same. Both clusters are configured to run their cache
>>> in memory with SMP and 4 workers configured.
>>>
>>> FATAL: Received Bus Error...dying.
>>
>>
>> There are at least two possible reasons:
>>
>>   1. A bug in Squid and
>>   2. Memory overallocation by the OS kernel.
>>
>> To fix the former, the developers will need a stack trace (at least). I
>> recommend filing a bug report after getting that trace and excluding
>> reason #2. Squid wiki and various system administration guides explain
>> how to make Squid dump core files.
>>
>> To check for memory overallocation, you can temporary start Squid v4.0
>> with "shared_memory_locking on". Unfortunately, that squid.conf
>> directive is not available in Squid v3. You may be able to emulate it
>> using some OS-specific sysctl or environment variables, but doing so may
>> be far from trivial, and I do not have instructions.
>>
>>
>> HTH,
>>
>> Alex.


From suxikang1 at hisilicon.com  Tue Nov 12 02:34:13 2019
From: suxikang1 at hisilicon.com (hindsight1)
Date: Mon, 11 Nov 2019 20:34:13 -0600 (CST)
Subject: [squid-users] Squid crash - 3.5.21
In-Reply-To: <31926e6a-d867-c8ad-4ad7-c1402867ab59@measurement-factory.com>
References: <1475491833.8486.41.camel@shoprite.co.za>
 <aff9b1ab-5233-5e28-af18-5b867dad89df@measurement-factory.com>
 <1573387989765-0.post@n4.nabble.com>
 <31926e6a-d867-c8ad-4ad7-c1402867ab59@measurement-factory.com>
Message-ID: <1573526053795-0.post@n4.nabble.com>

Hi,Alex

thank you for your reply
Sorry for my English?First of all?
> I am not sure whether "it" in your sentence refers to gdb or Squid, but
> if Squid dereferences an unaligned data field in shared memory, then it
> is most likely a Squid bug.

"it" is refers to Squid.

i want to Explain my problem again 


I run Squid4.8 using SMP mode on the Arm64 platform. When setting some
worker numbers, for example 4 or 7,9 the same error Received Bus
Error...dying appears in the log.
Using gdb debugging, I found an error when accessing theLevels variable in
the Ipc::Mem::PagePool::level function, due to the non-aligned address
access caused by the atomic operation load.
Here is stacktrace:

#0  0x0000ffff9c945228 in raise () from /lib64/libc.so.6
#1  0x0000ffff9c9468a0 in abort () from /lib64/libc.so.6
#2  0x00000000007c6238 in death (sig=7) at tools.cc:359
#3  <signal handler called>
#4  0x00000000007c50a4 in std::__atomic_base<unsigned long>::load
(this=0xfff9d9d40cec, __m=std::memory_order_seq_cst) at
/usr/include/c++/4.8.2/bits/atomic_base.h:496
#5  0x00000000007c4344 in std::__atomic_base<unsigned long>::operator
unsigned long (this=0xfff9d9d40cec) at
/usr/include/c++/4.8.2/bits/atomic_base.h:367
#6  0x0000000000937f2c in Ipc::Mem::PagePool::level (this=0xde9150,
purpose=0) at mem/PagePool.cc:46
#7  0x0000000000934ae8 in Ipc::Mem::PageLevel (purpose=0) at mem/Pages.cc:88
#8  0x00000000007c3db0 in Ipc::Mem::PagesAvailable (purpose=0) at
ipc/mem/Pages.h:51
#9  0x00000000009342a4 in Ipc::Mem::GetPage
(purpose=Ipc::Mem::PageId::cachePage, page=...) at mem/Pages.cc:36
#10 0x00000000007c241c in MemStore::reserveSapForWriting (this=0x10e3bb0,
page=...) at MemStore.cc:778
#11 0x00000000007c1c18 in MemStore::nextAppendableSlice (this=0x10e3bb0,
fileNo=513735, sliceOffset=@0x10e3bd8: -1) at MemStore.cc:731
#12 0x00000000007c145c in MemStore::copyToShm (this=0x10e3bb0, e=...) at
MemStore.cc:682
#13 0x00000000007c2cdc in MemStore::write (this=0x10e3bb0, e=...) at
MemStore.cc:856
#14 0x00000000009f9838 in Store::Controller::memoryOut (this=0xdd2e20,
e=..., preserveSwappable=true) at Controller.cc:550
#15 0x00000000007b50e8 in StoreEntry::swapOut (this=0x125e4e0) at
store_swapout.cc:175
#16 0x00000000007af4a4 in StoreEntry::invokeHandlers (this=0x125e4e0) at
store_client.cc:720
#17 0x00000000007a650c in StoreEntry::flush (this=0x125e4e0) at
store.cc:1674
#18 0x00000000007a6dcc in StoreEntry::startWriting (this=0x125e4e0) at
store.cc:1844
#19 0x000000000083b7e8 in Client::setFinalReply (this=0x1275b18,
rep=0x12b36c0) at Client.cc:164
#20 0x00000000008401bc in Client::adaptOrFinalizeReply (this=0x1275b18) at
Client.cc:974
#21 0x0000000000726344 in HttpStateData::processReply (this=0x1275b18) at
http.cc:1246
#22 0x0000000000725fec in HttpStateData::readReply (this=0x1275b18, io=...)
at http.cc:1223
#23 0x000000000072fc10 in CommCbMemFunT<HttpStateData,
CommIoCbParams>::doDial (this=0x1285f80) at CommCalls.h:205
#24 0x0000000000730070 in JobDialer<HttpStateData>::dial (this=0x1285f80,
call=...) at base/AsyncJobCalls.h:174
#25 0x000000000072f728 in AsyncCallT<CommCbMemFunT&lt;HttpStateData,
CommIoCbParams> >::fire (this=0x1285f50) at ../src/base/AsyncCall.h:145
#26 0x0000000000887728 in AsyncCall::make (this=0x1285f50) at
AsyncCall.cc:40
#27 0x0000000000888270 in AsyncCallQueue::fireNext (this=0xe11e80) at
AsyncCallQueue.cc:56
#28 0x0000000000888068 in AsyncCallQueue::fire (this=0xe11e80) at
AsyncCallQueue.cc:42
#29 0x00000000006f4948 in EventLoop::dispatchCalls (this=0xfffff17422e8) at
EventLoop.cc:144
#30 0x00000000006f485c in EventLoop::runOnce (this=0xfffff17422e8) at
EventLoop.cc:121
#31 0x00000000006f46a0 in EventLoop::run (this=0xfffff17422e8) at
EventLoop.cc:83
#32 0x000000000076090c in SquidMain (argc=4, argv=0xfffff1742638) at
main.cc:1709
#33 0x000000000075fe70 in SquidMainSafe (argc=4, argv=0xfffff1742638) at
main.cc:1417
#34 0x000000000075fe3c in main (argc=4, argv=0xfffff1742638) at main.cc:1405

When theLevels is initialized, the assigned value is a multiple of 4 and is
not aligned with the unsigned long.
Atomic operations on arm only support aligned address access.
This problem has not been encountered on the x86 platform.


In the previous reply, there was another question that was not answered.

> I tried another way to ensure that the theCapacity  parameter passed in to
> an even number also solves this problem.

Keep the item type uint32_t, modify the NoteMemoryNeeds method in the
IpcIoFile.cc file in NotePageNeed as follows

 Ipc:: Mem:: NotePageNeed(Ipc::Mem::PageId::ioPage,static_cast
<int>(itemsCount * 1.1)+ static_cast < Int>(itemsCount * 1.1)%2); 


This modification ensures that the value obtained by the theCapacity
parameter is even?which also solves this problem.


After your analysis, please evaluate whether this method is reasonable?thank
you.


Regards?
hindsight



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Nov 12 14:15:20 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Nov 2019 09:15:20 -0500
Subject: [squid-users] Squid crash - 3.5.21
In-Reply-To: <1573526053795-0.post@n4.nabble.com>
References: <1475491833.8486.41.camel@shoprite.co.za>
 <aff9b1ab-5233-5e28-af18-5b867dad89df@measurement-factory.com>
 <1573387989765-0.post@n4.nabble.com>
 <31926e6a-d867-c8ad-4ad7-c1402867ab59@measurement-factory.com>
 <1573526053795-0.post@n4.nabble.com>
Message-ID: <755aa523-6791-4043-c5cf-1646982e6f71@measurement-factory.com>

FTR: hindsight1 runs v4.8 despite the email subject saying "3.5.21".


On 11/11/19 9:34 PM, hindsight1 wrote:

> thank you for your reply

Thank you for detailing the problem. The best place to discuss these
low-level details is Squid Bugzilla. I suggest that you open a new bug
report there. If you want to continue here, then the primary remaining
question for me is _why_ theLevels array elements are misaligned in your
tests:

* theLevels[0]: The segments are allocated on page-boundaries so the
first level element (i.e. level[0]) should be properly aligned. I
believe your stack trace shows that this zero-offset access is misaligned.

* theLevels[n+1]: The levels array is declared using regular C++
constructs, without any casting, so subsequent elements should be
aligned properly if the first element is aligned properly.

So where does this misalignment originate from? Properly addressing this
bug probably requires answering this question.


Please note that there are GCC v4 bugs that might be relevant here:
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=62259
https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65147

What is your compiler? Does building with GCC v5.1 fix the problem?


BTW, there is a potentially useful alignas() workaround/trick shown at
https://stackoverflow.com/questions/26703297/alignment-of-atomic-variables



Thank you,

Alex.


> Sorry for my English?First of all?
>> I am not sure whether "it" in your sentence refers to gdb or Squid, but
>> if Squid dereferences an unaligned data field in shared memory, then it
>> is most likely a Squid bug.
> 
> "it" is refers to Squid.
> 
> i want to Explain my problem again 
> 
> 
> I run Squid4.8 using SMP mode on the Arm64 platform. When setting some
> worker numbers, for example 4 or 7,9 the same error Received Bus
> Error...dying appears in the log.
> Using gdb debugging, I found an error when accessing theLevels variable in
> the Ipc::Mem::PagePool::level function, due to the non-aligned address
> access caused by the atomic operation load.
> Here is stacktrace:
> 
> #0  0x0000ffff9c945228 in raise () from /lib64/libc.so.6
> #1  0x0000ffff9c9468a0 in abort () from /lib64/libc.so.6
> #2  0x00000000007c6238 in death (sig=7) at tools.cc:359
> #3  <signal handler called>
> #4  0x00000000007c50a4 in std::__atomic_base<unsigned long>::load
> (this=0xfff9d9d40cec, __m=std::memory_order_seq_cst) at
> /usr/include/c++/4.8.2/bits/atomic_base.h:496
> #5  0x00000000007c4344 in std::__atomic_base<unsigned long>::operator
> unsigned long (this=0xfff9d9d40cec) at
> /usr/include/c++/4.8.2/bits/atomic_base.h:367
> #6  0x0000000000937f2c in Ipc::Mem::PagePool::level (this=0xde9150,
> purpose=0) at mem/PagePool.cc:46
> #7  0x0000000000934ae8 in Ipc::Mem::PageLevel (purpose=0) at mem/Pages.cc:88
> #8  0x00000000007c3db0 in Ipc::Mem::PagesAvailable (purpose=0) at
> ipc/mem/Pages.h:51
> #9  0x00000000009342a4 in Ipc::Mem::GetPage
> (purpose=Ipc::Mem::PageId::cachePage, page=...) at mem/Pages.cc:36
> #10 0x00000000007c241c in MemStore::reserveSapForWriting (this=0x10e3bb0,
> page=...) at MemStore.cc:778
> #11 0x00000000007c1c18 in MemStore::nextAppendableSlice (this=0x10e3bb0,
> fileNo=513735, sliceOffset=@0x10e3bd8: -1) at MemStore.cc:731
> #12 0x00000000007c145c in MemStore::copyToShm (this=0x10e3bb0, e=...) at
> MemStore.cc:682
> #13 0x00000000007c2cdc in MemStore::write (this=0x10e3bb0, e=...) at
> MemStore.cc:856
> #14 0x00000000009f9838 in Store::Controller::memoryOut (this=0xdd2e20,
> e=..., preserveSwappable=true) at Controller.cc:550
> #15 0x00000000007b50e8 in StoreEntry::swapOut (this=0x125e4e0) at
> store_swapout.cc:175
> #16 0x00000000007af4a4 in StoreEntry::invokeHandlers (this=0x125e4e0) at
> store_client.cc:720
> #17 0x00000000007a650c in StoreEntry::flush (this=0x125e4e0) at
> store.cc:1674
> #18 0x00000000007a6dcc in StoreEntry::startWriting (this=0x125e4e0) at
> store.cc:1844
> #19 0x000000000083b7e8 in Client::setFinalReply (this=0x1275b18,
> rep=0x12b36c0) at Client.cc:164
> #20 0x00000000008401bc in Client::adaptOrFinalizeReply (this=0x1275b18) at
> Client.cc:974
> #21 0x0000000000726344 in HttpStateData::processReply (this=0x1275b18) at
> http.cc:1246
> #22 0x0000000000725fec in HttpStateData::readReply (this=0x1275b18, io=...)
> at http.cc:1223
> #23 0x000000000072fc10 in CommCbMemFunT<HttpStateData,
> CommIoCbParams>::doDial (this=0x1285f80) at CommCalls.h:205
> #24 0x0000000000730070 in JobDialer<HttpStateData>::dial (this=0x1285f80,
> call=...) at base/AsyncJobCalls.h:174
> #25 0x000000000072f728 in AsyncCallT<CommCbMemFunT&lt;HttpStateData,
> CommIoCbParams> >::fire (this=0x1285f50) at ../src/base/AsyncCall.h:145
> #26 0x0000000000887728 in AsyncCall::make (this=0x1285f50) at
> AsyncCall.cc:40
> #27 0x0000000000888270 in AsyncCallQueue::fireNext (this=0xe11e80) at
> AsyncCallQueue.cc:56
> #28 0x0000000000888068 in AsyncCallQueue::fire (this=0xe11e80) at
> AsyncCallQueue.cc:42
> #29 0x00000000006f4948 in EventLoop::dispatchCalls (this=0xfffff17422e8) at
> EventLoop.cc:144
> #30 0x00000000006f485c in EventLoop::runOnce (this=0xfffff17422e8) at
> EventLoop.cc:121
> #31 0x00000000006f46a0 in EventLoop::run (this=0xfffff17422e8) at
> EventLoop.cc:83
> #32 0x000000000076090c in SquidMain (argc=4, argv=0xfffff1742638) at
> main.cc:1709
> #33 0x000000000075fe70 in SquidMainSafe (argc=4, argv=0xfffff1742638) at
> main.cc:1417
> #34 0x000000000075fe3c in main (argc=4, argv=0xfffff1742638) at main.cc:1405
> 
> When theLevels is initialized, the assigned value is a multiple of 4 and is
> not aligned with the unsigned long.
> Atomic operations on arm only support aligned address access.
> This problem has not been encountered on the x86 platform.
> 
> 
> In the previous reply, there was another question that was not answered.
> 
>> I tried another way to ensure that the theCapacity  parameter passed in to
>> an even number also solves this problem.
> 
> Keep the item type uint32_t, modify the NoteMemoryNeeds method in the
> IpcIoFile.cc file in NotePageNeed as follows
> 
>  Ipc:: Mem:: NotePageNeed(Ipc::Mem::PageId::ioPage,static_cast
> <int>(itemsCount * 1.1)+ static_cast < Int>(itemsCount * 1.1)%2); 
> 
> 
> This modification ensures that the value obtained by the theCapacity
> parameter is even?which also solves this problem.
> 
> 
> After your analysis, please evaluate whether this method is reasonable?thank
> you.
> 
> 
> Regards?
> hindsight



From rafael.akchurin at diladele.com  Wed Nov 13 12:09:50 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 13 Nov 2019 12:09:50 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.9 (rebuilt with
 sslbump support from sources in Debian unstable)
In-Reply-To: <AM0PR04MB4753DC5BB0745F502B357E958F770@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <AM0PR04MB4753DC5BB0745F502B357E958F770@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <AM0PR04MB4753F609B5A2ED5DB78E89238F760@AM0PR04MB4753.eurprd04.prod.outlook.com>

Greeting all,

The online repository with latest Squid 4.9 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid49.diladele.com.
Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation.
Scripts for Ubuntu 16 are also available in that repo.

Hope you will find this helpful. Note that older repo of squid48.diladele.com will be taken down in 1 year.

Best regards,
Rafael Akchurin
Diladele B.V.

P.S. Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid49.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid49.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient



--
Please take a look at another our project - DNS Safety filtering server.  Sort of Web Safety implemented as DNS Server. Might be interesting in deployments where HTTPS decryption is not possible.
https://dnssafety.io/


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191113/d5b72098/attachment.htm>

From ahmed.zaeem at netstream.ps  Tue Nov 12 13:20:00 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 12 Nov 2019 16:20:00 +0300
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <20191110195506.GC20636@fantomas.sk>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
 <20191110195506.GC20636@fantomas.sk>
Message-ID: <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>

Hi ,

i have HP server which access it over IDRAC https and need java support .

i have proxy in same lan .
proxy ip is 10.0.0.200
ip of Idrac is 10.0.0.70 


i can?t access Console of Idrac using squid , that?d what i need to do  .

i need to be ale to access server Console ? which need java? too .

so not sure if its possible or not .

again its over https so i believe its listed already in squid safe ports 

let me know your thoughts .

Kind regards 



> On Nov 10, 2019, at 10:55 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:
> 
> listed in ssl_ports probably.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191112/52cd9290/attachment.htm>

From jimoe at sohnen-moe.com  Wed Nov 13 19:36:24 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Wed, 13 Nov 2019 12:36:24 -0700
Subject: [squid-users] After enabling IPv6 squid no longer responds
Message-ID: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>

Hello,
  squid v4.8

  I have started transitioning our local network to IPv6.
  After adding v6 addresses to the server and hosts, and enabling an RA, squid
no longer delivers anything from its cache, or is exceedingly slow about it.
  I have reviewed the wiki. The one section that discusses this issue has a
solution only for v3.1 or earlier. Does it also apply to later versions?
  What am I missing?

----[ squid.conf ]----
# acl manager url_regex -i ^cache_object:// /squid-internal-mgr/
acl manager_admin src 192.168.69.115
#
# acl localnet src fc00::/7
# acl localnet src fe80::/10
#
# https, cups
acl SSL_ports port 443
acl SSL_ports port 631
#
# Jumpline cPanel ports
acl SSL_ports port 2083
acl SSL_ports port 2096
#
# sma-nas-02, cgatePro, webadmin
acl SSL_ports port 5000
acl SSL_ports port 5001
acl SSL_ports port 9010
acl SSL_ports port 9100
acl SSL_ports port 10000
#
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 563
acl Safe_ports port 631
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl Safe_ports port 9100
#
acl CONNECT method CONNECT
acl localnet src 192.168.69.0/24
acl localnet src fd2f:4760:521f:3f3c::0/64

access_log /data01/var/log/squid/access.log
#
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow manager_admin
http_access allow manager localhost
http_access deny manager
http_access allow localnet
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
# cache_dir ufs /var/cache/squid 100 16 256
cache_dir ufs /data01/var/cache/squid 51200 16 256
maximum_object_size 99999 KB
cache_mem 256 MB

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp: 1440 20 10080
refresh_pattern ^gopher: 1440 0 1440
refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
refresh_pattern . 0 20 4320

cache_log /data01/var/log/squid/cache.log
cache_mgr jimoe at sohnen-moe.com
cache_replacement_policy lru
cache_store_log /data01/var/log/squid/store.log
cache_swap_high 95
cache_swap_low 90
client_lifetime 1 days
connect_timeout 2 minutes

logfile_rotate 0

error_directory /usr/share/squid/errors/en

ftp_passive on
memory_replacement_policy lru
minimum_object_size 0 KB
----[ end ]----

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191113/58d5ea67/attachment.sig>

From uhlar at fantomas.sk  Wed Nov 13 20:09:26 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 13 Nov 2019 21:09:26 +0100
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
 <20191110195506.GC20636@fantomas.sk>
 <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>
Message-ID: <20191113200926.GB32360@fantomas.sk>

On 12.11.19 16:20, --Ahmad-- wrote:
>i have HP server which access it over IDRAC https and need java support .

you don't need java support. Apparently your java needs to be configured
with proxy. And maybe the proxy needs to allow access to idrac ports.
for that you must have rejection in proxy logs.

>i have proxy in same lan .
>proxy ip is 10.0.0.200
>ip of Idrac is 10.0.0.70
>
>
>i can?t access Console of Idrac using squid , that?d what i need to do  .
>
>i need to be ale to access server Console ? which need java? too .
>
>so not sure if its possible or not .
>
>again its over https so i believe its listed already in squid safe ports
>
>let me know your thoughts .
>
>Kind regards
>
>
>
>> On Nov 10, 2019, at 10:55 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:
>>
>> listed in ssl_ports probably.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fighting for peace is like fucking for virginity...


From ahmed.zaeem at netstream.ps  Thu Nov 14 05:09:44 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 14 Nov 2019 08:09:44 +0300
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <20191113200926.GB32360@fantomas.sk>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
 <20191110195506.GC20636@fantomas.sk>
 <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>
 <20191113200926.GB32360@fantomas.sk>
Message-ID: <AC68DC07-2CE2-4939-BCF9-47F2D7DB4479@netstream.ps>

Hi Matus ,


Here is Log file squid , there is no Denied At all !

####################
1573682647.451      0 213.133.221.224 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
1573682647.455      0 213.133.221.224 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
1573682647.455      0 213.133.221.224 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
1573682647.456      0 213.133.221.224 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
1573682651.117    952 213.133.221.224 TCP_TUNNEL/200 2690 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682651.365   1200 213.133.221.224 TCP_TUNNEL/200 20663 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682651.414   1246 213.133.221.224 TCP_TUNNEL/200 11190 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682652.490   2935 213.133.221.224 TCP_TUNNEL/200 41968 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682657.175      0 213.133.221.224 NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
1573682661.827   8037 213.133.221.224 TCP_TUNNEL/200 63802 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682701.740  60994 213.133.221.224 TCP_TUNNEL/200 3680 CONNECT incoming.telemetry.mozilla.org:443 - HIER_DIRECT/52.35.171.123 -
1573682713.170  72358 213.133.221.224 TCP_TUNNEL/200 110961 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682714.170  62607 213.133.221.224 TCP_TUNNEL/200 1340 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -
1573682723.173  73017 213.133.221.224 TCP_TUNNEL/200 71908 CONNECT 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -

####################################



Here is Java Error log :

<?xml version="1.0" encoding="UTF-8"?>
<jnlp codebase="https://10.0.10.22:443" spec="1.0+">
<information>
  <title>iDRAC6 Virtual Console Client</title>
  <vendor>Dell Inc.</vendor>
   <icon href="https://10.0.10.22:443/images/logo.gif" kind="splash"/>
   <shortcut online="true"/>
 </information>
 <application-desc main-class="com.avocent.idrac.kvm.Main">
   <argument>ip=10.0.10.22</argument>
   <argument>vmprivilege=true</argument>
   <argument>helpurl=https://10.0.10.22:443/help/contents.html</argument>
   <argument>title=idrac-20RDVR1%2C+PowerEdge+R610%2C+User%3Aroot</argument>
   <argument>user=35005211</argument>
   <argument>passwd=521595368</argument>
   <argument>kmport=5900</argument>
   <argument>vport=5900</argument>
   <argument>apcp=1</argument>
   <argument>version=2</argument>
 </application-desc>
 <security>
   <all-permissions/>
 </security>
 <resources>
   <j2se version="1.6+"/>
   <jar href="https://10.0.10.22:443/software/avctKVM.jar" download="eager" main="true" />
 </resources>
 <resources os="Windows" arch="x86">
   <nativelib href="https://10.0.10.22:443/software/avctKVMIOWin32.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMWin32.jar" download="eager"/>
 </resources>
 <resources os="Windows" arch="amd64">
   <nativelib href="https://10.0.10.22:443/software/avctKVMIOWin64.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMWin64.jar" download="eager"/>
 </resources>
 <resources os="Windows" arch="x86_64">
   <nativelib href="https://10.0.10.22:443/software/avctKVMIOWin64.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMWin64.jar" download="eager"/>
 </resources>
  <resources os="Linux" arch="x86">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOLinux32.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMLinux32.jar" download="eager"/>
  </resources>
  <resources os="Linux" arch="i386">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOLinux32.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMLinux32.jar" download="eager"/>
  </resources>
  <resources os="Linux" arch="i586">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOLinux32.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMLinux32.jar" download="eager"/>
  </resources>
  <resources os="Linux" arch="i686">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOLinux32.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMLinux32.jar" download="eager"/>
  </resources>
  <resources os="Linux" arch="amd64">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOLinux64.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMLinux64.jar" download="eager"/>
  </resources>
  <resources os="Linux" arch="x86_64">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOLinux64.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMLinux64.jar" download="eager"/>
  </resources>
  <resources os="Mac OS X" arch="x86_64">
    <nativelib href="https://10.0.10.22:443/software/avctKVMIOMac64.jar" download="eager"/>
   <nativelib href="https://10.0.10.22:443/software/avctVMMac64.jar" download="eager"/>
  </resources>
</jnlp>




java.net.ConnectException: Operation timed out (Connection timed out)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)
	at sun.security.ssl.BaseSSLSocketImpl.connect(BaseSSLSocketImpl.java:173)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:463)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:558)
	at sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264)
	at sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367)
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156)
	at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1040)
	at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1038)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1037)
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1564)
	at sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:91)
	at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1484)
	at sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1482)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:782)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1481)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:263)
	at com.sun.deploy.net.HttpUtils.followRedirects(Unknown Source)
	at com.sun.deploy.net.BasicHttpRequest.doRequest(Unknown Source)
	at com.sun.deploy.net.BasicHttpRequest.doGetRequestEX(Unknown Source)
	at com.sun.deploy.net.DownloadEngine.actionDownload(Unknown Source)
	at com.sun.deploy.net.DownloadEngine.downloadResource(Unknown Source)
	at com.sun.deploy.cache.ResourceProviderImpl.getResource(Unknown Source)
	at com.sun.deploy.cache.ResourceProviderImpl.getResource(Unknown Source)
	at com.sun.javaws.LaunchDownload$DownloadTask.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)




com.sun.deploy.net.FailedDownloadException: Unable to load resource: https://10.0.10.22:443/software/avctKVM.jar
	at com.sun.deploy.net.DownloadEngine.actionDownload(Unknown Source)
	at com.sun.deploy.net.DownloadEngine.downloadResource(Unknown Source)
	at com.sun.deploy.cache.ResourceProviderImpl.getResource(Unknown Source)
	at com.sun.deploy.cache.ResourceProviderImpl.getResource(Unknown Source)
	at com.sun.javaws.LaunchDownload$DownloadTask.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)



> On Nov 13, 2019, at 11:09 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:
> 
> On 12.11.19 16:20, --Ahmad-- wrote:
>> i have HP server which access it over IDRAC https and need java support .
> 
> you don't need java support. Apparently your java needs to be configured
> with proxy. And maybe the proxy needs to allow access to idrac ports.
> for that you must have rejection in proxy logs.
> 
>> i have proxy in same lan .
>> proxy ip is 10.0.0.200
>> ip of Idrac is 10.0.0.70
>> 
>> 
>> i can?t access Console of Idrac using squid , that?d what i need to do  .
>> 
>> i need to be ale to access server Console ? which need java? too .
>> 
>> so not sure if its possible or not .
>> 
>> again its over https so i believe its listed already in squid safe ports
>> 
>> let me know your thoughts .
>> 
>> Kind regards
>> 
>> 
>> 
>>> On Nov 10, 2019, at 10:55 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:
>>> 
>>> listed in ssl_ports probably.
> 
> -- 
> Matus UHLAR - fantomas, uhlar at fantomas.sk <mailto:uhlar at fantomas.sk> ; http://www.fantomas.sk/ <http://www.fantomas.sk/>
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Fighting for peace is like fucking for virginity...
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191114/2f2b9d74/attachment.htm>

From vedavyas.vayalpadu at accenture.com  Thu Nov 14 05:33:46 2019
From: vedavyas.vayalpadu at accenture.com (Vayalpadu, Vedavyas)
Date: Thu, 14 Nov 2019 05:33:46 +0000
Subject: [squid-users] Automate SSL Certificate - Reverse Squid Proxy - vyas
Message-ID: <BYAP114MB0920613D74DDD12541413F848F710@BYAP114MB0920.NAMP114.PROD.OUTLOOK.COM>

Hi Guys,

I have a reverse proxy squid server, where we are maintaining SSL certificates for the webshop applications, I wanted to know if there is any mechanism to,


  1.  Alert our UNIX team with a mail before 30 days of expiry.


  1.  Automate the certificate renewal,


Any suggestions are welcome.



VYAS  (Vedavyas Vayalpadu)
IT Operations Specialist - UNIX-IBM-AIX
vedavyas.vayalpadu at accenture.com<mailto:vedavyas.vayalpadu at accenture.com>
+91-7032906468


________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy. Your privacy is important to us. Accenture uses your personal data only in compliance with data protection laws. For further information on how Accenture processes your personal data, please see our privacy statement at https://www.accenture.com/us-en/privacy-policy.
______________________________________________________________________________________

www.accenture.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191114/8b3362c5/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 14 07:49:36 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Nov 2019 20:49:36 +1300
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <AC68DC07-2CE2-4939-BCF9-47F2D7DB4479@netstream.ps>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
 <20191110195506.GC20636@fantomas.sk>
 <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>
 <20191113200926.GB32360@fantomas.sk>
 <AC68DC07-2CE2-4939-BCF9-47F2D7DB4479@netstream.ps>
Message-ID: <4f262b20-838f-49a0-f10f-54b4b245fc05@treenet.co.nz>

On 14/11/19 6:09 pm, --Ahmad-- wrote:
> Hi Matus ,
> 
> 
> Here is Log file squid , there is no Denied At all !
> 
> ####################
> 1573682647.451 ? ? ?0 213.133.221.224 NONE/000 0 NONE
> error:transaction-end-before-headers - HIER_NONE/- -

These are the client connecting, doing nothing. Then closing the connection.

No Squid problem visible. Whatever is going wrong is in the client
software. We see these a lot with "Happy Eyeballs" connections, so maybe
no problem at all there.


> 1573682651.117 ? ?952 213.133.221.224 TCP_TUNNEL/200 2690 CONNECT
> 10.0.10.22:443 - HIER_DIRECT/10.0.10.22 -

These are the client opening a tunnel to the origin server 10.0.10.22.
Which is successful and transfers some data around.

No Squid problem there either. Whatever is going wrong is in either the
client or server software - they are communicating directly with each
other over that tunnel.


> ####################################
> 
> 
> 
> Here is Java Error log :
...
> 
> java.net.ConnectException: Operation timed out (Connection timed out)
> at java.net.PlainSocketImpl.socketConnect(Native Method)

I suggest solving that problem. It does not seem related to Squid.


Amos


From uhlar at fantomas.sk  Thu Nov 14 07:55:27 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 14 Nov 2019 08:55:27 +0100
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <4f262b20-838f-49a0-f10f-54b4b245fc05@treenet.co.nz>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
 <20191110195506.GC20636@fantomas.sk>
 <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>
 <20191113200926.GB32360@fantomas.sk>
 <AC68DC07-2CE2-4939-BCF9-47F2D7DB4479@netstream.ps>
 <4f262b20-838f-49a0-f10f-54b4b245fc05@treenet.co.nz>
Message-ID: <20191114075527.GA21698@fantomas.sk>

>On 14/11/19 6:09 pm, --Ahmad-- wrote:
>> ####################################
>>
>>
>>
>> Here is Java Error log :
>...
>>
>> java.net.ConnectException: Operation timed out (Connection timed out)
>> at java.net.PlainSocketImpl.socketConnect(Native Method)

On 14.11.19 20:49, Amos Jeffries wrote:
>I suggest solving that problem. It does not seem related to Squid.

either the javaws does not have proxy set, or the iDrac6 HP 
(are you sure it's idrac HP? idrac is the DELL And HP is not dell, HPs have
ILO) does not support proxy.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
WinError #98652: Operation completed successfully.


From uhlar at fantomas.sk  Thu Nov 14 08:01:30 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 14 Nov 2019 09:01:30 +0100
Subject: [squid-users] Automate SSL Certificate - Reverse Squid Proxy -
 vyas
In-Reply-To: <BYAP114MB0920613D74DDD12541413F848F710@BYAP114MB0920.NAMP114.PROD.OUTLOOK.COM>
References: <BYAP114MB0920613D74DDD12541413F848F710@BYAP114MB0920.NAMP114.PROD.OUTLOOK.COM>
Message-ID: <20191114080130.GB21698@fantomas.sk>

On 14.11.19 05:33, Vayalpadu, Vedavyas wrote:
>I have a reverse proxy squid server, where we are maintaining SSL certificates for the webshop applications, I wanted to know if there is any mechanism to,
>
>
>  1.  Alert our UNIX team with a mail before 30 days of expiry.

certificate providers use to provide that service.

>  1.  Automate the certificate renewal,

any certificate provider supporting ACME protocol could do this. on your
side it could be any ACME client, certbot or dehydrated (I prefer the
latter) could do that. You'll need script that reloads squid config with
certificate

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Windows found: (R)emove, (E)rase, (D)elete


From jlowry at gmail.com  Thu Nov 14 17:29:07 2019
From: jlowry at gmail.com (John Lowry)
Date: Thu, 14 Nov 2019 09:29:07 -0800
Subject: [squid-users] acl whitelist ssl::server_name not working
Message-ID: <CAOWEjnBKthq3XS8vPe1cj8u9GdwAbxn_EDxeYsTOf3av5Ur1rg@mail.gmail.com>

Thanks to Alex Rousskov's excellent explanation in
http://squid-web-proxy-cache.1019090.n4.nabble.com/Cannot-configure-squid-4-6-to-splice-without-bumping-td4688482.html,
I have been able to set up Squid as a transparent proxy that splices
HTTPS connections.

I want to set up a whitelist. First, I tried to configure SquidGuard
but I couldn't find a way to pass the servername to SquidGuard when
connections were spliced.

So now I'm trying to use ACLs to whitelist by hostname.

acl whitelist ssl::server_name "/etc/squid/whitelist.txt" --client-requested

But I can't get it to work.The logs appeared to indicate that URLs in
the whitelist were first denied then bumped:

14/Nov/2019:08:46:25 -0800 192.168.2.43 TCP_DENIED/- 0 CONNECT
104.17.67.73:443 - HIER_NONE/- - www.headroyce.org
14/Nov/2019:08:46:25 -0800 192.168.2.43 NONE/- 3793 GET
https://www.headroyce.org/ - HIER_NONE/- text/html www.headroyce.org

I think that the ACLs are trying to match a spliced connection against
the IP address rather than SNI server name.

Any idea what I'm doing wrong here?

I'd also like to present a good error message if the outcome is
denied, and never bump connections.

My config is:

acl CONNECT method CONNECT
acl whitelist ssl::server_name "/etc/squid/whitelist.txt" --client-requested
http_access allow whitelist
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
include /etc/squid/conf.d/*
http_access allow localhost
http_access deny all
http_port 3127
http_port 3128 intercept
https_port 3129 intercept ssl-bump
tls-cert=/etc/squid/ssl_cert/myCA.pem
tls-key=/etc/squid/ssl_cert/myCA.pem
ssl_bump peek all
ssl_bump splice all
logformat sslbump     %tl %>a %Ss/%03<Hs %<st %rm %>ru %[un %Sh/%<a
%mt %ssl::>sni
access_log daemon:/var/log/squid/access.log sslbump
debug_options ALL,3 28,9


From jimoe at sohnen-moe.com  Thu Nov 14 17:41:06 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Thu, 14 Nov 2019 10:41:06 -0700
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
Message-ID: <sig.1221b1482c.02027f42-5e27-ff8a-8505-e954be0db017@sohnen-moe.com>

On 13/11/2019 12.36 pm, James Moe wrote:

>   After adding v6 addresses to the server and hosts, and enabling an RA, squid
> no longer delivers anything from its cache, or is exceedingly slow about it.
>
 Any one?

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191114/c273d0ff/attachment.sig>

From jimoe at sohnen-moe.com  Thu Nov 14 18:50:00 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Thu, 14 Nov 2019 11:50:00 -0700
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
Message-ID: <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>

On 13/11/2019 12.36 pm, James Moe wrote:

>   After adding v6 addresses to the server and hosts, and enabling an RA, squid
> no longer delivers anything from its cache, or is exceedingly slow about it.
>
  Here is a typical error message from squid:

The following error was encountered while trying to retrieve the URL:
http://dx.doi.org/
Connection to 2606:4700:20::681a:9ed failed.
The system returned: (110) Connection timed out

  There is nothing in the access.log; the request is utterly ignored.
  When I have the browser bypass the proxy, the site loads almost instantly.

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191114/c91798da/attachment.sig>

From Antony.Stone at squid.open.source.it  Thu Nov 14 19:00:12 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 14 Nov 2019 20:00:12 +0100
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
Message-ID: <201911142000.13012.Antony.Stone@squid.open.source.it>

On Thursday 14 November 2019 at 19:50:00, James Moe wrote:

> On 13/11/2019 12.36 pm, James Moe wrote:
> >   After adding v6 addresses to the server and hosts, and enabling an RA,
> >   squid no longer delivers anything from its cache, or is exceedingly slow
> >   about it.
> 
>   Here is a typical error message from squid:
> 
> The following error was encountered while trying to retrieve the URL:
> http://dx.doi.org/
> Connection to 2606:4700:20::681a:9ed failed.
> The system returned: (110) Connection timed out
> 
>   There is nothing in the access.log; the request is utterly ignored.
>   When I have the browser bypass the proxy, the site loads almost instantly.

Have you confirmed (for example with a network packet sniffer) that the browser 
is connecting directly to the site also using IPv6?


For that matter, have you used a packet sniffer to find out what Squid is doing, 
in terms of requests sent and possible responses received?


Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Walter.H at mathemainzel.info  Thu Nov 14 19:06:36 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Thu, 14 Nov 2019 20:06:36 +0100
Subject: [squid-users] difference of settings doing the same as it seems
Message-ID: <5DCDA5BC.1010608@mathemainzel.info>

Hello,

I found out something strange

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl nobumpsites ssl::server_name "/etc/squid/sslnobumpsites-acl.squid"

# I had these 3 settings - most worked, but only a few hosted at 
cloudflare worked: problems with SNI there, but only there
#ssl_bump stare step1 all
#ssl_bump splice nobumpsites
#ssl_bump bump all

# so I did these 3 settings
ssl_bump peek step1
ssl_bump splice nobumpsites
ssl_bump stare all

the file above contains server names where no SSL interception should be 
done, e.g. banking;

can someone explain the difference between these two ways - the 
commented ones and the other 3 settings?

Thanks,
Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191114/dd1f73ad/attachment.bin>

From rousskov at measurement-factory.com  Thu Nov 14 22:04:48 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Nov 2019 17:04:48 -0500
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
Message-ID: <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>

On 11/14/19 1:50 PM, James Moe wrote:
> On 13/11/2019 12.36 pm, James Moe wrote:
> 
>>   After adding v6 addresses to the server and hosts, and enabling an RA, squid
>> no longer delivers anything from its cache, or is exceedingly slow about it.

>   Here is a typical error message from squid:
> 
> The following error was encountered while trying to retrieve the URL:
> http://dx.doi.org/
> Connection to 2606:4700:20::681a:9ed failed.
> The system returned: (110) Connection timed out

Can you connect to port 80 of that IPv6 address using telnet, wget, or
curl running on the Squid box?


>   There is nothing in the access.log; the request is utterly ignored.

FYI: "utterly ignored" seems to contradict "error message from squid"
above. If Squid v4 sent an error response to the browser but logged
nothing to access.log, then there is a Squid bug that you should report
to Bugzilla.

HTH,

Alex.


From rousskov at measurement-factory.com  Thu Nov 14 22:38:25 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Nov 2019 17:38:25 -0500
Subject: [squid-users] difference of settings doing the same as it seems
In-Reply-To: <5DCDA5BC.1010608@mathemainzel.info>
References: <5DCDA5BC.1010608@mathemainzel.info>
Message-ID: <889572a2-7048-8b80-7871-3039e92febbf@measurement-factory.com>

On 11/14/19 2:06 PM, Walter H. wrote:

> #ssl_bump stare step1 all
> #ssl_bump splice nobumpsites
> #ssl_bump bump all

> ssl_bump peek step1
> ssl_bump splice nobumpsites
> ssl_bump stare all

Both configurations peek at the TLS client Hello. Both configurations
splice nobumpsites during step2 when nobumpsites matches during that
step. Now about the differences:

The first configuration bumps bumpsites (i.e. sites that did not match
nobumpsites) during step2, before the server certificate details are
known. It never reaches step3.

The second configuration uses the implicit "bump if the action during
the previous step was stare and no applicable actions matched during the
current step" rule to bump bumpsites during step3, after learning the
server certificate details.


You can rewrite these two configurations to be more symmetrical but
still have the same respective outcomes:

  # bump at step2
  ssl_bump peek step1
  ssl_bump splice nobumpsites
  ssl_bump bump all

  # bump at step3
  ssl_bump peek step1
  ssl_bump splice nobumpsites
  ssl_bump stare step2
  ssl_bump bump all

As you can see, the only difference is the "stare step2" rule which
allows Squid to learn the server certificate details and incorporate
those details into the generated fake certificate when the connections
are bumped.


> can someone explain the difference between these two ways - the
> commented ones and the other 3 settings?

If you had good reasons to think that the two configuration are the
same, consider contributing Squid documentation adjustments to better
explain why they are not.

Alex.


From rousskov at measurement-factory.com  Thu Nov 14 23:07:41 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Nov 2019 18:07:41 -0500
Subject: [squid-users] acl whitelist ssl::server_name not working
In-Reply-To: <CAOWEjnBKthq3XS8vPe1cj8u9GdwAbxn_EDxeYsTOf3av5Ur1rg@mail.gmail.com>
References: <CAOWEjnBKthq3XS8vPe1cj8u9GdwAbxn_EDxeYsTOf3av5Ur1rg@mail.gmail.com>
Message-ID: <c70eae84-f8f6-4e88-8fc3-52d112237f19@measurement-factory.com>

On 11/14/19 12:29 PM, John Lowry wrote:
> I have been able to set up Squid as a transparent proxy that splices
> HTTPS connections.

> now I'm trying to use ACLs to whitelist by hostname.
> 
> acl whitelist ssl::server_name "/etc/squid/whitelist.txt" --client-requested


FWIW, I do not know whether the above syntax is supported. I recommend
starting with a single whitelisted name. For example:

  acl whitelist ssl::server_name --client-requested example.com

and then, if the above works, migrate to importing parameters from a
file (but start with one domain name in that file):

  acl whitelist ssl::server_name --client-requested
"/etc/squid/whitelist.txt"


> But I can't get it to work.The logs appeared to indicate that URLs in
> the whitelist were first denied then bumped:
> 
> 14/Nov/2019:08:46:25 -0800 192.168.2.43 TCP_DENIED/- 0 CONNECT
> 104.17.67.73:443 - HIER_NONE/- - www.headroyce.org
> 14/Nov/2019:08:46:25 -0800 192.168.2.43 NONE/- 3793 GET
> https://www.headroyce.org/ - HIER_NONE/- text/html www.headroyce.org
> 
> I think that the ACLs are trying to match a spliced connection against
> the IP address rather than SNI server name.
> 
> Any idea what I'm doing wrong here?

If you only want to act based on SNI, then do not use an http_access
rule during step1 when SNI is not yet known. There may be several ways
to accomplish that. However, in most cases, you want to act ASAP,
regardless of whether the [sufficient] information came from the TCP
layer or the TLS layer. If that is your use case, then it is OK to apply
the http_access rule during step1 as well (assuming your ACL will simply
not match when there is not enough information yet).


> http_access allow whitelist

Even if the request is for an "unsafe" port? I doubt you want this rule
so high. See squid.conf.default for the recommended access controls order.


> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> include /etc/squid/conf.d/*
> http_access allow localhost
> http_access deny all

FYI: The last rule will deny access to non-localhost CONNECT requests
during step1 if they do not carry enough information to qualify for the
whitelist exception.

Keep in mind that http_access rules are evaluated several times during a
single master transaction. For details, please see
https://wiki.squid-cache.org/Features/SslPeekAndSplice


HTH,

Alex.


From chammidhan at gmail.com  Fri Nov 15 01:56:45 2019
From: chammidhan at gmail.com (chammidhan)
Date: Thu, 14 Nov 2019 19:56:45 -0600 (CST)
Subject: [squid-users] logformat for requests using PROXY protocol
Message-ID: <1573783005972-0.post@n4.nabble.com>

I have configured a Squid ECS cluster behind a network load balancer in AWS.
To reflect the original client IP, I needed to enable PROXY Protocol V2 on
the load balancer. The service itself is working fine and I can create rules
based on the original client IP and these are applied as expected. However,
it doesn't seem that logformat format codes are working as expected. No
matter how I format the logs, I'm always seeing the logs in the same format.
Which looks like below.

1573771498.693 240116 10.181.3.10 TCP_TUNNEL/200 1742 CONNECT
id.google.com:443 - HIER_DIRECT/172.217.167.67 -

My logformat directive is the default
logformat squid %{%Y/%m/%d-%H:%M:%S}tl %>A/%>a %un %rm/%rv %ru %mt
%{User-Agent}>h %>st/%<st %tr %>Hs %Ss %Sh/%<A

Appreciate any insight to what I may be doing wrong. Things were working
fine before enabling PROXY protocol on the NLB

Squid version I'm using is 4.4



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From suxikang1 at hisilicon.com  Fri Nov 15 03:23:47 2019
From: suxikang1 at hisilicon.com (hindsight1)
Date: Thu, 14 Nov 2019 21:23:47 -0600 (CST)
Subject: [squid-users] Squid crash - 3.5.21
In-Reply-To: <755aa523-6791-4043-c5cf-1646982e6f71@measurement-factory.com>
References: <1475491833.8486.41.camel@shoprite.co.za>
 <aff9b1ab-5233-5e28-af18-5b867dad89df@measurement-factory.com>
 <1573387989765-0.post@n4.nabble.com>
 <31926e6a-d867-c8ad-4ad7-c1402867ab59@measurement-factory.com>
 <1573526053795-0.post@n4.nabble.com>
 <755aa523-6791-4043-c5cf-1646982e6f71@measurement-factory.com>
Message-ID: <1573788227848-0.post@n4.nabble.com>

Hi Alex,

> So where does this misalignment originate from? Properly addressing this
bug probably requires answering this question.


Let's discuss it on Squid Bugzilla, I have already mentioned the bug above,
the bug number is 5008

> Please note that there are GCC v4 bugs that might be relevant here:
>https://gcc.gnu.org/bugzilla/show_bug.cgi?id=62259
>https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65147
>What is your compiler? Does building with GCC v5.1 fix the problem?

Originally used 4.8.5?but updated to 7.3.0. This problem still exists

> BTW, there is a potentially useful alignas() workaround/trick shown at
> https://stackoverflow.com/questions/26703297/alignment-of-atomic-variables


This method I have tried when the problem appeared, it no use

Regards?
hindsight





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Nov 15 05:15:16 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Nov 2019 18:15:16 +1300
Subject: [squid-users] logformat for requests using PROXY protocol
In-Reply-To: <1573783005972-0.post@n4.nabble.com>
References: <1573783005972-0.post@n4.nabble.com>
Message-ID: <91eb6ee0-bcbc-0598-351b-36737d990749@treenet.co.nz>

On 15/11/19 2:56 pm, chammidhan wrote:
> I have configured a Squid ECS cluster behind a network load balancer in AWS.
> To reflect the original client IP, I needed to enable PROXY Protocol V2 on
> the load balancer. The service itself is working fine and I can create rules
> based on the original client IP and these are applied as expected. However,
> it doesn't seem that logformat format codes are working as expected. No
> matter how I format the logs, I'm always seeing the logs in the same format.
> Which looks like below.
> 
> 1573771498.693 240116 10.181.3.10 TCP_TUNNEL/200 1742 CONNECT
> id.google.com:443 - HIER_DIRECT/172.217.167.67 -
> 
> My logformat directive is the default
> logformat squid %{%Y/%m/%d-%H:%M:%S}tl %>A/%>a %un %rm/%rv %ru %mt
> %{User-Agent}>h %>st/%<st %tr %>Hs %Ss %Sh/%<A
> 
> Appreciate any insight to what I may be doing wrong. Things were working
> fine before enabling PROXY protocol on the NLB
> 

Please run "squid -k parse" on your config and fix the errors and
warnings it produces.

"
2019/11/15 18:11:50| Processing: logformat squid %{%Y/%m/%d-...
2019/11/15 18:11:50| ERROR: logformat squid is already defined. Ignoring.
"

To use a custom log format you need a custom name.


Amos


From zebox at tutanota.com  Mon Nov 18 08:18:43 2019
From: zebox at tutanota.com (zebox at tutanota.com)
Date: Mon, 18 Nov 2019 09:18:43 +0100 (CET)
Subject: [squid-users] ecap trickling (antivirus scan)
Message-ID: <Ltx_uU4--3-1@tutanota.com>

Hello,

I'm trying to enable trickling for the ecap clamav adapter during download, in order to prevent browser timeout for large files.

So far, I have added "adaptation_access trickling_drop_size=10" in squid.conf but it seems to have no effect.

Versions :

squid 3.5.20
ecap_clamav_adapter-2.0.0
clamd 0.101.2
Centos 7

Regards


From uhlar at fantomas.sk  Mon Nov 18 14:42:27 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 18 Nov 2019 15:42:27 +0100
Subject: [squid-users] single acl analysis
Message-ID: <20191118144227.GA8813@fantomas.sk>

Hello,

I'm going to migrate squid server to new machine and found this ACL:

acl freedst1 dstdom_regex -i www\.___\.sk none

http_access allow freedst1

I believe it could be replaces by:

acl freedst1 dstdomain -i www.___.sk

which would allow connection to said website.  However the "none" part
confuses me. According to the docs:

The name "none" is used if the reverse lookup fails.

does that mean the directives above allow access to any site without rDNS?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Silvester Stallone: Father of the RISC concept.


From rousskov at measurement-factory.com  Mon Nov 18 16:09:31 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Nov 2019 11:09:31 -0500
Subject: [squid-users] ecap trickling (antivirus scan)
In-Reply-To: <Ltx_uU4--3-1@tutanota.com>
References: <Ltx_uU4--3-1@tutanota.com>
Message-ID: <4dfae30a-ca88-a736-fb2c-8b13404b37e8@measurement-factory.com>

On 11/18/19 3:18 AM, zebox at tutanota.com wrote:

> I'm trying to enable trickling for the ecap clamav adapter during download


> So far, I have added "adaptation_access trickling_drop_size=10" in squid.conf but it seems to have no effect.

Squid uses eCAP service configuration to supply eCAP adapter services
with configuration options. Here is an example showing a mixture of
standard and custom configuration options:

ecap_service responseFilter respmod_precache \
    bypass=0 \
    uri=ecap://e-cap.org/ecap/services/clamav?mode=RESPMOD \
    on_error=block \
    staging_dir=/tmp/eclamav/resp- \
    trickling_drop_size=10 \
    debug=none

HTH,

Alex.


From ishtiak.ikbal at gmail.com  Tue Nov 19 00:34:45 2019
From: ishtiak.ikbal at gmail.com (Ishtiaq Iqbal)
Date: Tue, 19 Nov 2019 05:34:45 +0500
Subject: [squid-users] Facebook Cache
Message-ID: <CAHp_96bziQJD-N+H2=MQ8imDajgK0=LAV=acco_R+Ym3B-xnug@mail.gmail.com>

Hi
Anybody who can help me to Set up Squid Cache server for me to cache
Facebook,
I ll pay for his services
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191119/34d2325a/attachment.htm>

From ishtiak.ikbal at gmail.com  Tue Nov 19 00:39:56 2019
From: ishtiak.ikbal at gmail.com (ishtiak)
Date: Mon, 18 Nov 2019 18:39:56 -0600 (CST)
Subject: [squid-users] Squid 3.5 https facebook caching
In-Reply-To: <67347320-3125-f3c4-d706-46820eea5718@treenet.co.nz>
References: <1555427055618-0.post@n4.nabble.com>
 <95e8facb-4e51-09a4-b505-0a6e13367419@treenet.co.nz>
 <1555545793141-0.post@n4.nabble.com>
 <29c923ba-93ef-7f10-585c-9cf54383a1eb@treenet.co.nz>
 <6f81e912-2c71-b25e-818b-7c16df7298e7@gmail.com>
 <67347320-3125-f3c4-d706-46820eea5718@treenet.co.nz>
Message-ID: <1574123996225-0.post@n4.nabble.com>

Please guide me how  to setup caching Facebook
I will pay for the services  




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Nov 19 05:00:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Nov 2019 18:00:08 +1300
Subject: [squid-users] squid with Java Problem - Idrac 6 Hp servers
In-Reply-To: <670B0BCE-517A-48B5-BB28-9D0B7576ED45@netstream.ps>
References: <EBD2F38C-C746-4687-ACFF-8A860FAFADCA@netstream.ps>
 <20191110195506.GC20636@fantomas.sk>
 <F2FB0D83-6871-48B2-B3E0-EEDAF554D115@netstream.ps>
 <20191113200926.GB32360@fantomas.sk>
 <AC68DC07-2CE2-4939-BCF9-47F2D7DB4479@netstream.ps>
 <670B0BCE-517A-48B5-BB28-9D0B7576ED45@netstream.ps>
Message-ID: <6125d94e-3a8f-8fac-d5af-b5bc826b4f8c@treenet.co.nz>

On 19/11/19 4:58 pm, --Ahmad-- wrote:
> Hello Amos , Are you able to help me out ?
> 

Apart from what Matus has already mentioned ...

* the Java traceback shows TCP socket setup is timing out.

* Squid access.log is showing those NONE transactions opening sockets
then timing out before any data arrives.

I would do a packet trace of the TCP the client software is sending to
Squid to find out what is happening to those TCP sockets.
 * Are they actually going to Squid or elsewhere?
 * Are the SYN+ACK packets getting back to the client software?

Amos


From squid3 at treenet.co.nz  Tue Nov 19 07:48:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Nov 2019 20:48:08 +1300
Subject: [squid-users] single acl analysis
In-Reply-To: <20191118144227.GA8813@fantomas.sk>
References: <20191118144227.GA8813@fantomas.sk>
Message-ID: <3ad720ed-b044-e492-098e-f343ccf60d10@treenet.co.nz>

On 19/11/19 3:42 am, Matus UHLAR - fantomas wrote:
> Hello,
> 
> I'm going to migrate squid server to new machine and found this ACL:
> 
> acl freedst1 dstdom_regex -i www\.___\.sk none
> 
> http_access allow freedst1
> 
> I believe it could be replaces by:
> 
> acl freedst1 dstdomain -i www.___.sk
> 
> which would allow connection to said website.? However the "none" part
> confuses me. According to the docs:
> 
> The name "none" is used if the reverse lookup fails.
> 
> does that mean the directives above allow access to any site without rDNS?
> 

Yes, exactly so.

Amos


From zebox at tutanota.com  Tue Nov 19 10:24:55 2019
From: zebox at tutanota.com (zbox)
Date: Tue, 19 Nov 2019 11:24:55 +0100 (CET)
Subject: [squid-users] ecap trickling (antivirus scan)
In-Reply-To: <Ltx_uU4--3-1@tutanota.com>
References: <Ltx_uU4--3-1@tutanota.com>
Message-ID: <Lu2BOIR--3-1@tutanota.com>

Thanks, I've made the changes, the files are now downloaded in the temp dir but the behaviour is the same on the browser...


From jimoe at sohnen-moe.com  Tue Nov 19 19:08:32 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Tue, 19 Nov 2019 12:08:32 -0700
Subject: [squid-users] Changing the time format for access_log
Message-ID: <sig.0226c53421.69da5304-2de2-03b6-20f8-d2a329cde807@sohnen-moe.com>

squid 4.8

I changed the default squid log format

logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt

to present a more readable fime:
logformat sma1 %Y-%m-%dT%H:%M:%S %z.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
%Sh/%<a %mt

This did not sit well with squid:
$ squid -k parse
...[ bunch o' stuff ]...
2019/11/19 11:32:51| Processing: logformat sma1  %Y-%m-%dT%H:%M:%S %z.%03tu %6tr
%>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
2019/11/19 11:32:51| storeDirWriteCleanLogs: Operation aborted.
2019/11/19 11:32:51| FATAL: Can't parse configuration token: '%Y-%m-%dT%H:%M:%S
%z.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt'

What is the format error?

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191119/3a33d8fa/attachment.sig>

From rousskov at measurement-factory.com  Tue Nov 19 20:48:28 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Nov 2019 15:48:28 -0500
Subject: [squid-users] Changing the time format for access_log
In-Reply-To: <sig.0226c53421.69da5304-2de2-03b6-20f8-d2a329cde807@sohnen-moe.com>
References: <sig.0226c53421.69da5304-2de2-03b6-20f8-d2a329cde807@sohnen-moe.com>
Message-ID: <7c3ed523-082f-1055-a133-3bc9af5c91aa@measurement-factory.com>

On 11/19/19 2:08 PM, James Moe wrote:
> FATAL: Can't parse configuration token: '%Y-%m-%dT%H:%M:%S
> %z.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt'
> 
> What is the format error?

You are using logformat %codes unsupported by Squid, such as %Y, %m, and
%d. Please search the logformat documentation for "Time related format
codes": http://www.squid-cache.org/Doc/config/logformat/

Please note that %tl and %tg codes support a parameter. That parameter
is a strftime(3) format. strftime(3) does support %Y, %m, and %d
conversion specifications.

To specify a Squid %code parameter, use {curly braces}. IIRC, modern
Squids support a natural position for such parameters -- after the
%code. Here is an untested example:

 %tl{%Y-%m-%dT%H:%M:%S}


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Nov 19 22:41:57 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Nov 2019 17:41:57 -0500
Subject: [squid-users] ecap trickling (antivirus scan)
In-Reply-To: <Lu2BOIR--3-1@tutanota.com>
References: <Ltx_uU4--3-1@tutanota.com> <Lu2BOIR--3-1@tutanota.com>
Message-ID: <8dd29887-c20b-33f1-5a93-f376d6016ee7@measurement-factory.com>

On 11/19/19 5:24 AM, zbox wrote:

> I've made the changes, the files are now downloaded in the temp dir
> but the behaviour is the same on the browser...

Glad your configuration problem got resolved.

There is not enough information to figure out what else, if anything, is
broken (and where that breakage is). However, if the adapter was
configured to trickle, and you see a temporary download file growing in
size _while_ Squid sends _nothing_ to the client (at HTTP level) for
several seconds, then there is probably a bug in Squid and/or the
adapter. At the very least, Squid should sent the response header to the
client.

Without looking at low-level debugging logs (at least), I cannot tell
who is at fault, but perhaps others on the list can share trickling
success stories or help you with the triage.

Alex.


From squid3 at treenet.co.nz  Wed Nov 20 05:12:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Nov 2019 18:12:24 +1300
Subject: [squid-users] Changing the time format for access_log
In-Reply-To: <7c3ed523-082f-1055-a133-3bc9af5c91aa@measurement-factory.com>
References: <sig.0226c53421.69da5304-2de2-03b6-20f8-d2a329cde807@sohnen-moe.com>
 <7c3ed523-082f-1055-a133-3bc9af5c91aa@measurement-factory.com>
Message-ID: <387ae115-835a-493f-295b-c411b0c822aa@treenet.co.nz>

On 20/11/19 9:48 am, Alex Rousskov wrote:
> IIRC, modern
> Squids support a natural position for such parameters -- after the
> %code. Here is an untested example:
> 
>  %tl{%Y-%m-%dT%H:%M:%S}
> 

Yes, all current Squid support parameters ({arg}) being either before OR
after the code letters. That is OR, not both.

As per the docs:
"
 % [encoding] [-] [[0]width] [{arg}] formatcode [{arg}]
"

Amos


From nicklas.berger at scania.com  Wed Nov 20 08:31:04 2019
From: nicklas.berger at scania.com (Berger J Nicklas)
Date: Wed, 20 Nov 2019 08:31:04 +0000
Subject: [squid-users] squid 4.1 transparent https issue "curl: (60) SSL
 certificate problem: self signed certificate in certificate chain"
Message-ID: <HE1PR0401MB265128E2A54F6D48573F86E4F84F0@HE1PR0401MB2651.eurprd04.prod.outlook.com>

Hello,
I want to start saying I'm new working with squid so bear with me. We are at my company trying to use squid as egress solution for our servers running in AWS.
We need to have a whitelisting function in place.

HTTP works fine but not HTTPS.

When trying to run curl from another server using squid to access internet we receive this message:
 #curl https://microsoft.com
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.

When checking the squid cache log file this is showing:

# tail -f /var/log/squid/cache.log
2019/11/20 08:25:01 kid1| HTCP Disabled.
2019/11/20 08:25:01 kid1| Squid plugin modules loaded: 0
2019/11/20 08:25:01 kid1| Adaptation support is off.
2019/11/20 08:25:01 kid1| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 23 flags=9
2019/11/20 08:25:01 kid1| Accepting NAT intercepted HTTP Socket connections at local=[::]:3129 remote=[::] FD 24 flags=41
2019/11/20 08:25:01 kid1| Accepting NAT intercepted SSL bumped HTTPS Socket connections at local=[::]:3130 remote=[::] FD 25 flags=41
2019/11/20 08:25:02 kid1| storeLateRelease: released 0 objects
security_file_certgen helper database '/var/spool/squid/ssl_db' failed: Failed to open file /var/spool/squid/ssl_db/index.txt
2019/11/20 08:25:10 kid1| Error negotiating SSL connection on FD 12: error:00000001:lib(0):func(0):reason(1) (1/0)
2019/11/20 08:25:12 kid1| Error negotiating SSL connection on FD 12: error:00000001:lib(0):func(0):reason(1) (1/0)
2019/11/20 08:25:14 kid1| Error negotiating SSL connection on FD 12: error:00000001:lib(0):func(0):reason(1) (1/0)
2019/11/20 08:25:19 kid1| Error negotiating SSL connection on FD 12: error:00000001:lib(0):func(0):reason(1) (1/0)
2019/11/20 08:25:19 kid1| Error negotiating SSL connection on FD 12: error:00000001:lib(0):func(0):reason(1) (1/0)
2019/11/20 08:25:20 kid1| Error negotiating SSL connection on FD 12: error:00000001:lib(0):func(0):reason(1) (1/0)

The squid.conf looks like this:

#acl localnet src 10.0.0.0/8
visible_hostname centos-squid-4.1

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT


acl allowed_http_sites dstdomain .microsoft.com
acl allowed_http_sites dstdomain .google.com
acl allowed_http_sites dstdomain .redhat.com


http_access allow allowed_http_sites Safe_ports
http_port 3128
http_port 3129 intercept

#SSL Settings
acl allowed_https_sites dstdomain .microsoft.com

http_access allow CONNECT allowed_https_sites
options=SINGLE_DH_USE,SINGLE_ECDH_USE tls-dh=/etc/squid/dhparam.pem
https_port 3130 intercept ssl-bump generate-host-certificates=on cert=/etc/squid/fredrik_cert/squid.pem key=/etc/squid/fredrik_cert/squid.key options=SINGLE_DH_USE,SINGLE_ECDH_USE tls-dh=/etc/squid/dhparam
.pem

# And finally deny all other access to this proxy
http_access deny all
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
#refresh_pattern .


acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate step2 all

cache_mem 1024 MB
sslcrtd_program /usr/lib64/squid/security_file_certgen -s /var/spool/squid/ssl_db -M 16MB
sslcrtd_children 10
ssl_bump bump all

Please assist me!
Nick

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191120/7d8a2bfb/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov 20 16:43:49 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Nov 2019 11:43:49 -0500
Subject: [squid-users] squid 4.1 transparent https issue "curl: (60) SSL
 certificate problem: self signed certificate in certificate chain"
In-Reply-To: <HE1PR0401MB265128E2A54F6D48573F86E4F84F0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
References: <HE1PR0401MB265128E2A54F6D48573F86E4F84F0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
Message-ID: <27c28ff5-043c-7fd3-876e-3d262382f9cf@measurement-factory.com>

On 11/20/19 3:31 AM, Berger J Nicklas wrote:

> squid 4.1

Start by upgrading to the latest Squid v4 available.


> curl: (60) SSL certificate problem: self signed certificate in
> certificate chain

What was Squid trying to tell curl? Was Squid sending an error response?
Tell curl to run --insecure to find out what happened.


> security_file_certgen helper database '/var/spool/squid/ssl_db' failed:
> Failed to open file /var/spool/squid/ssl_db/index.txt

You should fix this. Perhaps you did not initialize the database (see
"man security_file_certgen")? Or perhaps the permissions are wrong
(checks them using something like "ls -Rla /var/spool/squid/ssl_db")?

> acl allowed_http_sites dstdomain .microsoft.com
> acl allowed_http_sites dstdomain .google.com
> acl allowed_http_sites dstdomain .redhat.com

> http_access allow allowed_http_sites Safe_ports

This allows CONNECT to port 80, which is probably not what you want. See
squid.conf.default for the recommended layout of https_access rules.


> #SSL Settings
> acl allowed_https_sites dstdomain .microsoft.com

Do not add one site twice.


> http_access allow CONNECT allowed_https_sites

This allows CONNECT to any port of the allowed_https_sites. See
squid.conf.default for the recommended layout of https_access rules.


> options=SINGLE_DH_USE,SINGLE_ECDH_USE tls-dh=/etc/squid/dhparam.pem

A copy-paste typo? There is no "options=..." directive.


> http_access deny all
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

The last two lines are unreachable. You probably want to review how
http_access (and most other) ACL-driven directives work, including the
"first match ends the search" rule.

> ssl_bump peek step1 all
> ssl_bump peek step2 allowed_https_sites
> ssl_bump splice step2 allowed_https_sites
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate step2 all
> ssl_bump bump all

To learn how ssl_bump rules work, please see
https://wiki.squid-cache.org/Features/SslPeekAndSplice

AFAICT, the above rules are equivalent to:

  ssl_bump peek step1
  ssl_bump peek step2 allowed_https_sites
  ssl_bump terminate step2
  ssl_bump splice all

or, roughly speaking, "splice allowed_https_sites (after peeking at
their server) and terminate everything else (ASAP)"

... which is rather different from what the original rules may have
tried to accomplish (whatever that was).


HTH,

Alex.


From jimoe at sohnen-moe.com  Wed Nov 20 18:23:41 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Wed, 20 Nov 2019 11:23:41 -0700
Subject: [squid-users] Changing the time format for access_log
In-Reply-To: <387ae115-835a-493f-295b-c411b0c822aa@treenet.co.nz>
References: <sig.0226c53421.69da5304-2de2-03b6-20f8-d2a329cde807@sohnen-moe.com>
 <7c3ed523-082f-1055-a133-3bc9af5c91aa@measurement-factory.com>
 <387ae115-835a-493f-295b-c411b0c822aa@treenet.co.nz>
Message-ID: <sig.0227716af6.66c65a49-c1ca-3877-ce04-431a6b1e7f4d@sohnen-moe.com>

On 2019-11-19 10:12 PM, Amos Jeffries wrote:

> IIRC, modern
> Squids support a natural position for such parameters -- after the
> %code. Here is an untested example:
>
>  %tl{%Y-%m-%dT%H:%M:%S}
>
  Thank you. That works quite nicely.

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191120/08be2944/attachment.sig>

From tannmann at gmail.com  Wed Nov 20 21:37:43 2019
From: tannmann at gmail.com (tannmann)
Date: Wed, 20 Nov 2019 15:37:43 -0600 (CST)
Subject: [squid-users] Problem with
 ssl_choose_client_version:inappropriate fallback on some sites when using
 TLS1.2
In-Reply-To: <CAAOXCCeWVD_zVbd8gc2aaynbbw23ACycPA+Bi3wetp2Cg1sLXg@mail.gmail.com>
References: <CAAOXCCd_i8hM3+Lp0om9isJVPPqpCVmBD1XhUiimCX_9yYjNxA@mail.gmail.com>
 <02b683f8-454b-552a-3845-a6daa0f90055@treenet.co.nz>
 <CAAOXCCeWVD_zVbd8gc2aaynbbw23ACycPA+Bi3wetp2Cg1sLXg@mail.gmail.com>
Message-ID: <1574285863898-0.post@n4.nabble.com>

Hey John,

It looks like we have a very similar setup and configuration as you, and we
are experiencing the same problem. Have you been able to figure out a way to
get connections to google to work with Squid 4.8 as a transparent proxy?

Thanks,

Tanner



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eype69 at gmail.com  Wed Nov 20 23:09:20 2019
From: eype69 at gmail.com (John Sweet-Escott)
Date: Wed, 20 Nov 2019 23:09:20 +0000
Subject: [squid-users] Problem with
	ssl_choose_client_version:inappropriate fallback on some
	sites when using TLS1.2
In-Reply-To: <1574285863898-0.post@n4.nabble.com>
References: <1574285863898-0.post@n4.nabble.com>
Message-ID: <B3ED8644-7E7E-47F3-BE2D-CA658353B06E@gmail.com>

Hi Tanner

Unfortunately not. We have tried everything we can think of, plus suggested items from this list, with no success. If you figure it out let me know. 

Many thanks 
John 

Sent from my iPhone

> On 20 Nov 2019, at 21:34, tannmann <tannmann at gmail.com> wrote:
> 
> ?Hey John,
> 
> It looks like we have a very similar setup and configuration as you, and we
> are experiencing the same problem. Have you been able to figure out a way to
> get connections to google to work with Squid 4.8 as a transparent proxy?
> 
> Thanks,
> 
> Tanner
> 
> 
> 
> --
> Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From nicklas.berger at scania.com  Thu Nov 21 08:21:57 2019
From: nicklas.berger at scania.com (Berger J Nicklas)
Date: Thu, 21 Nov 2019 08:21:57 +0000
Subject: [squid-users] squid 4.1 transparent https issue "curl: (60) SSL
 certificate problem: self signed certificate in certificate chain"
In-Reply-To: <27c28ff5-043c-7fd3-876e-3d262382f9cf@measurement-factory.com>
References: <HE1PR0401MB265128E2A54F6D48573F86E4F84F0@HE1PR0401MB2651.eurprd04.prod.outlook.com>,
 <27c28ff5-043c-7fd3-876e-3d262382f9cf@measurement-factory.com>
Message-ID: <HE1PR0401MB2651DA06D0A789EB75BC24E6F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>

A colleague provided this squid.conf and now https working fine with curl as well!

visible_hostname localhost

# Handling HTTP requests
http_port 3128
http_port 3129 intercept

acl allowed_http_sites dstdomain .microsoft.com
acl allowed_http_sites dstdomain .google.com
acl allowed_http_sites dstdomain .redhat.com


http_access allow allowed_http_sites

# Handling HTTPS requests
acl SSL_port port 443
http_access allow SSL_port

acl allowed_https_sites ssl::server_name .microsoft.com
acl allowed_https_sites ssl::server_name .google.com
acl allowed_https_sites ssl::server_name .redhat.com

https_port 3130 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate

http_access deny all
________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Alex Rousskov <rousskov at measurement-factory.com>
Sent: Wednesday, November 20, 2019 17:43
To: squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] squid 4.1 transparent https issue "curl: (60) SSL certificate problem: self signed certificate in certificate chain"

On 11/20/19 3:31 AM, Berger J Nicklas wrote:

> squid 4.1

Start by upgrading to the latest Squid v4 available.


> curl: (60) SSL certificate problem: self signed certificate in
> certificate chain

What was Squid trying to tell curl? Was Squid sending an error response?
Tell curl to run --insecure to find out what happened.


> security_file_certgen helper database '/var/spool/squid/ssl_db' failed:
> Failed to open file /var/spool/squid/ssl_db/index.txt

You should fix this. Perhaps you did not initialize the database (see
"man security_file_certgen")? Or perhaps the permissions are wrong
(checks them using something like "ls -Rla /var/spool/squid/ssl_db")?

> acl allowed_http_sites dstdomain .microsoft.com
> acl allowed_http_sites dstdomain .google.com
> acl allowed_http_sites dstdomain .redhat.com

> http_access allow allowed_http_sites Safe_ports

This allows CONNECT to port 80, which is probably not what you want. See
squid.conf.default for the recommended layout of https_access rules.


> #SSL Settings
> acl allowed_https_sites dstdomain .microsoft.com

Do not add one site twice.


> http_access allow CONNECT allowed_https_sites

This allows CONNECT to any port of the allowed_https_sites. See
squid.conf.default for the recommended layout of https_access rules.


> options=SINGLE_DH_USE,SINGLE_ECDH_USE tls-dh=/etc/squid/dhparam.pem

A copy-paste typo? There is no "options=..." directive.


> http_access deny all
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

The last two lines are unreachable. You probably want to review how
http_access (and most other) ACL-driven directives work, including the
"first match ends the search" rule.

> ssl_bump peek step1 all
> ssl_bump peek step2 allowed_https_sites
> ssl_bump splice step2 allowed_https_sites
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate step2 all
> ssl_bump bump all

To learn how ssl_bump rules work, please see
https://wiki.squid-cache.org/Features/SslPeekAndSplice

AFAICT, the above rules are equivalent to:

  ssl_bump peek step1
  ssl_bump peek step2 allowed_https_sites
  ssl_bump terminate step2
  ssl_bump splice all

or, roughly speaking, "splice allowed_https_sites (after peeking at
their server) and terminate everything else (ASAP)"

... which is rather different from what the original rules may have
tried to accomplish (whatever that was).


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/ef12b2bc/attachment.htm>

From nicklas.berger at scania.com  Thu Nov 21 09:16:17 2019
From: nicklas.berger at scania.com (Berger J Nicklas)
Date: Thu, 21 Nov 2019 09:16:17 +0000
Subject: [squid-users] yum update fails when using squid even though
 .redhat.com is whitelisted
Message-ID: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>

We are using squid for both http and https whitelisting for egress. Most of the whitelisting works fine but some specific once do not work.
We have tried this on this versions of squid 3.5(amazon linux 2), 4.1(centos7) and 4.4(centos8).
For instance when running yum update for redhat linux in aws from a server using squid for egress it fails:

ec2-user]# yum update -v
Failed to set locale, defaulting to C
Loaded plugins: AmazonID, builddep, changelog, config-manager, copr, debug, debuginfo-install, download, generate_completion_cache, needs-restarting, playground, repoclosure, repodiff, repograph, repomanage, reposync, uploadprofile
DNF version: 4.0.9
cachedir: /var/cache/dnf
repo: downloading from remote: rhui-client-config-server-8
error: Curl error (60): Peer certificate cannot be authenticated with given CA certificates for https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os [SSL certificate problem: self signed certificate in certificate chain] (https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os).
Red Hat Update Infrastructure 3 Client Configuration Server 8                                                                                                                0.0  B/s |   0  B     00:01
Cannot download 'https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os': Cannot prepare internal mirrorlist: Curl error (60): Peer certificate cannot be authenticated with given CA certificates for https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os [SSL certificate problem: self signed certificate in certificate chain].
Error: Failed to synchronize cache for repo 'rhui-client-config-server-8'

If I run curl against this URL:

ec2-user]# curl -v https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os
*   Trying 13.53.105.186...
* TCP_NODELAY set
* Connected to rhui3.eu-north-1.aws.ce.redhat.com (13.53.105.186) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self signed certificate in certificate chain
* Closing connection 0
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.

Curl against https://www.redhat.com works fine.

ec2-user]# curl -v https://www.redhat.com
* Rebuilt URL to: https://www.redhat.com/
*   Trying 23.52.28.149...
* TCP_NODELAY set
* Connected to www.redhat.com (23.52.28.149) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: businessCategory=Private Organization; jurisdictionC=US; jurisdictionST=Delaware; serialNumber=2945436; C=US; ST=North Carolina; L=Raleigh; O=Red Hat, Inc.; OU=IT; CN=www.redhat.com
*  start date: Mar 21 00:00:00 2018 GMT
*  expire date: Mar 20 12:00:00 2020 GMT
*  subjectAltName: host "www.redhat.com" matched cert's "www.redhat.com"
*  issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=DigiCert SHA2 Extended Validation Server CA
*  SSL certificate verify ok.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x56153e589630)
> GET / HTTP/2
> Host: www.redhat.com
> User-Agent: curl/7.61.1
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS == 100)!
< HTTP/2 301
< server: AkamaiGHost
< content-length: 0
< location: https://www.redhat.com/en
< date: Thu, 21 Nov 2019 08:47:55 GMT
<
* Connection #0 to host www.redhat.com left intact

My squid.conf looks like this:

visible_hostname localhost

# Handling HTTP requests
http_port 3128
http_port 3129 intercept

acl allowed_http_sites dstdomain .microsoft.com
acl allowed_http_sites dstdomain .google.com
acl allowed_http_sites dstdomain .redhat.com


http_access allow allowed_http_sites

# Handling HTTPS requests
acl SSL_port port 443
http_access allow SSL_port

acl allowed_https_sites ssl::server_name .microsoft.com
acl allowed_https_sites ssl::server_name .google.com
acl allowed_https_sites ssl::server_name .redhat.com

https_port 3130 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate

http_access deny all


I assume this is related to that there is no certificate for this subdomain or similar? Is there a way to ignore this for ".redhat.com" or get yum update to work anyway?

// Nick








-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/9c1123bd/attachment.htm>

From giles at coochey.net  Thu Nov 21 09:31:30 2019
From: giles at coochey.net (Giles Coochey)
Date: Thu, 21 Nov 2019 09:31:30 +0000
Subject: [squid-users] yum update fails when using squid even though
 .redhat.com is whitelisted
In-Reply-To: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
References: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
Message-ID: <f7f44bdd-f3c2-c87e-1392-526e34f38e6e@coochey.net>


On 21/11/2019 09:16, Berger J Nicklas wrote:
> We are using squid for both http and https whitelisting for egress. 
> Most of the whitelisting works fine but some specific once do not work.
> We have tried this on this versions of squid 3.5(amazon linux 2), 
> 4.1(centos7) and 4.4(centos8).
> For instance when running yum update for redhat linux in aws from a 
> server using squid for egress it fails:
>
> ec2-user]# yum update -v
> *Failed to set locale, defaulting to C
> *
> *Loaded plugins: AmazonID, builddep, changelog, config-manager, copr, 
> debug, debuginfo-install, download, generate_completion_cache, 
> needs-restarting, playground, repoclosure, repodiff, repograph, 
> repomanage, reposync, uploadprofile
> *
> *DNF version: 4.0.9
> *
> *cachedir: /var/cache/dnf
> *
> *repo: downloading from remote: rhui-client-config-server-8
> *
> *error: Curl error (60): Peer certificate cannot be authenticated with 
> given CA certificates for 
> https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os 
> [SSL certificate problem: self signed certificate in certificate 
> chain] 
> (https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os).
> *
> *Red Hat Update Infrastructure 3 Client Configuration Server 8 ? ? ? ? 
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0.0 ?B/s | ? 0 ?B ? ? 00:01
> *
> *Cannot download 
> 'https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os': 
> Cannot prepare internal mirrorlist: Curl error (60): Peer certificate 
> cannot be authenticated with given CA certificates for 
> https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os 
> [SSL certificate problem: self signed certificate in certificate chain].
> *
> *Error: Failed to synchronize cache for repo 
> 'rhui-client-config-server-8'*
>
The problem has nothing to do with Squid, 
https://rhui3.eu-north-1.aws.ce.redhat.com is indeed using a self-signed 
certificate.


You could add that cert to CA trust in your system, once you have 
verified the authenticity.


-- 
Giles Coochey

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/2a68ae78/attachment.htm>

From bariq.kassir at scania.com  Thu Nov 21 12:51:01 2019
From: bariq.kassir at scania.com (Kassir Bariq)
Date: Thu, 21 Nov 2019 12:51:01 +0000
Subject: [squid-users] yum update fails when using squid even though
 .redhat.com is whitelisted
In-Reply-To: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
References: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
Message-ID: <HE1PR0402MB274751FBE9BDCF83CE0E6051F44E0@HE1PR0402MB2747.eurprd04.prod.outlook.com>

Hi,

You can add this line in your squid.conf

                    sslproxy_cert_error allow allowed_https_sites
this should fix your issue to bypass sites without a valid certificate.

Regards
Bariq

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Berger J Nicklas
Sent: den 21 november 2019 10:16
To: squid-users at lists.squid-cache.org
Subject: [squid-users] yum update fails when using squid even though .redhat.com is whitelisted

We are using squid for both http and https whitelisting for egress. Most of the whitelisting works fine but some specific once do not work.
We have tried this on this versions of squid 3.5(amazon linux 2), 4.1(centos7) and 4.4(centos8).
For instance when running yum update for redhat linux in aws from a server using squid for egress it fails:

ec2-user]# yum update -v
Failed to set locale, defaulting to C
Loaded plugins: AmazonID, builddep, changelog, config-manager, copr, debug, debuginfo-install, download, generate_completion_cache, needs-restarting, playground, repoclosure, repodiff, repograph, repomanage, reposync, uploadprofile
DNF version: 4.0.9
cachedir: /var/cache/dnf
repo: downloading from remote: rhui-client-config-server-8
error: Curl error (60): Peer certificate cannot be authenticated with given CA certificates for https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os [SSL certificate problem: self signed certificate in certificate chain] (https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os).
Red Hat Update Infrastructure 3 Client Configuration Server 8                                                                                                                0.0  B/s |   0  B     00:01
Cannot download 'https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os': Cannot prepare internal mirrorlist: Curl error (60): Peer certificate cannot be authenticated with given CA certificates for https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os [SSL certificate problem: self signed certificate in certificate chain].
Error: Failed to synchronize cache for repo 'rhui-client-config-server-8'

If I run curl against this URL:

ec2-user]# curl -v https://rhui3.eu-north-1.aws.ce.redhat.com/pulp/mirror/protected/rhui-client-config/rhel/server/8/x86_64/os
*   Trying 13.53.105.186...
* TCP_NODELAY set
* Connected to rhui3.eu-north-1.aws.ce.redhat.com (13.53.105.186) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self signed certificate in certificate chain
* Closing connection 0
curl: (60) SSL certificate problem: self signed certificate in certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.

Curl against https://www.redhat.com works fine.

ec2-user]# curl -v https://www.redhat.com
* Rebuilt URL to: https://www.redhat.com/
*   Trying 23.52.28.149...
* TCP_NODELAY set
* Connected to www.redhat.com<http://www.redhat.com> (23.52.28.149) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: businessCategory=Private Organization; jurisdictionC=US; jurisdictionST=Delaware; serialNumber=2945436; C=US; ST=North Carolina; L=Raleigh; O=Red Hat, Inc.; OU=IT; CN=www.redhat.com
*  start date: Mar 21 00:00:00 2018 GMT
*  expire date: Mar 20 12:00:00 2020 GMT
*  subjectAltName: host "www.redhat.com<http://www.redhat.com>" matched cert's "www.redhat.com<http://www.redhat.com>"
*  issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=DigiCert SHA2 Extended Validation Server CA
*  SSL certificate verify ok.
* Using HTTP2, server supports multi-use
* Connection state changed (HTTP/2 confirmed)
* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0
* Using Stream ID: 1 (easy handle 0x56153e589630)
> GET / HTTP/2
> Host: www.redhat.com<http://www.redhat.com>
> User-Agent: curl/7.61.1
> Accept: */*
>
* Connection state changed (MAX_CONCURRENT_STREAMS == 100)!
< HTTP/2 301
< server: AkamaiGHost
< content-length: 0
< location: https://www.redhat.com/en
< date: Thu, 21 Nov 2019 08:47:55 GMT
<
* Connection #0 to host www.redhat.com<http://www.redhat.com> left intact

My squid.conf looks like this:

visible_hostname localhost

# Handling HTTP requests
http_port 3128
http_port 3129 intercept

acl allowed_http_sites dstdomain .microsoft.com
acl allowed_http_sites dstdomain .google.com
acl allowed_http_sites dstdomain .redhat.com


http_access allow allowed_http_sites

# Handling HTTPS requests
acl SSL_port port 443
http_access allow SSL_port

acl allowed_https_sites ssl::server_name .microsoft.com
acl allowed_https_sites ssl::server_name .google.com
acl allowed_https_sites ssl::server_name .redhat.com

https_port 3130 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/squid/ssl/squid.pem key=/etc/squid/ssl/squid.key

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate

http_access deny all


I assume this is related to that there is no certificate for this subdomain or similar? Is there a way to ignore this for ".redhat.com" or get yum update to work anyway?

// Nick








-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/405addb3/attachment.htm>

From monahbaki at gmail.com  Thu Nov 21 14:25:28 2019
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 21 Nov 2019 09:25:28 -0500
Subject: [squid-users] Squid and SSLBump
Message-ID: <CALP3=x-hi=het4ifP6WYJQWU3n9fM_CZYChvFGa213aQ6Sd_aA@mail.gmail.com>

Hi all,

I'm trying to configure my Centos 7 running:
Squid Cache: Version 3.5.28
configure options:  '--with-openssl' '--enable-ssl-crtd'
--enable-ltdl-convenience

The certs/keys are legit from my company.

My squid.conf is very simple since it's for proof of concept

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

# Squid normally listens to port 3128
http_port 172.16.84.242:3128 ssl-bump \
  cert=/etc/squid/certs/wildcardcert.pem \
  key=/etc/squid/certs/wildcardkey.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
acl step1 at_step SSlBump1
ssl_bump peek step1
ssl_bump bump all
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 16MB
sslcrtd_children 32 startup=5 idle=1

cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

strip_query_terms off
# logformat squid %>a - %un [%{%d/%b/%Y:%H:%M:%S %z}tl] "%rm %ru" %Hs %st
"%{Referer}>h" "%{User-agent}>h"
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
[%>h] [%<h]
access_log  /var/log/squid/access.log squid


Browsing http sites works fine, but I am having issues with https

In my access.log I get:
1574346211.538     30 172.16.84.241 TAG_NONE/200 0 CONNECT www.cnn.com:443
- HIER_DIRECT/www.cnn.com - [User-Agent: Mozilla/5.0 (Windows NT 10.0;
WOW64; Trident/7.0; rv:11.0) like Gecko\r\nContent-Length: 0\r\nDNT:
1\r\nProxy-Connection: Keep-Alive\r\nPragma: no-cache\r\nHost:
www.cnn.com:443\r\n] [-]


In Internet explorer I get the following:

Certificate Error: Navigation Blocked
There is a problem with this website?s security certificate.


The security certificate presented by this website is not secure.

Security certificate problems may indicate an attempt to fool you or
intercept any data you send to the server.
  *We recommend that you close this webpage and do not continue to this
website.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/ce959f22/attachment.htm>

From giles at coochey.net  Thu Nov 21 16:29:56 2019
From: giles at coochey.net (Giles Coochey)
Date: Thu, 21 Nov 2019 16:29:56 +0000
Subject: [squid-users] yum update fails when using squid even though
 .redhat.com is whitelisted
In-Reply-To: <HE1PR0402MB274751FBE9BDCF83CE0E6051F44E0@HE1PR0402MB2747.eurprd04.prod.outlook.com>
References: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
 <HE1PR0402MB274751FBE9BDCF83CE0E6051F44E0@HE1PR0402MB2747.eurprd04.prod.outlook.com>
Message-ID: <ce2d3c66-500b-4a10-605c-64e6cd63512c@coochey.net>


On 21/11/2019 12:51, Kassir Bariq wrote:
>
> Hi,
>
> You can add this line in your squid.conf
>
> sslproxy_cert_error allow allowed_https_sites
>
> this should fix your issue to bypass sites without a valid certificate.
>
>
I probably wouldn't do this blindly, either use a different acl such as 
known_broken_cert_sites and add sites that you have trouble with to that 
ACL.

I believe Palo Alto and Bluecoats have a feature mechanism to provide 
the client with an appropriately broken cert , e.g. if the cert is 
expired, but has a trusted chain then it uses an expired cert with a 
trusted chain to the client, and if a cert is self signed, then it sends 
a self-signed cert to the client.

I don't know whether Squid also has that mechanism, but would probably 
be preferred.

-- 
Giles Coochey

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/75185f5a/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov 21 18:10:30 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Nov 2019 13:10:30 -0500
Subject: [squid-users] yum update fails when using squid even though
 .redhat.com is whitelisted
In-Reply-To: <ce2d3c66-500b-4a10-605c-64e6cd63512c@coochey.net>
References: <HE1PR0401MB265194BFF34806DF0C00D103F84E0@HE1PR0401MB2651.eurprd04.prod.outlook.com>
 <HE1PR0402MB274751FBE9BDCF83CE0E6051F44E0@HE1PR0402MB2747.eurprd04.prod.outlook.com>
 <ce2d3c66-500b-4a10-605c-64e6cd63512c@coochey.net>
Message-ID: <2330570f-36eb-af55-731b-0bab3a49c824@measurement-factory.com>

On 11/21/19 11:29 AM, Giles Coochey wrote:

> I believe Palo Alto and Bluecoats have a feature mechanism to provide
> the client with an appropriately broken cert , e.g. if the cert is
> expired, but has a trusted chain then it uses an expired cert with a
> trusted chain to the client, and if a cert is self signed, then it sends
> a self-signed cert to the client.

> I don't know whether Squid also has that mechanism

Yes, Squid also tries to mimic various aspects of origin server
certificate brokenness. Unfortunately, I do not think there is a wiki
table that fully documents which problems are mimicked by default, and I
do not remember all of the specifics. It would be great if somebody
would build such a table (e.g., by observing what Squid does with broken
certificates provided by various TLS testing web sites/services).

Alex.


From rousskov at measurement-factory.com  Thu Nov 21 18:18:18 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Nov 2019 13:18:18 -0500
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <CALP3=x-hi=het4ifP6WYJQWU3n9fM_CZYChvFGa213aQ6Sd_aA@mail.gmail.com>
References: <CALP3=x-hi=het4ifP6WYJQWU3n9fM_CZYChvFGa213aQ6Sd_aA@mail.gmail.com>
Message-ID: <76d7e813-d982-bd91-2e2c-9ddfae901c4f@measurement-factory.com>

On 11/21/19 9:25 AM, Monah Baki wrote:

> The certs/keys are legit from my company.

Is your signing certificate (i.e. wildcardcert.pem) a CA certificate? If
not, then you cannot use it to sign other certificates. SslBump with
dynamic certificate generation requires a CA certificate to sign the
generated certificates.

CA certificates have a "true" CA basic constraint:

    $ openssl x509 -in wildcardcert.pem -noout -text | \
      grep -A1 'Basic Constraints'
                X509v3 Basic Constraints:
                   CA:TRUE


If they are CA certificates, did you import them into the browser/OS
trusted certificates store? In most environments, a browser will not. by
default, trust a CA certificate that Squid can use to sign dynamically
generated certificates.

Alex.


> My squid.conf is very simple since it's for proof of concept
> 
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8> ? ? # RFC1918 possible
> internal network
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12> ?# RFC1918
> possible internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> # RFC1918
> possible internal network
> acl SSL_ports port 443
> acl Safe_ports port 80 ? ? ? ? ?# http
> acl Safe_ports port 21 ? ? ? ? ?# ftp
> acl Safe_ports port 443 ? ? ? ? # https
> acl Safe_ports port 70 ? ? ? ? ?# gopher
> acl Safe_ports port 210 ? ? ? ? # wais
> acl Safe_ports port 1025-65535 ?# unregistered ports
> acl Safe_ports port 280 ? ? ? ? # http-mgmt
> acl Safe_ports port 488 ? ? ? ? # gss-http
> acl Safe_ports port 591 ? ? ? ? # filemaker
> acl Safe_ports port 777 ? ? ? ? # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 172.16.84.242:3128 <http://172.16.84.242:3128> ssl-bump \
> ? cert=/etc/squid/certs/wildcardcert.pem \
> ? key=/etc/squid/certs/wildcardkey.pem \
> ? generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
> acl step1 at_step SSlBump1
> ssl_bump peek step1
> ssl_bump bump all
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 16MB
> sslcrtd_children 32 startup=5 idle=1
> 
> cache_dir ufs /var/spool/squid 100 16 256
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp: ? ? ? ? ? 1440 ? ?20% ? ? 10080
> refresh_pattern ^gopher: ? ? ? ?1440 ? ?0% ? ? ?1440
> refresh_pattern -i (/cgi-bin/|\?) 0 ? ? 0% ? ? ?0
> refresh_pattern . ? ? ? ? ? ? ? 0 ? ? ? 20% ? ? 4320
> 
> strip_query_terms off
> # logformat squid %>a - %un [%{%d/%b/%Y:%H:%M:%S %z}tl] "%rm %ru" %Hs
> %st "%{Referer}>h" "%{User-agent}>h"
> logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A
> %mt [%>h] [%<h]
> access_log ?/var/log/squid/access.log squid
> 
> 
> Browsing http sites works fine, but I am having issues with https
> 
> In my access.log I get:
> 1574346211.538 ? ? 30 172.16.84.241 TAG_NONE/200 0 CONNECT
> www.cnn.com:443 <http://www.cnn.com:443> - HIER_DIRECT/www.cnn.com
> <http://www.cnn.com> - [User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64;
> Trident/7.0; rv:11.0) like Gecko\r\nContent-Length: 0\r\nDNT:
> 1\r\nProxy-Connection: Keep-Alive\r\nPragma: no-cache\r\nHost:
> www.cnn.com:443 <http://www.cnn.com:443>\r\n] [-]
> 
> 
> In Internet explorer I get the following:
> 
> Certificate Error: Navigation Blocked
> 
> 
>   There is a problem with this website?s security certificate.
> 
> 
> ?
> 	
> 
> 
>       The security certificate presented by this website is not secure.
> 
>       Security certificate problems may indicate an attempt to fool you
>       or intercept any data you send to the server.	
> 
> ?	
> 
> 
>     *We recommend that you close this webpage and do not continue to
>     this website.*
> 
> 
>     *
>     *
> 
> 
>     *?*
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From monahbaki at gmail.com  Thu Nov 21 20:19:46 2019
From: monahbaki at gmail.com (Monah Baki)
Date: Thu, 21 Nov 2019 15:19:46 -0500
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <76d7e813-d982-bd91-2e2c-9ddfae901c4f@measurement-factory.com>
References: <CALP3=x-hi=het4ifP6WYJQWU3n9fM_CZYChvFGa213aQ6Sd_aA@mail.gmail.com>
 <76d7e813-d982-bd91-2e2c-9ddfae901c4f@measurement-factory.com>
Message-ID: <CALP3=x8S8WFv7NqqNGHyxiVAU5HX3Dr35RvHwdKAMOUBqBt9UQ@mail.gmail.com>

I added the following:

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

and it works now.

In my access.log:

172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT
static.xx.fbcdn.net:443" 200 4199 "-" "Mozilla/5.0 (Windows NT 10.0; WOW64;
Trident/7.0; rv:11.0) like Gecko"
172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT fbcdn.net:443" 200
5431 "-" "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like
Gecko"
172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT fbsbx.com:443" 200
5439 "-" "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like
Gecko"
172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT
connect.facebook.net:443" 200 6085 "-" "Mozilla/5.0 (Windows NT 10.0;
WOW64; Trident/7.0; rv:11.0) like Gecko"
172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT www.cnn.com:443"
200 155123 "-" "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0)
like Gecko"


So since I am new to sslbump, what am I benefiting from this? Will I be
able to see unencrypted data?

Thanks


On Thu, Nov 21, 2019 at 1:18 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/21/19 9:25 AM, Monah Baki wrote:
>
> > The certs/keys are legit from my company.
>
> Is your signing certificate (i.e. wildcardcert.pem) a CA certificate? If
> not, then you cannot use it to sign other certificates. SslBump with
> dynamic certificate generation requires a CA certificate to sign the
> generated certificates.
>
> CA certificates have a "true" CA basic constraint:
>
>     $ openssl x509 -in wildcardcert.pem -noout -text | \
>       grep -A1 'Basic Constraints'
>                 X509v3 Basic Constraints:
>                    CA:TRUE
>
>
> If they are CA certificates, did you import them into the browser/OS
> trusted certificates store? In most environments, a browser will not. by
> default, trust a CA certificate that Squid can use to sign dynamically
> generated certificates.
>
> Alex.
>
>
> > My squid.conf is very simple since it's for proof of concept
> >
> > acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>     # RFC1918 possible
> > internal network
> > acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  # RFC1918
> > possible internal network
> > acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> # RFC1918
> > possible internal network
> > acl SSL_ports port 443
> > acl Safe_ports port 80          # http
> > acl Safe_ports port 21          # ftp
> > acl Safe_ports port 443         # https
> > acl Safe_ports port 70          # gopher
> > acl Safe_ports port 210         # wais
> > acl Safe_ports port 1025-65535  # unregistered ports
> > acl Safe_ports port 280         # http-mgmt
> > acl Safe_ports port 488         # gss-http
> > acl Safe_ports port 591         # filemaker
> > acl Safe_ports port 777         # multiling http
> > acl CONNECT method CONNECT
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > http_access allow localhost manager
> > http_access deny manager
> > http_access allow localnet
> > http_access allow localhost
> > http_access deny all
> >
> > # Squid normally listens to port 3128
> > http_port 172.16.84.242:3128 <http://172.16.84.242:3128> ssl-bump \
> >   cert=/etc/squid/certs/wildcardcert.pem \
> >   key=/etc/squid/certs/wildcardkey.pem \
> >   generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
> > acl step1 at_step SSlBump1
> > ssl_bump peek step1
> > ssl_bump bump all
> > sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
> 16MB
> > sslcrtd_children 32 startup=5 idle=1
> >
> > cache_dir ufs /var/spool/squid 100 16 256
> > coredump_dir /var/spool/squid
> > refresh_pattern ^ftp:           1440    20%     10080
> > refresh_pattern ^gopher:        1440    0%      1440
> > refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> > refresh_pattern .               0       20%     4320
> >
> > strip_query_terms off
> > # logformat squid %>a - %un [%{%d/%b/%Y:%H:%M:%S %z}tl] "%rm %ru" %Hs
> > %st "%{Referer}>h" "%{User-agent}>h"
> > logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A
> > %mt [%>h] [%<h]
> > access_log  /var/log/squid/access.log squid
> >
> >
> > Browsing http sites works fine, but I am having issues with https
> >
> > In my access.log I get:
> > 1574346211.538     30 172.16.84.241 TAG_NONE/200 0 CONNECT
> > www.cnn.com:443 <http://www.cnn.com:443> - HIER_DIRECT/www.cnn.com
> > <http://www.cnn.com> - [User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64;
> > Trident/7.0; rv:11.0) like Gecko\r\nContent-Length: 0\r\nDNT:
> > 1\r\nProxy-Connection: Keep-Alive\r\nPragma: no-cache\r\nHost:
> > www.cnn.com:443 <http://www.cnn.com:443>\r\n] [-]
> >
> >
> > In Internet explorer I get the following:
> >
> > Certificate Error: Navigation Blocked
> >
> >
> >   There is a problem with this website?s security certificate.
> >
> >
> >
> >
> >
> >
> >       The security certificate presented by this website is not secure.
> >
> >       Security certificate problems may indicate an attempt to fool you
> >       or intercept any data you send to the server.
> >
> >
> >
> >
> >     *We recommend that you close this webpage and do not continue to
> >     this website.*
> >
> >
> >     *
> >     *
> >
> >
> >     * *
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191121/ec9f9daf/attachment.htm>

From chammidhan at gmail.com  Fri Nov 22 03:05:18 2019
From: chammidhan at gmail.com (Chammi Kumarapathirage)
Date: Fri, 22 Nov 2019 13:05:18 +1000
Subject: [squid-users] logformat for requests using PROXY protocol
In-Reply-To: <91eb6ee0-bcbc-0598-351b-36737d990749@treenet.co.nz>
References: <1573783005972-0.post@n4.nabble.com>
 <91eb6ee0-bcbc-0598-351b-36737d990749@treenet.co.nz>
Message-ID: <CAKZYigFFXj8nojT4YMEEV8qWh1OFV5qA3UP_yJKo7aV6RsuhFg@mail.gmail.com>

I have my logformat as follows.
logformat jsonformat {"Client Hostname":"%>A","Source IP":"%>a","HTTP Method
":"%rm","HTTP Protocol version":"%rv","Request Domain":"%>rd","Port":"%>rP",
"User Agent":"%{User-Agent}>h","Request Size":"%>st","Reply
Size":"%<st","Response
Time(ms)":"%tr","Status Code":"%>Hs","Request Status":"%Ss","Server FQDN":"
%<A"}

The proxy is sitting behind a load balancer in AWS and Proxy Protocol V2 is
enabled on both the LB and Squid. Everything seems to work fine. I can
create rules based on source IP of the client. However. I want to be able
to  create rules based on the hostname of the original client. But it
doesn't seem that Squid sees the original client's hostname. Rather it
takes the hostname of the LB as seen by below log.

{ "Client Hostname": "ip-10-181-3-213.ap-southeast-2.compute.internal", "Source
IP": "10.181.3.10", "HTTP Method": "CONNECT", "HTTP Protocol version": "1.1",
"Request Domain": "clientservices.googleapis.com", "Port": "443", "User
Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36", "Request Size":
"253", "Reply Size": "4138", "Response Time(ms)": "0", "Status Code": "403",
"Request Status": "TCP_DENIED", "Server FQDN": "-" }

On Fri, Nov 15, 2019 at 3:15 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 15/11/19 2:56 pm, chammidhan wrote:
> > I have configured a Squid ECS cluster behind a network load balancer in
> AWS.
> > To reflect the original client IP, I needed to enable PROXY Protocol V2
> on
> > the load balancer. The service itself is working fine and I can create
> rules
> > based on the original client IP and these are applied as expected.
> However,
> > it doesn't seem that logformat format codes are working as expected. No
> > matter how I format the logs, I'm always seeing the logs in the same
> format.
> > Which looks like below.
> >
> > 1573771498.693 240116 10.181.3.10 TCP_TUNNEL/200 1742 CONNECT
> > id.google.com:443 - HIER_DIRECT/172.217.167.67 -
> >
> > My logformat directive is the default
> > logformat squid %{%Y/%m/%d-%H:%M:%S}tl %>A/%>a %un %rm/%rv %ru %mt
> > %{User-Agent}>h %>st/%<st %tr %>Hs %Ss %Sh/%<A
> >
> > Appreciate any insight to what I may be doing wrong. Things were working
> > fine before enabling PROXY protocol on the NLB
> >
>
> Please run "squid -k parse" on your config and fix the errors and
> warnings it produces.
>
> "
> 2019/11/15 18:11:50| Processing: logformat squid %{%Y/%m/%d-...
> 2019/11/15 18:11:50| ERROR: logformat squid is already defined. Ignoring.
> "
>
> To use a custom log format you need a custom name.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191122/e97c68ce/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 22 08:43:29 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Nov 2019 21:43:29 +1300
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <CALP3=x8S8WFv7NqqNGHyxiVAU5HX3Dr35RvHwdKAMOUBqBt9UQ@mail.gmail.com>
References: <CALP3=x-hi=het4ifP6WYJQWU3n9fM_CZYChvFGa213aQ6Sd_aA@mail.gmail.com>
 <76d7e813-d982-bd91-2e2c-9ddfae901c4f@measurement-factory.com>
 <CALP3=x8S8WFv7NqqNGHyxiVAU5HX3Dr35RvHwdKAMOUBqBt9UQ@mail.gmail.com>
Message-ID: <c851b92a-1ef9-fd58-8559-792926012c3c@treenet.co.nz>

On 22/11/19 9:19 am, Monah Baki wrote:
> I added the following:
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> 
> and it works now.
> 
> In my access.log:
> 
> 172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT
> static.xx.fbcdn.net:443 <http://static.xx.fbcdn.net:443>" 200 4199 "-"
> "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko"
> 172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT fbcdn.net:443
> <http://fbcdn.net:443>" 200 5431 "-" "Mozilla/5.0 (Windows NT 10.0;
> WOW64; Trident/7.0; rv:11.0) like Gecko"
> 172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT fbsbx.com:443
> <http://fbsbx.com:443>" 200 5439 "-" "Mozilla/5.0 (Windows NT 10.0;
> WOW64; Trident/7.0; rv:11.0) like Gecko"
> 172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT
> connect.facebook.net:443 <http://connect.facebook.net:443>" 200 6085 "-"
> "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko"
> 172.16.84.241 - - [21/Nov/2019:15:15:05 -0500] "CONNECT www.cnn.com:443
> <http://www.cnn.com:443>" 200 155123 "-" "Mozilla/5.0 (Windows NT 10.0;
> WOW64; Trident/7.0; rv:11.0) like Gecko"
> 
> 
> So since I am new to sslbump, what am I benefiting from this?

You are not benefiting. Problems the users ask you to track down with
TLS will now be hidden from your debugging attempts. Users TLS can now
be intercepted and the traffic replaced by anyone. You will not be shown
the signs of that happening since you told Squid to hide them.


> able to see unencrypted data?

No more than before. Its just that Squid will no longer attempt to
verify the certs are valid or report in logs etc about problems.
Basically your users traffic can now be intercepted by anybody, anywhere
along the Internet paths and replaced with other content - your Squid
will not report anything amiss.

Basically any TLS through your proxy is no longer secure.



In general you will always see sites having trouble with TLS. This is
normal, expected, and sometimes a *good* thing.

Change your focus to identifying *what* is failing for each site that
you want to work but fails. Sometimes it is a problem you can fix,
sometimes can be ignored (sslproxy_cert_error directive is for these).
But definitely decide what to do case-by-case instead of "allow all".


Amos


From squid3 at treenet.co.nz  Fri Nov 22 09:00:54 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Nov 2019 22:00:54 +1300
Subject: [squid-users] logformat for requests using PROXY protocol
In-Reply-To: <CAKZYigFFXj8nojT4YMEEV8qWh1OFV5qA3UP_yJKo7aV6RsuhFg@mail.gmail.com>
References: <1573783005972-0.post@n4.nabble.com>
 <91eb6ee0-bcbc-0598-351b-36737d990749@treenet.co.nz>
 <CAKZYigFFXj8nojT4YMEEV8qWh1OFV5qA3UP_yJKo7aV6RsuhFg@mail.gmail.com>
Message-ID: <26e20d7f-57e2-6627-69d9-15959612ec53@treenet.co.nz>

On 22/11/19 4:05 pm, Chammi Kumarapathirage wrote:
> I have my logformat as follows.
> logformat jsonformat {"Client Hostname":"%>A","Source IP":"%>a","HTTP
> Method":"%rm","HTTP Protocol version":"%rv","Request
> Domain":"%>rd","Port":"%>rP","User Agent":"%{User-Agent}>h","Request
> Size":"%>st","Reply Size":"%<st","Response Time(ms)":"%tr","Status
> Code":"%>Hs","Request Status":"%Ss","Server FQDN":"%<A"}?
> 
> The proxy is sitting behind a load balancer in AWS and Proxy Protocol V2
> is enabled on both the LB and Squid. Everything seems to work fine. I
> can create rules based on source IP of the client. However. I want to be
> able to? create rules based on the hostname of the original client. But
> it doesn't seem that Squid sees the original?client's hostname. Rather
> it takes the hostname of the LB as seen by below log.

The %>A log code is still tied to old logging state instead of the IP
values updated by PROXY protocol.

The only way I can see to log that value without patching Squid is with
something complicated like an external_acl_type helper to do the lookup
and supply it as a tag or note to Squid.

If you are happy to patch I can make a PR for you to try.


Amos


From chammidhan at gmail.com  Fri Nov 22 20:45:11 2019
From: chammidhan at gmail.com (Chammi Kumarapathirage)
Date: Sat, 23 Nov 2019 06:45:11 +1000
Subject: [squid-users] logformat for requests using PROXY protocol
In-Reply-To: <26e20d7f-57e2-6627-69d9-15959612ec53@treenet.co.nz>
References: <1573783005972-0.post@n4.nabble.com>
 <91eb6ee0-bcbc-0598-351b-36737d990749@treenet.co.nz>
 <CAKZYigFFXj8nojT4YMEEV8qWh1OFV5qA3UP_yJKo7aV6RsuhFg@mail.gmail.com>
 <26e20d7f-57e2-6627-69d9-15959612ec53@treenet.co.nz>
Message-ID: <CAKZYigHHW8iJpMb3FnpbkeB_dAFs_OiUbJ4dBaJJ1kQMc2hRWQ@mail.gmail.com>

Thanks for the response Amos. This is an AWS Fargate instance and I'm not
exactly sure how patching works in that space. I'm rather new to both
serverless concept and Squid. I will research this and get back to you.
Thanks!

On Friday, November 22, 2019, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 22/11/19 4:05 pm, Chammi Kumarapathirage wrote:
> > I have my logformat as follows.
> > logformat jsonformat {"Client Hostname":"%>A","Source IP":"%>a","HTTP
> > Method":"%rm","HTTP Protocol version":"%rv","Request
> > Domain":"%>rd","Port":"%>rP","User Agent":"%{User-Agent}>h","Request
> > Size":"%>st","Reply Size":"%<st","Response Time(ms)":"%tr","Status
> > Code":"%>Hs","Request Status":"%Ss","Server FQDN":"%<A"}
> >
> > The proxy is sitting behind a load balancer in AWS and Proxy Protocol V2
> > is enabled on both the LB and Squid. Everything seems to work fine. I
> > can create rules based on source IP of the client. However. I want to be
> > able to  create rules based on the hostname of the original client. But
> > it doesn't seem that Squid sees the original client's hostname. Rather
> > it takes the hostname of the LB as seen by below log.
>
> The %>A log code is still tied to old logging state instead of the IP
> values updated by PROXY protocol.
>
> The only way I can see to log that value without patching Squid is with
> something complicated like an external_acl_type helper to do the lookup
> and supply it as a tag or note to Squid.
>
> If you are happy to patch I can make a PR for you to try.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191123/e7016fba/attachment.htm>

From ithelpdesk at thesanantonioresort.com  Sat Nov 23 02:02:32 2019
From: ithelpdesk at thesanantonioresort.com (I.T. Helpdesk - San Antonio Resort)
Date: Sat, 23 Nov 2019 10:02:32 +0800
Subject: [squid-users] not getting tcp_hit on squid 3.5.27
Message-ID: <000001d5a1a2$12eaf6a0$38c0e3e0$@thesanantonioresort.com>

Hi,

 

i have setup squid 3.5.27 as proxy server, on my access log, I don't get
"tcp_hit" but "tcp_tunnel", for sure squid in working fine but it not
getting data from local disk cache. do I need to tweak on my squid conf
file?

 

Thanks.

 

 

Voltaire O. Calara II

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191123/0b00cdfd/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov 23 03:46:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 Nov 2019 16:46:43 +1300
Subject: [squid-users] not getting tcp_hit on squid 3.5.27
In-Reply-To: <000001d5a1a2$12eaf6a0$38c0e3e0$@thesanantonioresort.com>
References: <000001d5a1a2$12eaf6a0$38c0e3e0$@thesanantonioresort.com>
Message-ID: <50eb7e43-247e-82b0-21c4-65ef1807b16e@treenet.co.nz>

On 23/11/19 3:02 pm, I.T. Helpdesk - San Antonio Resort wrote:
> Hi,
> 
> i have setup squid 3.5.27 as proxy server, on my access log, I don?t get
> ?tcp_hit? but ?tcp_tunnel?, for sure squid in working fine but it not
> getting data from local disk cache. do I need to tweak on my squid conf
> file?

TUNNEL traffic is opaque binary stream, it cannot be cached.

If these tunnels are CONNECT requests to port 443, then they are
probably HTTPS and you can achieve some amount of caching by decrypting
the traffic. Up to you whether you want to do that though, you need to
be able to install CA certificate on all your clients machines.

There are legal issues too that you will need to look into before doing
anything like decrypting clients traffic.

Amos


From zebox at tutanota.com  Mon Nov 25 17:31:16 2019
From: zebox at tutanota.com (zbox)
Date: Mon, 25 Nov 2019 18:31:16 +0100 (CET)
Subject: [squid-users] ecap trickling (antivirus scan)
In-Reply-To: <8dd29887-c20b-33f1-5a93-f376d6016ee7@measurement-factory.com>
References: <Ltx_uU4--3-1@tutanota.com> <Lu2BOIR--3-1@tutanota.com>
 <8dd29887-c20b-33f1-5a93-f376d6016ee7@measurement-factory.com>
Message-ID: <LuYbW-w--3-1@tutanota.com>


Hello,

It's not downloading anything anymore.

I already have this line before your snippet :

ecap_service clamav_service_resp respmod_precache uri=ecap://e-cap.org/ecap/services/clamav?mode=RESPMOD <http://e-cap.org/ecap/services/clamav?mode=RESPMOD> bypass=on

Sould I try to merge them ?

Regards

19 nov. 2019 ? 23:41 de rousskov at measurement-factory.com:

> On 11/19/19 5:24 AM, zbox wrote:
>
>> I've made the changes, the files are now downloaded in the temp dir
>> but the behaviour is the same on the browser...
>>
>
> Glad your configuration problem got resolved.
>
> There is not enough information to figure out what else, if anything, is
> broken (and where that breakage is). However, if the adapter was
> configured to trickle, and you see a temporary download file growing in
> size _while_ Squid sends _nothing_ to the client (at HTTP level) for
> several seconds, then there is probably a bug in Squid and/or the
> adapter. At the very least, Squid should sent the response header to the
> client.
>
> Without looking at low-level debugging logs (at least), I cannot tell
> who is at fault, but perhaps others on the list can share trickling
> success stories or help you with the triage.
>
> Alex.
>



From rousskov at measurement-factory.com  Mon Nov 25 17:57:16 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Nov 2019 12:57:16 -0500
Subject: [squid-users] ecap trickling (antivirus scan)
In-Reply-To: <LuYbW-w--3-1@tutanota.com>
References: <Ltx_uU4--3-1@tutanota.com> <Lu2BOIR--3-1@tutanota.com>
 <8dd29887-c20b-33f1-5a93-f376d6016ee7@measurement-factory.com>
 <LuYbW-w--3-1@tutanota.com>
Message-ID: <e7e67f09-c488-0daf-0324-03e00e641216@measurement-factory.com>

On 11/25/19 12:31 PM, zbox wrote:

> I already have this line before your snippet :
> 
> ecap_service clamav_service_resp respmod_precache uri=ecap://e-cap.org/ecap/services/clamav?mode=RESPMOD bypass=on
> 
> Sould I try to merge them ?

Short answer: Yes.

You should have one ecap_service directive per unique eCAP service (that
could mean one ecap_service directive per service URI, but Squid also
takes into account the vectoring point IIRC). Each ecap_service
directive tells Squid everything about the service expectations and
behavior. If you tell Squid two different sets of facts about the same
service, Squid would not know which set is the correct one.

I do not know whether modern Squids warn admins about
conflicting/duplicated ecap_service directives. If Squid does not, that
is a bug. Eventually, duplicated ecap_service directives should be
treated as fatal configuration errors.


HTH,

Alex.



> 19 nov. 2019 ? 23:41 de rousskov at measurement-factory.com:
> 
>> On 11/19/19 5:24 AM, zbox wrote:
>>
>>> I've made the changes, the files are now downloaded in the temp dir
>>> but the behaviour is the same on the browser...
>>>
>>
>> Glad your configuration problem got resolved.
>>
>> There is not enough information to figure out what else, if anything, is
>> broken (and where that breakage is). However, if the adapter was
>> configured to trickle, and you see a temporary download file growing in
>> size _while_ Squid sends _nothing_ to the client (at HTTP level) for
>> several seconds, then there is probably a bug in Squid and/or the
>> adapter. At the very least, Squid should sent the response header to the
>> client.
>>
>> Without looking at low-level debugging logs (at least), I cannot tell
>> who is at fault, but perhaps others on the list can share trickling
>> success stories or help you with the triage.
>>
>> Alex.
>>



From jimoe at sohnen-moe.com  Mon Nov 25 18:53:41 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Mon, 25 Nov 2019 11:53:41 -0700
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
 <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
Message-ID: <sig.1232384139.3338fee0-31be-5268-d52a-d0d375148894@sohnen-moe.com>

On 2019-11-14 3:04 PM, Alex Rousskov wrote:

> Can you connect to port 80 of that IPv6 address using telnet, wget, or
> curl running on the Squid box?
> 
  Yes.
$ telnet fd2f:4760:521f:3f3c::c0a8:45f6 80
Trying fd2f:4760:521f:3f3c::c0a8:45f6...
Connected to fd2f:4760:521f:3f3c::c0a8:45f6.
Escape character is '^]'.

> 
>>   There is nothing in the access.log; the request is utterly ignored.
> FYI: "utterly ignored" seems to contradict "error message from squid"
> above. 
>
  I know. Confusing.
  I have narrowed the problem space. The issue occurs only with https:, and not
always. Most sites timeout, others (partially) load after a delay of 5 - 20 seconds.
  The delay never occurs for non-secure traffic.

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.



-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191125/250c2535/attachment.sig>

From jimoe at sohnen-moe.com  Mon Nov 25 19:11:29 2019
From: jimoe at sohnen-moe.com (James Moe)
Date: Mon, 25 Nov 2019 12:11:29 -0700
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
 <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
Message-ID: <sig.023222656c.8bb0f79d-0507-b043-f501-7481d3f31aa9@sohnen-moe.com>

On 2019-11-14 3:04 PM, Alex Rousskov wrote:

> FYI: "utterly ignored" seems to contradict "error message from squid"
> above.
>
  The command "ip a" produces the following rather intimidating output. Should I
add some more IPv6 addresses to the configuration parameter "localnet"?
  Address fd2f:4760:521f:3f3c::c0a8:45f6 is the IPv6 address given as the static
entry for the network interface.

2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1460 qdisc pfifo_fast state UP
group default qlen 1000
    link/ether 00:24:8c:9a:f4:f4 brd ff:ff:ff:ff:ff:ff
    inet 192.168.69.246/24 brd 192.168.69.255 scope global eth0:smasvr3
       valid_lft forever preferred_lft forever
    inet6 fd2f:4760:521f:3f3c:4dfa:4b86:934:5684/64 scope global temporary dynamic
       valid_lft 602374sec preferred_lft 83376sec
    inet6 fd2f:4760:521f:3f3c:1f0:8b81:2a1e:bb1f/64 scope global temporary
deprecated dynamic
       valid_lft 516573sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:38ef:8276:b87b:5f8d/64 scope global temporary
deprecated dynamic
       valid_lft 430773sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:d4c3:7847:797c:37da/64 scope global temporary
deprecated dynamic
       valid_lft 344973sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:c02e:96a3:1557:88ec/64 scope global temporary
deprecated dynamic
       valid_lft 259173sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:3598:28d1:3525:e51e/64 scope global temporary
deprecated dynamic
       valid_lft 173373sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:913c:74dd:d2fd:dc66/64 scope global temporary
deprecated dynamic
       valid_lft 87572sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:f592:3b23:f025:50ba/64 scope global temporary
deprecated dynamic
       valid_lft 1773sec preferred_lft 0sec
    inet6 fd2f:4760:521f:3f3c:224:8cff:fe9a:f4f4/64 scope global mngtmpaddr dynamic
       valid_lft 2591781sec preferred_lft 604581sec
    inet6 fd2f:4760:521f:3f3c::c0a8:45f6/64 scope global
       valid_lft forever preferred_lft forever
    inet6 fe80::224:8cff:fe9a:f4f4/64 scope link
       valid_lft forever preferred_lft forever



-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191125/3a6ce3e5/attachment.sig>

From rousskov at measurement-factory.com  Mon Nov 25 22:52:34 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Nov 2019 17:52:34 -0500
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <sig.1232384139.3338fee0-31be-5268-d52a-d0d375148894@sohnen-moe.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
 <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
 <sig.1232384139.3338fee0-31be-5268-d52a-d0d375148894@sohnen-moe.com>
Message-ID: <55929161-8fb1-322b-0dbf-cb7c80671a12@measurement-factory.com>

On 11/25/19 1:53 PM, James Moe wrote:

>>>   There is nothing in the access.log; the request is utterly ignored.

>> FYI: "utterly ignored" seems to contradict "error message from squid"
>> above. 

>   I know. Confusing.

My remark was meant as a hint that something in your description needs
adjustment: "error message from squid" is mutually exclusive with "the
request is utterly ignored". Going forward, I will assume that the
request was not ignored; I will assume that Squid received the request
and responded with an error message (after a timeout).

Do you see Squid making DNS queries when handling the problematic
transaction?

Can you reproduce the problem using a single transaction on an otherwise
idle Squid?


> I have narrowed the problem space. The issue occurs only with https:, and not
> always. Most sites timeout, others (partially) load after a delay of 5 - 20 seconds.
> The delay never occurs for non-secure traffic.

After the timeout and client-to-Squid connection closure, is there a
corresponding CONNECT record in access.log?

And just to double check, the error message from Squid is in response to
a CONNECT request, right? I see no SslBump rules in your configuration
so this must be a simple case of trying to establish a TCP tunnel with
the address specified by the CONNECT request.

Alex.


From robertkwild at gmail.com  Tue Nov 26 00:55:49 2019
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 26 Nov 2019 00:55:49 +0000
Subject: [squid-users] building squid 4 terminates with fatal error but
	doesnt say the error
Message-ID: <CAGU_Ci+LCHMzE1HL+rcxEyjR97NhQ6vvc2pp1sE1GShzh_9S-A@mail.gmail.com>

hi all,

im building squid 4 by following this guide -

https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

and when i try to run it, i get this error -

[root at squid ~]# /usr/local/squid/sbin/squid -NCd1
2019/11/26 00:43:59| Created PID file (/usr/local/squid/var/run/squid.pid)
2019/11/26 00:43:59| Set Current Directory to
/usr/local/squid/var/cache/squid
2019/11/26 00:43:59| Starting Squid Cache version 4.9 for
x86_64-pc-linux-gnu...
2019/11/26 00:43:59| Service Name: squid
2019/11/26 00:43:59| Process ID 1774
2019/11/26 00:43:59| Process Roles: master worker
2019/11/26 00:43:59| With 1024 file descriptors available
2019/11/26 00:43:59| Initializing IP Cache...
2019/11/26 00:43:59| DNS Socket created at [::], FD 3
2019/11/26 00:43:59| DNS Socket created at 0.0.0.0, FD 9
2019/11/26 00:43:59| Adding domain robo84.home from /etc/resolv.conf
2019/11/26 00:43:59| Adding nameserver 8.8.8.8 from /etc/resolv.conf
2019/11/26 00:43:59| Logfile: opening log
daemon:/usr/local/squid/var/logs/access.log
2019/11/26 00:43:59| Logfile Daemon: opening log
/usr/local/squid/var/logs/access.log
2019/11/26 00:43:59| Store logging disabled
2019/11/26 00:43:59| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2019/11/26 00:43:59| Target number of buckets: 1008
2019/11/26 00:43:59| Using 8192 Store buckets
2019/11/26 00:43:59| Max Mem  size: 262144 KB
2019/11/26 00:43:59| Max Swap size: 0 KB
2019/11/26 00:43:59| Using Least Load store dir selection
2019/11/26 00:43:59| Set Current Directory to
/usr/local/squid/var/cache/squid
2019/11/26 00:43:59| Finished loading MIME types and icons.
2019/11/26 00:43:59| HTCP Disabled.
2019/11/26 00:43:59| Squid plugin modules loaded: 0
2019/11/26 00:43:59| Adaptation support is off.
2019/11/26 00:43:59| Accepting HTTP Socket connections at local=[::]:3128
remote=[::] FD 12 flags=9
2019/11/26 00:44:00| logfileHandleWrite:
daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken
pipe)
2019/11/26 00:44:00| Closing HTTP(S) port [::]:3128
2019/11/26 00:44:00| storeDirWriteCleanLogs: Starting...
2019/11/26 00:44:00|   Finished.  Wrote 0 entries.
2019/11/26 00:44:00|   Took 0.00 seconds (  0.00 entries/sec).
2019/11/26 00:44:00| FATAL: I don't handle this error well!
2019/11/26 00:44:00| Squid Cache (Version 4.9): Terminated abnormally.
2019/11/26 00:44:00| Removing PID file (/usr/local/squid/var/run/squid.pid)

if anyone could help out, it would be very grateful

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/a5722eb8/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 26 06:10:49 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Nov 2019 19:10:49 +1300
Subject: [squid-users] building squid 4 terminates with fatal error but
 doesnt say the error
In-Reply-To: <CAGU_Ci+LCHMzE1HL+rcxEyjR97NhQ6vvc2pp1sE1GShzh_9S-A@mail.gmail.com>
References: <CAGU_Ci+LCHMzE1HL+rcxEyjR97NhQ6vvc2pp1sE1GShzh_9S-A@mail.gmail.com>
Message-ID: <86fabf2d-159e-3800-9973-bc0062f1dea5@treenet.co.nz>

On 26/11/19 1:55 pm, robert k Wild wrote:
> hi all,
> 
> im building squid 4 by following this guide -
> 
> https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> 
> and when i try to run it, i get this error -
> 
> [root at squid ~]# /usr/local/squid/sbin/squid -NCd1
> 2019/11/26 00:43:59| Created PID file (/usr/local/squid/var/run/squid.pid)
> 2019/11/26 00:43:59| Set Current Directory to
> /usr/local/squid/var/cache/squid
> 2019/11/26 00:43:59| Starting Squid Cache version 4.9 for
> x86_64-pc-linux-gnu...
> 2019/11/26 00:43:59| Service Name: squid
> 2019/11/26 00:43:59| Process ID 1774
> 2019/11/26 00:43:59| Process Roles: master worker
> 2019/11/26 00:43:59| With 1024 file descriptors available
> 2019/11/26 00:43:59| Initializing IP Cache...
> 2019/11/26 00:43:59| DNS Socket created at [::], FD 3
> 2019/11/26 00:43:59| DNS Socket created at 0.0.0.0, FD 9
> 2019/11/26 00:43:59| Adding domain robo84.home from /etc/resolv.conf
> 2019/11/26 00:43:59| Adding nameserver 8.8.8.8 from /etc/resolv.conf

Not a good idea to use this resolver with SSL-Bump features. See
<https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>.


> 2019/11/26 00:43:59| Logfile: opening log
> daemon:/usr/local/squid/var/logs/access.log
> 2019/11/26 00:43:59| Logfile Daemon: opening log
> /usr/local/squid/var/logs/access.log
...
> 2019/11/26 00:44:00| logfileHandleWrite:
> daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken
> pipe)

There is your error message.

Check that the folder path all exists and has the correct permissions
for Squid's low-privilege account to create the access.log file there.


> 2019/11/26 00:44:00| FATAL: I don't handle this error well!
> 2019/11/26 00:44:00| Squid Cache (Version 4.9): Terminated abnormally.


Amos


From robertkwild at gmail.com  Tue Nov 26 09:54:04 2019
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 26 Nov 2019 09:54:04 +0000
Subject: [squid-users] building squid 4 terminates with fatal error but
 doesnt say the error
In-Reply-To: <86fabf2d-159e-3800-9973-bc0062f1dea5@treenet.co.nz>
References: <CAGU_Ci+LCHMzE1HL+rcxEyjR97NhQ6vvc2pp1sE1GShzh_9S-A@mail.gmail.com>
 <86fabf2d-159e-3800-9973-bc0062f1dea5@treenet.co.nz>
Message-ID: <CAGU_CiJLscYJPM2dsMPEkoK87BMuvPU9TQpb76cwAEVvWFuXag@mail.gmail.com>

nice one Amos, i did -

touch /usr/local/squid/var/logs/access.log
chmod 777 /usr/local/squid/var/logs/access.log

On Tue, 26 Nov 2019 at 06:11, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 26/11/19 1:55 pm, robert k Wild wrote:
> > hi all,
> >
> > im building squid 4 by following this guide -
> >
> > https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> >
> > and when i try to run it, i get this error -
> >
> > [root at squid ~]# /usr/local/squid/sbin/squid -NCd1
> > 2019/11/26 00:43:59| Created PID file
> (/usr/local/squid/var/run/squid.pid)
> > 2019/11/26 00:43:59| Set Current Directory to
> > /usr/local/squid/var/cache/squid
> > 2019/11/26 00:43:59| Starting Squid Cache version 4.9 for
> > x86_64-pc-linux-gnu...
> > 2019/11/26 00:43:59| Service Name: squid
> > 2019/11/26 00:43:59| Process ID 1774
> > 2019/11/26 00:43:59| Process Roles: master worker
> > 2019/11/26 00:43:59| With 1024 file descriptors available
> > 2019/11/26 00:43:59| Initializing IP Cache...
> > 2019/11/26 00:43:59| DNS Socket created at [::], FD 3
> > 2019/11/26 00:43:59| DNS Socket created at 0.0.0.0, FD 9
> > 2019/11/26 00:43:59| Adding domain robo84.home from /etc/resolv.conf
> > 2019/11/26 00:43:59| Adding nameserver 8.8.8.8 from /etc/resolv.conf
>
> Not a good idea to use this resolver with SSL-Bump features. See
> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>.
>
>
> > 2019/11/26 00:43:59| Logfile: opening log
> > daemon:/usr/local/squid/var/logs/access.log
> > 2019/11/26 00:43:59| Logfile Daemon: opening log
> > /usr/local/squid/var/logs/access.log
> ...
> > 2019/11/26 00:44:00| logfileHandleWrite:
> > daemon:/usr/local/squid/var/logs/access.log: error writing ((32) Broken
> > pipe)
>
> There is your error message.
>
> Check that the folder path all exists and has the correct permissions
> for Squid's low-privilege account to create the access.log file there.
>
>
> > 2019/11/26 00:44:00| FATAL: I don't handle this error well!
> > 2019/11/26 00:44:00| Squid Cache (Version 4.9): Terminated abnormally.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/fbd3b486/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 26 10:01:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Nov 2019 23:01:10 +1300
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <sig.023222656c.8bb0f79d-0507-b043-f501-7481d3f31aa9@sohnen-moe.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
 <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
 <sig.023222656c.8bb0f79d-0507-b043-f501-7481d3f31aa9@sohnen-moe.com>
Message-ID: <24a49187-dded-17c1-a0cb-5b1cc7f8ef47@treenet.co.nz>

On 26/11/19 8:11 am, James Moe wrote:
> On 2019-11-14 3:04 PM, Alex Rousskov wrote:
> 
>> FYI: "utterly ignored" seems to contradict "error message from squid"
>> above.
>>
>   The command "ip a" produces the following rather intimidating output. Should I
> add some more IPv6 addresses to the configuration parameter "localnet"?

You could add the fe80::/10 subnet back in. But it should not have any
noticeable effect on your current problem.


The number of "temporary deprecated dynamic" means your server is
changing its public IP randomly and frequently (so-called 'privacy
addressing'). The addresses marked 'deprecated' can only be used by
existing fully-open TCP connections. New connections to that IP are
rejected as if it did not exist - these addresses are supposed to be
only for outbound traffic anyway.

So ... check if you have any firewall rules or DNS entries regarding
traffic *to* the server. Make sure they only use the addresses marked
'forever' in that list, or the whole fd2f:4760:521f:3f3c::/64 range.

Amos


From jmperrote at policia.rionegro.gov.ar  Tue Nov 26 10:50:14 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Tue, 26 Nov 2019 07:50:14 -0300
Subject: [squid-users] Help with squid proxy parent directive
In-Reply-To: <2d446514-e713-a9c8-4b5c-107780749fee@policia.rionegro.gov.ar>
References: <2d446514-e713-a9c8-4b5c-107780749fee@policia.rionegro.gov.ar>
Message-ID: <277fc7ea-643f-6fd8-aeb0-970548e6dbf5@policia.rionegro.gov.ar>


Hello we are trying to configure two squid reverse proxy, one frontend 
to internet and the other inside the network.
Both on reverse proxy mode, and one dependening the other.

 ??? internet ---> squid reverse proxy one ---> squid reverse proxy two 
---> inside web server

Accessing to first reverse proxy is ok, when try to acces to second 
proxy have a error ((104) Connection reset by peer)

 ??? ??? ??? ??? El sistema ha devuelto: (104) Connection reset by peer
 ??? ?? ? ?? ??? Se ha producido un error al leer datos de la red. Por 
favor, int?ntelo de nuevo.
 ??? ?? ? ?? ??? Su administrador del cach? es soporte at xxxxxxx.

Regards.


Config:

 ??? Squid 3.5

 ??? enabled ssl








From squid3 at treenet.co.nz  Tue Nov 26 11:58:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Nov 2019 00:58:24 +1300
Subject: [squid-users] After enabling IPv6 squid no longer responds
In-Reply-To: <55929161-8fb1-322b-0dbf-cb7c80671a12@measurement-factory.com>
References: <sig.0220b7c3fc.a74dc21c-a500-a9c5-3912-f7db7e68c02d@sohnen-moe.com>
 <sig.0221a8f531.a3675c69-d9c3-a208-8a1c-e20e52a309dd@sohnen-moe.com>
 <70cc5b03-6ceb-ea0f-2b83-41f1e9b81b57@measurement-factory.com>
 <sig.1232384139.3338fee0-31be-5268-d52a-d0d375148894@sohnen-moe.com>
 <55929161-8fb1-322b-0dbf-cb7c80671a12@measurement-factory.com>
Message-ID: <228c4f0c-30ff-1fbe-8da3-2c906fc2e044@treenet.co.nz>

On 26/11/19 11:52 am, Alex Rousskov wrote:
> On 11/25/19 1:53 PM, James Moe wrote:
> 
>> I have narrowed the problem space. The issue occurs only with https:, and not
>> always. Most sites timeout, others (partially) load after a delay of 5 - 20 seconds.
>> The delay never occurs for non-secure traffic.
> 
> After the timeout and client-to-Squid connection closure, is there a
> corresponding CONNECT record in access.log?
> 

If not, double-check that the traffic is actually going to the Squid you
think it is (that may require one or more packet traces). There have
been a few cases in the past where it turned out sometimes traffic was
going to a proxy it was not supposed to.


Amos


From squid3 at treenet.co.nz  Tue Nov 26 11:58:19 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Nov 2019 00:58:19 +1300
Subject: [squid-users] building squid 4 terminates with fatal error but
 doesnt say the error
In-Reply-To: <CAGU_CiJLscYJPM2dsMPEkoK87BMuvPU9TQpb76cwAEVvWFuXag@mail.gmail.com>
References: <CAGU_Ci+LCHMzE1HL+rcxEyjR97NhQ6vvc2pp1sE1GShzh_9S-A@mail.gmail.com>
 <86fabf2d-159e-3800-9973-bc0062f1dea5@treenet.co.nz>
 <CAGU_CiJLscYJPM2dsMPEkoK87BMuvPU9TQpb76cwAEVvWFuXag@mail.gmail.com>
Message-ID: <87f22d49-3ef4-0d60-024a-93e650155eb6@treenet.co.nz>

On 26/11/19 10:54 pm, robert k Wild wrote:
> nice one Amos, i did -
> 
> touch /usr/local/squid/var/logs/access.log
> chmod 777 /usr/local/squid/var/logs/access.log
> 


This will break again when the log rotation happens. The *directory*
(folder path) is the important piece to get right. That is
/usr/local/squid/var/logs in your case.


Amos


From robertkwild at gmail.com  Tue Nov 26 14:25:12 2019
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 26 Nov 2019 14:25:12 +0000
Subject: [squid-users] building squid 4 terminates with fatal error but
 doesnt say the error
In-Reply-To: <87f22d49-3ef4-0d60-024a-93e650155eb6@treenet.co.nz>
References: <CAGU_Ci+LCHMzE1HL+rcxEyjR97NhQ6vvc2pp1sE1GShzh_9S-A@mail.gmail.com>
 <86fabf2d-159e-3800-9973-bc0062f1dea5@treenet.co.nz>
 <CAGU_CiJLscYJPM2dsMPEkoK87BMuvPU9TQpb76cwAEVvWFuXag@mail.gmail.com>
 <87f22d49-3ef4-0d60-024a-93e650155eb6@treenet.co.nz>
Message-ID: <CAGU_CiJ0VOLe5eD7SpZ-C8BN4JzvR0rX3R7z5yDZH2MJ+iC9rg@mail.gmail.com>

Amos,

thanks so much for all your help!!!!! :)

On Tue, 26 Nov 2019 at 11:58, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 26/11/19 10:54 pm, robert k Wild wrote:
> > nice one Amos, i did -
> >
> > touch /usr/local/squid/var/logs/access.log
> > chmod 777 /usr/local/squid/var/logs/access.log
> >
>
>
> This will break again when the log rotation happens. The *directory*
> (folder path) is the important piece to get right. That is
> /usr/local/squid/var/logs in your case.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/b47ba4bf/attachment.htm>

From felipeapolanco at gmail.com  Tue Nov 26 15:15:33 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 26 Nov 2019 11:15:33 -0400
Subject: [squid-users] What is the proper way to close an ICAP transaction?
Message-ID: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>

Hi,

We have an ICAP server for Squid 4.

While we can successfully scan our files and do content adaptation, we have
been struggling to find a way to close the ICAP transaction before passing
the whole body back to squid and at the same time avoid squid marking one
icap failure.

This is for an ICAP server that does Virus scanning and if virus found, the
body is not sent back.

If we send an ICAP header with 500 then Squid mark us as ICAP FAILURE, if
we don't send anything then Squid keeps awaiting on us and then timeout,
increasing the icap failure counter by one and so on.

At some point squid just mark the server as down and stop sending
transactions to it.

We have been overcoming this by having a low OPTIONs TTL but that seems
inefficient for high traffic squid nodes.

Does anyone know how to proceed with this?

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/fd493614/attachment.htm>

From robertkwild at gmail.com  Tue Nov 26 15:54:32 2019
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 26 Nov 2019 15:54:32 +0000
Subject: [squid-users] making proxy-int to talk to proxy-ext
Message-ID: <CAGU_Ci+qqfiboRwxQ_b6aMT80yNxXe2VW4+wNQuRCg7zSH+XpA@mail.gmail.com>

hi all,

as i have configured both internal proxy (non internet facing) and external
proxy (internet facing) from source, followed this guide -

https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

it works if i comment out the ssl lines -

#SSL
#http_port 3128 ssl-bump \
#cert=/etc/squid/ssl_cert/myCA.pem \
#generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
#sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB
#acl step1 at_step SslBump1
#ssl_bump peek step1
#ssl_bump bump all

but as soon as i uncomment them it breaks the link between both servers

this is the error i get from the internal proxy when it tries to contact
the external proxy

https://i.postimg.cc/JzC29gh8/ssl.png
-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/80699adc/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov 26 16:59:37 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 26 Nov 2019 11:59:37 -0500
Subject: [squid-users] making proxy-int to talk to proxy-ext
In-Reply-To: <CAGU_Ci+qqfiboRwxQ_b6aMT80yNxXe2VW4+wNQuRCg7zSH+XpA@mail.gmail.com>
References: <CAGU_Ci+qqfiboRwxQ_b6aMT80yNxXe2VW4+wNQuRCg7zSH+XpA@mail.gmail.com>
Message-ID: <065009ab-3993-f44f-7757-43ee81e10a7e@measurement-factory.com>

On 11/26/19 10:54 AM, robert k Wild wrote:

> as i have configured both internal proxy (non internet facing) and
> external proxy (internet facing) from source, 

Please show the essential parts of both internal and external Squid
configurations for the broken setup (at least).

It is difficult to guess what went wrong because the guide you are
quoting does not talk about internal and external proxy instances _and_,
in most cases, simply adding a valid http_port line has no effect on
test cases that worked before -- the new port will be unused by the old
test traffic. It is not even clear which proxy you are adding the
SslBump configuration to.


Thank you,

Alex.


> followed this guide - 
> https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> 
> it works if i comment out the ssl lines -
> 
> #SSL
> #http_port 3128 ssl-bump \
> #cert=/etc/squid/ssl_cert/myCA.pem \
> #generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> #sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
> #acl step1 at_step SslBump1
> #ssl_bump peek step1
> #ssl_bump bump all
> 
> but as soon as i uncomment them it breaks the link between both servers
> 
> this is the error i get from the internal proxy when it tries to contact
> the external proxy
> 
> https://i.postimg.cc/JzC29gh8/ssl.png
> -- 
> Regards,
> 
> Robert K Wild.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Nov 26 19:44:58 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 26 Nov 2019 14:44:58 -0500
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
Message-ID: <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>

On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:

> While we can successfully scan our files and do content adaptation, we
> have been struggling?to find a way to close the ICAP transaction before
> passing the whole body back to squid and at the same time avoid squid
> marking one icap failure.

Squid needs a valid ICAP response. The right ICAP response status code
depends on what you want Squid to do after receiving that response. You
have mentioned what you do _not_ want Squid to do (i.e. increase the
failure count), but that still leaves a lot of options.


> This is for an ICAP server that does Virus scanning and if virus found,
> the body is not sent back.

What do you want Squid to do when the ICAP service finds a virus? For
example, what message do you want Squid to send to the next HTTP hop?

Alex.


From felipeapolanco at gmail.com  Tue Nov 26 19:52:58 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 26 Nov 2019 15:52:58 -0400
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
Message-ID: <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>

Hi,

We are sending an encapsulated HTTP 307 redirect webpage header whenever a
Virus is found and stop sending any other data after that, but squid
complains about ICAP failure when we do that:
Adaptation::Icap::Xaction::noteCommRead threw exception: corrupted chunk
size

We are not sending an ICAP header at this point because we already told
Squid ICAP 200 OK header and begun a body transaction, we send some chunks
back to the client for progress and hold the last part for scanning.

Ideally, we would like to just send our 307 to Squid and not having it
count as a failure.

On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
>
> > While we can successfully scan our files and do content adaptation, we
> > have been struggling to find a way to close the ICAP transaction before
> > passing the whole body back to squid and at the same time avoid squid
> > marking one icap failure.
>
> Squid needs a valid ICAP response. The right ICAP response status code
> depends on what you want Squid to do after receiving that response. You
> have mentioned what you do _not_ want Squid to do (i.e. increase the
> failure count), but that still leaves a lot of options.
>
>
> > This is for an ICAP server that does Virus scanning and if virus found,
> > the body is not sent back.
>
> What do you want Squid to do when the ICAP service finds a virus? For
> example, what message do you want Squid to send to the next HTTP hop?
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/180e7ac5/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov 26 20:52:27 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 26 Nov 2019 15:52:27 -0500
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
Message-ID: <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>

On 11/26/19 2:52 PM, Felipe Arturo Polanco wrote:

> We are sending an encapsulated HTTP 307 redirect webpage header whenever
> a Virus is found and stop sending any other data after that

You must use ICAP status code 200 then. Make sure your encapsulated HTTP
307 body (if any) is properly sent to Squid.


> but squid
> complains about ICAP failure when we do that:
> Adaptation::Icap::Xaction::noteCommRead threw exception: corrupted chunk
> size

What chunk size did Squid not like? You should be able to tell by
looking at the packet capture of the failed transaction (or low-level
Squid debugging).


> We are not sending an ICAP header at this point because we already told
> Squid ICAP 200 OK header and begun a body transaction, we send some
> chunks back to the client for progress and hold the last part for scanning.

Are you sending HTTP 307 body chunks to Squid? How do you indicate that
no more chunks will be coming?

It sounds like you are trying to cram two HTTP messages (one with the
original HTTP response body prefix and one with a generated 307
redirect) into one ICAP response, which is impossible, but perhaps I
misunderstood your description. It would help if you post a sample (but
complete) ICAP response that Squid does not like.


> Ideally, we would like to just send our 307 to Squid and not having it
> count as a failure.

Yes, a 200 ICAP response with an embedded HTTP 307 response should work
just fine, but all its pieces should be properly formed (and there
should be no extras).

Alex.


> On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov wrote:
> 
>     On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
> 
>     > While we can successfully scan our files and do content adaptation, we
>     > have been struggling?to find a way to close the ICAP transaction
>     before
>     > passing the whole body back to squid and at the same time avoid squid
>     > marking one icap failure.
> 
>     Squid needs a valid ICAP response. The right ICAP response status code
>     depends on what you want Squid to do after receiving that response. You
>     have mentioned what you do _not_ want Squid to do (i.e. increase the
>     failure count), but that still leaves a lot of options.
> 
> 
>     > This is for an ICAP server that does Virus scanning and if virus
>     found,
>     > the body is not sent back.
> 
>     What do you want Squid to do when the ICAP service finds a virus? For
>     example, what message do you want Squid to send to the next HTTP hop?
> 
>     Alex.
> 



From felipeapolanco at gmail.com  Tue Nov 26 21:12:23 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 26 Nov 2019 17:12:23 -0400
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
 <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
Message-ID: <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>

Hi,

The flow is the following:
ICAP transaction is sent to ICAP server with a PREVIEW header
ICAP server sends ICAP header 100 Continue
ICAP server sends ICAP header 200 OK to start data transfer
<data transfer begins>
ICAP server receives a chunk, checks if its the last chunk, if not then
append to temp file and send it back to Squid; if it is the last chunk then
analyze the temp file for virus.
<repeat for next data transfer>
If virus found then send encapsulated HTTP header 307 redirect.
If virus not found, send the last chunk to squid.

The part where we send 307 is the part that Squid doesn't like, I believe
is because we are not sending the last chunk since the file is a virus.

On Tue, Nov 26, 2019 at 4:52 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/26/19 2:52 PM, Felipe Arturo Polanco wrote:
>
> > We are sending an encapsulated HTTP 307 redirect webpage header whenever
> > a Virus is found and stop sending any other data after that
>
> You must use ICAP status code 200 then. Make sure your encapsulated HTTP
> 307 body (if any) is properly sent to Squid.
>
>
> > but squid
> > complains about ICAP failure when we do that:
> > Adaptation::Icap::Xaction::noteCommRead threw exception: corrupted chunk
> > size
>
> What chunk size did Squid not like? You should be able to tell by
> looking at the packet capture of the failed transaction (or low-level
> Squid debugging).
>
>
> > We are not sending an ICAP header at this point because we already told
> > Squid ICAP 200 OK header and begun a body transaction, we send some
> > chunks back to the client for progress and hold the last part for
> scanning.
>
> Are you sending HTTP 307 body chunks to Squid? How do you indicate that
> no more chunks will be coming?
>
> It sounds like you are trying to cram two HTTP messages (one with the
> original HTTP response body prefix and one with a generated 307
> redirect) into one ICAP response, which is impossible, but perhaps I
> misunderstood your description. It would help if you post a sample (but
> complete) ICAP response that Squid does not like.
>
>
> > Ideally, we would like to just send our 307 to Squid and not having it
> > count as a failure.
>
> Yes, a 200 ICAP response with an embedded HTTP 307 response should work
> just fine, but all its pieces should be properly formed (and there
> should be no extras).
>
> Alex.
>
>
> > On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov wrote:
> >
> >     On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
> >
> >     > While we can successfully scan our files and do content
> adaptation, we
> >     > have been struggling to find a way to close the ICAP transaction
> >     before
> >     > passing the whole body back to squid and at the same time avoid
> squid
> >     > marking one icap failure.
> >
> >     Squid needs a valid ICAP response. The right ICAP response status
> code
> >     depends on what you want Squid to do after receiving that response.
> You
> >     have mentioned what you do _not_ want Squid to do (i.e. increase the
> >     failure count), but that still leaves a lot of options.
> >
> >
> >     > This is for an ICAP server that does Virus scanning and if virus
> >     found,
> >     > the body is not sent back.
> >
> >     What do you want Squid to do when the ICAP service finds a virus? For
> >     example, what message do you want Squid to send to the next HTTP hop?
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/6592d1e4/attachment.htm>

From ahmed.zaeem at netstream.ps  Tue Nov 26 22:24:35 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 27 Nov 2019 01:24:35 +0300
Subject: [squid-users] Squid Send to Cache Peer based on Header Access if
	not matched .
Message-ID: <45B32E98-45FD-44FA-9ED2-D07706CB9634@netstream.ps>

Hello Floks ,


i have squid ACL/TCP Outgoing based on incoming header .

as an example below :


acl requestheader5000 req_header X-Proxy 1.2.3.4
acl requestheader5001 req_header X-Proxy 1.2.3.5
acl requestheader5002 req_header X-Proxy 1.2.3.6
acl requestheader5003 req_header X-Proxy 1.2.3.7

#########################

tcp_outgoing_address 1.2.3.4 requestheader5000
tcp_outgoing_address 1.2.3.5 requestheader5001
tcp_outgoing_address 1.2.3.6 requestheader5002
tcp_outgoing_address 1.2.3.7 requestheader5003


So if an incoming request with X-Proxy header 1.2.3.4 , it will match the Acl requestheader5000 and will have outgoing address as 1.2.3.4 . ??> no problem here .



Now Say the incoming X-Proxy header was 9.9.9.9 which is a value not matched in the current ACL .

How can we let squid to send those Type of requests  ? not match with ACL ? to remote Cache peer squid in case not  ? matched X-proxy header?  ?





Thanks 



From robertkwild at gmail.com  Tue Nov 26 22:56:39 2019
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 26 Nov 2019 22:56:39 +0000
Subject: [squid-users] making proxy-int to talk to proxy-ext
In-Reply-To: <065009ab-3993-f44f-7757-43ee81e10a7e@measurement-factory.com>
References: <CAGU_Ci+qqfiboRwxQ_b6aMT80yNxXe2VW4+wNQuRCg7zSH+XpA@mail.gmail.com>
 <065009ab-3993-f44f-7757-43ee81e10a7e@measurement-factory.com>
Message-ID: <CAGU_CiLRUyCdJStXFEhj-MBrdgvxhUJapDh2XXVHvLBZVpT27g@mail.gmail.com>

Hi Alex,

i have done some more troubleshooting and my external proxy is good, i get
no errors and i have got one of my DMZ hosts connected to it and i can
browse the web, but my internal proxy cant contact my external proxy, this
is the error when i run it -

2019/11/26 22:53:28| Error parsing SSL Server Hello Message on FD 15
2019/11/26 22:53:28| ERROR: negotiating TLS on FD 15: error:140770FC:SSL
routines:SSL23_GET_SERVER_HELLO:          unknown protocol (1/-1/0)
2019/11/26 22:53:28| TCP connection to 172.16.55.21/3128 failed
2019/11/26 22:53:28| Detected DEAD Parent: 172.16.55.21
2019/11/26 22:53:28| Error negotiating SSL connection on FD 13:
error:00000001:lib(0):func(0):reason(1) (          1/0)

this is my config on my internal proxy -

#
# Recommended minimum configuration:
#

#SSL
http_port 3128 ssl-bump \
cert=/etc/squid/ssl_cert/myCA.pem \
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8             # RFC 1918 local private network
(LAN)
acl localnet src 100.64.0.0/10          # RFC 6598 shared address space
(CGN)
acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly
plugged) machines
acl localnet src 172.16.0.0/12          # RFC 1918 local private network
(LAN)
acl localnet src 192.168.0.0/16         # RFC 1918 local private network
(LAN)
acl localnet src fc00::/7               # RFC 4193 local private network
range
acl localnet src fe80::/10              # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#squid proxy in DMZ on internet
cache_peer 172.16.55.21 parent 3128 0 default
acl all src all
http_access allow all
never_direct allow all

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

my external proxy uses the same config but without the lines "squid proxy
in DMZ on internet"

thanks,
rob

On Tue, 26 Nov 2019 at 16:59, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/26/19 10:54 AM, robert k Wild wrote:
>
> > as i have configured both internal proxy (non internet facing) and
> > external proxy (internet facing) from source,
>
> Please show the essential parts of both internal and external Squid
> configurations for the broken setup (at least).
>
> It is difficult to guess what went wrong because the guide you are
> quoting does not talk about internal and external proxy instances _and_,
> in most cases, simply adding a valid http_port line has no effect on
> test cases that worked before -- the new port will be unused by the old
> test traffic. It is not even clear which proxy you are adding the
> SslBump configuration to.
>
>
> Thank you,
>
> Alex.
>
>
> > followed this guide -
> > https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> >
> > it works if i comment out the ssl lines -
> >
> > #SSL
> > #http_port 3128 ssl-bump \
> > #cert=/etc/squid/ssl_cert/myCA.pem \
> > #generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> > #sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> > /var/lib/ssl_db -M 4MB
> > #acl step1 at_step SslBump1
> > #ssl_bump peek step1
> > #ssl_bump bump all
> >
> > but as soon as i uncomment them it breaks the link between both servers
> >
> > this is the error i get from the internal proxy when it tries to contact
> > the external proxy
> >
> > https://i.postimg.cc/JzC29gh8/ssl.png
> > --
> > Regards,
> >
> > Robert K Wild.
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191126/89c62184/attachment.htm>

From ahmed.zaeem at netstream.ps  Wed Nov 27 05:31:28 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 27 Nov 2019 08:31:28 +0300
Subject: [squid-users] limit  new req/sec on squid to X per sec
Message-ID: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>

Hello Folks ,


im looking for limiting TCP req/sec on squid to X speed .


say i have an instance running .


i want to limit it to 100 req/sec for ?new connections ? not  just for concurrent connections .

so if connection is old or ? established ? its out of the game .
if the connection is new , all new should be limited to 100 req/sec .

i made search on all max_conn but it seems count ?concurrent sessions ? even old +  new .

is there a way in squid to limit only new sessions ?


Thanks 



From zebox at tutanota.com  Wed Nov 27 09:40:01 2019
From: zebox at tutanota.com (zbox)
Date: Wed, 27 Nov 2019 10:40:01 +0100 (CET)
Subject: [squid-users] ecap trickling (antivirus scan)
In-Reply-To: <e7e67f09-c488-0daf-0324-03e00e641216@measurement-factory.com>
References: <Ltx_uU4--3-1@tutanota.com> <Lu2BOIR--3-1@tutanota.com>
 <8dd29887-c20b-33f1-5a93-f376d6016ee7@measurement-factory.com>
 <LuYbW-w--3-1@tutanota.com>
 <e7e67f09-c488-0daf-0324-03e00e641216@measurement-factory.com>
Message-ID: <LugDpXO--3-1@tutanota.com>

Hello,

Ok it seems to be working now (10 bytes by 10 bytes and then the rest of the file) the browser download gauge is useless but I suppose it's normal behavior.

Here is my conf :

ecap_service clamav_service_req reqmod_precache uri=ecap://e-cap.org/ecap/services/clamav?mode=REQMOD <http://e-cap.org/ecap/services/clamav?mode=REQMOD> bypass=off

ecap_service clamav_service_resp respmod_precache uri=ecap://e-cap.org/ecap/services/clamav?mode=RESPMOD <http://e-cap.org/ecap/services/clamav?mode=RESPMOD> bypass=on staging_dir=/var/eclamav/resp- trickling_drop_size=10

Do you think it's ok ?

Regards


25 nov. 2019 ? 18:57 de rousskov at measurement-factory.com:

> On 11/25/19 12:31 PM, zbox wrote:
>
>> I already have this line before your snippet :
>>
>> ecap_service clamav_service_resp respmod_precache uri=ecap://e-cap.org/ecap/services/clamav?mode=RESPMOD bypass=on
>>
>> Sould I try to merge them ?
>>
>
> Short answer: Yes.
>
> You should have one ecap_service directive per unique eCAP service (that
> could mean one ecap_service directive per service URI, but Squid also
> takes into account the vectoring point IIRC). Each ecap_service
> directive tells Squid everything about the service expectations and
> behavior. If you tell Squid two different sets of facts about the same
> service, Squid would not know which set is the correct one.
>
> I do not know whether modern Squids warn admins about
> conflicting/duplicated ecap_service directives. If Squid does not, that
> is a bug. Eventually, duplicated ecap_service directives should be
> treated as fatal configuration errors.
>
>
> HTH,
>
> Alex.
>
>
>
>> 19 nov. 2019 ? 23:41 de rousskov at measurement-factory.com:
>>
>>> On 11/19/19 5:24 AM, zbox wrote:
>>>
>>>> I've made the changes, the files are now downloaded in the temp dir
>>>> but the behaviour is the same on the browser...
>>>>
>>>
>>> Glad your configuration problem got resolved.
>>>
>>> There is not enough information to figure out what else, if anything, is
>>> broken (and where that breakage is). However, if the adapter was
>>> configured to trickle, and you see a temporary download file growing in
>>> size _while_ Squid sends _nothing_ to the client (at HTTP level) for
>>> several seconds, then there is probably a bug in Squid and/or the
>>> adapter. At the very least, Squid should sent the response header to the
>>> client.
>>>
>>> Without looking at low-level debugging logs (at least), I cannot tell
>>> who is at fault, but perhaps others on the list can share trickling
>>> success stories or help you with the triage.
>>>
>>> Alex.
>>>



From squid3 at treenet.co.nz  Wed Nov 27 11:57:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Nov 2019 00:57:12 +1300
Subject: [squid-users] limit new req/sec on squid to X per sec
In-Reply-To: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
References: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
Message-ID: <76eb37dc-2e63-502e-ea59-4eb6dc09f2af@treenet.co.nz>

On 27/11/19 6:31 pm, --Ahmad-- wrote:
> Hello Folks ,
> 
> 
> im looking for limiting TCP req/sec on squid to X speed .
> 

TCP does not make requests.

> 
> say i have an instance running .
> 
> 
> i want to limit it to 100 req/sec for ?new connections ? not  just for concurrent connections .
> 

req/sec is an HTTP term to Squid. It has nothing to do with "connections".

The part where you say "not just for concurrent connections" implies
that is something Squid does, does not match up with any existing Squid
behaviour or features. Squid does not limit req/sec for anything.

Squid can limit *bytes* per second. Or limit total connections a given
client has open concurrently.


> so if connection is old or ? established ? its out of the game .

In HTTP terms there is no such thing as a connection.

In TCP terms a connection is established as soon as it exists. If you
mean the TCP handshake process, that is a thing for firewall rules to
control. Squid cannot prevent SYN packets being sent to it.


If you mean something else, then please define this concept you have of
"new connection".


> if the connection is new , all new should be limited to 100 req/sec .
> 
> i made search on all max_conn but it seems count ?concurrent sessions ? even old +  new .
> 
> is there a way in squid to limit only new sessions ?
> 

Sessions are a very different thing to connections.

max_conn as its name should indicate sets the maximum connection count a
client can open *concurrently*.


Why exactly do you want this?

What problem will it solve?


Amos


From ahmed.zaeem at netstream.ps  Wed Nov 27 12:03:44 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 27 Nov 2019 15:03:44 +0300
Subject: [squid-users] limit new req/sec on squid to X per sec
In-Reply-To: <76eb37dc-2e63-502e-ea59-4eb6dc09f2af@treenet.co.nz>
References: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
 <76eb37dc-2e63-502e-ea59-4eb6dc09f2af@treenet.co.nz>
Message-ID: <BCD5FED6-8FCC-49B7-A165-0B423DB79908@netstream.ps>

Hello Amos , Thank you for your response .

we have an APP behind squid http APP that will crash if # of (req/sec ) exceeded X .
it won?t crash about Already established session , it only care about new req/sec hitting squid .

I think its doable by iptables , but i really was hopping we can do it from squid level .

so you can imagine http req/sec or tcp req/sec same here as squid is being used only on http protocol .


Let me know your thoughts .


Thanks 


> On Nov 27, 2019, at 2:57 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 27/11/19 6:31 pm, --Ahmad-- wrote:
>> Hello Folks ,
>> 
>> 
>> im looking for limiting TCP req/sec on squid to X speed .
>> 
> 
> TCP does not make requests.
> 
>> 
>> say i have an instance running .
>> 
>> 
>> i want to limit it to 100 req/sec for ?new connections ? not  just for concurrent connections .
>> 
> 
> req/sec is an HTTP term to Squid. It has nothing to do with "connections".
> 
> The part where you say "not just for concurrent connections" implies
> that is something Squid does, does not match up with any existing Squid
> behaviour or features. Squid does not limit req/sec for anything.
> 
> Squid can limit *bytes* per second. Or limit total connections a given
> client has open concurrently.
> 
> 
>> so if connection is old or ? established ? its out of the game .
> 
> In HTTP terms there is no such thing as a connection.
> 
> In TCP terms a connection is established as soon as it exists. If you
> mean the TCP handshake process, that is a thing for firewall rules to
> control. Squid cannot prevent SYN packets being sent to it.
> 
> 
> If you mean something else, then please define this concept you have of
> "new connection".
> 
> 
>> if the connection is new , all new should be limited to 100 req/sec .
>> 
>> i made search on all max_conn but it seems count ?concurrent sessions ? even old +  new .
>> 
>> is there a way in squid to limit only new sessions ?
>> 
> 
> Sessions are a very different thing to connections.
> 
> max_conn as its name should indicate sets the maximum connection count a
> client can open *concurrently*.
> 
> 
> Why exactly do you want this?
> 
> What problem will it solve?
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Nov 27 12:03:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Nov 2019 01:03:50 +1300
Subject: [squid-users] Squid Send to Cache Peer based on Header Access
 if not matched .
In-Reply-To: <45B32E98-45FD-44FA-9ED2-D07706CB9634@netstream.ps>
References: <45B32E98-45FD-44FA-9ED2-D07706CB9634@netstream.ps>
Message-ID: <46352ff2-5c15-ebc6-7414-d51a71075be2@treenet.co.nz>

On 27/11/19 11:24 am, --Ahmad-- wrote:
> Hello Floks ,
> 
> 
> i have squid ACL/TCP Outgoing based on incoming header .
> 
> as an example below :
> 
> 
> acl requestheader5000 req_header X-Proxy 1.2.3.4
> acl requestheader5001 req_header X-Proxy 1.2.3.5
> acl requestheader5002 req_header X-Proxy 1.2.3.6
> acl requestheader5003 req_header X-Proxy 1.2.3.7
> 
> #########################
> 
> tcp_outgoing_address 1.2.3.4 requestheader5000
> tcp_outgoing_address 1.2.3.5 requestheader5001
> tcp_outgoing_address 1.2.3.6 requestheader5002
> tcp_outgoing_address 1.2.3.7 requestheader5003
> 
> 
> So if an incoming request with X-Proxy header 1.2.3.4 , it will match the Acl requestheader5000 and will have outgoing address as 1.2.3.4 . ??> no problem here .
> 
> 
> 
> Now Say the incoming X-Proxy header was 9.9.9.9 which is a value not matched in the current ACL .
> 
> How can we let squid to send those Type of requests  ? not match with ACL ? to remote Cache peer squid in case not  ? matched X-proxy header?  ?
> 

You are mixing up concepts here.

tcp_outgoing_address sets the address the TCP packets say they are
coming *from*. It has nothing to do with where they are going *to*.


For controlling which requests go to a peer use
 <http://www.squid-cache.org/Doc/config/cache_peer_access/>.


With maybe one or more of these directives as well, depending on your
configuration complexity and exact routing policy:
 <http://www.squid-cache.org/Doc/config/always_direct/>
 <http://www.squid-cache.org/Doc/config/never_direct/>
 <http://www.squid-cache.org/Doc/config/prefer_direct/>
 <http://www.squid-cache.org/Doc/config/nonhierarchical_direct/>


Amos


From 0xff1f at gmail.com  Wed Nov 27 12:09:19 2019
From: 0xff1f at gmail.com (Ahmad Alzaeem)
Date: Wed, 27 Nov 2019 15:09:19 +0300
Subject: [squid-users] Testing
Message-ID: <79B42221-48EF-414D-8880-CFFB5A61037D@gmail.com>

Testing 123 .


From squid3 at treenet.co.nz  Wed Nov 27 12:20:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Nov 2019 01:20:18 +1300
Subject: [squid-users] limit new req/sec on squid to X per sec
In-Reply-To: <BCD5FED6-8FCC-49B7-A165-0B423DB79908@netstream.ps>
References: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
 <76eb37dc-2e63-502e-ea59-4eb6dc09f2af@treenet.co.nz>
 <BCD5FED6-8FCC-49B7-A165-0B423DB79908@netstream.ps>
Message-ID: <0e4eec16-0e03-21e0-c03c-63df9e963581@treenet.co.nz>

On 28/11/19 1:03 am, --Ahmad-- wrote:
> Hello Amos , Thank you for your response .
> 
> we have an APP behind squid http APP that will crash if # of (req/sec ) exceeded X .
> it won?t crash about Already established session , it only care about new req/sec hitting squid .
> 

That does not make sense. Any server (aka. app *behind* Squid) does not
see all requests *arriving* at Squid, only the ones Squid sends to it.


> I think its doable by iptables , but i really was hopping we can do it from squid level .
> 

iptables would be right if you actually mean new TCP connections per second.

If you actually mean HTTP requests per second, then you would need
Squid. But since this is completely counter to the goals of a proxy
(*increasing* req/sec) you will need an external_acl_type helper to
delay requests.

In current Squid we have a helper called ext_delayer_acl which delays
each request by a fixed amount of time. You may be able to use that as
the basis of one that does what you need.


>
> so you can imagine http req/sec or tcp req/sec same here as squid is
being used only on http protocol .


Er, that does not make sense. HTTP protocol has infinite number of
requests per single TCP connection. There is no equivalence.



Amos


From ahmed.zaeem at netstream.ps  Wed Nov 27 12:32:47 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 27 Nov 2019 15:32:47 +0300
Subject: [squid-users] limit new req/sec on squid to X per sec
In-Reply-To: <0e4eec16-0e03-21e0-c03c-63df9e963581@treenet.co.nz>
References: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
 <76eb37dc-2e63-502e-ea59-4eb6dc09f2af@treenet.co.nz>
 <BCD5FED6-8FCC-49B7-A165-0B423DB79908@netstream.ps>
 <0e4eec16-0e03-21e0-c03c-63df9e963581@treenet.co.nz>
Message-ID: <2A1CF533-7632-4C8C-A64A-AC7FABBA2776@netstream.ps>

Hi Amos , Thank you for your reply ,



We ll you correct corresponding to TCP/HTTP .

but my main concern is here its just POST/GET with single reply from our API server .

Its  just one TCP connection  one HTTP connection .


But yes i will work on other solutions since squid is not the right place for that .

Thanks a lot ! 


> On Nov 27, 2019, at 3:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 28/11/19 1:03 am, --Ahmad-- wrote:
>> Hello Amos , Thank you for your response .
>> 
>> we have an APP behind squid http APP that will crash if # of (req/sec ) exceeded X .
>> it won?t crash about Already established session , it only care about new req/sec hitting squid .
>> 
> 
> That does not make sense. Any server (aka. app *behind* Squid) does not
> see all requests *arriving* at Squid, only the ones Squid sends to it.
> 
> 
>> I think its doable by iptables , but i really was hopping we can do it from squid level .
>> 
> 
> iptables would be right if you actually mean new TCP connections per second.
> 
> If you actually mean HTTP requests per second, then you would need
> Squid. But since this is completely counter to the goals of a proxy
> (*increasing* req/sec) you will need an external_acl_type helper to
> delay requests.
> 
> In current Squid we have a helper called ext_delayer_acl which delays
> each request by a fixed amount of time. You may be able to use that as
> the basis of one that does what you need.
> 
> 
>> 
>> so you can imagine http req/sec or tcp req/sec same here as squid is
> being used only on http protocol .
> 
> 
> Er, that does not make sense. HTTP protocol has infinite number of
> requests per single TCP connection. There is no equivalence.
> 
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Nov 27 13:35:22 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Nov 2019 02:35:22 +1300
Subject: [squid-users] making proxy-int to talk to proxy-ext
In-Reply-To: <CAGU_CiLRUyCdJStXFEhj-MBrdgvxhUJapDh2XXVHvLBZVpT27g@mail.gmail.com>
References: <CAGU_Ci+qqfiboRwxQ_b6aMT80yNxXe2VW4+wNQuRCg7zSH+XpA@mail.gmail.com>
 <065009ab-3993-f44f-7757-43ee81e10a7e@measurement-factory.com>
 <CAGU_CiLRUyCdJStXFEhj-MBrdgvxhUJapDh2XXVHvLBZVpT27g@mail.gmail.com>
Message-ID: <2f7ba677-dd2d-f319-df21-e4669a64a783@treenet.co.nz>

On 27/11/19 11:56 am, robert k Wild wrote:
> Hi Alex,
> 
> i have done some more troubleshooting and my external proxy is good, i
> get no errors and i have got one of my DMZ hosts connected to it and i
> can browse the web, but my internal proxy cant contact my external
> proxy, this is the error when i run it -
> 
> 2019/11/26 22:53:28| Error parsing SSL Server Hello Message on FD 15
> 2019/11/26 22:53:28| ERROR: negotiating TLS on FD 15: error:140770FC:SSL
> routines:SSL23_GET_SERVER_HELLO: ? ? ? ? ?unknown protocol (1/-1/0)
> 2019/11/26 22:53:28| TCP connection to 172.16.55.21/3128
> <http://172.16.55.21/3128> failed
> 2019/11/26 22:53:28| Detected DEAD Parent: 172.16.55.21
> 2019/11/26 22:53:28| Error negotiating SSL connection on FD 13:
> error:00000001:lib(0):func(0):reason(1) ( ? ? ? ? ?1/0)
> 
> this is my config on my internal proxy -
> 
> #
> # Recommended minimum configuration:
> #
> 
> #SSL
> http_port 3128 ssl-bump \
> cert=/etc/squid/ssl_cert/myCA.pem \
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all
> 
...
> #squid proxy in DMZ on internet
> cache_peer 172.16.55.21 parent 3128 0 default
...
> never_direct allow all
> 

So, all traffic MUST use the cache_peer which cannot handle TLS input.


You need to either configure TLS/SSL in the peer and set the cache_peer
line appropriately for that so this proxy can re-encrypt traffic going
there,

OR, upgrade to Squid-5 which has the ability to re-encrypt and send to a
regular peer proxy.


Amos


From rousskov at measurement-factory.com  Wed Nov 27 14:54:05 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Nov 2019 09:54:05 -0500
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
 <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
 <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>
Message-ID: <aac5f6fd-7267-6148-b3f5-98e5b7e33ff0@measurement-factory.com>

On 11/26/19 4:12 PM, Felipe Arturo Polanco wrote:

> The flow is the following:
> ICAP transaction is sent to ICAP server with a PREVIEW header
> ICAP server sends ICAP header 100 Continue
> ICAP server sends ICAP header 200 OK to start data transfer

(*) Your notes below imply that the ICAP server also sends the embedded
HTTP message header back to Squid (and not just the ICAP 200 header).


> <data transfer begins>
> ICAP server receives a chunk, checks if its the last chunk, if not then 
> append to temp file and send it back to Squid;

The "send it back to Squid" part implies that not only your ICAP server
sent an ICAP 200 response header, but it sent the HTTP message header as
well. See (*) above. Once the ICAP server sent the HTTP message header,
it is committed to finish sending that HTTP message (and nothing else).


> if it is the last chunk then analyze the temp file for virus.
> <repeat for next data transfer>
> If virus found then send encapsulated HTTP header 307 redirect.

This is an ICAP service bug: One cannot send a second encapsulated HTTP
message in one ICAP response. A single ICAP response cannot contain
multiple HTTP messages. The ICAP protocol does not allow for that, and
there would be no way to actually support something like that in an ICAP
client because the HTTP message cannot be interrupted and replaced with
another mid-flight. You may assume that the HTTP client is already
seeing the beginning of the first HTTP response. The train has left the
station.

If the above is accurate, and you are using the ICAP service correctly,
then the ICAP service that allowed you to do the above is badly broken,
probably written by somebody who thought that ICAP is "easy". You may be
better off reusing an existing higher-quality ICAP service instead.


> If virus not found, send the last chunk to squid.
> 
> The part where we send 307 is the part that Squid doesn't like, I 
> believe is because we are not sending the last chunk since the file is a 
> virus.

Not exactly: Even if you send the last HTTP chunk, you would not be able
to follow up with an HTTP 307 message. The HTTP fate of this transaction
was sealed when you started trickling virgin HTTP message pieces back to
Squid (and, hence, back to the HTTP client talking to Squid).


HTH,

Alex.


> On Tue, Nov 26, 2019 at 4:52 PM Alex Rousskov wrote:
> 
>     On 11/26/19 2:52 PM, Felipe Arturo Polanco wrote:
> 
>      > We are sending an encapsulated HTTP 307 redirect webpage header
>     whenever
>      > a Virus is found and stop sending any other data after that
> 
>     You must use ICAP status code 200 then. Make sure your encapsulated HTTP
>     307 body (if any) is properly sent to Squid.
> 
> 
>      > but squid
>      > complains about ICAP failure when we do that:
>      > Adaptation::Icap::Xaction::noteCommRead threw exception:
>     corrupted chunk
>      > size
> 
>     What chunk size did Squid not like? You should be able to tell by
>     looking at the packet capture of the failed transaction (or low-level
>     Squid debugging).
> 
> 
>      > We are not sending an ICAP header at this point because we
>     already told
>      > Squid ICAP 200 OK header and begun a body transaction, we send some
>      > chunks back to the client for progress and hold the last part for
>     scanning.
> 
>     Are you sending HTTP 307 body chunks to Squid? How do you indicate that
>     no more chunks will be coming?
> 
>     It sounds like you are trying to cram two HTTP messages (one with the
>     original HTTP response body prefix and one with a generated 307
>     redirect) into one ICAP response, which is impossible, but perhaps I
>     misunderstood your description. It would help if you post a sample (but
>     complete) ICAP response that Squid does not like.
> 
> 
>      > Ideally, we would like to just send our 307 to Squid and not
>     having it
>      > count as a failure.
> 
>     Yes, a 200 ICAP response with an embedded HTTP 307 response should work
>     just fine, but all its pieces should be properly formed (and there
>     should be no extras).
> 
>     Alex.
> 
> 
>      > On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov wrote:
>      >
>      >? ? ?On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
>      >
>      >? ? ?> While we can successfully scan our files and do content
>     adaptation, we
>      >? ? ?> have been struggling?to find a way to close the ICAP
>     transaction
>      >? ? ?before
>      >? ? ?> passing the whole body back to squid and at the same time
>     avoid squid
>      >? ? ?> marking one icap failure.
>      >
>      >? ? ?Squid needs a valid ICAP response. The right ICAP response
>     status code
>      >? ? ?depends on what you want Squid to do after receiving that
>     response. You
>      >? ? ?have mentioned what you do _not_ want Squid to do (i.e.
>     increase the
>      >? ? ?failure count), but that still leaves a lot of options.
>      >
>      >
>      >? ? ?> This is for an ICAP server that does Virus scanning and if
>     virus
>      >? ? ?found,
>      >? ? ?> the body is not sent back.
>      >
>      >? ? ?What do you want Squid to do when the ICAP service finds a
>     virus? For
>      >? ? ?example, what message do you want Squid to send to the next
>     HTTP hop?
>      >
>      >? ? ?Alex.
>      >
> 



From felipeapolanco at gmail.com  Wed Nov 27 16:01:04 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Wed, 27 Nov 2019 12:01:04 -0400
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <aac5f6fd-7267-6148-b3f5-98e5b7e33ff0@measurement-factory.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
 <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
 <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>
 <aac5f6fd-7267-6148-b3f5-98e5b7e33ff0@measurement-factory.com>
Message-ID: <CADcj3=4MpB6cMjKHLS=OUnR3XZQr0bBC2RCbkFfXuk98FEgVyA@mail.gmail.com>

Thanks for the detailed response Alex, this is very helpful.

How can we then terminate an ICAP 200 OK transaction to squid without
sending the complete body back to it? We don't want squid to mark an ICAP
failure on us if we just close the TCP connection.

On Wed, Nov 27, 2019 at 10:54 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/26/19 4:12 PM, Felipe Arturo Polanco wrote:
>
> > The flow is the following:
> > ICAP transaction is sent to ICAP server with a PREVIEW header
> > ICAP server sends ICAP header 100 Continue
> > ICAP server sends ICAP header 200 OK to start data transfer
>
> (*) Your notes below imply that the ICAP server also sends the embedded
> HTTP message header back to Squid (and not just the ICAP 200 header).
>
>
> > <data transfer begins>
> > ICAP server receives a chunk, checks if its the last chunk, if not then
> > append to temp file and send it back to Squid;
>
> The "send it back to Squid" part implies that not only your ICAP server
> sent an ICAP 200 response header, but it sent the HTTP message header as
> well. See (*) above. Once the ICAP server sent the HTTP message header,
> it is committed to finish sending that HTTP message (and nothing else).
>
>
> > if it is the last chunk then analyze the temp file for virus.
> > <repeat for next data transfer>
> > If virus found then send encapsulated HTTP header 307 redirect.
>
> This is an ICAP service bug: One cannot send a second encapsulated HTTP
> message in one ICAP response. A single ICAP response cannot contain
> multiple HTTP messages. The ICAP protocol does not allow for that, and
> there would be no way to actually support something like that in an ICAP
> client because the HTTP message cannot be interrupted and replaced with
> another mid-flight. You may assume that the HTTP client is already
> seeing the beginning of the first HTTP response. The train has left the
> station.
>
> If the above is accurate, and you are using the ICAP service correctly,
> then the ICAP service that allowed you to do the above is badly broken,
> probably written by somebody who thought that ICAP is "easy". You may be
> better off reusing an existing higher-quality ICAP service instead.
>
>
> > If virus not found, send the last chunk to squid.
> >
> > The part where we send 307 is the part that Squid doesn't like, I
> > believe is because we are not sending the last chunk since the file is a
> > virus.
>
> Not exactly: Even if you send the last HTTP chunk, you would not be able
> to follow up with an HTTP 307 message. The HTTP fate of this transaction
> was sealed when you started trickling virgin HTTP message pieces back to
> Squid (and, hence, back to the HTTP client talking to Squid).
>
>
> HTH,
>
> Alex.
>
>
> > On Tue, Nov 26, 2019 at 4:52 PM Alex Rousskov wrote:
> >
> >     On 11/26/19 2:52 PM, Felipe Arturo Polanco wrote:
> >
> >      > We are sending an encapsulated HTTP 307 redirect webpage header
> >     whenever
> >      > a Virus is found and stop sending any other data after that
> >
> >     You must use ICAP status code 200 then. Make sure your encapsulated
> HTTP
> >     307 body (if any) is properly sent to Squid.
> >
> >
> >      > but squid
> >      > complains about ICAP failure when we do that:
> >      > Adaptation::Icap::Xaction::noteCommRead threw exception:
> >     corrupted chunk
> >      > size
> >
> >     What chunk size did Squid not like? You should be able to tell by
> >     looking at the packet capture of the failed transaction (or low-level
> >     Squid debugging).
> >
> >
> >      > We are not sending an ICAP header at this point because we
> >     already told
> >      > Squid ICAP 200 OK header and begun a body transaction, we send
> some
> >      > chunks back to the client for progress and hold the last part for
> >     scanning.
> >
> >     Are you sending HTTP 307 body chunks to Squid? How do you indicate
> that
> >     no more chunks will be coming?
> >
> >     It sounds like you are trying to cram two HTTP messages (one with the
> >     original HTTP response body prefix and one with a generated 307
> >     redirect) into one ICAP response, which is impossible, but perhaps I
> >     misunderstood your description. It would help if you post a sample
> (but
> >     complete) ICAP response that Squid does not like.
> >
> >
> >      > Ideally, we would like to just send our 307 to Squid and not
> >     having it
> >      > count as a failure.
> >
> >     Yes, a 200 ICAP response with an embedded HTTP 307 response should
> work
> >     just fine, but all its pieces should be properly formed (and there
> >     should be no extras).
> >
> >     Alex.
> >
> >
> >      > On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov wrote:
> >      >
> >      >     On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
> >      >
> >      >     > While we can successfully scan our files and do content
> >     adaptation, we
> >      >     > have been struggling to find a way to close the ICAP
> >     transaction
> >      >     before
> >      >     > passing the whole body back to squid and at the same time
> >     avoid squid
> >      >     > marking one icap failure.
> >      >
> >      >     Squid needs a valid ICAP response. The right ICAP response
> >     status code
> >      >     depends on what you want Squid to do after receiving that
> >     response. You
> >      >     have mentioned what you do _not_ want Squid to do (i.e.
> >     increase the
> >      >     failure count), but that still leaves a lot of options.
> >      >
> >      >
> >      >     > This is for an ICAP server that does Virus scanning and if
> >     virus
> >      >     found,
> >      >     > the body is not sent back.
> >      >
> >      >     What do you want Squid to do when the ICAP service finds a
> >     virus? For
> >      >     example, what message do you want Squid to send to the next
> >     HTTP hop?
> >      >
> >      >     Alex.
> >      >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191127/bc0dfa4b/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov 27 16:44:51 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Nov 2019 11:44:51 -0500
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <CADcj3=4MpB6cMjKHLS=OUnR3XZQr0bBC2RCbkFfXuk98FEgVyA@mail.gmail.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
 <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
 <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>
 <aac5f6fd-7267-6148-b3f5-98e5b7e33ff0@measurement-factory.com>
 <CADcj3=4MpB6cMjKHLS=OUnR3XZQr0bBC2RCbkFfXuk98FEgVyA@mail.gmail.com>
Message-ID: <6b22633b-9daf-71aa-cd91-1e9d48fd2942@measurement-factory.com>

On 11/27/19 11:01 AM, Felipe Arturo Polanco wrote:

> How can we then terminate an ICAP 200 OK transaction to squid without
> sending the complete body back to it? We don't want squid to mark an
> ICAP failure on us if we just close the TCP connection.

To properly answer your question, I have to come back to the question I
asked earlier: What do you want Squid to do when your ICAP service finds
a virus after trickling a few virgin HTTP body bytes to the HTTP client?

The "we want Squid to send an HTTP 307 redirect to the client" desire
you responded with earlier was ruled impossible to satisfy in your
trickling environment. A few realistic options remain though, including:

1. Terminate Squid-to-client transmission as if the whole virgin HTTP
response body was sent to the client.

2. Terminate Squid-to-client transmission while indicating (to that same
client) that the HTTP response body was cut short (i.e. that the
delivery of the response was aborted).

3. #2 plus annotate the transaction specially in Squid access.log and/or
detect this special outcome using Squid ACLs.

Alex.


> On Wed, Nov 27, 2019 at 10:54 AM Alex Rousskov wrote:
> 
>     On 11/26/19 4:12 PM, Felipe Arturo Polanco wrote:
> 
>     > The flow is the following:
>     > ICAP transaction is sent to ICAP server with a PREVIEW header
>     > ICAP server sends ICAP header 100 Continue
>     > ICAP server sends ICAP header 200 OK to start data transfer
> 
>     (*) Your notes below imply that the ICAP server also sends the embedded
>     HTTP message header back to Squid (and not just the ICAP 200 header).
> 
> 
>     > <data transfer begins>
>     > ICAP server receives a chunk, checks if its the last chunk, if not
>     then
>     > append to temp file and send it back to Squid;
> 
>     The "send it back to Squid" part implies that not only your ICAP server
>     sent an ICAP 200 response header, but it sent the HTTP message header as
>     well. See (*) above. Once the ICAP server sent the HTTP message header,
>     it is committed to finish sending that HTTP message (and nothing else).
> 
> 
>     > if it is the last chunk then analyze the temp file for virus.
>     > <repeat for next data transfer>
>     > If virus found then send encapsulated HTTP header 307 redirect.
> 
>     This is an ICAP service bug: One cannot send a second encapsulated HTTP
>     message in one ICAP response. A single ICAP response cannot contain
>     multiple HTTP messages. The ICAP protocol does not allow for that, and
>     there would be no way to actually support something like that in an ICAP
>     client because the HTTP message cannot be interrupted and replaced with
>     another mid-flight. You may assume that the HTTP client is already
>     seeing the beginning of the first HTTP response. The train has left the
>     station.
> 
>     If the above is accurate, and you are using the ICAP service correctly,
>     then the ICAP service that allowed you to do the above is badly broken,
>     probably written by somebody who thought that ICAP is "easy". You may be
>     better off reusing an existing higher-quality ICAP service instead.
> 
> 
>     > If virus not found, send the last chunk to squid.
>     >
>     > The part where we send 307 is the part that Squid doesn't like, I
>     > believe is because we are not sending the last chunk since the
>     file is a
>     > virus.
> 
>     Not exactly: Even if you send the last HTTP chunk, you would not be able
>     to follow up with an HTTP 307 message. The HTTP fate of this transaction
>     was sealed when you started trickling virgin HTTP message pieces back to
>     Squid (and, hence, back to the HTTP client talking to Squid).
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > On Tue, Nov 26, 2019 at 4:52 PM Alex Rousskov wrote:
>     >
>     >? ? ?On 11/26/19 2:52 PM, Felipe Arturo Polanco wrote:
>     >
>     >? ? ? > We are sending an encapsulated HTTP 307 redirect webpage header
>     >? ? ?whenever
>     >? ? ? > a Virus is found and stop sending any other data after that
>     >
>     >? ? ?You must use ICAP status code 200 then. Make sure your
>     encapsulated HTTP
>     >? ? ?307 body (if any) is properly sent to Squid.
>     >
>     >
>     >? ? ? > but squid
>     >? ? ? > complains about ICAP failure when we do that:
>     >? ? ? > Adaptation::Icap::Xaction::noteCommRead threw exception:
>     >? ? ?corrupted chunk
>     >? ? ? > size
>     >
>     >? ? ?What chunk size did Squid not like? You should be able to tell by
>     >? ? ?looking at the packet capture of the failed transaction (or
>     low-level
>     >? ? ?Squid debugging).
>     >
>     >
>     >? ? ? > We are not sending an ICAP header at this point because we
>     >? ? ?already told
>     >? ? ? > Squid ICAP 200 OK header and begun a body transaction, we
>     send some
>     >? ? ? > chunks back to the client for progress and hold the last
>     part for
>     >? ? ?scanning.
>     >
>     >? ? ?Are you sending HTTP 307 body chunks to Squid? How do you
>     indicate that
>     >? ? ?no more chunks will be coming?
>     >
>     >? ? ?It sounds like you are trying to cram two HTTP messages (one
>     with the
>     >? ? ?original HTTP response body prefix and one with a generated 307
>     >? ? ?redirect) into one ICAP response, which is impossible, but
>     perhaps I
>     >? ? ?misunderstood your description. It would help if you post a
>     sample (but
>     >? ? ?complete) ICAP response that Squid does not like.
>     >
>     >
>     >? ? ? > Ideally, we would like to just send our 307 to Squid and not
>     >? ? ?having it
>     >? ? ? > count as a failure.
>     >
>     >? ? ?Yes, a 200 ICAP response with an embedded HTTP 307 response
>     should work
>     >? ? ?just fine, but all its pieces should be properly formed (and there
>     >? ? ?should be no extras).
>     >
>     >? ? ?Alex.
>     >
>     >
>     >? ? ? > On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov wrote:
>     >? ? ? >
>     >? ? ? >? ? ?On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
>     >? ? ? >
>     >? ? ? >? ? ?> While we can successfully scan our files and do content
>     >? ? ?adaptation, we
>     >? ? ? >? ? ?> have been struggling?to find a way to close the ICAP
>     >? ? ?transaction
>     >? ? ? >? ? ?before
>     >? ? ? >? ? ?> passing the whole body back to squid and at the same time
>     >? ? ?avoid squid
>     >? ? ? >? ? ?> marking one icap failure.
>     >? ? ? >
>     >? ? ? >? ? ?Squid needs a valid ICAP response. The right ICAP response
>     >? ? ?status code
>     >? ? ? >? ? ?depends on what you want Squid to do after receiving that
>     >? ? ?response. You
>     >? ? ? >? ? ?have mentioned what you do _not_ want Squid to do (i.e.
>     >? ? ?increase the
>     >? ? ? >? ? ?failure count), but that still leaves a lot of options.
>     >? ? ? >
>     >? ? ? >
>     >? ? ? >? ? ?> This is for an ICAP server that does Virus scanning
>     and if
>     >? ? ?virus
>     >? ? ? >? ? ?found,
>     >? ? ? >? ? ?> the body is not sent back.
>     >? ? ? >
>     >? ? ? >? ? ?What do you want Squid to do when the ICAP service finds a
>     >? ? ?virus? For
>     >? ? ? >? ? ?example, what message do you want Squid to send to the next
>     >? ? ?HTTP hop?
>     >? ? ? >
>     >? ? ? >? ? ?Alex.
>     >? ? ? >
>     >
> 



From felipeapolanco at gmail.com  Wed Nov 27 18:42:10 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Wed, 27 Nov 2019 14:42:10 -0400
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <6b22633b-9daf-71aa-cd91-1e9d48fd2942@measurement-factory.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
 <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
 <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>
 <aac5f6fd-7267-6148-b3f5-98e5b7e33ff0@measurement-factory.com>
 <CADcj3=4MpB6cMjKHLS=OUnR3XZQr0bBC2RCbkFfXuk98FEgVyA@mail.gmail.com>
 <6b22633b-9daf-71aa-cd91-1e9d48fd2942@measurement-factory.com>
Message-ID: <CADcj3=7WTWzVb2G+bnM12BiWPiOb15aXH-fSEp75rbERC+MeLw@mail.gmail.com>

Can you describe the process of option 2?

Terminate Squid-to-client transmission while indicating (to that same
client) that the HTTP response body was cut short (i.e. that the
delivery of the response was aborted).

What TCP Flag or ICAP header should the ICAP server send in order to inform
connection aborted??

On Wed, Nov 27, 2019 at 12:44 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/27/19 11:01 AM, Felipe Arturo Polanco wrote:
>
> > How can we then terminate an ICAP 200 OK transaction to squid without
> > sending the complete body back to it? We don't want squid to mark an
> > ICAP failure on us if we just close the TCP connection.
>
> To properly answer your question, I have to come back to the question I
> asked earlier: What do you want Squid to do when your ICAP service finds
> a virus after trickling a few virgin HTTP body bytes to the HTTP client?
>
> The "we want Squid to send an HTTP 307 redirect to the client" desire
> you responded with earlier was ruled impossible to satisfy in your
> trickling environment. A few realistic options remain though, including:
>
> 1. Terminate Squid-to-client transmission as if the whole virgin HTTP
> response body was sent to the client.
>
> 2. Terminate Squid-to-client transmission while indicating (to that same
> client) that the HTTP response body was cut short (i.e. that the
> delivery of the response was aborted).
>
> 3. #2 plus annotate the transaction specially in Squid access.log and/or
> detect this special outcome using Squid ACLs.
>
> Alex.
>
>
> > On Wed, Nov 27, 2019 at 10:54 AM Alex Rousskov wrote:
> >
> >     On 11/26/19 4:12 PM, Felipe Arturo Polanco wrote:
> >
> >     > The flow is the following:
> >     > ICAP transaction is sent to ICAP server with a PREVIEW header
> >     > ICAP server sends ICAP header 100 Continue
> >     > ICAP server sends ICAP header 200 OK to start data transfer
> >
> >     (*) Your notes below imply that the ICAP server also sends the
> embedded
> >     HTTP message header back to Squid (and not just the ICAP 200 header).
> >
> >
> >     > <data transfer begins>
> >     > ICAP server receives a chunk, checks if its the last chunk, if not
> >     then
> >     > append to temp file and send it back to Squid;
> >
> >     The "send it back to Squid" part implies that not only your ICAP
> server
> >     sent an ICAP 200 response header, but it sent the HTTP message
> header as
> >     well. See (*) above. Once the ICAP server sent the HTTP message
> header,
> >     it is committed to finish sending that HTTP message (and nothing
> else).
> >
> >
> >     > if it is the last chunk then analyze the temp file for virus.
> >     > <repeat for next data transfer>
> >     > If virus found then send encapsulated HTTP header 307 redirect.
> >
> >     This is an ICAP service bug: One cannot send a second encapsulated
> HTTP
> >     message in one ICAP response. A single ICAP response cannot contain
> >     multiple HTTP messages. The ICAP protocol does not allow for that,
> and
> >     there would be no way to actually support something like that in an
> ICAP
> >     client because the HTTP message cannot be interrupted and replaced
> with
> >     another mid-flight. You may assume that the HTTP client is already
> >     seeing the beginning of the first HTTP response. The train has left
> the
> >     station.
> >
> >     If the above is accurate, and you are using the ICAP service
> correctly,
> >     then the ICAP service that allowed you to do the above is badly
> broken,
> >     probably written by somebody who thought that ICAP is "easy". You
> may be
> >     better off reusing an existing higher-quality ICAP service instead.
> >
> >
> >     > If virus not found, send the last chunk to squid.
> >     >
> >     > The part where we send 307 is the part that Squid doesn't like, I
> >     > believe is because we are not sending the last chunk since the
> >     file is a
> >     > virus.
> >
> >     Not exactly: Even if you send the last HTTP chunk, you would not be
> able
> >     to follow up with an HTTP 307 message. The HTTP fate of this
> transaction
> >     was sealed when you started trickling virgin HTTP message pieces
> back to
> >     Squid (and, hence, back to the HTTP client talking to Squid).
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >     > On Tue, Nov 26, 2019 at 4:52 PM Alex Rousskov wrote:
> >     >
> >     >     On 11/26/19 2:52 PM, Felipe Arturo Polanco wrote:
> >     >
> >     >      > We are sending an encapsulated HTTP 307 redirect webpage
> header
> >     >     whenever
> >     >      > a Virus is found and stop sending any other data after that
> >     >
> >     >     You must use ICAP status code 200 then. Make sure your
> >     encapsulated HTTP
> >     >     307 body (if any) is properly sent to Squid.
> >     >
> >     >
> >     >      > but squid
> >     >      > complains about ICAP failure when we do that:
> >     >      > Adaptation::Icap::Xaction::noteCommRead threw exception:
> >     >     corrupted chunk
> >     >      > size
> >     >
> >     >     What chunk size did Squid not like? You should be able to tell
> by
> >     >     looking at the packet capture of the failed transaction (or
> >     low-level
> >     >     Squid debugging).
> >     >
> >     >
> >     >      > We are not sending an ICAP header at this point because we
> >     >     already told
> >     >      > Squid ICAP 200 OK header and begun a body transaction, we
> >     send some
> >     >      > chunks back to the client for progress and hold the last
> >     part for
> >     >     scanning.
> >     >
> >     >     Are you sending HTTP 307 body chunks to Squid? How do you
> >     indicate that
> >     >     no more chunks will be coming?
> >     >
> >     >     It sounds like you are trying to cram two HTTP messages (one
> >     with the
> >     >     original HTTP response body prefix and one with a generated 307
> >     >     redirect) into one ICAP response, which is impossible, but
> >     perhaps I
> >     >     misunderstood your description. It would help if you post a
> >     sample (but
> >     >     complete) ICAP response that Squid does not like.
> >     >
> >     >
> >     >      > Ideally, we would like to just send our 307 to Squid and not
> >     >     having it
> >     >      > count as a failure.
> >     >
> >     >     Yes, a 200 ICAP response with an embedded HTTP 307 response
> >     should work
> >     >     just fine, but all its pieces should be properly formed (and
> there
> >     >     should be no extras).
> >     >
> >     >     Alex.
> >     >
> >     >
> >     >      > On Tue, Nov 26, 2019 at 3:44 PM Alex Rousskov wrote:
> >     >      >
> >     >      >     On 11/26/19 10:15 AM, Felipe Arturo Polanco wrote:
> >     >      >
> >     >      >     > While we can successfully scan our files and do
> content
> >     >     adaptation, we
> >     >      >     > have been struggling to find a way to close the ICAP
> >     >     transaction
> >     >      >     before
> >     >      >     > passing the whole body back to squid and at the same
> time
> >     >     avoid squid
> >     >      >     > marking one icap failure.
> >     >      >
> >     >      >     Squid needs a valid ICAP response. The right ICAP
> response
> >     >     status code
> >     >      >     depends on what you want Squid to do after receiving
> that
> >     >     response. You
> >     >      >     have mentioned what you do _not_ want Squid to do (i.e.
> >     >     increase the
> >     >      >     failure count), but that still leaves a lot of options.
> >     >      >
> >     >      >
> >     >      >     > This is for an ICAP server that does Virus scanning
> >     and if
> >     >     virus
> >     >      >     found,
> >     >      >     > the body is not sent back.
> >     >      >
> >     >      >     What do you want Squid to do when the ICAP service
> finds a
> >     >     virus? For
> >     >      >     example, what message do you want Squid to send to the
> next
> >     >     HTTP hop?
> >     >      >
> >     >      >     Alex.
> >     >      >
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191127/730143d2/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov 27 20:07:47 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Nov 2019 15:07:47 -0500
Subject: [squid-users] What is the proper way to close an ICAP
	transaction?
In-Reply-To: <CADcj3=7WTWzVb2G+bnM12BiWPiOb15aXH-fSEp75rbERC+MeLw@mail.gmail.com>
References: <CADcj3=5-SWRAZBJcPX1V5osf1=hn25Bcw_6kqSp1+RJQkqHppg@mail.gmail.com>
 <14ab3319-ce9a-0655-d565-b079b720322d@measurement-factory.com>
 <CADcj3=6f-WVK2d9rkb9m5MNjNrmmU-70Om3XyABOv0m-nOyyOg@mail.gmail.com>
 <b39868dc-7236-1f37-79ae-4a9343d3a472@measurement-factory.com>
 <CADcj3=7wahJoL3-+itCnt13LEpQ6a3JDDh0rfLf=8cQDGO4DWA@mail.gmail.com>
 <aac5f6fd-7267-6148-b3f5-98e5b7e33ff0@measurement-factory.com>
 <CADcj3=4MpB6cMjKHLS=OUnR3XZQr0bBC2RCbkFfXuk98FEgVyA@mail.gmail.com>
 <6b22633b-9daf-71aa-cd91-1e9d48fd2942@measurement-factory.com>
 <CADcj3=7WTWzVb2G+bnM12BiWPiOb15aXH-fSEp75rbERC+MeLw@mail.gmail.com>
Message-ID: <5e297aa4-c454-a513-f7c1-a30b85b62132@measurement-factory.com>

On 11/27/19 1:42 PM, Felipe Arturo Polanco wrote:
> Can you describe the process of option 2?
> 
> Terminate Squid-to-client transmission while indicating (to that same
> client) that the HTTP response body was cut short (i.e. that the
> delivery of the response was aborted).?

For HTTP responses where the body size is determined by the
Content-Length header, this option is already supported by the ICAP
protocol: The ICAP service can send the last-chunk at any time after
sending the HTTP header. A last-chunk sent before the last response body
byte indicates a truncated response. I have not tested Squid support for
this use case, but either Squid already closes the TCP client-to-Squid
connection after delivering the already trickled data, or Squid can be
modified to do that.

For all the other HTTP responses (including chunked), an ICAP extension
would be required to trigger the desired behavior in Squid. That ICAP
extension can be implemented as a chunk extension, similar to the
"use-original-body" chunk extension defined in ICAP 206 Partial Content
responses[1].

[1]
http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt

I would consider reusing the "ieof" spelling for this new extension
because the semantics of this new ICAP extension is equivalent to the
standard ieof semantics (only their use cases/scope differ). Support for
this "ieof response" extension would need to be signaled by ICAP clients
via the Allow header, similar to how support for ICAP 206 Partial
Content responses is signaled in [1].

Squid will need to be modified to honor response ieof by closing the
corresponding client-to-Squid TCP connection. With a bit more effort,
the ieof response extension can be adjusted to also signal that Squid
does not need to finish sending any buffered response data, but that
enhancement is optional.


If ieof response extension is supported, then there is no need to treat
"HTTP responses where the body size is determined by the Content-Length
header" (discussed as a special use case in the beginning of this email)
specially -- the ICAP service should use ieof for all truncated
responses. I documented that special use case because you do not need
ieof response support if all blocked responses in your use case fall
into that "body size is determined by the Content-Length header"
category; you may still need to modify Squid to abort the TCP
client-to-Squid connection though.


Quality pull requests adding support for the ieof response extension to
Squid (or their sponsorships) are welcomed. Same for fixing Squid
behavior when reacting to a truncated embedded HTTP response (if such a
fix is needed).

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.


> What TCP Flag or ICAP header should the ICAP server send in order to
> inform connection aborted???
> 
> On Wed, Nov 27, 2019 at 12:44 PM Alex Rousskov wrote:
> 
>     On 11/27/19 11:01 AM, Felipe Arturo Polanco wrote:
> 
>     > How can we then terminate an ICAP 200 OK transaction to squid without
>     > sending the complete body back to it? We don't want squid to mark an
>     > ICAP failure on us if we just close the TCP connection.
> 
>     To properly answer your question, I have to come back to the question I
>     asked earlier: What do you want Squid to do when your ICAP service finds
>     a virus after trickling a few virgin HTTP body bytes to the HTTP client?
> 
>     The "we want Squid to send an HTTP 307 redirect to the client" desire
>     you responded with earlier was ruled impossible to satisfy in your
>     trickling environment. A few realistic options remain though, including:
> 
>     1. Terminate Squid-to-client transmission as if the whole virgin HTTP
>     response body was sent to the client.
> 
>     2. Terminate Squid-to-client transmission while indicating (to that same
>     client) that the HTTP response body was cut short (i.e. that the
>     delivery of the response was aborted).
> 
>     3. #2 plus annotate the transaction specially in Squid access.log and/or
>     detect this special outcome using Squid ACLs.
> 
>     Alex.
> 


From rousskov at measurement-factory.com  Wed Nov 27 20:42:14 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Nov 2019 15:42:14 -0500
Subject: [squid-users] limit new req/sec on squid to X per sec
In-Reply-To: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
References: <2F3C20C4-7DA3-4C25-9E06-4C55A1E2A916@netstream.ps>
Message-ID: <417d403f-4035-a178-5715-6173ff8b5654@measurement-factory.com>

On 11/27/19 12:31 AM, --Ahmad-- wrote:

> im looking for limiting TCP req/sec on squid to X speed .


There are many terminology problems on this thread, but just for the
record, you can use Squid external ACLs to limit:

1. the rate of incoming HTTP requests
2. the rate of outgoing HTTP requests
3. the acceptance rate of incoming HTTP/TCP connections
4. the establishment rate of outgoing HTTP/TCP connections

In all these cases, Squid would have to act (i.e. block or delay) the
requests or connections exceeding the configured rate _after_ parsing
the offending request[1,2,3 and may be 4] or even response[4]. This
delayed reaction may be enough for your use case of protecting a service
behind Squid, but it is a deadly limitation in many contexts (e.g., DoS
mitigation).

Until support for connection IDs is added to Squid (there is a project
for that), your external ACL would have to rely on TCP/IP addresses to
identify new HTTP/TCP connections (if needed).


Whether Squid is the right tool for the job depends on many factors. One
of the primary factors is whether you need HTTP-level information to
make some of the rate limiting decisions. Another factor is whether you
want to send a user an error response when they exceed the configured
rate. My guess is that cases 1 and 2 are best supported using Squid
while cases 3 and especially 4 may be best implemented using
TCP/IP-level tools such as iptables.


HTH,

Alex.

> say i have an instance running .
> 
> 
> i want to limit it to 100 req/sec for ?new connections ? not  just for concurrent connections .
> 
> so if connection is old or ? established ? its out of the game .
> if the connection is new , all new should be limited to 100 req/sec .
> 
> i made search on all max_conn but it seems count ?concurrent sessions ? even old +  new .
> 
> is there a way in squid to limit only new sessions ?
> 
> 
> Thanks 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rs-squid at lists.microscopium.de  Thu Nov 28 21:34:53 2019
From: rs-squid at lists.microscopium.de (Robert Senger)
Date: Thu, 28 Nov 2019 22:34:53 +0100
Subject: [squid-users] Proper separation of multiple squid instances on the
	same machine
Message-ID: <b65b4463881f7380719fca77bec0815608dcd80e.camel@lists.microscopium.de>

Hi there,

a couple of week ago I've separated a rather complex squid installation
into two instances running on the same machine.

The setup of these two instances is almost identical, despite different
tcp_outgoing_address and stuff like that.

All files on disk and all listening ports are set to different paths /
values, so the instances should not interfere with each other. 

However, today I realized that squid creates files in /dev/shm:

root at prokyon:/dev/shm# ls -l
insgesamt 2292
-rw-------. 1 proxy    proxy          8 Nov 28 22:27 squid-cf__metadata.shm
-rw-------. 1 proxy    proxy       8216 Nov 28 22:27 squid-cf__queues.shm
-rw-------. 1 proxy    proxy         36 Nov 28 22:27 squid-cf__readers.shm
-rw-------. 1 proxy    proxy    2103672 Nov 28 22:27 squid-tls_session_cache.shm

These files are created when the first instance starts. When the second
instance starts, the files get updated/recreated (new mtime/ctime). 

When any of the running instances is stopped, the files are deleted.

So, the two instances collide here.

My questions:

1. Is this a problem?
2. If so, what must I do?

And yes, I am having issues with browsers failing to load page elements from time to time (mostly images, sometimes styles or scripts).

Thanks for help,

Robert


-- 
Robert Senger




From rousskov at measurement-factory.com  Thu Nov 28 23:55:07 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Nov 2019 18:55:07 -0500
Subject: [squid-users] Proper separation of multiple squid instances on
 the same machine
In-Reply-To: <b65b4463881f7380719fca77bec0815608dcd80e.camel@lists.microscopium.de>
References: <b65b4463881f7380719fca77bec0815608dcd80e.camel@lists.microscopium.de>
Message-ID: <11f5a87f-c775-dc43-4b45-ac038ae82827@measurement-factory.com>

On 11/28/19 4:34 PM, Robert Senger wrote:

> a couple of week ago I've separated a rather complex squid installation
> into two instances running on the same machine.
> 
> The setup of these two instances is almost identical, despite different
> tcp_outgoing_address and stuff like that.
> 
> All files on disk and all listening ports are set to different paths /
> values, so the instances should not interfere with each other. 
> 
> However, today I realized that squid creates files in /dev/shm:
> 
> root at prokyon:/dev/shm# ls -l
> insgesamt 2292
> -rw-------. 1 proxy    proxy          8 Nov 28 22:27 squid-cf__metadata.shm
> -rw-------. 1 proxy    proxy       8216 Nov 28 22:27 squid-cf__queues.shm
> -rw-------. 1 proxy    proxy         36 Nov 28 22:27 squid-cf__readers.shm
> -rw-------. 1 proxy    proxy    2103672 Nov 28 22:27 squid-tls_session_cache.shm
> 
> These files are created when the first instance starts. When the second
> instance starts, the files get updated/recreated (new mtime/ctime). 
> 
> When any of the running instances is stopped, the files are deleted.
> 
> So, the two instances collide here.



> My questions:

> 1. Is this a problem?

Yes.


> 2. If so, what must I do?

Use different service names for different Squid instances running
concurrently on the same machine. The service name is specified on the
command line using "squid -n".

AFAICT, this trick is poorly documented and is not supported on some
platforms, but I hope it works in your environment.


HTH,

Alex.


From rs-squid at lists.microscopium.de  Fri Nov 29 11:15:59 2019
From: rs-squid at lists.microscopium.de (Robert Senger)
Date: Fri, 29 Nov 2019 12:15:59 +0100
Subject: [squid-users] Proper separation of multiple squid instances on
 the same machine
In-Reply-To: <11f5a87f-c775-dc43-4b45-ac038ae82827@measurement-factory.com>
References: <b65b4463881f7380719fca77bec0815608dcd80e.camel@lists.microscopium.de>
 <11f5a87f-c775-dc43-4b45-ac038ae82827@measurement-factory.com>
Message-ID: <a0402521ddc94fcb9fb773830a6367ff2c2a23db.camel@lists.microscopium.de>

Thank you Alex, this works fine!

Robert


Am Donnerstag, den 28.11.2019, 18:55 -0500 schrieb Alex Rousskov:
> On 11/28/19 4:34 PM, Robert Senger wrote:
> 
> > a couple of week ago I've separated a rather complex squid
> > installation
> > into two instances running on the same machine.
> > 
> > The setup of these two instances is almost identical, despite
> > different
> > tcp_outgoing_address and stuff like that.
> > 
> > All files on disk and all listening ports are set to different
> > paths /
> > values, so the instances should not interfere with each other. 
> > 
> > However, today I realized that squid creates files in /dev/shm:
> > 
> > root at prokyon:/dev/shm# ls -l
> > insgesamt 2292
> > -rw-------. 1 proxy    proxy          8 Nov 28 22:27 squid-
> > cf__metadata.shm
> > -rw-------. 1 proxy    proxy       8216 Nov 28 22:27 squid-
> > cf__queues.shm
> > -rw-------. 1 proxy    proxy         36 Nov 28 22:27 squid-
> > cf__readers.shm
> > -rw-------. 1 proxy    proxy    2103672 Nov 28 22:27 squid-
> > tls_session_cache.shm
> > 
> > These files are created when the first instance starts. When the
> > second
> > instance starts, the files get updated/recreated (new
> > mtime/ctime). 
> > 
> > When any of the running instances is stopped, the files are
> > deleted.
> > 
> > So, the two instances collide here.
> 
> 
> > My questions:
> > 1. Is this a problem?
> 
> Yes.
> 
> 
> > 2. If so, what must I do?
> 
> Use different service names for different Squid instances running
> concurrently on the same machine. The service name is specified on
> the
> command line using "squid -n".
> 
> AFAICT, this trick is poorly documented and is not supported on some
> platforms, but I hope it works in your environment.
> 
> 
> HTH,
> 
> Alex.
-- 
Robert Senger




From rvonamor at yandex.com  Fri Nov 29 15:49:49 2019
From: rvonamor at yandex.com (Romanov Vonamor)
Date: Fri, 29 Nov 2019 15:49:49 +0000
Subject: [squid-users] Squid 4.9 Client IP PTR lookup on connect
Message-ID: <112604241575042589@vla1-b1f71bfb4f06.qloud-c.yandex.net>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191129/2a462df7/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 29 16:43:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Nov 2019 05:43:08 +1300
Subject: [squid-users] Squid 4.9 Client IP PTR lookup on connect
In-Reply-To: <112604241575042589@vla1-b1f71bfb4f06.qloud-c.yandex.net>
References: <112604241575042589@vla1-b1f71bfb4f06.qloud-c.yandex.net>
Message-ID: <24104547-ce2e-2150-5d25-d6b1165bc972@treenet.co.nz>

On 30/11/19 4:49 am, Romanov Vonamor wrote:
> Hello.
> ?
> I'm trying to configure Squid 4.9 in such a way that it does not perform
> a reverse IP lookup of the client at approximately every HTTP request.
> The PTR lookup happens immediately after the connection, before the HTTP
> request is even parsed.
> Any insight would be greatly appreciated.
> ?

The PTR should only need to be looked up at all if something needs to
use the client FQDN. Usually that is logging. I suspect your build
auto-enabled ICAP features which uses the FQDN for icap_log.

If you do not need or plan to use ICAP features you can rebuild with
--disable-icap which should resolve this.


> Romanov
> ?
> -------- 8< --------
> Log:
> ?
> 2019/11/29 14:02:15.765 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
> connection on FD 8
> 2019/11/29 14:02:15.765 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
> connection on local=0.0.0.0:3130 remote=[::] FD 8 flags=9
> 2019/11/29 14:02:15.770 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 9
> HTTP Request
> 2019/11/29 14:02:15.770 kid1| 33,4| client_side.cc(2520) httpAccept:
> local=10.254.236.19:3130 remote=10.229.200.152:56040 FD 9 flags=1: accepted
> 2019/11/29 14:02:15.770 kid1| 35,4| fqdncache.cc(420)
> fqdncache_nbgethostbyaddr: fqdncache_nbgethostbyaddr: Name '10.229.200.152'.
> 2019/11/29 14:02:15.771 kid1| 78,3| dns_internal.cc(1831) idnsPTRLookup:
> idnsPTRLookup: buf is 45 bytes for 10.229.200.152, id = 0x5eb3
> ?
> -------- 8< --------
> [root at sls squid-4.9]# squid -v
> Squid Cache: Version 4.9
> Service Name: squid
> configure options: --enable-ltdl-convenience
> ?
> -------- 8< --------
> [root at sls sls]# squid -u0 -f /etc/squid/sites/sls/sls.conf -k parse
> 2019/11/29 14:49:21| Startup: Initializing Authentication Schemes ...
> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'basic'
> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'digest'
> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'negotiate'
> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'ntlm'
> 2019/11/29 14:49:21| Startup: Initialized Authentication.
> 2019/11/29 14:49:21| aclIpParseIpData: IPv6 has not been enabled.
> 2019/11/29 14:49:21| aclIpParseIpData: IPv6 has not been enabled.
> 2019/11/29 14:49:21| Processing Configuration File:
> /etc/squid/sites/sls/sls.conf (depth 0)
> 2019/11/29 14:49:21| Processing: visible_hostname sls

> 2019/11/29 14:49:21| Processing: acl from-all src all

That is pretty pointless. "src all" is the definition of the built-in
"all" ACL. Might as well use that instead of these 'from-all' and make
it more clear that you have no restrictions on what clients can do with
your proxy.

> 2019/11/29 14:49:21| Processing: http_access deny !safe-ports
> 2019/11/29 14:49:21| Processing: http_access deny CONNECT !ssl-ports
> 2019/11/29 14:49:21| Processing: http_access allow from-all
> 2019/11/29 14:49:21| Processing: cache_log
> stdio:/proxy/logs/squid/sls/cache-sls.log
> ?
> ?
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From darren at ksn-systems.com  Fri Nov 29 17:20:43 2019
From: darren at ksn-systems.com (Darren Breeze)
Date: Sat, 30 Nov 2019 06:20:43 +1300
Subject: [squid-users] icap result caching in squid
Message-ID: <1e800569-1d1d-42d4-bad8-41a2100430d9@www.fastmail.com>


Hi all.

Some quick question about icap result caching in squid. 

Does the returned Expires header control how long squid will cache the result (for both a req and resp mod)? 

Are the values that are cached keyed to the queried URL or is it cached per user / url?

thanks

Darren B.




This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the system manager. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. If you are not the intended recipient you are notified that disclosing, copying, distributing or taking any action in reliance on the contents of this information is strictly prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191130/894a4029/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov 30 14:42:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 1 Dec 2019 03:42:12 +1300
Subject: [squid-users] Squid 4.9 Client IP PTR lookup on connect
In-Reply-To: <24104547-ce2e-2150-5d25-d6b1165bc972@treenet.co.nz>
References: <112604241575042589@vla1-b1f71bfb4f06.qloud-c.yandex.net>
 <24104547-ce2e-2150-5d25-d6b1165bc972@treenet.co.nz>
Message-ID: <556c92d2-42b8-1083-9389-c84898b96e16@treenet.co.nz>

On 30/11/19 5:43 am, Amos Jeffries wrote:
> On 30/11/19 4:49 am, Romanov Vonamor wrote:
>> Hello.
>> ?
>> I'm trying to configure Squid 4.9 in such a way that it does not perform
>> a reverse IP lookup of the client at approximately every HTTP request.
>> The PTR lookup happens immediately after the connection, before the HTTP
>> request is even parsed.
>> Any insight would be greatly appreciated.
>> ?
> 
> The PTR should only need to be looked up at all if something needs to
> use the client FQDN. Usually that is logging. I suspect your build
> auto-enabled ICAP features which uses the FQDN for icap_log.
> 
> If you do not need or plan to use ICAP features you can rebuild with
> --disable-icap which should resolve this.

Sorry that should have been --disable-icap-client

Amos


From rousskov at measurement-factory.com  Sat Nov 30 17:22:32 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 30 Nov 2019 12:22:32 -0500
Subject: [squid-users] icap result caching in squid
In-Reply-To: <1e800569-1d1d-42d4-bad8-41a2100430d9@www.fastmail.com>
References: <1e800569-1d1d-42d4-bad8-41a2100430d9@www.fastmail.com>
Message-ID: <c6de8a70-e0d7-c6cc-b0bf-58902e34db66@measurement-factory.com>

On 11/29/19 12:20 PM, Darren Breeze wrote:

> Some quick question about icap result caching in squid.
> 
> Does the returned Expires header control how long squid will cache the
> result (for both a req and resp mod)?
> 
> Are the values that are cached keyed to the queried URL or is it cached
> per user / url?

Squid only supports pre-cache vectoring points. Thus, bugs
notwithstanding, post-ICAP headers should be treated (for caching
purposes) as if Squid received the same adjusted HTTP message directly
from an HTTP agent, and there were no ICAP modifications at all.

The above statement does not answer your question, but it changes that
question from "How ICAP-set X affects caching?" to "How X affects
caching?" -- a question that you may already know the answer to or, if
you do not, a question that others on the list may be able to answer
better or faster than I currently can.


HTH,

Alex.


From rousskov at measurement-factory.com  Sat Nov 30 17:31:50 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 30 Nov 2019 12:31:50 -0500
Subject: [squid-users] Squid 4.9 Client IP PTR lookup on connect
In-Reply-To: <24104547-ce2e-2150-5d25-d6b1165bc972@treenet.co.nz>
References: <112604241575042589@vla1-b1f71bfb4f06.qloud-c.yandex.net>
 <24104547-ce2e-2150-5d25-d6b1165bc972@treenet.co.nz>
Message-ID: <ab0880df-fa27-69b7-46f0-9d86ae1fad19@measurement-factory.com>

On 11/29/19 11:43 AM, Amos Jeffries wrote:

> The PTR should only need to be looked up at all if something needs to
> use the client FQDN. Usually that is logging. I suspect your build
> auto-enabled ICAP features which uses the FQDN for icap_log.

... but icap_log is disabled by default, even in Squid builds that have
ICAP support enabled, right? If a disabled icap_log triggers DNS
lookups, there is a Squid bug we should fix.

FWIW, the easiest way to figure out what triggered the lookup could be
to start Squid in a debugger, and then, before starting the test
transaction, add a breakpoint for fqdncache_nbgethostbyaddr. Post a
stack trace from that function (when it is triggered after the
httpAccept line is logged as shown in your cache.log).

Alex.


>> -------- 8< --------
>> Log:
>> ?
>> 2019/11/29 14:02:15.765 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
>> connection on FD 8
>> 2019/11/29 14:02:15.765 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
>> connection on local=0.0.0.0:3130 remote=[::] FD 8 flags=9
>> 2019/11/29 14:02:15.770 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 9
>> HTTP Request
>> 2019/11/29 14:02:15.770 kid1| 33,4| client_side.cc(2520) httpAccept:
>> local=10.254.236.19:3130 remote=10.229.200.152:56040 FD 9 flags=1: accepted
>> 2019/11/29 14:02:15.770 kid1| 35,4| fqdncache.cc(420)
>> fqdncache_nbgethostbyaddr: fqdncache_nbgethostbyaddr: Name '10.229.200.152'.
>> 2019/11/29 14:02:15.771 kid1| 78,3| dns_internal.cc(1831) idnsPTRLookup:
>> idnsPTRLookup: buf is 45 bytes for 10.229.200.152, id = 0x5eb3
>> ?
>> -------- 8< --------
>> [root at sls squid-4.9]# squid -v
>> Squid Cache: Version 4.9
>> Service Name: squid
>> configure options: --enable-ltdl-convenience
>> ?
>> -------- 8< --------
>> [root at sls sls]# squid -u0 -f /etc/squid/sites/sls/sls.conf -k parse
>> 2019/11/29 14:49:21| Startup: Initializing Authentication Schemes ...
>> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'basic'
>> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'digest'
>> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'negotiate'
>> 2019/11/29 14:49:21| Startup: Initialized Authentication Scheme 'ntlm'
>> 2019/11/29 14:49:21| Startup: Initialized Authentication.
>> 2019/11/29 14:49:21| aclIpParseIpData: IPv6 has not been enabled.
>> 2019/11/29 14:49:21| aclIpParseIpData: IPv6 has not been enabled.
>> 2019/11/29 14:49:21| Processing Configuration File:
>> /etc/squid/sites/sls/sls.conf (depth 0)
>> 2019/11/29 14:49:21| Processing: visible_hostname sls
> 
>> 2019/11/29 14:49:21| Processing: acl from-all src all
> 
> That is pretty pointless. "src all" is the definition of the built-in
> "all" ACL. Might as well use that instead of these 'from-all' and make
> it more clear that you have no restrictions on what clients can do with
> your proxy.
> 
>> 2019/11/29 14:49:21| Processing: http_access deny !safe-ports
>> 2019/11/29 14:49:21| Processing: http_access deny CONNECT !ssl-ports
>> 2019/11/29 14:49:21| Processing: http_access allow from-all
>> 2019/11/29 14:49:21| Processing: cache_log
>> stdio:/proxy/logs/squid/sls/cache-sls.log
 ?


