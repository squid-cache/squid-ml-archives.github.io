From rousskov at measurement-factory.com  Tue Oct  1 14:28:56 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Oct 2019 10:28:56 -0400
Subject: [squid-users] Does Squid support ICAP early responses?
In-Reply-To: <CADcj3=6fh_d3e365RqGHa3D0KJXJrBM7p7Mi=+hc-45cHWh5YA@mail.gmail.com>
References: <CADcj3=6fh_d3e365RqGHa3D0KJXJrBM7p7Mi=+hc-45cHWh5YA@mail.gmail.com>
Message-ID: <b1d15198-cde2-f6b5-1281-2cb9cc16a5fc@measurement-factory.com>

On 9/30/19 12:23 PM, Felipe Arturo Polanco wrote:

> Does anyone know if Squid support early ICAP responses from the ICAP
> server in the middle of a body transfer?

I do not know for sure. A source code comment implies that Squid ICAP
client expects early responses to be errors, but I did not see any
obvious signs that a positive early response cannot be handled correctly
in my cursory review of the corresponding code.

I can say that Squid _should_ support early responses. If it does not,
quality pull requests that fix or add that support would be welcomed.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.
P.S. Please note that an early response itself does _not_ end an ICAP
transaction. To preserve the ICAP connection, the ICAP client (i.e.
Squid) still has to finish sending the request.


From felipeapolanco at gmail.com  Tue Oct  1 14:47:34 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Tue, 1 Oct 2019 10:47:34 -0400
Subject: [squid-users] Does Squid support ICAP early responses?
In-Reply-To: <b1d15198-cde2-f6b5-1281-2cb9cc16a5fc@measurement-factory.com>
References: <CADcj3=6fh_d3e365RqGHa3D0KJXJrBM7p7Mi=+hc-45cHWh5YA@mail.gmail.com>
 <b1d15198-cde2-f6b5-1281-2cb9cc16a5fc@measurement-factory.com>
Message-ID: <CADcj3=5oBcAVdyjZ-bN3RYRqaKfj_gdD8zHBxumsKHd2ZbVQVw@mail.gmail.com>

Thanks for that Alex,

Do you know if squid, after receiving an early response that the file is
good, will start immediately the transfer to the downstream client in
parallel while finishing up the ICAP transaction with the ICAP server?

On Tue, Oct 1, 2019 at 10:28 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 9/30/19 12:23 PM, Felipe Arturo Polanco wrote:
>
> > Does anyone know if Squid support early ICAP responses from the ICAP
> > server in the middle of a body transfer?
>
> I do not know for sure. A source code comment implies that Squid ICAP
> client expects early responses to be errors, but I did not see any
> obvious signs that a positive early response cannot be handled correctly
> in my cursory review of the corresponding code.
>
> I can say that Squid _should_ support early responses. If it does not,
> quality pull requests that fix or add that support would be welcomed.
>
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> Alex.
> P.S. Please note that an early response itself does _not_ end an ICAP
> transaction. To preserve the ICAP connection, the ICAP client (i.e.
> Squid) still has to finish sending the request.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191001/b43b672f/attachment.htm>

From rousskov at measurement-factory.com  Tue Oct  1 14:55:12 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Oct 2019 10:55:12 -0400
Subject: [squid-users] Does Squid support ICAP early responses?
In-Reply-To: <CADcj3=5oBcAVdyjZ-bN3RYRqaKfj_gdD8zHBxumsKHd2ZbVQVw@mail.gmail.com>
References: <CADcj3=6fh_d3e365RqGHa3D0KJXJrBM7p7Mi=+hc-45cHWh5YA@mail.gmail.com>
 <b1d15198-cde2-f6b5-1281-2cb9cc16a5fc@measurement-factory.com>
 <CADcj3=5oBcAVdyjZ-bN3RYRqaKfj_gdD8zHBxumsKHd2ZbVQVw@mail.gmail.com>
Message-ID: <6ba2cfb9-862f-635d-9c69-e9df2e116688@measurement-factory.com>

On 10/1/19 10:47 AM, Felipe Arturo Polanco wrote:

> Do you know if squid, after receiving an early response that the file is
> good, will start immediately the transfer to the downstream client in
> parallel while finishing up the ICAP transaction?with the ICAP server?

Same answer: I do not know for sure, but it is possible that Squid will
do that. Squid should do that. If it does not, then quality pull
requests that fix or add that support would be welcomed.

Alex.


> On Tue, Oct 1, 2019 at 10:28 AM Alex Rousskov wrote:
> 
>     On 9/30/19 12:23 PM, Felipe Arturo Polanco wrote:
> 
>     > Does anyone know if Squid support early ICAP responses from the ICAP
>     > server in the middle of a body transfer?
> 
>     I do not know for sure. A source code comment implies that Squid ICAP
>     client expects early responses to be errors, but I did not see any
>     obvious signs that a positive early response cannot be handled correctly
>     in my cursory review of the corresponding code.
> 
>     I can say that Squid _should_ support early responses. If it does not,
>     quality pull requests that fix or add that support would be welcomed.
> 
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
>     Alex.
>     P.S. Please note that an early response itself does _not_ end an ICAP
>     transaction. To preserve the ICAP connection, the ICAP client (i.e.
>     Squid) still has to finish sending the request.
> 



From augustus_meyer at gmx.net  Wed Oct  2 09:12:51 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 2 Oct 2019 04:12:51 -0500 (CDT)
Subject: [squid-users] Squid Benchmark
In-Reply-To: <CACgYPG3tWfMcj5jxghnvh2Jfm0wh2+KP3gY=+9qRa4VRPf2FQg@mail.gmail.com>
References: <CACgYPG3tWfMcj5jxghnvh2Jfm0wh2+KP3gY=+9qRa4VRPf2FQg@mail.gmail.com>
Message-ID: <1570007571822-0.post@n4.nabble.com>

>... considering security and filtering ...<
Regarding filtering, you might consider DNS-based filtering.
I did special developments in this area, i.e. for "Parental Control" and
ad/tracker-blocking.

"TLS everywhere" I consider a special trick of goggle, to protect their
ads/trackers from being easily filtered.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Wed Oct  2 11:49:30 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 2 Oct 2019 13:49:30 +0200
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <20191002114930.GB2108@fantomas.sk>

Hello,

We have recently upgraded debian 9/squid 3.5.23 to debian 10/squid 4.6

Since then, we see many errors like these:

1569967208.535   3651 192.168.aa.bbb TCP_TUNNEL_ABORTED/200 7200 CONNECT arc.msn.com:443 - HIER_DIRECT/52.229.207.60 -
1569967212.488 170668 192.168.cc.ddd TCP_TUNNEL_ABORTED/200 1979 CONNECT play.google.com:443 - HIER_DIRECT/216.58.201.110 -

Is there anything that could cause high number of these errors?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Saving Private Ryan...
Private Ryan exists. Overwrite? (Y/N)


From rousskov at measurement-factory.com  Wed Oct  2 14:16:14 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 Oct 2019 10:16:14 -0400
Subject: [squid-users] (no subject)
In-Reply-To: <20191002114930.GB2108@fantomas.sk>
References: <20191002114930.GB2108@fantomas.sk>
Message-ID: <69dd9276-3a23-93d2-e07d-4637b2e5fffa@measurement-factory.com>

On 10/2/19 7:49 AM, Matus UHLAR - fantomas wrote:
> Hello,
> 
> We have recently upgraded debian 9/squid 3.5.23 to debian 10/squid 4.6
> 
> Since then, we see many errors like these:
> 
> 1569967208.535?? 3651 192.168.aa.bbb TCP_TUNNEL_ABORTED/200 7200 CONNECT
> arc.msn.com:443 - HIER_DIRECT/52.229.207.60 -
> 1569967212.488 170668 192.168.cc.ddd TCP_TUNNEL_ABORTED/200 1979 CONNECT
> play.google.com:443 - HIER_DIRECT/216.58.201.110 -

> Is there anything that could cause high number of these errors?

Yes, bug fixes (e.g., logging transactions that were never logged
before) and/or new bugs (e.g., stuck transactions).

Alex.


From uhlar at fantomas.sk  Wed Oct  2 14:22:39 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 2 Oct 2019 16:22:39 +0200
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
Message-ID: <20191002142239.GA10220@fantomas.sk>

Hello,

related to http://bugs.squid-cache.org/show_bug.cgi?id=4977

    (In reply to Matus UHLAR - fantomas from comment #12)
    > We notice the same problem after few minutes running squid 4.6 (debian 10,
    > x86_64):
    >
    > 2019/10/02 05:00:53 kid1| assertion failed: stmem.cc:98: "lowestOffset () <=
    > target_offset"

    (In reply to Matus UHLAR - fantomas from comment #14)
    > I would like to note that we have no SSL support and no header modification
    > besides "forwarded_for delete"

    This bug report was about adaptation (or, more precisely, new adapted message
    creation) triggering the above assertion. It sounds like your environment is
    very different -- no new messages are created. The bug may or may not be the
    same, of course.

    Can you reproduce the bug with specific message(s), on demand?

I could try to reproduce by using newer squid, hoping the customer won't get
very angry.  I would need some hints on what more to watch for and what more
to log. (debug options probably) 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Holmes, what kind of school did you study to be a detective?
- Elementary, Watkins.  -- Daffy Duck & Porky Pig


From uhlar at fantomas.sk  Wed Oct  2 14:50:04 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 2 Oct 2019 16:50:04 +0200
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <20191002142239.GA10220@fantomas.sk>
References: <20191002142239.GA10220@fantomas.sk>
Message-ID: <20191002145003.GA10823@fantomas.sk>

On 02.10.19 16:22, Matus UHLAR - fantomas wrote:
>related to http://bugs.squid-cache.org/show_bug.cgi?id=4977
>
>   (In reply to Matus UHLAR - fantomas from comment #12)
>   > We notice the same problem after few minutes running squid 4.6 (debian 10,
>   > x86_64):
>   >
>   > 2019/10/02 05:00:53 kid1| assertion failed: stmem.cc:98: "lowestOffset () <=
>   > target_offset"
>
>   (In reply to Matus UHLAR - fantomas from comment #14)
>   > I would like to note that we have no SSL support and no header modification
>   > besides "forwarded_for delete"
>
>   This bug report was about adaptation (or, more precisely, new adapted message
>   creation) triggering the above assertion. It sounds like your environment is
>   very different -- no new messages are created. The bug may or may not be the
>   same, of course.
>
>   Can you reproduce the bug with specific message(s), on demand?
>
>I could try to reproduce by using newer squid, hoping the customer won't get
>very angry.  I would need some hints on what more to watch for and what more
>to log. (debug options probably)

OK, now I've got 1.7G pcap,  2.8M access.log (22k requests) and cache.log
saying when did the problem appear.
Any idea what to search for?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Windows found: (R)emove, (E)rase, (D)elete


From rousskov at measurement-factory.com  Wed Oct  2 17:48:39 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 Oct 2019 13:48:39 -0400
Subject: [squid-users] Fw: [Bug 4977] stmem.cc:98: "lowestOffset () <=
 target_offset" assertion when adapting Content-Range value
In-Reply-To: <20191002145003.GA10823@fantomas.sk>
References: <20191002142239.GA10220@fantomas.sk>
 <20191002145003.GA10823@fantomas.sk>
Message-ID: <49dbc593-62b7-7ecf-93ba-b4b667f9919f@measurement-factory.com>

On 10/2/19 10:50 AM, Matus UHLAR - fantomas wrote:
> On 02.10.19 16:22, Matus UHLAR - fantomas wrote:
>> http://bugs.squid-cache.org/show_bug.cgi?id=4977

> OK, now I've got 1.7G pcap,? 2.8M access.log (22k requests) and cache.log
> saying when did the problem appear.
> Any idea what to search for?

cache.log with debug_options set to just "ALL,1" (or default), right?

I cannot give you specific instructions, but you can try to find the
transaction that caused the crash by correlating packet capture with the
timing of the crash. If you are lucky, the transaction would not be
encrypted and will have few other transactions nearby. In that case, we
may be able to learn something from the transaction bytes received and
sent by Squid.

Alex.


From johnrefwe at mail.com  Wed Oct  2 18:51:15 2019
From: johnrefwe at mail.com (johnr)
Date: Wed, 2 Oct 2019 13:51:15 -0500 (CDT)
Subject: [squid-users] Annotating transaction from inside ICAP
Message-ID: <1570042275836-0.post@n4.nabble.com>

Hi,

I was wondering how / if it was possible to annotate a request (basically,
set a note/tag similarly to how the external ACL can) from within an ICAP
service.

My use-case is that I would like to add a note with information about the
request that is then available for use within external helpers. I believe I
can do a similar type of information share by setting HTTP headers from
within ICAP and reading them from the external helpers, but that seems to be
a bit more clunky of an approach.

Thanks!





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Oct  2 19:21:18 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 Oct 2019 15:21:18 -0400
Subject: [squid-users] Annotating transaction from inside ICAP
In-Reply-To: <1570042275836-0.post@n4.nabble.com>
References: <1570042275836-0.post@n4.nabble.com>
Message-ID: <8283a238-e965-d5e0-e723-d0db989af205@measurement-factory.com>

On 10/2/19 2:51 PM, johnr wrote:

> I was wondering how / if it was possible to annotate a request from 
> within an ICAP service.

A long-term solution requires two steps:

1. Modify your ICAP service to return a custom ICAP response header.

2. Modify Squid to treat ICAP response headers (with names listed in a
new squid.conf directive) as master transaction annotations.

Squid eCAP implementation already treats eCAP response metadata as
annotations, but there is no such code in ICAP yet IIRC. The ICAP use
case is complicated by the fact that not all ICAP response headers
should become annotations by default, but it is not very difficult to
add this configuration support (and it can be useful for eCAP
transactions as a way to tell Squid whether the metadata should annotate
the connection or just the current master transaction.


> My use-case is that I would like to add a note with information about
> the request that is then available for use within external helpers. I
> believe I can do a similar type of information share by setting HTTP
> headers from within ICAP and reading them from the external helpers,
> but that seems to be a bit more clunky of an approach.

Exactly. This workaround requires adapting the HTTP message (an
expensive and sometimes dangerous operation) as well as removing the
customer headers from outgoing messages using request_header_access (or
reply_header_access).


HTH,

Alex.



From eperez at quadrianweb.com  Thu Oct  3 03:47:02 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Wed, 2 Oct 2019 22:47:02 -0500
Subject: [squid-users] Guidance needed. Issues with WPAD and Firefox
Message-ID: <CACXMG+tCs132K=Q+OTiX7cqTN11y_deMdnUktSnhfM3mTR+dgQ@mail.gmail.com>

Hi, maybe somebody here experienced the same issues while working with
WPAD files.

I have a working Centos 7.6/SQUID v4.8 that works as expected.
It works as long as I manually configure the proxy in the network
settings of each workstation (windows) firefox browser 69.0.1 (64bit)
and the same behavior with IE and Chrome.

if I open http://squid_fqdn/wpad.dat the file gets downloaded and has
all the info.
apache logs show me the client is properly connecting and requesting the file.

the wpad.dat has two lines
#
if (isPlainHostName(host)) return "DIRECT";
return "PROXY squid_fqdn:8080";
#

However, the moment I switch to using WPAD, the browsers stop working,
and I see nothing on the squid logs. So it seems it is a browser issue
regarding the WPAD.
Has anyone here knows in firefox what a PR_END_OF_FILE_ERROR   means?

FWIW: I'm not using SSL bump, not caching anything and only using:
#acl CONNECT method CONNECT
my squid is only used for allowing access to specific domains.

thanks for your help,




-- 

---------------------
Erick Perez


From eperez at quadrianweb.com  Thu Oct  3 04:55:43 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Wed, 2 Oct 2019 23:55:43 -0500
Subject: [squid-users] Guidance needed. Issues with WPAD and Firefox
In-Reply-To: <CACXMG+tCs132K=Q+OTiX7cqTN11y_deMdnUktSnhfM3mTR+dgQ@mail.gmail.com>
References: <CACXMG+tCs132K=Q+OTiX7cqTN11y_deMdnUktSnhfM3mTR+dgQ@mail.gmail.com>
Message-ID: <CACXMG+tCWsp2mcAQZ7WeB6Zt3go=SNsVkm6HE74TLPdgVPU+Ng@mail.gmail.com>

On Wed, Oct 2, 2019 at 10:47 PM Erick Perez - Quadrian Enterprises
<eperez at quadrianweb.com> wrote:
>
> Hi, maybe somebody here experienced the same issues while working with
> WPAD files.
>
> I have a working Centos 7.6/SQUID v4.8 that works as expected.
> It works as long as I manually configure the proxy in the network
> settings of each workstation (windows) firefox browser 69.0.1 (64bit)
> and the same behavior with IE and Chrome.
>
> if I open http://squid_fqdn/wpad.dat the file gets downloaded and has
> all the info.
> apache logs show me the client is properly connecting and requesting the file.
>
> the wpad.dat has two lines
> #
> if (isPlainHostName(host)) return "DIRECT";
> return "PROXY squid_fqdn:8080";
> #
>
> However, the moment I switch to using WPAD, the browsers stop working,
> and I see nothing on the squid logs. So it seems it is a browser issue
> regarding the WPAD.
> Has anyone here knows in firefox what a PR_END_OF_FILE_ERROR   means?
>
> FWIW: I'm not using SSL bump, not caching anything and only using:
> #acl CONNECT method CONNECT
> my squid is only used for allowing access to specific domains.
>
> thanks for your help,
>
>
>
>
> --
>
> ---------------------
> Erick Perez


This image shows WPAD config in firefox. this is not working
https://ibb.co/D5302s6

This image shows manual configuration which is working fine.
https://ibb.co/cXgmxnj

There are no group policies, the computers used to test are NOT Active
directory domain joined and I have not configured any DHCP/DNS
options.


From squid3 at treenet.co.nz  Thu Oct  3 05:45:29 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 Oct 2019 18:45:29 +1300
Subject: [squid-users] Guidance needed. Issues with WPAD and Firefox
In-Reply-To: <CACXMG+tCWsp2mcAQZ7WeB6Zt3go=SNsVkm6HE74TLPdgVPU+Ng@mail.gmail.com>
References: <CACXMG+tCs132K=Q+OTiX7cqTN11y_deMdnUktSnhfM3mTR+dgQ@mail.gmail.com>
 <CACXMG+tCWsp2mcAQZ7WeB6Zt3go=SNsVkm6HE74TLPdgVPU+Ng@mail.gmail.com>
Message-ID: <e6fad9e0-d495-b85c-8868-643be15b9326@treenet.co.nz>

On 3/10/19 5:55 pm, Erick Perez - Quadrian Enterprises wrote:
> On Wed, Oct 2, 2019 at 10:47 PM Erick Perez - Quadrian Enterprises wrote:
>>
>> Hi, maybe somebody here experienced the same issues while working with
>> WPAD files.
>>
>> I have a working Centos 7.6/SQUID v4.8 that works as expected.
>> It works as long as I manually configure the proxy in the network
>> settings of each workstation (windows) firefox browser 69.0.1 (64bit)
>> and the same behavior with IE and Chrome.
>>
>> if I open http://squid_fqdn/wpad.dat the file gets downloaded and has
>> all the info.
>> apache logs show me the client is properly connecting and requesting the file.
>>
>> the wpad.dat has two lines
>

That is one problem. The PAC file is supposed to define a fully formed
function like so:

function FindProxyForURL(url, host) {
  if (isPlainHostName(host)) return "DIRECT";
    return "PROXY squid_fqdn:8080";}

You also need to ensure the web server is producing the HTTP response
with a "Content-Type: application/x-ns-proxy-autoconfig" header or it
will fail.

see <http://findproxyforurl.com/> for more info about WPAD/PAC.

Amos


From eperez at quadrianweb.com  Thu Oct  3 07:07:23 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Thu, 3 Oct 2019 02:07:23 -0500
Subject: [squid-users] Guidance needed. Issues with WPAD and Firefox
In-Reply-To: <e6fad9e0-d495-b85c-8868-643be15b9326@treenet.co.nz>
References: <CACXMG+tCs132K=Q+OTiX7cqTN11y_deMdnUktSnhfM3mTR+dgQ@mail.gmail.com>
 <CACXMG+tCWsp2mcAQZ7WeB6Zt3go=SNsVkm6HE74TLPdgVPU+Ng@mail.gmail.com>
 <e6fad9e0-d495-b85c-8868-643be15b9326@treenet.co.nz>
Message-ID: <CACXMG+tGhnE+mU_F4NtUutYD89bwghrJGpkv1-tqtmE7zLm7Jg@mail.gmail.com>

On Thu, Oct 3, 2019 at 12:45 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 3/10/19 5:55 pm, Erick Perez - Quadrian Enterprises wrote:
> > On Wed, Oct 2, 2019 at 10:47 PM Erick Perez - Quadrian Enterprises wrote:
> >>
> >> Hi, maybe somebody here experienced the same issues while working with
> >> WPAD files.
> >>
> >> I have a working Centos 7.6/SQUID v4.8 that works as expected.
> >> It works as long as I manually configure the proxy in the network
> >> settings of each workstation (windows) firefox browser 69.0.1 (64bit)
> >> and the same behavior with IE and Chrome.
> >>
> >> if I open http://squid_fqdn/wpad.dat the file gets downloaded and has
> >> all the info.
> >> apache logs show me the client is properly connecting and requesting the file.
> >>
> >> the wpad.dat has two lines
> >
>
> That is one problem. The PAC file is supposed to define a fully formed
> function like so:
>
> function FindProxyForURL(url, host) {
>   if (isPlainHostName(host)) return "DIRECT";
>     return "PROXY squid_fqdn:8080";}
>
> You also need to ensure the web server is producing the HTTP response
> with a "Content-Type: application/x-ns-proxy-autoconfig" header or it
> will fail.
>
> see <http://findproxyforurl.com/> for more info about WPAD/PAC.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

So sorry Amos,
actually the function is implemented properly. I just didnt paste it
And yes, wireshark shows me HTTP/1.1 200 OK (application/x-ns-proxy-autoconfig)
I followed this document: https://www.giantstride.eu/proxy-autodiscovery/
I created wpad CNAME in DNS that points to squidfqdn
I created option 252 in DHCP
and also unblocked wpad block in DNS according to same document
https://www.giantstride.eu/proxy-autodiscovery/

-- 

---------------------
Erick Perez


From eperez at quadrianweb.com  Thu Oct  3 08:35:22 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Thu, 3 Oct 2019 03:35:22 -0500
Subject: [squid-users] Guidance needed. Issues with WPAD and Firefox
In-Reply-To: <CACXMG+tGhnE+mU_F4NtUutYD89bwghrJGpkv1-tqtmE7zLm7Jg@mail.gmail.com>
References: <CACXMG+tCs132K=Q+OTiX7cqTN11y_deMdnUktSnhfM3mTR+dgQ@mail.gmail.com>
 <CACXMG+tCWsp2mcAQZ7WeB6Zt3go=SNsVkm6HE74TLPdgVPU+Ng@mail.gmail.com>
 <e6fad9e0-d495-b85c-8868-643be15b9326@treenet.co.nz>
 <CACXMG+tGhnE+mU_F4NtUutYD89bwghrJGpkv1-tqtmE7zLm7Jg@mail.gmail.com>
Message-ID: <CACXMG+uGAjm8Co7J3sxFv2ogrcbxZJYd6vbqtoJDM+q5a+d3-g@mail.gmail.com>

On Thu, Oct 3, 2019, 2:07 AM Erick Perez - Quadrian Enterprises <
eperez at quadrianweb.com> wrote:

> On Thu, Oct 3, 2019 at 12:45 AM Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
> >
> > On 3/10/19 5:55 pm, Erick Perez - Quadrian Enterprises wrote:
> > > On Wed, Oct 2, 2019 at 10:47 PM Erick Perez - Quadrian Enterprises
> wrote:
> > >>
> > >> Hi, maybe somebody here experienced the same issues while working with
> > >> WPAD files.
> > >>
> > >> I have a working Centos 7.6/SQUID v4.8 that works as expected.
> > >> It works as long as I manually configure the proxy in the network
> > >> settings of each workstation (windows) firefox browser 69.0.1 (64bit)
> > >> and the same behavior with IE and Chrome.
> > >>
> > >> if I open http://squid_fqdn/wpad.dat the file gets downloaded and has
> > >> all the info.
> > >> apache logs show me the client is properly connecting and requesting
> the file.
> > >>
> > >> the wpad.dat has two lines
> > >
> >
> > That is one problem. The PAC file is supposed to define a fully formed
> > function like so:
> >
> > function FindProxyForURL(url, host) {
> >   if (isPlainHostName(host)) return "DIRECT";
> >     return "PROXY squid_fqdn:8080";}
> >
> > You also need to ensure the web server is producing the HTTP response
> > with a "Content-Type: application/x-ns-proxy-autoconfig" header or it
> > will fail.
> >
> > see <http://findproxyforurl.com/> for more info about WPAD/PAC.
> >
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> So sorry Amos,
> actually the function is implemented properly. I just didnt paste it
> And yes, wireshark shows me HTTP/1.1 200 OK
> (application/x-ns-proxy-autoconfig)
> I followed this document: https://www.giantstride.eu/proxy-autodiscovery/
> I created wpad CNAME in DNS that points to squidfqdn
> I created option 252 in DHCP
> and also unblocked wpad block in DNS according to same document
> https://www.giantstride.eu/proxy-autodiscovery/
>
> --
>
---------------------
> Erick Perez
>

Well, I believe it is because is 3:26am here and I am tired.

The issue was that Wpad.dat file was not being processed properly.

So, I opened another wpad from a working implementation and compared it.

They were the same EXCEPT that my non working file had some "white spaces"
at the end.
 It makes no sense for me (I'm mostly a Windows guy) but I removed the
empty spaces and the file was loaded and interpreted correctly.

I'm not sure if the problem is that Apache was serving the file with some
issues or if the browser was misinterpreting the file.

But it is working now.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191003/01700527/attachment.htm>

From eperez at quadrianweb.com  Thu Oct  3 08:39:45 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Thu, 3 Oct 2019 03:39:45 -0500
Subject: [squid-users] IE and EDGE and Microsoft yammer
Message-ID: <CACXMG+uCFvzGqOzuS0eUGg9vR49Yb67Ath_DeaDjK1KT2XaO+g@mail.gmail.com>

To all people that have issues accessing Yammer WITH IE/Edge (invalid TLS /
SSL protocol), do remember that you need to only enable TLS 1.1 and TLS 1.2
in browser settings or active directory group policies.

Chrome and Firefox are not affected.

Cheers.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191003/2ac004e4/attachment.htm>

From service at 4gproxies.net  Thu Oct  3 12:06:11 2019
From: service at 4gproxies.net (service at 4gproxies.net)
Date: Thu, 3 Oct 2019 07:06:11 -0500
Subject: [squid-users] Squid to Android
Message-ID: <025c01d579e2$f2dc95d0$d895c170$@4gproxies.net>

Hello,

 

We are trying to set up proxy based server on AWS and using Squid. As well
we have been trying to integrate the process of port forwarding with the
dynamic IP address using Squid server. So is it possible?

 

We are trying to build the TCP tunnel using Squid and android device to
connect them with the server. We succeed to build the tunnel with the
forwarding port 3000 and even we are receiving the request. Problem is that
we are receiving the request but the request is not coming in the form of an
open port. So is it possible to set up with the Squid?

 

We are almost to finish line of this work so the kindly response to us as
quickly as possible.

 

Thanks!!

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191003/8a495e0f/attachment.htm>

From rousskov at measurement-factory.com  Thu Oct  3 14:26:21 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 3 Oct 2019 10:26:21 -0400
Subject: [squid-users] Squid to Android
In-Reply-To: <025c01d579e2$f2dc95d0$d895c170$@4gproxies.net>
References: <025c01d579e2$f2dc95d0$d895c170$@4gproxies.net>
Message-ID: <d80804ab-078a-09c3-5f16-058eb81d556e@measurement-factory.com>

On 10/3/19 8:06 AM, service at 4gproxies.net wrote:

> We are trying to set up proxy based server on AWS and using Squid. As
> well we have been trying to integrate the process of port forwarding
> with the dynamic IP address using Squid server. So is it possible?

I am not sure what you mean by "the process of port forwarding" and
"dynamic IP address": Those words could mean a lot of different things
to different people. Perhaps you can give an example of what should be
happening and then say what is actually happening compared to that
example? Sharing the relevant portions of your squid.conf may also be
useful.


> We are trying to build the TCP tunnel using Squid and android device to
> connect them with the server. We succeed to build the tunnel with the
> forwarding port 3000 and even we are receiving the request. Problem is
> that we are receiving the request but the request is not coming in the
> form of an open port. So is it possible to set up with the Squid?

Same problem -- the phrase "open port" can be interpreted in many ways.

Alex.


From rst at fomar.com.pl  Sat Oct  5 02:34:00 2019
From: rst at fomar.com.pl (washuu)
Date: Fri, 4 Oct 2019 21:34:00 -0500 (CDT)
Subject: [squid-users] Peek and splice where SNI not present
Message-ID: <1570242840937-0.post@n4.nabble.com>


Hi, 

I'm using Squid 3.5.27, and I want to filter some HTTPS traffic, based on
the hostnames. 

my ssl-related config is as follows: 

acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump peek step1 all
acl global_https_dst_allow ssl::server_name
"/chroot/squid/etc/squid/global_dst_whitelist"
ssl_bump splice step2 global_https_dst_allow
ssl_bump terminate step2 proxyclients
http_access allow SSL_ports
http_access allow proxyclients
http_access deny all

Now I see, that several SSL clients do NOT send SNI hostname in the Client
Hello message, and what I got is denied access, with the following entry in
the log: 

1570241666.136      5 192.168.3.99 TAG_NONE/200 0 CONNECT 52.202.211.224:443
- HIER_NONE/- - -

I have two questions then: 

1) For such cases, is there a possibility to filter traffic based on
certificate provided by the Server Hello (instead of SNI from Client Hello)
in step3? 
2) Is there a way, to allow (by additional ACL rule, perhaps) traffic
without SNI field set? so actually I would be filtering OUT only the
sessions where SNI was present, but the hostname did not match my whitelist. 

Best regards, 

Washuu K.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sat Oct  5 09:44:26 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Oct 2019 22:44:26 +1300
Subject: [squid-users] Peek and splice where SNI not present
In-Reply-To: <1570242840937-0.post@n4.nabble.com>
References: <1570242840937-0.post@n4.nabble.com>
Message-ID: <d0f41b70-e7d6-63de-6a1f-e7671744b094@treenet.co.nz>

On 5/10/19 3:34 pm, washuu wrote:
> 
> Hi, 
> 
> I'm using Squid 3.5.27, and I want to filter some HTTPS traffic, based on
> the hostnames.

When Using SSL-Bump features, even for things like this you should
follow the latest Squid version to make sure the TLS handling is up to
date. Currently Squid-4.8 is minimal for SSL-Bump features to work well.


> 
> my ssl-related config is as follows: 
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> ssl_bump peek step1 all
> acl global_https_dst_allow ssl::server_name
> "/chroot/squid/etc/squid/global_dst_whitelist"
> ssl_bump splice step2 global_https_dst_allow
> ssl_bump terminate step2 proxyclients
> http_access allow SSL_ports
> http_access allow proxyclients

NP: at a guess based on their names these two ACLs are redundant. You
should be able to remove the "allow SSL_Ports" line and let proxyclients
do the allow. Unless you want any random external client to be allowed
just because they want your proxy for HTTPS relay.


> http_access deny all
> 
> Now I see, that several SSL clients do NOT send SNI hostname in the Client
> Hello message, and what I got is denied access, with the following entry in
> the log: 
> 
> 1570241666.136      5 192.168.3.99 TAG_NONE/200 0 CONNECT 52.202.211.224:443
> - HIER_NONE/- - -
> 

There is no indication of which SSL-Bump step is being performed when
this log entry is recorded. This may be from the initial CONNECT request
before the ClientHello is received.


> I have two questions then: 
> 
> 1) For such cases, is there a possibility to filter traffic based on
> certificate provided by the Server Hello (instead of SNI from Client Hello)
> in step3?

Only in Squid-4+, with the --server-provided flag. Like so:

 acl foo ssl::server_name --server-provided .example.com


> 2) Is there a way, to allow (by additional ACL rule, perhaps) traffic
> without SNI field set? so actually I would be filtering OUT only the
> sessions where SNI was present, but the hostname did not match my whitelist.

There is a special value "none" for the ssl::server_name ACL which will
match if there is no server name found. (NP: It is broken prior to
Squid-3.5.23 and Squid-4.1)

You will need the --client-requested flag (also only in Squid-4+) to
limit the server name to SNI.


Be careful using this type of bypass. It essentially makes the whitelist
pointless, clients just avoid sending SNI and they can do whatever they
like with your proxy. That is a major security hole.

Amos


From chip_pop at hotmail.com  Sat Oct  5 23:27:50 2019
From: chip_pop at hotmail.com (joseph)
Date: Sat, 5 Oct 2019 18:27:50 -0500 (CDT)
Subject: [squid-users] cache_peer and ssl
Message-ID: <1570318070049-0.post@n4.nabble.com>

dose squid send to cache peer   ssl  after ssl_bump  clear link or ?
how ssl work between squid and peer ? do i need keys  



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Oct  6 04:45:17 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Oct 2019 17:45:17 +1300
Subject: [squid-users] cache_peer and ssl
In-Reply-To: <1570318070049-0.post@n4.nabble.com>
References: <1570318070049-0.post@n4.nabble.com>
Message-ID: <c2439f1f-0084-01ab-efb1-22179af08fac@treenet.co.nz>

On 6/10/19 12:27 pm, joseph wrote:
> dose squid send to cache peer   ssl  after ssl_bump  clear link or ?

What is "ssl_bump clear link" ?

ssl_bump is used only when TLS bytes are expected from the client.

cache_peer is used whenever a server connection is made, except when
always_direct prevents it.


Squid-4 and older requires the peer to use encrypted connections when
the traffic delivered there has been *decrypted* by Squid. So that the
security is not compromised. Squid-5 allows CONNECT tunnels to be
generated, so can re-encrypt over a non-secure peer.



> how ssl work between squid and peer ? do i need keys  
> 

The same way TLS/SSL works between any software. Keys being needed, and
which type depend on the TLS features used.

Amos


From mohammad.reza.sheikh at gmail.com  Sun Oct  6 07:15:17 2019
From: mohammad.reza.sheikh at gmail.com (mohammad reza sheikh)
Date: Sun, 6 Oct 2019 10:45:17 +0330
Subject: [squid-users] how to use squid authentication with expiration?
Message-ID: <CANf5Wi9KGNTVuLvR5c9yGKwKTaCWysbBdqfbfe=S9nL4VqtcZQ@mail.gmail.com>

 hi.
I want to use squid proxy on my server. i would like to know that is the
way to set expiration period for my users??
how to set that config in relate of squid.
thanks for your help.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191006/33c9bbc6/attachment.htm>

From squid3 at treenet.co.nz  Sun Oct  6 08:46:29 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 6 Oct 2019 21:46:29 +1300
Subject: [squid-users] how to use squid authentication with expiration?
In-Reply-To: <CANf5Wi9KGNTVuLvR5c9yGKwKTaCWysbBdqfbfe=S9nL4VqtcZQ@mail.gmail.com>
References: <CANf5Wi9KGNTVuLvR5c9yGKwKTaCWysbBdqfbfe=S9nL4VqtcZQ@mail.gmail.com>
Message-ID: <ffc768bb-bb02-f965-1987-e21671c2293e@treenet.co.nz>

On 6/10/19 8:15 pm, mohammad reza sheikh wrote:
> hi.
> I want to use squid proxy on my server. i would like to know that is the
> way to set expiration period for my users??

You set credentials expiration in your auth backend.


> how to set that config in relate of squid.

Every individual HTTP request needs to be authenticated individually.
Squid uses helpers to check against the backend auth system. There is
some amount of caching auth results from the helper, based on which auth
scheme you are using.


Amos


From rousskov at measurement-factory.com  Sun Oct  6 14:38:25 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 6 Oct 2019 10:38:25 -0400
Subject: [squid-users] Peek and splice where SNI not present
In-Reply-To: <1570242840937-0.post@n4.nabble.com>
References: <1570242840937-0.post@n4.nabble.com>
Message-ID: <cec99479-24dc-7523-b00e-761daa317c16@measurement-factory.com>

On 10/4/19 10:34 PM, washuu wrote:
> ssl_bump peek step1
> ssl_bump splice step2 foo
> ssl_bump terminate step2 bar

FYI: You did not tell Squid what to do when neither foo nor bar ACLs
match during step2. Thus, older Squid will use some hard-to-predict
action, while modern Squids will splice (because a peek action matched
at the previous step). If splicing is the step2 default you want, then
consider making that decision explicit by rewriting this as

  ssl_bump peek step1
  ssl_bump splice foo
  ssl_bump terminate bar
  ssl_bump splice all

Alex.


From johnrefwe at mail.com  Mon Oct  7 23:15:50 2019
From: johnrefwe at mail.com (johnr)
Date: Mon, 7 Oct 2019 18:15:50 -0500 (CDT)
Subject: [squid-users] External ACL undocumented changes?
Message-ID: <1570490150306-0.post@n4.nabble.com>

In squid 3.5, passing a request header into the external ACL was post reqmod
(if I added a header in ICAP, it was available in the external ACL). In
squid 4, this doesn't seem to be the case. Is that intentional?

Further, the logformat codes that the external ACL now supports should allow
access to the post reqmod headers, but that also doesn't seem to work.

I'm not sure if this is user error or if there is actually a bug.

Thank you!

John



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Oct  8 00:23:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 08 Oct 2019 13:23:10 +1300
Subject: [squid-users] External ACL undocumented changes?
In-Reply-To: <1570490150306-0.post@n4.nabble.com>
References: <1570490150306-0.post@n4.nabble.com>
Message-ID: <0d128041309eecc0bc604861ce68e16c@treenet.co.nz>

On 2019-10-08 12:15, johnr wrote:
> In squid 3.5, passing a request header into the external ACL was post 
> reqmod
> (if I added a header in ICAP, it was available in the external ACL).

Not quite. It depended on when the ACL was tested: pre or pos- 
adaptation. Obviously testing the ACL in http_access before adaptation 
took place would not be able to send adapted headers.

see 
<https://wiki.squid-cache.org/ProgrammingGuide/Architecture#Transaction_Processing> 
for processing order.

>  In
> squid 4, this doesn't seem to be the case. Is that intentional?
> 

Yes, in Squid-4 the old header codes are mapped to the pre-adaptation 
header logformat codes because the pre-adaptation http_access directive 
is the most common use of ACLs.


> Further, the logformat codes that the external ACL now supports should 
> allow
> access to the post reqmod headers, but that also doesn't seem to work.
> 

If the ">ha" logformat code does not send adapted HTTP request headers 
*after* adaptation completes, that would be a bug.


> I'm not sure if this is user error or if there is actually a bug.
> 

Either or both. You have not provided any actual details of the config 
and what Squid is sending and/or logging for us to say which.

Amos


From johnrefwe at mail.com  Tue Oct  8 01:17:30 2019
From: johnrefwe at mail.com (johnr)
Date: Mon, 7 Oct 2019 20:17:30 -0500 (CDT)
Subject: [squid-users] External ACL undocumented changes?
In-Reply-To: <0d128041309eecc0bc604861ce68e16c@treenet.co.nz>
References: <1570490150306-0.post@n4.nabble.com>
 <0d128041309eecc0bc604861ce68e16c@treenet.co.nz>
Message-ID: <1570497450366-0.post@n4.nabble.com>

My config is as follows: 

Squid 3:

external_acl_type should_not_ssl_bump ipv4 ttl=10 %DST %>{User-Agent}
/etc/should_not_ssl_bump.py
acl should_not_ssl external should_not_ssl_bump
ssl_bump none should_not_ssl

Squid 4:

external_acl_type should_not_ssl_bump ipv4 ttl=10 children-startup=1
children-max=5 children-idle=1 %DST %{User-Agent}>ha
/etc/should_not_ssl_bump.py
acl should_not_ssl external should_not_ssl_bump
ssl_bump none should_not_ssl

The rest of my config is the same between squid 3 and squid 4. I have an
ICAP reqmod that modifies the request user-agent. In squid 3, the value is
the properly modified value. In squid 4, the value is the original UA.
According to the transaction processing document, the ssl bump decision is
post reqmod changes, right? So, I guess that means bug?




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Oct  8 05:56:11 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 Oct 2019 18:56:11 +1300
Subject: [squid-users] External ACL undocumented changes?
In-Reply-To: <1570497450366-0.post@n4.nabble.com>
References: <1570490150306-0.post@n4.nabble.com>
 <0d128041309eecc0bc604861ce68e16c@treenet.co.nz>
 <1570497450366-0.post@n4.nabble.com>
Message-ID: <06566e2e-5edc-9a5c-89e3-b056ee99de8e@treenet.co.nz>

On 8/10/19 2:17 pm, johnr wrote:
> The rest of my config is the same between squid 3 and squid 4. I have an
> ICAP reqmod that modifies the request user-agent. In squid 3, the value is
> the properly modified value. In squid 4, the value is the original UA.
> According to the transaction processing document, the ssl bump decision is
> post reqmod changes, right? So, I guess that means bug?
> 

It sounds that way yes.

Amos


From johnrefwe at mail.com  Tue Oct  8 17:08:32 2019
From: johnrefwe at mail.com (johnr)
Date: Tue, 8 Oct 2019 12:08:32 -0500 (CDT)
Subject: [squid-users] External ACL undocumented changes?
In-Reply-To: <06566e2e-5edc-9a5c-89e3-b056ee99de8e@treenet.co.nz>
References: <1570490150306-0.post@n4.nabble.com>
 <0d128041309eecc0bc604861ce68e16c@treenet.co.nz>
 <1570497450366-0.post@n4.nabble.com>
 <06566e2e-5edc-9a5c-89e3-b056ee99de8e@treenet.co.nz>
Message-ID: <1570554512825-0.post@n4.nabble.com>

Okay, thank you.

To submit a bug, what logs/logging level would be helpful?

Thank you,

John



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From cofkomail at gmail.com  Wed Oct  9 07:36:00 2019
From: cofkomail at gmail.com (torson)
Date: Wed, 9 Oct 2019 02:36:00 -0500 (CDT)
Subject: [squid-users] Working peek/splice no longer functioning on some
	sites
In-Reply-To: <43802926-c454-112d-f0bd-c0b377f5b058@treenet.co.nz>
References: <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
 <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
 <1511794211.2147.4.camel@slave-tothe-box.net>
 <7e3e81c9-0949-9f16-f1f8-7efc052dae78@treenet.co.nz>
 <7512b52427c65ed6a0364885bc359d48@localhost>
 <fc698c26-ea16-2a0c-f83b-e4b1609f0aaa@treenet.co.nz>
 <1567370644478-0.post@n4.nabble.com>
 <43802926-c454-112d-f0bd-c0b377f5b058@treenet.co.nz>
Message-ID: <1570606560820-0.post@n4.nabble.com>

@Amos thank you for your detailed reply. It took me a while to get back to
this task, sorry.
I did some changes, added your suggestions, tested some more and here are my
results using Squid 4.8 with a couple of questions:

A short summary of my setup: Squid that does only intercept for all servers
in the same network. There's Dnsmasq set up on the same server and is used
by Squid and all other servers. 

### 1. https certificate check
I've managed to make it work with "ssl_bump peek all" but it started working
only after I've switched the order of splice and peek:
---
ssl_bump splice allowed_https_sites
ssl_bump peek all
ssl_bump terminate all
---

Though now I don't get "terminate" entries in the logfile for blocked
requests for most domains I'm testing with, instead those are now "peek"
entries. 

Here are logs from the previous configuration (using "ssl_bump peek step1"
with logformat: %tl %6tr %>a %Ss/%03>Hs %<st %rm %ssl::bump_mode
%ssl::<cert_subject %ssl::<cert_errors %ru %ssl::>sni). Here I'm testing
with 6 domains, first 3 are blocked and last 3 are whitelisted:
---
08/Oct/2019:13:31:03 +0000     56 172.16.1.11 NONE_ABORTED/200 0 CONNECT
terminate - - 151.101.129.67:443 edition.cnn.com
08/Oct/2019:13:31:03 +0000     62 172.16.1.11 NONE_ABORTED/200 0 CONNECT
terminate - - 52.57.238.9:443 www.bbc.com
08/Oct/2019:13:31:03 +0000     59 172.16.1.11 NONE_ABORTED/200 0 CONNECT
terminate - - 172.217.23.100:443 www.google.com
08/Oct/2019:13:31:03 +0000     57 172.16.1.11 NONE/200 0 CONNECT splice - -
104.20.22.46:443 nodejs.org
08/Oct/2019:13:31:03 +0000    103 172.16.1.11 TCP_TUNNEL/200 6597 CONNECT
splice - - nodejs.org:443 nodejs.org
08/Oct/2019:13:31:03 +0000     54 172.16.1.11 NONE/200 0 CONNECT splice - -
18.205.93.2:443 bitbucket.org
08/Oct/2019:13:31:04 +0000    356 172.16.1.11 TCP_TUNNEL/200 4230 CONNECT
splice - - bitbucket.org:443 bitbucket.org
08/Oct/2019:13:31:04 +0000     47 172.16.1.11 NONE/200 0 CONNECT splice - -
140.82.118.4:443 github.com
08/Oct/2019:13:31:04 +0000    595 172.16.1.11 TCP_TUNNEL/200 5900 CONNECT
splice - - github.com:443 github.com
---

And here are logs from the new configuration (above with "ssl_bump peek
all"):
---
08/Oct/2019:13:32:09 +0000     63 172.16.1.11 NONE_ABORTED/200 0 CONNECT
terminate /C=US/ST=California/L=San Francisco/O=Fastly,
Inc./CN=turner-tls.map.fastly.net - 151.101.1.67:443 edition.cnn.com
08/Oct/2019:13:32:10 +0000     75 172.16.1.11 NONE/200 0 CONNECT peek - -
18.195.39.25:443 www.bbc.com
08/Oct/2019:13:32:10 +0000    116 172.16.1.11 NONE/200 0 CONNECT peek - -
172.217.23.100:443 www.google.com
08/Oct/2019:13:32:10 +0000     65 172.16.1.11 NONE/200 0 CONNECT splice - -
104.20.22.46:443 nodejs.org
08/Oct/2019:13:32:10 +0000     82 172.16.1.11 TCP_TUNNEL/200 6596 CONNECT
splice - - nodejs.org:443 nodejs.org
08/Oct/2019:13:32:10 +0000     58 172.16.1.11 NONE/200 0 CONNECT splice - -
18.205.93.1:443 bitbucket.org
08/Oct/2019:13:32:11 +0000    358 172.16.1.11 TCP_TUNNEL/200 4235 CONNECT
splice - - bitbucket.org:443 bitbucket.org
08/Oct/2019:13:32:11 +0000     50 172.16.1.11 NONE/200 0 CONNECT splice - -
140.82.118.3:443 github.com
08/Oct/2019:13:32:11 +0000    542 172.16.1.11 TCP_TUNNEL/200 5929 CONNECT
splice - - github.com:443 github.com
---

So there seems to be variations on how Squid is handling traffic for various
sites. 
I lean more towards having increased security than consistent logging, seems
like "peek" effectively means "terminate" in this case, so I'll just count
that together for alerting.

Do you have some suggestion to make this work better or is this just the
best I can squeeze out of Squid at this time?

### 2. false host-forgery blocking due to stale DNS record

Currently I'm still using "positive_dns_ttl 0" and "negative_dns_ttl 0"
because otherwise I get a lot of false host-forgery blocking after TTL gets
past 0; the more I increase these two the more false blockings I get. Having
many more DNS requests from Squid due to this should be no issue since local
Dnsmasq should handle them all. 

AWS S3 DNS A records have a single IP with 5s TTL (I never saw a number
higher than 5). The IP together with the TTL is different on each request
and they have a large pool, you could be hitting a few records in a row with
TTL 0.

I'm using these 2 commands to do the testing:
while true; do dig s3-eu-west-1.amazonaws.com | grep "IN" ; sleep 1 ; done
while true; do if curl -I https://s3-eu-west-1.amazonaws.com 2>/dev/null |
grep -q " 405 " ; then echo OK ; else echo BLOCKED; fi ; done

The corner case when false blocking happens is when DNS record TTL gets to a
low value, lower than the interval between when the client gets the record
and when Squid gets the record ; client gets the old IP and Squid gets a new
IP from the upstream ; it could take a client a few seconds between when it
does a DNS query and when it issues the HTTP request itself.
I've remedied this a bit by enabling caching in Dnsmasq and setting
min-cache-ttl to 60 for example, so instead of TTL being 5s (even less, 5s
is the best case and rare with S3 records) it can be 60s and that would
lower the occurance of false blocking to 8% of the original.

I guess the proper solution would be if Squid itself would also be a DNS
forwarder that all clients would use, and would be extending every DNS
record validity for a few seconds (configurable) for the host validity check
so it always uses the same record as the one that it sent to the client. 

Alternative would be to have a DNS forwarder with the support of a secondary
cache with extended TTLs for a configured number of seconds that is
available for specific requesters so that the old record would still be
available and served to requests coming from the Squid IP. I haven't found
such yet, I'll probably have to add that feature to one, like
https://github.com/shawn1m/overture or
https://github.com/socketry/async-dns.

Do you have some suggestions regarding this? Am I thinking in the right
direction?

I don't see how to otherwise make this setup work, currently it's always
some % chance of false positive no matter how much I increase min-cache-ttl.
Though if clients don't request low TTL domains frequently then it's better
to have min-cache-ttl set to a low value so it expires faster in the cache.
And if they request it frequently it's better to increase it to a
(relatively) high value so it's lower chance of a client request hitting
that cached TTL 0. It's a search for the sweet spot.

My full config:
---
visible_hostname squid
cache deny all
via off
httpd_suppress_version_string on
debug_options ALL,1
dns_nameservers 172.16.11.11
positive_dns_ttl 0
negative_dns_ttl 0
acl localnet src 0.0.0.1-0.255.255.255	# RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8		# RFC 1918 local private network (LAN)
acl localnet src 100.64.0.0/10		# RFC 6598 shared address space (CGN)
acl localnet src 169.254.0.0/16 	# RFC 3927 link-local (directly plugged)
machines
acl localnet src 172.16.0.0/12		# RFC 1918 local private network (LAN)
acl localnet src 192.168.0.0/16		# RFC 1918 local private network (LAN)
acl localnet src fc00::/7       	# RFC 4193 local private network range
acl localnet src fe80::/10      	# RFC 4291 link-local (directly plugged)
machines
acl CONNECT method CONNECT
acl SSL_port port 443
http_access allow SSL_port
http_access deny CONNECT !SSL_port
http_access allow localhost manager
http_access deny manager
http_access allow localhost
reply_header_access Server deny all
reply_header_access X-Squid-Error deny all
reply_header_access X-Cache deny all
reply_header_access X-Cache-Lookup deny all
client_persistent_connections off
http_port 3128
http_port 3129 intercept
host_verify_strict on
acl allowed_http_sites dstdom_regex "/etc/squid/allow_list.conf"
http_access allow allowed_http_sites
https_port 3130 ssl-bump intercept tls-cert=/etc/squid/ssl/squid.pem 
tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt
acl allowed_https_sites ssl::server_name_regex "/etc/squid/allow_list.conf"
ssl_bump splice allowed_https_sites
ssl_bump peek all
ssl_bump terminate all
http_access deny all
logformat general      %tl %6tr %>a %Ss/%03>Hs %<st %rm %ssl::bump_mode
%ssl::<cert_subject %ssl::<cert_errors %ru %ssl::>sni
access_log daemon:/var/log/squid/access.log general
---



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From trapexit at spawn.link  Wed Oct  9 17:34:49 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Wed, 9 Oct 2019 13:34:49 -0400
Subject: [squid-users] re-forwarding questions
Message-ID: <7062c786-4f98-47c1-fee4-f8dd3c0efaf9@spawn.link>

I've a setup with N origin servers and N squid instances setup with the 
intent that each squid instance is logically associated with one of the 
origin servers and lists the other N-1 origin servers as other parents. 
Each origin has the same immutable data but spread across buildings. I 
only want them to reach out across buildings if the initial forward 
fails (or perhaps is overloaded). Initial tests seem to work but they 
always try the first origin server and when it fails (in this case with 
a 502 as the backend of a webservice is down) it tries the second. It 
doesn't appear that there is a negative cache on the failure. Is that 
configurable? Regardless, it doesn't cache the re-forward response (a 
200). Is that expected? Configurable? It caches the primary fine when 
functional. A snippet of my config is below. I've looked over the docs 
and source but I'm not finding anything. Also, is it possible from the 
logs to see the name or hostname:port of the cache_peer being forwarded to?

cache_peer HOSTNAME0 parent PORT 0 no-query originserver no-digest ignore-cc
cache_peer HOSTNAME1 parent PORT 0 no-query originserver no-digest ignore-cc
retry_on_error on
cache allow all
refresh_pattern . 1440 100% 10080 ignore-no-cache ignore-no-store 
ignore-private

Thanks



From rousskov at measurement-factory.com  Wed Oct  9 20:35:44 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Oct 2019 16:35:44 -0400
Subject: [squid-users] re-forwarding questions
In-Reply-To: <7062c786-4f98-47c1-fee4-f8dd3c0efaf9@spawn.link>
References: <7062c786-4f98-47c1-fee4-f8dd3c0efaf9@spawn.link>
Message-ID: <40f4005f-f765-73e3-9368-de209eff5a33@measurement-factory.com>

On 10/9/19 1:34 PM, Antonio SJ Musumeci wrote:
> I've a setup with N origin servers and N squid instances setup with the
> intent that each squid instance is logically associated with one of the
> origin servers and lists the other N-1 origin servers as other parents.
> Each origin has the same immutable data but spread across buildings. I
> only want them to reach out across buildings if the initial forward
> fails (or perhaps is overloaded). Initial tests seem to work but they
> always try the first origin server and when it fails (in this case with
> a 502 as the backend of a webservice is down) it tries the second.

Sounds good.


> It doesn't appear that there is a negative cache on the failure.

The peer up/down state is "cached", but

1) Squid does not treat an HTTP 502 response as a peer failure (even
though the request may be re-forwarded after a 502 response). Roughly
speaking, only TCP- and TLS-level failures are considered to be peer
failures. HTTP responses, regardless of their status code, do not
increment the peer failure counter AFAICT.

2) By default, a peer is not considered down after the first 9 peer
failures. See cache_peer connect-fail-limit.

There is also a (complicated) peer revival mechanism that may mark a
previously "down" peer as "up" even though there were no HTTP requests
sent to that peer IIRC. The details are complex and depend on the Squid
version.


> it doesn't cache the re-forward response (a 200). Is that expected?

I do not think it should be expected. The number of past/failed
forwarding attempts should not affect response cachability IMO.


> is it possible from the logs to see the name or hostname:port of the
> cache_peer being forwarded to?

The following logformat %codes may be helpful here (%<a is logged by
default and is given here for completeness sake):

  %<a  Server IP address of the last server or peer connection
  %<A  Server FQDN or peer name
  %<p  Server port number of the last server or peer connection

To log the last cache_peer_access match, you will need to use
annotate_transaction ACL and %note logformat code (where available).
However, IIRC, official Squids may evaluate the same matching
cache_peer_access rule many times per transaction, which may result in
confusing annotations.


HTH,

Alex.


From marcelosantos.mail at uol.com.br  Thu Oct 10 02:59:10 2019
From: marcelosantos.mail at uol.com.br (Marcelo Rodrigo - Graminsta.com.br)
Date: Wed, 9 Oct 2019 23:59:10 -0300
Subject: [squid-users] How to make only IPV6 visible even incoming via IPV4?
Message-ID: <000001d57f16$b1381b70$13a85250$@uol.com.br>

Hi folks,

Great job you are doing on the Squid Project!!!

 

I have a Squid setup I am using as proxy for over a year now all working
just fine;)

 

Now I am implementing IPV6 as outgoing_address. So a customer enters with an
IPV4 and is routed out via IPV6 like below in squid.conf:

 

http_port 182.XX.XX.97:4444 name=166

acl ip166 myportname 166

tcp_outgoing_address XXXX:XXXX:XXX::7bb

 

The issue is when I verify IP leads to avoid proxy detection using websites
like https://ipleak.net it shows both IPs, IPV4 and IPV6.

I need it to show only IPV6.

I am using this setup to access just one IPV6 service and in the future my
plans are to access IPV4 websites too.

 

I am using squid 4.8 on Ubuntu 18.04.3 recently upgraded from version 16.04.

As the upgrade did not enabled netplan, I am still using the
/etc/network/interfaces legacy as below:

 

 

auto lo

iface lo inet loopback

 

auto eth0

iface eth0 inet6 static

address XXXX:XXXX:XXX::6CA

netmask 64

broadcast XXXX:XXXX:XXX::6CA

gateway XXXX:XXXX:XXX::1

 

dns-nameservers  2620:119:35::35 2606:4700:4700::1111

 

#---------------------------------------------------------

 

auto eth0:1

iface eth0:1 inet static

address 182.XX.XX.97

netmask 255.255.255.0

broadcast 182.XX.XX.97

gateway 182.XX.XX.254

 

 

auto eth0:2

iface eth0:2 inet6 static

address XXXX:XXXX:XXX::7BB

netmask 64

broadcast XXXX:XXXX:XXX::7BB

gateway XXXX:XXXX:XXX::1

 

Any ideas about how to make Squid shows only the IPV6 from
tcp_outgoing_address?

 

Thanks in advance for all the help!

Marcelo Rodrigo

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191009/d267487b/attachment.htm>

From squid3 at treenet.co.nz  Thu Oct 10 05:33:50 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Oct 2019 18:33:50 +1300
Subject: [squid-users] How to make only IPV6 visible even incoming via
 IPV4?
In-Reply-To: <000001d57f16$b1381b70$13a85250$@uol.com.br>
References: <000001d57f16$b1381b70$13a85250$@uol.com.br>
Message-ID: <6300c252-09b1-88ef-a091-8a8f2156e9df@treenet.co.nz>

On 10/10/19 3:59 pm, Marcelo Rodrigo - Graminsta.com.br wrote:
> 
> Now I am implementing IPV6 as outgoing_address. So a customer enters
> with an IPV4 and is routed out via IPV6 like below in squid.conf:
> 
> ?
> 
> http_port 182.XX.XX.97:4444 name=166
> 
> acl ip166 myportname 166
> 
> tcp_outgoing_address XXXX:XXXX:XXX::7bb
> 
> ?
> 
> The issue is when I verify IP leads to avoid proxy detection using
> websites like https://ipleak.net it shows both IPs, IPV4 and IPV6.
> 
> I need it to show only IPV6.
> 
...
> 
> Any ideas about how to make Squid shows only the IPV6 from
> tcp_outgoing_address?
> 

That website you are using for your checks actively tests for ability to
connect to IPv4-only servers. So long as your network does IPv4 this
type of test will show it.

The solution (if you really want to) is one of these:

 * disable IPv4 on your network. If the connectivity for IPv4 does not
exist those addresses cannot be "leaked".

 * configure your DNS to not produce A responses. If Squid cannot
resolve server IPv4 addresses, it will not try to connect to any.

 * configure your firewall to reject (not drop) attempts to connect via
IPv4.


Naturally, expect to have some amount of the Internet to be unusable.
That amount is much smaller than most people think, but if you have a
client depending on even one IPv4-only site on a regular basis it can be
extremely annoying for them.


Amos


From josepjones at expediagroup.com  Thu Oct 10 14:43:09 2019
From: josepjones at expediagroup.com (Joseph Jones)
Date: Thu, 10 Oct 2019 14:43:09 +0000
Subject: [squid-users] Clarification on behavior
Message-ID: <CY4PR02MB272512CE602386C773E28F02B0940@CY4PR02MB2725.namprd02.prod.outlook.com>

we are using squid a a perimeter egress filter. one think I've recently noticed is based on my current config it's possible to make a request through squid to an HTTPS endpoint with out doing a CONNECT request. 

I was wondering if this should be allowed behavior for a proxy or if it's just a business requirement to deny that type of request or if that behavior shouldn't be allowed anyway.  My concern being if squid is deployed in an environment that has PCI/PII data I wouldn't want squid to hold that data decrypted even for a little while and the client should have encrypted tunnel through to the server. 

the following request gets rejected and should be. (This is because of the rule: http_access deny CONNECT !SSL_ports)
cat <<EOL | nc localhost 3128
CONNECT ifconfig.io:80 HTTP/1.1
Host: ifconfig.io:80
User-Agent: curl/7.29.0
Proxy-Connection: Keep-Alive

EOL


however this request is allowed unless I add (http_access deny SSL_Ports !CONNECT)
cat <<EOL | nc localhost 3128
GET https://ifconfig.io/ip HTTP/1.1
User-Agent: curl/7.29.0
Host: ifconfig.io
Accept: */*
Proxy-Connection: Keep-Alive

EOL

This request is of concern because this means squid is doing the https request and decrypting the response before returning it to the client. I can solve this by making the squid endpoint SSL too. but even then I don't want squid to have the data decrypted at all. Which is why I've added the extra rule.

I'm testing this off of latest master commit. 

basically I'm wonder if my extra access rule of http_access deny SSL_PORTS !CONNECT is sufficient enough to make sure squid doesn't decrypt the response. 


$ ./src/squid -v
Squid Cache: Version 5.0.0-VCS
Service Name: squid

This binary uses OpenSSL 1.1.1d FIPS  10 Sep 2019. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/home/josepjones/.local/squid' '--enable-icmp' '--with-openssl'


squid.conf:
debug_options ALL,1 11,3 rotate=0

# tg  - GMT time
# >a  - Client source IP address
# >p  - Client source port
# Ss  - Squid request status
# >Hs - HTTP status code sent to client
# <st - Total size of reply sent to client
# >st - Total size of request received from client. Excluding chunked encoding bytes.
# >rm - Request method from client
# >ru - Request URL received from client
# >rd - Request URL domain from client
# <a  - Server IP address of the last server or peer connection
logformat my_squid [%tl] %>a %6>p %Ss/%03>Hs %>st %<st %>rm %>ru %>rd/%<a

access_log stdio:/dev/stdout logformat=my_squid rotate=0
cache_log stdio:/dev/stderr

acl SSL_ports port 443
acl Safe_ports port 80    # http
acl Safe_ports port 8080  # http
acl Safe_ports port 443   # https
acl CONNECT method CONNECT

acl http_whitelist dstdomain "/home/josepjones/.local/squid/etc/whitelist.txt"
acl http_blacklist dstdomain "/home/josepjones/.local/squid/etc/blacklist.txt"

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

http_access deny SSL_Ports !CONNECT

http_access deny http_blacklist
http_access allow http_whitelist

http_access deny CONNECT http_blacklist
http_access allow CONNECT http_whitelist

http_access deny all

# disable caching
cache deny all


# Squid normally listens to port 3128
http_port 3128
visible_hostname squid

# Uncomment and adjust the following to add a disk cache directory.
cache_mem 0
# cache_dir rock /home/josepjones/.local/squid/var/spool 100

# Leave coredumps in the first cache dir
coredump_dir /home/josepjones/.local/squid

--?



Joseph M Jones





From darren at ksn-systems.com  Thu Oct 10 19:46:57 2019
From: darren at ksn-systems.com (Darren Breeze)
Date: Fri, 11 Oct 2019 08:46:57 +1300
Subject: [squid-users] Icap redirection issues
Message-ID: <aa0e42b9-a3a7-442f-8b3e-1d2f49339a9f@www.fastmail.com>

HI

I am trying to set up redirection via icap for a project I am working on.

my current icap config looks like this

icap_service service_res respmod_precache icap://127.0.0.1:1344/respmode bypass=0
adaptation_access service_res allow all

and I am generating responses that look like this (in the log)

2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(654) parseMore:
ICAP/1.0 200 OK
Connection: close
Date: Thu Oct 10 19:42:48 2019 UTC
ISTag: BITZ-1570736568-132184
Server: bitz-server 2.0.0
Encapsulated: req-hdr=0, res-hdr=615

GET http://www.frip.com/favicon.ico HTTP/1.1
Pragma: no-cache
Cache-Control: no-cache
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36
Accept: image/webp,image/apng,image/*,*/*;q=0.8
Referer: http://www.frip.com/
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9
Cookie: viewmode=regular; browser_id=130.211.1.243%3A5d93bd8d5d5f0; language=en-us; _ga=GA1.2.2047886438.1569970932; _gid=GA1.2.1012209846.1570399958; count=12; gamexsess=fbr4qbg0fp4q111jq6pj5r9517; env=%7Bww%3A1903%2Cwh%3A969%7D
Host: www.frip.com

HTTP/1.1 307 Moved Temporarily
Location: http://www.test.com/
Date: Thu, 10 Oct 2019 19:42:48 UTC
Origin: Local


2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(749) parseHeaders: parse ICAP headers

looking at the log, there seesm to be something in the header that is not correct

2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(1079) parseHead: have 905 head bytes to parse; state: 0
2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(1094) parseHead: parse success, consume 170 bytes, return true
2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(785) parseIcapHead: found connection close
2019/10/10 19:42:48.138 kid1| 93,7| ModXact.cc(523) stopBackup: will no longer backup [FD 16;RrBp(1)S(2)G/w job23]
2019/10/10 19:42:48.138 kid1| 93,9| ModXact.cc(427) virginConsume: consumption guards: 00001
2019/10/10 19:42:48.138 kid1| 93,8| ModXact.cc(446) virginConsume: postponing consumption from [0<=1066<=1066 1066+981 pipe0x55a883de4f08 cons0x55a883de6208]
2019/10/10 19:42:48.138 kid1| 93,7| ModXact.cc(646) checkConsuming: will stop consuming [FD 16;Rrp(1)S(2)G/w job23]
2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(754) parseHeaders: parse HTTP headers
2019/10/10 19:42:48.138 kid1| 93,4| Xaction.cc(514) setOutcome: ICAP_MOD
2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(1079) parseHead: have 735 head bytes to parse; state: 1
2019/10/10 19:42:48.138 kid1| 93,3| ../../../src/base/AsyncJobCalls.h(177) dial: Adaptation::Icap::Xaction::noteCommRead threw exception: parsed || !error
2019/10/10 19:42:48.138 kid1| 93,3| Xaction.cc(512) setOutcome: Warning: reseting outcome: from ICAP_MOD to ICAP_ERR_OTHER
2019/10/10 19:42:48.138 kid1| 93,4| ServiceRep.cc(80) noteFailure: failure 2 out of 10 allowed in 0sec [up,fail2]
2019/10/10 19:42:48.138 kid1| 93,5| AsyncJob.cc(84) mustStop: Adaptation::Icap::ModXact will stop, reason: exception
2019/10/10 19:42:48.138 kid1| 93,5| AsyncJob.cc(137) callEnd: Adaptation::Icap::Xaction::noteCommRead(local=127.0.0.1:36938 remote=127.0.0.1:1344 FD 16 flags=1, data=0x55a883de6108, size=905, buf=0x55a883df7190) ends job [FD 16;rp(1)S(2)G/Rw job23]
2019/10/10 19:42:48.138 kid1| 93,5| ModXact.cc(1242) swanSong: swan sings [FD 16;rp(1)S(2)G/Rw job23]


I can't find any further detail / logging for the exception that was thrown.

Am I missing something obvious in my approach or something else?

thanks in advance

Darren B.




















This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the system manager. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. If you are not the intended recipient you are notified that disclosing, copying, distributing or taking any action in reliance on the contents of this information is strictly prohibited.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191011/498f2731/attachment.htm>

From rousskov at measurement-factory.com  Thu Oct 10 21:28:16 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 10 Oct 2019 17:28:16 -0400
Subject: [squid-users] Icap redirection issues
In-Reply-To: <aa0e42b9-a3a7-442f-8b3e-1d2f49339a9f@www.fastmail.com>
References: <aa0e42b9-a3a7-442f-8b3e-1d2f49339a9f@www.fastmail.com>
Message-ID: <3df89698-6aca-a71b-1987-05a54d87bb2a@measurement-factory.com>

On 10/10/19 3:46 PM, Darren Breeze wrote:
> I am generating responses that look like this

> ICAP/1.0 200 OK
...
> Encapsulated: req-hdr=0, res-hdr=615

Depending on the ICAP request method and other details, the ICAP server
must send either an embedded HTTP request or an embedded HTTP response.

Currently, your ICAP server sends an embedded HTTP request _and_ an
embedded HTTP response (header) which may violate the ICAP specs and is
definitely not supported by Squid (because it does not make sense in
Squid context -- only one message can be adapted at a time).


Please note that the ICAP RESPMOD _request_ does (usually) contain both
an embedded HTTP request and an embedded HTTP response, but since the
ICAP server cannot modify the embedded request (too late for that!), the
ICAP server should only return an embedded response (the same or a
different/adapted one).


HTH,

Alex.


From darren at ksn-systems.com  Thu Oct 10 21:37:00 2019
From: darren at ksn-systems.com (Darren Breeze)
Date: Fri, 11 Oct 2019 10:37:00 +1300
Subject: [squid-users] Icap redirection issues
In-Reply-To: <3df89698-6aca-a71b-1987-05a54d87bb2a@measurement-factory.com>
References: <aa0e42b9-a3a7-442f-8b3e-1d2f49339a9f@www.fastmail.com>
 <3df89698-6aca-a71b-1987-05a54d87bb2a@measurement-factory.com>
Message-ID: <24a91dbe-d849-4fc3-9cb4-b6d47c6adca8@www.fastmail.com>

Hi Alex

Thank you, I have removed that req header header and it is now redirecting

thanks

Darren B.
This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you have received this email in error please notify the system manager. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. If you are not the intended recipient you are notified that disclosing, copying, distributing or taking any action in reliance on the contents of this information is strictly prohibited.



From rousskov at measurement-factory.com  Thu Oct 10 22:06:49 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 10 Oct 2019 18:06:49 -0400
Subject: [squid-users] Clarification on behavior
In-Reply-To: <CY4PR02MB272512CE602386C773E28F02B0940@CY4PR02MB2725.namprd02.prod.outlook.com>
References: <CY4PR02MB272512CE602386C773E28F02B0940@CY4PR02MB2725.namprd02.prod.outlook.com>
Message-ID: <4b35ce30-36b6-ac39-5c59-e72088ef3c41@measurement-factory.com>

On 10/10/19 10:43 AM, Joseph Jones wrote:

> I've recently noticed is based on my current config it's possible to
> make a request through squid to an HTTPS endpoint with out doing a
> CONNECT request.


> I was wondering if this should be allowed behavior for a proxy or if
> it's just a business requirement to deny that type of request or if
> that behavior shouldn't be allowed anyway.

This is not a protocol issue: There are Squid deployments where "GET
https://..." requests are perfectly normal, and there are Squid
deployments where such requests must be blocked. Configure your Squid to
match your environment.


> My concern being if squid is deployed in an environment that has
> PCI/PII data I wouldn't want squid to hold that data decrypted even
> for a little while and the client should have encrypted tunnel
> through to the server.


> I'm wonder if my extra access rule of http_access deny SSL_PORTS
> !CONNECT is sufficient enough to make sure squid doesn't decrypt the
> response.

> http_access deny CONNECT !SSL_ports
> http_access deny SSL_PORTS !CONNECT

The above does not deny "GET https://example.com:80/". That may be OK in
your setup because there are no TLS servers on port 80 behind your
Squid, but, in general, it is not enough to prevent Squid decryption. If
you want to follow this path, you probably want access rules based on
request-target (a.k.a. URI) scheme -- prohibit requests with an "https:"
scheme.

If you do not want Squid to decrypt, consider building Squid without
OpenSSL (and GnuTLS) support. That way, if something slips past your
rules (for any reason), you can still be sure that Squid will not
decrypt anything.


HTH,

Alex.


> acl SSL_ports port 443
> acl Safe_ports port 80    # http
> acl Safe_ports port 8080  # http
> acl Safe_ports port 443   # https

> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> http_access deny SSL_Ports !CONNECT
> 
> http_access deny http_blacklist
> http_access allow http_whitelist
> 
> http_access deny CONNECT http_blacklist
> http_access allow CONNECT http_whitelist
> 
> http_access deny all
> 
> # disable caching
> cache deny all
> 
> 
> # Squid normally listens to port 3128
> http_port 3128
> visible_hostname squid
> 
> # Uncomment and adjust the following to add a disk cache directory.
> cache_mem 0
> # cache_dir rock /home/josepjones/.local/squid/var/spool 100
> 
> # Leave coredumps in the first cache dir
> coredump_dir /home/josepjones/.local/squid
> 
> --?
> 
> 
> 
> Joseph M Jones
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Fri Oct 11 06:28:09 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Oct 2019 19:28:09 +1300
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <1570606560820-0.post@n4.nabble.com>
References: <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
 <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
 <1511794211.2147.4.camel@slave-tothe-box.net>
 <7e3e81c9-0949-9f16-f1f8-7efc052dae78@treenet.co.nz>
 <7512b52427c65ed6a0364885bc359d48@localhost>
 <fc698c26-ea16-2a0c-f83b-e4b1609f0aaa@treenet.co.nz>
 <1567370644478-0.post@n4.nabble.com>
 <43802926-c454-112d-f0bd-c0b377f5b058@treenet.co.nz>
 <1570606560820-0.post@n4.nabble.com>
Message-ID: <7b7cac7d-0acd-5823-d247-5aeb3d2ab7e9@treenet.co.nz>

On 9/10/19 8:36 pm, torson wrote:
> @Amos thank you for your detailed reply. It took me a while to get back to
> this task, sorry.
> I did some changes, added your suggestions, tested some more and here are my
> results using Squid 4.8 with a couple of questions:
> 
> A short summary of my setup: Squid that does only intercept for all servers
> in the same network. There's Dnsmasq set up on the same server and is used
> by Squid and all other servers. 
> 
> ### 1. https certificate check
> I've managed to make it work with "ssl_bump peek all" but it started working
> only after I've switched the order of splice and peek:
> ---
> ssl_bump splice allowed_https_sites
> ssl_bump peek all
> ssl_bump terminate all
> ---

With an intercept proxy this makes the proxy do:

 step 1)
   peek (allowed_https_sites does not match), then

 step 2)
   splice if allowed_https_sites matched, else peek

 step 3)
   splice if allowed_https_sites matched, else terminate


> 
> Though now I don't get "terminate" entries in the logfile for blocked
> requests for most domains I'm testing with, instead those are now "peek"
> entries. 
> 

Do you actually understand why?

The earlier setups you were either splicing only at step 1 OR only at
step 2. This new setup can splice at whichever step supplies a
whitelisted domain name.

So now clients which lie are allowed to splice, AND clients which do not
supply SNI but go to an okay server are allowed to splice.

NP: clients which lie may not be as bad as it seems. With splice the
server gets the lie and can reject the request if it is legitimate. For
there to be a security problem the client and server need to collude.

...
> 
> So there seems to be variations on how Squid is handling traffic for various
> sites.

Of course, you put data-dependent conditions (allowed_https_sites ACL)
on what handling can be performed. Thus making the proxy behaviour
conditional on what Hello data the remote TLS agents are supplying.

As clients and servers differ so will the resulting proxy behaviour.


> I lean more towards having increased security than consistent logging, seems
> like "peek" effectively means "terminate" in this case, so I'll just count
> that together for alerting.

On the contrary, since this is an interception proxy peek will always
preceed both splice and terminate.

If the logging is working correctly you will see the peek's AND the
splice's AND any terminate's.


> 
> Do you have some suggestion to make this work better or is this just the
> best I can squeeze out of Squid at this time?
> 

AFAIK this is the best available in Squid-4. The latest Squid-5 may be
slightly better, it has a number of quality-of-life feature additions
for TLS related things.



> ### 2. false host-forgery blocking due to stale DNS record
> 
...
> 
> Do you have some suggestions regarding this? Am I thinking in the right
> direction?

AFAIK those directions are reasonable for Squid today. We are still
waiting on someone having the time/resources to contribute a refactor to
Squid DNS that further reduces these false-positives.
 I have a plan, but no time to implement it. Factory people have laid a
lot of the groundwork in Squid-5 but we are not completely there yet.

Amos


From squid3 at treenet.co.nz  Fri Oct 11 07:14:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Oct 2019 20:14:10 +1300
Subject: [squid-users] Icap redirection issues
In-Reply-To: <24a91dbe-d849-4fc3-9cb4-b6d47c6adca8@www.fastmail.com>
References: <aa0e42b9-a3a7-442f-8b3e-1d2f49339a9f@www.fastmail.com>
 <3df89698-6aca-a71b-1987-05a54d87bb2a@measurement-factory.com>
 <24a91dbe-d849-4fc3-9cb4-b6d47c6adca8@www.fastmail.com>
Message-ID: <a9691ffc-cd76-f06d-3638-dd8a2afd9159@treenet.co.nz>

On 11/10/19 10:37 am, Darren Breeze wrote:
> Hi Alex
> 
> Thank you, I have removed that req header header and it is now redirecting
> 

Since you seem to be in a position to fix these things;

Date is supposed to always use "GMT" timezone in HTTP, and have a comma
after the day name.

Server is supposed to have a slash ('/') between the server software
name and version number.

Origin is supposed to have a URL origin, scheme://host:port


Amos


From vieridipaola at gmail.com  Fri Oct 11 13:35:03 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Fri, 11 Oct 2019 15:35:03 +0200
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>

Hi,

I'm trying to connect from a LAN client with IP addr. 10.215.144.48 to
a web server through Squid 3 + Tproxy.

As you can see from the logs here below, there seems to be a timeout:

https://pastebin.com/2Jka4es1

The Squid machine has no issues if I browse the web from command line,
eg. 'links http://www.linuxheadquarters.com' works fine.

What should I be looking for?

Thanks,

Vieri


From squid3 at treenet.co.nz  Fri Oct 11 13:50:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 Oct 2019 02:50:24 +1300
Subject: [squid-users] (no subject)
In-Reply-To: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
Message-ID: <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>

On 12/10/19 2:35 am, Vieri Di Paola wrote:
> Hi,
> 
> I'm trying to connect from a LAN client with IP addr. 10.215.144.48 to
> a web server through Squid 3 + Tproxy.
> 
> As you can see from the logs here below, there seems to be a timeout:
> 
> https://pastebin.com/2Jka4es1

That log contains only a few lines of actual relevance to the problem:

2019/10/11 15:13:48.003 kid1| 5,5| comm.cc(1574) checkTimeouts:
checkTimeouts: FD 14 Expired
2019/10/11 15:13:48.003 kid1| 5,5| comm.cc(1577) checkTimeouts:
checkTimeouts: FD 14: Call timeout handler
2019/10/11 15:13:48.003 kid1| 5,4| AsyncCall.cc(93) ScheduleCall:
comm.cc(1580) will call Comm::ConnOpener::timeout(local=10.215.144.48
remote=172.217.17.5:443 flags=25, data=0x149ac58) [call7547]
2019/10/11 15:13:48.023 kid1| 5,4| AsyncCallQueue.cc(55) fireNext:
entering Comm::ConnOpener::timeout(local=10.215.144.48
remote=172.217.17.5:443 flags=25, data=0x149ac58)
2019/10/11 15:13:48.023 kid1| 5,4| AsyncCall.cc(38) make: make call
Comm::ConnOpener::timeout [call7547]
2019/10/11 15:13:48.023 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x149ac58
2019/10/11 15:13:48.023 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x149ac58
2019/10/11 15:13:48.023 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x149ac58
2019/10/11 15:13:48.023 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x149ac58
2019/10/11 15:13:48.023 kid1| 5,4| AsyncJob.cc(123) callStart:
Comm::ConnOpener status in: [ job929]
2019/10/11 15:13:48.023 kid1| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x149ac58
2019/10/11 15:13:48.023 kid1| 5,5| ConnOpener.cc(442) timeout:
local=10.215.144.48 remote=172.217.17.5:443 flags=25: * - ERR took too
long to receive response.


Note that this last entry is about a connection to port 443, whereas the
rest of the log is all about traffic to port 80.


> 
> The Squid machine has no issues if I browse the web from command line,
> eg. 'links http://www.linuxheadquarters.com' works fine.
> 
> What should I be looking for?

TCP/IP level packet routing. Squid is trying to open a TCP connection to
that "remote=" server. TCP SYN is sent, and then ... ... ... nothing.


Amos


From ali.galip.camli at epati.com.tr  Mon Oct 14 08:51:46 2019
From: ali.galip.camli at epati.com.tr (=?UTF-8?B?QWxpIEdhbGlwIMOHYW1sxLE=?=)
Date: Mon, 14 Oct 2019 11:51:46 +0300
Subject: [squid-users] Filtering cipher suites and certificate algorithms
 without man-in-the-middle
Message-ID: <8a40d46a-6a73-b4fd-b4b1-b976a4c6c23c@epati.com.tr>

Hi,

I've set up a firewall and proxy with pf & Squid on FreeBSD. Is it
possible to observe and filter with squid which cipher suite is selected
between end points (client and server) without changing their SSL
certificate, without mimicking server certificate?

My main goal is to avoid weak ciphers that parties agree upon. I want to
force my clients to use modern algorithms while surfing on internet
filtered by Squid.

For example, if client and server get on MD5 or SHA1, DES or RC4
included cipher suite, or on SSLv3, or, if server sends my client a
certificate signed with SHA1, or an expired certificate etc., I want to
ban the traffic.

There is a directive '*tls_outgoing_options*' in Squid and it has
'*cipher*' and '*min-version*' configurations. Do these configurations
satisfy my goal?

Sincerely,
Ali

Note: I already asked this question in
https://serverfault.com/questions/987463/filtering-cipher-suites-and-certificate-algorithms-without-man-in-the-middle
&?
https://crypto.stackexchange.com/questions/74936/observing-cipher-suites-and-certificate-algorithms-without-man-in-the-middle

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191014/37584928/attachment.htm>

From rousskov at measurement-factory.com  Mon Oct 14 18:46:40 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Oct 2019 14:46:40 -0400
Subject: [squid-users] Filtering cipher suites and certificate
 algorithms without man-in-the-middle
In-Reply-To: <8a40d46a-6a73-b4fd-b4b1-b976a4c6c23c@epati.com.tr>
References: <8a40d46a-6a73-b4fd-b4b1-b976a4c6c23c@epati.com.tr>
Message-ID: <fa2f5546-23a0-a99b-a9b8-cf83dcdb64e1@measurement-factory.com>

On 10/14/19 4:51 AM, Ali Galip ?aml? wrote:

> Is it
> possible to observe and filter with squid which cipher suite is selected
> between end points (client and server) without changing their SSL
> certificate, without mimicking server certificate?

It is possible to observe unencrypted handshake details, but you cannot
change anything without bumping.


> My main goal is to avoid weak ciphers that parties agree upon. 

You may be able to block TCP connections carrying cipher offers that
include "weak" ciphers, but a non-bumping Squid cannot remove a cipher
from the offered cipher list. IIRC, the mutually agreed upon cipher is
encrypted, even before TLS v1.3, so you would not be able to see it.


> For example, if client and server get on MD5 or SHA1, DES or RC4
> included cipher suite, or on SSLv3, or, if server sends my client a
> certificate signed with SHA1, or an expired certificate etc., I want to
> ban the traffic.

You can do some of the above using "ssl_bump peek/terminate" and/or
"http_access deny" rules. I am not sure there are built-in ACLs for
detecting all the handshake aspects you want to detect; you may have to
use an external ACL with various %ssl:... logformat codes and/or
%>handshake logformat code.


> There is a directive '*tls_outgoing_options*' in Squid and it has
> '*cipher*' and '*min-version*' configurations. Do these configurations
> satisfy my goal?

IIRC, no. That directive applies to TLS connections initiated by Squid,
not TLS connections tunneled by Squid.


HTH,

Alex.


From nicolas.doigny at gmail.com  Tue Oct 15 18:03:49 2019
From: nicolas.doigny at gmail.com (ndoigny)
Date: Tue, 15 Oct 2019 13:03:49 -0500 (CDT)
Subject: [squid-users] Problem webpagge filter - acl & http_access
Message-ID: <1571162629103-0.post@n4.nabble.com>

Hi All,

I did a Squid basic configuration on the port 3128.

The server proxy works correcly and I can browser from a client machine when
I configure the proxy configuration.
But when I try to do some web filtering on some sites, I always manage to
access it

I created a txt file 'blocked_sites' in the path 'C:\squid\etc\squid'.
I created the following ACL :
acl blocked_sites dstdomain '/etc/squid/blocked_sites.txt'
http_access deny blocked_sites

In the blocked_sites file txt, I indicate :

.facebook.com
.msn.com
.orange.be

I restarted the Squid service but the filter isn't working.

Can you help me ?

Thanks in advance.

Nicolas



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From trapexit at spawn.link  Tue Oct 15 21:24:19 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Tue, 15 Oct 2019 17:24:19 -0400
Subject: [squid-users] pinger replacements?
Message-ID: <3511a39e-2b57-5c4f-8faf-5207fb9d5db2@spawn.link>

I'm deploying Squid in a container and the environment is fairly locked 
down. No CAP_NET_RAW or CAP_NET_ADMIN. Dropping of CAP_SETUID and 
CAP_SETGID. Has anyone created a replacement that uses some other 
mechanism? Is the ipc Squid and pinger documented?



From m_zouhairy at skno.by  Wed Oct 16 05:52:20 2019
From: m_zouhairy at skno.by (Vacheslav)
Date: Wed, 16 Oct 2019 08:52:20 +0300
Subject: [squid-users] Problem webpagge filter - acl & http_access
In-Reply-To: <1571162629103-0.post@n4.nabble.com>
References: <1571162629103-0.post@n4.nabble.com>
Message-ID: <b5db74acd85ea0b9c07c46f88a28fad1c89a047c.camel@skno.by>

I too wanted to block, but it was youtube, and eventually the best way
to do it was squid + ufdbguard

On Tue, 2019-10-15 at 13:03 -0500, ndoigny wrote:
> Hi All,
> 
> I did a Squid basic configuration on the port 3128.
> 
> The server proxy works correcly and I can browser from a client
> machine when
> I configure the proxy configuration.
> But when I try to do some web filtering on some sites, I always
> manage to
> access it
> 
> I created a txt file 'blocked_sites' in the path
> 'C:\squid\etc\squid'.
> I created the following ACL :
> acl blocked_sites dstdomain '/etc/squid/blocked_sites.txt'
> http_access deny blocked_sites
> 
> In the blocked_sites file txt, I indicate :
> 
> .facebook.com
> .msn.com
> .orange.be
> 
> I restarted the Squid service but the filter isn't working.
> 
> Can you help me ?
> 
> Thanks in advance.
> 
> Nicolas
> 
> 
> 
> --
> Sent from: 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




From rst at fomar.com.pl  Wed Oct 16 07:50:19 2019
From: rst at fomar.com.pl (=?UTF-8?Q?Rafa=C5=82_Stanilewicz?=)
Date: Wed, 16 Oct 2019 08:50:19 +0100
Subject: [squid-users] Problem webpagge filter - acl & http_access
In-Reply-To: <b5db74acd85ea0b9c07c46f88a28fad1c89a047c.camel@skno.by>
References: <1571162629103-0.post@n4.nabble.com>
 <b5db74acd85ea0b9c07c46f88a28fad1c89a047c.camel@skno.by>
Message-ID: <CAPnyBTMNZsSVD+5zrxLpW0sxwv=BkpUh+ezJZAYD5bxTnc1_oQ@mail.gmail.com>

Most likely your browser redirects you to HTTPS, instead of HTTP, so
what you need is peek and splice.

On Wed, 16 Oct 2019 at 06:52, Vacheslav <m_zouhairy at skno.by> wrote:
>
> I too wanted to block, but it was youtube, and eventually the best way
> to do it was squid + ufdbguard
>
> On Tue, 2019-10-15 at 13:03 -0500, ndoigny wrote:
> > Hi All,
> >
> > I did a Squid basic configuration on the port 3128.
> >
> > The server proxy works correcly and I can browser from a client
> > machine when
> > I configure the proxy configuration.
> > But when I try to do some web filtering on some sites, I always
> > manage to
> > access it
> >
> > I created a txt file 'blocked_sites' in the path
> > 'C:\squid\etc\squid'.
> > I created the following ACL :
> > acl blocked_sites dstdomain '/etc/squid/blocked_sites.txt'
> > http_access deny blocked_sites
> >
> > In the blocked_sites file txt, I indicate :
> >
> > .facebook.com
> > .msn.com
> > .orange.be
> >
> > I restarted the Squid service but the filter isn't working.
> >
> > Can you help me ?
> >
> > Thanks in advance.
> >
> > Nicolas
> >
> >
> >
> > --
> > Sent from:
> > http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Zanim wydrukujesz, pomy?l o ?rodowisku.


From rs-squid at lists.microscopium.de  Wed Oct 16 14:38:47 2019
From: rs-squid at lists.microscopium.de (Robert)
Date: Wed, 16 Oct 2019 16:38:47 +0200
Subject: [squid-users] How to use "cache",
	"store_miss" and "send_hit" directives?
Message-ID: <99ed2b95f3cb0e5dd2bb188708637eaf7486ab64.camel@lists.microscopium.de>

Hi there,

after upgrading to 4.6 from 3.x, I am completely confused about how to
use the cache, store_miss and send_hit directives correctly.

I am using ACLs for different handling of clients connecting to
different local ports:

  acl proxy-basic localip 172.16.2.243
  acl proxy-standard localip 172.16.3.243

These ACLs are used to determine outgoing address, which are routed to
different outgoing interfaces like this:

  tcp_outgoing_address 172.16.3.244 proxy-basic
  tcp_outgoing_address 172.16.4.244 proxy-standard

This works as desired.

Now I am struggling with caching objects. The goal is, to have objects
requested by proxy-basic clients not to be cached, but objects
requested by proxy-standard to be cached normally.

Tried this:

  cache deny proxy-basic
  cache allow all

And this:

  cache allow proxy-standard
  cache deny all

And many other different configs. Does not work as desired.

The result is always the same, either all objects are cached, or no
objects at all are cached.

Even if I try a simple configuration like this, objects are never
cached:

  cache allow proxy-standard

If I use ANY "cache ___" directive other than a (useless) "cache allow
all", caching is completely disabled for all ACLs.

What am I doing wrong? Are those ACLs unsuitable for configuring
caching with cache directives?

I haven't found any examples in the net about how to do achieve what I
want to do. As far as I remember, this used to work with the old 3.x
setup (this is gone, cannot test this any more).

Thanks for help.

Robert


-- 
Robert Senger




From nicolas.doigny at gmail.com  Wed Oct 16 15:18:47 2019
From: nicolas.doigny at gmail.com (ndoigny)
Date: Wed, 16 Oct 2019 10:18:47 -0500 (CDT)
Subject: [squid-users] Problem webpagge filter - acl & http_access
In-Reply-To: <1571162629103-0.post@n4.nabble.com>
References: <1571162629103-0.post@n4.nabble.com>
Message-ID: <1571239127583-0.post@n4.nabble.com>

Hi,

I tested with an acl .eurosport.com* and garagegardi.be* who aren't https
and it's working.

I see that for https pages, I need a root certificate that will be exported
to all machines.

Thank you for you help.

Nicolas



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Oct 16 15:38:28 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 16 Oct 2019 11:38:28 -0400
Subject: [squid-users] How to use "cache",
 "store_miss" and "send_hit" directives?
In-Reply-To: <99ed2b95f3cb0e5dd2bb188708637eaf7486ab64.camel@lists.microscopium.de>
References: <99ed2b95f3cb0e5dd2bb188708637eaf7486ab64.camel@lists.microscopium.de>
Message-ID: <1d2c576f-d2a2-cb07-31ca-a3b3f55ea64f@measurement-factory.com>

On 10/16/19 10:38 AM, Robert wrote:

> after upgrading to 4.6 from 3.x
> I am struggling with caching objects. The goal is, to have objects
> requested by proxy-basic clients not to be cached, but objects
> requested by proxy-standard to be cached normally.
> 
> Tried this:
> 
>   cache deny proxy-basic
>   cache allow all
> 
> And this:
> 
>   cache allow proxy-standard
>   cache deny all

Based on your description, you probably want the former or its simpler
version:

    cache deny proxy-basic


> If I use ANY "cache ___" directive other than a (useless) "cache allow
> all", caching is completely disabled for all ACLs.

FYI: Squid does not (yet) treat the "all" ACL specially -- Squid does
not ignore or automatically apply seemingly "useless" rules with it. If
you are getting correct results with "allow all" and incorrect results
with "allow foo", then your foo ACL does not match (in that specific
context). Why it does not match is a separate question.


> What am I doing wrong?

Nothing that warrants discussing here IMO. I suggest trying the latest
v4 release and, if the problem is still there, filing a bug report. If
you can share a compressed ALL,7+ cache.log while reproducing the
problem with a single transaction, we may be able to triage this problem
faster. Squid wiki has instructions at
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction


HTH,

Alex.

> I am using ACLs for different handling of clients connecting to
> different local ports:
> 
>   acl proxy-basic localip 172.16.2.243
>   acl proxy-standard localip 172.16.3.243
> 
> These ACLs are used to determine outgoing address, which are routed to
> different outgoing interfaces like this:
> 
>   tcp_outgoing_address 172.16.3.244 proxy-basic
>   tcp_outgoing_address 172.16.4.244 proxy-standard
> 
> This works as desired.



From joaolopes at gmx.com  Wed Oct 16 17:12:46 2019
From: joaolopes at gmx.com (jl)
Date: Wed, 16 Oct 2019 12:12:46 -0500 (CDT)
Subject: [squid-users] Overwrite an URL containing an IP when it is
 requested with a custom Host header
Message-ID: <1571245966452-0.post@n4.nabble.com>

Hi,

It's possible to configure Squid to overwrite an URL containing an IP when
it is requested with a custom Host header passed by the client when the Host
header resolves to the IP in the URL?

For example for this:
curl -v -k -x IP:PORT http://34.201.191.134/headers -H "Host: httpbin.org"

to return:

"headers": {
  "Accept": "*/*", 
  "Host": "httpbin.org", 
  "User-Agent": "curl/7.58.0"
}

instead of:

"headers": {
  "Accept": "*/*", 
  "Host": "34.201.191.134", 
  "If-Modified-Since": "Wed, 16 Oct 2019 16:08:42 GMT", 
  "User-Agent": "curl/7.58.0"
}

Or for this:
curl -v -k -x IP:PORT http://192.121.151.106/doc/search/ -H "Host:
erlang.org"

to return "HTTP/1.1 200 OK" instead of "HTTP/1.1 404 Not Found"

Thanks.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Oct 16 18:20:30 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 16 Oct 2019 14:20:30 -0400
Subject: [squid-users] Overwrite an URL containing an IP when it is
 requested with a custom Host header
In-Reply-To: <1571245966452-0.post@n4.nabble.com>
References: <1571245966452-0.post@n4.nabble.com>
Message-ID: <eb973867-d292-e41c-5de4-203c69b6a3e0@measurement-factory.com>

On 10/16/19 1:12 PM, jl wrote:

> It's possible to configure Squid to overwrite an URL containing an IP when
> it is requested with a custom Host header passed by the client when the Host
> header resolves to the IP in the URL?

You can probably accomplish the above using a URL rewriting helper or an
adaptation service. Those things can receive requested headers, do DNS
lookups, and rewrite URLs as needed.

If the IP and host values can be hard-coded into Squid configuration,
then it might be possible to accomplish what you want using Squid
configuration alone (e.g., via a dedicated cache_peer originserver
setting), but I am not sure.

Alex.


> For example for this:
> curl -v -k -x IP:PORT http://34.201.191.134/headers -H "Host: httpbin.org"
> 
> to return:
> 
> "headers": {
>   "Accept": "*/*", 
>   "Host": "httpbin.org", 
>   "User-Agent": "curl/7.58.0"
> }
> 
> instead of:
> 
> "headers": {
>   "Accept": "*/*", 
>   "Host": "34.201.191.134", 
>   "If-Modified-Since": "Wed, 16 Oct 2019 16:08:42 GMT", 
>   "User-Agent": "curl/7.58.0"
> }
> 
> Or for this:
> curl -v -k -x IP:PORT http://192.121.151.106/doc/search/ -H "Host:
> erlang.org"
> 
> to return "HTTP/1.1 200 OK" instead of "HTTP/1.1 404 Not Found"


From rs-squid at lists.microscopium.de  Wed Oct 16 23:17:56 2019
From: rs-squid at lists.microscopium.de (Robert Senger)
Date: Thu, 17 Oct 2019 01:17:56 +0200
Subject: [squid-users] How to use "cache",
 "store_miss" and "send_hit" directives?
In-Reply-To: <1d2c576f-d2a2-cb07-31ca-a3b3f55ea64f@measurement-factory.com>
References: <99ed2b95f3cb0e5dd2bb188708637eaf7486ab64.camel@lists.microscopium.de>
 <1d2c576f-d2a2-cb07-31ca-a3b3f55ea64f@measurement-factory.com>
Message-ID: <9eb09a6373cbb26e70a6437737129dfc80525c84.camel@lists.microscopium.de>

Hi Alex,

you're right, the correct way is to use "cache deny <aclname>" only,
all others are allowed then.

I tried this before, but it did not work, because of my very special
setup. 

I need to encrypt browser->squid connection (on mobile devices). With
squid 3.x, I used stunnel client on the mobile device and stunnel
server on squid's machine. With squid 4.6, I wanted to get rid of
stunnel server and use squid's https_port directive instead, but
https_port + sslbump did not go together. So, I created a loop that
forwarded https_port connections with a cache_peer directive to squid's
own http_port. That worked, except for caching... The http_port ACLs
never matched in the cache directive, instead, the https_port ACLs did,
but that is not what I want and need. Some coincidence made that
tcp_outgoing_address matched and routing was correct, anyway.

I switched back to the old stunnel server setup, and things are fine
now. But I still don't know why the http_port connections ACLs do not
match...

Regards,

Robert

 
Am Mittwoch, den 16.10.2019, 11:38 -0400 schrieb Alex Rousskov:
> On 10/16/19 10:38 AM, Robert wrote:
> 
> > after upgrading to 4.6 from 3.x
> > I am struggling with caching objects. The goal is, to have objects
> > requested by proxy-basic clients not to be cached, but objects
> > requested by proxy-standard to be cached normally.
> > 
> > Tried this:
> > 
> >   cache deny proxy-basic
> >   cache allow all
> > 
> > And this:
> > 
> >   cache allow proxy-standard
> >   cache deny all
> 
> Based on your description, you probably want the former or its
> simpler
> version:
> 
>     cache deny proxy-basic
> 
> 
> > If I use ANY "cache ___" directive other than a (useless) "cache
> > allow
> > all", caching is completely disabled for all ACLs.
> 
> FYI: Squid does not (yet) treat the "all" ACL specially -- Squid does
> not ignore or automatically apply seemingly "useless" rules with it.
> If
> you are getting correct results with "allow all" and incorrect
> results
> with "allow foo", then your foo ACL does not match (in that specific
> context). Why it does not match is a separate question.
> 
> 
> > What am I doing wrong?
> 
> Nothing that warrants discussing here IMO. I suggest trying the
> latest
> v4 release and, if the problem is still there, filing a bug report.
> If
> you can share a compressed ALL,7+ cache.log while reproducing the
> problem with a single transaction, we may be able to triage this
> problem
> faster. Squid wiki has instructions at
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
> 
> 
> HTH,
> 
> Alex.
> 
> > I am using ACLs for different handling of clients connecting to
> > different local ports:
> > 
> >   acl proxy-basic localip 172.16.2.243
> >   acl proxy-standard localip 172.16.3.243
> > 
> > These ACLs are used to determine outgoing address, which are routed
> > to
> > different outgoing interfaces like this:
> > 
> >   tcp_outgoing_address 172.16.3.244 proxy-basic
> >   tcp_outgoing_address 172.16.4.244 proxy-standard
> > 
> > This works as desired.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-- 
Robert Senger <robert.senger at familie-senger.net>
PGP/GPG Public Key ID: A51A4BCD



From rousskov at measurement-factory.com  Thu Oct 17 02:18:17 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 16 Oct 2019 22:18:17 -0400
Subject: [squid-users] How to use "cache",
 "store_miss" and "send_hit" directives?
In-Reply-To: <9eb09a6373cbb26e70a6437737129dfc80525c84.camel@lists.microscopium.de>
References: <99ed2b95f3cb0e5dd2bb188708637eaf7486ab64.camel@lists.microscopium.de>
 <1d2c576f-d2a2-cb07-31ca-a3b3f55ea64f@measurement-factory.com>
 <9eb09a6373cbb26e70a6437737129dfc80525c84.camel@lists.microscopium.de>
Message-ID: <436e5a01-50f8-500b-5e31-0d1dff1168fb@measurement-factory.com>

On 10/16/19 7:17 PM, Robert Senger wrote:

> I need to encrypt browser->squid connection (on mobile devices). With
> squid 3.x, I used stunnel client on the mobile device and stunnel
> server on squid's machine. With squid 4.6, I wanted to get rid of
> stunnel server and use squid's https_port directive instead, but
> https_port + sslbump did not go together. So, I created a loop that
> forwarded https_port connections with a cache_peer directive to squid's
> own http_port. 

IIRC, this trick also creates problems for built-in cache_peer checks
that may fail because those checks start before Squid starts listening
on its own ports. This problem may be specific to SMP setups. YMMV.


> That worked, except for caching... The http_port ACLs
> never matched in the cache directive, instead, the https_port ACLs did,
> but that is not what I want and need. Some coincidence made that
> tcp_outgoing_address matched and routing was correct, anyway.


AFAICT, bugs notwithstanding, those ACLs should have matched in the
"cache" directive context, especially if they actually matched in the
tcp_outgoing_address context later.

Alex.



> Am Mittwoch, den 16.10.2019, 11:38 -0400 schrieb Alex Rousskov:
>> On 10/16/19 10:38 AM, Robert wrote:
>>
>>> after upgrading to 4.6 from 3.x
>>> I am struggling with caching objects. The goal is, to have objects
>>> requested by proxy-basic clients not to be cached, but objects
>>> requested by proxy-standard to be cached normally.
>>>
>>> Tried this:
>>>
>>>   cache deny proxy-basic
>>>   cache allow all
>>>
>>> And this:
>>>
>>>   cache allow proxy-standard
>>>   cache deny all
>>
>> Based on your description, you probably want the former or its
>> simpler
>> version:
>>
>>     cache deny proxy-basic
>>
>>
>>> If I use ANY "cache ___" directive other than a (useless) "cache
>>> allow
>>> all", caching is completely disabled for all ACLs.
>>
>> FYI: Squid does not (yet) treat the "all" ACL specially -- Squid does
>> not ignore or automatically apply seemingly "useless" rules with it.
>> If
>> you are getting correct results with "allow all" and incorrect
>> results
>> with "allow foo", then your foo ACL does not match (in that specific
>> context). Why it does not match is a separate question.
>>
>>
>>> What am I doing wrong?
>>
>> Nothing that warrants discussing here IMO. I suggest trying the
>> latest
>> v4 release and, if the problem is still there, filing a bug report.
>> If
>> you can share a compressed ALL,7+ cache.log while reproducing the
>> problem with a single transaction, we may be able to triage this
>> problem
>> faster. Squid wiki has instructions at
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>>
>>
>> HTH,
>>
>> Alex.
>>
>>> I am using ACLs for different handling of clients connecting to
>>> different local ports:
>>>
>>>   acl proxy-basic localip 172.16.2.243
>>>   acl proxy-standard localip 172.16.3.243
>>>
>>> These ACLs are used to determine outgoing address, which are routed
>>> to
>>> different outgoing interfaces like this:
>>>
>>>   tcp_outgoing_address 172.16.3.244 proxy-basic
>>>   tcp_outgoing_address 172.16.4.244 proxy-standard
>>>
>>> This works as desired.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Oct 17 08:31:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Oct 2019 21:31:18 +1300
Subject: [squid-users] Overwrite an URL containing an IP when it is
 requested with a custom Host header
In-Reply-To: <eb973867-d292-e41c-5de4-203c69b6a3e0@measurement-factory.com>
References: <1571245966452-0.post@n4.nabble.com>
 <eb973867-d292-e41c-5de4-203c69b6a3e0@measurement-factory.com>
Message-ID: <a538265e-8911-a442-3f64-8baa56a3fda6@treenet.co.nz>

On 17/10/19 7:20 am, Alex Rousskov wrote:
> On 10/16/19 1:12 PM, jl wrote:
> 
>> It's possible to configure Squid to overwrite an URL containing an IP when
>> it is requested with a custom Host header passed by the client when the Host
>> header resolves to the IP in the URL?
> 
> You can probably accomplish the above using a URL rewriting helper or an
> adaptation service. Those things can receive requested headers, do DNS
> lookups, and rewrite URLs as needed.
> 
> If the IP and host values can be hard-coded into Squid configuration,
> then it might be possible to accomplish what you want using Squid
> configuration alone (e.g., via a dedicated cache_peer originserver
> setting), but I am not sure.
> 
> Alex.
> 
> 
>> For example for this:
>> curl -v -k -x IP:PORT http://34.201.191.134/headers -H "Host: httpbin.org"
>>
>> to return:
>>
>> "headers": {
>>   "Accept": "*/*", 
>>   "Host": "httpbin.org", 
>>   "User-Agent": "curl/7.58.0"
>> }
>>
>> instead of:
>>
>> "headers": {
>>   "Accept": "*/*", 
>>   "Host": "34.201.191.134", 
>>   "If-Modified-Since": "Wed, 16 Oct 2019 16:08:42 GMT", 
>>   "User-Agent": "curl/7.58.0"
>> }

Please be aware that a client sending that combination of absolute-URL
and mismatching Host header is one of three things:

 1) a malware attack

 2) broken client software

 3) a proxy attempting to avoid producing errors while still protecting
against the above. eg interception proxy receiving suspected
CVE-2009-0801 attack traffic.


>>
>> Or for this:
>> curl -v -k -x IP:PORT http://192.121.151.106/doc/search/ -H "Host:
>> erlang.org"
>>
>> to return "HTTP/1.1 200 OK" instead of "HTTP/1.1 404 Not Found"

That one is not a good idea. The origin server is producing that 404,
nothing to do with Squid.


Amos


From nandha25vcet at gmail.com  Thu Oct 17 10:07:17 2019
From: nandha25vcet at gmail.com (Nandha Kumar)
Date: Thu, 17 Oct 2019 15:37:17 +0530
Subject: [squid-users] Fwd: 407 error
In-Reply-To: <CAAyKoxJK6A8xSXEt9gPB7-sss6J=UTzQwB7QwGPK9KMFmYcSCA@mail.gmail.com>
References: <CAAyKoxJK6A8xSXEt9gPB7-sss6J=UTzQwB7QwGPK9KMFmYcSCA@mail.gmail.com>
Message-ID: <CAAyKoxLBQc+wLf3vixAi-vv_zNQ_YSXk34icDhfrX7-=g19hnw@mail.gmail.com>

Hi Team,

WE are using the squid proxy on Ubuntu.
WE are facing a strange error when try  to community through proxy.
I am able to access the proxy in the server browser suing the username and
password.  When I try to push/enable an module throw the proxy we are
getting the error message

1568886656.016      0 192.168.211.54 TCP_DENIED/407 4099 POST
http://172.16.255.6/ - HIER_NONE/- text/html

1568886656.022      0 192.168.211.54 TCP_DENIED/407 4235 POST
http://172.16.255.6/ cache HIER_NONE/- text/html

1568886656.027      0 192.168.211.54 TCP_DENIED/407 4235 POST
http://172.16.255.6/ cache HIER_NONE/- text/html


Without proxy I am able to push the module.


Regards

Nandha
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191017/789800b7/attachment.htm>

From joaolopes at gmx.com  Thu Oct 17 10:33:39 2019
From: joaolopes at gmx.com (jl)
Date: Thu, 17 Oct 2019 05:33:39 -0500 (CDT)
Subject: [squid-users] Overwrite an URL containing an IP when it is
 requested with a custom Host header
In-Reply-To: <a538265e-8911-a442-3f64-8baa56a3fda6@treenet.co.nz>
References: <1571245966452-0.post@n4.nabble.com>
 <eb973867-d292-e41c-5de4-203c69b6a3e0@measurement-factory.com>
 <a538265e-8911-a442-3f64-8baa56a3fda6@treenet.co.nz>
Message-ID: <1571308419878-0.post@n4.nabble.com>

Thanks both for your replies.

>>> Or for this: 
>>> curl -v -k -x IP:PORT http://192.121.151.106/doc/search/ -H "Host: 
>>> erlang.org" 
>>> 
>>> to return "HTTP/1.1 200 OK" instead of "HTTP/1.1 404 Not Found" >

>That one is not a good idea. The origin server is producing that 404, 
>nothing to do with Squid. 

But in this case the Host header resolves to the IP in the URL and if we
simply do `curl -v -k -x IP:PORT http://erlang.org/doc/search/` it returns a
"HTTP/1.1 200 OK". Shouldn't be possible for Squid to use the Host header
instead of the IP in such cases and not rewriting the Host header with the
IP? Or such behavior would go against the RFC 7230 (HTTP/1.1):

   When a proxy receives a request with an absolute-form of
   request-target, the proxy MUST ignore the received Host header field
   (if any) and instead replace it with the host information of the
   request-target.  A proxy that forwards such a request MUST generate a
   new Host field-value based on the received request-target rather than
   forward the received Host field-value

?

Thanks.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Oct 17 10:55:23 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 Oct 2019 23:55:23 +1300
Subject: [squid-users] Overwrite an URL containing an IP when it is
 requested with a custom Host header
In-Reply-To: <1571308419878-0.post@n4.nabble.com>
References: <1571245966452-0.post@n4.nabble.com>
 <eb973867-d292-e41c-5de4-203c69b6a3e0@measurement-factory.com>
 <a538265e-8911-a442-3f64-8baa56a3fda6@treenet.co.nz>
 <1571308419878-0.post@n4.nabble.com>
Message-ID: <c8ecb44a-a020-0177-3317-34d463a41e2a@treenet.co.nz>

On 17/10/19 11:33 pm, jl wrote:
> Thanks both for your replies.
> 
>>>> Or for this: 
>>>> curl -v -k -x IP:PORT http://192.121.151.106/doc/search/ -H "Host: 
>>>> erlang.org" 
>>>>
>>>> to return "HTTP/1.1 200 OK" instead of "HTTP/1.1 404 Not Found" >
> 
>> That one is not a good idea. The origin server is producing that 404, 
>> nothing to do with Squid. 
> 
> But in this case the Host header resolves to the IP in the URL and if we
> simply do `curl -v -k -x IP:PORT http://erlang.org/doc/search/` it returns a
> "HTTP/1.1 200 OK". Shouldn't be possible for Squid to use the Host header
> instead of the IP in such cases and not rewriting the Host header with the
> IP? Or such behavior would go against the RFC 7230 (HTTP/1.1):
> 
>    When a proxy receives a request with an absolute-form of
>    request-target, the proxy MUST ignore the received Host header field
>    (if any) and instead replace it with the host information of the
>    request-target.  A proxy that forwards such a request MUST generate a
>    new Host field-value based on the received request-target rather than
>    forward the received Host field-value
> 
> ?

It leads to issues like this one:
 <http://www.squid-cache.org/Advisories/SQUID-2011_1.txt>
(but in a way that does not require interception to trigger.)

side-effects of those type of vulnerability are cache injection, network
hijacking, cross-site scripting, the cited same-origin bypass, and the
source of the problems being granted anonymity.

Amos


From rs-squid at lists.microscopium.de  Thu Oct 17 12:53:42 2019
From: rs-squid at lists.microscopium.de (Robert)
Date: Thu, 17 Oct 2019 14:53:42 +0200
Subject: [squid-users] How to use "cache",
 "store_miss" and "send_hit" directives?
In-Reply-To: <436e5a01-50f8-500b-5e31-0d1dff1168fb@measurement-factory.com>
References: <99ed2b95f3cb0e5dd2bb188708637eaf7486ab64.camel@lists.microscopium.de>
 <1d2c576f-d2a2-cb07-31ca-a3b3f55ea64f@measurement-factory.com>
 <9eb09a6373cbb26e70a6437737129dfc80525c84.camel@lists.microscopium.de>
 <436e5a01-50f8-500b-5e31-0d1dff1168fb@measurement-factory.com>
Message-ID: <19a46b4987f3a332160aa74024f135d5383ef066.camel@lists.microscopium.de>

Am Mittwoch, den 16.10.2019, 22:18 -0400 schrieb Alex Rousskov:
> On 10/16/19 7:17 PM, Robert Senger wrote:
> 
> > I need to encrypt browser->squid connection (on mobile devices).
> > With
> > squid 3.x, I used stunnel client on the mobile device and stunnel
> > server on squid's machine. With squid 4.6, I wanted to get rid of
> > stunnel server and use squid's https_port directive instead, but
> > https_port + sslbump did not go together. So, I created a loop that
> > forwarded https_port connections with a cache_peer directive to
> > squid's
> > own http_port. 
> 
> IIRC, this trick also creates problems for built-in cache_peer checks
> that may fail because those checks start before Squid starts
> listening
> on its own ports. This problem may be specific to SMP setups. YMMV.
> 
Well, worked for me ;)

> > That worked, except for caching... The http_port ACLs
> > never matched in the cache directive, instead, the https_port ACLs
> > did,
> > but that is not what I want and need. Some coincidence made that
> > tcp_outgoing_address matched and routing was correct, anyway.
> 
> 
> AFAICT, bugs notwithstanding, those ACLs should have matched in the
> "cache" directive context, especially if they actually matched in the
> tcp_outgoing_address context later.
> 
> Alex.

I am not sure if they matched at all. As I said, by chance, default
rules for tcp_outgoing_address and policy based routing might have
produced right results (at least at where I looked at, there are more
than 2 client ACLs), but based on wrong decisions. Can't check this
right now.

Anyway, I am thinking about running multiple squid instances with
simple setups and chain them rather than just one with a very complex
setup, maybe that would make things easier. It also would make it
possible to query different nameservers (or bind9 views) for different
ACLs, which is not possible within one single instance
(udp_outgoing_address does not take ACLs).

Thanks for the help,

Robert



> 
> > Am Mittwoch, den 16.10.2019, 11:38 -0400 schrieb Alex Rousskov:
> > > On 10/16/19 10:38 AM, Robert wrote:
> > > 
> > > > after upgrading to 4.6 from 3.x
> > > > I am struggling with caching objects. The goal is, to have
> > > > objects
> > > > requested by proxy-basic clients not to be cached, but objects
> > > > requested by proxy-standard to be cached normally.
> > > > 
> > > > Tried this:
> > > > 
> > > >   cache deny proxy-basic
> > > >   cache allow all
> > > > 
> > > > And this:
> > > > 
> > > >   cache allow proxy-standard
> > > >   cache deny all
> > > 
> > > Based on your description, you probably want the former or its
> > > simpler
> > > version:
> > > 
> > >     cache deny proxy-basic
> > > 
> > > 
> > > > If I use ANY "cache ___" directive other than a (useless)
> > > > "cache
> > > > allow
> > > > all", caching is completely disabled for all ACLs.
> > > 
> > > FYI: Squid does not (yet) treat the "all" ACL specially -- Squid
> > > does
> > > not ignore or automatically apply seemingly "useless" rules with
> > > it.
> > > If
> > > you are getting correct results with "allow all" and incorrect
> > > results
> > > with "allow foo", then your foo ACL does not match (in that
> > > specific
> > > context). Why it does not match is a separate question.
> > > 
> > > 
> > > > What am I doing wrong?
> > > 
> > > Nothing that warrants discussing here IMO. I suggest trying the
> > > latest
> > > v4 release and, if the problem is still there, filing a bug
> > > report.
> > > If
> > > you can share a compressed ALL,7+ cache.log while reproducing the
> > > problem with a single transaction, we may be able to triage this
> > > problem
> > > faster. Squid wiki has instructions at
> > > 
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
> > > 
> > > 
> > > HTH,
> > > 
> > > Alex.
> > > 
> > > > I am using ACLs for different handling of clients connecting to
> > > > different local ports:
> > > > 
> > > >   acl proxy-basic localip 172.16.2.243
> > > >   acl proxy-standard localip 172.16.3.243
> > > > 
> > > > These ACLs are used to determine outgoing address, which are
> > > > routed
> > > > to
> > > > different outgoing interfaces like this:
> > > > 
> > > >   tcp_outgoing_address 172.16.3.244 proxy-basic
> > > >   tcp_outgoing_address 172.16.4.244 proxy-standard
> > > > 
> > > > This works as desired.
> > > 
> > > _______________________________________________
> > > squid-users mailing list
> > > squid-users at lists.squid-cache.org
> > > http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-- 
Robert Senger




From trapexit at spawn.link  Thu Oct 17 16:28:23 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Thu, 17 Oct 2019 12:28:23 -0400
Subject: [squid-users] SIGBUS attempting to use rock
Message-ID: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>

I'm sending this out for those that might run into this in the future.

After a lot of tinkering and turning on full debug I realized the reason 
rock was failing for me in my container was due to the small default SHM 
size allocated by Docker. Increasing the SHM size with `--shm-size` 
fixed the issue.

It'd be significantly more helpful if the Squid reported precisely what 
the issue and exited gracefully rather than SIGBUS'ing.



From rousskov at measurement-factory.com  Thu Oct 17 18:53:54 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Oct 2019 14:53:54 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
Message-ID: <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>

On 10/17/19 12:28 PM, Antonio SJ Musumeci wrote:

> After a lot of tinkering and turning on full debug I realized the reason
> rock was failing for me in my container was due to the small default SHM
> size allocated by Docker. Increasing the SHM size with `--shm-size`
> fixed the issue.
> 
> It'd be significantly more helpful if the Squid reported precisely what
> the issue and exited gracefully rather than SIGBUS'ing.

Does enabling shared_memory_locking in squid.conf trigger the behavior
you want?

If yes, then you may be wondering why this is not the default setting.
The answer is in the following two squid-dev messages. Message [1] is a
high-level description. Message [2] contains more low-level details and
examples of ~60 second startup delays that locking may cause.

[1] http://lists.squid-cache.org/pipermail/squid-dev/2016-March/005260.html

[2]
http://lists.squid-cache.org/pipermail/squid-dev/2015-December/004112.html

Enhancing shared memory management (e.g., implementing ideas mentioned
in the "As we gain more experience" paragraph of [1]) is welcomed.


HTH,

Alex.


From rs-squid at lists.microscopium.de  Thu Oct 17 20:52:14 2019
From: rs-squid at lists.microscopium.de (Robert)
Date: Thu, 17 Oct 2019 22:52:14 +0200
Subject: [squid-users] SSL negotiation errors on https_port
Message-ID: <bd894c8473925a91e05edd8eb11617eee8d72e3b.camel@lists.microscopium.de>

Hi there,

I have configured squid's https_port for client certificate
authorization:

https_port [2001:XXX:XX:XXX::2]:8008 cert=/etc/ssl/private/mydomain_de/mydomain_de.crt key=/etc/ssl/private/mydomain_de/mydomain_de.key clientca=/etc/squid/ssl-proxy/ca.crt tls-dh=/etc/squid/ssl/dh_2048.pem

This works as expected. Clients connect via client side stunnel4 using
their individual client certificates.

However, I see many lines like these in the cache.log file:

2019/10/17 22:38:33.552 kid1| Error negotiating SSL connection on FD 44: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:41.619 kid1| Error negotiating SSL connection on FD 37: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:42.174 kid1| Error negotiating SSL connection on FD 40: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:42.312 kid1| Error negotiating SSL connection on FD 42: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:42.507 kid1| Error negotiating SSL connection on FD 44: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:46.755 kid1| Error negotiating SSL connection on FD 48: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:46.763 kid1| Error negotiating SSL connection on FD 58: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:46.771 kid1| Error negotiating SSL connection on FD 48: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:50.306 kid1| Error negotiating SSL connection on FD 77: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:50.314 kid1| Error negotiating SSL connection on FD 80: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:38:50.324 kid1| Error negotiating SSL connection on FD 77: error:00000001:lib(0):func(0):reason(1) (1/-1)
2019/10/17 22:40:01.898 kid1| Error negotiating SSL connection on FD 13: error:00000001:lib(0):func(0):reason(1) (1/-1)

Increasing debug output tells me that SSL negotiation fails and then
succeeds, but I have no idea what causes these failures. Is it just
related to the ssl handshake and not to worry about? If so, why is that
reported to the logs? Setting min and max TLS version on the client
does not change the log outpu. TLS version used is 1.3 if allowed on
the client.

Thanks for clarification,

Robert 


-- 
Robert Senger




From trapexit at spawn.link  Thu Oct 17 21:07:58 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Thu, 17 Oct 2019 17:07:58 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
 <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
Message-ID: <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>

I'm currently unable to test fully due to my environment lacking 
CAP_IPC_LOCK. It does fail more gracefully saying it can't use mlock at all.

That said... the option is listed in relation to SMP and not referenced 
by rock docs. Also it should be possible to check /dev/shm's free space 
and compare that against the file size needed. Clearly it knows based on 
the requested storage and slot sizes. Even a guess with a warning rather 
than leaving it to fail otherwise silently with a SIGBUS.

On 10/17/2019 2:53 PM, Alex Rousskov wrote:
> On 10/17/19 12:28 PM, Antonio SJ Musumeci wrote:
>
>> After a lot of tinkering and turning on full debug I realized the reason
>> rock was failing for me in my container was due to the small default SHM
>> size allocated by Docker. Increasing the SHM size with `--shm-size`
>> fixed the issue.
>>
>> It'd be significantly more helpful if the Squid reported precisely what
>> the issue and exited gracefully rather than SIGBUS'ing.
> Does enabling shared_memory_locking in squid.conf trigger the behavior
> you want?
>
> If yes, then you may be wondering why this is not the default setting.
> The answer is in the following two squid-dev messages. Message [1] is a
> high-level description. Message [2] contains more low-level details and
> examples of ~60 second startup delays that locking may cause.
>
> [1] http://lists.squid-cache.org/pipermail/squid-dev/2016-March/005260.html
>
> [2]
> http://lists.squid-cache.org/pipermail/squid-dev/2015-December/004112.html
>
> Enhancing shared memory management (e.g., implementing ideas mentioned
> in the "As we gain more experience" paragraph of [1]) is welcomed.
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Thu Oct 17 21:32:50 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Oct 2019 17:32:50 -0400
Subject: [squid-users] SSL negotiation errors on https_port
In-Reply-To: <bd894c8473925a91e05edd8eb11617eee8d72e3b.camel@lists.microscopium.de>
References: <bd894c8473925a91e05edd8eb11617eee8d72e3b.camel@lists.microscopium.de>
Message-ID: <adbcba3f-b244-5939-5e0e-414dff9e7f3e@measurement-factory.com>

On 10/17/19 4:52 PM, Robert wrote:

> I see many lines like these in the cache.log file:

> 2019/10/17 22:38:33.552 kid1| Error negotiating SSL connection on FD 44: error:00000001:lib(0):func(0):reason(1) (1/-1)

OpenSSL refused to accept a TLS client connection with a generic
SSL_ERROR_SSL:

    A non-recoverable, fatal error in the SSL library occurred, usually
    a protocol error.  The OpenSSL error queue contains more information
    on the error.

AFAICT, Squid does not have the code to examine OpenSSL error queue
beyond the surface level (shown in your cache.log), but that does not
mean there is actually more information in that queue in your particular
case. Said that, quality pull requests (or sponsorship for) adding deep
error queue inspection support are welcomed.

I trust there are no other potentially relevant ERRORs and WARNINGs in
your cache.log.

It sounds like you can trivially reproduce the problem. If so, a capable
developer with access to your box should be able to triage it further.


> Is it just related to the ssl handshake and not to worry about?

It is most likely related to the SSL handshake. FWIW, I would worry
about it because it should not be happening under normal conditions.


HTH,

Alex.


From rousskov at measurement-factory.com  Thu Oct 17 21:47:50 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Oct 2019 17:47:50 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
 <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
 <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>
Message-ID: <b03b57ee-02ce-06f5-276f-8cbeb8d58ba1@measurement-factory.com>

On 10/17/19 5:07 PM, Antonio SJ Musumeci wrote:

> the option is listed in relation to SMP and not referenced by rock docs.

The second paragraph in cache_dir rock directive documentation implies
SMP -- it talks about various processes that rock uses to avoid locking
(if it can). SMP usage is the primary rock scalability feature. There is
more info at https://wiki.squid-cache.org/Features/RockStore (and
https://wiki.squid-cache.org/Features/LargeRockStore).


> Also it should be possible to check /dev/shm's free space
> and compare that against the file size needed. Clearly it knows based on
> the requested storage and slot sizes. Even a guess with a warning rather
> than leaving it to fail otherwise silently with a SIGBUS.

Unfortunately, a simple implementation may produce a lot of false
warnings in some environments while a quality implementation may not be
as easy as you think: Accessing free space info may require special
permissions and correctly accounting for the existing shared memory
segments in that partition would be tricky (they can be leftovers from
the previous Squid run that will be overwritten or something completely
unrelated to Squid). Even finding the right partition name in a portable
way may be tricky!

IMHO, the future development directions outlined when adding
shared_memory_locking are more promising in general, but I would be
happy to learn that there are even better options.


Cheers,

Alex.


> On 10/17/2019 2:53 PM, Alex Rousskov wrote:
>> On 10/17/19 12:28 PM, Antonio SJ Musumeci wrote:
>>
>>> After a lot of tinkering and turning on full debug I realized the reason
>>> rock was failing for me in my container was due to the small default SHM
>>> size allocated by Docker. Increasing the SHM size with `--shm-size`
>>> fixed the issue.
>>>
>>> It'd be significantly more helpful if the Squid reported precisely what
>>> the issue and exited gracefully rather than SIGBUS'ing.

>> Does enabling shared_memory_locking in squid.conf trigger the behavior
>> you want?
>>
>> If yes, then you may be wondering why this is not the default setting.
>> The answer is in the following two squid-dev messages. Message [1] is a
>> high-level description. Message [2] contains more low-level details and
>> examples of ~60 second startup delays that locking may cause.
>>
>> [1]
>> http://lists.squid-cache.org/pipermail/squid-dev/2016-March/005260.html
>>
>> [2]
>> http://lists.squid-cache.org/pipermail/squid-dev/2015-December/004112.html
>>
>>
>> Enhancing shared memory management (e.g., implementing ideas mentioned
>> in the "As we gain more experience" paragraph of [1]) is welcomed.



From trapexit at spawn.link  Thu Oct 17 23:13:02 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Thu, 17 Oct 2019 19:13:02 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <b03b57ee-02ce-06f5-276f-8cbeb8d58ba1@measurement-factory.com>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
 <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
 <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>
 <b03b57ee-02ce-06f5-276f-8cbeb8d58ba1@measurement-factory.com>
Message-ID: <20f13601-400f-2983-2ca1-afbb71678e12@spawn.link>

On 10/17/2019 5:47 PM, Alex Rousskov wrote:
> Unfortunately, a simple implementation may produce a lot of false
> warnings in some environments while a quality implementation may not be
> as easy as you think: Accessing free space info may require special
> permissions and correctly accounting for the existing shared memory
> segments in that partition would be tricky (they can be leftovers from
> the previous Squid run that will be overwritten or something completely
> unrelated to Squid). Even finding the right partition name in a portable
> way may be tricky!
>
> IMHO, the future development directions outlined when adding
> shared_memory_locking are more promising in general, but I would be
> happy to learn that there are even better options.

Clearly Squid is aware of the path where these temp files are being 
created and can simply statvfs the base path can it not? If it's 
creating new files in /dev/shm it can statvfs and compare the available 
space to what it intends to create and in the least warn.



From vieridipaola at gmail.com  Fri Oct 18 12:21:47 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Fri, 18 Oct 2019 14:21:47 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
Message-ID: <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>

On Fri, Oct 11, 2019 at 3:50 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> Note that this last entry is about a connection to port 443, whereas the
> rest of the log is all about traffic to port 80.
> >
> > The Squid machine has no issues if I browse the web from command line,
> > eg. 'links http://www.linuxheadquarters.com' works fine.
> >
> > What should I be looking for?
>
> TCP/IP level packet routing. Squid is trying to open a TCP connection to
> that "remote=" server. TCP SYN is sent, and then ... ... ... nothing.

I noticed the ":80 to :443" flaw in the log, and I don't know why this
shows up if it's not a redirection.
So I did another test to another destination, and I tried to connect
to host with IP addr. 104.113.250.104 on port 80.
Now the log is consistent, but I'm still getting the same connection
timeout even though I can connect without any issues with an HTTP
client from the Squid machine itself. If it were a packet routing
issue, wouldn't the connection time out also with this HTTP client on
the server itself?

Do you see anything fishy in the squid log I've pasted below?

https://pastebin.com/yJZYw28A

Thanks again,

Vieri


From rousskov at measurement-factory.com  Fri Oct 18 13:49:15 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Oct 2019 09:49:15 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <20f13601-400f-2983-2ca1-afbb71678e12@spawn.link>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
 <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
 <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>
 <b03b57ee-02ce-06f5-276f-8cbeb8d58ba1@measurement-factory.com>
 <20f13601-400f-2983-2ca1-afbb71678e12@spawn.link>
Message-ID: <7abb674e-e413-bd6c-08e1-53b013199800@measurement-factory.com>

On 10/17/19 7:13 PM, Antonio SJ Musumeci wrote:
> On 10/17/2019 5:47 PM, Alex Rousskov wrote:
>> Unfortunately, a simple implementation may produce a lot of false
>> warnings in some environments while a quality implementation may not be
>> as easy as you think: Accessing free space info may require special
>> permissions and correctly accounting for the existing shared memory
>> segments in that partition would be tricky (they can be leftovers from
>> the previous Squid run that will be overwritten or something completely
>> unrelated to Squid). Even finding the right partition name in a portable
>> way may be tricky!
>>
>> IMHO, the future development directions outlined when adding
>> shared_memory_locking are more promising in general, but I would be
>> happy to learn that there are even better options.

> Clearly Squid is aware of the path where these temp files are being
> created

Actually, the location of shared memory segments is chosen by the OS.
Not all OSes do what Linux does. The code dealing with [naming] shared
memory segments is more complicated than you probably imagine. However,
let's not spend time arguing about these low-level specifics here: If
you can contribute an improvement, please post a plan on squid-dev.
Otherwise, let's leave these low-level details to those contributing
improvements.


Cheers,

Alex.


From rs-squid at lists.microscopium.de  Fri Oct 18 14:17:51 2019
From: rs-squid at lists.microscopium.de (Robert Senger)
Date: Fri, 18 Oct 2019 16:17:51 +0200
Subject: [squid-users] How to make squid use ipv4 only for connecting to
	websites
Message-ID: <93dd1309ece4caf77115eba0d2bafdece6e847c1.camel@lists.microscopium.de>

Hi there,

I am running squid on a dual stacked host. Both ipv4 and ipv6
connectivity is fully functional.

Now there's a group of clients that should be configured to load
websites via ipv4 only.

  acl proxy-extra localip fd10:96e4:b552::43
  acl proxy-extra localip 172.16.4.243

I know there's an acl that can be used to identify ipv6 destinations.

  acl to_ipv6 dst ipv6

I tried to block outgoing ipv6 with 
  
  always_direct deny to_ipv6

but that makes dual stacked websites completely unreachable, no
fallback etc.

The only way I found so far is to set an invalid ipv6 outgoing address:

  tcp_outgoing_address fd20::1  proxy-extra
  tcp_outgoing_address 172.16.4.244 proxy-extra

where fd20::1 simply does not exist on the host system.

This shows the results I want when browsing to test sites like 
http://ipv6-test.com (ipv4 connectivity only).

But I am not sure if setting invalid addresses is really desirable...

So, is there a better / more elegant way to tell squid to use ipv4 only
when serving request for certain clients? 

Thanks,

Robert

 
-- 
Robert Senger




From trapexit at spawn.link  Fri Oct 18 14:25:06 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Fri, 18 Oct 2019 10:25:06 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <7abb674e-e413-bd6c-08e1-53b013199800@measurement-factory.com>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
 <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
 <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>
 <b03b57ee-02ce-06f5-276f-8cbeb8d58ba1@measurement-factory.com>
 <20f13601-400f-2983-2ca1-afbb71678e12@spawn.link>
 <7abb674e-e413-bd6c-08e1-53b013199800@measurement-factory.com>
Message-ID: <6d561be2-ee79-b460-1378-1f3df49cc5cb@spawn.link>

On 10/18/2019 9:49 AM, Alex Rousskov wrote:
> Actually, the location of shared memory segments is chosen by the OS.
> Not all OSes do what Linux does. The code dealing with [naming] shared
> memory segments is more complicated than you probably imagine. However,
> let's not spend time arguing about these low-level specifics here: If
> you can contribute an improvement, please post a plan on squid-dev.
> Otherwise, let's leave these low-level details to those contributing
> improvements.

You don't need to know where the OS put the file (though /dev/shm is a 
well known location and easily could be checked explicitly).

$ man fstatvfs

The attached example seems to work fine and could be incorporated into 
ipc/mem/Segment.cc.

I'd be happy to post this on squid-dev / submit a pull request.


-------------- next part --------------
/*
  $ gcc -o shm_statvfs shm_statvfs.c -lrt
 */

#include <fcntl.h>
#include <stdio.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <sys/statvfs.h>
#include <unistd.h>

const char FILENAME[] = "statvfs.test.tmp";

int
main(int   argc,
     char *argv)
{
  int rv;
  int fd;
  struct statvfs vfs;

  fd = shm_open(FILENAME,O_CREAT|O_RDWR,S_IRUSR|S_IWUSR);

  rv = fstatvfs(fd,&vfs);
  printf("fstatvfs(%d,%p) = %d\n",fd,&vfs,rv);
  if(rv == -1)
    return 1;

  printf("frsize: %ld\n"
         "bavail: %ld\n"
         "total avail: %ld\n",
         vfs.f_frsize,
         vfs.f_bavail,
         vfs.f_frsize*vfs.f_bavail);

  close(fd);

  shm_unlink(FILENAME);

  return 0;
}

From rousskov at measurement-factory.com  Fri Oct 18 14:57:02 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Oct 2019 10:57:02 -0400
Subject: [squid-users] How to make squid use ipv4 only for connecting to
 websites
In-Reply-To: <93dd1309ece4caf77115eba0d2bafdece6e847c1.camel@lists.microscopium.de>
References: <93dd1309ece4caf77115eba0d2bafdece6e847c1.camel@lists.microscopium.de>
Message-ID: <db3adeef-8ad5-fa1f-6737-53ba948f7c6f@measurement-factory.com>

On 10/18/19 10:17 AM, Robert Senger wrote:

> there's a group of clients that should be configured to load
> websites via ipv4 only.

> The only way I found so far is to set an invalid ipv6 outgoing address:
> 
>   tcp_outgoing_address fd20::1  proxy-extra
>   tcp_outgoing_address 172.16.4.244 proxy-extra
> 
> where fd20::1 simply does not exist on the host system.
> 
> This shows the results I want when browsing to test sites like 
> http://ipv6-test.com (ipv4 connectivity only).

Glad you found a workaround!


> But I am not sure if setting invalid addresses is really desirable...

It is not. I have not checked, but I would expect that Squid master/v5
(at least) will try to use that invalid outgoing address (and fail).
Needless to say, such futile attempts waste time and other resources.
They may also lead to misleading user-visible errors.


> So, is there a better / more elegant way to tell squid to use ipv4 only
> when serving request for certain clients? 

tcp_outgoing_address is not meant for prohibiting destinations. I think
Squid should support use cases like yours explicitly: We should add a
new directive that only applies to direct destinations selected by the
existing peer selection algorithms:

    acl to_ipv6 dst ipv6
    direct_access deny to_ipv6 proxy-extra

I also considered extending cache_peer_access to apply to direct
destinations, but rejected that idea because we want to preserve
existing checks for cache_peer names in cache_peer_access and because
applying a directive called "cache_peer..." to direct connections is
unexpected/confusing.

We could also add a new ACL-driven directive to prohibit A or AAAA DNS
queries for certain names:

    dns_query_access AAAA deny proxy-extra

Using dns_query_access would save DNS resources in your use case, but
direct_access would cover a lot more use cases because it is a lot more
precise/selective (and not subject to DNS caching concerns).

Perhaps both directives should be added.


Quality pull requests or their sponsorship welcomed:
https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


Cheers,

Alex.


From rousskov at measurement-factory.com  Fri Oct 18 15:11:02 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Oct 2019 11:11:02 -0400
Subject: [squid-users] SIGBUS attempting to use rock
In-Reply-To: <6d561be2-ee79-b460-1378-1f3df49cc5cb@spawn.link>
References: <1895525e-3be9-8762-b703-42eab2b9cd26@spawn.link>
 <5bf6c15a-72c0-6e60-1fae-6ba901c83d8d@measurement-factory.com>
 <58f2af4a-a3b3-2b5a-41b3-e01ae3c5ec36@spawn.link>
 <b03b57ee-02ce-06f5-276f-8cbeb8d58ba1@measurement-factory.com>
 <20f13601-400f-2983-2ca1-afbb71678e12@spawn.link>
 <7abb674e-e413-bd6c-08e1-53b013199800@measurement-factory.com>
 <6d561be2-ee79-b460-1378-1f3df49cc5cb@spawn.link>
Message-ID: <d52a1703-8522-5bb3-003f-397dc9832f8d@measurement-factory.com>

On 10/18/19 10:25 AM, Antonio SJ Musumeci wrote:

> I'd be happy to post this on squid-dev / submit a pull request.

Please do, assuming you are willing to do the legwork necessary to go
from a proof of concept (that has already changed several times during
this short discussion!) to the final officially-acceptable solution.

    https://wiki.squid-cache.org/MergeProcedure

You do not have to get the overall design approved on squid-dev first,
but I would recommend that step to reduce code rewrites in this case.
The design RFC needs no code. I would focus on how the new feature will
avoid false warnings (especially in cases where the partition may have
stale Squid files that this instance will overwrite). BTW, please keep
in mind that Squid creates many segments of various sizes, in
decentralized fashion.

Alex.


From squid3 at treenet.co.nz  Fri Oct 18 20:05:48 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 19 Oct 2019 09:05:48 +1300
Subject: [squid-users] (no subject)
In-Reply-To: <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
Message-ID: <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>

On 19/10/19 1:21 am, Vieri Di Paola wrote:
> On Fri, Oct 11, 2019 at 3:50 PM Amos Jeffries wrote:
>>
>> Note that this last entry is about a connection to port 443, whereas the
>> rest of the log is all about traffic to port 80.
>>>
>>> The Squid machine has no issues if I browse the web from command line,
>>> eg. 'links http://www.linuxheadquarters.com' works fine.
>>>
>>> What should I be looking for?
>>
>> TCP/IP level packet routing. Squid is trying to open a TCP connection to
>> that "remote=" server. TCP SYN is sent, and then ... ... ... nothing.
> 
> I noticed the ":80 to :443" flaw in the log, and I don't know why this
> shows up if it's not a redirection.

If you are able to share your config maybe we could help spot something,
both for that and for the timeout issue.


> So I did another test to another destination, and I tried to connect
> to host with IP addr. 104.113.250.104 on port 80.
> Now the log is consistent, but I'm still getting the same connection
> timeout even though I can connect without any issues with an HTTP
> client from the Squid machine itself. If it were a packet routing
> issue, wouldn't the connection time out also with this HTTP client on
> the server itself?

You said Squid used TPROXY. The spoofing of packets causes a different
set of routing tables and rules to be applied than normal server
outgoing traffic.

> 
> Do you see anything fishy in the squid log I've pasted below?
> 

Looks like Squid is doing everything right and the issues is somewhere
between the TCP SYN send and SYN ACK returning.


Amos


From eperez at quadrianweb.com  Fri Oct 18 22:21:59 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Fri, 18 Oct 2019 17:21:59 -0500
Subject: [squid-users] Is an ACL read from memory or disk?
Message-ID: <CACXMG+soKWGYSbzrXgbagqX07aqF6Fz5s_sXyYjsBE6iDyTtRg@mail.gmail.com>

Hi,
I'm running squid with only allowed domains in
acl allow_domains dstdomain "/etc/squid/allow_domains.txt"

question: Does squid reads the file at startup/reload and checks the ACL in
memory?
or is there a disk read for every time the ACL needs to be checked?
If there's a disk read, will it benefit if I use a small RAM disk?

squid is compiled with:
[root at s03-prxy squid]# squid --version
Squid Cache: Version 4.8
Service Name: squid

This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions
on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--exec_prefix=/usr'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
'--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake'
'--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp'
'--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi'
'--enable-security-cert-generators' '--enable-security-cert-validators'
'--enable-icmp' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=16384' '--with-dl' '--with-openssl'
'--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl'
'--disable-arch-native' '--without-nettle'
'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
--param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
-fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
--enable-ltdl-convenience
[root at s03-prxy squid]#

Thanks!

-- 

---------------------
Erick Perez
---------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191018/2a54a21b/attachment.htm>

From rousskov at measurement-factory.com  Fri Oct 18 23:09:17 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 18 Oct 2019 19:09:17 -0400
Subject: [squid-users] Is an ACL read from memory or disk?
In-Reply-To: <CACXMG+soKWGYSbzrXgbagqX07aqF6Fz5s_sXyYjsBE6iDyTtRg@mail.gmail.com>
References: <CACXMG+soKWGYSbzrXgbagqX07aqF6Fz5s_sXyYjsBE6iDyTtRg@mail.gmail.com>
Message-ID: <48833053-7cbf-6095-9d2a-b6b771e38c8f@measurement-factory.com>

On 10/18/19 6:21 PM, Erick Perez - Quadrian Enterprises wrote:
> acl allow_domains dstdomain "/etc/squid/allow_domains.txt"

> Does squid reads the file at startup/reload and checks the ACL
> in memory?

Yes.

> or is there a disk read for every time the ACL needs to be checked?

No.

Alex.


From eperez at quadrianweb.com  Sat Oct 19 02:38:38 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Fri, 18 Oct 2019 21:38:38 -0500
Subject: [squid-users] Is an ACL read from memory or disk?
In-Reply-To: <48833053-7cbf-6095-9d2a-b6b771e38c8f@measurement-factory.com>
References: <CACXMG+soKWGYSbzrXgbagqX07aqF6Fz5s_sXyYjsBE6iDyTtRg@mail.gmail.com>
 <48833053-7cbf-6095-9d2a-b6b771e38c8f@measurement-factory.com>
Message-ID: <CACXMG+udLO53QMOiKYWLPx870dhCe4gJFox0h89e4s9zmW05Kg@mail.gmail.com>

Thanks.

On Fri, Oct 18, 2019, 6:09 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 10/18/19 6:21 PM, Erick Perez - Quadrian Enterprises wrote:
> > acl allow_domains dstdomain "/etc/squid/allow_domains.txt"
>
> > Does squid reads the file at startup/reload and checks the ACL
> > in memory?
>
> Yes.
>
> > or is there a disk read for every time the ACL needs to be checked?
>
> No.
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191018/50836d62/attachment.htm>

From vieridipaola at gmail.com  Tue Oct 22 06:31:36 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Tue, 22 Oct 2019 08:31:36 +0200
Subject: [squid-users] external_acl_type and ipv6
Message-ID: <CABLYT9jK8s3+AUBwfrTecZR+Wa1wAa-MQ6evbTdfmF+OTKAEQg@mail.gmail.com>

Hi,

What is the advantage of using ipv6 instead of ipv4 by default for
external_acl_type?

http://www.squid-cache.org/Doc/config/external_acl_type/

Thanks,

Vieri


From squid3 at treenet.co.nz  Tue Oct 22 08:58:51 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Oct 2019 21:58:51 +1300
Subject: [squid-users] external_acl_type and ipv6
In-Reply-To: <CABLYT9jK8s3+AUBwfrTecZR+Wa1wAa-MQ6evbTdfmF+OTKAEQg@mail.gmail.com>
References: <CABLYT9jK8s3+AUBwfrTecZR+Wa1wAa-MQ6evbTdfmF+OTKAEQg@mail.gmail.com>
Message-ID: <384cc4c8-399c-b69d-992d-26096bd93c12@treenet.co.nz>

On 22/10/19 7:31 pm, Vieri Di Paola wrote:
> Hi,
> 
> What is the advantage of using ipv6 instead of ipv4 by default for
> external_acl_type?
> 

* larger packet sizes,

* simplicity, and

* all modern OS support IPv6 by default.

There is also:

<https://tools.ietf.org/html/bcp177>

"
   o  IPv6 support must be equivalent or better in quality and
      functionality when compared to IPv4 support in a new or updated IP
      implementation.

   o  New and updated IP networking implementations should support IPv4
      and IPv6 coexistence (dual-stack), but must not require IPv4 for
      proper and complete function.

   o  Implementers are encouraged to update existing hardware and
      software to enable IPv6 wherever technically feasible.
"

No reason not to comply on private channels created between Squid and
its helpers.

Amos


From vieridipaola at gmail.com  Tue Oct 22 10:22:01 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Tue, 22 Oct 2019 12:22:01 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
 <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
Message-ID: <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>

Hi,

On Fri, Oct 18, 2019 at 10:13 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> If you are able to share your config maybe we could help spot something,
> both for that and for the timeout issue.

I prepared and tested a trimmed-down squid conf:

# cat squid.conf
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 901         # SWAT
acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl explicit myportname 3128
acl intercepted myportname 3129
acl interceptedssl myportname 3130

http_port 3128
http_port 3129 tproxy
https_port 3130 tproxy ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem
sslflags=NO_DEFAULT_CA
sslproxy_flags DONT_VERIFY_PEER

sslcrtd_program /usr/libexec/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 16MB
sslcrtd_children 40 startup=20 idle=10

cache_dir diskd /var/cache/squid 32 16 256

acl localnet src 10.0.0.0/8
acl localnet src 192.168.0.0/16

acl good_useragents req_header User-Agent Firefox/
acl good_useragents req_header User-Agent Edge/
acl good_useragents req_header User-Agent Microsoft-CryptoAPI/

http_access deny intercepted !localnet
http_access deny interceptedssl !localnet

http_access allow CONNECT interceptedssl SSL_ports
http_access deny !good_useragents

http_access allow localnet

debug_options rotate=1 ALL,9

reply_header_access Alternate-Protocol deny all
ssl_bump stare all
ssl_bump bump all

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service antivirus respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access antivirus allow all

email_err_data on
client_lifetime 480 minutes

httpd_suppress_version_string on
dns_v4_first on
via off
forwarded_for transparent

cache_mem 32 MB

max_filedescriptors 65536
icap_service_failure_limit -1
icap_persistent_connections off

http_access allow localhost

http_access deny all

coredump_dir /var/cache/squid

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

> You said Squid used TPROXY. The spoofing of packets causes a different
> set of routing tables and rules to be applied than normal server
> outgoing traffic.

I use Shorewall on this system. This program configures iptables and routing.
I dumped all the network information while trying to access port 80 on
host with IP addr. 104.113.250.104 form local host with IP addr.
10.215.144.48:
https://drive.google.com/file/d/13Pr2OCgCInY6E72krCci9BiHrB1lrMce/view?usp=sharing

> Looks like Squid is doing everything right and the issues is somewhere
> between the TCP SYN send and SYN ACK returning.

I suspect there must be something wrong with my routing or marking
(please see dump).

Thanks,

Vieri


From squid3 at treenet.co.nz  Tue Oct 22 11:40:06 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Oct 2019 00:40:06 +1300
Subject: [squid-users] (no subject)
In-Reply-To: <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
 <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
 <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>
Message-ID: <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>

On 22/10/19 11:22 pm, Vieri Di Paola wrote:
> 
> I use Shorewall on this system. This program configures iptables and routing.
> I dumped all the network information while trying to access port 80 on
> host with IP addr. 104.113.250.104 form local host with IP addr.
> 10.215.144.48:


I do not see any DIVERT rule at all in your firewall config dump. That
is at least part of the problem.

Have you run through the notes and troubleshooting checks on the TPROXY
feature page?
<https://wiki.squid-cache.org/Features/Tproxy4>


Amos


From vieridipaola at gmail.com  Tue Oct 22 12:17:29 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Tue, 22 Oct 2019 14:17:29 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
 <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
 <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>
 <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>
Message-ID: <CABLYT9gqJ1g7nkJ9fetHy4dfsv7Xv6zOUxnJ9z2yBQMpg2yF9w@mail.gmail.com>

On Tue, Oct 22, 2019 at 1:48 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 22/10/19 11:22 pm, Vieri Di Paola wrote:
> >
> > I use Shorewall on this system. This program configures iptables and routing.
> > I dumped all the network information while trying to access port 80 on
> > host with IP addr. 104.113.250.104 form local host with IP addr.
> > 10.215.144.48:
> I do not see any DIVERT rule at all in your firewall config dump. That
> is at least part of the problem.

I don't know why.. I must have taken the wrong dump. Here's a new one
I just tested:

https://drive.google.com/file/d/1iqIU8SrvmOfSHs7wv2tjLLx1DXWNrP8h/view?usp=sharing

> Have you run through the notes and troubleshooting checks on the TPROXY
> feature page?
> <https://wiki.squid-cache.org/Features/Tproxy4>

Yes, but I'm obviously overlooking something.
I'll work on it.

Thanks,

Vieri


From vieridipaola at gmail.com  Tue Oct 22 12:23:17 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Tue, 22 Oct 2019 14:23:17 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
 <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
 <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>
 <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>
Message-ID: <CABLYT9gUxrDsW1=67HAsbJdCiE8p1gS-=+vE8U=3L62E5ofKqg@mail.gmail.com>

On Tue, Oct 22, 2019 at 1:48 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> I do not see any DIVERT rule at all in your firewall config dump. That
> is at least part of the problem.

I opened the previous dump and saw the divert rules here below:

Chain PREROUTING (policy ACCEPT 573K packets, 462M bytes)
 pkts bytes target     prot opt in     out     source
destination
 573K  462M CONNMARK   all  --  *      *       0.0.0.0/0
0.0.0.0/0            CONNMARK restore mask 0xff
 1213  181K routemark  all  --  ppp1   *       0.0.0.0/0
0.0.0.0/0            mark match 0x0/0xff
 3195  308K routemark  all  --  ppp2   *       0.0.0.0/0
0.0.0.0/0            mark match 0x0/0xff
 1320 79360 routemark  all  --  ppp3   *       0.0.0.0/0
0.0.0.0/0            mark match 0x0/0xff
 311K  277M tcpre      all  --  *      *       0.0.0.0/0
0.0.0.0/0            mark match 0x0/0xff
    0     0 divert     tcp  --  ppp1   *       0.0.0.0/0
10.215.144.48       [goto]  tcp spt:80 flags:!0x17/0x02 socket
--transparent
    0     0 divert     tcp  --  ppp2   *       0.0.0.0/0
10.215.144.48       [goto]  tcp spt:80 flags:!0x17/0x02 socket
--transparent
    0     0 divert     tcp  --  ppp3   *       0.0.0.0/0
10.215.144.48       [goto]  tcp spt:80 flags:!0x17/0x02 socket
--transparent
   76  7484 TPROXY     tcp  --  enp10s0 *       10.215.144.48
0.0.0.0/0            tcp dpt:80 TPROXY redirect 0.0.0.0:3129 mark
0x200/0x200
    0     0 divert     tcp  --  ppp1   *       0.0.0.0/0
10.215.144.48       [goto]  tcp spt:443 flags:!0x17/0x02 socket
--transparent
    0     0 divert     tcp  --  ppp2   *       0.0.0.0/0
10.215.144.48       [goto]  tcp spt:443 flags:!0x17/0x02 socket
--transparent
    0     0 divert     tcp  --  ppp3   *       0.0.0.0/0
10.215.144.48       [goto]  tcp spt:443 flags:!0x17/0x02 socket
--transparent
   10  1060 TPROXY     tcp  --  enp10s0 *       10.215.144.48
0.0.0.0/0            tcp dpt:443 TPROXY redirect 0.0.0.0:3130 mark
0x200/0x200

Aren't these the DIVERT rules you are referring to?


From trapexit at spawn.link  Tue Oct 22 19:24:53 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Tue, 22 Oct 2019 15:24:53 -0400
Subject: [squid-users] assertion failed: Controller.cc:838: "!transients ||
 e.hasTransients()"
Message-ID: <33ab9e2e-a886-c75a-d26c-a3f1a7674319@spawn.link>

Squid 4.8

Attempting to get a SMP setup with rock enabled. Instance points to an 
origin web server.

With "workers 1" everything appears to work fine. If I set "workers 2" I 
get a number of issues:

If I request a particular object a kid will fail with:

assertion failed: Controller.cc:838: "!transients || e.hasTransients()"

Pulling other objects sometimes works but speeds continuously decline to 
the point of being unusable. I'm not seeing any consistency between what 
objects cause what issues and any acls or refresh patterns.

Is there a simple, modern, functional SMP example to compare? Does that 
assert stand out? Didn't find anything on search engines.


From rousskov at measurement-factory.com  Tue Oct 22 20:07:59 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Oct 2019 16:07:59 -0400
Subject: [squid-users] assertion failed: Controller.cc:838: "!transients
 || e.hasTransients()"
In-Reply-To: <33ab9e2e-a886-c75a-d26c-a3f1a7674319@spawn.link>
References: <33ab9e2e-a886-c75a-d26c-a3f1a7674319@spawn.link>
Message-ID: <e6016958-c44e-9079-d171-4290196d46d4@measurement-factory.com>

On 10/22/19 3:24 PM, Antonio SJ Musumeci wrote:
> Squid 4.8
> 
> Attempting to get a SMP setup with rock enabled. Instance points to an
> origin web server.
> 
> With "workers 1" everything appears to work fine. If I set "workers 2" I
> get a number of issues:
> 
> If I request a particular object a kid will fail with:

> assertion failed: Controller.cc:838: "!transients || e.hasTransients()"

For supported Squid configurations with no other reported errors or
warnings, this assertion indicates a Squid bug. Consider filing a bug
report and posting a stack trace (as well as other relevant reproduction
info) there. Posting an ALL,9 cache.log collected while executing a
single asserting transaction may speed up triage.


> Is there a simple, modern, functional SMP example to compare?

To enable basic SMP features, adding "cache_dir rock..." or "workers N"
(with N greater than 1) is enough.

Alex.


From jbhasin83 at gmail.com  Wed Oct 23 02:15:15 2019
From: jbhasin83 at gmail.com (Jatin Bhasin)
Date: Wed, 23 Oct 2019 13:15:15 +1100
Subject: [squid-users] Call for adaptation after sni peeked
In-Reply-To: <CAGRJihUSS194CqRGG699K5a7MvAMiojC_gvQ8hV5+Cm2bsU0wg@mail.gmail.com>
References: <CAGRJihUSS194CqRGG699K5a7MvAMiojC_gvQ8hV5+Cm2bsU0wg@mail.gmail.com>
Message-ID: <CAGRJihU58aNMWjWFEVoboZugeXs9MFzqic6so=V4VWUXaE6d3A@mail.gmail.com>

Hi All,

This question is related to ssl decryption and ecap adaptation call.
When the ssl connection starts then before it even extracts sni squid sends
 fakeConnect which comes to ecap as well.
I am using peek in step 1 and after fakeConnect squid extracts the sni, but
at this point squid does not make another call to ecap. This function in
squid is startPeekAndSpliceDone in file client_side.cc
In this function it only makes a call to acl for ssl bump to check but no
call to ecap adaptation checks.

I was hoping at this point I can put a call to http->doCallouts which can
make the call to ecap adapter and this time we have sni as well?

I needed this functionality as I want to make the decision using sni
whether to bump the connection or not.

Thanks,
Jatin Bhasin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191023/984e4174/attachment.htm>

From squid3 at treenet.co.nz  Wed Oct 23 10:47:46 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Oct 2019 23:47:46 +1300
Subject: [squid-users] RES: How to make only IPV6 visible even incoming
 via IPV4?
In-Reply-To: <000401d58947$a08d5d40$e1a817c0$@uol.com.br>
References: <000001d57f16$b1381b70$13a85250$@uol.com.br>
 <6300c252-09b1-88ef-a091-8a8f2156e9df@treenet.co.nz>
 <000401d58947$a08d5d40$e1a817c0$@uol.com.br>
Message-ID: <7ebdc79f-93ef-9aa3-e8af-786d9300224e@treenet.co.nz>

On 23/10/19 3:14 pm, Marcelo Rodrigo wrote:
> 
> The way this setup is even if I visit simple websites like http://www.meuip.com.br it will show the V4 instead of V6 address.
> I have to find a way to force tcp_outgoing_address to really go out just via V6 in a way that V4 cannot be seen.

That website is IPv4-only. It cannot be connected to over IPv6.

The default for Squid is to prefer and use IPv6 whenever that protocol
is possible. IPv4 is only used when it is required to be used - such as
to contact IPv4-only services or dual-stack servers whose IPv6
connectivity has failed.


Amos


From squid3 at treenet.co.nz  Wed Oct 23 11:06:14 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 Oct 2019 00:06:14 +1300
Subject: [squid-users] (no subject)
In-Reply-To: <CABLYT9gUxrDsW1=67HAsbJdCiE8p1gS-=+vE8U=3L62E5ofKqg@mail.gmail.com>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
 <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
 <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>
 <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>
 <CABLYT9gUxrDsW1=67HAsbJdCiE8p1gS-=+vE8U=3L62E5ofKqg@mail.gmail.com>
Message-ID: <a2a3249e-a2fd-74ee-c608-6951c6427743@treenet.co.nz>

On 23/10/19 1:23 am, Vieri Di Paola wrote:
> On Tue, Oct 22, 2019 at 1:48 PM Amos Jeffries wrote:
>>
>> I do not see any DIVERT rule at all in your firewall config dump. That
>> is at least part of the problem.
> 
> I opened the previous dump and saw the divert rules here below:
> 
> Chain PREROUTING (policy ACCEPT 573K packets, 462M bytes)
>  pkts bytes target     prot opt in     out     source
> destination
>  573K  462M CONNMARK   all  --  *      *       0.0.0.0/0
> 0.0.0.0/0            CONNMARK restore mask 0xff
>  1213  181K routemark  all  --  ppp1   *       0.0.0.0/0
> 0.0.0.0/0            mark match 0x0/0xff
>  3195  308K routemark  all  --  ppp2   *       0.0.0.0/0
> 0.0.0.0/0            mark match 0x0/0xff
>  1320 79360 routemark  all  --  ppp3   *       0.0.0.0/0
> 0.0.0.0/0            mark match 0x0/0xff
>  311K  277M tcpre      all  --  *      *       0.0.0.0/0
> 0.0.0.0/0            mark match 0x0/0xff
>     0     0 divert     tcp  --  ppp1   *       0.0.0.0/0
> 10.215.144.48       [goto]  tcp spt:80 flags:!0x17/0x02 socket
> --transparent
>     0     0 divert     tcp  --  ppp2   *       0.0.0.0/0
> 10.215.144.48       [goto]  tcp spt:80 flags:!0x17/0x02 socket
> --transparent
>     0     0 divert     tcp  --  ppp3   *       0.0.0.0/0
> 10.215.144.48       [goto]  tcp spt:80 flags:!0x17/0x02 socket
> --transparent
>    76  7484 TPROXY     tcp  --  enp10s0 *       10.215.144.48
> 0.0.0.0/0            tcp dpt:80 TPROXY redirect 0.0.0.0:3129 mark
> 0x200/0x200
>     0     0 divert     tcp  --  ppp1   *       0.0.0.0/0
> 10.215.144.48       [goto]  tcp spt:443 flags:!0x17/0x02 socket
> --transparent
>     0     0 divert     tcp  --  ppp2   *       0.0.0.0/0
> 10.215.144.48       [goto]  tcp spt:443 flags:!0x17/0x02 socket
> --transparent
>     0     0 divert     tcp  --  ppp3   *       0.0.0.0/0
> 10.215.144.48       [goto]  tcp spt:443 flags:!0x17/0x02 socket
> --transparent
>    10  1060 TPROXY     tcp  --  enp10s0 *       10.215.144.48
> 0.0.0.0/0            tcp dpt:443 TPROXY redirect 0.0.0.0:3130 mark
> 0x200/0x200
> 
> Aren't these the DIVERT rules you are referring to?
> 


Oh, case sensitivity. I was grep'ing for the upper case chain name.

So you have a 'divert' chain.

First problem with these rules is they depend on an IP address. IP is
the one detail guaranteed not to match properly when TPROXY spoofing is
going on.

Second problem is that they also depend on a source port number of 80 or
443. The traffic needing to be marked comes from both directions, so
this will break half the traffic flow.


Third is that you are using the --transparent option. If I am
understanding it correctly, that will cause the connections out of Squid
(which are marked as transparent) to skip divert action and hit the
TPROXY intercept all over again - infinite loop.

Amos


From vieridipaola at gmail.com  Wed Oct 23 12:08:34 2019
From: vieridipaola at gmail.com (Vieri Di Paola)
Date: Wed, 23 Oct 2019 14:08:34 +0200
Subject: [squid-users] (no subject)
In-Reply-To: <a2a3249e-a2fd-74ee-c608-6951c6427743@treenet.co.nz>
References: <CABLYT9j0Lq3qNC0xM5hXzMkx0+GOraWN3=+CRzYMovRpG9xngg@mail.gmail.com>
 <1f521791-5a70-5418-3d60-b0fca8eda0d2@treenet.co.nz>
 <CABLYT9jy+9JnC2RLWfCMMtGB956AgvH6uvXOOojbJw_3CgSkFQ@mail.gmail.com>
 <1414da61-bf3a-336f-eae1-39da40ce2bd6@treenet.co.nz>
 <CABLYT9hQQH2bufz8cWSihfcySj3+CP3K+Y=RL0HgabWjn1iqEQ@mail.gmail.com>
 <cb4e7a5d-2300-71ec-c6bc-6e153cc10f9f@treenet.co.nz>
 <CABLYT9gUxrDsW1=67HAsbJdCiE8p1gS-=+vE8U=3L62E5ofKqg@mail.gmail.com>
 <a2a3249e-a2fd-74ee-c608-6951c6427743@treenet.co.nz>
Message-ID: <CABLYT9gH2Kihyo-aXoUsSzin2yEB0vfk0nHYQXTOcWDEHPoNUw@mail.gmail.com>

On Wed, Oct 23, 2019 at 1:06 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> First problem with these rules is they depend on an IP address. IP is
> the one detail guaranteed not to match properly when TPROXY spoofing is
> going on.

Thank you for giving me clues.
Actually, my whole setup was OK except for one detail.
Where I specify only "10.215.144.48" for TProxy, I needed to also add
the public IP addresses of my 3 ppp links to Internet, ie. the "inet"
values that are shown with:
# ip a s ppp1
# ip a s ppp2
# ip a s ppp3

I don't know how to avoid that. However, it's not a big deal because
they are static addresses.

Thanks again,

Vieri


From jbhasin83 at gmail.com  Wed Oct 23 19:37:19 2019
From: jbhasin83 at gmail.com (Jatin Bhasin)
Date: Thu, 24 Oct 2019 06:37:19 +1100
Subject: [squid-users] Call for adaptation after sni peeked
Message-ID: <CAGRJihX_X+FFK-M0UOg6UhnPL3_TeOykxY8+F64L5AL7teRfVg@mail.gmail.com>

Hi All,

This question is related to ssl decryption and ecap adaptation call.
When the ssl connection starts then before it even extracts sni squid sends
 fakeConnect which comes to ecap as well.
I am using peek in step 1 and after fakeConnect squid extracts the sni, but
at this point squid does not make another call to ecap. This function in
squid is startPeekAndSpliceDone in file client_side.cc
In this function it only makes a call to acl for ssl bump to check but no
call to ecap adaptation checks.

I was hoping at this point I can put a call to http->doCallouts which can
make the call to ecap adapter and this time we have sni as well?

I needed this functionality as I want to make the decision using sni
whether to bump the connection or not.

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191024/bb3e5d0c/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 23 20:55:07 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 Oct 2019 16:55:07 -0400
Subject: [squid-users] Call for adaptation after sni peeked
In-Reply-To: <CAGRJihX_X+FFK-M0UOg6UhnPL3_TeOykxY8+F64L5AL7teRfVg@mail.gmail.com>
References: <CAGRJihX_X+FFK-M0UOg6UhnPL3_TeOykxY8+F64L5AL7teRfVg@mail.gmail.com>
Message-ID: <dec5c19f-be1d-f6f3-eda2-5c0d8b96f492@measurement-factory.com>

On 10/23/19 3:37 PM, Jatin Bhasin wrote:

> This question is related to ssl decryption and ecap adaptation call.?
> When the ssl connection starts then before it even extracts sni squid sends?
> fakeConnect which comes to ecap as well.

Yes, this happens during SslBump step1 as described at
https://wiki.squid-cache.org/Features/SslPeekAndSplice


> I am using peek in step 1 and after fakeConnect squid extracts the sni,
> but at this point squid does not make another call to ecap.

According to the above wiki page (and my understanding of how SslBump
should work), Squid should make another adaptation pass during step2.
You may want to make sure that your Squid does not discover some error
_before_ it can start doing eCAP during step2.

If your eCAP service does not see the second CONNECT (during step2), I
suggest using the latest Squid v4 with the following "minimal" SslBump
configuration:

    ssl_bump peek step1
    ssl_bump splice all

Does the above work without problems when eCAP is turned off?

Does the above deliver the second CONNECT to eCAP when it is enabled?


> This function in squid is startPeekAndSpliceDone in file
> client_side.cc


We should not be discussing code details on squid-users, but the latest
Squid v4 does not have that function AFAICT:

> $ git grep startPeekAndSpliceDone SQUID_4_8 | wc -l
> 0


Alex.


From felix.leimbach at gmail.com  Sun Oct 27 17:10:53 2019
From: felix.leimbach at gmail.com (Felix Leimbach)
Date: Sun, 27 Oct 2019 18:10:53 +0100
Subject: [squid-users] acl for matching URLs (non-regex)
Message-ID: <d41dc9f3-6868-bbac-c264-d2aaa96d97dd@gmail.com>

Hi,

I want to include the URLhaus blacklist [1] to protect my users from malware, since squidblacklists.org doesn't seem to be actively maintained anymore (RIP Ben Nichols).

However there does not seem to exist an acl type that can match plain URLs in this form:

http://tvmarket.co.kr/Order/Document.zip?natcanotti.biz[document_pdf_____+.exe%5D%2F%3F

Note the URL is not regex-friendly so url_regex is not an option.

How do people block exact URLs?
Is a c-icap module needed for this?

Thanks

Felix

[1] https://urlhaus.abuse.ch/api/#retrieve



From jbhasin83 at gmail.com  Mon Oct 28 07:25:16 2019
From: jbhasin83 at gmail.com (Jatin Bhasin)
Date: Mon, 28 Oct 2019 18:25:16 +1100
Subject: [squid-users] Call for adaptation after sni peeked
In-Reply-To: <dec5c19f-be1d-f6f3-eda2-5c0d8b96f492@measurement-factory.com>
References: <CAGRJihX_X+FFK-M0UOg6UhnPL3_TeOykxY8+F64L5AL7teRfVg@mail.gmail.com>
 <dec5c19f-be1d-f6f3-eda2-5c0d8b96f492@measurement-factory.com>
Message-ID: <CAGRJihX7G3qUp2ApFxbJJzrPKifs0LbiSq53CA-i07T6Ny7+uA@mail.gmail.com>

Hi Alex,

If I use below squid configuration:

    ssl_bump peek step1
    ssl_bump splice all

I would see fake connect request in step 2 as well. I did not check squid
version 4 but squid version 3 will send second fake connect in ecap adapter
only if we splice step 2 which will be true in above configuration.
But I don't want to splice step 2, well not always. I want my ecap adapter
to get fake connect in all cases in step 2 so that I can then make a
decision on step 2 whether to splice or bump in step 2.
In other words at the end of step 1 squid could make a call to adaptation
acl (it does not currently) which will help to make decisions based on sni
(if available).

As per my understanding squid makes call to adaptation acl in following
cases:
Step 1 - At start of connection but here only ip is available.
Step 2 - only when splicing
I did not check any further from here because then mostly its too late to
bump anyway.

I am happy to send following to another group if you can suggest:
I made a manual code change for acl adaptation at the end of step 1 and I
was able to send fake connect with sni to ecap. I wanted to understand from
experts if these changes are incorrect and may causes issues in some cases
I don't know about?

Thanks,
Jatin

On Thu., 24 Oct. 2019, 07:55 Alex Rousskov, <
rousskov at measurement-factory.com> wrote:

> On 10/23/19 3:37 PM, Jatin Bhasin wrote:
>
> > This question is related to ssl decryption and ecap adaptation call.
> > When the ssl connection starts then before it even extracts sni squid
> sends
> > fakeConnect which comes to ecap as well.
>
> Yes, this happens during SslBump step1 as described at
> https://wiki.squid-cache.org/Features/SslPeekAndSplice
>
>
> > I am using peek in step 1 and after fakeConnect squid extracts the sni,
> > but at this point squid does not make another call to ecap.
>
> According to the above wiki page (and my understanding of how SslBump
> should work), Squid should make another adaptation pass during step2.
> You may want to make sure that your Squid does not discover some error
> _before_ it can start doing eCAP during step2.
>
> If your eCAP service does not see the second CONNECT (during step2), I
> suggest using the latest Squid v4 with the following "minimal" SslBump
> configuration:
>
>     ssl_bump peek step1
>     ssl_bump splice all
>
> Does the above work without problems when eCAP is turned off?
>
> Does the above deliver the second CONNECT to eCAP when it is enabled?
>
>
> > This function in squid is startPeekAndSpliceDone in file
> > client_side.cc
>
>
> We should not be discussing code details on squid-users, but the latest
> Squid v4 does not have that function AFAICT:
>
> > $ git grep startPeekAndSpliceDone SQUID_4_8 | wc -l
> > 0
>
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191028/d14a43aa/attachment.htm>

From mgresko8 at gmail.com  Wed Oct 30 04:59:59 2019
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Wed, 30 Oct 2019 05:59:59 +0100
Subject: [squid-users] ssl bump intermediate certificate
Message-ID: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>

Hello,

I am trying to configure ssl bumping on squid 4.8 but my browser is
not able to validate the certificate due to intermediate certificate
missing. How could I convince squid to send it?

Thanks

Marek


From Walter.H at mathemainzel.info  Wed Oct 30 09:11:56 2019
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 30 Oct 2019 10:11:56 +0100
Subject: [squid-users] ssl bump intermediate certificate
In-Reply-To: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>
References: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>
Message-ID: <5DB953DC.1020306@mathemainzel.info>

On 30.10.2019 05:59, Marek Gre?ko wrote:
> Hello,
>
> I am trying to configure ssl bumping on squid 4.8 but my browser is
> not able to validate the certificate due to intermediate certificate
> missing. How could I convince squid to send it?
>
> Thanks
>
> Marek
the ssl-bum certificate is either a root certificate itself which must 
be installed on the clients or an intermediate, where
the root and all intermediates between must be installed on the clients


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191030/442ec02b/attachment.bin>

From uhlar at fantomas.sk  Wed Oct 30 09:42:06 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 30 Oct 2019 10:42:06 +0100
Subject: [squid-users] ssl bump intermediate certificate
In-Reply-To: <5DB953DC.1020306@mathemainzel.info>
References: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>
 <5DB953DC.1020306@mathemainzel.info>
Message-ID: <20191030094206.GA32145@fantomas.sk>

>On 30.10.2019 05:59, Marek Gre?ko wrote:
>>I am trying to configure ssl bumping on squid 4.8 but my browser is
>>not able to validate the certificate due to intermediate certificate
>>missing. How could I convince squid to send it?

On 30.10.19 10:11, Walter H. wrote:
>the ssl-bum certificate is either a root certificate itself which must 
>be installed on the clients or an intermediate, where
>the root and all intermediates between must be installed on the clients

do you mean that squid won't send intermediate certificate?

this should be:

https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Honk if you love peace and quiet.


From sfoutrel at ecritel.net  Wed Oct 30 16:11:29 2019
From: sfoutrel at ecritel.net (=?utf-8?B?Rk9VVFJFTCBTw6liYXN0aWVu?=)
Date: Wed, 30 Oct 2019 16:11:29 +0000
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
Message-ID: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>

Hello, I would like to use squid as a transparent proxy for my users.


My platform is pretty simple ?


"Clients" are behind a Debian "Router" which MASQUERADE them (as they use RFC 1918 ips).

I have a Squid 4.6 from Debian Buster packages installed on a "Proxy" server which is outside my network.


I read a lot of tutorials and examples from squid site...


I Applied a DNAT to trafic coming from Clients thru Router to Proxy.

iptables -tnat -A PREROUTING -i LAN_3500 -p tcp -m tcp --dport 80 -j DNAT --to-destination <Proxy>:3129


HTTP is coming to squid successfully but squid logs show a request coming from proxy himself and a request coming from Router (as Clients are NATed by Router)


if I allow in squid.conf the Proxy IP, I end up with a Forward loop...


I also tried the tproxy scenario with no success.


I'd really like some help.


Thanks !

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191030/e8f0743b/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Oct 30 16:39:28 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 30 Oct 2019 17:39:28 +0100
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
In-Reply-To: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>
References: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>
Message-ID: <201910301739.28311.Antony.Stone@squid.open.source.it>

On Wednesday 30 October 2019 at 17:11:29, FOUTREL S?bastien wrote:

> Hello, I would like to use squid as a transparent proxy for my users.

> "Clients" are behind a Debian "Router" which MASQUERADE them (as they use
> RFC 1918 ips).
> 
> I have a Squid 4.6 from Debian Buster packages installed on a "Proxy"
> server which is outside my network.
> 
> I read a lot of tutorials and examples from squid site...

Did that include the links I've given below?

> I Applied a DNAT to trafic coming from Clients thru Router to Proxy.
> 
> iptables -tnat -A PREROUTING -i LAN_3500 -p tcp -m tcp --dport 80 -j DNAT
> --to-destination <Proxy>:3129

Have you put this rule onto the firewall you mention, or the Squid box itself?

https://wiki.squid-cache.org/SquidFaq/InterceptionProxy
#Requirements_and_methods_for_Interception_Caching

states "NAT configuration will only work when used *on the squid box* ."

So, you *must* put that rule on the Squid machine itself, not on the firewall.

It goes on to say "To intercept from a gateway machine and direct traffic at a 
separate squid box use policy routing." with a link to 
https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute

> HTTP is coming to squid successfully but squid logs show a request coming
> from proxy himself and a request coming from Router (as Clients are NATed
> by Router)

Ah, so you *are* doing the NAT on the router :)  Don't :)

> if I allow in squid.conf the Proxy IP, I end up with a Forward loop...
> 
> 
> I also tried the tproxy scenario with no success.

Well, give us some details of what you tried, how you configured it, what 
worked, and what didn't work, and we might be able to help, otherwise we can 
only say "well, tproxy does work if set up properly, so if yours doesn't work, 
it isn't set up properly", which isn't a very helpful answer...


Antony.

-- 
If at first you don't succeed, destroy all the evidence that you tried.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From mgresko8 at gmail.com  Wed Oct 30 20:49:54 2019
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Wed, 30 Oct 2019 21:49:54 +0100
Subject: [squid-users] ssl bump intermediate certificate
In-Reply-To: <20191030094206.GA32145@fantomas.sk>
References: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>
 <5DB953DC.1020306@mathemainzel.info> <20191030094206.GA32145@fantomas.sk>
Message-ID: <CAChjPdTMwPFN8eNacz+UYsEB2DzcmM=V9gg6neK5J6M22nUTSQ@mail.gmail.com>

Hello,

Matus, I also found the document. It should be sending the chain, but
is not. When I specify cafile option it responds I shoud use
tls-cafile. But in either case it is not sending.

Walter, if squid has such requirement, then it is unfinished. Every
other proxy is able to run its CA as an intermediate and clients
install only root CA. The proxy should be responsible to hold the
chain. The url Matus sent is the correct way how to do it, but is is
not working. At least not in 4.8 vesion.

Marek


2019-10-30 10:42 GMT+01:00, Matus UHLAR - fantomas <uhlar at fantomas.sk>:
>>On 30.10.2019 05:59, Marek Gre?ko wrote:
>>>I am trying to configure ssl bumping on squid 4.8 but my browser is
>>>not able to validate the certificate due to intermediate certificate
>>>missing. How could I convince squid to send it?
>
> On 30.10.19 10:11, Walter H. wrote:
>>the ssl-bum certificate is either a root certificate itself which must
>>be installed on the clients or an intermediate, where
>>the root and all intermediates between must be installed on the clients
>
> do you mean that squid won't send intermediate certificate?
>
> this should be:
>
> https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Honk if you love peace and quiet.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Thu Oct 31 07:38:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Oct 2019 20:38:08 +1300
Subject: [squid-users] ssl bump intermediate certificate
In-Reply-To: <CAChjPdTMwPFN8eNacz+UYsEB2DzcmM=V9gg6neK5J6M22nUTSQ@mail.gmail.com>
References: <CAChjPdQ818t5MMxA+XfknHTpXg=OcLTACgaMKv=1s4Mce-hwgQ@mail.gmail.com>
 <5DB953DC.1020306@mathemainzel.info> <20191030094206.GA32145@fantomas.sk>
 <CAChjPdTMwPFN8eNacz+UYsEB2DzcmM=V9gg6neK5J6M22nUTSQ@mail.gmail.com>
Message-ID: <64c612ec-9f72-2565-b0dd-aabe2aaf2f69@treenet.co.nz>

On 31/10/19 9:49 am, Marek Gre?ko wrote:
> Hello,
> 
> Matus, I also found the document. It should be sending the chain, but
> is not. When I specify cafile option it responds I shoud use
> tls-cafile. But in either case it is not sending.
> 
> Walter, if squid has such requirement, then it is unfinished. Every
> other proxy is able to run its CA as an intermediate and clients
> install only root CA. The proxy should be responsible to hold the
> chain. The url Matus sent is the correct way how to do it, but is is
> not working. At least not in 4.8 vesion.
> 

"
cafile=
  File containing additional CA certificates to use
  when verifying client certificates.
"

Note that last line. Squid-4 is more strict about its configured inputs
being used for what they are documented as.

The best place to put the chain is actually in the PEM file used in the
cert= parameter. It should contain as much of the chain as you want
Squid to send, starting with the proxies signing CA cert and going up
the chained intermediate CA certs towards the root CA.


Squid-4 will validate all certificates actually are a chain with correct
sequence, ignoring any which are incorrect or out of sequence. Running
"squid -k parse" will reports any errors loading the chain.

Amos


From xychix2011 at gmail.com  Thu Oct 31 07:48:37 2019
From: xychix2011 at gmail.com (Mark Bergman)
Date: Thu, 31 Oct 2019 08:48:37 +0100
Subject: [squid-users] Squid proxy will forward message with 'alternating
 host header' but logs another?
Message-ID: <CAK9S3GogS1Xm=rXJ8mpk5yoKD1stcLu5C0GZZMg9kh-nJFwMVg@mail.gmail.com>

Can i stop squid from 'repairing' host headers?
I've been all over this for hours, I can only lead this back to this change
in 1999
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=50292

I wan't this sample code to do trough squid what it would do without a
proxy at all and i want squid to log the host header set by the sample code.

```
import requests

proxies = {'http': 'http://10.0.0.4:8080',}
headers = {"Host":"someevilhost.appspot.com","Tester":"xychix",}

s = requests.Session()
#### proxy is set OFF
s.proxies = proxies
r = s.get('http://www.google.com/',headers=headers)

print(r.status_code)
print(r.text[:80])
```
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191031/bc9501d5/attachment.htm>

From squid3 at treenet.co.nz  Thu Oct 31 08:31:32 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Oct 2019 21:31:32 +1300
Subject: [squid-users] acl for matching URLs (non-regex)
In-Reply-To: <d41dc9f3-6868-bbac-c264-d2aaa96d97dd@gmail.com>
References: <d41dc9f3-6868-bbac-c264-d2aaa96d97dd@gmail.com>
Message-ID: <4424da41-aeb8-5237-e281-95e090698d93@treenet.co.nz>

On 28/10/19 6:10 am, Felix Leimbach wrote:
> Hi,
> 
> I want to include the URLhaus blacklist [1] to protect my users from malware, since squidblacklists.org doesn't seem to be actively maintained anymore (RIP Ben Nichols).
> 
> However there does not seem to exist an acl type that can match plain URLs in this form:
> 
> http://tvmarket.co.kr/Order/Document.zip?natcanotti.biz[document_pdf_____+.exe%5D%2F%3F
> 
> Note the URL is not regex-friendly so url_regex is not an option.

What isn't regex friendly about it?


> 
> How do people block exact URLs?

By making a regex which can only match that exact URL.

acl foo url_regex
^http://tvmarket\.co\.kr/Order/Document\.zip\?natcanotti\.biz\[document_pdf_____\+\.exe\]/\?



Amos


From squid3 at treenet.co.nz  Thu Oct 31 09:03:58 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 31 Oct 2019 22:03:58 +1300
Subject: [squid-users] Squid proxy will forward message with
 'alternating host header' but logs another?
In-Reply-To: <CAK9S3GogS1Xm=rXJ8mpk5yoKD1stcLu5C0GZZMg9kh-nJFwMVg@mail.gmail.com>
References: <CAK9S3GogS1Xm=rXJ8mpk5yoKD1stcLu5C0GZZMg9kh-nJFwMVg@mail.gmail.com>
Message-ID: <59334bb4-f2e1-09fe-6e3c-6fef0c69d796@treenet.co.nz>

On 31/10/19 8:48 pm, Mark Bergman wrote:
> Can i stop squid from 'repairing' host headers?

Yes.

For context:

RFC 7230 :

"If the target URI includes an authority component, then a
   client MUST send a field-value for Host that is identical to that
   authority component"

"If the target URI includes an authority component, then a
   client MUST send a field-value for Host that is identical to that
   authority component"

"A server MUST respond with a 400 (Bad Request) status code to any
   HTTP/1.1 request message that ... contains ... a
   Host header field with an invalid field-value."


When the host_verify_strict directive is set to "on" then Squid will
produce a 4XX status code to any traffic received with invalid Host
headers. A Host header that conflicts with info in the URL is always
invalid.

Amos


From xychix2011 at gmail.com  Thu Oct 31 11:48:45 2019
From: xychix2011 at gmail.com (Mark Bergman)
Date: Thu, 31 Oct 2019 12:48:45 +0100
Subject: [squid-users] Squid proxy will forward message with
 'alternating host header' but logs another?
In-Reply-To: <CAK9S3GpVoJi8dXmAg+LCe-cXB2SP5L21mMXz8aoA8Le0SnGiCg@mail.gmail.com>
References: <CAK9S3GogS1Xm=rXJ8mpk5yoKD1stcLu5C0GZZMg9kh-nJFwMVg@mail.gmail.com>
 <59334bb4-f2e1-09fe-6e3c-6fef0c69d796@treenet.co.nz>
 <CAK9S3GpVoJi8dXmAg+LCe-cXB2SP5L21mMXz8aoA8Le0SnGiCg@mail.gmail.com>
Message-ID: <CAK9S3GrGPdXYGBLcyv=NHwsS13+v8a93mKtOqYsa0oWHkGoLFg@mail.gmail.com>

reincluded the list for completeness and archiving.

We're building a setup where I want to be able to find domain fronting [
https://en.wikipedia.org/wiki/Domain_fronting] attempts in the logs

used test script:
>
> import requests
> proxies = {'http': 'http://10.0.0.4:8080',}
> headers = {"Host":"someevilhost.appspot.com","Orig-Host":"
> someevilhost.appspot.com"}
> s = requests.Session()
> s.proxies = proxies
> r = s.get('http://www.google.com/',headers=headers)
> print(r.status_code)
> print(r.text[:80])


my loglines keep showing www.google.com in the host header regardless of
how I set my config. Current config (as added in my pfsense setup)

> host_verify_strict on
> strip_query_terms off
> client_dst_passthru off
> logformat combined2 %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st
> "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh "%>h"
> access_log tcp://10.1.2.15:1025 combined2
> #access_log /var/squid/logs/combined2


example log line:

> 10.1.2.15 - - [31/Oct/2019:11:42:53 +0000] "GET http://www.google.com/
> HTTP/1.1" 200 6261 "-" "python-requests/2.9.1" TCP_MISS:HIER_DIRECT
> "User-Agent: python-requests/2.9.1\r\nAccept: */*\r\nConnection:
> keep-alive\r\nAccept-Encoding: gzip, deflate\r\nOrig-Host:
> someevilhost.appspot.com\r\nHost: www.google.com\r\n"


I'm looking for a  way to have Squid log the original request, whatever it
does after that is for this particular test less important (/dev/null or
out to the internet.. both are OK for me as long as 'RFC compliant' traffic
from the webbrowser does get out and logged).

regards,

Mark

On Thu, Oct 31, 2019 at 12:35 PM Mark Bergman <xychix2011 at gmail.com> wrote:

> Ok, so there is no way I can have Squid act as most corporate other
> proxies (just forward the request without manipulation)?
> We are building a setup where we want people to recognise domain fronting
> from logs.
> https://en.wikipedia.org/wiki/Domain_fronting
>
> But as I understand now this technique would never work trough a Squid
> proxy (if SSL inspection is enabled). Wonder then if there never had been
> complaints from signal (messaging app) users as they relied on this
> technology for years :)
> We might have to switch to a less RCF compliant proxy for that.
>
> Any help and suggestions are really appreciated.
>
> Regards,
>
> Mark / xychix
>
>
> On Thu, Oct 31, 2019 at 10:04 AM Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 31/10/19 8:48 pm, Mark Bergman wrote:
>> > Can i stop squid from 'repairing' host headers?
>>
>> Yes.
>>
>> For context:
>>
>> RFC 7230 :
>>
>> "If the target URI includes an authority component, then a
>>    client MUST send a field-value for Host that is identical to that
>>    authority component"
>>
>> "If the target URI includes an authority component, then a
>>    client MUST send a field-value for Host that is identical to that
>>    authority component"
>>
>> "A server MUST respond with a 400 (Bad Request) status code to any
>>    HTTP/1.1 request message that ... contains ... a
>>    Host header field with an invalid field-value."
>>
>>
>> When the host_verify_strict directive is set to "on" then Squid will
>> produce a 4XX status code to any traffic received with invalid Host
>> headers. A Host header that conflicts with info in the URL is always
>> invalid.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191031/bbe25397/attachment.htm>

From rousskov at measurement-factory.com  Thu Oct 31 14:18:33 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 31 Oct 2019 10:18:33 -0400
Subject: [squid-users] Squid proxy will forward message with
 'alternating host header' but logs another?
In-Reply-To: <CAK9S3GrGPdXYGBLcyv=NHwsS13+v8a93mKtOqYsa0oWHkGoLFg@mail.gmail.com>
References: <CAK9S3GogS1Xm=rXJ8mpk5yoKD1stcLu5C0GZZMg9kh-nJFwMVg@mail.gmail.com>
 <59334bb4-f2e1-09fe-6e3c-6fef0c69d796@treenet.co.nz>
 <CAK9S3GpVoJi8dXmAg+LCe-cXB2SP5L21mMXz8aoA8Le0SnGiCg@mail.gmail.com>
 <CAK9S3GrGPdXYGBLcyv=NHwsS13+v8a93mKtOqYsa0oWHkGoLFg@mail.gmail.com>
Message-ID: <50498d63-f15f-d706-c315-dd7140820969@measurement-factory.com>

On 10/31/19 7:48 AM, Mark Bergman wrote:
>     logformat combined2 ... %ru ... "%>h"

> example log line:

>     ... http://www.google.com/ ... "...Orig-Host:
>     someevilhost.appspot.com\r\nHost:
>     www.google.com\r\n"?


> I'm looking for a? way to have Squid log the original request

You are already doing the right thing. Squid promises that "%>h" is the
received request header [before any adaptations, including the Host
header rewrites]. There is a bug in Squid that breaks that documentation
promise and makes it impossible to log what you want without changing
Squid code. Quality pull requests fixing that bug (or their sponsorship)
are welcomed.


HTH,

Alex.


From sfoutrel at ecritel.net  Thu Oct 31 16:53:17 2019
From: sfoutrel at ecritel.net (=?iso-8859-1?Q?FOUTREL_S=E9bastien?=)
Date: Thu, 31 Oct 2019 16:53:17 +0000
Subject: [squid-users] Unsuccessful at using Squid v4 with intercept
In-Reply-To: <201910301739.28311.Antony.Stone@squid.open.source.it>
References: <e3579749f1a347e9b46f6cb53a446138@ecritel.net>,
 <201910301739.28311.Antony.Stone@squid.open.source.it>
Message-ID: <fbd9fefd24994631b233a8872612f865@ecritel.net>




________________________________
De : squid-users <squid-users-bounces at lists.squid-cache.org> de la part de Antony Stone <Antony.Stone at squid.open.source.it>
Envoy? : mercredi 30 octobre 2019 17:39
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] Unsuccessful at using Squid v4 with intercept

On Wednesday 30 October 2019 at 17:11:29, FOUTREL S?bastien wrote:

> Hello, I would like to use squid as a transparent proxy for my users.

> "Clients" are behind a Debian "Router" which MASQUERADE them (as they use
> RFC 1918 ips).
>
> I have a Squid 4.6 from Debian Buster packages installed on a "Proxy"
> server which is outside my network.
>
> I read a lot of tutorials and examples from squid site...

Did that include the links I've given below?

Yes I read almost all examples config from wiki.squid-cache.org<https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>
<https://wiki.squid-cache.org/SquidFaq/InterceptionProxy>And I was mislead by the fact that there is a DNAT config and a REDIRECT config.. DNAT is completely useless if Squid only support to be on the router.
Wasn't it possible to dnat to a different server with older versions (my memory is faulty) ?
http://tldp.org/HOWTO/TransparentProxy-6.html for example.



I read the "fw mark and route policy" method as an alternative not the only way to go. My mistake.


> I Applied a DNAT to trafic coming from Clients thru Router to Proxy.
>
> iptables -tnat -A PREROUTING -i LAN_3500 -p tcp -m tcp --dport 80 -j DNAT
> --to-destination <Proxy>:3129

Have you put this rule onto the firewall you mention, or the Squid box itself?

https://wiki.squid-cache.org/SquidFaq/InterceptionProxy
#Requirements_and_methods_for_Interception_Caching

states "NAT configuration will only work when used *on the squid box* ."

So, you *must* put that rule on the Squid machine itself, not on the firewall.

It goes on to say "To intercept from a gateway machine and direct traffic at a
separate squid box use policy routing." with a link to
https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute

> HTTP is coming to squid successfully but squid logs show a request coming
> from proxy himself and a request coming from Router (as Clients are NATed
> by Router)

Ah, so you *are* doing the NAT on the router :)  Don't :)

> if I allow in squid.conf the Proxy IP, I end up with a Forward loop...
>
>
> I also tried the tproxy scenario with no success.

Well, give us some details of what you tried, how you configured it, what
worked, and what didn't work, and we might be able to help, otherwise we can
only say "well, tproxy does work if set up properly, so if yours doesn't work,
it isn't set up properly", which isn't a very helpful answer...

I read with a new eye the tproxy page https://wiki.squid-cache.org/ConfigExamples/FullyTransparentWithTPROXY and found that I forgot the policy routing part.
Will try again.

Thanks for your help.
Sebastien.

Antony.

--
If at first you don't succeed, destroy all the evidence that you tried.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20191031/ad75cd9b/attachment.htm>

From trapexit at spawn.link  Thu Oct 31 20:19:58 2019
From: trapexit at spawn.link (Antonio SJ Musumeci)
Date: Thu, 31 Oct 2019 16:19:58 -0400
Subject: [squid-users] optional verification of clients?
Message-ID: <e35417dc-192c-d7ca-cb1c-1e4b2498d2a8@spawn.link>

Is there a way to do something similar to NGINX's "ssl_verify_client 
optional;"?


