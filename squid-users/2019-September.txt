From cofkomail at gmail.com  Sun Sep  1 20:44:04 2019
From: cofkomail at gmail.com (torson)
Date: Sun, 1 Sep 2019 15:44:04 -0500 (CDT)
Subject: [squid-users] Working peek/splice no longer functioning on some
	sites
In-Reply-To: <fc698c26-ea16-2a0c-f83b-e4b1609f0aaa@treenet.co.nz>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
 <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
 <1511794211.2147.4.camel@slave-tothe-box.net>
 <7e3e81c9-0949-9f16-f1f8-7efc052dae78@treenet.co.nz>
 <7512b52427c65ed6a0364885bc359d48@localhost>
 <fc698c26-ea16-2a0c-f83b-e4b1609f0aaa@treenet.co.nz>
Message-ID: <1567370644478-0.post@n4.nabble.com>

For me it works with "ssl_bump peek step1", not with "ssl_bump peek all".

My working config using Squid 4.8:
---
visible_hostname squid
debug_options ALL,1
positive_dns_ttl 0
negative_dns_ttl 0
client_persistent_connections off
http_port 3128
http_port 3129 intercept
acl allowed_http_sites dstdom_regex "/etc/squid/allow_list.conf"
http_access allow allowed_http_sites
https_port 3130 intercept ssl-bump \
  tls-cert=/etc/squid/ssl/squid-ca-cert-key.pem \
  options=SINGLE_DH_USE,SINGLE_ECDH_USE,NO_SSLv2,NO_SSLv3 \
  tls-dh=/etc/squid/ssl/dhparam.pem
acl SSL_port port 443
http_access allow SSL_port
acl allowed_https_sites ssl::server_name_regex "/etc/squid/allow_list.conf"
tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice allowed_https_sites
ssl_bump terminate all
http_access deny all
logformat general      %tl %6tr %>a %Ss/%03>Hs %<st %rm %ssl::bump_mode %ru
%ssl::>sni 
access_log daemon:/var/log/squid/access.log general
---

One thing to note are the "positive_dns_ttl 0" and "negative_dns_ttl 0"
directives ; my findings are that DNS caching needs to be set to zero in
cases where DNS records get changed every minute due to roundrobin combined
with hosting in environments where record changes faster than TTL - on AWS
where you're hitting different DNS servers with each having a different TTL.
I was getting a lot of host forgery errors before setting those to 0.
This is in addition to all the servers using the same DNS address.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Sep  2 06:36:11 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 2 Sep 2019 18:36:11 +1200
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <1567370644478-0.post@n4.nabble.com>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
 <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
 <1511794211.2147.4.camel@slave-tothe-box.net>
 <7e3e81c9-0949-9f16-f1f8-7efc052dae78@treenet.co.nz>
 <7512b52427c65ed6a0364885bc359d48@localhost>
 <fc698c26-ea16-2a0c-f83b-e4b1609f0aaa@treenet.co.nz>
 <1567370644478-0.post@n4.nabble.com>
Message-ID: <43802926-c454-112d-f0bd-c0b377f5b058@treenet.co.nz>

On 2/09/19 8:44 am, torson wrote:
> For me it works with "ssl_bump peek step1", not with "ssl_bump peek all".
> 

That tells me that your clients are lying to your proxy.

"peek step1" means only the client-provided detail is available. eg the
client says it is going to example.net (a domain which you allow) but
actually goes to othersite.example.com.

"peek all" means Squid also checks at step 2 against the server
certificate. eg Squid now sees the "othersite.example.com" detail and
rejects/terminates the bad client.



> My working config using Squid 4.8:
> ---
> visible_hostname squid
> debug_options ALL,1
> positive_dns_ttl 0
> negative_dns_ttl 0

Minimum value for negative_dns_ttl is 1. positive_dns_ttl must be
*greater* than negative_dns_ttl.

> client_persistent_connections off
> http_port 3128
> http_port 3129 intercept

You are missing the basic DoS and cross-protocol smuggling protections
provided in the default squid.conf.

Without those default rules your proxy is far more vulnerable to DoS
attack than it needs to be. This setup it will perform the complex and
slow regex ACL checking for each DoS request arriving. The defaults are
very fast port/method matches.

I suggest you extend the logformat to record the %ssl::<cert_subject
details from server certificate. Then decide whether (and how) to adjust
the server_name ACL rules.

You may want to use %ssl::<cert_errors as well if the problem is due to
server validation errors.



> acl allowed_http_sites dstdom_regex "/etc/squid/allow_list.conf"
> http_access allow allowed_http_sites
> https_port 3130 intercept ssl-bump \
>   tls-cert=/etc/squid/ssl/squid-ca-cert-key.pem \
>   options=SINGLE_DH_USE,SINGLE_ECDH_USE,NO_SSLv2,NO_SSLv3 \

SSLv2 related settings are obsolete in Squid-4. Even ones disabling it.

>   tls-dh=/etc/squid/ssl/dhparam.pem
> acl SSL_port port 443
> http_access allow SSL_port
> acl allowed_https_sites ssl::server_name_regex "/etc/squid/allow_list.conf"
> tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump splice allowed_https_sites
> ssl_bump terminate all
> http_access deny all
> logformat general      %tl %6tr %>a %Ss/%03>Hs %<st %rm %ssl::bump_mode %ru
> %ssl::>sni 
> access_log daemon:/var/log/squid/access.log general
> ---
> 
> One thing to note are the "positive_dns_ttl 0" and "negative_dns_ttl 0"
> directives ; my findings are that DNS caching needs to be set to zero in
> cases where DNS records get changed every minute due to roundrobin combined
> with hosting in environments where record changes faster than TTL - on AWS
> where you're hitting different DNS servers with each having a different TTL.
> I was getting a lot of host forgery errors before setting those to 0.
> This is in addition to all the servers using the same DNS address.
> 

IMO you are misunderstanding what the problem is and causing yourself
potentially worse problems in the long run.

FYI: Round-robin DNS has almost nothing to do with this issue. Under
round-robin the IPs Squid is checking for are still present, just not
first in the set. So the Host verify passes.
 The only part it might have is when the IP address set is too big for
the DNS packets your network (and your upstreams) allow. EDNS and
Jumbogram support resolves these issues entirely.


 The problem is that the DNS responses from the providers are *entirely*
different from one lookup to the next. This is guaranteed to happen when
Squid and the client are using completely different DNS resolver chains
for their lookups.

 -> If the DNS resolver difference is within your network, you need to
fix *that* instead of hacking away at the DNS cache in Squid to violate
DNS protocol requirements. The client and other DNS caches in your
network are still using those TTLs properly - so you are just shifting
the verify failure from one HTTP transaction to another.

 -> If the DNS chain difference is at the provider end (as it is for
Akamai CDN, and anyone using Google resolvers). Then you need to
*reduce* the number of out-of-sync lookups being done by both Squid and
the client. This TTL hack is the exact opposite of what you need. Best
workaround is to have your internal resolver setup to extend the TTLs
for the known problematic services.


Setting negative_dns_ttl to value less than 1 second causes every
*internal* use of the IP address by Squid to require a new DNS lookup.
This can result is weird behaviour like the arriving request http_access
rules matching against one IP, the server being connected to having a
second IP and the log entry recording yet another one.

Ignoring the proper TTLs can also result in your server being detected
by anti-DoS protection at your upstream DNS provider(s) and blocked from
service entirely.

You have been lucky that your config is so simple it does not do more
than 2 DNS queries per request, and/or that your traffic volume is so
low the premature repeat queries are not being noticed (yet).


FWIW: the use of "client_persistent_connections off" which you have does
more to address the problem those CDN create. It prevents clients
re-using a TCP connection setup with now-stale DNS details. Boosting
Squid's chance of seeing the same DNS the client has.


HTH
Amos


From ilari.laitinen at iki.fi  Mon Sep  2 11:22:47 2019
From: ilari.laitinen at iki.fi (Ilari Laitinen)
Date: Mon, 2 Sep 2019 14:22:47 +0300
Subject: [squid-users] Linearly increasing delays in HTTPS proxy
 CONNECTS / 3.5.20
In-Reply-To: <05ec929a-8e17-1b8a-c95d-02fc299623f2@measurement-factory.com>
References: <592B861A-9199-4497-8BFF-9F253F62AFBA@iki.fi>
 <d5759d03-ead0-c441-029f-288955ec4cd9@measurement-factory.com>
 <E8E8363F-0CED-47EC-BFB1-5C5E791C5B64@iki.fi>
 <05ec929a-8e17-1b8a-c95d-02fc299623f2@measurement-factory.com>
Message-ID: <380CBB39-AAC1-48D9-823D-3B7312A16399@iki.fi>

> On 30 Aug 2019, at 16.40, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 8/30/19 8:16 AM, Ilari Laitinen wrote:
> 
>> I suspect Squid might be waiting for local TCP ports from the kernel
>> (or something related).
> 
> IIRC, ephemeral source port allocator is instantaneous -- Squid either
> gets a port or a port allocation error, without waiting. When we
> overload the server with high-performance tests (without an explicit
> port manager), we see port allocation errors rather than stalled tests.
> However, perhaps that is not true in your OS/environment.

Ah yes, of course. I actually saw this error first-hand when setting up the test environment.

>> Right now, there are four different IP addresses returned for the
>> target cloud service. For practical purposes, they are returned in a
>> random order. The traffic would ideally be spread over all of them.
>> Unfortunately it is evident both from the debug log and from the TCP
>> dump that Squid is using only one of the addresses at a time. The
>> amount of connections in the TIME_WAIT state for that single IP
>> address gets very close to the maximum defined by the
>> net.ipv4.ip_local_port_range sysctl. After a while (a minute or so in
>> the recording) this address changes presumably in response to a new
>> DNS query result.
> 
> In theory, Squid should round-robin across all destination IP addresses
> for a single host name. If your Squid v3 does not, it is probably a
> Squid bug that can be fixed [by upgrading].
> 
> Said that, IIRC, the notion of "round robin" is rather vague in Squid
> because there are several places where an IP may be requested for the
> same host name inside the same transaction. I would not be surprised if
> that low-level round-robin behavior results in the same IP being used
> for most transactions in some environments (until an error or a new DNS
> query reshuffles the IPs). Debugging logs may expose this problem.

I looked into this further. All our tcp dumps so far show the same: Squid uses (almost always) exactly one remote address at a time. The increasing delays start right after Squid has switched to a new remote IP and last precisely until another switch happens (typically the next one). The problem does not occur every time and is not limited to a single target IP.

Now that I know that this is not expected and is possibly related to a bug, I?ll look into upgrading Squid from the platform default.

>> One possible workaround that I can think of is setting a short
>> positive_dns_ttl, but this doesn?t fully guarantee an even
>> distribution, now does it?
> 
> No, it does not. Moreover, Squid v3 had some TTL handling bugs that were
> fixed (in v4 and later code) by the Happy Eyeballs project. Taking all
> the known problems into the account, it is difficult for me to predict
> the effect of changing TTLs. Said that, it does not hurt to try! Maybe
> you will be lucky, and a simple configuration change will remove the
> cause of increasing transaction delays.

Thank you very much for your informed and timely replies!

I?ll report our results here.

Best,

-- 
Ilari Laitinen




From service at 4gproxies.net  Mon Sep  2 22:53:30 2019
From: service at 4gproxies.net (service at 4gproxies.net)
Date: Mon, 2 Sep 2019 16:53:30 -0600
Subject: [squid-users] Squid 4.8 Issue
Message-ID: <08bd01d561e1$3dddce60$b9996b20$@4gproxies.net>

Hi All, 

 

I'm wondering how we can implement rules inside Squid 4.8 that if a parent
cache_peer is dead at the time of the proxy request, squid automatically
bounces it to another parent cache_peer instead of the connection just
timing out and not fulfilling the request due to the current cache_peer
being offline or otherwise unavailable. Basically what I want to do here is
to have squid automatically choose any working cache_peer parent instead of
the request just timing out or dying. Hopefully you can understand what I'm
trying to say here. Thank you.

 

Robert Miller

RNS4G Administrator

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190902/ab3c1a59/attachment.htm>

From rousskov at measurement-factory.com  Tue Sep  3 00:22:47 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 2 Sep 2019 20:22:47 -0400
Subject: [squid-users] Squid 4.8 Issue
In-Reply-To: <08bd01d561e1$3dddce60$b9996b20$@4gproxies.net>
References: <08bd01d561e1$3dddce60$b9996b20$@4gproxies.net>
Message-ID: <29ae78d3-7a2a-f59d-60b3-b3fbc5c9fc8c@measurement-factory.com>

On 9/2/19 6:53 PM, service at 4gproxies.net wrote:

> I?m wondering how we can implement rules inside Squid 4.8 that if a
> parent cache_peer is dead at the time of the proxy request, squid
> automatically bounces it to another parent cache_peer instead of the
> connection just timing out and not fulfilling the request due to the
> current cache_peer being offline or otherwise unavailable. Basically
> what I want to do here is to have squid automatically choose any working
> cache_peer parent instead of the request just timing out or dying.

If there is enough time (see connect_timeout and forward_timeout), there
are other destinations to try, and a few other special conditions are
met, then Squid should automatically re-forward the request to the next
peer or origin server upon hitting a connection timeout with the
previous peer or origin server. For details, see
https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F

If your Squid does not re-forward, check the above-mentioned options and
the wiki page to determine why. If all those preconditions are met, but
your Squid still does not re-forward, file a bug report. Posting the
corresponding access.log records and compressed cache.log (with
debug_options set to "ALL,2 44,5 17,5" or higher) collected while
reproducing the problem using a single HTTP transaction may help in triage.

Alex.


From ahmed.zaeem at netstream.ps  Tue Sep  3 04:45:03 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 3 Sep 2019 07:45:03 +0300
Subject: [squid-users] cache peer , force peer to use dns ipv4 not ipv6
Message-ID: <BD6B6A9A-3A96-4664-819D-8080376C9BFA@netstream.ps>

Hello Team ,

just wondering .

using cache peer to FWD request to upstream squid .

the problem is sometimes the Upstream go to destination over ipv6 .

is there an option can be used to force the peer to use ipv4 dns ?

agian , we dont have an access to upstream upstream  , just wondering can we do something our side ?

Thanks 




From squid3 at treenet.co.nz  Tue Sep  3 08:23:24 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 3 Sep 2019 20:23:24 +1200
Subject: [squid-users] cache peer , force peer to use dns ipv4 not ipv6
In-Reply-To: <BD6B6A9A-3A96-4664-819D-8080376C9BFA@netstream.ps>
References: <BD6B6A9A-3A96-4664-819D-8080376C9BFA@netstream.ps>
Message-ID: <17407d35-f3f5-b240-0e84-154e86d88da5@treenet.co.nz>

On 3/09/19 4:45 pm, --Ahmad-- wrote:
> Hello Team ,
> 
> just wondering .
> 
> using cache peer to FWD request to upstream squid .
> 
> the problem is sometimes the Upstream go to destination over ipv6 .
> 
> is there an option can be used to force the peer to use ipv4 dns ?
> 

Put the IPv4 address of the peer into the cache_peer line instead of its
hostname.


Amos


From giles at coochey.net  Tue Sep  3 08:46:40 2019
From: giles at coochey.net (Giles Coochey)
Date: Tue, 3 Sep 2019 09:46:40 +0100
Subject: [squid-users] cache peer , force peer to use dns ipv4 not ipv6
In-Reply-To: <17407d35-f3f5-b240-0e84-154e86d88da5@treenet.co.nz>
References: <BD6B6A9A-3A96-4664-819D-8080376C9BFA@netstream.ps>
 <17407d35-f3f5-b240-0e84-154e86d88da5@treenet.co.nz>
Message-ID: <f8a8186d-84ed-0653-7b46-2b8cb80c0966@coochey.net>


On 03/09/2019 09:23, Amos Jeffries wrote:
> On 3/09/19 4:45 pm, --Ahmad-- wrote:
>> Hello Team ,
>>
>> just wondering .
>>
>> using cache peer to FWD request to upstream squid .
>>
>> the problem is sometimes the Upstream go to destination over ipv6 .
>>
>> is there an option can be used to force the peer to use ipv4 dns ?
>>
> Put the IPv4 address of the peer into the cache_peer line instead of its
> hostname.
If you are trying to influence how your system chooses between IPv4 and 
IPv6 for its outbound connections then you should look to understand how 
/etc/gai.conf operates on your system. Otherwise, if you only want squid 
in this instance to use IPv4 then go with Amos' suggestion above.
> =-
> Giles Coochey


From nabil1385 at gmail.com  Tue Sep  3 08:46:59 2019
From: nabil1385 at gmail.com (fansari)
Date: Tue, 3 Sep 2019 03:46:59 -0500 (CDT)
Subject: [squid-users] cannot access squid with https_port: 403
Message-ID: <1567500419266-0.post@n4.nabble.com>

I have to setup a TLS proxy connection between client and squid. My config is
working with http_port (without TLS) but as soon as I try https_port it does
not work (squid 3.5.23 compiled with --enable-ssl' '--enable-ssl-crtd'
'--with-openssl').

What I am trying to achieve is a proxy for https content. When I access the
squid I always get a 403 error code (I am testing with curl).

curl --proxy ${PROXY} --cacert ${CERT} --proxy-insecure --insecure ${URL}

1567498682.392     3 xxx.xxx.0.239 TCP_DENIED/200 0 CONNECT xxx.xxx.0.1:3129
- HIER_NONE/- -
1567498682.498     1 xxx.xxx.0.239 TAG_NONE/403 3825 CONNECT mydomain:443 -
HIER_NONE/- text/html

Here my squid.conf. What am I doing wrong?

acl wifi_net src xxx.xxx.0.0/24
acl our_proxy localip xxx.xxx.0.1/32
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl step1 at_step SslBump1
acl bumpedPorts myportname 3129
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
http_access allow localhost
http_access allow wifi_net
http_access allow CONNECT bumpedPorts
http_access allow CONNECT our_proxy
http_access deny all
http_port 3128 ssl-bump \
  cert=/etc/squid/certs/squid-ca-cert-key.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
https_port 3129 intercept ssl-bump \
  cert=/etc/squid/certs/squid-ca-cert-key.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
ssl_bump peek step1
ssl_bump bump all
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
cache_dir ufs /var/spool/squid 1024 16 256
debug_options ALL,2
coredump_dir /var/spool/squid
refresh_pattern .               30      20%     1440 override-expire




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ahmed.zaeem at netstream.ps  Tue Sep  3 10:33:00 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 3 Sep 2019 13:33:00 +0300
Subject: [squid-users] cache peer , force peer to use dns ipv4 not ipv6
In-Reply-To: <17407d35-f3f5-b240-0e84-154e86d88da5@treenet.co.nz>
References: <BD6B6A9A-3A96-4664-819D-8080376C9BFA@netstream.ps>
 <17407d35-f3f5-b240-0e84-154e86d88da5@treenet.co.nz>
Message-ID: <F1C3881F-6865-4A1F-89AA-CCC9B7708AAE@netstream.ps>

Hello Team , thank you for replies .

########################################
http_port 10.61.8.189:10000 name=10000
acl 10000 myportname 10000
never_direct allow 10000
cache_peer 192.247.37.193 parent 12847 0 no-query  round-robin no-digest no-tproxy proxy-only name=peer10000
cache_peer_access peer10000 allow 10000
cache_peer_access peer10000 deny all
########################################

Amos do you mean name should be 192.247.37.193 now name=peer10000 ?
is that what you mean ?



Thanks 


> On 3 Sep 2019, at 11:23, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> Put the IPv4 address of the peer into the cache_peer line instead of its

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190903/fed64d5b/attachment.htm>

From jmperrote at policia.rionegro.gov.ar  Tue Sep  3 10:35:22 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Tue, 3 Sep 2019 07:35:22 -0300
Subject: [squid-users] help with helper
Message-ID: <d9220500-ab7f-3a00-7092-0e41797bc3a4@policia.rionegro.gov.ar>

Hello we have a helper to validate users on squid reverse proxy, and 
have a problem on the first validation time !!

On a normal day the first validation, when a user open the client 
browser squid invoque the pop/up and users insert user/password correct 
to validate, and later squid

apparently run the helper requesting again the user and password.


I need help to know it is possible to identify when a users run on first 
time.


regards.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190903/d9800aff/attachment.htm>

From 23dmitry23 at gmail.com  Tue Sep  3 11:47:09 2019
From: 23dmitry23 at gmail.com (KOTOXJle6)
Date: Tue, 3 Sep 2019 06:47:09 -0500 (CDT)
Subject: [squid-users] Cant open some HTTPS with Squid 4.8
Message-ID: <1567511229549-0.post@n4.nabble.com>

Im trying to setup Squid 4.8 on Ubuntu 18.04 LTS with HTTPS redirecting to
squid error page for sites in ACL's. Yesterday i faced major problem HTTPS
sites doesnt open normally in IE11/EDGE and show blank page only + squid
replace certificate. If i tap F5, sometimes site opens like it should and
certificate replacement doesnt happen...and it works not for all sites. I
couldn't pinpoint the dependencies. I also can open some sites like
rambler.ru, kanobu.ru, alexa.com normally. The most interesting thing is
that other browsers like Chrome, FF and even Opera open all sites like it
should and spoof cert + redirect to error page only if site persist in ACL.

What i already did:
- Disabled IPv6 on Squid host
- Disabled/Enabled TLS in IE in any variations
- Disabled SPDY/3

Bump settings in squid.conf:

/http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/squidCA.pem
ssl_bump peek all/

I have this errors in /var/log/squid/cache.log

/ERROR: negotiating TLS on FD 46: error:1425F175:SSL
routines:ssl_choose_client_version:inappropriate fallback (1/-1/0)/

/ERROR: negotiating TLS on FD 104: error:14094410:SSL
routines:ssl3_read_bytes:sslv3 alert handshake failure (1/-1/0)
/

/ERROR: negotiating TLS on FD 27: error:1423406E:SSL
routines:tls_parse_stoc_sct:bad extension (1/-1/0)/

Error in access.log

/TCP_DENIED/407 4141 CONNECT i.ibb.co:443 - HIER_NONE/- text/html/

Same configuration work well on Squid 4.1. 

Sorry for complicated description, im new here and its really hard f or me.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Sep  3 12:03:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 00:03:47 +1200
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <1567500419266-0.post@n4.nabble.com>
References: <1567500419266-0.post@n4.nabble.com>
Message-ID: <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>

On 3/09/19 8:46 pm, fansari wrote:
> I have to setup a TLS proxy connection between client and squid. My config is
> working with http_port (without TLS) but as soon as I try https_port it does
> not work (squid 3.5.23 compiled with --enable-ssl' '--enable-ssl-crtd'
> '--with-openssl').
> 
> What I am trying to achieve is a proxy for https content. When I access the
> squid I always get a 403 error code (I am testing with curl).
> 
> curl --proxy ${PROXY} --cacert ${CERT} --proxy-insecure --insecure ${URL}
> 
> 1567498682.392     3 xxx.xxx.0.239 TCP_DENIED/200 0 CONNECT xxx.xxx.0.1:3129
> - HIER_NONE/- -


You have either opened a TCP connection directly to the "intercept" port
or told Squid to do so on a CONNECT transaction to port 3128.

Only NAT systems can send traffic to an intercept port. That's what the
intercept means.

You must test the proxy with traffic a client would actually send. In
the same way the clients would normally use it.

Amos


From nabil1385 at gmail.com  Tue Sep  3 12:29:25 2019
From: nabil1385 at gmail.com (fansari)
Date: Tue, 3 Sep 2019 07:29:25 -0500 (CDT)
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
Message-ID: <1567513765050-0.post@n4.nabble.com>

Thank you for your reply.

If I drop the keyword "intercept" I get this error message when starting
squid:

FATAL: ssl-bump on https_port requires tproxy/intercept which is missing.

Using "tproxy" does not help me either - I also end up with 403.

What I want to achieve with my scenario is just caching of https content.

The scenario consists of one central system (the "server") and then the
"clients".

For the interface used I work with private IPv4 addresses.

On the server there is IP masquerading for NAT - but it seems that the
traffic even gets stuck before it is sent out.

I tested with tcpdump on the outgoing interface and checked the logs of my
webserver (which I used for a test) and both tests show that the connection
is not going out to my webserver but gets stuck on the squid.

Is "intercept" the correct way when I want to cache https content?

Regarding the clients of the real scenario: this will be a Chromium
application so I could setup a .pac file for example. But before testing
this I want to have a successful curl test.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Sep  3 12:31:13 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 00:31:13 +1200
Subject: [squid-users] cache peer , force peer to use dns ipv4 not ipv6
In-Reply-To: <F1C3881F-6865-4A1F-89AA-CCC9B7708AAE@netstream.ps>
References: <BD6B6A9A-3A96-4664-819D-8080376C9BFA@netstream.ps>
 <17407d35-f3f5-b240-0e84-154e86d88da5@treenet.co.nz>
 <F1C3881F-6865-4A1F-89AA-CCC9B7708AAE@netstream.ps>
Message-ID: <b48fff91-3f13-46c2-36e5-77b86a4d7ab5@treenet.co.nz>

On 3/09/19 10:33 pm, --Ahmad-- wrote:
> Hello Team , thank you for replies .
> 
> ########################################
> http_port 10.61.8.189:10000 name=10000
> acl 10000 myportname 10000
> never_direct allow 10000
> cache_peer 192.247.37.193 parent 12847 0 no-query ?round-robin no-digest
> no-tproxy proxy-only name=peer10000
> cache_peer_access peer10000 allow 10000
> cache_peer_access peer10000 deny all
> ########################################
> 
> Amos do you mean name should be 192.247.37.193 now name=peer10000 ?
> is that what you mean ?
> 

I mean what you have in the above config snippet.

The peer "cache_peer 192.247.37.193 parent" cannot be sometimes
connected to using IPv6 as you described happening.

Amos


From squid3 at treenet.co.nz  Tue Sep  3 12:41:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 00:41:47 +1200
Subject: [squid-users] help with helper
In-Reply-To: <d9220500-ab7f-3a00-7092-0e41797bc3a4@policia.rionegro.gov.ar>
References: <d9220500-ab7f-3a00-7092-0e41797bc3a4@policia.rionegro.gov.ar>
Message-ID: <427df498-26b8-f61d-b6f1-660f093da00c@treenet.co.nz>

On 3/09/19 10:35 pm, jmperrote wrote:
> Hello we have a helper to validate users on squid reverse proxy, and
> have a problem on the first validation time !!
> 
> On a normal day the first validation, when a user open the client
> browser squid invoque the pop/up and users insert user/password correct
> to validate, and later squid
> 
> apparently run the helper requesting again the user and password.
> 
> I need help to know it is possible to identify when a users run on first
> time.

Users cannot be identified until they provide credentials.

This being a reverse-proxy means to the Browser it is no different than
any web server. No sane software will ever blindly assume that the users
LAN account credentials are going to be valid when connecting to a
random web server.

Thus the Browser needs to have stored credentials in its password
manager for that 'website' being hosted by your proxy, or use the popup
to discover them when it starts to do traffic there.

Amos


From squid3 at treenet.co.nz  Tue Sep  3 13:06:45 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 01:06:45 +1200
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <1567513765050-0.post@n4.nabble.com>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
Message-ID: <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>

On 4/09/19 12:29 am, fansari wrote:
> Thank you for your reply.
> 
> If I drop the keyword "intercept" I get this error message when starting
> squid:
> 
> FATAL: ssl-bump on https_port requires tproxy/intercept which is missing.
> 
> Using "tproxy" does not help me either - I also end up with 403.
> 
> What I want to achieve with my scenario is just caching of https content.

What you have configured is *a* valid configuration for that to happen.

Your test is what is wrong _for that port_.


> 
> Regarding the clients of the real scenario: this will be a Chromium
> application so I could setup a .pac file for example. But before testing
> this I want to have a successful curl test.
> 

Aha. This was the critical missing information.

That means the http_port and ssl_bump lines are what you actually need
to be using.

Remove that https_port line entirely.

Also, remove these lines:
"
 acl bumpedPorts myportname 3129

 http_access allow CONNECT bumpedPorts
 http_access allow CONNECT our_proxy
"

Instead you should have your normal http_access rule(s) for determining
which clients are allowed to use the proxy. If they are allowed to use
the proxy they are allowed to do CONNECT already for the https:// traffic.

Test it like this:
  curl --proxy 192.168.0.1:3128 --cacert ${CERT} https://example.com/


Amos


From nabil1385 at gmail.com  Tue Sep  3 13:21:09 2019
From: nabil1385 at gmail.com (fansari)
Date: Tue, 3 Sep 2019 08:21:09 -0500 (CDT)
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
 <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
Message-ID: <1567516869841-0.post@n4.nabble.com>

I have tested this and it is working.

This is what I said: when I use this http_port directive then it works.

So what is still unclear to me is: what is this https_port directive for? I
understood from one of you answers I found to someone else that this will
lead to something like double stacked TLS encryption. Is this correct?

http://squid-web-proxy-cache.1019090.n4.nabble.com/https-port-td4682718.html

The most important question is: using just the http_port directive - will
the connection between client and squid still be https (TLS  encrypted)?
This is important to understand for me because we need https because our
nodejs application will not work with http connections.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From jmperrote at policia.rionegro.gov.ar  Tue Sep  3 13:13:12 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Tue, 3 Sep 2019 10:13:12 -0300
Subject: [squid-users] help with helper
In-Reply-To: <427df498-26b8-f61d-b6f1-660f093da00c@treenet.co.nz>
References: <d9220500-ab7f-3a00-7092-0e41797bc3a4@policia.rionegro.gov.ar>
 <427df498-26b8-f61d-b6f1-660f093da00c@treenet.co.nz>
Message-ID: <f7575444-862f-8be4-8434-c381af450c20@policia.rionegro.gov.ar>

Hello Amos, yes but how can I identified that is on the first request ??

Else squid request to autentificate and later when invoque the helper 
again request to autentificate.

I handle recover the user from squid cache (cachmanager) on the helper, 
for asking if the user previous exist, but squid refresh cache and users 
disapearing time to time.

The exact question is: how to know is the user is previous logued, so 
the helper just validate user/password and later ALLOW to continue.

Regards.


El 3/9/19 a las 09:41, Amos Jeffries escribi?:
> On 3/09/19 10:35 pm, jmperrote wrote:
>> Hello we have a helper to validate users on squid reverse proxy, and
>> have a problem on the first validation time !!
>>
>> On a normal day the first validation, when a user open the client
>> browser squid invoque the pop/up and users insert user/password correct
>> to validate, and later squid
>>
>> apparently run the helper requesting again the user and password.
>>
>> I need help to know it is possible to identify when a users run on first
>> time.
> Users cannot be identified until they provide credentials.
>
> This being a reverse-proxy means to the Browser it is no different than
> any web server. No sane software will ever blindly assume that the users
> LAN account credentials are going to be valid when connecting to a
> random web server.
>
> Thus the Browser needs to have stored credentials in its password
> manager for that 'website' being hosted by your proxy, or use the popup
> to discover them when it starts to do traffic there.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Sep  3 13:33:58 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 01:33:58 +1200
Subject: [squid-users] Cant open some HTTPS with Squid 4.8
In-Reply-To: <1567511229549-0.post@n4.nabble.com>
References: <1567511229549-0.post@n4.nabble.com>
Message-ID: <881f770c-e591-9475-bd52-d604d9847f40@treenet.co.nz>

On 3/09/19 11:47 pm, KOTOXJle6 wrote:
> Im trying to setup Squid 4.8 on Ubuntu 18.04 LTS with HTTPS redirecting to
> squid error page for sites in ACL's. Yesterday i faced major problem HTTPS
> sites doesnt open normally in IE11/EDGE and show blank page only + squid
> replace certificate. If i tap F5, sometimes site opens like it should and
> certificate replacement doesnt happen...and it works not for all sites. I
> couldn't pinpoint the dependencies. I also can open some sites like
> rambler.ru, kanobu.ru, alexa.com normally. The most interesting thing is
> that other browsers like Chrome, FF and even Opera open all sites like it
> should and spoof cert + redirect to error page only if site persist in ACL.
> 

Huh? what is "only if site persist in ACL" meaning?


> What i already did:
> - Disabled IPv6 on Squid host
> - Disabled/Enabled TLS in IE in any variations
> - Disabled SPDY/3
> 
> Bump settings in squid.conf:
> 
> /http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/squidCA.pem
> ssl_bump peek all/
> 
> I have this errors in /var/log/squid/cache.log
> 
> /ERROR: negotiating TLS on FD 46: error:1425F175:SSL
> routines:ssl_choose_client_version:inappropriate fallback (1/-1/0)/
> 
> /ERROR: negotiating TLS on FD 104: error:14094410:SSL
> routines:ssl3_read_bytes:sslv3 alert handshake failure (1/-1/0)
> /
> 
> /ERROR: negotiating TLS on FD 27: error:1423406E:SSL
> routines:tls_parse_stoc_sct:bad extension (1/-1/0)/
> 

Any of the above errors may occur when connecting a specific client to a
specific server. SSL-Bump is riding the fine line of capability/feature
matching between three TLS/SSL librarys and the software using them -
two sets of which are remote machinery.
 All of the above errors (and more) will result in the symptoms you
describe happening. If you don't already know what they mean, use your
favourite search engine. They are quite common and well explained
already by others.

To make any real progress you (or someone) will need to view the TLS
Hello exchanges happening on *both* the client<->Squid and the
Squid<->server connections. I suggest combining a tcpdump capture
(_full_ packets) compared with Squid "debug_options 11,2" info about
what the FD are being used for.
 It may be obvious what is going on when you look at that info.


[ If that process is new to you, then I do highly recommend you take a
little time to become familiar. TLS is a changing environment and
SSL-Bump will be presenting you with more these types of error/problem
that need dealing with in future. ]


> Error in access.log
> 
> /TCP_DENIED/407 4141 CONNECT i.ibb.co:443 - HIER_NONE/- text/html/
> 

407 - HTTP authentication credentials are required for this CONNECT
transaction to happen.

IMPORTANT:  When you have configured proxy authentication and SSL-Bump
you need to be *very* careful to ensure the proxy requests (and gets)
the credentials on  the initial client CONNECT request - *and* that the
credentials remain valid for the entire time the HTTPS tunnel is going
to be open. If their need is only discovered later (or any
refresh/update to them) then all Squid can do is abort the HTTPS with an
error.

Amos


From rousskov at measurement-factory.com  Tue Sep  3 13:43:29 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Sep 2019 09:43:29 -0400
Subject: [squid-users] Cant open some HTTPS with Squid 4.8
In-Reply-To: <1567511229549-0.post@n4.nabble.com>
References: <1567511229549-0.post@n4.nabble.com>
Message-ID: <813b398a-a18e-e5ae-3692-b846d846819e@measurement-factory.com>

On 9/3/19 7:47 AM, KOTOXJle6 wrote:

> I have this errors in /var/log/squid/cache.log
> 
> /ERROR: negotiating TLS on FD 46: error:1425F175:SSL
> routines:ssl_choose_client_version:inappropriate fallback (1/-1/0)/

According to the discussion linked below, these errors may be "normal":
https://security.stackexchange.com/questions/160922/ssl-error-inappropriate-fallback-and-tls-fallback-scsv

To confirm that they are normal, you would need to isolate traffic from
the affected client and see whether its previous connection or tunneling
attempt has failed for some reason.


> /ERROR: negotiating TLS on FD 104: error:14094410:SSL
> routines:ssl3_read_bytes:sslv3 alert handshake failure (1/-1/0)
> /
> 
> /ERROR: negotiating TLS on FD 27: error:1423406E:SSL
> routines:tls_parse_stoc_sct:bad extension (1/-1/0)/

A similar problem was discussed at
http://lists.squid-cache.org/pipermail/squid-users/2019-April/020506.html

If your OpenSSL installation is reasonably fresh, then you will need to
isolate the failure to where you can connect TCP packet samples and/or
Squid debugging logs.


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Sep  3 14:20:26 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 02:20:26 +1200
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <1567516869841-0.post@n4.nabble.com>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
 <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
 <1567516869841-0.post@n4.nabble.com>
Message-ID: <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>

On 4/09/19 1:21 am, fansari wrote:
> I have tested this and it is working.
> 
> This is what I said: when I use this http_port directive then it works.
> 
> So what is still unclear to me is: what is this https_port directive for? I
> understood from one of you answers I found to someone else that this will
> lead to something like double stacked TLS encryption. Is this correct?

It is for;
 a) receiving port 443 traffic from a NAT system,
or
 b) receiving TLS explicit proxy traffic.


> 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/https-port-td4682718.html
> 
> The most important question is: using just the http_port directive - will
> the connection between client and squid still be https (TLS  encrypted)?

The answer you are looking for is both Yes and No.

Traffic to that port must always start with an un-encrypted request. In
the case of HTTP it starts with an unencrypted CONNECT request. The TLS
is embedded within the resulting tunnel.

The traffic which was going to be encrypted stays encrypted. But there
is a non-encrypted portion ahead of it at the transport protocol level.


> This is important to understand for me because we need https because our
> nodejs application will not work with http connections.
> 

If it can rely on a Browser to do the CONNECT tunnel part, then it
should be fine.

Otherwise, if it does everything above TCP itself and can only start
with the TLS handshake. Then you are going to need to use one of the
https_port setups.

Amos


From squid3 at treenet.co.nz  Tue Sep  3 14:28:48 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Sep 2019 02:28:48 +1200
Subject: [squid-users] help with helper
In-Reply-To: <f7575444-862f-8be4-8434-c381af450c20@policia.rionegro.gov.ar>
References: <d9220500-ab7f-3a00-7092-0e41797bc3a4@policia.rionegro.gov.ar>
 <427df498-26b8-f61d-b6f1-660f093da00c@treenet.co.nz>
 <f7575444-862f-8be4-8434-c381af450c20@policia.rionegro.gov.ar>
Message-ID: <24302a32-3954-170f-bb21-6b69530ccede@treenet.co.nz>

On 4/09/19 1:13 am, jmperrote wrote:
> Hello Amos, yes but how can I identified that is on the first request ??
> 

It will be first? but what does first actually mean?
  first this year? first today? first this second?

HTTP is stateless. There is no concept of "second request" etc. outside
of feature which are *not* related to users or useful to you here.

_Every_ request that your config requires credentials to accept, needs
credentials provided or will get a 401/407 response. That is just how
auth works in HTTP. There are likely many of those which are handled by
the Browser without any popup at all.
 To Squid there is no difference between request 1 without credentials
and request 2 without credentials.


> Else squid request to autentificate and later when invoque the helper
> again request to autentificate.

Every time Squid is handed never-before-seen credentials the helper will
be asked to check them.

Every time Squid is handed credentials that are apparently expired, the
helper will be asked to check them.


> 
> I handle recover the user from squid cache (cachmanager) on the helper,
> for asking if the user previous exist, but squid refresh cache and users
> disapearing time to time.

Yes. Computers do not have infinite memory. Things that are clearly
obsolete are thrown away after a reasonable time.


To make credentials stick around longer you can do two things;

 1) increase their TTL. The longer they are considered valid the longer
they are retained as possibly useful.

 Pros: they stick around. Less CPU load on the auth system.

 Cons: they stick around. Increased memory usage. Reduced ability to
change passwords. Reduced ability to kick malicious users off the proxy
by disabling hacked credentials.


 2) increase the garbage collection interval Squid uses. This keeps
obsolete logins around longer.

 Pros: more known logins.

 Cons: more memory used storing logins.


Both have the possibility/risk that users "login session" goes longer
than you might be expecting.

For example; if set to 10hrs (one working day). A user may "logout" late
one night, then re-login early the next day (9hrs of sleep later) and be
seen by Squid as having continued the same login started yesterday.
 Even 2hrs is too long to cover lunch breaks etc.
 Up to you of course, just consider what type of activities may be
problematic for your system for any given time range.

> 
> The exact question is: how to know is the user is previous logued, so
> the helper just validate user/password 

Yes.

> and later ALLOW to continue.
> 

No.

Authentication vs Authorization. There is a thin difference, but it is
very important to understanding these things going on.

The auth helper only does Authentication - checking that credentials are
*correct*.

Squid ACLs do the Authorization - allow/deny actions. Which may (or not)
be based on whether credentials are correct / authenticated.



HTH
Amos


From nabil1385 at gmail.com  Tue Sep  3 14:59:54 2019
From: nabil1385 at gmail.com (fansari)
Date: Tue, 3 Sep 2019 09:59:54 -0500 (CDT)
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
 <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
 <1567516869841-0.post@n4.nabble.com>
 <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>
Message-ID: <1567522794035-0.post@n4.nabble.com>

OK - I cannot figure out the whole requirement right now.

In case it will not not work like this: with a) you mean "intercept" and
with b) "tproxy"?

Which of these scenarios would you recommend in case http_port will not do
for us?




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From nabil1385 at gmail.com  Tue Sep  3 16:44:25 2019
From: nabil1385 at gmail.com (fansari)
Date: Tue, 3 Sep 2019 11:44:25 -0500 (CDT)
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
 <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
 <1567516869841-0.post@n4.nabble.com>
 <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>
Message-ID: <1567529065185-0.post@n4.nabble.com>

Seems that intercept is easier than tproxy.

I have now this config:

acl wifi_net src  xxx.xxx.0.0/24
acl our_proxy localip  xxx.xxx.0.1/32
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl step1 at_step SslBump1
acl bumpedPorts myportname 3129
http_access deny !Safe_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
http_access allow localhost
http_access allow wifi_net
http_access allow CONNECT bumpedPorts
http_access allow CONNECT our_proxy
http_access allow CONNECT wifi_net
http_access deny all
http_port 3130
http_port 3128 intercept
https_port 3129 intercept ssl-bump \
  cert=/etc/squid/certs/squid-ca-cert-key.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
ssl_bump peek step1 
ssl_bump bump all
ssl_bump server-first
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
cache_dir ufs /var/spool/squid 1024 16 256
debug_options ALL,2
coredump_dir /var/spool/squid
refresh_pattern .               30      20%     1440 override-expire

When I add these rules on the server in /etc/firewalld/direct.xml

<rule ipv="ipv4" table="nat" chain="PREROUTING" priority="0">-i wlan1 -p tcp
-s xxx.xxx.0.0/24 --dport 80 -j DNAT --to xxx.xxx.0.1:3128</rule>
<rule ipv="ipv4" table="nat" chain="PREROUTING" priority="0">-i wlan1 -p tcp
-s xxx.xxx.0.0/24 --dport 443 -j DNAT --to xxx.xxx.0.1:3129</rule>

then I receive the content and also see a TCP_MEM_MISS or TCP_MEM_HIT in the
access.log.

So maybe this could be a scenario to use in case http_port does not work.

>From this server itself the squid seems not to be used - but this is
probably more routing than squid stuff.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From 23dmitry23 at gmail.com  Tue Sep  3 16:50:01 2019
From: 23dmitry23 at gmail.com (KOTOXJle6)
Date: Tue, 3 Sep 2019 11:50:01 -0500 (CDT)
Subject: [squid-users] Cant open some HTTPS with Squid 4.8
In-Reply-To: <881f770c-e591-9475-bd52-d604d9847f40@treenet.co.nz>
References: <1567511229549-0.post@n4.nabble.com>
 <881f770c-e591-9475-bd52-d604d9847f40@treenet.co.nz>
Message-ID: <1567529401506-0.post@n4.nabble.com>

Amos Jeffries wrote
> Huh? what is "only if site persist in ACL" meaning?

Ill try to explain by example. I have 2 ACL - blockvideo and blockvpn, they
contain urls for video hostings and vpn services. This ACLs appyed to domain
groups blockvideo and blockvpn. I have 2 users - adm1 and user1. User1 is a
member of both groups and adm1 is not. 
When user1 trying to open any sites from acls (even https), he should be
redirected to squid error page wich tells him that access restricted and he
should contact local administrator.


Amos Jeffries wrote
> Any of the above errors may occur when connecting a specific client to a
> specific server. SSL-Bump is riding the fine line of capability/feature
> matching between three TLS/SSL librarys and the software using them -
> two sets of which are remote machinery.
>  All of the above errors (and more) will result in the symptoms you
> describe happening. If you don't already know what they mean, use your
> favourite search engine. They are quite common and well explained
> already by others.

At this point I realized that the problem is not in the browser settings.


Amos Jeffries wrote
> To make any real progress you (or someone) will need to view the TLS
> Hello exchanges happening on *both* the client<->Squid and the
> Squid<->server connections. I suggest combining a tcpdump capture
> (_full_ packets) compared with Squid "debug_options 11,2" info about
> what the FD are being used for.
>  It may be obvious what is going on when you look at that info.
> 
> 
> [ If that process is new to you, then I do highly recommend you take a
> little time to become familiar. TLS is a changing environment and
> SSL-Bump will be presenting you with more these types of error/problem
> that need dealing with in future. ]

Thank you. Missed that section somehow. Ill try to do this.



Amos Jeffries wrote
> 407 - HTTP authentication credentials are required for this CONNECT
> transaction to happen.
> 
> IMPORTANT:  When you have configured proxy authentication and SSL-Bump
> you need to be *very* careful to ensure the proxy requests (and gets)
> the credentials on  the initial client CONNECT request - *and* that the
> credentials remain valid for the entire time the HTTPS tunnel is going
> to be open. If their need is only discovered later (or any
> refresh/update to them) then all Squid can do is abort the HTTPS with an
> error.

It looks like problem starts at first step of ssl_bump peek. If i understand
it right, problems apper when squid trying to get inside https session to
decide should it bump (if site in acl and user in group) or splice this
connection.  


Alex Rousskov wrote
> According to the discussion linked below, these errors may be "normal":
> https://security.stackexchange.com/questions/160922/ssl-error-inappropriate-fallback-and-tls-fallback-scsv
> 
> To confirm that they are normal, you would need to isolate traffic from
> the affected client and see whether its previous connection or tunneling
> attempt has failed for some reason.
> 
> A similar problem was discussed at
> http://lists.squid-cache.org/pipermail/squid-users/2019-April/020506.html
> 
> If your OpenSSL installation is reasonably fresh, then you will need to
> isolate the failure to where you can connect TCP packet samples and/or
> Squid debugging logs.

Thanks for links. Squid -v shows that this binary uses OpenSSL 1.1.1  11 Sep
2018.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Wed Sep  4 08:03:09 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 4 Sep 2019 10:03:09 +0200
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <1567529065185-0.post@n4.nabble.com>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
 <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
 <1567516869841-0.post@n4.nabble.com>
 <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>
 <1567529065185-0.post@n4.nabble.com>
Message-ID: <20190904080309.GB28846@fantomas.sk>

On 03.09.19 11:44, fansari wrote:
>Seems that intercept is easier than tproxy.

FYI, tproxy means incercept AND changing outgoing IP address to the IP
address of the original client.

yes, intercept alone is easier, because tproxy means implementing
intercepting and something in addition.

nowadays when most of clients are behing NAT firewall, tproxy may not be so
important, since the source IP will often get translated to the same ip on
the gateway, no matter which IP (proxy or client) goes through it.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Posli tento mail 100 svojim znamim - nech vidia aky si idiot
Send this email to 100 your friends - let them see what an idiot you are


From nabil1385 at gmail.com  Wed Sep  4 09:54:54 2019
From: nabil1385 at gmail.com (fansari)
Date: Wed, 4 Sep 2019 04:54:54 -0500 (CDT)
Subject: [squid-users] HEAD requests: pass through?
Message-ID: <1567590894512-0.post@n4.nabble.com>

If my understanding is correct when the client already has the content it
sends a HEAD request to the squid and it will be checked whether the content
on the squid is newer than the local cache of the client.

Is it possible to configure the squid in a way that such requests are not
answered by the squid itself but passed through to the internet? Because it
may happen that the content on the internet has changed - in this case the
client would compare against the older content from the suqid cache (this
should be avoided).

The scenario should be: 

1. If the client does not have the content ask the squid and
  1a)  If it has the content take this. 
  1b) If it does not have the content get it from the internet. 

2. If the client already has the content and just sends a HEAD request pass
this to the internet in order to check against the newest version.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From wesley.peng1 at googlemail.com  Wed Sep  4 10:03:08 2019
From: wesley.peng1 at googlemail.com (Wesley Peng)
Date: Wed, 4 Sep 2019 18:03:08 +0800
Subject: [squid-users] HEAD requests: pass through?
In-Reply-To: <1567590894512-0.post@n4.nabble.com>
References: <1567590894512-0.post@n4.nabble.com>
Message-ID: <60f27701-1f39-835c-19a5-b6a5325b728c@googlemail.com>



on 2019/9/4 17:54, fansari wrote:
> Is it possible to configure the squid in a way that such requests are not
> answered by the squid itself but passed through to the internet? Because it
> may happen that the content on the internet has changed - in this case the
> client would compare against the older content from the suqid cache (this
> should be avoided).

Can you setup squid to no-cache such HEAD requests?

regards.


From squid3 at treenet.co.nz  Wed Sep  4 13:54:40 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Sep 2019 01:54:40 +1200
Subject: [squid-users] HEAD requests: pass through?
In-Reply-To: <1567590894512-0.post@n4.nabble.com>
References: <1567590894512-0.post@n4.nabble.com>
Message-ID: <7747c75f-a1d1-1ef4-7723-fae18bf858ed@treenet.co.nz>

On 4/09/19 9:54 pm, fansari wrote:
> If my understanding is correct when the client already has the content it
> sends a HEAD request to the squid and it will be checked whether the content
> on the squid is newer than the local cache of the client.

Maybe.

HTTP/1.0-only clients are likely to do so. Since all they can really
depend on is the small set of 1.0 protocol features.

HTTP/1.1 clients (the majority these days) have access to conditional
requests and may send a GET request with conditions about what it wants.


> 
> Is it possible to configure the squid in a way that such requests are not
> answered by the squid itself but passed through to the internet?

It is BUT ...

> Because it
> may happen that the content on the internet has changed - in this case the
> client would compare against the older content from the suqid cache (this
> should be avoided).
> 

For Squid to provide an answer from its cached object that object needs
to have information telling Squid what to do when constructing such an
response. There is no need to setup hard-coded behaviour with dependency
on a particular request method.


If the resource is likely to change, then the origin server is supposed
to be sending the Cache-Control header with option to tells the proxy to
always check for updates before answering any client, or to tell the
proxy an interval to perform re-checks.



> The scenario should be: 
> 
> 1. If the client does not have the content ask the squid and
>   1a)  If it has the content take this. 
>   1b) If it does not have the content get it from the internet. 
> 

You just described HTTP caching in a very simplistic way. For more
details you can read the specification itself:
 <https://tools.ietf.org/html/rfc7234>


> 2. If the client already has the content and just sends a HEAD request pass
> this to the internet in order to check against the newest version.
> 

HEAD method _can_ be used for this, but its intended main purpose is for
finding out details about an object the client *does not* already have.
Without incurring the time and bandwidth costs of fetching the entire thing.

This relaying every request is terribly inefficient thing to be doing.
One of the core functions of a proxy cache is to *reduce*
traffic/bandwidth to upstream servers. The HTTP conditional requests and
caching controls are far more efficient at doing these updates on an
as-needed basis.

For more details on conditional requests see:
 <https://tools.ietf.org/html/rfc7232>


Also, HEAD responses are not cacheable. So by relaying the HEAD requests
you are guaranteeing that any update to that resource does not get
cached by the proxy until much later when the client is already waiting
for it to arrive. If you let the proxy decide what to send the server it
can choose to send a conditional request itself to update the cached
content ready for when these clients do their followup GET.


If you want to still insist on doing this weird thing with HEAD requests
you can configure:
 acl HEAD method HEAD
 send_hit deny HEAD


HTH
Amos


From squid3 at treenet.co.nz  Wed Sep  4 14:05:09 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Sep 2019 02:05:09 +1200
Subject: [squid-users] cannot access squid with https_port: 403
In-Reply-To: <1567522794035-0.post@n4.nabble.com>
References: <1567500419266-0.post@n4.nabble.com>
 <833be8ae-5eff-1e26-3408-ef112dcaebfe@treenet.co.nz>
 <1567513765050-0.post@n4.nabble.com>
 <7263d237-0508-844b-d210-cd2fa3c645e8@treenet.co.nz>
 <1567516869841-0.post@n4.nabble.com>
 <bfd598d7-4d5d-873c-d1a5-2ca0c8d5e801@treenet.co.nz>
 <1567522794035-0.post@n4.nabble.com>
Message-ID: <8c4a6ca8-12f2-7f99-7f36-6d647bd9b14b@treenet.co.nz>

On 4/09/19 2:59 am, fansari wrote:
> OK - I cannot figure out the whole requirement right now.
> 
> In case it will not not work like this: with a) you mean "intercept" and
> with b) "tproxy"?
> 

No for (b) I mean "TLS explicit". New connections from clients start
with TLS handshake immediately, no CONNECT message or HTTP layering
going on. Just a client talking to a proxy - explicitly over TLS instead
of TCP.



> Which of these scenarios would you recommend in case http_port will not do
> for us?
> 

I cannot answer that without details of the client application and how
it performs connection setup. You seem reluctant to go into detail over
that, so I mentioned all the options.


Amos


From nabil1385 at gmail.com  Thu Sep  5 09:52:53 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 5 Sep 2019 04:52:53 -0500 (CDT)
Subject: [squid-users] simultanous requests: collapsed_forwarding
Message-ID: <1567677173515-0.post@n4.nabble.com>

In our scenario it might/will happen that clients will request the same
resources simultaneously.

I ran a test where I start one download with curl and with short delay
(about 1s) I request the same content by another client.

1567673769.781  12123 xxx.xxx.0.1 TCP_MISS/200 15553655 GET
https://xxx/1.mp4 - HIER_NONE/- video/mp4
1567673769.782  13992 xxx.xxx.0.239 TCP_MISS/200 15553655 GET
https://xxx/1.mp4 - HIER_DIRECT/xxx.xxx.xxx.xxx video/mp4

>From the documentation I found that there is a setting called
"collapsed_forwarding" which when enabled gives the behaviour we want to
achieve: let the first request finish with download and then handle the
others from squid cash.

Since our scenario is not time critical this would be the behaviour we want.

I am just wondering about one thing: I did not enable this setting (I also
tried to set it to "off" explicitly) and nevertheless it already behaves
this way.

When I interpret the logs correctly the first line indicates a waiting and
the second shows the download.

With "collapsed_forwarding on" it behaves in the same way as with "off". To
me it looks like as if the collapsing is used regardless whether it is
enabled or not.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Sep  5 11:00:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Sep 2019 23:00:18 +1200
Subject: [squid-users] simultanous requests: collapsed_forwarding
In-Reply-To: <1567677173515-0.post@n4.nabble.com>
References: <1567677173515-0.post@n4.nabble.com>
Message-ID: <fa20ac4b-d78b-d0d4-7a4e-fe0b08369c76@treenet.co.nz>

On 5/09/19 9:52 pm, fansari wrote:
> In our scenario it might/will happen that clients will request the same
> resources simultaneously.
> 
> I ran a test where I start one download with curl and with short delay
> (about 1s) I request the same content by another client.
> 
> 1567673769.781  12123 xxx.xxx.0.1 TCP_MISS/200 15553655 GET
> https://xxx/1.mp4 - HIER_NONE/- video/mp4
> 1567673769.782  13992 xxx.xxx.0.239 TCP_MISS/200 15553655 GET
> https://xxx/1.mp4 - HIER_DIRECT/xxx.xxx.xxx.xxx video/mp4
> 
> From the documentation I found that there is a setting called
> "collapsed_forwarding" which when enabled gives the behaviour we want to
> achieve: let the first request finish with download and then handle the
> others from squid cash.

Not quite, collapsing does not let the first request complete. It starts
sending to all waiting clients as soon as it has response data to
deliver them.


> 
> Since our scenario is not time critical this would be the behaviour we want.
> 
> I am just wondering about one thing: I did not enable this setting (I also
> tried to set it to "off" explicitly) and nevertheless it already behaves
> this way.
> 
> When I interpret the logs correctly the first line indicates a waiting and
> the second shows the download.

I agree, that is what it looks like. The request that finished+logged
first (by 0.001 seconds) took 0.2 seconds less time and did not involve
server contact (HIER_NONE), yet was a "MISS".

> 
> With "collapsed_forwarding on" it behaves in the same way as with "off". To
> me it looks like as if the collapsing is used regardless whether it is
> enabled or not.
> 

Looks to me like you may have found a bug. What exact version of Squid
are you using? (output of "squid -v" please).

If you are using an older Squid can you try the latest v4 and/or
development version to see if the odd behaviour still happens?

Amos


From nabil1385 at gmail.com  Thu Sep  5 11:46:37 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 5 Sep 2019 06:46:37 -0500 (CDT)
Subject: [squid-users] simultanous requests: collapsed_forwarding
In-Reply-To: <fa20ac4b-d78b-d0d4-7a4e-fe0b08369c76@treenet.co.nz>
References: <1567677173515-0.post@n4.nabble.com>
 <fa20ac4b-d78b-d0d4-7a4e-fe0b08369c76@treenet.co.nz>
Message-ID: <1567683997493-0.post@n4.nabble.com>

I have compiled squid 3.5.23 on a Debian Stretch (because I need SSL). 

This means i could enable the collapsed_forwarding feature because this is
what we need (we don't have a case where we need to disable it - this could
be interesting if you have a scenario where speed is more important than
traffic limitation).

squid -v gives this output:

Squid Cache: Version 3.5.23
Service Name: squid
Raspbian linux
configure options:  '--build=arm-linux-gnueabihf' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g
-O2 -fdebug-prefix-map=/home/pi/squid3-3.5.23=. -fstack-protector-strong
-Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2
-Wl,-z,relro -Wl,-z,now -Wl,--as-needed' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--enable-ssl'
'--enable-ssl-crtd' '--with-openssl' '--enable-build-info=Raspbian linux'
'--enable-linux-netfilter' 'build_alias=arm-linux-gnueabihf' 'CFLAGS=-g -O2
-fdebug-prefix-map=/home/pi/squid3-3.5.23=. -fstack-protector-strong
-Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now
-Wl,--as-needed' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2
-fdebug-prefix-map=/home/pi/squid3-3.5.23=. -fstack-protector-strong
-Wformat -Werror=format-security'





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Thu Sep  5 12:03:29 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 5 Sep 2019 14:03:29 +0200
Subject: [squid-users] perl script for translating log times
Message-ID: <20190905120329.GA2528@fantomas.sk>

Hello,

if anyone is interested in translating log timestamps, here it is:

perl -pe 's/(\d+\.\d+)(\s+)(\d+)/localtime($1-$3\/1000)." - ".localtime($1)/e'


This translates log timestamps from:

1567673687.237 1737748

e.g. date of end, time in microseconds into format of:

date of start - date of end

e.g. from:

1567673687.237 1737748 192.168.X.Y TCP_TUNNEL/200 2758742 CONNECT A.B.C.D:8089 - HIER_DIRECT/A.B.C.D -

to:

Thu Sep  5 10:25:49 2019 - Thu Sep  5 10:54:47 2019 192.168.X.Y TCP_TUNNEL/200 2758742 CONNECT A.B.C.D:8089 - HIER_DIRECT/A.B.C.D -

in local time.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fighting for peace is like fucking for virginity...


From uhlar at fantomas.sk  Thu Sep  5 13:24:34 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 5 Sep 2019 15:24:34 +0200
Subject: [squid-users] simultanous requests: collapsed_forwarding
In-Reply-To: <1567683997493-0.post@n4.nabble.com>
References: <1567677173515-0.post@n4.nabble.com>
 <fa20ac4b-d78b-d0d4-7a4e-fe0b08369c76@treenet.co.nz>
 <1567683997493-0.post@n4.nabble.com>
Message-ID: <20190905132434.GB2528@fantomas.sk>

On 05.09.19 06:46, fansari wrote:
>I have compiled squid 3.5.23 on a Debian Stretch (because I need SSL).
>
>This means i could enable the collapsed_forwarding feature because this is
>what we need (we don't have a case where we need to disable it - this could
>be interesting if you have a scenario where speed is more important than
>traffic limitation).

in fact, you can use collapsed forwarding with stock debian 9 squid.

However, I miss your question here.

>
>squid -v gives this output:
>
>Squid Cache: Version 3.5.23
>Service Name: squid
>Raspbian linux

doesn't look like debian, does it? :)
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
They that can give up essential liberty to obtain a little temporary
safety deserve neither liberty nor safety. -- Benjamin Franklin, 1759


From rousskov at measurement-factory.com  Thu Sep  5 14:24:23 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 5 Sep 2019 10:24:23 -0400
Subject: [squid-users] simultanous requests: collapsed_forwarding
In-Reply-To: <1567677173515-0.post@n4.nabble.com>
References: <1567677173515-0.post@n4.nabble.com>
Message-ID: <a3c74d5d-9248-9915-0d8d-c3f2fe80ed03@measurement-factory.com>

On 9/5/19 5:52 AM, fansari wrote:

> I ran a test where I start one download with curl and with short delay
> (about 1s) I request the same content by another client.
> 
> 1567673769.781 12123 xxx.xxx.0.1   TCP_MISS/200 15553655 ... HIER_NONE
> 1567673769.782 13992 xxx.xxx.0.239 TCP_MISS/200 15553655 ... HIER_DIRECT
> 
> From the documentation I found that there is a setting called
> "collapsed_forwarding" which when enabled gives the behaviour we want to
> achieve: let the first request finish with download and then handle the
> others from squid cash.

Regardless of the collapsed_forwarding setting, the second request never
waits for the first request to finish downloading the response. Instead:

* When collapsed forwarding is prohibited: There is no waiting at all.

* When collapsed forwarding is allowed: The second request waits until
the response headers arrive.


> I am just wondering about one thing: I did not enable this setting (I also
> tried to set it to "off" explicitly) and nevertheless it already behaves
> this way.

You cannot make that conclusion by examining v3-4 access log records.
Latest (unreleased) Squids mark collapsed requests using a CF tag (e.g.
TCP_CF_MISS): https://github.com/squid-cache/squid/commit/d2a6dcb


> When I interpret the logs correctly the first line indicates a waiting and
> the second shows the download.

Not necessarily:

1. The first access.log record could be a cache hit (by the second
transaction) of an in-progress download initiated by the first
transaction. You build Squid with --enable-delay-pools which marks such
transactions as misses after bug 1001 changes:
https://bugs.squid-cache.org/show_bug.cgi?id=1001

2. The second access.log record can be a regular cache miss (by the
first transaction). It just happened to end a bit later so it was logged
second. You can check request:record mapping by looking at logged client
IPs.



> With "collapsed_forwarding on" it behaves in the same way as with "off". To
> me it looks like as if the collapsing is used regardless whether it is
> enabled or not.

Yes, but probably because there are no collapsable (in Squid
terminology) requests in your tests.

To test collapsed forwarding, you need Squid to receive the second
request _before_ Squid receives the response headers for the first request.


HTH,

Alex.


From dario.basset at unimi.it  Fri Sep  6 07:50:08 2019
From: dario.basset at unimi.it (Dario Basset)
Date: Fri, 06 Sep 2019 09:50:08 +0200
Subject: [squid-users] Squid CAS integration
Message-ID: <6da0981a20a71.5d722bd0@unimi.it>

My institution has been asked to integrate Squid and CAS. We want to integrate Squid and CAS in its simplest way, that is:
1) redirect the navigation to the CAS site, 
2) let the user input login/password, 
3) then, after successfull login, check with PHP all nnecessary permissions, 
4) proceed with Squid Proxy. 

I can't understand how to code Squid configuration and PHP helpers.
I have seen here http://squid-web-proxy-cache.1019090.n4.nabble.com/Need-help-for-ACL-Authentication-web-Form-Cookies-td4555576.html


But I cannot understand how to make it work. Can you please show me a link to simple example? Or tell me where are samples sources with PHP helpers and SQUID configuration in order ro have the full example working? 

Dario Basset

<signatureafterquotedtext>Dario Basset dario.basset at unimi.it 
Direzione Servizio bibliotecario d?Ateneo

</signatureafterquotedtext>



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190906/08f47998/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.jpg
Type: image/jpeg
Size: 15760 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190906/08f47998/attachment.jpg>

From squid3 at treenet.co.nz  Fri Sep  6 09:16:17 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Sep 2019 21:16:17 +1200
Subject: [squid-users] Squid CAS integration
In-Reply-To: <6da0981a20a71.5d722bd0@unimi.it>
References: <6da0981a20a71.5d722bd0@unimi.it>
Message-ID: <8fd01203-9687-956e-aef7-e5d7c8a7e63c@treenet.co.nz>

On 6/09/19 7:50 pm, Dario Basset wrote:
> My institution has been asked to integrate Squid and CAS. We want to
> integrate Squid and CAS in its simplest way, that is:

Details about this CAS ?
 Does it have a specific name?
  "CAS" is like saying "proxy" - it is a type.

 What type(s) of authentication is it doing?
 What APIs does it provide for checking credentials validity?
 What APIs does it provide for initial user login?

Note that all of those 'What ...' questions are plural. Authenticators
tend to have multiple APIs for each activity.


> 1) redirect the navigation to the CAS site,
> 2) let the user input login/password,
> 3) then, after successfull login, check with PHP all nnecessary
> permissions,

FWIW: my advice is to avoid PHP for Squid helpers. That language has
problems keeping helpers running long-term.
 <https://wiki.squid-cache.org/Features/AddonHelpers#What_language_are_helper_meant_to_be_written_in.3F>



> 4) proceed with Squid Proxy.
> 
> I can't understand how to code Squid configuration and PHP helpers.
> I have seen here
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Need-help-for-ACL-Authentication-web-Form-Cookies-td4555576.html
> 
> But I cannot understand how to make it work. Can you please show me a
> link to simple example?

All the helpers called "fake" are examples of how to write helpers for
their Squid helper interface. Which is essentially the same these days
with a (somewhat) unified protocol they all speak.


> Or tell me where are samples sources with PHP
> helpers and SQUID configuration in order ro have the full example working?
> 

Not without the details asked for above. The conversation you found
David and I are mentioning BerkleyDB and SQL helpers. Those are the
"CAS" we use. The squid.conf part is essentially what you see in that
thread.

You will need a helper to access whatever the CAS database is (via any
API it provides for that access).


Amos


From dario.basset at unimi.it  Fri Sep  6 09:36:46 2019
From: dario.basset at unimi.it (Dario Basset)
Date: Fri, 06 Sep 2019 11:36:46 +0200
Subject: [squid-users] Squid CAS integration
In-Reply-To: <8fd01203-9687-956e-aef7-e5d7c8a7e63c@treenet.co.nz>
References: <6da0981a20a71.5d722bd0@unimi.it>
 <8fd01203-9687-956e-aef7-e5d7c8a7e63c@treenet.co.nz>
Message-ID: <6bd0a80a26a72.5d7244ce@unimi.it>

Thanks. 

-> With CAS I mean the Central Authentication Service, which is supported
here: https://github.com/apereo/cas or here:
https://www.apereo.org/projects/cas It is a system for Single Sign On
authentication with Service Ticket, and it is quite used in Universities. We
want to integrate Squid with CAS auth.
The authentication provided by CAS is based on a mechanism which redirect
user navigation to CAS University site, and proceed only when credentials
are valid. In this way the site that picks the credentials is not an
application site, but it is University CAS itself. The application that uses
University CAS is simply redirecting user navigation, that it takes the
control. 

-> Ok for PHP

-> For what concerns Squid helpers, I saw some examples, but most of those
examples are based never-ending loops that wait for standard input and then
proceed with authentication. In this loop, the credentials are picked by
Squid web server. We do not want this. We want credentials to be inputted in
our CAS portal system. But I don't know how to code configuration file for
Squid and related helpers. 

<signaturebeforequotedtext></signaturebeforequotedtext>
Il 06/09/19 11:16, Amos Jeffries  <squid3 at treenet.co.nz> ha scritto: 
> 
> On 6/09/19 7:50 pm, Dario Basset wrote:
> > My institution has been asked to integrate Squid and CAS. We want to
> > integrate Squid and CAS in its simplest way, that is:
> 
> Details about this CAS ?
> ?Does it have a specific name?
>  "CAS" is like saying "proxy" - it is a type.
> 
> ?What type(s) of authentication is it doing?
> ?What APIs does it provide for checking credentials validity?
> ?What APIs does it provide for initial user login?
> 
> Note that all of those 'What ...' questions are plural. Authenticators
> tend to have multiple APIs for each activity.
> 
> 
> > 1) redirect the navigation to the CAS site,
> > 2) let the user input login/password,
> > 3) then, after successfull login, check with PHP all nnecessary
> > permissions,
> 
> FWIW: my advice is to avoid PHP for Squid helpers. That language has
> problems keeping helpers running long-term.
> ?<https://wiki.squid-cache.org/Features/AddonHelpers#What_language_are_helper_meant_to_be_written_in.3F>
> 
> 
> 
> > 4) proceed with Squid Proxy.
> > 
> > I can't understand how to code Squid configuration and PHP helpers.
> > I have seen here
> > http://squid-web-proxy-cache.1019090.n4.nabble.com/Need-help-for-ACL-Authentication-web-Form-Cookies-td4555576.html
> > 
> > But I cannot understand how to make it work. Can you please show me a
> > link to simple example?
> 
> All the helpers called "fake" are examples of how to write helpers for
> their Squid helper interface. Which is essentially the same these days
> with a (somewhat) unified protocol they all speak.
> 
> 
> > Or tell me where are samples sources with PHP
> > helpers and SQUID configuration in order ro have the full example working?
> > 
> 
> Not without the details asked for above. The conversation you found
> David and I are mentioning BerkleyDB and SQL helpers. Those are the
> "CAS" we use. The squid.conf part is essentially what you see in that
> thread.
> 
> You will need a helper to access whatever the CAS database is (via any
> API it provides for that access).
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
<signatureafterquotedtext>-- 
------------------------------------------------------------
Dario Basset dario.basset at unimi.it 
Direzione Servizio bibliotecario d?Ateneo
Via G. Colombo, 46 02-50315296
------------------------------------------------------------
</signatureafterquotedtext>



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190906/d7864e51/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.jpg
Type: image/jpeg
Size: 15760 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190906/d7864e51/attachment.jpg>

From squid3 at treenet.co.nz  Fri Sep  6 12:48:42 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 7 Sep 2019 00:48:42 +1200
Subject: [squid-users] Squid CAS integration
In-Reply-To: <6bd0a80a26a72.5d7244ce@unimi.it>
References: <6da0981a20a71.5d722bd0@unimi.it>
 <8fd01203-9687-956e-aef7-e5d7c8a7e63c@treenet.co.nz>
 <6bd0a80a26a72.5d7244ce@unimi.it>
Message-ID: <9a3e1b8c-f8be-67de-d350-655c399d8ed1@treenet.co.nz>

On 6/09/19 9:36 pm, Dario Basset wrote:
> Thanks.
> 
> -> ?With CAS I mean the Central Authentication Service, which is supported
> here: https://github.com/apereo/cas ?or here:
> https://www.apereo.org/projects/cas ? ? It is a system for Single Sign On
> authentication with Service Ticket, and it is quite used in Universities. We
> want to integrate Squid with CAS auth.
> The authentication provided by CAS is based on a mechanism which redirect
> user navigation to CAS University site, and proceed only when credentials
> are valid. In this way the site that picks the credentials is not an
> application site, but it is University CAS itself. The application that uses
> University CAS is simply redirecting user navigation, that it takes the
> control.
> 
> -> ?Ok for PHP
> 
> -> ?For what concerns Squid helpers, I saw some examples, but most of those
> examples are based never-ending loops that wait for standard input and then
> proceed with authentication. In this loop, the credentials are picked by
> Squid web server. We do not want this. We want credentials to be inputted in
> our CAS portal system. But I don't know how to code configuration file for
> Squid and related helpers.

Ah, you are misunderstanding the purpose of the helpers.

In order to handle a clients traffic Squid needs to process the rules
you configured. Sometimes those rules need the answers to questions
Squid cannot answer by itself; eg "are the user credentials valid", or
"should this URL be redirected elsewhere?"

What Squid does then is send a query to the helper which can answer that
question, and the helper responds with OK/ERR (yes or no) and maybe some
parameters Squid can use to continue the processing. To the helper HTTP
traffic is just an infinite series of "Can I do X?" questions from Squid.


Looking at the CAS documentation, they have helpfully provided a message
flow diagram for how web traffic needs to be handled and authenticated
against the CAS server.

see "Web Flow Diagram" at
 <https://apereo.github.io/cas/4.2.x/protocol/CAS-Protocol.html>

Squid and  URL-redirect helper would be doing the actions in the
"Protected App" column, and generating the 302 responses.

[ It occurs to me, that if you ask about the CAS community you may find
someone who already did this integration and has a Squid helper. Even
though CAS has not been mentioned here before. ]


Your squid.conf would look like this:

 # check every URL to see if 302 redirect is needed
 url_rewrite_helper /path/to/your/helper

 # tell the URL helper about any Cookie header
 url_rewrite_extras %#>h{Cookie}

 # do not try to redirect clients visiting the CAS login page
 acl CAS_server dstdomain cas.example.com
 url_rewrite_access deny !CAS_server

 # add a Set-Cookie header supplied by the URL helper (if any)
 acl CAS_cookie note cas-cookie_
 reply_header_add Cookie "%note{cas-cookie_}" CAS_cookie


For every HTTP request Squid handles, it will send your helper one line
ending with a newline (\n) character. That line will contain the URL the
client was trying to visit, followed by the Cookie header(s) straight
from HTTP message with URL / %-encoding.


The helper needs to respond back with a line containing some values
depending on which of the yellow boxes is happening.

* When there is no CAS Cookie in the ones supplied by Squid (or Squid
cannot deliver any), *and* no CAS ticket on the URL then the first / top
yellow box is happening.
 The helper should produce:

  OK status=302 url=https://cas.example.com/cas/login?service=...

where "..." is the %-encoded value of URL Squid said the client was
trying to fetch. Notice how these details correlate to what the CAS
diagram first column is saying.


* When there is no Cookie etc and there _is_ a ticket parameter. The
second yellow box has been reached.
 The helper needs to do the background GET request to verify the ticket
with the CAS server. GET /serviceValidate request to the CAS server and
process the response to find the value needing to go into Set-Cookie header.
 When it has those details it should produce:

  OK status=302 url=... cas-cookie_=BLAH

where "..." is the URL the client was trying to fetch but without the
"ticket=" parameter; and "BLAH" is the string to put in the Cookie header.

NOTE: since Cookie header values contain quotes, whitespace and special
characters the BLAH string should be %-encoded. Squid should decode it
before use.


* When there is a CAS server Cookie and no ticket parameter. The third
(or fourth) yellow box has been reached. The helper should just validate
the cookie.

If the Cookie validates the helper should produce:

  ERR

Despite what the letters may imply this just means not to redirect.

If the Cookie failed to validate. I guess you redirect to the CAS server
login page as per earlier at the first yellow box, maybe? the diagram
does not mention that situation.



PS. If you are interested the full details of helper protocol for URL
redirector is at:
 <https://wiki.squid-cache.org/Features/AddonHelpers#URL_manipulation>

Since the Cookie handling could be quite slow, you might want to use the
concurrency channels feature to allow parallel processing by the helper.
Get the helper working with the basics first though.

Amos


From nabil1385 at gmail.com  Fri Sep  6 13:21:14 2019
From: nabil1385 at gmail.com (fansari)
Date: Fri, 6 Sep 2019 08:21:14 -0500 (CDT)
Subject: [squid-users] usage of etag
Message-ID: <1567776074863-0.post@n4.nabble.com>

Assuming my web application sends headers with information like this:

Cache-Control: public, max-age=0
Last-Modified: Thu, 22 Aug 2019 08:50:29 GMT
ETag: W/"ed52a4-16cb8852696"

Now I was under the impression I could do something like this with curl:

ETAG='ed52a4-16cb8852696'
curl --head --header 'if-none-match: W/'\"${ETAG}\" --cacert ${CERT} --proxy
${PROXY} --proxy-insecure --insecure ${URL}

and then set ETAG either to the real ETag (then I would expect a HIT) or to
some other value (then I would expect a MISS and the content reloaded again
from the internet).

Nevertheless when I run curl with wrong etag I get HITs. 

Does squid not work this way or have I to configure something special?




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From nabil1385 at gmail.com  Fri Sep  6 14:34:10 2019
From: nabil1385 at gmail.com (fansari)
Date: Fri, 6 Sep 2019 09:34:10 -0500 (CDT)
Subject: [squid-users] usage of etag
In-Reply-To: <1567776074863-0.post@n4.nabble.com>
References: <1567776074863-0.post@n4.nabble.com>
Message-ID: <1567780450580-0.post@n4.nabble.com>

Problem was the --head option. 

For real downloading it works.

ETAG='ed52a4-16cb8852696'
curl --header 'if-none-match: W/'\"${ETAG}\" --cacert ${CERT} --proxy
${PROXY} --proxy-insecure --insecure ${URL} > ${FILE}

With correct ETag the download is an emtpy file (TCP_INM_HIT/304).

With incorrect ETag the content is downloaded into the file
(TCP_MEM_HIT/200).




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ser.vieira at gmail.com  Sat Sep  7 10:41:48 2019
From: ser.vieira at gmail.com (=?utf-8?Q?S=C3=A9rgio_Vieira?=)
Date: Sat, 7 Sep 2019 11:41:48 +0100
Subject: [squid-users] squid.config
In-Reply-To: <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
 <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
 <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>
Message-ID: <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>

Hello,

I?m trying to ignore some domains (like facebook.com, youtube.com, etc), meaning that I don?t want logs from this domains. 

I already inserted in the config file the following:
acl nolog dstdomain ?/etc/squid/acl-nolog.txt?
access_log none nolog

In the txt, i have:
.facebook.com
.instagram.com

But it?s not working. How to achieve this? I?m using SquidMan v4.0 on macOS.

Thanks in advance.
Best regards

Sent from my iPad

> On 14 Aug 2019, at 06:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 14/08/19 9:11 am, S?rgio Vieira wrote:
>> Hello,
>> 
>> I followed the instructions on this
>> site: https://howchoo.com/g/mwi3ntu1mjq/how-to-set-up-a-proxy-server-on-mac
>> 
>> Regarding your questions:
>> - macOS Mojave 10.14.6
>> - Squid v4.0
>> - Instructions in the site mentioned above
>> - I use SquidMan
>> 
>> Why the changes in the config file don?t got permanent?
>> 
> 
> It sounds like SquidMan is writing a new config file on each restart.
> 
> AFAIK you have to edit the "Template" file used by SquidMan to retain
> any settings in the squid.conf the Squid gets handed.
> 
> If that template is what you are already editing (via the SquidMan UI?),
> then it is probably a bug in SquidMan and you will need to contact the
> author about that.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ahmed.zaeem at netstream.ps  Sat Sep  7 20:24:54 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 7 Sep 2019 23:24:54 +0300
Subject: [squid-users] squid email using curl/smtp  using squid
Message-ID: <52F553BA-0162-434A-82AF-988E0C9381E2@netstream.ps>

Hello Team 

i enabled port port in squid for mailing  in squid ssl ports 587.



curl  --url 'smtp://smtp.gmail.com:587' --ssl-reqd --mail-from 'xxxxxxxx at gcom' --mail-rcpt 'yyyyyyyy at gmail.com'  --upload-file mail.txt --user 'zzzzzzz at gmail.com:mmmmmm' --insecure  -x  5.5.152.44:32000 -U xpostfix:xpostfix -vv

here what i get in squid  error :

07/Sep/2019:16:23:59 -0400      0 1.1.124.243 - 2.2.152.44 32000 TCP_DENIED_REPLY/403 290 PUT ://smtp.gmail.com:587/mail.txt - HIER_NONE/ - - -

if i remove squid section :

-x  5.5.152.44:32000 -U xpostfix:xpostfix

im able to send the email .


anything else do i need to do in squid ?


Thanks 








From squid3 at treenet.co.nz  Sun Sep  8 06:52:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 8 Sep 2019 18:52:43 +1200
Subject: [squid-users] squid.config
In-Reply-To: <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
 <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
 <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>
 <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>
Message-ID: <b16f139d-5ac7-0338-8fca-9907c4374978@treenet.co.nz>

On 7/09/19 10:41 pm, S?rgio Vieira wrote:
> Hello,
> 
> I?m trying to ignore some domains (like facebook.com, youtube.com, etc), meaning that I don?t want logs from this domains. 
> 
> I already inserted in the config file the following:
> acl nolog dstdomain ?/etc/squid/acl-nolog.txt?
> access_log none nolog
> 
> In the txt, i have:
> .facebook.com
> .instagram.com
> 
> But it?s not working. How to achieve this? I?m using SquidMan v4.0 on macOS.
> 


Did you place this "access_log none" line above any other log lines in
your squid.conf?
 The "none" will only prevent access_log lines following its line
producing output.

Amos


From ahmed.zaeem at netstream.ps  Sun Sep  8 15:35:24 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 8 Sep 2019 18:35:24 +0300
Subject: [squid-users] squid email using curl/smtp  using squid
In-Reply-To: <52F553BA-0162-434A-82AF-988E0C9381E2@netstream.ps>
References: <52F553BA-0162-434A-82AF-988E0C9381E2@netstream.ps>
Message-ID: <F09EA0A7-DF36-4805-85C5-41271A4287FD@netstream.ps>

 ?



> On 7 Sep 2019, at 23:24, --Ahmad-- <ahmed.zaeem at netstream.ps> wrote:
> 
> Hello Team 
> 
> i enabled port port in squid for mailing  in squid ssl ports 587.
> 
> 
> 
> curl  --url 'smtp://smtp.gmail.com:587' --ssl-reqd --mail-from 'xxxxxxxx at gcom' --mail-rcpt 'yyyyyyyy at gmail.com'  --upload-file mail.txt --user 'zzzzzzz at gmail.com:mmmmmm' --insecure  -x  5.5.152.44:32000 -U xpostfix:xpostfix -vv
> 
> here what i get in squid  error :
> 
> 07/Sep/2019:16:23:59 -0400      0 1.1.124.243 - 2.2.152.44 32000 TCP_DENIED_REPLY/403 290 PUT ://smtp.gmail.com:587/mail.txt - HIER_NONE/ - - -
> 
> if i remove squid section :
> 
> -x  5.5.152.44:32000 -U xpostfix:xpostfix
> 
> im able to send the email .
> 
> 
> anything else do i need to do in squid ?
> 
> 
> Thanks 
> 
> 
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From Antony.Stone at squid.open.source.it  Sun Sep  8 19:31:57 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 8 Sep 2019 21:31:57 +0200
Subject: [squid-users] squid email using curl/smtp  using squid
In-Reply-To: <F09EA0A7-DF36-4805-85C5-41271A4287FD@netstream.ps>
References: <52F553BA-0162-434A-82AF-988E0C9381E2@netstream.ps>
 <F09EA0A7-DF36-4805-85C5-41271A4287FD@netstream.ps>
Message-ID: <201909082131.57742.Antony.Stone@squid.open.source.it>

On Sunday 08 September 2019 at 17:35:24, --Ahmad-- wrote:

>  ?

It might be that:

a) we don't quite understand what you have done: "i enabled port port in squid 
for mailing in squid ssl ports 587" is not easy to understand

or

b) Squid is not designed to be an email proxy, so why are you try to use it as 
one?


Antony.

> > On 7 Sep 2019, at 23:24, --Ahmad-- wrote:
> > 
> > Hello Team
> > 
> > i enabled port port in squid for mailing  in squid ssl ports 587.

I do not understand what that means.

> > curl  --url 'smtp://smtp.gmail.com:587' --ssl-reqd --mail-from
> > 'xxxxxxxx at gcom' --mail-rcpt 'yyyyyyyy at gmail.com'  --upload-file mail.txt
> > --user 'zzzzzzz at gmail.com:mmmmmm' --insecure  -x  5.5.152.44:32000 -U
> > xpostfix:xpostfix -vv
> > 
> > here what i get in squid  error :
> > 
> > 07/Sep/2019:16:23:59 -0400      0 1.1.124.243 - 2.2.152.44 32000
> > TCP_DENIED_REPLY/403 290 PUT ://smtp.gmail.com:587/mail.txt - HIER_NONE/
> > - - -
> > 
> > if i remove squid section :
> > 
> > -x  5.5.152.44:32000 -U xpostfix:xpostfix
> > 
> > im able to send the email .
> > 
> > 
> > anything else do i need to do in squid ?

Don't use it as a mail proxy?


Antony.

-- 
If the human brain were so simple that we could understand it,
we'd be so simple that we couldn't.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From igoryonya at yahoo.com  Mon Sep  9 22:28:18 2019
From: igoryonya at yahoo.com (Igor Rylov)
Date: Mon, 9 Sep 2019 22:28:18 +0000 (UTC)
Subject: [squid-users] request_header_replace User-Agent 'UA string' for
 certain URLs/domains/ACLs
References: <563346210.3159294.1568068098603.ref@mail.yahoo.com>
Message-ID: <563346210.3159294.1568068098603@mail.yahoo.com>

How to change User-Agent string only for certain ACLs, for example,If I set up an: acl acl_name dst some.urlor: acl acl_name url_regex some\.url\/pathor sources IPs: acl acl_name src 192.168.0.123
Then use the User-Agent replacement for the configured acl:
request_header_replace User-Agent User-Agent-String
I can only do as above, but how can I apply the request_header_replace to only work with a configured acl_name?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190909/30629630/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep 10 06:54:17 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Sep 2019 18:54:17 +1200
Subject: [squid-users] request_header_replace User-Agent 'UA string' for
 certain URLs/domains/ACLs
In-Reply-To: <563346210.3159294.1568068098603@mail.yahoo.com>
References: <563346210.3159294.1568068098603.ref@mail.yahoo.com>
 <563346210.3159294.1568068098603@mail.yahoo.com>
Message-ID: <ab300371-0bb2-e95b-bcad-82ddbd3b0a96@treenet.co.nz>

On 10/09/19 10:28 am, Igor Rylov wrote:
> How to change User-Agent string only for certain ACLs, for example,
> If I set up an: acl acl_name dst some.url
> or: acl acl_name url_regex some\.url\/path
> or sources IPs: acl acl_name src 192.168.0.123
> 
> Then use the User-Agent replacement for the configured acl:
> 
> request_header_replace User-Agent User-Agent-String
> 
> I can only do as above, but how can I apply the request_header_replace
> to only work with a configured acl_name?


To use the *_header_replace directives you need to also have a matching
*_header_access deny line to prevent the original header(s) from being used.

Like so:

  request_header_access User-Agent deny acl_name

  request_header_replace User-Agent User-Agent-String


Amos


From ser.vieira at gmail.com  Tue Sep 10 10:16:55 2019
From: ser.vieira at gmail.com (=?utf-8?Q?S=C3=A9rgio_Vieira?=)
Date: Tue, 10 Sep 2019 11:16:55 +0100
Subject: [squid-users] squid.config
In-Reply-To: <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
 <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
 <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>
 <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>
Message-ID: <0E279060-CE2F-44FA-B584-F7E8794A8EE4@gmail.com>

Hello, 

Can you help me with the query below?
Thanks in advance.
Best regards


> On 7 Sep 2019, at 11:41, S?rgio Vieira <ser.vieira at gmail.com> wrote:
> 
> Hello,
> 
> I?m trying to ignore some domains (like facebook.com, youtube.com, etc), meaning that I don?t want logs from this domains. 
> 
> I already inserted in the config file the following:
> acl nolog dstdomain ?/etc/squid/acl-nolog.txt?
> access_log none nolog
> 
> In the txt, i have:
> .facebook.com
> .instagram.com
> 
> But it?s not working. How to achieve this? I?m using SquidMan v4.0 on macOS.
> 
> Thanks in advance.
> Best regards
> 
> Sent from my iPad
> 
>>> On 14 Aug 2019, at 06:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>> 
>>> On 14/08/19 9:11 am, S?rgio Vieira wrote:
>>> Hello,
>>> 
>>> I followed the instructions on this
>>> site: https://howchoo.com/g/mwi3ntu1mjq/how-to-set-up-a-proxy-server-on-mac
>>> 
>>> Regarding your questions:
>>> - macOS Mojave 10.14.6
>>> - Squid v4.0
>>> - Instructions in the site mentioned above
>>> - I use SquidMan
>>> 
>>> Why the changes in the config file don?t got permanent?
>>> 
>> 
>> It sounds like SquidMan is writing a new config file on each restart.
>> 
>> AFAIK you have to edit the "Template" file used by SquidMan to retain
>> any settings in the squid.conf the Squid gets handed.
>> 
>> If that template is what you are already editing (via the SquidMan UI?),
>> then it is probably a bug in SquidMan and you will need to contact the
>> author about that.
>> 
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Tue Sep 10 10:39:55 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Sep 2019 22:39:55 +1200
Subject: [squid-users] squid.config
In-Reply-To: <0E279060-CE2F-44FA-B584-F7E8794A8EE4@gmail.com>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
 <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
 <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>
 <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>
 <0E279060-CE2F-44FA-B584-F7E8794A8EE4@gmail.com>
Message-ID: <144c95a1-778a-ee6e-34e2-e4d09593aedb@treenet.co.nz>

On 10/09/19 10:16 pm, S?rgio Vieira wrote:
> Hello, 
> 
> Can you help me with the query below?

Only as far as the line ordering issue I mentioned the two days ago on-list.

If that does not work, then all I can do is suggest a Squid upgrade. The
latest version works for me when the log line order is correct.

Amos


From igoryonya at yahoo.com  Wed Sep 11 19:45:33 2019
From: igoryonya at yahoo.com (Igor Rylov)
Date: Wed, 11 Sep 2019 19:45:33 +0000 (UTC)
Subject: [squid-users] Disable 302 redirect in squid,
	but only to http://eais.rkn.gov.ru
References: <1913342111.3948186.1568231133574.ref@mail.yahoo.com>
Message-ID: <1913342111.3948186.1568231133574@mail.yahoo.com>

It is known, that RosComNadzor is blocking certain domains/IPs.Often it is bloking by rewriting url in place, i.e., when I try to access http(s)://blocked.domain, it changes in the location bar of the browser to http://eais.rkn.gov.ru (RosComNadzor's bloking page) in place, i.e. no ability to click the back button to see which url was blocked. RosComNadzor's blocking page has no mention of what url was blocked either. When I try to use Firefox'es or Chrom(e|ium)'s Developer Tools's Network section on blocked page to see, if I can find out what page was blocked by looking at Referer param of the HTTP header, it's already too late, because those Network sections don't show anything, because they were not started in advance, before the page was blocked. I have to reload the page, in order to see something in that networking section, but I would be reloading http://eais.rkn.gov.ru page already, not the required url and http://eais.rkn.gov.ru doesn't show any Referer in HTTP header, because, it's reloaded in place, and did not come from some other page. It's frustrating, because, when you have many tabs open, you have no way of knowing, which url's were blocked and no way of recovering the blocked address. squid's access log doesn't help either, because you can't tell for certain, that the log entry, previous to log entry with http://eais.rkn.gov.ru address belongs to the same tab of the browser.

I've found out, one of the urls, that is being blocked, because it happened in front of my eyes, so I've tested it with:$ curl -v 'http://blocked.domain/'
I got the dump:---DUMP START---
*?? Trying 127.0.0.1...
* Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
> GET http://blocked.domain/ HTTP/1.1> Host: blocked.domain> User-Agent: curl/7.47.0
> Accept: */*
> Proxy-Connection: Keep-Alive
>
< HTTP/1.1 302 Found
< Date: Wed, 11 Sep 2019 08:48:44 GMT
< Content-Length: 205
< Location: http://eais.rkn.gov.ru
< Content-Type: text/html; charset=UTF-8
< X-Cache: MISS from ls02800008008u
< X-Cache-Lookup: MISS from ls02800008008u:3128
< X-Cache: MISS from ws02800008006
< X-Cache-Lookup: MISS from ws02800008006:3128
< X-Cache: MISS from cooldown-nb
< X-Cache-Lookup: MISS from cooldown-nb:3128
< Via: 1.1 ls02800008008u (squid/3.5.12), 1.1 ws02800008006 (squid/3.5.27), 1.1 cooldown-nb (squid/3.5.12)
< Connection: keep-alive
<
<HTML><HEAD><meta http-equiv="content-type" content="text/html;charset=utf-8">
<TITLE>302 Found</TITLE></HEAD><BODY>
<H1>302 Found</H1>
The document has moved
<A HREF="http://eais.rkn.gov.ru">here</A>
* Connection #0 to host 127.0.0.1 left intact---DUMP END---

So, there is a 302 redirect, that happens automatically:Location: http://eais.rkn.gov.ru

How do I disable 302 redirect in squid, but only to http://eais.rkn.gov.ru address, so, in browser I see that page is blocked, but at least, I don't loose the information of what page is blocked, because it's not automatically redirected to http://eais.rkn.gov.ru and the location in the browser showing the original url?
After I've wrote my question, I thought, if it's possible to to do it with:
acl sites_blocking_redirect url_regex eais\.rkn\.gov\.ru
reply_header_access Location deny sites_blocking_redirect
Is it a workable or the correct way to do it, so it solves my problem?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190911/2b17ee16/attachment.htm>

From fosi41 at gmail.com  Thu Sep 12 03:53:19 2019
From: fosi41 at gmail.com (MoFaiz)
Date: Wed, 11 Sep 2019 22:53:19 -0500 (CDT)
Subject: [squid-users] Squid config
Message-ID: <1568260399339-0.post@n4.nabble.com>

Hi Team, 

We are using are using transparent proxy, Squid (Squid Cache: Version 3.5.28
Service Name: squid). Any idea what does the line 2 and 3 mean in the squid
config ? I am more interested in intercept and ssl-bump intercept
respectively. 

http_port 3128 
http_port 3129 intercept 
https_port 3130 ssl-bump intercept cert=/etc/squid/squid.crt
key=/etc/squid/squid.key generate-host-certificates=on 


Thanks in advance, 
M



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Sep 12 05:25:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Sep 2019 17:25:08 +1200
Subject: [squid-users] Disable 302 redirect in squid,
 but only to http://eais.rkn.gov.ru
In-Reply-To: <1913342111.3948186.1568231133574@mail.yahoo.com>
References: <1913342111.3948186.1568231133574.ref@mail.yahoo.com>
 <1913342111.3948186.1568231133574@mail.yahoo.com>
Message-ID: <3e50f0a6-dbd8-fb01-3207-5846b7ff6176@treenet.co.nz>

On 12/09/19 7:45 am, Igor Rylov wrote:
> After I've wrote my question, I thought, if it's possible to to do it with:
> 
> acl sites_blocking_redirect url_regex eais\.rkn\.gov\.ru
> <http://eais.rkn.gov.ru>
> reply_header_access Location deny sites_blocking_redirect
> 
> Is it a workable or the correct way to do it, so it solves my problem?

It is close. That would stop the Location header getting to the client
Browser. But the 302 status still will, and given that the Browser back
button functionality is not working like it used to (a browser bug?),
the full result may still not be worth it.

I would use the http_reply_access to deny instead. That way the client
gets a 403 status from Squid and definitely none of the upstream's redirect.

If you want to be more fancy than 403, the current Squid can attach a
deny_info to the ACL to make the denial have 451 status with custom
template page explaining the block to any user that sees it.
 (Which is what your upstream should have been doing.)

Amos


From uhlar at fantomas.sk  Thu Sep 12 08:12:03 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 12 Sep 2019 10:12:03 +0200
Subject: [squid-users] Squid config
In-Reply-To: <1568260399339-0.post@n4.nabble.com>
References: <1568260399339-0.post@n4.nabble.com>
Message-ID: <20190912081203.GC19799@fantomas.sk>

On 11.09.19 22:53, MoFaiz wrote:
>We are using are using transparent proxy, Squid (Squid Cache: Version 3.5.28
>Service Name: squid). Any idea what does the line 2 and 3 mean in the squid
>config ? I am more interested in intercept and ssl-bump intercept
>respectively.
>
>http_port 3128
>http_port 3129 intercept
>https_port 3130 ssl-bump intercept cert=/etc/squid/squid.crt
>key=/etc/squid/squid.key generate-host-certificates=on

I believe squid wiki explains it perfectly, simple web search returns these
answers:

squid intercept:
https://wiki.squid-cache.org/SquidFaq/InterceptionProxy

squid ssl-bump:
https://wiki.squid-cache.org/Features/SslBump

squid ssl-bump intercept:
https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Please read these first. If you don't understand them, you can ask here.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux is like a teepee: no Windows, no Gates and an apache inside...


From nabil1385 at gmail.com  Thu Sep 12 08:25:49 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 12 Sep 2019 03:25:49 -0500 (CDT)
Subject: [squid-users] caching and changing content
Message-ID: <1568276749562-0.post@n4.nabble.com>

In my scenrio (squid 3.5.23) I have several clients which download content.

Now I want to achieve two things: if one client has already downloaded the
content the second client requesting the same content should take it from
the squid cache.

But: when this content resource on the internet is changing the squid should
not give a HIT but a MISS and refresh the content the next time a client is
requesting it.

It is not clear to me how to achieve this behaviour.

The application (server on internet) sends headers like this:

cache-control: public, max-age=0
last-modified: Thu, 12 Sep 2019 07:30:56 GMT
etag: W/"6699e3-16d2461dba1"

So far I could only achieve HITs using refresh_pattern with min value > 0.

refresh_pattern .               30       20%     1440 override-expire

But also this configuration is not exactly what  I want. 

Now even if the content of a resource on the internet is changing I get HITs
for 30 min.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sakibnizam at gmail.com  Thu Sep 12 08:43:20 2019
From: sakibnizam at gmail.com (sknz)
Date: Thu, 12 Sep 2019 03:43:20 -0500 (CDT)
Subject: [squid-users] intercept vs. accel vhost allow-direct
Message-ID: <1568277800425-0.post@n4.nabble.com>

I'm running a hotspot(CoovaChilli, Freeradius, etc.) server where
Squid-3.4.8(SSL enabled) for caching and logging. My machine is running on
Debian 8.1.1 with 2 NIC card. One for WAN and another for LAN to manage
hotspot AP(s).

ERROR
The requested URL could not be retrieved

Below configuration is throwing this above error page :
http_port 3128
http_port 3127 intercept

Instead, I have to use this :
http_port 3128 accel vhost allow-direct                                                                                                                                                                                                                   

Now it works! Squid is not throwing any error log for both cases. Why
INTERCEPT is not working?


I've attached my iptables rules below for a deeper look.

====================

-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-A INPUT -i eth1 -j DROP
-A INPUT -d 10.1.0.1/32 -i tun0 -p icmp -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 53 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
-A INPUT -d 255.255.255.255/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3128 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 53 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 2812 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 443 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 4990 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -j DROP
-A FORWARD -i tun0 -o eth0 -j ACCEPT
-A FORWARD -i tun0 ! -o eth0 -j DROP
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu
-A FORWARD -o tun0 -j ACCEPT
-A FORWARD -i tun0 -j ACCEPT
-A FORWARD -o eth1 -j DROP
-A FORWARD -i eth1 -j DROP

====================

Chain INPUT (policy ACCEPT 693 packets, 123K bytes)
 pkts bytes target     prot opt in     out     source              
destination
  652 80697 DROP       all  --  eth1   any     anywhere             anywhere
    2   702 ACCEPT     icmp --  tun0   any     anywhere             10.1.0.1
   95  6428 ACCEPT     udp  --  tun0   any     anywhere             10.1.0.1            
udp dpt:domain
    0     0 ACCEPT     udp  --  tun0   any     anywhere             10.1.0.1            
udp dpts:bootps:bootpc
    0     0 ACCEPT     udp  --  tun0   any     anywhere            
255.255.255.255      udp dpts:bootps:bootpc
  191 29716 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:3128
  200 22838 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:3990
    0     0 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:domain
    0     0 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:2812
    0     0 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:ssh
    0     0 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:https
  239 25386 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:http
    0     0 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:4990
    0     0 ACCEPT     tcp  --  tun0   any     anywhere             10.1.0.1            
tcp dpt:3990
    0     0 DROP       all  --  tun0   any     anywhere             10.1.0.1

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source              
destination
 1259  191K ACCEPT     all  --  tun0   eth0    anywhere             anywhere
    0     0 DROP       all  --  tun0   !eth0   anywhere             anywhere
  274 16408 TCPMSS     tcp  --  any    any     anywhere             anywhere            
tcp flags:SYN,RST/SYN TCPMSS clamp to PMTU
 1127 1186K ACCEPT     all  --  any    tun0    anywhere             anywhere
    0     0 ACCEPT     all  --  tun0   any     anywhere             anywhere
    0     0 DROP       all  --  any    eth1    anywhere             anywhere
 1503  228K DROP       all  --  eth1   any     anywhere             anywhere

Chain OUTPUT (policy ACCEPT 1192 packets, 490K bytes)
 pkts bytes target     prot opt in     out     source              
destination

====================



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Sep 12 09:37:41 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Sep 2019 21:37:41 +1200
Subject: [squid-users] caching and changing content
In-Reply-To: <1568276749562-0.post@n4.nabble.com>
References: <1568276749562-0.post@n4.nabble.com>
Message-ID: <3367c6cc-1a99-b257-5dfd-f66415cd81c3@treenet.co.nz>

On 12/09/19 8:25 pm, fansari wrote:
> In my scenrio (squid 3.5.23) I have several clients which download content.
> 
> Now I want to achieve two things: if one client has already downloaded the
> content the second client requesting the same content should take it from
> the squid cache.
> 
> But: when this content resource on the internet is changing the squid should
> not give a HIT but a MISS and refresh the content the next time a client is
> requesting it.

No. HTTP specifies that the cache should update its copy of the content,
and deliver the new result. This is *not* a MISS. It is a REFRESH in the
logs, or a "Near-HIT" in the traffic statistics report.

> 
> It is not clear to me how to achieve this behaviour.

You do not have to do anything. For content which this behaviour is
possible, Squid does it by default.

> 
> The application (server on internet) sends headers like this:
> 
> cache-control: public, max-age=0

'public' - this object is allowed to be stored in caches.

'max-age=0' - the cache is required to contact the server for updates
before delivering this object to any clients.


> last-modified: Thu, 12 Sep 2019 07:30:56 GMT
> etag: W/"6699e3-16d2461dba1"
> 
> So far I could only achieve HITs using refresh_pattern with min value > 0.
> 
> refresh_pattern .               30       20%     1440 override-expire
> 

refresh_pattern provides default values for Squid to use in its caching
algorithm if the server does not provide them.

The headers you show above contain clear and precise cache-control
instructions. So refresh_pattern is not normally relevant to that object.

By adding the override-expires option you are *forcing* the object to be
cached for no less than 30 minutes and no more than 1 week...


> But also this configuration is not exactly what  I want. 
> 
> Now even if the content of a resource on the internet is changing I get HITs
> for 30 min.
> 

Not just that resource. By using the '.' pattern you are forcing this
30min HIT on *every* object any client fetches through this cache.

It is best to be a specific as you can in the pattern field. Especially
when using the ignore-* and override-* options. To reduce nasty side
effects.


PS. The server is delivering cache controls, what makes you think the
server developer and admin are wrong about their own content and systems?


Amos


From squid3 at treenet.co.nz  Thu Sep 12 09:39:52 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Sep 2019 21:39:52 +1200
Subject: [squid-users] intercept vs. accel vhost allow-direct
In-Reply-To: <1568277800425-0.post@n4.nabble.com>
References: <1568277800425-0.post@n4.nabble.com>
Message-ID: <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>

On 12/09/19 8:43 pm, sknz wrote:
> I'm running a hotspot(CoovaChilli, Freeradius, etc.) server where
> Squid-3.4.8(SSL enabled) for caching and logging. My machine is running on
> Debian 8.1.1 with 2 NIC card. One for WAN and another for LAN to manage
> hotspot AP(s).
> 
> ERROR
> The requested URL could not be retrieved
> 
> Below configuration is throwing this above error page :
> http_port 3128
> http_port 3127 intercept
> 
> Instead, I have to use this :
> http_port 3128 accel vhost allow-direct                                                                                                                                                                                                                   
> 

(Congratulations you now have CVE-2009-0801)

> Now it works! Squid is not throwing any error log for both cases. Why
> INTERCEPT is not working?

Because "The requested URL could not be retrieved".

intercept means take the origin server details from the NAT system.
Squid will act as transparently as possible, sending the traffic on to
the same server IP address the client was trying to deliver that request to.

accel means Squid is providing CDN services for the domain being
fetched. It has full authority as the origin server and any source of
data is accepted as valid response to the client.

Without any further information I guess that Squid is not able to
connect to the dst-IP the client is trying to connect to. But when DNS
is consulted in Squid's role as CDN, one of the domains other IP
addresses works.
... or maybe the client was actually not going to the server its TCP
claims and you just let malware loose.


(All those firewall settings mean nothing without details about which
IPs Squid is using and which NIC is which.)


Amos


From sakibnizam at gmail.com  Thu Sep 12 09:53:58 2019
From: sakibnizam at gmail.com (sknz)
Date: Thu, 12 Sep 2019 04:53:58 -0500 (CDT)
Subject: [squid-users] intercept vs. accel vhost allow-direct
In-Reply-To: <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>
References: <1568277800425-0.post@n4.nabble.com>
 <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>
Message-ID: <1568282038926-0.post@n4.nabble.com>

Hello,

<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377788/test.png> 

etho0 is for WAN and eth1 is for LAN side.

and more detailed firewall settings:

# Generated by iptables-save v1.4.21 on Thu Sep 12 15:46:58 2019
*nat
:PREROUTING ACCEPT [3911:298328]
:INPUT ACCEPT [384:30494]
:OUTPUT ACCEPT [273:20568]
:POSTROUTING ACCEPT [13:3456]
-A PREROUTING -s 10.1.0.0/24 ! -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport
80 -j REDIRECT --to-ports 3128
-A POSTROUTING -o eth0 -j MASQUERADE
COMMIT
# Completed on Thu Sep 12 15:46:58 2019
# Generated by iptables-save v1.4.21 on Thu Sep 12 15:46:58 2019
*mangle
:PREROUTING ACCEPT [10761:3310565]
:INPUT ACCEPT [3211:587384]
:FORWARD ACCEPT [6306:2611786]
:OUTPUT ACCEPT [2279:577020]
:POSTROUTING ACCEPT [5283:2937872]
-A PREROUTING -s 10.1.0.0/24 -d 10.1.0.1/32 -p tcp -m tcp --dport 3128 -j
DROP
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu
COMMIT
# Completed on Thu Sep 12 15:46:58 2019
# Generated by iptables-save v1.4.21 on Thu Sep 12 15:46:58 2019
*filter
:INPUT ACCEPT [1989:462678]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [2279:577020]
-A INPUT -i eth1 -j DROP
-A INPUT -d 10.1.0.1/32 -i tun0 -p icmp -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 53 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
-A INPUT -d 255.255.255.255/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3128 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 53 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 2812 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 443 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 4990 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -j DROP
-A FORWARD -i tun0 -o eth0 -j ACCEPT
-A FORWARD -i tun0 ! -o eth0 -j DROP
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu
-A FORWARD -o tun0 -j ACCEPT
-A FORWARD -i tun0 -j ACCEPT
-A FORWARD -o eth1 -j DROP
-A FORWARD -i eth1 -j DROP
COMMIT
# Completed on Thu Sep 12 15:46:58 2019




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From antonino.sanacori at unibs.it  Thu Sep 12 10:41:40 2019
From: antonino.sanacori at unibs.it (Antonino Sanacori)
Date: Thu, 12 Sep 2019 12:41:40 +0200
Subject: [squid-users] Multiple LDAP authentication server for Squid
Message-ID: <9607080a-6631-1e84-88bc-13f09f500988@unibs.it>

Hi.

I use one ldap server for authentication of my users but now i have new 
users on another branch of same ldap server.

How can I configure squid.conf for support ldap authentication of my 
users on different branches?

Many thanks.

Antonino Sanacori


-- 


Informativa sulla Privacy: http://www.unibs.it/node/8155 
<http://www.unibs.it/node/8155>


From nabil1385 at gmail.com  Thu Sep 12 11:53:45 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 12 Sep 2019 06:53:45 -0500 (CDT)
Subject: [squid-users] caching and changing content
In-Reply-To: <3367c6cc-1a99-b257-5dfd-f66415cd81c3@treenet.co.nz>
References: <1568276749562-0.post@n4.nabble.com>
 <3367c6cc-1a99-b257-5dfd-f66415cd81c3@treenet.co.nz>
Message-ID: <1568289225653-0.post@n4.nabble.com>

I will begin with you last question: the scenario I work on was not meant to
work with proxy so far but now we want to integrate it because of traffic
costs. Second point is that we cannot change anything on the application
right now.

This means I have to take everything as it is and to configure squid the
best way I can for our needs.

I have tested again putting a # before all refresh lines so from my
understanding it uses default behaviour.

My first test goes like this: I download the content - I get a MISS so it is
downloaded and should be cached.

When I request the content the second time (I clear local browser cache
before to simulate a second client loading the content for the first time) I
get a MISS again. So the content is downloaded again which is not what I
want.

The second test (changing the content) now works of course since the content
was downloaded again.

But as I said: I need to fulfill both requirements.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Sep 12 13:07:33 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Sep 2019 09:07:33 -0400
Subject: [squid-users] Disable 302 redirect in squid,
 but only to http://eais.rkn.gov.ru
In-Reply-To: <3e50f0a6-dbd8-fb01-3207-5846b7ff6176@treenet.co.nz>
References: <1913342111.3948186.1568231133574.ref@mail.yahoo.com>
 <1913342111.3948186.1568231133574@mail.yahoo.com>
 <3e50f0a6-dbd8-fb01-3207-5846b7ff6176@treenet.co.nz>
Message-ID: <1d2e62f3-b02e-c8a2-844a-e8e8f3b4e8bd@measurement-factory.com>

On 9/12/19 1:25 AM, Amos Jeffries wrote:
> On 12/09/19 7:45 am, Igor Rylov wrote:
>> After I've wrote my question, I thought, if it's possible to to do it with:
>>
>> acl sites_blocking_redirect url_regex BLOCKER
>> reply_header_access Location deny sites_blocking_redirect
>>
>> Is it a workable or the correct way to do it, so it solves my problem?

> It is close. That would stop the Location header getting to the client

The url_regex ACL matches the request URL, and the request URL is not
BLOCKER in this example. I would use the rep_header ACL instead.

Alex.


> Browser. But the 302 status still will, and given that the Browser back
> button functionality is not working like it used to (a browser bug?),
> the full result may still not be worth it.
> 
> I would use the http_reply_access to deny instead. That way the client
> gets a 403 status from Squid and definitely none of the upstream's redirect.
> 
> If you want to be more fancy than 403, the current Squid can attach a
> deny_info to the ACL to make the denial have 451 status with custom
> template page explaining the block to any user that sees it.
>  (Which is what your upstream should have been doing.)
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Thu Sep 12 13:35:21 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Sep 2019 09:35:21 -0400
Subject: [squid-users] caching and changing content
In-Reply-To: <1568276749562-0.post@n4.nabble.com>
References: <1568276749562-0.post@n4.nabble.com>
Message-ID: <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>

On 9/12/19 4:25 AM, fansari wrote:
> In my scenrio (squid 3.5.23) I have several clients which download content.
> 
> Now I want to achieve two things: if one client has already downloaded the
> content the second client requesting the same content should take it from
> the squid cache.

When handling the second client request, can you allow Squid to ask the
origin server whether the content is still the same as it was when the
first client downloaded it? If you can, will the origin server respond
with a 304 (Not Modified) to Squid's conditional request?


> But: when this content resource on the internet is changing the squid should
> not give a HIT but a MISS and refresh the content the next time a client is
> requesting it.

When handling this "next time" request, can you allow Squid to ask the
origin server whether the content is still the same as it was when the
first client downloaded it? If you can, will the origin server respond
with the fresh content to Squid's conditional request?


> It is not clear to me how to achieve this behaviour.

> The application (server on internet) sends headers like this:
> 
> cache-control: public, max-age=0
> last-modified: Thu, 12 Sep 2019 07:30:56 GMT
> etag: W/"6699e3-16d2461dba1"

The answers to the above four questions may help us find solution(s) to
your problem.

Alex.


From nabil1385 at gmail.com  Thu Sep 12 14:21:32 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 12 Sep 2019 09:21:32 -0500 (CDT)
Subject: [squid-users] caching and changing content
In-Reply-To: <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
References: <1568276749562-0.post@n4.nabble.com>
 <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
Message-ID: <1568298092695-0.post@n4.nabble.com>

In case a client has already downloaded the content and I request the content
again without deleting the local cache (this would be the scenario "same
client") in this case I see a 304 HEAD request which is sent out to the
server (no new download from the server). Probably this works due to the
etag and/or last-modified headers so it can be sorted out whether the
content is still fresh or not.

But this does not work when the client has an empty cache or not downloaded
this resource before.

The desired behaviour would be that in this case the squid compares the
resource on the server (internet side) with the resouce from its own cache
in order to decide whether the content is still fresh or not.

Yes - I can allow squid to ask the origin server whether the content is
still the same. Probably my problem is that I don't know how to do that.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ser.vieira at gmail.com  Thu Sep 12 14:21:58 2019
From: ser.vieira at gmail.com (=?utf-8?Q?S=C3=A9rgio_Vieira?=)
Date: Thu, 12 Sep 2019 15:21:58 +0100
Subject: [squid-users] squid.config Ignore domains
In-Reply-To: <0E279060-CE2F-44FA-B584-F7E8794A8EE4@gmail.com>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
 <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
 <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>
 <2DFF9AD6-0644-4ED9-9630-9D554BB02A69@gmail.com>
 <0E279060-CE2F-44FA-B584-F7E8794A8EE4@gmail.com>
Message-ID: <DF571E64-17CE-4942-A6CA-3C8BB9B0CA63@gmail.com>


> Hello, 
> 
> Can you help me with the query below?
> Thanks in advance.
> Best regards
> 
> 
>> On 7 Sep 2019, at 11:41, S?rgio Vieira <ser.vieira at gmail.com> wrote:
>> 
>> Hello,
>> 
>> I?m trying to ignore some domains (like facebook.com, youtube.com, etc), meaning that I don?t want logs from this domains. 
>> 
>> I already inserted in the config file the following:
>> acl nolog dstdomain ?/etc/squid/acl-nolog.txt?
>> access_log none nolog
>> 
>> In the txt, i have:
>> .facebook.com
>> .instagram.com
>> 
>> But it?s not working. How to achieve this? I?m using SquidMan v4.0 on macOS.
>> 
>> Thanks in advance.
>> Best regards


From rousskov at measurement-factory.com  Thu Sep 12 14:33:29 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Sep 2019 10:33:29 -0400
Subject: [squid-users] caching and changing content
In-Reply-To: <1568298092695-0.post@n4.nabble.com>
References: <1568276749562-0.post@n4.nabble.com>
 <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
 <1568298092695-0.post@n4.nabble.com>
Message-ID: <25a1f743-1588-dee0-a556-52f19b06dc39@measurement-factory.com>

On 9/12/19 10:21 AM, fansari wrote:

> Yes - I can allow squid to ask the origin server whether the content is
> still the same. Probably my problem is that I don't know how to do that.

Most likely, Squid will do that automatically.

You have not answered my other questions so it is still unclear what the
best configuration for your needs is, but I recommend starting with the
_default_ configuration (as far as caching and refreshing rules are
concerned), posting the access.log records from your test, and
indicating the first access log record that does not match your needs.

Alex.


From nabil1385 at gmail.com  Thu Sep 12 15:02:54 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 12 Sep 2019 10:02:54 -0500 (CDT)
Subject: [squid-users] caching and changing content
In-Reply-To: <25a1f743-1588-dee0-a556-52f19b06dc39@measurement-factory.com>
References: <1568276749562-0.post@n4.nabble.com>
 <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
 <1568298092695-0.post@n4.nabble.com>
 <25a1f743-1588-dee0-a556-52f19b06dc39@measurement-factory.com>
Message-ID: <1568300574577-0.post@n4.nabble.com>

This is the first behaviour I don't want like this:

1568300283.479   3896 xxx.xxx.0.239 TCP_MISS/200 6724533 GET
https://xxx/1.mp4 - HIER_DIRECT/xxx.xxx.24.241 video/mp4

The content of the mp4 file has not changed. Nevertheless after clearing the
browser cache the content produces a MISS instead of a HIT. 






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Sep 12 15:30:59 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Sep 2019 11:30:59 -0400
Subject: [squid-users] caching and changing content
In-Reply-To: <1568300574577-0.post@n4.nabble.com>
References: <1568276749562-0.post@n4.nabble.com>
 <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
 <1568298092695-0.post@n4.nabble.com>
 <25a1f743-1588-dee0-a556-52f19b06dc39@measurement-factory.com>
 <1568300574577-0.post@n4.nabble.com>
Message-ID: <5d55482e-da10-c64e-624c-c57de29f2a6c@measurement-factory.com>

On 9/12/19 11:02 AM, fansari wrote:
> This is the first behaviour I don't want like this:

In the future, please post all access.log records from the test case up
to and including the first one you do not like (not just the first one
you do not like). In caching, _history_ matters.


> 1568300283.479   3896 xxx.xxx.0.239 TCP_MISS/200 6724533 GET
> https://xxx/1.mp4 - HIER_DIRECT/xxx.xxx.24.241 video/mp4

> The content of the mp4 file has not changed. Nevertheless after clearing the
> browser cache the content produces a MISS instead of a HIT. 

Understood. What does your "squid -v" say (I am looking for ./configure
options). If you see --enable-delay-pools, then do not trust the
TCP_MISS status code. It can be a cache hit in that case. I am not
saying it _was_ a hit. Just cautioning from trusting TCP_MISS too much
when running with --enable-delay-pools.

Please set debug_options to ALL,2, repeat the test from scratch, and
share the request headers received by Squid, the request headers sent by
Squid, the response headers received by Squid, and the response headers
sent by Squid. You will find all these headers in cache.log. You can
just share a (compressed) cache.log if you prefer.

Alex.


From nabil1385 at gmail.com  Thu Sep 12 16:22:24 2019
From: nabil1385 at gmail.com (fansari)
Date: Thu, 12 Sep 2019 11:22:24 -0500 (CDT)
Subject: [squid-users] caching and changing content
In-Reply-To: <5d55482e-da10-c64e-624c-c57de29f2a6c@measurement-factory.com>
References: <1568276749562-0.post@n4.nabble.com>
 <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
 <1568298092695-0.post@n4.nabble.com>
 <25a1f743-1588-dee0-a556-52f19b06dc39@measurement-factory.com>
 <1568300574577-0.post@n4.nabble.com>
 <5d55482e-da10-c64e-624c-c57de29f2a6c@measurement-factory.com>
Message-ID: <1568305344504-0.post@n4.nabble.com>

squid -v gives this:

Squid Cache: Version 3.5.23
Service Name: squid
Raspbian linux
configure options:  '--build=arm-linux-gnueabihf' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g
-O2 -fdebug-prefix-map=/home/pi/squid3-3.5.23=. -fstack-protector-strong
-Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2
-Wl,-z,relro -Wl,-z,now -Wl,--as-needed' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--enable-ssl'
'--enable-ssl-crtd' '--with-openssl' '--enable-build-info=Raspbian linux'
'--enable-linux-netfilter' 'build_alias=arm-linux-gnueabihf' 'CFLAGS=-g -O2
-fdebug-prefix-map=/home/pi/squid3-3.5.23=. -fstack-protector-strong
-Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now
-Wl,--as-needed' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2
-fdebug-prefix-map=/home/pi/squid3-3.5.23=. -fstack-protector-strong
-Wformat -Werror=format-security'

I have stopped squid, cleared the cache and started it again.

Then I have ran the same test again. As you can see the the 689.mp4 is
downloaded twice.

access.log
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377780/access.log>  
cache.log
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377780/cache.log>  
squid.conf
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377780/squid.conf>  



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Sep 12 17:14:01 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Sep 2019 13:14:01 -0400
Subject: [squid-users] caching and changing content
In-Reply-To: <1568305344504-0.post@n4.nabble.com>
References: <1568276749562-0.post@n4.nabble.com>
 <45c12d0d-c860-bd45-9357-f28e31c85802@measurement-factory.com>
 <1568298092695-0.post@n4.nabble.com>
 <25a1f743-1588-dee0-a556-52f19b06dc39@measurement-factory.com>
 <1568300574577-0.post@n4.nabble.com>
 <5d55482e-da10-c64e-624c-c57de29f2a6c@measurement-factory.com>
 <1568305344504-0.post@n4.nabble.com>
Message-ID: <7947cd1c-8396-60d9-3cab-d5a7c8b3626b@measurement-factory.com>

On 9/12/19 12:22 PM, fansari wrote:
> '--enable-delay-pools'

OK, so we should be careful with not trusting TCP_MISS too much.


> http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377780/cache.log


Squid shows these headers for the second response (near 15:40:03.814):

> X-Cache: MISS from my-client-02
> X-Cache-Lookup: HIT from my-client-02:3128

This means that Squid found the requested resource in the cache (good!)
but decided not to use that cached response (bad).

Moreover, according to your cache.log, Squid decided not to send a
conditional request to check that cached response freshness. Squid sent
a regular/unconditional request instead. I do not know why. Perhaps your
old Squid does not support revalidation using weak E-Tags, but that is
just a wild guess.

If others cannot find the answer in your logs, then one way to figure it
out would be to increase debugging (debug_options ALL,3 88,7 22,7). An
alternative strategy would be to upgrade to Squid v4 in hope that it
would be better at revalidating this particular resource.

HTH,

Alex.






From sakibnizam at gmail.com  Fri Sep 13 04:27:41 2019
From: sakibnizam at gmail.com (sknz)
Date: Thu, 12 Sep 2019 23:27:41 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
Message-ID: <1568348861951-0.post@n4.nabble.com>

I'm running an AP-Hotspot server(coovachilli, freeradius, squid, etc.) with
two NIC(eth0 and eth1). eth0 is for WAN(internet) and eth1 is for managing
LAN(APs). Coovachilli is created tun0 under the eth1 interface. I'm using
squid3 as an HTTP transparent proxy.

Hardware Setup Diagram <https://i.stack.imgur.com/sKF9e.png>  

*SQUID.CONF:*
http_port 3128
http_port 3127 intercept

*IPTABLES Filter & Nat Rules(similar):*
-A PREROUTING -s 10.1.0.0/24 ! -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport
80 -j REDIRECT --to-ports 3127 #redirect http to squid intercept port
-A POSTROUTING -o eth0 -j MASQUERADE

-A PREROUTING -s 10.1.0.0/24 -d 10.1.0.1/32 -p tcp -m tcp --dport 3127 -j
DROP #drop direct attempts to proxy intercept port
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu

-A INPUT -i eth1 -j DROP
-A INPUT -d 10.1.0.1/32 -i tun0 -p icmp -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3128 -j ACCEPT #
opening squid port
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3127 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 443 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -j DROP
-A FORWARD -i tun0 -o eth0 -j ACCEPT                                                                                                                            
-A FORWARD -i tun0 ! -o eth0 -j DROP
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu
-A FORWARD -o tun0 -j ACCEPT
-A FORWARD -i tun0 -j ACCEPT
-A FORWARD -o eth1 -j DROP
-A FORWARD -i eth1 -j DROP

HTTPS connection from AP side is working as in squid don't intercept it, but
HTTP connection doesn't work. Squid_3.4.8_Debian starts
normally(active/running), no error in cache.log. If I change squid
configuration(http_port 3127 accel vhost allow-direct) to reverse proxy, it
works. I need a transparent proxy to work. Please help me to figure it out.
Thanks.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Sep 13 05:46:25 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Sep 2019 17:46:25 +1200
Subject: [squid-users] intercept vs. accel vhost allow-direct
In-Reply-To: <1568282038926-0.post@n4.nabble.com>
References: <1568277800425-0.post@n4.nabble.com>
 <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>
 <1568282038926-0.post@n4.nabble.com>
Message-ID: <68044a53-b15c-2ef7-6817-88e956709bdb@treenet.co.nz>

On 12/09/19 9:53 pm, sknz wrote:
> Hello,
> 
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377788/test.png> 
> 
> etho0 is for WAN and eth1 is for LAN side.
> 
> and more detailed firewall settings:
> 
> # Generated by iptables-save v1.4.21 on Thu Sep 12 15:46:58 2019
> *nat
> :PREROUTING ACCEPT [3911:298328]
> :INPUT ACCEPT [384:30494]
> :OUTPUT ACCEPT [273:20568]
> :POSTROUTING ACCEPT [13:3456]
> -A PREROUTING -s 10.1.0.0/24 ! -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport
> 80 -j REDIRECT --to-ports 3128


There are two suspect things about this rule.
 1) the port here does not match the port 3129 you mentioned earlier as
having the intercept flag. Those two must be the same. Avoiding port
3128 in these things is a good idea, it is well-known and registered for
other uses.

 2) the interface here is tun0, you said eth1 is your LAN side.

You do not technically need the interface name in this rule, it is just
an extra protection against spoofed IPs coming from the WAN.

I would try without the -i option. If that works, then you can test
which interface is needed to continue working when the -i is added back.


> -A POSTROUTING -o eth0 -j MASQUERADE
> COMMIT
> # Completed on Thu Sep 12 15:46:58 2019
> # Generated by iptables-save v1.4.21 on Thu Sep 12 15:46:58 2019
> *mangle
> :PREROUTING ACCEPT [10761:3310565]
> :INPUT ACCEPT [3211:587384]
> :FORWARD ACCEPT [6306:2611786]
> :OUTPUT ACCEPT [2279:577020]
> :POSTROUTING ACCEPT [5283:2937872]
> -A PREROUTING -s 10.1.0.0/24 -d 10.1.0.1/32 -p tcp -m tcp --dport 3128 -j
> DROP

The above rule is dropping access to your proxy port 3128. Which is the
forward-proxy port for clients configured properly to use the proxy,
error page icons, and other things needing direct client<->proxy contact.

The recommended mangle table rule to protect the proxy intercept port
would be just "-A PREROUTING -p tcp -m tcp --dport 3129 -j DROP"

So the proxy is protected from all traffic. Even from localhost, or
outside your network.


> # Completed on Thu Sep 12 15:46:58 2019
> # Generated by iptables-save v1.4.21 on Thu Sep 12 15:46:58 2019
> *filter
> :INPUT ACCEPT [1989:462678]
> :FORWARD ACCEPT [0:0]
> :OUTPUT ACCEPT [2279:577020]
> -A INPUT -i eth1 -j DROP
> -A INPUT -d 10.1.0.1/32 -i tun0 -p icmp -j ACCEPT
> -A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 53 -j ACCEPT
> -A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
> -A INPUT -d 255.255.255.255/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
> -A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3128 -j ACCEPT

If you need this rule to get traffic to " http_port 3128", then you
probably also need one for the "http_port 3129 intercept".


Amos


From sakibnizam at gmail.com  Fri Sep 13 08:12:39 2019
From: sakibnizam at gmail.com (sknz)
Date: Fri, 13 Sep 2019 03:12:39 -0500 (CDT)
Subject: [squid-users] intercept vs. accel vhost allow-direct
In-Reply-To: <68044a53-b15c-2ef7-6817-88e956709bdb@treenet.co.nz>
References: <1568277800425-0.post@n4.nabble.com>
 <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>
 <1568282038926-0.post@n4.nabble.com>
 <68044a53-b15c-2ef7-6817-88e956709bdb@treenet.co.nz>
Message-ID: <1568362359475-0.post@n4.nabble.com>

Hello Amos,
For clarification, I'm running an AP-Hotspot server(coovachilli, freeradius,
squid, etc.) with two NIC(eth0 and eth1). eth0 is for WAN(internet) and eth1
is for managing LAN(APs). Coovachilli is created tun0 under the eth1
interface. I'm using squid-3.4.8 as an HTTP transparent proxy.

# Hardware Setup Diagram <https://i.stack.imgur.com/sKF9e.png> 

# ifconfig:
eth0    Link encap:Ethernet  HWaddr d8:cb:8a:53:b5:ff
          inet addr:192.168.0.100  Bcast:192.168.0.255  Mask:255.255.255.0
          RX bytes:145897 (142.4 KiB)  TX bytes:86949 (84.9 KiB)

eth1    Link encap:Ethernet  HWaddr 00:e0:4c:53:44:58
          inet6 addr: fe80::2e0:4cff:fe53:4458/64 Scope:Link
          RX bytes:178346 (174.1 KiB)  TX bytes:366000 (357.4 KiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          RX bytes:15724 (15.3 KiB)  TX bytes:15724 (15.3 KiB)

tun0    Link encap:UNSPEC  HWaddr 00-00-00-00-00-00-00-00
          inet addr:10.1.0.1  P-t-P:10.1.0.1  Mask:255.255.255.0
          RX bytes:111251 (108.6 KiB)  TX bytes:347971 (339.8 KiB)


# This is my updated squid.conf as your suggestion, 3129 for forward-proxy
and 3130 for intercepting HTTP:
http_port 3129
http_port 3130 intercept


# Squid is listening on expected ports; netstat -tunlp:
tcp6   0   0 :::3129    :::*    LISTEN      1754/(squid-1)
tcp6   0   0 :::3130    :::*    LISTEN      1754/(squid-1)
udp6  0  0 :::41845   :::*                     1754/(squid-1)


#Squid is not throwing any error; tail -4 /etc/squid3/cache.log:
Accepting HTTP Socket connections at local=[::]:3129 remote=[::] FD 11
flags=9
Accepting NAT intercepted HTTP Socket connections at local=[::]:3130
remote=[::] FD 12 flags=41
ICMP socket opened.
storeLateRelease: released 0 objects


# No response, if I do SquidClient under SSH to server; squidclient -p 3129
http://www.example.com
Sending HTTP request ... done.


I've tried removing all "-i" options and updating mangle rules as your
suggestion from iptables; no effects on it. I've opened intercept port also.
This is my original iptables with adjusted rules:

#nat
-A PREROUTING -s 10.1.0.0/24 ! -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport
80 -j REDIRECT --to-ports 3130    #redirect http to squid intercept port
-A POSTROUTING -o eth0 -j MASQUERADE

#mangle
-A PREROUTING -s 10.1.0.0/24 -d 10.1.0.1/32 -p tcp -m tcp --dport 3130 -j
DROP    #drop direct attempts to proxy intercept port
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu

#filters
-A INPUT -i eth1 -j DROP
-A INPUT -d 10.1.0.1/32 -i tun0 -p icmp -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 53 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
-A INPUT -d 255.255.255.255/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3130 -j ACCEPT   
#squid intercept
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3129 -j ACCEPT   
#squid forward
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT   
#chilli controller
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 53 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 2812 -j ACCEPT #
freeradius
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 443 -j ACCEPT    #
https
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 80 -j ACCEPT    #http
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 4990 -j ACCEPT   
#hotspot UAM
-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT 
-A INPUT -d 10.1.0.1/32 -i tun0 -j DROP
-A FORWARD -i tun0 -o eth0 -j ACCEPT
-A FORWARD -i tun0 ! -o eth0 -j DROP
-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
--clamp-mss-to-pmtu
-A FORWARD -o tun0 -j ACCEPT
-A FORWARD -i tun0 -j ACCEPT
-A FORWARD -o eth1 -j DROP
-A FORWARD -i eth1 -j DROP


So from here, all I can do HTTPS connection, no HTTP connection allowed from
AP side.











--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Fri Sep 13 08:36:06 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 13 Sep 2019 10:36:06 +0200
Subject: [squid-users] intercept vs. accel vhost allow-direct
In-Reply-To: <1568362359475-0.post@n4.nabble.com>
References: <1568277800425-0.post@n4.nabble.com>
 <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>
 <1568282038926-0.post@n4.nabble.com>
 <68044a53-b15c-2ef7-6817-88e956709bdb@treenet.co.nz>
 <1568362359475-0.post@n4.nabble.com>
Message-ID: <20190913083606.GA21563@fantomas.sk>

On 13.09.19 03:12, sknz wrote:
>For clarification, I'm running an AP-Hotspot server(coovachilli, freeradius,
>squid, etc.) with two NIC(eth0 and eth1). eth0 is for WAN(internet) and eth1
>is for managing LAN(APs). Coovachilli is created tun0 under the eth1
>interface. I'm using squid-3.4.8 as an HTTP transparent proxy.

you still don't accept nor NAT connections from eth0 to port 80 in the world.

you only do that with tunneled connections.

># This is my updated squid.conf as your suggestion, 3129 for forward-proxy
>and 3130 for intercepting HTTP:
>http_port 3129
>http_port 3130 intercept

I really wonder why didn't you keep 3128 for forward proxy as before.
People using explicit proxy on port 3128 wouldn't have to change it.

The intercepting port doesn't matter much, because it's only between
firewall/NAT and squid, users won't see it.

>I've tried removing all "-i" options and updating mangle rules as your
>suggestion from iptables; no effects on it. I've opened intercept port also.
>This is my original iptables with adjusted rules:
>
>#nat
>-A PREROUTING -s 10.1.0.0/24 ! -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport
>80 -j REDIRECT --to-ports 3130    #redirect http to squid intercept port
>-A POSTROUTING -o eth0 -j MASQUERADE
>
>#mangle
>-A PREROUTING -s 10.1.0.0/24 -d 10.1.0.1/32 -p tcp -m tcp --dport 3130 -j
>DROP    #drop direct attempts to proxy intercept port

don't you drop all connections that should be natted to 3130 here?
Try dropping this rule.

>-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
>--clamp-mss-to-pmtu
>
>#filters
>-A INPUT -i eth1 -j DROP

I wonder you can communicate with LAN at all, when you drop anyting coming from it.

>-A INPUT -d 10.1.0.1/32 -i tun0 -p icmp -j ACCEPT
>-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 53 -j ACCEPT
>-A INPUT -d 10.1.0.1/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
>-A INPUT -d 255.255.255.255/32 -i tun0 -p udp -m udp --dport 67:68 -j ACCEPT
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3130 -j ACCEPT
>#squid intercept
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3129 -j ACCEPT
>#squid forward
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT
>#chilli controller
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 53 -j ACCEPT
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 2812 -j ACCEPT #
>freeradius
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 22 -j ACCEPT
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 443 -j ACCEPT    #
>https
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 80 -j ACCEPT    #http
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 4990 -j ACCEPT
>#hotspot UAM
>-A INPUT -d 10.1.0.1/32 -i tun0 -p tcp -m tcp --dport 3990 -j ACCEPT
>-A INPUT -d 10.1.0.1/32 -i tun0 -j DROP
>-A FORWARD -i tun0 -o eth0 -j ACCEPT
>-A FORWARD -i tun0 ! -o eth0 -j DROP
>-A FORWARD -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS
>--clamp-mss-to-pmtu
>-A FORWARD -o tun0 -j ACCEPT
>-A FORWARD -i tun0 -j ACCEPT
>-A FORWARD -o eth1 -j DROP
>-A FORWARD -i eth1 -j DROP

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I wonder how much deeper the ocean would be without sponges.


From squid3 at treenet.co.nz  Fri Sep 13 09:17:47 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Sep 2019 21:17:47 +1200
Subject: [squid-users] Multiple LDAP authentication server for Squid
In-Reply-To: <9607080a-6631-1e84-88bc-13f09f500988@unibs.it>
References: <9607080a-6631-1e84-88bc-13f09f500988@unibs.it>
Message-ID: <0830fc57-f4a8-fd61-3a82-1422755b5d17@treenet.co.nz>

On 12/09/19 10:41 pm, Antonino Sanacori wrote:
> Hi.
> 
> I use one ldap server for authentication of my users but now i have new
> users on another branch of same ldap server.
> 
> How can I configure squid.conf for support ldap authentication of my
> users on different branches?
> 

Squid does not do LDAP itself.

Find out instead whether the helper you are using for auth is capable of it.

The Basic auth can use a search filter instead of an absolute UID
parameter.
<http://www.squid-cache.org/Versions/v4/manuals/basic_ldap_auth.html>

Amos


From sakibnizam at gmail.com  Fri Sep 13 09:23:01 2019
From: sakibnizam at gmail.com (sknz)
Date: Fri, 13 Sep 2019 04:23:01 -0500 (CDT)
Subject: [squid-users] intercept vs. accel vhost allow-direct
In-Reply-To: <20190913083606.GA21563@fantomas.sk>
References: <1568277800425-0.post@n4.nabble.com>
 <ae1d3991-9959-e1a2-27c5-060ebc4fad76@treenet.co.nz>
 <1568282038926-0.post@n4.nabble.com>
 <68044a53-b15c-2ef7-6817-88e956709bdb@treenet.co.nz>
 <1568362359475-0.post@n4.nabble.com> <20190913083606.GA21563@fantomas.sk>
Message-ID: <1568366581628-0.post@n4.nabble.com>

Hello,
Okay, forward proxy port updated to 3128.  This is my scenario here:

Internet <> eth0 LAN port <> Server
WIFI AP-User <> WIFI AP <> eth1 LAN port <> Server

So my AP-USER can browse HTTPS for now, but not HTTP. And Squid3 is handling
HTTP transparent proxy here. If I remove Squid3 from the scenerio, AP-USER
can browse both HTTP and HTTPS. In the server, CoovaChilli takes control of
the internal interface (eth1) using a raw promiscuous socket. It then uses
the tun0 to pass and receive packets to and from the WAN(eth0).

http_port 3128
http_port 3130 intercept

Even AP-USER can browse both http and https, if I replace the above config
with: 

http_port 3128 accel vhost allow-direct
-A PREROUTING ..... --dport 80 -j REDIRECT --to-ports 3128

It only doesn't work when I configure two-port and HTTP transparent proxy.

I tried dropping the drop rule, it doesn't work. Please check my full
iptables here: 
https://paste.grasehotspot.org/view/raw/529efd6c
<https://paste.grasehotspot.org/view/raw/529efd6c>  







--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From augustus_meyer at gmx.net  Fri Sep 13 16:40:04 2019
From: augustus_meyer at gmx.net (reinerotto)
Date: Fri, 13 Sep 2019 11:40:04 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
In-Reply-To: <1568348861951-0.post@n4.nabble.com>
References: <1568348861951-0.post@n4.nabble.com>
Message-ID: <1568392804228-0.post@n4.nabble.com>

Looks like an issue regarding iptables. Because coova-chilli modifies the
rules, during start-up.
So I doubt, the rules in your post are incomplete, _not_ after start of
coova.
Definitely, this is not a squid issue.
BTW: I  have squid intercept running on openwrt devices. For commercial
hotspots.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sakibnizam at gmail.com  Fri Sep 13 16:48:41 2019
From: sakibnizam at gmail.com (sknz)
Date: Fri, 13 Sep 2019 11:48:41 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
In-Reply-To: <1568392804228-0.post@n4.nabble.com>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com>
Message-ID: <1568393321356-0.post@n4.nabble.com>

Hello reinerotto,
 I've been stuck here for 3 days! This is complete iptable rules after
coova-chilli starts : https://paste.grasehotspot.org/view/raw/529efd6c

Please have a look at it.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sat Sep 14 05:10:58 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Sep 2019 17:10:58 +1200
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
 working
In-Reply-To: <1568393321356-0.post@n4.nabble.com>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com> <1568393321356-0.post@n4.nabble.com>
Message-ID: <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>

On 14/09/19 4:48 am, sknz wrote:
> Hello reinerotto,
>  I've been stuck here for 3 days! This is complete iptable rules after
> coova-chilli starts : https://paste.grasehotspot.org/view/raw/529efd6c
> 

Each time you have posted details about your situation the ports used
have been different from the ones the question prompting your response
were asking about and/or suggesting you try. Perhapse it is time to
start methodical debugging instead of experiments based on guesses and
partial information.

Reset your config to match this known working config:
<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Then start with looking at your test. What tool are you using to test,
how is it configured, and what is your exact test command to it?

What output does your test tool deliver to the Chilli box?
Is there anything coming out from Squid in response to that?

Both TCP packet details and if possible HTTP message syntax for those
last two questions. Do a tcpdump packet capture to get all that in one
place.


Amos


From sakibnizam at gmail.com  Sat Sep 14 07:43:07 2019
From: sakibnizam at gmail.com (sknz)
Date: Sat, 14 Sep 2019 02:43:07 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
In-Reply-To: <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com> <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
Message-ID: <1568446987786-0.post@n4.nabble.com>

Hello Amos,
Okay, ports are fixed from here and forwarded 80 to 3127 in iptables.

http_port 3128 # for proxy client
http_port 3127 intercept # for http intercept


When a user tries to connect an HTTP site,

tcpdump -vv -ni eth1 port 80 >>>
https://paste.grasehotspot.org/view/raw/f81a60e4

tcpdump -vv -ni tun0 port 80 >>>
https://paste.grasehotspot.org/view/raw/bb0a4bc1

tcpdump -vv -ni eth0 port 80 >>>
https://paste.grasehotspot.org/view/raw/563912fd

... and the user never sees any output in the browser window. It's not
working somewhere in between tun0 <--> eth0. eth0 is WAN here. When I use a
forward proxy(http_port 3127 accel allow-direct), I can see the data passing
through all three interfaces, and it works.


### For an HTTPS site - Only 1st 5 packets(though squid is not handling
https),

tcpdump -vv -ni eth1 port 443 >>>
https://paste.grasehotspot.org/view/raw/11120563

tcpdump -vv -ni tun0 port 443 >>>
https://paste.grasehotspot.org/view/raw/2d38b62b

tcpdump -vv -ni eth0 port 443 >>>
https://paste.grasehotspot.org/view/raw/1a62299b









--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sat Sep 14 10:09:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Sep 2019 22:09:10 +1200
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
 working
In-Reply-To: <1568446987786-0.post@n4.nabble.com>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com> <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
 <1568446987786-0.post@n4.nabble.com>
Message-ID: <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>

On 14/09/19 7:43 pm, sknz wrote:
> Hello Amos,
> Okay, ports are fixed from here and forwarded 80 to 3127 in iptables.
> 
> http_port 3128 # for proxy client
> http_port 3127 intercept # for http intercept
> 

This does not match the config suggested.

Can you please re-post the config used with the below captures.


> 
> When a user tries to connect an HTTP site,
> 
> tcpdump -vv -ni eth1 port 80 >>>
> https://paste.grasehotspot.org/view/raw/f81a60e4
> 
> tcpdump -vv -ni tun0 port 80 >>>
> https://paste.grasehotspot.org/view/raw/bb0a4bc1
> 
> tcpdump -vv -ni eth0 port 80 >>>
> https://paste.grasehotspot.org/view/raw/563912fd
> 
> ... and the user never sees any output in the browser window. It's not
> working somewhere in between tun0 <--> eth0. eth0 is WAN here.


The thing is - Squid, four layers of NAT, one more trip through the
Chilli portal engine, and two cycles through the firewall all sit in
that problem area. That is a LOT of complexity - figuring out what is
going on is difficult enough before you go changing the settings in
unexpected ways with every post to the mailing list.

What we are doing here is working through those carefully checking what
the traffic is doing until the exact problem point is found.

So far the traces show one trip through Chilli is working okay.

Amos


From sakibnizam at gmail.com  Sat Sep 14 11:01:14 2019
From: sakibnizam at gmail.com (sknz)
Date: Sat, 14 Sep 2019 06:01:14 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
In-Reply-To: <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com> <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
 <1568446987786-0.post@n4.nabble.com>
 <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
Message-ID: <1568458874507-0.post@n4.nabble.com>

Sorry if I make it more puzzled.

Here full packets and config :
https://paste.grasehotspot.org/view/raw/384d2a8b

Here full iptable rules : https://paste.grasehotspot.org/view/raw/eaf29a16



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Sat Sep 14 13:49:29 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 14 Sep 2019 15:49:29 +0200
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
 working
In-Reply-To: <1568458874507-0.post@n4.nabble.com>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com>
 <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
 <1568446987786-0.post@n4.nabble.com>
 <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
 <1568458874507-0.post@n4.nabble.com>
Message-ID: <20190914134929.GA10511@fantomas.sk>

On 14.09.19 06:01, sknz wrote:
>Sorry if I make it more puzzled.
>
>Here full packets and config :
>https://paste.grasehotspot.org/view/raw/384d2a8b
>
>Here full iptable rules : https://paste.grasehotspot.org/view/raw/eaf29a16

- do you really use IP to IP tunelling ? Does not look like it.

- from your config I can only guess that eth1 is useless because all traffic
  is dropped. Yet your tcpdump shows packets entering eth1.

- have you ever heard about statefull firewall?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
It's now safe to throw off your computer.


From eperez at quadrianweb.com  Sat Sep 14 21:07:10 2019
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Sat, 14 Sep 2019 16:07:10 -0500
Subject: [squid-users] squid 4.8 web reports
Message-ID: <CACXMG+uDuwKBaYfLOpWVcyTJJYVhbfo=f-d722xLo=JfJ1=Few@mail.gmail.com>

Good day,
using Squid 4.8 for about 200 users here.
I would like to generate reports, but on the squid website page most
of the tools listed are several years outdated. (example: calamaris)
Are there some recent web-based report generators for squid 4.8?
Tried Screen Squid but never got it to work...interface never showed anything.

Please advise
---------------------
Erick Perez

---------------------


From squid3 at treenet.co.nz  Sun Sep 15 00:32:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Sep 2019 12:32:43 +1200
Subject: [squid-users] squid 4.8 web reports
In-Reply-To: <CACXMG+uDuwKBaYfLOpWVcyTJJYVhbfo=f-d722xLo=JfJ1=Few@mail.gmail.com>
References: <CACXMG+uDuwKBaYfLOpWVcyTJJYVhbfo=f-d722xLo=JfJ1=Few@mail.gmail.com>
Message-ID: <cde71bd4-178c-9fa6-c120-37314a8e7ef3@treenet.co.nz>

On 15/09/19 9:07 am, Erick Perez - Quadrian Enterprises wrote:
> Good day,
> using Squid 4.8 for about 200 users here.
> I would like to generate reports, but on the squid website page most
> of the tools listed are several years outdated. (example: calamaris)
> Are there some recent web-based report generators for squid 4.8?

The old tools for log analysis should still be good. The native log
format is essentially static, additions need to fit the defined format
or be for optional custom logs. So while some specific field values may
not be known to the tool, the good tools should be able to handle that
cleanly.


> Tried Screen Squid but never got it to work...interface never showed anything.
> 

That seems odd. IIRC it should at least be able to show you the log
entries. Though I have not used that helper myself.

Amos


From sakibnizam at gmail.com  Sun Sep 15 04:57:48 2019
From: sakibnizam at gmail.com (sknz)
Date: Sat, 14 Sep 2019 23:57:48 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
In-Reply-To: <20190914134929.GA10511@fantomas.sk>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com> <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
 <1568446987786-0.post@n4.nabble.com>
 <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
 <1568458874507-0.post@n4.nabble.com> <20190914134929.GA10511@fantomas.sk>
Message-ID: <1568523468700-0.post@n4.nabble.com>

eht1 is not useless really, Coovachilli created tun0 under eth1. Yes, I've
heard about stateful firewall, though this is not my domain of expertise.

/CoovaChilli takes control of the internal interface (eth1) using a raw
promiscuous socket. It then uses the vtun kernel module to bring up a
virtual interface tun0 to pass and receive packets to and from the
eth0(WAN). In fact, the vtun kernel module is used to move IP packets from
the kernel to user mode, in such a way that CoovaChilli can function without
any non-standard kernel modules. CoovaChilli then provides DHCP, ARP, and
HTTP Hijacking on the "dhcpif" interface, in our case that's eth0/



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sakibnizam at gmail.com  Sun Sep 15 05:03:47 2019
From: sakibnizam at gmail.com (sknz)
Date: Sun, 15 Sep 2019 00:03:47 -0500 (CDT)
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
	working
In-Reply-To: <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com> <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
 <1568446987786-0.post@n4.nabble.com>
 <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
Message-ID: <1568523827664-0.post@n4.nabble.com>

So I was testing from a client device(10.1.0.2) which is connected over WiFi
to an AP and that AP is connected to eth1 physically. In case you're
wondering, eth1 is connected to the server physically. Trying to connect an
HTTP website from the above-mentioned client device...

================

https://paste.grasehotspot.org/view/raw/4bdf03c0

So in eth1, we received the request from the client.

================

https://paste.grasehotspot.org/view/raw/b5db07c0

So tun0 also received it from eth1. Now, tun0 is supposed to send it to
eth0(WAN)...

================

https://paste.grasehotspot.org/view/raw/0b69d9db

But we don't receive anything at all in eth0(WAN)... Now let's see what
we've got back from tun0/eth1 to client device.

================

https://paste.grasehotspot.org/view/raw/9ad866a5

tun0 is passing back the acknowledgment to eth1 and eth1 is also doing same
to client device... and it stops here. So no actual data!

================

I guess from here tun0 is supposed to send it to squid and squid is suppose
to send it to eth0(WAN). Correct me if I'm wrong. I have got a Perl script
which shows TOP like interface for real-time packets. So the moment client
device tries to connect to HTTP, port 3127 gets GREEN which means something
sent/received. But it never gets green for forward-proxy port 3128.

Perl Script Screenshot: https://ibb.co/5nc0rWb









--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Sun Sep 15 09:09:12 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 15 Sep 2019 11:09:12 +0200
Subject: [squid-users] Squid Transparent Proxy with Coovachilli is not
 working
In-Reply-To: <1568523468700-0.post@n4.nabble.com>
References: <1568348861951-0.post@n4.nabble.com>
 <1568392804228-0.post@n4.nabble.com>
 <1568393321356-0.post@n4.nabble.com>
 <e32de5c4-ce88-4111-91a5-da970b086457@treenet.co.nz>
 <1568446987786-0.post@n4.nabble.com>
 <ad8b0b84-73bb-274e-247c-ac1dec2f6841@treenet.co.nz>
 <1568458874507-0.post@n4.nabble.com>
 <20190914134929.GA10511@fantomas.sk>
 <1568523468700-0.post@n4.nabble.com>
Message-ID: <20190915090912.GA4063@fantomas.sk>

On 14.09.19 23:57, sknz wrote:
>eht1 is not useless really, Coovachilli created tun0 under eth1. Yes, I've
>heard about stateful firewall, though this is not my domain of expertise.

it's very hard to guess what's the problem and how should the solution look
like, when someone does this to passing network traffic. Correct solutions
may work, incorrect may not, when someone does modify traffic like this.

>/CoovaChilli takes control of the internal interface (eth1) using a raw
>promiscuous socket. It then uses the vtun kernel module to bring up a
>virtual interface tun0 to pass and receive packets to and from the
>eth0(WAN). In fact, the vtun kernel module is used to move IP packets from
>the kernel to user mode, in such a way that CoovaChilli can function without
>any non-standard kernel modules. CoovaChilli then provides DHCP, ARP, and
>HTTP Hijacking on the "dhcpif" interface, in our case that's eth0/

I believe you should ask in coovachilli forums/lists for proper solutions.

However, from packet capture it seems that requests are really getting to
squid (they are being responded to), so squid logs shouls show.

Or, it may be the coovachilli manipulating them. Try asking coovchilli.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Atheism is a non-prophet organization.


From sakibnizam at gmail.com  Sun Sep 15 10:34:52 2019
From: sakibnizam at gmail.com (sknz)
Date: Sun, 15 Sep 2019 05:34:52 -0500 (CDT)
Subject: [squid-users] Non-Transparent HTTP+HTTP Proxy
Message-ID: <1568543692104-0.post@n4.nabble.com>

This is the configuration for my HTTP+HTTPS transparent proxy. I'm using this
for logging HTTP and HTTPS traffic without issuing a client certificate. How
to modify this configuration to make it NON-TRANSPARENT?  In WEB-PROXY which
is based on Squid, we can disable it by adding "disable-transparent" clause.

http_port 3128
http_port 3126 intercept
https_port 3127 intercept ssl-bump generate-host-certificates=off
cert=/etc/squid3/certs/squid.pem

acl ssl-bump_port myportname 3127
always_direct allow ssl-bump_port 
ssl-bump none all

Then we add iptable rules to redirect port 80->3126 and 443->3127, so
proxy-client can connect on 3128.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eype69 at gmail.com  Sun Sep 15 10:41:39 2019
From: eype69 at gmail.com (John Sweet-Escott)
Date: Sun, 15 Sep 2019 11:41:39 +0100
Subject: [squid-users] Problem with ssl_choose_client_version:inappropriate
 fallback on some sites when using TLS1.2
Message-ID: <CAAOXCCd_i8hM3+Lp0om9isJVPPqpCVmBD1XhUiimCX_9yYjNxA@mail.gmail.com>

Hi All

We are trying to run Squid 4.8, compiled with OpenSSL 1.1.1 (see [1]) on
Ubuntu 18.04 as a transparent proxy for the purpose of egress filtering of
HTTPS traffic using SNI (see config in [2]). It it works correctly when
contacting some addresses (e.g. https://www.ubuntu.com) but not others
(e.g. https://www.google.com). When we contact https://www.google.com using
TLS1.2 we get the error in the logs:
2019/09/15 10:33:09 kid1| ERROR: negotiating TLS on FD 19:
error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
fallback (1/-1/0)
and the page returned to the client contains ERR_SECURE_CONNECT_FAIL. When
TLS1.3 is used, the connections are made correctly, however my application
is constrained to java8 & tomcat8 which does not support TLS1.3.

Connections are made using curl or openssl s_client. For example:
openssl s_client -tls1_2  -CAfile squid.crt -connect www.google.com:443
-tlsextdebug
[237/1854]CONNECTED(00000005)
TLS server extension "renegotiation info" (id=65281), len=1
0000 - 00                                                .
TLS server extension "EC point formats" (id=11), len=4
0000 - 03 00 01 02                                       ....
TLS server extension "session ticket" (id=35), len=0
TLS server extension "extended master secret" (id=23), len=0
depth=1 C = GB, ST = London, L = squid, O = squid, CN = squid
verify return:1
depth=0 CN = www.google.com
verify return:1
---
Certificate chain
 0 s:CN = www.google.com
   i:C = GB, ST = London, L = squid, O = squid, CN = squid
 1 s:C = GB, ST = London, L = squid, O = squid, CN = squid
   i:C = GB, ST = London, L = squid, O = squid, CN = squid
 2 s:C = GB, ST = London, L = squid, O = squid, CN = squid
   i:C = GB, ST = London, L = squid, O = squid, CN = squid
etc

Attached are pcap files showing first a bad connection to google and then a
working connection to ubuntu. Looking at the pcap files the difference in
the google and ubuntu server hello lies in the extensions and the cypher:
Google:
Handshake Protocol: Server Hello
    Handshake Type: Server Hello (2)
    Length: 59
    Version: TLS 1.2 (0x0303)
    Random: 5d7e05552e1fdea260f67e0bdf413f6a9837fbaffdebeb35?
    Session ID Length: 0
    Cipher Suite: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 (0xcca9)
    Compression Method: null (0)
    Extensions Length: 19
    Extension: extended_master_secret (len=0)
    Extension: renegotiation_info (len=1)
    Extension: ec_point_formats (len=2)
    Extension: session_ticket (len=0)
Ubuntu:
Handshake Protocol: Server Hello
    Handshake Type: Server Hello (2)
    Length: 61
    Version: TLS 1.2 (0x0303)
    Random: 7ec2c3a2554bac610e0290ac1f160c3ed185bdd1159e377c?
    Session ID Length: 0
    Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)
    Compression Method: null (0)
    Extensions Length: 21
    Extension: server_name (len=0)
    Extension: renegotiation_info (len=1)
    Extension: ec_point_formats (len=4)
    Extension: session_ticket (len=0)
Differences are that Google supplies extended_master_secretand Ubuntu
supplies server_name extensions. The cyphers chosen by the server is also
different. Enabling debug using squid -k debug indicates that this is the
likely problematic area:

2019/09/15 11:21:02.486 kid1| 83,5| PeerConnector.cc(712)
checkForMissingCertificates: SSL server sent 2 certificates
2019/09/15 11:21:02.486 kid1| 83,7| AsyncCall.cc(26) AsyncCall: The
AsyncCall Security::PeerConnector::negotiate constructed,
this=0x560d937c94b0 [call515701]2019/09/15 11:21:02.486 kid1| 83,7|
AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(391) will call
Security::PeerConnector::negotiate() [call515701]
2019/09/15 11:21:02.486 kid1| 83,7| AsyncJob.cc(154) callEnd:
Ssl::PeekingPeerConnector status out: [ FD 19 job24663]
2019/09/15 11:21:02.486 kid1| 83,7| AsyncCallQueue.cc(57) fireNext: leaving
Security::PeerConnector::negotiate()
2019/09/15 11:21:02.486 kid1| 83,7| AsyncCallQueue.cc(55) fireNext:
entering Security::PeerConnector::negotiate()
2019/09/15 11:21:02.486 kid1| 83,7| AsyncCall.cc(38) make: make call
Security::PeerConnector::negotiate [call515701]
2019/09/15 11:21:02.486 kid1| 83,7| AsyncJob.cc(123) callStart:
Ssl::PeekingPeerConnector status in: [ FD 19 job24663]
2019/09/15 11:21:02.486 kid1| 83,5| PeerConnector.cc(188) negotiate:
SSL_connect session=0x560d93835950
2019/09/15 11:21:02.486 kid1| 83,7| bio.cc(356) giveBuffered: 5<=5 bytes to
OpenSSL
2019/09/15 11:21:02.486 kid1| 83,7| bio.cc(356) giveBuffered: 63<=63 bytes
to OpenSSL
2019/09/15 11:21:02.486 kid1| 83,7| bio.cc(164) stateChanged: FD 19 now:
0x1001 TWCH (SSLv3/TLS write client hello)
2019/09/15 11:21:02.486 kid1| 83,7| bio.cc(471) write: postpone writing 7
bytes to SSL FD 19
2019/09/15 11:21:02.486 kid1| 83,7| bio.cc(164) stateChanged: FD 19 now:
0x4008 SSLERR (error)
2019/09/15 11:21:02.486 kid1| 83,7| bio.cc(164) stateChanged: FD 19 now:
0x1002 SSLERR (error)
2019/09/15 11:21:02.486 kid1| 83,5| NegotiationHistory.cc(83)
retrieveNegotiatedInfo: SSL connection info on FD 19 SSL version NONE/0.0
negotiated cipher
2019/09/15 11:21:02.486 kid1| ERROR: negotiating TLS on FD 19:
error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
fallback (1/-1/0)
2019/09/15 11:21:02.486 kid1| 83,5| PeerConnector.cc(570) callBack: TLS
setup ended for local=10.20.251.235:37882 remote=216.58.210.196:443 FD 19
flags=1

I am not sure how to resolve this issue. I am guessing that there is
something in the response from Google that is causing OpenSSL to reject the
connections. From what I can see, there is no evidence of a downgrade being
attempted, which is what the inappropriate fallback error message might
indicate.

Any advice/guidance greatfully recieved.
John

[1] OpenSSL and Squid versions
openssl version
OpenSSL 1.1.1  11 Sep 2018

squid -v
Squid Cache: Version 4.8
Service Name: squid
Ubuntu linux
This binary uses OpenSSL 1.1.1  11 Sep 2018. For legal restrictions on
distribution see https://www.openssl.org/source/license.html
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--$
ysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid'
'--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silen$
-rules' 'BUILDCXXFLAGS=-g -O2
-fdebug-prefix-map=/home/builder/diladele/squid-ubuntu/src/ubuntu18/scripts.squid4/build/squid/squid-4.8=.
-fstack-protector-strong -$
format -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2
-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic'
'BUILDCXX=x86_64-linux-gn$
-g++' '--with-build-environment=default' '--enable-build-info=Ubuntu linux'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '-$
mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' $
--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client'
'--enable-ssl' '--enable-ssl-crtd' '--with-openssl'
'--enable-follow-x-forwarded-for' '--enab$
e-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntl
m=fake,SMB_LM'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
'--enable-security-
cert-validators=fake' '--enable-storeid-rewrite-helpers=file'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '
--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-filedescriptors=
65536' '--with-large-files' '--with-default-user=proxy' '--with-gnutls'
'--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu'
'CC=x86_64-linux-gnu-gcc' 'CFLAGS=
-g -O2
-fdebug-prefix-map=/home/builder/diladele/squid-ubuntu/src/ubuntu18/scripts.squid4/build/squid/squid-4.8=.
-fstack-protector-strong -Wformat -Werror=format-s
ecurity -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now
-Wl,--as-needed -latomic' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
'CXX=x86_64-linux-gnu-
g++' 'CXXFLAGS=-g -O2
-fdebug-prefix-map=/home/builder/diladele/squid-ubuntu/src/ubuntu18/scripts.squid4/build/squid/squid-4.8=.
-fstack-protector-strong -Wformat -
Werror=format-security'

[2] Configuration squid.conf
visible_hostname squid
max_filedesc 4096

http_port 3128
#Handling HTTP requests
http_port 3129 intercept
include /etc/squid/http_sites.conf
http_access allow allowed_http_sites

#Handling HTTPS requests
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
acl SSL_port port 443http_access allow SSL_port
# Allows access to www.google.com and www.ubuntu.com
include /etc/squid/https_sites.conf

# TLS peek-and-splice configuration
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sitesssl_bump terminate step2 all
http_access deny all
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190915/2a92d803/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: badgoodonly.pcap
Type: application/octet-stream
Size: 15722 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190915/2a92d803/attachment.obj>

From sakibnizam at gmail.com  Mon Sep 16 08:27:00 2019
From: sakibnizam at gmail.com (sknz)
Date: Mon, 16 Sep 2019 03:27:00 -0500 (CDT)
Subject: [squid-users] Non-Transparent HTTP+HTTP Proxy
In-Reply-To: <1568543692104-0.post@n4.nabble.com>
References: <1568543692104-0.post@n4.nabble.com>
Message-ID: <1568622420517-0.post@n4.nabble.com>

So my straight-forward goal is here,

i. Non-transparent proxy ( transparent doesn't work with captive portal )
ii. LOG HTTP and HTTPS traffic ( for HTTPS hostname will do )
iii. Without issuing any certificate in client device

Is it possible with Squid 3.5?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Mon Sep 16 10:07:04 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 16 Sep 2019 12:07:04 +0200
Subject: [squid-users] Non-Transparent HTTP+HTTP Proxy
In-Reply-To: <1568622420517-0.post@n4.nabble.com>
References: <1568543692104-0.post@n4.nabble.com>
 <1568622420517-0.post@n4.nabble.com>
Message-ID: <20190916100704.GA2934@fantomas.sk>

On 16.09.19 03:27, sknz wrote:
>So my straight-forward goal is here,
>
>i. Non-transparent proxy ( transparent doesn't work with captive portal )

if you mean, explicit proxy (client must explicitly configure it), not an
intercepting proxy (http connections are redirected to proxy, browser
doesn't know), then it's the default squid configuration.

You must explicitly allow connections from client ips.

however, captive portals could work with transparent proxy too. 
With http, no problem, with https, you need certificate.
But, mobile phones test connections when checking new wi-fi network, so they
can find out and report authorisation requirements on caprivw portals.

>ii. LOG HTTP and HTTPS traffic ( for HTTPS hostname will do )

this is on by default, unless you use distribution that turned it off.

>iii. Without issuing any certificate in client device

dtandard behaviour.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fucking windows! Bring Bill Gates! (Southpark the movie)


From antonino.sanacori at unibs.it  Mon Sep 16 10:17:12 2019
From: antonino.sanacori at unibs.it (Antonino Sanacori)
Date: Mon, 16 Sep 2019 12:17:12 +0200
Subject: [squid-users] Multiple LDAP authentication server for Squid
In-Reply-To: <0830fc57-f4a8-fd61-3a82-1422755b5d17@treenet.co.nz>
References: <9607080a-6631-1e84-88bc-13f09f500988@unibs.it>
 <0830fc57-f4a8-fd61-3a82-1422755b5d17@treenet.co.nz>
Message-ID: <bddbb530-8ee9-2c7e-2ebd-256faaaa26c1@unibs.it>

Thanks Amos but I have a 3.x version.

Antonino

On 13/09/2019 11:17, Amos Jeffries wrote:
> On 12/09/19 10:41 pm, Antonino Sanacori wrote:
>> Hi.
>>
>> I use one ldap server for authentication of my users but now i have new
>> users on another branch of same ldap server.
>>
>> How can I configure squid.conf for support ldap authentication of my
>> users on different branches?
>>
> Squid does not do LDAP itself.
>
> Find out instead whether the helper you are using for auth is capable of it.
>
> The Basic auth can use a search filter instead of an absolute UID
> parameter.
> <http://www.squid-cache.org/Versions/v4/manuals/basic_ldap_auth.html>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Gestione Portale d'Ateneo

Universit? degli Studi di Brescia

Tel. 030298.8325


-- 


Informativa sulla Privacy: http://www.unibs.it/node/8155 
<http://www.unibs.it/node/8155>


From Antony.Stone at squid.open.source.it  Mon Sep 16 10:22:47 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 16 Sep 2019 12:22:47 +0200
Subject: [squid-users] Multiple LDAP authentication server for Squid
In-Reply-To: <bddbb530-8ee9-2c7e-2ebd-256faaaa26c1@unibs.it>
References: <9607080a-6631-1e84-88bc-13f09f500988@unibs.it>
 <0830fc57-f4a8-fd61-3a82-1422755b5d17@treenet.co.nz>
 <bddbb530-8ee9-2c7e-2ebd-256faaaa26c1@unibs.it>
Message-ID: <201909161222.47231.Antony.Stone@squid.open.source.it>

On Monday 16 September 2019 at 12:17:12, Antonino Sanacori wrote:

> Thanks Amos but I have a 3.x version.

Try http://www.squid-cache.org/Versions/v3/3.5/manuals/basic_ldap_auth.html 
then.

Antony.

> On 13/09/2019 11:17, Amos Jeffries wrote:
> > On 12/09/19 10:41 pm, Antonino Sanacori wrote:
> >> Hi.
> >> 
> >> I use one ldap server for authentication of my users but now i have new
> >> users on another branch of same ldap server.
> >> 
> >> How can I configure squid.conf for support ldap authentication of my
> >> users on different branches?
> > 
> > Squid does not do LDAP itself.
> > 
> > Find out instead whether the helper you are using for auth is capable of
> > it.
> > 
> > The Basic auth can use a search filter instead of an absolute UID
> > parameter.
> > <http://www.squid-cache.org/Versions/v4/manuals/basic_ldap_auth.html>
> > 
> > Amos

-- 
Numerous psychological studies over the years have demonstrated that the 
majority of people genuinely believe they are not like the majority of people.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From sakibnizam at gmail.com  Mon Sep 16 10:41:41 2019
From: sakibnizam at gmail.com (sknz)
Date: Mon, 16 Sep 2019 05:41:41 -0500 (CDT)
Subject: [squid-users] Non-Transparent HTTP+HTTP Proxy
In-Reply-To: <20190916100704.GA2934@fantomas.sk>
References: <1568543692104-0.post@n4.nabble.com>
 <1568622420517-0.post@n4.nabble.com> <20190916100704.GA2934@fantomas.sk>
Message-ID: <1568630501827-0.post@n4.nabble.com>

I'm trying to configure Squid 3.5.3 for access controller/captive portal last
few days.

#1 For this config, on client device:  *URL could not be retrieved - Invalid
Url*
http_port 

#2 Squid log throws an Error - No forward port
http_port 3128 intercept    

#3 On client device:  *URL could not be retrieved - Invalid Url*
http_port 3128                                                                                                                                                                                                    
http_port 3127 intercept 

#4 On client device: Unable to forward this request
http_port 3128 accel

#5 Now this works!
http_port 3128 accel allow-direct


Under same settings in other things, I've changed Squid config # 1 to 5, can
you guess what's happening here? What's so special about "allow-direct"
here?  Why transparent proxy is not working? Why forward proxy is working
only with "allow-direct"?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sakibnizam at gmail.com  Mon Sep 16 10:45:39 2019
From: sakibnizam at gmail.com (sknz)
Date: Mon, 16 Sep 2019 05:45:39 -0500 (CDT)
Subject: [squid-users] Non-Transparent HTTP+HTTP Proxy
In-Reply-To: <1568630501827-0.post@n4.nabble.com>
References: <1568543692104-0.post@n4.nabble.com>
 <1568622420517-0.post@n4.nabble.com> <20190916100704.GA2934@fantomas.sk>
 <1568630501827-0.post@n4.nabble.com>
Message-ID: <1568630739661-0.post@n4.nabble.com>

[Updated] I'm trying to configure Squid 3.5.3 for access controller/captive
portal last few days.

#1 For this config, on client device:  *URL could not be retrieved - Invalid
Url*
http_port 3128

#2 Squid log throws an Error - No forward port
http_port 3128 intercept    

#3 On client device:  *URL could not be retrieved - Invalid Url*
http_port 3128                                                                                                                                                                                                    
http_port 3127 intercept

#4 On client device: Unable to forward this request
http_port 3128 accel

#5 Now this works!
http_port 3128 accel allow-direct

Under same settings in other things, I've changed Squid config # 1 to 5, can
you guess what's happening here? What's so special about "allow-direct"
here?  Why transparent proxy is not working? Why forward proxy is working
only with "allow-direct"?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Mon Sep 16 10:55:07 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 16 Sep 2019 12:55:07 +0200
Subject: [squid-users] Non-Transparent HTTP+HTTP Proxy
In-Reply-To: <1568630739661-0.post@n4.nabble.com>
References: <1568543692104-0.post@n4.nabble.com>
 <1568622420517-0.post@n4.nabble.com>
 <20190916100704.GA2934@fantomas.sk>
 <1568630501827-0.post@n4.nabble.com>
 <1568630739661-0.post@n4.nabble.com>
Message-ID: <20190916105506.GB2934@fantomas.sk>

On 16.09.19 05:45, sknz wrote:
>[Updated] I'm trying to configure Squid 3.5.3 for access controller/captive
>portal last few days.
>
>#1 For this config, on client device:  *URL could not be retrieved - Invalid
>Url*
>http_port 3128
>
>#2 Squid log throws an Error - No forward port
>http_port 3128 intercept
>
>#3 On client device:  *URL could not be retrieved - Invalid Url*
>http_port 3128
>http_port 3127 intercept
>
>#4 On client device: Unable to forward this request
>http_port 3128 accel
>
>#5 Now this works!
>http_port 3128 accel allow-direct
>
>Under same settings in other things, I've changed Squid config # 1 to 5, can
>you guess what's happening here? What's so special about "allow-direct"
>here?  Why transparent proxy is not working? Why forward proxy is working
>only with "allow-direct"?

first, configure proxy with port3128 without "accel", "intercept", "tproxy",
and "ssl-bump".

port 3128 should not use first three, and using the fourth can make things
more compicated.

then, cofigure your browser to use proxy at port 3128. This must work.

"accel" and further "allow-direct" should be used on reverse proxies, not
when you use proxy for connecting clients to the world. They need proper
configuration on squid. They must not be used on forward proxy ports.

"intercept" and further "tproxy" should be used on different port, both need
special configuration on the router/firewall.


Note that clients with explicit proxy should not connect to intercept port,
and clients without explicit proxy should not connect to the standard 3128
port without intercept and tproxy.

when using intercept, you should allow connections from proxy to the world
and not redirect them back to the proxy.

As I have already said, if you use solutions like coovachilli, they should
provide instructions on how to configure intercepting proxy.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
99 percent of lawyers give the rest a bad name.


From felipeapolanco at gmail.com  Mon Sep 16 14:37:11 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Mon, 16 Sep 2019 10:37:11 -0400
Subject: [squid-users] How to extend Squid ICAP preview size
Message-ID: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>

Hi,

We would like to add some logic to our custom made ICAP server, one of
these logics is to analyze up to 10MB of data of a given file and if the
file is larger than that, squid should not keep sending it to icap,
basically, a 204 message should be returned.

We understand this is not possible with Transfer-Complete OPTION, because
that will send the whole file to the ICAP server, no matter the size.

And while Transfer-Preview may be good for this, squid has a cap of 64K of
preview size that limit us in the aspect.

Is there a way to extend this limit? Modifying the source code perhaps?
If so, what are the disadvantages of doing so? Would it require more RAM
per response or is RAM dynamically allocated as the file is being received?

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190916/9320cfe5/attachment.htm>

From rousskov at measurement-factory.com  Mon Sep 16 15:54:23 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Sep 2019 11:54:23 -0400
Subject: [squid-users] How to extend Squid ICAP preview size
In-Reply-To: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>
References: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>
Message-ID: <aedf10fe-5b19-5455-6ac0-9e34331fe79d@measurement-factory.com>

On 9/16/19 10:37 AM, Felipe Arturo Polanco wrote:

> We would like to add some logic to our custom made ICAP server, one of
> these logics is to analyze up to 10MB of data of a given file and if the
> file is larger than that, squid should not keep sending it to icap,
> basically, a 204 message should be returned.

> squid has a cap of 64K of preview size that limit us in the aspect.

True.


> Is there a way to extend this limit? Modifying the source code perhaps?

Yes, of course.


> If so, what are the disadvantages of doing so? 

Simply increasing BodyPipe::MaxCapacity might open you up to
difficult-to-find vulnerabilities where some old Squid code still
assumes that the buffers are limited by 64KB and crashes/asserts when
that assumption becomes false. FWIW, I have not audited all
BodyPipe-related code and do not know whether such bad code exists today.

Increasing BodyPipe::MaxCapacity will also increase Squid RAM
requirements if your Squid receives requests (and/or ICAP
REQMOD-generated HTTP responses) that exceed 64KB body limit. I am not
sure about regular responses -- their accumulation is limited by
other/independent restrictions.


> Would it require more RAM per response 

[You should worry about "per message" accumulation -- requests will use
the same limit, even if you do not have REQMOD services.]

Yes, but only for messages that exceed the limit.


> or is RAM dynamically allocated as the file is being received?

Yes, the RAM in question is allocated dynamically on "as needed" basis.

YMMV, but Squid may slow down a lot while dealing with the associated
reallocations because of the too-small 64KB increment used for each such
reallocation.


An alternative solution to consider here is extending ICAP features to
allow for splicing of the 10MB prefix (analyzed _and_ returned back by
the ICAP server) and the remaining virgin response suffix (paused
without excessive accumulation on the Squid side). This would be similar
to the existing ICAP 206 use-original-body="10MB" feature[1], but would
require additional negotiation and changes to disable storage of the
10MB prefix on the Squid side.

This alternative can be implemented without worrying about hidden 64KB
body buffer assumptions, but the implementation will not be trivial.

[1]
http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt


HTH,

Alex.


From felipeapolanco at gmail.com  Mon Sep 16 18:58:14 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Mon, 16 Sep 2019 14:58:14 -0400
Subject: [squid-users] How to extend Squid ICAP preview size
In-Reply-To: <aedf10fe-5b19-5455-6ac0-9e34331fe79d@measurement-factory.com>
References: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>
 <aedf10fe-5b19-5455-6ac0-9e34331fe79d@measurement-factory.com>
Message-ID: <CADcj3=4yiKaecFui1umBGhTK3b=G9zp6FC5Q8tVdj5u8DFJ6YQ@mail.gmail.com>

Thanks for the detailed response Alex.

In our case we don't need to modify the initial 10MB, just scan it for
virus and if found, send a reset back to squid to not transmit the file.

Do you have any additional recommendation for such case?

Thanks,



On Mon, Sep 16, 2019 at 11:54 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 9/16/19 10:37 AM, Felipe Arturo Polanco wrote:
>
> > We would like to add some logic to our custom made ICAP server, one of
> > these logics is to analyze up to 10MB of data of a given file and if the
> > file is larger than that, squid should not keep sending it to icap,
> > basically, a 204 message should be returned.
>
> > squid has a cap of 64K of preview size that limit us in the aspect.
>
> True.
>
>
> > Is there a way to extend this limit? Modifying the source code perhaps?
>
> Yes, of course.
>
>
> > If so, what are the disadvantages of doing so?
>
> Simply increasing BodyPipe::MaxCapacity might open you up to
> difficult-to-find vulnerabilities where some old Squid code still
> assumes that the buffers are limited by 64KB and crashes/asserts when
> that assumption becomes false. FWIW, I have not audited all
> BodyPipe-related code and do not know whether such bad code exists today.
>
> Increasing BodyPipe::MaxCapacity will also increase Squid RAM
> requirements if your Squid receives requests (and/or ICAP
> REQMOD-generated HTTP responses) that exceed 64KB body limit. I am not
> sure about regular responses -- their accumulation is limited by
> other/independent restrictions.
>
>
> > Would it require more RAM per response
>
> [You should worry about "per message" accumulation -- requests will use
> the same limit, even if you do not have REQMOD services.]
>
> Yes, but only for messages that exceed the limit.
>
>
> > or is RAM dynamically allocated as the file is being received?
>
> Yes, the RAM in question is allocated dynamically on "as needed" basis.
>
> YMMV, but Squid may slow down a lot while dealing with the associated
> reallocations because of the too-small 64KB increment used for each such
> reallocation.
>
>
> An alternative solution to consider here is extending ICAP features to
> allow for splicing of the 10MB prefix (analyzed _and_ returned back by
> the ICAP server) and the remaining virgin response suffix (paused
> without excessive accumulation on the Squid side). This would be similar
> to the existing ICAP 206 use-original-body="10MB" feature[1], but would
> require additional negotiation and changes to disable storage of the
> 10MB prefix on the Squid side.
>
> This alternative can be implemented without worrying about hidden 64KB
> body buffer assumptions, but the implementation will not be trivial.
>
> [1]
>
> http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190916/accd2b04/attachment.htm>

From rousskov at measurement-factory.com  Mon Sep 16 19:54:29 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Sep 2019 15:54:29 -0400
Subject: [squid-users] How to extend Squid ICAP preview size
In-Reply-To: <CADcj3=4yiKaecFui1umBGhTK3b=G9zp6FC5Q8tVdj5u8DFJ6YQ@mail.gmail.com>
References: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>
 <aedf10fe-5b19-5455-6ac0-9e34331fe79d@measurement-factory.com>
 <CADcj3=4yiKaecFui1umBGhTK3b=G9zp6FC5Q8tVdj5u8DFJ6YQ@mail.gmail.com>
Message-ID: <f4e6645b-cdc1-ee78-7020-4f9d821764ac@measurement-factory.com>

On 9/16/19 2:58 PM, Felipe Arturo Polanco wrote:

> In our case we don't need to modify the initial 10MB, just scan it for
> virus and if found, send a reset back to squid to not transmit the file.

Yes, my original response was based on that assumption.

In summary, you can ask Squid to own the 10MB prefix (i.e. do a huge
ICAP Preview) or you can ask the ICAP server to own the 10MB prefix
(i.e. extend ICAP 206). The choice balances various trade-offs,
including different overheads and different risks related to Squid
development associated with each option. Pick your poison :-).

Alex.


> On Mon, Sep 16, 2019 at 11:54 AM Alex Rousskov wrote:
> 
>     On 9/16/19 10:37 AM, Felipe Arturo Polanco wrote:
> 
>     > We would like to add some logic to our custom made ICAP server, one of
>     > these logics is to analyze up to 10MB of data of a given file and
>     if the
>     > file is larger than that, squid should not keep sending it to icap,
>     > basically, a 204 message should be returned.
> 
>     > squid has a cap of 64K of preview size that limit us in the aspect.
> 
>     True.
> 
> 
>     > Is there a way to extend this limit? Modifying the source code
>     perhaps?
> 
>     Yes, of course.
> 
> 
>     > If so, what are the disadvantages of doing so?
> 
>     Simply increasing BodyPipe::MaxCapacity might open you up to
>     difficult-to-find vulnerabilities where some old Squid code still
>     assumes that the buffers are limited by 64KB and crashes/asserts when
>     that assumption becomes false. FWIW, I have not audited all
>     BodyPipe-related code and do not know whether such bad code exists
>     today.
> 
>     Increasing BodyPipe::MaxCapacity will also increase Squid RAM
>     requirements if your Squid receives requests (and/or ICAP
>     REQMOD-generated HTTP responses) that exceed 64KB body limit. I am not
>     sure about regular responses -- their accumulation is limited by
>     other/independent restrictions.
> 
> 
>     > Would it require more RAM per response
> 
>     [You should worry about "per message" accumulation -- requests will use
>     the same limit, even if you do not have REQMOD services.]
> 
>     Yes, but only for messages that exceed the limit.
> 
> 
>     > or is RAM dynamically allocated as the file is being received?
> 
>     Yes, the RAM in question is allocated dynamically on "as needed" basis.
> 
>     YMMV, but Squid may slow down a lot while dealing with the associated
>     reallocations because of the too-small 64KB increment used for each such
>     reallocation.
> 
> 
>     An alternative solution to consider here is extending ICAP features to
>     allow for splicing of the 10MB prefix (analyzed _and_ returned back by
>     the ICAP server) and the remaining virgin response suffix (paused
>     without excessive accumulation on the Squid side). This would be similar
>     to the existing ICAP 206 use-original-body="10MB" feature[1], but would
>     require additional negotiation and changes to disable storage of the
>     10MB prefix on the Squid side.
> 
>     This alternative can be implemented without worrying about hidden 64KB
>     body buffer assumptions, but the implementation will not be trivial.
> 
>     [1]
>     http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt
> 
> 
>     HTH,
> 
>     Alex.
> 



From felipeapolanco at gmail.com  Mon Sep 16 21:26:21 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Mon, 16 Sep 2019 17:26:21 -0400
Subject: [squid-users] How to extend Squid ICAP preview size
In-Reply-To: <f4e6645b-cdc1-ee78-7020-4f9d821764ac@measurement-factory.com>
References: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>
 <aedf10fe-5b19-5455-6ac0-9e34331fe79d@measurement-factory.com>
 <CADcj3=4yiKaecFui1umBGhTK3b=G9zp6FC5Q8tVdj5u8DFJ6YQ@mail.gmail.com>
 <f4e6645b-cdc1-ee78-7020-4f9d821764ac@measurement-factory.com>
Message-ID: <CADcj3=651qbva5O-wCbHLOTEJKBikpiNV=s7mhZ8gKDaAL+JXw@mail.gmail.com>

Thanks for that Alex.

I do have another related question regarding Squid 206 handling.

I have a RESPMOD icap server that supports Allow:206 in the OPTIONS
response.

I can see that for html webpages, squid sends Allow: 204, 206, trailers in
RESPMOD requests, and my server can handle that fine.
But for files, specifically for zip files, squid sends Allow: trailers ,
showing no support for 206. Even if I reply with a 206, I get TCP reset and
connection is closed.

Do you know why squid doesn't Allow 206 for files?
============================
Here is my configured OPTIONS response.
ICAP/1.0 200 OK
Methods: RESPMOD
Service: PyICAP Server 1.0
Preview: 0
Transfer-Complete: *
Max-Connections: 100
Options-TTL: 3600
Allow: 206
ISTag: "CmLYfciETxyLwZW3XYsPwJdCXvVehd"
Date: b'Mon', 16 b'Sep' 2019 20:50:52 GMT
Server: BaseICAP/1.0 Python/3.7.3

==================================================

And here is the RESPMOD request.
RESPMOD icap://127.0.0.1:13440/example ICAP/1.0
Host: 127.0.0.1:13440
Date: Mon, 16 Sep 2019 20:51:00 GMT
Encapsulated: req-hdr=0, res-hdr=337, res-body=579
Allow: trailers
X-Client-IP: 192.168.0.6

GET http://203.0.113.1/file.zip HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:69.0)
Gecko/20100101 Firefox/69.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Upgrade-Insecure-Requests: 1
Host: 203.0.113.1

HTTP/1.1 200 OK
Date: Mon, 16 Sep 2019 20:51:00 GMT
Server: Apache/2.4.6 (CentOS)
Last-Modified: Thu, 29 Aug 2019 13:54:58 GMT
ETag: "989680-59141d9f0557a"
Accept-Ranges: bytes
Content-Length: 10000000
Content-Type: application/zip




On Mon, Sep 16, 2019 at 3:54 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 9/16/19 2:58 PM, Felipe Arturo Polanco wrote:
>
> > In our case we don't need to modify the initial 10MB, just scan it for
> > virus and if found, send a reset back to squid to not transmit the file.
>
> Yes, my original response was based on that assumption.
>
> In summary, you can ask Squid to own the 10MB prefix (i.e. do a huge
> ICAP Preview) or you can ask the ICAP server to own the 10MB prefix
> (i.e. extend ICAP 206). The choice balances various trade-offs,
> including different overheads and different risks related to Squid
> development associated with each option. Pick your poison :-).
>
> Alex.
>
>
> > On Mon, Sep 16, 2019 at 11:54 AM Alex Rousskov wrote:
> >
> >     On 9/16/19 10:37 AM, Felipe Arturo Polanco wrote:
> >
> >     > We would like to add some logic to our custom made ICAP server,
> one of
> >     > these logics is to analyze up to 10MB of data of a given file and
> >     if the
> >     > file is larger than that, squid should not keep sending it to icap,
> >     > basically, a 204 message should be returned.
> >
> >     > squid has a cap of 64K of preview size that limit us in the aspect.
> >
> >     True.
> >
> >
> >     > Is there a way to extend this limit? Modifying the source code
> >     perhaps?
> >
> >     Yes, of course.
> >
> >
> >     > If so, what are the disadvantages of doing so?
> >
> >     Simply increasing BodyPipe::MaxCapacity might open you up to
> >     difficult-to-find vulnerabilities where some old Squid code still
> >     assumes that the buffers are limited by 64KB and crashes/asserts when
> >     that assumption becomes false. FWIW, I have not audited all
> >     BodyPipe-related code and do not know whether such bad code exists
> >     today.
> >
> >     Increasing BodyPipe::MaxCapacity will also increase Squid RAM
> >     requirements if your Squid receives requests (and/or ICAP
> >     REQMOD-generated HTTP responses) that exceed 64KB body limit. I am
> not
> >     sure about regular responses -- their accumulation is limited by
> >     other/independent restrictions.
> >
> >
> >     > Would it require more RAM per response
> >
> >     [You should worry about "per message" accumulation -- requests will
> use
> >     the same limit, even if you do not have REQMOD services.]
> >
> >     Yes, but only for messages that exceed the limit.
> >
> >
> >     > or is RAM dynamically allocated as the file is being received?
> >
> >     Yes, the RAM in question is allocated dynamically on "as needed"
> basis.
> >
> >     YMMV, but Squid may slow down a lot while dealing with the associated
> >     reallocations because of the too-small 64KB increment used for each
> such
> >     reallocation.
> >
> >
> >     An alternative solution to consider here is extending ICAP features
> to
> >     allow for splicing of the 10MB prefix (analyzed _and_ returned back
> by
> >     the ICAP server) and the remaining virgin response suffix (paused
> >     without excessive accumulation on the Squid side). This would be
> similar
> >     to the existing ICAP 206 use-original-body="10MB" feature[1], but
> would
> >     require additional negotiation and changes to disable storage of the
> >     10MB prefix on the Squid side.
> >
> >     This alternative can be implemented without worrying about hidden
> 64KB
> >     body buffer assumptions, but the implementation will not be trivial.
> >
> >     [1]
> >
> http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190916/ec7d4154/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep 17 06:26:10 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Sep 2019 18:26:10 +1200
Subject: [squid-users] Problem with
 ssl_choose_client_version:inappropriate fallback on some sites when using
 TLS1.2
In-Reply-To: <CAAOXCCd_i8hM3+Lp0om9isJVPPqpCVmBD1XhUiimCX_9yYjNxA@mail.gmail.com>
References: <CAAOXCCd_i8hM3+Lp0om9isJVPPqpCVmBD1XhUiimCX_9yYjNxA@mail.gmail.com>
Message-ID: <02b683f8-454b-552a-3845-a6daa0f90055@treenet.co.nz>


On 15/09/19 10:41 pm, John Sweet-Escott wrote:
> Hi All
> 
> We are trying to run Squid 4.8, compiled with OpenSSL 1.1.1 (see [1]) on
> Ubuntu 18.04 as a transparent proxy for the purpose of egress filtering
> of HTTPS traffic using SNI (see config in [2]). It it works correctly
> when contacting some addresses (e.g. https://www.ubuntu.com) but not
> others (e.g. https://www.google.com). When we contact
> https://www.google.com using TLS1.2 we get the error in the logs:
> 2019/09/15 10:33:09 kid1| ERROR: negotiating TLS on FD 19:
> error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
> fallback (1/-1/0)
...
> ? ? Cipher Suite: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 (0xcca9)

I suspect it might have something to do with these ECDSA keys.

You do not have Elliptic-Curves enabled on the https_port client-facing
connection. So the TLS extensions associated are likely not to be
compatible between the client and the server connections Squid is
attempting to bridge between.

Amos


From rousskov at measurement-factory.com  Tue Sep 17 15:46:55 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 17 Sep 2019 11:46:55 -0400
Subject: [squid-users] How to extend Squid ICAP preview size
In-Reply-To: <CADcj3=651qbva5O-wCbHLOTEJKBikpiNV=s7mhZ8gKDaAL+JXw@mail.gmail.com>
References: <CADcj3=4fNho5AA3FmBJnnpsfc4ZuLb6VbBnVhT3rCcJfih1Acw@mail.gmail.com>
 <aedf10fe-5b19-5455-6ac0-9e34331fe79d@measurement-factory.com>
 <CADcj3=4yiKaecFui1umBGhTK3b=G9zp6FC5Q8tVdj5u8DFJ6YQ@mail.gmail.com>
 <f4e6645b-cdc1-ee78-7020-4f9d821764ac@measurement-factory.com>
 <CADcj3=651qbva5O-wCbHLOTEJKBikpiNV=s7mhZ8gKDaAL+JXw@mail.gmail.com>
Message-ID: <ed80bedc-f613-2a55-e1cd-eace6107dde3@measurement-factory.com>

On 9/16/19 5:26 PM, Felipe Arturo Polanco wrote:

> I have a RESPMOD icap server that supports Allow:206 in the OPTIONS
> response.

> Do you know why squid doesn't Allow 206 for files?


Squid follows the ICAP 206 extension specs and does not send Allow:206
unless it can buffer the entire HTTP message. In your particular case,
the "file" is 10000000 bytes long. That size probably exceeds Squid
buffering ability (64KB IIRC).

AFAICT, Squid also does not send Allow:206 for messages without bodies.

Alex.
P.S. Please note that Squid has no notion of "HTML", "archive", or
"file". Squid does not understand MIME types and such.


> RESPMOD icap://127.0.0.1:13440/example ICAP/1.0
> Host: 127.0.0.1:13440 <http://127.0.0.1:13440>
> Date: Mon, 16 Sep 2019 20:51:00 GMT
> Encapsulated: req-hdr=0, res-hdr=337, res-body=579
> Allow: trailers
> X-Client-IP: 192.168.0.6
> 
> GET http://203.0.113.1/file.zip HTTP/1.1
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:69.0)
> Gecko/20100101 Firefox/69.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
> Accept-Language: en-US,en;q=0.5
> Accept-Encoding: gzip, deflate
> DNT: 1
> Upgrade-Insecure-Requests: 1
> Host: 203.0.113.1
> 
> HTTP/1.1 200 OK
> Date: Mon, 16 Sep 2019 20:51:00 GMT
> Server: Apache/2.4.6 (CentOS)
> Last-Modified: Thu, 29 Aug 2019 13:54:58 GMT
> ETag: "989680-59141d9f0557a"
> Accept-Ranges: bytes
> Content-Length: 10000000
> Content-Type: application/zip


> http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt


From sam.holden at steeprockinc.com  Tue Sep 17 18:07:42 2019
From: sam.holden at steeprockinc.com (Sam Holden)
Date: Tue, 17 Sep 2019 14:07:42 -0400
Subject: [squid-users] SSL termination problem - squid's requests using https
Message-ID: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>

I'm converting a reasonably large configuration from squid v3 to squid
v4 and I'm having a problem with SSL termination. I'm clearly missing
something but I haven't been able to work out what. I'm using openssl
not gnutls.

Using the following:

https_port 4277 accel defaultsite=<THE_BACKEND_SITE>
cert=/etc/pki/tls/site.crt key=/etc/pki/tls/site.key
options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
cipher=<A-GIANT-LIST-OF-CIPHERS>

sees https requests to port 4277 connect fine, but squid tries to
reach the backend using https as well which fails because it is
serving http only.

using the following:

https_port 4277 accel defaultsite=<THE_BACKEND_SITE>
cert=/etc/pki/tls/site.crt key=/etc/pki/tls/site.key
options=NO_SSLv2,NO_SSLv3,CIPHER_SERVER_PREFERENCE
cipher=<A-GIANT-LIST-OF-CIPHERS> protocol=http

sees port 4227 act as an http port (no ssl) but the requests to the
backend from squid are correctly made via http. (protocol=HTTP/1.1 has
the same effect).

Is there an option to have squid make HTTP requests on behalf of HTTPS
requests from clients?

-- 
Sam Holden


From rousskov at measurement-factory.com  Tue Sep 17 20:07:15 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 17 Sep 2019 16:07:15 -0400
Subject: [squid-users] SSL termination problem - squid's requests using
 https
In-Reply-To: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>
References: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>
Message-ID: <f8fb07b7-baff-85e5-46b6-72c261ad822e@measurement-factory.com>

On 9/17/19 2:07 PM, Sam Holden wrote:

> https_port 4277 accel ... protocol=http

> sees port 4227 act as an http port (no ssl)

Assuming you meant "4277" when you said "4227" (or vice versa), your
statement sounds like an indication of a Squid bug to me: The "protocol"
option is documented to affect Squid-to-origin URL reconstruction. It
should have no effect on client-to-Squid communication (and https_port,
of course, expects TLS connections). In other words, the above
configuration should do what you want in principle AFAICT.

How does Squid report the above https_port at startup? Look for the
"Accepting ... at ..." line early in your cache.log.

What happens when you connect to the above https_port using a TLS
connection?

Alex.


From sam.holden at steeprockinc.com  Tue Sep 17 21:02:55 2019
From: sam.holden at steeprockinc.com (Sam Holden)
Date: Tue, 17 Sep 2019 17:02:55 -0400
Subject: [squid-users] SSL termination problem - squid's requests using
	https
In-Reply-To: <f8fb07b7-baff-85e5-46b6-72c261ad822e@measurement-factory.com>
References: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>
 <f8fb07b7-baff-85e5-46b6-72c261ad822e@measurement-factory.com>
Message-ID: <CAE5AiybpjcsqTUWeUcOWvY0V7bO0PDvrpiiSYP+6vWBC=qMfwQ@mail.gmail.com>

On Tue, Sep 17, 2019 at 4:07 PM Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>
> On 9/17/19 2:07 PM, Sam Holden wrote:
>
> > https_port 4277 accel ... protocol=http
>
> > sees port 4227 act as an http port (no ssl)
>
> Assuming you meant "4277" when you said "4227" (or vice versa), your
> statement sounds like an indication of a Squid bug to me: The "protocol"
> option is documented to affect Squid-to-origin URL reconstruction. It
> should have no effect on client-to-Squid communication (and https_port,
> of course, expects TLS connections). In other words, the above
> configuration should do what you want in principle AFAICT.
>
> How does Squid report the above https_port at startup? Look for the
> "Accepting ... at ..." line early in your cache.log.

Yes I made typo on the port number in my text.

When I have protocol=http is reports:
 2019/09/17 20:08:55| Accepting reverse-proxy HTTP Socket connections
at local=0.0.0.0:4277 remote=[::] FD 13 flags=9

When I don't set the protocol is reports:
2019/09/17 20:17:38| Accepting reverse-proxy HTTPS Socket connections
at local=0.0.0.0:4277 remote=[::] FD 13 flags=9

So it seems to be following the protocol= for the incoming protocol
rather than just the outgoing. I've tried compiling the 4.6 source
tarball and building the debian source package (to add openssl) which
is a few minor versions older but with the normal debian back porting.

I'm going to try the old stock debian one again - I think it was
working with gnutls though I couldn't see a way to make the screen
long options list work with  gnutls.


>
> What happens when you connect to the above https_port using a TLS
> connection?

When I have the protocol=http I get (443 is being mapped to 4277 elsewhere):

$  wget https://127.0.0.1:4277/ --no-check-certificate
--2019-09-17 20:53:04--  https://127.0.0.1:4277/
Connecting to 127.0.0.1:443... connected.
GnuTLS: An unexpected TLS packet was received.
Unable to establish SSL connection.
$  wget  http://127.0.0.1:4277/
--2019-09-17 20:54:17--  http://127.0.0.1:4277/
Connecting to 127.0.0.1:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 61979 (61K) [text/html]



>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Tue Sep 17 22:22:44 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 17 Sep 2019 18:22:44 -0400
Subject: [squid-users] SSL termination problem - squid's requests using
 https
In-Reply-To: <CAE5AiybpjcsqTUWeUcOWvY0V7bO0PDvrpiiSYP+6vWBC=qMfwQ@mail.gmail.com>
References: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>
 <f8fb07b7-baff-85e5-46b6-72c261ad822e@measurement-factory.com>
 <CAE5AiybpjcsqTUWeUcOWvY0V7bO0PDvrpiiSYP+6vWBC=qMfwQ@mail.gmail.com>
Message-ID: <dc8d9a48-093a-7357-3373-892270594507@measurement-factory.com>

On 9/17/19 5:02 PM, Sam Holden wrote:

> When I have protocol=http is reports:
> 2019/09/17 20:08:55| Accepting reverse-proxy HTTP Socket connections

> When I don't set the protocol is reports:
> 2019/09/17 20:17:38| Accepting reverse-proxy HTTPS Socket connections

> So it seems to be following the protocol= for the incoming protocol
> rather than just the outgoing.

Agreed. That (still) looks like a bug to me. [PROXY protocol prefix
aside], an https_port ought to expect TLS traffic, regardless of any
port tuning options, including the poorly named "protocol" option.

FWIW, I tried to quickly figure out what is really going on in the code,
but ran out of time -- configuration parsing code does appear to
overwrite the data member used as the incoming protocol of a listening
port which makes no sense to me and contradicts documentation, but I am
probably missing something in this mess. Hopefully, somebody else can
help you triage this further.

Alex.


>> What happens when you connect to the above https_port using a TLS
>> connection?
> 
> When I have the protocol=http I get (443 is being mapped to 4277 elsewhere):
> 
> $  wget https://127.0.0.1:4277/ --no-check-certificate
> --2019-09-17 20:53:04--  https://127.0.0.1:4277/
> Connecting to 127.0.0.1:443... connected.
> GnuTLS: An unexpected TLS packet was received.
> Unable to establish SSL connection.
> $  wget  http://127.0.0.1:4277/
> --2019-09-17 20:54:17--  http://127.0.0.1:4277/
> Connecting to 127.0.0.1:443... connected.
> HTTP request sent, awaiting response... 200 OK
> Length: 61979 (61K) [text/html]



From squid3 at treenet.co.nz  Wed Sep 18 11:10:39 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Sep 2019 23:10:39 +1200
Subject: [squid-users] SSL termination problem - squid's requests using
 https
In-Reply-To: <dc8d9a48-093a-7357-3373-892270594507@measurement-factory.com>
References: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>
 <f8fb07b7-baff-85e5-46b6-72c261ad822e@measurement-factory.com>
 <CAE5AiybpjcsqTUWeUcOWvY0V7bO0PDvrpiiSYP+6vWBC=qMfwQ@mail.gmail.com>
 <dc8d9a48-093a-7357-3373-892270594507@measurement-factory.com>
Message-ID: <e8e7de84-ecce-d239-5f1e-f5fb4eddd19c@treenet.co.nz>

On 18/09/19 10:22 am, Alex Rousskov wrote:
> On 9/17/19 5:02 PM, Sam Holden wrote:
> 
>> When I have protocol=http is reports:
>> 2019/09/17 20:08:55| Accepting reverse-proxy HTTP Socket connections
> 
>> When I don't set the protocol is reports:
>> 2019/09/17 20:17:38| Accepting reverse-proxy HTTPS Socket connections
> 
>> So it seems to be following the protocol= for the incoming protocol
>> rather than just the outgoing.
> 
> Agreed. That (still) looks like a bug to me. [PROXY protocol prefix
> aside], an https_port ought to expect TLS traffic, regardless of any
> port tuning options, including the poorly named "protocol" option.
> 
> FWIW, I tried to quickly figure out what is really going on in the code,
> but ran out of time -- configuration parsing code does appear to
> overwrite the data member used as the incoming protocol of a listening
> port which makes no sense to me and contradicts documentation, but I am
> probably missing something in this mess. Hopefully, somebody else can
> help you triage this further.
> 

FYI: the protocol= current behaviour is initial support for the HTTP
versions where layering is not as simple as HTTP vs HTTPS. For example;
ICY, Secure-HTTP, h2, HTTP/3.


All these *_port things are a red herring. The initial problem was
connections to the origin server using HTTPS.

Connections to originserver peer do not send URL scheme, and use the
settings on the cache_peer directive as the protocol layering and
message framing. So the http(s)_port options should be having no input
into the problem. The problem is something in the unknown cache_peer
settings, or maybe a bug in the new peer selection code.

Amos


From bcook at poughkeepsieschools.org  Wed Sep 18 17:59:37 2019
From: bcook at poughkeepsieschools.org (B. Cook)
Date: Wed, 18 Sep 2019 13:59:37 -0400
Subject: [squid-users] --foreground vs -N
Message-ID: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>

tl;dr:
is there any functional difference between the two?


Using runit to handle my squid processes.

I have Observium graphing system information.

My data/traffic is consistent

In my run file I changed from squid -N to squid --foreground

#!/bin/sh -e

exec \
chpst -o 131070 \
chpst -e ./env \
/usr/sbin/squid --foreground -s
# /usr/sbin/squid -N -s

I made the change at 10:30 and the cpu usage increased.

cpu graph: https://imgur.com/a/PWrpDcM
memory graph: https://imgur.com/a/uUhwHA6
bandwidth graph: https://imgur.com/a/dzHDiKO
(bandwidth has trend applied)

I can not tell from the graph if 'performance' increased or 'latency'
decreased.. but cpu usage certainly did increase.

I have no workers defined.

stock squid.conf plus this:

<code>
cache deny all
shutdown_lifetime 1 seconds
dns_nameservers 10.20.8.29
cache_mem 1024 MB
cache_mgr Support_ at _PCSD_0_120
dns_defnames off
dns_v4_first on
max_filedescriptors 65536
ipcache_size 4096
read_ahead_gap 512 KB
quick_abort_min 1024 KB
pid_filename /run/squid/squid.pid
forwarded_for off
eui_lookup off
hosts_file none
url_rewrite_extras "%>a %un %>rm myip=%la myport=%lp"
store_id_extras "%>a %un %>rm myip=%la myport=%lp"
log_uses_indirect_client off
</code>

Archlinux squid 4.8, 4.19.72-1-lts kernel

Thank you in advance for taking the time to read this..

-- 
Network Analyst
Poughkeepsie City School District
SMS & Mobile: (202) 810-5827
twitter.com/bcookatpcsd

If you can't explain it simply, you don't understand it well enough.

-- 

This message may contain confidential information and is intended only for 
the individual(s) named. If you are not an intended recipient you are not 
authorized to disseminate, distribute or copy this e-mail. Please notify 
the sender immediately if you have received this e-mail by mistake and 
delete this e-mail from your system.


From rousskov at measurement-factory.com  Wed Sep 18 18:38:32 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Sep 2019 14:38:32 -0400
Subject: [squid-users] --foreground vs -N
In-Reply-To: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>
References: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>
Message-ID: <60bc4670-cd07-2133-57c3-abaf9f52d413@measurement-factory.com>

On 9/18/19 1:59 PM, B. Cook wrote:

> is there any functional difference between the two?

Yes:

  --foreground disables backgrounding of the master process.
  -N disables all SMP features and also implies --foreground.

Bugs notwithstanding, the two options should have the same effect in
your particular setup because you do not enable any SMP features.


> In my run file I changed from squid -N to squid --foreground
> [and] cpu usage certainly did increase.

Restarting Squid can increase CPU usage (primarily due to "cold caches"
of various sorts, not just the HTTP response cache), especially when you
restart in the middle of a rising traffic trend.

To expose a bug (if it exists), comparing performance over a week, where
each test starts with a clean same-day-of-week reboot during idle time
would be more convincing, but I do understand that such a comparison
requires a lot of patience and may still be susceptible to traffic
irregularities. Comparing two similar weekdays each after a clean
same-day-of-week reboot during idle time could be good enough.


One thing you may want to check is whether your --foreground Squid is
creating shared memory segments (look in /dev/shm/ or equivalent).
Creating shared memory segments in non-SMP configurations is a bug.
IIRC, we have fixed one or two of those bugs, but there may be more, and
using shared memory tables can decrease performance of non-SMP
configurations (there is no free lunch).


HTH,

Alex.


> stock squid.conf plus this:

> cache deny all
> shutdown_lifetime 1 seconds
> dns_nameservers 10.20.8.29
> cache_mem 1024 MB
> cache_mgr Support_ at _PCSD_0_120
> dns_defnames off
> dns_v4_first on
> max_filedescriptors 65536
> ipcache_size 4096
> read_ahead_gap 512 KB
> quick_abort_min 1024 KB
> pid_filename /run/squid/squid.pid
> forwarded_for off
> eui_lookup off
> hosts_file none
> url_rewrite_extras "%>a %un %>rm myip=%la myport=%lp"
> store_id_extras "%>a %un %>rm myip=%la myport=%lp"
> log_uses_indirect_client off


From sam.holden at steeprockinc.com  Wed Sep 18 19:23:26 2019
From: sam.holden at steeprockinc.com (Sam Holden)
Date: Wed, 18 Sep 2019 15:23:26 -0400
Subject: [squid-users] SSL termination problem - squid's requests using
	https
In-Reply-To: <e8e7de84-ecce-d239-5f1e-f5fb4eddd19c@treenet.co.nz>
References: <CAE5AiybUHdEfj+gk+efjCj-s1evKad+0j=4nE1oBWyoJ_R8=Uw@mail.gmail.com>
 <f8fb07b7-baff-85e5-46b6-72c261ad822e@measurement-factory.com>
 <CAE5AiybpjcsqTUWeUcOWvY0V7bO0PDvrpiiSYP+6vWBC=qMfwQ@mail.gmail.com>
 <dc8d9a48-093a-7357-3373-892270594507@measurement-factory.com>
 <e8e7de84-ecce-d239-5f1e-f5fb4eddd19c@treenet.co.nz>
Message-ID: <CAE5AiyYBx_GR=kJr5eaV0YvyeeF0OHDfNthd4VXis0HNT2EjDg@mail.gmail.com>

On Wed, Sep 18, 2019 at 7:11 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>
> All these *_port things are a red herring. The initial problem was
> connections to the origin server using HTTPS.
>
> Connections to originserver peer do not send URL scheme, and use the
> settings on the cache_peer directive as the protocol layering and
> message framing. So the http(s)_port options should be having no input
> into the problem. The problem is something in the unknown cache_peer
> settings, or maybe a bug in the new peer selection code.

Thanks.

I think I've got that bit working. The old v3 config didn't have
cache_peers for all of the 150 odd https_port entries the squid server
is running. It was a very old version of 3 so it's likely a default
changed or the config was relying on an old bug/accidental behaviour
to work.

However, I don't understand how to send traffic to different ports on
the same servers. The reverse-proxy faq and sample configs cover
multiple servers and name based virtual hosting but I can't find how
to direct to specific ports.

For example, some test config (with combinations of vhost and vport):

https_port 9000 accel defaultsite=10.240.0.6:80
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key vhost
https_port 9001 accel defaultsite=10.240.0.6:81
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key vhost
https_port 9002 accel defaultsite=10.240.0.6:80
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key no-vhost
https_port 9003 accel defaultsite=10.240.0.6:81
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key no-vhost
https_port 9004 accel defaultsite=10.240.0.6
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key vport=80 vhost
https_port 9005 accel defaultsite=10.240.0.6
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key vport=81 vhost
https_port 9006 accel defaultsite=10.240.0.6
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key vport=80 no-vhost
https_port 9007 accel defaultsite=10.240.0.6
cert=/etc/pki/tls/cert.crt key=/etc/pki/tls/cert.key vport=81 no-vhost
http_port 8000 accel defaultsite=10.240.0.6:80 vhost
http_port 8001 accel defaultsite=10.240.0.6:81 vhost
http_port 8002 accel defaultsite=10.240.0.6:80 no-vhost
http_port 8003 accel defaultsite=10.240.0.6:81 no-vhost
http_port 8004 accel defaultsite=10.240.0.6 vport=80 vhost
http_port 8005 accel defaultsite=10.240.0.6 vport=81 vhost
http_port 8006 accel defaultsite=10.240.0.6 vport=80 no-vhost
http_port 8007 accel defaultsite=10.240.0.6 vport=81 no-vhost

cache_peer 10.240.0.6 parent 80 0 no-query no-query originserver
no-digest login=PASSTHRU name=test80
cache_peer 10.240.0.6 parent 81 0 no-query no-query originserver
no-digest login=PASSTHRU name=test81
# end config


Requests to 900[0145]  see squid make http connections 10.240.0.6:80.
Requests 900[2367] see squid make https connections to 10.240.0.6 - my
logs don't record the port on the error.
Requests to 800[012456] see squid make http connections to 10.240.0.6:80.
Requests to 800[37] see squid make http connections to 10.240.0.7:81.

So the no-vhost option seems to give me what I want for http_port.

However, when I use an https_port with no-vhost squid's requests are
being done in https instead of http and without no-vhost all the
traffic is directed to port 80 even when 81 is specified in the
https_port line (the same applied to http_port for that part).

I'm sure I'm missing something obvious, I'll be rereading the squid
docs tonight but there's a lot that I don't understand.





>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From bcook at poughkeepsieschools.org  Wed Sep 18 19:37:05 2019
From: bcook at poughkeepsieschools.org (B. Cook)
Date: Wed, 18 Sep 2019 15:37:05 -0400
Subject: [squid-users] --foreground vs -N
In-Reply-To: <60bc4670-cd07-2133-57c3-abaf9f52d413@measurement-factory.com>
References: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>
 <60bc4670-cd07-2133-57c3-abaf9f52d413@measurement-factory.com>
Message-ID: <CAOyb_EwYJMQ+1CQNNBOGUq7MisqNPCu8gH_bJRD7HDDLAvovOg@mail.gmail.com>

Thank you for the response..

Confused what you mean..

this is /dev/shm with --foreground (no workers)

root:/dev/shm # ls -al
total 12
drwxrwxrwt  2 root  root   100 2019-09-18 10:30 .
drwxr-xr-x 17 root  root  3120 2019-09-17 09:08 ..
-rw-------  1 proxy proxy    8 2019-09-18 10:30 squid-cf__metadata.shm
-rw-------  1 proxy proxy 8216 2019-09-18 10:30 squid-cf__queues.shm
-rw-------  1 proxy proxy   36 2019-09-18 10:30 squid-cf__readers.shm

This is --foreground with workers 2
(different machine but same configuration..)

root:/dev/shm # ls -al
total 5856
drwxrwxrwt  2 root  root         280 2019-09-18 15:15 .
drwxr-xr-x 19 root  root        3340 2019-09-18 06:52 ..
-rw-------  1 proxy proxy     393232 2019-09-18 15:15 squid-cache_mem_ex.shm
-rw-------  1 proxy proxy    3145840 2019-09-18 15:16
squid-cache_mem_map_anchors.shm
-rw-------  1 proxy proxy     131080 2019-09-18 15:15
squid-cache_mem_map_filenos.shm
-rw-------  1 proxy proxy     262156 2019-09-18 15:15
squid-cache_mem_map_slices.shm
-rw-------  1 proxy proxy     131112 2019-09-18 15:15 squid-cache_mem_space.shm
-rw-------  1 proxy proxy          8 2019-09-18 15:15 squid-cf__metadata.shm
-rw-------  1 proxy proxy      32852 2019-09-18 15:15 squid-cf__queues.shm
-rw-------  1 proxy proxy         52 2019-09-18 15:15 squid-cf__readers.shm
-rw-------  1 proxy proxy 1073872936 2019-09-18 15:15 squid-squid-page-pool.shm
-rw-------  1 proxy proxy    1572976 2019-09-18 15:15
squid-transients_map_anchors.shm
-rw-------  1 proxy proxy      65544 2019-09-18 15:15
squid-transients_map_filenos.shm
-rw-------  1 proxy proxy     131084 2019-09-18 15:15
squid-transients_map_slices.shm


Confused if this is good or bad, or evidence of an 'SMP vs non-SMP
configuration' issue..

Thank you again..

On Wed, Sep 18, 2019 at 2:38 PM Alex Rousskov
<rousskov at measurement-factory.com> wrote:

> One thing you may want to check is whether your --foreground Squid is
> creating shared memory segments (look in /dev/shm/ or equivalent).
> Creating shared memory segments in non-SMP configurations is a bug.
> IIRC, we have fixed one or two of those bugs, but there may be more, and
> using shared memory tables can decrease performance of non-SMP
> configurations (there is no free lunch).
>

-- 

This message may contain confidential information and is intended only for 
the individual(s) named. If you are not an intended recipient you are not 
authorized to disseminate, distribute or copy this e-mail. Please notify 
the sender immediately if you have received this e-mail by mistake and 
delete this e-mail from your system.


From eype69 at gmail.com  Wed Sep 18 19:55:40 2019
From: eype69 at gmail.com (John)
Date: Wed, 18 Sep 2019 20:55:40 +0100
Subject: [squid-users] Problem with
 ssl_choose_client_version:inappropriate fallback on some sites when using
 TLS1.2
In-Reply-To: <02b683f8-454b-552a-3845-a6daa0f90055@treenet.co.nz>
References: <CAAOXCCd_i8hM3+Lp0om9isJVPPqpCVmBD1XhUiimCX_9yYjNxA@mail.gmail.com>
 <02b683f8-454b-552a-3845-a6daa0f90055@treenet.co.nz>
Message-ID: <CAAOXCCeWVD_zVbd8gc2aaynbbw23ACycPA+Bi3wetp2Cg1sLXg@mail.gmail.com>

Hi Amos

Thank you for your help.

On Tue, 17 Sep 2019 at 07:26, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> ...
> >     Cipher Suite: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 (0xcca9)
>
> I suspect it might have something to do with these ECDSA keys.
>
> You do not have Elliptic-Curves enabled on the https_port client-facing
> connection. So the TLS extensions associated are likely not to be
> compatible between the client and the server connections Squid is
> attempting to bridge between.
>
I generated a dhparams file using the command:
openssl dhparam -out dhparams.pem 2048
and then I configured the port with the following options:
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
tls-dh=prime256v1:/etc/squid/dhparams.pem
options=SINGLE_ECDH_USE,SINGLE_DH_USE

But this still gives this in the log when I connect:
2019/09/18 08:19:44 kid1| ERROR: negotiating TLS on FD 17:
error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
fallback (1/-1/0)

I have also tried restricting the cipher to the same cipher that works
for the ubuntu connection and I get the same error:
openssl s_client -tls1_2  -CAfile squid.crt -cipher
ECDHE-RSA-AES128-GCM-SHA256  -connect www.google.com:443

With this restriction, the client hello to squid is:
Handshake Protocol: Client Hello
    Handshake Type: Client Hello (1)
    Length: 156
    Version: TLS 1.2 (0x0303)
    Random: e52eb8a54705dc32774c5832694dd4567cd9b0f34556ebf3?
    Session ID Length: 0
    Cipher Suites Length: 4
    Cipher Suites (2 suites)
        Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)
        Cipher Suite: TLS_EMPTY_RENEGOTIATION_INFO_SCSV (0x00ff)
    Compression Methods Length: 1
    Compression Methods (1 method)
    Extensions Length: 111
    Extension: server_name (len=19)
    Extension: ec_point_formats (len=4)
    Extension: supported_groups (len=12)
    Extension: session_ticket (len=0)
    Extension: encrypt_then_mac (len=0)
    Extension: extended_master_secret (len=0)
    Extension: signature_algorithms (len=48)
The proxied hello to google is identical to the above.
The server hello from google is:
Transport Layer Security
    TLSv1.2 Record Layer: Handshake Protocol: Server Hello
        Content Type: Handshake (22)
        Version: TLS 1.2 (0x0303)
        Length: 63
        Handshake Protocol: Server Hello
            Handshake Type: Server Hello (2)
            Length: 59
            Version: TLS 1.2 (0x0303)
            Random: 5d81da909e779d7e67f2663d6563236721b0906d09dacf02?
            Session ID Length: 0
            Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)
            Compression Method: null (0)
            Extensions Length: 19
            Extension: extended_master_secret (len=0)
            Extension: renegotiation_info (len=1)
            Extension: ec_point_formats (len=2)
            Extension: session_ticket (len=0)
    TLSv1.2 Record Layer: Handshake Protocol: Certificate
        Content Type: Handshake (22)
        Version: TLS 1.2 (0x0303)
        Length: 2537
        Handshake Protocol: Certificate
            Handshake Type: Certificate (11)
            Length: 2533
            Certificates Length: 2530
            Certificates (2530 bytes)
                Certificate Length: 1422
                Certificate:
3082058a30820472a0030201020210556630a312faeab908?
(id-at-commonName=www.google.com,id-at-organizationName=Google
LLC,id-at-localityName=Mountain
View,id-at-stateOrProvinceName=California,id-at-countryName=US)
                Certificate Length: 1102
                Certificate:
3082044a30820332a003020102020d01e3b49aa18d8aa981?
(id-at-commonName=GTS CA 1O1,id-at-organizationName=Google Trust
Services,id-at-countryName=US)
    TLSv1.2 Record Layer: Handshake Protocol: Server Key Exchange
        Content Type: Handshake (22)
        Version: TLS 1.2 (0x0303)
        Length: 300
        Handshake Protocol: Server Key Exchange
            Handshake Type: Server Key Exchange (12)
            Length: 296
            EC Diffie-Hellman Server Params
    TLSv1.2 Record Layer: Handshake Protocol: Server Hello Done
        Content Type: Handshake (22)
        Version: TLS 1.2 (0x0303)
        Length: 4
        Handshake Protocol: Server Hello Done

If you have any further suggestions as to how/where I should debug I
would be extremely grateful.

John

On Tue, 17 Sep 2019 at 07:26, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>
> On 15/09/19 10:41 pm, John Sweet-Escott wrote:
> > Hi All
> >
> > We are trying to run Squid 4.8, compiled with OpenSSL 1.1.1 (see [1]) on
> > Ubuntu 18.04 as a transparent proxy for the purpose of egress filtering
> > of HTTPS traffic using SNI (see config in [2]). It it works correctly
> > when contacting some addresses (e.g. https://www.ubuntu.com) but not
> > others (e.g. https://www.google.com). When we contact
> > https://www.google.com using TLS1.2 we get the error in the logs:
> > 2019/09/15 10:33:09 kid1| ERROR: negotiating TLS on FD 19:
> > error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
> > fallback (1/-1/0)
> ...
> >     Cipher Suite: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 (0xcca9)
>
> I suspect it might have something to do with these ECDSA keys.
>
> You do not have Elliptic-Curves enabled on the https_port client-facing
> connection. So the TLS extensions associated are likely not to be
> compatible between the client and the server connections Squid is
> attempting to bridge between.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Wed Sep 18 20:14:13 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Sep 2019 16:14:13 -0400
Subject: [squid-users] --foreground vs -N
In-Reply-To: <CAOyb_EwYJMQ+1CQNNBOGUq7MisqNPCu8gH_bJRD7HDDLAvovOg@mail.gmail.com>
References: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>
 <60bc4670-cd07-2133-57c3-abaf9f52d413@measurement-factory.com>
 <CAOyb_EwYJMQ+1CQNNBOGUq7MisqNPCu8gH_bJRD7HDDLAvovOg@mail.gmail.com>
Message-ID: <84681ac2-9825-8379-64c4-139957fef798@measurement-factory.com>

On 9/18/19 3:37 PM, B. Cook wrote:

> this is /dev/shm with --foreground (no workers)

> -rw-------  1 proxy proxy    8 2019-09-18 10:30 squid-cf__metadata.shm
> -rw-------  1 proxy proxy 8216 2019-09-18 10:30 squid-cf__queues.shm
> -rw-------  1 proxy proxy   36 2019-09-18 10:30 squid-cf__readers.shm

Your Squid is buggy: These collapsed forwarding shared memory segments
should not be created for non-SMP configurations. Using these collapsed
forwarding segments might slow down a non-SMP Squid instance, but I do
not know whether they are actually _used_. I suspect they are not.

The latest (future v5) Squid has the same bug AFAICT.

Unfortunately, in my future-v5 tests, the same or similar bug exists in
Squids started with -N, so this bug is probably not a good suspect in
your investigation. I have no other suspects to offer at this time.

Alex.


> On Wed, Sep 18, 2019 at 2:38 PM Alex Rousskov wrote:
> 
>> One thing you may want to check is whether your --foreground Squid is
>> creating shared memory segments (look in /dev/shm/ or equivalent).
>> Creating shared memory segments in non-SMP configurations is a bug.
>> IIRC, we have fixed one or two of those bugs, but there may be more, and
>> using shared memory tables can decrease performance of non-SMP
>> configurations (there is no free lunch).



From bcook at poughkeepsieschools.org  Wed Sep 18 20:24:24 2019
From: bcook at poughkeepsieschools.org (B. Cook)
Date: Wed, 18 Sep 2019 16:24:24 -0400
Subject: [squid-users] --foreground vs -N
In-Reply-To: <84681ac2-9825-8379-64c4-139957fef798@measurement-factory.com>
References: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>
 <60bc4670-cd07-2133-57c3-abaf9f52d413@measurement-factory.com>
 <CAOyb_EwYJMQ+1CQNNBOGUq7MisqNPCu8gH_bJRD7HDDLAvovOg@mail.gmail.com>
 <84681ac2-9825-8379-64c4-139957fef798@measurement-factory.com>
Message-ID: <CAOyb_EzfgN-ZM8hOQH+=CjkrigbsqUdH9jLj82vPCPKOvB5oNw@mail.gmail.com>

Would you suggest I go back to -N?

I've not seen anything awkward or out of the ordinary in cache.log..

I downloaded and tried to build squid-5 from AUR..

 squid -v
Squid Cache: Version 5.0.0-20190909-ra70e75b76
Service Name: squid
Intercept/WCCPv2/SSL/CRTD/(A)UFS/DISKD/ROCK/eCAP/ICAP/64/GCC Production

This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal restrictions
on distribution see https://www.openssl.org/source/license.html

(had to go back a  few days for a successful build..)

root:/dev/shm # ls -al
total 5876
drwxrwxrwt  2 root  root         280 2019-09-18 15:15 .
drwxr-xr-x 19 root  root        3340 2019-09-18 06:52 ..
-rw-------  1 proxy proxy     393232 2019-09-18 15:15 squid-cache_mem_ex.shm
-rw-------  1 proxy proxy    3145840 2019-09-18 16:16
squid-cache_mem_map_anchors.shm
-rw-------  1 proxy proxy     131080 2019-09-18 15:15
squid-cache_mem_map_filenos.shm
-rw-------  1 proxy proxy     262156 2019-09-18 15:15
squid-cache_mem_map_slices.shm
-rw-------  1 proxy proxy     131112 2019-09-18 15:15 squid-cache_mem_space.shm
-rw-------  1 proxy proxy          8 2019-09-18 15:15 squid-cf__metadata.shm
-rw-------  1 proxy proxy      32852 2019-09-18 15:15 squid-cf__queues.shm
-rw-------  1 proxy proxy         52 2019-09-18 15:15 squid-cf__readers.shm
-rw-------  1 proxy proxy 1073872936 2019-09-18 15:15 squid-squid-page-pool.shm
-rw-------  1 proxy proxy    1572976 2019-09-18 15:25
squid-transients_map_anchors.shm
-rw-------  1 proxy proxy      65544 2019-09-18 15:15
squid-transients_map_filenos.shm
-rw-------  1 proxy proxy     131084 2019-09-18 15:15
squid-transients_map_slices.shm
root:/dev/shm # uname -a
Linux arch0033-0f2b4d 4.19.73-1-lts #1 SMP Mon Sep 16 17:15:25 CEST
2019 x86_64 GNU/Linux

is this 'buggy' as well?

Thank you for your interest in my question(s); greatly appreciated.

On Wed, Sep 18, 2019 at 4:14 PM Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>
> On 9/18/19 3:37 PM, B. Cook wrote:
>
> > this is /dev/shm with --foreground (no workers)
>
> > -rw-------  1 proxy proxy    8 2019-09-18 10:30 squid-cf__metadata.shm
> > -rw-------  1 proxy proxy 8216 2019-09-18 10:30 squid-cf__queues.shm
> > -rw-------  1 proxy proxy   36 2019-09-18 10:30 squid-cf__readers.shm
>
> Your Squid is buggy: These collapsed forwarding shared memory segments
> should not be created for non-SMP configurations. Using these collapsed
> forwarding segments might slow down a non-SMP Squid instance, but I do
> not know whether they are actually _used_. I suspect they are not.
>
> The latest (future v5) Squid has the same bug AFAICT.
>
> Unfortunately, in my future-v5 tests, the same or similar bug exists in
> Squids started with -N, so this bug is probably not a good suspect in
> your investigation. I have no other suspects to offer at this time.
>
> Alex.
>
>
> > On Wed, Sep 18, 2019 at 2:38 PM Alex Rousskov wrote:
> >
> >> One thing you may want to check is whether your --foreground Squid is
> >> creating shared memory segments (look in /dev/shm/ or equivalent).
> >> Creating shared memory segments in non-SMP configurations is a bug.
> >> IIRC, we have fixed one or two of those bugs, but there may be more, and
> >> using shared memory tables can decrease performance of non-SMP
> >> configurations (there is no free lunch).
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Network Analyst
Poughkeepsie City School District
SMS & Mobile: (202) 810-5827
twitter.com/bcookatpcsd

If you can't explain it simply, you don't understand it well enough.

-- 

This message may contain confidential information and is intended only for 
the individual(s) named. If you are not an intended recipient you are not 
authorized to disseminate, distribute or copy this e-mail. Please notify 
the sender immediately if you have received this e-mail by mistake and 
delete this e-mail from your system.


From rousskov at measurement-factory.com  Wed Sep 18 21:40:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Sep 2019 17:40:09 -0400
Subject: [squid-users] --foreground vs -N
In-Reply-To: <CAOyb_EzfgN-ZM8hOQH+=CjkrigbsqUdH9jLj82vPCPKOvB5oNw@mail.gmail.com>
References: <CAOyb_Ex0X=Ru28yw1VL+iDZXgH+nGgeCf1j_iYMV24Zkuyg-Gw@mail.gmail.com>
 <60bc4670-cd07-2133-57c3-abaf9f52d413@measurement-factory.com>
 <CAOyb_EwYJMQ+1CQNNBOGUq7MisqNPCu8gH_bJRD7HDDLAvovOg@mail.gmail.com>
 <84681ac2-9825-8379-64c4-139957fef798@measurement-factory.com>
 <CAOyb_EzfgN-ZM8hOQH+=CjkrigbsqUdH9jLj82vPCPKOvB5oNw@mail.gmail.com>
Message-ID: <27ee1f6b-037d-cf89-9eb7-9001c64ee629@measurement-factory.com>

On 9/18/19 4:24 PM, B. Cook wrote:
> Would you suggest I go back to -N?

I cannot make a specific recommendation due to insufficient information.

In general, --foreground is meant for startup scripts, while -N is meant
for triage and development. For example, scripts using --foreground
would not need to be rewritten when the admin enables SMP features (or
if they become enabled by default).

However, if, in your environment, squid-N performs much better, then
recommending that you switch to --foreground would be silly! If you
gather sufficient proof of poor performance, you should file a bug
report instead (and/or sponsor the fix).


> Squid Cache: Version 5.0.0-20190909-ra70e75b76

> root:/dev/shm # ls -al
>     393232 2019-09-18 15:15 squid-cache_mem_ex.shm
>    3145840 2019-09-18 16:16 squid-cache_mem_map_anchors.shm
>     131080 2019-09-18 15:15 squid-cache_mem_map_filenos.shm
>     262156 2019-09-18 15:15 squid-cache_mem_map_slices.shm
>     131112 2019-09-18 15:15 squid-cache_mem_space.shm
>          8 2019-09-18 15:15 squid-cf__metadata.shm
>      32852 2019-09-18 15:15 squid-cf__queues.shm
>         52 2019-09-18 15:15 squid-cf__readers.shm
> 1073872936 2019-09-18 15:15 squid-squid-page-pool.shm
>    1572976 2019-09-18 15:25 squid-transients_map_anchors.shm
>      65544 2019-09-18 15:15 squid-transients_map_filenos.shm
>     131084 2019-09-18 15:15 squid-transients_map_slices.shm

> is this 'buggy' as well?

* If the above segments were created by a single-worker Squid without
any special squid.conf directives (i.e., Squid configured as you have
shown earlier), then, yes, it is very buggy.

* If the above segments were created by Squid configured with multiple
workers and started with -N, then, yes, it is very buggy.

* If the above segments were created by Squid configured with multiple
workers and started without -N, then creation of all those shared memory
segments is expected and does not indicate a bug. I suspect that is what
you have tested.


HTH,

Alex.


> On Wed, Sep 18, 2019 at 4:14 PM Alex Rousskov wrote:
>>
>> On 9/18/19 3:37 PM, B. Cook wrote:
>>
>>> this is /dev/shm with --foreground (no workers)
>>
>>> -rw-------  1 proxy proxy    8 2019-09-18 10:30 squid-cf__metadata.shm
>>> -rw-------  1 proxy proxy 8216 2019-09-18 10:30 squid-cf__queues.shm
>>> -rw-------  1 proxy proxy   36 2019-09-18 10:30 squid-cf__readers.shm
>>
>> Your Squid is buggy: These collapsed forwarding shared memory segments
>> should not be created for non-SMP configurations. Using these collapsed
>> forwarding segments might slow down a non-SMP Squid instance, but I do
>> not know whether they are actually _used_. I suspect they are not.
>>
>> The latest (future v5) Squid has the same bug AFAICT.
>>
>> Unfortunately, in my future-v5 tests, the same or similar bug exists in
>> Squids started with -N, so this bug is probably not a good suspect in
>> your investigation. I have no other suspects to offer at this time.
>>
>> Alex.
>>
>>
>>> On Wed, Sep 18, 2019 at 2:38 PM Alex Rousskov wrote:
>>>
>>>> One thing you may want to check is whether your --foreground Squid is
>>>> creating shared memory segments (look in /dev/shm/ or equivalent).
>>>> Creating shared memory segments in non-SMP configurations is a bug.
>>>> IIRC, we have fixed one or two of those bugs, but there may be more, and
>>>> using shared memory tables can decrease performance of non-SMP
>>>> configurations (there is no free lunch).
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 



From sakibnizam at gmail.com  Thu Sep 19 14:29:31 2019
From: sakibnizam at gmail.com (sknz)
Date: Thu, 19 Sep 2019 09:29:31 -0500 (CDT)
Subject: [squid-users] access log without hostname
Message-ID: <1568903371300-0.post@n4.nabble.com>

I'm using squid 3.5.3 to intercept https without issuing the client
certificate.  

https_port 3127 intercept ssl-bump generate-host-certificates=off
cert=certs/squid.pem
ssl_bump none all

So my squid access log is similar to this. Is there any way to make it more
meaningful? perhaps hostname?

...............................
1568902948.817  65168 10.1.0.1 TCP_TUNNEL/200 891 CONNECT 157.240.16.63:443
- ORIGINAL_DST/157.240.16.63 - 10.1.0.1
1568903081.342 240109 10.1.0.1 TCP_TUNNEL/200 458 CONNECT
172.217.163.132:443 - ORIGINAL_DST/172.217.163.132 - 10.1.0.1
1568903132.645 240133 10.1.0.1 TCP_TUNNEL/200 99047 CONNECT
172.217.31.214:443 - ORIGINAL_DST/172.217.31.214 - 10.1.0.1
...............................



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Sep 19 15:00:45 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 19 Sep 2019 11:00:45 -0400
Subject: [squid-users] access log without hostname
In-Reply-To: <1568903371300-0.post@n4.nabble.com>
References: <1568903371300-0.post@n4.nabble.com>
Message-ID: <9f2a02ed-c280-dce9-26c5-29159c2961cf@measurement-factory.com>

On 9/19/19 10:29 AM, sknz wrote:
> I'm using squid 3.5.3 to intercept https without issuing the client
> certificate.  
> 
> https_port 3127 intercept ssl-bump generate-host-certificates=off
> cert=certs/squid.pem
> ssl_bump none all

> So my squid access log is similar to this. Is there any way to make it more
> meaningful? perhaps hostname?

You can peek at step1 to get access to TLS client handshake information,
which may include TLS SNI. You can also peek at step2 to get access to
TLS server handshake information, which may include TLS server CN and
other details. IIRC, some of those details will be logged automatically
with the default logformat. Others can be logged using TLS-specific
logformat %codes.

  https://wiki.squid-cache.org/Features/SslPeekAndSplice


HTH,

Alex.


> ...............................
> 1568902948.817  65168 10.1.0.1 TCP_TUNNEL/200 891 CONNECT 157.240.16.63:443
> - ORIGINAL_DST/157.240.16.63 - 10.1.0.1
> 1568903081.342 240109 10.1.0.1 TCP_TUNNEL/200 458 CONNECT
> 172.217.163.132:443 - ORIGINAL_DST/172.217.163.132 - 10.1.0.1
> 1568903132.645 240133 10.1.0.1 TCP_TUNNEL/200 99047 CONNECT
> 172.217.31.214:443 - ORIGINAL_DST/172.217.31.214 - 10.1.0.1
> ...............................



From sakibnizam at gmail.com  Fri Sep 20 04:07:16 2019
From: sakibnizam at gmail.com (sknz)
Date: Thu, 19 Sep 2019 23:07:16 -0500 (CDT)
Subject: [squid-users] access log without hostname
In-Reply-To: <9f2a02ed-c280-dce9-26c5-29159c2961cf@measurement-factory.com>
References: <1568903371300-0.post@n4.nabble.com>
 <9f2a02ed-c280-dce9-26c5-29159c2961cf@measurement-factory.com>
Message-ID: <1568952436254-0.post@n4.nabble.com>

Okay, I'm using this configuration. For config 1 and 2, there is no
difference in access log. For config 3, site is not loading... How to
optimize it for a better result?

My goal is here, get as much as info about client without issuing a client
certificate.

---
http_port 3128
http_port 3129 intercept
https_port 3130 intercept ssl-bump generate-host-certificates=off
cert=/etc/squid3/certs/squid.pem
---

#config 1
ssl_bump none all

#config 2
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice all

#config 3
acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump peek step1
ssl_bump peek step2
ssl_bump splice all



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sakibnizam at gmail.com  Fri Sep 20 07:49:01 2019
From: sakibnizam at gmail.com (sknz)
Date: Fri, 20 Sep 2019 02:49:01 -0500 (CDT)
Subject: [squid-users] access log without hostname
In-Reply-To: <1568952436254-0.post@n4.nabble.com>
References: <1568903371300-0.post@n4.nabble.com>
 <9f2a02ed-c280-dce9-26c5-29159c2961cf@measurement-factory.com>
 <1568952436254-0.post@n4.nabble.com>
Message-ID: <1568965741610-0.post@n4.nabble.com>

I upgraded Squid to 3.5.28 September 2019 and compiled it with SSL. Now it's
working for both Http and Https site, also logging traffic with hostname.

For example, when I write "hotmail.com" in a fresh browser address bar and
press Enter, it shows this:

Our services aren't available right now

<p>We're working to restore all services as soon as possible. Please check
back soon</p>
0v4KEXQAAAACY6CM2x6+tS6+eNfa3kUWNU0cyRURHRTExMTIARWRnZQ==

it supposes to update url for https version automatically. if I type full
address "https://www.hotmal.com", now it works! How to solve this issue?

================== 
http_port 3128
http_port 3126 intercept
https_port 3127 intercept ssl-bump generate-host-certificates=off
cert=/etc/squid3/certs/squid.pem

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice all

and iptables redirected, 80>3126 and 443>3127.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Sep 20 12:48:00 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 20 Sep 2019 08:48:00 -0400
Subject: [squid-users] access log without hostname
In-Reply-To: <1568965741610-0.post@n4.nabble.com>
References: <1568903371300-0.post@n4.nabble.com>
 <9f2a02ed-c280-dce9-26c5-29159c2961cf@measurement-factory.com>
 <1568952436254-0.post@n4.nabble.com> <1568965741610-0.post@n4.nabble.com>
Message-ID: <258ff3ce-1230-edef-bde0-51e0835ef995@measurement-factory.com>

On 9/20/19 3:49 AM, sknz wrote:
> I upgraded Squid to 3.5.28 September 2019 and compiled it with SSL. Now it's
> working for both Http and Https site, also logging traffic with hostname.
> 
> For example, when I write "hotmail.com" in a fresh browser address bar and
> press Enter, it shows this:
> 
> Our services aren't available right now
> 
> <p>We're working to restore all services as soon as possible. Please check
> back soon</p>
> 0v4KEXQAAAACY6CM2x6+tS6+eNfa3kUWNU0cyRURHRTExMTIARWRnZQ==
> 
> it supposes to update url for https version automatically. if I type full
> address "https://www.hotmal.com", now it works! How to solve this issue?

A splicing Squid does not participate in redirection from
http[s]://example.com to http://www.example.com. If that redirection
works fine without Squid and does not work with Squid, then most likely
your Squid installation is outdated and/or buggy.

I would start by upgrading to the latest Squid v4 or better. SslBump
support in recent Squid releases is usually better, and the difference
is often important.

Alex.


> ================== 
> http_port 3128
> http_port 3126 intercept
> https_port 3127 intercept ssl-bump generate-host-certificates=off
> cert=/etc/squid3/certs/squid.pem
> 
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump splice all
> 
> and iptables redirected, 80>3126 and 443>3127.
> 


From chirayu.patel at truecomtelesoft.com  Fri Sep 20 13:03:05 2019
From: chirayu.patel at truecomtelesoft.com (Chirayu Patel)
Date: Fri, 20 Sep 2019 18:33:05 +0530
Subject: [squid-users] Protecting squid against ddos attacks
Message-ID: <CAOhxsyzLzLrbvPBdbMKe4mkmkMjz4_C9ej2BmPd_90+_A0zjAQ@mail.gmail.com>

This is my squid config file :

------------------------------------------
http_port 3129 intercept
https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
## For Captive Portal
http_port 3132 intercept
https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=1MB

#sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
#sslcrtd_children 5

# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

# TLS/SSL bumping steps
ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data
ssl_bump splice all # splice: no active bumping
on_unsupported_protocol tunnel all

pinger_enable off
digest_generation off
netdb_filename none
ipcache_size 128
fqdncache_size 128
via off
forwarded_for transparent
httpd_suppress_version_string on
cache deny all
cache_mem 0 MB
memory_pools off
shutdown_lifetime 0 seconds

#logfile_daemon /dev/null
access_log none

#acl good_url dstdomain .yahoo.com
http_access allow all

url_rewrite_program /tmp/squid/urlcat_server_start.sh
#url_rewrite_bypass on
url_rewrite_children 1 startup=1 idle=1 concurrency=30 queue-size=10000
on-persistent-overload=ERR
#url_rewrite_access allow all
#url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_extras "%>a %lp %ssl::>sni"

max_filedesc 5120
coredump_dir /tmp
client_lifetime 30 minutes
read_ahead_gap 8 KB

-------------------------------

--> I have installed squid in a wifi access point which will in many cases
behave as an edge gateway as well.. So basically it itself is the firewall.
There is nothing in front to protect it.
--> There are 4 ports that are opened.. If someone decides to do a DDOS
attack on them, what options do I have to protect against them.
--
Thank You
Chirayu Patel
Truecom Telesoft
+91 8758484287
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190920/830d12d9/attachment.htm>

From dc.sqml at ntcomputer.de  Fri Sep 20 14:53:41 2019
From: dc.sqml at ntcomputer.de (Nikolaus)
Date: Fri, 20 Sep 2019 16:53:41 +0200
Subject: [squid-users] Peek-and-splice not working when mixing TLS1.3
 servers and TLS1.2 clients
Message-ID: <71523daa-6521-118f-7a59-b47930e562e0@ntcomputer.de>

I have a transparent squid 4.8 proxy peek-and-splice setup acting as a
TLS domain filtering proxy. The setup worked well, until more and more
servers started adopting TLS 1.3. In this case, depending on the client
TLS version, errors started to appear:

If server, squid and client use TLS 1.3: Everything works as expected.
If server and squid use TLS 1.3, but client only supports TLS 1.2: The
client terminates the connection due to certificate verification errors.

I have had a look at what happens at TLS protocol level using wireshark,
and it seems that in the latter case, squid - for some reason - performs
(something similar to) bumping instead of splicing! That is, squid sends
back certificates to the client which are completely different than the
ones received from the server, and appear to be generated. Any
ClientKeyExchange received from the client also wouldn't be forwarded to
the server.

The following is the relevant part of my squid config:

https_port 3443 intercept ssl-bump cert=/etc/squid/dummy.pem.crt
key=/etc/squid/dummy.pem.key
ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_connections
ssl_bump terminate step2 all
ssl_bump splice step3 allowed_https_connections
ssl_bump terminate all

where allowed_https_connections is an ACL checking ssl::server_name.

How can I get the splicing setup working when mixing TLS 1.3 servers and
TLS 1.2 clients?

Many thanks!
Nikolaus


From tevfik.ceydeliler at gmail.com  Fri Sep 20 15:10:02 2019
From: tevfik.ceydeliler at gmail.com (Tevfik Ceydeliler)
Date: Fri, 20 Sep 2019 18:10:02 +0300
Subject: [squid-users] always bungled to my config
Message-ID: <CAJTWAhHUY769VSg4GEsATgVPOFvF6q=EV2CxpNsurFGAw3_2_A@mail.gmail.com>

Hi, I have Centos 7 and squid  3.5.20 that installed from repo
I try to run squid after achsnge my configuration but I get same error :

Bungled /etc/squid/cfg_acls.conf line 11: acl mediastreaming external
nt_group "/etc/squid/group_g_internet_mediastreaming.acl"

squid.conf is:
...
include /etc/squid/cfg_acls.conf
include /etc/squid/cfg_accessrules.conf
include /etc/squid/cfg_outgoing_addresses.conf
...
...

Here is  cfg_acls.conf:
...
...
acl mediastreaming external nt_group
"/etc/squid/group_g_internet_mediastreaming.acl"
...
...

 cfg_accessrules.conf:
...
...
http_access allow mediastreaming media_streaming_domains
..
...

is there any problem about "external nt_group" usage?
Regards,

-- 
Tevfik Ceydeliler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190920/74432634/attachment.htm>

From rousskov at measurement-factory.com  Fri Sep 20 16:01:58 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 20 Sep 2019 12:01:58 -0400
Subject: [squid-users] Peek-and-splice not working when mixing TLS1.3
 servers and TLS1.2 clients
In-Reply-To: <71523daa-6521-118f-7a59-b47930e562e0@ntcomputer.de>
References: <71523daa-6521-118f-7a59-b47930e562e0@ntcomputer.de>
Message-ID: <43742d32-1bf7-b194-2641-d29273588d4d@measurement-factory.com>

On 9/20/19 10:53 AM, Nikolaus wrote:

> If server and squid use TLS 1.3, but client only supports TLS 1.2: The
> client terminates the connection due to certificate verification errors.
> 
> I have had a look at what happens at TLS protocol level using wireshark,
> and it seems that in the latter case, squid - for some reason - performs
> (something similar to) bumping instead of splicing!

Bumping happens when a splicing Squid wants to report an SslBump-related
error to the client.


> How can I get the splicing setup working when mixing TLS 1.3 servers and
> TLS 1.2 clients?

I do not know the exact answer to that question, but I would start by
figuring out what error Squid is trying to serve to the client. You may
be able to figure it out by looking at the corresponding access.log
records, especially if you log %err_code and %err_detail. In the worst
case, enabling and looking at debugging info in cache.log may be
necessary, but I would start with access.log anyway.

Alex.


From rousskov at measurement-factory.com  Fri Sep 20 16:10:03 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 20 Sep 2019 12:10:03 -0400
Subject: [squid-users] always bungled to my config
In-Reply-To: <CAJTWAhHUY769VSg4GEsATgVPOFvF6q=EV2CxpNsurFGAw3_2_A@mail.gmail.com>
References: <CAJTWAhHUY769VSg4GEsATgVPOFvF6q=EV2CxpNsurFGAw3_2_A@mail.gmail.com>
Message-ID: <bd9db48a-8933-b6be-2e2e-18c9ffb19d8e@measurement-factory.com>

On 9/20/19 11:10 AM, Tevfik Ceydeliler wrote:
> Hi, I have Centos 7 and squid? 3.5.20 that installed from repo
> I try to run squid after achsnge my configuration but I get same error :

> Bungled /etc/squid/cfg_acls.conf line 11: acl mediastreaming external
> nt_group "/etc/squid/group_g_internet_mediastreaming.acl"

> squid.conf is:
> ...
> include /etc/squid/cfg_acls.conf
> include /etc/squid/cfg_accessrules.conf
> include /etc/squid/cfg_outgoing_addresses.conf
> ...
> ...
> 
> Here is??cfg_acls.conf:
> ...
> ...
> acl mediastreaming external nt_group
> "/etc/squid/group_g_internet_mediastreaming.acl"
> ...
> ...
> 
> ?cfg_accessrules.conf:
> ...
> ...
> http_access allow mediastreaming media_streaming_domains
> ..
> ...
> 
> is there any problem about "external nt_group" usage?

Did you configure the external nt_group helper somewhere before
configuring the mediastreaming ACL? Look for a configuration line like this:

    external_acl_type nt_group ... /path/to/helper ...

If that configuration looks valid and precedes mediastreaming ACL on
line 11 of /etc/squid/cfg_acls.conf, then check the existence and the
contents of /etc/squid/group_g_internet_mediastreaming.acl file.

Alex.


From squid3 at treenet.co.nz  Sat Sep 21 00:19:18 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Sep 2019 12:19:18 +1200
Subject: [squid-users] Protecting squid against ddos attacks
In-Reply-To: <CAOhxsyzLzLrbvPBdbMKe4mkmkMjz4_C9ej2BmPd_90+_A0zjAQ@mail.gmail.com>
References: <CAOhxsyzLzLrbvPBdbMKe4mkmkMjz4_C9ej2BmPd_90+_A0zjAQ@mail.gmail.com>
Message-ID: <835c4d02-4246-8c65-f9ce-cf91c7dd9e92@treenet.co.nz>

On 21/09/19 1:03 am, Chirayu Patel wrote:
> --> I have installed squid in a wifi access point which will in many
> cases behave as an edge gateway as well.. So basically it itself is the
> firewall. There is nothing in front to protect it.
> --> There are 4 ports that are opened.. If someone decides to do a DDOS
> attack on them, what options do I have to protect against them.


Pretty much the exact opposite of what you have this proxy configured to
be doing.

Right now you have it setup to allow all traffic *from* anywhere *to*
anywhere, with no controls, no logging, and no report to any backend
where the traffic originated.


Squid default configuration comes with some DoS protections as
recommended config, some are built-in and always working.

> This is my squid config file :
> 
> ------------------------------------------
> http_port 3129 intercept
> https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
> ? ? generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
> ## For Captive Portal ? ?
> http_port 3132 intercept
> https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
> ? ? generate-host-certificates=off dynamic_cert_mem_cache_size=1MB
> 

That comment "For Captive Portal" is out of place. Interception *is*
captive portal, so all your ports above are captive portal ports.

Usually you would only need one port of each type. Including the
forward-proxy port (3128 with no mode flag, which you are missing).

For DoS and DDoS protection, having more ports receiving traffic does
help by allowing more TCP port numbers to be available for use. But you
need firewall rules to spread the traffic load across those ports. See
the "Frontend Alternative 1" section of
 <https://wiki.squid-cache.org/ConfigExamples/ExtremeCarpFrontend>

For the best DDoS protection Squid can offer you would have a
multi-machine setup like that config page is part of. The particular
Squid you have right now though can gain from just having the port load
balancing part. You can extend the backend part on other machines later
if you want / need.


> 
> # TLS/SSL bumping definitions
> acl tls_s1_connect at_step SslBump1
> acl tls_s2_client_hello at_step SslBump2
> acl tls_s3_server_hello at_step SslBump3
> 

Unusued ACLs still consume memory. Not much, but still thats memory.


> # TLS/SSL bumping steps
> ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data

The "all" on the above line is unnecessary and a waste of CPU cycles on
ever next connection. Remove it.

> ssl_bump splice all # splice: no active bumping
> on_unsupported_protocol tunnel all

The tunnel action causes Squid to setup a server connection. That costs
2x TCP ports, 2x FDs, client I/O, server I/O, CPU cycles to perform all
the I/O, and memory for all the state and I/O buffers

While this may give you good service for weird client traffic. If your
DDoS risk is high, it may be better to use "respond" instead and an ACL
with "deny_info TCP_RESET attached".


> 
> pinger_enable off
> digest_generation off
> netdb_filename none
> ipcache_size 128

ipcache being larger will help your high-traffic periods by helping
reduce delays on traffic you let through the proxy.

DDoS can reduce that benefit. But that is only a *visual* effect, there
is no more resource consumption than the DDoS would cause with a smaller
ipcache size.

So reducing this cache size only slows your normal peak traffic at times
when it needs fastest service. That is a tradeoff against your AP
machines memory available.


> fqdncache_size 128

Large fqdncache for intercept proxies helps retain valid Host header
records longer and reduce delays receiving new messages. So larger here
is better protection, against both normal traffic problems and DDoS.


> via off
> forwarded_for transparent
> httpd_suppress_version_string on
> cache deny all
> cache_mem 0 MB

Using memory to store objects recently used gives 100x speed increase
(aka DoS handling capacity).

This though is a tradeoff with the memory you have available. Whether
that speed gain is nanoseconds, milliseconds or whole seconds depends on
your network speeds.

FYI: The model of a frontend LB with backend cache machine (like that
CARP setup earlier) is designed to reduce that speed difference so both
the resource consumption and speed gain cache gives is primarily
happening at the backends - which are very close in the network so
minimal extra delay for the frontend LB.


> memory_pools off

Only if you have to. The memory usage patterns of high-traffic software
like Squid is quite different from what most OS malloc are optimized
for. The memory pools in Squid are optimized to reduce that to a number
of larger more consistently sized allocations.

Without these pools memory allocation cycles add a bit of speed
reduction to the proxy, and worse can easily lead to memory
fragmentation issues. Normal traffic speeds these effects are not easily
noticed, but under DoS or DDoS conditions they can drag the entire
machine to a crawl if not a complete halt on low-memory systems (like
yours?).


> shutdown_lifetime 0 seconds
> 
> #logfile_daemon /dev/null
> access_log none

A bit part of DoS or DDoS protection is identifying the attack as it
starts. That requires the information about what traffic is happening to
go somewhere for processing.

Even if you have something else doing deep packet inspection I would
enable logs. Use one of the network logging modules to send them to
another machine if necessary for processing.


> 
> #acl good_url dstdomain .yahoo.com <http://yahoo.com>
> http_access allow all
> 

See the "default config" section of
<https://wiki.squid-cache.org/Squid-3.5>. The default rules are
primarily DoS protections these days, with some other nasty attacks
(potentially leading to DDoS indirectly) as well.

You need those rules, and you need a clear policy on what traffic you
allow through the proxy (from where, to where). Once you have that in
place you can reasonably consider what DoS/DDoS risk is left to deal
with. So long as your policy and rule is "http_access allow all" you can
be DoS'ed by a single 38 byte HTTP request message - at least the proxy
killed completely, possibly the whole machine.
 (FYI the other protection against this attack is the Via header, which
you have disabled).


> url_rewrite_program /tmp/squid/urlcat_server_start.sh
> #url_rewrite_bypass on
> url_rewrite_children 1 startup=1 idle=1 concurrency=30 queue-size=10000
> on-persistent-overload=ERR

Having only one helper may be a source of problems under any conditions.
The ERR will help, but ideally you don't want to reach that state.

Consider whether the thing this helper is doing can be done by ACLs and
deny_info instead. That would avoid all the helper delays or I/O
resource needs, and any clients getting those ERR error pages.


> #url_rewrite_access allow all
> #url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
> sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
> url_rewrite_extras "%>a %lp %ssl::>sni"
> 
> max_filedesc 5120

This is the most direct measure of how large a DoS has to be to kill
your traffic. The smaller it is the fewer connections the DoS needs to open.

There is a tradeoff though between the memory each of these needs to
allocate (~500 bytes just to exist, up to 256KB when in use) vs the
memory your machine has available.

The reduction of that "in use" time also matters as one might expect.
Which is where the cache_mem speedup comes in, to answer repeat queries
(eg those seen in a classical DoS) at orders of magnitude higher speed
than the backend network can provide the same answer.


> coredump_dir /tmp
> client_lifetime 30 minutes
> read_ahead_gap 8 KB
> 


Additional to all the above, you can setup a "deny_info TCP_RESET ..."
for any ACLs which does a deny action in your rules. That will prevent
Squid generating an error page and consuming bandwidth to deliver it
when that ACL blocks access.

There is a tradeoff between annoying clients who no longer know why
their connection ended, but under DoS or DDoS it is a huge bandwidth saver.


Amos


From SEO.Workwide at gmail.com  Sat Sep 21 07:51:05 2019
From: SEO.Workwide at gmail.com (KleinEdith)
Date: Sat, 21 Sep 2019 02:51:05 -0500 (CDT)
Subject: [squid-users] Help with HTTPS SQUID 3.1.23 https proxy not
	working
In-Reply-To: <73b43b98-c819-45e8-57b7-988eaae91eb1@treenet.co.nz>
References: <CACtB8=AXp23Zi15EY-+5JGG=R+RyP-DFB-pmrQRdCZ7ETVffDw@mail.gmail.com>
 <73b43b98-c819-45e8-57b7-988eaae91eb1@treenet.co.nz>
Message-ID: <1569052265880-0.post@n4.nabble.com>

Squid as the https proxy not working

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl localnet src 10.0.0.188 # David Computer
acl SSL_ports port 443
acl Safe_ports port 80      # http
acl Safe_ports port 21      # ftp
acl Safe_ports port 443     # https
acl Safe_ports port 70      # gopher
acl Safe_ports port 210     # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280     # http-mgmt
acl Safe_ports port 488     # gss-http
acl Safe_ports port 591     # filemaker
acl Safe_ports port 777     # multiling http
acl CONNECT method CONNECT

acl bad_urls dstdomain "/etc/squid/blacklisted_sites.acl"
acl good_url dstdomain "/etc/squid/good_sites.acl"
#http_access deny bad_url

I can?t connect to:

Outlook.com <https://outlook.live.com>   
Yahoo.com <https://www.yahoo.com/>  
SuCarroRD.com <https://sucarrord.com/>  
Gmail.com <http://gmail.com/>  
Bing <https://www.bing.com/>  

And more. I need help please to fix this problem 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Sat Sep 21 08:07:31 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 21 Sep 2019 10:07:31 +0200
Subject: [squid-users] Help with HTTPS SQUID 3.1.23 https proxy not
 working
In-Reply-To: <1569052265880-0.post@n4.nabble.com>
References: <CACtB8=AXp23Zi15EY-+5JGG=R+RyP-DFB-pmrQRdCZ7ETVffDw@mail.gmail.com>
 <73b43b98-c819-45e8-57b7-988eaae91eb1@treenet.co.nz>
 <1569052265880-0.post@n4.nabble.com>
Message-ID: <20190921080731.GB29045@fantomas.sk>

On 21.09.19 02:51, KleinEdith wrote:
>Squid as the https proxy not working
>
># Example rule allowing access from your local networks.
># Adapt to list your (internal) IP networks from where browsing
># should be allowed
>acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
>acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>#acl localnet src fc00::/7       # RFC 4193 local private network range
>#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
>machines
>acl localnet src 10.0.0.188 # David Computer
>acl SSL_ports port 443
>acl Safe_ports port 80      # http
>acl Safe_ports port 21      # ftp
>acl Safe_ports port 443     # https
>acl Safe_ports port 70      # gopher
>acl Safe_ports port 210     # wais
>acl Safe_ports port 1025-65535  # unregistered ports
>acl Safe_ports port 280     # http-mgmt
>acl Safe_ports port 488     # gss-http
>acl Safe_ports port 591     # filemaker
>acl Safe_ports port 777     # multiling http
>acl CONNECT method CONNECT
>
>acl bad_urls dstdomain "/etc/squid/blacklisted_sites.acl"
>acl good_url dstdomain "/etc/squid/good_sites.acl"
>#http_access deny bad_url
>
>I can?t connect to:
>
>Outlook.com <https://outlook.live.com>
>Yahoo.com <https://www.yahoo.com/>
>SuCarroRD.com <https://sucarrord.com/>
>Gmail.com <http://gmail.com/>
>Bing <https://www.bing.com/>

You haven't post whole squid config, did you?
If you did, you definitely need to llow some access (for limited set of
IPs), because the default is deny.



-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese.


From ahmed.zaeem at netstream.ps  Sun Sep 22 10:25:29 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 22 Sep 2019 13:25:29 +0300
Subject: [squid-users] Delay pools not working with squid 4.x  ,
	and more Question !!
Message-ID: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>

Hello Folks ,

i tested squid 4.8 and delay pools not working with it at all .
i reverted back to squid 3.5.x and i had delay pools working .

Q1- do squid 4 support delay pools ?


Q2- with squid 3.5.x we have SMP about 4 childs , and we are running delay pools .

say i limited in the main config file 1/1 Mbps 

does that mean speed ( with all 4 instances ) is 1/1 Mbps
or
 speed ( with all 4 instances ) is 4/4  Mbps

?


Many Thanks 

From squid3 at treenet.co.nz  Sun Sep 22 10:46:09 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 22 Sep 2019 22:46:09 +1200
Subject: [squid-users] Delay pools not working with squid 4.x ,
 and more Question !!
In-Reply-To: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
References: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
Message-ID: <3605b570-18ef-f05a-2ae6-96272b4380d3@treenet.co.nz>

On 22/09/19 10:25 pm, --Ahmad-- wrote:
> Hello Folks ,
> 
> i tested squid 4.8 and delay pools not working with it at all .
> i reverted back to squid 3.5.x and i had delay pools working .
> 
> Q1- do squid 4 support delay pools ?
> 

Yes.

> 
> Q2- with squid 3.5.x we have SMP about 4 childs , and we are running delay pools .
> 
> say i limited in the main config file 1/1 Mbps 
> 

What did you configure *exactly*?

> does that mean speed ( with all 4 instances ) is 1/1 Mbps
> or
>  speed ( with all 4 instances ) is 4/4  Mbps
> 
> ?

Neither.

Amos


From ahmed.zaeem at netstream.ps  Sun Sep 22 10:52:49 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 22 Sep 2019 13:52:49 +0300
Subject: [squid-users] Delay pools not working with squid 4.x ,
 and more Question !!
In-Reply-To: <3605b570-18ef-f05a-2ae6-96272b4380d3@treenet.co.nz>
References: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
 <3605b570-18ef-f05a-2ae6-96272b4380d3@treenet.co.nz>
Message-ID: <A5BD7F64-373B-4F1E-8432-66A126CEC6BE@netstream.ps>

Hi Amos but squid 4.8  did not get the config below to work :


delay_pools 1
delay_class 1 1
delay_parameters 1 35000000/35000000
delay_access 1 allow minh


but on squid 3.5 it worked .

Plz For Q2 , what will be speed if we have  4 worker

is it above ? or above * 4 ?

Thanks 



> On Sep 22, 2019, at 1:46 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 22/09/19 10:25 pm, --Ahmad-- wrote:
>> Hello Folks ,
>> 
>> i tested squid 4.8 and delay pools not working with it at all .
>> i reverted back to squid 3.5.x and i had delay pools working .
>> 
>> Q1- do squid 4 support delay pools ?
>> 
> 
> Yes.
> 
>> 
>> Q2- with squid 3.5.x we have SMP about 4 childs , and we are running delay pools .
>> 
>> say i limited in the main config file 1/1 Mbps 
>> 
> 
> What did you configure *exactly*?
> 
>> does that mean speed ( with all 4 instances ) is 1/1 Mbps
>> or
>> speed ( with all 4 instances ) is 4/4  Mbps
>> 
>> ?
> 
> Neither.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From dc.sqml at ntcomputer.de  Sun Sep 22 13:18:01 2019
From: dc.sqml at ntcomputer.de (Nikolaus)
Date: Sun, 22 Sep 2019 15:18:01 +0200
Subject: [squid-users] Peek-and-splice not working when mixing TLS1.3
 servers and TLS1.2 clients
In-Reply-To: <43742d32-1bf7-b194-2641-d29273588d4d@measurement-factory.com>
References: <71523daa-6521-118f-7a59-b47930e562e0@ntcomputer.de>
 <43742d32-1bf7-b194-2641-d29273588d4d@measurement-factory.com>
Message-ID: <1b4aabc3-fe2d-fef1-e7ff-f55bc50a3c74@ntcomputer.de>


> You may
> be able to figure it out by looking at the corresponding access.log
> records, especially if you log %err_code and %err_detail. In the worst
> case, enabling and looking at debugging info in cache.log may be
> necessary, but I would start with access.log anyway.

Thank you for the suggestion Alex!

The access.log contains error code / detail "ERR_SECURE_CONNECT_FAIL /
SQUID_ERR_SSL_HANDSHAKE" - which is not too helpful - but the cache.log
contains the more detailed "ERROR: negotiating TLS on FD 19:
error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
fallback (1/-1/0)".

Is a TLS fallback prevention mechanism kicking in by error? If so, how
to fix it?
Please let me know if additional log output (the debug log around the
error location did not seem helpful to me though) or a configuration to
reproduce the error are needed.

Nikolaus


From rousskov at measurement-factory.com  Sun Sep 22 13:41:11 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 22 Sep 2019 09:41:11 -0400
Subject: [squid-users] Peek-and-splice not working when mixing TLS1.3
 servers and TLS1.2 clients
In-Reply-To: <1b4aabc3-fe2d-fef1-e7ff-f55bc50a3c74@ntcomputer.de>
References: <71523daa-6521-118f-7a59-b47930e562e0@ntcomputer.de>
 <43742d32-1bf7-b194-2641-d29273588d4d@measurement-factory.com>
 <1b4aabc3-fe2d-fef1-e7ff-f55bc50a3c74@ntcomputer.de>
Message-ID: <a84f26eb-0580-f134-4d36-bfa7b1a4d5ea@measurement-factory.com>

On 9/22/19 9:18 AM, Nikolaus wrote:

> The access.log contains error code / detail "ERR_SECURE_CONNECT_FAIL /
> SQUID_ERR_SSL_HANDSHAKE" - which is not too helpful - but the cache.log
> contains the more detailed "ERROR: negotiating TLS on FD 19:
> error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
> fallback (1/-1/0)".

> Is a TLS fallback prevention mechanism kicking in by error? If so, how
> to fix it?

I do not know the answers to your questions, but I am sure that it is
possible to figure it out by looking at either packet captures or
detailed debugging logs. Unfortunately, I do not have enough free time
to guide you through this triage. There were several similar complains
about "inappropriate fallback" errors on this list recently. I would
start by revisiting those threads for more clues.

Alex.


From chirayu.patel at truecomtelesoft.com  Sun Sep 22 13:59:46 2019
From: chirayu.patel at truecomtelesoft.com (Chirayu Patel)
Date: Sun, 22 Sep 2019 19:29:46 +0530
Subject: [squid-users] Protecting squid against ddos attacks
In-Reply-To: <mailman.1.1569067202.2422.squid-users@lists.squid-cache.org>
References: <mailman.1.1569067202.2422.squid-users@lists.squid-cache.org>
Message-ID: <CAOhxsyxkNViKYsz1PxLJmSU8Uu5=EsUY_EF8m_Gv1esCnbwnXg@mail.gmail.com>

Hi Amos,

Thanks a lot for giving some amazing insights..

So currently I am using Squid to achieve 2 things :
a) Content Filtering - by checking the url against an external db and allow
and block it accordingly. (using url_rewriter).
b) To popup captive portal

1) Regarding the use of 4 ports
Using iptables, I am redirecting the non authenticated users to ports  3132
 & 3133. And then in squid i am checking the port on which I have received
the request from. If its the above ports, then I run the captive portal flow

Once the user is authenticated , then I redirect their traffic to ports 3129
and 3131, and for those ports I run the content policy flow.

I am not sure if this is the right way of choosing the flow. Please advise
if there is any other way to run two different flows with one squid.

2) Actually 3128 port is there in the config.. missed to attach that..

3) Right now I have configured firewall to only allow these 4 ports on the
INPUT chain, so I am not expecting traffic to come from any other ports. In
that case, is it okay if I have removed the default config and kept
"http_access allow all" ??
The only issue is that the attacker now has 4 ports to run attacks on.

4) > on_unsupported_protocol tunnel all

I had added this when I faced issue with one of the Apps, Whatsapp which
send the http traffic on https port. If I replace that with *respond*, I
guess Whatsapp will become unusable.. right ?

5) ipcache_size & fqdncache_size.
I was too concerned about the memory usage but I believe it does more bad
than good.  I will increase them to their defaults.

6) cache_mem 0 MB
The default cache memory is quite huge (256 MB). That is approx the total
usable memory I have on the AP. In that case, what do you think should be a
good starting point in case I keep it to a non zero value ?

7) memory_pools off
Again, I was too concerned about memory use and I got scared because of
this comment
------ ------ ------ ------ ------ ------ ------ ------ ------ ------
------ ------ ------ ------
If set, Squid will keep pools of allocated (but unused) memory available
for future use. If memory is a premium on your system and you believe your
malloc library outperforms Squid  routines, disable this.
------------- ------ ------ ------ ------ ------ ------ ------ ------
------ ------ ------
But I believe some memory in exchange of performance is OK. So I am going
to enable it.

8) The reason for keeping a single url_rewrite process has got to do with
caching of the external content policy api replies which is mainly to avoid
making external calls if the cache is available. If I have multiple
processes, the cache will get divided amongst multiple processes depending
on which one has made the call.

9) max_filedesc 5120.
I had kept this number bigger because we were getting "out of file
descriptors error"

10) Above all, the best thing would be a way to differentiate between a
high traffic flow of http(s) requests coming in from legitimate users vs a
high traffic flow generated by an attacker with a simple script.

--
Thank You
Chirayu Patel
Truecom Telesoft
+91 8758484287




On Sat, 21 Sep 2019 at 17:33, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Protecting squid against ddos attacks (Amos Jeffries)
>    2. Re: Help with HTTPS SQUID 3.1.23 https proxy not  working
>       (KleinEdith)
>    3. Re: Help with HTTPS SQUID 3.1.23 https proxy not working
>       (Matus UHLAR - fantomas)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sat, 21 Sep 2019 12:19:18 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Protecting squid against ddos attacks
> Message-ID: <835c4d02-4246-8c65-f9ce-cf91c7dd9e92 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 21/09/19 1:03 am, Chirayu Patel wrote:
> > --> I have installed squid in a wifi access point which will in many
> > cases behave as an edge gateway as well.. So basically it itself is the
> > firewall. There is nothing in front to protect it.
> > --> There are 4 ports that are opened.. If someone decides to do a DDOS
> > attack on them, what options do I have to protect against them.
>
>
> Pretty much the exact opposite of what you have this proxy configured to
> be doing.
>
> Right now you have it setup to allow all traffic *from* anywhere *to*
> anywhere, with no controls, no logging, and no report to any backend
> where the traffic originated.
>
>
> Squid default configuration comes with some DoS protections as
> recommended config, some are built-in and always working.
>
> > This is my squid config file :
> >
> > ------------------------------------------
> > http_port 3129 intercept
> > https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
> >     generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
> > ## For Captive Portal
> > http_port 3132 intercept
> > https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
> >     generate-host-certificates=off dynamic_cert_mem_cache_size=1MB
> >
>
> That comment "For Captive Portal" is out of place. Interception *is*
> captive portal, so all your ports above are captive portal ports.
>
> Usually you would only need one port of each type. Including the
> forward-proxy port (3128 with no mode flag, which you are missing).
>
> For DoS and DDoS protection, having more ports receiving traffic does
> help by allowing more TCP port numbers to be available for use. But you
> need firewall rules to spread the traffic load across those ports. See
> the "Frontend Alternative 1" section of
>  <https://wiki.squid-cache.org/ConfigExamples/ExtremeCarpFrontend>
>
> For the best DDoS protection Squid can offer you would have a
> multi-machine setup like that config page is part of. The particular
> Squid you have right now though can gain from just having the port load
> balancing part. You can extend the backend part on other machines later
> if you want / need.
>
>
> >
> > # TLS/SSL bumping definitions
> > acl tls_s1_connect at_step SslBump1
> > acl tls_s2_client_hello at_step SslBump2
> > acl tls_s3_server_hello at_step SslBump3
> >
>
> Unusued ACLs still consume memory. Not much, but still thats memory.
>
>
> > # TLS/SSL bumping steps
> > ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data
>
> The "all" on the above line is unnecessary and a waste of CPU cycles on
> ever next connection. Remove it.
>
> > ssl_bump splice all # splice: no active bumping
> > on_unsupported_protocol tunnel all
>
> The tunnel action causes Squid to setup a server connection. That costs
> 2x TCP ports, 2x FDs, client I/O, server I/O, CPU cycles to perform all
> the I/O, and memory for all the state and I/O buffers
>
> While this may give you good service for weird client traffic. If your
> DDoS risk is high, it may be better to use "respond" instead and an ACL
> with "deny_info TCP_RESET attached".
>
>
> >
> > pinger_enable off
> > digest_generation off
> > netdb_filename none
> > ipcache_size 128
>
> ipcache being larger will help your high-traffic periods by helping
> reduce delays on traffic you let through the proxy.
>
> DDoS can reduce that benefit. But that is only a *visual* effect, there
> is no more resource consumption than the DDoS would cause with a smaller
> ipcache size.
>
> So reducing this cache size only slows your normal peak traffic at times
> when it needs fastest service. That is a tradeoff against your AP
> machines memory available.
>
>
> > fqdncache_size 128
>
> Large fqdncache for intercept proxies helps retain valid Host header
> records longer and reduce delays receiving new messages. So larger here
> is better protection, against both normal traffic problems and DDoS.
>
>
> > via off
> > forwarded_for transparent
> > httpd_suppress_version_string on
> > cache deny all
> > cache_mem 0 MB
>
> Using memory to store objects recently used gives 100x speed increase
> (aka DoS handling capacity).
>
> This though is a tradeoff with the memory you have available. Whether
> that speed gain is nanoseconds, milliseconds or whole seconds depends on
> your network speeds.
>
> FYI: The model of a frontend LB with backend cache machine (like that
> CARP setup earlier) is designed to reduce that speed difference so both
> the resource consumption and speed gain cache gives is primarily
> happening at the backends - which are very close in the network so
> minimal extra delay for the frontend LB.
>
>
> > memory_pools off
>
> Only if you have to. The memory usage patterns of high-traffic software
> like Squid is quite different from what most OS malloc are optimized
> for. The memory pools in Squid are optimized to reduce that to a number
> of larger more consistently sized allocations.
>
> Without these pools memory allocation cycles add a bit of speed
> reduction to the proxy, and worse can easily lead to memory
> fragmentation issues. Normal traffic speeds these effects are not easily
> noticed, but under DoS or DDoS conditions they can drag the entire
> machine to a crawl if not a complete halt on low-memory systems (like
> yours?).
>
>
> > shutdown_lifetime 0 seconds
> >
> > #logfile_daemon /dev/null
> > access_log none
>
> A bit part of DoS or DDoS protection is identifying the attack as it
> starts. That requires the information about what traffic is happening to
> go somewhere for processing.
>
> Even if you have something else doing deep packet inspection I would
> enable logs. Use one of the network logging modules to send them to
> another machine if necessary for processing.
>
>
> >
> > #acl good_url dstdomain .yahoo.com <http://yahoo.com>
> > http_access allow all
> >
>
> See the "default config" section of
> <https://wiki.squid-cache.org/Squid-3.5>. The default rules are
> primarily DoS protections these days, with some other nasty attacks
> (potentially leading to DDoS indirectly) as well.
>
> You need those rules, and you need a clear policy on what traffic you
> allow through the proxy (from where, to where). Once you have that in
> place you can reasonably consider what DoS/DDoS risk is left to deal
> with. So long as your policy and rule is "http_access allow all" you can
> be DoS'ed by a single 38 byte HTTP request message - at least the proxy
> killed completely, possibly the whole machine.
>  (FYI the other protection against this attack is the Via header, which
> you have disabled).
>
>
> > url_rewrite_program /tmp/squid/urlcat_server_start.sh
> > #url_rewrite_bypass on
> > url_rewrite_children 1 startup=1 idle=1 concurrency=30 queue-size=10000
> > on-persistent-overload=ERR
>
> Having only one helper may be a source of problems under any conditions.
> The ERR will help, but ideally you don't want to reach that state.
>
> Consider whether the thing this helper is doing can be done by ACLs and
> deny_info instead. That would avoid all the helper delays or I/O
> resource needs, and any clients getting those ERR error pages.
>
>
> > #url_rewrite_access allow all
> > #url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
> > sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
> > url_rewrite_extras "%>a %lp %ssl::>sni"
> >
> > max_filedesc 5120
>
> This is the most direct measure of how large a DoS has to be to kill
> your traffic. The smaller it is the fewer connections the DoS needs to
> open.
>
> There is a tradeoff though between the memory each of these needs to
> allocate (~500 bytes just to exist, up to 256KB when in use) vs the
> memory your machine has available.
>
> The reduction of that "in use" time also matters as one might expect.
> Which is where the cache_mem speedup comes in, to answer repeat queries
> (eg those seen in a classical DoS) at orders of magnitude higher speed
> than the backend network can provide the same answer.
>
>
> > coredump_dir /tmp
> > client_lifetime 30 minutes
> > read_ahead_gap 8 KB
> >
>
>
> Additional to all the above, you can setup a "deny_info TCP_RESET ..."
> for any ACLs which does a deny action in your rules. That will prevent
> Squid generating an error page and consuming bandwidth to deliver it
> when that ACL blocks access.
>
> There is a tradeoff between annoying clients who no longer know why
> their connection ended, but under DoS or DDoS it is a huge bandwidth saver.
>
>
> Amos
>
>
> ------------------------------
>
> Message: 2
> Date: Sat, 21 Sep 2019 02:51:05 -0500 (CDT)
> From: KleinEdith <SEO.Workwide at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Help with HTTPS SQUID 3.1.23 https proxy
>         not     working
> Message-ID: <1569052265880-0.post at n4.nabble.com>
> Content-Type: text/plain; charset=UTF-8
>
> Squid as the https proxy not working
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> #acl localnet src fc00::/7       # RFC 4193 local private network range
> #acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
> acl localnet src 10.0.0.188 # David Computer
> acl SSL_ports port 443
> acl Safe_ports port 80      # http
> acl Safe_ports port 21      # ftp
> acl Safe_ports port 443     # https
> acl Safe_ports port 70      # gopher
> acl Safe_ports port 210     # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280     # http-mgmt
> acl Safe_ports port 488     # gss-http
> acl Safe_ports port 591     # filemaker
> acl Safe_ports port 777     # multiling http
> acl CONNECT method CONNECT
>
> acl bad_urls dstdomain "/etc/squid/blacklisted_sites.acl"
> acl good_url dstdomain "/etc/squid/good_sites.acl"
> #http_access deny bad_url
>
> I can?t connect to:
>
> Outlook.com <https://outlook.live.com>
> Yahoo.com <https://www.yahoo.com/>
> SuCarroRD.com <https://sucarrord.com/>
> Gmail.com <http://gmail.com/>
> Bing <https://www.bing.com/>
>
> And more. I need help please to fix this problem
>
>
>
>
> --
> Sent from:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
>
>
> ------------------------------
>
> Message: 3
> Date: Sat, 21 Sep 2019 10:07:31 +0200
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Help with HTTPS SQUID 3.1.23 https proxy
>         not working
> Message-ID: <20190921080731.GB29045 at fantomas.sk>
> Content-Type: text/plain; charset=iso-8859-2; format=flowed
>
> On 21.09.19 02:51, KleinEdith wrote:
> >Squid as the https proxy not working
> >
> ># Example rule allowing access from your local networks.
> ># Adapt to list your (internal) IP networks from where browsing
> ># should be allowed
> >acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
> >acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> >acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> >#acl localnet src fc00::/7       # RFC 4193 local private network range
> >#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> >machines
> >acl localnet src 10.0.0.188 # David Computer
> >acl SSL_ports port 443
> >acl Safe_ports port 80      # http
> >acl Safe_ports port 21      # ftp
> >acl Safe_ports port 443     # https
> >acl Safe_ports port 70      # gopher
> >acl Safe_ports port 210     # wais
> >acl Safe_ports port 1025-65535  # unregistered ports
> >acl Safe_ports port 280     # http-mgmt
> >acl Safe_ports port 488     # gss-http
> >acl Safe_ports port 591     # filemaker
> >acl Safe_ports port 777     # multiling http
> >acl CONNECT method CONNECT
> >
> >acl bad_urls dstdomain "/etc/squid/blacklisted_sites.acl"
> >acl good_url dstdomain "/etc/squid/good_sites.acl"
> >#http_access deny bad_url
> >
> >I can?t connect to:
> >
> >Outlook.com <https://outlook.live.com>
> >Yahoo.com <https://www.yahoo.com/>
> >SuCarroRD.com <https://sucarrord.com/>
> >Gmail.com <http://gmail.com/>
> >Bing <https://www.bing.com/>
>
> You haven't post whole squid config, did you?
> If you did, you definitely need to llow some access (for limited set of
> IPs), because the default is deny.
>
>
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> The early bird may get the worm, but the second mouse gets the cheese.
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 61, Issue 28
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190922/3f28ceba/attachment.htm>

From rousskov at measurement-factory.com  Sun Sep 22 14:07:27 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 22 Sep 2019 10:07:27 -0400
Subject: [squid-users] Delay pools not working with squid 4.x ,
 and more Question !!
In-Reply-To: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
References: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
Message-ID: <6674560c-074b-58cb-5f64-b55c0c735cfe@measurement-factory.com>

On 9/22/19 6:25 AM, --Ahmad-- wrote:

> i tested squid 4.8 and delay pools not working with it at all .
> i reverted back to squid 3.5.x and i had delay pools working .

> Q1- do squid 4 support delay pools ?

It should. If it does not, there is a bug somewhere.


> Q2- with squid 3.5.x we have SMP about 4 childs , and we are running delay pools .
> does that mean speed ( with all 4 instances ) is 1/1 Mbps
> or speed ( with all 4 instances ) is 4/4  Mbps?

According to [1], delay pools are not SMP-aware yet so you are
essentially configuring individual worker limits: Workers do not share
their limits and pools with each other. Hence, the effective Squid
instance limit is, very approximately, the aggregate of those configured
individual worker limits. For example, if each worker is limited by
1Mbps, then the 4-worker instance may produce up to 4Mbps traffic.

In reality, since individual workers usually receive different amounts
of traffic (especially until [2] is unblocked), the effective instance
limit will be more than 1Mbps and less than 4Mbps.

[1] https://wiki.squid-cache.org/Features/SmpScale#What_can_workers_share.3F

[2] https://github.com/squid-cache/squid/pull/369

Alex.


From ahmed.zaeem at netstream.ps  Sun Sep 22 14:28:09 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 22 Sep 2019 17:28:09 +0300
Subject: [squid-users] Delay pools not working with squid 4.x ,
 and more Question !!
In-Reply-To: <6674560c-074b-58cb-5f64-b55c0c735cfe@measurement-factory.com>
References: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
 <6674560c-074b-58cb-5f64-b55c0c735cfe@measurement-factory.com>
Message-ID: <BF177C67-DF72-4EA2-B361-5BEF574D30CC@netstream.ps>

Hi Alex thanks for info .
so i can confirm 100 % its a bug 

bec same config exactly work on 3.5

if you recommend me any thing 4.x that work with delay pools or 5.x i would be thankful ! 


and thank you very much when you answered me about SMP and delay pools .

all is clear , Looking forward to hearing that bug fixed .

Thanks a lot .

> On Sep 22, 2019, at 5:07 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 9/22/19 6:25 AM, --Ahmad-- wrote:
> 
>> i tested squid 4.8 and delay pools not working with it at all .
>> i reverted back to squid 3.5.x and i had delay pools working .
> 
>> Q1- do squid 4 support delay pools ?
> 
> It should. If it does not, there is a bug somewhere.
> 
> 
>> Q2- with squid 3.5.x we have SMP about 4 childs , and we are running delay pools .
>> does that mean speed ( with all 4 instances ) is 1/1 Mbps
>> or speed ( with all 4 instances ) is 4/4  Mbps?
> 
> According to [1], delay pools are not SMP-aware yet so you are
> essentially configuring individual worker limits: Workers do not share
> their limits and pools with each other. Hence, the effective Squid
> instance limit is, very approximately, the aggregate of those configured
> individual worker limits. For example, if each worker is limited by
> 1Mbps, then the 4-worker instance may produce up to 4Mbps traffic.
> 
> In reality, since individual workers usually receive different amounts
> of traffic (especially until [2] is unblocked), the effective instance
> limit will be more than 1Mbps and less than 4Mbps.
> 
> [1] https://wiki.squid-cache.org/Features/SmpScale#What_can_workers_share.3F
> 
> [2] https://github.com/squid-cache/squid/pull/369
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Sun Sep 22 16:06:56 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 22 Sep 2019 12:06:56 -0400
Subject: [squid-users] Delay pools not working with squid 4.x ,
 and more Question !!
In-Reply-To: <BF177C67-DF72-4EA2-B361-5BEF574D30CC@netstream.ps>
References: <8A7AD161-7574-491C-8AE4-09C6E2A05845@netstream.ps>
 <6674560c-074b-58cb-5f64-b55c0c735cfe@measurement-factory.com>
 <BF177C67-DF72-4EA2-B361-5BEF574D30CC@netstream.ps>
Message-ID: <b9ec1943-5e21-6367-1557-404d488c7e60@measurement-factory.com>

On 9/22/19 10:28 AM, --Ahmad-- wrote:

> Looking forward to hearing that bug fixed .

Me too! However, please do not misinterpret my response as a
confirmation of the bug existence or an implication that somebody is
working on a fix. I do not know whether anybody is working on this. I do
not even recall if somebody has filed a corresponding bug report.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


>> On Sep 22, 2019, at 5:07 PM, Alex Rousskov wrote:
>>
>> On 9/22/19 6:25 AM, --Ahmad-- wrote:
>>
>>> i tested squid 4.8 and delay pools not working with it at all .
>>> i reverted back to squid 3.5.x and i had delay pools working .
>>
>>> Q1- do squid 4 support delay pools ?
>>
>> It should. If it does not, there is a bug somewhere.
>>
>>
>>> Q2- with squid 3.5.x we have SMP about 4 childs , and we are running delay pools .
>>> does that mean speed ( with all 4 instances ) is 1/1 Mbps
>>> or speed ( with all 4 instances ) is 4/4  Mbps?
>>
>> According to [1], delay pools are not SMP-aware yet so you are
>> essentially configuring individual worker limits: Workers do not share
>> their limits and pools with each other. Hence, the effective Squid
>> instance limit is, very approximately, the aggregate of those configured
>> individual worker limits. For example, if each worker is limited by
>> 1Mbps, then the 4-worker instance may produce up to 4Mbps traffic.
>>
>> In reality, since individual workers usually receive different amounts
>> of traffic (especially until [2] is unblocked), the effective instance
>> limit will be more than 1Mbps and less than 4Mbps.
>>
>> [1] https://wiki.squid-cache.org/Features/SmpScale#What_can_workers_share.3F
>>
>> [2] https://github.com/squid-cache/squid/pull/369
>>
>> Alex.



From SEO.Workwide at gmail.com  Mon Sep 23 00:31:59 2019
From: SEO.Workwide at gmail.com (KleinEdith)
Date: Sun, 22 Sep 2019 19:31:59 -0500 (CDT)
Subject: [squid-users] Help with HTTPS SQUID 3.1.23 https proxy not
	working
In-Reply-To: <20190921080731.GB29045@fantomas.sk>
References: <CACtB8=AXp23Zi15EY-+5JGG=R+RyP-DFB-pmrQRdCZ7ETVffDw@mail.gmail.com>
 <73b43b98-c819-45e8-57b7-988eaae91eb1@treenet.co.nz>
 <1569052265880-0.post@n4.nabble.com> <20190921080731.GB29045@fantomas.sk>
Message-ID: <1569198719890-0.post@n4.nabble.com>

Thanks for help me, I fix my problem now I can see  SuCarroRD.com
<https://sucarrord.com/>    Bing.com <https://www.bing.com/>   and more.
Thanks for your Help. I will recommend this site to my another friends. Have
good day



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From frio_cervesa at hotmail.com  Mon Sep 23 04:49:06 2019
From: frio_cervesa at hotmail.com (senor)
Date: Mon, 23 Sep 2019 04:49:06 +0000
Subject: [squid-users] Error page headers
Message-ID: <BYAPR01MB49686CA2E9698737B4070315F7850@BYAPR01MB4968.prod.exchangelabs.com>

Hi All,
I have custom error pages with content needing the Content-Type header to reflect what it is (like JSON). I don't see any current options providing that option for error page handling.

Before I dig into the code, I wanted to see if anyone had any ideas. Adaptation could work but seems like overkill. 

Thanks for any help.
Senor

From tevfik.ceydeliler at gmail.com  Mon Sep 23 12:45:27 2019
From: tevfik.ceydeliler at gmail.com (Tevfik Ceydeliler)
Date: Mon, 23 Sep 2019 15:45:27 +0300
Subject: [squid-users] Squid Can't catch AD user's group
Message-ID: <CAJTWAhFvmWW44OSAvoa_7tPtn_B7avz691Q9cczH62kn6zbHpw@mail.gmail.com>

Hi,
My squid ACL can't catch AD user's group of membership.That's why can't
send the request correct outgoing interface
Users member of group_g_internet_socialmediausers and its correct interface
IP address is 10.65.12.247. 10.65.12.250 is general outgoing address

### NTLM
> auth_param ntlm program /usr/bin/ntlm_auth --diagnostic
> --helper-protocol=squid-2.5-ntlmssp --domain=COMPANY
> auth_param ntlm children 100
> auth_param ntlm max_challenge_reuses 0
> auth_param ntlm max_challenge_lifetime 2 minutes
> auth_param ntlm keep_alive off


group_g_internet_socialmediausers.acl:

> CN=G_Internet_SocialMedisUsers,OU=Internet Groups,DC=company,DC=grp


and  Configuration file:

> acl group_g_internet_socialmediausers  external nt_group
> "/etc/squid/group_g_internet_socialmediausers.acl"



> http_access allow group_g_internet_socialmediausers



> tcp_outgoing_address 10.65.12.250



and outgoing part:
tcp_outgoing_address 10.65.12.247 group_g_internet_socialmediausers


cache.log shows:

> (truncated)
> 2019/09/23 15:31:45.811 kid1| 28,5| Checklist.cc(400) bannedAction: Action
> 'ALLOWED/0is not banned
> 2019/09/23 15:31:45.811 kid1| 28,5| Acl.cc(138) matches: checking
> http_access#10
> 2019/09/23 15:31:45.811 kid1| 28,5| Acl.cc(138) matches: checking
> group_g_internet_socialmediausers
> 2019/09/23 15:31:45.811 kid1| 28,3| Acl.cc(158) matches: checked:
> group_g_internet_socialmediausers = 0
> 2019/09/23 15:31:45.811 kid1| 28,3| Acl.cc(158) matches: checked:
> http_access#10 = 0
> (truncated)
> 2019/09/23 15:31:45.811 kid1| 28,3| Checklist.cc(70) preCheck:
> 0x7fff26947320 checking fast ACLs
> 2019/09/23 15:31:45.811 kid1| 28,5| Acl.cc(138) matches: checking
> tcp_outgoing_address 10.65.12.247
> 2019/09/23 15:31:45.811 kid1| 28,5| Acl.cc(138) matches: checking
> (tcp_outgoing_address 10.65.12.247 line)
> 2019/09/23 15:31:45.811 kid1| 28,5| Acl.cc(138) matches: checking
> group_g_internet_socialmediausers
> 2019/09/23 15:31:45.811 kid1| 28,3| Acl.cc(158) matches: checked:
> group_g_internet_socialmediausers = 0
> 2019/09/23 15:31:45.811 kid1| 28,3| Acl.cc(158) matches: checked:
> (tcp_outgoing_address 10.65.12.247 line) = 0
> 2019/09/23 15:31:45.811 kid1| 28,3| Acl.cc(158) matches: checked:
> tcp_outgoing_address 10.65.12.247 = 0
> 2019/09/23 15:31:46.094 kid1| 28,3| Checklist.cc(63) markFinished:
> 0x7fff26946d40 answer AUTH_REQUIRED for aclMatchExternal exception
> 2019/09/23 15:31:46.094 kid1| 28,3| Acl.cc(158) matches: checked:
> group_g_internet_socialmediausers = -1
> 2019/09/23 15:31:46.094 kid1| 28,3| Acl.cc(158) matches: checked:
> (tcp_outgoing_address 10.65.12.247 line) = -1
> 2019/09/23 15:31:46.094 kid1| 28,3| Acl.cc(158) matches: checked:
> tcp_outgoing_address 10.65.12.247 = -1
> 2019/09/23 15:31:46.094 kid1| 28,3| Checklist.cc(70) preCheck:
> 0x7fff26946d40 checking fast ACLs
> (truncated)
> 2019/09/23 15:31:52.069 kid1| 28,3| Checklist.cc(70) preCheck:
> 0x7fff26947320 checking fast ACLs
> 2019/09/23 15:31:52.069 kid1| 28,5| Acl.cc(138) matches: checking
> tcp_outgoing_address 10.65.12.247
> 2019/09/23 15:31:52.069 kid1| 28,5| Acl.cc(138) matches: checking
> (tcp_outgoing_address 10.65.12.247 line)
> 2019/09/23 15:31:52.069 kid1| 28,5| Acl.cc(138) matches: checking
> group_g_internet_socialmediausers
> 2019/09/23 15:31:52.069 kid1| 28,3| Acl.cc(158) matches: checked:
> group_g_internet_socialmediausers = 0
> 2019/09/23 15:31:52.069 kid1| 28,3| Acl.cc(158) matches: checked:
> (tcp_outgoing_address 10.65.12.247 line) = 0
> 2019/09/23 15:31:52.069 kid1| 28,3| Acl.cc(158) matches: checked:
> tcp_outgoing_address 10.65.12.247 = 0
> 2019/09/23 15:31:52.069 kid1| 28,3| Checklist.cc(63) markFinished:
> 0x7fff26947320 answer DENIED for ACLs failed to match
> 2019/09/23 15:31:52.069 kid1| 28,3| Checklist.cc(70) preCheck:
> 0x7fff26947320 checking fast ACLs
> 2019/09/23 15:31:52.069 kid1| 28,5| Acl.cc(138) matches: checking
> tcp_outgoing_address 10.65.12.250
> 2019/09/23 15:31:52.069 kid1| 28,5| Acl.cc(138) matches: checking
> (tcp_outgoing_address 10.65.12.250 line)
> 2019/09/23 15:31:52.069 kid1| 28,5| Acl.cc(138) matches: checking
> 8_18_sinirsiz
> 2019/09/23 15:31:52.069 kid1| 28,3| Acl.cc(158) matches: checked:
> 8_18_sinirsiz = 1
> 2019/09/23 15:31:52.069 kid1| 28,3| Acl.cc(158) matches: checked:
> (tcp_outgoing_address 10.65.12.250 line) = 1
> 2019/09/23 15:31:52.069 kid1| 28,3| Acl.cc(158) matches: checked:
> tcp_outgoing_address 10.65.12.250 = 1
> 2019/09/23 15:31:52.069 kid1| 28,3| Checklist.cc(63) markFinished:
> 0x7fff26947320 answer ALLOWED for match
> 2019/09/23 15:31:52.069 kid1| 28,4| FilledChecklist.cc(66)
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff26947320
> 2019/09/23 15:31:52.069 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
> ACLChecklist::~ACLChecklist: destroyed 0x7fff26947320
> 2019/09/23 15:31:52.069 kid1| 28,4| FilledChecklist.cc(66)
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff269470c0
> 2019/09/23 15:31:52.069 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
> ACLChecklist::~ACLChecklist: destroyed 0x7fff269470c0
> 2019/09/23 15:31:52.069 kid1| 28,4| FilledChecklist.cc(66)
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x56416a6dc118
> 2019/09/23 15:31:52.069 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
> ACLChecklist::~ACLChecklist: destroyed 0x56416a6dc118
> 2019/09/23 15:31:54.699 kid1| 28,3| Checklist.cc(70) preCheck:
> 0x7fff269480a0 checking fast ACLs
> 2019/09/23 15:31:54.700 kid1| 28,5| Acl.cc(138) matches: checking
> cache_access_log /var/log/squid/access.log
> 2019/09/23 15:31:54.700 kid1| 28,5| Acl.cc(138) matches: checking
> (cache_access_log /var/log/squid/access.log line)
> 2019/09/23 15:31:54.700 kid1| 28,3| Acl.cc(158) matches: checked:
> (cache_access_log /var/log/squid/access.log line) = 1
> 2019/09/23 15:31:54.700 kid1| 28,3| Acl.cc(158) matches: checked:
> cache_access_log /var/log/squid/access.log = 1
> 2019/09/23 15:31:54.700 kid1| 28,3| Checklist.cc(63) markFinished:
> 0x7fff269480a0 answer ALLOWED for match
> 2019/09/23 15:31:54.700 kid1| 28,4| FilledChecklist.cc(66)
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff269480a0
> 2019/09/23 15:31:54.700 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
> ACLChecklist::~ACLChecklist: destroyed 0x7fff269480a0
> 2019/09/23 15:31:59.925 kid1| 28,8| Acl.cc(355) aclCacheMatchFlush:
> aclCacheMatchFlush called for cache 0x56416a71d1a8
> 2019/09/23 15:33:11.925 kid1| 28,3| Checklist.cc(70) preCheck:
> 0x7fff269480a0 checking fast ACLs
> 2019/09/23 15:33:11.925 kid1| 28,5| Acl.cc(138) matches: checking
> cache_access_log /var/log/squid/access.log
> 2019/09/23 15:33:11.925 kid1| 28,5| Acl.cc(138) matches: checking
> (cache_access_log /var/log/squid/access.log line)
> 2019/09/23 15:33:11.925 kid1| 28,3| Acl.cc(158) matches: checked:
> (cache_access_log /var/log/squid/access.log line) = 1
> 2019/09/23 15:33:11.925 kid1| 28,3| Acl.cc(158) matches: checked:
> cache_access_log /var/log/squid/access.log = 1
> 2019/09/23 15:33:11.925 kid1| 28,3| Checklist.cc(63) markFinished:
> 0x7fff269480a0 answer ALLOWED for match
> 2019/09/23 15:33:11.925 kid1| 28,4| FilledChecklist.cc(66)
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff269480a0
> 2019/09/23 15:33:11.925 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
> ACLChecklist::~ACLChecklist: destroyed 0x7fff269480a0
> 2019/09/23 15:33:11.925 kid1| 28,8| Acl.cc(355) aclCacheMatchFlush:
> aclCacheMatchFlush called for cache 0x56416a6ec138


At the end user routes to 10.65.12.250 which is not allowed for this users.
What is wrong?




-- 
Tevfik Ceydeliler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190923/4f1ce2f3/attachment.htm>

From rousskov at measurement-factory.com  Mon Sep 23 13:03:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Sep 2019 09:03:09 -0400
Subject: [squid-users] Error page headers
In-Reply-To: <BYAPR01MB49686CA2E9698737B4070315F7850@BYAPR01MB4968.prod.exchangelabs.com>
References: <BYAPR01MB49686CA2E9698737B4070315F7850@BYAPR01MB4968.prod.exchangelabs.com>
Message-ID: <96cc2ab9-f832-c783-3325-4c42cba3f49b@measurement-factory.com>

On 9/23/19 12:49 AM, senor wrote:

> I have custom error pages with content needing the Content-Type
> header to reflect what it is (like JSON). I don't see any current
> options providing that option for error page handling.

I would start with http_reply_access/reply_header_replace combo, denying
the Content-Type response header in applicable Squid-generated error
responses (and only them) and then providing a replacement.


> Adaptation could work but seems like overkill. 

There is not post-cache RESPMOD vectoring point support in Squid (and
most other proxies) -- adaptation is not applied to error pages.

You can also modify Squid sources, of course. This feature can probably
be quickly hacked into ErrorState::BuildHttpReply(). If you want
official admission, then a proper implementation would probably require
adding support for loading arbitrary response headers from the error
page template itself.

Alex.


From frio_cervesa at hotmail.com  Mon Sep 23 17:08:58 2019
From: frio_cervesa at hotmail.com (senor)
Date: Mon, 23 Sep 2019 17:08:58 +0000
Subject: [squid-users] Error page headers
In-Reply-To: <96cc2ab9-f832-c783-3325-4c42cba3f49b@measurement-factory.com>
References: <BYAPR01MB49686CA2E9698737B4070315F7850@BYAPR01MB4968.prod.exchangelabs.com>,
 <96cc2ab9-f832-c783-3325-4c42cba3f49b@measurement-factory.com>
Message-ID: <BYAPR01MB496835A35056EE5C30CACDE4F7850@BYAPR01MB4968.prod.exchangelabs.com>

Thank you Alex. I suspected I was missing something. In this case I didn't realize the error page would still need to flow through http_reply_access/reply_header_replace. I think that's what I need.

I didn't want to touch the code unless I could do something as complete as what you described. It's an enticing project for when extra time comes my way.

Thanks!

________________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Alex Rousskov <rousskov at measurement-factory.com>
Sent: Monday, September 23, 2019 6:03 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error page headers

On 9/23/19 12:49 AM, senor wrote:

> I have custom error pages with content needing the Content-Type
> header to reflect what it is (like JSON). I don't see any current
> options providing that option for error page handling.

I would start with http_reply_access/reply_header_replace combo, denying
the Content-Type response header in applicable Squid-generated error
responses (and only them) and then providing a replacement.


> Adaptation could work but seems like overkill.

There is not post-cache RESPMOD vectoring point support in Squid (and
most other proxies) -- adaptation is not applied to error pages.

You can also modify Squid sources, of course. This feature can probably
be quickly hacked into ErrorState::BuildHttpReply(). If you want
official admission, then a proper implementation would probably require
adding support for loading arbitrary response headers from the error
page template itself.

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From frio_cervesa at hotmail.com  Mon Sep 23 17:17:37 2019
From: frio_cervesa at hotmail.com (senor)
Date: Mon, 23 Sep 2019 17:17:37 +0000
Subject: [squid-users] Error page headers
In-Reply-To: <BYAPR01MB496835A35056EE5C30CACDE4F7850@BYAPR01MB4968.prod.exchangelabs.com>
References: <BYAPR01MB49686CA2E9698737B4070315F7850@BYAPR01MB4968.prod.exchangelabs.com>, 
 <96cc2ab9-f832-c783-3325-4c42cba3f49b@measurement-factory.com>,
 <BYAPR01MB496835A35056EE5C30CACDE4F7850@BYAPR01MB4968.prod.exchangelabs.com>
Message-ID: <BYAPR01MB4968CB1107441B34C95158AAF7850@BYAPR01MB4968.prod.exchangelabs.com>

Just for clarity for future searches I think that was intended to be
reply_header_access/reply_header_replace combo. It all starts sounding the same after a while.

________________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of senor <frio_cervesa at hotmail.com>
Sent: Monday, September 23, 2019 10:08 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error page headers

Thank you Alex. I suspected I was missing something. In this case I didn't realize the error page would still need to flow through http_reply_access/reply_header_replace. I think that's what I need.

I didn't want to touch the code unless I could do something as complete as what you described. It's an enticing project for when extra time comes my way.

Thanks!

________________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Alex Rousskov <rousskov at measurement-factory.com>
Sent: Monday, September 23, 2019 6:03 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error page headers

On 9/23/19 12:49 AM, senor wrote:

> I have custom error pages with content needing the Content-Type
> header to reflect what it is (like JSON). I don't see any current
> options providing that option for error page handling.

I would start with http_reply_access/reply_header_replace combo, denying
the Content-Type response header in applicable Squid-generated error
responses (and only them) and then providing a replacement.


> Adaptation could work but seems like overkill.

There is not post-cache RESPMOD vectoring point support in Squid (and
most other proxies) -- adaptation is not applied to error pages.

You can also modify Squid sources, of course. This feature can probably
be quickly hacked into ErrorState::BuildHttpReply(). If you want
official admission, then a proper implementation would probably require
adding support for loading arbitrary response headers from the error
page template itself.

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From eype69 at gmail.com  Mon Sep 23 17:23:33 2019
From: eype69 at gmail.com (John Sweet-Escott)
Date: Mon, 23 Sep 2019 18:23:33 +0100
Subject: [squid-users] Peek-and-splice not working when mixing TLS1.3
	servers and TLS1.2 clients
In-Reply-To: <a84f26eb-0580-f134-4d36-bfa7b1a4d5ea@measurement-factory.com>
References: <a84f26eb-0580-f134-4d36-bfa7b1a4d5ea@measurement-factory.com>
Message-ID: <C0120FA3-A50E-4AB5-AE98-FD7874C073E7@gmail.com>



>> On 22 Sep 2019, at 14:41, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> ?On 9/22/19 9:18 AM, Nikolaus wrote:
> 
>> The access.log contains error code / detail "ERR_SECURE_CONNECT_FAIL /
>> SQUID_ERR_SSL_HANDSHAKE" - which is not too helpful - but the cache.log
>> contains the more detailed "ERROR: negotiating TLS on FD 19:
>> error:1425F175:SSL routines:ssl_choose_client_version:inappropriate
>> fallback (1/-1/0)".
> 
>> Is a TLS fallback prevention mechanism kicking in by error? If so, how
>> to fix it?
> 
> I do not know the answers to your questions, but I am sure that it is
> possible to figure it out by looking at either packet captures or
> detailed debugging logs. Unfortunately, I do not have enough free time
> to guide you through this triage. There were several similar complains
> about "inappropriate fallback" errors on this list recently. I would
> start by revisiting those threads for more clues.
> 
> Alex
Unfortunately we have not been able to work out the inappropriate fallback issue described http://lists.squid-cache.org/pipermail/squid-users/2019-September/021047.html. If you do fix your issue, please do share. 
John
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190923/c71a12e2/attachment.htm>

From twk at ncsu.edu  Mon Sep 23 20:14:42 2019
From: twk at ncsu.edu (Tom Karches)
Date: Mon, 23 Sep 2019 16:14:42 -0400
Subject: [squid-users] Working proxy_protocol_access settings on Squid 3.5
	or 4?
Message-ID: <CAGZ9WNr9Hs5v1ToBiwGh0yAd1TgDFb4UQahq5sJ5i7OzMZTmtg@mail.gmail.com>

I am enabling proxy protocol on our FortiADC load balancer so that the
source IP of the proxy request can be logged. In the current configuration,
the address that is logged belongs to the NAT pool used by the load
balancer.

I added these config settings to configure the proxy_protocol_access. The
fortiadc ACL is the IP range of the NAT pool :

acl fortiadc src 10.50.54.0/24
proxy_protocol_access allow fortiadc

proxy_protocol_access allow localnet
follow_x_forwarded_for allow localhost
follow_x_forwarded_for allow localnet
acl_uses_indirect_client on
delay_pool_uses_indirect_client on
log_uses_indirect_client on
tproxy_uses_indirect_client off

I have updated my http_port line as such :
http_port 3128 require-proxy-header

I am now getting the error :
2019/09/23 16:03:15 kid1| PROXY protocol error: invalid header from local=
152.7.114.135:3128 remote=10.50.54.65:5028 FD 12 flags=1

The suggestion was to move to Squid 4 as noted here :
http://squid-web-proxy-cache.1019090.n4.nabble.com/error-in-parsing-Proxy-protocol-version-2-by-Squid-proxy-protocol-td4686763.html

This was back in Oct 2018. Has anything changed since then? Do I need to
upgrade to Squid 4? Currently running 3.5.20.

Thanks,
Tom

-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - Technology Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190923/2f89b32d/attachment.htm>

From rousskov at measurement-factory.com  Mon Sep 23 20:47:03 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Sep 2019 16:47:03 -0400
Subject: [squid-users] Working proxy_protocol_access settings on Squid
 3.5 or 4?
In-Reply-To: <CAGZ9WNr9Hs5v1ToBiwGh0yAd1TgDFb4UQahq5sJ5i7OzMZTmtg@mail.gmail.com>
References: <CAGZ9WNr9Hs5v1ToBiwGh0yAd1TgDFb4UQahq5sJ5i7OzMZTmtg@mail.gmail.com>
Message-ID: <9823677f-cd1c-129d-15d6-b3b7b4c84cac@measurement-factory.com>

On 9/23/19 4:14 PM, Tom Karches wrote:

> The suggestion was to move to Squid 4 as noted here :
> http://squid-web-proxy-cache.1019090.n4.nabble.com/error-in-parsing-Proxy-protocol-version-2-by-Squid-proxy-protocol-td4686763.html
> 
> This was back in Oct 2018. Has anything changed since then?

Yes, the changes I mentioned then have been accepted:

   https://github.com/squid-cache/squid/pull/342

The above pull request contained lots of PROXY protocol fixes and
several important improvements. Those changes are not in v4, but master
(future v5) code is available and works well for some. YMMV.

I do not recall any fixes going back into v3, but I did not check.

Alex.


From twk at ncsu.edu  Tue Sep 24 16:02:07 2019
From: twk at ncsu.edu (Tom Karches)
Date: Tue, 24 Sep 2019 12:02:07 -0400
Subject: [squid-users] Working proxy_protocol_access settings on Squid
 3.5 or 4?
In-Reply-To: <9823677f-cd1c-129d-15d6-b3b7b4c84cac@measurement-factory.com>
References: <CAGZ9WNr9Hs5v1ToBiwGh0yAd1TgDFb4UQahq5sJ5i7OzMZTmtg@mail.gmail.com>
 <9823677f-cd1c-129d-15d6-b3b7b4c84cac@measurement-factory.com>
Message-ID: <CAGZ9WNqkkEuPM2jQqCFrXdj1idOrMQekUFWQXA0GcaDW8Gmg8w@mail.gmail.com>

Alex,

Our current production configuration is squid 3.123 with LVS load
balancing. The desired production configuration is 3.5.20 with a FortiADC
load balancer. I am working with networking staff on the configuration. If
I directly connect to the actual proxy server behind the load balancer, I
get :

2019/09/24 11:31:46 kid1| PROXY protocol error: invalid header from local=
152.7.114.8:3128 remote=152.7.148.47:65220 FD 16 flags=1

Relevant squid.conf on the server looks like this :

acl fortiadc src 10.50.54.0/24
acl fortiadc src 152.7.148.0/24 <----temporary for testing by going
directly to the proxy server and not the load balancer
proxy_protocol_access allow fortiadc

proxy_protocol_access allow localnet
follow_x_forwarded_for allow localhost
follow_x_forwarded_for allow localnet
acl_uses_indirect_client on
delay_pool_uses_indirect_client on
log_uses_indirect_client on
tproxy_uses_indirect_client off
...
http_port 3128 require-proxy-header

I have tried adding the following...it appears to make no difference.

http_port 127.0.0.1:3128

So, you are saying that v4 does not contain changes to fix the "PROXY
protocol error" and my only option at this point is v5 code? (or fall back
to using LVS with 3.5.20) Just trying to understand my options.

Thanks,
Tom

On Mon, Sep 23, 2019 at 4:47 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 9/23/19 4:14 PM, Tom Karches wrote:
>
> > The suggestion was to move to Squid 4 as noted here :
> >
> http://squid-web-proxy-cache.1019090.n4.nabble.com/error-in-parsing-Proxy-protocol-version-2-by-Squid-proxy-protocol-td4686763.html
> >
> > This was back in Oct 2018. Has anything changed since then?
>
> Yes, the changes I mentioned then have been accepted:
>
>    https://github.com/squid-cache/squid/pull/342
>
> The above pull request contained lots of PROXY protocol fixes and
> several important improvements. Those changes are not in v4, but master
> (future v5) code is available and works well for some. YMMV.
>
> I do not recall any fixes going back into v3, but I did not check.
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - Technology Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190924/d84a9fb3/attachment.htm>

From rousskov at measurement-factory.com  Tue Sep 24 18:41:34 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Sep 2019 14:41:34 -0400
Subject: [squid-users] Working proxy_protocol_access settings on Squid
 3.5 or 4?
In-Reply-To: <CAGZ9WNqkkEuPM2jQqCFrXdj1idOrMQekUFWQXA0GcaDW8Gmg8w@mail.gmail.com>
References: <CAGZ9WNr9Hs5v1ToBiwGh0yAd1TgDFb4UQahq5sJ5i7OzMZTmtg@mail.gmail.com>
 <9823677f-cd1c-129d-15d6-b3b7b4c84cac@measurement-factory.com>
 <CAGZ9WNqkkEuPM2jQqCFrXdj1idOrMQekUFWQXA0GcaDW8Gmg8w@mail.gmail.com>
Message-ID: <505612d6-e8b6-c238-99a2-02780f91fdd9@measurement-factory.com>

On 9/24/19 12:02 PM, Tom Karches wrote:

> 2019/09/24 11:31:46 kid1| PROXY protocol error: invalid header ...

> So, you are saying that v4 does not contain changes to fix the "PROXY
> protocol error" and my only option at this point is v5 code?

Unfortunately, I do not know what causes that specific error, I do not
know whether it was fixed, and, if it was fixed, which Squid versions
contain the fix. In other words, my previous answer did not imply
anything beyond the stated facts.

There may be no good way to answer your questions, but the best way
could be to test the latest Squid v4 release and the latest Squid v5
snapshot.

Alex.


> On Mon, Sep 23, 2019 at 4:47 PM Alex Rousskov wrote:
> 
>     On 9/23/19 4:14 PM, Tom Karches wrote:
> 
>     > The suggestion was to move to Squid 4 as noted here :
>     >
>     http://squid-web-proxy-cache.1019090.n4.nabble.com/error-in-parsing-Proxy-protocol-version-2-by-Squid-proxy-protocol-td4686763.html
>     >
>     > This was back in Oct 2018. Has anything changed since then?
> 
>     Yes, the changes I mentioned then have been accepted:
> 
>     ? ?https://github.com/squid-cache/squid/pull/342
> 
>     The above pull request contained lots of PROXY protocol fixes and
>     several important improvements. Those changes are not in v4, but master
>     (future v5) code is available and works well for some. YMMV.
> 
>     I do not recall any fixes going back into v3, but I did not check.
> 
>     Alex.



From alexandrei5691 at gmail.com  Wed Sep 25 11:12:48 2019
From: alexandrei5691 at gmail.com (Alessandro Andrei)
Date: Wed, 25 Sep 2019 13:12:48 +0200
Subject: [squid-users] Warning: ACL is used in context without an HTTP
	response
Message-ID: <B0C85BFE-E4F1-42C8-8357-DB322EB5DB82@gmail.com>

My access_log file il flooded with messages that I do not want to see

Specifically 
1) CONNECT vortex-win.data.microsoft.com
2) TCP_DENIED/407

So I created two ACLs to exclude them from logging and applied it to my access log

acl AuthRequest http_status 407
acl excludefromlog dstdomain .vortex-win.data.microsoft.com
access_log stdio:/var/log/squid/access.log logformat=squid !AuthRequest !excludefromlog

It works but now my cache.log is flooded with
WARNING: AuthRequest ACL is used in context without an HTTP response. Assuming mismatch.


I read in the mailing list archives this is the correct behaviour, but isn't there a way to avoid all this?

Thanks


From tevfik.ceydeliler at gmail.com  Wed Sep 25 11:58:43 2019
From: tevfik.ceydeliler at gmail.com (Tevfik Ceydeliler)
Date: Wed, 25 Sep 2019 14:58:43 +0300
Subject: [squid-users] Kerberos nad keytab problem
Message-ID: <CAJTWAhEy7ec=HwDNUKeWuHOL+idvX5nrws9GJjQncR++LrHWqw@mail.gmail.com>

Hi, I try to use kerberos in my squid. Nut I get an error message :

############################33
msktutil --auto-update --verbose --computer-name suqidpnb1 --server
dctoyo1.toyo.grp -k /etc/squid/PROXY.keytab
 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the
computer account
 -- generate_new_password:  Characters read from /dev/urandom = 95
 -- create_fake_krb5_conf: Created a fake krb5.conf file:
/tmp/.msktkrb5.conf-QCbGC5
 -- destroy_g_context: Destroying Kerberos Context
 -- initialize_g_context: Creating Kerberos Context
 -- finalize_exec: SAM Account Name is: suqidpnb1$
 -- try_machine_keytab_princ: Trying to authenticate for suqidpnb1$ from
local keytab
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key
table entry not found)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for SUQIDPNB1$ from
local keytab
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key
table entry not found)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/localhost
from local keytab
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key
table entry not found)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for suqidpnb1$ with
password
 -- create_default_machine_password: Default machine password for
suqidpnb1$ is suqidpnb1
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client
not found in Kerberos database)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets
 -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials
cache found)
 -- try_user_creds: User ticket cache was not valid
Error: could not find any credentials to authenticate with. Neither keytab,
default machine password, nor calling user's tickets worked. Try
"kinit"ing yourself some tickets with permission to create computer
objects, or pre-creating the computer object in AD and selecting
'reset account'.

#############################33
Can't find why this happen:


My AD is 2012R2 function level
I create keytab with this:
msktutil -c -b "OU=Servers,DC=toyo,DC=grp" -s HTTP/squidtoyopnb1.toyo.grp
-k /etc/squid/PROXY.keytab --computer-name SQUIDPNB1 --upn
HTTP/squidtoyopnb1.toyo.grp --server dctoyo1.toyo.grp --verbose --enctypes
28

Keytab file permission is:
-rw-r----- 1 root squid 933 Sep 25 13:37 PROXY.keytab

and keytab file (klist -k output):

   3 SQUIDPNB1$@TOYO.GRP
   3 SQUIDPNB1$@TOYO.GRP
   3 SQUIDPNB1$@TOYO.GRP
   3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
   3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
   3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
   3 host/squidtoyopnb1 at TOYO.GRP
   3 host/squidtoyopnb1 at TOYO.GRP
   3 host/squidtoyopnb1 at TOYO.GRP
   3 host/squidtoyopnb1.toyo.grp at TOYO.GRP
   3 host/squidtoyopnb1.toyo.grp at TOYO.GRP
   3 host/squidtoyopnb1.toyo.grp at TOYO.GRP

krb5.conf:
[libdefaults]
default_realm = TOYO.GRP
        dns_lookup_kdc = no
        dns_lookup_realm = no
        ticket_lifetime = 24h
        default_keytab_name = /etc/squid/PROXY.keytab

    ; for Windows 2008 with AES
          default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac
des-cbc-crc des-cbc-md5
          default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac
des-cbc-crc des-cbc-md5
          permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc
des-cbc-md5

    [realms]
TOYO.GRP = {
                kdc = dctoyo1.toyo.grp
                kdc = DCTOYO2.toyo.grp
                admin_server = 10.65.12.254
                default_domain = toyo.grp
     }

    [domain_realm]
     toyo.grp = TOYO.GRP
     .toyo.grp = TOYO.GRP

    [logging]
      kdc = FILE:/var/log/kdc.log
      admin_server = FILE:/var/log/kadmin.log
      default = FILE:/var/log/krb5lib.log




-- 
Tevfik Ceydeliler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190925/b46c996d/attachment.htm>

From squid3 at treenet.co.nz  Wed Sep 25 12:12:39 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Sep 2019 00:12:39 +1200
Subject: [squid-users] Working proxy_protocol_access settings on Squid
 3.5 or 4?
In-Reply-To: <505612d6-e8b6-c238-99a2-02780f91fdd9@measurement-factory.com>
References: <CAGZ9WNr9Hs5v1ToBiwGh0yAd1TgDFb4UQahq5sJ5i7OzMZTmtg@mail.gmail.com>
 <9823677f-cd1c-129d-15d6-b3b7b4c84cac@measurement-factory.com>
 <CAGZ9WNqkkEuPM2jQqCFrXdj1idOrMQekUFWQXA0GcaDW8Gmg8w@mail.gmail.com>
 <505612d6-e8b6-c238-99a2-02780f91fdd9@measurement-factory.com>
Message-ID: <d440bafe-1bf5-7b33-8106-d16696effe5d@treenet.co.nz>

On 25/09/19 6:41 am, Alex Rousskov wrote:
> On 9/24/19 12:02 PM, Tom Karches wrote:
> 
>> 2019/09/24 11:31:46 kid1| PROXY protocol error: invalid header ...
> 
>> So, you are saying that v4 does not contain changes to fix the "PROXY
>> protocol error" and my only option at this point is v5 code?
> 

My understanding of the situation is that the bugs only affect
interpretation of PROXYv2 headers.

PROXYv1 should work in v4. At least, it is being used by several testers.

One thing to note is that PROXY protocol requires clients send the
entire header as one packet such that it is read as one operation by the
receiver. Those recipients are *required* to reject PROXYv1 that do not
arrive as a single I/O.

Amos


From rousskov at measurement-factory.com  Wed Sep 25 13:29:17 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 25 Sep 2019 09:29:17 -0400
Subject: [squid-users] Warning: ACL is used in context without an HTTP
 response
In-Reply-To: <B0C85BFE-E4F1-42C8-8357-DB322EB5DB82@gmail.com>
References: <B0C85BFE-E4F1-42C8-8357-DB322EB5DB82@gmail.com>
Message-ID: <263921f2-b2cd-30ea-341b-8cb59702e11a@measurement-factory.com>

On 9/25/19 7:12 AM, Alessandro Andrei wrote:
> My access_log file il flooded with messages that I do not want to see
> 
> Specifically 
> 1) CONNECT vortex-win.data.microsoft.com
> 2) TCP_DENIED/407
> 
> So I created two ACLs to exclude them from logging and applied it to my access log
> 
> acl AuthRequest http_status 407
> acl excludefromlog dstdomain .vortex-win.data.microsoft.com
> access_log stdio:/var/log/squid/access.log logformat=squid !AuthRequest !excludefromlog

FYI: If you have two unwanted record kinds (one CONNECT and one
TCP_DENIED/407), then you should use a different rule to block _each_
record kind. The current rule only blocks logging of records that match
_both_ conditions. Look for any-of ACL.


> It works but now my cache.log is flooded with
> WARNING: AuthRequest ACL is used in context without an HTTP response. Assuming mismatch.

> I read in the mailing list archives this is the correct behaviour,

In the context of access_log, it is most likely a Squid bug. We have
recently fixed one similar bug but there may be more:
https://github.com/squid-cache/squid/pull/476


BTW, AuthRequest name is misleading. I know what you meant, but the
http_status ACL checks the response, not the request.


> but isn't there a way to avoid all this?

You can try patching your Squid or testing the fixed version. The former
may be difficult if you are not running master-based code (future v5).

You may also be able to avoid the WARNING if you adjust the rule to
check the "has response" condition before checking AuthRequest. Look for
the "has" ACL. However, you would have to decide whether to log all
records that lack responses. If any of those responses are absent due to
Squid bugs, your decision, whatever it is, may be incorrect in some cases.

Alex.


From belle at bazuin.nl  Wed Sep 25 15:01:49 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 25 Sep 2019 17:01:49 +0200
Subject: [squid-users] Kerberos nad keytab problem
In-Reply-To: <CAJTWAhEy7ec=HwDNUKeWuHOL+idvX5nrws9GJjQncR++LrHWqw@mail.gmail.com>
References: <CAJTWAhEy7ec=HwDNUKeWuHOL+idvX5nrws9GJjQncR++LrHWqw@mail.gmail.com>
Message-ID: <vmime.5d8b815d.644e.66bf9b8b2d7cc2ad@ms249-lin-003.rotterdam.bazuin.nl>

I also had problems with msktutil.. so i suggest you try this, see below.. 
Im using it for few years and it always works (for me offcourse).. 
?
It?should be pretty simple, but the site squid-cache (wiki) is in my opinion a bit outdated. 
And its for Amos to adapt it on the site.
?
Amos or Alex, please review below, you might want to add it. 
And add your parts to it, like running this without a correct spn. 
?
Its tested in use?and and working since squid 3.1 upto 4.8. 
Tested on debian Wheezy (7) upto Buster (10)
?
Below assumes the server your setting up, does have an A and PTR record. 

(note, which should be added at the?domain join of winbind, as of samba4.x )
?
This is my howto. 
A?Debian based,?with Kerberos Auth against an Samba Active Directory
Should be adaptable for any OS, should also work with MS Active Directory. 
But since i dont have any, im not testing it. 
?
?
# Install a minimal OS, at install only choose base + ssh server. 
# Setup these variable for a copy/past, might be handy, and then "it just works"? 
?
# Obligated to set.? # ADDOM; 
# This should match the netbios (NT4) domain name in caps, per example from a login: NTDOM\username 
ADDOM="NTDOM" 
?
# These should be fine, but if you have multiple ipnumbers and hostnames, you might want to adjust these. 
FQDN="$(hostname -f)"
HOSTN="$(hostname -s)"

# Requirements before you start installing the sofrware like: squid winbind krb5-user
?
# Login, sudo to root.
#?/etc/resolv.conf, set as followed. 
#search must.match.your.primarydnsdomain.tld
# nameserver ip_of_AD_DC
?
# Verify it: 
grep search /etc/resolv.conf
grep nameserver /etc/resolv.conf

?
# If ok, then run : 
apt update?
apt install squid winbind krb5-user -y
?
# Just hit enter on every question, the defaults are fine. (verified in Debian).
?
# And now verify /etc/krb5.conf
less /etc/krb5.conf
?
?
# It should look like this :? 
#[libdefaults]
#??????? default_realm = YOUR.Detected_REALM.TLD 
#
# The following krb5.conf variables are only for MIT Kerberos.
#?????? kdc_timesync = 1
#??????? ccache_type = 4
#??????? forwardable = true
#??????? proxiable = true
?
# ... and more.. 

#? >>? P.s.? i never touch krb5.conf, never needed, it "just works" << 
?
# Set REALM Variable now, default should be ok. dont touch it. 
REALM="$(grep default_realm /etc/krb5.conf |awk {' print $NF '}) "
# It's used for smb.conf and the auth part of squid. 
?

# then stop squid and samba and configure it.
systemctl stop squid winbind
?
# flush the log, so if you start it you?start with a clean log. ?
>?/var/log/squid/cache.log
?
#?Configure smb.conf and join the AD domain,? the minimal setting for smb.conf.
cp /etc/samba/smb.conf{,.original}
?
echo "# Auth-Only setup with winbind. ( no Shares )
?
??? workgroup = ${ADDOM}
??? security = ADS
??? realm = ${REALM}
??? netbios name = $(echo ${HOSTN^^})
?
??? ## make sure the below number never overlap system ranges, see /etc/adduser.conf 
??? ## map id's outside to domain to tdb files.
??? idmap config *: backend = tdb
??? idmap config *: range = 2000-9999
?
??? ## map ids from the domain and (*) the range may not overlap !
??? idmap config ${ADDOM} : backend = rid
??? idmap config ${ADDOM} : range = 10000-3999999
?
??? kerberos method = secrets and keytab
??? dedicated keytab file = /etc/krb5.keytab
?
??? # renew the kerberos ticket
??? winbind refresh tickets = yes
" > /etc/samba/smb.conf
?
# And verify it.
less /etc/samba/smb.conf
?
# Next step, join the AD domain. 
# Login/auth with kerberos. 
kinit Administrator
?
# and join the domain.
net ads join -k
?
# Creating the squid keytab file.
?
export KRB5_KTNAME=FILE:/etc/squid/squid-HTTP-${HOSTN}.keytab
net ads keytab ADD HTTP/${FQDN}

#Verify the keytab file : 
klist -ke /etc/squid/squid-HTTP-${HOSTN}.keytab
?
# destroy you authentication ticket for Administrator. 
kdestroy 
?
# set correct rights. 
chmod 640 /etc/squid/squid-HTTP-${HOSTN}.keytab
chown root:proxy /etc/squid/squid-HTTP-${HOSTN}.keytab
# Note, you might need to change the "proxy" group name here. 
?
# and setup you squid auth. 
echo "auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \\
??? --kerberos /usr/lib/squid/negotiate_kerberos_auth \\
????? -k etc/squid/squid-HTTP-${HOSTN}.keytab" \\
????? -s HTTP/"${FQDN}"@"${REALM}"? \\
??? --ntlm /usr/bin/ntlm_auth \\
????? --helper-protocol=gss-spnego --domain="${ADDOM}"
?
auth_param negotiate children 30 startup=5 idle=5
auth_param negotiate children 10
auth_param negotiate keep_alive on" > /etc/squid/conf.d/auth.conf
?
systemctl start winbind squid 
?
# Done 
# And check squid log how it started. 
cat /var/log/squid/cache.log

Now go configure the other parts you need of squid. 

And enjoy..? :-) 
?
?
Greetz, 
?
Louis
?
?
?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Tevfik Ceydeliler
Verzonden: woensdag 25 september 2019 13:59
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] Kerberos nad keytab problem



Hi, I try to use kerberos in my squid. Nut I get an error message :


############################33
msktutil --auto-update --verbose --computer-name suqidpnb1 --server dctoyo1.toyo.grp -k /etc/squid/PROXY.keytab ?
?-- init_password: Wiping the computer password structure
?-- generate_new_password: Generating a new, random password for the computer account
?-- generate_new_password: ?Characters read from /dev/urandom = 95
?-- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-QCbGC5
?-- destroy_g_context: Destroying Kerberos Context
?-- initialize_g_context: Creating Kerberos Context
?-- finalize_exec: SAM Account Name is: suqidpnb1$
?-- try_machine_keytab_princ: Trying to authenticate for suqidpnb1$ from local keytab
?-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
?-- try_machine_keytab_princ: Authentication with keytab failed
?-- try_machine_keytab_princ: Trying to authenticate for SUQIDPNB1$ from local keytab
?-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
?-- try_machine_keytab_princ: Authentication with keytab failed
?-- try_machine_keytab_princ: Trying to authenticate for host/localhost from local keytab
?-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
?-- try_machine_keytab_princ: Authentication with keytab failed
?-- try_machine_password: Trying to authenticate for suqidpnb1$ with password
?-- create_default_machine_password: Default machine password for suqidpnb1$ is suqidpnb1
?-- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
?-- try_machine_password: Authentication with password failed
?-- try_user_creds: Checking if default ticket cache has tickets
?-- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
?-- try_user_creds: User ticket cache was not valid
Error: could not find any credentials to authenticate with. Neither keytab,
default machine password, nor calling user's tickets worked. Try
"kinit"ing yourself some tickets with permission to create computer
objects, or pre-creating the computer object in AD and selecting
'reset account'.



#############################33
Can't find why this happen:




My AD is 2012R2 function level
I create keytab with this:
msktutil -c -b "OU=Servers,DC=toyo,DC=grp" -s HTTP/squidtoyopnb1.toyo.grp -k /etc/squid/PROXY.keytab --computer-name SQUIDPNB1 --upn HTTP/squidtoyopnb1.toyo.grp --server dctoyo1.toyo.grp --verbose --enctypes 28



Keytab file permission is:
-rw-r----- 1 root squid 933 Sep 25 13:37 PROXY.keytab



and keytab file (klist -k output):


? ?3 SQUIDPNB1$@TOYO.GRP
? ?3 SQUIDPNB1$@TOYO.GRP
? ?3 SQUIDPNB1$@TOYO.GRP
? ?3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 host/squidtoyopnb1 at TOYO.GRP
? ?3 host/squidtoyopnb1 at TOYO.GRP
? ?3 host/squidtoyopnb1 at TOYO.GRP
? ?3 host/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 host/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 host/squidtoyopnb1.toyo.grp at TOYO.GRP



krb5.conf:
[libdefaults]
default_realm = TOYO.GRP
? ? ? ? dns_lookup_kdc = no
? ? ? ? dns_lookup_realm = no
? ? ? ? ticket_lifetime = 24h
? ? ? ? default_keytab_name = /etc/squid/PROXY.keytab

? ? ; for Windows 2008 with AES
? ? ? ? ? default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
? ? ? ? ? default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
? ? ? ? ? permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

? ? [realms]
TOYO.GRP = {
? ? ? ? ? ? ? ? kdc = dctoyo1.toyo.grp
? ? ? ? ? ? ? ? kdc = DCTOYO2.toyo.grp
? ? ? ? ? ? ? ? admin_server = 10.65.12.254
? ? ? ? ? ? ? ? default_domain = toyo.grp
? ? ?}

? ? [domain_realm]
? ? ?toyo.grp = TOYO.GRP
? ? ?.toyo.grp = TOYO.GRP

? ? [logging]
? ? ? kdc = FILE:/var/log/kdc.log
? ? ? admin_server = FILE:/var/log/kadmin.log
? ? ? default = FILE:/var/log/krb5lib.log









-- 
Tevfik Ceydeliler

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190925/cd55eb1c/attachment.htm>

From rousskov at measurement-factory.com  Wed Sep 25 15:07:42 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 25 Sep 2019 11:07:42 -0400
Subject: [squid-users] Kerberos nad keytab problem
In-Reply-To: <vmime.5d8b815d.644e.66bf9b8b2d7cc2ad@ms249-lin-003.rotterdam.bazuin.nl>
References: <CAJTWAhEy7ec=HwDNUKeWuHOL+idvX5nrws9GJjQncR++LrHWqw@mail.gmail.com>
 <vmime.5d8b815d.644e.66bf9b8b2d7cc2ad@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <762c0b0f-3485-b638-24d6-02d18a6f7ffb@measurement-factory.com>

On 9/25/19 11:01 AM, L.P.H. van Belle wrote:
> I also had problems with msktutil.. so i suggest you try this, see below..
> Im using it for few years and it always works (for me offcourse)..
> ?
> It?should be pretty simple, but the site squid-cache (wiki) is in my
> opinion a bit outdated.

Anybody knowledgeable about a particular subject can register and update
the Squid Project wiki. Rule of thumb: If you can improve the wiki, do
not wait for others.


> Amos or Alex, please review below, you might want to add it.

Sorry, this is not my area of expertise -- I would not be able to review
the details you have provided.

Alex.


From alexandrei5691 at gmail.com  Wed Sep 25 15:07:41 2019
From: alexandrei5691 at gmail.com (Alessandro Andrei)
Date: Wed, 25 Sep 2019 17:07:41 +0200
Subject: [squid-users] Warning: ACL is used in context without an HTTP
 response
In-Reply-To: <263921f2-b2cd-30ea-341b-8cb59702e11a@measurement-factory.com>
References: <B0C85BFE-E4F1-42C8-8357-DB322EB5DB82@gmail.com>
 <263921f2-b2cd-30ea-341b-8cb59702e11a@measurement-factory.com>
Message-ID: <3728fa84-7123-70c7-5e8e-8ec1208668e6@gmail.com>



On 25/09/2019 15:29, Alex Rousskov wrote:
> On 9/25/19 7:12 AM, Alessandro Andrei wrote:
>> My access_log file il flooded with messages that I do not want to see
>>
>> Specifically
>> 1) CONNECT vortex-win.data.microsoft.com
>> 2) TCP_DENIED/407
>>
>> So I created two ACLs to exclude them from logging and applied it to my access log
>>
>> acl AuthRequest http_status 407
>> acl excludefromlog dstdomain .vortex-win.data.microsoft.com
>> access_log stdio:/var/log/squid/access.log logformat=squid !AuthRequest !excludefromlog
> 
> FYI: If you have two unwanted record kinds (one CONNECT and one
> TCP_DENIED/407), then you should use a different rule to block _each_
> record kind. The current rule only blocks logging of records that match
> _both_ conditions. Look for any-of ACL.


Of course, silly me!
Changed to

acl AuthResponse407 http_status 407
acl excludefromlog dstdomain .vortex-win.data.microsoft.com
acl DoNotLog any-of AuthResponse407 excludefromlog
access_log stdio:/var/log/squid/access.log logformat=squid !DoNotLog




>> but isn't there a way to avoid all this?
> 
> You can try patching your Squid or testing the fixed version. The former
> may be difficult if you are not running master-based code (future v5).


I'm running version 3.5.20, the one that is installed by default with 
Centos 7 repos

I guess I should download and compile version 4.8...


> You may also be able to avoid the WARNING if you adjust the rule to
> check the "has response" condition before checking AuthRequest. Look for
> the "has" ACL. However, you would have to decide whether to log all
> records that lack responses. If any of those responses are absent due to
> Squid bugs, your decision, whatever it is, may be incorrect in some cases.

OK, if I get it correctly the "has" ACL requires version 4, so I DO have 
to upgrade
Anyway it's not clear to me how this check should be done in my 
configuration


Thanks!


From rafael.akchurin at diladele.com  Wed Sep 25 15:27:30 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 25 Sep 2019 15:27:30 +0000
Subject: [squid-users] Kerberos nad keytab problem
In-Reply-To: <vmime.5d8b815d.644e.66bf9b8b2d7cc2ad@ms249-lin-003.rotterdam.bazuin.nl>
References: <CAJTWAhEy7ec=HwDNUKeWuHOL+idvX5nrws9GJjQncR++LrHWqw@mail.gmail.com>
 <vmime.5d8b815d.644e.66bf9b8b2d7cc2ad@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <AM0PR04MB47531AEF11A32F576882CAB98F870@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello everyone,

Just my two cents too. Note you can map the *user* to the Kerberos SPN - this lets you have your squid proxy live outside of the AD.
Just setup the dedicated user in the AD, map SPN to it and export the keytab to your squid.

See https://docs.diladele.com/administrator_guide_stable/active_directory/index.html

Downside - the password for that designated user needs to be non expiring or you'd be regenerating keytabs everytime the password changes. Which is not difficult anyway too.

Best regards,
Rafael Akchurin
Diladele B.V.



From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of L.P.H. van Belle
Sent: Wednesday, 25 September 2019 17:02
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Kerberos nad keytab problem

I also had problems with msktutil.. so i suggest you try this, see below..
Im using it for few years and it always works (for me offcourse)..

It should be pretty simple, but the site squid-cache (wiki) is in my opinion a bit outdated.
And its for Amos to adapt it on the site.

Amos or Alex, please review below, you might want to add it.
And add your parts to it, like running this without a correct spn.

Its tested in use and and working since squid 3.1 upto 4.8.
Tested on debian Wheezy (7) upto Buster (10)

Below assumes the server your setting up, does have an A and PTR record.
(note, which should be added at the domain join of winbind, as of samba4.x )

This is my howto.
A Debian based, with Kerberos Auth against an Samba Active Directory
Should be adaptable for any OS, should also work with MS Active Directory.
But since i dont have any, im not testing it.


# Install a minimal OS, at install only choose base + ssh server.
# Setup these variable for a copy/past, might be handy, and then "it just works"

# Obligated to set.  # ADDOM;
# This should match the netbios (NT4) domain name in caps, per example from a login: NTDOM\username
ADDOM="NTDOM"

# These should be fine, but if you have multiple ipnumbers and hostnames, you might want to adjust these.
FQDN="$(hostname -f)"
HOSTN="$(hostname -s)"

# Requirements before you start installing the sofrware like: squid winbind krb5-user

# Login, sudo to root.
# /etc/resolv.conf, set as followed.
#search must.match.your.primarydnsdomain.tld
# nameserver ip_of_AD_DC

# Verify it:
grep search /etc/resolv.conf
grep nameserver /etc/resolv.conf

# If ok, then run :
apt update
apt install squid winbind krb5-user -y

# Just hit enter on every question, the defaults are fine. (verified in Debian).

# And now verify /etc/krb5.conf
less /etc/krb5.conf


# It should look like this :
#[libdefaults]
#        default_realm = YOUR.Detected_REALM.TLD
#
# The following krb5.conf variables are only for MIT Kerberos.
#       kdc_timesync = 1
#        ccache_type = 4
#        forwardable = true
#        proxiable = true

# ... and more..

#  >>  P.s.  i never touch krb5.conf, never needed, it "just works" <<

# Set REALM Variable now, default should be ok. dont touch it.
REALM="$(grep default_realm /etc/krb5.conf |awk {' print $NF '}) "
# It's used for smb.conf and the auth part of squid.

# then stop squid and samba and configure it.
systemctl stop squid winbind

# flush the log, so if you start it you start with a clean log.
> /var/log/squid/cache.log

# Configure smb.conf and join the AD domain,  the minimal setting for smb.conf.
cp /etc/samba/smb.conf{,.original}

echo "# Auth-Only setup with winbind. ( no Shares )

    workgroup = ${ADDOM}
    security = ADS
    realm = ${REALM}
    netbios name = $(echo ${HOSTN^^})

    ## make sure the below number never overlap system ranges, see /etc/adduser.conf
    ## map id's outside to domain to tdb files.
    idmap config *: backend = tdb
    idmap config *: range = 2000-9999

    ## map ids from the domain and (*) the range may not overlap !
    idmap config ${ADDOM} : backend = rid
    idmap config ${ADDOM} : range = 10000-3999999

    kerberos method = secrets and keytab
    dedicated keytab file = /etc/krb5.keytab

    # renew the kerberos ticket
    winbind refresh tickets = yes
" > /etc/samba/smb.conf

# And verify it.
less /etc/samba/smb.conf

# Next step, join the AD domain.
# Login/auth with kerberos.
kinit Administrator

# and join the domain.
net ads join -k

# Creating the squid keytab file.

export KRB5_KTNAME=FILE:/etc/squid/squid-HTTP-${HOSTN}.keytab
net ads keytab ADD HTTP/${FQDN}

#Verify the keytab file :
klist -ke /etc/squid/squid-HTTP-${HOSTN}.keytab

# destroy you authentication ticket for Administrator.
kdestroy

# set correct rights.
chmod 640 /etc/squid/squid-HTTP-${HOSTN}.keytab
chown root:proxy /etc/squid/squid-HTTP-${HOSTN}.keytab
# Note, you might need to change the "proxy" group name here.

# and setup you squid auth.
echo "auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \\
    --kerberos /usr/lib/squid/negotiate_kerberos_auth \\
      -k etc/squid/squid-HTTP-${HOSTN}.keytab" \\
      -s HTTP/"${FQDN}"@"${REALM}"  \\
    --ntlm /usr/bin/ntlm_auth \\
      --helper-protocol=gss-spnego --domain="${ADDOM}"

auth_param negotiate children 30 startup=5 idle=5
auth_param negotiate children 10
auth_param negotiate keep_alive on" > /etc/squid/conf.d/auth.conf

systemctl start winbind squid

# Done
# And check squid log how it started.
cat /var/log/squid/cache.log
Now go configure the other parts you need of squid.
And enjoy..  :-)


Greetz,

Louis




________________________________
Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Tevfik Ceydeliler
Verzonden: woensdag 25 september 2019 13:59
Aan: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Onderwerp: [squid-users] Kerberos nad keytab problem
Hi, I try to use kerberos in my squid. Nut I get an error message :

############################33
msktutil --auto-update --verbose --computer-name suqidpnb1 --server dctoyo1.toyo.grp -k /etc/squid/PROXY.keytab
 -- init_password: Wiping the computer password structure
 -- generate_new_password: Generating a new, random password for the computer account
 -- generate_new_password:  Characters read from /dev/urandom = 95
 -- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-QCbGC5
 -- destroy_g_context: Destroying Kerberos Context
 -- initialize_g_context: Creating Kerberos Context
 -- finalize_exec: SAM Account Name is: suqidpnb1$
 -- try_machine_keytab_princ: Trying to authenticate for suqidpnb1$ from local keytab
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for SUQIDPNB1$ from local keytab
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_keytab_princ: Trying to authenticate for host/localhost from local keytab
 -- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
 -- try_machine_keytab_princ: Authentication with keytab failed
 -- try_machine_password: Trying to authenticate for suqidpnb1$ with password
 -- create_default_machine_password: Default machine password for suqidpnb1$ is suqidpnb1
 -- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
 -- try_machine_password: Authentication with password failed
 -- try_user_creds: Checking if default ticket cache has tickets
 -- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
 -- try_user_creds: User ticket cache was not valid
Error: could not find any credentials to authenticate with. Neither keytab,
default machine password, nor calling user's tickets worked. Try
"kinit"ing yourself some tickets with permission to create computer
objects, or pre-creating the computer object in AD and selecting
'reset account'.

#############################33
Can't find why this happen:


My AD is 2012R2 function level
I create keytab with this:
msktutil -c -b "OU=Servers,DC=toyo,DC=grp" -s HTTP/squidtoyopnb1.toyo.grp -k /etc/squid/PROXY.keytab --computer-name SQUIDPNB1 --upn HTTP/squidtoyopnb1.toyo.grp --server dctoyo1.toyo.grp --verbose --enctypes 28

Keytab file permission is:
-rw-r----- 1 root squid 933 Sep 25 13:37 PROXY.keytab

and keytab file (klist -k output):

   3 SQUIDPNB1$@TOYO.GRP<mailto:SQUIDPNB1$@TOYO.GRP>
   3 SQUIDPNB1$@TOYO.GRP<mailto:SQUIDPNB1$@TOYO.GRP>
   3 SQUIDPNB1$@TOYO.GRP<mailto:SQUIDPNB1$@TOYO.GRP>
   3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP<mailto:HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP>
   3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP<mailto:HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP>
   3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP<mailto:HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP>
   3 host/squidtoyopnb1 at TOYO.GRP<mailto:host/squidtoyopnb1 at TOYO.GRP>
   3 host/squidtoyopnb1 at TOYO.GRP<mailto:host/squidtoyopnb1 at TOYO.GRP>
   3 host/squidtoyopnb1 at TOYO.GRP<mailto:host/squidtoyopnb1 at TOYO.GRP>
   3 host/squidtoyopnb1.toyo.grp at TOYO.GRP<mailto:host/squidtoyopnb1.toyo.grp at TOYO.GRP>
   3 host/squidtoyopnb1.toyo.grp at TOYO.GRP<mailto:host/squidtoyopnb1.toyo.grp at TOYO.GRP>
   3 host/squidtoyopnb1.toyo.grp at TOYO.GRP<mailto:host/squidtoyopnb1.toyo.grp at TOYO.GRP>

krb5.conf:
[libdefaults]
default_realm = TOYO.GRP
        dns_lookup_kdc = no
        dns_lookup_realm = no
        ticket_lifetime = 24h
        default_keytab_name = /etc/squid/PROXY.keytab

    ; for Windows 2008 with AES
          default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
          default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
          permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

    [realms]
TOYO.GRP = {
                kdc = dctoyo1.toyo.grp
                kdc = DCTOYO2.toyo.grp
                admin_server = 10.65.12.254
                default_domain = toyo.grp
     }

    [domain_realm]
     toyo.grp = TOYO.GRP
     .toyo.grp = TOYO.GRP

    [logging]
      kdc = FILE:/var/log/kdc.log
      admin_server = FILE:/var/log/kadmin.log
      default = FILE:/var/log/krb5lib.log




--
Tevfik Ceydeliler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190925/ac0479df/attachment.htm>

From belle at bazuin.nl  Wed Sep 25 15:40:59 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 25 Sep 2019 17:40:59 +0200
Subject: [squid-users] Kerberos nad keytab problem
In-Reply-To: <AM0PR04MB47531AEF11A32F576882CAB98F870@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <vmime.5d8b815d.644e.66bf9b8b2d7cc2ad@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.5d8b8a8b.2d5b.749447cd32f5e3bb@ms249-lin-003.rotterdam.bazuin.nl>

Hai Rafael, 
?
Yes, i did that in an older setup, with you site guidance.. 
That works also very good .. 
?
Once i have time i'll see if i can update the squid wiki. 
?
?
Greetz, 
?
Louis
?

Van: Rafael Akchurin [mailto:rafael.akchurin at diladele.com] 
Verzonden: woensdag 25 september 2019 17:27
Aan: L.P.H. van Belle; squid-users at lists.squid-cache.org
Onderwerp: RE: [squid-users] Kerberos nad keytab problem




Hello everyone,

?

Just my two cents too. Note you can map the *user* to the Kerberos SPN ? this lets you have your squid proxy live outside of the AD.

Just setup the dedicated user in the AD, map SPN to it and export the keytab to your squid.

?

See https://docs.diladele.com/administrator_guide_stable/active_directory/index.html

?

Downside ? the password for that designated user needs to be non expiring or you?d be regenerating keytabs everytime the password changes. Which is not difficult anyway too.

?

Best regards,

Rafael Akchurin

Diladele B.V.

?

?

?

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of L.P.H. van Belle
Sent: Wednesday, 25 September 2019 17:02
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Kerberos nad keytab problem



?

I also had problems with msktutil.. so i suggest you try this, see below.. 

Im using it for few years and it always works (for me offcourse).. 

?

It?should be pretty simple, but the site squid-cache (wiki) is in my opinion a bit outdated. 

And its for Amos to adapt it on the site.

?

Amos or Alex, please review below, you might want to add it. 

And add your parts to it, like running this without a correct spn. 

?

Its tested in use?and and working since squid 3.1 upto 4.8. 

Tested on debian Wheezy (7) upto Buster (10)

?

Below assumes the server your setting up, does have an A and PTR record. 

(note, which should be added at the?domain join of winbind, as of samba4.x )

?


This is my howto. 

A?Debian based,?with Kerberos Auth against an Samba Active Directory
Should be adaptable for any OS, should also work with MS Active Directory. 

But since i dont have any, im not testing it. 

?


?


# Install a minimal OS, at install only choose base + ssh server. 

# Setup these variable for a copy/past, might be handy, and then "it just works"? 


?


# Obligated to set.? # ADDOM; 


# This should match the netbios (NT4) domain name in caps, per example from a login: NTDOM\username 


ADDOM="NTDOM" 


?


# These should be fine, but if you have multiple ipnumbers and hostnames, you might want to adjust these. 


FQDN="$(hostname -f)"
HOSTN="$(hostname -s)"

# Requirements before you start installing the sofrware like: squid winbind krb5-user


?


# Login, sudo to root.


#?/etc/resolv.conf, set as followed. 
#search must.match.your.primarydnsdomain.tld
# nameserver ip_of_AD_DC

?

# Verify it: 

grep search /etc/resolv.conf

grep nameserver /etc/resolv.conf

?


# If ok, then run : 


apt update?


apt install squid winbind krb5-user -y


?


# Just hit enter on every question, the defaults are fine. (verified in Debian).


?


# And now verify /etc/krb5.conf
less /etc/krb5.conf

?


?


# It should look like this :? 
#[libdefaults]
#??????? default_realm = YOUR.Detected_REALM.TLD 

#


# The following krb5.conf variables are only for MIT Kerberos.
#?????? kdc_timesync = 1
#??????? ccache_type = 4
#??????? forwardable = true
#??????? proxiable = true

?


# ... and more.. 


#? >>? P.s.? i never touch krb5.conf, never needed, it "just works" << 

?

# Set REALM Variable now, default should be ok. dont touch it. 


REALM="$(grep default_realm /etc/krb5.conf |awk {' print $NF '}) "


# It's used for smb.conf and the auth part of squid. 


?


# then stop squid and samba and configure it.
systemctl stop squid winbind

?

# flush the log, so if you start it you?start with a clean log. ?

>?/var/log/squid/cache.log


?


#?Configure smb.conf and join the AD domain,? the minimal setting for smb.conf.
cp /etc/samba/smb.conf{,.original}

?


echo "# Auth-Only setup with winbind. ( no Shares )

?


??? workgroup = ${ADDOM}
??? security = ADS
??? realm = ${REALM}
??? netbios name = $(echo ${HOSTN^^})

?


??? ## make sure the below number never overlap system ranges, see /etc/adduser.conf 
??? ## map id's outside to domain to tdb files.
??? idmap config *: backend = tdb
??? idmap config *: range = 2000-9999

?


??? ## map ids from the domain and (*) the range may not overlap !
??? idmap config ${ADDOM} : backend = rid
??? idmap config ${ADDOM} : range = 10000-3999999

?


??? kerberos method = secrets and keytab
??? dedicated keytab file = /etc/krb5.keytab

?


??? # renew the kerberos ticket
??? winbind refresh tickets = yes
" > /etc/samba/smb.conf

?


# And verify it.
less /etc/samba/smb.conf

?


# Next step, join the AD domain. 

# Login/auth with kerberos. 
kinit Administrator

?

# and join the domain.

net ads join -k

?


# Creating the squid keytab file.

?

export KRB5_KTNAME=FILE:/etc/squid/squid-HTTP-${HOSTN}.keytab
net ads keytab ADD HTTP/${FQDN}



#Verify the keytab file : 
klist -ke /etc/squid/squid-HTTP-${HOSTN}.keytab


?


# destroy you authentication ticket for Administrator. 


kdestroy 


?


# set correct rights. 
chmod 640 /etc/squid/squid-HTTP-${HOSTN}.keytab
chown root:proxy /etc/squid/squid-HTTP-${HOSTN}.keytab
# Note, you might need to change the "proxy" group name here. 

?


# and setup you squid auth. 
echo "auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \\
??? --kerberos /usr/lib/squid/negotiate_kerberos_auth \\
????? -k etc/squid/squid-HTTP-${HOSTN}.keytab" \\
????? -s HTTP/"${FQDN}"@"${REALM}"? \\
??? --ntlm /usr/bin/ntlm_auth \\
????? --helper-protocol=gss-spnego --domain="${ADDOM}"

?


auth_param negotiate children 30 startup=5 idle=5
auth_param negotiate children 10
auth_param negotiate keep_alive on" > /etc/squid/conf.d/auth.conf

?


systemctl start winbind squid 


?


# Done 


# And check squid log how it started. 


cat /var/log/squid/cache.log


Now go configure the other parts you need of squid. 



And enjoy..? :-) 


?


?


Greetz, 

?


Louis

?

?

?

?


Van: squid-users [ MailScanner heeft een e-mail met mogelijk een poging tot fraude gevonden van "lists.squid-cache.org" mailto:squid-users-bounces at lists.squid-cache.org] Namens Tevfik Ceydeliler
Verzonden: woensdag 25 september 2019 13:59
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] Kerberos nad keytab problem

Hi, I try to use kerberos in my squid. Nut I get an error message :


?


############################33


msktutil --auto-update --verbose --computer-name suqidpnb1 --server dctoyo1.toyo.grp -k /etc/squid/PROXY.keytab ?
?-- init_password: Wiping the computer password structure
?-- generate_new_password: Generating a new, random password for the computer account
?-- generate_new_password: ?Characters read from /dev/urandom = 95
?-- create_fake_krb5_conf: Created a fake krb5.conf file: /tmp/.msktkrb5.conf-QCbGC5
?-- destroy_g_context: Destroying Kerberos Context
?-- initialize_g_context: Creating Kerberos Context
?-- finalize_exec: SAM Account Name is: suqidpnb1$
?-- try_machine_keytab_princ: Trying to authenticate for suqidpnb1$ from local keytab
?-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
?-- try_machine_keytab_princ: Authentication with keytab failed
?-- try_machine_keytab_princ: Trying to authenticate for SUQIDPNB1$ from local keytab
?-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
?-- try_machine_keytab_princ: Authentication with keytab failed
?-- try_machine_keytab_princ: Trying to authenticate for host/localhost from local keytab
?-- try_machine_keytab_princ: Error: krb5_get_init_creds_keytab failed (Key table entry not found)
?-- try_machine_keytab_princ: Authentication with keytab failed
?-- try_machine_password: Trying to authenticate for suqidpnb1$ with password
?-- create_default_machine_password: Default machine password for suqidpnb1$ is suqidpnb1
?-- try_machine_password: Error: krb5_get_init_creds_keytab failed (Client not found in Kerberos database)
?-- try_machine_password: Authentication with password failed
?-- try_user_creds: Checking if default ticket cache has tickets
?-- try_user_creds: Error: krb5_cc_get_principal failed (No credentials cache found)
?-- try_user_creds: User ticket cache was not valid
Error: could not find any credentials to authenticate with. Neither keytab,
default machine password, nor calling user's tickets worked. Try
"kinit"ing yourself some tickets with permission to create computer
objects, or pre-creating the computer object in AD and selecting
'reset account'.


?


#############################33


Can't find why this happen:


?


?


My AD is 2012R2 function level


I create keytab with this:


msktutil -c -b "OU=Servers,DC=toyo,DC=grp" -s HTTP/squidtoyopnb1.toyo.grp -k /etc/squid/PROXY.keytab --computer-name SQUIDPNB1 --upn HTTP/squidtoyopnb1.toyo.grp --server dctoyo1.toyo.grp --verbose --enctypes 28


?


Keytab file permission is:


-rw-r----- 1 root squid 933 Sep 25 13:37 PROXY.keytab


?


and keytab file (klist -k output):


?


? ?3 SQUIDPNB1$@TOYO.GRP
? ?3 SQUIDPNB1$@TOYO.GRP
? ?3 SQUIDPNB1$@TOYO.GRP
? ?3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 HTTP/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 host/squidtoyopnb1 at TOYO.GRP
? ?3 host/squidtoyopnb1 at TOYO.GRP
? ?3 host/squidtoyopnb1 at TOYO.GRP
? ?3 host/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 host/squidtoyopnb1.toyo.grp at TOYO.GRP
? ?3 host/squidtoyopnb1.toyo.grp at TOYO.GRP


?


krb5.conf:


[libdefaults]
default_realm = TOYO.GRP
? ? ? ? dns_lookup_kdc = no
? ? ? ? dns_lookup_realm = no
? ? ? ? ticket_lifetime = 24h
? ? ? ? default_keytab_name = /etc/squid/PROXY.keytab

? ? ; for Windows 2008 with AES
? ? ? ? ? default_tgs_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
? ? ? ? ? default_tkt_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5
? ? ? ? ? permitted_enctypes = aes256-cts-hmac-sha1-96 rc4-hmac des-cbc-crc des-cbc-md5

? ? [realms]
TOYO.GRP = {
? ? ? ? ? ? ? ? kdc = dctoyo1.toyo.grp
? ? ? ? ? ? ? ? kdc = DCTOYO2.toyo.grp
? ? ? ? ? ? ? ? admin_server = 10.65.12.254
? ? ? ? ? ? ? ? default_domain = toyo.grp
? ? ?}

? ? [domain_realm]
? ? ?toyo.grp = TOYO.GRP
? ? ?.toyo.grp = TOYO.GRP

? ? [logging]
? ? ? kdc = FILE:/var/log/kdc.log
? ? ? admin_server = FILE:/var/log/kadmin.log
? ? ? default = FILE:/var/log/krb5lib.log


?


?


?


?


-- 

Tevfik Ceydeliler



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190925/59ab4edf/attachment.htm>

From granaivo at olfeo.com  Wed Sep 25 17:27:43 2019
From: granaivo at olfeo.com (=?UTF-8?Q?Ga=c3=abl_Ranaivo?=)
Date: Wed, 25 Sep 2019 19:27:43 +0200
Subject: [squid-users] Squid sends 2 replies after peek/splice when using an
 user-agent http_access rule
Message-ID: <8fed39a6-01c1-d3ba-4c10-b6c39142556f@olfeo.com>

Hello,

Here is a minimal squid config that demonstrates this weird issue:

   http_port 3128 ssl-bump tls-cert=/tmp/cert.pem tls-key=/tmp/key.pem

   acl regua browser .*Firefox.*
   http_access allow regua
   http_access deny all

   acl step1 at_step SslBump1
   acl step2 at_step SslBump2
   acl youtube dstdomain .youtube.com

   ssl_bump peek step1
   ssl_bump splice step2 youtube
   ssl_bump bump step2 all

With this config and using Firefox to go to https://youtube.com/,
squid replies to the CONNECT with 2 different replies, causing
an SSL_ERROR_RX_RECORD_TOO_LONG error in the browser:

   HTTP/1.1 200 Connection established
   HTTP/1.1 403 Forbidden

After reading the wiki (https://wiki.squid-cache.org/Features/SslPeekAndSplice)
I suspect this has to do with the "fake" CONNECT request, which I guess fail
to pass the http_access rule. But why would squid send 2 replies? Is this
a bug?

Adding this line:

   http_access allow step2

seems to "fix" the problem, but I'm not sure if this is the right thing to do?

Squid version is 4.6 on debian recompiled with ssl support.

Regards,

--
Gael


From rousskov at measurement-factory.com  Wed Sep 25 20:21:11 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 25 Sep 2019 16:21:11 -0400
Subject: [squid-users] Warning: ACL is used in context without an HTTP
 response
In-Reply-To: <3728fa84-7123-70c7-5e8e-8ec1208668e6@gmail.com>
References: <B0C85BFE-E4F1-42C8-8357-DB322EB5DB82@gmail.com>
 <263921f2-b2cd-30ea-341b-8cb59702e11a@measurement-factory.com>
 <3728fa84-7123-70c7-5e8e-8ec1208668e6@gmail.com>
Message-ID: <3f8afcab-d6dd-dff1-f0ce-0c04b5c66096@measurement-factory.com>

On 9/25/19 11:07 AM, Alessandro Andrei wrote:

> acl AuthResponse407 http_status 407
> acl excludefromlog dstdomain .vortex-win.data.microsoft.com
> acl DoNotLog any-of AuthResponse407 excludefromlog
> access_log stdio:/var/log/squid/access.log logformat=squid !DoNotLog

> it's not clear to me how this check should be done in my
> configuration


Here are two untested sketches using the following basic ACL:

  acl Response407 http_status 407
  acl ToVortex dstdomain .vortex-win.data.microsoft.com
  acl HasResponse has response

* If you want to log non-vortex transactions without responses:

  acl Silenced all-of ToVortex
  acl Silenced all-of HasResponse Response407
  access_log ... !Silenced

* If you do _not_ want to log transactions without responses:

  acl Silenced all-of ToVortex
  acl Silenced all-of !HasResponse
  acl Silenced all-of Response407
  access_log ... !Silenced


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Sep 25 21:22:11 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 25 Sep 2019 17:22:11 -0400
Subject: [squid-users] Squid sends 2 replies after peek/splice when
 using an user-agent http_access rule
In-Reply-To: <8fed39a6-01c1-d3ba-4c10-b6c39142556f@olfeo.com>
References: <8fed39a6-01c1-d3ba-4c10-b6c39142556f@olfeo.com>
Message-ID: <3ddb59b5-fedd-e4de-78f2-0f8cba8fbdc4@measurement-factory.com>

On 9/25/19 1:27 PM, Ga?l Ranaivo wrote:

> Here is a minimal squid config that demonstrates this weird issue:
> 
> ? http_port 3128 ssl-bump tls-cert=/tmp/cert.pem tls-key=/tmp/key.pem
> 
> ? acl regua browser .*Firefox.*
> ? http_access allow regua
> ? http_access deny all
> 
> ? acl step1 at_step SslBump1
> ? acl step2 at_step SslBump2
> ? acl youtube dstdomain .youtube.com
> 
> ? ssl_bump peek step1
> ? ssl_bump splice step2 youtube
> ? ssl_bump bump step2 all
> 
> With this config and using Firefox to go to https://youtube.com/,
> squid replies to the CONNECT with 2 different replies, causing
> an SSL_ERROR_RX_RECORD_TOO_LONG error in the browser:
> 
> ? HTTP/1.1 200 Connection established
> ? HTTP/1.1 403 Forbidden

If the second response is sent in plain text, without waiting for an
encrypted request from the browser, then this is a Squid bug.

If the second response is sent encrypted, after receiving an encrypted
GET (or similar) request from the browser, then this is intended (for
now) behavior: SslBump bumps tunnels to report errors to users because
browsers refuse to support CONNECT errors properly.


> Adding this line:
> 
> ? http_access allow step2
> 
> seems to "fix" the problem, but I'm not sure if this is the right thing
> to do?

Using step2 like that feels dangerous and goes against (possibly
excessively conservative) at_step documentation.

If you want to allow all CONNECT requests, I would allow all CONNECT
requests explicitly instead of (ab)using step2 ACL to do the same.


HTH,

Alex.


> Squid version is 4.6 on debian recompiled with ssl support.


From chip_pop at hotmail.com  Wed Sep 25 22:52:49 2019
From: chip_pop at hotmail.com (joseph)
Date: Wed, 25 Sep 2019 17:52:49 -0500 (CDT)
Subject: [squid-users] real time priority
Message-ID: <1569451969283-0.post@n4.nabble.com>

what is the benefit from setting real-time priority to squid
or at least higher priority then the default ??
using nice -n 20 or a bit higher would it harm the sistem ??




-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From tevfik.ceydeliler at gmail.com  Thu Sep 26 10:34:49 2019
From: tevfik.ceydeliler at gmail.com (Tevfik Ceydeliler)
Date: Thu, 26 Sep 2019 13:34:49 +0300
Subject: [squid-users] Squid instalaltion for NTLM error
Message-ID: <CAJTWAhGk29vZaOc6oZSbrdGMBCncFbAraJv7hNU4THXrg5wMTA@mail.gmail.com>

Hi
In
https://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory#Kerberos
page,
there is directive to add crontab:

05  4  *   *   *     net rpc changetrustpw -d 1 | logger -t changetrustpw

when I run this command as:
net rpc changetrustpw -d 5
I get an error:
rocessing section "[global]"
Registered MSG_REQ_POOL_USAGE
Registered MSG_REQ_DMALLOC_MARK and LOG_CHANGED
lp_load_ex: refreshing parameters
Initialising global parameters
rlimit_max: increasing rlimit_max (1024) to minimum Windows limit (16384)
Processing section "[global]"
added interface ens192:1 ip=10.65.12.246 bcast=10.65.12.255
netmask=255.255.255.192
added interface ens192:0 ip=10.65.12.247 bcast=10.65.12.255
netmask=255.255.255.192
added interface ens192 ip=10.65.12.250 bcast=10.65.12.255
netmask=255.255.255.192
Connecting to 10.65.12.254 at port 445
got OID=1.3.6.1.4.1.311.2.2.30
got OID=1.2.840.48018.1.2.2
GENSEC backend 'gssapi_spnego' registered
GENSEC backend 'gssapi_krb5' registered
GENSEC backend 'gssapi_krb5_sasl' registered
GENSEC backend 'spnego' registered
GENSEC backend 'schannel' registered
GENSEC backend 'naclrpc_as_system' registered
GENSEC backend 'sasl-EXTERNAL' registered
GENSEC backend 'ntlmssp' registered
GENSEC backend 'ntlmssp_resume_ccache' registered
GENSEC backend 'http_basic' registered
GENSEC backend 'http_ntlm' registered
GENSEC backend 'http_negotiate' registered
Got challenge flags:
Got NTLMSSP neg_flags=0x62898215
NTLMSSP: Set final flags:
Got NTLMSSP neg_flags=0x62008a15
NTLMSSP Sign/Seal - Initialising with flags:
Got NTLMSSP neg_flags=0x62008a15
SPNEGO login failed: {Access Denied} A process has requested access to an
object but has not been granted those access rights.
Cannot connect to server (anonymously).  Error was NT_STATUS_ACCESS_DENIED
Connection failed: NT_STATUS_ACCESS_DENIED
failed to make ipc connection: NT_STATUS_ACCESS_DENIED
return code = -1

I search it but there is nothing appropriate solution about failed to make
ipc connection: NT_STATUS_ACCESS_DENIED

error
What is missing?
Regards.

-- 
Tevfik Ceydeliler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190926/67eabbba/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep 26 11:02:42 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Sep 2019 23:02:42 +1200
Subject: [squid-users] Protecting squid against ddos attacks
In-Reply-To: <CAOhxsyxkNViKYsz1PxLJmSU8Uu5=EsUY_EF8m_Gv1esCnbwnXg@mail.gmail.com>
References: <mailman.1.1569067202.2422.squid-users@lists.squid-cache.org>
 <CAOhxsyxkNViKYsz1PxLJmSU8Uu5=EsUY_EF8m_Gv1esCnbwnXg@mail.gmail.com>
Message-ID: <b0f7061b-6076-49f0-a9f7-a037c7f49b7b@treenet.co.nz>

On 23/09/19 1:59 am, Chirayu Patel wrote:
> Hi Amos,
> 
> Thanks a lot for giving some amazing insights..
> 
> So currently I am using Squid to achieve 2 things :
> a) Content Filtering - by checking the url against an external db and
> allow and block it accordingly. (using url_rewriter).?
> b) To popup captive portal
> 
> 1) Regarding the use of 4 ports
> Using iptables, I am redirecting the non authenticated users to
> ports?3132? ?&?3133. And then in squid i am checking the port on which I
> have received the request from. If its the above ports, then I run the
> captive portal flow
> 
> Once the user is authenticated , then I redirect their traffic to
> ports?3129 and 3131, and for those ports I run the content policy flow.
> 
> I am not sure if this is the right way of choosing the flow. Please
> advise if there is any other way to run two different flows with one squid.

TOS or NFMARK are designed for that type of traffic control. Squid
integrates with those system features.

The ports design works too though. It is just a matter of choice between
the three mechanisms, up to you.

> 
> 3) Right now I have configured firewall to only allow these 4 ports on
> the INPUT chain, so I am not expecting traffic to come from any other
> ports. In that case, is it okay if I have removed the default config and
> kept? "http_access allow all" ??
> The only issue is that the attacker now has 4 ports to run attacks on.
> 

I hope you restrict which clients are being NAT'ed into the proxy, *and*
prohibit clients connecting directly to the http(s)_port's (that mangle
rule in our wiki config example).
 That would restrict the 'from anywhere', issues somewhat.


The remaining problem is that clients can instruct the proxy to open
opaque TCP tunnels _to anywhere_. That is buried inside the HTTP syntax
and can occur any distance down the stream of HTTP messages clients
send. Also, some protocols (eg email spam delivery) can be passed
through an HTTP proxy as custom HTTP message headers.
 A firewall can only be used on the Squid<->server connections. Which
leaves DoS vulnerability from all the resource consumption by the proxy
handling those messages.


> 4)?> on_unsupported_protocol tunnel all?
> 
> I had added this when I faced issue with one of the Apps, Whatsapp which
> send the http traffic on https port. If I replace that with *respond*, I
> guess Whatsapp will become unusable.. right ?
> 

Yes. The key thing here is being aware of the issue, and handling it as
best you can at the firewall as traffic goes in/out of the proxy.

> 
> 6)?cache_mem 0 MB
> The default cache memory is quite huge (256 MB). That is approx the
> total usable memory I have on the AP. In that case, what do you think
> should be a good starting point in case I keep it to a non zero value ?

You can gain a fair amount of the benefits from just a few MB. You could
start with 1-2 MB and adjust from there.


> 
> 7) memory_pools off
> Again, I was too concerned about memory use and I got scared because of
> this comment?
> ------ ------ ------ ------ ------ ------ ------ ------ ------ ------
> ------ ------ ------ ------
> If set, Squid will keep pools of allocated (but unused) memory?available
> for future use. If memory is a premium on your?system and you believe
> your malloc library outperforms Squid??routines, disable this.
> ------------- ------ ------ ------ ------ ------ ------ ------ ------
> ------ ------ ------ ?
> But I believe some memory in exchange of performance is OK. So I am
> going to enable it.

You can use <http://www.squid-cache.org/Doc/config/memory_pools_limit/>
to restrict how much extra memory Squid holds onto in the pools. You
probably want to try that at 1 MB or 512 KB.


> 
> 9)?max_filedesc 5120.
> I had kept this number bigger because we were getting "out of file
> descriptors error"

This is still quite small. The default should be 64KB on most systems
these days. The 1024 is used as an absolute minimal value when system
capability detection fails.


> 
> 10) Above all, the best thing would be a way to differentiate between a
> high traffic flow of http(s) requests coming in from legitimate users vs
> a high traffic flow generated by an attacker with a simple script.?
> 
Without knowing in advance what that "simple script" is going to send
that is a very hard problem. It could literally be anything.

You can possibly make an external ACL helper that takes details about
the HTTP request and does some ML processing to decide whether to block
it. But ironically the delays inherent in that processing make the proxy
more at risk for DoS until it detects the attack pattern.


Amos


From shieldfire at gmail.com  Fri Sep 27 07:04:34 2019
From: shieldfire at gmail.com (Martin S)
Date: Fri, 27 Sep 2019 09:04:34 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
Message-ID: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>

Hi,

Returning user to squid. I've managed to set it up to allow access to
external addresses, but it is blocking all my 192.168.x.x web
services.
What am I missing here?


Regards,

Martin S


From squid3 at treenet.co.nz  Fri Sep 27 07:08:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Sep 2019 19:08:12 +1200
Subject: [squid-users] Squid instalaltion for NTLM error
In-Reply-To: <CAJTWAhGk29vZaOc6oZSbrdGMBCncFbAraJv7hNU4THXrg5wMTA@mail.gmail.com>
References: <CAJTWAhGk29vZaOc6oZSbrdGMBCncFbAraJv7hNU4THXrg5wMTA@mail.gmail.com>
Message-ID: <d8f2ebab-c7da-fab5-2ea9-e395a8207984@treenet.co.nz>

On 26/09/19 10:34 pm, Tevfik Ceydeliler wrote:
> Hi
> In?https://wiki.squid-cache.org/ConfigExamples/Authenticate/WindowsActiveDirectory#Kerberos?page,

That is for Kerberos, not NTLM.


> there is directive to add crontab:
> 
> 05 ?4 ?* ? * ? * ? ? net rpc changetrustpw -d 1 | logger -t changetrustpw
> 
> when I run this command as:
> net rpc changetrustpw -d 5
> I get an error:
...
> NTLMSSP Sign/Seal - Initialising with flags:
> Got NTLMSSP neg_flags=0x62008a15

> SPNEGO login failed: {Access Denied} A process has requested access to
> an object but has not been granted those access rights.

 ^ That is the error to look for. Everything that follows it are just
side effects.


Amos


From squid3 at treenet.co.nz  Fri Sep 27 07:09:25 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Sep 2019 19:09:25 +1200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
Message-ID: <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>

On 27/09/19 7:04 pm, Martin S wrote:
> Hi,
> 
> Returning user to squid. I've managed to set it up to allow access to
> external addresses, but it is blocking all my 192.168.x.x web
> services.
> What am I missing here?
> 

Your squid.conf contents.

Amos


From shieldfire at gmail.com  Fri Sep 27 07:16:51 2019
From: shieldfire at gmail.com (Martin S)
Date: Fri, 27 Sep 2019 09:16:51 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
Message-ID: <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>

All of it?
Or some particular passage.

//Martin S

Den fre 27 sep. 2019 kl 09:09 skrev Amos Jeffries <squid3 at treenet.co.nz>:
>
> On 27/09/19 7:04 pm, Martin S wrote:
> > Hi,
> >
> > Returning user to squid. I've managed to set it up to allow access to
> > external addresses, but it is blocking all my 192.168.x.x web
> > services.
> > What am I missing here?
> >
>
> Your squid.conf contents.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Regards,

Martin S


From uhlar at fantomas.sk  Fri Sep 27 07:18:22 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 27 Sep 2019 09:18:22 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
Message-ID: <20190927071822.GB29072@fantomas.sk>

On 27.09.19 09:16, Martin S wrote:
>All of it?
>Or some particular passage.

missing any part of it could hide the reasons of your problems.
using partebin is fine though.

>Den fre 27 sep. 2019 kl 09:09 skrev Amos Jeffries <squid3 at treenet.co.nz>:
>>
>> On 27/09/19 7:04 pm, Martin S wrote:
>> > Hi,
>> >
>> > Returning user to squid. I've managed to set it up to allow access to
>> > external addresses, but it is blocking all my 192.168.x.x web
>> > services.
>> > What am I missing here?
>> >
>>
>> Your squid.conf contents.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"One World. One Web. One Program." - Microsoft promotional advertisement
"Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler


From shieldfire at gmail.com  Fri Sep 27 07:29:41 2019
From: shieldfire at gmail.com (Martin S)
Date: Fri, 27 Sep 2019 09:29:41 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <20190927071822.GB29072@fantomas.sk>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
Message-ID: <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>

OK, here we go

https://pastebin.com/JS2QWHVG

//Martin S

Den fre 27 sep. 2019 kl 09:18 skrev Matus UHLAR - fantomas <uhlar at fantomas.sk>:
>
> On 27.09.19 09:16, Martin S wrote:
> >All of it?
> >Or some particular passage.
>
> missing any part of it could hide the reasons of your problems.
> using partebin is fine though.
>
> >Den fre 27 sep. 2019 kl 09:09 skrev Amos Jeffries <squid3 at treenet.co.nz>:
> >>
> >> On 27/09/19 7:04 pm, Martin S wrote:
> >> > Hi,
> >> >
> >> > Returning user to squid. I've managed to set it up to allow access to
> >> > external addresses, but it is blocking all my 192.168.x.x web
> >> > services.
> >> > What am I missing here?
> >> >
> >>
> >> Your squid.conf contents.
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> "One World. One Web. One Program." - Microsoft promotional advertisement
> "Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Regards,

Martin S


From uhlar at fantomas.sk  Fri Sep 27 07:34:24 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 27 Sep 2019 09:34:24 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
 <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
Message-ID: <20190927073423.GC29072@fantomas.sk>

>> >> On 27/09/19 7:04 pm, Martin S wrote:
>> >> > Returning user to squid. I've managed to set it up to allow access to
>> >> > external addresses, but it is blocking all my 192.168.x.x web
>> >> > services.
>> >> > What am I missing here?

>> On 27.09.19 09:16, Martin S wrote:
>> >All of it?
>> >Or some particular passage.

>> >Den fre 27 sep. 2019 kl 09:09 skrev Amos Jeffries <squid3 at treenet.co.nz>:
>> >> Your squid.conf contents.

>Den fre 27 sep. 2019 kl 09:18 skrev Matus UHLAR - fantomas <uhlar at fantomas.sk>:
>> missing any part of it could hide the reasons of your problems.
>> using partebin is fine though.

On 27.09.19 09:29, Martin S wrote:
>https://pastebin.com/JS2QWHVG

you uncommented acl for localnet, but didn't give localnet access to the
proxy:

#http_access allow localnet
http_access allow localhost
 
# And finally deny all other access to this proxy
http_access deny all

you must uncomment first line above too,


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese.


From shieldfire at gmail.com  Fri Sep 27 07:56:44 2019
From: shieldfire at gmail.com (Martin S)
Date: Fri, 27 Sep 2019 09:56:44 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <20190927073423.GC29072@fantomas.sk>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
 <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
 <20190927073423.GC29072@fantomas.sk>
Message-ID: <CAHXoDSDAZ=K7JKZ7v0P_VmPMtGmNToPZOu7beEvu15UYdCNiGA@mail.gmail.com>

OK, did that

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

I now noticed that it denies access through https. There must be
somewhere this is denied as well.

//Martin S

Den fre 27 sep. 2019 kl 09:34 skrev Matus UHLAR - fantomas <uhlar at fantomas.sk>:
>
> >> >> On 27/09/19 7:04 pm, Martin S wrote:
> >> >> > Returning user to squid. I've managed to set it up to allow access to
> >> >> > external addresses, but it is blocking all my 192.168.x.x web
> >> >> > services.
> >> >> > What am I missing here?
>
> >> On 27.09.19 09:16, Martin S wrote:
> >> >All of it?
> >> >Or some particular passage.
>
> >> >Den fre 27 sep. 2019 kl 09:09 skrev Amos Jeffries <squid3 at treenet.co.nz>:
> >> >> Your squid.conf contents.
>
> >Den fre 27 sep. 2019 kl 09:18 skrev Matus UHLAR - fantomas <uhlar at fantomas.sk>:
> >> missing any part of it could hide the reasons of your problems.
> >> using partebin is fine though.
>
> On 27.09.19 09:29, Martin S wrote:
> >https://pastebin.com/JS2QWHVG
>
> you uncommented acl for localnet, but didn't give localnet access to the
> proxy:
>
> #http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> you must uncomment first line above too,
>
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> The early bird may get the worm, but the second mouse gets the cheese.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



-- 
Regards,

Martin S


From squid3 at treenet.co.nz  Fri Sep 27 08:26:30 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Sep 2019 20:26:30 +1200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <CAHXoDSDAZ=K7JKZ7v0P_VmPMtGmNToPZOu7beEvu15UYdCNiGA@mail.gmail.com>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
 <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
 <20190927073423.GC29072@fantomas.sk>
 <CAHXoDSDAZ=K7JKZ7v0P_VmPMtGmNToPZOu7beEvu15UYdCNiGA@mail.gmail.com>
Message-ID: <81476490-2d1c-1568-76a4-bf8fec2d2a05@treenet.co.nz>

On 27/09/19 7:56 pm, Martin S wrote:
> OK, did that
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> I now noticed that it denies access through https.

What are you seeing that makes yo think that?

The config shown with the above change made allows localnet clients to
request https:// URLs through the proxy as well as http:// and the other
protocols.

Amos


From shieldfire at gmail.com  Fri Sep 27 08:54:26 2019
From: shieldfire at gmail.com (Martin S)
Date: Fri, 27 Sep 2019 10:54:26 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <81476490-2d1c-1568-76a4-bf8fec2d2a05@treenet.co.nz>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
 <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
 <20190927073423.GC29072@fantomas.sk>
 <CAHXoDSDAZ=K7JKZ7v0P_VmPMtGmNToPZOu7beEvu15UYdCNiGA@mail.gmail.com>
 <81476490-2d1c-1568-76a4-bf8fec2d2a05@treenet.co.nz>
Message-ID: <CAHXoDSAvh5AULrA4o0_NH2nx9KBe_u2vWMDAqcpimDJ-0hOJuQ@mail.gmail.com>

Going to 1http://92.168.0.6:10000 now lets me access the page. However,
that page requires a https login. So, there is a link to
https://192.168.0.6:10000

Clicking the link to go to the https site produces "The proxyserver denies
the connection".

Changing the internet connection to *not* use Squid, then I have no
problems accessing https://192.168.0.6:10000.

//Martin S


Den fre 27 sep. 2019 kl 10:26 skrev Amos Jeffries <squid3 at treenet.co.nz>:

> On 27/09/19 7:56 pm, Martin S wrote:
> > OK, did that
> >
> > # Example rule allowing access from your local networks.
> > # Adapt localnet in the ACL section to list your (internal) IP networks
> > # from where browsing should be allowed
> > http_access allow localnet
> > http_access allow localhost
> >
> > # And finally deny all other access to this proxy
> > http_access deny all
> >
> > I now noticed that it denies access through https.
>
> What are you seeing that makes yo think that?
>
> The config shown with the above change made allows localnet clients to
> request https:// URLs through the proxy as well as http:// and the other
> protocols.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Martin S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190927/abda1eee/attachment.htm>

From uhlar at fantomas.sk  Fri Sep 27 10:03:28 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 27 Sep 2019 12:03:28 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <CAHXoDSAvh5AULrA4o0_NH2nx9KBe_u2vWMDAqcpimDJ-0hOJuQ@mail.gmail.com>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
 <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
 <20190927073423.GC29072@fantomas.sk>
 <CAHXoDSDAZ=K7JKZ7v0P_VmPMtGmNToPZOu7beEvu15UYdCNiGA@mail.gmail.com>
 <81476490-2d1c-1568-76a4-bf8fec2d2a05@treenet.co.nz>
 <CAHXoDSAvh5AULrA4o0_NH2nx9KBe_u2vWMDAqcpimDJ-0hOJuQ@mail.gmail.com>
Message-ID: <20190927100328.GA736@fantomas.sk>

>> On 27/09/19 7:56 pm, Martin S wrote:
>> > OK, did that
>> >
>> > # Example rule allowing access from your local networks.
>> > # Adapt localnet in the ACL section to list your (internal) IP networks
>> > # from where browsing should be allowed
>> > http_access allow localnet
>> > http_access allow localhost
>> >
>> > # And finally deny all other access to this proxy
>> > http_access deny all
>> >
>> > I now noticed that it denies access through https.

>Den fre 27 sep. 2019 kl 10:26 skrev Amos Jeffries <squid3 at treenet.co.nz>:
>> What are you seeing that makes yo think that?
>>
>> The config shown with the above change made allows localnet clients to
>> request https:// URLs through the proxy as well as http:// and the other
>> protocols.

On 27.09.19 10:54, Martin S wrote:
>Going to 1http://92.168.0.6:10000 now lets me access the page. However,
>that page requires a https login. So, there is a link to
>https://192.168.0.6:10000
>
>Clicking the link to go to the https site produces "The proxyserver denies
>the connection".
>
>Changing the internet connection to *not* use Squid, then I have no
>problems accessing https://192.168.0.6:10000.

10000 (i assume webmin) is non-standard port from https point of view.
within squid it must be allowed.

acl SSL_ports port 10000

However, if you can connect directly, you should do so (configure web
browser to use direct connections to 192.168.*).

It's possible that CONNECT ports over 1024 will be allowed but this must be
discused and agreed on (unless such discussion was already done in the near
past and the agreement was not made).

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Spam is for losers who can't get business any other way.


From shieldfire at gmail.com  Fri Sep 27 10:52:16 2019
From: shieldfire at gmail.com (Martin S)
Date: Fri, 27 Sep 2019 12:52:16 +0200
Subject: [squid-users] Squid blocking 192.168.x.x
In-Reply-To: <20190927100328.GA736@fantomas.sk>
References: <CAHXoDSDiUnhoZnRXT6GdRt1dx0=Qpk4dMH80z8c-w54n_zC5jA@mail.gmail.com>
 <41db57db-eaed-9a75-5e5a-ac84b0ffd779@treenet.co.nz>
 <CAHXoDSDdRJWNnx4witA-YZPfWNJ-T25CndSvwarx6u9xBpyMmQ@mail.gmail.com>
 <20190927071822.GB29072@fantomas.sk>
 <CAHXoDSAgQ0Ngfh4Yni5seykhuV64odzGK8MJw46MJk_ugcvu3w@mail.gmail.com>
 <20190927073423.GC29072@fantomas.sk>
 <CAHXoDSDAZ=K7JKZ7v0P_VmPMtGmNToPZOu7beEvu15UYdCNiGA@mail.gmail.com>
 <81476490-2d1c-1568-76a4-bf8fec2d2a05@treenet.co.nz>
 <CAHXoDSAvh5AULrA4o0_NH2nx9KBe_u2vWMDAqcpimDJ-0hOJuQ@mail.gmail.com>
 <20190927100328.GA736@fantomas.sk>
Message-ID: <CAHXoDSBxpvsg0kq4xjLMPcgpKKvNTJMTEJp4AUSuJMyfcn-5FQ@mail.gmail.com>

Ah!

That was it. Somehow I was confused and thought that Safe_ports and
SSL_ports were kindof similar. I obviously need to be less confused and
"think" less (rather "know" more).

Thank you for you time and help!

//Martin S

Den fre 27 sep. 2019 kl 12:03 skrev Matus UHLAR - fantomas <
uhlar at fantomas.sk>:

> >> On 27/09/19 7:56 pm, Martin S wrote:
> >> > OK, did that
> >> >
> >> > # Example rule allowing access from your local networks.
> >> > # Adapt localnet in the ACL section to list your (internal) IP
> networks
> >> > # from where browsing should be allowed
> >> > http_access allow localnet
> >> > http_access allow localhost
> >> >
> >> > # And finally deny all other access to this proxy
> >> > http_access deny all
> >> >
> >> > I now noticed that it denies access through https.
>
> >Den fre 27 sep. 2019 kl 10:26 skrev Amos Jeffries <squid3 at treenet.co.nz>:
> >> What are you seeing that makes yo think that?
> >>
> >> The config shown with the above change made allows localnet clients to
> >> request https:// URLs through the proxy as well as http:// and the
> other
> >> protocols.
>
> On 27.09.19 10:54, Martin S wrote:
> >Going to 1http://92.168.0.6:10000 now lets me access the page. However,
> >that page requires a https login. So, there is a link to
> >https://192.168.0.6:10000
> >
> >Clicking the link to go to the https site produces "The proxyserver denies
> >the connection".
> >
> >Changing the internet connection to *not* use Squid, then I have no
> >problems accessing https://192.168.0.6:10000.
>
> 10000 (i assume webmin) is non-standard port from https point of view.
> within squid it must be allowed.
>
> acl SSL_ports port 10000
>
> However, if you can connect directly, you should do so (configure web
> browser to use direct connections to 192.168.*).
>
> It's possible that CONNECT ports over 1024 will be allowed but this must be
> discused and agreed on (unless such discussion was already done in the near
> past and the agreement was not made).
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Spam is for losers who can't get business any other way.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Martin S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190927/de105bf1/attachment.htm>

From gaia45500 at gmail.com  Sat Sep 28 16:59:16 2019
From: gaia45500 at gmail.com (Emmanuel BILLOT)
Date: Sat, 28 Sep 2019 18:59:16 +0200
Subject: [squid-users] Squid Benchmark
Message-ID: <CACgYPG3tWfMcj5jxghnvh2Jfm0wh2+KP3gY=+9qRa4VRPf2FQg@mail.gmail.com>

Hi,

This is probably a bring recurring subject, but users in our company
frequently say that "Internet navigating is slow..."
While "slow" is such a relative feeling, depending on computer health, user
behaviour or network state, we have to be aware for bottlenecks.

We are using Squid servers behind a LB, providing Internet or Intranet
access for up to 3000 users.

NAT is for sure a faster way to access web, but considering security and
filtering, we have to use Squid and we want to be sure that Squid is not
the main cause for slow-down.

Is there a strong way, and moreover an efficient tool to check that using
Squid is not a bottleneck compared to a direct to Internet access (NAT) ?

Regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190928/a246b3bb/attachment.htm>

From rousskov at measurement-factory.com  Sat Sep 28 18:00:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 28 Sep 2019 14:00:09 -0400
Subject: [squid-users] Squid Benchmark
In-Reply-To: <CACgYPG3tWfMcj5jxghnvh2Jfm0wh2+KP3gY=+9qRa4VRPf2FQg@mail.gmail.com>
References: <CACgYPG3tWfMcj5jxghnvh2Jfm0wh2+KP3gY=+9qRa4VRPf2FQg@mail.gmail.com>
Message-ID: <4d9aa8c4-7f75-50d4-e19f-e2af4d437bc8@measurement-factory.com>

On 9/28/19 12:59 PM, Emmanuel BILLOT wrote:

> This is probably a bring recurring?subject, but users in our company
> frequently say that "Internet navigating is slow..."
> While "slow" is such a relative feeling, depending on computer health,
> user behaviour or network state, we have to be aware for bottlenecks.
> 
> We are using Squid servers behind a LB, providing Internet or Intranet
> access for up to 3000 users.
> 
> NAT is for sure a faster way to access web, but considering security and
> filtering, we have to use Squid and we want to be sure that Squid is not
> the main cause for slow-down.
> 
> Is there a strong way, and moreover an efficient tool to check that
> using Squid is not a bottleneck compared to a direct to Internet access
> (NAT) ?

The answer to your question does not require a tool: Squid does slow
things down in most modern "TLS everywhere" environments, possibly
including yours. And if you do not decrypt traffic, then the answer is
likely even simpler: Squid slows down your traffic.

However, you are asking the wrong question. Since you _have to_ use
Squid (as you have said), the questions to ask are:

1. Does Squid slow things down right now more than it usually does? You
can answer that question by collecting and plotting various performance
stats from access logs and cache manager. You will see (or, better,
automatically detect!) when things suddenly go south.

2. Can we make Squid faster? The answer to that question is usually
"yes", but it requires significant investment in bottleneck analysis
and, in some cases, development/optimization.

Alex.


From felipeapolanco at gmail.com  Mon Sep 30 16:23:51 2019
From: felipeapolanco at gmail.com (Felipe Arturo Polanco)
Date: Mon, 30 Sep 2019 12:23:51 -0400
Subject: [squid-users] Does Squid support ICAP early responses?
Message-ID: <CADcj3=6fh_d3e365RqGHa3D0KJXJrBM7p7Mi=+hc-45cHWh5YA@mail.gmail.com>

Hi,

Does anyone know if Squid support early ICAP responses from the ICAP server
in the middle of a body transfer?

If so, can anybody provide an example of early responses? I have Squid
sending me a TCP Reset whenever I send out an early response in the middle
of a transfer.

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190930/b217154b/attachment.htm>

