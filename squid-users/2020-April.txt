From raghu.vdm at gmail.com  Wed Apr  1 13:34:26 2020
From: raghu.vdm at gmail.com (Raghu nathan)
Date: Wed, 1 Apr 2020 19:04:26 +0530
Subject: [squid-users] Need support on HTTPs port forward
Message-ID: <CAJea6dNw+i0H-eX07Zm3YzKJky76mF4sXjUv8s4RJjsSbozBWw@mail.gmail.com>

*Hello Team,*


I?m new to Squid and need some support on advance topic, request you to
advice on this requirement.


I?m trying to achieve below scenario for HTTPs traffic.


Client???>VPN???>SQUID?????>Internet


*Condition:*


   - VPN and SQUID on same machine
   - I didn?t want end-user (client) to know about proxy, so no
   configuration allowed on client end.
   - I?m ok to restrict/allow traffic based on SNI, so no need to decrypt
   the SSL packet


I was following this article to achieve this

https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit


I have success fully rerouted the traffic on CentOS7 machine towards SQUID

HTTP traffic is working as expected, I?m able to apply policy based on
custom build Helper (Python)

SQUID not able to decode the HTTPs URL even I used ssl-bump with intercept
after going over resources on internet.

Notably I got  below errors,


X-Squid-Error: ERR_INVALID_REQ 0

ERROR: No forward-proxy ports configured.



*Below is the Firewalld config*


  services: http https ipsec

  ports: 500/udp 4500/udp 3127/tcp 3126/tcp

  protocols:


>   masquerade: yes

  forward-ports: port=80:proto=tcp:toport=3127:toaddr=10.128.0.4

port=443:proto=tcp:toport=3126:toaddr=10.128.0.4


*Below is the Squid.conf*


debug_options ALL,1 11,3 20,3

#

# Recommended minimum configuration:

#


> # Example rule allowing access from your local networks.

# Adapt to list your (internal) IP networks from where browsing

# should be allowed

acl localnet src 10.0.0.0/8 # RFC1918 possible internal network

acl localnet src 172.16.0.0/12 # RFC1918 possible internal network

acl localnet src 192.168.0.0/16 # RFC1918 possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines


> acl SSL_ports port 443

acl Safe_ports port 80 # http

acl Safe_ports port 21 # ftp

acl Safe_ports port 443 # https

acl Safe_ports port 70 # gopher

acl Safe_ports port 210 # wais

acl Safe_ports port 1025-65535 # unregistered ports

acl Safe_ports port 280 # http-mgmt

acl Safe_ports port 488 # gss-http

acl Safe_ports port 591 # filemaker

acl Safe_ports port 777 # multiling http

acl CONNECT method CONNECT


> #

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

#http_access deny !Safe_ports


> # Deny CONNECT to other than secure SSL ports

#http_access deny CONNECT !SSL_ports


> # Only allow cachemgr access from localhost

http_access allow localhost manager

http_access deny manager


> # do not cache anything

cache deny all


> # We strongly recommend the following be uncommented to protect innocent

# web applications running on the proxy server who think the only

# one who can access services on "localhost" is a local user

#http_access deny to_localhost


> #

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#


> # Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost


> # And finally deny all other access to this proxy

http_access allow all


> # Squid normally listens to port 3128

http_port 3127 intercept

#http_port 3126

http_port 3126 ssl-bump intercept \

  cert=/etc/squid/cert/myCA.pem \

  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB


>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/spool/squid/ssl_db -M 4MB


> # Uncomment and adjust the following to add a disk cache directory.

#cache_dir ufs /var/spool/squid 100 16 256


> # Leave coredumps in the first cache dir

coredump_dir /var/spool/squid


> #

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp: 1440 20% 10080

refresh_pattern ^gopher: 1440 0% 1440

refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

refresh_pattern . 0 20% 4320


>
> sslproxy_cert_error allow all


> #ssl bump

#ssl_bump peek all

ssl_bump stare all

ssl_bump bump all



*Error log:*

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(521) unlock:
> store_client::copy unlocking key 67A641E2D51E4D78DC31FD1B1792BE59
> e:=sXINV/0x5560eb4c70a0*3

2020/04/01 12:55:13.459 kid1| 20,2| store.cc(949) checkCachable:
> StoreEntry::checkCachable: NO: not cachable

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(483) lock: storeUnregister
> locked key 67A641E2D51E4D78DC31FD1B1792BE59 e:=sXINV/0x5560eb4c70a0*3

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(521) unlock: storeUnregister
> unlocking key 67A641E2D51E4D78DC31FD1B1792BE59 e:=sXINV/0x5560eb4c70a0*3

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(521) unlock:
> clientReplyContext::removeStoreReference unlocking key
> 67A641E2D51E4D78DC31FD1B1792BE59 e:=sXINV/0x5560eb4c70a0*2

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(521) unlock:
> ClientHttpRequest::loggingEntry unlocking key
> 67A641E2D51E4D78DC31FD1B1792BE59 e:=sXINV/0x5560eb4c70a0*1

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(1239) release: releasing
> e:=sXINV/0x5560eb4c70a0*0 67A641E2D51E4D78DC31FD1B1792BE59

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(402) destroyMemObject:
> destroyMemObject 0x5560eb4d0f40

2020/04/01 12:55:13.459 kid1| 20,3| MemObject.cc(110) ~MemObject: del
> MemObject 0x5560eb4d0f40

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(420) destroyStoreEntry:
> destroyStoreEntry: destroying 0x5560eb4c70a8

2020/04/01 12:55:13.459 kid1| 20,3| store.cc(402) destroyMemObject:
> destroyMemObject 0

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(774) storeCreatePureEntry:
> storeCreateEntry: 'error:invalid-request'

2020/04/01 12:55:14.008 kid1| 20,3| MemObject.cc(97) MemObject: new
> MemObject 0x5560eb4d0f40

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(499) setReleaseFlag:
> StoreEntry::setReleaseFlag: '[null_store_key]'

2020/04/01 12:55:14.008 kid1| 20,3| store_key_md5.cc(89) storeKeyPrivate:
> storeKeyPrivate: NONE error:invalid-request

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(447) hashInsert:
> StoreEntry::hashInsert: Inserting Entry e:=XI/0x5560eb4c70a0*0 key
> '0255EA5E5890358FC3F4287C4CA0E49F'

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(483) lock: storeCreateEntry
> locked key 0255EA5E5890358FC3F4287C4CA0E49F e:=XIV/0x5560eb4c70a0*1

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(483) lock:
> StoreEntry::storeErrorResponse locked key 0255EA5E5890358FC3F4287C4CA0E49F
> e:=XIV/0x5560eb4c70a0*2

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(1862) replaceHttpReply:
> StoreEntry::replaceHttpReply: error:invalid-request

2020/04/01 12:55:14.008 kid1| 20,2| store.cc(949) checkCachable:
> StoreEntry::checkCachable: NO: not cachable

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(1048) complete: storeComplete:
> '0255EA5E5890358FC3F4287C4CA0E49F'

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(1337) validLength:
> storeEntryValidLength: Checking '0255EA5E5890358FC3F4287C4CA0E49F'

2020/04/01 12:55:14.008 kid1| 20,2| store.cc(949) checkCachable:
> StoreEntry::checkCachable: NO: not cachable

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(521) unlock:
> StoreEntry::storeErrorResponse unlocking key
> 0255EA5E5890358FC3F4287C4CA0E49F e:=sXINV/0x5560eb4c70a0*2

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(483) lock: store_client::copy
> locked key 0255EA5E5890358FC3F4287C4CA0E49F e:=sXINV/0x5560eb4c70a0*2

2020/04/01 12:55:14.008 kid1| ERROR: No forward-proxy ports configured.

2020/04/01 12:55:14.008 kid1| 20,3| store.cc(483) lock:
> ClientHttpRequest::loggingEntry locked key 0255EA5E5890358FC3F4287C4CA0E49F
> e:=sXINV/0x5560eb4c70a0*3

2020/04/01 12:55:14.008 kid1| 11,2| client_side.cc(1393)
> sendStartOfMessage: HTTP Client local=172.217.4.46:443 remote=
> 10.15.1.100:53362 FD 10 flags=33

2020/04/01 12:55:14.008 kid1| 11,2| client_side.cc(1394)
> sendStartOfMessage: HTTP Client REPLY:

---------

HTTP/1.1 400 Bad Request

Server: squid/3.5.20

Mime-Version: 1.0

Date: Wed, 01 Apr 2020 12:55:14 GMT

Content-Type: text/html;charset=utf-8

Content-Length: 4033

X-Squid-Error: ERR_INVALID_REQ 0

Vary: Accept-Language

Content-Language: en

X-Cache: MISS from proto-vpn

X-Cache-Lookup: NONE from proto-vpn:0

Via: 1.1 proto-vpn (squid/3.5.20)

Connection: close
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200401/5f136665/attachment.htm>

From lameventanas at gmail.com  Wed Apr  1 15:21:10 2020
From: lameventanas at gmail.com (lameventanas at gmail.com)
Date: Thu, 2 Apr 2020 00:21:10 +0900
Subject: [squid-users] Multiple access logs
Message-ID: <0c1b32a3-2afa-b077-756a-9af82d473925@gmail.com>

Is it possible to have multiple access_log entries in Squid 3.5?

For example, to write to a file and a daemon at the same time?

In my testing a second entry overrides the first one, but I hope I'm
missing something here.

Thanks.

Alan



From rousskov at measurement-factory.com  Wed Apr  1 18:29:19 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 1 Apr 2020 14:29:19 -0400
Subject: [squid-users] Multiple access logs
In-Reply-To: <0c1b32a3-2afa-b077-756a-9af82d473925@gmail.com>
References: <0c1b32a3-2afa-b077-756a-9af82d473925@gmail.com>
Message-ID: <0122fef4-26a8-f721-462f-00aba30a555f@measurement-factory.com>

On 4/1/20 11:21 AM, lameventanas at gmail.com wrote:
> Is it possible to have multiple access_log entries in Squid 3.5?

I have not checked Squid v3, but I suspect the answer has not changed
much since that version.


> For example, to write to a file and a daemon at the same time?

Multiple access logs are supported, but there are caveats.

A matching "access_log none" directive terminates logging -- directives
below it are not checked. Some believe that this is desirable and
documented behavior, similar to a "deny" rule match, I guess.

Also, the related code is buggy AFAICT. To make sure that all configured
access logs are used, try this _untested_ workaround: Do not use more
than one access_log directive without ACLs. If you use one such
directive, then make it the _last_ access_log directive.

For developer-level details see accessLogLogTo().

HTH,

Alex.


From gengchao62 at gmail.com  Thu Apr  2 04:42:31 2020
From: gengchao62 at gmail.com (saiyan_gc)
Date: Wed, 1 Apr 2020 23:42:31 -0500 (CDT)
Subject: [squid-users] Error negotiating SSL connection on FD 16
In-Reply-To: <1585522706899-0.post@n4.nabble.com>
References: <1585522706899-0.post@n4.nabble.com>
Message-ID: <1585802551220-0.post@n4.nabble.com>

Hi, thank you for reply me. Really appreciated!

I modified the squid conf file to:

http_port 2128 ssl-bump cert=/etc/squid/ssl_cert/example.com.cert \
    key=/etc/squid/ssl_cert/example.com.private \
    generate-host-certificates=on \
    dynamic_cert_mem_cache_size=4MB
https_port 3130 cert=/etc/squid/ssl_cert/example.com.cert \
    key=/etc/squid/ssl_cert/example.com.private  
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwords
auth_param basic children 5 startup=0 idle=1
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
acl ncsa_users proxy_auth REQUIRED
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
http_access deny !ncsa_users
http_access allow ncsa_users

And it's working for http_port. I put the cert into
/etc/pki/trust-ca/source/anchor, and run a update-ca-trust command. And both
aws cli and curl command work now. I am still not sure why https_port desn't
work.

The previous setting work with curl but not aws cli, not sure why it failure
during tls handshake.

Thank you





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From lameventanas at gmail.com  Thu Apr  2 09:06:45 2020
From: lameventanas at gmail.com (lameventanas at gmail.com)
Date: Thu, 2 Apr 2020 18:06:45 +0900
Subject: [squid-users] Multiple access logs
In-Reply-To: <0122fef4-26a8-f721-462f-00aba30a555f@measurement-factory.com>
References: <0c1b32a3-2afa-b077-756a-9af82d473925@gmail.com>
 <0122fef4-26a8-f721-462f-00aba30a555f@measurement-factory.com>
Message-ID: <4ab7c23d-dc98-d47b-1679-e2d1be84d084@gmail.com>

On 02/04/2020 03:29, Alex Rousskov wrote:
> On 4/1/20 11:21 AM, lameventanas at gmail.com wrote:
>> Is it possible to have multiple access_log entries in Squid 3.5?
> 
> I have not checked Squid v3, but I suspect the answer has not changed
> much since that version.
> 
> 
>> For example, to write to a file and a daemon at the same time?
> 
> Multiple access logs are supported, but there are caveats.
> 
> A matching "access_log none" directive terminates logging -- directives
> below it are not checked. Some believe that this is desirable and
> documented behavior, similar to a "deny" rule match, I guess.
> 
> Also, the related code is buggy AFAICT. To make sure that all configured
> access logs are used, try this _untested_ workaround: Do not use more
> than one access_log directive without ACLs. If you use one such
> directive, then make it the _last_ access_log directive.
> 
> For developer-level details see accessLogLogTo().

Thanks for your answer.

Here is what I've found after testing:

1. I thought the UDP logging module didn't work, but I didn't realize
there is a buffer of 1400 bytes (that can't be configured).
2. The TCP logging module works fine.
3. We can't use 2 different log_daemon programs, which is really
inconvenient.  But the same program can be used with different
parameters for different instances (eg: if different formats are
needed), so it's possible to use a wrapper to overcome this.

Regards,

Alan



From rousskov at measurement-factory.com  Thu Apr  2 13:28:23 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Apr 2020 09:28:23 -0400
Subject: [squid-users] Multiple access logs
In-Reply-To: <4ab7c23d-dc98-d47b-1679-e2d1be84d084@gmail.com>
References: <0c1b32a3-2afa-b077-756a-9af82d473925@gmail.com>
 <0122fef4-26a8-f721-462f-00aba30a555f@measurement-factory.com>
 <4ab7c23d-dc98-d47b-1679-e2d1be84d084@gmail.com>
Message-ID: <162f9345-c4cd-c530-b488-89d30f75f778@measurement-factory.com>

On 4/2/20 5:06 AM, lameventanas at gmail.com wrote:

> 1. I thought the UDP logging module didn't work, but I didn't realize
> there is a buffer of 1400 bytes (that can't be configured).

It can be configured in Squid v4+. Look for access_log buffer-size=...

Alex.


From zrm at trustiosity.com  Fri Apr  3 20:26:13 2020
From: zrm at trustiosity.com (zrm)
Date: Fri, 3 Apr 2020 16:26:13 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
	deb.debian.org
Message-ID: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>

Greetings! Today I bring you a Squid cache mystery.

I configured a simple transparent proxy to cache package downloads for 
Debian, using Squid on Debian 10. When apt clients download packages 
from deb.debian.org, Squid says TCP_MISS, downloads the package, and 
then doesn't cache it. The mystery is, why not.

It caches other requests by other applications. It caches other requests 
by apt -- packages from security.debian.org get cached. It caches 
requests for deb.debian.org when the client is not apt -- using wget or 
curl on the same URL causes it to be cached, as does pasting the HTTP 
request into netcat. The same HTTP request that apt sends. I checked the 
packets with WireShark and the TCP payload for the request is byte for 
byte identical, but when apt does it, it isn't cached. I'm not sure how 
it's even distinguishing the requests in order to behave differently.

These are the changes from the default squid.conf packaged with Debian 10:
http_access allow localnet
http_port 3130
http_port 3128 intercept
maximum_object_size_in_memory 4 MB

Here are repeated requests using "apt remove -y vim;apt install vim", 
access.log:

1585891724.223    560 192.168.111.55 TCP_MISS/200 1281195 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.64.204 application/x-debian-package
1585891726.697    277 192.168.111.55 TCP_MISS/200 1281195 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.64.204 application/x-debian-package

store.log:

1585891535.154 RELEASE -1 FFFFFFFF 04000000000000001712000001000000  200 
1585891535 1560620795 1588483535 application/x-debian-package 
1280476/1280476 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb
1585891724.223 RELEASE -1 FFFFFFFF 05000000000000001712000001000000  200 
1585891723 1560620795 1588483723 application/x-debian-package 
1280476/1280476 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb

But here are repeated requests for the same url using wget, access.log:

1585891729.700    433 192.168.111.55 TCP_MISS/200 1281195 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.66.133 application/x-debian-package
1585891731.089     70 192.168.111.55 TCP_REFRESH_UNMODIFIED/200 1281232 
GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.66.133 application/x-debian-package

store.log:

1585891726.697 RELEASE -1 FFFFFFFF 06000000000000001712000001000000  200 
1585891726 1560620795 1588483726 application/x-debian-package 
1280476/1280476 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb
1585891731.047 RELEASE -1 FFFFFFFF 08000000000000001712000001000000  304 
1585891731        -1 1588483731 unknown -1/-293 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb

In the first case we get TCP_MISS every time because it isn't caching 
the data, in the second case it's only the first time and after that we 
get TCP_REFRESH_UNMODIFIED. But how and why is this happening?


From Antony.Stone at squid.open.source.it  Fri Apr  3 20:32:16 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 3 Apr 2020 22:32:16 +0200
Subject: [squid-users] Squid transparent not caching apt requests from
	deb.debian.org
In-Reply-To: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
Message-ID: <202004032232.16885.Antony.Stone@squid.open.source.it>

On Friday 03 April 2020 at 22:26:13, zrm wrote:

> Greetings! Today I bring you a Squid cache mystery.

> In the first case we get TCP_MISS every time because it isn't caching
> the data, in the second case it's only the first time and after that we
> get TCP_REFRESH_UNMODIFIED. But how and why is this happening?

Given that this is an intercepting proxy and you're using HTTP (not HTTPS), 
can you do a packet capture with tshark or similar on the internal interface, 
to show the full details of the HTTP request which comes in to Squid from apt, 
and the same for wget, to see what difference there is?


Antony.

-- 
Normal people think "If it ain't broke, don't fix it".
Engineers think "If it ain't broke, it doesn't have enough features yet".

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Fri Apr  3 20:34:01 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Apr 2020 16:34:01 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
Message-ID: <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>

On 4/3/20 4:26 PM, zrm wrote:
> In the first case we get TCP_MISS every time because it isn't caching
> the data, in the second case it's only the first time and after that we
> get TCP_REFRESH_UNMODIFIED. But how and why is this happening?

Those questions can often be answered by looking at HTTP headers
received and sent by Squid when talking to clients and origin servers.
You can see those headers in basic packet captures or if you set
debug_options to ALL,2.

Alex.


From zrm at trustiosity.com  Fri Apr  3 20:55:13 2020
From: zrm at trustiosity.com (zrm)
Date: Fri, 3 Apr 2020 16:55:13 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
Message-ID: <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>

On 4/3/20 16:34, Alex Rousskov wrote:
> On 4/3/20 4:26 PM, zrm wrote:
>> In the first case we get TCP_MISS every time because it isn't caching
>> the data, in the second case it's only the first time and after that we
>> get TCP_REFRESH_UNMODIFIED. But how and why is this happening?
> 
> Those questions can often be answered by looking at HTTP headers
> received and sent by Squid when talking to clients and origin servers.
> You can see those headers in basic packet captures or if you set
> debug_options to ALL,2.
> 
> Alex.
> 

apt to squid:
---------
GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
Host: deb.debian.org
User-Agent: Debian APT-HTTP/1.3 (1.8.2)


----------
squid to deb.debian.org:
---------
GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
User-Agent: Debian APT-HTTP/1.3 (1.8.2)
Host: deb.debian.org
Via: 1.1 tproxy (squid/4.6)
X-Forwarded-For: 192.168.111.55
Cache-Control: max-age=259200
Connection: keep-alive


----------
deb.debian.org to squid:
---------
HTTP/1.1 200 OK
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
ETag: "1389dc-58b605823fa6e"
X-Clacks-Overhead: GNU Terry Pratchett
Cache-Control: public, max-age=2592000
Content-Type: application/x-debian-package
Via: 1.1 varnish
Content-Length: 1280476
Accept-Ranges: bytes
Date: Fri, 03 Apr 2020 05:28:46 GMT
Via: 1.1 varnish
Age: 4248100
Connection: keep-alive
X-Served-By: cache-ams21028-AMS, cache-wdc5559-WDC
X-Cache: HIT, HIT
X-Cache-Hits: 1, 2
X-Timer: S1585891726.434375,VS0,VE0

----------
squid to apt:
---------
HTTP/1.1 200 OK
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
ETag: "1389dc-58b605823fa6e"
X-Clacks-Overhead: GNU Terry Pratchett
Cache-Control: public, max-age=2592000
Content-Type: application/x-debian-package
Content-Length: 1280476
Accept-Ranges: bytes
Date: Fri, 03 Apr 2020 05:28:46 GMT
Age: 4248100
X-Served-By: cache-ams21028-AMS, cache-wdc5559-WDC
X-Cache: HIT, HIT
X-Cache-Hits: 1, 2
X-Timer: S1585891726.434375,VS0,VE0
X-Cache: MISS from tproxy
X-Cache-Lookup: MISS from tproxy:3130
Via: 1.1 varnish, 1.1 varnish, 1.1 tproxy (squid/4.6)
Connection: keep-alive


----------



wget to squid:
---------
GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
User-Agent: Wget/1.20.1 (linux-gnu)
Accept: */*
Accept-Encoding: identity
Host: deb.debian.org
Connection: Keep-Alive


----------
squid to deb.debian.org:
----------
GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
User-Agent: Wget/1.20.1 (linux-gnu)
Accept: */*
Accept-Encoding: identity
Host: deb.debian.org
Via: 1.1 tproxy (squid/4.6)
X-Forwarded-For: 192.168.111.55
Cache-Control: max-age=259200
Connection: keep-alive


----------
deb.debian.org to squid:
---------
HTTP/1.1 200 OK
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
ETag: "1389dc-58b605823fa6e"
X-Clacks-Overhead: GNU Terry Pratchett
Cache-Control: public, max-age=2592000
Content-Type: application/x-debian-package
Via: 1.1 varnish
Content-Length: 1280476
Accept-Ranges: bytes
Date: Fri, 03 Apr 2020 05:28:49 GMT
Via: 1.1 varnish
Age: 4248102
Connection: keep-alive
X-Served-By: cache-ams21028-AMS, cache-wdc5583-WDC
X-Cache: HIT, HIT
X-Cache-Hits: 1, 1
X-Timer: S1585891729.316362,VS0,VE0

----------
squid to wget:
---------
HTTP/1.1 200 OK
Server: Apache
X-Content-Type-Options: nosniff
X-Frame-Options: sameorigin
Referrer-Policy: no-referrer
X-Xss-Protection: 1
Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
ETag: "1389dc-58b605823fa6e"
X-Clacks-Overhead: GNU Terry Pratchett
Cache-Control: public, max-age=2592000
Content-Type: application/x-debian-package
Content-Length: 1280476
Accept-Ranges: bytes
Date: Fri, 03 Apr 2020 05:28:49 GMT
Age: 4248102
X-Served-By: cache-ams21028-AMS, cache-wdc5583-WDC
X-Cache: HIT, HIT
X-Cache-Hits: 1, 1
X-Timer: S1585891729.316362,VS0,VE0
X-Cache: MISS from tproxy
X-Cache-Lookup: MISS from tproxy:3130
Via: 1.1 varnish, 1.1 varnish, 1.1 tproxy (squid/4.6)
Connection: keep-alive


----------



From squid3 at treenet.co.nz  Sat Apr  4 05:48:33 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Apr 2020 18:48:33 +1300
Subject: [squid-users] Error negotiating SSL connection on FD 16
In-Reply-To: <1585802551220-0.post@n4.nabble.com>
References: <1585522706899-0.post@n4.nabble.com>
 <1585802551220-0.post@n4.nabble.com>
Message-ID: <96388971-f784-fce5-c6c3-69e38384f4ef@treenet.co.nz>

On 2/04/20 5:42 pm, saiyan_gc wrote:
> Hi, thank you for reply me. Really appreciated!
> 
> I modified the squid conf file to:
> 
> http_port 2128 ssl-bump cert=/etc/squid/ssl_cert/example.com.cert \
>     key=/etc/squid/ssl_cert/example.com.private \
>     generate-host-certificates=on \
>     dynamic_cert_mem_cache_size=4MB
> https_port 3130 cert=/etc/squid/ssl_cert/example.com.cert \
>     key=/etc/squid/ssl_cert/example.com.private  
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwords
> auth_param basic children 5 startup=0 idle=1
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive off
> acl ncsa_users proxy_auth REQUIRED
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all
> http_access deny !ncsa_users
> http_access allow ncsa_users
> 
> And it's working for http_port. I put the cert into
> /etc/pki/trust-ca/source/anchor, and run a update-ca-trust command. And both
> aws cli and curl command work now. I am still not sure why https_port desn't
> work.


What you have here is:

* TLS explicit/forward proxy on port 3130.

This requires a regular server certificate for the proxy to use it as a
server encrypting traffic between the client and proxy.


* Interception of HTTPS sent in CONNECT tunnels over a plain-text proxy
on port 2128.

This requires a CA certificate to sign auto-generated server
certificates encrypting the traffic between client and origin server.


That difference in cert type is why when one port works, the other will not.

So first thing to do is make sure the cert types are correct.


> The previous setting work with curl but not aws cli, not sure why it failure
> during tls handshake.
> 

The second thing you will need to do, is find out which port the tool is
using and whether it is using it in the right mode of traffic.


Amos


From rousskov at measurement-factory.com  Sat Apr  4 14:53:05 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 4 Apr 2020 10:53:05 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
Message-ID: <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>

On 4/3/20 4:55 PM, zrm wrote:
> On 4/3/20 16:34, Alex Rousskov wrote:
>> On 4/3/20 4:26 PM, zrm wrote:
>>> In the first case we get TCP_MISS every time because it isn't caching
>>> the data, in the second case it's only the first time and after that we
>>> get TCP_REFRESH_UNMODIFIED. But how and why is this happening?

> squid to apt:
> ---------
> X-Cache: MISS from tproxy
> X-Cache-Lookup: MISS from tproxy:3130

> squid to wget:
> ---------
> X-Cache: MISS from tproxy
> X-Cache-Lookup: MISS from tproxy:3130

The headers you have posted tell us that the object was not in Squid
cache when apt and wget transactions started. Since wget was started
after apt, we can speculate that apt transaction did not cache the
object. This is consistent with your observations. I saw nothing in the
posted headers that would explain the difference between apt and wget
outcomes.

Unfortunately, you did not show the headers for the case where the
object actually got cached by Squid. You can probably do that by
repeating the wget transaction twice. I would also repeat the apt
transaction twice after that. It would also be interesting to see the
wget-apt and apt-wget sequences. In summary, I would do
wget-wget-apt-apt-wget-wget. Sleep for 10+ seconds between each
transaction to reduce chances of overlapping cache operations.

BTW, you probably do not need to make ALL,2 logs pretty -- we can figure
out what happens based on Squid messages if you submit one transaction
at a time and disclose transaction sequence. You can just post (a link
to) raw (or sanitized) logs. Compress them if they are too big.

Before sharing the logs, please double check that the problem you want
to address was reproduced during the test.


> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
> ETag: "1389dc-58b605823fa6e"
> Cache-Control: public, max-age=2592000
> Content-Length: 1280476
> Age: 4248100

FWIW: The object is 4'248'100 seconds old. The object max-age is
2'592'000 seconds. Your Squid is probably using an internal max-age of
259'200 seconds, so Squid will require cache hit revalidation during
subsequent transactions after Squid caches the object (if it caches it).


HTH,

Alex.


> apt to squid:
> ---------
> GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
> Host: deb.debian.org
> User-Agent: Debian APT-HTTP/1.3 (1.8.2)
> 
> 
> ----------
> squid to deb.debian.org:
> ---------
> GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
> User-Agent: Debian APT-HTTP/1.3 (1.8.2)
> Host: deb.debian.org
> Via: 1.1 tproxy (squid/4.6)
> X-Forwarded-For: 192.168.111.55
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> deb.debian.org to squid:
> ---------
> HTTP/1.1 200 OK
> Server: Apache
> X-Content-Type-Options: nosniff
> X-Frame-Options: sameorigin
> Referrer-Policy: no-referrer
> X-Xss-Protection: 1
> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
> ETag: "1389dc-58b605823fa6e"
> X-Clacks-Overhead: GNU Terry Pratchett
> Cache-Control: public, max-age=2592000
> Content-Type: application/x-debian-package
> Via: 1.1 varnish
> Content-Length: 1280476
> Accept-Ranges: bytes
> Date: Fri, 03 Apr 2020 05:28:46 GMT
> Via: 1.1 varnish
> Age: 4248100
> Connection: keep-alive
> X-Served-By: cache-ams21028-AMS, cache-wdc5559-WDC
> X-Cache: HIT, HIT
> X-Cache-Hits: 1, 2
> X-Timer: S1585891726.434375,VS0,VE0
> 
> ----------
> squid to apt:
> ---------
> HTTP/1.1 200 OK
> Server: Apache
> X-Content-Type-Options: nosniff
> X-Frame-Options: sameorigin
> Referrer-Policy: no-referrer
> X-Xss-Protection: 1
> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
> ETag: "1389dc-58b605823fa6e"
> X-Clacks-Overhead: GNU Terry Pratchett
> Cache-Control: public, max-age=2592000
> Content-Type: application/x-debian-package
> Content-Length: 1280476
> Accept-Ranges: bytes
> Date: Fri, 03 Apr 2020 05:28:46 GMT
> Age: 4248100
> X-Served-By: cache-ams21028-AMS, cache-wdc5559-WDC
> X-Cache: HIT, HIT
> X-Cache-Hits: 1, 2
> X-Timer: S1585891726.434375,VS0,VE0
> X-Cache: MISS from tproxy
> X-Cache-Lookup: MISS from tproxy:3130
> Via: 1.1 varnish, 1.1 varnish, 1.1 tproxy (squid/4.6)
> Connection: keep-alive
> 
> 
> ----------
> 
> 
> 
> wget to squid:
> ---------
> GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
> User-Agent: Wget/1.20.1 (linux-gnu)
> Accept: */*
> Accept-Encoding: identity
> Host: deb.debian.org
> Connection: Keep-Alive
> 
> 
> ----------
> squid to deb.debian.org:
> ----------
> GET /debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb HTTP/1.1
> User-Agent: Wget/1.20.1 (linux-gnu)
> Accept: */*
> Accept-Encoding: identity
> Host: deb.debian.org
> Via: 1.1 tproxy (squid/4.6)
> X-Forwarded-For: 192.168.111.55
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> deb.debian.org to squid:
> ---------
> HTTP/1.1 200 OK
> Server: Apache
> X-Content-Type-Options: nosniff
> X-Frame-Options: sameorigin
> Referrer-Policy: no-referrer
> X-Xss-Protection: 1
> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
> ETag: "1389dc-58b605823fa6e"
> X-Clacks-Overhead: GNU Terry Pratchett
> Cache-Control: public, max-age=2592000
> Content-Type: application/x-debian-package
> Via: 1.1 varnish
> Content-Length: 1280476
> Accept-Ranges: bytes
> Date: Fri, 03 Apr 2020 05:28:49 GMT
> Via: 1.1 varnish
> Age: 4248102
> Connection: keep-alive
> X-Served-By: cache-ams21028-AMS, cache-wdc5583-WDC
> X-Cache: HIT, HIT
> X-Cache-Hits: 1, 1
> X-Timer: S1585891729.316362,VS0,VE0
> 
> ----------
> squid to wget:
> ---------
> HTTP/1.1 200 OK
> Server: Apache
> X-Content-Type-Options: nosniff
> X-Frame-Options: sameorigin
> Referrer-Policy: no-referrer
> X-Xss-Protection: 1
> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
> ETag: "1389dc-58b605823fa6e"
> X-Clacks-Overhead: GNU Terry Pratchett
> Cache-Control: public, max-age=2592000
> Content-Type: application/x-debian-package
> Content-Length: 1280476
> Accept-Ranges: bytes
> Date: Fri, 03 Apr 2020 05:28:49 GMT
> Age: 4248102
> X-Served-By: cache-ams21028-AMS, cache-wdc5583-WDC
> X-Cache: HIT, HIT
> X-Cache-Hits: 1, 1
> X-Timer: S1585891729.316362,VS0,VE0
> X-Cache: MISS from tproxy
> X-Cache-Lookup: MISS from tproxy:3130
> Via: 1.1 varnish, 1.1 varnish, 1.1 tproxy (squid/4.6)
> Connection: keep-alive
> 
> 
> ----------



From squid3 at treenet.co.nz  Sat Apr  4 15:18:23 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 5 Apr 2020 03:18:23 +1200
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
Message-ID: <98fe4ab9-507b-f7b0-6f64-1eef126a7701@treenet.co.nz>


On 5/04/20 2:53 am, Alex Rousskov wrote:
> On 4/3/20 4:55 PM, zrm wrote:
>> On 4/3/20 16:34, Alex Rousskov wrote:
>>> On 4/3/20 4:26 PM, zrm wrote:
>>>> In the first case we get TCP_MISS every time because it isn't caching
>>>> the data, in the second case it's only the first time and after that we
>>>> get TCP_REFRESH_UNMODIFIED. But how and why is this happening?
> 
>> squid to apt:
>> ---------
>> X-Cache: MISS from tproxy
>> X-Cache-Lookup: MISS from tproxy:3130
> 
>> squid to wget:
>> ---------
>> X-Cache: MISS from tproxy
>> X-Cache-Lookup: MISS from tproxy:3130
> 
> The headers you have posted tell us that the object was not in Squid
> cache when apt and wget transactions started. Since wget was started
> after apt, we can speculate that apt transaction did not cache the
> object. This is consistent with your observations. I saw nothing in the
> posted headers that would explain the difference between apt and wget
> outcomes.
> 
> Unfortunately, you did not show the headers for the case where the
> object actually got cached by Squid. You can probably do that by
> repeating the wget transaction twice. I would also repeat the apt
> transaction twice after that. It would also be interesting to see the
> wget-apt and apt-wget sequences. In summary, I would do
> wget-wget-apt-apt-wget-wget. Sleep for 10+ seconds between each
> transaction to reduce chances of overlapping cache operations.
> 
> BTW, you probably do not need to make ALL,2 logs pretty -- we can figure
> out what happens based on Squid messages if you submit one transaction
> at a time and disclose transaction sequence. You can just post (a link
> to) raw (or sanitized) logs. Compress them if they are too big.
> 
> Before sharing the logs, please double check that the problem you want
> to address was reproduced during the test.
> 
> 
>> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
>> ETag: "1389dc-58b605823fa6e"
>> Cache-Control: public, max-age=2592000
>> Content-Length: 1280476
>> Age: 4248100
> 
> FWIW: The object is 4'248'100 seconds old. The object max-age is
> 2'592'000 seconds. Your Squid is probably using an internal max-age of
> 259'200 seconds, so Squid will require cache hit revalidation during
> subsequent transactions after Squid caches the object (if it caches it).
> 
> 

One thing to notice as well is that the object delivered by the upstream
caches expired over 19 days before these tests took place:

> Cache-Control: public, max-age=2592000
> Age: 4248100

The request from Squid in both transactions was to receive content no
longer than 3 days old. The other caches ignored that requirement and
served old content from their storage, apparently without even checking
an origin whether it was stale.

Amos


From zrm at trustiosity.com  Sun Apr  5 00:02:31 2020
From: zrm at trustiosity.com (zrm)
Date: Sat, 4 Apr 2020 20:02:31 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
Message-ID: <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>

On 4/4/20 10:53, Alex Rousskov wrote:
> The headers you have posted tell us that the object was not in Squid
> cache when apt and wget transactions started. Since wget was started
> after apt, we can speculate that apt transaction did not cache the
> object. This is consistent with your observations. I saw nothing in the
> posted headers that would explain the difference between apt and wget
> outcomes.
> 
> Unfortunately, you did not show the headers for the case where the
> object actually got cached by Squid. You can probably do that by
> repeating the wget transaction twice. I would also repeat the apt
> transaction twice after that. It would also be interesting to see the
> wget-apt and apt-wget sequences. In summary, I would do
> wget-wget-apt-apt-wget-wget. Sleep for 10+ seconds between each
> transaction to reduce chances of overlapping cache operations.

Attached cache.log excerpt for wget-wget-apt-apt-wget-wget. It answers 
the apt requests from the cache once it's in there, it just won't cache 
it to begin with when apt makes the request:

[wget] 1586041686.600    725 192.168.111.55 TCP_MISS/200 1281195 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.66.133 application/x-debian-package
[wget] 1586041710.518    107 192.168.111.55 TCP_REFRESH_UNMODIFIED/200 
1281232 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.66.133 application/x-debian-package
[apt] 1586041733.058     69 192.168.111.55 TCP_REFRESH_UNMODIFIED/200 
1281234 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/151.101.200.204 application/x-debian-package
[apt] 1586041753.971    101 192.168.111.55 TCP_REFRESH_UNMODIFIED/200 
1281234 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/151.101.200.204 application/x-debian-package
[wget] 1586041769.162    160 192.168.111.55 TCP_REFRESH_UNMODIFIED/200 
1281232 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/199.232.66.133 application/x-debian-package
[wget] 1586041786.916     71 192.168.111.55 TCP_REFRESH_UNMODIFIED/200 
1281232 GET 
http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb - 
ORIGINAL_DST/151.101.250.133 application/x-debian-package

> BTW, you probably do not need to make ALL,2 logs pretty -- we can figure
> out what happens based on Squid messages if you submit one transaction
> at a time and disclose transaction sequence. You can just post (a link
> to) raw (or sanitized) logs. Compress them if they are too big.

> Before sharing the logs, please double check that the problem you want
> to address was reproduced during the test.

In this case we start with wget and then it is already in the cache for 
the requests made by apt. The problem is the data not being cached when 
apt makes the request and it isn't already there. The apt requests do 
get answered from the cache if it is already there.

The headers from the previous email show what happens when apt makes the 
request and it's not already in the cache.

>> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
>> ETag: "1389dc-58b605823fa6e"
>> Cache-Control: public, max-age=2592000
>> Content-Length: 1280476
>> Age: 4248100
> 
> FWIW: The object is 4'248'100 seconds old. The object max-age is
> 2'592'000 seconds. Your Squid is probably using an internal max-age of
> 259'200 seconds, so Squid will require cache hit revalidation during
> subsequent transactions after Squid caches the object (if it caches it).

That makes sense. These packages never really change at all -- the 
package version is part of the URI so if it's updated the package URI 
changes rather than the data at the old URI. I might set a longer max 
age in the config once this is worked out.

> HTH,
> 
> Alex.
> 
> 

Thanks.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cache.log
Type: text/x-log
Size: 30262 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200404/64c9f79b/attachment.bin>

From rousskov at measurement-factory.com  Mon Apr  6 15:49:17 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 6 Apr 2020 11:49:17 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
Message-ID: <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>

On 4/4/20 8:02 PM, zrm wrote:
> Attached cache.log excerpt for wget-wget-apt-apt-wget-wget. It answers
> the apt requests from the cache once it's in there, it just won't cache
> it to begin with when apt makes the request

Thank you for sharing this log. I agree with your conclusion. The apt
query results in cache revalidation and does not purge the already
cached copy. This conclusion eliminates a few suspects.

There is probably something special about the combination of an apt
request and a 200 OK miss response that prevents Squid from caching that
response. I do not see anything wrong in the logs you have already
already posted. Perhaps others will spot something.

If you get no better responses, please post a link to a compressed
apt-apt-wget-wget log, starting from a cache that does not contain the
response in question and after enabling elevated Squid debugging with
"squid -k debug" or similar. You can find more instructions about
debugging individual transactions at
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

A detailed apt-apt-wget-wget log should tell us why Squid is refusing to
cache a 200 OK response to the apt query but caches a very similar
response to a very similar wget query.


Thank you,

Alex.



> [wget] 1586041686.600??? 725 192.168.111.55 TCP_MISS/200 1281195 GET
> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
> ORIGINAL_DST/199.232.66.133 application/x-debian-package
> [wget] 1586041710.518??? 107 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
> 1281232 GET
> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
> ORIGINAL_DST/199.232.66.133 application/x-debian-package
> [apt] 1586041733.058???? 69 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
> 1281234 GET
> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
> ORIGINAL_DST/151.101.200.204 application/x-debian-package
> [apt] 1586041753.971??? 101 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
> 1281234 GET
> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
> ORIGINAL_DST/151.101.200.204 application/x-debian-package
> [wget] 1586041769.162??? 160 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
> 1281232 GET
> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
> ORIGINAL_DST/199.232.66.133 application/x-debian-package
> [wget] 1586041786.916???? 71 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
> 1281232 GET
> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
> ORIGINAL_DST/151.101.250.133 application/x-debian-package
> 
>> BTW, you probably do not need to make ALL,2 logs pretty -- we can figure
>> out what happens based on Squid messages if you submit one transaction
>> at a time and disclose transaction sequence. You can just post (a link
>> to) raw (or sanitized) logs. Compress them if they are too big.
> 
>> Before sharing the logs, please double check that the problem you want
>> to address was reproduced during the test.
> 
> In this case we start with wget and then it is already in the cache for
> the requests made by apt. The problem is the data not being cached when
> apt makes the request and it isn't already there. The apt requests do
> get answered from the cache if it is already there.
> 
> The headers from the previous email show what happens when apt makes the
> request and it's not already in the cache.
> 
>>> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
>>> ETag: "1389dc-58b605823fa6e"
>>> Cache-Control: public, max-age=2592000
>>> Content-Length: 1280476
>>> Age: 4248100
>>
>> FWIW: The object is 4'248'100 seconds old. The object max-age is
>> 2'592'000 seconds. Your Squid is probably using an internal max-age of
>> 259'200 seconds, so Squid will require cache hit revalidation during
>> subsequent transactions after Squid caches the object (if it caches it).
> 
> That makes sense. These packages never really change at all -- the
> package version is part of the URI so if it's updated the package URI
> changes rather than the data at the old URI. I might set a longer max
> age in the config once this is worked out.
> 
>> HTH,
>>
>> Alex.
>>
>>
> 
> Thanks.



From jmpatagonia at gmail.com  Mon Apr  6 22:13:14 2020
From: jmpatagonia at gmail.com (Juan Manuel P)
Date: Mon, 6 Apr 2020 19:13:14 -0300
Subject: [squid-users] help with TC_MISS/200
Message-ID: <CADZCxstdJwZKSeSWDGYSTE7TmvLdfCsPfBhKeCadd_eJAE_wvw@mail.gmail.com>

Hello a implementing a reverse transparent  proxy, connected directed to
internet with round-robin balance to two internal again reverse transparent
proxy.



              -----  parent one (reverse & balance )

            ONE CHILD  (reverse & balance)

                                           ----- parent two (reverse &
balance)



A note that allways my request is serving by a 0 TCP_ MISS/200, on three
squids

That mean never take from cache ?

Squid conf:

cache_dir ufs /var/spool/squid-app/ 300 16 256


Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200406/103dc0bf/attachment.htm>

From squid3 at treenet.co.nz  Mon Apr  6 22:27:43 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Apr 2020 10:27:43 +1200
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
 <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
Message-ID: <cba7dda1-b552-0225-7daa-e26e1b1b2433@treenet.co.nz>

On 7/04/20 3:49 am, Alex Rousskov wrote:
> On 4/4/20 8:02 PM, zrm wrote:
>> Attached cache.log excerpt for wget-wget-apt-apt-wget-wget. It answers
>> the apt requests from the cache once it's in there, it just won't cache
>> it to begin with when apt makes the request
> 
> Thank you for sharing this log. I agree with your conclusion. The apt
> query results in cache revalidation and does not purge the already
> cached copy. This conclusion eliminates a few suspects.
> 
> There is probably something special about the combination of an apt
> request and a 200 OK miss response that prevents Squid from caching that
> response. I do not see anything wrong in the logs you have already
> already posted. Perhaps others will spot something.


zrm, can you please do a clean test for the apt sequence exactly as Alex
requested rather than just referring to the earlier one done. A swapout
timing overlap by as much as a few nanoseconds in the original test
could mean the different between HIT and MISS. So anyone able to find
this will need those details to work with.

Amos


From squid3 at treenet.co.nz  Mon Apr  6 22:39:40 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Apr 2020 10:39:40 +1200
Subject: [squid-users] help with TC_MISS/200
In-Reply-To: <CADZCxstdJwZKSeSWDGYSTE7TmvLdfCsPfBhKeCadd_eJAE_wvw@mail.gmail.com>
References: <CADZCxstdJwZKSeSWDGYSTE7TmvLdfCsPfBhKeCadd_eJAE_wvw@mail.gmail.com>
Message-ID: <15625df7-5a8f-bb3e-f2b2-a20d450bc16c@treenet.co.nz>

On 7/04/20 10:13 am, Juan Manuel P wrote:
> Hello a implementing a reverse transparent ?proxy, connected directed to
> internet with round-robin balance to two internal again reverse
> transparent proxy.
> 

There is no such thing as "reverse transparent proxy".

"reverse proxy" and "transparent proxy" have incompatible requirements
at the HTTP semantic behaviour level.

Also, "round-robin balance" and "transparent proxy" have mutually
exclusive packet routing requirements at the TCP level.

What exactly have you configured? at both proxy levels, and in the
networking systems between them?

> 
> A note that allways my request is serving by a 0 TCP_ MISS/200, on three
> squids
> 
> That mean never take from cache ?
> 

That is what it means, yes.


> Squid conf:
> 
> cache_dir ufs /var/spool/squid-app/ 300 16 256
> 

This cache cannot store more than 300MB total data. That may or may not
be relevant. We will need to see the HTTP messages at both client and
server connections for at least the proxy first receiving HTTP requests,
maybe the backends as well.

For current Squid versions you can retrieve those details from cache.log
after configuring "debug_options 11,2". Make sure the cache is empty
first, make an HTTP request for one object, wait 10sec then after that
transaction *finishes*, then make a second identical request.

If the URLs are publicly accessible, you can use the tool at redbot.org
to fetch them and it will report details about the HTT response
cacheability and any problems with the HTTP syntax.

Amos


From silamael at coronamundi.de  Tue Apr  7 06:19:25 2020
From: silamael at coronamundi.de (Silamael Darkomen)
Date: Tue, 7 Apr 2020 08:19:25 +0200
Subject: [squid-users] Distributing users according to their LDAP groups on
 multiple cache peers
Message-ID: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>

Hello,

Is there any possibility to distribute a bunch of users to different
cache peers based on the user group in LDAP?

For older versions this was possible by using the slow external ACL
first for evaluation in the http_access clause and latter using the slow
external ACLs again in the cache_peer_access option.

With the update from 4.9 to 4.10 this behavior seems to be broken.

Thanks for any hints!

-- Matthias


From squid3 at treenet.co.nz  Tue Apr  7 07:01:15 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Apr 2020 19:01:15 +1200
Subject: [squid-users] Distributing users according to their LDAP groups
 on multiple cache peers
In-Reply-To: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>
References: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>
Message-ID: <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>

On 7/04/20 6:19 pm, Silamael Darkomen wrote:
> Hello,
> 
> Is there any possibility to distribute a bunch of users to different
> cache peers based on the user group in LDAP?
> 
> For older versions this was possible by using the slow external ACL
> first for evaluation in the http_access clause and latter using the slow
> external ACLs again in the cache_peer_access option.
> 
> With the update from 4.9 to 4.10 this behavior seems to be broken.


That trick has never been properly consistent. It relies on the first
entry not being pushed out of cache before the second check. Under any
type of load it starts to fail.


In current Squid you can have the helper deliver group=blah and use the
note ACL type to check it in the fast checks. It works reliably, and
with multiple groups.

Amos


From silamael at coronamundi.de  Tue Apr  7 13:48:38 2020
From: silamael at coronamundi.de (Silamael Darkomen)
Date: Tue, 7 Apr 2020 15:48:38 +0200
Subject: [squid-users] Distributing users according to their LDAP groups
 on multiple cache peers
In-Reply-To: <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>
References: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>
 <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>
Message-ID: <1f33ddd3-2905-970a-3deb-87efef699a2e@coronamundi.de>

Hello Amos,

Thank you for your quick reply.
Could you perhaps give me a short configuration example, how this should
lool like?

Thank you very much!

-- Matthias

On 07.04.2020 09:01, Amos Jeffries wrote:
> On 7/04/20 6:19 pm, Silamael Darkomen wrote:
>> Hello,
>>
>> Is there any possibility to distribute a bunch of users to different
>> cache peers based on the user group in LDAP?
>>
>> For older versions this was possible by using the slow external ACL
>> first for evaluation in the http_access clause and latter using the slow
>> external ACLs again in the cache_peer_access option.
>>
>> With the update from 4.9 to 4.10 this behavior seems to be broken.
> 
> 
> That trick has never been properly consistent. It relies on the first
> entry not being pushed out of cache before the second check. Under any
> type of load it starts to fail.
> 
> 
> In current Squid you can have the helper deliver group=blah and use the
> note ACL type to check it in the fast checks. It works reliably, and
> with multiple groups.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From rousskov at measurement-factory.com  Tue Apr  7 14:17:52 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 7 Apr 2020 10:17:52 -0400
Subject: [squid-users] Distributing users according to their LDAP groups
 on multiple cache peers
In-Reply-To: <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>
References: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>
 <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>
Message-ID: <2f4b4ec5-d8b0-cf2c-66d0-06ea0b241813@measurement-factory.com>

On 4/7/20 3:01 AM, Amos Jeffries wrote:
> On 7/04/20 6:19 pm, Silamael Darkomen wrote:
>> Hello,
>>
>> Is there any possibility to distribute a bunch of users to different
>> cache peers based on the user group in LDAP?
>>
>> For older versions this was possible by using the slow external ACL
>> first for evaluation in the http_access clause and latter using the slow
>> external ACLs again in the cache_peer_access option.
>>
>> With the update from 4.9 to 4.10 this behavior seems to be broken.
> 
> 
> That trick has never been properly consistent. It relies on the first
> entry not being pushed out of cache before the second check. Under any
> type of load it starts to fail.
> 
> 
> In current Squid you can have the helper deliver group=blah and use the
> note ACL type to check it in the fast checks. It works reliably, and
> with multiple groups.

I agree with Amos, but want to add that there are no known new breakages
of that unreliable "cache and reuse external ACL results" approach. If
you can use this suspected regression as an excuse to implement a more
reliable scheme, please follow Amos' advice. Otherwise, perhaps there is
a regression bug we should fix.

Alex.


From squid3 at treenet.co.nz  Tue Apr  7 14:52:59 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Apr 2020 02:52:59 +1200
Subject: [squid-users] Distributing users according to their LDAP groups
 on multiple cache peers
In-Reply-To: <1f33ddd3-2905-970a-3deb-87efef699a2e@coronamundi.de>
References: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>
 <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>
 <1f33ddd3-2905-970a-3deb-87efef699a2e@coronamundi.de>
Message-ID: <4bf56793-d994-7c0e-2b82-41af7b3f4139@treenet.co.nz>

On 8/04/20 1:48 am, Silamael Darkomen wrote:
> Hello Amos,
> 
> Thank you for your quick reply.
> Could you perhaps give me a short configuration example, how this should
> lool like?
> 


It would be something like this:

 acl groupCheck external ...
 acl groupFoo note group foo

 http_access allow groupCheck
 ...
 cache_peer_access fooBar allow groupFoo


Amos


From silamael at coronamundi.de  Tue Apr  7 15:43:52 2020
From: silamael at coronamundi.de (Silamael Darkomen)
Date: Tue, 7 Apr 2020 17:43:52 +0200
Subject: [squid-users] Distributing users according to their LDAP groups
 on multiple cache peers
In-Reply-To: <4bf56793-d994-7c0e-2b82-41af7b3f4139@treenet.co.nz>
References: <0d9611c6-12a6-028a-ee48-453cecdecc6a@coronamundi.de>
 <f6bf87e4-daa2-18d9-1b3a-be999f6e3230@treenet.co.nz>
 <1f33ddd3-2905-970a-3deb-87efef699a2e@coronamundi.de>
 <4bf56793-d994-7c0e-2b82-41af7b3f4139@treenet.co.nz>
Message-ID: <538d550d-fd84-51b5-4503-f2e71427ed08@coronamundi.de>

On 07.04.2020 16:52, Amos Jeffries wrote:
> It would be something like this:
> 
>  acl groupCheck external ...
>  acl groupFoo note group foo
> 
>  http_access allow groupCheck
>  ...
>  cache_peer_access fooBar allow groupFoo
> 
> 
> Amos

Hi Amos,

Thank you again for the quick reply, seems to work for us :)

Cheers,
Matthias


From gkinkie at gmail.com  Tue Apr  7 16:46:00 2020
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 7 Apr 2020 17:46:00 +0100
Subject: [squid-users] cache_diff
Message-ID: <CA+Y8hcOiPgGMHRU49ec8AfzMpoo_0cmPLg98cixc8bOrvL-=-A@mail.gmail.com>

Hi all,
  has anybody used the cache_diff program that comes with the squid sources
(but is not installed anywhere) in the past year or so?
  I'm asking because unless someone has, I'm planning to deprecate and
remove it in about a week. So if you used it it, please speak up!
-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200407/171c7015/attachment.htm>

From zrm at trustiosity.com  Wed Apr  8 00:48:07 2020
From: zrm at trustiosity.com (zrm)
Date: Tue, 7 Apr 2020 20:48:07 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
 <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
Message-ID: <b88e4265-916d-ecea-31c6-225449df37a1@trustiosity.com>

cache.log for apt-apt-wget-wget with squid -k debug:

https://www.trustiosity.com/squid/cache-debug.log.xz

On 4/6/20 11:49, Alex Rousskov wrote:
> On 4/4/20 8:02 PM, zrm wrote:
>> Attached cache.log excerpt for wget-wget-apt-apt-wget-wget. It answers
>> the apt requests from the cache once it's in there, it just won't cache
>> it to begin with when apt makes the request
> 
> Thank you for sharing this log. I agree with your conclusion. The apt
> query results in cache revalidation and does not purge the already
> cached copy. This conclusion eliminates a few suspects.
> 
> There is probably something special about the combination of an apt
> request and a 200 OK miss response that prevents Squid from caching that
> response. I do not see anything wrong in the logs you have already
> already posted. Perhaps others will spot something.
> 
> If you get no better responses, please post a link to a compressed
> apt-apt-wget-wget log, starting from a cache that does not contain the
> response in question and after enabling elevated Squid debugging with
> "squid -k debug" or similar. You can find more instructions about
> debugging individual transactions at
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
> 
> A detailed apt-apt-wget-wget log should tell us why Squid is refusing to
> cache a 200 OK response to the apt query but caches a very similar
> response to a very similar wget query.
> 
> 
> Thank you,
> 
> Alex.
> 
> 
> 
>> [wget] 1586041686.600??? 725 192.168.111.55 TCP_MISS/200 1281195 GET
>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>> ORIGINAL_DST/199.232.66.133 application/x-debian-package
>> [wget] 1586041710.518??? 107 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>> 1281232 GET
>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>> ORIGINAL_DST/199.232.66.133 application/x-debian-package
>> [apt] 1586041733.058???? 69 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>> 1281234 GET
>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>> ORIGINAL_DST/151.101.200.204 application/x-debian-package
>> [apt] 1586041753.971??? 101 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>> 1281234 GET
>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>> ORIGINAL_DST/151.101.200.204 application/x-debian-package
>> [wget] 1586041769.162??? 160 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>> 1281232 GET
>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>> ORIGINAL_DST/199.232.66.133 application/x-debian-package
>> [wget] 1586041786.916???? 71 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>> 1281232 GET
>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>> ORIGINAL_DST/151.101.250.133 application/x-debian-package
>>
>>> BTW, you probably do not need to make ALL,2 logs pretty -- we can figure
>>> out what happens based on Squid messages if you submit one transaction
>>> at a time and disclose transaction sequence. You can just post (a link
>>> to) raw (or sanitized) logs. Compress them if they are too big.
>>
>>> Before sharing the logs, please double check that the problem you want
>>> to address was reproduced during the test.
>>
>> In this case we start with wget and then it is already in the cache for
>> the requests made by apt. The problem is the data not being cached when
>> apt makes the request and it isn't already there. The apt requests do
>> get answered from the cache if it is already there.
>>
>> The headers from the previous email show what happens when apt makes the
>> request and it's not already in the cache.
>>
>>>> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
>>>> ETag: "1389dc-58b605823fa6e"
>>>> Cache-Control: public, max-age=2592000
>>>> Content-Length: 1280476
>>>> Age: 4248100
>>>
>>> FWIW: The object is 4'248'100 seconds old. The object max-age is
>>> 2'592'000 seconds. Your Squid is probably using an internal max-age of
>>> 259'200 seconds, so Squid will require cache hit revalidation during
>>> subsequent transactions after Squid caches the object (if it caches it).
>>
>> That makes sense. These packages never really change at all -- the
>> package version is part of the URI so if it's updated the package URI
>> changes rather than the data at the old URI. I might set a longer max
>> age in the config once this is worked out.
>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>
>> Thanks.
> 


From squid.org at bloms.de  Wed Apr  8 11:36:59 2020
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 8 Apr 2020 13:36:59 +0200
Subject: [squid-users] sometimes intermediate certificates were not
 downloaded when using sslbump
Message-ID: <20200408113659.dhqxatdaal5o7dex@bloms.de>

Hello,

I use a self compiled squid 4.10 compiled as follow:

~# squid --version
Squid Cache: Version 4.10
Service Name: squid

This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--sysconfdir=/etc/squid' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--localstatedir=/var' '--libexecdir=/usr/sbin' '--datadir=/usr/share/squid' '--mandir=/usr/share/man' '--with-default-user=squid' '--with-filedescriptors=131072' '--with-logdir=/var/log/squid' '--disable-auto-locale' '--disable-auth-negotiate' '--disable-auth-ntlm' '--disable-eui' '--disable-carp' '--disable-htcp' '--disable-ident-lookups' '--disable-loadable-modules' '--disable-translation' '--disable-wccp' '--disable-wccpv2' '--enable-async-io=128' '--enable-auth' '--enable-auth-basic=LDAP NCSA' '--enable-auth-digest=LDAP file' '--enable-epoll' '--enable-log-daemon-helpers=file' '--enable-icap-client' '--enable-inline' '--enable-snmp' '--enable-disk-io=AIO,DiskThreads,IpcIo,Blocking' '--enable-storeio=ufs,aufs,rock' '--enable-referer-log' '--enable-useragent-log' '--enable-large-cache-files' '--enable-removal-policies=lru,heap' '--enable-follow-x-forwarded-for' '--enable-ssl-crtd' '--with-openssl'

in squid.conf I set following acl at the very benning of acl section:

# allow fetching of missing intermediate certificates
acl fetch_intermediate_certificate transaction_initiator certificate-fetching
cache allow fetch_intermediate_certificate
cache deny all
http_access allow fetch_intermediate_certificate

and squid fetches intermediate certificates for websites like: https://incomplete-chain.badssl.com/
But squid doesn't fetch the intermediate certificates for the site https://www.formulare-bfinv.de/
and I don't know why.

I checked all AiA entries in the certificates and it looks good to me.

Can anybody try the site https://www.formulare-bfinv.de/ with enabled sslbump,
so I can see whether my installation is broken or the webserver configuration isn't correct ?

Thank you very much.

-- 
Best regards

  Dieter Bloms

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From belle at bazuin.nl  Wed Apr  8 11:48:14 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 8 Apr 2020 13:48:14 +0200
Subject: [squid-users] sometimes intermediate certificates were not
 downloaded when using sslbump
In-Reply-To: <20200408113659.dhqxatdaal5o7dex@bloms.de>
References: <20200408113659.dhqxatdaal5o7dex@bloms.de>
Message-ID: <vmime.5e8db9fe.2794.67c3a1f25cbd16de@ms249-lin-003.rotterdam.bazuin.nl>

This is a simple one. 

The certificate chain of that website is incorrect. 
As shown here : https://www.ssllabs.com/ssltest/analyze.html?d=www.formulare%2dbfinv.de&latest 

Check you webserver first and correct you ciphers in your apache webserver. 

Greetz, 

Louis
 

> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens Dieter Bloms
> Verzonden: woensdag 8 april 2020 13:37
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: [squid-users] sometimes intermediate certificates 
> were not downloaded when using sslbump
> 
> Hello,
> 
> I use a self compiled squid 4.10 compiled as follow:
> 
> ~# squid --version
> Squid Cache: Version 4.10
> Service Name: squid
> 
> This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal 
> restrictions on distribution see 
> https://www.openssl.org/source/license.html
> 
> configure options:  '--prefix=/usr' '--sysconfdir=/etc/squid' 
> '--bindir=/usr/sbin' '--sbindir=/usr/sbin' 
> '--localstatedir=/var' '--libexecdir=/usr/sbin' 
> '--datadir=/usr/share/squid' '--mandir=/usr/share/man' 
> '--with-default-user=squid' '--with-filedescriptors=131072' 
> '--with-logdir=/var/log/squid' '--disable-auto-locale' 
> '--disable-auth-negotiate' '--disable-auth-ntlm' 
> '--disable-eui' '--disable-carp' '--disable-htcp' 
> '--disable-ident-lookups' '--disable-loadable-modules' 
> '--disable-translation' '--disable-wccp' '--disable-wccpv2' 
> '--enable-async-io=128' '--enable-auth' 
> '--enable-auth-basic=LDAP NCSA' '--enable-auth-digest=LDAP 
> file' '--enable-epoll' '--enable-log-daemon-helpers=file' 
> '--enable-icap-client' '--enable-inline' '--enable-snmp' 
> '--enable-disk-io=AIO,DiskThreads,IpcIo,Blocking' 
> '--enable-storeio=ufs,aufs,rock' '--enable-referer-log' 
> '--enable-useragent-log' '--enable-large-cache-files' 
> '--enable-removal-policies=lru,heap' 
> '--enable-follow-x-forwarded-for' '--enable-ssl-crtd' '--with-openssl'
> 
> in squid.conf I set following acl at the very benning of acl section:
> 
> # allow fetching of missing intermediate certificates
> acl fetch_intermediate_certificate transaction_initiator 
> certificate-fetching
> cache allow fetch_intermediate_certificate
> cache deny all
> http_access allow fetch_intermediate_certificate
> 
> and squid fetches intermediate certificates for websites 
> like: https://incomplete-chain.badssl.com/
> But squid doesn't fetch the intermediate certificates for the 
> site https://www.formulare-bfinv.de/
> and I don't know why.
> 
> I checked all AiA entries in the certificates and it looks good to me.
> 
> Can anybody try the site https://www.formulare-bfinv.de/ with 
> enabled sslbump,
> so I can see whether my installation is broken or the 
> webserver configuration isn't correct ?
> 
> Thank you very much.
> 
> -- 
> Best regards
> 
>   Dieter Bloms
> 
> --
> I do not get viruses because I do not use MS software.
> If you use Outlook then please do not put my email address in your
> address-book so that WHEN you get a virus it won't use my 
> address in the
> From field.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid.org at bloms.de  Wed Apr  8 13:14:11 2020
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 8 Apr 2020 15:14:11 +0200
Subject: [squid-users] sometimes intermediate certificates were not
 downloaded when using sslbump
In-Reply-To: <vmime.5e8db9fe.2794.67c3a1f25cbd16de@ms249-lin-003.rotterdam.bazuin.nl>
References: <20200408113659.dhqxatdaal5o7dex@bloms.de>
 <vmime.5e8db9fe.2794.67c3a1f25cbd16de@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <20200408131411.ofzazios7qelnfxr@bloms.de>

Hello Louis,

thank you for your answer.

It is not my webserver. Am a user who wants to connect to the webserver.
I know that the certificate chain is incomplete.
As far as I know squid should be able to fetch the missing intermediate
certificates on its own with the help of Authority Information Access (AIA) to get the complete list.
So squid should be able to verify the server certificate even the
webserver doesn't deliver the intermediate certificates.

On Wed, Apr 08, L.P.H. van Belle wrote:

> This is a simple one. 
> 
> The certificate chain of that website is incorrect. 
> As shown here : https://www.ssllabs.com/ssltest/analyze.html?d=www.formulare%2dbfinv.de&latest 
> 
> Check you webserver first and correct you ciphers in your apache webserver. 
> 
> Greetz, 
> 
> Louis
>  
> 
> > -----Oorspronkelijk bericht-----
> > Van: squid-users 
> > [mailto:squid-users-bounces at lists.squid-cache.org] Namens Dieter Bloms
> > Verzonden: woensdag 8 april 2020 13:37
> > Aan: squid-users at lists.squid-cache.org
> > Onderwerp: [squid-users] sometimes intermediate certificates 
> > were not downloaded when using sslbump
> > 
> > Hello,
> > 
> > I use a self compiled squid 4.10 compiled as follow:
> > 
> > ~# squid --version
> > Squid Cache: Version 4.10
> > Service Name: squid
> > 
> > This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal 
> > restrictions on distribution see 
> > https://www.openssl.org/source/license.html
> > 
> > configure options:  '--prefix=/usr' '--sysconfdir=/etc/squid' 
> > '--bindir=/usr/sbin' '--sbindir=/usr/sbin' 
> > '--localstatedir=/var' '--libexecdir=/usr/sbin' 
> > '--datadir=/usr/share/squid' '--mandir=/usr/share/man' 
> > '--with-default-user=squid' '--with-filedescriptors=131072' 
> > '--with-logdir=/var/log/squid' '--disable-auto-locale' 
> > '--disable-auth-negotiate' '--disable-auth-ntlm' 
> > '--disable-eui' '--disable-carp' '--disable-htcp' 
> > '--disable-ident-lookups' '--disable-loadable-modules' 
> > '--disable-translation' '--disable-wccp' '--disable-wccpv2' 
> > '--enable-async-io=128' '--enable-auth' 
> > '--enable-auth-basic=LDAP NCSA' '--enable-auth-digest=LDAP 
> > file' '--enable-epoll' '--enable-log-daemon-helpers=file' 
> > '--enable-icap-client' '--enable-inline' '--enable-snmp' 
> > '--enable-disk-io=AIO,DiskThreads,IpcIo,Blocking' 
> > '--enable-storeio=ufs,aufs,rock' '--enable-referer-log' 
> > '--enable-useragent-log' '--enable-large-cache-files' 
> > '--enable-removal-policies=lru,heap' 
> > '--enable-follow-x-forwarded-for' '--enable-ssl-crtd' '--with-openssl'
> > 
> > in squid.conf I set following acl at the very benning of acl section:
> > 
> > # allow fetching of missing intermediate certificates
> > acl fetch_intermediate_certificate transaction_initiator 
> > certificate-fetching
> > cache allow fetch_intermediate_certificate
> > cache deny all
> > http_access allow fetch_intermediate_certificate
> > 
> > and squid fetches intermediate certificates for websites 
> > like: https://incomplete-chain.badssl.com/
> > But squid doesn't fetch the intermediate certificates for the 
> > site https://www.formulare-bfinv.de/
> > and I don't know why.
> > 
> > I checked all AiA entries in the certificates and it looks good to me.
> > 
> > Can anybody try the site https://www.formulare-bfinv.de/ with 
> > enabled sslbump,
> > so I can see whether my installation is broken or the 
> > webserver configuration isn't correct ?
> > 
> > Thank you very much.
> > 
> > -- 
> > Best regards
> > 
> >   Dieter Bloms
> > 
> > --
> > I do not get viruses because I do not use MS software.
> > If you use Outlook then please do not put my email address in your
> > address-book so that WHEN you get a virus it won't use my 
> > address in the
> > From field.
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> > 
> 

-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From rousskov at measurement-factory.com  Wed Apr  8 14:46:20 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 8 Apr 2020 10:46:20 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <b88e4265-916d-ecea-31c6-225449df37a1@trustiosity.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
 <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
 <b88e4265-916d-ecea-31c6-225449df37a1@trustiosity.com>
Message-ID: <7d189ee7-af59-337d-6ac8-2cc937692d2f@measurement-factory.com>

On 4/7/20 8:48 PM, zrm wrote:

> https://www.trustiosity.com/squid/cache-debug.log.xz

I found the reason for the difference.

After the destination IP address of your apt requests fails Host header
validation, Squid marks the request as "not cachable":

> hostHeaderIpVerify: IP 151.101.248.204:80 does not match from-Host IP 151.101.202.133
> hostHeaderIpVerify: FAILED to validate IP 151.101.248.204:80
> clientInterpretRequestHeaders: REQ_CACHABLE = NOT SET


After the destination IP address of your wget requests passes Host
header validation, Squid marks the request as "cachable":

> hostHeaderIpVerify: validated IP 151.101.202.133:80
> clientInterpretRequestHeaders: REQ_CACHABLE = SET


N.B. The log lines above have been slightly adjusted for readability
(this particular raw output is rather difficult to interpret correctly
IMO), but you can easily find raw lines if you look for the preserved
function names.


I hope others on the list will guide you towards a resolution of this
problem.


HTH,

Alex.

> On 4/6/20 11:49, Alex Rousskov wrote:
>> On 4/4/20 8:02 PM, zrm wrote:
>>> Attached cache.log excerpt for wget-wget-apt-apt-wget-wget. It answers
>>> the apt requests from the cache once it's in there, it just won't cache
>>> it to begin with when apt makes the request
>>
>> Thank you for sharing this log. I agree with your conclusion. The apt
>> query results in cache revalidation and does not purge the already
>> cached copy. This conclusion eliminates a few suspects.
>>
>> There is probably something special about the combination of an apt
>> request and a 200 OK miss response that prevents Squid from caching that
>> response. I do not see anything wrong in the logs you have already
>> already posted. Perhaps others will spot something.
>>
>> If you get no better responses, please post a link to a compressed
>> apt-apt-wget-wget log, starting from a cache that does not contain the
>> response in question and after enabling elevated Squid debugging with
>> "squid -k debug" or similar. You can find more instructions about
>> debugging individual transactions at
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>>
>>
>> A detailed apt-apt-wget-wget log should tell us why Squid is refusing to
>> cache a 200 OK response to the apt query but caches a very similar
>> response to a very similar wget query.
>>
>>
>> Thank you,
>>
>> Alex.
>>
>>
>>
>>> [wget] 1586041686.600??? 725 192.168.111.55 TCP_MISS/200 1281195 GET
>>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>>> ORIGINAL_DST/199.232.66.133 application/x-debian-package
>>> [wget] 1586041710.518??? 107 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>>> 1281232 GET
>>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>>> ORIGINAL_DST/199.232.66.133 application/x-debian-package
>>> [apt] 1586041733.058???? 69 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>>> 1281234 GET
>>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>>> ORIGINAL_DST/151.101.200.204 application/x-debian-package
>>> [apt] 1586041753.971??? 101 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>>> 1281234 GET
>>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>>> ORIGINAL_DST/151.101.200.204 application/x-debian-package
>>> [wget] 1586041769.162??? 160 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>>> 1281232 GET
>>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>>> ORIGINAL_DST/199.232.66.133 application/x-debian-package
>>> [wget] 1586041786.916???? 71 192.168.111.55 TCP_REFRESH_UNMODIFIED/200
>>> 1281232 GET
>>> http://deb.debian.org/debian/pool/main/v/vim/vim_8.1.0875-5_amd64.deb -
>>> ORIGINAL_DST/151.101.250.133 application/x-debian-package
>>>
>>>> BTW, you probably do not need to make ALL,2 logs pretty -- we can
>>>> figure
>>>> out what happens based on Squid messages if you submit one transaction
>>>> at a time and disclose transaction sequence. You can just post (a link
>>>> to) raw (or sanitized) logs. Compress them if they are too big.
>>>
>>>> Before sharing the logs, please double check that the problem you want
>>>> to address was reproduced during the test.
>>>
>>> In this case we start with wget and then it is already in the cache for
>>> the requests made by apt. The problem is the data not being cached when
>>> apt makes the request and it isn't already there. The apt requests do
>>> get answered from the cache if it is already there.
>>>
>>> The headers from the previous email show what happens when apt makes the
>>> request and it's not already in the cache.
>>>
>>>>> Last-Modified: Sat, 15 Jun 2019 17:46:35 GMT
>>>>> ETag: "1389dc-58b605823fa6e"
>>>>> Cache-Control: public, max-age=2592000
>>>>> Content-Length: 1280476
>>>>> Age: 4248100
>>>>
>>>> FWIW: The object is 4'248'100 seconds old. The object max-age is
>>>> 2'592'000 seconds. Your Squid is probably using an internal max-age of
>>>> 259'200 seconds, so Squid will require cache hit revalidation during
>>>> subsequent transactions after Squid caches the object (if it caches
>>>> it).
>>>
>>> That makes sense. These packages never really change at all -- the
>>> package version is part of the URI so if it's updated the package URI
>>> changes rather than the data at the old URI. I might set a longer max
>>> age in the config once this is worked out.
>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>>
>>>>
>>>
>>> Thanks.
>>



From zrm at trustiosity.com  Wed Apr  8 17:01:25 2020
From: zrm at trustiosity.com (zrm)
Date: Wed, 8 Apr 2020 13:01:25 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <7d189ee7-af59-337d-6ac8-2cc937692d2f@measurement-factory.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
 <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
 <b88e4265-916d-ecea-31c6-225449df37a1@trustiosity.com>
 <7d189ee7-af59-337d-6ac8-2cc937692d2f@measurement-factory.com>
Message-ID: <94789648-3497-9b45-04fd-0aa9e419f0f0@trustiosity.com>

On 4/8/20 10:46, Alex Rousskov wrote:
> On 4/7/20 8:48 PM, zrm wrote:
> 
>> https://www.trustiosity.com/squid/cache-debug.log.xz
> 
> I found the reason for the difference.
> 
> After the destination IP address of your apt requests fails Host header
> validation, Squid marks the request as "not cachable":

I checked the DNS query apt is making to see why it's different. It's 
making a SRV query for _http._tcp.deb.debian.org and then using the IP 
address of the name (prod.debian.map.fastly.net) returned in the SRV 
query. By contrast, squid does the A record query for deb.debian.org and 
gets a CNAME for debian.map.fastly.net. Almost the same, but since it's 
a CDN with many IP addresses, enough different that they happen to not 
both return the same address and then validation fails.

Meanwhile wget does the same A record query as squid and gets the same 
address.

The question then becomes what to do about it. Maybe if squid fails the 
validation for the A query, it should try the SRV query and accept the 
address as valid if it matches that. Another possibility would be a 
config option to have squid completely ignore the address the client 
used and always use the address it gets by doing its own DNS query for 
the host, in which case the result would be safe to cache.

But these are obviously changes requiring a new version of squid. Is 
there any way to make it work without that?


From uhlar at fantomas.sk  Wed Apr  8 17:13:39 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 8 Apr 2020 19:13:39 +0200
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <94789648-3497-9b45-04fd-0aa9e419f0f0@trustiosity.com>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
 <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
 <b88e4265-916d-ecea-31c6-225449df37a1@trustiosity.com>
 <7d189ee7-af59-337d-6ac8-2cc937692d2f@measurement-factory.com>
 <94789648-3497-9b45-04fd-0aa9e419f0f0@trustiosity.com>
Message-ID: <20200408171339.GA23153@fantomas.sk>

>>On 4/7/20 8:48 PM, zrm wrote:
>>>https://www.trustiosity.com/squid/cache-debug.log.xz

>On 4/8/20 10:46, Alex Rousskov wrote:
>>I found the reason for the difference.
>>
>>After the destination IP address of your apt requests fails Host header
>>validation, Squid marks the request as "not cachable":

On 08.04.20 13:01, zrm wrote:
>I checked the DNS query apt is making to see why it's different. It's 
>making a SRV query for _http._tcp.deb.debian.org and then using the IP 
>address of the name (prod.debian.map.fastly.net) returned in the SRV 
>query. By contrast, squid does the A record query for deb.debian.org 
>and gets a CNAME for debian.map.fastly.net. Almost the same, but since 
>it's a CDN with many IP addresses, enough different that they happen 
>to not both return the same address and then validation fails.
>
>Meanwhile wget does the same A record query as squid and gets the same 
>address.
>
>The question then becomes what to do about it. Maybe if squid fails 
>the validation for the A query, it should try the SRV query and accept 
>the address as valid if it matches that. Another possibility would be 
>a config option to have squid completely ignore the address the client 
>used and always use the address it gets by doing its own DNS query for 
>the host, in which case the result would be safe to cache.
>
>But these are obviously changes requiring a new version of squid. Is 
>there any way to make it work without that?

I'd contact debian.org DNS masters. I believe CDN wasn't designedto cause this
kind of issues.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Despite the cost of living, have you noticed how popular it remains?


From zrm at trustiosity.com  Fri Apr 10 17:54:04 2020
From: zrm at trustiosity.com (zrm)
Date: Fri, 10 Apr 2020 13:54:04 -0400
Subject: [squid-users] Squid transparent not caching apt requests from
 deb.debian.org
In-Reply-To: <20200408171339.GA23153@fantomas.sk>
References: <a177d89a-e7cc-173b-5b0d-12544ac693d5@trustiosity.com>
 <c3c1263b-8616-a1f2-53ad-63f32f2ba0d7@measurement-factory.com>
 <c0b06351-863e-29da-85c3-a3c2383b1ec4@trustiosity.com>
 <841e139c-6795-1275-6f31-9746f1bb36c5@measurement-factory.com>
 <66a94023-5f8b-fc79-254e-066ca390a085@trustiosity.com>
 <b94c87ab-ceed-c9e4-6aaf-1c0e80aa7839@measurement-factory.com>
 <b88e4265-916d-ecea-31c6-225449df37a1@trustiosity.com>
 <7d189ee7-af59-337d-6ac8-2cc937692d2f@measurement-factory.com>
 <94789648-3497-9b45-04fd-0aa9e419f0f0@trustiosity.com>
 <20200408171339.GA23153@fantomas.sk>
Message-ID: <651bc475-bbaf-4c79-e975-5642502d9807@trustiosity.com>

On 4/8/20 13:13, Matus UHLAR - fantomas wrote:
> On 08.04.20 13:01, zrm wrote:
>> I checked the DNS query apt is making to see why it's different. It's 
>> making a SRV query for _http._tcp.deb.debian.org and then using the IP 
>> address of the name (prod.debian.map.fastly.net) returned in the SRV 
>> query. By contrast, squid does the A record query for deb.debian.org 
>> and gets a CNAME for debian.map.fastly.net. Almost the same, but since 
>> it's a CDN with many IP addresses, enough different that they happen 
>> to not both return the same address and then validation fails.
>>
>> Meanwhile wget does the same A record query as squid and gets the same 
>> address.
>>
>> The question then becomes what to do about it. Maybe if squid fails 
>> the validation for the A query, it should try the SRV query and accept 
>> the address as valid if it matches that. Another possibility would be 
>> a config option to have squid completely ignore the address the client 
>> used and always use the address it gets by doing its own DNS query for 
>> the host, in which case the result would be safe to cache.
>>
>> But these are obviously changes requiring a new version of squid. Is 
>> there any way to make it work without that?
> 
> I'd contact debian.org DNS masters. I believe CDN wasn't designedto 
> cause this
> kind of issues.
> 

It seems to be like this on purpose:

https://deb.debian.org/

I'm not sure there is actually anything wrong with their DNS 
configuration, it's not required for the SRV record to point to the same 
name/address as the A record (which would in fact defeat the point of 
the SRV record).

It *might* be a bug in apt, since if the SRV record says to use 
prod.debian.map.fastly.net then maybe it ought to be using that as 
"Host: " in the HTTP headers, but I'm not sure about that. If it's 
supposed to use the original domain in the HTTP headers when using a SRV 
record then this could still be a failing of squid for not checking and 
considering valid the address from the HTTP SRV record.

I'll send them an email and see what they have to say about it.


From chris.bidwell at noaa.gov  Mon Apr 13 19:19:04 2020
From: chris.bidwell at noaa.gov (Chris Bidwell - NOAA Federal)
Date: Mon, 13 Apr 2020 13:19:04 -0600
Subject: [squid-users] Setting up proxy with private to public
Message-ID: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>

Hi all,

Very new to squid and am looking to setup several internal subnets to
access external network (internet) through squid on a separate interface.

Server has two IP's.  One private internal and one public.  Can someone
point me in the right direction to get this setup?  Running RHEL7.

Do I need to create static routes?  Do I need firewalld rules in place?

Thanks!

Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200413/da2ec3dc/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Apr 13 21:37:05 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 13 Apr 2020 23:37:05 +0200
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
Message-ID: <202004132337.05953.Antony.Stone@squid.open.source.it>

On Monday 13 April 2020 at 21:19:04, Chris Bidwell - NOAA Federal wrote:

> Hi all,
> 
> Very new to squid and am looking to setup several internal subnets to
> access external network (internet) through squid on a separate interface.

What are you trying to achieve by using Squid?  What is your objective, 
compared to giving clients direct access to the Internet?

> Server has two IP's.  One private internal and one public.  Can someone
> point me in the right direction to get this setup?  Running RHEL7.

Firstly, install Squid and look at its configuration file.  It is *very* well 
commented / documented, and there is *very* little you need to change in order 
to get it working on your network.

For more details, see:

https://wiki.squid-cache.org/SquidFaq/BinaryPackages

https://wiki.squid-cache.org/SquidFaq/ConfiguringSquid
https://wiki.squid-cache.org/SquidFaq
https://wiki.squid-cache.org/ConfigExamples

https://www.packtpub.com/squid-proxy-server-31-beginners-guide/book
http://www.oreilly.com/catalog/squid/

(All the above available from http://www.squid-cache.org )


> Do I need to create static routes?

Provided the machine you want install Squid on can reach (a) arbitrary web 
servers on the Internet, and (b) the client machines on your internal 
networks, then no.

If not, then yes, you will need to add suitable routes so that the Squid 
server can find both origin servers and clients.

> Do I need firewalld rules in place?

A firewall is always a good idea, however Squid imposes no special requirement 
of its own here.

A very good starting point for firewalls is "allow the traffic you know you want, 
block the traffic you know you do not want, and log and block the traffic you're 
not sure about - then look at the logs and adjust the rules as necessary to 
keep the log entries minimal".


Finally, if you run into problems, come back here and tell us:

 - what you want to achieve
 - what you did to try to achieve it
 - how you tested whether it worked
 - what you found which told you it didn't work

Basically, give us enough information to understand what you're trying to do, 
what you've done to get there, and what went wrong (such that we could 
reproduce the problem for ourselves if need be), and people here will happily 
help out.


Regards,


Antony.

-- 
Pavlov is in the pub enjoying a pint.
The barman rings for last orders, and Pavlov jumps up exclaiming "Damn!  I 
forgot to feed the dog!"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chris.bidwell at noaa.gov  Mon Apr 13 21:46:46 2020
From: chris.bidwell at noaa.gov (Chris Bidwell - NOAA Federal)
Date: Mon, 13 Apr 2020 15:46:46 -0600
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <202004132337.05953.Antony.Stone@squid.open.source.it>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
 <202004132337.05953.Antony.Stone@squid.open.source.it>
Message-ID: <CAHKi8ChiBm4V4XhmohsvSYHx8psOxsN_qQX=8+Y=0em3Ea+VFg@mail.gmail.com>

Sure.  So we have a few internal networks that aren't meant to have direct
internet access without access through a proxy so that it can be better
regulated and monitored.

That being said, we've previously used a microsoft product that is EOL and
I thought I could do much of what it's wanting with Linux and squid and
nginx for reverse proxy.

We've got several internal subnets that need to be able to talk through
squid (I've chosen tcp/8080) to connect to from internally and want to
translate that to an external IP address that does have access to the
outside world.  Using the acl's that squid provides and allowing for
various ports (80/443, etc) I'd like to use this functionality.

Once again, the squid server has two IP addresses.  One internal, and one
external.  The outbound traffic would be accessible through that external
ip.

I hope I'm making *some* sense.  :)

Thanks

On Mon, Apr 13, 2020 at 3:38 PM Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Monday 13 April 2020 at 21:19:04, Chris Bidwell - NOAA Federal wrote:
>
> > Hi all,
> >
> > Very new to squid and am looking to setup several internal subnets to
> > access external network (internet) through squid on a separate interface.
>
> What are you trying to achieve by using Squid?  What is your objective,
> compared to giving clients direct access to the Internet?
>
> > Server has two IP's.  One private internal and one public.  Can someone
> > point me in the right direction to get this setup?  Running RHEL7.
>
> Firstly, install Squid and look at its configuration file.  It is *very*
> well
> commented / documented, and there is *very* little you need to change in
> order
> to get it working on your network.
>
> For more details, see:
>
> https://wiki.squid-cache.org/SquidFaq/BinaryPackages
>
> https://wiki.squid-cache.org/SquidFaq/ConfiguringSquid
> https://wiki.squid-cache.org/SquidFaq
> https://wiki.squid-cache.org/ConfigExamples
>
> https://www.packtpub.com/squid-proxy-server-31-beginners-guide/book
> http://www.oreilly.com/catalog/squid/
>
> (All the above available from http://www.squid-cache.org )
>
>
> > Do I need to create static routes?
>
> Provided the machine you want install Squid on can reach (a) arbitrary web
> servers on the Internet, and (b) the client machines on your internal
> networks, then no.
>
> If not, then yes, you will need to add suitable routes so that the Squid
> server can find both origin servers and clients.
>
> > Do I need firewalld rules in place?
>
> A firewall is always a good idea, however Squid imposes no special
> requirement
> of its own here.
>
> A very good starting point for firewalls is "allow the traffic you know
> you want,
> block the traffic you know you do not want, and log and block the traffic
> you're
> not sure about - then look at the logs and adjust the rules as necessary
> to
> keep the log entries minimal".
>
>
> Finally, if you run into problems, come back here and tell us:
>
>  - what you want to achieve
>  - what you did to try to achieve it
>  - how you tested whether it worked
>  - what you found which told you it didn't work
>
> Basically, give us enough information to understand what you're trying to
> do,
> what you've done to get there, and what went wrong (such that we could
> reproduce the problem for ourselves if need be), and people here will
> happily
> help out.
>
>
> Regards,
>
>
> Antony.
>
> --
> Pavlov is in the pub enjoying a pint.
> The barman rings for last orders, and Pavlov jumps up exclaiming "Damn!  I
> forgot to feed the dog!"
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

Chris Bidwell, CISSP
Space Weather Prediction Center
National Oceanic Atmospheric Administration
email: c <cbidwell at usgs.gov>hris.bidwell at noaa.gov
office: 303-497-3204
mobile: 720-496-3126
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200413/cf244a8e/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Apr 13 22:10:43 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 14 Apr 2020 00:10:43 +0200
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <CAHKi8ChiBm4V4XhmohsvSYHx8psOxsN_qQX=8+Y=0em3Ea+VFg@mail.gmail.com>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
 <202004132337.05953.Antony.Stone@squid.open.source.it>
 <CAHKi8ChiBm4V4XhmohsvSYHx8psOxsN_qQX=8+Y=0em3Ea+VFg@mail.gmail.com>
Message-ID: <202004140010.43614.Antony.Stone@squid.open.source.it>

On Monday 13 April 2020 at 23:46:46, Chris Bidwell - NOAA Federal wrote:

> Sure.  So we have a few internal networks that aren't meant to have direct
> internet access without access through a proxy so that it can be better
> regulated and monitored.

Okay, that's a useful starting point.

> We've got several internal subnets that need to be able to talk through
> squid (I've chosen tcp/8080) to connect to from internally and want to
> translate that to an external IP address that does have access to the
> outside world.

That sounds perfectly straightforward, provided your Squid server has routing 
to connect back to those internal networks.

> Once again, the squid server has two IP addresses.  One internal, and one
> external.  The outbound traffic would be accessible through that external
> ip.

So, you configure your internal clients to connect to the internal address of 
the Squid machine, and tell them that the proxy is listening on port 8080.

Add the subnet definitions (if they are not 10.0.0.0/8, 172.16.0.0/12 or 
192.168.0.0/16) to Squid's configuration file.  If you *are* using such RFC1918 
addresses, these are automatically supported by Squid and you do not need to 
configure for your internal network ranges.

You don't need to do anything special to get Squid to use its external address 
for the connections out to the Internet - that's handled by the Linux 
networking stack.

> I hope I'm making *some* sense.  :)

I think so.

My suggestion from here on is: install Squid, configure a test client to use 
it, and see if it works.

If not, give us enough information to understand what you've done (both the 
setup and the testing) so we could reproduce it for ourselves, and we'll try 
to help further.


Best wishes,


Antony.

-- 
Warum k?nnen Seer?uber nicht den Umfang eines Kreises berechnen?
Weil sie Piraten...


                                                   Please reply to the list;
                                                         please *don't* CC me.


From uhlar at fantomas.sk  Tue Apr 14 11:27:37 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 14 Apr 2020 13:27:37 +0200
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
Message-ID: <20200414112737.GA11768@fantomas.sk>

On 13.04.20 13:19, Chris Bidwell - NOAA Federal wrote:
>Very new to squid and am looking to setup several internal subnets to
>access external network (internet) through squid on a separate interface.

squid does not use interfaces, squid uses IP addresses.
interfaces are up to underlying OS.

>Server has two IP's.  One private internal and one public.  Can someone
>point me in the right direction to get this setup?  Running RHEL7.

this way all internal clients must connect to SQUID's internal IP and squid
will connect to the net using extenral IP.

>Do I need to create static routes?

maybe, however this is unrelated to squid

> Do I need firewalld rules in place?

no, unless you want to use HTTP interception.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Windows found: (R)emove, (E)rase, (D)elete


From chris.bidwell at noaa.gov  Tue Apr 14 14:03:19 2020
From: chris.bidwell at noaa.gov (Chris Bidwell - NOAA Federal)
Date: Tue, 14 Apr 2020 08:03:19 -0600
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <20200414112737.GA11768@fantomas.sk>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
 <20200414112737.GA11768@fantomas.sk>
Message-ID: <CAHKi8Chwm4rXRXM03jNRVZ=HOWTXL3Q7PxZPMwffJncG9V_VwQ@mail.gmail.com>

Okay, so I think I'm starting to get somewhere but the connection isn't
completing.  I can see the connection come through my firewall, but the
handshake doesn't appear to be happening.

My squid access log is saying:  TCP_MISS/503.

On Tue, Apr 14, 2020 at 5:28 AM Matus UHLAR - fantomas <uhlar at fantomas.sk>
wrote:

> On 13.04.20 13:19, Chris Bidwell - NOAA Federal wrote:
> >Very new to squid and am looking to setup several internal subnets to
> >access external network (internet) through squid on a separate interface.
>
> squid does not use interfaces, squid uses IP addresses.
> interfaces are up to underlying OS.
>
> >Server has two IP's.  One private internal and one public.  Can someone
> >point me in the right direction to get this setup?  Running RHEL7.
>
> this way all internal clients must connect to SQUID's internal IP and squid
> will connect to the net using extenral IP.
>
> >Do I need to create static routes?
>
> maybe, however this is unrelated to squid
>
> > Do I need firewalld rules in place?
>
> no, unless you want to use HTTP interception.
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Windows found: (R)emove, (E)rase, (D)elete
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200414/c85a952a/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Apr 14 14:31:45 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 14 Apr 2020 16:31:45 +0200
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <CAHKi8Chwm4rXRXM03jNRVZ=HOWTXL3Q7PxZPMwffJncG9V_VwQ@mail.gmail.com>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
 <20200414112737.GA11768@fantomas.sk>
 <CAHKi8Chwm4rXRXM03jNRVZ=HOWTXL3Q7PxZPMwffJncG9V_VwQ@mail.gmail.com>
Message-ID: <202004141631.45983.Antony.Stone@squid.open.source.it>

On Tuesday 14 April 2020 at 16:03:19, Chris Bidwell - NOAA Federal wrote:

> Okay, so I think I'm starting to get somewhere but the connection isn't
> completing. I can see the connection come through my firewall, but the
> handshake doesn't appear to be happening.

Tell us more about your network setup.  Is the firewall between the clients and 
Squid, between Squid and the Internet, or do you have both?

Can you do a simple Ping test from a client machine to the Squid server (and 
get replies)?

Can you do the same from the Squid server to some Internet-based web server 
(making sure it's one which replies to pings - some machines are badly 
configured and don't do this).

> My squid access log is saying:  TCP_MISS/503.

I'm sure it says a lot more than that, but at least it's an indication that 
your client is getting the request through to Squid okay.

Assuming the Ping test from Squid to an Internet web server works, what 
happens if you try wget, lynx, curl or even telnet to port 80, from the Squid 
server to some external web server?  Does it indicate that the Squid server 
has "Internet access"?


Antony.

-- 
Programming is a Dark Art, and it will always be. The programmer is
fighting against the two most destructive forces in the universe:
entropy and human stupidity. They're not things you can always
overcome with a "methodology" or on a schedule.

 - Damian Conway, Perl God

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chris.bidwell at noaa.gov  Wed Apr 15 13:08:36 2020
From: chris.bidwell at noaa.gov (Chris Bidwell - NOAA Federal)
Date: Wed, 15 Apr 2020 07:08:36 -0600
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <202004141631.45983.Antony.Stone@squid.open.source.it>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
 <20200414112737.GA11768@fantomas.sk>
 <CAHKi8Chwm4rXRXM03jNRVZ=HOWTXL3Q7PxZPMwffJncG9V_VwQ@mail.gmail.com>
 <202004141631.45983.Antony.Stone@squid.open.source.it>
Message-ID: <CAHKi8Ci+VsPWRxdGj-i1bJOk09QRKzpSoSiRYJ7f89Zk=2cdiQ@mail.gmail.com>

Sure thing.

On Tue, Apr 14, 2020 at 8:32 AM Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 14 April 2020 at 16:03:19, Chris Bidwell - NOAA Federal wrote:
>
> > Okay, so I think I'm starting to get somewhere but the connection isn't
> > completing. I can see the connection come through my firewall, but the
> > handshake doesn't appear to be happening.
>
> Tell us more about your network setup.  Is the firewall between the
> clients and
> Squid, between Squid and the Internet, or do you have both?
>

There is a firewall between my internal clients and squid.  There is a
firewall rule allowing tcp/8080 from my clients to the squid server.  And
from the squid server, it is allowed to the internet.

>
> Can you do a simple Ping test from a client machine to the Squid server
> (and
> get replies)?
>
> Can you do the same from the Squid server to some Internet-based web
> server
> (making sure it's one which replies to pings - some machines are badly
> configured and don't do this).
>
> > My squid access log is saying:  TCP_MISS/503.
>
> I'm sure it says a lot more than that, but at least it's an indication
> that
> your client is getting the request through to Squid okay.
>

Here is the full output of my access.log:
1586873819.383      0 192.168.226.241 TAG_NONE/409 4108 CONNECT
www.nginx.com:443 - HIER_NONE/- text/html

>
> Assuming the Ping test from Squid to an Internet web server works, what
> happens if you try wget, lynx, curl or even telnet to port 80, from the
> Squid
> server to some external web server?  Does it indicate that the Squid
> server
> has "Internet access"?
>
>
> Antony.
>

So after looking further.  It looks like when I'm trying to wget from my
squid server, which has the two nics (internal and public), it's trying to
send it through the internal
connection.  It doesn't seem to want to route through the external nic.

>
> --
> Programming is a Dark Art, and it will always be. The programmer is
> fighting against the two most destructive forces in the universe:
> entropy and human stupidity. They're not things you can always
> overcome with a "methodology" or on a schedule.
>
>  - Damian Conway, Perl God
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200415/2c343053/attachment.htm>

From bigmyx at gmail.com  Thu Apr 16 01:23:00 2020
From: bigmyx at gmail.com (Michael Leikind)
Date: Wed, 15 Apr 2020 18:23:00 -0700
Subject: [squid-users] Squid proxy configuration for client SSL termination
Message-ID: <CA+2-1o8t_2wekmhBEpab0j6evcTjEWDNrneCS_FMEqkjqnnLYA@mail.gmail.com>

Greetings to the Squid community!

I would like to get the recommendation on how to configure Squid (latest
version) with client SSL termination.

The requirement is to provide proxy access to the internet for the client
who has no ability to install a custom CA certificate.

Following the documentation here
<https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection>,
it is possible to use HTTPS for the browser-proxy connection the same way
as HTTP.

However, the only way to achieve that
<https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit> is
to use SSL Interception with self-signed CA certificate, which cannot work
in my case.

Can someone please advise?

Thank you!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200415/54c6dea2/attachment.htm>

From squid3 at treenet.co.nz  Thu Apr 16 05:06:02 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2020 17:06:02 +1200
Subject: [squid-users] Squid proxy configuration for client SSL
 termination
In-Reply-To: <CA+2-1o8t_2wekmhBEpab0j6evcTjEWDNrneCS_FMEqkjqnnLYA@mail.gmail.com>
References: <CA+2-1o8t_2wekmhBEpab0j6evcTjEWDNrneCS_FMEqkjqnnLYA@mail.gmail.com>
Message-ID: <a5b28cbe-fbde-ea98-7949-f67ad4acdee4@treenet.co.nz>

On 16/04/20 1:23 pm, Michael Leikind wrote:
> Greetings to the Squid community!
> 
> I would like to get the recommendation on how to configure Squid (latest
> version) with client SSL termination.
> 
> The requirement is to provide proxy access to the internet for the
> client who has no ability to install a custom CA certificate.
> 
> Following the documentation?here
> <https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection>,
> it is possible to use HTTPS for the browser-proxy connection the same
> way as HTTP.
> 
> However, the only way to?achieve that
> <https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit>?is
> to use SSL Interception with self-signed CA certificate, which cannot
> work in my case.
> 
> Can someone please advise?
> 

Clients *always* need a CA to trust TLS connections.

But, there are two types of "client termination".  Only intercepted
traffic requires the CA private keys to be on the proxy - which is where
the custom CA installation comes from.


A TLS explicit proxy using TLS to receive traffic (HTTP, HTTPS and
other) can use a normal server certificate signed by a global CA the
clients *may* already trust.


Amos


From csp.shubham at gmail.com  Thu Apr 16 05:15:00 2020
From: csp.shubham at gmail.com (shubham jain)
Date: Thu, 16 Apr 2020 10:45:00 +0530
Subject: [squid-users] Header Detection Post SSL Bump in Squid 4.10
Message-ID: <CAB4jwzffxXgP_jqoJHozw1v98Sb7CsRXZ19s6s-e_xtmfUmZZA@mail.gmail.com>

Hi,

*Context*:
I want to use Squid as a forward proxy, where I want to
1) send all the Image requests directly, presumably using request header
'accept'
2) send all other requests through a cache peer Proxy service

The req_header directive is working fine for HTTP Requests, but not for
HTTPS.

I've done the setup for SSL Bump in here and that's giving decrypted HTTPS
requests in the access.log as well.

*Issue:*
The req_header directive is not working on the decrypted HTTPS requests.

*Squid.conf*

# SSL Bump Port
http_port 127.0.0.1:3128 ssl-bump cert=/usr/local/etc/cert/example.com.cert
key=/usr/local/etc/cert/example.com.private generate-host-certificates=on
version=1 options=SINGLE_DH_USE

# SSL Bump Config
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

acl imageIsBlocked req_header accept -i image

ssl_bump terminate imageIsBlocked    #terminate is just for testing, to be
replaced by splice
ssl_bump bump all

*Access.log*

1587011751.217    204 127.0.0.1 TCP_MISS/200 393 GET
https://dt.adsafeprotected.com/dt? - HIER_DIRECT/104.244.39.20 image/gif
1587011751.264   1050 127.0.0.1 NONE/200 0 CONNECT
pagead2.googlesyndication.com:443 - HIER_DIRECT/172.217.13.226 -
1587011751.303    787 127.0.0.1 NONE/200 0 CONNECT
pagead2.googlesyndication.com:443 - HIER_DIRECT/172.217.13.226 -
1587011752.246   2846 127.0.0.1 NONE/200 0 CONNECT
partners.tremorhub.com:443 - HIER_DIRECT/3.224.28.212 -
1587011753.348   1096 127.0.0.1 TCP_MISS/200 1105 GET
https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/3.224.28.212 text/xml
1587011754.152    799 127.0.0.1 TCP_MISS/200 1124 GET
https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/3.224.28.212 text/xml
1587011756.091   1934 127.0.0.1 TCP_MISS/200 1086 GET
https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/3.224.28.212 text/xml
1587011760.264   4169 127.0.0.1 TCP_MISS_ABORTED/200 1113 GET
https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/3.224.28.212 text/xml
1587011760.822    367 127.0.0.1 TCP_MISS/200 1185 POST
https://pagead2.googlesyndication.com/pcs/activeview? - HIER_DIRECT/
172.217.13.226 image/gif
1587011760.862    407 127.0.0.1 TCP_MISS/200 1185 GET
https://pagead2.googlesyndication.com/pcs/activeview? - HIER_DIRECT/
172.217.13.226 image/gif

Any help would be appreciated, as I have spent weeks trying to get around
the work post SSL Bumping.

*Thanks & Regards,*

*Shubham Jain*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200416/350dc071/attachment.htm>

From squid3 at treenet.co.nz  Thu Apr 16 06:02:34 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Apr 2020 18:02:34 +1200
Subject: [squid-users] Header Detection Post SSL Bump in Squid 4.10
In-Reply-To: <CAB4jwzffxXgP_jqoJHozw1v98Sb7CsRXZ19s6s-e_xtmfUmZZA@mail.gmail.com>
References: <CAB4jwzffxXgP_jqoJHozw1v98Sb7CsRXZ19s6s-e_xtmfUmZZA@mail.gmail.com>
Message-ID: <ff477dc4-37da-d68b-32ef-efc98e7cef5e@treenet.co.nz>

On 16/04/20 5:15 pm, shubham jain wrote:
> Hi,
> 
> *Context*:
> I want to use Squid as a forward proxy, where I want to
> 1) send all the Image requests directly, presumably using request header
> 'accept'
> 2) send all other requests through a cache peer Proxy service
> 
> The req_header directive is working fine for HTTP Requests, but not for
> HTTPS.
> 
> I've done the setup for SSL Bump in here and that's giving decrypted
> HTTPS requests in the access.log as well.
> 
> *Issue:*
> The req_header directive is not working on the decrypted HTTPS requests.
> 
> *Squid.conf*
> 
> # SSL Bump Port
> http_port 127.0.0.1:3128 <http://127.0.0.1:3128> ssl-bump
> cert=/usr/local/etc/cert/example.com.cert
> key=/usr/local/etc/cert/example.com.private
> generate-host-certificates=on version=1 options=SINGLE_DH_USE ?
> 
> # SSL Bump Config
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> acl imageIsBlocked req_header accept -i image
> 
> ssl_bump terminate imageIsBlocked? ? #terminate is just for testing, to
> be replaced by splice
> ssl_bump bump all


Do the CONNECT tunnels Accept headers contain "image" ?

ssl_bump decides what to do during the TLS handshake process. For your
setup that is only the CONNECT requests.

Once decrypted HTTPS is just HTTP with https:// URLs schemes. It is
controlled by http_access and does not pass through ssl_bump rules again.


Amos


From Antony.Stone at squid.open.source.it  Thu Apr 16 08:44:27 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 16 Apr 2020 10:44:27 +0200
Subject: [squid-users] Setting up proxy with private to public
In-Reply-To: <CAHKi8Ci+VsPWRxdGj-i1bJOk09QRKzpSoSiRYJ7f89Zk=2cdiQ@mail.gmail.com>
References: <CAHKi8Cjwt30TQMvSa3tpToy7GLypsiu1KWLjJArvAAcGbJ9VAA@mail.gmail.com>
 <202004141631.45983.Antony.Stone@squid.open.source.it>
 <CAHKi8Ci+VsPWRxdGj-i1bJOk09QRKzpSoSiRYJ7f89Zk=2cdiQ@mail.gmail.com>
Message-ID: <202004161044.27550.Antony.Stone@squid.open.source.it>

Sorry, replying to the list this time - for some reason my previous reply went 
to your private address.


On Wednesday 15 April 2020 at 15:08:36, Chris Bidwell - NOAA Federal wrote:

> So after looking further.  It looks like when I'm trying to wget from my
> squid server, which has the two nics (internal and public), it's trying to
> send it through the internal connection.  It doesn't seem to want to route
> through the external nic.

Okay, so not currently a Squid problem, then.

What does "route -n" tell you, and what do you think your default gateway 
address to the Internet should be (ie: what's the address of the router which 
you think Squid should be using from its external interface to get to the 
Internet)?


Antony.

-- 
Python is executable pseudocode.
Perl is executable line noise.

                                                   Please reply to the list;
                                                         please don't CC me.


From rentorbuy at yahoo.com  Thu Apr 16 09:09:53 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Apr 2020 09:09:53 +0000 (UTC)
Subject: [squid-users] dynamic ACLs
References: <960995098.1458217.1587028193077.ref@mail.yahoo.com>
Message-ID: <960995098.1458217.1587028193077@mail.yahoo.com>

Hi,

In sslbump tproxy "mode" one cannot authenticate user to limit/allow their access to web content.

I was thinking however of making a web form with auth within a custom Squid error page. This way a user would "automatically" whitelist a web site and have access to it while the IT dep. would know which user accessed where despite the site being blacklisted.

>From the error page I can tell which ACL is blocking that site so I could create an "exception" ACL for that ACL.
My question is: can this whitelist or graylist ACL be dynamic without needing to reload Squid, a bit like ipsets with iptables/nftables without the need to reload rules?

Vieri


From tarotapprentice at yahoo.com  Fri Apr 17 13:32:38 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Fri, 17 Apr 2020 23:32:38 +1000
Subject: [squid-users] Confirmation page not working
References: <53104F02-F5CC-45F0-AA88-9B9D3625326C.ref@yahoo.com>
Message-ID: <53104F02-F5CC-45F0-AA88-9B9D3625326C@yahoo.com>

Trying to visit the confirmation page at http://lists.squid-cache.org/confirm/squid-users/ but it doesn?t seem to be responding. I?ve tried over a couple of days.

MarkJ 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200417/d284c10b/attachment.htm>

From rousskov at measurement-factory.com  Thu Apr 16 12:35:34 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Apr 2020 08:35:34 -0400
Subject: [squid-users] dynamic ACLs
In-Reply-To: <960995098.1458217.1587028193077@mail.yahoo.com>
References: <960995098.1458217.1587028193077.ref@mail.yahoo.com>
 <960995098.1458217.1587028193077@mail.yahoo.com>
Message-ID: <b935d785-b2de-5546-6b71-8b7ebb9e6c42@measurement-factory.com>

On 4/16/20 5:09 AM, Vieri wrote:
> In sslbump tproxy "mode" one cannot authenticate user to limit/allow their access to web content.
> 
> I was thinking however of making a web form with auth within a custom Squid error page. This way a user would "automatically" whitelist a web site and have access to it while the IT dep. would know which user accessed where despite the site being blacklisted.
> 
> From the error page I can tell which ACL is blocking that site so I could create an "exception" ACL for that ACL.
> My question is: can this whitelist or graylist ACL be dynamic without needing to reload Squid, a bit like ipsets with iptables/nftables without the need to reload rules?

Yes, there are several ways to change Squid decisions without
reconfiguring Squid. The simplest one is the "external acl" mechanism:
http://www.squid-cache.org/Doc/config/external_acl_type/
 Alex.


From squid3 at treenet.co.nz  Thu Apr 16 12:43:45 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Apr 2020 00:43:45 +1200
Subject: [squid-users] dynamic ACLs
In-Reply-To: <960995098.1458217.1587028193077@mail.yahoo.com>
References: <960995098.1458217.1587028193077.ref@mail.yahoo.com>
 <960995098.1458217.1587028193077@mail.yahoo.com>
Message-ID: <f76bd8f7-e588-b22f-b15a-4e68418b299f@treenet.co.nz>

On 16/04/20 9:09 pm, Vieri wrote:
> Hi,
> 
> In sslbump tproxy "mode" one cannot authenticate user to limit/allow their access to web content.
> 
> I was thinking however of making a web form with auth within a custom Squid error page. This way a user would "automatically" whitelist a web site and have access to it while the IT dep. would know which user accessed where despite the site being blacklisted.
> 
> From the error page I can tell which ACL is blocking that site so I could create an "exception" ACL for that ACL.
> My question is: can this whitelist or graylist ACL be dynamic without needing to reload Squid, a bit like ipsets with iptables/nftables without the need to reload rules?
> 


Squid comes with an external ACL helper that authorizes access based on
DB entries. You can use any system you like to manage the DB entries.
 see
<http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html>


Amos


From Antony.Stone at squid.open.source.it  Fri Apr 17 15:08:16 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 17 Apr 2020 17:08:16 +0200
Subject: [squid-users] Confirmation page not working
In-Reply-To: <53104F02-F5CC-45F0-AA88-9B9D3625326C@yahoo.com>
References: <53104F02-F5CC-45F0-AA88-9B9D3625326C.ref@yahoo.com>
 <53104F02-F5CC-45F0-AA88-9B9D3625326C@yahoo.com>
Message-ID: <202004171708.16978.Antony.Stone@squid.open.source.it>

On Friday 17 April 2020 at 15:32:38, TarotApprentice wrote:

> Trying to visit the confirmation page at
> http://lists.squid-cache.org/confirm/squid-users/ but it doesn?t seem to
> be responding. I?ve tried over a couple of days.

When you say "not responding", do you mean you get no page content shown in 
your browser, or do you mean that you fill in the confirmation string and click 
on 'submit' but it then doesn't accept your confirmation?

The page itself loads fine for me here, and I've only ever confirmed mailman 
subscriptions by email - just reply to the email you got asking you to visit 
the confirmation page, making sure you reply from the address you asked to 
subscribe to the list.

No need to change anything about the email you get, just do a "reply" and 
"send" exactly as it is.


Antony.

-- 
Success is a lousy teacher.  It seduces smart people into thinking they can't 
lose.

 - William H Gates III

                                                   Please reply to the list;
                                                         please *don't* CC me.


From scastellano at quadrantsec.com  Fri Apr 17 15:22:46 2020
From: scastellano at quadrantsec.com (Sam Castellano)
Date: Fri, 17 Apr 2020 11:22:46 -0400 (EDT)
Subject: [squid-users] ssl proxy and decrypted forwarding
In-Reply-To: <644827944.34461140.1587136713598.JavaMail.zimbra@quadrantsec.com>
References: <623449268.34416752.1587126557375.JavaMail.zimbra@quadrantsec.com>
 <644827944.34461140.1587136713598.JavaMail.zimbra@quadrantsec.com>
Message-ID: <1294604998.34462352.1587136966978.JavaMail.zimbra@quadrantsec.com>

Good morning, 
My question relates to ssl bumping and potentially Icap/Ecap functionality. I currently have ssl bump/ interception working and communicating with a local ICAP server. Im trying to understand the process of how the decrypted data gets sent to the ICAP server for analysis in things such as clamav etc. My goal is to have the decrypted traffic analyzed by Suricata preferably on a separate box if possible. 

Best regards 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200417/6dca72e7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2144 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200417/6dca72e7/attachment.bin>

From rousskov at measurement-factory.com  Fri Apr 17 15:49:13 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Apr 2020 11:49:13 -0400
Subject: [squid-users] ssl proxy and decrypted forwarding
In-Reply-To: <1294604998.34462352.1587136966978.JavaMail.zimbra@quadrantsec.com>
References: <623449268.34416752.1587126557375.JavaMail.zimbra@quadrantsec.com>
 <644827944.34461140.1587136713598.JavaMail.zimbra@quadrantsec.com>
 <1294604998.34462352.1587136966978.JavaMail.zimbra@quadrantsec.com>
Message-ID: <a3a5d2d0-cc3e-e06a-f79d-9f972e60b3d2@measurement-factory.com>

On 4/17/20 11:22 AM, Sam Castellano wrote:

> My question relates to ssl bumping and potentially Icap/Ecap
> functionality. I currently have ssl bump/ interception working and
> communicating with a local ICAP server. Im trying to understand the
> process of how the decrypted data gets sent to the ICAP server for
> analysis in things such as clamav etc. My goal is to have the decrypted
> traffic analyzed by Suricata preferably on a separate box if possible.??

I do not know what particular information you are looking for, but ICAP
mechanics are documented in RFC 3507 while eCAP mechanics are documented
at www.e-cap.org.

If you are worried about exposing proxied HTTP[S] messages in transit to
your ICAP service, then consider using a "Secure ICAP" service (for a
starting point, look for those two words in squid.conf.documented).

N.B. Neither ICAP nor eCAP know about SslBump. In an SslBump context,
they just get CONNECT requests and the HTTP messages decrypted by Squid.
The same is true for the majority of Squid features -- while inside
Squid, decrypted HTTP traffic is usually handled similar to plain HTTP
traffic.


HTH,

Alex.


From scastellano at quadrantsec.com  Fri Apr 17 16:00:11 2020
From: scastellano at quadrantsec.com (Sam Castellano)
Date: Fri, 17 Apr 2020 12:00:11 -0400 (EDT)
Subject: [squid-users] ssl proxy and decrypted forwarding
In-Reply-To: <a3a5d2d0-cc3e-e06a-f79d-9f972e60b3d2@measurement-factory.com>
References: <623449268.34416752.1587126557375.JavaMail.zimbra@quadrantsec.com>
 <644827944.34461140.1587136713598.JavaMail.zimbra@quadrantsec.com>
 <1294604998.34462352.1587136966978.JavaMail.zimbra@quadrantsec.com>
 <a3a5d2d0-cc3e-e06a-f79d-9f972e60b3d2@measurement-factory.com>
Message-ID: <2061590632.34473388.1587139211439.JavaMail.zimbra@quadrantsec.com>

Thank you for the swift response Alex, my main goal is to be able to use suricata or snort to analyze the decrypted https traffic/payload. Suricata/Snort is looking at the interface and naturally will only see the https messages encrypted as the squid server receives the messages encrypted and sends them out encrypted. So I am actually trying to send the proxied https messages decrypted. I hope that makes sense.... Sorry if I misunderstood your explanation and all the help is greatly appreciated so thank you ! 

Best regards- 

Sam Castellano 


----- Original Message -----
From: "Alex Rousskov" <rousskov at measurement-factory.com>
To: "Sam Castellano" <scastellano at quadrantsec.com>, "squid-users" <squid-users at lists.squid-cache.org>
Sent: Friday, April 17, 2020 11:49:13 AM
Subject: Re: [squid-users] ssl proxy and decrypted forwarding

On 4/17/20 11:22 AM, Sam Castellano wrote:

> My question relates to ssl bumping and potentially Icap/Ecap
> functionality. I currently have ssl bump/ interception working and
> communicating with a local ICAP server. Im trying to understand the
> process of how the decrypted data gets sent to the ICAP server for
> analysis in things such as clamav etc. My goal is to have the decrypted
> traffic analyzed by Suricata preferably on a separate box if possible.??

I do not know what particular information you are looking for, but ICAP
mechanics are documented in RFC 3507 while eCAP mechanics are documented
at www.e-cap.org.

If you are worried about exposing proxied HTTP[S] messages in transit to
your ICAP service, then consider using a "Secure ICAP" service (for a
starting point, look for those two words in squid.conf.documented).

N.B. Neither ICAP nor eCAP know about SslBump. In an SslBump context,
they just get CONNECT requests and the HTTP messages decrypted by Squid.
The same is true for the majority of Squid features -- while inside
Squid, decrypted HTTP traffic is usually handled similar to plain HTTP
traffic.


HTH,

Alex.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2144 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200417/d40211b6/attachment.bin>

From gkinkie at gmail.com  Fri Apr 17 16:36:33 2020
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Fri, 17 Apr 2020 17:36:33 +0100
Subject: [squid-users] Confirmation page not working
In-Reply-To: <202004171708.16978.Antony.Stone@squid.open.source.it>
References: <53104F02-F5CC-45F0-AA88-9B9D3625326C.ref@yahoo.com>
 <53104F02-F5CC-45F0-AA88-9B9D3625326C@yahoo.com>
 <202004171708.16978.Antony.Stone@squid.open.source.it>
Message-ID: <CA+Y8hcNRj557QsjWHDpJwk_4XCbi4aYZy7iFdFTaw6O8eaECgw@mail.gmail.com>

Hi,
  there was a problem on the server, now fixed.
Apologies

On Fri, Apr 17, 2020 at 4:08 PM Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Friday 17 April 2020 at 15:32:38, TarotApprentice wrote:
>
> > Trying to visit the confirmation page at
> > http://lists.squid-cache.org/confirm/squid-users/ but it doesn?t seem to
> > be responding. I?ve tried over a couple of days.
>
> When you say "not responding", do you mean you get no page content shown
> in
> your browser, or do you mean that you fill in the confirmation string and
> click
> on 'submit' but it then doesn't accept your confirmation?
>
> The page itself loads fine for me here, and I've only ever confirmed
> mailman
> subscriptions by email - just reply to the email you got asking you to
> visit
> the confirmation page, making sure you reply from the address you asked to
> subscribe to the list.
>
> No need to change anything about the email you get, just do a "reply" and
> "send" exactly as it is.
>
>
> Antony.
>
> --
> Success is a lousy teacher.  It seduces smart people into thinking they
> can't
> lose.
>
>  - William H Gates III
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200417/856c3570/attachment.htm>

From rousskov at measurement-factory.com  Fri Apr 17 18:20:38 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Apr 2020 14:20:38 -0400
Subject: [squid-users] ssl proxy and decrypted forwarding
In-Reply-To: <2061590632.34473388.1587139211439.JavaMail.zimbra@quadrantsec.com>
References: <623449268.34416752.1587126557375.JavaMail.zimbra@quadrantsec.com>
 <644827944.34461140.1587136713598.JavaMail.zimbra@quadrantsec.com>
 <1294604998.34462352.1587136966978.JavaMail.zimbra@quadrantsec.com>
 <a3a5d2d0-cc3e-e06a-f79d-9f972e60b3d2@measurement-factory.com>
 <2061590632.34473388.1587139211439.JavaMail.zimbra@quadrantsec.com>
Message-ID: <241cedbf-eef5-813d-92ad-681b11d54159@measurement-factory.com>

On 4/17/20 12:00 PM, Sam Castellano wrote:

> Suricata/Snort is looking at the interface

If listening on a network interface is all these tools can do, and you
do not want to modify Squid, then you can [pay somebody to] write an
eCAP adapter (or an ICAP service) that will send decrypted messages to
that network interface as if it were plain HTTP/TCP/IP/Ethernet traffic.
It is not easy to do, and there are dangers related to (and limitations
of) this approach, but I know it is possible because we have
successfully done that for a customer a few years ago.

For a bit more info, follow a similar old squid-users thread:

http://lists.squid-cache.org/pipermail/squid-users/2016-September/012689.html


HTH,

Alex.

> ----- Original Message -----
> From: "Alex Rousskov" <rousskov at measurement-factory.com>
> To: "Sam Castellano" <scastellano at quadrantsec.com>, "squid-users" <squid-users at lists.squid-cache.org>
> Sent: Friday, April 17, 2020 11:49:13 AM
> Subject: Re: [squid-users] ssl proxy and decrypted forwarding
> 
> On 4/17/20 11:22 AM, Sam Castellano wrote:
> 
>> My question relates to ssl bumping and potentially Icap/Ecap
>> functionality. I currently have ssl bump/ interception working and
>> communicating with a local ICAP server. Im trying to understand the
>> process of how the decrypted data gets sent to the ICAP server for
>> analysis in things such as clamav etc. My goal is to have the decrypted
>> traffic analyzed by Suricata preferably on a separate box if possible.??
> 
> I do not know what particular information you are looking for, but ICAP
> mechanics are documented in RFC 3507 while eCAP mechanics are documented
> at www.e-cap.org.
> 
> If you are worried about exposing proxied HTTP[S] messages in transit to
> your ICAP service, then consider using a "Secure ICAP" service (for a
> starting point, look for those two words in squid.conf.documented).
> 
> N.B. Neither ICAP nor eCAP know about SslBump. In an SslBump context,
> they just get CONNECT requests and the HTTP messages decrypted by Squid.
> The same is true for the majority of Squid features -- while inside
> Squid, decrypted HTTP traffic is usually handled similar to plain HTTP
> traffic.
> 
> 
> HTH,
> 
> Alex.
> 



From tarotapprentice at yahoo.com  Fri Apr 17 21:23:02 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 18 Apr 2020 07:23:02 +1000
Subject: [squid-users] Confirmation page not working
In-Reply-To: <CA+Y8hcNRj557QsjWHDpJwk_4XCbi4aYZy7iFdFTaw6O8eaECgw@mail.gmail.com>
References: <CA+Y8hcNRj557QsjWHDpJwk_4XCbi4aYZy7iFdFTaw6O8eaECgw@mail.gmail.com>
Message-ID: <27024393-088E-4928-864E-B25CA12972F1@yahoo.com>

Thanks. The page wasn?t loading at all. It does now.

> On 18 Apr 2020, at 2:36 am, Francesco Chemolli <gkinkie at gmail.com> wrote:
> 
> ?
> Hi,
>   there was a problem on the server, now fixed.
> Apologies
> 
>> On Fri, Apr 17, 2020 at 4:08 PM Antony Stone <Antony.Stone at squid.open.source.it> wrote:
>> On Friday 17 April 2020 at 15:32:38, TarotApprentice wrote:
>> 
>> > Trying to visit the confirmation page at
>> > http://lists.squid-cache.org/confirm/squid-users/ but it doesn?t seem to
>> > be responding. I?ve tried over a couple of days.
>> 
>> When you say "not responding", do you mean you get no page content shown in 
>> your browser, or do you mean that you fill in the confirmation string and click 
>> on 'submit' but it then doesn't accept your confirmation?
>> 
>> The page itself loads fine for me here, and I've only ever confirmed mailman 
>> subscriptions by email - just reply to the email you got asking you to visit 
>> the confirmation page, making sure you reply from the address you asked to 
>> subscribe to the list.
>> 
>> No need to change anything about the email you get, just do a "reply" and 
>> "send" exactly as it is.
>> 
>> 
>> Antony.
>> 
>> -- 
>> Success is a lousy teacher.  It seduces smart people into thinking they can't 
>> lose.
>> 
>>  - William H Gates III
>> 
>>                                                    Please reply to the list;
>>                                                          please *don't* CC me.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> -- 
>     Francesco
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200418/ec5b09fa/attachment.htm>

From tarotapprentice at yahoo.com  Fri Apr 17 21:23:02 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 18 Apr 2020 07:23:02 +1000
Subject: [squid-users] Confirmation page not working
In-Reply-To: <CA+Y8hcNRj557QsjWHDpJwk_4XCbi4aYZy7iFdFTaw6O8eaECgw@mail.gmail.com>
References: <CA+Y8hcNRj557QsjWHDpJwk_4XCbi4aYZy7iFdFTaw6O8eaECgw@mail.gmail.com>
Message-ID: <27024393-088E-4928-864E-B25CA12972F1@yahoo.com>

Thanks. The page wasn?t loading at all. It does now.

> On 18 Apr 2020, at 2:36 am, Francesco Chemolli <gkinkie at gmail.com> wrote:
> 
> ?
> Hi,
>   there was a problem on the server, now fixed.
> Apologies
> 
>> On Fri, Apr 17, 2020 at 4:08 PM Antony Stone <Antony.Stone at squid.open.source.it> wrote:
>> On Friday 17 April 2020 at 15:32:38, TarotApprentice wrote:
>> 
>> > Trying to visit the confirmation page at
>> > http://lists.squid-cache.org/confirm/squid-users/ but it doesn?t seem to
>> > be responding. I?ve tried over a couple of days.
>> 
>> When you say "not responding", do you mean you get no page content shown in 
>> your browser, or do you mean that you fill in the confirmation string and click 
>> on 'submit' but it then doesn't accept your confirmation?
>> 
>> The page itself loads fine for me here, and I've only ever confirmed mailman 
>> subscriptions by email - just reply to the email you got asking you to visit 
>> the confirmation page, making sure you reply from the address you asked to 
>> subscribe to the list.
>> 
>> No need to change anything about the email you get, just do a "reply" and 
>> "send" exactly as it is.
>> 
>> 
>> Antony.
>> 
>> -- 
>> Success is a lousy teacher.  It seduces smart people into thinking they can't 
>> lose.
>> 
>>  - William H Gates III
>> 
>>                                                    Please reply to the list;
>>                                                          please *don't* CC me.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> -- 
>     Francesco
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200418/ec5b09fa/attachment-0001.htm>

From squid3 at treenet.co.nz  Sat Apr 18 13:10:00 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Apr 2020 01:10:00 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
	Issues in HTTP	Request processing
Message-ID: <e01028a2-8ba8-9097-552e-bc5a07bb7f29@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:4
__________________________________________________________________

Advisory ID:        SQUID-2019:4
Date:               April 18, 2020
Summary:            Multiple Issues
                    in HTTP Request processing.
Affected versions:  Squid 3.5.18 -> 3.5.28
                    Squid 4.0.10 -> 4.7
Fixed in version:   Squid 4.8
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_4.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12520
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12524
__________________________________________________________________

Problem Description:

 Due to incorrect URL handling Squid is vulnerable to access
 control bypass, cache poisoning and cross-site scripting attacks
 when processing HTTP Request messages.

__________________________________________________________________

Severity:

 A remote client can deliver crafted URLs to bypass cache manager
 security controls and retrieve confidential details about the
 proxy and traffic it is handling.

 A remote client can deliver crafted URLs which cause arbitrary
 content from one origin server to be stored in cache as URLs
 within another origin. This opens a window of opportunity for
 clients to be tricked into fetching and XSS execution of that
 content via side channels.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.8.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2019_4.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x are not vulnerable.

 All Squid-3.x up to and including 3.5.17 are not vulnerable.

 All Squid-3.5.18 up to and including 3.5.28 are vulnerable.

 All Squid-4.x up to and including 4.0.9 are not vulnerable.

 All Squid-4.x up to and including 4.7 without HTTPS support are
 not vulnerable.

 All Squid-4.0.10 up to and including 4.7 with HTTPS support are
 vulnerable.

__________________________________________________________________

Workarounds:

 There are no workarounds for Squid-3.5.

 For Squid-4 build using --without-openssl --without-gnutls


__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-06-23 15:15:56 UTC Patches Released
 2019-06-05 15:52:17 UTC CVE Assignment
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From marcus.kool at urlfilterdb.com  Sat Apr 18 18:52:46 2020
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 18 Apr 2020 19:52:46 +0100
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <e01028a2-8ba8-9097-552e-bc5a07bb7f29@treenet.co.nz>
References: <e01028a2-8ba8-9097-552e-bc5a07bb7f29@treenet.co.nz>
Message-ID: <1728874c-5a6a-94a3-59e1-6e73aaa805f9@urlfilterdb.com>

Amos,
The latest version of Squid is 4.10.? Do you mean "fixed in 4.10" instead of "fixed in 4.8" ?

Thanks,
Marcus

On 18/04/2020 14:10, Amos Jeffries wrote:
> __________________________________________________________________
>
>      Squid Proxy Cache Security Update Advisory SQUID-2019:4
> __________________________________________________________________
>
> Advisory ID:        SQUID-2019:4
> Date:               April 18, 2020
> Summary:            Multiple Issues
>                      in HTTP Request processing.
> Affected versions:  Squid 3.5.18 -> 3.5.28
>                      Squid 4.0.10 -> 4.7
> Fixed in version:   Squid 4.8
> __________________________________________________________________
>
>      http://www.squid-cache.org/Advisories/SQUID-2019_4.txt
>      http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12520
>      http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12524
> __________________________________________________________________
>
> Problem Description:
>
>   Due to incorrect URL handling Squid is vulnerable to access
>   control bypass, cache poisoning and cross-site scripting attacks
>   when processing HTTP Request messages.
>
> __________________________________________________________________
>
> Severity:
>
>   A remote client can deliver crafted URLs to bypass cache manager
>   security controls and retrieve confidential details about the
>   proxy and traffic it is handling.
>
>   A remote client can deliver crafted URLs which cause arbitrary
>   content from one origin server to be stored in cache as URLs
>   within another origin. This opens a window of opportunity for
>   clients to be tricked into fetching and XSS execution of that
>   content via side channels.
>
> __________________________________________________________________
>
> Updated Packages:
>
>   This bug is fixed by Squid version 4.8.
>
>   In addition, patches addressing this problem for the stable
>   releases can be found in our patch archives:
>
> Squid 4:
>   <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2019_4.patch>
>
>   If you are using a prepackaged version of Squid then please refer
>   to the package vendor for availability information on updated
>   packages.
>
> __________________________________________________________________
>
> Determining if your version is vulnerable:
>
>   All Squid-2.x are not vulnerable.
>
>   All Squid-3.x up to and including 3.5.17 are not vulnerable.
>
>   All Squid-3.5.18 up to and including 3.5.28 are vulnerable.
>
>   All Squid-4.x up to and including 4.0.9 are not vulnerable.
>
>   All Squid-4.x up to and including 4.7 without HTTPS support are
>   not vulnerable.
>
>   All Squid-4.0.10 up to and including 4.7 with HTTPS support are
>   vulnerable.
>
> __________________________________________________________________
>
> Workarounds:
>
>   There are no workarounds for Squid-3.5.
>
>   For Squid-4 build using --without-openssl --without-gnutls
>
>
> __________________________________________________________________
>
> Contact details for the Squid project:
>
>   For installation / upgrade support on binary packaged versions
>   of Squid: Your first point of contact should be your binary
>   package vendor.
>
>   If your install and build Squid from the original Squid sources
>   then the squid-users at lists.squid-cache.org mailing list is your
>   primary support point. For subscription details see
>   <http://www.squid-cache.org/Support/mailing-lists.html>.
>
>   For reporting of non-security bugs in the latest STABLE release
>   the squid bugzilla database should be used
>   <http://bugs.squid-cache.org/>.
>
>   For reporting of security sensitive bugs send an email to the
>   squid-bugs at lists.squid-cache.org mailing list. It's a closed
>   list (though anyone can post) and security related bug reports
>   are treated in confidence until the impact has been established.
>
> __________________________________________________________________
>
> Credits:
>
>   This vulnerability was discovered by Jeriko One
>   <jeriko.one at gmx.us>.
>
>   Fixed by Amos Jeffries of Treehouse Networks Ltd.
>
> __________________________________________________________________
>
> Revision history:
>
>   2019-05-14 14:56:49 UTC Initial Report
>   2019-06-23 15:15:56 UTC Patches Released
>   2019-06-05 15:52:17 UTC CVE Assignment
> __________________________________________________________________
> END
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Sun Apr 19 03:32:28 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Apr 2020 15:32:28 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <1728874c-5a6a-94a3-59e1-6e73aaa805f9@urlfilterdb.com>
References: <e01028a2-8ba8-9097-552e-bc5a07bb7f29@treenet.co.nz>
 <1728874c-5a6a-94a3-59e1-6e73aaa805f9@urlfilterdb.com>
Message-ID: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>


On 19/04/20 6:52 am, Marcus Kool wrote:
> Amos,
> The latest version of Squid is 4.10.? Do you mean "fixed in 4.10"
> instead of "fixed in 4.8" ?
> 

No, these CVE were fixed in 4.8. The advisory was embargoed for another
issue, which is has taken too long and now going to be fixed in a later
release.

Amos



> Thanks,
> Marcus
> 
> On 18/04/2020 14:10, Amos Jeffries wrote:
>> __________________________________________________________________
>>
>> ???? Squid Proxy Cache Security Update Advisory SQUID-2019:4
>> __________________________________________________________________
>>
>> Advisory ID:??????? SQUID-2019:4
>> Date:?????????????? April 18, 2020
>> Summary:??????????? Multiple Issues
>> ???????????????????? in HTTP Request processing.
>> Affected versions:? Squid 3.5.18 -> 3.5.28
>> ???????????????????? Squid 4.0.10 -> 4.7
>> Fixed in version:?? Squid 4.8
>> __________________________________________________________________
>>
>> ???? http://www.squid-cache.org/Advisories/SQUID-2019_4.txt
>> ???? http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12520
>> ???? http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12524
>> __________________________________________________________________
>>


From tarotapprentice at yahoo.com  Sun Apr 19 08:18:20 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 19 Apr 2020 18:18:20 +1000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
	Issues in HTTP Request processing
In-Reply-To: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
Message-ID: <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>

I am not sure if you have any contact with the Debian maintainers. I raised a bug with Debian in March asking for 4.10 to get promoted to buster-backports on the grounds of security fixes. If we?re on the stable release (buster) we are stuck with 4.6 until the next stable release (up to 2 years), use the testing release which has other changes or we have to compile our own.

Link to bug: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=954488

MarkJ 


> On 19 Apr 2020, at 1:33 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> ?
>> On 19/04/20 6:52 am, Marcus Kool wrote:
>> Amos,
>> The latest version of Squid is 4.10.  Do you mean "fixed in 4.10"
>> instead of "fixed in 4.8" ?
>> 
> 
> No, these CVE were fixed in 4.8. The advisory was embargoed for another
> issue, which is has taken too long and now going to be fixed in a later
> release.
> 
> Amos
> 
> 
> 
>> Thanks,
>> Marcus
>> 
>>> On 18/04/2020 14:10, Amos Jeffries wrote:
>>> __________________________________________________________________
>>> 
>>>      Squid Proxy Cache Security Update Advisory SQUID-2019:4
>>> __________________________________________________________________
>>> 
>>> Advisory ID:        SQUID-2019:4
>>> Date:               April 18, 2020
>>> Summary:            Multiple Issues
>>>                      in HTTP Request processing.
>>> Affected versions:  Squid 3.5.18 -> 3.5.28
>>>                      Squid 4.0.10 -> 4.7
>>> Fixed in version:   Squid 4.8
>>> __________________________________________________________________
>>> 
>>>      http://www.squid-cache.org/Advisories/SQUID-2019_4.txt
>>>      http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12520
>>>      http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12524
>>> __________________________________________________________________
>>> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200419/49490f84/attachment.htm>

From tarotapprentice at yahoo.com  Sun Apr 19 08:18:20 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 19 Apr 2020 18:18:20 +1000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
	Issues in HTTP Request processing
In-Reply-To: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
Message-ID: <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>

I am not sure if you have any contact with the Debian maintainers. I raised a bug with Debian in March asking for 4.10 to get promoted to buster-backports on the grounds of security fixes. If we?re on the stable release (buster) we are stuck with 4.6 until the next stable release (up to 2 years), use the testing release which has other changes or we have to compile our own.

Link to bug: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=954488

MarkJ 


> On 19 Apr 2020, at 1:33 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> ?
>> On 19/04/20 6:52 am, Marcus Kool wrote:
>> Amos,
>> The latest version of Squid is 4.10.  Do you mean "fixed in 4.10"
>> instead of "fixed in 4.8" ?
>> 
> 
> No, these CVE were fixed in 4.8. The advisory was embargoed for another
> issue, which is has taken too long and now going to be fixed in a later
> release.
> 
> Amos
> 
> 
> 
>> Thanks,
>> Marcus
>> 
>>> On 18/04/2020 14:10, Amos Jeffries wrote:
>>> __________________________________________________________________
>>> 
>>>      Squid Proxy Cache Security Update Advisory SQUID-2019:4
>>> __________________________________________________________________
>>> 
>>> Advisory ID:        SQUID-2019:4
>>> Date:               April 18, 2020
>>> Summary:            Multiple Issues
>>>                      in HTTP Request processing.
>>> Affected versions:  Squid 3.5.18 -> 3.5.28
>>>                      Squid 4.0.10 -> 4.7
>>> Fixed in version:   Squid 4.8
>>> __________________________________________________________________
>>> 
>>>      http://www.squid-cache.org/Advisories/SQUID-2019_4.txt
>>>      http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12520
>>>      http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12524
>>> __________________________________________________________________
>>> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200419/49490f84/attachment-0001.htm>

From dm at belkam.com  Sun Apr 19 08:22:24 2020
From: dm at belkam.com (Dmitry Melekhov)
Date: Sun, 19 Apr 2020 12:22:24 +0400
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
 <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>
Message-ID: <49ab44f9-47b7-8e83-91f1-d1ab5b6b7d4e@belkam.com>


19.04.2020 12:18, TarotApprentice ?????:
> I am not sure if you have any contact with the Debian maintainers. I 
> raised a bug with Debian in March asking for 4.10 to get promoted to 
> buster-backports on the grounds of security fixes. If we?re on the 
> stable release (buster) we are stuck with 4.6 until the next stable 
> release (up to 2 years), use the testing release which has other 
> changes or we have to compile our own.
>
> Link to bug: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=954488
>
>

4.10 does not contain fix :-)


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200419/6fa0ca49/attachment.htm>

From squid3 at treenet.co.nz  Sun Apr 19 08:31:53 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Apr 2020 20:31:53 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
 <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>
Message-ID: <f5a6774e-8734-5ffe-ef61-686dedc47d06@treenet.co.nz>

On 19/04/20 8:18 pm, TarotApprentice wrote:
> I am not sure if you have any contact with the Debian maintainers. I
> raised a bug with Debian in March asking for 4.10 to get promoted to
> buster-backports on the grounds of security fixes. If we?re on the
> stable release (buster) we are stuck with 4.6 until the next stable
> release (up to 2 years), use the testing release which has other changes
> or we have to compile our own.

I am part of the Debian packaging team assisting Luigi. AFAIK this is in
the hands of the security team since it would be those grounds for backport.

Security have just been in contact after a review and update of the open
issues they are tracking against Debian Squid packages. Though I have
not heard if any decision has been made about this request.

What I do know is that many of the CVE with 4.x patches have had those
applied to the Debian package available in Buster. There are some which
do not backport easily, so not 100%, but the old package is not as
vulnerable as it may appear from just the number.

Amos


From squid3 at treenet.co.nz  Sun Apr 19 08:37:01 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Apr 2020 20:37:01 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <49ab44f9-47b7-8e83-91f1-d1ab5b6b7d4e@belkam.com>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
 <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>
 <49ab44f9-47b7-8e83-91f1-d1ab5b6b7d4e@belkam.com>
Message-ID: <7926767d-c463-1ef5-1afc-eb654340446d@treenet.co.nz>

On 19/04/20 8:22 pm, Dmitry Melekhov wrote:
> 
> 19.04.2020 12:18, TarotApprentice ?????:
>> I am not sure if you have any contact with the Debian maintainers. I
>> raised a bug with Debian in March asking for 4.10 to get promoted to
>> buster-backports on the grounds of security fixes. If we?re on the
>> stable release (buster) we are stuck with 4.6 until the next stable
>> release (up to 2 years), use the testing release which has other
>> changes or we have to compile our own.
>>
>> Link to bug:?https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=954488
>>
> 
> 4.10 does not contain fix :-)
> 

Which fix are you talking about?

 The bug TarotApprentice referenced is a publishing issue within Debian.
Requesting an event which has not happened yet.

 The bug this advisory is talking about definitely is fixed in Squid
4.10 code. The patch was added way back in 4.8 release.


Amos


From dm at belkam.com  Sun Apr 19 09:47:41 2020
From: dm at belkam.com (Dmitry Melekhov)
Date: Sun, 19 Apr 2020 13:47:41 +0400
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <7926767d-c463-1ef5-1afc-eb654340446d@treenet.co.nz>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
 <1CAFAAF5-84E1-408D-BCA5-1B14267CDF1E@yahoo.com>
 <49ab44f9-47b7-8e83-91f1-d1ab5b6b7d4e@belkam.com>
 <7926767d-c463-1ef5-1afc-eb654340446d@treenet.co.nz>
Message-ID: <399c208e-cc1e-2773-92c7-24c1d0a64168@belkam.com>


19.04.2020 12:37, Amos Jeffries ?????:
> On 19/04/20 8:22 pm, Dmitry Melekhov wrote:
>> 19.04.2020 12:18, TarotApprentice ?????:
>>> I am not sure if you have any contact with the Debian maintainers. I
>>> raised a bug with Debian in March asking for 4.10 to get promoted to
>>> buster-backports on the grounds of security fixes. If we?re on the
>>> stable release (buster) we are stuck with 4.6 until the next stable
>>> release (up to 2 years), use the testing release which has other
>>> changes or we have to compile our own.
>>>
>>> Link to bug:?https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=954488
>>>
>> 4.10 does not contain fix :-)
>>
> Which fix are you talking about?
>
>   The bug TarotApprentice referenced is a publishing issue within Debian.
> Requesting an event which has not happened yet.
>
>   The bug this advisory is talking about definitely is fixed in Squid
> 4.10 code. The patch was added way back in 4.8 release.


Affected versions:  Squid 3.5.18 -> 3.5.28
                     Squid 4.0.10 -> 4.7


Well, this announcement is extremely misleading then...




From Antony.Stone at squid.open.source.it  Sun Apr 19 09:53:21 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 19 Apr 2020 11:53:21 +0200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
	Issues in HTTP Request processing
In-Reply-To: <399c208e-cc1e-2773-92c7-24c1d0a64168@belkam.com>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
 <7926767d-c463-1ef5-1afc-eb654340446d@treenet.co.nz>
 <399c208e-cc1e-2773-92c7-24c1d0a64168@belkam.com>
Message-ID: <202004191153.22238.Antony.Stone@squid.open.source.it>

On Sunday 19 April 2020 at 11:47:41, Dmitry Melekhov wrote:

> 19.04.2020 12:37, Amos Jeffries ?????:
> > On 19/04/20 8:22 pm, Dmitry Melekhov wrote:
> >
> > > 4.10 does not contain fix :-)
> > 
> > Which fix are you talking about?
> > 
> > The bug this advisory is talking about definitely is fixed in Squid
> > 4.10 code. The patch was added way back in 4.8 release.
> 
> Affected versions:  Squid 3.5.18 -> 3.5.28
>                      Squid 4.0.10 -> 4.7

You omitted the next line:

Fixed in version:   Squid 4.8

> Well, this announcement is extremely misleading then...

What's misleading?

It's a standard security advisory telling us what the vulnerability is, which 
versions are affected, and which version it is fixed from.


Regards,


Antony.

-- 
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From dm at belkam.com  Sun Apr 19 10:14:06 2020
From: dm at belkam.com (Dmitry Melekhov)
Date: Sun, 19 Apr 2020 14:14:06 +0400
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:4 Multiple
 Issues in HTTP Request processing
In-Reply-To: <202004191153.22238.Antony.Stone@squid.open.source.it>
References: <046caa57-410f-10a1-4b54-e767475fbedb@treenet.co.nz>
 <7926767d-c463-1ef5-1afc-eb654340446d@treenet.co.nz>
 <399c208e-cc1e-2773-92c7-24c1d0a64168@belkam.com>
 <202004191153.22238.Antony.Stone@squid.open.source.it>
Message-ID: <335ed520-552e-088d-e250-a3e49bba3280@belkam.com>


19.04.2020 13:53, Antony Stone ?????:
>
> What's misleading?


Sorry, I read it wrong.

Thank you!





From jadams at modmc.net  Mon Apr 20 14:48:18 2020
From: jadams at modmc.net (James Adams)
Date: Mon, 20 Apr 2020 09:48:18 -0500
Subject: [squid-users] squid 3.5 conf setup
Message-ID: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>

We have a few squid setups to handle large number of /24 IP blocks.
I want to know is there an easier configuration to do this as we have to 
manually configure each conf file and can take a long time plus the fact 
of human error. I am trying to reduce the number of lines needed and 
make it a little more efficient. Below is a snippet of a conf file. The 
bold text is what I am trying to reduce.
acl localnet src all

acl SSL_ports port 443
acl Safe_ports port 80????????? # http
acl Safe_ports port 21????????? # ftp
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher
acl Safe_ports port 210???????? # wais
acl Safe_ports port 1025-65535? # unregistered ports
acl Safe_ports port 280???????? # http-mgmt
acl Safe_ports port 488???????? # gss-http
acl Safe_ports port 591???????? # filemaker
acl Safe_ports port 777???????? # multiling http
max_filedesc 65535

acl CONNECT method CONNECT

#http_access allow localnet
#http_access allow all
visible_hostname this.that.com

auth_param basic program basic_ncsa_auth squid/etc/passwords
auth_param basic realm proxy
acl authenticated proxy_auth REQUIRED
http_access allow authenticated
###########################################################
# Section: xxx.yyy.zzz.0 / 24
###########################################################
*http_port xxx.yyy.zzz.1:3128 name=1**
**acl 1 myportname 1 src all**
**http_access allow 1**
**tcp_outgoing_address xxx.yyy.zzz.1 1**
**http_port xxx.yyy.zzz.2:3128 name=2**
**acl 2 myportname 2 src all**
**http_access allow 2**
**tcp_outgoing_address xxx.yyy.zzz.2 2**
**http_port xxx.yyy.zzz.3:3128 name=3**
**acl 3 myportname 3 src all**
**http_access allow 3**
**tcp_outgoing_address xxx.yyy.zzz.3 3**
**http_port xxx.yyy.zzz.4:3128 name=4**
**acl 4 myportname 4 src all**
**http_access allow 4**
**tcp_outgoing_address xxx.yyy.zzz.4 4**
**http_port xxx.yyy.zzz.5:3128 name=5**
**acl 5 myportname 5 src all**
**http_access allow 5**
**tcp_outgoing_address xxx.yyy.zzz.5 5*
......

......
request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access User-Agent allow all
request_header_access Cookie allow all
request_header_access All deny all

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
refresh_pattern ^gopher:??? 1440??? 0%??? 1440
refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
refresh_pattern .??? ??? 0??? 20%??? 4320

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200420/a42246ca/attachment.htm>

From squid3 at treenet.co.nz  Mon Apr 20 16:37:41 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2020 04:37:41 +1200
Subject: [squid-users] squid 3.5 conf setup
In-Reply-To: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>
References: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>
Message-ID: <eab4b214-a962-e3bb-0c9e-2db0b0133147@treenet.co.nz>

On 21/04/20 2:48 am, James Adams wrote:
> We have a few squid setups to handle large number of /24 IP blocks.
> I want to know is there an easier configuration to do this as we have to
> manually configure each conf file and can take a long time plus the fact
> of human error. I am trying to reduce the number of lines needed and
> make it a little more efficient. Below is a snippet of a conf file. The
> bold text is what I am trying to reduce.


>From what I can see in this config file that is not going to be possible
for any Squid.

Best you will get is automating the config lines creation to avoid that
human error situation. Squid can use "include /path/to/file" to pull in
config lines from an auto-generated file.



> acl localnet src all
> 

Your LAN is the entire Internet? I think not.


> ......
> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access WWW-Authenticate allow all

I suggest looking at all these headers. Many are not relevant to *requests*.


Amos



From leomessi983 at yahoo.com  Mon Apr 20 18:04:32 2020
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Mon, 20 Apr 2020 18:04:32 +0000 (UTC)
Subject: [squid-users] squid access.log
References: <1041534266.3581145.1587405872277.ref@mail.yahoo.com>
Message-ID: <1041534266.3581145.1587405872277@mail.yahoo.com>

hi 
I have one question.why for each https request that squid do peek or bump or splice ,squid logs 2 lines?one with connect method and one with head method?
thanx

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200420/61d9e463/attachment.htm>

From rousskov at measurement-factory.com  Mon Apr 20 19:09:20 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Apr 2020 15:09:20 -0400
Subject: [squid-users] squid access.log
In-Reply-To: <1041534266.3581145.1587405872277@mail.yahoo.com>
References: <1041534266.3581145.1587405872277.ref@mail.yahoo.com>
 <1041534266.3581145.1587405872277@mail.yahoo.com>
Message-ID: <9650ad56-e0d9-b1bc-87dc-84c3a7490a15@measurement-factory.com>

On 4/20/20 2:04 PM, leomessi983 at yahoo.com wrote:
> hi
> I have one question.
> why for each https request that squid do peek or bump or splice ,squid
> logs 2 lines?
> one with connect method and one with head method?

... because there are two HTTP[S] requests in those cases, one with the
CONNECT method and one with the HEAD method. There are other cases where
one bumped CONNECT tunnel carries hundreds or even thousands of
GET/HEAD/PUT/POST/CONNECT/etc. requests. And there are also cases where
a bumped CONNECT tunnel carries no requests at all.

In summary, one bumped CONNECT tunnel will (by default) result in one or
more access.log records, starting with the CONNECT record.

Alex.


From leomessi983 at yahoo.com  Mon Apr 20 20:13:45 2020
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Mon, 20 Apr 2020 20:13:45 +0000 (UTC)
Subject: [squid-users] squid access.log
References: <675667919.3672374.1587413625622.ref@mail.yahoo.com>
Message-ID: <675667919.3672374.1587413625622@mail.yahoo.com>

 Well in my case for my single web request in first CONNECT log entry, the domain address is IP address of server and URL is IP:PORT of server and in second log entry domain is example.com and URL is example.com:443 .but why? I dont undrestand it, this confuses me !!
I dont bump anything in this requests!If I use ssl::server_name and specify IP address of server?to bump https request,  my https://example.com request will be blocked, I dont send requests in the example format of https://1.1.1.1 .but they will be blocked while I dont want to.

    On Monday, April 20, 2020, 11:39:23 PM GMT+4:30, Alex Rousskov <rousskov at measurement-factory.com> wrote:  
 
 On 4/20/20 2:04 PM, leomessi983 at yahoo.com wrote:
> hi
> I have one question.
> why for each https request that squid do peek or bump or splice ,squid
> logs 2 lines?
> one with connect method and one with head method?

... because there are two HTTP[S] requests in those cases, one with the
CONNECT method and one with the HEAD method. There are other cases where
one bumped CONNECT tunnel carries hundreds or even thousands of
GET/HEAD/PUT/POST/CONNECT/etc. requests. And there are also cases where
a bumped CONNECT tunnel carries no requests at all.

In summary, one bumped CONNECT tunnel will (by default) result in one or
more access.log records, starting with the CONNECT record.

Alex.
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200420/a9136de1/attachment.htm>

From rousskov at measurement-factory.com  Mon Apr 20 20:16:23 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Apr 2020 16:16:23 -0400
Subject: [squid-users] squid 3.5 conf setup
In-Reply-To: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>
References: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>
Message-ID: <f9a2f5f0-faba-dbdd-b91d-257751eed41a@measurement-factory.com>

On 4/20/20 10:48 AM, James Adams wrote:
> acl 1 myportname 1 src all

I agree with Amos that you should consider automating these
configuration lines. Your access control and request routing
requirements are probably too special to warrant adding a new Squid feature.

While writing the config-generating script, please remove the "src all"
trailing garbage(?) from myportname ACL definitions. I would also
recommend avoiding using bare numbers as port and ACL _names_: It is
very confusing for many human readers, and might even cause parsing
failures in future Squid versions.


HTH,

Alex.


From jadams at modmc.net  Mon Apr 20 20:29:40 2020
From: jadams at modmc.net (James Adams)
Date: Mon, 20 Apr 2020 15:29:40 -0500
Subject: [squid-users] squid 3.5 conf setup
In-Reply-To: <f9a2f5f0-faba-dbdd-b91d-257751eed41a@measurement-factory.com>
References: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>
 <f9a2f5f0-faba-dbdd-b91d-257751eed41a@measurement-factory.com>
Message-ID: <a9b83613-6fd1-324b-9908-8cf4a6237994@modmc.net>

So I am new to squid and trying to take over from what was already in 
place.
Do you have an example of what it should look like or a best practice 
guide to use.
 ?We can have up to 32 /24 blocks in our configurations.
I want to try and make the systems as efficient as possible knowing the 
large amount of IPs we are using.

Thanks

On 4/20/2020 15:16, Alex Rousskov wrote:
> On 4/20/20 10:48 AM, James Adams wrote:
>> acl 1 myportname 1 src all
> I agree with Amos that you should consider automating these
> configuration lines. Your access control and request routing
> requirements are probably too special to warrant adding a new Squid feature.
>
> While writing the config-generating script, please remove the "src all"
> trailing garbage(?) from myportname ACL definitions. I would also
> recommend avoiding using bare numbers as port and ACL _names_: It is
> very confusing for many human readers, and might even cause parsing
> failures in future Squid versions.
>
>
> HTH,
>
> Alex.

-- 
James Adams
Sr. Systems Engineer
MOD Mission Critical

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200420/d4c02f73/attachment.htm>

From rousskov at measurement-factory.com  Mon Apr 20 20:54:31 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Apr 2020 16:54:31 -0400
Subject: [squid-users] squid access.log
In-Reply-To: <675667919.3672374.1587413625622@mail.yahoo.com>
References: <675667919.3672374.1587413625622.ref@mail.yahoo.com>
 <675667919.3672374.1587413625622@mail.yahoo.com>
Message-ID: <012aa72f-b893-ec9b-9f2b-00a65b434355@measurement-factory.com>

On 4/20/20 4:13 PM, leomessi983 at yahoo.com wrote:
> Well in my case for my single web request in first CONNECT log entry,
> the domain address is IP address of server and URL is IP:PORT of server
> and in second log entry domain is example.com and URL is example.com:443 .

Yes, this is typical.

> but why?

You see IP addresses in CONNECT URIs because that is what the client
(e.g., a browser) sent to Squid or, if you are intercepting, that is how
Squid shows intercepted TCP connections.

Per protocol specification, A CONNECT request URI (or request target)
syntax differs from the syntax of other common request URIs (e.g.,
HEAD). For details, see request-target at
https://tools.ietf.org/html/rfc7230#section-3.1.1


> I dont bump anything in this requests!

I probably do not know what you mean by this remark. You other comments
indicate that you do bump CONNECT tunnels. If you use "ssl_bump bump" or
equivalent deprecated rules, then, for the purposes of this discussion,
you are probably bumping (i.e., decrypting) some CONNECT tunnels.


> If I use ssl::server_name and specify IP address of server?to bump
> https request, my https://example.com request will be blocked, I dont
> send requests in the example format of https://1.1.1.1 .but they will be
> blocked while I dont want to.

Your http_access and ssl_bump rules have to match reality. There is no
way around that. In reality, CONNECT requests use different request
target than, say, HEAD requests inside those CONNECT tunnels.

If you can configure Wireshark or a similar packet inspection tool to
decrypt CONNECT tunnels and show you both CONNECT requests and the
requests inside the tunnel, all these details may become a bit easier to
grasp. Unfortunately, I do not have ready-to-use instructions on how to
configure Wireshark to decrypt to- and from-Squid communications.


HTH,

Alex.


> On Monday, April 20, 2020, 11:39:23 PM GMT+4:30, Alex Rousskov wrote:
> 
> 
> On 4/20/20 2:04 PM, leomessi983 at yahoo.com <mailto:leomessi983 at yahoo.com>
> wrote:
> 
>> hi
>> I have one question.
>> why for each https request that squid do peek or bump or splice ,squid
>> logs 2 lines?
>> one with connect method and one with head method?
> 
> 
> ... because there are two HTTP[S] requests in those cases, one with the
> CONNECT method and one with the HEAD method. There are other cases where
> one bumped CONNECT tunnel carries hundreds or even thousands of
> GET/HEAD/PUT/POST/CONNECT/etc. requests. And there are also cases where
> a bumped CONNECT tunnel carries no requests at all.
> 
> In summary, one bumped CONNECT tunnel will (by default) result in one or
> more access.log records, starting with the CONNECT record.
> 
> Alex.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rentorbuy at yahoo.com  Mon Apr 20 23:08:31 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 20 Apr 2020 23:08:31 +0000 (UTC)
Subject: [squid-users] tproxy sslbump and user authentication
References: <111546434.3854722.1587424111575.ref@mail.yahoo.com>
Message-ID: <111546434.3854722.1587424111575@mail.yahoo.com>

Hi,

Is it possible to somehow combine the filtering capabilities of tproxy ssl-bump for access to https sites and the access control flexibility of proxy_auth (eg. kerberos)?

Is having two proxy servers in sequence an acceptable approach, or can it be done within the same instance with the CONNECT method?

My first approach would be to configure clients to send their user credentials to an explicit proxy (Squid #1) which would then proxy_auth via Kerberos to a PDC. ACL rules would be applied here based on users, domains, IP addr., etc.

The http/https traffic would then go forcibly through a tproxy ssl-bump host (Squid #2) which would basically analyze/filter traffic via ICAP.

Has anyone already dealt with this problem, and how?

Regards,

Vieri



From dawood.aijaz97 at gmail.com  Mon Apr 20 23:22:27 2020
From: dawood.aijaz97 at gmail.com (Dawood Aijaz)
Date: Tue, 21 Apr 2020 04:22:27 +0500
Subject: [squid-users] Subject: Expose FTP data using Squid
Message-ID: <CAHkxSOkSmPTGXrgYwsH6KZiWHGrrOQ8OULbo1MOzTea0sSW0XA@mail.gmail.com>

 Hi,
Currently, I am developing a project in which I am required to monitor FTP
traffic. So I am using Squid proxy with FTP over HTTP enabled. The Squid
icap sends the request to the cap server however in the icap server it only
shows the "CONNECT request" to the destination FTP server but does not show
the request which actually contains the FTP data.
I want to see all the data passing  using FTP can somebody help me

Regards,
Dawood Aijaz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200421/816834b3/attachment.htm>

From squid3 at treenet.co.nz  Tue Apr 21 06:18:41 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2020 18:18:41 +1200
Subject: [squid-users] Subject: Expose FTP data using Squid
In-Reply-To: <CAHkxSOkSmPTGXrgYwsH6KZiWHGrrOQ8OULbo1MOzTea0sSW0XA@mail.gmail.com>
References: <CAHkxSOkSmPTGXrgYwsH6KZiWHGrrOQ8OULbo1MOzTea0sSW0XA@mail.gmail.com>
Message-ID: <edad229e-c3b5-8966-5600-adbe2c27c3d0@treenet.co.nz>

On 21/04/20 11:22 am, Dawood Aijaz wrote:
> ?Hi,
> Currently, I am developing?a project in which I am required to monitor
> FTP traffic. So I am using Squid proxy with FTP over HTTP enabled.


What do you mean by "FTP over HTTP" ? and how have you enabled it?

Squid has native support for both accessing FTP servers using HTTP
clients and acting as a proxy for FTP port.


> The
> Squid icap sends the request to the?cap server however in the icap
> server it only shows the "CONNECT request" to the destination FTP server
> but does not show the request which actually contains the FTP data.
> I want to see all the data passing? using FTP can somebody help me
> 

This sounds like you have configured the client FTP software to see HTTP
proxy. The CONNECT you see is all that happens with Squid in that situation.

Amos


From squid3 at treenet.co.nz  Tue Apr 21 06:28:53 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2020 18:28:53 +1200
Subject: [squid-users] tproxy sslbump and user authentication
In-Reply-To: <111546434.3854722.1587424111575@mail.yahoo.com>
References: <111546434.3854722.1587424111575.ref@mail.yahoo.com>
 <111546434.3854722.1587424111575@mail.yahoo.com>
Message-ID: <a2d8da57-6a3b-839a-7400-31b6ab467f99@treenet.co.nz>

On 21/04/20 11:08 am, Vieri wrote:
> Hi,
> 
> Is it possible to somehow combine the filtering capabilities of tproxy ssl-bump for access to https sites and the access control flexibility of proxy_auth (eg. kerberos)?

Please see the FAQ:
 <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Why_can.27t_I_use_authentication_together_with_interception_proxying.3F>


> 
> Is having two proxy servers in sequence an acceptable approach, or can it be done within the same instance with the CONNECT method?
> 
> My first approach would be to configure clients to send their user credentials to an explicit proxy (Squid #1) which would then proxy_auth via Kerberos to a PDC. ACL rules would be applied here based on users, domains, IP addr., etc.
> 
> The http/https traffic would then go forcibly through a tproxy ssl-bump host (Squid #2) which would basically analyze/filter traffic via ICAP.


Why bother with the second proxy at all? The explicit proxy has access
to all the details the interception one does (and more - such as
credentials). It should be able to do all filtering necessary.

TPROXY and NAT are for proxying traffic of clients which do not support
HTTP proxies. They are hugely limited in what they can do. If you have
ability to use explicit-proxy, do so.


Amos


From typudsta60 at gmail.com  Tue Apr 21 06:34:39 2020
From: typudsta60 at gmail.com (cryptexslayer)
Date: Tue, 21 Apr 2020 01:34:39 -0500 (CDT)
Subject: [squid-users] How to block images
Message-ID: <1587450879893-0.post@n4.nabble.com>

I am trying to save data by blocking .png, .jpg etc. I have tried for the
past 2 hours to setup ACL will block list but it doesn't seem to work.

Here is my current config





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Tue Apr 21 08:55:36 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 21 Apr 2020 10:55:36 +0200
Subject: [squid-users] squid 3.5 conf setup
In-Reply-To: <a9b83613-6fd1-324b-9908-8cf4a6237994@modmc.net>
References: <7e8b7c3b-71e8-a0f8-e1ea-091650bd23dd@modmc.net>
 <f9a2f5f0-faba-dbdd-b91d-257751eed41a@measurement-factory.com>
 <a9b83613-6fd1-324b-9908-8cf4a6237994@modmc.net>
Message-ID: <20200421085536.GB8129@fantomas.sk>

On 20.04.20 15:29, James Adams wrote:
>So I am new to squid and trying to take over from what was already in 
>place.
>Do you have an example of what it should look like or a best practice 
>guide to use.
>?We can have up to 32 /24 blocks in our configurations.
>I want to try and make the systems as efficient as possible knowing 
>the large amount of IPs we are using.

is there any need to use multiple IP addresses on squid for multiple client
subnets?
Can't one address manage all those clients?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Saving Private Ryan...
Private Ryan exists. Overwrite? (Y/N)


From squid3 at treenet.co.nz  Tue Apr 21 09:23:33 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Apr 2020 21:23:33 +1200
Subject: [squid-users] How to block images
In-Reply-To: <1587450879893-0.post@n4.nabble.com>
References: <1587450879893-0.post@n4.nabble.com>
Message-ID: <014d5686-5755-cf14-721a-17a91892c705@treenet.co.nz>

On 21/04/20 6:34 pm, cryptexslayer wrote:
> I am trying to save data by blocking .png, .jpg etc. I have tried for the
> past 2 hours to setup ACL will block list but it doesn't seem to work.
> 
> Here is my current config
> 

HTML content does not get delivered to this text-only mailing list.
Please include your config as part of the text rather than a display object.


Amos


From hdkutz at hdkutz.de  Tue Apr 21 11:06:11 2020
From: hdkutz at hdkutz.de (kutz)
Date: Tue, 21 Apr 2020 13:06:11 +0200
Subject: [squid-users] cache_peer configuration
Message-ID: <20200421110611.GA4470@hdkutz.de>

Hello List,
I'm trying to establish a proxy2proxy configuration with my
squid-3.5.28-1.el6.x86_64
on
Centos 6.10

cache_peer IP_OF_PEER parent 8080 0 no-query originserver name=server_1
acl sites_server_1 dstdomain url_1 url_2 url_3 url_4
cache_peer_access server_1 allow sites_server_1
cache_peer_access server_1 deny all

When trying to access one of the URLs the requests are not answered (or
delivered) by the remote proxy - instead local Squid tries to answer.
As my squid has no direct access to those URLs I'm getting 503 in my Squid Logs with a timeout in my
browers.

What am I missing here?

Cheers,
ku

Ben (Obi-Wan) Kenobi:
	A young Jedi named Darth Vader, who was a pupil
	of mine until he turned to evil, helped the
	Empire hunt down and destroy the Jedi knights.
	He betrayed and murdered your father.


From rentorbuy at yahoo.com  Tue Apr 21 12:33:10 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 21 Apr 2020 12:33:10 +0000 (UTC)
Subject: [squid-users] tproxy sslbump and user authentication
In-Reply-To: <a2d8da57-6a3b-839a-7400-31b6ab467f99@treenet.co.nz>
References: <111546434.3854722.1587424111575.ref@mail.yahoo.com>
 <111546434.3854722.1587424111575@mail.yahoo.com>
 <a2d8da57-6a3b-839a-7400-31b6ab467f99@treenet.co.nz>
Message-ID: <633376515.127049.1587472390341@mail.yahoo.com>


On Tuesday, April 21, 2020, 8:29:28 AM GMT+2, Amos Jeffries <squid3 at treenet.co.nz> wrote: 
>
> Please see the FAQ:
> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Why_can.27t_I_use_authentication_together_with_interception_proxying.3F>
>
> Why bother with the second proxy at all? The explicit proxy has access
> to all the details the interception one does (and more - such as
> credentials). It should be able to do all filtering necessary.

Can the explicit proxy ssl-bump HTTPS traffic and thus analyze traffic with ICAP + squidclamav, for instance?
Simply put, will I be able to block, eg. https://secure.eicar.org/eicarcom2.zip not by mimetype, file extension, url matching, etc., but by analyzing its content with clamav via ICAP?

> TPROXY and NAT are for proxying traffic of clients which do not support
> HTTP proxies. They are hugely limited in what they can do. If you have
> ability to use explicit-proxy, do so.

Unfortunately, some programs don't support proxies, or we simply don't care and want to force-filter traffic anyway.

Vieri


From uhlar at fantomas.sk  Tue Apr 21 12:40:55 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 21 Apr 2020 14:40:55 +0200
Subject: [squid-users] tproxy sslbump and user authentication
In-Reply-To: <633376515.127049.1587472390341@mail.yahoo.com>
References: <111546434.3854722.1587424111575.ref@mail.yahoo.com>
 <111546434.3854722.1587424111575@mail.yahoo.com>
 <a2d8da57-6a3b-839a-7400-31b6ab467f99@treenet.co.nz>
 <633376515.127049.1587472390341@mail.yahoo.com>
Message-ID: <20200421124055.GA13094@fantomas.sk>

>On Tuesday, April 21, 2020, 8:29:28 AM GMT+2, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>
>> Please see the FAQ:
>> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Why_can.27t_I_use_authentication_together_with_interception_proxying.3F>
>>
>> Why bother with the second proxy at all? The explicit proxy has access
>> to all the details the interception one does (and more - such as
>> credentials). It should be able to do all filtering necessary.

On 21.04.20 12:33, Vieri wrote:
>Can the explicit proxy ssl-bump HTTPS traffic and thus analyze traffic with ICAP + squidclamav, for instance?

yes.

>Simply put, will I be able to block, eg. 
> https://secure.eicar.org/eicarcom2.zip not by mimetype, file extension,
> url matching, etc., but by analyzing its content with clamav via ICAP?

without bumping, you won't be able to block by anything, only by secure.eicar.org
hostname.

>> TPROXY and NAT are for proxying traffic of clients which do not support
>> HTTP proxies. They are hugely limited in what they can do. If you have
>> ability to use explicit-proxy, do so.
>
>Unfortunately, some programs don't support proxies, or we simply don't care
> and want to force-filter traffic anyway.

of course, but it has drawbacks.
You need to create own certificate and push it to clients/applications.
Some applications may refuse the certificate anyway 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fucking windows! Bring Bill Gates! (Southpark the movie)


From squid3 at treenet.co.nz  Tue Apr 21 13:30:48 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2020 01:30:48 +1200
Subject: [squid-users] cache_peer configuration
In-Reply-To: <20200421110611.GA4470@hdkutz.de>
References: <20200421110611.GA4470@hdkutz.de>
Message-ID: <25f4bb06-106f-f7c7-14a3-4c4edd4cab96@treenet.co.nz>

On 21/04/20 11:06 pm, kutz wrote:
> Hello List,
> I'm trying to establish a proxy2proxy configuration with my
> squid-3.5.28-1.el6.x86_64
> on
> Centos 6.10
> 
> cache_peer IP_OF_PEER parent 8080 0 no-query originserver name=server_1

If the upstream is a proxy, do not tell Squid it is an origin server.


> acl sites_server_1 dstdomain url_1 url_2 url_3 url_4
> cache_peer_access server_1 allow sites_server_1
> cache_peer_access server_1 deny all
> 
> When trying to access one of the URLs the requests are not answered (or
> delivered) by the remote proxy - instead local Squid tries to answer.
> As my squid has no direct access to those URLs I'm getting 503 in my Squid Logs with a timeout in my
> browers.
> 
> What am I missing here?
> 

Some detail you are not providing for us either.

This is why we always ask for as much detail as possible. Logs, full
config. As much as you can about the failing transaction.


Amos


From rousskov at measurement-factory.com  Tue Apr 21 14:09:44 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Apr 2020 10:09:44 -0400
Subject: [squid-users] cache_peer configuration
In-Reply-To: <20200421110611.GA4470@hdkutz.de>
References: <20200421110611.GA4470@hdkutz.de>
Message-ID: <a1cd8495-d335-eff9-f7d0-ad7ce1ab8fb2@measurement-factory.com>

On 4/21/20 7:06 AM, kutz wrote:

> cache_peer IP_OF_PEER parent 8080 0 no-query originserver name=server_1
> acl sites_server_1 dstdomain url_1 url_2 url_3 url_4
> cache_peer_access server_1 allow sites_server_1
> cache_peer_access server_1 deny all
> 
> When trying to access one of the URLs the requests are not answered (or
> delivered) by the remote proxy - instead local Squid tries to answer.


I assume that the URL in question matches the sites_server_1 ACL. Please
note that IP-based URLs may not match that ACL, especially if the
reverse DNS lookup does not work the way you would expect it to work.

If the URL in question matches and setting "nonhierarchical_direct off"
does not help, please see Amos response.


HTH,

Alex.


From hdkutz at hdkutz.de  Tue Apr 21 14:25:52 2020
From: hdkutz at hdkutz.de (kutz)
Date: Tue, 21 Apr 2020 16:25:52 +0200
Subject: [squid-users] cache_peer configuration
In-Reply-To: <25f4bb06-106f-f7c7-14a3-4c4edd4cab96@treenet.co.nz>
References: <20200421110611.GA4470@hdkutz.de>
 <25f4bb06-106f-f7c7-14a3-4c4edd4cab96@treenet.co.nz>
Message-ID: <20200421142552.GB4470@hdkutz.de>

On Wed, Apr 22, 2020 at 01:30:48AM +1200, Amos Jeffries wrote:
> On 21/04/20 11:06 pm, kutz wrote:
> > Hello List,
> > I'm trying to establish a proxy2proxy configuration with my
> > squid-3.5.28-1.el6.x86_64
> > on
> > Centos 6.10
> > 
> > cache_peer IP_OF_PEER parent 8080 0 no-query originserver name=server_1
> 
> If the upstream is a proxy, do not tell Squid it is an origin server.
> 
> 
> > acl sites_server_1 dstdomain url_1 url_2 url_3 url_4
> > cache_peer_access server_1 allow sites_server_1
> > cache_peer_access server_1 deny all
> > 
> > When trying to access one of the URLs the requests are not answered (or
> > delivered) by the remote proxy - instead local Squid tries to answer.
> > As my squid has no direct access to those URLs I'm getting 503 in my Squid Logs with a timeout in my
> > browers.
> > 
> > What am I missing here?
> > 
> 
> Some detail you are not providing for us either.
> 
> This is why we always ask for as much detail as possible. Logs, full
> config. As much as you can about the failing transaction.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Hello Amos,
thanks for your suggestion.
I removed "originserver" from my config.
Still I see the requests answered by my own squid and not relayed over the cache_peer.

I'm limited with providing my squid configuration - I can't provide the config to hole
audience due to security restrictions.
Tell me which squid config details you like to have and I see what i can do.

Logs:
<snip>
Apr 21 16:18:12 2020.850     45 machine_1.domain.intern TCP_TUNNEL/200 1179 1329 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.852     47 machine_1.domain.intern TCP_TUNNEL/200 2222 1329 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.857     52 machine_1.domain.intern TCP_TUNNEL/200 1610 1334 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.862     53 machine_1.domain.intern TCP_TUNNEL/200 7189 1323 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.864     56 machine_1.domain.intern TCP_TUNNEL/200 1383 1321 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.864     57 machine_1.domain.intern TCP_TUNNEL/200 1651 1323 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.911     45 machine_1.domain.intern TCP_TUNNEL/200 1372 1336 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.920     48 machine_1.domain.intern TCP_TUNNEL/200 7762 1324 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.927     54 machine_1.domain.intern TCP_TUNNEL/200 1054 1362 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.929     55 machine_1.domain.intern TCP_TUNNEL/200 1039 1326 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.932     60 machine_1.domain.intern TCP_TUNNEL/200 13666 1313 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.932     55 machine_1.domain.intern TCP_TUNNEL/200 1730 1328 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.974     44 machine_1.domain.intern TCP_TUNNEL/200 2639 1325 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.976     45 machine_1.domain.intern TCP_TUNNEL/200 3061 1320 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:12 2020.999     46 machine_1.domain.intern TCP_TUNNEL/200 1204 1327 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.001     48 machine_1.domain.intern TCP_TUNNEL/200 2988 1330 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.033     81 machine_1.domain.intern TCP_TUNNEL/200 41463 1316 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.043     93 machine_1.domain.intern TCP_TUNNEL/200 1128 1326 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.058     53 machine_1.domain.intern TCP_TUNNEL/200 3689 1334 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.060     54 machine_1.domain.intern TCP_TUNNEL/200 2038 1329 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.064     58 machine_1.domain.intern TCP_TUNNEL/200 3407 1331 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.074     58 machine_1.domain.intern TCP_TUNNEL/200 6434 1329 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.122     45 machine_1.domain.intern TCP_TUNNEL/200 1349 1326 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.123     58 machine_1.domain.intern TCP_TUNNEL/200 2482 1334 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.125     59 machine_1.domain.intern TCP_TUNNEL/200 5282 1328 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.125     48 machine_1.domain.intern TCP_TUNNEL/200 1673 1327 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.126     59 machine_1.domain.intern TCP_TUNNEL/200 1600 1323 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.151     47 machine_1.domain.intern TCP_TUNNEL/200 1535 1321 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.196     53 machine_1.domain.intern TCP_TUNNEL/200 5397 1353 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.201     57 machine_1.domain.intern TCP_TUNNEL/200 1205 1356 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.202     58 machine_1.domain.intern TCP_TUNNEL/200 1312 1353 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.209     64 machine_1.domain.intern TCP_TUNNEL/200 1764 1357 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.237     92 machine_1.domain.intern TCP_TUNNEL/200 37417 1337 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.271     57 machine_1.domain.intern TCP_TUNNEL/200 3478 1337 CONNECT url_1:443 - HIER_DIRECT/url_1 -
Apr 21 16:18:13 2020.272     58 machine_1.domain.intern TCP_TUNNEL/200 6071 1344 CONNECT url_1:443 - HIER_DIRECT/url_1 -
<snip>

-- 
https://www.xing.com/hp/HansDieter_Kutz/
http://www.hdkutz.de
Using the Force, Yoda effortlessly frees the X-Wing from the bog.
Luke Skywalker:
	I don't believe it.
Yoda:
	That is why you fail.


From squid3 at treenet.co.nz  Tue Apr 21 14:39:32 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Apr 2020 02:39:32 +1200
Subject: [squid-users] cache_peer configuration
In-Reply-To: <20200421142552.GB4470@hdkutz.de>
References: <20200421110611.GA4470@hdkutz.de>
 <25f4bb06-106f-f7c7-14a3-4c4edd4cab96@treenet.co.nz>
 <20200421142552.GB4470@hdkutz.de>
Message-ID: <bc5a75d0-1de7-858e-0335-6fe3f00b25e1@treenet.co.nz>

On 22/04/20 2:25 am, kutz wrote:
> On Wed, Apr 22, 2020 at 01:30:48AM +1200, Amos Jeffries wrote:
>> On 21/04/20 11:06 pm, kutz wrote:
>>> Hello List,
>>> I'm trying to establish a proxy2proxy configuration with my
>>> squid-3.5.28-1.el6.x86_64
>>> on
>>> Centos 6.10
>>>
>>> cache_peer IP_OF_PEER parent 8080 0 no-query originserver name=server_1
>>
>> If the upstream is a proxy, do not tell Squid it is an origin server.
>>
>>
>>> acl sites_server_1 dstdomain url_1 url_2 url_3 url_4
>>> cache_peer_access server_1 allow sites_server_1
>>> cache_peer_access server_1 deny all
>>>
>>> When trying to access one of the URLs the requests are not answered (or
>>> delivered) by the remote proxy - instead local Squid tries to answer.
>>> As my squid has no direct access to those URLs I'm getting 503 in my Squid Logs with a timeout in my
>>> browers.
>>>
>>> What am I missing here?
>>>
>>
>> Some detail you are not providing for us either.
>>
>> This is why we always ask for as much detail as possible. Logs, full
>> config. As much as you can about the failing transaction.
>>
>>
>> Amos


> 
> Hello Amos,
> thanks for your suggestion.
> I removed "originserver" from my config.
> Still I see the requests answered by my own squid and not relayed over the cache_peer.
> 
> I'm limited with providing my squid configuration - I can't provide the config to hole
> audience due to security restrictions.
> Tell me which squid config details you like to have and I see what i can do.
> 
> Logs:
> <snip>
> Apr 21 16:18:12 2020.850     45 machine_1.domain.intern TCP_TUNNEL/200 1179 1329 CONNECT url_1:443 - HIER_DIRECT/url_1 -


What *_port line(s) do you have configured in the frontend proxy?


Amos


From dawood.aijaz97 at gmail.com  Tue Apr 21 15:26:05 2020
From: dawood.aijaz97 at gmail.com (Dawood Aijaz)
Date: Tue, 21 Apr 2020 20:26:05 +0500
Subject: [squid-users] setup FTP proxy and FTP content monitoring
Message-ID: <CAHkxSOnrYBsVrkPV=dKvDxq8FUv6Y1rCf4ALzTG7Pbvmwa2uDA@mail.gmail.com>

Hi,
I am currently working on a task to monitor FTP traffic and analyze it.
So can somebody help me to set up FTP proxy for squid and to analyze FTP
data

Regards,
Dawood Aijaz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200421/696024da/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Apr 21 15:33:01 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 21 Apr 2020 17:33:01 +0200
Subject: [squid-users] setup FTP proxy and FTP content monitoring
In-Reply-To: <CAHkxSOnrYBsVrkPV=dKvDxq8FUv6Y1rCf4ALzTG7Pbvmwa2uDA@mail.gmail.com>
References: <CAHkxSOnrYBsVrkPV=dKvDxq8FUv6Y1rCf4ALzTG7Pbvmwa2uDA@mail.gmail.com>
Message-ID: <202004211733.01912.Antony.Stone@squid.open.source.it>

On Tuesday 21 April 2020 at 17:26:05, Dawood Aijaz wrote:

> Hi,
> I am currently working on a task to monitor FTP traffic and analyze it.
> So can somebody help me to set up FTP proxy for squid and to analyze FTP
> data

Squid supports FTP natively.  You don't need to configure anything special 
provided your FTP setup is operating on standard ports etc.

Have you tried telling your client/s to use Squid for FTP and run into 
problems?

If so, give us details of your network arrangement, what configuration you have 
done, and what problems you run into, and we can try to help.

If you haven't tried yet, give it a go and see if it "just works" :)

Regarding the data analysis, this is not really a Squid question - we'd need 
to know what sort of analysis you want to do and what sort of results you're 
looking to get out of the end of it, and that question may be more suited to 
another forum such as Icinga or Grafana, for example.

Squid will do the proxying, but it's not a data analysis tool.


Regards,


Antony

-- 
The next sentence is untrue.
The previous sentence is true.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From hdkutz at hdkutz.de  Tue Apr 21 17:21:47 2020
From: hdkutz at hdkutz.de (kutz)
Date: Tue, 21 Apr 2020 19:21:47 +0200
Subject: [squid-users] cache_peer configuration
In-Reply-To: <bc5a75d0-1de7-858e-0335-6fe3f00b25e1@treenet.co.nz>
References: <20200421110611.GA4470@hdkutz.de>
 <25f4bb06-106f-f7c7-14a3-4c4edd4cab96@treenet.co.nz>
 <20200421142552.GB4470@hdkutz.de>
 <bc5a75d0-1de7-858e-0335-6fe3f00b25e1@treenet.co.nz>
Message-ID: <20200421172147.GC4470@hdkutz.de>

On Wed, Apr 22, 2020 at 02:39:32AM +1200, Amos Jeffries wrote:
> On 22/04/20 2:25 am, kutz wrote:
> > On Wed, Apr 22, 2020 at 01:30:48AM +1200, Amos Jeffries wrote:
> >> On 21/04/20 11:06 pm, kutz wrote:
> >>> Hello List,
> >>> I'm trying to establish a proxy2proxy configuration with my
> >>> squid-3.5.28-1.el6.x86_64
> >>> on
> >>> Centos 6.10
> >>>
> >>> cache_peer IP_OF_PEER parent 8080 0 no-query originserver name=server_1
> >>
> >> If the upstream is a proxy, do not tell Squid it is an origin server.
> >>
> >>
> >>> acl sites_server_1 dstdomain url_1 url_2 url_3 url_4
> >>> cache_peer_access server_1 allow sites_server_1
> >>> cache_peer_access server_1 deny all
> >>>
> >>> When trying to access one of the URLs the requests are not answered (or
> >>> delivered) by the remote proxy - instead local Squid tries to answer.
> >>> As my squid has no direct access to those URLs I'm getting 503 in my Squid Logs with a timeout in my
> >>> browers.
> >>>
> >>> What am I missing here?
> >>>
> >>
> >> Some detail you are not providing for us either.
> >>
> >> This is why we always ask for as much detail as possible. Logs, full
> >> config. As much as you can about the failing transaction.
> >>
> >>
> >> Amos
> 
> 
> > 
> > Hello Amos,
> > thanks for your suggestion.
> > I removed "originserver" from my config.
> > Still I see the requests answered by my own squid and not relayed over the cache_peer.
> > 
> > I'm limited with providing my squid configuration - I can't provide the config to hole
> > audience due to security restrictions.
> > Tell me which squid config details you like to have and I see what i can do.
> > 
> > Logs:
> > <snip>
> > Apr 21 16:18:12 2020.850     45 machine_1.domain.intern TCP_TUNNEL/200 1179 1329 CONNECT url_1:443 - HIER_DIRECT/url_1 -
> 
> 
> What *_port line(s) do you have configured in the frontend proxy?
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

The hint from Alex did the trick.
After inserting
nonhierarchical_direct off
into squid.conf it worked.
Thanks for the hint.

-- 
Han Solo:
	Wonderful girl! Either I'm going to kill her
	or I'm beginning to like her.


From tarotapprentice at yahoo.com  Wed Apr 22 11:56:31 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Wed, 22 Apr 2020 21:56:31 +1000
Subject: [squid-users] QUIC support in Squid
References: <4206B4C5-B0C7-46D4-B46A-FD7F17E1F60E.ref@yahoo.com>
Message-ID: <4206B4C5-B0C7-46D4-B46A-FD7F17E1F60E@yahoo.com>

I know QUIC has been around for a while. I see the IETF have a proposed standard[1].

OpenSSL have also expressed interest, but not until OpenSSL 3 is out[2].

Are there any plans for Squid to support the QUIC protocol in a future version?

[1] https://datatracker.ietf.org/doc/draft-ietf-quic-transport/
[2] https://www.openssl.org/blog/blog/2020/02/17/QUIC-and-OpenSSL/

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200422/182a3503/attachment.htm>

From squid3 at treenet.co.nz  Wed Apr 22 12:56:47 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 00:56:47 +1200
Subject: [squid-users] QUIC support in Squid
In-Reply-To: <4206B4C5-B0C7-46D4-B46A-FD7F17E1F60E@yahoo.com>
References: <4206B4C5-B0C7-46D4-B46A-FD7F17E1F60E.ref@yahoo.com>
 <4206B4C5-B0C7-46D4-B46A-FD7F17E1F60E@yahoo.com>
Message-ID: <737cdd94-2d4d-98b0-7bc9-4d65e209bec2@treenet.co.nz>

On 22/04/20 11:56 pm, TarotApprentice wrote:
> I know QUIC has been around for a while. I see the IETF have a proposed
> standard[1].
> 
> OpenSSL have also expressed interest, but not until OpenSSL 3 is out[2].
> 
> Are there any plans for Squid to support the QUIC protocol in a future
> version?

We have been looking into it, but no solid plans yet. I expect we will
be implementing HTTP/3 instead though.


Amos


From dawood.aijaz97 at gmail.com  Wed Apr 22 13:48:57 2020
From: dawood.aijaz97 at gmail.com (Dawood Aijaz)
Date: Wed, 22 Apr 2020 18:48:57 +0500
Subject: [squid-users] setup FTP proxy and FTP content monitoring
	(Antony Stone)
Message-ID: <CAHkxSO=EGaksn9aLakZfA=n6NVPB9RfspLc7CSnLT63-QRK+qw@mail.gmail.com>

Hi,

these are my squid.conf configurations

acl localnet src 10.0.0.0/8

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

http_access allow all

http_access deny !Safe_ports


http_access deny CONNECT !SSL_ports


http_access allow localhost manager
http_access deny manager


http_access allow localnet
http_access allow localhost

http_port 8001

coredump_dir /var/spool/squid

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320

acl FTP proto FTP
always_direct allow FTP

acl FTP proto FTP
always_direct allow FTP

icap_enable on

icap_preview_enable on

icap_service service_req reqmod_precache icap://127.0.0.1:8008/reqmod
bypass=0
icap_service service_resp reqmod_precache icap://127.0.0.1:8008/respmod
bypass=0

adaptation_access service_req allow all
adaptation_access service_resp allow all


"a*ll I want from squid proxy is to intercept FTP and expose all the FTP
data "*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200422/dd6120e9/attachment.htm>

From andreas at canonical.com  Wed Apr 22 14:29:35 2020
From: andreas at canonical.com (Andreas Hasenack)
Date: Wed, 22 Apr 2020 11:29:35 -0300
Subject: [squid-users] logformat: override %tg, but keep subsecond resolution
Message-ID: <CANYNYEGeZstc=m=sT2qKZ5vkkFn-Y5pbs88pzpSjCyxv6RYLtg@mail.gmail.com>

Hi,

I'm trying to override the %tg log format but keep its subsecond time
resolution.

As a reminder, %tg gives you this:

22/Apr/2020:14:14:18.360

I would like it to be

2020-04-22 14:14:18.360 +0000

I tried this first: %tg{%F %H:%M:%S %z}

That gave me 2020-04-22 14:14:18 +0000, so it's just missing the
milliseconds. And this is where I couldn't figure it out anymore.

Some combinations I tried, ranging from thoughtful experiments to just
random attempts:
%tg{%F %H:%M:%S %z}.%03tu -> 2020-04-22 14:14:18 +0000.360
%tg{%F %H:%M:%S.%03f %z} -> 2020-04-22 14:14:18.%03f +0000
%tg{%F %H:%M:%S.%03tu %z} -> ??? 2020-04-22 14:14:18.00  u +0000

What's the right format string I should use? I also checked
strftime(3) but couldn't figure it out.

Thanks!


From squid3 at treenet.co.nz  Thu Apr 23 09:02:59 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 21:02:59 +1200
Subject: [squid-users] [squid-announce] Squid 4.11 is available
Message-ID: <aa911a27-1628-e4ad-1e9c-33b461d0660e@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.11 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:


 * SQUID-2019:12 Multiple issues in ESI Response processing
   (CVE-2019-12519, CVE-2019-12521)

These problems allow a remote server delivering certain ESI
response syntax to trigger a buffer overflow.

On systems with heap overflow protection overflow will shutdown
the proxy causing a denial of service for all clients accessing
the Squid service.

On systems with ESI buffer pooling (the default) overflow will
truncate portions of generated payloads. Poisoning the HTTP
response cache with corrupted objects.

The CVE-2019-12519 issue also overwrites arbitrary attacker
controlled information onto the process stack. Allowing remote
code execution with certain crafted ESI payloads.

These problems are restricted to ESI responses received from an
upstream server. Attackers have to compromise the server or
transmission channel to utilize these vulnerabilities.

See the advisory for updated patches:
 <http://www.squid-cache.org/Advisories/SQUID-2019_12.txt>


 * SQUID-2020:4 Multiple issues in HTTP Digest authentication.
   (CVE-2020-11945)

Due to an integer overflow bug Squid is vulnerable to credential
replay and remote code execution attacks against HTTP Digest
Authentication tokens.

When memory pooling is used this problem allows a remote client
to replay a sniffed Digest Authentication nonce to gain access
to resources that are otherwise forbidden.

When memory pooling is disabled this problem allows a remote
client to perform remote code execution through the free'd nonce
credentials.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2020_4.txt>


* SQUID-2019:11 (CVE-2019-18679) complete fix

The initial patch for this vulnerability significantly hardened
against attacks. However it was still possible for an attacker
to gain information over time about a Squid instance.

This release completely removes that possibility.


 * Bug 5036: capital 'L's in logs when daemon queue overflows

This shows up on proxies which are too busy for the daemon I/O
or trying to output very long access.log lines.

This but is just an annoyance, all other operations of the proxy
remain unaffected but the extra characters can interferes with
data processing of the logs.


 * Bug 5022: Reconfigure kills Coordinator in SMP+ufs configurations

This bug shows up on caching proxies with multiple SMP workers. The
visible symptoms are;
 - SNMP begins producing errors or NULL values instead of data,
 - cache manager reports indicate no traffic, or zero values
 - possibly reduced cache HIT rate


 * Bug 5016: systemd thinks Squid is ready before Squid listens

systemd has been found to still have problems with the recent
--foreground behaviour updates. This release adds support for the
sd_notify systemd feature to workaround that problem.

Please note this automatically adds libsystemd dependency when that
library is available on the build machine. To prevent this dependency
and retain the existing behavuiour the --without-systemd build option
is provided.



  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Apr 23 09:03:02 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 21:03:02 +1200
Subject: [squid-users] [squid-announce] Squid 5.0.2 beta is available
Message-ID: <403ee08e-7768-7a68-1417-92e9fe26f7d8@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-5.0.2 beta release!


This release is a security and feature update release resolving
several issues found in the prior Squid releases.


The major changes to be aware of:


 * SQUID-2019:12 Multiple issues in ESI Response processing
   (CVE-2019-12519, CVE-2019-12521)

These problems allow a remote server delivering certain ESI
response syntax to trigger a buffer overflow.

On systems with heap overflow protection overflow will shutdown
the proxy causing a denial of service for all clients accessing
the Squid service.

On systems with ESI buffer pooling (the default) overflow will
truncate portions of generated payloads. Poisoning the HTTP
response cache with corrupted objects.

The CVE-2019-12519 issue also overwrites arbitrary attacker
controlled information onto the process stack. Allowing remote
code execution with certain crafted ESI payloads.

These problems are restricted to ESI responses received from an
upstream server. Attackers have to compromise the server or
transmission channel to utilize these vulnerabilities.

See the advisory for updated patches:
 <http://www.squid-cache.org/Advisories/SQUID-2019_12.txt>


 * SQUID-2020:4 Multiple issues in HTTP Digest authentication.
   (CVE-2020-11945)

Due to an integer overflow bug Squid is vulnerable to credential
replay and remote code execution attacks against HTTP Digest
Authentication tokens.

When memory pooling is used this problem allows a remote client
to replay a sniffed Digest Authentication nonce to gain access
to resources that are otherwise forbidden.

When memory pooling is disabled this problem allows a remote
client to perform remote code execution through the free'd nonce
credentials.

See the advisory for more details:
 <http://www.squid-cache.org/Advisories/SQUID-2020_4.txt>


* SQUID-2019:11 (CVE-2019-18679) complete fix

The initial patch for this vulnerability significantly hardened
against attacks. However it was still possible for an attacker
to gain information over time about a Squid instance.

This release completely removes that possibility.


 * Bug 5030: Negative responses are never cached

This bug shows up as cacheable 4xx and 5xx responses not being
cached despite negative_ttl configuration. This release brings
4xx and 5xx responses inline with the expected caching behaviour.


 * Bug 4796: comm.cc !isOpen(conn->fd) assertion when rotating logs

Please note that as of this fix cache.log and stderr output from
several Squid processes has changed significantly.

All output from the Squid master process is now delivered to its
stderr and logged by the OS according to kernel policy for daemons.
Typically that means the kernel 'messages' log and/or system boot
log. This includes all information logged during the lifetime of
the Squid instance by the master process.

All output from the coordinator and kid processes is logged to
cache.log. cache.log is opened slightly earlier than in previous
Squid releases, and information logged prior to its opening is no
longer logged.


 * High precision time units

This feature addition to squid.conf allows some configuration options
to accept high precision (nanosecond) resolution settings. At this
time most settings are left with their existing range of values.
Changes are detailed in the release notes for altered directives.


  All users of Squid-5 are urged to upgrade as soon as possible.

  All users of Squid-4 and older are encouraged to plan for upgrade.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v5/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Apr 23 09:02:57 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 21:02:57 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:4 Multiple
	issues in HTTP	Digest authentication
Message-ID: <93cd1cb2-720b-c8e2-054a-df3a3a9a7239@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2020:4
__________________________________________________________________

Advisory ID:        SQUID-2020:4
Date:               April 23, 2020
Summary:            Multiple issues
                    in HTTP Digest authentication.
Affected versions:  Squid 2.x -> 2.7.STABLE9
                    Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.10
                    Squid 5.x -> 5.0.1
Fixed in version:   Squid 4.11 and 5.0.2
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2020_4.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11945
__________________________________________________________________

Problem Description:

 Due to an integer overflow bug Squid is vulnerable to credential
 replay and remote code execution attacks against HTTP Digest
 Authentication tokens.

__________________________________________________________________

Severity:

 When memory pooling is used this problem allows a remote client
 to replay a sniffed Digest Authentication nonce to gain access
 to resources that are otherwise forbidden.

 When memory pooling is disabled this problem allows a remote
 client to perform remote code execution through the free'd nonce
 credentials.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid versions 4.11 and 5.0.2.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-eeebf0f37a72a2de08348e85ae34b02c34e9a811.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x up to and including 2.4.STABLE7 are not vulnerable.

 All Squid-2.5 up to and including 2.7.STABLE9 are vulnerable.

 All Squid-2.x up to and including 2.7.STABLE9 configured with
 "auth_param digest" are vulnerable.

 All Squid-2.x up to and including 2.7.STABLE9 configured without
 "auth_param digest" are not vulnerable.

 All Squid-3.x up to and including 3.5.28 built with
 --disable-auth are not vulnerable.

 All Squid-3.2 up to and including 3.5.28 built with
 --disable-auth-digest are not vulnerable.

 All Squid-3.x up to and including 3.5.28 configured with
 "auth_param digest" are vulnerable.

 All Squid-3.x up to and including 3.5.28 configured without
 "auth_param digest" are not vulnerable.

 All Squid-4.x up to and including 4.10 built with
 --disable-auth are not vulnerable.

 All Squid-4.x up to and including 4.10 built with
 --disable-auth-digest are not vulnerable.

 All Squid-4.x up to and including 4.10 configured with
 "auth_param digest" are vulnerable.

 All Squid-4.x up to and including 4.10 configured without
 "auth_param digest" are not vulnerable.

 Squid-5.0.1 built with --disable-auth-digest is not vulnerable.

 Squid-5.0.1 configured with "auth_param digest" are vulnerable.

 Squid-5.0.1 configured without "auth_param digest" are not
 vulnerable.

__________________________________________________________________

Workaround:

Either,

 Remove all "auth_param digest" lines from squid.conf

Or,

 Build Squid with --disable-auth-digest

Or,

 Build Squid with --disable-auth

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Cl?ment Berthaux and
 Florian Guilbert of Synacktiv.

 Fixed by Maxime Desbrus of Synacktiv.

__________________________________________________________________

Revision history:

 2019-11-20 13:39:07 UTC Initial Report
 2020-04-02 11:16:45 UTC Patches Released
 2020-04-20 20:08:14 UTC CVE Assignment
 2020-04-23 08:00:00 UTC Advisory Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Thu Apr 23 09:03:07 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 21:03:07 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2019:12 Multiple
	issues in ESI	Response processing
Message-ID: <bdb18b4f-3ea9-54ab-a145-a0427715f8f5@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2019:12
__________________________________________________________________

Advisory ID:        SQUID-2019:12
Date:               April 23, 2020
Summary:            Multiple issues
                    in ESI Response processing.
Affected versions:  Squid 3.x -> 3.5.28
                    Squid 4.x -> 4.10
                    Squid 5.x -> 5.0.1
Fixed in version:   Squid 4.11 and 5.0.2
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2019_12.txt
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12519
    http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12521
__________________________________________________________________

Problem Description:

 Due to incorrect buffer handling Squid is vulnerable to cache
 poisoning, remote execution, and denial of service attacks when
 processing ESI responses.

__________________________________________________________________

Severity:

 These problems allow a remote server delivering certain ESI
 response syntax to trigger a buffer overflow.

 On systems with heap overflow protection overflow will shutdown
 the proxy causing a denial of service for all clients accessing
 the Squid service.

 On systems with ESI buffer pooling (the default) overflow will
 truncate portions of generated payloads. Poisoning the HTTP
 response cache with corrupted objects.

 The CVE-2019-12519 issue also overwrites arbitrary attacker
 controlled information onto the process stack. Allowing remote
 code execution with certain crafted ESI payloads.

 These problems are restricted to ESI responses received from an
 upstream server. Attackers have to compromise the server or
 transmission channel to utilize these vulnerabilities.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid versions 4.11 and 5.0.2.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-fdd4123629320aa1ee4c3481bb392437c90d188d.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x are not vulnerable.

 All Squid built with --disable-esi are not vulnerable.

 All Squid-3.0 versions built without --enable-esi are not
 vulnerable.

 All Squid-3.x versions built with --enable-esi are vulnerable.

 All Squid-4.x up to and including Squid-4.10 are vulnerable.

 Squid-5.0.1 is not vulnerable to the CVE-2019-12519 remote code
 execution issue.

 Squid-5.0.1 is vulnerable to the CVE-2019-12521 issues.

__________________________________________________________________

Workaround:

 Build Squid with --disable-esi

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jeriko One
 <jeriko.one at gmx.us>.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-05-14 14:56:49 UTC Initial Report
 2019-05-20 11:23:13 UTC Patches Released
 2019-06-05 15:52:17 UTC CVE Assignment
 2020-04-23 08:00:00 UTC Advisory Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From Antony.Stone at squid.open.source.it  Thu Apr 23 09:56:43 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 23 Apr 2020 11:56:43 +0200
Subject: [squid-users] setup FTP proxy and FTP content monitoring
	(Antony Stone)
In-Reply-To: <CAHkxSO=EGaksn9aLakZfA=n6NVPB9RfspLc7CSnLT63-QRK+qw@mail.gmail.com>
References: <CAHkxSO=EGaksn9aLakZfA=n6NVPB9RfspLc7CSnLT63-QRK+qw@mail.gmail.com>
Message-ID: <202004231156.43406.Antony.Stone@squid.open.source.it>

On Wednesday 22 April 2020 at 15:48:57, Dawood Aijaz wrote:

> "a*ll I want from squid proxy is to intercept FTP and expose all the FTP
> data "*

I think you're looking at the wrong tool for a job like this.

When you say "intercept" it sounds like you want something which will act in 
between an FTP client and an FTP server, without either of them being 
configured to use it or knowing that it is there.

Squid is not such a thing.  It *can* be made work in intercept mode for HTTP, 
but not (as far as I know) for FTP.

When you say "expose all the FTP data", the simplest approach to this might be 
a packet capture application on your router (such as tshark), getting data 
from ports 20 and 21 (although active FTP mode would make this considerably 
more challenging).

Maybe you want to look at a tool such as frox.  It's an old project, but then 
FTP is an old protocol (and frankly I'm surprised that anyone wants to use 
something so insecure these days).


Best wishes,


Antony.

-- 
I bought a book on memory techniques, but I've forgotten where I put it.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Thu Apr 23 09:56:31 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 21:56:31 +1200
Subject: [squid-users] logformat: override %tg,
 but keep subsecond resolution
In-Reply-To: <CANYNYEGeZstc=m=sT2qKZ5vkkFn-Y5pbs88pzpSjCyxv6RYLtg@mail.gmail.com>
References: <CANYNYEGeZstc=m=sT2qKZ5vkkFn-Y5pbs88pzpSjCyxv6RYLtg@mail.gmail.com>
Message-ID: <54a4f0b1-4c92-e6aa-503a-abc4c3cd40ef@treenet.co.nz>

On 23/04/20 2:29 am, Andreas Hasenack wrote:
> Hi,
> 
> I'm trying to override the %tg log format but keep its subsecond time
> resolution.
> 
> As a reminder, %tg gives you this:
> 
> 22/Apr/2020:14:14:18.360
> 
> I would like it to be
> 
> 2020-04-22 14:14:18.360 +0000
> 
> I tried this first: %tg{%F %H:%M:%S %z}
> 
> That gave me 2020-04-22 14:14:18 +0000, so it's just missing the
> milliseconds. And this is where I couldn't figure it out anymore.
> 
> Some combinations I tried, ranging from thoughtful experiments to just
> random attempts:
> %tg{%F %H:%M:%S %z}.%03tu -> 2020-04-22 14:14:18 +0000.360
> %tg{%F %H:%M:%S.%03f %z} -> 2020-04-22 14:14:18.%03f +0000
> %tg{%F %H:%M:%S.%03tu %z} -> ??? 2020-04-22 14:14:18.00  u +0000
> 

The content of {} brackets on Squid time macros is all arguments for
strftime(). Supported codes are system locale dependent. Documentation
can be found at <http://www.cplusplus.com/reference/ctime/strftime/>

The characters outside {} are Squid logformat codes. Documentation for
those can be found at <http://www.squid-cache.org/Doc/config/logformat/>


> What's the right format string I should use? I also checked
> strftime(3) but couldn't figure it out.
> 

strftime() displays UNIX time values. Unix time_t has a 1 second
resolution, so sub-second values are not supported.

To inject a custom value into the middle of a standardized time format
string you need multiple %code's for Squid. Like so:

  %tg{%F %H:%M:%S}.%03tu %tg{%z}


Or, since the timezone is always expected to be "+0000" you can just
configure this and save CPU cycles looking up the timezone offset:

 logformat customTime %tg{%F %H:%M:%S}.%03tu +0000


HTH
Amos


From silamael at coronamundi.de  Thu Apr 23 11:41:03 2020
From: silamael at coronamundi.de (Silamael Darkomen)
Date: Thu, 23 Apr 2020 13:41:03 +0200
Subject: [squid-users] Squid 4.11 not building with Heimdal Kerberos
Message-ID: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>

Hi,

Just trying to build the new Squid 4.11 with Heimdal as Kerberos5 library.
Unfortunately, the enctype fix made in
src/acl/external/kerberos_ldap_group/support_krb5.cc does not compile
with Heimdal.
Their krb5_creds structure does not have a keyblock member.

For now I disabled the fix with #ifndef USE_HEIMDAL_KRB5.
No clue, if this enctype fix is needed for Heimdal too and how it has to
be made.

Greetings,
Matthias


From squid3 at treenet.co.nz  Thu Apr 23 11:50:42 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Apr 2020 23:50:42 +1200
Subject: [squid-users] Squid 4.11 not building with Heimdal Kerberos
In-Reply-To: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
References: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
Message-ID: <0384d433-888b-3783-5ddd-4bbd089ce549@treenet.co.nz>

On 23/04/20 11:41 pm, Silamael Darkomen wrote:
> Hi,
> 
> Just trying to build the new Squid 4.11 with Heimdal as Kerberos5 library.
> Unfortunately, the enctype fix made in
> src/acl/external/kerberos_ldap_group/support_krb5.cc does not compile
> with Heimdal.
> Their krb5_creds structure does not have a keyblock member.
> 
> For now I disabled the fix with #ifndef USE_HEIMDAL_KRB5.
> No clue, if this enctype fix is needed for Heimdal too and how it has to
> be made.


Please open a bugzilla report about this so it does not get forgotten.


Amos


From belle at bazuin.nl  Thu Apr 23 12:00:02 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 23 Apr 2020 14:00:02 +0200
Subject: [squid-users] Squid 4.11 not building with ssl enable on Buster
In-Reply-To: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
References: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
Message-ID: <vmime.5ea18342.7ae3.1b4750c7670838d1@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

Im currently building squid 411 on debian buster. Cowbuilder setup. 
I re-used the debian.tar.gz from squid-4.10-1 Debian Testing/Sid. 
Which i have done since squid 3.2, first time it fails. 
but only AMD64 fails to build, while i386 build fine. 
That a part im not familiar with, any suggestions?  

part of the logs with the errors. of the amd64 build. 

x86_64-linux-gnu-g++ -DHAVE_CONFIG_H -DDEFAULT_CONFIG_FILE=\"/etc/squid/squid.conf\" -DDEFAULT_SQUID_DATA_DIR=\"/usr/share/squid\" -DDEFAULT_SQUID_CONFIG_DIR=\"/etc/squid\"   -I.. -I../include -I../lib -I../src -I../include  -isystem /usr/include/mit-krb5  -I../src  -isystem /usr/include/mit-krb5 -Wdate-time -D_FORTIFY_SOURCE=2 -I/usr/include/libxml2 -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Woverloaded-virtual -Werror -pipe -D_REENTRANT -I/usr/include/libxml2 -I/usr/include/p11-kit-1 -g -O2 -fdebug-prefix-map=/build/squid-4.11=. -fstack-protector-strong -Wformat -Werror=format-security -c -o CommandLine.o CommandLine.cc
In file included from ../src/sbuf/SBuf.h:16,
                 from ../src/anyp/PortCfg.h:16,
                 from ../src/AccessLogEntry.h:12,
                 from acl/FilledChecklist.h:12,
                 from client_side.cc:61:
client_side.cc: In function 'void clientListenerConnectionOpened(AnyP::PortCfgPointer&, Ipc::FdNoteId, const Pointer&)':
client_side.cc:3663:27: error: 'Extra' is not a member of 'Debug'
                    Debug::Extra << "sd_notify() error: " << xstrerr(-result));
                           ^~~~~
../src/Debug.h:133:21: note: in definition of macro 'debugs'
             _dbo << CONTENT; \
                     ^~~~~~~
make[4]: *** [Makefile:7011: client_side.o] Error 1
make[4]: *** Waiting for unfinished jobs....
make[4]: Leaving directory '/build/squid-4.11/src'
make[3]: *** [Makefile:7128: all-recursive] Error 1
make[3]: Leaving directory '/build/squid-4.11/src'
make[2]: *** [Makefile:6138: all] Error 2
make[2]: Leaving directory '/build/squid-4.11/src'
make[1]: *** [Makefile:586: all-recursive] Error 1
make[1]: Leaving directory '/build/squid-4.11'
make: *** [/usr/share/cdbs/1/class/makefile.mk:77: debian/stamp-makefile-build] Error 2
dpkg-buildpackage: error: debian/rules build subprocess returned exit status 2



Best regards, 

Louis







From squid3 at treenet.co.nz  Thu Apr 23 12:28:28 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2020 00:28:28 +1200
Subject: [squid-users] Squid 4.11 not building with ssl enable on Buster
In-Reply-To: <vmime.5ea18342.7ae3.1b4750c7670838d1@ms249-lin-003.rotterdam.bazuin.nl>
References: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
 <vmime.5ea18342.7ae3.1b4750c7670838d1@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <7a0f9fdc-729f-f6a3-4422-7c8b0528adea@treenet.co.nz>

On 24/04/20 12:00 am, L.P.H. van Belle wrote:
> Hai, 
> 
> Im currently building squid 411 on debian buster. Cowbuilder setup. 
> I re-used the debian.tar.gz from squid-4.10-1 Debian Testing/Sid. 
> Which i have done since squid 3.2, first time it fails. 
> but only AMD64 fails to build, while i386 build fine. 
> That a part im not familiar with, any suggestions?  
> 
...
> client_side.cc: In function 'void clientListenerConnectionOpened(AnyP::PortCfgPointer&, Ipc::FdNoteId, const Pointer&)':
> client_side.cc:3663:27: error: 'Extra' is not a member of 'Debug'

You will need to add this patch:

 <https://github.com/squid-cache/squid/commit/c26cd1cb6a60ff196ef13c00e82576d3bfeb2e30>


Amos


From belle at bazuin.nl  Thu Apr 23 12:31:10 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 23 Apr 2020 14:31:10 +0200
Subject: [squid-users] Squid 4.11 not building with ssl enable on Buster
In-Reply-To: <7a0f9fdc-729f-f6a3-4422-7c8b0528adea@treenet.co.nz>
References: <vmime.5ea18342.7ae3.1b4750c7670838d1@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.5ea18a8e.41c3.478ef5e27ab0c86e@ms249-lin-003.rotterdam.bazuin.nl>

I'll report as some as i know.
I'll add it to the build, thats for the very quick reply! 

You guys are the best. 

Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Amos Jeffries
> Verzonden: donderdag 23 april 2020 14:28
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Squid 4.11 not building with ssl 
> enable on Buster
> 
> On 24/04/20 12:00 am, L.P.H. van Belle wrote:
> > Hai, 
> > 
> > Im currently building squid 411 on debian buster. Cowbuilder setup. 
> > I re-used the debian.tar.gz from squid-4.10-1 Debian Testing/Sid. 
> > Which i have done since squid 3.2, first time it fails. 
> > but only AMD64 fails to build, while i386 build fine. 
> > That a part im not familiar with, any suggestions?  
> > 
> ...
> > client_side.cc: In function 'void 
> clientListenerConnectionOpened(AnyP::PortCfgPointer&, 
> Ipc::FdNoteId, const Pointer&)':
> > client_side.cc:3663:27: error: 'Extra' is not a member of 'Debug'
> 
> You will need to add this patch:
> 
>  
> <https://github.com/squid-cache/squid/commit/c26cd1cb6a60ff196
> ef13c00e82576d3bfeb2e30>
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From silamael at coronamundi.de  Thu Apr 23 12:40:07 2020
From: silamael at coronamundi.de (Silamael Darkomen)
Date: Thu, 23 Apr 2020 14:40:07 +0200
Subject: [squid-users] Squid 4.11 not building with Heimdal Kerberos
In-Reply-To: <0384d433-888b-3783-5ddd-4bbd089ce549@treenet.co.nz>
References: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
 <0384d433-888b-3783-5ddd-4bbd089ce549@treenet.co.nz>
Message-ID: <8e0f686c-fa60-a916-3e70-8d497ad4bca7@coronamundi.de>

On 23.04.2020 13:50, Amos Jeffries wrote:
> On 23/04/20 11:41 pm, Silamael Darkomen wrote:
>> Hi,
>>
>> Just trying to build the new Squid 4.11 with Heimdal as Kerberos5 library.
>> Unfortunately, the enctype fix made in
>> src/acl/external/kerberos_ldap_group/support_krb5.cc does not compile
>> with Heimdal.
>> Their krb5_creds structure does not have a keyblock member.
>>
>> For now I disabled the fix with #ifndef USE_HEIMDAL_KRB5.
>> No clue, if this enctype fix is needed for Heimdal too and how it has to
>> be made.
> 
> 
> Please open a bugzilla report about this so it does not get forgotten.
> 
> 
> Amos

Done: https://bugs.squid-cache.org/show_bug.cgi?id=5042


From belle at bazuin.nl  Thu Apr 23 12:42:26 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 23 Apr 2020 14:42:26 +0200
Subject: [squid-users] FW: Squid 4.11 not building with ssl enable on Buster
Message-ID: <vmime.5ea18d32.573e.50a8305e51ff6b52@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 
 
The folder test-suite/buildtests/ 
Is an not exiting folder in current 4.11 tar.gz  
 
Can you verify that? I thinks thats also from 5.x
 

Greetz, 

Louis

> 
> 
> > -----Oorspronkelijk bericht-----
> > Van: squid-users 
> > [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> > Amos Jeffries
> > Verzonden: donderdag 23 april 2020 14:28
> > Aan: squid-users at lists.squid-cache.org
> > Onderwerp: Re: [squid-users] Squid 4.11 not building with ssl 
> > enable on Buster
> > 
> > On 24/04/20 12:00 am, L.P.H. van Belle wrote:
> > > Hai, 
> > > 
> > > Im currently building squid 411 on debian buster. 
> Cowbuilder setup. 
> > > I re-used the debian.tar.gz from squid-4.10-1 Debian Testing/Sid. 
> > > Which i have done since squid 3.2, first time it fails. 
> > > but only AMD64 fails to build, while i386 build fine. 
> > > That a part im not familiar with, any suggestions?  
> > > 
> > ...
> > > client_side.cc: In function 'void 
> > clientListenerConnectionOpened(AnyP::PortCfgPointer&, 
> > Ipc::FdNoteId, const Pointer&)':
> > > client_side.cc:3663:27: error: 'Extra' is not a member of 'Debug'
> > 
> > You will need to add this patch:
> > 
> >  
> > <https://github.com/squid-cache/squid/commit/c26cd1cb6a60ff196
> > ef13c00e82576d3bfeb2e30>
> > 
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> > 
> 



From squid3 at treenet.co.nz  Thu Apr 23 12:51:49 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2020 00:51:49 +1200
Subject: [squid-users] Squid 4.11 not building with ssl enable on Buster
In-Reply-To: <7a0f9fdc-729f-f6a3-4422-7c8b0528adea@treenet.co.nz>
References: <b939e3e8-64cc-361e-217a-8b2616571ddd@coronamundi.de>
 <vmime.5ea18342.7ae3.1b4750c7670838d1@ms249-lin-003.rotterdam.bazuin.nl>
 <7a0f9fdc-729f-f6a3-4422-7c8b0528adea@treenet.co.nz>
Message-ID: <393e63ec-1ca6-8739-b14c-dbc1c6c7f002@treenet.co.nz>

On 24/04/20 12:28 am, Amos Jeffries wrote:
> On 24/04/20 12:00 am, L.P.H. van Belle wrote:
>> Hai, 
>>
>> Im currently building squid 411 on debian buster. Cowbuilder setup. 
>> I re-used the debian.tar.gz from squid-4.10-1 Debian Testing/Sid. 
>> Which i have done since squid 3.2, first time it fails. 
>> but only AMD64 fails to build, while i386 build fine. 
>> That a part im not familiar with, any suggestions?  
>>
> ...
>> client_side.cc: In function 'void clientListenerConnectionOpened(AnyP::PortCfgPointer&, Ipc::FdNoteId, const Pointer&)':
>> client_side.cc:3663:27: error: 'Extra' is not a member of 'Debug'
> 
> You will need to add this patch:
> 
>  <https://github.com/squid-cache/squid/commit/c26cd1cb6a60ff196ef13c00e82576d3bfeb2e30>
> 

Or for a more easily used plain-text link:
 <http://master.squid-cache.org/Versions/v4/changesets/squid-4-c26cd1cb6a60ff196ef13c00e82576d3bfeb2e30.patch>


Amos


From squid3 at treenet.co.nz  Thu Apr 23 12:54:54 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2020 00:54:54 +1200
Subject: [squid-users] FW: Squid 4.11 not building with ssl enable on
 Buster
In-Reply-To: <vmime.5ea18d32.573e.50a8305e51ff6b52@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.5ea18d32.573e.50a8305e51ff6b52@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <c2d8a4fe-f4c5-ecf4-6c7f-cdb7b14fed6d@treenet.co.nz>

On 24/04/20 12:42 am, L.P.H. van Belle wrote:
> Hai, 
>  
> The folder test-suite/buildtests/ 
> Is an not exiting folder in current 4.11 tar.gz  
>  
> Can you verify that? I thinks thats also from 5.x
>  

It is part of our CI unit test setup. That part of the patch is only
useful for git checkout builds.

Amos


From andreas at canonical.com  Thu Apr 23 13:00:57 2020
From: andreas at canonical.com (Andreas Hasenack)
Date: Thu, 23 Apr 2020 10:00:57 -0300
Subject: [squid-users] logformat: override %tg,
	but keep subsecond resolution
In-Reply-To: <54a4f0b1-4c92-e6aa-503a-abc4c3cd40ef@treenet.co.nz>
References: <CANYNYEGeZstc=m=sT2qKZ5vkkFn-Y5pbs88pzpSjCyxv6RYLtg@mail.gmail.com>
 <54a4f0b1-4c92-e6aa-503a-abc4c3cd40ef@treenet.co.nz>
Message-ID: <CANYNYEGO1+S_Jge4wpieyh+dd9AA=ev3ww19RhSUUXKpCVXfoQ@mail.gmail.com>

Hello,

On Thu, Apr 23, 2020 at 6:57 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>   %tg{%F %H:%M:%S}.%03tu %tg{%z}
>
>
> Or, since the timezone is always expected to be "+0000" you can just
> configure this and save CPU cycles looking up the timezone offset:
>
>  logformat customTime %tg{%F %H:%M:%S}.%03tu +0000

Thanks! Worked perferctly


From donmuller22 at outlook.com  Thu Apr 23 14:39:21 2020
From: donmuller22 at outlook.com (Donald Muller)
Date: Thu, 23 Apr 2020 14:39:21 +0000
Subject: [squid-users] 5 v 4
Message-ID: <MN2PR22MB17764AF386057DF58F781619B6D30@MN2PR22MB1776.namprd22.prod.outlook.com>

What are the differences between V5.x and V4.x?

-----
"Everyone is entitled to his own opinion, but not to his own facts." - Daniel Patrick Moynihan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200423/b6f8c8f5/attachment.htm>

From squid3 at treenet.co.nz  Thu Apr 23 14:55:57 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Apr 2020 02:55:57 +1200
Subject: [squid-users] 5 v 4
In-Reply-To: <MN2PR22MB17764AF386057DF58F781619B6D30@MN2PR22MB1776.namprd22.prod.outlook.com>
References: <MN2PR22MB17764AF386057DF58F781619B6D30@MN2PR22MB1776.namprd22.prod.outlook.com>
Message-ID: <e7f691a4-3282-a8f1-0318-918988c034f8@treenet.co.nz>

On 24/04/20 2:39 am, Donald Muller wrote:
> What are the differences between V5.x and V4.x?
> 

v5 is more advanced code, and currently still in beta.

See <https://wiki.squid-cache.org/Squid-5> and
<http://www.squid-cache.org/Versions/v5/RELEASENOTES.html>.

Amos


From belle at bazuin.nl  Thu Apr 23 15:06:05 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 23 Apr 2020 17:06:05 +0200
Subject: [squid-users] Squid 4.11 not building with ssl enable on Buster
In-Reply-To: <393e63ec-1ca6-8739-b14c-dbc1c6c7f002@treenet.co.nz>
References: <7a0f9fdc-729f-f6a3-4422-7c8b0528adea@treenet.co.nz>
Message-ID: <vmime.5ea1aedd.6c05.24a9da0bb2a082d@ms249-lin-003.rotterdam.bazuin.nl>

Hai Amos

Thank you for all the help, it nicely builded now..
:-) 

My used changes 

0004-fix-var-run.patch
From: Louis van Belle <louis at van-belle.nl>
Date Thu, 23 Apr 2020 15:22:00 +0200
Subject : fix-var-run location to debian standards /run
--- a/tools/systemd/squid.service
+++ b/tools/systemd/squid.service
@@ -12,7 +12,7 @@

 [Service]
 Type=notify
-PIDFile=/var/run/squid.pid
+PIDFile=/run/squid.pid
 ExecStartPre=/usr/sbin/squid --foreground -z
 ExecStart=/usr/sbin/squid --foreground -sYC
 ExecReload=/bin/kill -HUP $MAINPID


0005-fix-missing-debug-break-bz5041.patch
commit c26cd1cb6a60ff196ef13c00e82576d3bfeb2e30 (HEAD, refs/remotes/origin/v4, refs/heads/v4)
Author: Alex Rousskov <rousskov at measurement-factory.com>
Date:   2020-04-23 05:56:35 -0600
Subject: Bug 5041: Missing Debug::Extra breaks build on hosts with systemd (#611)

--- a/configure.ac
+++ b/configure.ac
@@ -2162,6 +2162,7 @@
   fi
   if test "x$SYSTEMD_LIBS" != "x" ; then
     CXXFLAGS="$SYSTEMD_CFLAGS $CXXFLAGS"
+    LDFLAGS="$SYSTEMD_LIBS $LDFLAGS"
     AC_DEFINE(USE_SYSTEMD,1,[systemd support is available])
   else
     with_systemd=no
--- a/src/Debug.h
+++ b/src/Debug.h
@@ -99,6 +99,10 @@

     /// configures the active debugging context to write syslog ALERT
     static void ForceAlert();
+
+    /// prefixes each grouped debugs() line after the first one in the group
+    static std::ostream& Extra(std::ostream &os) { return os << "\n    "; }
+
 private:
     static Context *Current; ///< deepest active context; nil outside debugs()
 };
 

Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Amos Jeffries
> Verzonden: donderdag 23 april 2020 14:52
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Squid 4.11 not building with ssl 
> enable on Buster
> 
> On 24/04/20 12:28 am, Amos Jeffries wrote:
> > On 24/04/20 12:00 am, L.P.H. van Belle wrote:
> >> Hai, 
> >>
> >> Im currently building squid 411 on debian buster. 
> Cowbuilder setup. 
> >> I re-used the debian.tar.gz from squid-4.10-1 Debian Testing/Sid. 
> >> Which i have done since squid 3.2, first time it fails. 
> >> but only AMD64 fails to build, while i386 build fine. 
> >> That a part im not familiar with, any suggestions?  
> >>
> > ...
> >> client_side.cc: In function 'void 
> clientListenerConnectionOpened(AnyP::PortCfgPointer&, 
> Ipc::FdNoteId, const Pointer&)':
> >> client_side.cc:3663:27: error: 'Extra' is not a member of 'Debug'
> > 
> > You will need to add this patch:
> > 
> >  
> <https://github.com/squid-cache/squid/commit/c26cd1cb6a60ff196
> ef13c00e82576d3bfeb2e30>
> > 
> 
> Or for a more easily used plain-text link:
>  
> <http://master.squid-cache.org/Versions/v4/changesets/squid-4-
> c26cd1cb6a60ff196ef13c00e82576d3bfeb2e30.patch>
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rafael.akchurin at diladele.com  Fri Apr 24 11:40:42 2020
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 24 Apr 2020 11:40:42 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.11 (rebuilt with
 sslbump support from sources in Debian unstable)
Message-ID: <AM0PR04MB47535CCCE89E9CF9FFBB6F408FD00@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello everyone,

The online repository with latest Squid 4.11 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid411.diladele.com.
Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation. Scripts for Ubuntu 16 are also available in that repo.
We plan to add Ubuntu 20 in the near future too.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid411.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid411.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient

Hope you will find this useful. Note that older repo of squid410.diladele.com will be taken down in 1 year.

Best regards,
Rafael Akchurin
Diladele B.V.

--
The same Squid 4.11 will be part of upcoming Web Safety 7.4 planned for release in early June, this version has some improvements in the report generation module and support for delay pools per policy. It is now very easy to restrict bandwidth usage by Active Directory groups directly from Admin UI. Download the latest virtual appliance from https://docs.diladele.com/index.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200424/c68c394a/attachment.htm>

From rentorbuy at yahoo.com  Fri Apr 24 14:54:20 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 24 Apr 2020 14:54:20 +0000 (UTC)
Subject: [squid-users] tproxy sslbump and user authentication
In-Reply-To: <20200421124055.GA13094@fantomas.sk>
References: <111546434.3854722.1587424111575.ref@mail.yahoo.com>
 <111546434.3854722.1587424111575@mail.yahoo.com>
 <a2d8da57-6a3b-839a-7400-31b6ab467f99@treenet.co.nz>
 <633376515.127049.1587472390341@mail.yahoo.com>
 <20200421124055.GA13094@fantomas.sk>
Message-ID: <1967071390.219443.1587740060894@mail.yahoo.com>


On Tuesday, April 21, 2020, 2:41:02 PM GMT+2, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote: 

>>On Tuesday, April 21, 2020, 8:29:28 AM GMT+2, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>>
>>> Please see the FAQ:
>>> <https://wiki.squid-cache.org/SquidFaq/InterceptionProxy#Why_can.27t_I_use_authentication_together_with_interception_proxying.3F>
>>>
>>> Why bother with the second proxy at all? The explicit proxy has access
>>> to all the details the interception one does (and more - such as
>>> credentials). It should be able to do all filtering necessary.
>
> On 21.04.20 12:33, Vieri wrote:
>>Can the explicit proxy ssl-bump HTTPS traffic and thus analyze traffic with ICAP + squidclamav, for instance?
>
> yes.
>
>>Simply put, will I be able to block, eg. 
>> https://secure.eicar.org/eicarcom2.zip not by mimetype, file extension,
>> url matching, etc., but by analyzing its content with clamav via ICAP?
>
> without bumping, you won't be able to block by anything, only by secure.eicar.org hostname.

Hi,

I'm not sure I understand how that should be configured.

I whipped up a test instance with the configuration I'm showing below.

My browser can authenticate via kerberos and access several web sites (http & https) if I explicitly set it to proxy everything to squid10.mydomain.org on port 3228.
However, icap/clamav filtering is "not working" for neither http nor https.
My cache log shows a lot of messages regarding "icap" when I try to download an eicar test file. So something is triggered, but before sending a huge log to the mailing list, what should I be looking for exactly, or is there a specific loglevel I should set?

acl SSL_ports port 443
acl Safe_ports port 80????????? # http
acl Safe_ports port 21????????? # ftp
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher
acl Safe_ports port 210???????? # wais
acl Safe_ports port 1025-65535? # unregistered ports
acl Safe_ports port 280???????? # http-mgmt
acl Safe_ports port 488???????? # gss-http
acl Safe_ports port 591???????? # filemaker
acl Safe_ports port 777???????? # multiling http
acl Safe_ports port 901???????? # SWAT
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

pid_filename /run/squid.testexplicit.pid
access_log daemon:/var/log/squid/access.test.log squid
cache_log /var/log/squid/cache.test.log

acl explicit myportname 3227
acl explicitbump myportname 3228
acl interceptedssl myportname 3229

http_port 3227
# http_port 3228 tproxy
http_port 3228 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem sslflags=NO_DEFAULT_CA
https_port 3229 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem sslflags=NO_DEFAULT_CA
sslproxy_flags DONT_VERIFY_PEER

sslcrtd_program /usr/libexec/squid/ssl_crtd -s /var/lib/squid/ssl_db_test -M 16MB
sslcrtd_children 40 startup=20 idle=10

cache_dir diskd /var/cache/squid.test 32 16 256

external_acl_type nt_group ttl=0 children-max=50 %LOGIN /usr/libexec/squid/ext_wbinfo_group_acl -K

auth_param negotiate program /usr/libexec/squid/negotiate_kerberos_auth -s HTTP/squid10.mydomain.org at MYREALNAME
auth_param negotiate children 60
auth_param negotiate keep_alive on

acl localnet src 10.0.0.0/8
acl localnet src 192.168.0.0/16
acl localnet src 172.16.0.1
acl localnet src fc00::/7

acl ORG_all proxy_auth REQUIRED

http_access deny explicit !ORG_all
#http_access deny explicit SSL_ports
http_access deny explicitbump !localnet
http_access deny explicitbump !ORG_all
http_access deny interceptedssl !localnet
http_access deny interceptedssl !ORG_all

http_access allow CONNECT interceptedssl SSL_ports

http_access allow localnet
http_reply_access allow localnet

http_access allow ORG_all

debug_options rotate=1 ALL,9
# debug_options rotate=1 ALL,1

append_domain .mydomain.org

ssl_bump stare all
ssl_bump bump all

http_access allow localhost

http_access deny all

coredump_dir /var/cache/squid

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service antivirus respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access antivirus allow all
icap_service_failure_limit -1
icap_persistent_connections off


--
Vieri


From adamw at matrixscience.com  Fri Apr 24 15:46:54 2020
From: adamw at matrixscience.com (Adam Weremczuk)
Date: Fri, 24 Apr 2020 16:46:54 +0100
Subject: [squid-users] failing https requests
Message-ID: <650bea86-07dd-07f5-f5e2-eee7089d7384@matrixscience.com>

Hi all,

I run squid-3.5.27_3 on pfSense 2.4.4 as well as in house Sugar CRM server.

Recently Sugar license validation and updates checks made to 
https://updates.sugarcrm.com/heartbeat/soap.php started failing (no 
changes made at our end).

In squid logs requests only produce 2 lines:

1587737506.670????? 0 192.168.5.30 TAG_NONE/400 4360 NONE 
error:invalid-request - HIER_NONE/- text/html
1587737506.978??? 301 192.168.5.30 TCP_MISS/301 464 POST 
http://updates.sugarcrm.com/heartbeat/soap.php - 
HIER_DIRECT/54.177.58.238 text/html

It looks like client error followed by a redirection to http.

Direct requests (no web proxy) as well as telnet, wget and curl work fine.

Could somebody explain what exactly the errors mean and why the requests 
fail?

Thanks,
Adam




From squid3 at treenet.co.nz  Fri Apr 24 15:57:40 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Apr 2020 03:57:40 +1200
Subject: [squid-users] failing https requests
In-Reply-To: <650bea86-07dd-07f5-f5e2-eee7089d7384@matrixscience.com>
References: <650bea86-07dd-07f5-f5e2-eee7089d7384@matrixscience.com>
Message-ID: <263217bf-2d25-361b-9f54-e3e29e6445f6@treenet.co.nz>

On 25/04/20 3:46 am, Adam Weremczuk wrote:
> Hi all,
> 
> I run squid-3.5.27_3 on pfSense 2.4.4 as well as in house Sugar CRM server.
> 
> Recently Sugar license validation and updates checks made to
> https://updates.sugarcrm.com/heartbeat/soap.php started failing (no
> changes made at our end).
> 
> In squid logs requests only produce 2 lines:
> 
> 1587737506.670????? 0 192.168.5.30 TAG_NONE/400 4360 NONE
> error:invalid-request - HIER_NONE/- text/html
> 1587737506.978??? 301 192.168.5.30 TCP_MISS/301 464 POST
> http://updates.sugarcrm.com/heartbeat/soap.php -
> HIER_DIRECT/54.177.58.238 text/html
> 
> It looks like client error followed by a redirection to http.
> 
> Direct requests (no web proxy) as well as telnet, wget and curl work fine.
> 
> Could somebody explain what exactly the errors mean and why the requests
> fail?
> 

It means the client delivered some bytes which do not in any way conform
to HTTP request syntax. Not even similar.

The best thing to do is to get a full-packet capture and investigate
with wireshark what is going on.


Amos


From osantosmyr at gmail.com  Fri Apr 24 21:09:03 2020
From: osantosmyr at gmail.com (russel0901)
Date: Fri, 24 Apr 2020 16:09:03 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
Message-ID: <1587762543811-0.post@n4.nabble.com>

I am having a problem on my squid proxy

this settings is allow all but i can't visit sites like bancnetonline, rcbc,
philhealth (govt and bank site)

sometimes it can be visited, sometimes not... (weird???)

Please Help thank you.


here is my squid conf...

max_filedesc 4096
request_header_access X-Forwarded-For allow all
via off
httpd_suppress_version_string on

http_port 3333
icp_port 3535

hierarchy_stoplist cgi-bin ?
acl QUERY urlpath_regex cgi-bin \?
no_cache deny QUERY
cache_mem 32 MB
maximum_object_size 5480 KB
cache_dir ufs /home/squidcache 6000 16 256
#cache_dir ufs /home/squidcache2 6000 16 256
cache_access_log /home/squidcache/access.log
cache_log /dev/null
cache_store_log none
ftp_user Squid at mds.com.sg
dns_defnames on
request_body_max_size 10000 MB
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern .               0       20%     4320
negative_ttl 1 minute
negative_dns_ttl 5 minute
connect_timeout 60 minute
read_timeout 5 minute
request_timeout 60 second
client_lifetime 4 hour
half_closed_clients off
pconn_timeout 240 second
shutdown_lifetime 5 second
#acl localhost src 127.0.0.1/32 ::1
#acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl SSL_ports port 443 563 8003 8000 8080 8020 8021 8030 8031 8053 9053
acl Safe_ports port 80 81 88 21 443 563 70 210 1025-65535
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
acl PURGE method purge
acl manager proto cache_object
acl apache src 10.20.0.245

acl QUERY urlpath_regex -i owa
acl QUERY2 urlpath_regex cgi-bin \?
acl QUERY3 urlpath_regex -i php
acl dontcache dstdomain "/etc/squid/dontcache"
no_cache deny QUERY
no_cache deny QUERY2
no_cache deny QUERY3
always_direct allow dontcache


#allowed sites
acl blockedsites dstdomain "/etc/squid/blockedsites"
acl allowedsites dstdomain "/etc/squid/authorizedsites"
acl tahiti src 172.16.20.254/32
acl elmo src 10.20.0.254/32
acl mnlnet2 src "/etc/squid/authorized"


http_access allow dontcache
http_access allow manager apache
http_access allow all
http_access allow elmo
#http_access allow localhost
#http_access allow purge localhost
#http_access allow manager localhost
http_access allow mnlnet2
http_access allow tahiti
http_access deny !Safe_ports
#http_access deny manager
http_access deny CONNECT !SSL_ports
http_access deny purge
http_access deny blockedsites


#icp_access  allow  localhost
icp_access allow all
icp_access allow elmo
icp_access allow tahiti
icp_access allow mnlnet2
miss_access allow all

cache_mgr xxxxxx

cache_effective_user squid
cache_effective_group squid
visible_hostname xxxxxx
append_domain .globalsources.com
memory_pools off
log_icp_queries off
client_db off

check_hostnames off



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From dawood.aijaz97 at gmail.com  Sat Apr 25 17:27:51 2020
From: dawood.aijaz97 at gmail.com (Dawood Aijaz)
Date: Sat, 25 Apr 2020 22:27:51 +0500
Subject: [squid-users] Configure A Native FTP proxy on Squid
Message-ID: <CAHkxSOn17cJOtih8iiTZkbSgwusAYqdR5KDF87n8=ibetMJ+Jw@mail.gmail.com>

Hi,

Currently, I am developing a Data Loss Prevention Tool. One of the
requirements is to monitor FTP traffic. So can someone help me set up an
FTP native proxy is squid and how will I be able to monitor FTP traffic

Regards,
Dawood Aijaz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200425/4f11ff7c/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Apr 25 20:26:33 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 25 Apr 2020 22:26:33 +0200
Subject: [squid-users] Configure A Native FTP proxy on Squid
In-Reply-To: <CAHkxSOn17cJOtih8iiTZkbSgwusAYqdR5KDF87n8=ibetMJ+Jw@mail.gmail.com>
References: <CAHkxSOn17cJOtih8iiTZkbSgwusAYqdR5KDF87n8=ibetMJ+Jw@mail.gmail.com>
Message-ID: <202004252226.33541.Antony.Stone@squid.open.source.it>

On Saturday 25 April 2020 at 19:27:51, Dawood Aijaz wrote:

> Hi,
> 
> Currently, I am developing a Data Loss Prevention Tool. One of the
> requirements is to monitor FTP traffic. So can someone help me set up an
> FTP native proxy is squid and how will I be able to monitor FTP traffic

Why do you want to use Squid for this purpose when Squid is not a native FTP 
proxy?

Squid can handle FTP traffic when it is proxied by an HTTP-proxy-aware client.

It will not "transparently" intercept and proxy standard FTP traffic without the 
client being specifically configured to use Squid.

In my opinion you are trying to use the wrong tool for the job.

Also, if you are developing a Data Loss Prevention Tool, then in my opinion 
one of the first things you should do is to stop people using FTP and make them 
use something secure instead.

Good luck in your endeavours.


Antony.

-- 
Was ist braun, liegt ins Gras, und raucht?
Ein Kaminchen...

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Sun Apr 26 06:42:11 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 26 Apr 2020 18:42:11 +1200
Subject: [squid-users] Configure A Native FTP proxy on Squid
In-Reply-To: <202004252226.33541.Antony.Stone@squid.open.source.it>
References: <CAHkxSOn17cJOtih8iiTZkbSgwusAYqdR5KDF87n8=ibetMJ+Jw@mail.gmail.com>
 <202004252226.33541.Antony.Stone@squid.open.source.it>
Message-ID: <199faafa-c42b-5324-74df-914997f045e1@treenet.co.nz>

On 26/04/20 8:26 am, Antony Stone wrote:
> On Saturday 25 April 2020 at 19:27:51, Dawood Aijaz wrote:
> 
>> Hi,
>>
>> Currently, I am developing a Data Loss Prevention Tool. One of the
>> requirements is to monitor FTP traffic. So can someone help me set up an
>> FTP native proxy is squid and how will I be able to monitor FTP traffic
> 
> Why do you want to use Squid for this purpose when Squid is not a native FTP 
> proxy?

As of v3.5 the latest Squid actually can do native FTP relay.

<http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.6>


Amos


From squid3 at treenet.co.nz  Sun Apr 26 08:18:02 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 26 Apr 2020 20:18:02 +1200
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <1587762543811-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
Message-ID: <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>

On 25/04/20 9:09 am, russel0901 wrote:
> I am having a problem on my squid proxy
> 

Which version of Squid are you using?
Output of squid -v would be best if you can provide.


> this settings is allow all but i can't visit sites like bancnetonline, rcbc,
> philhealth (govt and bank site)
> 
> sometimes it can be visited, sometimes not... (weird???)
> 
> Please Help thank you.
> 

Following is a free review of your config settings.

To actually determine your problem we will need log records of a failing
transaction. At least access.log entries you see for it, and maybe also
something from cache.log if that is not enough.

... which brings me to the first problem in your config.

"cache_log /dev/null" is a very bad idea. This completely hides all
information about problems from *you* - the problems still exist, still
seen by everyone else involved.
 All this does is erase most of your ability to troubleshoot.

If your objective is reduced log verbosity use this setting instead:
  debug_options ALL,0

That reduces cache.log contents to mentions about critical failures of
Squid.


> 
> here is my squid conf...
> 
> max_filedesc 4096

Why so low? and why the deprecated RedHat experimental directive?

Current squid.conf directive is max_filedescriptors. It is a backup to
the --with-max-filedescriptors build option and system ulimit setup.



> request_header_access X-Forwarded-For allow all

This is pointless. All it does is waste CPU cycles on every request
through Squid.

> via off
> httpd_suppress_version_string on
> 
> http_port 3333
> icp_port 3535
> 
> hierarchy_stoplist cgi-bin ?

This is pointless. It is the default setting for all Squid-3 and later
versions.

> acl QUERY urlpath_regex cgi-bin \?
> no_cache deny QUERY

QUERY is obsolete and actually somewhat harmful in current Squid.

For much improved caching you can add the missing refresh_pattern
mentioned below, then erase these and all other rules using QUERY ACL name.


> cache_mem 32 MB
> maximum_object_size 5480 KB
> cache_dir ufs /home/squidcache 6000 16 256
> #cache_dir ufs /home/squidcache2 6000 16 256
> cache_access_log /home/squidcache/access.log

This directive has been deprecated since early Squid-2.
Current Squid use:
  access_log /home/squidcache/access.log


> cache_log /dev/null

Already mentioned the problems with this. Please revert it to the
default for your Squid version. You will need this log to investigate
the current problem.


> cache_store_log none

This is pointless. It is the default for all current Squid.

> ftp_user Squid at mds.com.sg
> dns_defnames on
> request_body_max_size 10000 MB
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440

Missing pattern:

  refresh_pattern -i (/cgi-bin/|\?) 0     0%      0


> refresh_pattern .               0       20%     4320
> negative_ttl 1 minute
> negative_dns_ttl 5 minute
> connect_timeout 60 minute
> read_timeout 5 minute
> request_timeout 60 second
> client_lifetime 4 hour
> half_closed_clients off
> pconn_timeout 240 second
> shutdown_lifetime 5 second
> #acl localhost src 127.0.0.1/32 ::1
> #acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> acl SSL_ports port 443 563 8003 8000 8080 8020 8021 8030 8031 8053 9053
> acl Safe_ports port 80 81 88 21 443 563 70 210 1025-65535
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
> acl PURGE method purge

Do you or clients actually use PURGE method requests?

It would be worth looking into why. That old Squid custom extension to
HTTP is deprecated.

Current Squid obey HTTP/1.1 caching far better than old Squid-2 and
earlier versions. You can use Cache-Control:no-cache *request* header to
update cache contents better than PURGE ever could.

Also, HTCP protocol is better for cache management with HTTP/1.1 than
either PURGE or ICP protocol. If you can find or adapt tools to use that
protocol they will be much better off.




> acl manager proto cache_object

This is also a deprecated manager ACL definition. This implies that your
Squid is quite old. Please upgrade to a more current version.


> acl apache src 10.20.0.245
> 
> acl QUERY urlpath_regex -i owa
> acl QUERY2 urlpath_regex cgi-bin \?
> acl QUERY3 urlpath_regex -i php
> acl dontcache dstdomain "/etc/squid/dontcache"
> no_cache deny QUERY
> no_cache deny QUERY2
> no_cache deny QUERY3

"no_cache" is deprecated. Above rules are actually doing "cache deny".


It would be worth investigating why any URL containing the letters "owa"
or "php" are apparently trying to be forced to cache.

Please notice these ACL regex match if those letters occur *anywhere* in
the URL path portion. That includes 'folder' , 'filename', query-string,
and fragment strings. Also in non-HTTP URLs which have 'path' portions
and such.


> always_direct allow dontcache

This is a routing control directive. ACL called 'dontcache' is confusing
as reason to prevent routing to cache_peer - which do not exist in this
config anyway.

As a result of this any domain not listed in "dontcache" ACL will be
prevented from service by this proxy.

If that is actually what you want to happen, it would be better
configuring this:

 http_access deny !dontcache

... but you have explicitly put the exact opposite in your http_access
rules below. Which implies these rules are completely broken.


> 
> #allowed sites
> acl blockedsites dstdomain "/etc/squid/blockedsites"
> acl allowedsites dstdomain "/etc/squid/authorizedsites"
> acl tahiti src 172.16.20.254/32
> acl elmo src 10.20.0.254/32
> acl mnlnet2 src "/etc/squid/authorized"
> 
> 
> http_access allow dontcache
> http_access allow manager apache
> http_access allow all

All following http_access rules are pointless.

Since all previous http_access rules are 'allow' they are also pointless
waste of CPU cycles.

This is an open proxy, with no logging. As such the only security
protection you have is the miss_access which *breaks* a huge amount of
traffic. If it were not for that your network would be completely open
to any type of attack.



> http_access allow elmo
> #http_access allow localhost
> #http_access allow purge localhost
> #http_access allow manager localhost
> http_access allow mnlnet2
> http_access allow tahiti
> http_access deny !Safe_ports
> #http_access deny manager
> http_access deny CONNECT !SSL_ports
> http_access deny purge
> http_access deny blockedsites
> 
> 
> #icp_access  allow  localhost
> icp_access allow all

None of the following icp_access rules have any effect.

This proxy does not have any cache_peer to send ICP traffic to.


> icp_access allow elmo
> icp_access allow tahiti
> icp_access allow mnlnet2
> miss_access allow all

This miss_access is pointless. It is the default behaviour of Squid.


Amos


From Antony.Stone at squid.open.source.it  Sun Apr 26 08:43:09 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 26 Apr 2020 10:43:09 +0200
Subject: [squid-users] Configure A Native FTP proxy on Squid
In-Reply-To: <199faafa-c42b-5324-74df-914997f045e1@treenet.co.nz>
References: <CAHkxSOn17cJOtih8iiTZkbSgwusAYqdR5KDF87n8=ibetMJ+Jw@mail.gmail.com>
 <202004252226.33541.Antony.Stone@squid.open.source.it>
 <199faafa-c42b-5324-74df-914997f045e1@treenet.co.nz>
Message-ID: <202004261043.09330.Antony.Stone@squid.open.source.it>

On Sunday 26 April 2020 at 08:42:11, Amos Jeffries wrote:

> On 26/04/20 8:26 am, Antony Stone wrote:
> > On Saturday 25 April 2020 at 19:27:51, Dawood Aijaz wrote:
> >> 
> >> Currently, I am developing a Data Loss Prevention Tool. One of the
> >> requirements is to monitor FTP traffic. So can someone help me set up an
> >> FTP native proxy is squid and how will I be able to monitor FTP traffic
> > 
> > Why do you want to use Squid for this purpose when Squid is not a native
> > FTP proxy?
> 
> As of v3.5 the latest Squid actually can do native FTP relay.
> 
> <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.6>

Oh!

Thanks for the correction, Amos.

Apologies to Dawood for giving outdated information.


Regards,


Antony.

-- 
Most people have more than the average number of legs.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From osantosmyr at gmail.com  Sun Apr 26 13:14:40 2020
From: osantosmyr at gmail.com (russel0901)
Date: Sun, 26 Apr 2020 08:14:40 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
Message-ID: <1587906880308-0.post@n4.nabble.com>

Hi, upon checking I am using squid version 3.1 on CentOS 6.10



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Sun Apr 26 13:40:37 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 26 Apr 2020 15:40:37 +0200
Subject: [squid-users] Squid - Can't visit (government site and Banking
	Site) - Please help
In-Reply-To: <1587906880308-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com>
Message-ID: <202004261540.37513.Antony.Stone@squid.open.source.it>

On Sunday 26 April 2020 at 15:14:40, russel0901 wrote:

> Hi, upon checking I am using squid version 3.1 on CentOS 6.10

Wow, that's impressive (in a way).

Squid 3.1 was released ten years ago (29 March 2010).

On Wednesday 15 August 2012 at 13:29:07, Amos Jeffries wrote:

> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-3.2.1 release.
> 
> Support for Squid-3.1 bug fixes has now officially ceased. Bugs in 3.1
> will continue to be fixed, however the fixes will be added to the 3.2
> series. All users of Squid-3.1 are encouraged to plan for upgrades.

The current stable version of Squid is 4.11

I recommend you upgrade.  Aside from anything else, you're not going to find 
many people in a position to help out with such an old version, as well as the 
fact that it alomst certainly doesn't support certain features required by 
modern web browsers or servers.

Regards,


Antony.

-- 
I lay awake all night wondering where the sun went, and then it dawned on me.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From uhlar at fantomas.sk  Sun Apr 26 16:33:20 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 26 Apr 2020 18:33:20 +0200
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <1587906880308-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com>
Message-ID: <20200426163320.GA21705@fantomas.sk>

On 26.04.20 08:14, russel0901 wrote:
>Subject: Re: [squid-users] Squid - Can't visit (government site and Banking
> Site) - Please help
>
>Hi, upon checking I am using squid version 3.1 on CentOS 6.10

1. It is nice to mention your problem in mail body, not only in Subject:

2. As already advised, upgrade. CendOT 6.10 will only last 7 month from now.

3. Are you trying fo filter HTTPS  connections using sslbump?
   in such case, upgrade is even more important.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
My mind is like a steel trap - rusty and illegal in 37 states.


From osantosmyr at gmail.com  Mon Apr 27 07:17:11 2020
From: osantosmyr at gmail.com (russel0901)
Date: Mon, 27 Apr 2020 02:17:11 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <20200426163320.GA21705@fantomas.sk>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
Message-ID: <1587971831736-0.post@n4.nabble.com>

okay will try to upgarde...

our goal is to have a squid proxy that will allow all website (without any
restriction)

reason: I only need the squid proxy to monitor the website visit of the user
via sqstat and SARG (squid analyze report generator)

Problem: all website is okay only government site and banking sites is
having a problem...

upon checking on the access.log  (HTTP 200 0 Connect) that is the result of
the website if i can't connect to to it.

weird problem: sometimes the website can be visited and sometimes not



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Mon Apr 27 12:26:36 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 27 Apr 2020 14:26:36 +0200
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <1587971831736-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com>
 <20200426163320.GA21705@fantomas.sk>
 <1587971831736-0.post@n4.nabble.com>
Message-ID: <20200427122635.GA18112@fantomas.sk>

On 27.04.20 02:17, russel0901 wrote:
>okay will try to upgarde...
>
>our goal is to have a squid proxy that will allow all website (without any
>restriction)

this is the standard behaviour. Note that you should only allow your
clients, not clients from the internet

>reason: I only need the squid proxy to monitor the website visit of the user
>via sqstat and SARG (squid analyze report generator)
>
>Problem: all website is okay only government site and banking sites is
>having a problem...

this is indicating that you have some blocing implemented

>upon checking on the access.log  (HTTP 200 0 Connect) that is the result of
>the website if i can't connect to to it.

200 it code for success, apparently the connection was successful.


>weird problem: sometimes the website can be visited and sometimes not

logs can say more. 
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
M$ Win's are shit, do not use it !


From osantosmyr at gmail.com  Mon Apr 27 12:31:01 2020
From: osantosmyr at gmail.com (russel0901)
Date: Mon, 27 Apr 2020 07:31:01 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <20200426163320.GA21705@fantomas.sk>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
Message-ID: <1587990661078-0.post@n4.nabble.com>

I made a new Config and upgrade to CentOS 8.1xxx and Squid 4.4

STILL CAN'T VISIT THE WEBSITE (GOVT SITE AND BANKING SITES)


This is my Squid.conf

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 0.0.0.1-0.255.255.255	# RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8		# RFC 1918 local private network (LAN)
acl localnet src 100.64.0.0/10		# RFC 6598 shared address space (CGN)
acl localnet src 169.254.0.0/16 	# RFC 3927 link-local (directly plugged)
machines
acl localnet src 172.16.0.0/12		# RFC 1918 local private network (LAN)
acl localnet src 192.168.0.0/16		# RFC 1918 local private network (LAN)
acl localnet src fc00::/7       	# RFC 4193 local private network range
acl localnet src fe80::/10      	# RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT


http_access allow all
http_access allow localhost manager
http_access allow localnet
http_access allow localhost
http_access deny !Safe_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access deny CONNECT !SSL_ports
http_access deny manager


http_access deny all


http_port 3333

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256
cache_dir ufs /home/squidcache 100 16 256
cache_access_log /home/squidcache/access.log

# Leave coredumps in the first cache dir
coredump_dir /home/squidcache

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320
cache_effective_user squid
cache_effective_group squid




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Mon Apr 27 13:12:04 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 27 Apr 2020 15:12:04 +0200
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <1587990661078-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com>
 <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com>
Message-ID: <20200427131204.GA19023@fantomas.sk>

On 27.04.20 07:31, russel0901 wrote:
>I made a new Config and upgrade to CentOS 8.1xxx and Squid 4.4
>
>STILL CAN'T VISIT THE WEBSITE (GOVT SITE AND BANKING SITES)

stop shouting...
what is your error message and what is the message in logs?
what do your clients have configured in browsers?

are you aware that your first access directive is "http_access allow all"
which makes you open proxy?

>This is my Squid.conf
>
>#
># Recommended minimum configuration:
>#
>
># Example rule allowing access from your local networks.
># Adapt to list your (internal) IP networks from where browsing
># should be allowed
>acl localnet src 0.0.0.1-0.255.255.255	# RFC 1122 "this" network (LAN)
>acl localnet src 10.0.0.0/8		# RFC 1918 local private network (LAN)
>acl localnet src 100.64.0.0/10		# RFC 6598 shared address space (CGN)
>acl localnet src 169.254.0.0/16 	# RFC 3927 link-local (directly plugged)
>machines
>acl localnet src 172.16.0.0/12		# RFC 1918 local private network (LAN)
>acl localnet src 192.168.0.0/16		# RFC 1918 local private network (LAN)
>acl localnet src fc00::/7       	# RFC 4193 local private network range
>acl localnet src fe80::/10      	# RFC 4291 link-local (directly plugged)
>machines
>
>acl SSL_ports port 443
>acl Safe_ports port 80		# http
>acl Safe_ports port 21		# ftp
>acl Safe_ports port 443		# https
>acl Safe_ports port 70		# gopher
>acl Safe_ports port 210		# wais
>acl Safe_ports port 1025-65535	# unregistered ports
>acl Safe_ports port 280		# http-mgmt
>acl Safe_ports port 488		# gss-http
>acl Safe_ports port 591		# filemaker
>acl Safe_ports port 777		# multiling http
>acl CONNECT method CONNECT
>
>
>http_access allow all
>http_access allow localhost manager
>http_access allow localnet
>http_access allow localhost
>http_access deny !Safe_ports
>
># We strongly recommend the following be uncommented to protect innocent
># web applications running on the proxy server who think the only
># one who can access services on "localhost" is a local user
>#http_access deny to_localhost
>
>#
># INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>#
>
># Example rule allowing access from your local networks.
># Adapt localnet in the ACL section to list your (internal) IP networks
># from where browsing should be allowed
>http_access deny CONNECT !SSL_ports
>http_access deny manager
>
>
>http_access deny all
>
>
>http_port 3333
>
># Uncomment and adjust the following to add a disk cache directory.
>#cache_dir ufs /var/spool/squid 100 16 256
>cache_dir ufs /home/squidcache 100 16 256
>cache_access_log /home/squidcache/access.log
>
># Leave coredumps in the first cache dir
>coredump_dir /home/squidcache
>
>#
># Add any of your own refresh_pattern entries above these.
>#
>refresh_pattern ^ftp:		1440	20%	10080
>refresh_pattern ^gopher:	1440	0%	1440
>refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
>refresh_pattern .		0	20%	4320
>cache_effective_user squid
>cache_effective_group squid


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The 3 biggets disasters: Hiroshima 45, Tschernobyl 86, Windows 95


From dawood.aijaz97 at gmail.com  Mon Apr 27 13:46:28 2020
From: dawood.aijaz97 at gmail.com (Dawood Aijaz)
Date: Mon, 27 Apr 2020 18:46:28 +0500
Subject: [squid-users] Help regarding configuring a native FTP proxy
Message-ID: <CAHkxSO=SdUJNANKdyDZXzuJFWitRNxtk8_3gA+SSGX22=xe2-w@mail.gmail.com>

Hi,
After Amos Jeffries pointed out that there is native FTP support in squid
as of Cv3.5.But I am unable to find any help regarding configuration and
any tutorial to help me do this task

Can anyone share configuration for setting up native FTP proxy,

Regards,
Dawood Aijaz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200427/c5f2d534/attachment.htm>

From adamw at matrixscience.com  Mon Apr 27 14:03:24 2020
From: adamw at matrixscience.com (Adam Weremczuk)
Date: Mon, 27 Apr 2020 15:03:24 +0100
Subject: [squid-users] failing https requests
In-Reply-To: <263217bf-2d25-361b-9f54-e3e29e6445f6@treenet.co.nz>
References: <650bea86-07dd-07f5-f5e2-eee7089d7384@matrixscience.com>
 <263217bf-2d25-361b-9f54-e3e29e6445f6@treenet.co.nz>
Message-ID: <fc4f8cad-3585-b6a0-13dc-72df7610801f@matrixscience.com>

Thanks Amos for the hint.

Tcpdump in source reveals the following:

HTTP/1.1 400 Bad Request
Server: squid/3.5.27
Mime-Version: 1.0
Date: Mon, 27 Apr 2020 13:34:47 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 4000
X-Squid-Error: ERR_INVALID_REQ 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from PROXY
X-Cache-Lookup: NONE from PROXY:3128
Via: 1.1 PROXY (squid/3.5.27)
Connection: close

It also produces:

Some possible problems are:
- Missing or unknown request method.
- Missing URL.
- Missing HTTP Identifier (HTTP/1.0).
- Request is too large.
- Content-Length missing for POST or PUT requests.
- Illegal character in hostname; underscores are not allowed.
- HTTP/1.1 feature is being asked from an HTTP/1.0 software.

Can I determine which of the above is actually causing failures?

Increasing debug level to 9 in squid config hasn't resulted in any more 
info being logged :(

Cheers,
Adam

On 24/04/2020 16:57, Amos Jeffries wrote:
> On 25/04/20 3:46 am, Adam Weremczuk wrote:
>> Hi all,
>>
>> I run squid-3.5.27_3 on pfSense 2.4.4 as well as in house Sugar CRM server.
>>
>> Recently Sugar license validation and updates checks made to
>> https://updates.sugarcrm.com/heartbeat/soap.php started failing (no
>> changes made at our end).
>>
>> In squid logs requests only produce 2 lines:
>>
>> 1587737506.670????? 0 192.168.5.30 TAG_NONE/400 4360 NONE
>> error:invalid-request - HIER_NONE/- text/html
>> 1587737506.978??? 301 192.168.5.30 TCP_MISS/301 464 POST
>> http://updates.sugarcrm.com/heartbeat/soap.php -
>> HIER_DIRECT/54.177.58.238 text/html
>>
>> It looks like client error followed by a redirection to http.
>>
>> Direct requests (no web proxy) as well as telnet, wget and curl work fine.
>>
>> Could somebody explain what exactly the errors mean and why the requests
>> fail?
>>
> It means the client delivered some bytes which do not in any way conform
> to HTTP request syntax. Not even similar.
>
> The best thing to do is to get a full-packet capture and investigate
> with wireshark what is going on.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From uhlar at fantomas.sk  Mon Apr 27 14:07:19 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 27 Apr 2020 16:07:19 +0200
Subject: [squid-users] Help regarding configuring a native FTP proxy
In-Reply-To: <CAHkxSO=SdUJNANKdyDZXzuJFWitRNxtk8_3gA+SSGX22=xe2-w@mail.gmail.com>
References: <CAHkxSO=SdUJNANKdyDZXzuJFWitRNxtk8_3gA+SSGX22=xe2-w@mail.gmail.com>
Message-ID: <20200427140719.GB19023@fantomas.sk>

On 27.04.20 18:46, Dawood Aijaz wrote:
>After Amos Jeffries pointed out that there is native FTP support in squid
>as of Cv3.5.But I am unable to find any help regarding configuration and
>any tutorial to help me do this task
>
>Can anyone share configuration for setting up native FTP proxy,

I believe this requires ftp_port and standard access directives further.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
How does cat play with mouse? cat /dev/mouse


From uhlar at fantomas.sk  Mon Apr 27 14:09:20 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 27 Apr 2020 16:09:20 +0200
Subject: [squid-users] failing https requests
In-Reply-To: <fc4f8cad-3585-b6a0-13dc-72df7610801f@matrixscience.com>
References: <650bea86-07dd-07f5-f5e2-eee7089d7384@matrixscience.com>
 <263217bf-2d25-361b-9f54-e3e29e6445f6@treenet.co.nz>
 <fc4f8cad-3585-b6a0-13dc-72df7610801f@matrixscience.com>
Message-ID: <20200427140920.GC19023@fantomas.sk>

On 27.04.20 15:03, Adam Weremczuk wrote:
>Tcpdump in source reveals the following:
>HTTP/1.1 400 Bad Request

does TCPDUMP show the request too?

Maybe you use intercepted connections on standard http_port or you use squid
as destination server without specifying vhost?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
We are but packets in the Internet of life (userfriendly.org)


From osantosmyr at gmail.com  Mon Apr 27 15:17:05 2020
From: osantosmyr at gmail.com (russel0901)
Date: Mon, 27 Apr 2020 10:17:05 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <20200427131204.GA19023@fantomas.sk>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com> <20200427131204.GA19023@fantomas.sk>
Message-ID: <1588000625812-0.post@n4.nabble.com>

Hi again... sorry i was not shouting just making the message capitalize.

the message on my logs is...

TCP_TUNNEL/200 39 CONNECT www.bancnetonline.com:443 -
HIER_DIRECT/203.131.77.194 -

but still i can't visit the site...

weird problem: sometimes the website can be visited but rarely happen, most
of the time its not..

upon pinging the server i can't ping the said server..

also (www.rcbc.com) i can't visit the said site but i can ping the website,
weird right?

note: we don't have any configuration of the browser (just default only)



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rentorbuy at yahoo.com  Mon Apr 27 15:27:24 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 27 Apr 2020 15:27:24 +0000 (UTC)
Subject: [squid-users] explicit proxy and iptables
References: <1943883120.975103.1588001244329.ref@mail.yahoo.com>
Message-ID: <1943883120.975103.1588001244329@mail.yahoo.com>

Hi,

I've been using Squid + TPROXY in transparent sslbump mode for quite a while now, but I'd like to use an explicit proxy with user authentication instead.

I have Squid on my first firewall/gateway node, and then I have another gateway (node 2) where all the HTTP requests go through, with multiple ISPs.

In transparent tproxy mode, I can obviously mark packets according to the "real" client src IP addresses and then use, eg., different ISPs based on client src addr.

In the explicit setup, the gateway (node 2) only sees one IP address as HTTP source -- the one on the "first node" with the explicit Squid proxy. I presume that in this case there is NO WAY I can somehow inform the gateway on node 2 of the "real" clent IP addresses?

I can imagine the answer to this silly question, but nonetheless I prefer to ask just to make sure. ;-)

Thanks,

Vieri


From 3m9n51s2ewut at thismonkey.com  Mon Apr 27 16:21:13 2020
From: 3m9n51s2ewut at thismonkey.com (Scott)
Date: Tue, 28 Apr 2020 02:21:13 +1000
Subject: [squid-users] Best way to prevent squid from bumping CONNECTs
Message-ID: <20200427162110.GA50659@thismonkey.com>

Hi,

my experience with ssl_bump is that it tries to bump SSL connections whether 
presented to Squid explicitly or implicitly.

I have a device with two pieces of software, one configured with Squid 
explicitly, one that requires intercept (via WCCP).

So both explicit CONNECT messages arrive at squid (on 3128/TCP) and SSL (on 
443/TCP).

When simply configuring `ssl_bump bump host_acl' the Squid logs show Squid 
trying, and failing, to bump CONNECT requests.  They may be failing due to 
certificate issue most likely, I'm not sure.  I can't add to the certificate 
store of the software that has the proxy configured (i.e. it will not permit 
bumping).

Is it expected that Squid will bump/splice CONNECT requests?
Because not all CONNECT sessions are SSL, if the CONNECT destination does not 
begin a TLS handshake will Squid revert to simply creating a TCP tunnel 
instead of bumping?

My workaround has been to simply add `!CONNECT' to the `ssl_bump host_acl' 
statements.  Squid will happily bump the SSL sessions and proxy the CONNECT 
sessions.

Thanks,
Scott


From rousskov at measurement-factory.com  Mon Apr 27 19:09:03 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 27 Apr 2020 15:09:03 -0400
Subject: [squid-users] Best way to prevent squid from bumping CONNECTs
In-Reply-To: <20200427162110.GA50659@thismonkey.com>
References: <20200427162110.GA50659@thismonkey.com>
Message-ID: <8e4ac1e0-871f-5532-116f-904df0ad9c80@measurement-factory.com>

On 4/27/20 12:21 PM, Scott wrote:

> my experience with ssl_bump is that it tries to bump SSL connections whether 
> presented to Squid explicitly or implicitly.

* For http_port configured with an ssl-bump flag, HTTP CONNECT tunnels
are sent to the SslBump code.

* For https_port configured with an ssl-bump flag, all traffic is sent
to the SslBump code (by faking a corresponding HTTP CONNECT request).

* All other traffic is not sent to the SslBump code.

SslBump code honors ssl_bump rules when inspecting and
splicing/bumping/terminating/etc. traffic.


> When simply configuring `ssl_bump bump host_acl' the Squid logs show Squid 
> trying, and failing, to bump CONNECT requests.  They may be failing due to 
> certificate issue most likely, I'm not sure.  I can't add to the certificate 
> store of the software that has the proxy configured (i.e. it will not permit 
> bumping).

* If you do not want Squid to use SslBump features on traffic arriving
on port X, then do not add the ssl-bump flag to that port X.

* If you want to use the same port for traffic that should be bumped and
traffic that should not be inspected beyond step1, adjust your ssl_bump
step1 rules to distinguish the two kinds of messages.

Needless to say, you decide which traffic goes to which listening port
and whether a single port serves multiple traffic categories.


> Is it expected that Squid will bump/splice CONNECT requests?

It depends -- some CONNECT tunnels are expected to be inspected and
bumped, spliced, terminated, etc., according to the configuration.
Please see above for the details. Squid does not know what it will find
inside the CONNECT tunnel until it starts inspecting that tunnel.


> Because not all CONNECT sessions are SSL, if the CONNECT destination does not 
> begin a TLS handshake will Squid revert to simply creating a TCP tunnel 
> instead of bumping?

SslBump expects SSL/TLS traffic inside CONNECT tunnels that it is
configured to inspect. If an inspecting Squid decides that it got some
other traffic, Squid follows the on_unsupported_protocol configuration.


> My workaround has been to simply add `!CONNECT' to the `ssl_bump host_acl' 
> statements.  Squid will happily bump the SSL sessions and proxy the CONNECT 
> sessions.

AFAICT, that workaround cannot work on modern Squids because all traffic
subject to ssl_bump rules will start as a (real or fake) CONNECT request.


HTH,

Alex.


From leiwen14 at gmail.com  Mon Apr 27 21:44:41 2020
From: leiwen14 at gmail.com (Lei Wen)
Date: Mon, 27 Apr 2020 14:44:41 -0700
Subject: [squid-users] Using a Baltimore root certificate in transparent ssl
	proxying
Message-ID: <CAPu9cN7hDXdRTsFEwfrXO45GpFrd5Oom39pcs8oP4cZBWmP1_g@mail.gmail.com>

Hi,



We were able to set up the squid in a host to container infrastructure.
That is saying the squid is installed on host, proxying traffic from the
container on the same host. With transparent proxy including SSL traffic.

Another feature we enabled is request_header_access and
request_header_replace, to spoof and modify token in HTTP headers sending
to target dstdomain.



The issue we are having right now is the certificate installed on the
container is a self signed cert, we were trying to migrate this cert to a
real trusted CA cert, or a Baltimore root cert.

The issues seems to be in the subject name of the cert. In the self signed
cert, I simply leave everything blank. In the Baltimore root cert(squid.key
and squid.crt in below squid.conf example, request through Microsoft
internal service and it is Baltimore root), even if I have the dstdomain in
squid.conf as subject name(abc.microsoft.com in below squid.conf example),
I am still getting ?server certificate verification failed? error in CURL.
Is there anything I am missing or it simply doesn?t support? In my
understanding, it should has no difference with squid as root CA signer in
self signed cert?



P.S. I do notice that it is illegal for a trusted CA to issue official cert
to squid because squid itself is man-in-the-middle, so Squid can only
accept self signed cert and squid as root CA? I tried to search the email
archive but no luck.



I have such a squid.conf



acl abc dstdomain .abc.microsoft.com

request_header_access Authorization deny abc

request_header_replace Authorization Basic
whateverYourTokeisButForBasicItHasToBeBase64Encoded

request_header_access All allow all



https_port 3129 cert=/etc/squid3/squid.crt key=/etc/squid3/squid.key
ssl-bump intercept generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB

acl SSL_port port 443

http_access allow SSL_port

acl allowed_https_sites ssl::server_name "/etc/squid3/ssl_sites.txt"



ssl_bump server-first all

always_direct allow all



acl step1 at_step SslBump1

acl step2 at_step SslBump2

acl step3 at_step SslBump3

ssl_bump peek step1 all

ssl_bump peek step2 allowed_https_sites

ssl_bump splice step3 allowed_https_sites







Thanks,

Lei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200427/6034a3ba/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Apr 28 08:42:27 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 28 Apr 2020 10:42:27 +0200
Subject: [squid-users] Using a Baltimore root certificate in transparent
	ssl proxying
In-Reply-To: <CAPu9cN7hDXdRTsFEwfrXO45GpFrd5Oom39pcs8oP4cZBWmP1_g@mail.gmail.com>
References: <CAPu9cN7hDXdRTsFEwfrXO45GpFrd5Oom39pcs8oP4cZBWmP1_g@mail.gmail.com>
Message-ID: <202004281042.27133.Antony.Stone@squid.open.source.it>

On Monday 27 April 2020 at 23:44:41, Lei Wen wrote:

> The issue we are having right now is the certificate installed on the
> container is a self signed cert, we were trying to migrate this cert to a
> real trusted CA cert, or a Baltimore root cert.

That will not work for an intercepting ("transparent") proxy.

> I do notice that it is illegal for a trusted CA to issue official cert to
> squid because squid itself is man-in-the-middle, so Squid can only accept
> self signed cert and squid as root CA?

This is correct.

Squid is acting as a man-in-the-middle for *any* web request your users choose 
to pass through it, therefore it has to present a certificate to their browser 
which is valid for whatever domain they have requested.

In effect, it would need a wildcard certificate for the entire Internet.

No CA is going to give you that.


Regards,


Antony.

-- 
"How I managed so long without this book baffles the mind."

 - Richard Stoakley, Group Program Manager, Microsoft Corporation,
   referring to "The Art of Project Management", O'Reilly press

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Tue Apr 28 12:53:32 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2020 00:53:32 +1200
Subject: [squid-users] failing https requests
In-Reply-To: <fc4f8cad-3585-b6a0-13dc-72df7610801f@matrixscience.com>
References: <650bea86-07dd-07f5-f5e2-eee7089d7384@matrixscience.com>
 <263217bf-2d25-361b-9f54-e3e29e6445f6@treenet.co.nz>
 <fc4f8cad-3585-b6a0-13dc-72df7610801f@matrixscience.com>
Message-ID: <9f6f4d71-f36f-4096-86e4-21c6647f3fef@treenet.co.nz>

On 28/04/20 2:03 am, Adam Weremczuk wrote:
> Thanks Amos for the hint.
> 
> Tcpdump in source reveals the following:
> 
> HTTP/1.1 400 Bad Request
...
> 
> Can I determine which of the above is actually causing failures?
> 

The response says the request is bad. So look at the request message to
figure out what is bad about it.

Amos


From uhlar at fantomas.sk  Tue Apr 28 12:59:14 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 28 Apr 2020 14:59:14 +0200
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <1588000625812-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com>
 <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com>
 <20200427131204.GA19023@fantomas.sk>
 <1588000625812-0.post@n4.nabble.com>
Message-ID: <20200428125914.GC16296@fantomas.sk>

On 27.04.20 10:17, russel0901 wrote:
>Hi again... sorry i was not shouting just making the message capitalize.
>
>the message on my logs is...
>
>TCP_TUNNEL/200 39 CONNECT www.bancnetonline.com:443 -
>HIER_DIRECT/203.131.77.194 -

this means that the proxy was asked to connect to destination server and
succeeded.

>but still i can't visit the site...

well, the connection to the server above was creates (200 code)
what is the error message you see?

>note: we don't have any configuration of the browser (just default only)

what do you mean "Default"? Do you use proxy autodetection?
If not, configure the browser to use the proxy if it helps.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
You have the right to remain silent. Anything you say will be misquoted,
then used against you.


From uhlar at fantomas.sk  Tue Apr 28 13:02:23 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 28 Apr 2020 15:02:23 +0200
Subject: [squid-users] explicit proxy and iptables
In-Reply-To: <1943883120.975103.1588001244329@mail.yahoo.com>
References: <1943883120.975103.1588001244329.ref@mail.yahoo.com>
 <1943883120.975103.1588001244329@mail.yahoo.com>
Message-ID: <20200428130223.GD16296@fantomas.sk>

On 27.04.20 15:27, Vieri wrote:
>I've been using Squid + TPROXY in transparent sslbump mode for quite a
> while now, but I'd like to use an explicit proxy with user authentication
> instead.
>
>I have Squid on my first firewall/gateway node, and then I have another
> gateway (node 2) where all the HTTP requests go through, with multiple
> ISPs.
>
>In transparent tproxy mode, I can obviously mark packets according to the
> "real" client src IP addresses and then use, eg., different ISPs based on
> client src addr.
>
>In the explicit setup, the gateway (node 2) only sees one IP address as
> HTTP source -- the one on the "first node" with the explicit Squid proxy. 
> I presume that in this case there is NO WAY I can somehow inform the
> gateway on node 2 of the "real" clent IP addresses?

Correct.  However, you can configure first proxy to add proper
X-Forwarded-For address and configure the second proxy to trust the
X-Forwarded-For from the first proxy, so the second proxy can make decision
on how to route the request, based on trusted client's source IP address
passed through X-Forwarded-For header.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
On the other hand, you have different fingers.


From osantosmyr at gmail.com  Tue Apr 28 14:56:37 2020
From: osantosmyr at gmail.com (russel0901)
Date: Tue, 28 Apr 2020 09:56:37 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <20200428125914.GC16296@fantomas.sk>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com> <20200427131204.GA19023@fantomas.sk>
 <1588000625812-0.post@n4.nabble.com> <20200428125914.GC16296@fantomas.sk>
Message-ID: <1588085797128-0.post@n4.nabble.com>

Hi again...

sorry the browser has a configuration, we already static the browser to our
server 10.20.X.X to port 3333


about on the message of error: 

This site can?t be reached (on the browser error)

www.bancnetonline.com took too long to respond.

Try:

Checking the connection
Checking the proxy and the firewall
Running Windows Network Diagnostics
ERR_TIMED_OUT


note: sometimes it can be visited and sometimes not.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Apr 28 16:49:28 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Apr 2020 04:49:28 +1200
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <1588085797128-0.post@n4.nabble.com>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com> <20200427131204.GA19023@fantomas.sk>
 <1588000625812-0.post@n4.nabble.com> <20200428125914.GC16296@fantomas.sk>
 <1588085797128-0.post@n4.nabble.com>
Message-ID: <5741c5aa-27cc-6e46-942f-62cf7cb6d058@treenet.co.nz>

ONn 29/04/20 2:56 am, russel0901 wrote:
> Hi again...
> 
> sorry the browser has a configuration, we already static the browser to our
> server 10.20.X.X to port 3333
> 
> 
> about on the message of error: 
> 
> This site can?t be reached (on the browser error)
> 
> www.bancnetonline.com took too long to respond.
> > Try:
> 
> Checking the connection
> Checking the proxy and the firewall
> Running Windows Network Diagnostics
> ERR_TIMED_OUT
> 

All worth doing to the best of your abilities, regardless of what help
we provide.


Since Squid-4 said 200 status the TCP connection is _setup_ fine -
implying DNS also okay. However, the time that setup takes may be
relevant. Even if successful it may take long enough to impact the other
layers handshakes.

Path-MTU discovery may still be having issues with packet sizes after
TCP establishment. Missing ACK on any packets is the thing to be looking
for on the TCP connections - both client-Squid and Squid-server.


Then there is the TLS layer handshake. This is across the tunnel between
the client and server.
 You can use a TCP packet dump to track the TLS handshake messages
inside the tunnel with wireshark. Or a Squid-4 cache.log at level 9 will
give some indication of what TLS is doing via the I/O sizes. Timing is
again the thing to look for here.


Amos


From osantosmyr at gmail.com  Wed Apr 29 06:09:02 2020
From: osantosmyr at gmail.com (russel0901)
Date: Wed, 29 Apr 2020 01:09:02 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <5741c5aa-27cc-6e46-942f-62cf7cb6d058@treenet.co.nz>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com> <20200427131204.GA19023@fantomas.sk>
 <1588000625812-0.post@n4.nabble.com> <20200428125914.GC16296@fantomas.sk>
 <1588085797128-0.post@n4.nabble.com>
 <5741c5aa-27cc-6e46-942f-62cf7cb6d058@treenet.co.nz>
Message-ID: <1588140542555-0.post@n4.nabble.com>

Hi again sir,

deep is kinda deep to absorb on what you said about TLS, handshake and tcp
connection will try to research about this and trace the using tcp packet
dump, wireshark or cache.log of squid.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From osantosmyr at gmail.com  Wed Apr 29 14:16:27 2020
From: osantosmyr at gmail.com (russel0901)
Date: Wed, 29 Apr 2020 09:16:27 -0500 (CDT)
Subject: [squid-users] Squid - Can't visit (government site and Banking
 Site) - Please help
In-Reply-To: <5741c5aa-27cc-6e46-942f-62cf7cb6d058@treenet.co.nz>
References: <1587762543811-0.post@n4.nabble.com>
 <f2e77210-801a-d043-f007-4d97ca95c318@treenet.co.nz>
 <1587906880308-0.post@n4.nabble.com> <20200426163320.GA21705@fantomas.sk>
 <1587990661078-0.post@n4.nabble.com> <20200427131204.GA19023@fantomas.sk>
 <1588000625812-0.post@n4.nabble.com> <20200428125914.GC16296@fantomas.sk>
 <1588085797128-0.post@n4.nabble.com>
 <5741c5aa-27cc-6e46-942f-62cf7cb6d058@treenet.co.nz>
Message-ID: <1588169787879-0.post@n4.nabble.com>

Hi again,

as per checking using wireshark on my client-pc

This are my error messages


Client PC  -----  Proxy Server    TCP     54 [TCP Retransmission] 49804 ->
3333 [FIN, ACK] Seq=1 Ack=2 Win=1020 Len=0

Client PC  -----  Proxy Server    TCP     55 [TCP Keep-Alive] 49847 -> 3333
[ACK] Seq=0 Ack=1 Win=65536 Len=1





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From anthony_mead at progressive.com  Wed Apr 29 16:10:35 2020
From: anthony_mead at progressive.com (AMead)
Date: Wed, 29 Apr 2020 11:10:35 -0500 (CDT)
Subject: [squid-users] Ubuntu 18 with Squid 4.11 SSL_BUMP
Message-ID: <1588176635929-0.post@n4.nabble.com>

1. Compiled Squid 4.11 on Ubuntu 18 T3 EC2 instance:

./configure \
    --prefix=/usr \
    --exec-prefix=/usr \
    --bindir=/usr/bin \
    --sbindir=/usr/sbin \
    --libdir=/usr/lib \
    --libexecdir=/usr/libexec/squid \
    --includedir=/usr/include \
    --mandir=/usr/share/man \
    --infodir=/usr/share/info \
    --datadir=/usr/share/squid \
    --sysconfdir=/etc/squid \
    --localstatedir=/var \
    --sharedstatedir=/var/lib \
    --with-logdir=/var/log/squid \
    --with-pidfile=/var/run/squid.pid \
    --with-default-user=squid \
    --with-openssl \
    --enable-ssl \
    --enable-ssl-crtd


2. Initialized the ssl database:

sudo /usr/libexec/squid/security_file_certgen -c -s /var/cache/squid/ssl_db
-M 4MB


3. I've tried to read through a few similar posts, and got something
reasonably working for the allowance, but now it's appearing to allow
everything:

> /etc/squid/whitelist.txt
*.github.com

> /etc/squid/squid.conf

visible_hostname squid
cache deny all

# Handling HTTP requests
http_port 3128
http_port 3129 intercept
acl allowed_http_sites dstdomain "/etc/squid/whitelist.txt"
http_access allow allowed_http_sites

# Handling HTTPS requests
acl SSL_port port 443
http_access allow SSL_port

https_port 3130 intercept ssl-bump    \
        cert=/etc/squid/ssl/squid.pem \
        # generate-host-certificates=on \ # Defaulted with 4.11
        dynamic_cert_mem_cache_size=16MB

# HTTPS - Peek & Splice
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

# Alex R
# 10.0.1.93 NONE_ABORTED/200 0 CONNECT 209.216.230.240:443 - HIER_NONE/- -
#
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-4-6-Transparent-HTTP-amp-HTTPS-Proxy-td4687578.html
#ssl_bump peek step1
#ssl_bump peek step2 allowed_https_sites
#ssl_bump terminate step2
#ssl_bump splice all

# Berger
# 10.0.1.93 NONE_ABORTED/200 0 CONNECT 209.216.230.240:443 - HIER_NONE/- -
#
http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-4-1-transparent-https-issue-quot-curl-60-SSL-certificate-problem-self-signed-certificate-in-ce-td4688553.html
#ssl_bump peek step1 all
#ssl_bump peek step2 allowed_https_sites
#ssl_bump splice step3 allowed_https_sites
#ssl_bump terminate

#dkanejs
# 10.0.1.93 TCP_TUNNEL/200 25082 CONNECT 185.199.111.153:443
# Allows https://example.com, https://github.com, but not
https://news.ycombinator.com
ssl_bump peek all
acl allowed_https_sites ssl::server_name "/etc/squid/whitelist.txt"
ssl_bump splice allowed_https_sites
ssl_bump terminate all

http_access deny all




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Walter.H at mathemainzel.info  Wed Apr 29 17:26:59 2020
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 29 Apr 2020 19:26:59 +0200
Subject: [squid-users] Gateway Proxy failure - but only with one browser ...
Message-ID: <ceb3d13d-6154-fcc2-ac71-e09063cb7efa@mathemainzel.info>

I have two squids,

one does SSL bump (3.5latest CentOS 6)
the other doesn't SSL bump (3.4latest CentOS 6)

everything works,

I have a site that uses SSL/TLS, and two different browsers (one in a VM 
with old windows),

when I use the squid without SSL bump, the site works with both browsers,

but when I use the squid with SSL bump, with the old browser I get a 
"Gateway Proxy failure"

the log shows this:

host - - [29/Apr/2020:19:04:11 +0200] "CONNECT ssl.mathemainzel.info:443 
HTTP/1.1" 200 0 "-" "Mozilla/5.0 (Windows; U; WinNT4.0; en-US; 
rv:1.8.1.20) Gecko/20081217 Firefox/2.0.0.20" TAG_NONE:HIER_DIRECT 
SNI:ssl.mathemainzel.info
host - - [29/Apr/2020:19:04:11 +0200] "GET 
https://ssl.mathemainzel.info/sslinfo/ HTTP/1.1" 500 1679 "-" 
"Mozilla/5.0 (Windows; U; WinNT4.0; en-US; rv:1.8.1.20) Gecko/20081217 
Firefox/2.0.0.20" TAG_NONE:HIER_NONE SNI:ssl.mathemainzel.info

in compare to the log when using the other browser ...

host - - [29/Apr/2020:19:05:53 +0200] "CONNECT ssl.mathemainzel.info:443 
HTTP/1.1" 200 0 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.9) 
Goanna/4.5 PaleMoon/28.9.1" TAG_NONE:HIER_DIRECT SNI:ssl.mathemainzel.info
host - - [29/Apr/2020:19:05:53 +0200] "GET 
https://ssl.mathemainzel.info/sslinfo/ HTTP/1.1" 200 1977 
"https://ssl.mathemainzel.info/" "Mozilla/5.0 (Windows NT 10.0; Win64; 
x64; rv:68.9) Goanna/4.5 PaleMoon/28.9.1" TCP_MISS:HIER_DIRECT 
SNI:ssl.mathemainzel.info

is this caused by the browser on old OS itself?

squid.conf (of squid with SSL bump)

reply_header_access Public-Key-Pins deny all

reply_header_access Strict-Transport-Security deny all
reply_header_replace Strict-Transport-Security max-age=0; includeSubDomains

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl nobumpsites ssl::server_name "/etc/squid/sslnobumpsites-acl.squid"

ssl_bump peek step1
ssl_bump splice nobumpsites
ssl_bump stare step2
ssl_bump bump all

sslproxy_cafile /etc/squid/ca-bundle.trust.crt
sslproxy_cipher 
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
sslproxy_flags DONT_VERIFY_PEER,NO_DEFAULT_CA
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/local/squid/ssl_db -M 16MB
sslcrtd_children 8

http_port 3128 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=16MB cert=/etc/squid/cert/squidCA.pem 
options=NO_SSLv2,NO_SSLv3


Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3511 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200429/6a293d6b/attachment.bin>

From Walter.H at mathemainzel.info  Wed Apr 29 18:16:42 2020
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 29 Apr 2020 20:16:42 +0200
Subject: [squid-users] Gateway Proxy failure - but only with one browser
 ...
In-Reply-To: <ceb3d13d-6154-fcc2-ac71-e09063cb7efa@mathemainzel.info>
References: <ceb3d13d-6154-fcc2-ac71-e09063cb7efa@mathemainzel.info>
Message-ID: <f57ec196-1053-f1b1-7fd1-de26f7ade6ac@mathemainzel.info>

It is very probable that the following has the same reason - but I don't 
know what's causing it ...

the old browser on old OS gives this

<errorpage>
While trying to retrieve the URL: https://mein.elba.hypo.at/*

The following error was encountered:

 ??? * Failed to establish a secure connection to 217.13.188.204

The system returned:

 ??? (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)

 ??? Handshake with SSL server failed: error:1407742E:SSL 
routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version
...
</errorpage>

the? new browser works ...

I thought that the SSL connection between browser and squid is different 
from the one between squid and server;
how can there be a SSL handshake problem between squid and server when 
using an old browser?


On 29.04.2020 19:26, Walter H. wrote:
> I have two squids,
>
> one does SSL bump (3.5latest CentOS 6)
> the other doesn't SSL bump (3.4latest CentOS 6)
>
> everything works,
>
> I have a site that uses SSL/TLS, and two different browsers (one in a 
> VM with old windows),
>
> when I use the squid without SSL bump, the site works with both browsers,
>
> but when I use the squid with SSL bump, with the old browser I get a 
> "Gateway Proxy failure"
>
> the log shows this:
>
> host - - [29/Apr/2020:19:04:11 +0200] "CONNECT 
> ssl.mathemainzel.info:443 HTTP/1.1" 200 0 "-" "Mozilla/5.0 (Windows; 
> U; WinNT4.0; en-US; rv:1.8.1.20) Gecko/20081217 Firefox/2.0.0.20" 
> TAG_NONE:HIER_DIRECT SNI:ssl.mathemainzel.info
> host - - [29/Apr/2020:19:04:11 +0200] "GET 
> https://ssl.mathemainzel.info/sslinfo/ HTTP/1.1" 500 1679 "-" 
> "Mozilla/5.0 (Windows; U; WinNT4.0; en-US; rv:1.8.1.20) Gecko/20081217 
> Firefox/2.0.0.20" TAG_NONE:HIER_NONE SNI:ssl.mathemainzel.info
>
> in compare to the log when using the other browser ...
>
> host - - [29/Apr/2020:19:05:53 +0200] "CONNECT 
> ssl.mathemainzel.info:443 HTTP/1.1" 200 0 "-" "Mozilla/5.0 (Windows NT 
> 10.0; Win64; x64; rv:68.9) Goanna/4.5 PaleMoon/28.9.1" 
> TAG_NONE:HIER_DIRECT SNI:ssl.mathemainzel.info
> host - - [29/Apr/2020:19:05:53 +0200] "GET 
> https://ssl.mathemainzel.info/sslinfo/ HTTP/1.1" 200 1977 
> "https://ssl.mathemainzel.info/" "Mozilla/5.0 (Windows NT 10.0; Win64; 
> x64; rv:68.9) Goanna/4.5 PaleMoon/28.9.1" TCP_MISS:HIER_DIRECT 
> SNI:ssl.mathemainzel.info
>
> is this caused by the browser on old OS itself?
>
> squid.conf (of squid with SSL bump)
>
> reply_header_access Public-Key-Pins deny all
>
> reply_header_access Strict-Transport-Security deny all
> reply_header_replace Strict-Transport-Security max-age=0; 
> includeSubDomains
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> acl nobumpsites ssl::server_name "/etc/squid/sslnobumpsites-acl.squid"
>
> ssl_bump peek step1
> ssl_bump splice nobumpsites
> ssl_bump stare step2
> ssl_bump bump all
>
> sslproxy_cafile /etc/squid/ca-bundle.trust.crt
> sslproxy_cipher 
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
> sslproxy_flags DONT_VERIFY_PEER,NO_DEFAULT_CA
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/local/squid/ssl_db 
> -M 16MB
> sslcrtd_children 8
>
> http_port 3128 ssl-bump generate-host-certificates=on 
> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/cert/squidCA.pem 
> options=NO_SSLv2,NO_SSLv3
>
>
> Thanks,
> Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3511 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200429/f2e6db28/attachment.bin>

From squid3 at treenet.co.nz  Wed Apr 29 18:39:10 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Apr 2020 06:39:10 +1200
Subject: [squid-users] Gateway Proxy failure - but only with one browser
 ...
In-Reply-To: <f57ec196-1053-f1b1-7fd1-de26f7ade6ac@mathemainzel.info>
References: <ceb3d13d-6154-fcc2-ac71-e09063cb7efa@mathemainzel.info>
 <f57ec196-1053-f1b1-7fd1-de26f7ade6ac@mathemainzel.info>
Message-ID: <7d3ceedc-4c10-71f6-0211-cf2412306948@treenet.co.nz>

On 30/04/20 6:16 am, Walter H. wrote:
> It is very probable that the following has the same reason - but I don't
> know what's causing it ...
> 
> the old browser on old OS gives this
> 
> <errorpage>
> While trying to retrieve the URL: https://mein.elba.hypo.at/*
> 
> The following error was encountered:
> 
> ??? * Failed to establish a secure connection to 217.13.188.204
> 
> The system returned:
> 
> ??? (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> 
> ??? Handshake with SSL server failed: error:1407742E:SSL
> routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version
> ...
> </errorpage>
> 
> the? new browser works ...
> 
> I thought that the SSL connection between browser and squid is different
> from the one between squid and server;
> how can there be a SSL handshake problem between squid and server when
> using an old browser?
> 

For transparency and because TLS requirements are embedded in the
certificates Squid makes the connection to the server as close as
possible to the same properties the client connection uses.
 The change in browser thus affects both what Squid can pass on to the
server, and what can be passed back from the server to the client.

...

>> sslproxy_flags DONT_VERIFY_PEER,NO_DEFAULT_CA

This is a misconfiguration. Please drop the DONT_VERIFY_PEER.

If the server is not validating using the CA certs you told Squid were
the *only* acceptible CAs:

  sslproxy_cafile /etc/squid/ca-bundle.trust.crt

... then either the contents of that file are wrong, or the server
connection is compromised. Determining the latter is the whole point of TLS.


Amos


From rousskov at measurement-factory.com  Wed Apr 29 18:41:52 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 29 Apr 2020 14:41:52 -0400
Subject: [squid-users] Gateway Proxy failure - but only with one browser
 ...
In-Reply-To: <f57ec196-1053-f1b1-7fd1-de26f7ade6ac@mathemainzel.info>
References: <ceb3d13d-6154-fcc2-ac71-e09063cb7efa@mathemainzel.info>
 <f57ec196-1053-f1b1-7fd1-de26f7ade6ac@mathemainzel.info>
Message-ID: <5026cb9d-4ce3-3dd0-f6bc-7a311b62799b@measurement-factory.com>

On 4/29/20 2:16 PM, Walter H. wrote:
> It is very probable that the following has the same reason - but I don't
> know what's causing it ...

While your symptoms are a bit different, you might be suffering from the
problem fixed by https://github.com/squid-cache/squid/pull/588


> Handshake with SSL server failed: error:1407742E:SSL
> routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version


> I thought that the SSL connection between browser and squid is different
> from the one between squid and server;

When staring or bumping, it is. However, "different" does not imply
"unrelated" (as discussed below).


> how can there be a SSL handshake problem between squid and server when
> using an old browser?

Depending on the conditions, Squid relays parts of the browser handshake
when talking to the server. For more (incomplete/stale) details, please
see the "Mimicking TLS Client Hello properties when staring" section at
https://wiki.squid-cache.org/Features/SslPeekAndSplice

IIRC, Squid mimics at least some properties because we wanted Squid to
"represent" the client to the server as faithfylly as possible (i.e.,
minimize Squid-introduced changes to the TLS-negotiated parameters). In
retrospect, I am not sure that was the right decision. Perhaps the
choice should be the opposite or configurable.

Please note that I am not trying to justify Squid actions. I am only
explaining why what you observe may be possible. One could argue that
Squid should not mimic the TLS client at all (when staring). I do not
recall whether anybody has tried to make that argument.


HTH,

Alex.


> On 29.04.2020 19:26, Walter H. wrote:
>> I have two squids,
>>
>> one does SSL bump (3.5latest CentOS 6)
>> the other doesn't SSL bump (3.4latest CentOS 6)
>>
>> everything works,
>>
>> I have a site that uses SSL/TLS, and two different browsers (one in a
>> VM with old windows),
>>
>> when I use the squid without SSL bump, the site works with both browsers,
>>
>> but when I use the squid with SSL bump, with the old browser I get a
>> "Gateway Proxy failure"
>>
>> the log shows this:
>>
>> host - - [29/Apr/2020:19:04:11 +0200] "CONNECT
>> ssl.mathemainzel.info:443 HTTP/1.1" 200 0 "-" "Mozilla/5.0 (Windows;
>> U; WinNT4.0; en-US; rv:1.8.1.20) Gecko/20081217 Firefox/2.0.0.20"
>> TAG_NONE:HIER_DIRECT SNI:ssl.mathemainzel.info
>> host - - [29/Apr/2020:19:04:11 +0200] "GET
>> https://ssl.mathemainzel.info/sslinfo/ HTTP/1.1" 500 1679 "-"
>> "Mozilla/5.0 (Windows; U; WinNT4.0; en-US; rv:1.8.1.20) Gecko/20081217
>> Firefox/2.0.0.20" TAG_NONE:HIER_NONE SNI:ssl.mathemainzel.info
>>
>> in compare to the log when using the other browser ...
>>
>> host - - [29/Apr/2020:19:05:53 +0200] "CONNECT
>> ssl.mathemainzel.info:443 HTTP/1.1" 200 0 "-" "Mozilla/5.0 (Windows NT
>> 10.0; Win64; x64; rv:68.9) Goanna/4.5 PaleMoon/28.9.1"
>> TAG_NONE:HIER_DIRECT SNI:ssl.mathemainzel.info
>> host - - [29/Apr/2020:19:05:53 +0200] "GET
>> https://ssl.mathemainzel.info/sslinfo/ HTTP/1.1" 200 1977
>> "https://ssl.mathemainzel.info/" "Mozilla/5.0 (Windows NT 10.0; Win64;
>> x64; rv:68.9) Goanna/4.5 PaleMoon/28.9.1" TCP_MISS:HIER_DIRECT
>> SNI:ssl.mathemainzel.info
>>
>> is this caused by the browser on old OS itself?
>>
>> squid.conf (of squid with SSL bump)
>>
>> reply_header_access Public-Key-Pins deny all
>>
>> reply_header_access Strict-Transport-Security deny all
>> reply_header_replace Strict-Transport-Security max-age=0;
>> includeSubDomains
>>
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> acl nobumpsites ssl::server_name "/etc/squid/sslnobumpsites-acl.squid"
>>
>> ssl_bump peek step1
>> ssl_bump splice nobumpsites
>> ssl_bump stare step2
>> ssl_bump bump all
>>
>> sslproxy_cafile /etc/squid/ca-bundle.trust.crt
>> sslproxy_cipher
>> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA:EECDH:EDH+AESGCM:EDH:ECDH+AESGCM:ECDH+AES:ECDH:AES:HIGH:MEDIUM:!SSLv2:+SSLv3:!3DES:!RC4:!MD5:!IDEA:!SEED:!aNULL:!eNULL:!LOW:!EXP:!DSS:!PSK:!RSA:!SRP
>>
>> sslproxy_flags DONT_VERIFY_PEER,NO_DEFAULT_CA
>> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/local/squid/ssl_db
>> -M 16MB
>> sslcrtd_children 8
>>
>> http_port 3128 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=16MB cert=/etc/squid/cert/squidCA.pem
>> options=NO_SSLv2,NO_SSLv3
>>
>>
>> Thanks,
>> Walter
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Wed Apr 29 18:57:58 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Apr 2020 06:57:58 +1200
Subject: [squid-users] Ubuntu 18 with Squid 4.11 SSL_BUMP
In-Reply-To: <1588176635929-0.post@n4.nabble.com>
References: <1588176635929-0.post@n4.nabble.com>
Message-ID: <2232755d-6e94-324a-114a-1a6b87cf1b39@treenet.co.nz>

On 30/04/20 4:10 am, AMead wrote:
> 1. Compiled Squid 4.11 on Ubuntu 18 T3 EC2 instance:
> 
> ./configure \

...
>     --with-openssl \
>     --enable-ssl \

"--enable-ssl" is not a Squid build option.

>     --enable-ssl-crtd
> 
> 
> 2. Initialized the ssl database:
> 
> sudo /usr/libexec/squid/security_file_certgen -c -s /var/cache/squid/ssl_db
> -M 4MB
> 
> 
> 3. I've tried to read through a few similar posts, and got something
> reasonably working for the allowance, but now it's appearing to allow
> everything:
> 
>> /etc/squid/whitelist.txt
> *.github.com
> 

This is not dstdomain syntax. Remove the "*" character.


Amos


From dawood.aijaz97 at gmail.com  Wed Apr 29 19:45:31 2020
From: dawood.aijaz97 at gmail.com (Dawood Aijaz)
Date: Thu, 30 Apr 2020 00:45:31 +0500
Subject: [squid-users] Help with FTP native proxy squid 3.5
Message-ID: <CAHkxSOk1Pp8fnqPRc8Fho4SSkety3-0hKKi_aZ8D8T4Neqja3A@mail.gmail.com>

Hi,
I am able to configure an FTP proxy through HTTP however I need a native
FTP. I was told squid supports as of Cv3.5.But I am unable to find any help
regarding configuration and any tutorial to help me do this task

Can anyone share configuration for setting up native FTP proxy,

Regards,
Dawood Aijaz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200430/4f7ee0bd/attachment.htm>

From ANTHONY_MEAD at progressive.com  Wed Apr 29 20:15:09 2020
From: ANTHONY_MEAD at progressive.com (Anthony Mead)
Date: Wed, 29 Apr 2020 20:15:09 +0000
Subject: [squid-users] [EXTERNAL] Re: Ubuntu 18 with Squid 4.11 SSL_BUMP
In-Reply-To: <2232755d-6e94-324a-114a-1a6b87cf1b39@treenet.co.nz>
References: <1588176635929-0.post@n4.nabble.com>
 <2232755d-6e94-324a-114a-1a6b87cf1b39@treenet.co.nz>
Message-ID: <AA42C786-AAAA-4101-9AF8-03A3834F2C66@progressive.com>

Thanks!  I've re-compiled without the unnecessary flag, and restarted the service with a new whitelist, unfortunately i'm getting such a varying of /var/log/squid/access.log messages that I'm not sure what to google anymore.

I want to deny all access to external sites except http/https github.com but some sites seem to connect, while others don't:

~$ # this is correct
~$ curl http://github.com/
10.0.1.180 TCP_MISS/301 200 GET http://github.com/

~$ # this is correct
~$ curl https://github.com/ 
10.0.1.180 TCP_TUNNEL/200 107323 CONNECT 140.82.114.4:443

~$ # this should deny
~$ curl https://youtube.com/
10.0.1.180 TCP_TUNNEL/200 4844 CONNECT 172.217.15.110:443

~$ # this should deny
~$ curl https://google.com/
10.0.1.180 TCP_TUNNEL_ABORTED/200 5103 CONNECT 172.217.2.110:443

~$ # this is denying - but not from squid, but openssl?
~$ curl https://news.ycombinator.com/
curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to news.ycombinator.com:443
10.0.1.180 NONE_ABORTED/200 0 CONNECT 209.216.230.240:443



?On 4/29/20, 2:59 PM, "squid-users on behalf of Amos Jeffries" <squid-users-bounces at lists.squid-cache.org on behalf of squid3 at treenet.co.nz> wrote:

    On 30/04/20 4:10 am, AMead wrote:
    > 1. Compiled Squid 4.11 on Ubuntu 18 T3 EC2 instance:
    > 
    > ./configure \

    ...
    >     --with-openssl \
    >     --enable-ssl \

    "--enable-ssl" is not a Squid build option.

    >     --enable-ssl-crtd
    > 
    > 
    > 2. Initialized the ssl database:
    > 
    > sudo /usr/libexec/squid/security_file_certgen -c -s /var/cache/squid/ssl_db
    > -M 4MB
    > 
    > 
    > 3. I've tried to read through a few similar posts, and got something
    > reasonably working for the allowance, but now it's appearing to allow
    > everything:
    > 
    >> /etc/squid/whitelist.txt
    > *.github.com
    > 

    This is not dstdomain syntax. Remove the "*" character.


    Amos
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Apr 29 20:33:17 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Apr 2020 08:33:17 +1200
Subject: [squid-users] Ubuntu 18 with Squid 4.11 SSL_BUMP
In-Reply-To: <AA42C786-AAAA-4101-9AF8-03A3834F2C66@progressive.com>
References: <1588176635929-0.post@n4.nabble.com>
 <2232755d-6e94-324a-114a-1a6b87cf1b39@treenet.co.nz>
 <AA42C786-AAAA-4101-9AF8-03A3834F2C66@progressive.com>
Message-ID: <e8cb2ef2-347e-d0ff-e92c-14ffb1665bef@treenet.co.nz>

On 30/04/20 8:15 am, Anthony Mead wrote:
> Thanks!  I've re-compiled without the unnecessary flag, and restarted the service with a new whitelist, unfortunately i'm getting such a varying of /var/log/squid/access.log messages that I'm not sure what to google anymore.
> 
> I want to deny all access to external sites except http/https github.com but some sites seem to connect, while others don't:
> 

There are a lot of details missing from your quoted log lines. Details
such as which server was contacted are important when there are more
than one TCP connection involved.

Since this is SSL-Bump _each_ curl request should result in _3_
access.log lines - with varying client, server and URI values.

You are only showing us one log line at a time. With only the client and
URI parts.


Bellow is a *guess* about what is going on, based on what the status
says. This is only to demonstrate that for each line you show there is
at least one situation where your squid.conf file tells Squid to do an
action which would result in that line. Whether these guesses are right
requires all the information you are omitting.



> ~$ # this is correct
> ~$ curl http://github.com/
> 10.0.1.180 TCP_MISS/301 200 GET http://github.com/
> 

 acl allowed_http_sites dstdomain "/etc/squid/whitelist.txt"
 http_access allow allowed_http_sites


> ~$ # this is correct
> ~$ curl https://github.com/ 
> 10.0.1.180 TCP_TUNNEL/200 107323 CONNECT 140.82.114.4:443
> 

  acl SSL_port port 443
  http_access allow SSL_port

  ssl_bump peek all


> ~$ # this should deny
> ~$ curl https://youtube.com/
> 10.0.1.180 TCP_TUNNEL/200 4844 CONNECT 172.217.15.110:443
> 

  acl SSL_port port 443
  http_access allow SSL_port

  ssl_bump peek all


> ~$ # this should deny
> ~$ curl https://google.com/
> 10.0.1.180 TCP_TUNNEL_ABORTED/200 5103 CONNECT 172.217.2.110:443
> 

  acl SSL_port port 443
  http_access allow SSL_port

  ssl_bump peek all


> ~$ # this is denying - but not from squid, but openssl?
> ~$ curl https://news.ycombinator.com/
> curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to news.ycombinator.com:443
> 10.0.1.180 NONE_ABORTED/200 0 CONNECT 209.216.230.240:443
> 

  acl SSL_port port 443
  http_access allow SSL_port

  ssl_bump terminate all



Amos


From ANTHONY_MEAD at progressive.com  Wed Apr 29 21:11:25 2020
From: ANTHONY_MEAD at progressive.com (Anthony Mead)
Date: Wed, 29 Apr 2020 21:11:25 +0000
Subject: [squid-users] [EXTERNAL] Re: Ubuntu 18 with Squid 4.11 SSL_BUMP
In-Reply-To: <e8cb2ef2-347e-d0ff-e92c-14ffb1665bef@treenet.co.nz>
References: <1588176635929-0.post@n4.nabble.com>
 <2232755d-6e94-324a-114a-1a6b87cf1b39@treenet.co.nz>
 <AA42C786-AAAA-4101-9AF8-03A3834F2C66@progressive.com>
 <e8cb2ef2-347e-d0ff-e92c-14ffb1665bef@treenet.co.nz>
Message-ID: <39EEF70C-5A29-4C54-90BF-8896C9BDCE56@progressive.com>

Hmm, if there were more logs I'd share them!  Any reason why I'd only see a access.log line?

I promise if I curl https://google.com  this is the only line I see: 
1588193897.852     20 10.0.1.180 TCP_TUNNEL_ABORTED/200 5103 CONNECT 172.217.15.78:443 - ORIGINAL_DST/172.217.15.78 -

Or curl https://youtube.com :
1588194262.880     32 10.0.1.180 TCP_TUNNEL/200 4824 CONNECT 172.217.13.78:443 - ORIGINAL_DST/172.217.13.78 -

Or curl https://github.com/:
1588194657.291     45 10.0.1.180 TCP_TUNNEL/200 107344 CONNECT 140.82.113.4:443 - ORIGINAL_DST/140.82.113.4 -

To avoid an X/Y problem the rest of my setup mimics a few blog posts - An EC2 in a private subnet that has all traffic being forwarded to the squid instance, which has iptables forwarding http/https to 3129/3130.  All approved traffic is then forwarded onto a NAT Gateway.  Maybe another piece of the "puzzle" is capturing the logs.

Also I really appreciate your help so far!


?On 4/29/20, 4:35 PM, "squid-users on behalf of Amos Jeffries" <squid-users-bounces at lists.squid-cache.org on behalf of squid3 at treenet.co.nz> wrote:

    On 30/04/20 8:15 am, Anthony Mead wrote:
    > Thanks!  I've re-compiled without the unnecessary flag, and restarted the service with a new whitelist, unfortunately i'm getting such a varying of /var/log/squid/access.log messages that I'm not sure what to google anymore.
    > 
    > I want to deny all access to external sites except http/https github.com but some sites seem to connect, while others don't:
    > 

    There are a lot of details missing from your quoted log lines. Details
    such as which server was contacted are important when there are more
    than one TCP connection involved.

    Since this is SSL-Bump _each_ curl request should result in _3_
    access.log lines - with varying client, server and URI values.

    You are only showing us one log line at a time. With only the client and
    URI parts.


    Bellow is a *guess* about what is going on, based on what the status
    says. This is only to demonstrate that for each line you show there is
    at least one situation where your squid.conf file tells Squid to do an
    action which would result in that line. Whether these guesses are right
    requires all the information you are omitting.



    > ~$ # this is correct
    > ~$ curl http://github.com/
    > 10.0.1.180 TCP_MISS/301 200 GET http://github.com/
    > 

     acl allowed_http_sites dstdomain "/etc/squid/whitelist.txt"
     http_access allow allowed_http_sites


    > ~$ # this is correct
    > ~$ curl https://github.com/ 
    > 10.0.1.180 TCP_TUNNEL/200 107323 CONNECT 140.82.114.4:443
    > 

      acl SSL_port port 443
      http_access allow SSL_port

      ssl_bump peek all


    > ~$ # this should deny
    > ~$ curl https://youtube.com/
    > 10.0.1.180 TCP_TUNNEL/200 4844 CONNECT 172.217.15.110:443
    > 

      acl SSL_port port 443
      http_access allow SSL_port

      ssl_bump peek all


    > ~$ # this should deny
    > ~$ curl https://google.com/
    > 10.0.1.180 TCP_TUNNEL_ABORTED/200 5103 CONNECT 172.217.2.110:443
    > 

      acl SSL_port port 443
      http_access allow SSL_port

      ssl_bump peek all


    > ~$ # this is denying - but not from squid, but openssl?
    > ~$ curl https://news.ycombinator.com/
    > curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to news.ycombinator.com:443
    > 10.0.1.180 NONE_ABORTED/200 0 CONNECT 209.216.230.240:443
    > 

      acl SSL_port port 443
      http_access allow SSL_port

      ssl_bump terminate all



    Amos
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Apr 29 21:50:14 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Apr 2020 09:50:14 +1200
Subject: [squid-users] [EXTERNAL] Re: Ubuntu 18 with Squid 4.11 SSL_BUMP
In-Reply-To: <39EEF70C-5A29-4C54-90BF-8896C9BDCE56@progressive.com>
References: <1588176635929-0.post@n4.nabble.com>
 <2232755d-6e94-324a-114a-1a6b87cf1b39@treenet.co.nz>
 <AA42C786-AAAA-4101-9AF8-03A3834F2C66@progressive.com>
 <e8cb2ef2-347e-d0ff-e92c-14ffb1665bef@treenet.co.nz>
 <39EEF70C-5A29-4C54-90BF-8896C9BDCE56@progressive.com>
Message-ID: <8d8571d3-1dbb-cc5b-f7aa-ad1e4dc05068@treenet.co.nz>

On 30/04/20 9:11 am, Anthony Mead wrote:
> Hmm, if there were more logs I'd share them!  Any reason why I'd only see a access.log line?
> 
> I promise if I curl https://google.com  this is the only line I see: 
> 1588193897.852     20 10.0.1.180 TCP_TUNNEL_ABORTED/200 5103 CONNECT 172.217.15.78:443 - ORIGINAL_DST/172.217.15.78 -
> 
> Or curl https://youtube.com :
> 1588194262.880     32 10.0.1.180 TCP_TUNNEL/200 4824 CONNECT 172.217.13.78:443 - ORIGINAL_DST/172.217.13.78 -
> 
> Or curl https://github.com/:
> 1588194657.291     45 10.0.1.180 TCP_TUNNEL/200 107344 CONNECT 140.82.113.4:443 - ORIGINAL_DST/140.82.113.4 -
> 


Hm. There should at least be a second line showing what server name was
sent in the peek'd SNI or server cert.

The first looks like it reached "terminate all" at step3 of the bumping
process.

The last looks like it was spliced (by the data size transferred). But
that definitely requires the server name to happen.


Amos


From rousskov at measurement-factory.com  Thu Apr 30 15:58:26 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 30 Apr 2020 11:58:26 -0400
Subject: [squid-users] Help with FTP native proxy squid 3.5
In-Reply-To: <CAHkxSOk1Pp8fnqPRc8Fho4SSkety3-0hKKi_aZ8D8T4Neqja3A@mail.gmail.com>
References: <CAHkxSOk1Pp8fnqPRc8Fho4SSkety3-0hKKi_aZ8D8T4Neqja3A@mail.gmail.com>
Message-ID: <ff2889a6-f084-f8c9-9add-12939484dcd9@measurement-factory.com>

On 4/29/20 3:45 PM, Dawood Aijaz wrote:

> I am able to configure an FTP proxy through HTTP however I need a native
> FTP. I was told squid supports as of Cv3.5.But I am unable to find any
> help regarding configuration and any tutorial to help me do this task
> 
> Can anyone share configuration for setting up native FTP proxy,

Here is one example:

    ftp_port 21

Please see http://www.squid-cache.org/Doc/config/ftp_port/ for details.

Alex.


From 3m9n51s2ewut at thismonkey.com  Thu Apr 30 16:10:30 2020
From: 3m9n51s2ewut at thismonkey.com (Scott)
Date: Fri, 1 May 2020 02:10:30 +1000
Subject: [squid-users] Best way to prevent squid from bumping CONNECTs
In-Reply-To: <mailman.520.1588063344.3558.squid-users@lists.squid-cache.org>
References: <mailman.520.1588063344.3558.squid-users@lists.squid-cache.org>
Message-ID: <20200430161030.GA87833@thismonkey.com>

> Date: Mon, 27 Apr 2020 15:09:03 -0400
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Best way to prevent squid from bumping CONNECTs
> 
> On 4/27/20 12:21 PM, Scott wrote:
> 
> > my experience with ssl_bump is that it tries to bump SSL connections whether 
> > presented to Squid explicitly or implicitly.
> 
> * For http_port configured with an ssl-bump flag, HTTP CONNECT tunnels
> are sent to the SslBump code.
> 
> * For https_port configured with an ssl-bump flag, all traffic is sent
> to the SslBump code (by faking a corresponding HTTP CONNECT request).
> 
Indeed.  I have ssl-bump on both my http_port and https_port (intercept).

These `fake' CONNECT requests I assume only contain the IP address of the 
upstream server, not the hostname, as intercepted SSL connections are TCP 
OPENs.

Am I right then in saying that using ssl::server_name is useless for bumped 
intercepted connections?  (Even if Squid did reverse proxy the IP it would be 
fairly unreliable).

> > Because not all CONNECT sessions are SSL, if the CONNECT destination does 
> > not begin a TLS handshake will Squid revert to simply creating a TCP 
> > tunnel instead of bumping?
> 
> SslBump expects SSL/TLS traffic inside CONNECT tunnels that it is
> configured to inspect. If an inspecting Squid decides that it got some
> other traffic, Squid follows the on_unsupported_protocol configuration.

Thanks, I was unaware of that option.

> > My workaround has been to simply add `!CONNECT' to the `ssl_bump 
> > host_acl' statements.  Squid will happily bump the SSL sessions and proxy 
> > the CONNECT sessions.
> 
> AFAICT, that workaround cannot work on modern Squids because all traffic
> subject to ssl_bump rules will start as a (real or fake) CONNECT request.

I'm running `Squid Cache: Version 5.0.1-20200312-r8a511d5e0'


From rousskov at measurement-factory.com  Thu Apr 30 20:05:43 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 30 Apr 2020 16:05:43 -0400
Subject: [squid-users] Best way to prevent squid from bumping CONNECTs
In-Reply-To: <20200430161030.GA87833@thismonkey.com>
References: <mailman.520.1588063344.3558.squid-users@lists.squid-cache.org>
 <20200430161030.GA87833@thismonkey.com>
Message-ID: <9d5db430-5186-1080-fb3e-abe409aed50a@measurement-factory.com>

On 4/30/20 12:10 PM, Scott wrote:

>> * For http_port configured with an ssl-bump flag, HTTP CONNECT tunnels
>> are sent to the SslBump code.
>>
>> * For https_port configured with an ssl-bump flag, all traffic is sent
>> to the SslBump code (by faking a corresponding HTTP CONNECT request).


> These `fake' CONNECT requests I assume only contain the IP address of the 
> upstream server, not the hostname, as intercepted SSL connections are TCP 
> OPENs.

Modern Squid replaces TCP-derived destination IP address with TLS
SNI-derived domain name when generating the second fake CONNECT request.
The second CONNECT is generated during SslBump step2, after parsing TLS
client handshake.


> Am I right then in saying that using ssl::server_name is useless for bumped 
> intercepted connections?

It may be useful for ACLs checked during SslBump step2 (because it will
check the TLS client SNI-derived domain name) and during step3 (when it
will check TLS server certificate-derived CN and SubjectAltName).


HTH,

Alex.


