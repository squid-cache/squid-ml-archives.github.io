From ngtech1ltd at gmail.com  Sat Aug  1 17:55:56 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Sat, 1 Aug 2020 20:55:56 +0300
Subject: [squid-users] youtube-dl
In-Reply-To: <trinity-42a09e87-601c-4c75-b73e-1e3e37661271-1596237449359@3c-app-mailcom-lxa07>
References: <trinity-42a09e87-601c-4c75-b73e-1e3e37661271-1596237449359@3c-app-mailcom-lxa07>
Message-ID: <001f01d6682d$02cfacf0$086f06d0$@gmail.com>

Technically speaking it might be possible.

 

However if you need a specific function you will need to be more specific about what you want to achive.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of skel eton
Sent: Saturday, August 1, 2020 2:17 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] youtube-dl

 

Hey, sorry if this been asked before, the search function is gone of course. 

I was wondering if squid would fit my needs, I wanted to do something in particular with streaming sites, I will explain.. 

 

So basically, I wanted to try setthing this up where when I play videos on video streaming sites, the url is parsed from squid and passed to a utility such as youtube-dl

 

Could this work?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200801/a77384ea/attachment.htm>

From ivan.bulatovic at gmail.com  Mon Aug  3 13:11:31 2020
From: ivan.bulatovic at gmail.com (Ivan Bulatovic)
Date: Mon, 3 Aug 2020 15:11:31 +0200
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
Message-ID: <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>

Hi all,

I tried with stock squid from Ubuntu 18.04 (version 4.10) and basic
config, but still no luck. Here is the config I tried:
-----------------------------------------
acl localnet src 10.20.0.0/16           # My network
acl localnet src fc00::/7               # RFC 4193 local private network range
acl localnet src fe80::/10              # RFC 4291 link-local
(directly plugged) machines

acl SSL_ports port 443

acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

cache deny all

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access allow localnet manager
http_access deny manager
http_access deny to_localhost
http_access allow localnet
http_access allow localhost
http_access deny all

http_port localhost:3128
http_port 10.20.6.8:50505

coredump_dir /var/spool/squid

shutdown_lifetime 2 seconds
max_filedescriptors 262143

server_persistent_connections off
client_persistent_connections off

request_header_access Via deny all
request_header_access X-Forwarded-For deny all
dns_v4_first on
------------------------------------

Looks like squid has some serious memory issues when under heavy load
(90 servers that crawl Internet sites). It just eats up memory, and
does not free it up even days after it is being used (with no load on
the proxy for days). So I guess I have to look for another solution.

Best regards,
Ivan Bulatovic


On Mon, Jul 20, 2020 at 10:46 PM Ivan Bulatovic
<ivan.bulatovic at gmail.com> wrote:
>
> Hi all,
>
> I am trying to configure squid to run as a forward proxy with no
> caching (cache deny all) with an option to choose the outgoing IP
> address based on the username. So all squid has to do is to use a
> certain outgoing IP address for a certain user, return the data from
> the server to that user and cache nothing.
>
> For that I created a special authentication helper and used the ACLs
> and tcp_outgoing_address to create a lot of users and outgoing IP
> addresses (about 260 at the moment). Example (not the real IP I use,
> of course):
>
> acl use_IP1 proxy_auth user1
> tcp_outgoing_address 1.2.3.4   use_IP1
>
> I also configured the squid to use 4 workers, but this happens even
> when I use only one worker (default)
>
> And this works. However, under heavy load, Squid eats all of the RAM
> and then starts going to swap. And the memory usage does not drop when
> I remove all the load from squid (I shut down all clients).
>
> I left it to see if the memory will be freed but even after leaving it
> for an hour the info page reports this:
> Cache information for squid:
>         Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
>         Hits as % of bytes sent:        5min: 0.0%, 60min: 1.1%
>         Memory hits as % of hit requests:       5min: 0.0%, 60min: 0.0%
>         Disk hits as % of hit requests: 5min: 0.0%, 60min: 100.0%
>         Storage Swap size:      0 KB
>         Storage Swap capacity:   0.0% used, 100.0% free
>         Storage Mem size:       0 KB
>         Storage Mem capacity:    0.0% used, 100.0% free
>         Mean Object Size:       0.00 KB
>         Requests given to unlinkd:      0
>
> Resource usage for squid:
>         UP Time:        255334.875 seconds
>         CPU Time:       7122.436 seconds
>         CPU Usage:      2.79%
>         CPU Usage, 5 minute avg:        0.05%
>         CPU Usage, 60 minute avg:       37.66%
>         Maximum Resident Size: 41500720 KB
>         Page faults with physical i/o: 1003410
>
> And here is the listing of free and top commands (with no load on the server):
>
> # free -h
>               total        used        free      shared  buff/cache   available
> Mem:            11G         10G        791M        676K        491M        1.0G
> Swap:           11G        5.5G        6.5G
>
> # top
> top - 14:12:32 up 3 days,  1:30,  1 user,  load average: 0.00, 0.00, 0.00
> Tasks: 177 total,   1 running, 102 sleeping,   0 stopped,   0 zombie
> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu2  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> %Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> KiB Mem : 91.2/12251688
> [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
>         ]
> KiB Swap: 45.8/12582904
> [||||||||||||||||||||||||||||||||||||||||||||||
>                               ]
>
>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
>   7851 proxy     20   0 6946872 2.514g   8084 S   0.0 21.5  29:43.74 squid
>   7832 proxy     20   0 6711480 2.464g   8040 S   0.0 21.1  29:58.17 squid
>   7814 proxy     20   0 6834928 2.454g  10024 S   0.0 21.0  29:47.56 squid
>   7843 proxy     20   0 6906252 2.436g   8208 S   0.0 20.8  29:15.60 squid
>   1329 root      20   0 2416672 167272  12680 S   0.0  1.4 136:18.57 metricbeat
>   1321 root      20   0 1831804  48364  11648 S   0.0  0.4  14:32.10 filebeat
>    474 root      19  -1  127796  17576  17144 S   0.0  0.1   0:27.01
> systemd-journal
>   7811 proxy     20   0  549384  14168   8372 S   0.0  0.1   0:20.87 squid
>   1166 root      20   0 1749724  10596   4468 S   0.0  0.1   0:31.83 snapd
>  43940 proxy     20   0   28884   9608   5384 S   0.0  0.1   0:00.14 python3
>  43941 proxy     20   0   28884   9552   5328 S   0.0  0.1   0:00.10 python3
>  43939 proxy     20   0   28884   9524   5308 S   0.0  0.1   0:00.12 python3
>  43938 proxy     20   0   28884   9452   5232 S   0.0  0.1   0:00.16 python3
>  48848 root      20   0  105688   6960   5968 S   0.0  0.1   0:00.02 sshd
>  48974 janitor   20   0  108120   5380   4372 S   0.0  0.0   0:00.00 sshd
>      1 root      20   0   86360   4364   2488 S   0.0  0.0  32:46.22 systemd
> ...
> ... lines ommited
> ...
>
> In the attachment you can find the printout from squidclient mgr:info
> and squidclient mgr:mem. These are both taken at the moment when there
> is no more load on the proxy. I also included my squid.conf file
> (minus the two files where acls are defined and outgoing IP addresses,
> these two contain only acl and tcp_outgoing_address lines as in the
> example above).
>
> Machine info:
> OS: Ubuntu 18.04 with latest updates
> Squid version 4.12 (from diladele repository)
> Hardware: Hyper-V virtual machine with 8 vCPU, 12GB of RAM
>
> I can not understand what is eating all of the memory, if I disabled the cache.
>
> Maybe I configured something wrong but I can not find what.
>
> Thank you for any help you can provide.
>
> Best regards,
> Ivan


From rousskov at measurement-factory.com  Mon Aug  3 20:01:47 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 3 Aug 2020 16:01:47 -0400
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
Message-ID: <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>

On 8/3/20 9:11 AM, Ivan Bulatovic wrote:

> Looks like squid has some serious memory issues when under heavy load
> (90 servers that crawl Internet sites). 

>         Maximum Resident Size: 41500720 KB

If the above (unreliable) report matches your observations using system
tools like "top", then it is indeed likely that your Squid is suffering
from a memory leak -- 41GB is usually too much for most non-caching
Squid instances.

Identifying the leak may take some time, and I am not volunteering to do
the necessary legwork personally, but the Squid Project does fix
virtually all runtime leaks that we know about. If you want to speed up
the process, one of the best things you can do is to run Squid under
valgrind with a good suppression file. This requires building Squid with
a special ./configure option. Several testing iterations may be
necessary. If you are willing to do this, please file a bug report and
somebody will guide you through the steps.


> It just eats up memory, and
> does not free it up even days after it is being used (with no load on
> the proxy for days).

Some memory retention is expected by default. See
http://www.squid-cache.org/Doc/config/memory_pools/

Unfortunately, AFAICT, your mgr:mem output does not show any obvious
leaks -- all numbers are very small. If something is leaking a lot, then
it is probably not pooled by Squid.


HTH,

Alex.


> On Mon, Jul 20, 2020 at 10:46 PM Ivan Bulatovic wrote:
>>
>> Hi all,
>>
>> I am trying to configure squid to run as a forward proxy with no
>> caching (cache deny all) with an option to choose the outgoing IP
>> address based on the username. So all squid has to do is to use a
>> certain outgoing IP address for a certain user, return the data from
>> the server to that user and cache nothing.
>>
>> For that I created a special authentication helper and used the ACLs
>> and tcp_outgoing_address to create a lot of users and outgoing IP
>> addresses (about 260 at the moment). Example (not the real IP I use,
>> of course):
>>
>> acl use_IP1 proxy_auth user1
>> tcp_outgoing_address 1.2.3.4   use_IP1
>>
>> I also configured the squid to use 4 workers, but this happens even
>> when I use only one worker (default)
>>
>> And this works. However, under heavy load, Squid eats all of the RAM
>> and then starts going to swap. And the memory usage does not drop when
>> I remove all the load from squid (I shut down all clients).
>>
>> I left it to see if the memory will be freed but even after leaving it
>> for an hour the info page reports this:
>> Cache information for squid:
>>         Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
>>         Hits as % of bytes sent:        5min: 0.0%, 60min: 1.1%
>>         Memory hits as % of hit requests:       5min: 0.0%, 60min: 0.0%
>>         Disk hits as % of hit requests: 5min: 0.0%, 60min: 100.0%
>>         Storage Swap size:      0 KB
>>         Storage Swap capacity:   0.0% used, 100.0% free
>>         Storage Mem size:       0 KB
>>         Storage Mem capacity:    0.0% used, 100.0% free
>>         Mean Object Size:       0.00 KB
>>         Requests given to unlinkd:      0
>>
>> Resource usage for squid:
>>         UP Time:        255334.875 seconds
>>         CPU Time:       7122.436 seconds
>>         CPU Usage:      2.79%
>>         CPU Usage, 5 minute avg:        0.05%
>>         CPU Usage, 60 minute avg:       37.66%
>>         Maximum Resident Size: 41500720 KB
>>         Page faults with physical i/o: 1003410
>>
>> And here is the listing of free and top commands (with no load on the server):
>>
>> # free -h
>>               total        used        free      shared  buff/cache   available
>> Mem:            11G         10G        791M        676K        491M        1.0G
>> Swap:           11G        5.5G        6.5G
>>
>> # top
>> top - 14:12:32 up 3 days,  1:30,  1 user,  load average: 0.00, 0.00, 0.00
>> Tasks: 177 total,   1 running, 102 sleeping,   0 stopped,   0 zombie
>> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu2  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> %Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> KiB Mem : 91.2/12251688
>> [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
>>         ]
>> KiB Swap: 45.8/12582904
>> [||||||||||||||||||||||||||||||||||||||||||||||
>>                               ]
>>
>>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
>>   7851 proxy     20   0 6946872 2.514g   8084 S   0.0 21.5  29:43.74 squid
>>   7832 proxy     20   0 6711480 2.464g   8040 S   0.0 21.1  29:58.17 squid
>>   7814 proxy     20   0 6834928 2.454g  10024 S   0.0 21.0  29:47.56 squid
>>   7843 proxy     20   0 6906252 2.436g   8208 S   0.0 20.8  29:15.60 squid
>>   1329 root      20   0 2416672 167272  12680 S   0.0  1.4 136:18.57 metricbeat
>>   1321 root      20   0 1831804  48364  11648 S   0.0  0.4  14:32.10 filebeat
>>    474 root      19  -1  127796  17576  17144 S   0.0  0.1   0:27.01
>> systemd-journal
>>   7811 proxy     20   0  549384  14168   8372 S   0.0  0.1   0:20.87 squid
>>   1166 root      20   0 1749724  10596   4468 S   0.0  0.1   0:31.83 snapd
>>  43940 proxy     20   0   28884   9608   5384 S   0.0  0.1   0:00.14 python3
>>  43941 proxy     20   0   28884   9552   5328 S   0.0  0.1   0:00.10 python3
>>  43939 proxy     20   0   28884   9524   5308 S   0.0  0.1   0:00.12 python3
>>  43938 proxy     20   0   28884   9452   5232 S   0.0  0.1   0:00.16 python3
>>  48848 root      20   0  105688   6960   5968 S   0.0  0.1   0:00.02 sshd
>>  48974 janitor   20   0  108120   5380   4372 S   0.0  0.0   0:00.00 sshd
>>      1 root      20   0   86360   4364   2488 S   0.0  0.0  32:46.22 systemd
>> ...
>> ... lines ommited
>> ...
>>
>> In the attachment you can find the printout from squidclient mgr:info
>> and squidclient mgr:mem. These are both taken at the moment when there
>> is no more load on the proxy. I also included my squid.conf file
>> (minus the two files where acls are defined and outgoing IP addresses,
>> these two contain only acl and tcp_outgoing_address lines as in the
>> example above).
>>
>> Machine info:
>> OS: Ubuntu 18.04 with latest updates
>> Squid version 4.12 (from diladele repository)
>> Hardware: Hyper-V virtual machine with 8 vCPU, 12GB of RAM
>>
>> I can not understand what is eating all of the memory, if I disabled the cache.
>>
>> Maybe I configured something wrong but I can not find what.
>>
>> Thank you for any help you can provide.
>>
>> Best regards,
>> Ivan
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From tamurin0525 at gmail.com  Tue Aug  4 23:28:02 2020
From: tamurin0525 at gmail.com (m k)
Date: Wed, 5 Aug 2020 08:28:02 +0900
Subject: [squid-users] I would like to know performance sizing aspects.
Message-ID: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>

We are considering to use Squid for our proxy, and would like to know
performance sizing aspects.


Current web access request averages per 1 hour are as followings
Clients?30,000?
Page Views:141,741/hour
*Requests:4,893,106

We will install Squid on CentOS 8.1.   Please kindly share your thoughts /
advices
Is there sizing methodology and tools?
How much resources are generally recommended for our environment?
CPU: Memory: Disk space : Other factors to be considered if any:
Do you have a generally recommended performance testing tools? Any
suggested guidelines?

Best regards,
kitamura
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200805/bca38226/attachment.htm>

From squid3 at treenet.co.nz  Wed Aug  5 01:27:58 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Aug 2020 13:27:58 +1200
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
Message-ID: <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>

On 5/08/20 11:28 am, m k wrote:
>> We are considering to use Squid for our proxy, and?would like to know
>> performance sizing aspects.
>>
>> Current web access request averages per 1 hour are as followings?
>> Clients?30,000?
>> Page Views:141,741/hour
>> *Requests:4,893,106
>>

Okay. Requests and client count are the important numbers there.

The ~1359 req/sec is well within a default Squid capabilities, which can
extend up to around 10k req/sec before needing careful tuning.

That number was gained before HTTPS became so popular. So YMMV depending
on how many CONNECT tunnels you have to deal with. That HTTPS traffic
can possibly be decrypted and cached but performance trade-offs are
quite large.


>> We will install Squid on CentOS 8.1.? ?Please kindly share your
>> thoughts / advices

Whatever OS you are most comfortable with administering. Be aware that
CentOS official Squid packages are very slow to update - Apparently they
still have only v4.4 (8 months old) despite a 8.2 point release only a
few weeks ago.

So you may need to be building your own from sources and/or using other
semi-official packagers such as the ones from Eliezer at NGTech when he
gets around to CentOS 8 packages.
  <https://wiki.squid-cache.org/KnowledgeBase/CentOS>


FYI; If you find yourself having to use SSL-Bump, then we highly
recommended to follow the latest Squid releases with fairly frequent
updates (at minimum a few times per year - worst case monthly). If you
like CentOS you may find Fedora more suitable to track the security
environment volatility and update churn.


>> Is there sizing methodology and tools?

There are a couple of methodologies, depending on what aspect you are
tuning towards - and one for identifying the limitation points to begin
a tuning process tuning.

The info you gave above is the beginning. Checking to see if your
traffic rate is reasonably within capability of a single Squid instance.

Yours is reasonable, so next step is to get Squid running and see where
the trouble points (if any) are.

 For more see <https://wiki.squid-cache.org/SquidFaq/>



>> How much resources are generally recommended for our environment?
>> ?CPU:? Memory:? Disk space : Other factors to be considered if any:
>> Do you have a generally recommended performance testing tools? Any
>> suggested guidelines?
>>


 CPU - squid is still mostly single-process. So prioritize faster GHz
rates over core number. Multi-core can help of course, but not as much
as cycle speeds do. Hyper-threading is useless for Squid.

 Memory - Squid will use as much as you can give it. Let your budget
govern this.

 Disk - Squid will happily run with no disk - or lots of large ones.

   - Avoid RAID. Squid *will* shorten disk lifetimes with its unusually
high write I/O pattern. How much shorter varies by disk type (HDD vs
SSD). So you may find it better to plan budget towards maintenance costs
of replacing disks in future rather than buying multiple up-front for
RAID use.
 see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.

    - Up to a few hundred GB per cache_dir can be good for large caches.
Going up to TB is not (yet) worth the disk cost as Squid has a per-cache
limit on stored objects.

   - Disk caches can be re-tuned, added, moved, removed, and/or extended
at any time and will depend on the profile of object sizes your proxy
handles - which itself likely changes over time. So general let your
budget decide the initial disks and work from there.



Load Testing - the tools us dev use to review performance are listed at
the bottom of the profiling FAQ page. These are best for testing the
theoretical limits of a particular installation - real traffic tends to
be somewhat lower. So I personally prefer taking stats from the running
proxy on real traffic and seeing what I can observe from those.


HTH
Amos


From ngtech1ltd at gmail.com  Wed Aug  5 07:38:12 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Wed, 5 Aug 2020 10:38:12 +0300
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
Message-ID: <007801d66afb$600145c0$2003d140$@gmail.com>

Hey Amos,

I got to CentOS 8...
RedHat claimed they will keep the module up-to-date and I would be able to stop building them.
>From what you describe I understand their speed is the same as it was before.

I can build the RPMs but cannot host them 24/7.
For now if and when 8.2 RPMs will be built the squid version of CentOS should be blocked (excluded) from CentOS AppStream repo.

Do we have a tested that can test 8.2 with my RPMs?
( I will try to share 4.12 RPM's via OneDrive )

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, August 5, 2020 4:28 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] I would like to know performance sizing aspects.

On 5/08/20 11:28 am, m k wrote:
>> We are considering to use Squid for our proxy, and would like to know 
>> performance sizing aspects.
>>
>> Current web access request averages per 1 hour are as followings
>> Clients?30,000?
>> Page Views:141,741/hour
>> *Requests:4,893,106
>>

Okay. Requests and client count are the important numbers there.

The ~1359 req/sec is well within a default Squid capabilities, which can
extend up to around 10k req/sec before needing careful tuning.

That number was gained before HTTPS became so popular. So YMMV depending
on how many CONNECT tunnels you have to deal with. That HTTPS traffic
can possibly be decrypted and cached but performance trade-offs are
quite large.


>> We will install Squid on CentOS 8.1.   Please kindly share your
>> thoughts / advices

Whatever OS you are most comfortable with administering. Be aware that
CentOS official Squid packages are very slow to update - Apparently they
still have only v4.4 (8 months old) despite a 8.2 point release only a
few weeks ago.

So you may need to be building your own from sources and/or using other
semi-official packagers such as the ones from Eliezer at NGTech when he
gets around to CentOS 8 packages.
  <https://wiki.squid-cache.org/KnowledgeBase/CentOS>


FYI; If you find yourself having to use SSL-Bump, then we highly
recommended to follow the latest Squid releases with fairly frequent
updates (at minimum a few times per year - worst case monthly). If you
like CentOS you may find Fedora more suitable to track the security
environment volatility and update churn.


>> Is there sizing methodology and tools?

There are a couple of methodologies, depending on what aspect you are
tuning towards - and one for identifying the limitation points to begin
a tuning process tuning.

The info you gave above is the beginning. Checking to see if your
traffic rate is reasonably within capability of a single Squid instance.

Yours is reasonable, so next step is to get Squid running and see where
the trouble points (if any) are.

 For more see <https://wiki.squid-cache.org/SquidFaq/>



>> How much resources are generally recommended for our environment?
>> ?CPU:? Memory:? Disk space : Other factors to be considered if any:
>> Do you have a generally recommended performance testing tools? Any
>> suggested guidelines?
>>


 CPU - squid is still mostly single-process. So prioritize faster GHz
rates over core number. Multi-core can help of course, but not as much
as cycle speeds do. Hyper-threading is useless for Squid.

 Memory - Squid will use as much as you can give it. Let your budget
govern this.

 Disk - Squid will happily run with no disk - or lots of large ones.

   - Avoid RAID. Squid *will* shorten disk lifetimes with its unusually
high write I/O pattern. How much shorter varies by disk type (HDD vs
SSD). So you may find it better to plan budget towards maintenance costs
of replacing disks in future rather than buying multiple up-front for
RAID use.
 see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.

    - Up to a few hundred GB per cache_dir can be good for large caches.
Going up to TB is not (yet) worth the disk cost as Squid has a per-cache
limit on stored objects.

   - Disk caches can be re-tuned, added, moved, removed, and/or extended
at any time and will depend on the profile of object sizes your proxy
handles - which itself likely changes over time. So general let your
budget decide the initial disks and work from there.



Load Testing - the tools us dev use to review performance are listed at
the bottom of the profiling FAQ page. These are best for testing the
theoretical limits of a particular installation - real traffic tends to
be somewhat lower. So I personally prefer taking stats from the running
proxy on real traffic and seeing what I can observe from those.


HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Wed Aug  5 07:55:54 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Wed, 5 Aug 2020 10:55:54 +0300
Subject: [squid-users] ERROR: helper
In-Reply-To: <20200706100205.GA6185@fantomas.sk>
References: <c300e6a5-563b-e38a-3620-81c9d1a920e0@unibs.it>
 <20200706100205.GA6185@fantomas.sk>
Message-ID: <007e01d66afd$d926d800$8b748800$@gmail.com>

If you need to build a squid binary you can try to use these docker containers:
* https://github.com/elico/squid-docker-build-nodes

Hope It Helps

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Matus UHLAR - fantomas
Sent: Monday, July 6, 2020 1:02 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] ERROR: helper

On 06.07.20 11:50, Antonino Gianfranco Sanacori wrote:
>In my cache.log i try very very much messages of this:
>
>kid1| ERROR: helper: {result=*BH*, notes={message: Success; }},
>attempt #1 of 2
>kid1| ERROR: helper: {result=*BH*, notes={message: Success; message: 
>Success; }}, attempt #2 of 2
>
>I  runned the comand "squid -k parse" and i got  the following messages:
>
>Processing: ssl_bump splice whitelist
>ERROR: 'ssl_bump' requires --with-openssl
>
>Is it possible that the helper error messages derive from incorrect 
>configuration of  my directive ssl_bump  configuration? I use "ssl_bump 
>splice whitelist" to tunnel ssl traffic on some external domains.

your squid instance was not compiled with SSL support.  If you are running a distro like debian or ubuntu, you will need to compile it yourself.


--
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
(R)etry, (A)bort, (C)ancer
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ivan.bulatovic at gmail.com  Wed Aug  5 16:59:17 2020
From: ivan.bulatovic at gmail.com (Ivan Bulatovic)
Date: Wed, 5 Aug 2020 18:59:17 +0200
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
Message-ID: <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>

Hi Alex,

Thank you very much for your help.

I opened a bug on bugs.squid-cache.org
(https://bugs.squid-cache.org/show_bug.cgi?id=5071).

Best regards,
Ivan

On Mon, Aug 3, 2020 at 10:02 PM Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>
> On 8/3/20 9:11 AM, Ivan Bulatovic wrote:
>
> > Looks like squid has some serious memory issues when under heavy load
> > (90 servers that crawl Internet sites).
>
> >         Maximum Resident Size: 41500720 KB
>
> If the above (unreliable) report matches your observations using system
> tools like "top", then it is indeed likely that your Squid is suffering
> from a memory leak -- 41GB is usually too much for most non-caching
> Squid instances.
>
> Identifying the leak may take some time, and I am not volunteering to do
> the necessary legwork personally, but the Squid Project does fix
> virtually all runtime leaks that we know about. If you want to speed up
> the process, one of the best things you can do is to run Squid under
> valgrind with a good suppression file. This requires building Squid with
> a special ./configure option. Several testing iterations may be
> necessary. If you are willing to do this, please file a bug report and
> somebody will guide you through the steps.
>
>
> > It just eats up memory, and
> > does not free it up even days after it is being used (with no load on
> > the proxy for days).
>
> Some memory retention is expected by default. See
> http://www.squid-cache.org/Doc/config/memory_pools/
>
> Unfortunately, AFAICT, your mgr:mem output does not show any obvious
> leaks -- all numbers are very small. If something is leaking a lot, then
> it is probably not pooled by Squid.
>
>
> HTH,
>
> Alex.
>
>
> > On Mon, Jul 20, 2020 at 10:46 PM Ivan Bulatovic wrote:
> >>
> >> Hi all,
> >>
> >> I am trying to configure squid to run as a forward proxy with no
> >> caching (cache deny all) with an option to choose the outgoing IP
> >> address based on the username. So all squid has to do is to use a
> >> certain outgoing IP address for a certain user, return the data from
> >> the server to that user and cache nothing.
> >>
> >> For that I created a special authentication helper and used the ACLs
> >> and tcp_outgoing_address to create a lot of users and outgoing IP
> >> addresses (about 260 at the moment). Example (not the real IP I use,
> >> of course):
> >>
> >> acl use_IP1 proxy_auth user1
> >> tcp_outgoing_address 1.2.3.4   use_IP1
> >>
> >> I also configured the squid to use 4 workers, but this happens even
> >> when I use only one worker (default)
> >>
> >> And this works. However, under heavy load, Squid eats all of the RAM
> >> and then starts going to swap. And the memory usage does not drop when
> >> I remove all the load from squid (I shut down all clients).
> >>
> >> I left it to see if the memory will be freed but even after leaving it
> >> for an hour the info page reports this:
> >> Cache information for squid:
> >>         Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
> >>         Hits as % of bytes sent:        5min: 0.0%, 60min: 1.1%
> >>         Memory hits as % of hit requests:       5min: 0.0%, 60min: 0.0%
> >>         Disk hits as % of hit requests: 5min: 0.0%, 60min: 100.0%
> >>         Storage Swap size:      0 KB
> >>         Storage Swap capacity:   0.0% used, 100.0% free
> >>         Storage Mem size:       0 KB
> >>         Storage Mem capacity:    0.0% used, 100.0% free
> >>         Mean Object Size:       0.00 KB
> >>         Requests given to unlinkd:      0
> >>
> >> Resource usage for squid:
> >>         UP Time:        255334.875 seconds
> >>         CPU Time:       7122.436 seconds
> >>         CPU Usage:      2.79%
> >>         CPU Usage, 5 minute avg:        0.05%
> >>         CPU Usage, 60 minute avg:       37.66%
> >>         Maximum Resident Size: 41500720 KB
> >>         Page faults with physical i/o: 1003410
> >>
> >> And here is the listing of free and top commands (with no load on the server):
> >>
> >> # free -h
> >>               total        used        free      shared  buff/cache   available
> >> Mem:            11G         10G        791M        676K        491M        1.0G
> >> Swap:           11G        5.5G        6.5G
> >>
> >> # top
> >> top - 14:12:32 up 3 days,  1:30,  1 user,  load average: 0.00, 0.00, 0.00
> >> Tasks: 177 total,   1 running, 102 sleeping,   0 stopped,   0 zombie
> >> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu2  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> %Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
> >> KiB Mem : 91.2/12251688
> >> [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
> >>         ]
> >> KiB Swap: 45.8/12582904
> >> [||||||||||||||||||||||||||||||||||||||||||||||
> >>                               ]
> >>
> >>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
> >>   7851 proxy     20   0 6946872 2.514g   8084 S   0.0 21.5  29:43.74 squid
> >>   7832 proxy     20   0 6711480 2.464g   8040 S   0.0 21.1  29:58.17 squid
> >>   7814 proxy     20   0 6834928 2.454g  10024 S   0.0 21.0  29:47.56 squid
> >>   7843 proxy     20   0 6906252 2.436g   8208 S   0.0 20.8  29:15.60 squid
> >>   1329 root      20   0 2416672 167272  12680 S   0.0  1.4 136:18.57 metricbeat
> >>   1321 root      20   0 1831804  48364  11648 S   0.0  0.4  14:32.10 filebeat
> >>    474 root      19  -1  127796  17576  17144 S   0.0  0.1   0:27.01
> >> systemd-journal
> >>   7811 proxy     20   0  549384  14168   8372 S   0.0  0.1   0:20.87 squid
> >>   1166 root      20   0 1749724  10596   4468 S   0.0  0.1   0:31.83 snapd
> >>  43940 proxy     20   0   28884   9608   5384 S   0.0  0.1   0:00.14 python3
> >>  43941 proxy     20   0   28884   9552   5328 S   0.0  0.1   0:00.10 python3
> >>  43939 proxy     20   0   28884   9524   5308 S   0.0  0.1   0:00.12 python3
> >>  43938 proxy     20   0   28884   9452   5232 S   0.0  0.1   0:00.16 python3
> >>  48848 root      20   0  105688   6960   5968 S   0.0  0.1   0:00.02 sshd
> >>  48974 janitor   20   0  108120   5380   4372 S   0.0  0.0   0:00.00 sshd
> >>      1 root      20   0   86360   4364   2488 S   0.0  0.0  32:46.22 systemd
> >> ...
> >> ... lines ommited
> >> ...
> >>
> >> In the attachment you can find the printout from squidclient mgr:info
> >> and squidclient mgr:mem. These are both taken at the moment when there
> >> is no more load on the proxy. I also included my squid.conf file
> >> (minus the two files where acls are defined and outgoing IP addresses,
> >> these two contain only acl and tcp_outgoing_address lines as in the
> >> example above).
> >>
> >> Machine info:
> >> OS: Ubuntu 18.04 with latest updates
> >> Squid version 4.12 (from diladele repository)
> >> Hardware: Hyper-V virtual machine with 8 vCPU, 12GB of RAM
> >>
> >> I can not understand what is eating all of the memory, if I disabled the cache.
> >>
> >> Maybe I configured something wrong but I can not find what.
> >>
> >> Thank you for any help you can provide.
> >>
> >> Best regards,
> >> Ivan
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>


From ngtech1ltd at gmail.com  Wed Aug  5 17:14:23 2020
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Wed, 5 Aug 2020 20:14:23 +0300
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
 <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
Message-ID: <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>

I think that the mgr:info or another page there contains the amount of
requests per second etc.
also netstat or ss -ntp might give some basic understanding about this
server size.

are you using dynamic memory on the hyper-v hypervisor?

Eliezer

On Wed, Aug 5, 2020, 19:59 Ivan Bulatovic <ivan.bulatovic at gmail.com> wrote:

> Hi Alex,
>
> Thank you very much for your help.
>
> I opened a bug on bugs.squid-cache.org
> (https://bugs.squid-cache.org/show_bug.cgi?id=5071).
>
> Best regards,
> Ivan
>
> On Mon, Aug 3, 2020 at 10:02 PM Alex Rousskov
> <rousskov at measurement-factory.com> wrote:
> >
> > On 8/3/20 9:11 AM, Ivan Bulatovic wrote:
> >
> > > Looks like squid has some serious memory issues when under heavy load
> > > (90 servers that crawl Internet sites).
> >
> > >         Maximum Resident Size: 41500720 KB
> >
> > If the above (unreliable) report matches your observations using system
> > tools like "top", then it is indeed likely that your Squid is suffering
> > from a memory leak -- 41GB is usually too much for most non-caching
> > Squid instances.
> >
> > Identifying the leak may take some time, and I am not volunteering to do
> > the necessary legwork personally, but the Squid Project does fix
> > virtually all runtime leaks that we know about. If you want to speed up
> > the process, one of the best things you can do is to run Squid under
> > valgrind with a good suppression file. This requires building Squid with
> > a special ./configure option. Several testing iterations may be
> > necessary. If you are willing to do this, please file a bug report and
> > somebody will guide you through the steps.
> >
> >
> > > It just eats up memory, and
> > > does not free it up even days after it is being used (with no load on
> > > the proxy for days).
> >
> > Some memory retention is expected by default. See
> > http://www.squid-cache.org/Doc/config/memory_pools/
> >
> > Unfortunately, AFAICT, your mgr:mem output does not show any obvious
> > leaks -- all numbers are very small. If something is leaking a lot, then
> > it is probably not pooled by Squid.
> >
> >
> > HTH,
> >
> > Alex.
> >
> >
> > > On Mon, Jul 20, 2020 at 10:46 PM Ivan Bulatovic wrote:
> > >>
> > >> Hi all,
> > >>
> > >> I am trying to configure squid to run as a forward proxy with no
> > >> caching (cache deny all) with an option to choose the outgoing IP
> > >> address based on the username. So all squid has to do is to use a
> > >> certain outgoing IP address for a certain user, return the data from
> > >> the server to that user and cache nothing.
> > >>
> > >> For that I created a special authentication helper and used the ACLs
> > >> and tcp_outgoing_address to create a lot of users and outgoing IP
> > >> addresses (about 260 at the moment). Example (not the real IP I use,
> > >> of course):
> > >>
> > >> acl use_IP1 proxy_auth user1
> > >> tcp_outgoing_address 1.2.3.4   use_IP1
> > >>
> > >> I also configured the squid to use 4 workers, but this happens even
> > >> when I use only one worker (default)
> > >>
> > >> And this works. However, under heavy load, Squid eats all of the RAM
> > >> and then starts going to swap. And the memory usage does not drop when
> > >> I remove all the load from squid (I shut down all clients).
> > >>
> > >> I left it to see if the memory will be freed but even after leaving it
> > >> for an hour the info page reports this:
> > >> Cache information for squid:
> > >>         Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
> > >>         Hits as % of bytes sent:        5min: 0.0%, 60min: 1.1%
> > >>         Memory hits as % of hit requests:       5min: 0.0%, 60min:
> 0.0%
> > >>         Disk hits as % of hit requests: 5min: 0.0%, 60min: 100.0%
> > >>         Storage Swap size:      0 KB
> > >>         Storage Swap capacity:   0.0% used, 100.0% free
> > >>         Storage Mem size:       0 KB
> > >>         Storage Mem capacity:    0.0% used, 100.0% free
> > >>         Mean Object Size:       0.00 KB
> > >>         Requests given to unlinkd:      0
> > >>
> > >> Resource usage for squid:
> > >>         UP Time:        255334.875 seconds
> > >>         CPU Time:       7122.436 seconds
> > >>         CPU Usage:      2.79%
> > >>         CPU Usage, 5 minute avg:        0.05%
> > >>         CPU Usage, 60 minute avg:       37.66%
> > >>         Maximum Resident Size: 41500720 KB
> > >>         Page faults with physical i/o: 1003410
> > >>
> > >> And here is the listing of free and top commands (with no load on the
> server):
> > >>
> > >> # free -h
> > >>               total        used        free      shared  buff/cache
>  available
> > >> Mem:            11G         10G        791M        676K        491M
>       1.0G
> > >> Swap:           11G        5.5G        6.5G
> > >>
> > >> # top
> > >> top - 14:12:32 up 3 days,  1:30,  1 user,  load average: 0.00, 0.00,
> 0.00
> > >> Tasks: 177 total,   1 running, 102 sleeping,   0 stopped,   0 zombie
> > >> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu2  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> %Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0
> si,  0.0 st
> > >> KiB Mem : 91.2/12251688
> > >>
> [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
> > >>         ]
> > >> KiB Swap: 45.8/12582904
> > >> [||||||||||||||||||||||||||||||||||||||||||||||
> > >>                               ]
> > >>
> > >>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+
> COMMAND
> > >>   7851 proxy     20   0 6946872 2.514g   8084 S   0.0 21.5  29:43.74
> squid
> > >>   7832 proxy     20   0 6711480 2.464g   8040 S   0.0 21.1  29:58.17
> squid
> > >>   7814 proxy     20   0 6834928 2.454g  10024 S   0.0 21.0  29:47.56
> squid
> > >>   7843 proxy     20   0 6906252 2.436g   8208 S   0.0 20.8  29:15.60
> squid
> > >>   1329 root      20   0 2416672 167272  12680 S   0.0  1.4 136:18.57
> metricbeat
> > >>   1321 root      20   0 1831804  48364  11648 S   0.0  0.4  14:32.10
> filebeat
> > >>    474 root      19  -1  127796  17576  17144 S   0.0  0.1   0:27.01
> > >> systemd-journal
> > >>   7811 proxy     20   0  549384  14168   8372 S   0.0  0.1   0:20.87
> squid
> > >>   1166 root      20   0 1749724  10596   4468 S   0.0  0.1   0:31.83
> snapd
> > >>  43940 proxy     20   0   28884   9608   5384 S   0.0  0.1   0:00.14
> python3
> > >>  43941 proxy     20   0   28884   9552   5328 S   0.0  0.1   0:00.10
> python3
> > >>  43939 proxy     20   0   28884   9524   5308 S   0.0  0.1   0:00.12
> python3
> > >>  43938 proxy     20   0   28884   9452   5232 S   0.0  0.1   0:00.16
> python3
> > >>  48848 root      20   0  105688   6960   5968 S   0.0  0.1   0:00.02
> sshd
> > >>  48974 janitor   20   0  108120   5380   4372 S   0.0  0.0   0:00.00
> sshd
> > >>      1 root      20   0   86360   4364   2488 S   0.0  0.0  32:46.22
> systemd
> > >> ...
> > >> ... lines ommited
> > >> ...
> > >>
> > >> In the attachment you can find the printout from squidclient mgr:info
> > >> and squidclient mgr:mem. These are both taken at the moment when there
> > >> is no more load on the proxy. I also included my squid.conf file
> > >> (minus the two files where acls are defined and outgoing IP addresses,
> > >> these two contain only acl and tcp_outgoing_address lines as in the
> > >> example above).
> > >>
> > >> Machine info:
> > >> OS: Ubuntu 18.04 with latest updates
> > >> Squid version 4.12 (from diladele repository)
> > >> Hardware: Hyper-V virtual machine with 8 vCPU, 12GB of RAM
> > >>
> > >> I can not understand what is eating all of the memory, if I disabled
> the cache.
> > >>
> > >> Maybe I configured something wrong but I can not find what.
> > >>
> > >> Thank you for any help you can provide.
> > >>
> > >> Best regards,
> > >> Ivan
> > > _______________________________________________
> > > squid-users mailing list
> > > squid-users at lists.squid-cache.org
> > > http://lists.squid-cache.org/listinfo/squid-users
> > >
> >
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200805/32c1e247/attachment.htm>

From ivan.bulatovic at gmail.com  Wed Aug  5 17:34:03 2020
From: ivan.bulatovic at gmail.com (Ivan Bulatovic)
Date: Wed, 5 Aug 2020 19:34:03 +0200
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
 <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
 <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>
Message-ID: <CAFJ4_4spgZ7o+1hm47uNnLmuOC0wEsmHGwyk0VtJm8xGqU7XXw@mail.gmail.com>

Hi Eliezer,

In the original message I sent to the squid-users mail list, I
attached listings from mgr:info and mgr:mem. The server is definitely
using a lot of connections (close to 200K connections), which is why I
increased the open files limits in linux as well as in squid.conf. And
there are a lot of requests per second, when the server is running. I
could understand that it needs a lot of memory for in-transit cache,
but that memory should be later released  back to OS, once the
requests load goes down. However, that is not the case, even days
after there is no load on the server, it stays at 11GB of RAM and 5.5
GB of swap used. The second I restart the squid process, everything
goes to normal, memory is released. That is why I suspect there is
some memory leak somewhere.

The server is a Ubuntu 18.04 LTS VM (running on Hyper-V 2019 server),
with 8 virtual processors and 12GB of RAM (although I can increase
that if that is the problem, but I thought that without caching this
would be more than enough).

I am not using dynamic memory on Hyper-V (it is turned off for this VM).

Best regards,
Ivan

On Wed, Aug 5, 2020 at 7:14 PM NgTech LTD <ngtech1ltd at gmail.com> wrote:
>
> I think that the mgr:info or another page there contains the amount of requests per second etc.
> also netstat or ss -ntp might give some basic understanding about this server size.
>
> are you using dynamic memory on the hyper-v hypervisor?
>
> Eliezer
>
> On Wed, Aug 5, 2020, 19:59 Ivan Bulatovic <ivan.bulatovic at gmail.com> wrote:
>>
>> Hi Alex,
>>
>> Thank you very much for your help.
>>
>> I opened a bug on bugs.squid-cache.org
>> (https://bugs.squid-cache.org/show_bug.cgi?id=5071).
>>
>> Best regards,
>> Ivan
>>
>> On Mon, Aug 3, 2020 at 10:02 PM Alex Rousskov
>> <rousskov at measurement-factory.com> wrote:
>> >
>> > On 8/3/20 9:11 AM, Ivan Bulatovic wrote:
>> >
>> > > Looks like squid has some serious memory issues when under heavy load
>> > > (90 servers that crawl Internet sites).
>> >
>> > >         Maximum Resident Size: 41500720 KB
>> >
>> > If the above (unreliable) report matches your observations using system
>> > tools like "top", then it is indeed likely that your Squid is suffering
>> > from a memory leak -- 41GB is usually too much for most non-caching
>> > Squid instances.
>> >
>> > Identifying the leak may take some time, and I am not volunteering to do
>> > the necessary legwork personally, but the Squid Project does fix
>> > virtually all runtime leaks that we know about. If you want to speed up
>> > the process, one of the best things you can do is to run Squid under
>> > valgrind with a good suppression file. This requires building Squid with
>> > a special ./configure option. Several testing iterations may be
>> > necessary. If you are willing to do this, please file a bug report and
>> > somebody will guide you through the steps.
>> >
>> >
>> > > It just eats up memory, and
>> > > does not free it up even days after it is being used (with no load on
>> > > the proxy for days).
>> >
>> > Some memory retention is expected by default. See
>> > http://www.squid-cache.org/Doc/config/memory_pools/
>> >
>> > Unfortunately, AFAICT, your mgr:mem output does not show any obvious
>> > leaks -- all numbers are very small. If something is leaking a lot, then
>> > it is probably not pooled by Squid.
>> >
>> >
>> > HTH,
>> >
>> > Alex.
>> >
>> >
>> > > On Mon, Jul 20, 2020 at 10:46 PM Ivan Bulatovic wrote:
>> > >>
>> > >> Hi all,
>> > >>
>> > >> I am trying to configure squid to run as a forward proxy with no
>> > >> caching (cache deny all) with an option to choose the outgoing IP
>> > >> address based on the username. So all squid has to do is to use a
>> > >> certain outgoing IP address for a certain user, return the data from
>> > >> the server to that user and cache nothing.
>> > >>
>> > >> For that I created a special authentication helper and used the ACLs
>> > >> and tcp_outgoing_address to create a lot of users and outgoing IP
>> > >> addresses (about 260 at the moment). Example (not the real IP I use,
>> > >> of course):
>> > >>
>> > >> acl use_IP1 proxy_auth user1
>> > >> tcp_outgoing_address 1.2.3.4   use_IP1
>> > >>
>> > >> I also configured the squid to use 4 workers, but this happens even
>> > >> when I use only one worker (default)
>> > >>
>> > >> And this works. However, under heavy load, Squid eats all of the RAM
>> > >> and then starts going to swap. And the memory usage does not drop when
>> > >> I remove all the load from squid (I shut down all clients).
>> > >>
>> > >> I left it to see if the memory will be freed but even after leaving it
>> > >> for an hour the info page reports this:
>> > >> Cache information for squid:
>> > >>         Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
>> > >>         Hits as % of bytes sent:        5min: 0.0%, 60min: 1.1%
>> > >>         Memory hits as % of hit requests:       5min: 0.0%, 60min: 0.0%
>> > >>         Disk hits as % of hit requests: 5min: 0.0%, 60min: 100.0%
>> > >>         Storage Swap size:      0 KB
>> > >>         Storage Swap capacity:   0.0% used, 100.0% free
>> > >>         Storage Mem size:       0 KB
>> > >>         Storage Mem capacity:    0.0% used, 100.0% free
>> > >>         Mean Object Size:       0.00 KB
>> > >>         Requests given to unlinkd:      0
>> > >>
>> > >> Resource usage for squid:
>> > >>         UP Time:        255334.875 seconds
>> > >>         CPU Time:       7122.436 seconds
>> > >>         CPU Usage:      2.79%
>> > >>         CPU Usage, 5 minute avg:        0.05%
>> > >>         CPU Usage, 60 minute avg:       37.66%
>> > >>         Maximum Resident Size: 41500720 KB
>> > >>         Page faults with physical i/o: 1003410
>> > >>
>> > >> And here is the listing of free and top commands (with no load on the server):
>> > >>
>> > >> # free -h
>> > >>               total        used        free      shared  buff/cache   available
>> > >> Mem:            11G         10G        791M        676K        491M        1.0G
>> > >> Swap:           11G        5.5G        6.5G
>> > >>
>> > >> # top
>> > >> top - 14:12:32 up 3 days,  1:30,  1 user,  load average: 0.00, 0.00, 0.00
>> > >> Tasks: 177 total,   1 running, 102 sleeping,   0 stopped,   0 zombie
>> > >> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu2  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> %Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
>> > >> KiB Mem : 91.2/12251688
>> > >> [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
>> > >>         ]
>> > >> KiB Swap: 45.8/12582904
>> > >> [||||||||||||||||||||||||||||||||||||||||||||||
>> > >>                               ]
>> > >>
>> > >>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
>> > >>   7851 proxy     20   0 6946872 2.514g   8084 S   0.0 21.5  29:43.74 squid
>> > >>   7832 proxy     20   0 6711480 2.464g   8040 S   0.0 21.1  29:58.17 squid
>> > >>   7814 proxy     20   0 6834928 2.454g  10024 S   0.0 21.0  29:47.56 squid
>> > >>   7843 proxy     20   0 6906252 2.436g   8208 S   0.0 20.8  29:15.60 squid
>> > >>   1329 root      20   0 2416672 167272  12680 S   0.0  1.4 136:18.57 metricbeat
>> > >>   1321 root      20   0 1831804  48364  11648 S   0.0  0.4  14:32.10 filebeat
>> > >>    474 root      19  -1  127796  17576  17144 S   0.0  0.1   0:27.01
>> > >> systemd-journal
>> > >>   7811 proxy     20   0  549384  14168   8372 S   0.0  0.1   0:20.87 squid
>> > >>   1166 root      20   0 1749724  10596   4468 S   0.0  0.1   0:31.83 snapd
>> > >>  43940 proxy     20   0   28884   9608   5384 S   0.0  0.1   0:00.14 python3
>> > >>  43941 proxy     20   0   28884   9552   5328 S   0.0  0.1   0:00.10 python3
>> > >>  43939 proxy     20   0   28884   9524   5308 S   0.0  0.1   0:00.12 python3
>> > >>  43938 proxy     20   0   28884   9452   5232 S   0.0  0.1   0:00.16 python3
>> > >>  48848 root      20   0  105688   6960   5968 S   0.0  0.1   0:00.02 sshd
>> > >>  48974 janitor   20   0  108120   5380   4372 S   0.0  0.0   0:00.00 sshd
>> > >>      1 root      20   0   86360   4364   2488 S   0.0  0.0  32:46.22 systemd
>> > >> ...
>> > >> ... lines ommited
>> > >> ...
>> > >>
>> > >> In the attachment you can find the printout from squidclient mgr:info
>> > >> and squidclient mgr:mem. These are both taken at the moment when there
>> > >> is no more load on the proxy. I also included my squid.conf file
>> > >> (minus the two files where acls are defined and outgoing IP addresses,
>> > >> these two contain only acl and tcp_outgoing_address lines as in the
>> > >> example above).
>> > >>
>> > >> Machine info:
>> > >> OS: Ubuntu 18.04 with latest updates
>> > >> Squid version 4.12 (from diladele repository)
>> > >> Hardware: Hyper-V virtual machine with 8 vCPU, 12GB of RAM
>> > >>
>> > >> I can not understand what is eating all of the memory, if I disabled the cache.
>> > >>
>> > >> Maybe I configured something wrong but I can not find what.
>> > >>
>> > >> Thank you for any help you can provide.
>> > >>
>> > >> Best regards,
>> > >> Ivan
>> > > _______________________________________________
>> > > squid-users mailing list
>> > > squid-users at lists.squid-cache.org
>> > > http://lists.squid-cache.org/listinfo/squid-users
>> > >
>> >
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users


From ngtech1ltd at gmail.com  Wed Aug  5 23:06:32 2020
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Thu, 6 Aug 2020 02:06:32 +0300
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CAFJ4_4spgZ7o+1hm47uNnLmuOC0wEsmHGwyk0VtJm8xGqU7XXw@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
 <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
 <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>
 <CAFJ4_4spgZ7o+1hm47uNnLmuOC0wEsmHGwyk0VtJm8xGqU7XXw@mail.gmail.com>
Message-ID: <CABA8h=RDxzCx6vcTfbRee=7JLE6ozE+2Nb-QYQev6peb3b6h4g@mail.gmail.com>

Hey Ivan,

>From what i remember there is a calculation for how much k per conn should
squid use.
another thing is that squid is not returning memory once ot took it.
Amos knows about this and might be able to respond.

Eliezer

On Wed, Aug 5, 2020, 20:34 Ivan Bulatovic <ivan.bulatovic at gmail.com> wrote:

> Hi Eliezer,
>
> In the original message I sent to the squid-users mail list, I
> attached listings from mgr:info and mgr:mem. The server is definitely
> using a lot of connections (close to 200K connections), which is why I
> increased the open files limits in linux as well as in squid.conf. And
> there are a lot of requests per second, when the server is running. I
> could understand that it needs a lot of memory for in-transit cache,
> but that memory should be later released  back to OS, once the
> requests load goes down. However, that is not the case, even days
> after there is no load on the server, it stays at 11GB of RAM and 5.5
> GB of swap used. The second I restart the squid process, everything
> goes to normal, memory is released. That is why I suspect there is
> some memory leak somewhere.
>
> The server is a Ubuntu 18.04 LTS VM (running on Hyper-V 2019 server),
> with 8 virtual processors and 12GB of RAM (although I can increase
> that if that is the problem, but I thought that without caching this
> would be more than enough).
>
> I am not using dynamic memory on Hyper-V (it is turned off for this VM).
>
> Best regards,
> Ivan
>
> On Wed, Aug 5, 2020 at 7:14 PM NgTech LTD <ngtech1ltd at gmail.com> wrote:
> >
> > I think that the mgr:info or another page there contains the amount of
> requests per second etc.
> > also netstat or ss -ntp might give some basic understanding about this
> server size.
> >
> > are you using dynamic memory on the hyper-v hypervisor?
> >
> > Eliezer
> >
> > On Wed, Aug 5, 2020, 19:59 Ivan Bulatovic <ivan.bulatovic at gmail.com>
> wrote:
> >>
> >> Hi Alex,
> >>
> >> Thank you very much for your help.
> >>
> >> I opened a bug on bugs.squid-cache.org
> >> (https://bugs.squid-cache.org/show_bug.cgi?id=5071).
> >>
> >> Best regards,
> >> Ivan
> >>
> >> On Mon, Aug 3, 2020 at 10:02 PM Alex Rousskov
> >> <rousskov at measurement-factory.com> wrote:
> >> >
> >> > On 8/3/20 9:11 AM, Ivan Bulatovic wrote:
> >> >
> >> > > Looks like squid has some serious memory issues when under heavy
> load
> >> > > (90 servers that crawl Internet sites).
> >> >
> >> > >         Maximum Resident Size: 41500720 KB
> >> >
> >> > If the above (unreliable) report matches your observations using
> system
> >> > tools like "top", then it is indeed likely that your Squid is
> suffering
> >> > from a memory leak -- 41GB is usually too much for most non-caching
> >> > Squid instances.
> >> >
> >> > Identifying the leak may take some time, and I am not volunteering to
> do
> >> > the necessary legwork personally, but the Squid Project does fix
> >> > virtually all runtime leaks that we know about. If you want to speed
> up
> >> > the process, one of the best things you can do is to run Squid under
> >> > valgrind with a good suppression file. This requires building Squid
> with
> >> > a special ./configure option. Several testing iterations may be
> >> > necessary. If you are willing to do this, please file a bug report and
> >> > somebody will guide you through the steps.
> >> >
> >> >
> >> > > It just eats up memory, and
> >> > > does not free it up even days after it is being used (with no load
> on
> >> > > the proxy for days).
> >> >
> >> > Some memory retention is expected by default. See
> >> > http://www.squid-cache.org/Doc/config/memory_pools/
> >> >
> >> > Unfortunately, AFAICT, your mgr:mem output does not show any obvious
> >> > leaks -- all numbers are very small. If something is leaking a lot,
> then
> >> > it is probably not pooled by Squid.
> >> >
> >> >
> >> > HTH,
> >> >
> >> > Alex.
> >> >
> >> >
> >> > > On Mon, Jul 20, 2020 at 10:46 PM Ivan Bulatovic wrote:
> >> > >>
> >> > >> Hi all,
> >> > >>
> >> > >> I am trying to configure squid to run as a forward proxy with no
> >> > >> caching (cache deny all) with an option to choose the outgoing IP
> >> > >> address based on the username. So all squid has to do is to use a
> >> > >> certain outgoing IP address for a certain user, return the data
> from
> >> > >> the server to that user and cache nothing.
> >> > >>
> >> > >> For that I created a special authentication helper and used the
> ACLs
> >> > >> and tcp_outgoing_address to create a lot of users and outgoing IP
> >> > >> addresses (about 260 at the moment). Example (not the real IP I
> use,
> >> > >> of course):
> >> > >>
> >> > >> acl use_IP1 proxy_auth user1
> >> > >> tcp_outgoing_address 1.2.3.4   use_IP1
> >> > >>
> >> > >> I also configured the squid to use 4 workers, but this happens even
> >> > >> when I use only one worker (default)
> >> > >>
> >> > >> And this works. However, under heavy load, Squid eats all of the
> RAM
> >> > >> and then starts going to swap. And the memory usage does not drop
> when
> >> > >> I remove all the load from squid (I shut down all clients).
> >> > >>
> >> > >> I left it to see if the memory will be freed but even after
> leaving it
> >> > >> for an hour the info page reports this:
> >> > >> Cache information for squid:
> >> > >>         Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
> >> > >>         Hits as % of bytes sent:        5min: 0.0%, 60min: 1.1%
> >> > >>         Memory hits as % of hit requests:       5min: 0.0%, 60min:
> 0.0%
> >> > >>         Disk hits as % of hit requests: 5min: 0.0%, 60min: 100.0%
> >> > >>         Storage Swap size:      0 KB
> >> > >>         Storage Swap capacity:   0.0% used, 100.0% free
> >> > >>         Storage Mem size:       0 KB
> >> > >>         Storage Mem capacity:    0.0% used, 100.0% free
> >> > >>         Mean Object Size:       0.00 KB
> >> > >>         Requests given to unlinkd:      0
> >> > >>
> >> > >> Resource usage for squid:
> >> > >>         UP Time:        255334.875 seconds
> >> > >>         CPU Time:       7122.436 seconds
> >> > >>         CPU Usage:      2.79%
> >> > >>         CPU Usage, 5 minute avg:        0.05%
> >> > >>         CPU Usage, 60 minute avg:       37.66%
> >> > >>         Maximum Resident Size: 41500720 KB
> >> > >>         Page faults with physical i/o: 1003410
> >> > >>
> >> > >> And here is the listing of free and top commands (with no load on
> the server):
> >> > >>
> >> > >> # free -h
> >> > >>               total        used        free      shared
> buff/cache   available
> >> > >> Mem:            11G         10G        791M        676K
> 491M        1.0G
> >> > >> Swap:           11G        5.5G        6.5G
> >> > >>
> >> > >> # top
> >> > >> top - 14:12:32 up 3 days,  1:30,  1 user,  load average: 0.00,
> 0.00, 0.00
> >> > >> Tasks: 177 total,   1 running, 102 sleeping,   0 stopped,   0
> zombie
> >> > >> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu2  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> %Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,
> 0.0 si,  0.0 st
> >> > >> KiB Mem : 91.2/12251688
> >> > >>
> [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
> >> > >>         ]
> >> > >> KiB Swap: 45.8/12582904
> >> > >> [||||||||||||||||||||||||||||||||||||||||||||||
> >> > >>                               ]
> >> > >>
> >> > >>    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM
>  TIME+ COMMAND
> >> > >>   7851 proxy     20   0 6946872 2.514g   8084 S   0.0 21.5
> 29:43.74 squid
> >> > >>   7832 proxy     20   0 6711480 2.464g   8040 S   0.0 21.1
> 29:58.17 squid
> >> > >>   7814 proxy     20   0 6834928 2.454g  10024 S   0.0 21.0
> 29:47.56 squid
> >> > >>   7843 proxy     20   0 6906252 2.436g   8208 S   0.0 20.8
> 29:15.60 squid
> >> > >>   1329 root      20   0 2416672 167272  12680 S   0.0  1.4
> 136:18.57 metricbeat
> >> > >>   1321 root      20   0 1831804  48364  11648 S   0.0  0.4
> 14:32.10 filebeat
> >> > >>    474 root      19  -1  127796  17576  17144 S   0.0  0.1
>  0:27.01
> >> > >> systemd-journal
> >> > >>   7811 proxy     20   0  549384  14168   8372 S   0.0  0.1
>  0:20.87 squid
> >> > >>   1166 root      20   0 1749724  10596   4468 S   0.0  0.1
>  0:31.83 snapd
> >> > >>  43940 proxy     20   0   28884   9608   5384 S   0.0  0.1
>  0:00.14 python3
> >> > >>  43941 proxy     20   0   28884   9552   5328 S   0.0  0.1
>  0:00.10 python3
> >> > >>  43939 proxy     20   0   28884   9524   5308 S   0.0  0.1
>  0:00.12 python3
> >> > >>  43938 proxy     20   0   28884   9452   5232 S   0.0  0.1
>  0:00.16 python3
> >> > >>  48848 root      20   0  105688   6960   5968 S   0.0  0.1
>  0:00.02 sshd
> >> > >>  48974 janitor   20   0  108120   5380   4372 S   0.0  0.0
>  0:00.00 sshd
> >> > >>      1 root      20   0   86360   4364   2488 S   0.0  0.0
> 32:46.22 systemd
> >> > >> ...
> >> > >> ... lines ommited
> >> > >> ...
> >> > >>
> >> > >> In the attachment you can find the printout from squidclient
> mgr:info
> >> > >> and squidclient mgr:mem. These are both taken at the moment when
> there
> >> > >> is no more load on the proxy. I also included my squid.conf file
> >> > >> (minus the two files where acls are defined and outgoing IP
> addresses,
> >> > >> these two contain only acl and tcp_outgoing_address lines as in the
> >> > >> example above).
> >> > >>
> >> > >> Machine info:
> >> > >> OS: Ubuntu 18.04 with latest updates
> >> > >> Squid version 4.12 (from diladele repository)
> >> > >> Hardware: Hyper-V virtual machine with 8 vCPU, 12GB of RAM
> >> > >>
> >> > >> I can not understand what is eating all of the memory, if I
> disabled the cache.
> >> > >>
> >> > >> Maybe I configured something wrong but I can not find what.
> >> > >>
> >> > >> Thank you for any help you can provide.
> >> > >>
> >> > >> Best regards,
> >> > >> Ivan
> >> > > _______________________________________________
> >> > > squid-users mailing list
> >> > > squid-users at lists.squid-cache.org
> >> > > http://lists.squid-cache.org/listinfo/squid-users
> >> > >
> >> >
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200806/0515ad3c/attachment.htm>

From tamurin0525 at gmail.com  Thu Aug  6 04:24:47 2020
From: tamurin0525 at gmail.com (m k)
Date: Thu, 6 Aug 2020 13:24:47 +0900
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
Message-ID: <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>

Amos,

Thank you for your reply.
It was very helpful.

> That number was gained before HTTPS became so popular. So YMMV depending
> on how many CONNECT tunnels you have to deal with. That HTTPS traffic can
possibly be decrypted
> and cached but performance trade-offs are quite large.

Squid uses SSL-Bump.
I'm very worried about the internet slowing down due to https decording.
and I'm also worried about the internet slowing down due to using Blacklist.
I load tens of thousands of URL(black list file) every time I set up ACL.

How many requests does SSL-Bump in one second?

Thank you,
kitamura

2020?8?5?(?) 10:32 Amos Jeffries <squid3 at treenet.co.nz>:

> On 5/08/20 11:28 am, m k wrote:
> >> We are considering to use Squid for our proxy, and would like to know
> >> performance sizing aspects.
> >>
> >> Current web access request averages per 1 hour are as followings
> >> Clients?30,000?
> >> Page Views:141,741/hour
> >> *Requests:4,893,106
> >>
>
> Okay. Requests and client count are the important numbers there.
>
> The ~1359 req/sec is well within a default Squid capabilities, which can
> extend up to around 10k req/sec before needing careful tuning.
>
> That number was gained before HTTPS became so popular. So YMMV depending
> on how many CONNECT tunnels you have to deal with. That HTTPS traffic
> can possibly be decrypted and cached but performance trade-offs are
> quite large.


> >> We will install Squid on CentOS 8.1.   Please kindly share your
> >> thoughts / advices
>
> Whatever OS you are most comfortable with administering. Be aware that
> CentOS official Squid packages are very slow to update - Apparently they
> still have only v4.4 (8 months old) despite a 8.2 point release only a
> few weeks ago.
>
> So you may need to be building your own from sources and/or using other
> semi-official packagers such as the ones from Eliezer at NGTech when he
> gets around to CentOS 8 packages.
>   <https://wiki.squid-cache.org/KnowledgeBase/CentOS>
>
>
> FYI; If you find yourself having to use SSL-Bump, then we highly
> recommended to follow the latest Squid releases with fairly frequent
> updates (at minimum a few times per year - worst case monthly). If you
> like CentOS you may find Fedora more suitable to track the security
> environment volatility and update churn.
>
>
> >> Is there sizing methodology and tools?
>
> There are a couple of methodologies, depending on what aspect you are
> tuning towards - and one for identifying the limitation points to begin
> a tuning process tuning.
>
> The info you gave above is the beginning. Checking to see if your
> traffic rate is reasonably within capability of a single Squid instance.
>
> Yours is reasonable, so next step is to get Squid running and see where
> the trouble points (if any) are.
>
>  For more see <https://wiki.squid-cache.org/SquidFaq/>
>
>
>
> >> How much resources are generally recommended for our environment?
> >> CPU: Memory: Disk space : Other factors to be considered if any:
> >> Do you have a generally recommended performance testing tools? Any
> >> suggested guidelines?
> >>
>
>
>  CPU - squid is still mostly single-process. So prioritize faster GHz
> rates over core number. Multi-core can help of course, but not as much
> as cycle speeds do. Hyper-threading is useless for Squid.
>
>  Memory - Squid will use as much as you can give it. Let your budget
> govern this.
>
>  Disk - Squid will happily run with no disk - or lots of large ones.
>
>    - Avoid RAID. Squid *will* shorten disk lifetimes with its unusually
> high write I/O pattern. How much shorter varies by disk type (HDD vs
> SSD). So you may find it better to plan budget towards maintenance costs
> of replacing disks in future rather than buying multiple up-front for
> RAID use.
>  see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.
>
>     - Up to a few hundred GB per cache_dir can be good for large caches.
> Going up to TB is not (yet) worth the disk cost as Squid has a per-cache
> limit on stored objects.
>
>    - Disk caches can be re-tuned, added, moved, removed, and/or extended
> at any time and will depend on the profile of object sizes your proxy
> handles - which itself likely changes over time. So general let your
> budget decide the initial disks and work from there.
>
>
>
> Load Testing - the tools us dev use to review performance are listed at
> the bottom of the profiling FAQ page. These are best for testing the
> theoretical limits of a particular installation - real traffic tends to
> be somewhat lower. So I personally prefer taking stats from the running
> proxy on real traffic and seeing what I can observe from those.
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200806/17326fdd/attachment.htm>

From ngtech1ltd at gmail.com  Thu Aug  6 04:38:29 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 6 Aug 2020 07:38:29 +0300
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
Message-ID: <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>

Kitamura,

 

About the tens of thousands of URLs, Have you considered using a Blacklisting utility, it might lower the memory footprint.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of m k
Sent: Thursday, August 6, 2020 7:25 AM
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] I would like to know performance sizing aspects.

 

Amos,

 

Thank you for your reply.

It was very helpful.

 

> That number was gained before HTTPS became so popular. So YMMV depending
> on how many CONNECT tunnels you have to deal with. That HTTPS traffic can possibly be decrypted 

> and cached but performance trade-offs are quite large.

 

Squid uses SSL-Bump.

I'm very worried about the internet slowing down due to https decording. and I'm also worried about the internet slowing down due to using Blacklist.

I load tens of thousands of URL(black list file) every time I set up ACL.

 

How many requests does SSL-Bump in one second?

 

Thank you,

kitamura

 

2020?8?5?(?) 10:32 Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> >:

On 5/08/20 11:28 am, m k wrote:
>> We are considering to use Squid for our proxy, and would like to know
>> performance sizing aspects.
>>
>> Current web access request averages per 1 hour are as followings 
>> Clients?30,000?
>> Page Views:141,741/hour
>> *Requests:4,893,106
>>

Okay. Requests and client count are the important numbers there.

The ~1359 req/sec is well within a default Squid capabilities, which can
extend up to around 10k req/sec before needing careful tuning.

That number was gained before HTTPS became so popular. So YMMV depending
on how many CONNECT tunnels you have to deal with. That HTTPS traffic
can possibly be decrypted and cached but performance trade-offs are
quite large.


>> We will install Squid on CentOS 8.1.   Please kindly share your
>> thoughts / advices

Whatever OS you are most comfortable with administering. Be aware that
CentOS official Squid packages are very slow to update - Apparently they
still have only v4.4 (8 months old) despite a 8.2 point release only a
few weeks ago.

So you may need to be building your own from sources and/or using other
semi-official packagers such as the ones from Eliezer at NGTech when he
gets around to CentOS 8 packages.
  <https://wiki.squid-cache.org/KnowledgeBase/CentOS>


FYI; If you find yourself having to use SSL-Bump, then we highly
recommended to follow the latest Squid releases with fairly frequent
updates (at minimum a few times per year - worst case monthly). If you
like CentOS you may find Fedora more suitable to track the security
environment volatility and update churn.


>> Is there sizing methodology and tools?

There are a couple of methodologies, depending on what aspect you are
tuning towards - and one for identifying the limitation points to begin
a tuning process tuning.

The info you gave above is the beginning. Checking to see if your
traffic rate is reasonably within capability of a single Squid instance.

Yours is reasonable, so next step is to get Squid running and see where
the trouble points (if any) are.

 For more see <https://wiki.squid-cache.org/SquidFaq/>



>> How much resources are generally recommended for our environment?
>>? CPU:? Memory:? Disk space : Other factors to be considered if any:
>> Do you have a generally recommended performance testing tools? Any
>> suggested guidelines?
>>


 CPU - squid is still mostly single-process. So prioritize faster GHz
rates over core number. Multi-core can help of course, but not as much
as cycle speeds do. Hyper-threading is useless for Squid.

 Memory - Squid will use as much as you can give it. Let your budget
govern this.

 Disk - Squid will happily run with no disk - or lots of large ones.

   - Avoid RAID. Squid *will* shorten disk lifetimes with its unusually
high write I/O pattern. How much shorter varies by disk type (HDD vs
SSD). So you may find it better to plan budget towards maintenance costs
of replacing disks in future rather than buying multiple up-front for
RAID use.
 see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.

    - Up to a few hundred GB per cache_dir can be good for large caches.
Going up to TB is not (yet) worth the disk cost as Squid has a per-cache
limit on stored objects.

   - Disk caches can be re-tuned, added, moved, removed, and/or extended
at any time and will depend on the profile of object sizes your proxy
handles - which itself likely changes over time. So general let your
budget decide the initial disks and work from there.



Load Testing - the tools us dev use to review performance are listed at
the bottom of the profiling FAQ page. These are best for testing the
theoretical limits of a particular installation - real traffic tends to
be somewhat lower. So I personally prefer taking stats from the running
proxy on real traffic and seeing what I can observe from those.


HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200806/356ab681/attachment.htm>

From tamurin0525 at gmail.com  Thu Aug  6 05:28:55 2020
From: tamurin0525 at gmail.com (m k)
Date: Thu, 6 Aug 2020 14:28:55 +0900
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
 <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
Message-ID: <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>

Eliezer,

Squid's default setting is 1 core CPU, 16GB mem.
How many URLs(Blacklist) will degrade Squid's performance?

Also, SSL-Bump.

Thank you,
kitamura


2020?8?6?(?) 13:38 Eliezer Croitor <ngtech1ltd at gmail.com>:

> Kitamura,
>
>
>
> About the tens of thousands of URLs, Have you considered using a
> Blacklisting utility, it might lower the memory footprint.
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *m k
> *Sent:* Thursday, August 6, 2020 7:25 AM
> *To:* Amos Jeffries <squid3 at treenet.co.nz>
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] I would like to know performance sizing
> aspects.
>
>
>
> Amos,
>
>
>
> Thank you for your reply.
>
> It was very helpful.
>
>
>
> > That number was gained before HTTPS became so popular. So YMMV depending
> > on how many CONNECT tunnels you have to deal with. That HTTPS
> traffic can possibly be decrypted
>
> > and cached but performance trade-offs are quite large.
>
>
>
> Squid uses SSL-Bump.
>
> I'm very worried about the internet slowing down due to https decording.
> and I'm also worried about the internet slowing down due to using Blacklist.
>
> I load tens of thousands of URL(black list file) every time I set up ACL.
>
>
>
> How many requests does SSL-Bump in one second?
>
>
>
> Thank you,
>
> kitamura
>
>
>
> 2020?8?5?(?) 10:32 Amos Jeffries <squid3 at treenet.co.nz>:
>
> On 5/08/20 11:28 am, m k wrote:
> >> We are considering to use Squid for our proxy, and would like to know
> >> performance sizing aspects.
> >>
> >> Current web access request averages per 1 hour are as followings
> >> Clients?30,000?
> >> Page Views:141,741/hour
> >> *Requests:4,893,106
> >>
>
> Okay. Requests and client count are the important numbers there.
>
> The ~1359 req/sec is well within a default Squid capabilities, which can
> extend up to around 10k req/sec before needing careful tuning.
>
> That number was gained before HTTPS became so popular. So YMMV depending
> on how many CONNECT tunnels you have to deal with. That HTTPS traffic
> can possibly be decrypted and cached but performance trade-offs are
> quite large.
>
>
> >> We will install Squid on CentOS 8.1.   Please kindly share your
> >> thoughts / advices
>
> Whatever OS you are most comfortable with administering. Be aware that
> CentOS official Squid packages are very slow to update - Apparently they
> still have only v4.4 (8 months old) despite a 8.2 point release only a
> few weeks ago.
>
> So you may need to be building your own from sources and/or using other
> semi-official packagers such as the ones from Eliezer at NGTech when he
> gets around to CentOS 8 packages.
>   <https://wiki.squid-cache.org/KnowledgeBase/CentOS>
>
>
> FYI; If you find yourself having to use SSL-Bump, then we highly
> recommended to follow the latest Squid releases with fairly frequent
> updates (at minimum a few times per year - worst case monthly). If you
> like CentOS you may find Fedora more suitable to track the security
> environment volatility and update churn.
>
>
> >> Is there sizing methodology and tools?
>
> There are a couple of methodologies, depending on what aspect you are
> tuning towards - and one for identifying the limitation points to begin
> a tuning process tuning.
>
> The info you gave above is the beginning. Checking to see if your
> traffic rate is reasonably within capability of a single Squid instance.
>
> Yours is reasonable, so next step is to get Squid running and see where
> the trouble points (if any) are.
>
>  For more see <https://wiki.squid-cache.org/SquidFaq/>
>
>
>
> >> How much resources are generally recommended for our environment?
> >> CPU: Memory: Disk space : Other factors to be considered if any:
> >> Do you have a generally recommended performance testing tools? Any
> >> suggested guidelines?
> >>
>
>
>  CPU - squid is still mostly single-process. So prioritize faster GHz
> rates over core number. Multi-core can help of course, but not as much
> as cycle speeds do. Hyper-threading is useless for Squid.
>
>  Memory - Squid will use as much as you can give it. Let your budget
> govern this.
>
>  Disk - Squid will happily run with no disk - or lots of large ones.
>
>    - Avoid RAID. Squid *will* shorten disk lifetimes with its unusually
> high write I/O pattern. How much shorter varies by disk type (HDD vs
> SSD). So you may find it better to plan budget towards maintenance costs
> of replacing disks in future rather than buying multiple up-front for
> RAID use.
>  see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.
>
>     - Up to a few hundred GB per cache_dir can be good for large caches.
> Going up to TB is not (yet) worth the disk cost as Squid has a per-cache
> limit on stored objects.
>
>    - Disk caches can be re-tuned, added, moved, removed, and/or extended
> at any time and will depend on the profile of object sizes your proxy
> handles - which itself likely changes over time. So general let your
> budget decide the initial disks and work from there.
>
>
>
> Load Testing - the tools us dev use to review performance are listed at
> the bottom of the profiling FAQ page. These are best for testing the
> theoretical limits of a particular installation - real traffic tends to
> be somewhat lower. So I personally prefer taking stats from the running
> proxy on real traffic and seeing what I can observe from those.
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200806/1b380f50/attachment.htm>

From ngtech1ltd at gmail.com  Thu Aug  6 10:52:32 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 6 Aug 2020 13:52:32 +0300
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
 <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
 <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
Message-ID: <001e01d66bdf$b0871490$11953db0$@gmail.com>

Did you mean 1 CPU with couple cores?

 

For squid it most of the time takes time to load these lists into ram.

It?s not wrong to do so since in many cases it?s the right thing to do.

In case these lists are stale for at-least a day I assume it should be fine.

 

However there are tools like ufdbguard and others which are very good in the sense of
memory footprint and fast URLs lookup.

 

How do you add these URLS, I am not mistaken about URLs and not domains right?

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: m k <tamurin0525 at gmail.com> 
Sent: Thursday, August 6, 2020 8:29 AM
To: Eliezer Croitor <ngtech1ltd at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] I would like to know performance sizing aspects.

 

Eliezer,

 

Squid's default setting is 1 core CPU, 16GB mem.

How many URLs(Blacklist) will degrade Squid's performance?

 

Also, SSL-Bump.

 

Thank you,

kitamura

 

 

2020?8?6?(?) 13:38 Eliezer Croitor <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> >:

Kitamura,

 

About the tens of thousands of URLs, Have you considered using a Blacklisting utility, it might lower the memory footprint.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of m k
Sent: Thursday, August 6, 2020 7:25 AM
To: Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> >
Cc: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] I would like to know performance sizing aspects.

 

Amos,

 

Thank you for your reply.

It was very helpful.

 

> That number was gained before HTTPS became so popular. So YMMV depending
> on how many CONNECT tunnels you have to deal with. That HTTPS traffic can possibly be decrypted 

> and cached but performance trade-offs are quite large.

 

Squid uses SSL-Bump.

I'm very worried about the internet slowing down due to https decording. and I'm also worried about the internet slowing down due to using Blacklist.

I load tens of thousands of URL(black list file) every time I set up ACL.

 

How many requests does SSL-Bump in one second?

 

Thank you,

kitamura

 

2020?8?5?(?) 10:32 Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> >:

On 5/08/20 11:28 am, m k wrote:
>> We are considering to use Squid for our proxy, and would like to know
>> performance sizing aspects.
>>
>> Current web access request averages per 1 hour are as followings 
>> Clients?30,000?
>> Page Views:141,741/hour
>> *Requests:4,893,106
>>

Okay. Requests and client count are the important numbers there.

The ~1359 req/sec is well within a default Squid capabilities, which can
extend up to around 10k req/sec before needing careful tuning.

That number was gained before HTTPS became so popular. So YMMV depending
on how many CONNECT tunnels you have to deal with. That HTTPS traffic
can possibly be decrypted and cached but performance trade-offs are
quite large.


>> We will install Squid on CentOS 8.1.   Please kindly share your
>> thoughts / advices

Whatever OS you are most comfortable with administering. Be aware that
CentOS official Squid packages are very slow to update - Apparently they
still have only v4.4 (8 months old) despite a 8.2 point release only a
few weeks ago.

So you may need to be building your own from sources and/or using other
semi-official packagers such as the ones from Eliezer at NGTech when he
gets around to CentOS 8 packages.
  <https://wiki.squid-cache.org/KnowledgeBase/CentOS>


FYI; If you find yourself having to use SSL-Bump, then we highly
recommended to follow the latest Squid releases with fairly frequent
updates (at minimum a few times per year - worst case monthly). If you
like CentOS you may find Fedora more suitable to track the security
environment volatility and update churn.


>> Is there sizing methodology and tools?

There are a couple of methodologies, depending on what aspect you are
tuning towards - and one for identifying the limitation points to begin
a tuning process tuning.

The info you gave above is the beginning. Checking to see if your
traffic rate is reasonably within capability of a single Squid instance.

Yours is reasonable, so next step is to get Squid running and see where
the trouble points (if any) are.

 For more see <https://wiki.squid-cache.org/SquidFaq/>



>> How much resources are generally recommended for our environment?
>>? CPU:? Memory:? Disk space : Other factors to be considered if any:
>> Do you have a generally recommended performance testing tools? Any
>> suggested guidelines?
>>


 CPU - squid is still mostly single-process. So prioritize faster GHz
rates over core number. Multi-core can help of course, but not as much
as cycle speeds do. Hyper-threading is useless for Squid.

 Memory - Squid will use as much as you can give it. Let your budget
govern this.

 Disk - Squid will happily run with no disk - or lots of large ones.

   - Avoid RAID. Squid *will* shorten disk lifetimes with its unusually
high write I/O pattern. How much shorter varies by disk type (HDD vs
SSD). So you may find it better to plan budget towards maintenance costs
of replacing disks in future rather than buying multiple up-front for
RAID use.
 see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.

    - Up to a few hundred GB per cache_dir can be good for large caches.
Going up to TB is not (yet) worth the disk cost as Squid has a per-cache
limit on stored objects.

   - Disk caches can be re-tuned, added, moved, removed, and/or extended
at any time and will depend on the profile of object sizes your proxy
handles - which itself likely changes over time. So general let your
budget decide the initial disks and work from there.



Load Testing - the tools us dev use to review performance are listed at
the bottom of the profiling FAQ page. These are best for testing the
theoretical limits of a particular installation - real traffic tends to
be somewhat lower. So I personally prefer taking stats from the running
proxy on real traffic and seeing what I can observe from those.


HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200806/ed6f9c65/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug  6 23:29:46 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Aug 2020 11:29:46 +1200
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
 <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
 <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
Message-ID: <a1bf0203-4db2-b5f9-2a1e-679e53000fab@treenet.co.nz>

On 6/08/20 5:28 pm, m k wrote:
> Eliezer,
> 
> Squid's default setting is 1 core CPU, 16GB mem.
> How many URLs(Blacklist) will degrade Squid's performance?
> 

Eliezer's answer covers that already, so I will skip here.


> Also, SSL-Bump.
> 

This is "unknown" - as far as I am aware none has published numbers
recently about it. There are a lot of factors in the network traffic and
your servers internal state (eg the RNG engine) that multiply up to
cause varying amounts of delay - plus the volatile nature of this
feature set itself month by month changes the effects or relevance of
each factor.
  So numbers from me today will be wrong in a few weeks, or may be wrong
for your network already. All that we can be sure of is that there is
extra work needed by Squid thus "slower" than plain-text HTTP is to be
expected.

For planning the consideration is just to be aware that the numbers we
can give you (for plain-text) will be over-estimates of capacity for
SSL-Bump traffic and allow some margins. Once you have an install
running you can test and measure the actual numbers for your traffic.


Amos


From squid3 at treenet.co.nz  Fri Aug  7 00:13:14 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Aug 2020 12:13:14 +1200
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CABA8h=RDxzCx6vcTfbRee=7JLE6ozE+2Nb-QYQev6peb3b6h4g@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
 <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
 <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>
 <CAFJ4_4spgZ7o+1hm47uNnLmuOC0wEsmHGwyk0VtJm8xGqU7XXw@mail.gmail.com>
 <CABA8h=RDxzCx6vcTfbRee=7JLE6ozE+2Nb-QYQev6peb3b6h4g@mail.gmail.com>
Message-ID: <d7b64824-3784-d44c-bced-e9f37580432e@treenet.co.nz>

On 6/08/20 11:06 am, NgTech LTD wrote:
> Hey Ivan,
> 
> From what i remember there is a calculation for how much k per conn
> should squid use.

Aye;
 256KB * number of currently open FD
 + read_ahead_gap
 + received size of current in-transit response (if cacheable MISS)


> another thing is that squid is not returning memory once ot took it.

The calculation for this is _minimum_ 5MB per type of memory allocating
object is retained by Squid for quick re-use. The mgr:mem report lists
details of those allocations.



Alex didn't mention this earlier but what I am seeing in your "top" tool
output is that there are 5x 'squid' processes running. It looks like 4
of them are SMP worker or disker processes each using 2.5GB of RAM.

The "free" tool is confirming this with its report of "used: 10G" (4x
2.5GB) of memory actually being used on the machine.

Most kernels fork() implementation is terrible with virtual memory
calculations. Most of that number will never actually be used. So they
can be ignored so long as the per-process number does not exceed the
actual physical RAM installed (beyond that kernel refuses to spawn with
fork()).
 The numbers your tools are reporting are kind of reasonable - maximum
about 7GB *per process* allocated.


The 41GB "resident size" is from old memory allocation APIs in the
kernel which suffer from 32-bit issues. When this value has odd numbers
and/or conflicts with the system tools - believe the tools instead.



So to summarize; what I am seeing there is that during *Peak* load times
your proxy workers (combined) are *maybe* using up to 41GB of memory. At
the off-peak time you are doing your analysis reports they have dropped
down to 10GB.
 With one data point there is no sign of a memory leak happening. Just a
normal machine handling far more peak traffic than its available amount
of memory can cope with.

 That is not to rule out a leak entirely. More measurements over time
may show a pattern of increasing off-peak memory allocated. But just one
comparison of peak vs off-peak is not going to reveal that type of pattern.


Amos


From ivan.bulatovic at gmail.com  Fri Aug  7 17:30:07 2020
From: ivan.bulatovic at gmail.com (Ivan Bulatovic)
Date: Fri, 7 Aug 2020 19:30:07 +0200
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <d7b64824-3784-d44c-bced-e9f37580432e@treenet.co.nz>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
 <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
 <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>
 <CAFJ4_4spgZ7o+1hm47uNnLmuOC0wEsmHGwyk0VtJm8xGqU7XXw@mail.gmail.com>
 <CABA8h=RDxzCx6vcTfbRee=7JLE6ozE+2Nb-QYQev6peb3b6h4g@mail.gmail.com>
 <d7b64824-3784-d44c-bced-e9f37580432e@treenet.co.nz>
Message-ID: <CAFJ4_4uvU+FtATcU_HOx1FKEDQPWJGm7td-fyO5BJ-dt+_ct3w@mail.gmail.com>

Hi Amos,

> > From what i remember there is a calculation for how much k per conn
> > should squid use.
>
> Aye;
>  256KB * number of currently open FD
>  + read_ahead_gap
>  + received size of current in-transit response (if cacheable MISS)

I tried to reduce the number of in-memory objects using following (I
read somewhere that 4KB is the minimum block of memory that squid can
allocate):
  cache_mem 8 MB
  maximum_object_size 4 KB
  maximum_object_size_in_memory 4 KB
  read_ahead_gap 4 KB

But the above settings did not help much.

At the moment I am running a much lighter load on the squid VM to see
how it behaves.

So, right now, the machine has about 110.000 open TCP connections (I
guess half are from clients and the other half is to the Internet,
which my firewall also confirms). It has been running like this for
the last 4 hours or so.

Here is the situation (in the attachment you will find full command
print-outs and config file):

- Running squid 4.12 from diladele repository on Ubuntu 18.04 LTS

- RAM used: around 9 GB out of 16 GB (no of swap is used)

- I am running 2 squid workers at the moment (see attached squid.conf)

- Top reports this (removed other processes, they practically have no
impact on the memory listing):

top - 10:22:03 up 18:00,  1 user,  load average: 1.88, 1.58, 1.43
Tasks: 169 total,   1 running,  94 sleeping,   0 stopped,   0 zombie
%Cpu(s): 13.6 us,  9.6 sy,  0.0 ni, 72.7 id,  0.1 wa,  0.0 hi,  3.9 si,  0.0 st
KiB Mem : 16380456 total,  5011560 free,  9205660 used,  2163236 buff/cache
KiB Swap: 12582904 total, 12582904 free,        0 used.  7121124 avail Mem

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
  1515 proxy     20   0 5713808 4.396g  14188 S  36.5 28.1 130:03.34 squid
  1514 proxy     20   0 4360348 3.329g  14380 S  28.9 21.3 104:15.21 squid

 - mgr:info show some weird stats:
    Number of clients accessing cache:      156   (which is exactly
twice the number of actual clients, but this is probably due to the
number of workers)
    Maximum Resident Size: 32467680 KB   (which is 32GB. At no time
during these 4 hours has this value of RAM consumption ever been
reached. The memory is steadily, but slowly increasing to where it is
now - at 9 GB. I have no idea what this value is.)

I have no idea if the rest of the stats from mgr:info are OK or not, I
really have no way of checking that.

I added to the configuration memory pools option, we will see if it
helps (I think I already tried this, but I can not be sure, I ran a
lot of tests trying to fix this myself before I reached out to you):
memory_pools off

If there is anything else I can do to help with debugging this, please
let me know.

Thank you for your time and help,
Ivan


On Fri, Aug 7, 2020 at 2:23 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 6/08/20 11:06 am, NgTech LTD wrote:
> > Hey Ivan,
> >
> > From what i remember there is a calculation for how much k per conn
> > should squid use.
>
> Aye;
>  256KB * number of currently open FD
>  + read_ahead_gap
>  + received size of current in-transit response (if cacheable MISS)
>
>
> > another thing is that squid is not returning memory once ot took it.
>
> The calculation for this is _minimum_ 5MB per type of memory allocating
> object is retained by Squid for quick re-use. The mgr:mem report lists
> details of those allocations.
>
>
>
> Alex didn't mention this earlier but what I am seeing in your "top" tool
> output is that there are 5x 'squid' processes running. It looks like 4
> of them are SMP worker or disker processes each using 2.5GB of RAM.
>
> The "free" tool is confirming this with its report of "used: 10G" (4x
> 2.5GB) of memory actually being used on the machine.
>
> Most kernels fork() implementation is terrible with virtual memory
> calculations. Most of that number will never actually be used. So they
> can be ignored so long as the per-process number does not exceed the
> actual physical RAM installed (beyond that kernel refuses to spawn with
> fork()).
>  The numbers your tools are reporting are kind of reasonable - maximum
> about 7GB *per process* allocated.
>
>
> The 41GB "resident size" is from old memory allocation APIs in the
> kernel which suffer from 32-bit issues. When this value has odd numbers
> and/or conflicts with the system tools - believe the tools instead.
>
>
>
> So to summarize; what I am seeing there is that during *Peak* load times
> your proxy workers (combined) are *maybe* using up to 41GB of memory. At
> the off-peak time you are doing your analysis reports they have dropped
> down to 10GB.
>  With one data point there is no sign of a memory leak happening. Just a
> normal machine handling far more peak traffic than its available amount
> of memory can cope with.
>
>  That is not to rule out a leak entirely. More measurements over time
> may show a pattern of increasing off-peak memory allocated. But just one
> comparison of peak vs off-peak is not going to reveal that type of pattern.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Proxy01 test 2020-08-07.zip
Type: application/x-zip-compressed
Size: 8605 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200807/671e6475/attachment.bin>

From ngtech1ltd at gmail.com  Mon Aug 10 05:49:55 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Mon, 10 Aug 2020 08:49:55 +0300
Subject: [squid-users] High memory usage under load with caching
 disabled, memory is not being freed even with no load
In-Reply-To: <CAFJ4_4uvU+FtATcU_HOx1FKEDQPWJGm7td-fyO5BJ-dt+_ct3w@mail.gmail.com>
References: <CAFJ4_4sj09OixfRXb=2WBXMfDWB3ecUQ+24wg6UOez9OKpMyOg@mail.gmail.com>
 <CAFJ4_4tiYEBq_i-RfXir3Ek56Jq3nZ8dVzyaYQeQT_GHfavayQ@mail.gmail.com>
 <01642842-e00a-cf95-c7da-b4b6067f7519@measurement-factory.com>
 <CAFJ4_4srYs59JHCmL4F+8Gi47SE3Q0xVDr+10qJr9Sq1amszcg@mail.gmail.com>
 <CABA8h=QQ3A-6SgO6xdALXoqoSGSiwqEOkDTdVGsUn_oeaAG_9Q@mail.gmail.com>
 <CAFJ4_4spgZ7o+1hm47uNnLmuOC0wEsmHGwyk0VtJm8xGqU7XXw@mail.gmail.com>
 <CABA8h=RDxzCx6vcTfbRee=7JLE6ozE+2Nb-QYQev6peb3b6h4g@mail.gmail.com>
 <d7b64824-3784-d44c-bced-e9f37580432e@treenet.co.nz>
 <CAFJ4_4uvU+FtATcU_HOx1FKEDQPWJGm7td-fyO5BJ-dt+_ct3w@mail.gmail.com>
Message-ID: <000b01d66eda$140228a0$3c0679e0$@gmail.com>

It's 700+ connections per client... assuming it's only half ie 400+-
It's a lot.. for a ssl-bump proxy.
A simple tcp proxy can take it without stressing too much but TLS bump is another story.
Can you verify if there are sessions which are open more then 1 hour?

The basic suggestion for many proxies is to limit the time which a connection can stay open.
conntrack tools can help to identify idle connections on Ubuntu 18.04.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Ivan Bulatovic
Sent: Friday, August 7, 2020 8:30 PM
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] High memory usage under load with caching disabled, memory is not being freed even with no load

Hi Amos,

> > From what i remember there is a calculation for how much k per conn 
> > should squid use.
>
> Aye;
>  256KB * number of currently open FD
>  + read_ahead_gap
>  + received size of current in-transit response (if cacheable MISS)

I tried to reduce the number of in-memory objects using following (I read somewhere that 4KB is the minimum block of memory that squid can
allocate):
  cache_mem 8 MB
  maximum_object_size 4 KB
  maximum_object_size_in_memory 4 KB
  read_ahead_gap 4 KB

But the above settings did not help much.

At the moment I am running a much lighter load on the squid VM to see how it behaves.

So, right now, the machine has about 110.000 open TCP connections (I guess half are from clients and the other half is to the Internet, which my firewall also confirms). It has been running like this for the last 4 hours or so.

Here is the situation (in the attachment you will find full command print-outs and config file):

- Running squid 4.12 from diladele repository on Ubuntu 18.04 LTS

- RAM used: around 9 GB out of 16 GB (no of swap is used)

- I am running 2 squid workers at the moment (see attached squid.conf)

- Top reports this (removed other processes, they practically have no impact on the memory listing):

top - 10:22:03 up 18:00,  1 user,  load average: 1.88, 1.58, 1.43
Tasks: 169 total,   1 running,  94 sleeping,   0 stopped,   0 zombie
%Cpu(s): 13.6 us,  9.6 sy,  0.0 ni, 72.7 id,  0.1 wa,  0.0 hi,  3.9 si,  0.0 st KiB Mem : 16380456 total,  5011560 free,  9205660 used,  2163236 buff/cache
KiB Swap: 12582904 total, 12582904 free,        0 used.  7121124 avail Mem

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
  1515 proxy     20   0 5713808 4.396g  14188 S  36.5 28.1 130:03.34 squid
  1514 proxy     20   0 4360348 3.329g  14380 S  28.9 21.3 104:15.21 squid

 - mgr:info show some weird stats:
    Number of clients accessing cache:      156   (which is exactly
twice the number of actual clients, but this is probably due to the number of workers)
    Maximum Resident Size: 32467680 KB   (which is 32GB. At no time
during these 4 hours has this value of RAM consumption ever been reached. The memory is steadily, but slowly increasing to where it is now - at 9 GB. I have no idea what this value is.)

I have no idea if the rest of the stats from mgr:info are OK or not, I really have no way of checking that.

I added to the configuration memory pools option, we will see if it helps (I think I already tried this, but I can not be sure, I ran a lot of tests trying to fix this myself before I reached out to you):
memory_pools off

If there is anything else I can do to help with debugging this, please let me know.

Thank you for your time and help,
Ivan


On Fri, Aug 7, 2020 at 2:23 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 6/08/20 11:06 am, NgTech LTD wrote:
> > Hey Ivan,
> >
> > From what i remember there is a calculation for how much k per conn 
> > should squid use.
>
> Aye;
>  256KB * number of currently open FD
>  + read_ahead_gap
>  + received size of current in-transit response (if cacheable MISS)
>
>
> > another thing is that squid is not returning memory once ot took it.
>
> The calculation for this is _minimum_ 5MB per type of memory 
> allocating object is retained by Squid for quick re-use. The mgr:mem 
> report lists details of those allocations.
>
>
>
> Alex didn't mention this earlier but what I am seeing in your "top" 
> tool output is that there are 5x 'squid' processes running. It looks 
> like 4 of them are SMP worker or disker processes each using 2.5GB of RAM.
>
> The "free" tool is confirming this with its report of "used: 10G" (4x
> 2.5GB) of memory actually being used on the machine.
>
> Most kernels fork() implementation is terrible with virtual memory 
> calculations. Most of that number will never actually be used. So they 
> can be ignored so long as the per-process number does not exceed the 
> actual physical RAM installed (beyond that kernel refuses to spawn 
> with fork()).
>  The numbers your tools are reporting are kind of reasonable - maximum 
> about 7GB *per process* allocated.
>
>
> The 41GB "resident size" is from old memory allocation APIs in the 
> kernel which suffer from 32-bit issues. When this value has odd 
> numbers and/or conflicts with the system tools - believe the tools instead.
>
>
>
> So to summarize; what I am seeing there is that during *Peak* load 
> times your proxy workers (combined) are *maybe* using up to 41GB of 
> memory. At the off-peak time you are doing your analysis reports they 
> have dropped down to 10GB.
>  With one data point there is no sign of a memory leak happening. Just 
> a normal machine handling far more peak traffic than its available 
> amount of memory can cope with.
>
>  That is not to rule out a leak entirely. More measurements over time 
> may show a pattern of increasing off-peak memory allocated. But just 
> one comparison of peak vs off-peak is not going to reveal that type of pattern.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From roberto.nunnari at edu.ti.ch  Mon Aug 10 08:40:56 2020
From: roberto.nunnari at edu.ti.ch (Roberto Nunnari)
Date: Mon, 10 Aug 2020 10:40:56 +0200
Subject: [squid-users] ext_ldap_group_acl
Message-ID: <019701d66ef1$f7054620$e50fd260$@edu.ti.ch>

Hello.

 

I'm setting up squid on a CentOS 8 server.

Authentication against active directory works well with basic_ldap_auth, but
I fail when trying to check that a user belongs to a group.

It seems to me that for ext_ldap_group_acl it's enough that both the user
and the group exist and it returns OK. It returns ERR when it cannot find
the group or the user.

 

To make it more clear, here are the queries and results I get.

user1.test exists and is a member of group My_Group

user2.test exists and is NOT a member of group My_Group

Group asdf does NOT exist

 

So, I expect that when asking for

-          user1.test My_Group >> OK

-          user2.test My_Group >> ERR

But I get:

-          user1.test My_Group >> OK

-          user2.test My_Group >> OK

 

Here it is:

 

# /usr/lib64/squid/ext_ldap_group_acl -d -R -b "dc=my,dc=domain" -D
"squid at my.domain <mailto:squid at my.domain> " -W /etc/squid/ldappass.txt -F
"(sAMAccountName=%s)" -f "(memberof=CN=%g,DC=my,DC=domain)" -h
sv-102-dc.my.domain

user1.test asdf

ext_ldap_group_acl.cc(589): pid=194302 :Connected OK

ext_ldap_group_acl.cc(772): pid=194302 :user filter
'(sAMAccountName=user1.test)', searchbase 'dc=my,dc=domain'

ext_ldap_group_acl.cc(736): pid=194302 :group filter
'(memberof=CN=asdf,DC=my,DC=domain)', searchbase 'dc=my,dc=domain'

ERR

user1.test My_Group

ext_ldap_group_acl.cc(589): pid=194302 :Connected OK

ext_ldap_group_acl.cc(772): pid=194302 :user filter
'(sAMAccountName=user1.test)', searchbase 'dc=my,dc=domain'

ext_ldap_group_acl.cc(736): pid=194302 :group filter '(memberof=CN=My_Group,
DC=my,DC=domain)', searchbase 'dc=my,DC=domain'

OK

user2.test My_Group

ext_ldap_group_acl.cc(589): pid=194302 :Connected OK

ext_ldap_group_acl.cc(772): pid=194302 :user filter
'(sAMAccountName=user2.test)', searchbase 'dc=my,dc=domain'

ext_ldap_group_acl.cc(736): pid=194302 :group filter '(memberof=CN=My_Group,
DC=my,DC=domain)', searchbase 'dc=my,DC=domain'

OK

 

My env:

# uname -rms

Linux 4.18.0-193.14.2.el8_2.x86_64 x86_64

# rpm -qa | grep squid

squid-4.4-8.module_el8.2.0+319+d18e041f.1.x86_64

 

Could any kind soul help me out?

 

Thank you and best regards.

Robi

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200810/f03fa0cf/attachment.htm>

From roberto.nunnari at edu.ti.ch  Mon Aug 10 08:43:37 2020
From: roberto.nunnari at edu.ti.ch (Roberto Nunnari)
Date: Mon, 10 Aug 2020 10:43:37 +0200
Subject: [squid-users] Basic explanation on configuration
Message-ID: <01b101d66ef2$572630f0$057292d0$@edu.ti.ch>

Hello.

 

I need to build a new linux server with squid to replace an old one.

The old server is running squid version 3.3.8 and authenticates against
Active Directory. In the conf I see ldap, ntlm, kerberos and negotiator +
wbinfo.

 

The new server is running squid version 4.4.8. I'm trying to keep it simple
and keep the conf file clean.

That's why for authentication and authorization I try to use only
basic_ldap_auth and ext_ldap_group_acl.

 

I would like to understand the basics of squid.conf but I find the online
documentation is missing the basics.. for instance I believe the acl
directive uses logical 'and' when using multiple values on the same line,
and uses logical 'or' when using multiple lines for the same acl name..

 

That is something it should be written clear in the documentation. Maybe it
is somewhere, but I could not find that information.

 

Same for http_access.. how does it works? What happens when the first match
is found? It applies the rule and exits or it goes on to the next lines?

 

What I need to implement is more or less this :

 

1)      Every user needs to provide valid username and password (from AD).

2)      Users who belongs to a given AD group, can go on and access the
internet

3)      Other users need to be inside a file. If they are found in that
file, they can access the internet

4)      Some websites are accessible without being in group 2) or in file 3)

5)      Some websites are forbidden for everybody

6)      Some websites are allowed only for users in group 2)

 

I'll appreciate some help.

 

Thank you and best regards.

Robi

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200810/fb6a81d2/attachment.htm>

From roberto.nunnari at edu.ti.ch  Mon Aug 10 09:51:55 2020
From: roberto.nunnari at edu.ti.ch (Roberto Nunnari)
Date: Mon, 10 Aug 2020 11:51:55 +0200
Subject: [squid-users] R:  Basic explanation on configuration
In-Reply-To: <01b101d66ef2$572630f0$057292d0$@edu.ti.ch>
References: <01b101d66ef2$572630f0$057292d0$@edu.ti.ch>
Message-ID: <01d001d66efb$e1cec740$a56c55c0$@edu.ti.ch>

Hello.

 

In the previous message I forgot to include what I did till now in
squid.conf (edited to replace sensitive information).

In part from default conf and in part from old installation and in part
adapted to my needs.

 

Thank you and best regards.

Robi

 

 

Da: squid-users <squid-users-bounces at lists.squid-cache.org> Per conto di
Roberto Nunnari
Inviato: luned?, 10 agosto 2020 10:44
A: squid-users at lists.squid-cache.org
Oggetto: [squid-users] Basic explanation on configuration

 

Hello.

 

I need to build a new linux server with squid to replace an old one.

The old server is running squid version 3.3.8 and authenticates against
Active Directory. In the conf I see ldap, ntlm, kerberos and negotiator +
wbinfo.

 

The new server is running squid version 4.4.8. I?m trying to keep it simple
and keep the conf file clean.

That?s why for authentication and authorization I try to use only
basic_ldap_auth and ext_ldap_group_acl.

 

I would like to understand the basics of squid.conf but I find the online
documentation is missing the basics.. for instance I believe the acl
directive uses logical ?and? when using multiple values on the same line,
and uses logical ?or? when using multiple lines for the same acl name..

 

That is something it should be written clear in the documentation. Maybe it
is somewhere, but I could not find that information.

 

Same for http_access.. how does it works? What happens when the first match
is found? It applies the rule and exits or it goes on to the next lines?

 

What I need to implement is more or less this :

 

1)      Every user needs to provide valid username and password (from AD).

2)      Users who belongs to a given AD group, can go on and access the
internet

3)      Other users need to be inside a file. If they are found in that
file, they can access the internet

4)      Some websites are accessible without being in group 2) or in file 3)

5)      Some websites are forbidden for everybody

6)      Some websites are allowed only for users in group 2)

 

I?ll appreciate some help.

 

Thank you and best regards.

Robi

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200810/386c97c4/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: out.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200810/386c97c4/attachment.txt>

From roberto.nunnari at edu.ti.ch  Mon Aug 10 16:01:39 2020
From: roberto.nunnari at edu.ti.ch (Roberto Nunnari)
Date: Mon, 10 Aug 2020 18:01:39 +0200
Subject: [squid-users] SOLVED:  ext_ldap_group_acl
Message-ID: <020b01d66f2f$888fcd80$99af6880$@edu.ti.ch>

Hello.

 

I just solved myself this problem. It was my mistake with the filters.

Here?s how it goes :

 

/usr/lib64/squid/ext_ldap_group_acl -R -b "dc=my,dc=domain" -D
"squid at my.domain" -W /etc/squid/ldappass.txt -f
"(&(sAMAccountName=%u)(memberof:1.2.840.113556.1.4.1941:=CN=%g,DC=my,dc=doma
in)(objectClass=user))" -h mydc.my.domain

 

That ?:1.2.840.113556.1.4.1941:? will cause a recursive lookup until it
finds a user. Useful when the user is not directly member of that group, but
is member of a group that is member of that group.

 

Best regards.

Robi

 

 

Da: squid-users <squid-users-bounces at lists.squid-cache.org> Per conto di
Roberto Nunnari
Inviato: luned?, 10 agosto 2020 10:41
A: squid-users at lists.squid-cache.org
Oggetto: [squid-users] ext_ldap_group_acl

 

Hello.

 

I?m setting up squid on a CentOS 8 server.

Authentication against active directory works well with basic_ldap_auth, but
I fail when trying to check that a user belongs to a group.

It seems to me that for ext_ldap_group_acl it?s enough that both the user
and the group exist and it returns OK. It returns ERR when it cannot find
the group or the user.

 

To make it more clear, here are the queries and results I get.

user1.test exists and is a member of group My_Group

user2.test exists and is NOT a member of group My_Group

Group asdf does NOT exist

 

So, I expect that when asking for

-          user1.test My_Group >> OK

-          user2.test My_Group >> ERR

But I get:

-          user1.test My_Group >> OK

-          user2.test My_Group >> OK

 

Here it is:

 

# /usr/lib64/squid/ext_ldap_group_acl -d -R -b "dc=my,dc=domain" -D
"squid at my.domain <mailto:squid at my.domain> " -W /etc/squid/ldappass.txt -F
"(sAMAccountName=%s)" -f "(memberof=CN=%g,DC=my,DC=domain)" -h
sv-102-dc.my.domain

user1.test asdf

ext_ldap_group_acl.cc(589): pid=194302 :Connected OK

ext_ldap_group_acl.cc(772): pid=194302 :user filter
'(sAMAccountName=user1.test)', searchbase 'dc=my,dc=domain'

ext_ldap_group_acl.cc(736): pid=194302 :group filter
'(memberof=CN=asdf,DC=my,DC=domain)', searchbase 'dc=my,dc=domain'

ERR

user1.test My_Group

ext_ldap_group_acl.cc(589): pid=194302 :Connected OK

ext_ldap_group_acl.cc(772): pid=194302 :user filter
'(sAMAccountName=user1.test)', searchbase 'dc=my,dc=domain'

ext_ldap_group_acl.cc(736): pid=194302 :group filter '(memberof=CN=My_Group,
DC=my,DC=domain)', searchbase 'dc=my,DC=domain'

OK

user2.test My_Group

ext_ldap_group_acl.cc(589): pid=194302 :Connected OK

ext_ldap_group_acl.cc(772): pid=194302 :user filter
'(sAMAccountName=user2.test)', searchbase 'dc=my,dc=domain'

ext_ldap_group_acl.cc(736): pid=194302 :group filter '(memberof=CN=My_Group,
DC=my,DC=domain)', searchbase 'dc=my,DC=domain'

OK

 

My env:

# uname -rms

Linux 4.18.0-193.14.2.el8_2.x86_64 x86_64

# rpm -qa | grep squid

squid-4.4-8.module_el8.2.0+319+d18e041f.1.x86_64

 

Could any kind soul help me out?

 

Thank you and best regards.

Robi

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200810/d445ec50/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 10 22:46:36 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Aug 2020 10:46:36 +1200
Subject: [squid-users] Basic explanation on configuration
In-Reply-To: <01b101d66ef2$572630f0$057292d0$@edu.ti.ch>
References: <01b101d66ef2$572630f0$057292d0$@edu.ti.ch>
Message-ID: <cff11910-ffbc-f55d-03db-dd745485e4f4@treenet.co.nz>

On 10/08/20 8:43 pm, Roberto Nunnari wrote:
> Hello.
> 
> ?
> 
> I need to build a new linux server with squid to replace an old one.
> 
> The old server is running squid version 3.3.8 and authenticates against
> Active Directory. In the conf I see ldap, ntlm, kerberos and negotiator
> + wbinfo.
> 
> ?
> 
> The new server is running squid version 4.4.8. I?m trying to keep it
> simple and keep the conf file clean.
> 
> That?s why for authentication and authorization I try to use only
> basic_ldap_auth and ext_ldap_group_acl.
> 
> ?
> 
> I would like to understand the basics of squid.conf but I find the
> online documentation is missing the basics.. for instance I believe the
> acl directive uses logical ?and? when using multiple values on the same
> line, and uses logical ?or? when using multiple lines for the same acl
> name..
> 


Which part of the online documentation are you looking at?

On the official website (<http://www.squid-cache.org/>) menu under
"Documentation" we have several sources:

 * Reference guide - for detailed description of a specific directive if
you are needing reminder of usage or specific details of its operation.

 * Examples - how-to config snippets for common installation needs.

 * Books for learning Squid; beginners guide, and expert reference.

 * FAQ and Wiki for more up to date alternative to the books.


> 
> That is something it should be written clear in the documentation. Maybe
> it is somewhere, but I could not find that information.
> 

 <https://wiki.squid-cache.org/SquidFaq/SquidAcl#And.2FOr_logic>


> 
> Same for http_access.. how does it works? What happens when the first
> match is found? It applies the rule and exits or it goes on to the next
> lines?
> 

<https://wiki.squid-cache.org/SquidFaq/SquidAcl#Access_Lists>


> 
> What I need to implement is more or less this?:
> 
> ?

> 5)      Some websites are forbidden for everybody

  acl blacklist dstdomain ...
  http_access deny blacklist


> 
> 1)????? Every user needs to provide valid username and password (from AD).
> 

 auth_param ...

 acl login proxy_auth REQUIRED
 http_access deny !login


> 4)      Some websites are accessible without being in group 2) or in
file 3)
>

  acl whitelist dstdomain ...
  http_access allow whitelist


> 2)????? Users who belongs to a given AD group, can go on and access the
> internet
> 

  external_acl_type groups ...

  acl groupCheck external groupName
  http_access allow groupCheck


> > 6)      Some websites are allowed only for users in group 2)

  acl forbidOthers dstdomain ...


> 3)????? Other users need to be inside a file. If they are found in that
> file, they can access the internet
> 

  acl otherUsers proxy_auth parameters("/etc/squid/usernames_allowed")
  http_access allow !forbidOthers otherUsers

  http_accss deny all


Note the order of policy enforcement. Deny as much as possible first,
allow later. Faster ACL types first whenever possible.

Amos


From lhumphries.cw at mmm.com  Tue Aug 11 06:02:18 2020
From: lhumphries.cw at mmm.com (Leonard Humphries CW)
Date: Tue, 11 Aug 2020 06:02:18 +0000
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?
Message-ID: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>

I have a task of STIGing Squid on CentOS7.  Does anyone have recommended STIG checklists or SRG's for Squid on CentOS7? Also, It is my understanding that if Squid isn't utilizing caching , then it might be better to use the Application SRG instead of the Webserver SRG. Does anyone have any insight to this?

Leonard Humphries

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200811/e40a909a/attachment.htm>

From ngtech1ltd at gmail.com  Tue Aug 11 06:58:16 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Tue, 11 Aug 2020 09:58:16 +0300
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?
In-Reply-To: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>
References: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>
Message-ID: <004701d66fac$ca970040$5fc500c0$@gmail.com>

Hey Leonard,

 

Can you clarify what do you mean by STIGing and SRG etc..

What are you trying to achieve?

Plain text might make more sense to these who doesn't understand these
terms.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Leonard Humphries CW
Sent: Tuesday, August 11, 2020 9:02 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?

 

I have a task of STIGing Squid on CentOS7.  Does anyone have recommended
STIG checklists or SRG's for Squid on CentOS7? Also, It is my understanding
that if Squid isn't utilizing caching , then it might be better to use the
Application SRG instead of the Webserver SRG. Does anyone have any insight
to this?

 

Leonard Humphries

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200811/00d96998/attachment.htm>

From lhumphries.cw at mmm.com  Tue Aug 11 07:11:01 2020
From: lhumphries.cw at mmm.com (Leonard Humphries CW)
Date: Tue, 11 Aug 2020 07:11:01 +0000
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?
In-Reply-To: <004701d66fac$ca970040$5fc500c0$@gmail.com>
References: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>,
 <004701d66fac$ca970040$5fc500c0$@gmail.com>
Message-ID: <8EDDD41A-1FAA-4A3E-BFBE-B6B029B06333@mmm.com>

STIG stands for Secure Technical Implementation Guide. It?s the standard  by which the DoD and other government entities measure whether a system, application, etc is in compliance with their protocols. SRG stands for Security Requirements Guides. They are both way for implementing security changes to your systems to keep them secure and compliant.

Leonard Humphries


On Aug 11, 2020, at 2:58 AM, Eliezer Croitor <ngtech1ltd at gmail.com> wrote:

?
Hey Leonard,

Can you clarify what do you mean by STIGing and SRG etc..
What are you trying to achieve?
Plain text might make more sense to these who doesn?t understand these terms.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com<mailto:ngtech1ltd at gmail.com>

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Leonard Humphries CW
Sent: Tuesday, August 11, 2020 9:02 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?

I have a task of STIGing Squid on CentOS7.  Does anyone have recommended STIG checklists or SRG?s for Squid on CentOS7? Also, It is my understanding that if Squid isn?t utilizing caching , then it might be better to use the Application SRG instead of the Webserver SRG. Does anyone have any insight to this?

Leonard Humphries

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200811/23ec9937/attachment.htm>

From ngtech1ltd at gmail.com  Tue Aug 11 07:27:41 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Tue, 11 Aug 2020 10:27:41 +0300
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?
In-Reply-To: <8EDDD41A-1FAA-4A3E-BFBE-B6B029B06333@mmm.com>
References: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>,
 <004701d66fac$ca970040$5fc500c0$@gmail.com>
 <8EDDD41A-1FAA-4A3E-BFBE-B6B029B06333@mmm.com>
Message-ID: <006001d66fb0$e6720e50$b3562af0$@gmail.com>

OK, so..

The protocols 100% require a proper QA for something.

Currently from what I understand the Squid-Cache project doesn?t take ?fast steps? since it?s trying to be RFC compatible.

Also the project has couple guide lines about new features so it?s striving to stay as ?Stable? as possible.

 

I think that for CentOS X there should be a proper packaging and a proper way to be able to roll forward and back versions.

(since packaging for CentOS 7 is kind of my domain for a while)

 

I do not what are the requirements are there for a system to stay compliance with regulation but my assumption is that a basic:

Configuration ?code? management should be in place.

Any other hints?

 

Elileze

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: Leonard Humphries CW <lhumphries.cw at mmm.com> 
Sent: Tuesday, August 11, 2020 10:11 AM
To: Eliezer Croitor <ngtech1ltd at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: RE: [squid-users] WebServer-SRG or Application SRG for Squid?

 

STIG stands for Secure Technical Implementation Guide. It?s the standard  by which the DoD and other government entities measure whether a system, application, etc is in compliance with their protocols. SRG stands for Security Requirements Guides. They are both way for implementing security changes to your systems to keep them secure and compliant. 

Leonard Humphries 

 





On Aug 11, 2020, at 2:58 AM, Eliezer Croitor <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > wrote:

? 

Hey Leonard,

 

Can you clarify what do you mean by STIGing and SRG etc..

What are you trying to achieve?

Plain text might make more sense to these who doesn?t understand these terms.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of Leonard Humphries CW
Sent: Tuesday, August 11, 2020 9:02 AM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?

 

I have a task of STIGing Squid on CentOS7.  Does anyone have recommended STIG checklists or SRG?s for Squid on CentOS7? Also, It is my understanding that if Squid isn?t utilizing caching , then it might be better to use the Application SRG instead of the Webserver SRG. Does anyone have any insight to this?

 

Leonard Humphries

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200811/92e9d9b6/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 11 11:26:12 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Aug 2020 23:26:12 +1200
Subject: [squid-users] WebServer-SRG or Application SRG for Squid?
In-Reply-To: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>
References: <BY5PR03MB5111CBCEED73CE12FEC8108BEB450@BY5PR03MB5111.namprd03.prod.outlook.com>
Message-ID: <8ce69e59-b1ce-1c1c-ceed-a8fb7877d4ab@treenet.co.nz>

On 11/08/20 6:02 pm, Leonard Humphries CW wrote:
> I have a task of STIGing Squid on CentOS7.? Does anyone have recommended
> STIG checklists or SRG?s for Squid on CentOS7?

Squid is cross-platform software. So guides for Squid configuration will
be platform agnostic.

Guides for securing the OS environment will be OS-specific, but that is
not restricted to Squid. Any web accessible or accessing software will
likely share the same requirements and solutions.


> Also, It is my
> understanding that if Squid isn?t utilizing caching , then it might be
> better to use the Application SRG instead of the Webserver SRG. Does
> anyone have any insight to this?
> 

I think that is incorrect. Without knowing the specific SRG you are
talking about its unclear which you should be using - but either way
caching should not be the factor to decide between them.


Amos


From squid at sdeziel.info  Tue Aug 11 21:24:47 2020
From: squid at sdeziel.info (Simon Deziel)
Date: Tue, 11 Aug 2020 17:24:47 -0400
Subject: [squid-users] CVE-2019-12522
Message-ID: <aa44eeef-36e2-c931-14ba-11cc2d947988@sdeziel.info>

Hello,

I noticed that CVE-2019-12522 [*] was not yet fixed. I could confirm the
saved UID is indeed 0 (root) on a Ubuntu 20.04.1 machine (5.4 kernel) so
I was wondering if a fix was on the way. Thanks

Regards,
Simon

*:
https://gitlab.com/jeriko.one/security/-/blob/master/squid/CVEs/CVE-2019-12522.txt


From squid3 at treenet.co.nz  Wed Aug 12 03:00:15 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Aug 2020 15:00:15 +1200
Subject: [squid-users] CVE-2019-12522
In-Reply-To: <aa44eeef-36e2-c931-14ba-11cc2d947988@sdeziel.info>
References: <aa44eeef-36e2-c931-14ba-11cc2d947988@sdeziel.info>
Message-ID: <0faa47ca-e128-690b-b3cc-2f6948a66162@treenet.co.nz>

On 12/08/20 9:24 am, Simon Deziel wrote:
> Hello,
> 
> I noticed that CVE-2019-12522 [*] was not yet fixed. I could confirm the
> saved UID is indeed 0 (root) on a Ubuntu 20.04.1 machine (5.4 kernel) so
> I was wondering if a fix was on the way. Thanks
> 

We do not have an ETA on this issue. Risk is relatively low and several
features of Squid require the capability this allows in order to
reconfigure. So we will not be implementing the quick fix of fully
dropping root.


Amos


From squid at sdeziel.info  Wed Aug 12 11:54:09 2020
From: squid at sdeziel.info (Simon Deziel)
Date: Wed, 12 Aug 2020 07:54:09 -0400
Subject: [squid-users] CVE-2019-12522
In-Reply-To: <0faa47ca-e128-690b-b3cc-2f6948a66162@treenet.co.nz>
References: <aa44eeef-36e2-c931-14ba-11cc2d947988@sdeziel.info>
 <0faa47ca-e128-690b-b3cc-2f6948a66162@treenet.co.nz>
Message-ID: <b59d45be-1b25-6233-b131-4f41f72c303c@sdeziel.info>

On 2020-08-11 11:00 p.m., Amos Jeffries wrote:
> On 12/08/20 9:24 am, Simon Deziel wrote:
>> Hello,
>>
>> I noticed that CVE-2019-12522 [*] was not yet fixed. I could confirm the
>> saved UID is indeed 0 (root) on a Ubuntu 20.04.1 machine (5.4 kernel) so
>> I was wondering if a fix was on the way. Thanks
>>
> 
> We do not have an ETA on this issue. Risk is relatively low and several
> features of Squid require the capability this allows in order to
> reconfigure. So we will not be implementing the quick fix of fully
> dropping root.

OK, thanks for the quick feedback and explanations.

Regards,
Simon


From doctor at doctor.nl2k.ab.ca  Thu Aug 13 03:48:39 2020
From: doctor at doctor.nl2k.ab.ca (The Doctor)
Date: Wed, 12 Aug 2020 21:48:39 -0600
Subject: [squid-users] Speed compression
Message-ID: <20200813034839.GA52318@doctor.nl2k.ab.ca>

Is there a way to use speed compression in squid say 1000:1 or 1000000:1 ?

-- 
Member - Liberal International This is doctor@@nl2k.ab.ca Ici doctor@@nl2k.ab.ca
Yahweh, Queen & country!Never Satan President Republic!Beware AntiChrist rising!
https://www.empire.kred/ROOTNK?t=94a1f39b  
That man is the richest whose pleasures are the cheapest.  -Henry David Thoreau


From squid3 at treenet.co.nz  Thu Aug 13 03:47:13 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2020 15:47:13 +1200
Subject: [squid-users] Speed compression
In-Reply-To: <20200813034839.GA52318@doctor.nl2k.ab.ca>
References: <20200813034839.GA52318@doctor.nl2k.ab.ca>
Message-ID: <9c843137-2de1-ad6e-833e-6ef866a81e45@treenet.co.nz>

On 13/08/20 3:48 pm, The Doctor wrote:
> Is there a way to use speed compression in squid say 1000:1 or 1000000:1 ?
> 

Er, what are you asking about?


Amos


From doctor at doctor.nl2k.ab.ca  Thu Aug 13 04:02:49 2020
From: doctor at doctor.nl2k.ab.ca (The Doctor)
Date: Wed, 12 Aug 2020 22:02:49 -0600
Subject: [squid-users] Speed compression
In-Reply-To: <9c843137-2de1-ad6e-833e-6ef866a81e45@treenet.co.nz>
References: <20200813034839.GA52318@doctor.nl2k.ab.ca>
 <9c843137-2de1-ad6e-833e-6ef866a81e45@treenet.co.nz>
Message-ID: <20200813040249.GA53973@doctor.nl2k.ab.ca>

On Thu, Aug 13, 2020 at 03:47:13PM +1200, Amos Jeffries wrote:
> On 13/08/20 3:48 pm, The Doctor wrote:
> > Is there a way to use speed compression in squid say 1000:1 or 1000000:1 ?
> > 
> 
> Er, what are you asking about?
>

I am asking if squid users can benefit from 1000:1 or 1000000:1 speed compression.

> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Member - Liberal International This is doctor@@nl2k.ab.ca Ici doctor@@nl2k.ab.ca
Yahweh, Queen & country!Never Satan President Republic!Beware AntiChrist rising!
https://www.empire.kred/ROOTNK?t=94a1f39b  
That man is the richest whose pleasures are the cheapest.  -Henry David Thoreau


From squid3 at treenet.co.nz  Thu Aug 13 05:02:19 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Aug 2020 17:02:19 +1200
Subject: [squid-users] Speed compression
In-Reply-To: <20200813040249.GA53973@doctor.nl2k.ab.ca>
References: <20200813034839.GA52318@doctor.nl2k.ab.ca>
 <9c843137-2de1-ad6e-833e-6ef866a81e45@treenet.co.nz>
 <20200813040249.GA53973@doctor.nl2k.ab.ca>
Message-ID: <39964a2a-ed19-14a5-a924-04343d3f8ba2@treenet.co.nz>

On 13/08/20 4:02 pm, The Doctor wrote:
> On Thu, Aug 13, 2020 at 03:47:13PM +1200, Amos Jeffries wrote:
>> On 13/08/20 3:48 pm, The Doctor wrote:
>>> Is there a way to use speed compression in squid say 1000:1 or 1000000:1 ?
>>>
>>
>> Er, what are you asking about?
>>
> 
> I am asking if squid users can benefit from 1000:1 or 1000000:1 speed compression.
> 

What is "speed compression"?


Amos


From mabi at protonmail.ch  Thu Aug 13 10:24:43 2020
From: mabi at protonmail.ch (mabi)
Date: Thu, 13 Aug 2020 10:24:43 +0000
Subject: [squid-users] Chrome not working anymore since 4.12 release
Message-ID: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>

Hello,

I am using Squid as transparent proxy for HTTP and HTTPS traffic on an  OpenBSD 6.7 firewall. Since I have upgraded from Squid version 4.11 to 4.12 my Chrome browser is not able to visit any HTTPS websites. Firefox continues to work fine.

Older versions of Chrome show a ERR_SSL_VERSION_INTERFERENCE error and newer versions of Chrome show a ERR_CONNECTION_REFUSED error.

If I revert back to Squid 4.11 Chrome works again. I have not changed anything in my Squid config or any other configuration file of the firewall.

I believe this issue might have something to do with the following change introduced in 4.12:

 2020-05-06 10:09:50 +0300	Christos Tsantilas	+199 -28		SslBump: Disable OpenSSL TLSv1.3 support for older TLS traffic (#620)

Source: http://www.squid-cache.org/Versions/v4/changesets/SQUID_4_12.html

Thank you in advance for your feedback.

Best regards,
Mabi





From eperez at quadrianweb.com  Thu Aug 13 12:54:58 2020
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Thu, 13 Aug 2020 07:54:58 -0500
Subject: [squid-users] Chrome not working anymore since 4.12 release
In-Reply-To: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
References: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
Message-ID: <CACXMG+s3R8DfhV8H+rKqbZUy3DsxX_H87cBjjsqQsKX3nzw8TA@mail.gmail.com>

Just a quick question Mabi,
in chrome address bar please type chrome://flags/#tls13-variant
what tells you about TLS and QUIC?
do you have any nonstandard value?
By any chance can you share the  (potentially) offending website?
thanks.

On Thu, Aug 13, 2020 at 5:25 AM mabi <mabi at protonmail.ch> wrote:

> Hello,
>
> I am using Squid as transparent proxy for HTTP and HTTPS traffic on an
> OpenBSD 6.7 firewall. Since I have upgraded from Squid version 4.11 to 4.12
> my Chrome browser is not able to visit any HTTPS websites. Firefox
> continues to work fine.
>
> Older versions of Chrome show a ERR_SSL_VERSION_INTERFERENCE error and
> newer versions of Chrome show a ERR_CONNECTION_REFUSED error.
>
> If I revert back to Squid 4.11 Chrome works again. I have not changed
> anything in my Squid config or any other configuration file of the firewall.
>
> I believe this issue might have something to do with the following change
> introduced in 4.12:
>
>  2020-05-06 10:09:50 +0300      Christos Tsantilas      +199 -28
>       SslBump: Disable OpenSSL TLSv1.3 support for older TLS traffic (#620)
>
> Source: http://www.squid-cache.org/Versions/v4/changesets/SQUID_4_12.html
>
> Thank you in advance for your feedback.
>
> Best regards,
> Mabi
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

---------------------
Erick Perez
Quadrian Enterprises S.A. - Panama, Republica de Panama
Skype chat: eaperezh
WhatsApp IM: +507-6675-5083
---------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200813/24ac6f65/attachment.htm>

From mabi at protonmail.ch  Thu Aug 13 13:06:01 2020
From: mabi at protonmail.ch (mabi)
Date: Thu, 13 Aug 2020 13:06:01 +0000
Subject: [squid-users] Chrome not working anymore since 4.12 release
In-Reply-To: <CACXMG+s3R8DfhV8H+rKqbZUy3DsxX_H87cBjjsqQsKX3nzw8TA@mail.gmail.com>
References: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
 <CACXMG+s3R8DfhV8H+rKqbZUy3DsxX_H87cBjjsqQsKX3nzw8TA@mail.gmail.com>
Message-ID: <ZM6ELYWnwIVHV2dM4oGWJibvKBzO12qes12buDgCtRo12yv9aQk1OYFlQI53HcblYsvJUdLxnorYKchIR-f1UFqYTOND2QIgVKPRC_pZEs0=@protonmail.ch>

Dear Erick,

Thank you for your answer.

I tried accessing chrome://flags/#tls13-variant but nothing appears. If I search for a flag containing TLS the only one is #enforce-tls13-downgrade. This is my Chrome version:

Version 73.0.3683.75 (Developer Build) built on Debian 9.8, running on Debian 9.13 (64-bit)

It's not a specific website the problem it is all websites using HTTPS.

Best regards,
Mabi

??????? Original Message ???????
On Thursday, August 13, 2020 2:54 PM, Erick Perez - Quadrian Enterprises <eperez at quadrianweb.com> wrote:

> Just a quick question Mabi,
> in chrome address bar please type chrome://flags/#tls13-variant
> what tells you about TLS and QUIC?
> do you have any nonstandard value?
> By any chance can you share the (potentially) offending website?
> thanks.
>
> On Thu, Aug 13, 2020 at 5:25 AM mabi <mabi at protonmail.ch> wrote:
>
>> Hello,
>>
>> I am using Squid as transparent proxy for HTTP and HTTPS traffic on an OpenBSD 6.7 firewall. Since I have upgraded from Squid version 4.11 to 4.12 my Chrome browser is not able to visit any HTTPS websites. Firefox continues to work fine.
>>
>> Older versions of Chrome show a ERR_SSL_VERSION_INTERFERENCE error and newer versions of Chrome show a ERR_CONNECTION_REFUSED error.
>>
>> If I revert back to Squid 4.11 Chrome works again. I have not changed anything in my Squid config or any other configuration file of the firewall.
>>
>> I believe this issue might have something to do with the following change introduced in 4.12:
>>
>> 2020-05-06 10:09:50 +0300 Christos Tsantilas +199 -28 SslBump: Disable OpenSSL TLSv1.3 support for older TLS traffic (#620)
>>
>> Source: http://www.squid-cache.org/Versions/v4/changesets/SQUID_4_12.html
>>
>> Thank you in advance for your feedback.
>>
>> Best regards,
>> Mabi
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> --
>
> ---------------------
> Erick Perez
> Quadrian Enterprises S.A. - Panama, Republica de Panama
> Skype chat: eaperezh
> WhatsApp IM: +507-6675-5083
> ---------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200813/e4b5b092/attachment.htm>

From rousskov at measurement-factory.com  Thu Aug 13 13:13:31 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Aug 2020 09:13:31 -0400
Subject: [squid-users] Chrome not working anymore since 4.12 release
In-Reply-To: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
References: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
Message-ID: <544862b3-ae96-8cfc-5139-df2bb1a78d80@measurement-factory.com>

On 8/13/20 6:24 AM, mabi wrote:

> Since I have upgraded from Squid version 4.11 to 4.12 my Chrome
> browser is not able to visit any HTTPS websites. Firefox continues to
> work fine.

You probably need a fix for TLS GREASEd values. Here is its v4 flavor:

    https://github.com/squid-cache/squid/commit/f7a315f.patch

Alex.


From roberto.nunnari at edu.ti.ch  Thu Aug 13 15:00:14 2020
From: roberto.nunnari at edu.ti.ch (Roberto Nunnari)
Date: Thu, 13 Aug 2020 17:00:14 +0200
Subject: [squid-users] R:  Basic explanation on configuration
In-Reply-To: <cff11910-ffbc-f55d-03db-dd745485e4f4@treenet.co.nz>
References: <01b101d66ef2$572630f0$057292d0$@edu.ti.ch>
 <cff11910-ffbc-f55d-03db-dd745485e4f4@treenet.co.nz>
Message-ID: <013801d67182$7379fd30$5a6df790$@edu.ti.ch>

Thank you for your precious help, Amos.
It was very helpful. :-)

Best regards.
Robi


-----Messaggio originale-----
Da: squid-users <squid-users-bounces at lists.squid-cache.org> Per conto di Amos Jeffries
Inviato: marted?, 11 agosto 2020 00:47
A: squid-users at lists.squid-cache.org
Oggetto: Re: [squid-users] Basic explanation on configuration

On 10/08/20 8:43 pm, Roberto Nunnari wrote:
> Hello.
> 
>  
> 
> I need to build a new linux server with squid to replace an old one.
> 
> The old server is running squid version 3.3.8 and authenticates 
> against Active Directory. In the conf I see ldap, ntlm, kerberos and 
> negotiator
> + wbinfo.
> 
>  
> 
> The new server is running squid version 4.4.8. I?m trying to keep it 
> simple and keep the conf file clean.
> 
> That?s why for authentication and authorization I try to use only 
> basic_ldap_auth and ext_ldap_group_acl.
> 
>  
> 
> I would like to understand the basics of squid.conf but I find the 
> online documentation is missing the basics.. for instance I believe 
> the acl directive uses logical ?and? when using multiple values on the 
> same line, and uses logical ?or? when using multiple lines for the 
> same acl name..
> 


Which part of the online documentation are you looking at?

On the official website (<http://www.squid-cache.org/>) menu under "Documentation" we have several sources:

 * Reference guide - for detailed description of a specific directive if you are needing reminder of usage or specific details of its operation.

 * Examples - how-to config snippets for common installation needs.

 * Books for learning Squid; beginners guide, and expert reference.

 * FAQ and Wiki for more up to date alternative to the books.


> 
> That is something it should be written clear in the documentation. 
> Maybe it is somewhere, but I could not find that information.
> 

 <https://wiki.squid-cache.org/SquidFaq/SquidAcl#And.2FOr_logic>


> 
> Same for http_access.. how does it works? What happens when the first 
> match is found? It applies the rule and exits or it goes on to the 
> next lines?
> 

<https://wiki.squid-cache.org/SquidFaq/SquidAcl#Access_Lists>


> 
> What I need to implement is more or less this :
> 
>  

> 5)      Some websites are forbidden for everybody

  acl blacklist dstdomain ...
  http_access deny blacklist


> 
> 1)      Every user needs to provide valid username and password (from AD).
> 

 auth_param ...

 acl login proxy_auth REQUIRED
 http_access deny !login


> 4)      Some websites are accessible without being in group 2) or in
file 3)
>

  acl whitelist dstdomain ...
  http_access allow whitelist


> 2)      Users who belongs to a given AD group, can go on and access 
> the internet
> 

  external_acl_type groups ...

  acl groupCheck external groupName
  http_access allow groupCheck


> > 6)      Some websites are allowed only for users in group 2)

  acl forbidOthers dstdomain ...


> 3)      Other users need to be inside a file. If they are found in 
> that file, they can access the internet
> 

  acl otherUsers proxy_auth parameters("/etc/squid/usernames_allowed")
  http_access allow !forbidOthers otherUsers

  http_accss deny all


Note the order of policy enforcement. Deny as much as possible first, allow later. Faster ACL types first whenever possible.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From mabi at protonmail.ch  Thu Aug 13 15:01:46 2020
From: mabi at protonmail.ch (mabi)
Date: Thu, 13 Aug 2020 15:01:46 +0000
Subject: [squid-users] Chrome not working anymore since 4.12 release
In-Reply-To: <544862b3-ae96-8cfc-5139-df2bb1a78d80@measurement-factory.com>
References: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
 <544862b3-ae96-8cfc-5139-df2bb1a78d80@measurement-factory.com>
Message-ID: <vdX_c26JFk4fB0i_PYJGdPDTKFB4s3d78wAeY1vOPiBLXGWgV-qbaLtCqL3cXpAaC1C_L6e5aWXlU8QinKsWG2DEyr2tgQ_Bv0U1yIZjT5Y=@protonmail.ch>

??????? Original Message ???????
On Thursday, August 13, 2020 3:13 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:

> On 8/13/20 6:24 AM, mabi wrote:

> You probably need a fix for TLS GREASEd values. Here is its v4 flavor:
>
> https://github.com/squid-cache/squid/commit/f7a315f.patch

Thank you for the hint. Do you know is this patch will make it into version 4.13?


From squid3 at treenet.co.nz  Fri Aug 14 00:07:01 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Aug 2020 12:07:01 +1200
Subject: [squid-users] Chrome not working anymore since 4.12 release
In-Reply-To: <vdX_c26JFk4fB0i_PYJGdPDTKFB4s3d78wAeY1vOPiBLXGWgV-qbaLtCqL3cXpAaC1C_L6e5aWXlU8QinKsWG2DEyr2tgQ_Bv0U1yIZjT5Y=@protonmail.ch>
References: <OobwFE0nmekhVBKW_5c_ko_t7dmgCKr0dBpDjXID-B9G773QkPp2RoQK3VunEh3HS2UcJUyyta_cqKbuSx9ep8y5F5gorI0ZCVnIf6UeGq4=@protonmail.ch>
 <544862b3-ae96-8cfc-5139-df2bb1a78d80@measurement-factory.com>
 <vdX_c26JFk4fB0i_PYJGdPDTKFB4s3d78wAeY1vOPiBLXGWgV-qbaLtCqL3cXpAaC1C_L6e5aWXlU8QinKsWG2DEyr2tgQ_Bv0U1yIZjT5Y=@protonmail.ch>
Message-ID: <373431f7-3a39-9a08-81b3-6f2f3cf97602@treenet.co.nz>

On 14/08/20 3:01 am, mabi wrote:
> ??????? Original Message ???????
> On Thursday, August 13, 2020 3:13 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
>> On 8/13/20 6:24 AM, mabi wrote:
> 
>> You probably need a fix for TLS GREASEd values. Here is its v4 flavor:
>>
>> https://github.com/squid-cache/squid/commit/f7a315f.patch
> 
> Thank you for the hint. Do you know is this patch will make it into version 4.13?

Yes it will be there.

Amos


From tamurin0525 at gmail.com  Mon Aug 17 11:11:52 2020
From: tamurin0525 at gmail.com (m k)
Date: Mon, 17 Aug 2020 20:11:52 +0900
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <a1bf0203-4db2-b5f9-2a1e-679e53000fab@treenet.co.nz>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
 <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
 <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
 <a1bf0203-4db2-b5f9-2a1e-679e53000fab@treenet.co.nz>
Message-ID: <CAL-uOnG3V-izLFxO9Ry-=jNo01_==q0w_9c5ef=wvTx92dknhA@mail.gmail.com>

Hi all,

I built squid using SSL-bump. In addition, squid also authenticates users
with active directory. The hardware is openstack virtual. os is centos8.1.
There is one CPU. The memory is 16GB. The hard disk is SSD 200GB.

I'm thinking of load testing with Apache Jmeter in this environment.
I don't know the standard, so the test stops and I am in trouble.
How many simultaneous connection sessions?
How many requests per minute?
Help me.

thank you,
Kitamura

2020?8?7?(?) 8:53 Amos Jeffries <squid3 at treenet.co.nz>:

> On 6/08/20 5:28 pm, m k wrote:
>
> > Eliezer,
>
> >
>
> > Squid's default setting is 1 core CPU, 16GB mem.
>
> > How many URLs(Blacklist) will degrade Squid's performance?
>
> >
>
>
>
> Eliezer's answer covers that already, so I will skip here.
>
>
>
>
>
> > Also, SSL-Bump.
>
> >
>
>
>
> This is "unknown" - as far as I am aware none has published numbers
>
> recently about it. There are a lot of factors in the network traffic and
>
> your servers internal state (eg the RNG engine) that multiply up to
>
> cause varying amounts of delay - plus the volatile nature of this
>
> feature set itself month by month changes the effects or relevance of
>
> each factor.
>
>   So numbers from me today will be wrong in a few weeks, or may be wrong
>
> for your network already. All that we can be sure of is that there is
>
> extra work needed by Squid thus "slower" than plain-text HTTP is to be
>
> expected.
>
>
>
> For planning the consideration is just to be aware that the numbers we
>
> can give you (for plain-text) will be over-estimates of capacity for
>
> SSL-Bump traffic and allow some margins. Once you have an install
>
> running you can test and measure the actual numbers for your traffic.
>
>
>
>
>
> Amos
>
> _______________________________________________
>
> squid-users mailing list
>
> squid-users at lists.squid-cache.org
>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200817/c28cd3de/attachment.htm>

From rahul.negi at orange.com  Thu Aug 20 10:25:04 2020
From: rahul.negi at orange.com (rahul.negi at orange.com)
Date: Thu, 20 Aug 2020 10:25:04 +0000
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files for
 centos7 and x86_64 architecture
Message-ID: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>

Hi Team,

I am looking for a urgent support on squid latest version 4.12 RPM files based on CentOS7 and x86_64 architecture.

Also, I tried to install  it via source code and make it running but after every restart squid service is failing.

So, Could you please help me on the same.

Thanks and Regards,
Rahul Negi

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200820/09efb2ca/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Aug 20 10:47:35 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 20 Aug 2020 12:47:35 +0200
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
Message-ID: <202008201247.35836.Antony.Stone@squid.open.source.it>

On Thursday 20 August 2020 at 12:25:04, rahul.negi at orange.com wrote:

> Hi Team,
> 
> I am looking for a urgent support on squid latest version 4.12 RPM files
> based on CentOS7 and x86_64 architecture.

"Urgent" is all very well, but we can't help until you tell us what the 
problem is.

> Also, I tried to install  it via source code and make it running but after
> every restart squid service is failing.

Please define "failing" - give us some information, otherwise we have no clue 
what you did or what the results are.

> So, Could you please help me on the same.

Only if you help us - tell us what you've done, what the problem is, and what 
information you have in the log files etc.


Antony.

-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rahul.negi at orange.com  Thu Aug 20 10:52:14 2020
From: rahul.negi at orange.com (rahul.negi at orange.com)
Date: Thu, 20 Aug 2020 10:52:14 +0000
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <202008201247.35836.Antony.Stone@squid.open.source.it>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <202008201247.35836.Antony.Stone@squid.open.source.it>
Message-ID: <6724_1597920735_5F3E55DE_6724_395_3_BC8558EDF572F04D9DEAED25CCC290DB5B04F0@OPEXCNORM51.corporate.adroot.infra.ftgroup>

Hi Team,

Can anyone please share squid latest stable version 4.12 RPM packaged files for CentOS7  x86_64 architecture.

Thanks and Regards,
Rahul Negi

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Thursday, August 20, 2020 16:18
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need squid latest version 4.12 RPM packaged files for centos7 and x86_64 architecture

On Thursday 20 August 2020 at 12:25:04, rahul.negi at orange.com wrote:

> Hi Team,
> 
> I am looking for a urgent support on squid latest version 4.12 RPM files
> based on CentOS7 and x86_64 architecture.

"Urgent" is all very well, but we can't help until you tell us what the 
problem is.

> Also, I tried to install  it via source code and make it running but after
> every restart squid service is failing.

Please define "failing" - give us some information, otherwise we have no clue 
what you did or what the results are.

> So, Could you please help me on the same.

Only if you help us - tell us what you've done, what the problem is, and what 
information you have in the log files etc.


Antony.

-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.


From arsalan at preston.edu.pk  Thu Aug 20 11:10:56 2020
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Thu, 20 Aug 2020 16:10:56 +0500
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <6724_1597920735_5F3E55DE_6724_395_3_BC8558EDF572F04D9DEAED25CCC290DB5B04F0@OPEXCNORM51.corporate.adroot.infra.ftgroup>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <202008201247.35836.Antony.Stone@squid.open.source.it>
 <6724_1597920735_5F3E55DE_6724_395_3_BC8558EDF572F04D9DEAED25CCC290DB5B04F0@OPEXCNORM51.corporate.adroot.infra.ftgroup>
Message-ID: <CAMwDxM2yOUT5y3sNr0TSOCivijchWJvqGFiLni9ETqTTgL9x9w@mail.gmail.com>

Dear Rahul,

Try this source  , it is working for me

 http://rpmfind.net/linux/rpm2html/search.php?query=squid


On Thu, Aug 20, 2020 at 3:52 PM <rahul.negi at orange.com> wrote:

> Hi Team,
>
> Can anyone please share squid latest stable version 4.12 RPM packaged
> files for CentOS7  x86_64 architecture.
>
> Thanks and Regards,
> Rahul Negi
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Antony Stone
> Sent: Thursday, August 20, 2020 16:18
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Need squid latest version 4.12 RPM packaged
> files for centos7 and x86_64 architecture
>
> On Thursday 20 August 2020 at 12:25:04, rahul.negi at orange.com wrote:
>
> > Hi Team,
> >
> > I am looking for a urgent support on squid latest version 4.12 RPM files
> > based on CentOS7 and x86_64 architecture.
>
> "Urgent" is all very well, but we can't help until you tell us what the
> problem is.
>
> > Also, I tried to install  it via source code and make it running but
> after
> > every restart squid service is failing.
>
> Please define "failing" - give us some information, otherwise we have no
> clue
> what you did or what the results are.
>
> > So, Could you please help me on the same.
>
> Only if you help us - tell us what you've done, what the problem is, and
> what
> information you have in the log files etc.
>
>
> Antony.
>
> --
> "The problem with television is that the people must sit and keep their
> eyes
> glued on a screen; the average American family hasn't time for it."
>
>  - New York Times, following a demonstration at the 1939 World's Fair.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _________________________________________________________________________________________________________________________
>
> Ce message et ses pieces jointes peuvent contenir des informations
> confidentielles ou privilegiees et ne doivent donc
> pas etre diffuses, exploites ou copies sans autorisation. Si vous avez
> recu ce message par erreur, veuillez le signaler
> a l'expediteur et le detruire ainsi que les pieces jointes. Les messages
> electroniques etant susceptibles d'alteration,
> Orange decline toute responsabilite si ce message a ete altere, deforme ou
> falsifie. Merci.
>
> This message and its attachments may contain confidential or privileged
> information that may be protected by law;
> they should not be distributed, used or copied without authorisation.
> If you have received this email in error, please notify the sender and
> delete this message and its attachments.
> As emails may be altered, Orange is not liable for messages that have been
> modified, changed or falsified.
> Thank you.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
With Regards,


*Arsalan Hussain*


*Complaining is finding faults, wisdom is finding solutions*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200820/04fe215b/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 20 11:11:40 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Aug 2020 23:11:40 +1200
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <6724_1597920735_5F3E55DE_6724_395_3_BC8558EDF572F04D9DEAED25CCC290DB5B04F0@OPEXCNORM51.corporate.adroot.infra.ftgroup>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <202008201247.35836.Antony.Stone@squid.open.source.it>
 <6724_1597920735_5F3E55DE_6724_395_3_BC8558EDF572F04D9DEAED25CCC290DB5B04F0@OPEXCNORM51.corporate.adroot.infra.ftgroup>
Message-ID: <95e693a1-d568-f9ed-6492-39fe15db25ed@treenet.co.nz>

On 20/08/20 10:52 pm, rahul.negi wrote:
> Hi Team,
> 
> Can anyone please share squid latest stable version 4.12 RPM packaged files for CentOS7  x86_64 architecture.
> 

As Anthony wrote in the mail you replied to ... Please provide actual
details of what is going on with your existing proxy.

There are many, many, many possible problems that a different binary
package will not solve.


Amos



> Thanks and Regards,
> Rahul Negi
> 
> -----Original Message-----
> From: Antony Stone
> 
> On Thursday 20 August 2020 at 12:25:04, rahul.negi wrote:
> 
>> Hi Team,
>>
>> I am looking for a urgent support on squid latest version 4.12 RPM files
>> based on CentOS7 and x86_64 architecture.
> 
> "Urgent" is all very well, but we can't help until you tell us what the 
> problem is.
> 
>> Also, I tried to install  it via source code and make it running but after
>> every restart squid service is failing.
> 
> Please define "failing" - give us some information, otherwise we have no clue 
> what you did or what the results are.
> 
>> So, Could you please help me on the same.
> 
> Only if you help us - tell us what you've done, what the problem is, and what 
> information you have in the log files etc.
> 
> 
> Antony.
> 


From panchal.santosh1986 at gmail.com  Thu Aug 20 19:41:20 2020
From: panchal.santosh1986 at gmail.com (santosh panchal)
Date: Fri, 21 Aug 2020 01:11:20 +0530
Subject: [squid-users] Can squid proxy pass the SMTP port 587
Message-ID: <CABi3FS=KxaCkLuXy71=tEjHZnt367+p9As-b-6NKNhHsTRAXzw@mail.gmail.com>

Hi Team

How to configure squid to pass my smtp traffic on port 587

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200821/d46733e9/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Aug 20 19:43:36 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 20 Aug 2020 21:43:36 +0200
Subject: [squid-users] Can squid proxy pass the SMTP port 587
In-Reply-To: <CABi3FS=KxaCkLuXy71=tEjHZnt367+p9As-b-6NKNhHsTRAXzw@mail.gmail.com>
References: <CABi3FS=KxaCkLuXy71=tEjHZnt367+p9As-b-6NKNhHsTRAXzw@mail.gmail.com>
Message-ID: <202008202143.36766.Antony.Stone@squid.open.source.it>

On Thursday 20 August 2020 at 21:41:20, santosh panchal wrote:

> Hi Team
> 
> How to configure squid to pass my smtp traffic on port 587

Install sendmail, exim, postfix or any other MTA of your choice and configure it 
to relay your outbound email.

Squid is not an MTA.


Antony.

-- 
The truth is rarely pure, and never simple.

 - Oscar Wilde

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rahul.negi at orange.com  Fri Aug 21 10:37:17 2020
From: rahul.negi at orange.com (rahul.negi at orange.com)
Date: Fri, 21 Aug 2020 10:37:17 +0000
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <202008201247.35836.Antony.Stone@squid.open.source.it>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <202008201247.35836.Antony.Stone@squid.open.source.it>
Message-ID: <23494_1598006239_5F3FA3DF_23494_40_1_BC8558EDF572F04D9DEAED25CCC290DB5B0802@OPEXCNORM51.corporate.adroot.infra.ftgroup>

Hi Team,

I tried installing squid version 4.12 using RPM files but only one package is failing while installation i.e. squid-debuginfo-4.12-1.el7.x86_64.rpm. The link to this package is as follow https://onedrive.live.com/?authkey=%21AFs60Exv3C4B%2DNI&cid=6AB28772521B8B88&id=6AB28772521B8B88%214671&parId=6AB28772521B8B88%214664&o=OneUp

Please find attached full report with logs regarding the error which I am getting while installing squid-debuginfo file.

Also, The squid service is active and operational anyhow irrespective of failure of squid-debuginfo package installation. Can anyone please tell me the consequence of it later if squid debuginfo package is not installed properly.

Moreover, For OpenSSL I used Perl Crypto OpenSSL package as follow perl-Crypt-OpenSSL-X509-1.803-4.el7.x86_64.rpm. But the output of squid -v command is showing different OpenSSL version as following "This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html"

Below is the full output of squid -v
[root at localhost squid]# squid -v
Squid Cache: Version 4.12
Service Name: squid

This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-security-cert-generators' '--enable-security-cert-validators' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

Please help me on above queries. Thanks for your support.

Regards,
Rahul Negi

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Thursday, August 20, 2020 16:18
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need squid latest version 4.12 RPM packaged files for centos7 and x86_64 architecture

On Thursday 20 August 2020 at 12:25:04, rahul.negi at orange.com wrote:

> Hi Team,
> 
> I am looking for a urgent support on squid latest version 4.12 RPM files
> based on CentOS7 and x86_64 architecture.

"Urgent" is all very well, but we can't help until you tell us what the 
problem is.

> Also, I tried to install  it via source code and make it running but after
> every restart squid service is failing.

Please define "failing" - give us some information, otherwise we have no clue 
what you did or what the results are.

> So, Could you please help me on the same.

Only if you help us - tell us what you've done, what the problem is, and what 
information you have in the log files etc.


Antony.

-- 
"The problem with television is that the people must sit and keep their eyes 
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: logs.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200821/750b4b26/attachment.txt>

From rahul.negi at orange.com  Fri Aug 21 11:54:32 2020
From: rahul.negi at orange.com (rahul.negi at orange.com)
Date: Fri, 21 Aug 2020 11:54:32 +0000
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <CAMwDxM2yOUT5y3sNr0TSOCivijchWJvqGFiLni9ETqTTgL9x9w@mail.gmail.com>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <202008201247.35836.Antony.Stone@squid.open.source.it>
 <6724_1597920735_5F3E55DE_6724_395_3_BC8558EDF572F04D9DEAED25CCC290DB5B04F0@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <CAMwDxM2yOUT5y3sNr0TSOCivijchWJvqGFiLni9ETqTTgL9x9w@mail.gmail.com>
Message-ID: <1493_1598010873_5F3FB5F9_1493_317_1_BC8558EDF572F04D9DEAED25CCC290DB5B0845@OPEXCNORM51.corporate.adroot.infra.ftgroup>

Hi Arsalan,

Below source link is not having squid 4.12 versioned RPM files for CentOS distribution and x86_64 architecture.

 http://rpmfind.net/linux/rpm2html/search.php?query=squid

Could you please share other sources where I can get them.

Best Regards,
Rahul Negi

From: Arsalan Hussain [mailto:arsalan at preston.edu.pk]
Sent: Thursday, August 20, 2020 16:41
To: NEGI Rahul TGI/OLN
Cc: Antony Stone; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need squid latest version 4.12 RPM packaged files for centos7 and x86_64 architecture

Dear Rahul,

Try this source  , it is working for me

 http://rpmfind.net/linux/rpm2html/search.php?query=squid


On Thu, Aug 20, 2020 at 3:52 PM <rahul.negi at orange.com<mailto:rahul.negi at orange.com>> wrote:
Hi Team,

Can anyone please share squid latest stable version 4.12 RPM packaged files for CentOS7  x86_64 architecture.

Thanks and Regards,
Rahul Negi

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org<mailto:squid-users-bounces at lists.squid-cache.org>] On Behalf Of Antony Stone
Sent: Thursday, August 20, 2020 16:18
To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Need squid latest version 4.12 RPM packaged files for centos7 and x86_64 architecture

On Thursday 20 August 2020 at 12:25:04, rahul.negi at orange.com<mailto:rahul.negi at orange.com> wrote:

> Hi Team,
>
> I am looking for a urgent support on squid latest version 4.12 RPM files
> based on CentOS7 and x86_64 architecture.

"Urgent" is all very well, but we can't help until you tell us what the
problem is.

> Also, I tried to install  it via source code and make it running but after
> every restart squid service is failing.

Please define "failing" - give us some information, otherwise we have no clue
what you did or what the results are.

> So, Could you please help me on the same.

Only if you help us - tell us what you've done, what the problem is, and what
information you have in the log files etc.


Antony.

--
"The problem with television is that the people must sit and keep their eyes
glued on a screen; the average American family hasn't time for it."

 - New York Times, following a demonstration at the 1939 World's Fair.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users


--
With Regards,

Arsalan Hussain

Complaining is finding faults, wisdom is finding solutions

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200821/51660a81/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 21 11:58:22 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Aug 2020 23:58:22 +1200
Subject: [squid-users] Need squid latest version 4.12 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <23494_1598006239_5F3FA3DF_23494_40_1_BC8558EDF572F04D9DEAED25CCC290DB5B0802@OPEXCNORM51.corporate.adroot.infra.ftgroup>
References: <15202_1597919104_5F3E4F80_15202_99_2_BC8558EDF572F04D9DEAED25CCC290DB5B04B4@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <202008201247.35836.Antony.Stone@squid.open.source.it>
 <23494_1598006239_5F3FA3DF_23494_40_1_BC8558EDF572F04D9DEAED25CCC290DB5B0802@OPEXCNORM51.corporate.adroot.infra.ftgroup>
Message-ID: <c761dda1-d041-0c43-a01e-e1a61fc279b3@treenet.co.nz>

On 21/08/20 10:37 pm, rahul.negi wrote:
> Hi Team,
> 
> I tried installing squid version 4.12 using RPM files but only one package is failing while installation i.e. squid-debuginfo-4.12-1.el7.x86_64.rpm. The link to this package is as follow https://onedrive.live.com/?authkey=%21AFs60Exv3C4B%2DNI&cid=6AB28772521B8B88&id=6AB28772521B8B88%214671&parId=6AB28772521B8B88%214664&o=OneUp
> 
> Please find attached full report with logs regarding the error which I am getting while installing squid-debuginfo file.
> 
> Also, The squid service is active and operational anyhow irrespective of failure of squid-debuginfo package installation. Can anyone please tell me the consequence of it later if squid debuginfo package is not installed properly.
> 


AIUI, that package should only contain debug symbols for the binary in
the matching "squid" package. If correct the only result of not having
it, or not installing it correctly, is that tools like gdb will not be
able to produce human readable stack traces from core dumps or a running
squid.

So, no harm there unless the restart issue is an segfault crash.


> Moreover, For OpenSSL I used Perl Crypto OpenSSL package as follow perl-Crypt-OpenSSL-X509-1.803-4.el7.x86_64.rpm. But the output of squid -v command is showing different OpenSSL version as following "This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html"
> 

That piece of squid -v output is produced by the libssl library itself.
So the version shown there is the one Squid found using your OS PATH and
such environment or openssl config settings.

We have had a few reports of this same issue happening with FIPS libssl
packages overriding non-FIPS library versions. Unfortunately none of the
earlier reporters have came back with any details - so the above info is
all I can provide.


HTH
Amos


From squid3 at treenet.co.nz  Sun Aug 23 08:16:59 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2020 20:16:59 +1200
Subject: [squid-users] [squid-announce] Squid 4.13 is available
Message-ID: <244d24d9-b255-3c69-9178-643385b57c11@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.13 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:

 * SQUID-2020:8 HTTP(S) Request Splitting
   (CVE-2020-15811)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the browser
cache and any downstream caches with content from an arbitrary
source.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-c7p8-xqhm-49wv>


 * SQUID-2020:9 Denial of Service processing Cache Digest Response
   (CVE pending allocation)

This problem allows a trusted peer to deliver to perform Denial
of Service by consuming all available CPU cycles on the machine
running Squid when handling a crafted Cache Digest response
message.

This attack is limited to Squid using cache_peer with cache
digests feature.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-vvj7-xjgq-g2jg>


 * SQUID-2020:10 HTTP(S) Request Smuggling
   (CVE-2020-15810)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the proxy
cache and any downstream caches with content from an arbitrary
source.


See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-3365-q9qx-f98m>


 * Bug 5051: Some collapsed revalidation responses never expire

This bug appears as a 4xx or 5xx status response becoming the only
response delivered by Squid to a URL when Collapsed Forwarding
feature is used.

It primarily affects Squid which are caching the 4xx/5xx status
object since Bug 5030 fix in Squid-4.11. But may have been
occurring for short times on any proxy with Collapsed Forwarding.



 * SSL-Bump: Support parsing GREASEd (and future) TLS handshakes

Chrome Browser intentionally sends random garbage values in the
TLS handshake to force TLS implementations to cope with future TLS
extensions cleanly. The changes in Squid-4.12 to disable TLS/1.3
caused our parser to be extra strict and reject this TLS garbage.

This release adds explicit support for Chrome, or any other TLS
agent performing these "GREASE" behaviours.


 * Honor on_unsupported_protocol for intercepted https_port

This behaviour was one of the intended use-cases for unsupported
protocol handling, but somehow was not enabled earlier.

Squid should now be able to perform the on_unsupported_protocol
selected action for any traffic handled by SSL-Bump.


  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Aug 23 08:17:14 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2020 20:17:14 +1200
Subject: [squid-users] [squid-announce] Squid 5.0.4 beta is available
Message-ID: <b61e7f07-4e2a-e165-adf1-3e166e28cccf@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-5.0.4 beta release!


This release is a security and feature update release resolving
several issues found in the prior Squid releases.


The major changes to be aware of:

 * SQUID-2020:8 HTTP(S) Request Splitting
   (CVE-2020-15811)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the browser
cache and any downstream caches with content from an arbitrary
source.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-c7p8-xqhm-49wv>


 * SQUID-2020:9 Denial of Service processing Cache Digest Response
   (CVE pending allocation)

This problem allows a trusted peer to deliver to perform Denial
of Service by consuming all available CPU cycles on the machine
running Squid when handling a crafted Cache Digest response
message.

This attack is limited to Squid using cache_peer with cache
digests feature.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-vvj7-xjgq-g2jg>


 * SQUID-2020:10 HTTP(S) Request Smuggling
   (CVE-2020-15810)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the proxy
cache and any downstream caches with content from an arbitrary
source.


See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-3365-q9qx-f98m>


 * Add http_port sslflags=CONDITIONAL_AUTH

This release extends the client certificate features to allow
optional certificate authentication.

The existing DELAYED_AUTH flag would delay the certificate request,
then reject all clients who cannot present a valid certificate
on request.

With CONDITIONAL_AUTH Squid will just request and validate SSL
client certificates. Any rejection or use of those certificates
is left to other configuration settings.


 * Improved CONNECT tunnel handling

This release contains several small but important changes to how
Squid handles CONNECT tunnels opened with servers. Particularly
in cases of server TCP connection failure and switching between
upstream peers.

A lot of annoying on_unsupported_protocol and HTTPS forwarding
behaviour issues with previous releases should be resolved by
these changes.



  All users of Squid-5 are urged to upgrade as soon as possible.

  All users of Squid-4 and older are encouraged to plan for upgrade.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v5/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Aug 23 08:17:24 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2020 20:17:24 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:8 HTTP(S)
 Request Splitting
Message-ID: <4678bb72-8d7c-6329-e338-b41064a3f137@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2020:8
__________________________________________________________________

Advisory ID:       | SQUID-2020:8
Date:              | August 23, 2020
Summary:           | HTTP(S) Request Splitting.
Affected versions: | Squid 2.7 -> 2.7.STABLE9
                   | Squid 3.x -> 3.5.28
                   | Squid 4.x -> 4.12
                   | Squid 5.x -> 5.0.3
Fixed in version:  | Squid 4.13, 5.0.4
__________________________________________________________________

  <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15811>
__________________________________________________________________

Problem Description:

 Due to incorrect data validation Squid is vulnerable to HTTP
 Request Splitting attacks against HTTP and HTTPS traffic. This
 leads to cache poisoning.

__________________________________________________________________

Severity:

 This problem is serious because it allows any client, including
 browser scripts, to bypass local security and poison the browser
 cache and any downstream caches with content from an arbitrary
 source.

CVSS Score of 9.3
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:N/E:F/RL:O/RC:C/CR:H/IR:H/AR:X/MAV:N/MAC:L/MPR:L/MUI:N/MS:C/MC:H/MI:H/MA:N&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid versions 4.13 and 5.0.4.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2020_8.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid configured with "relaxed_header_parser off" are not vulnerable.

 All Squid-3.x up to and including 3.5.28 with
 relaxed_header_parser configured to "on" or "warn" are
 vulnerable.

 All Squid-3.x up to and including 3.5.28 without
 relaxed_header_parser configured are vulnerable.

 All Squid-4.x up to and including 4.12 with relaxed_header_parser
 configured to "on" or "warn" are vulnerable.

 All Squid-4.x up to and including 4.12 without
 relaxed_header_parser configured are vulnerable.

 All Squid-5.x up to and including 5.0.3 with
 relaxed_header_parser configured to "on" or "warn" are
 vulnerable.

 All Squid-5.x up to and including 5.0.3 without
 relaxed_header_parser configured are vulnerable.

__________________________________________________________________

Workaround:

 Disable the relaxed HTTP parser in squid.conf:

    relaxed_header_parser off

 Note, traffic which does not correctly obey HTTP specifications
 will be rejected instead of converted to standards compliance.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the <squid-users at lists.squid-cache.org> mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Regis Leroy (regilero
 from Makina Corpus).

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2019-07-24 11:52:51 UTC Initial Report
 2020-01-09 22:07:44 UTC Additional vectors discovered
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Aug 23 08:17:38 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2020 20:17:38 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:9 Denial of
 Service processing Cache Digest Response
Message-ID: <bebcbc56-ad06-704a-6f1f-171d978004b5@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2020:9
__________________________________________________________________

Advisory ID:       | SQUID-2020:9
Date:              | August 23, 2020
Summary:           | Denial of Service
                   | processing Cache Digest Response
Affected versions: | Squid 3.x -> 3.5.28
                   | Squid 4.x -> 4.12
                   | Squid 5.x -> 5.0.3
Fixed in version:  | Squid 4.13 and 5.0.4
__________________________________________________________________

    CVE Assignment pending
__________________________________________________________________

Problem Description:

 Due to Improper Input Validation Squid is vulnerable to a Denial
 of Service attack against the machine operating Squid.

__________________________________________________________________

Severity:

 This problem allows a trusted peer to deliver to perform Denial
 of Service by consuming all available CPU cycles on the machine
 running Squid when handling a crafted Cache Digest response
 message.

 This attack is limited to Squid using cache_peer with cache
 digests feature.

CVSS Score of 9.5
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:H/E:H/RL:O/RC:C/CR:X/IR:X/AR:H/MAV:N/MAC:L/MPR:X/MUI:N/MS:C/MC:N/MI:N/MA:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid versions 4.13 and 5.0.4.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2020_9.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid built using --disable-cache-digests are not vulnerable.

 All Squid without cache_peer directives configured are not
 vulnerable.

 All Squid-3.x up to and including 3.5.28 using cache_peer
 directives with no-digest option configured are not vulnerable.

 All Squid-3.x up to and including 3.5.28 using cache_peer
 directives without the no-digest option configured are
 vulnerable.

 All Squid-4.x up to and including 4.12 using cache_peer
 directives with no-digest option configured are not vulnerable.

 All Squid-4.x up to and including 4.12 using cache_peer
 directives without the no-digest option configured are
 vulnerable.

 All Squid-5.x up to and including 5.0.3 using cache_peer
 directives with no-digest option configured are not vulnerable.

 All Squid-5.x up to and including 5.0.3 using cache_peer
 directives without the no-digest option configured are
 vulnerable.

__________________________________________________________________

Workaround:

Either,

 Add the no-digest option to all cache_peer lines in squid.conf

Or,

 Build Squid with --disable-cache-digests

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the <squid-users at lists.squid-cache.org> mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Lubos Uhliarik of RedHat.

 Fixed by Eduard Bagdasaryan (The Measurement Factory).

__________________________________________________________________

Revision history:

 2019-09-30 17:12:18 UTC Initial Report
 2020-07-29 20:51:58 UTC Fix committed
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Aug 23 08:17:47 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 23 Aug 2020 20:17:47 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:10 HTTP(S)
 Request Smuggling
Message-ID: <6615617b-63ce-b25b-ae73-aea862a8ca15@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2020:10
__________________________________________________________________

Advisory ID:       | SQUID-2020:10
Date:              | August 1, 2020
Summary:           | HTTP(S) Request Smuggling.
Affected versions: | Squid 2.5 -> 2.7.STABLE9
                   | Squid 3.x -> 3.5.28
                   | Squid 4.x -> 4.12
                   | Squid 5.x -> 5.0.3
Fixed in version:  | Squid 4.13, 5.0.4
__________________________________________________________________

  <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15810>
__________________________________________________________________

Problem Description:

 Due to incorrect data validation Squid is vulnerable to HTTP
 Request Smuggling attacks against HTTP and HTTPS traffic. This
 leads to cache poisoning.

__________________________________________________________________

Severity:

 This problem is serious because it allows any client, including
 browser scripts, to bypass local security and poison the proxy
 cache and any downstream caches with content from an arbitrary
 source.

CVSS Score of 9.3
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:N/E:F/RL:O/RC:C/CR:H/IR:H/AR:X/MAV:N/MAC:L/MPR:L/MUI:N/MS:C/MC:H/MI:H/MA:N&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid versions 4.13 and 5.0.4.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2020_10.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid with relaxed_header_parser configured "off" are not
 vulnerable.

 All Squid-3.x up to and including 3.5.28 with
 relaxed_header_parser configured to "on" or "warn" are
 vulnerable.

 All Squid-3.x up to and including 3.5.28 without
 relaxed_header_parser configured are vulnerable.

 All Squid-4.x up to and including 4.12 with relaxed_header_parser
 configured to "on" or "warn" are vulnerable.

 All Squid-4.x up to and including 4.12 without
 relaxed_header_parser configured are vulnerable.

 All Squid-5.x up to and including 5.0.3 with
 relaxed_header_parser configured to "on" or "warn" are
 vulnerable.

 All Squid-5.x up to and including 5.0.3 without
 relaxed_header_parser configured are vulnerable.

__________________________________________________________________

Workaround:

 Disable the relaxed HTTP parser in squid.conf:

    relaxed_header_parser off

 Note, traffic which does not correctly obey HTTP specifications
 will be rejected instead of converted to standards compliance.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the <squid-users at lists.squid-cache.org> mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Amit Klein of Safebreach.

 Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

 2020-05-11 08:21:58 UTC Initial Report
 2020-07-17 17:11:50 UTC CVE Allocated
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From m_zouhairy at skno.by  Thu Aug  6 05:31:59 2020
From: m_zouhairy at skno.by (vacheslav)
Date: Thu, 06 Aug 2020 05:31:59 -0000
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
 <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
 <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
Message-ID: <d3d156ea-c902-1900-1e0c-d4af362dffaa@skno.by>

having 3GB memory with a ufdb improves performace

6.08.20 08:28, m k ?????:
> Eliezer,
>
> Squid's default setting is 1 core CPU, 16GB mem.
> How many URLs(Blacklist) will degrade Squid's performance?
>
> Also, SSL-Bump.
>
> Thank you,
> kitamura
>
>
> 2020?8?6?(?) 13:38 Eliezer Croitor <ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com>>:
>
>     Kitamura,
>
>     About the tens of thousands of URLs, Have you considered using a
>     Blacklisting utility, it might lower the memory footprint.
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>     *From:* squid-users <squid-users-bounces at lists.squid-cache.org
>     <mailto:squid-users-bounces at lists.squid-cache.org>> *On Behalf Of *m k
>     *Sent:* Thursday, August 6, 2020 7:25 AM
>     *To:* Amos Jeffries <squid3 at treenet.co.nz
>     <mailto:squid3 at treenet.co.nz>>
>     *Cc:* squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     *Subject:* Re: [squid-users] I would like to know performance
>     sizing aspects.
>
>     Amos,
>
>     Thank you for your reply.
>
>     It was very helpful.
>
>     > That number was gained before HTTPS became so popular. So YMMV
>     depending
>     > on how many CONNECT tunnels you have to deal with. That HTTPS
>     traffic?can possibly be decrypted
>
>     > and cached but performance trade-offs are?quite large.
>
>     Squid uses SSL-Bump.
>
>     I'm very worried about the internet slowing down due to https
>     decording. and I'm also worried about the internet slowing down
>     due to using Blacklist.
>
>     I load tens of thousands of URL(black list file) every time I set
>     up ACL.
>
>     How many requests does SSL-Bump in one second?
>
>     Thank you,
>
>     kitamura
>
>     2020?8?5?(?) 10:32 Amos Jeffries <squid3 at treenet.co.nz
>     <mailto:squid3 at treenet.co.nz>>:
>
>         On 5/08/20 11:28 am, m k wrote:
>         >> We are considering to use Squid for our proxy, and?would
>         like to know
>         >> performance sizing aspects.
>         >>
>         >> Current web access request averages per 1 hour are as
>         followings
>         >> Clients?30,000?
>         >> Page Views:141,741/hour
>         >> *Requests:4,893,106
>         >>
>
>         Okay. Requests and client count are the important numbers there.
>
>         The ~1359 req/sec is well within a default Squid capabilities,
>         which can
>         extend up to around 10k req/sec before needing careful tuning.
>
>         That number was gained before HTTPS became so popular. So YMMV
>         depending
>         on how many CONNECT tunnels you have to deal with. That HTTPS
>         traffic
>         can possibly be decrypted and cached but performance
>         trade-offs are
>         quite large.
>
>
>         >> We will install Squid on CentOS 8.1. ?Please kindly share your
>         >> thoughts / advices
>
>         Whatever OS you are most comfortable with administering. Be
>         aware that
>         CentOS official Squid packages are very slow to update -
>         Apparently they
>         still have only v4.4 (8 months old) despite a 8.2 point
>         release only a
>         few weeks ago.
>
>         So you may need to be building your own from sources and/or
>         using other
>         semi-official packagers such as the ones from Eliezer at
>         NGTech when he
>         gets around to CentOS 8 packages.
>         ? <https://wiki.squid-cache.org/KnowledgeBase/CentOS>
>
>
>         FYI; If you find yourself having to use SSL-Bump, then we highly
>         recommended to follow the latest Squid releases with fairly
>         frequent
>         updates (at minimum a few times per year - worst case
>         monthly). If you
>         like CentOS you may find Fedora more suitable to track the
>         security
>         environment volatility and update churn.
>
>
>         >> Is there sizing methodology and tools?
>
>         There are a couple of methodologies, depending on what aspect
>         you are
>         tuning towards - and one for identifying the limitation points
>         to begin
>         a tuning process tuning.
>
>         The info you gave above is the beginning. Checking to see if your
>         traffic rate is reasonably within capability of a single Squid
>         instance.
>
>         Yours is reasonable, so next step is to get Squid running and
>         see where
>         the trouble points (if any) are.
>
>         ?For more see <https://wiki.squid-cache.org/SquidFaq/>
>
>
>
>         >> How much resources are generally recommended for our
>         environment?
>         >>? CPU:? Memory:? Disk space : Other factors to be considered
>         if any:
>         >> Do you have a generally recommended performance testing
>         tools? Any
>         >> suggested guidelines?
>         >>
>
>
>         ?CPU - squid is still mostly single-process. So prioritize
>         faster GHz
>         rates over core number. Multi-core can help of course, but not
>         as much
>         as cycle speeds do. Hyper-threading is useless for Squid.
>
>         ?Memory - Squid will use as much as you can give it. Let your
>         budget
>         govern this.
>
>         ?Disk - Squid will happily run with no disk - or lots of large
>         ones.
>
>         ? ?- Avoid RAID. Squid *will* shorten disk lifetimes with its
>         unusually
>         high write I/O pattern. How much shorter varies by disk type
>         (HDD vs
>         SSD). So you may find it better to plan budget towards
>         maintenance costs
>         of replacing disks in future rather than buying multiple
>         up-front for
>         RAID use.
>         ?see <https://wiki.squid-cache.org/SquidFaq/RAID> for details.
>
>         ? ? - Up to a few hundred GB per cache_dir can be good for
>         large caches.
>         Going up to TB is not (yet) worth the disk cost as Squid has a
>         per-cache
>         limit on stored objects.
>
>         ? ?- Disk caches can be re-tuned, added, moved, removed,
>         and/or extended
>         at any time and will depend on the profile of object sizes
>         your proxy
>         handles - which itself likely changes over time. So general
>         let your
>         budget decide the initial disks and work from there.
>
>
>
>         Load Testing - the tools us dev use to review performance are
>         listed at
>         the bottom of the profiling FAQ page. These are best for
>         testing the
>         theoretical limits of a particular installation - real traffic
>         tends to
>         be somewhat lower. So I personally prefer taking stats from
>         the running
>         proxy on real traffic and seeing what I can observe from those.
>
>
>         HTH
>         Amos
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200806/ab9962aa/attachment.htm>

From service.mv at gmail.com  Sun Aug 23 15:51:37 2020
From: service.mv at gmail.com (Service MV)
Date: Sun, 23 Aug 2020 12:51:37 -0300
Subject: [squid-users] Limit large downloads to autenticated users
In-Reply-To: <087792a3-3c6b-c1bc-95a1-929ffba5ac87@treenet.co.nz>
References: <CA+d==oFfh+NPG_LgDONgTHwt9Mcw8doVWiOsbxyjC9roSM8b5Q@mail.gmail.com>
 <087792a3-3c6b-c1bc-95a1-929ffba5ac87@treenet.co.nz>
Message-ID: <CA+d==oFEV8fr0n2V=hnzo_-6-keZ0Dg4=9X=rkFFNc32skPeag@mail.gmail.com>

Thank you, Amos, for the clarification.
After making time for me to test some more with fast acl's I noticed that
it still didn't work. So after some more research I found out that the
problem is already reported as "Bug 4913 - Delay Pools don't work for
Tunneled traffic" which is exactly the problem I was having. HTTP traffic
is correctly limited in my tests.
For the time being I will see if I can limit it in another way until I can
fix it.

Best regards
Gabriel


El mar., 28 de jul. de 2020 a la(s) 10:26, Amos Jeffries (
squid3 at treenet.co.nz) escribi?:

> On 28/07/20 8:41 am, Service MV wrote:
> > Hi everybody!
> > I read in the squid mailing lists that delay_pools doesn't work in v4.x,
> > but in the documentation I don't see anything about it.
>
> * Delay pools is a fairly major feature.
>
> * "Dont work" is a very vague claim.
>
> * mailing list threads are typically started by people who don't know
> how to use a feature properly and having trouble because of that
> misunderstanding.
>
> * 4.x is an entire series of releases with many bug fixes across the
> (ongoing) year(s) long lifecycle.
>
> Draw your own conclusion about the accuracy of such statement on the
> mailing list.
>
>
>
> > I would like to know if in my SQUID 4.11 configuration with Kerberos +
> > LDAP authentication I can setup a delay_pools to limit large downloads
> > of any authenticated user.
> >
>
> Yes. That should be entirely possible.
>
>
> > This is my test configuration that I try to do, but I cannot limit the
> > downloads.
> >
> > squid.conf
> ...
> > acl auth proxy_auth REQUIRED
> > delay_pools 1
> > delay_class 1 2
> > delay_parameters 1 64000/64000 64000/64000
>
> > delay_access 1 allow auth
>
> The first problem is here. proxy_auth ACL is a "slow" type and
> delay_access only supports "fast" types.
>
> Squid-4 provides transaction annotations feature that can bridge this
> gap. It is a fast type ACL that checks for annotations set by helper
> lookups etc.
>
>   acl hasUsername note user
>   delay_access 1 allow hasUser
>   delay_access 1 deny all
>
>
>
> > http_access allow auth
>
> This should be down just above the "http_access deny all"
>
>
> > acl SSL_ports port 443
> > acl Safe_ports port 80
> > acl CONNECT method CONNECT
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> >
> > http_access deny all
> >
> >
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200823/28afee55/attachment.htm>

From rahul.negi at orange.com  Mon Aug 24 04:06:54 2020
From: rahul.negi at orange.com (rahul.negi at orange.com)
Date: Mon, 24 Aug 2020 04:06:54 +0000
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files for
 centos7 and x86_64 architecture
Message-ID: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>

Hi Team,

Can anyone please share squid latest stable version 4.13 RPM packaged  files for CentOS7  distribution and x86_64 architecture.

Thanks and Regards,
Rahul Negi


_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200824/9ff2fc56/attachment.htm>

From arsalan at preston.edu.pk  Mon Aug 24 06:59:08 2020
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Mon, 24 Aug 2020 11:59:08 +0500
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
References: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
Message-ID: <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>

 Dear Mr. Negi

Reference to email received from Squid forum regarding Squid-4.13 release
package by Mr. Amos Jeffries.

See below information. I am planning to upgrade my server by trying it soon

COPIED
........
On Sun, Aug 23, 2020 at 1:35 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-4.13 release!
>
>
> This release is a security release resolving several issues found in
> the prior Squid releases.
>
>
> The major changes to be aware of:
>
>  * SQUID-2020:8 HTTP(S) Request Splitting
>    (CVE-2020-15811)
>
> This problem is serious because it allows any client, including
> browser scripts, to bypass local security and poison the browser
> cache and any downstream caches with content from an arbitrary
> source.
>
> See the advisory for patches:
>  <
> https://github.com/squid-cache/squid/security/advisories/GHSA-c7p8-xqhm-49wv
> >
>
>
>  * SQUID-2020:9 Denial of Service processing Cache Digest Response
>    (CVE pending allocation)
>
> This problem allows a trusted peer to deliver to perform Denial
> of Service by consuming all available CPU cycles on the machine
> running Squid when handling a crafted Cache Digest response
> message.
>
> This attack is limited to Squid using cache_peer with cache
> digests feature.
>
> See the advisory for patches:
>  <
> https://github.com/squid-cache/squid/security/advisories/GHSA-vvj7-xjgq-g2jg
> >
>
>
>  * SQUID-2020:10 HTTP(S) Request Smuggling
>    (CVE-2020-15810)
>
> This problem is serious because it allows any client, including
> browser scripts, to bypass local security and poison the proxy
> cache and any downstream caches with content from an arbitrary
> source.
>
>
> See the advisory for patches:
>  <
> https://github.com/squid-cache/squid/security/advisories/GHSA-3365-q9qx-f98m
> >
>
>
>  * Bug 5051: Some collapsed revalidation responses never expire
>
> This bug appears as a 4xx or 5xx status response becoming the only
> response delivered by Squid to a URL when Collapsed Forwarding
> feature is used.
>
> It primarily affects Squid which are caching the 4xx/5xx status
> object since Bug 5030 fix in Squid-4.11. But may have been
> occurring for short times on any proxy with Collapsed Forwarding.
>
>
>
>  * SSL-Bump: Support parsing GREASEd (and future) TLS handshakes
>
> Chrome Browser intentionally sends random garbage values in the
> TLS handshake to force TLS implementations to cope with future TLS
> extensions cleanly. The changes in Squid-4.12 to disable TLS/1.3
> caused our parser to be extra strict and reject this TLS garbage.
>
> This release adds explicit support for Chrome, or any other TLS
> agent performing these "GREASE" behaviours.
>
>
>  * Honor on_unsupported_protocol for intercepted https_port
>
> This behaviour was one of the intended use-cases for unsupported
> protocol handling, but somehow was not enabled earlier.
>
> Squid should now be able to perform the on_unsupported_protocol
> selected action for any traffic handled by SSL-Bump.
>
>
>   All users of Squid are urged to upgrade as soon as possible.
>
>
> See the ChangeLog for the full list of changes in this and earlier
> releases.
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
> when you are ready to make the switch to Squid-4
>
> This new release can be downloaded from our HTTP or FTP servers
>
>   http://www.squid-cache.org/Versions/v4/
>   ftp://ftp.squid-cache.org/pub/squid/
>   ftp://ftp.squid-cache.org/pub/archive/4/
>
> or the mirrors. For a list of mirror sites see
>
>   http://www.squid-cache.org/Download/http-mirrors.html
>   http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
>   http://bugs.squid-cache.org/
>
>
> Amos Jeffries
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
>


On Mon, Aug 24, 2020 at 9:07 AM <rahul.negi at orange.com> wrote:

> Hi Team,
>
> Can anyone please share squid latest stable version 4.13 RPM packaged
>  files for CentOS7  distribution and x86_64 architecture.
>
>
>
> *Thanks and Regards,*
>
> *Rahul Negi*
>
>
>
> _________________________________________________________________________________________________________________________
>
> Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
> pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
> a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
> Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.
>
> This message and its attachments may contain confidential or privileged information that may be protected by law;
> they should not be distributed, used or copied without authorisation.
> If you have received this email in error, please notify the sender and delete this message and its attachments.
> As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
> Thank you.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
With Regards,


*Arsalan Hussain*
*Assistant Director, Networks & Information System*

*PRESTON UNIVERSITY*



*Complaining is finding faults, wisdom is finding solutions*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200824/e3f8c57e/attachment.htm>

From rahul.negi at orange.com  Mon Aug 24 09:25:42 2020
From: rahul.negi at orange.com (rahul.negi at orange.com)
Date: Mon, 24 Aug 2020 09:25:42 +0000
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>
References: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>
Message-ID: <14988_1598261144_5F438798_14988_314_2_064671a3-5e76-4f4c-9fd9-d0c43d49b84b@OPEXCNORM52.corporate.adroot.infra.ftgroup>

Hi Arsalan,

I hope to get your response soon including RPM files. Appreciate your help here!

Thanks and Regards,
Rahul Negi
From: Arsalan Hussain [mailto:arsalan at preston.edu.pk]
Sent: Monday, August 24, 2020 12:29
To: NEGI Rahul TGI/OLN
Cc: squid-users at lists.squid-cache.org; Antony Stone; Amos Jeffries
Subject: Re: [squid-users] Need squid latest version 4.13 RPM packaged files for centos7 and x86_64 architecture

Dear Mr. Negi

Reference to email received from Squid forum regarding Squid-4.13 release package by Mr. Amos Jeffries.

See below information. I am planning to upgrade my server by trying it soon

COPIED
........
On Sun, Aug 23, 2020 at 1:35 PM Amos Jeffries <squid3 at treenet.co.nz<mailto:squid3 at treenet.co.nz>> wrote:
The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.13 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:

 * SQUID-2020:8 HTTP(S) Request Splitting
   (CVE-2020-15811)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the browser
cache and any downstream caches with content from an arbitrary
source.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-c7p8-xqhm-49wv>


 * SQUID-2020:9 Denial of Service processing Cache Digest Response
   (CVE pending allocation)

This problem allows a trusted peer to deliver to perform Denial
of Service by consuming all available CPU cycles on the machine
running Squid when handling a crafted Cache Digest response
message.

This attack is limited to Squid using cache_peer with cache
digests feature.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-vvj7-xjgq-g2jg>


 * SQUID-2020:10 HTTP(S) Request Smuggling
   (CVE-2020-15810)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the proxy
cache and any downstream caches with content from an arbitrary
source.


See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-3365-q9qx-f98m>


 * Bug 5051: Some collapsed revalidation responses never expire

This bug appears as a 4xx or 5xx status response becoming the only
response delivered by Squid to a URL when Collapsed Forwarding
feature is used.

It primarily affects Squid which are caching the 4xx/5xx status
object since Bug 5030 fix in Squid-4.11. But may have been
occurring for short times on any proxy with Collapsed Forwarding.



 * SSL-Bump: Support parsing GREASEd (and future) TLS handshakes

Chrome Browser intentionally sends random garbage values in the
TLS handshake to force TLS implementations to cope with future TLS
extensions cleanly. The changes in Squid-4.12 to disable TLS/1.3
caused our parser to be extra strict and reject this TLS garbage.

This release adds explicit support for Chrome, or any other TLS
agent performing these "GREASE" behaviours.


 * Honor on_unsupported_protocol for intercepted https_port

This behaviour was one of the intended use-cases for unsupported
protocol handling, but somehow was not enabled earlier.

Squid should now be able to perform the on_unsupported_protocol
selected action for any traffic handled by SSL-Bump.


  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org<mailto:squid-announce at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-announce


On Mon, Aug 24, 2020 at 9:07 AM <rahul.negi at orange.com<mailto:rahul.negi at orange.com>> wrote:

Hi Team,

Can anyone please share squid latest stable version 4.13 RPM packaged  files for CentOS7  distribution and x86_64 architecture.

Thanks and Regards,
Rahul Negi


_________________________________________________________________________________________________________________________



Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc

pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler

a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,

Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.



This message and its attachments may contain confidential or privileged information that may be protected by law;

they should not be distributed, used or copied without authorisation.

If you have received this email in error, please notify the sender and delete this message and its attachments.

As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.

Thank you.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users


--
With Regards,

Arsalan Hussain
Assistant Director, Networks & Information System

PRESTON UNIVERSITY


Complaining is finding faults, wisdom is finding solutions

_________________________________________________________________________________________________________________________

Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.

This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200824/b51131b7/attachment.htm>

From tamurin0525 at gmail.com  Mon Aug 24 10:27:30 2020
From: tamurin0525 at gmail.com (m k)
Date: Mon, 24 Aug 2020 19:27:30 +0900
Subject: [squid-users] I want fine ACL control with squid.
Message-ID: <CAL-uOnHRS=gcqBPpOn2KnNDgJ88EfKKK2QRy5+jw8urwMwQQPA@mail.gmail.com>

Hi team,

I want fine ACL control with squid.
squid is AD certified.
1.
Is it possible for AD-authenticated users to set one side to read the
blacklist and the other side not to read the blacklist?
2.
I'm not sure about the behavior of ACL control.
Look at the rows from the top, and if it matches, is it done?

Thank you,
kitamura
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200824/240b133e/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 24 10:51:23 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Aug 2020 22:51:23 +1200
Subject: [squid-users] I want fine ACL control with squid.
In-Reply-To: <CAL-uOnHRS=gcqBPpOn2KnNDgJ88EfKKK2QRy5+jw8urwMwQQPA@mail.gmail.com>
References: <CAL-uOnHRS=gcqBPpOn2KnNDgJ88EfKKK2QRy5+jw8urwMwQQPA@mail.gmail.com>
Message-ID: <656c3de8-b265-3e0a-dd98-fd90701b149d@treenet.co.nz>

On 24/08/20 10:27 pm, m k wrote:
> Hi team,
> 
> I want fine ACL control with squid.
> squid is AD certified.
> 1.
> Is it possible for AD-authenticated users to set one side to read the
> blacklist and the other side not to read the blacklist?

The answer is "Yes".

To get better help than that please define what you means by "side".

One side of plant Earth? One side of the building you are in right now?
what?



> 2.
> I'm not sure about the behavior of ACL control.
> Look at the rows from the top, and if it matches, is it done?
> 

Yes.

Please see the FAQ:
 <https://wiki.squid-cache.org/SquidFaq/SquidAcl>


Amos


From tamurin0525 at gmail.com  Mon Aug 24 11:30:56 2020
From: tamurin0525 at gmail.com (m k)
Date: Mon, 24 Aug 2020 20:30:56 +0900
Subject: [squid-users] I want fine ACL control with squid.
In-Reply-To: <656c3de8-b265-3e0a-dd98-fd90701b149d@treenet.co.nz>
References: <CAL-uOnHRS=gcqBPpOn2KnNDgJ88EfKKK2QRy5+jw8urwMwQQPA@mail.gmail.com>
 <656c3de8-b265-3e0a-dd98-fd90701b149d@treenet.co.nz>
Message-ID: <CAL-uOnFUV+E6T5B1iWw_zD3cVjaMOv-YgPten5DsMERcyUbMFw@mail.gmail.com>

Hi Amos,

Thank you for your reply.
And sorry, I am not good at English.

1.
"side" is a group of Active Directory.
I want A group to have blacklist and B group not to have blacklist in squid.
Is it understandable?

2.
I will read the FAQ page.

Thank you,
kitamura

2020?8?24?(?) 19:57 Amos Jeffries <squid3 at treenet.co.nz>:

> On 24/08/20 10:27 pm, m k wrote:
>
> > Hi team,
>
> >
>
> > I want fine ACL control with squid.
>
> > squid is AD certified.
>
> > 1.
>
> > Is it possible for AD-authenticated users to set one side to read the
>
> > blacklist and the other side not to read the blacklist?
>
>
>
> The answer is "Yes".
>
>
>
> To get better help than that please define what you means by "side".
>
>
>
> One side of plant Earth? One side of the building you are in right now?
>
> what?
>
>
>
>
>
>
>
> > 2.
>
> > I'm not sure about the behavior of ACL control.
>
> > Look at the rows from the top, and if it matches, is it done?
>
> >
>
>
>
> Yes.
>
>
>
> Please see the FAQ:
>
>  <https://wiki.squid-cache.org/SquidFaq/SquidAcl>
>
>
>
>
>
> Amos
>
> _______________________________________________
>
> squid-users mailing list
>
> squid-users at lists.squid-cache.org
>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200824/872c4d6e/attachment.htm>

From mbrown8918 at outlook.com  Mon Aug 24 22:21:31 2020
From: mbrown8918 at outlook.com (Mathew Brown)
Date: Mon, 24 Aug 2020 22:21:31 +0000
Subject: [squid-users] Strange Squid SSL Interception Behavior
Message-ID: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>

Hi,

I'm currently trying to configure transparent SSL proxying and running into a strange error that has me scratching my head for hours. I'm using Squid 4.11 (I also tried this with 4.12) with SSL support from here - http://squid411.diladele.com/ubuntu/ on Ubuntu 18.04.

I set up the necessary iptables forwarding ports and SSL certificates and it sometimes works (as you will see below).

My current configuration adds just the following to the default squid.conf file:

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
include /etc/squid/conf.d/*

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
#http_access allow localnet
debug_options ALL,1, 33,2 2 28,9

http_port 3129 intercept
https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_cert/squid-ca.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB

acl whitelist ssl::server_name .httpbin.org
acl whitelist_http ssl::server_name .httpbin.org

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump splice all

http_access allow whitelist
http_access allow whitelist_http

# And finally deny all other access to this proxy
http_access deny all

so the above configuration should allow anyone with access to the Squid proxy access to httpbin.org over both HTTP and HTTPS

when I try to access:

http://httpbin.org (not SSL)

it works

when I try to access:

https://httpbin.org

it fails as shown below (I'm running this on the Squid proxy machine itself):

$ wget https://httpbin.org
--2020-08-24 17:48:34--  https://httpbin.org/
Resolving httpbin.org (httpbin.org)... 54.236.246.173, 3.220.112.94
Connecting to httpbin.org (httpbin.org)|54.236.246.173|:443... connected.
ERROR: cannot verify httpbin.org's certificate, issued by ?O=Internet Widgits Pty Ltd,ST=Some-State,C=AU?:
  Self-signed certificate encountered.
To connect to httpbin.org insecurely, use `--no-check-certificate'.

$ wget https://httpbin.org --no-check-certificate
--2020-08-24 17:48:40--  https://httpbin.org/
Resolving httpbin.org (httpbin.org)... 3.220.112.94, 54.236.246.173
Connecting to httpbin.org (httpbin.org)|3.220.112.94|:443... connected.
WARNING: cannot verify httpbin.org's certificate, issued by ?O=Internet Widgits Pty Ltd,ST=Some-State,C=AU?:
  Self-signed certificate encountered.
HTTP request sent, awaiting response... 403 Forbidden
2020-08-24 17:48:40 ERROR 403: Forbidden.

looking at access.log shows:

1598305800.974      2 192.168.123.214 TCP_DENIED/200 0 CONNECT 54.236.246.173:443 - HIER_NONE/- -

for the first request (without the --no-check-certificate) and the following for the 2nd request (with the --no-check-certificate):

1598305812.292      3 192.168.123.214 TCP_DENIED/200 0 CONNECT 54.236.246.173:443 - HIER_NONE/- -
1598305812.300      2 192.168.123.214 NONE/403 3795 GET https://httpbin.org/ - HIER_NONE/- text/html

looking at cache.log shows:

# cat /var/log/squid/cache.log  | grep -i "28" | grep -i httpbin
2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org

so it never matches on the httpbin.org

now, if I add the following line to my configuration:

http_access allow localnet

right before the:

http_access deny all

line it works and I see the following in access.log:

1598305979.004      4 192.168.123.214 NONE/200 0 CONNECT 54.236.246.173:443 - HIER_NONE/- -
1598305980.016   1012 192.168.123.214 TCP_TUNNEL/200 15370 CONNECT httpbin.org:443 - ORIGINAL_DST/54.236.246.173 -

and I see the following in cache.log:

# cat /var/log/squid/cache.log  | grep -i "28" | grep -i httpbin
2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 17:52:59.005 kid1| 28,3| RegexData.cc(43) match: checking 'httpbin.org:443'
2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(42) match: checking 'httpbin.org'
2020/08/24 17:52:59.005 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:httpbin.org <>  .httpbin.org
2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(47) match: 'httpbin.org' found

What's puzzling is why adding the 'allow localnet' line changes the ACL logic for .httpbin.org and why the original configuration does not work. Any ideas? Thanks

PS. I reproduced the exact same scenario on Ubuntu 20.04 with Squid 4.12


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200824/a7a0da7e/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Aug 24 22:32:15 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 25 Aug 2020 00:32:15 +0200
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
Message-ID: <202008250032.16069.Antony.Stone@squid.open.source.it>

On Tuesday 25 August 2020 at 00:21:31, Mathew Brown wrote:

> I set up the necessary iptables forwarding ports

Please show us what those iptables rules are.


Antony.

-- 
"It wouldn't be a good idea to talk about him behind his back in front of 
him."

 - murble

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Mon Aug 24 22:41:05 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 24 Aug 2020 18:41:05 -0400
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
Message-ID: <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>

On 8/24/20 6:21 PM, Mathew Brown wrote:

> acl whitelist ssl::server_name .httpbin.org
> acl whitelist_http ssl::server_name .httpbin.org

> ssl_bump peek step1
> ssl_bump splice all

> http_access allow whitelist
> http_access allow whitelist_http
> http_access deny all

The rules above only allow CONNECT requests to .httpbin.org domains.

During step1, when Squid intercepts a TLS connection to an IP address of
an .httpbin.org domain, Squid http_access rules are applied to a (fake)
CONNECT request to the destination IP address. There are no domain names
at that TCP-level bumping stage. Thus, you place your Squid at the mercy
of reverse DSN lookups.

In my environment, reverse DNS does not work for httpbin.org the way you
may expect:

> $ host 54.236.246.173
> 173.246.236.54.in-addr.arpa domain name pointer ec2-54-236-246-173.compute-1.amazonaws.com.

The above AWS domain name does not match your whitelist ACLs, of course,
and, hence, the fake CONNECT request is denied. Denied requests are
bumped to deliver the error message. Bumped requests require
--no-check-certificate or other means of trusting Squid's CA certificate.

If you cannot explicitly allow CONNECT requests to httpbin.org IP
addresses (e.g., because they change too often), then consider allowing
CONNECT to safe ports at any address at step1. If you only intercept
connections to httpbin.org IPs, then you can probably relax your step1
http_access rules to allow all CONNECTs (to safe addresses).

There are many similar questions about allowing CONNECT to IP addresses
on this mailing list. You may be able to find more detailed advice or
instructions by searching for those mailing list threads.


HTH,

Alex.


> $ wget https://httpbin.org
> --2020-08-24 17:48:34-- ?https://httpbin.org/
> Resolving httpbin.org (httpbin.org)... 54.236.246.173, 3.220.112.94
> Connecting to httpbin.org (httpbin.org)|54.236.246.173|:443... connected.
> ERROR: cannot verify httpbin.org's certificate, issued by ?O=Internet
> Widgits Pty Ltd,ST=Some-State,C=AU?:
> ? Self-signed certificate encountered.
> To connect to httpbin.org insecurely, use `--no-check-certificate'.
> 
> $ wget https://httpbin.org --no-check-certificate
> --2020-08-24 17:48:40-- ?https://httpbin.org/
> Resolving httpbin.org (httpbin.org)... 3.220.112.94, 54.236.246.173
> Connecting to httpbin.org (httpbin.org)|3.220.112.94|:443... connected.
> WARNING: cannot verify httpbin.org's certificate, issued by ?O=Internet
> Widgits Pty Ltd,ST=Some-State,C=AU?:
> ? Self-signed certificate encountered.
> HTTP request sent, awaiting response... 403 Forbidden
> 2020-08-24 17:48:40 ERROR 403: Forbidden.
> 
> looking at access.log shows:
> 
> 1598305800.974 ? ? ?2 192.168.123.214 TCP_DENIED/200 0 CONNECT
> 54.236.246.173:443 - HIER_NONE/- -
> 
> for the first request (without the --no-check-certificate) and the
> following for the 2nd request (with the --no-check-certificate):
> 
> 1598305812.292 ? ? ?3 192.168.123.214 TCP_DENIED/200 0 CONNECT
> 54.236.246.173:443 - HIER_NONE/- -
> 1598305812.300 ? ? ?2 192.168.123.214 NONE/403 3795 GET
> https://httpbin.org/ - HIER_NONE/- text/html
> 
> looking at cache.log shows:
> 
> # cat /var/log/squid/cache.log ?| grep -i "28" | grep -i httpbin
> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
> 
> so it never matches on the httpbin.org
> 
> now, if I add the following line to my configuration:
> 
> http_access allow localnet
> 
> right before the:
> 
> http_access deny all
> 
> line it works and I see the following in access.log:
> 
> 1598305979.004 ? ? ?4 192.168.123.214 NONE/200 0 CONNECT
> 54.236.246.173:443 - HIER_NONE/- -
> 1598305980.016 ? 1012 192.168.123.214 TCP_TUNNEL/200 15370 CONNECT
> httpbin.org:443 - ORIGINAL_DST/54.236.246.173 -
> 
> and I see the following in cache.log:
> 
> # cat /var/log/squid/cache.log ?| grep -i "28" | grep -i httpbin
> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
> 2020/08/24 17:52:59.005 kid1| 28,3| RegexData.cc(43) match: checking
> 'httpbin.org:443'
> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(42) match: checking
> 'httpbin.org'
> 2020/08/24 17:52:59.005 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:httpbin.org <> ?.httpbin.org
> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(47) match:
> 'httpbin.org' found
> 
> What's puzzling is why adding the 'allow localnet' line changes the ACL
> logic for .httpbin.org and why the original configuration does not work.
> Any ideas? Thanks
> 
> PS. I reproduced the exact same scenario on Ubuntu 20.04 with Squid 4.12
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ngtech1ltd at gmail.com  Mon Aug 24 23:45:25 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Tue, 25 Aug 2020 02:45:25 +0300
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <14988_1598261144_5F438798_14988_314_2_064671a3-5e76-4f4c-9fd9-d0c43d49b84b@OPEXCNORM52.corporate.adroot.infra.ftgroup>
References: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>
 <14988_1598261144_5F438798_14988_314_2_064671a3-5e76-4f4c-9fd9-d0c43d49b84b@OPEXCNORM52.corporate.adroot.infra.ftgroup>
Message-ID: <00ba01d67a70$a4eba200$eec2e600$@gmail.com>

Trying to understand something in the list.

 

Anyone interested funding the build of these RPM?s?

To power up some CPU, RAM etc requires food and other bills..

If for some reason many think that these RPM?s can pop up from /dev/null I believe they are wrong.

 

Let Me Know.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of rahul.negi at orange.com
Sent: Monday, August 24, 2020 12:26 PM
To: Arsalan Hussain <arsalan at preston.edu.pk>
Cc: squid-users at lists.squid-cache.org; VARSHNEY Praveen TGI/OLN <praveen.varshney at orange.com>
Subject: Re: [squid-users] Need squid latest version 4.13 RPM packaged files for centos7 and x86_64 architecture

 

Hi Arsalan,

 

I hope to get your response soon including RPM files. Appreciate your help here!

 

Thanks and Regards,

Rahul Negi

From: Arsalan Hussain [mailto:arsalan at preston.edu.pk] 
Sent: Monday, August 24, 2020 12:29
To: NEGI Rahul TGI/OLN
Cc: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> ; Antony Stone; Amos Jeffries
Subject: Re: [squid-users] Need squid latest version 4.13 RPM packaged files for centos7 and x86_64 architecture

 

Dear Mr. Negi

 

Reference to email received from Squid forum regarding Squid-4.13 release package by Mr. Amos Jeffries.

 

See below information. I am planning to upgrade my server by trying it soon

 

COPIED 

........

On Sun, Aug 23, 2020 at 1:35 PM Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> > wrote:

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.13 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:

 * SQUID-2020:8 HTTP(S) Request Splitting
   (CVE-2020-15811)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the browser
cache and any downstream caches with content from an arbitrary
source.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-c7p8-xqhm-49wv>


 * SQUID-2020:9 Denial of Service processing Cache Digest Response
   (CVE pending allocation)

This problem allows a trusted peer to deliver to perform Denial
of Service by consuming all available CPU cycles on the machine
running Squid when handling a crafted Cache Digest response
message.

This attack is limited to Squid using cache_peer with cache
digests feature.

See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-vvj7-xjgq-g2jg>


 * SQUID-2020:10 HTTP(S) Request Smuggling
   (CVE-2020-15810)

This problem is serious because it allows any client, including
browser scripts, to bypass local security and poison the proxy
cache and any downstream caches with content from an arbitrary
source.


See the advisory for patches:
 <https://github.com/squid-cache/squid/security/advisories/GHSA-3365-q9qx-f98m>


 * Bug 5051: Some collapsed revalidation responses never expire

This bug appears as a 4xx or 5xx status response becoming the only
response delivered by Squid to a URL when Collapsed Forwarding
feature is used.

It primarily affects Squid which are caching the 4xx/5xx status
object since Bug 5030 fix in Squid-4.11. But may have been
occurring for short times on any proxy with Collapsed Forwarding.



 * SSL-Bump: Support parsing GREASEd (and future) TLS handshakes

Chrome Browser intentionally sends random garbage values in the
TLS handshake to force TLS implementations to cope with future TLS
extensions cleanly. The changes in Squid-4.12 to disable TLS/1.3
caused our parser to be extra strict and reject this TLS garbage.

This release adds explicit support for Chrome, or any other TLS
agent performing these "GREASE" behaviours.


 * Honor on_unsupported_protocol for intercepted https_port

This behaviour was one of the intended use-cases for unsupported
protocol handling, but somehow was not enabled earlier.

Squid should now be able to perform the on_unsupported_protocol
selected action for any traffic handled by SSL-Bump.


  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org <mailto:squid-announce at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-announce




 

On Mon, Aug 24, 2020 at 9:07 AM <rahul.negi at orange.com <mailto:rahul.negi at orange.com> > wrote:

Hi Team,

Can anyone please share squid latest stable version 4.13 RPM packaged  files for CentOS7  distribution and x86_64 architecture.

 

Thanks and Regards,

Rahul Negi

 

_________________________________________________________________________________________________________________________
 
Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.
 
This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users



-- 

With Regards,


Arsalan Hussain
Assistant Director, Networks & Information System

PRESTON UNIVERSITY
 

Complaining is finding faults, wisdom is finding solutions

_________________________________________________________________________________________________________________________
 
Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.
 
This message and its attachments may contain confidential or privileged information that may be protected by law;
they should not be distributed, used or copied without authorisation.
If you have received this email in error, please notify the sender and delete this message and its attachments.
As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200825/c5c88373/attachment.htm>

From mbrown8918 at outlook.com  Tue Aug 25 01:09:27 2020
From: mbrown8918 at outlook.com (Mathew Brown)
Date: Tue, 25 Aug 2020 01:09:27 +0000
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>,
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
Message-ID: <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>

Thanks but even with the --no-check-certificate option and using a bump instead of splicing, it still fails as shown above unless I add the localnet rule. The question is: why does the same ACL line:

http_access allow whitelist

suddenly work when I add an unrelated ACL line after it (http_access allow localnet)? Why does it correctly determine the domain httpbin.org in the later case as shown by cache.log?

I updated my splice to a bump instead so my config looks like this:

acl whitelist ssl::server_name .httpbin.org
acl whitelist_http ssl::server_name .httpbin.org

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump bump all

http_access allow whitelist
http_access allow whitelist_http

#http_access allow localnet
http_access deny all

and so it will bump all connections:

I then ran:

wget https://httpbin.org --no-check-certificate

and get the following (it fails):

cat access.log

1598316641.358      3 192.168.123.214 TCP_DENIED/200 0 CONNECT 3.220.112.94:443 - HIER_NONE/- -
1598316641.366      1 192.168.123.214 NONE/403 3789 GET https://httpbin.org/ - HIER_NONE/- text/html

cat cache.log | grep -i httpbin.org

2020/08/24 20:50:41.356 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:3.220.112.94 <>  .httpbin.org
2020/08/24 20:50:41.356 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:3.220.112.94 <>  .httpbin.org

while once I add the localnet rule:

http_access allow localnet

it succeeds and access.log looks like this:

1598316682.044    753 192.168.123.214 NONE/200 0 CONNECT 54.236.246.173:443 - ORIGINAL_DST/54.236.246.173 -
1598316682.329    260 192.168.123.214 TCP_REFRESH_MODIFIED/200 9936 GET https://httpbin.org/ - ORIGINAL_DST/54.236.246.173 text/html

and cache.log shows that it actually does the proper domain comparison:

cat cache.log | grep -i httpbin.org

2020/08/24 20:51:21.292 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 20:51:21.292 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
2020/08/24 20:51:22.071 kid1| 28,3| RegexData.cc(43) match: checking 'https://httpbin.org/'
2020/08/24 20:51:22.071 kid1| 28,4| ServerName.cc(82) check_cert_domain: Verifying certificate name/subjectAltName httpbin.org
2020/08/24 20:51:22.071 kid1| 28,3| ServerName.cc(42) match: checking 'httpbin.org'
2020/08/24 20:51:22.071 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:httpbin.org <>  .httpbin.org
2020/08/24 20:51:22.071 kid1| 28,3| ServerName.cc(47) match: 'httpbin.org' found

>From my understanding, Squid should perform the exact same steps in both cases BUT then allow the connection because of the localnet ACL line that it sees right before the deny all line, not because it suddenly was able to match httpbin.org using a domain compare as shown by the debug logs. What am I missing?

Even if I add a peek at step2, the behavior is still the same for the first scenario although I no longer need to use --no-check-certificate in the 2nd scenario (where I whitelist localnet)

PS. I'm using the current iptable rules and making the wget call from a normal user account (so neither root or proxy):

iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner root -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner proxy -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner root -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner proxy -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3130


________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Tuesday, August 25, 2020 8:41 AM
To: Mathew Brown <mbrown8918 at outlook.com>; squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Strange Squid SSL Interception Behavior

On 8/24/20 6:21 PM, Mathew Brown wrote:

> acl whitelist ssl::server_name .httpbin.org
> acl whitelist_http ssl::server_name .httpbin.org

> ssl_bump peek step1
> ssl_bump splice all

> http_access allow whitelist
> http_access allow whitelist_http
> http_access deny all

The rules above only allow CONNECT requests to .httpbin.org domains.

During step1, when Squid intercepts a TLS connection to an IP address of
an .httpbin.org domain, Squid http_access rules are applied to a (fake)
CONNECT request to the destination IP address. There are no domain names
at that TCP-level bumping stage. Thus, you place your Squid at the mercy
of reverse DSN lookups.

In my environment, reverse DNS does not work for httpbin.org the way you
may expect:

> $ host 54.236.246.173
> 173.246.236.54.in-addr.arpa domain name pointer ec2-54-236-246-173.compute-1.amazonaws.com.

The above AWS domain name does not match your whitelist ACLs, of course,
and, hence, the fake CONNECT request is denied. Denied requests are
bumped to deliver the error message. Bumped requests require
--no-check-certificate or other means of trusting Squid's CA certificate.

If you cannot explicitly allow CONNECT requests to httpbin.org IP
addresses (e.g., because they change too often), then consider allowing
CONNECT to safe ports at any address at step1. If you only intercept
connections to httpbin.org IPs, then you can probably relax your step1
http_access rules to allow all CONNECTs (to safe addresses).

There are many similar questions about allowing CONNECT to IP addresses
on this mailing list. You may be able to find more detailed advice or
instructions by searching for those mailing list threads.


HTH,

Alex.


> $ wget https://httpbin.org
> --2020-08-24 17:48:34--  https://httpbin.org/
> Resolving httpbin.org (httpbin.org)... 54.236.246.173, 3.220.112.94
> Connecting to httpbin.org (httpbin.org)|54.236.246.173|:443... connected.
> ERROR: cannot verify httpbin.org's certificate, issued by ?O=Internet
> Widgits Pty Ltd,ST=Some-State,C=AU?:
>   Self-signed certificate encountered.
> To connect to httpbin.org insecurely, use `--no-check-certificate'.
>
> $ wget https://httpbin.org --no-check-certificate
> --2020-08-24 17:48:40--  https://httpbin.org/
> Resolving httpbin.org (httpbin.org)... 3.220.112.94, 54.236.246.173
> Connecting to httpbin.org (httpbin.org)|3.220.112.94|:443... connected.
> WARNING: cannot verify httpbin.org's certificate, issued by ?O=Internet
> Widgits Pty Ltd,ST=Some-State,C=AU?:
>   Self-signed certificate encountered.
> HTTP request sent, awaiting response... 403 Forbidden
> 2020-08-24 17:48:40 ERROR 403: Forbidden.
>
> looking at access.log shows:
>
> 1598305800.974      2 192.168.123.214 TCP_DENIED/200 0 CONNECT
> 54.236.246.173:443 - HIER_NONE/- -
>
> for the first request (without the --no-check-certificate) and the
> following for the 2nd request (with the --no-check-certificate):
>
> 1598305812.292      3 192.168.123.214 TCP_DENIED/200 0 CONNECT
> 54.236.246.173:443 - HIER_NONE/- -
> 1598305812.300      2 192.168.123.214 NONE/403 3795 GET
> https://httpbin.org/ - HIER_NONE/- text/html
>
> looking at cache.log shows:
>
> # cat /var/log/squid/cache.log  | grep -i "28" | grep -i httpbin
> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>
> so it never matches on the httpbin.org
>
> now, if I add the following line to my configuration:
>
> http_access allow localnet
>
> right before the:
>
> http_access deny all
>
> line it works and I see the following in access.log:
>
> 1598305979.004      4 192.168.123.214 NONE/200 0 CONNECT
> 54.236.246.173:443 - HIER_NONE/- -
> 1598305980.016   1012 192.168.123.214 TCP_TUNNEL/200 15370 CONNECT
> httpbin.org:443 - ORIGINAL_DST/54.236.246.173 -
>
> and I see the following in cache.log:
>
> # cat /var/log/squid/cache.log  | grep -i "28" | grep -i httpbin
> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
> 2020/08/24 17:52:59.005 kid1| 28,3| RegexData.cc(43) match: checking
> 'httpbin.org:443'
> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(42) match: checking
> 'httpbin.org'
> 2020/08/24 17:52:59.005 kid1| 28,7| ServerName.cc(32)
> aclHostDomainCompare: Match:httpbin.org <>  .httpbin.org
> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(47) match:
> 'httpbin.org' found
>
> What's puzzling is why adding the 'allow localnet' line changes the ACL
> logic for .httpbin.org and why the original configuration does not work.
> Any ideas? Thanks
>
> PS. I reproduced the exact same scenario on Ubuntu 20.04 with Squid 4.12
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200825/b62dfb39/attachment.htm>

From rst at fomar.com.pl  Tue Aug 25 01:14:35 2020
From: rst at fomar.com.pl (=?UTF-8?Q?Rafa=C5=82_Stanilewicz?=)
Date: Tue, 25 Aug 2020 02:14:35 +0100
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <00ba01d67a70$a4eba200$eec2e600$@gmail.com>
References: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>
 <14988_1598261144_5F438798_14988_314_2_064671a3-5e76-4f4c-9fd9-d0c43d49b84b@OPEXCNORM52.corporate.adroot.infra.ftgroup>
 <00ba01d67a70$a4eba200$eec2e600$@gmail.com>
Message-ID: <CAPnyBTNVQk6ESUBK2Kqp_=j7pHAmen4QV2Gv7uHQE3CSpoVNDg@mail.gmail.com>

>> If for some reason many think that these RPM?s can pop up from /dev/null
I believe they are wrong.

Actually, many people do build Squid by themselves successfully, and are
willing to share the builds.

Myself, I have slightly older version than the OP requested, so I cannot
help, but I see there is some build available at
https://cbs.centos.org/koji/buildinfo?buildID=26092 (I cannot tell anything
about quality of the build, of course).

Rafal


PS. I wish there was some central repo with binary builds of squid for
multiple linux distros, verified by community and available for everyone.
But now THIS requires some CPU, RAM, food and paying the bills, so we
cannot have it easily.

On Tue, 25 Aug 2020 at 00:45, Eliezer Croitor <ngtech1ltd at gmail.com> wrote:

> Trying to understand something in the list.
>
>
>
> Anyone interested funding the build of these RPM?s?
>
> To power up some CPU, RAM etc requires food and other bills..
>
> If for some reason many think that these RPM?s can pop up from /dev/null I
> believe they are wrong.
>
>
>
> Let Me Know.
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *rahul.negi at orange.com
> *Sent:* Monday, August 24, 2020 12:26 PM
> *To:* Arsalan Hussain <arsalan at preston.edu.pk>
> *Cc:* squid-users at lists.squid-cache.org; VARSHNEY Praveen TGI/OLN <
> praveen.varshney at orange.com>
> *Subject:* Re: [squid-users] Need squid latest version 4.13 RPM packaged
> files for centos7 and x86_64 architecture
>
>
>
> Hi Arsalan,
>
>
>
> I hope to get your response soon including RPM files. Appreciate your help
> here!
>
>
>
> Thanks and Regards,
>
> Rahul Negi
>
> *From:* Arsalan Hussain [mailto:arsalan at preston.edu.pk
> <arsalan at preston.edu.pk>]
> *Sent:* Monday, August 24, 2020 12:29
> *To:* NEGI Rahul TGI/OLN
> *Cc:* squid-users at lists.squid-cache.org; Antony Stone; Amos Jeffries
> *Subject:* Re: [squid-users] Need squid latest version 4.13 RPM packaged
> files for centos7 and x86_64 architecture
>
>
>
> Dear Mr. Negi
>
>
>
> Reference to email received from Squid forum regarding Squid-4.13 release
> package by Mr. Amos Jeffries.
>
>
>
> See below information. I am planning to upgrade my server by trying it soon
>
>
>
> COPIED
>
> ........
>
> On Sun, Aug 23, 2020 at 1:35 PM Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-4.13 release!
>
>
> This release is a security release resolving several issues found in
> the prior Squid releases.
>
>
> The major changes to be aware of:
>
>  * SQUID-2020:8 HTTP(S) Request Splitting
>    (CVE-2020-15811)
>
> This problem is serious because it allows any client, including
> browser scripts, to bypass local security and poison the browser
> cache and any downstream caches with content from an arbitrary
> source.
>
> See the advisory for patches:
>  <
> https://github.com/squid-cache/squid/security/advisories/GHSA-c7p8-xqhm-49wv
> >
>
>
>  * SQUID-2020:9 Denial of Service processing Cache Digest Response
>    (CVE pending allocation)
>
> This problem allows a trusted peer to deliver to perform Denial
> of Service by consuming all available CPU cycles on the machine
> running Squid when handling a crafted Cache Digest response
> message.
>
> This attack is limited to Squid using cache_peer with cache
> digests feature.
>
> See the advisory for patches:
>  <
> https://github.com/squid-cache/squid/security/advisories/GHSA-vvj7-xjgq-g2jg
> >
>
>
>  * SQUID-2020:10 HTTP(S) Request Smuggling
>    (CVE-2020-15810)
>
> This problem is serious because it allows any client, including
> browser scripts, to bypass local security and poison the proxy
> cache and any downstream caches with content from an arbitrary
> source.
>
>
> See the advisory for patches:
>  <
> https://github.com/squid-cache/squid/security/advisories/GHSA-3365-q9qx-f98m
> >
>
>
>  * Bug 5051: Some collapsed revalidation responses never expire
>
> This bug appears as a 4xx or 5xx status response becoming the only
> response delivered by Squid to a URL when Collapsed Forwarding
> feature is used.
>
> It primarily affects Squid which are caching the 4xx/5xx status
> object since Bug 5030 fix in Squid-4.11. But may have been
> occurring for short times on any proxy with Collapsed Forwarding.
>
>
>
>  * SSL-Bump: Support parsing GREASEd (and future) TLS handshakes
>
> Chrome Browser intentionally sends random garbage values in the
> TLS handshake to force TLS implementations to cope with future TLS
> extensions cleanly. The changes in Squid-4.12 to disable TLS/1.3
> caused our parser to be extra strict and reject this TLS garbage.
>
> This release adds explicit support for Chrome, or any other TLS
> agent performing these "GREASE" behaviours.
>
>
>  * Honor on_unsupported_protocol for intercepted https_port
>
> This behaviour was one of the intended use-cases for unsupported
> protocol handling, but somehow was not enabled earlier.
>
> Squid should now be able to perform the on_unsupported_protocol
> selected action for any traffic handled by SSL-Bump.
>
>
>   All users of Squid are urged to upgrade as soon as possible.
>
>
> See the ChangeLog for the full list of changes in this and earlier
> releases.
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
> when you are ready to make the switch to Squid-4
>
> This new release can be downloaded from our HTTP or FTP servers
>
>   http://www.squid-cache.org/Versions/v4/
>   ftp://ftp.squid-cache.org/pub/squid/
>   ftp://ftp.squid-cache.org/pub/archive/4/
>
> or the mirrors. For a list of mirror sites see
>
>   http://www.squid-cache.org/Download/http-mirrors.html
>   http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
>   http://bugs.squid-cache.org/
>
>
> Amos Jeffries
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
>
>
>
>
> On Mon, Aug 24, 2020 at 9:07 AM <rahul.negi at orange.com> wrote:
>
> Hi Team,
>
> Can anyone please share squid latest stable version 4.13 RPM packaged
>  files for CentOS7  distribution and x86_64 architecture.
>
>
>
> *Thanks and Regards,*
>
> *Rahul Negi*
>
>
>
> _________________________________________________________________________________________________________________________
>
>
>
> Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
>
> pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
>
> a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
>
> Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.
>
>
>
> This message and its attachments may contain confidential or privileged information that may be protected by law;
>
> they should not be distributed, used or copied without authorisation.
>
> If you have received this email in error, please notify the sender and delete this message and its attachments.
>
> As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
>
> Thank you.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> --
>
> With Regards,
>
>
> *Arsalan Hussain*
> *Assistant Director, Networks & Information System*
>
> *PRESTON UNIVERSITY*
>
>
> *Complaining is finding faults, wisdom is finding solutions*
>
> _________________________________________________________________________________________________________________________
>
>
>
> Ce message et ses pieces jointes peuvent contenir des informations confidentielles ou privilegiees et ne doivent donc
>
> pas etre diffuses, exploites ou copies sans autorisation. Si vous avez recu ce message par erreur, veuillez le signaler
>
> a l'expediteur et le detruire ainsi que les pieces jointes. Les messages electroniques etant susceptibles d'alteration,
>
> Orange decline toute responsabilite si ce message a ete altere, deforme ou falsifie. Merci.
>
>
>
> This message and its attachments may contain confidential or privileged information that may be protected by law;
>
> they should not be distributed, used or copied without authorisation.
>
> If you have received this email in error, please notify the sender and delete this message and its attachments.
>
> As emails may be altered, Orange is not liable for messages that have been modified, changed or falsified.
>
> Thank you.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Zanim wydrukujesz, pomy?l o ?rodowisku.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200825/2f5004a9/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 25 07:16:59 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Aug 2020 19:16:59 +1200
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
Message-ID: <25c4c817-f065-5dc6-b197-102581016969@treenet.co.nz>

On 25/08/20 1:09 pm, Mathew Brown wrote:
> Thanks but even with the --no-check-certificate option and using a bump
> instead of splicing, it still fails as shown above unless I add the
> localnet rule. The question is: why does the same ACL line:
> 
> http_access allow whitelist
> 
> suddenly work when I add an unrelated ACL line after it (http_access
> allow localnet)? Why does it correctly determine the domain httpbin.org
> in the later case as shown by cache.log?
> 

The email from Alex Alex you are replying to already answers both those
questions completely.


> *From:* Alex Rousskov
...
> 
> The rules above only allow CONNECT requests to .httpbin.org domains.
> 
> During step1, when Squid intercepts a TLS connection to an IP address of
> an .httpbin.org domain, Squid http_access rules are applied to a (fake)
> CONNECT request to the destination IP address. There are no domain names
> at that TCP-level bumping stage. Thus, you place your Squid at the mercy
> of reverse DSN lookups.
> 
> In my environment, reverse DNS does not work for httpbin.org the way you
> may expect:
> 
>> $ host 54.236.246.173
>> 173.246.236.54.in-addr.arpa domain name pointer ec2-54-236-246-173.compute-1.amazonaws.com.
> 
> The above AWS domain name does not match your whitelist ACLs, of course,
> and, hence, the fake CONNECT request is denied.


Amos


From rafael.akchurin at diladele.com  Tue Aug 25 07:27:43 2020
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 25 Aug 2020 07:27:43 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.13 (rebuilt with
 sslbump support from sources in Debian unstable)
Message-ID: <AM0PR04MB4753AC27E98583A18101FB868F570@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello everyone,

The online repository with latest Squid 4.13 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid413.diladele.com.
Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation.
Scripts for Ubuntu 20 and Ubuntu 16 are also available in that repo.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid413.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid413.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient

Hope you will find this useful. Note that older repo of squid412.diladele.com will be taken down today (due to sslbump issues with Chrome fixed in Squid 4.13).

Best regards,
Rafael Akchurin
Diladele B.V.

--
The same Squid 4.13 will be part of upcoming Web Safety 7.5 planned for release in November, this version has more improvements in the report generation module (upload reporting) and other various small fixes. Download the latest virtual appliance from https://docs.diladele.com/index.html

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200825/bbecfedb/attachment.htm>

From squid3 at treenet.co.nz  Tue Aug 25 07:35:57 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Aug 2020 19:35:57 +1200
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <CAPnyBTNVQk6ESUBK2Kqp_=j7pHAmen4QV2Gv7uHQE3CSpoVNDg@mail.gmail.com>
References: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>
 <14988_1598261144_5F438798_14988_314_2_064671a3-5e76-4f4c-9fd9-d0c43d49b84b@OPEXCNORM52.corporate.adroot.infra.ftgroup>
 <00ba01d67a70$a4eba200$eec2e600$@gmail.com>
 <CAPnyBTNVQk6ESUBK2Kqp_=j7pHAmen4QV2Gv7uHQE3CSpoVNDg@mail.gmail.com>
Message-ID: <a52fff3a-f03c-7ce7-05f8-67656503d1f6@treenet.co.nz>

On 25/08/20 1:14 pm, Rafa? Stanilewicz wrote:
> 
>>> If for some reason many think that these RPM?s can pop up from /dev/null I believe they are wrong.?
> 
> Actually, many people do build Squid by themselves successfully, and are
> willing to share the builds.
> 
> Myself, I have slightly older?version than the OP requested, so I cannot
> help, but I see there is some build available at?
> https://cbs.centos.org/koji/buildinfo?buildID=26092?(I cannot tell
> anything about quality of the build, of course).?
> 
> Rafal?
> 
> 
> PS. I wish there was some central repo with binary builds of squid for
> multiple linux distros, verified by community and available for
> everyone. But now THIS requires some CPU, RAM, food and paying the
> bills, so we cannot have it easily.?
> 


FYI, Eliezer has been providing such a repository at his own expense for
some years now. Due to the large number of OS he supports in that repo
it takes time to test and verify each package.

I suggest anyone happy to help collaborate with him.

Amos


> On Tue, 25 Aug 2020 at 00:45, Eliezer Croitor wrote:
> 
>     Trying to understand something in the list.____
> 
>     __?__
> 
>     Anyone interested funding the build of these RPM?s?____
> 
>     To power up some CPU, RAM etc requires food and other bills..____
> 
>     If for some reason many think that these RPM?s can pop up from
>     /dev/null I believe they are wrong.____
> 
>     __?__
> 
>     Let Me Know.____
> 
>     __?__
> 
>     Eliezer____
> 


From squid at loel.fr  Tue Aug 25 10:35:57 2020
From: squid at loel.fr (Eric F.)
Date: Tue, 25 Aug 2020 14:35:57 +0400
Subject: [squid-users] Squid Explicit Proxying
Message-ID: <c4df7508049525a26cbaea25f1e4ba74@loel.fr>

Hi,

I use OpenBSD 6.7 with Squid 4.12.
I want to filter http and https website, so i'm trying to use SSL 
bumping.
But unfortunately, my configuration doesn't work. I explain what i did:

The host is named : proxy.lab.local

I generated the certificate like that:

cd /etc/squid
openssl req -new -newkey rsa:4096 -sha256 -days 365 -nodes -x509 -keyout 
squid.pem -out squid.pem
openssl x509 -in /etc/squid/squid.pem -outform DER -out 
/etc/squid/browser.der
chown _squid:_squid *.pem

run squid with squid -z && rcctl start squid

no errors.

I installed the browser.der on my Windows 10 laptop (added the proxy), 
therefore i can't access any webpage.

I tried on the squid server the following tests (curl)

proxy# curl --proxy http://127.0.0.1:3128 https://www.google.com
curl: (60) SSL certificate problem: self signed certificate in 
certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could 
not
establish a secure connection to it. To learn more about this situation 
and
how to fix it, please visit the web page mentioned above.

proxy# curl --proxy http://127.0.0.1:3128 --cacert /etc/squid/squid.pem 
-l https://www.google.com
curl: (35) error:1401E410:SSL routines:CONNECT_CR_FINISHED:sslv3 alert 
handshake failure

Can you help me to troubleshoot this issue ?

Thank you very much.

Below my configuration :


proxy# squid -v
Squid Cache: Version 4.12
Service Name: squid

This binary uses LibreSSL 3.1.1. For legal restrictions on distribution 
see https://www.openssl.org/source/license.html

configure options:  '--disable-strict-error-checking' 
'--disable-arch-native' '--datadir=/usr/local/share/squid' 
'--libexecdir=/usr/local/libexec/squid' '--disable-loadable-modules' 
'--enable-arp-acl' '--enable-auth' '--enable-delay-pools' 
'--enable-digest' '--enable-follow-x-forwarded-for' 
'--enable-forw-via-db' '--enable-http-violations' '--enable-icap-client' 
'--enable-ipv6' '--enable-referer-log' '--enable-removal-policies=lru 
heap' '--enable-ssl' '--enable-ssl-crtd' '--with-openssl' 
'--enable-storeio=aufs ufs diskd' '--with-default-user=_squid' 
'--with-filedescriptors=8192' '--with-krb5-config=no' 
'--with-pidfile=/var/run/squid.pid' '--with-pthreads' 
'--with-swapdir=/var/squid/cache' '--disable-pf-transparent' 
'--enable-ipfw-transparent' '--enable-external-acl-helpers=SQL_session 
file_userip time_quota  unix_group wbinfo_group  LDAP_group 
eDirectory_userip' '--prefix=/usr/local' '--sysconfdir=/etc/squid' 
'--mandir=/usr/local/man' '--infodir=/usr/local/info' 
'--localstatedir=/var/squid' '--disable-silent-rules' 
'--disable-gtk-doc' 'CC=cc' 'CFLAGS=-O2 -pipe' 
'LDFLAGS=-L/usr/local/lib' 'CPPFLAGS=-I/usr/local/include' 'CXX=c++' 
'CXXFLAGS=-O2 -pipe'

proxy# cat /etc/squid/squid.conf
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localnet src 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localnet src 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localnet src 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localnet src fc00::/7               # RFC 4193 local private network 
range
acl localnet src fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

acl bad_urls urlpath_regex -i "/etc/squid/bad_urls"
acl bad_domains dstdomain "/etc/squid/bad_domains"

http_access deny bad_urls
http_access deny bad_domains

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128 ssl-bump \
   cert=/etc/squid/squid.pem \
   generate-host-certificates=on dynamic_cert_mem_cache_size=8MB

sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s 
/var/squid/ssl_db -M 8MB

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
sslcrtd_children 5
sslproxy_cert_sign signTrusted

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/squid/cache 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

cache_mgr support at lab.local
# EOF

Cheers,
Eric


From rousskov at measurement-factory.com  Tue Aug 25 13:24:27 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Aug 2020 09:24:27 -0400
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
Message-ID: <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>

On 8/24/20 9:09 PM, Mathew Brown wrote:
> Thanks but even with the --no-check-certificate option and using a bump
> instead of splicing, it still fails as shown above unless I add the
> localnet rule. The question is: why does the same ACL line:

> http_access allow whitelist

> suddenly work when I add an unrelated ACL line after it?

You are misinterpreting the outcome of the test. That whitelist
http_access line did not work (well) for fake CONNECTs before and does
not start working (well) after another http_access rule is added. It is
the added rule that "starts working" instead!

Most ACL-driven directives, including http_access, cannot be correctly
interpreted at single-ACL, single-line, or single-rule scope. You must
consider _all_ rules for a given directive to correctly predict the
outcome of that directive evaluation. When you add a rule, the outcome
of the directive evaluation may change if the previous set of rules did
not match and the added rule does match. The latter is exactly what
happens in your "add an unrelated ACL" test case.

For completeness sake: When no http_access rules match, Squid applies
the action (allow or deny) that is the opposite of the last configured
http_access rule action. If no http_access rules were configured at all,
Squid denies.


There is similar (albeit a bit incomplete) information in the (arguably
misplaced) FAQ section at
https://wiki.squid-cache.org/SquidFaq/SquidAcl#Access_Lists


> From my understanding, Squid should perform the exact same steps in both
> cases BUT then allow the connection because of the localnet ACL line
> that it sees right before the deny all line, not because it suddenly was
> able to match httpbin.org using a domain compare as shown by the debug
> logs. What am I missing?

You may also be missing the fact that http_access directive is applied
several times for one wget execution, once at every SslBump step. The
information available to Squid at each SslBump step differs.
https://wiki.squid-cache.org/Features/SslPeekAndSplice

Domain-based ACL matches you see in the second test log probably do not
exist in the first test because Squid does not get far enough during the
first test -- Squid denies the fake CONNECT (and bumps the connection)
at step1, when no domain information is available yet.


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Tuesday, August 25, 2020 8:41 AM
> *To:* Mathew Brown <mbrown8918 at outlook.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Strange Squid SSL Interception Behavior
> ?
> On 8/24/20 6:21 PM, Mathew Brown wrote:
> 
>> acl whitelist ssl::server_name .httpbin.org
>> acl whitelist_http ssl::server_name .httpbin.org
> 
>> ssl_bump peek step1
>> ssl_bump splice all
> 
>> http_access allow whitelist
>> http_access allow whitelist_http
>> http_access deny all
> 
> The rules above only allow CONNECT requests to .httpbin.org domains.
> 
> During step1, when Squid intercepts a TLS connection to an IP address of
> an .httpbin.org domain, Squid http_access rules are applied to a (fake)
> CONNECT request to the destination IP address. There are no domain names
> at that TCP-level bumping stage. Thus, you place your Squid at the mercy
> of reverse DSN lookups.
> 
> In my environment, reverse DNS does not work for httpbin.org the way you
> may expect:
> 
>> $ host 54.236.246.173
>> 173.246.236.54.in-addr.arpa domain name pointer ec2-54-236-246-173.compute-1.amazonaws.com.
> 
> The above AWS domain name does not match your whitelist ACLs, of course,
> and, hence, the fake CONNECT request is denied. Denied requests are
> bumped to deliver the error message. Bumped requests require
> --no-check-certificate or other means of trusting Squid's CA certificate.
> 
> If you cannot explicitly allow CONNECT requests to httpbin.org IP
> addresses (e.g., because they change too often), then consider allowing
> CONNECT to safe ports at any address at step1. If you only intercept
> connections to httpbin.org IPs, then you can probably relax your step1
> http_access rules to allow all CONNECTs (to safe addresses).
> 
> There are many similar questions about allowing CONNECT to IP addresses
> on this mailing list. You may be able to find more detailed advice or
> instructions by searching for those mailing list threads.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> $ wget https://httpbin.org
>> --2020-08-24 17:48:34-- ?https://httpbin.org/
>> Resolving httpbin.org (httpbin.org)... 54.236.246.173, 3.220.112.94
>> Connecting to httpbin.org (httpbin.org)|54.236.246.173|:443... connected.
>> ERROR: cannot verify httpbin.org's certificate, issued by ?O=Internet
>> Widgits Pty Ltd,ST=Some-State,C=AU?:
>> ? Self-signed certificate encountered.
>> To connect to httpbin.org insecurely, use `--no-check-certificate'.
>> 
>> $ wget https://httpbin.org --no-check-certificate
>> --2020-08-24 17:48:40-- ?https://httpbin.org/
>> Resolving httpbin.org (httpbin.org)... 3.220.112.94, 54.236.246.173
>> Connecting to httpbin.org (httpbin.org)|3.220.112.94|:443... connected.
>> WARNING: cannot verify httpbin.org's certificate, issued by ?O=Internet
>> Widgits Pty Ltd,ST=Some-State,C=AU?:
>> ? Self-signed certificate encountered.
>> HTTP request sent, awaiting response... 403 Forbidden
>> 2020-08-24 17:48:40 ERROR 403: Forbidden.
>> 
>> looking at access.log shows:
>> 
>> 1598305800.974 ? ? ?2 192.168.123.214 TCP_DENIED/200 0 CONNECT
>> 54.236.246.173:443 - HIER_NONE/- -
>> 
>> for the first request (without the --no-check-certificate) and the
>> following for the 2nd request (with the --no-check-certificate):
>> 
>> 1598305812.292 ? ? ?3 192.168.123.214 TCP_DENIED/200 0 CONNECT
>> 54.236.246.173:443 - HIER_NONE/- -
>> 1598305812.300 ? ? ?2 192.168.123.214 NONE/403 3795 GET
>> https://httpbin.org/ - HIER_NONE/- text/html
>> 
>> looking at cache.log shows:
>> 
>> # cat /var/log/squid/cache.log ?| grep -i "28" | grep -i httpbin
>> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>> 
>> so it never matches on the httpbin.org
>> 
>> now, if I add the following line to my configuration:
>> 
>> http_access allow localnet
>> 
>> right before the:
>> 
>> http_access deny all
>> 
>> line it works and I see the following in access.log:
>> 
>> 1598305979.004 ? ? ?4 192.168.123.214 NONE/200 0 CONNECT
>> 54.236.246.173:443 - HIER_NONE/- -
>> 1598305980.016 ? 1012 192.168.123.214 TCP_TUNNEL/200 15370 CONNECT
>> httpbin.org:443 - ORIGINAL_DST/54.236.246.173 -
>> 
>> and I see the following in cache.log:
>> 
>> # cat /var/log/squid/cache.log ?| grep -i "28" | grep -i httpbin
>> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>> 2020/08/24 17:52:59.005 kid1| 28,3| RegexData.cc(43) match: checking
>> 'httpbin.org:443'
>> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(42) match: checking
>> 'httpbin.org'
>> 2020/08/24 17:52:59.005 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:httpbin.org <> ?.httpbin.org
>> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(47) match:
>> 'httpbin.org' found
>> 
>> What's puzzling is why adding the 'allow localnet' line changes the ACL
>> logic for .httpbin.org and why the original configuration does not work.
>> Any ideas? Thanks
>> 
>> PS. I reproduced the exact same scenario on Ubuntu 20.04 with Squid 4.12
>> 
>> 
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 



From jsteinberg at mmm.com  Tue Aug 25 18:43:23 2020
From: jsteinberg at mmm.com (Jonas Steinberg)
Date: Tue, 25 Aug 2020 18:43:23 +0000
Subject: [squid-users] GENEVE?
Message-ID: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>

Do recent versions of Squid support GENEVE?

Thank you,

Jonas Steinberg
Software Engineer
3M HIS (remote)
(702) 807-9888

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200825/090dcb88/attachment.htm>

From rousskov at measurement-factory.com  Tue Aug 25 18:54:51 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Aug 2020 14:54:51 -0400
Subject: [squid-users] GENEVE?
In-Reply-To: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
Message-ID: <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>

On 8/25/20 2:43 PM, Jonas Steinberg wrote:
> Do recent versions of Squid support GENEVE?

I believe Squid is unaware of draft-ietf-nvo3-geneve.

Alex.


From jsteinberg at mmm.com  Tue Aug 25 19:21:51 2020
From: jsteinberg at mmm.com (Jonas Steinberg)
Date: Tue, 25 Aug 2020 19:21:51 +0000
Subject: [squid-users] GENEVE?
In-Reply-To: <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
Message-ID: <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>

Is there any way to definitively confirm this?  Also is this something I could submit as a feature request via github or is it too crazy or out-of-scope for the roadmap?

Jonas Steinberg
Software Engineer 
3M HIS (remote)
(702) 807-9888 
 

?On 8/25/20, 1:54 PM, "Alex Rousskov" <rousskov at measurement-factory.com> wrote:

    On 8/25/20 2:43 PM, Jonas Steinberg wrote:
    > Do recent versions of Squid support GENEVE?

    I believe Squid is unaware of draft-ietf-nvo3-geneve.

    Alex.


From rousskov at measurement-factory.com  Tue Aug 25 19:39:22 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Aug 2020 15:39:22 -0400
Subject: [squid-users] GENEVE?
In-Reply-To: <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
 <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
Message-ID: <9331b83a-1b2f-f4a4-e5f1-0d7581623df5@measurement-factory.com>

On 8/25/20 3:21 PM, Jonas Steinberg wrote:

> is this something I could submit as a feature request via github or
> is it too crazy or out-of-scope for the roadmap?

I am not familiar with draft-ietf-nvo3-geneve details, but I see nothing
particularly crazy on the surface of that draft: Squid is already
capable of tunneling intercepted TLS and forwarded HTTP CONNECT traffic
while GENEVE seems like one more way to tell Squid about the desired
tunnel end points.

Perhaps some form of GENEVE support is already possible via some kind of
3rd-party wrappers? FWIW, the possible existence of such protocol
wrappers was the primary reason I did not give a straight "no" answer to
your original question...


You may file a feature request on Squid Bugzilla, keeping the following
FAQ in mind:
https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.


> ?On 8/25/20, 1:54 PM, Alex Rousskov wrote:
> 
>     On 8/25/20 2:43 PM, Jonas Steinberg wrote:
>     > Do recent versions of Squid support GENEVE?
> 
>     I believe Squid is unaware of draft-ietf-nvo3-geneve.
> 
>     Alex.


From jsteinberg at mmm.com  Tue Aug 25 19:48:13 2020
From: jsteinberg at mmm.com (Jonas Steinberg)
Date: Tue, 25 Aug 2020 19:48:13 +0000
Subject: [squid-users] GENEVE?
In-Reply-To: <9331b83a-1b2f-f4a4-e5f1-0d7581623df5@measurement-factory.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
 <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
 <9331b83a-1b2f-f4a4-e5f1-0d7581623df5@measurement-factory.com>
Message-ID: <5CACDEF0-323B-4C90-926B-897EED4C5529@mmm.com>

Any advice on where I would find such a protocol wrapper, were one to exist?  Also I assume this would mean compiling my own squid then?

Jonas Steinberg
Software Engineer 
3M HIS (remote)
(702) 807-9888 
 

?On 8/25/20, 2:39 PM, "Alex Rousskov" <rousskov at measurement-factory.com> wrote:

    On 8/25/20 3:21 PM, Jonas Steinberg wrote:

    > is this something I could submit as a feature request via github or
    > is it too crazy or out-of-scope for the roadmap?

    I am not familiar with draft-ietf-nvo3-geneve details, but I see nothing
    particularly crazy on the surface of that draft: Squid is already
    capable of tunneling intercepted TLS and forwarded HTTP CONNECT traffic
    while GENEVE seems like one more way to tell Squid about the desired
    tunnel end points.

    Perhaps some form of GENEVE support is already possible via some kind of
    3rd-party wrappers? FWIW, the possible existence of such protocol
    wrappers was the primary reason I did not give a straight "no" answer to
    your original question...


    You may file a feature request on Squid Bugzilla, keeping the following
    FAQ in mind:
    https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


    HTH,

    Alex.


    > On 8/25/20, 1:54 PM, Alex Rousskov wrote:
    > 
    >     On 8/25/20 2:43 PM, Jonas Steinberg wrote:
    >     > Do recent versions of Squid support GENEVE?
    > 
    >     I believe Squid is unaware of draft-ietf-nvo3-geneve.
    > 
    >     Alex.


From rousskov at measurement-factory.com  Tue Aug 25 20:36:18 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Aug 2020 16:36:18 -0400
Subject: [squid-users] GENEVE?
In-Reply-To: <5CACDEF0-323B-4C90-926B-897EED4C5529@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
 <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
 <9331b83a-1b2f-f4a4-e5f1-0d7581623df5@measurement-factory.com>
 <5CACDEF0-323B-4C90-926B-897EED4C5529@mmm.com>
Message-ID: <59e3a078-5a34-41d7-ca21-cadaf6094e76@measurement-factory.com>

On 8/25/20 3:48 PM, Jonas Steinberg wrote:

> Any advice on where I would find such a protocol wrapper, were one to exist?

Sorry, I do not know enough about GENEVE to suggest GENEVE-specific tool
aggregators.


>  Also I assume this would mean compiling my own squid then?

I would expect Squid to be unaware of that 3rd-party protocol wrapper.

For example, you are probably familiar with stunnel that "wraps"
plain-text TCP traffic into TLS, enabling TLS-unaware applications to
"support" TLS. I speculate one could create a tool that wraps GENEVE
traffic into HTTP CONNECT transactions that GENEVE-unaware HTTP proxies
like Squid can tunnel.

Alex.


> ?On 8/25/20, 2:39 PM, Alex Rousskov wrote:
> 
>     On 8/25/20 3:21 PM, Jonas Steinberg wrote:
> 
>     > is this something I could submit as a feature request via github or
>     > is it too crazy or out-of-scope for the roadmap?
> 
>     I am not familiar with draft-ietf-nvo3-geneve details, but I see nothing
>     particularly crazy on the surface of that draft: Squid is already
>     capable of tunneling intercepted TLS and forwarded HTTP CONNECT traffic
>     while GENEVE seems like one more way to tell Squid about the desired
>     tunnel end points.
> 
>     Perhaps some form of GENEVE support is already possible via some kind of
>     3rd-party wrappers? FWIW, the possible existence of such protocol
>     wrappers was the primary reason I did not give a straight "no" answer to
>     your original question...
> 
> 
>     You may file a feature request on Squid Bugzilla, keeping the following
>     FAQ in mind:
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > On 8/25/20, 1:54 PM, Alex Rousskov wrote:
>     > 
>     >     On 8/25/20 2:43 PM, Jonas Steinberg wrote:
>     >     > Do recent versions of Squid support GENEVE?
>     > 
>     >     I believe Squid is unaware of draft-ietf-nvo3-geneve.
>     > 
>     >     Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From leolistas at solutti.com.br  Tue Aug 25 20:54:33 2020
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 25 Aug 2020 17:54:33 -0300
Subject: [squid-users] GENEVE?
In-Reply-To: <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
 <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
Message-ID: <9ee8f2e8-c788-db1d-d8c0-54749ff92b1f@solutti.com.br>

Em 25/08/2020 16:21, Jonas Steinberg escreveu:
> Is there any way to definitively confirm this?  Also is this something I could submit as a feature request via github or is it too crazy or out-of-scope for the roadmap?
>

 ??? And please never forget that if you need some feature that is not 
there yet, you can always sponsor the dev team to develop it :)

-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From mbrown8918 at outlook.com  Tue Aug 25 22:15:40 2020
From: mbrown8918 at outlook.com (Mathew Brown)
Date: Tue, 25 Aug 2020 22:15:40 +0000
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>,
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
Message-ID: <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>

Thank you Alex for your patience and clarification. After reading your explanation and based on my limited understanding, I was able to get the following to work:

...
acl whitelist ssl::server_name .httpbin.org

...
http_access deny CONNECT !SSL_ports
http_access allow localnet CONNECT

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump terminate !whitelist
ssl_bump splice all

The above allows access only to httpbin.org and no other domains

If I understand correctly, the original issue is that the CONNECT has to be allowed explicitly and it was not (I just had the typical DENY for non SSL ports) and because of this, it was not be able to proceed in the SSL bumping path. Is my understanding correct? Thanks


________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Tuesday, August 25, 2020 11:24 PM
To: Mathew Brown <mbrown8918 at outlook.com>; squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Strange Squid SSL Interception Behavior

On 8/24/20 9:09 PM, Mathew Brown wrote:
> Thanks but even with the --no-check-certificate option and using a bump
> instead of splicing, it still fails as shown above unless I add the
> localnet rule. The question is: why does the same ACL line:

> http_access allow whitelist

> suddenly work when I add an unrelated ACL line after it?

You are misinterpreting the outcome of the test. That whitelist
http_access line did not work (well) for fake CONNECTs before and does
not start working (well) after another http_access rule is added. It is
the added rule that "starts working" instead!

Most ACL-driven directives, including http_access, cannot be correctly
interpreted at single-ACL, single-line, or single-rule scope. You must
consider _all_ rules for a given directive to correctly predict the
outcome of that directive evaluation. When you add a rule, the outcome
of the directive evaluation may change if the previous set of rules did
not match and the added rule does match. The latter is exactly what
happens in your "add an unrelated ACL" test case.

For completeness sake: When no http_access rules match, Squid applies
the action (allow or deny) that is the opposite of the last configured
http_access rule action. If no http_access rules were configured at all,
Squid denies.


There is similar (albeit a bit incomplete) information in the (arguably
misplaced) FAQ section at
https://wiki.squid-cache.org/SquidFaq/SquidAcl#Access_Lists


> From my understanding, Squid should perform the exact same steps in both
> cases BUT then allow the connection because of the localnet ACL line
> that it sees right before the deny all line, not because it suddenly was
> able to match httpbin.org using a domain compare as shown by the debug
> logs. What am I missing?

You may also be missing the fact that http_access directive is applied
several times for one wget execution, once at every SslBump step. The
information available to Squid at each SslBump step differs.
https://wiki.squid-cache.org/Features/SslPeekAndSplice

Domain-based ACL matches you see in the second test log probably do not
exist in the first test because Squid does not get far enough during the
first test -- Squid denies the fake CONNECT (and bumps the connection)
at step1, when no domain information is available yet.


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Tuesday, August 25, 2020 8:41 AM
> *To:* Mathew Brown <mbrown8918 at outlook.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Strange Squid SSL Interception Behavior
>
> On 8/24/20 6:21 PM, Mathew Brown wrote:
>
>> acl whitelist ssl::server_name .httpbin.org
>> acl whitelist_http ssl::server_name .httpbin.org
>
>> ssl_bump peek step1
>> ssl_bump splice all
>
>> http_access allow whitelist
>> http_access allow whitelist_http
>> http_access deny all
>
> The rules above only allow CONNECT requests to .httpbin.org domains.
>
> During step1, when Squid intercepts a TLS connection to an IP address of
> an .httpbin.org domain, Squid http_access rules are applied to a (fake)
> CONNECT request to the destination IP address. There are no domain names
> at that TCP-level bumping stage. Thus, you place your Squid at the mercy
> of reverse DSN lookups.
>
> In my environment, reverse DNS does not work for httpbin.org the way you
> may expect:
>
>> $ host 54.236.246.173
>> 173.246.236.54.in-addr.arpa domain name pointer ec2-54-236-246-173.compute-1.amazonaws.com.
>
> The above AWS domain name does not match your whitelist ACLs, of course,
> and, hence, the fake CONNECT request is denied. Denied requests are
> bumped to deliver the error message. Bumped requests require
> --no-check-certificate or other means of trusting Squid's CA certificate.
>
> If you cannot explicitly allow CONNECT requests to httpbin.org IP
> addresses (e.g., because they change too often), then consider allowing
> CONNECT to safe ports at any address at step1. If you only intercept
> connections to httpbin.org IPs, then you can probably relax your step1
> http_access rules to allow all CONNECTs (to safe addresses).
>
> There are many similar questions about allowing CONNECT to IP addresses
> on this mailing list. You may be able to find more detailed advice or
> instructions by searching for those mailing list threads.
>
>
> HTH,
>
> Alex.
>
>
>> $ wget https://httpbin.org
>> --2020-08-24 17:48:34--  https://httpbin.org/
>> Resolving httpbin.org (httpbin.org)... 54.236.246.173, 3.220.112.94
>> Connecting to httpbin.org (httpbin.org)|54.236.246.173|:443... connected.
>> ERROR: cannot verify httpbin.org's certificate, issued by ?O=Internet
>> Widgits Pty Ltd,ST=Some-State,C=AU?:
>>   Self-signed certificate encountered.
>> To connect to httpbin.org insecurely, use `--no-check-certificate'.
>>
>> $ wget https://httpbin.org --no-check-certificate
>> --2020-08-24 17:48:40--  https://httpbin.org/
>> Resolving httpbin.org (httpbin.org)... 3.220.112.94, 54.236.246.173
>> Connecting to httpbin.org (httpbin.org)|3.220.112.94|:443... connected.
>> WARNING: cannot verify httpbin.org's certificate, issued by ?O=Internet
>> Widgits Pty Ltd,ST=Some-State,C=AU?:
>>   Self-signed certificate encountered.
>> HTTP request sent, awaiting response... 403 Forbidden
>> 2020-08-24 17:48:40 ERROR 403: Forbidden.
>>
>> looking at access.log shows:
>>
>> 1598305800.974      2 192.168.123.214 TCP_DENIED/200 0 CONNECT
>> 54.236.246.173:443 - HIER_NONE/- -
>>
>> for the first request (without the --no-check-certificate) and the
>> following for the 2nd request (with the --no-check-certificate):
>>
>> 1598305812.292      3 192.168.123.214 TCP_DENIED/200 0 CONNECT
>> 54.236.246.173:443 - HIER_NONE/- -
>> 1598305812.300      2 192.168.123.214 NONE/403 3795 GET
>> https://httpbin.org/ - HIER_NONE/- text/html
>>
>> looking at cache.log shows:
>>
>> # cat /var/log/squid/cache.log  | grep -i "28" | grep -i httpbin
>> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>>
>> so it never matches on the httpbin.org
>>
>> now, if I add the following line to my configuration:
>>
>> http_access allow localnet
>>
>> right before the:
>>
>> http_access deny all
>>
>> line it works and I see the following in access.log:
>>
>> 1598305979.004      4 192.168.123.214 NONE/200 0 CONNECT
>> 54.236.246.173:443 - HIER_NONE/- -
>> 1598305980.016   1012 192.168.123.214 TCP_TUNNEL/200 15370 CONNECT
>> httpbin.org:443 - ORIGINAL_DST/54.236.246.173 -
>>
>> and I see the following in cache.log:
>>
>> # cat /var/log/squid/cache.log  | grep -i "28" | grep -i httpbin
>> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:54.236.246.173 <>  .httpbin.org
>> 2020/08/24 17:52:59.005 kid1| 28,3| RegexData.cc(43) match: checking
>> 'httpbin.org:443'
>> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(42) match: checking
>> 'httpbin.org'
>> 2020/08/24 17:52:59.005 kid1| 28,7| ServerName.cc(32)
>> aclHostDomainCompare: Match:httpbin.org <>  .httpbin.org
>> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(47) match:
>> 'httpbin.org' found
>>
>> What's puzzling is why adding the 'allow localnet' line changes the ACL
>> logic for .httpbin.org and why the original configuration does not work.
>> Any ideas? Thanks
>>
>> PS. I reproduced the exact same scenario on Ubuntu 20.04 with Squid 4.12
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200825/84853709/attachment.htm>

From rousskov at measurement-factory.com  Tue Aug 25 22:39:55 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Aug 2020 18:39:55 -0400
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
 <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
Message-ID: <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>

On 8/25/20 6:15 PM, Mathew Brown wrote:

> http_access deny CONNECT !SSL_ports
> http_access allow localnet CONNECT

> ssl_bump peek step1
> ssl_bump terminate !whitelist
> ssl_bump splice all


FYI: The above ssl_bump rules can be rewritten to avoid negation (which
can have surprising/unwanted effects in error cases):

  ssl_bump peek step1
  ssl_bump splice whitelist
  ssl_bump terminate all


> The above allows access only to httpbin.org and no other domains

Yes, in cases where "to httpbin.org" can be determined based on TCP
client and TLS client handshake information. For example, if reverse DNS
lookup do not work well, then the above rules will terminate TLS clients
that omit TLS SNI information or that send fake/encrypted SNI, even if
those clients are trying to establish a TLS connection with one of the
httpbin.org servers.

My example is not meant as a criticism of your configuration. Only as an
additional information about that configuration effects.


> If I understand correctly, the original issue is that the CONNECT has
> to be allowed explicitly and it was not and because of this, it was
> not be able to proceed in the SSL bumping path. Is my understanding
> correct?

Just one minor correction/clarification: It is best to think of Squid
configuration as if _everything_ has to be allowed explicitly (rather
than that something is allowed implicitly and some exceptional cases
need special rules).

For example, AFAICT, your current configuration allows local [fake]
CONNECTs to safe ports (and nothing else). That access policy is fine if
it matches your intent. The policy will stop working if you decide to
bump and forward HTTPS traffic or forward plain text HTTP requests.


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Tuesday, August 25, 2020 11:24 PM
> *To:* Mathew Brown <mbrown8918 at outlook.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Strange Squid SSL Interception Behavior
> ?
> On 8/24/20 9:09 PM, Mathew Brown wrote:
>> Thanks but even with the --no-check-certificate option and using a bump
>> instead of splicing, it still fails as shown above unless I add the
>> localnet rule. The question is: why does the same ACL line:
> 
>> http_access allow whitelist
> 
>> suddenly work when I add an unrelated ACL line after it?
> 
> You are misinterpreting the outcome of the test. That whitelist
> http_access line did not work (well) for fake CONNECTs before and does
> not start working (well) after another http_access rule is added. It is
> the added rule that "starts working" instead!
> 
> Most ACL-driven directives, including http_access, cannot be correctly
> interpreted at single-ACL, single-line, or single-rule scope. You must
> consider _all_ rules for a given directive to correctly predict the
> outcome of that directive evaluation. When you add a rule, the outcome
> of the directive evaluation may change if the previous set of rules did
> not match and the added rule does match. The latter is exactly what
> happens in your "add an unrelated ACL" test case.
> 
> For completeness sake: When no http_access rules match, Squid applies
> the action (allow or deny) that is the opposite of the last configured
> http_access rule action. If no http_access rules were configured at all,
> Squid denies.
> 
> 
> There is similar (albeit a bit incomplete) information in the (arguably
> misplaced) FAQ section at
> https://wiki.squid-cache.org/SquidFaq/SquidAcl#Access_Lists
> 
> 
>> From my understanding, Squid should perform the exact same steps in both
>> cases BUT then allow the connection because of the localnet ACL line
>> that it sees right before the deny all line, not because it suddenly was
>> able to match httpbin.org using a domain compare as shown by the debug
>> logs. What am I missing?
> 
> You may also be missing the fact that http_access directive is applied
> several times for one wget execution, once at every SslBump step. The
> information available to Squid at each SslBump step differs.
> https://wiki.squid-cache.org/Features/SslPeekAndSplice
> 
> Domain-based ACL matches you see in the second test log probably do not
> exist in the first test because Squid does not get far enough during the
> first test -- Squid denies the fake CONNECT (and bumps the connection)
> at step1, when no domain information is available yet.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Tuesday, August 25, 2020 8:41 AM
>> *To:* Mathew Brown <mbrown8918 at outlook.com>;
>> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Subject:* Re: [squid-users] Strange Squid SSL Interception Behavior
>> ?
>> On 8/24/20 6:21 PM, Mathew Brown wrote:
>> 
>>> acl whitelist ssl::server_name .httpbin.org
>>> acl whitelist_http ssl::server_name .httpbin.org
>> 
>>> ssl_bump peek step1
>>> ssl_bump splice all
>> 
>>> http_access allow whitelist
>>> http_access allow whitelist_http
>>> http_access deny all
>> 
>> The rules above only allow CONNECT requests to .httpbin.org domains.
>> 
>> During step1, when Squid intercepts a TLS connection to an IP address of
>> an .httpbin.org domain, Squid http_access rules are applied to a (fake)
>> CONNECT request to the destination IP address. There are no domain names
>> at that TCP-level bumping stage. Thus, you place your Squid at the mercy
>> of reverse DSN lookups.
>> 
>> In my environment, reverse DNS does not work for httpbin.org the way you
>> may expect:
>> 
>>> $ host 54.236.246.173
>>> 173.246.236.54.in-addr.arpa domain name pointer ec2-54-236-246-173.compute-1.amazonaws.com.
>> 
>> The above AWS domain name does not match your whitelist ACLs, of course,
>> and, hence, the fake CONNECT request is denied. Denied requests are
>> bumped to deliver the error message. Bumped requests require
>> --no-check-certificate or other means of trusting Squid's CA certificate.
>> 
>> If you cannot explicitly allow CONNECT requests to httpbin.org IP
>> addresses (e.g., because they change too often), then consider allowing
>> CONNECT to safe ports at any address at step1. If you only intercept
>> connections to httpbin.org IPs, then you can probably relax your step1
>> http_access rules to allow all CONNECTs (to safe addresses).
>> 
>> There are many similar questions about allowing CONNECT to IP addresses
>> on this mailing list. You may be able to find more detailed advice or
>> instructions by searching for those mailing list threads.
>> 
>> 
>> HTH,
>> 
>> Alex.
>> 
>> 
>>> $ wget https://httpbin.org
>>> --2020-08-24 17:48:34-- ?https://httpbin.org/
>>> Resolving httpbin.org (httpbin.org)... 54.236.246.173, 3.220.112.94
>>> Connecting to httpbin.org (httpbin.org)|54.236.246.173|:443... connected.
>>> ERROR: cannot verify httpbin.org's certificate, issued by ?O=Internet
>>> Widgits Pty Ltd,ST=Some-State,C=AU?:
>>> ? Self-signed certificate encountered.
>>> To connect to httpbin.org insecurely, use `--no-check-certificate'.
>>> 
>>> $ wget https://httpbin.org --no-check-certificate
>>> --2020-08-24 17:48:40-- ?https://httpbin.org/
>>> Resolving httpbin.org (httpbin.org)... 3.220.112.94, 54.236.246.173
>>> Connecting to httpbin.org (httpbin.org)|3.220.112.94|:443... connected.
>>> WARNING: cannot verify httpbin.org's certificate, issued by ?O=Internet
>>> Widgits Pty Ltd,ST=Some-State,C=AU?:
>>> ? Self-signed certificate encountered.
>>> HTTP request sent, awaiting response... 403 Forbidden
>>> 2020-08-24 17:48:40 ERROR 403: Forbidden.
>>> 
>>> looking at access.log shows:
>>> 
>>> 1598305800.974 ? ? ?2 192.168.123.214 TCP_DENIED/200 0 CONNECT
>>> 54.236.246.173:443 - HIER_NONE/- -
>>> 
>>> for the first request (without the --no-check-certificate) and the
>>> following for the 2nd request (with the --no-check-certificate):
>>> 
>>> 1598305812.292 ? ? ?3 192.168.123.214 TCP_DENIED/200 0 CONNECT
>>> 54.236.246.173:443 - HIER_NONE/- -
>>> 1598305812.300 ? ? ?2 192.168.123.214 NONE/403 3795 GET
>>> https://httpbin.org/ - HIER_NONE/- text/html
>>> 
>>> looking at cache.log shows:
>>> 
>>> # cat /var/log/squid/cache.log ?| grep -i "28" | grep -i httpbin
>>> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>>> 2020/08/24 17:50:00.972 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>>> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>>> 2020/08/24 17:50:12.290 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>>> 
>>> so it never matches on the httpbin.org
>>> 
>>> now, if I add the following line to my configuration:
>>> 
>>> http_access allow localnet
>>> 
>>> right before the:
>>> 
>>> http_access deny all
>>> 
>>> line it works and I see the following in access.log:
>>> 
>>> 1598305979.004 ? ? ?4 192.168.123.214 NONE/200 0 CONNECT
>>> 54.236.246.173:443 - HIER_NONE/- -
>>> 1598305980.016 ? 1012 192.168.123.214 TCP_TUNNEL/200 15370 CONNECT
>>> httpbin.org:443 - ORIGINAL_DST/54.236.246.173 -
>>> 
>>> and I see the following in cache.log:
>>> 
>>> # cat /var/log/squid/cache.log ?| grep -i "28" | grep -i httpbin
>>> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>>> 2020/08/24 17:52:59.000 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:54.236.246.173 <> ?.httpbin.org
>>> 2020/08/24 17:52:59.005 kid1| 28,3| RegexData.cc(43) match: checking
>>> 'httpbin.org:443'
>>> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(42) match: checking
>>> 'httpbin.org'
>>> 2020/08/24 17:52:59.005 kid1| 28,7| ServerName.cc(32)
>>> aclHostDomainCompare: Match:httpbin.org <> ?.httpbin.org
>>> 2020/08/24 17:52:59.005 kid1| 28,3| ServerName.cc(47) match:
>>> 'httpbin.org' found
>>> 
>>> What's puzzling is why adding the 'allow localnet' line changes the ACL
>>> logic for .httpbin.org and why the original configuration does not work.
>>> Any ideas? Thanks
>>> 
>>> PS. I reproduced the exact same scenario on Ubuntu 20.04 with Squid 4.12
>>> 
>>> 
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>> 
>> 
> 



From tamurin0525 at gmail.com  Wed Aug 26 01:30:03 2020
From: tamurin0525 at gmail.com (m k)
Date: Wed, 26 Aug 2020 10:30:03 +0900
Subject: [squid-users] (71) Protocol error (TLS code:
 X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT)
Message-ID: <CAL-uOnGZ0z7VPuytTM0=hhEw827RL-VRk-8zouE56GXwBSYx7A@mail.gmail.com>

Hi team,

Sorry for the many questions.
As an in-house SE, I plan to switch from Bluecoat to Squid.
***I am Japanese. And I can not do English.
All are Google translations.***

I am doing a load test on Squid.
Apache Jmeter is loading the self-certified WEB server.
How can I test with a self-certified WEB server with Jmeter?

Squid server:
 -set SSL Bump
 -No authentication(no AD,no Basic)
 -Other https is OK(yahoo,google,etc)

Web server:
 -Apache and ssl_mod installed
 -Registered in DNS
 -Created a self-signed certificate
 -Set the p12 file to Jmeter's SSL Manager

Jmeter
 response header

HTTP/1.1 503 Service Unavailable
Server: squid/4.4
X-Squid-Error: ERR_SECURE_CONNECT_FAIL 71

 response body
(71) Protocol error (TLS code: X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT)

This proxy and the host to which it connects could not get security
settings to handle your request, which are accepted by each other. The host
you are connecting to may not support secure connections, or the proxy may
not be able to meet the certificate requested by the host you are
connecting to.

Thank you,
kitamura
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200826/9e272ddb/attachment.htm>

From squid3 at treenet.co.nz  Wed Aug 26 04:03:59 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2020 16:03:59 +1200
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
 <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>
Message-ID: <01c346ec-c183-30d0-c2ee-e259ba38e5f4@treenet.co.nz>

On 26/08/20 10:39 am, Alex Rousskov wrote:
> On 8/25/20 6:15 PM, Mathew Brown wrote:
> 
>> http_access deny CONNECT !SSL_ports
>> http_access allow localnet CONNECT
> 

AIUI, this would be better if it works:

 http_access deny CONNECT !SSL_ports
 http_access allow CONNECT step1


Amos


From squid3 at treenet.co.nz  Wed Aug 26 04:28:01 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2020 16:28:01 +1200
Subject: [squid-users] (71) Protocol error (TLS code:
 X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT)
In-Reply-To: <CAL-uOnGZ0z7VPuytTM0=hhEw827RL-VRk-8zouE56GXwBSYx7A@mail.gmail.com>
References: <CAL-uOnGZ0z7VPuytTM0=hhEw827RL-VRk-8zouE56GXwBSYx7A@mail.gmail.com>
Message-ID: <5c62a8a9-0458-3a9d-91f9-753b95558b88@treenet.co.nz>

On 26/08/20 1:30 pm, m k wrote:
> Hi team,
> 
> Sorry for the many questions.
> As an in-house SE, I plan to switch from Bluecoat to Squid.?
> ***I am Japanese. And I can not do English.
> All are Google translations.***
> 
> I am doing a load test on Squid.
> Apache Jmeter is loading the self-certified WEB server.
> How can I test with a self-certified WEB server with Jmeter?
> 

You can use cache_peer for custom connectivity to a server:

  cache_peer jmeter.local parent 443 0 originserver \
    tls-cafile=/etc/squid/jmeter_ca_cert.pem \
    tls-default-ca=off

  cache_peer_access jmeter.local allow ...
  never_direct allow ...


Put the CA cert for jmeter in /etc/squid/jmeter_ca_cert.pem.


FYI: it is best to keep the self-signed cert as your own private CA and
give jmeter a normal server cert. Then you only have to change the
jmeter config if its cert gets compromised or needs updating for any
other reason. Squid can continue to use your self-signed CA to verify
any server certs it signed for jmeter.


Amos


From squid3 at treenet.co.nz  Wed Aug 26 04:49:40 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2020 16:49:40 +1200
Subject: [squid-users] GENEVE?
In-Reply-To: <9331b83a-1b2f-f4a4-e5f1-0d7581623df5@measurement-factory.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <1c353a19-6738-72b7-0c2b-39904bf19285@measurement-factory.com>
 <8B9B1F27-1035-4DA6-9C54-4BD412C86413@mmm.com>
 <9331b83a-1b2f-f4a4-e5f1-0d7581623df5@measurement-factory.com>
Message-ID: <3201c8e3-9107-4c8b-c5bd-b94ff2ec0f2c@treenet.co.nz>

On 26/08/20 7:39 am, Alex Rousskov wrote:
> On 8/25/20 3:21 PM, Jonas Steinberg wrote:
> 
>> is this something I could submit as a feature request via github or
>> is it too crazy or out-of-scope for the roadmap?
> 
> I am not familiar with draft-ietf-nvo3-geneve details, but I see nothing
> particularly crazy on the surface of that draft: Squid is already
> capable of tunneling intercepted TLS and forwarded HTTP CONNECT traffic
> while GENEVE seems like one more way to tell Squid about the desired
> tunnel end points.
> 

First thing that I notice is that GENEVE is UDP/IP based. HTTP CONNECT
tunnels that Squid uses are for TCP based traffic.

Taking a slightly deeper (but still brief) look through its protocol
design I see just another IP based tunnel. There are hundreds of these
already. This type of protocol is best handled by a regular router
and/or firewall.

As Alex said, Squid can be extended. But IMO this is not worth the
effort. It would be better to wait on OS networking stacks to support
the decapsulation. The OS can pass any relevant traffic to Squid via the
regular socket APIs - like how GRE and IP-IP tunnels are supported.


Amos


From squid3 at treenet.co.nz  Wed Aug 26 05:36:44 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Aug 2020 17:36:44 +1200
Subject: [squid-users] Squid Explicit Proxying
In-Reply-To: <c4df7508049525a26cbaea25f1e4ba74@loel.fr>
References: <c4df7508049525a26cbaea25f1e4ba74@loel.fr>
Message-ID: <36411430-be53-334a-9ea0-7110adc8953e@treenet.co.nz>

On 25/08/20 10:35 pm, Eric F. wrote:
> Hi,
> 
> I use OpenBSD 6.7 with Squid 4.12.
> I want to filter http and https website, so i'm trying to use SSL bumping.
> But unfortunately, my configuration doesn't work. I explain what i did:
> 
> The host is named : proxy.lab.local
> 
> I generated the certificate like that:
> 
> cd /etc/squid
> openssl req -new -newkey rsa:4096 -sha256 -days 365 -nodes -x509 -keyout
> squid.pem -out squid.pem

This creates keys. The public cert still needs to be signed. Though curl
below indicates a self-signed cert is present in the chain it gets from
Squid.
 That is a bit odd.


> openssl x509 -in /etc/squid/squid.pem -outform DER -out
> /etc/squid/browser.der

This should be done after signing. Whether you do self-signed or not
export the DER from the same file you put in the --CA parameter for the
signing process.


> chown _squid:_squid *.pem
> 
> run squid with squid -z && rcctl start squid
> 
> no errors.
> 
> I installed the browser.der on my Windows 10 laptop (added the proxy),
> therefore i can't access any webpage.

Er. You should still be able to access web pages. The traffic should
just be going via Squid if you "added the proxy" right.


> 
> I tried on the squid server the following tests (curl)
> 
> proxy# curl --proxy http://127.0.0.1:3128 https://www.google.com
> curl: (60) SSL certificate problem: self signed certificate in
> certificate chain
> More details here: https://curl.haxx.se/docs/sslcerts.html


curl on the proxy machine does not know about browser.der on the Windows
machines. This is expected result.


> 
> curl failed to verify the legitimacy of the server and therefore could not
> establish a secure connection to it. To learn more about this situation and
> how to fix it, please visit the web page mentioned above.
> 
> proxy# curl --proxy http://127.0.0.1:3128 --cacert /etc/squid/squid.pem
> -l https://www.google.com
> curl: (35) error:1401E410:SSL routines:CONNECT_CR_FINISHED:sslv3 alert
> handshake failure
> 

The -l indicates an email or FTP server being connected to. Otherwise
this command looks correct.

I start by looking up the OpenSSL error message. Unfortunately that one
produces no search results for me. You might have better luck. In
absence of any useful info about what the error means next thing is to
get the verbose output from curl to see what is going on.
 And check the Squid cache.log with "debug_options ALL,5" to see what
Squid is doing at its end.

 If that does not provide more useful clues then TCP level packet trace
in wireshark as a last resort.



> Can you help me to troubleshoot this issue ?
> 
> Thank you very much.
> 
> Below my configuration :
> 
> 
> proxy# squid -v
> Squid Cache: Version 4.12
> Service Name: squid
> 
> This binary uses LibreSSL 3.1.1. For legal restrictions on distribution
> see https://www.openssl.org/source/license.html
> 

FYI, LibreSSL is not formally supported due to the number of behavioural
differences it now has with OpenSSL. SSL-Bump is a mix of custom Squid
code and relatively low-level calls into OpenSSL. While LibreSSL usually
builds, we cannot guarantee those low-level calls do what SSL-Bump expects.


...
> 
> acl bad_urls urlpath_regex -i "/etc/squid/bad_urls"
> acl bad_domains dstdomain "/etc/squid/bad_domains"
> 
> http_access deny bad_urls
> http_access deny bad_domains
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 

Nit: that line means all the bad_* checks should be down here.


> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 3128 ssl-bump \
> ? cert=/etc/squid/squid.pem \

Nit: the option is now named tls-cert=


> ? generate-host-certificates=on dynamic_cert_mem_cache_size=8MB
> 
> sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s
> /var/squid/ssl_db -M 8MB
> 
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all


This makes SSL-Bump generate the certificates without any details from
the actual server. You can expect a lot of issues with TLS features that
need end-to-end negotiation (eg TLS/1.3 connections).

To work around that:

  acl step1 at_step SslBump1
  ssl_bump peek step1

  acl step2 at_step SslBump2
  ssl_bump stare step2

  ssl_bump bump all


> sslcrtd_children 5
> sslproxy_cert_sign signTrusted
> 


HTH
Amos


From squid at loel.fr  Wed Aug 26 06:58:27 2020
From: squid at loel.fr (Eric F.)
Date: Wed, 26 Aug 2020 10:58:27 +0400
Subject: [squid-users] Squid Explicit Proxying
In-Reply-To: <36411430-be53-334a-9ea0-7110adc8953e@treenet.co.nz>
References: <c4df7508049525a26cbaea25f1e4ba74@loel.fr>
 <36411430-be53-334a-9ea0-7110adc8953e@treenet.co.nz>
Message-ID: <6c28b7c935cb94e3e6e11c70a3b0ec72@loel.fr>

First, thank you very much for your help, you re awesome !

I can in fact browse HTTP pages, but not HTTPS.

Can i ask you a bit more help :) ?

I applied some changes :

Regarding the certificate, i read the man page 
http://man.openbsd.org/ssl
cd /etc/squid
openssl genrsa -out squid.key 4096
openssl req -new -key squid.key -out squid.csr
openssl x509 -sha256 -req -days 365 -in squid.csr -signkey squid.key 
-out squid.crt
cat squid.crt squid.key > squid.pem
chown _squid:_squid *.pem
chmod 700 *.pem
openssl x509 -in squid.pem -outform DER -out browser.der

Now when i try : curl --proxy http://127.0.0.1:3128 --cacert 
/etc/squid/squid.pem -l https://www.google.com
I get : curl: (60) SSL certificate problem: unable to get local issuer 
certificate...

On the Windows 10 laptop, i configured the proxy using inetcpl.cpl see 
attached screenshot (IE options).
I also added the browser.der to root certification in the snap 
certmgr.msc (see attached screenshot).

On Firefox, I get SEC_ERROR_UNKNOWN_ISSUER when trying to browse https 
website.
On Chrome, I get ERR_CONNECTION_CLOSED when trying to browse https 
website.

HTTP website is OK.

I enabled the debug in squid.conf like you suggested me :)

Here the squid -k parse :

   --8<--

obsd-proxy# squid -k parse
2020/08/26 10:35:44| Startup: Initializing Authentication Schemes ...
2020/08/26 10:35:44| Startup: Initialized Authentication Scheme 'basic'
2020/08/26 10:35:44| Startup: Initialized Authentication Scheme 'digest'
2020/08/26 10:35:44| Startup: Initialized Authentication Scheme 
'negotiate'
2020/08/26 10:35:44| Startup: Initialized Authentication Scheme 'ntlm'
2020/08/26 10:35:44| Startup: Initialized Authentication.
2020/08/26 10:35:44| Processing Configuration File: 
/etc/squid/squid.conf (depth 0)
2020/08/26 10:35:44| Processing: debug_options ALL,5
2020/08/26 10:35:44| Processing: acl localnet src 0.0.0.1-0.255.255.255  
# RFC 1122 "this" network (LAN)
2020/08/26 10:35:44| Processing: acl localnet src 10.0.0.0/8             
# RFC 1918 local private network (LAN)
2020/08/26 10:35:44| Processing: acl localnet src 100.64.0.0/10          
# RFC 6598 shared address space (CGN)
2020/08/26 10:35:44| Processing: acl localnet src 169.254.0.0/16         
# RFC 3927 link-local (directly plugged) machines
2020/08/26 10:35:44| Processing: acl localnet src 172.16.0.0/12          
# RFC 1918 local private network (LAN)
2020/08/26 10:35:44| Processing: acl localnet src 192.168.0.0/16         
# RFC 1918 local private network (LAN)
2020/08/26 10:35:44| Processing: acl localnet src fc00::/7               
# RFC 4193 local private network range
2020/08/26 10:35:44| Processing: acl localnet src fe80::/10              
# RFC 4291 link-local (directly plugged) machines
2020/08/26 10:35:44| Processing: acl SSL_ports port 443
2020/08/26 10:35:44| Processing: acl Safe_ports port 80          # http
2020/08/26 10:35:44| Processing: acl Safe_ports port 21          # ftp
2020/08/26 10:35:44| Processing: acl Safe_ports port 443         # https
2020/08/26 10:35:44| Processing: acl Safe_ports port 70          # 
gopher
2020/08/26 10:35:44| Processing: acl Safe_ports port 210         # wais
2020/08/26 10:35:44| Processing: acl Safe_ports port 1025-65535  # 
unregistered ports
2020/08/26 10:35:44| Processing: acl Safe_ports port 280         # 
http-mgmt
2020/08/26 10:35:44| Processing: acl Safe_ports port 488         # 
gss-http
2020/08/26 10:35:44| Processing: acl Safe_ports port 591         # 
filemaker
2020/08/26 10:35:44| Processing: acl Safe_ports port 777         # 
multiling http
2020/08/26 10:35:44| Processing: acl CONNECT method CONNECT
2020/08/26 10:35:44| Processing: http_access deny !Safe_ports
2020/08/26 10:35:44| Processing: http_access deny CONNECT !SSL_ports
2020/08/26 10:35:44| Processing: http_access allow localhost manager
2020/08/26 10:35:44| Processing: http_access deny manager
2020/08/26 10:35:44| Processing: acl bad_urls urlpath_regex -i 
"/etc/squid/bad_urls"
2020/08/26 10:35:44| Processing: acl bad_domains dstdomain 
"/etc/squid/bad_domains"
2020/08/26 10:35:44| Processing: http_access deny bad_urls
2020/08/26 10:35:44| Processing: http_access deny bad_domains
2020/08/26 10:35:44| Processing: http_access allow localnet
2020/08/26 10:35:44| Processing: http_access allow localhost
2020/08/26 10:35:44| Processing: http_access deny all
2020/08/26 10:35:44| Processing: http_port 3128 ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
tls-cert=/etc/squid/squid.pem
2020/08/26 10:35:44| Processing: acl step1 at_step SslBump1
2020/08/26 10:35:44| Processing: ssl_bump peek step1
2020/08/26 10:35:44| Processing: acl step2 at_step SslBump2
2020/08/26 10:35:44| Processing: ssl_bump stare step2
2020/08/26 10:35:44| Processing: ssl_bump bump all
2020/08/26 10:35:44| Processing: sslcrtd_program 
/usr/local/libexec/squid/security_file_certgen -s /var/squid/ssl_db -M 
4MB
2020/08/26 10:35:44| Processing: sslcrtd_children 5
2020/08/26 10:35:44| Processing: sslproxy_cert_sign signTrusted
2020/08/26 10:35:44| Processing: coredump_dir /var/squid/cache
2020/08/26 10:35:44| Processing: refresh_pattern ^ftp:           1440    
20%     10080
2020/08/26 10:35:44| Processing: refresh_pattern ^gopher:        1440    
0%      1440
2020/08/26 10:35:44| Processing: refresh_pattern -i (/cgi-bin/|\?) 0     
0%      0
2020/08/26 10:35:44| Processing: refresh_pattern .               0       
20%     4320
2020/08/26 10:35:44| Processing: cache_mgr informatique at dmp.re
2020/08/26 10:35:44| Initializing https:// proxy context
2020/08/26 10:35:44| Initializing http_port [::]:3128 TLS contexts
2020/08/26 10:35:44| Using certificate in /etc/squid/squid.pem
2020/08/26 10:35:44| Using certificate chain in /etc/squid/squid.pem
2020/08/26 10:35:44| Adding issuer CA: ...CN=proxy.lab.local...
2020/08/26 10:35:44| Using key in /etc/squid/squid.pem
2020/08/26 10:35:44| Initializing http_port 0.0.0.0:3128 TLS contexts
2020/08/26 10:35:44| Using certificate in /etc/squid/squid.pem
2020/08/26 10:35:44| Using certificate chain in /etc/squid/squid.pem
2020/08/26 10:35:44| Adding issuer CA: ...CN=proxy.lab.local...
2020/08/26 10:35:44| Using key in /etc/squid/squid.pem
2020/08/26 10:35:44.677| 20,5| src/store.cc(352) ~StoreEntry: StoreEntry 
destructed, this=0x4b943c9abb0
2020/08/26 10:35:44.678| 83,5| src/security/PeerOptions.h(112) 
operator(): SSL_CTX destruct, this=0x4bb9e638b00
2020/08/26 10:35:44.686| 83,5| src/security/PeerOptions.h(112) 
operator(): SSL_CTX destruct, this=0x4bc0acfac80
2020/08/26 10:35:44.686| 83,5| src/security/PeerOptions.h(112) 
operator(): SSL_CTX destruct, this=0x4bb4a362600

   -->8--

squid.conf, now looks like :

   --8<--

debug_options ALL,5

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localnet src 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localnet src 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localnet src 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localnet src fc00::/7               # RFC 4193 local private network 
range
acl localnet src fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

acl bad_urls urlpath_regex -i "/etc/squid/bad_urls"
acl bad_domains dstdomain "/etc/squid/bad_domains"

http_access deny bad_urls
http_access deny bad_domains

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB \
   tls-cert=/etc/squid/squid.pem

acl step1 at_step SslBump1
ssl_bump peek step1
acl step2 at_step SslBump2
ssl_bump stare step2
ssl_bump bump all

sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s 
/var/squid/ssl_db -M 4MB

sslcrtd_children 5
sslproxy_cert_sign signTrusted

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/squid/cache 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

cache_mgr informatique at lab.local

   --8<--


Le 2020-08-26 09:36, Amos Jeffries a ?crit?:
> On 25/08/20 10:35 pm, Eric F. wrote:

> FYI, LibreSSL is not formally supported due to the number of 
> behavioural
> differences it now has with OpenSSL. SSL-Bump is a mix of custom Squid
> code and relatively low-level calls into OpenSSL. While LibreSSL 
> usually
> builds, we cannot guarantee those low-level calls do what SSL-Bump 
> expects.

Which Linux distribution do you advise me ?? Do i need to forget OpenBSD 
?


Thank you so much!
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cert-added.png
Type: image/png
Size: 2981 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200826/ec6e19ab/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: config-proxy-10.png
Type: image/png
Size: 12540 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200826/ec6e19ab/attachment-0001.png>

From mbrown8918 at outlook.com  Wed Aug 26 11:03:52 2020
From: mbrown8918 at outlook.com (Mathew Brown)
Date: Wed, 26 Aug 2020 11:03:52 +0000
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <01c346ec-c183-30d0-c2ee-e259ba38e5f4@treenet.co.nz>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
 <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>,
 <01c346ec-c183-30d0-c2ee-e259ba38e5f4@treenet.co.nz>
Message-ID: <VI1PR04MB5184538F4A936DB8AAD9BE83DF540@VI1PR04MB5184.eurprd04.prod.outlook.com>

Thank you Alex + Amos :) You've really helped clarify things. I had a final question regarding this setup. Does this configuration only look at the client side part of the SNI request or also the server certificate. If it only looks at the client-side, how would I tell it to look at the server response as well? Thanks.
________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Sent: Wednesday, August 26, 2020 2:03 PM
To: squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Strange Squid SSL Interception Behavior

On 26/08/20 10:39 am, Alex Rousskov wrote:
> On 8/25/20 6:15 PM, Mathew Brown wrote:
>
>> http_access deny CONNECT !SSL_ports
>> http_access allow localnet CONNECT
>

AIUI, this would be better if it works:

 http_access deny CONNECT !SSL_ports
 http_access allow CONNECT step1


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200826/d574572e/attachment.htm>

From squid at loel.fr  Wed Aug 26 11:52:37 2020
From: squid at loel.fr (Eric F.)
Date: Wed, 26 Aug 2020 15:52:37 +0400
Subject: [squid-users] error:transaction-end-before-headers
In-Reply-To: <6c28b7c935cb94e3e6e11c70a3b0ec72@loel.fr>
References: <c4df7508049525a26cbaea25f1e4ba74@loel.fr>
 <36411430-be53-334a-9ea0-7110adc8953e@treenet.co.nz>
 <6c28b7c935cb94e3e6e11c70a3b0ec72@loel.fr>
Message-ID: <0f21b22cd5b8186cab26d51652327da4@loel.fr>

Hi,

I use squid 4.12 with LDAP (Active Directory).
All works great except sometimes I have the following errors in my 
access.log file :

1598438527.315      0 192.168.0.50 NONE/000 0 NONE 
error:transaction-end-before-headers - HIER_NONE/- -

How can i correct that ? Any suggestions ?

Below my squid.conf file :

   --8<--

acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localnet src 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localnet src 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localnet src 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localnet src fc00::/7               # RFC 4193 local private network 
range
acl localnet src fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT


http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl bad_urls urlpath_regex -i "/etc/squid/bad_urls"
acl bad_domains dstdomain "/etc/squid/bad_domains"

http_access deny bad_urls
http_access deny bad_domains

auth_param basic program /usr/local/libexec/squid/basic_ldap_auth -P -R 
-b dc=lab,dc=local -D cn=squid,cn=users,dc=lab,dc=local -w squid -f 
"(&(objectClass=person)(sAMAccountName=%s))" -v 3 192.168.0.7:389

acl ldap-auth proxy_auth REQUIRED
http_access allow ldap-auth

http_access allow localnet
http_access allow localhost

http_access deny all

http_port 3128

coredump_dir /var/squid/cache

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

cache_mgr informatique at lab.local

   -->8--

Thank you very much !

Cheers,

Eric F.


From ngtech1ltd at gmail.com  Wed Aug 26 12:06:13 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Wed, 26 Aug 2020 15:06:13 +0300
Subject: [squid-users] I would like to know performance sizing aspects.
In-Reply-To: <CAL-uOnG3V-izLFxO9Ry-=jNo01_==q0w_9c5ef=wvTx92dknhA@mail.gmail.com>
References: <CAL-uOnEThtS-UFoxFuY+rmfpqi66G-7SPGmYv9LJXZLYkBZ3SQ@mail.gmail.com>
 <8de1e4e0-b816-596d-5d77-75ae164cc8e1@treenet.co.nz>
 <CAL-uOnEVgXNMEmhnC0nk8=O5whK+8D8LqwymvoFusmyH0NosqQ@mail.gmail.com>
 <000d01d66bab$6f7a1620$4e6e4260$@gmail.com>
 <CAL-uOnEBxDvYw898DVUZvzwa9dk2uyM2AxX=-tx5VrdTn35nYw@mail.gmail.com>
 <a1bf0203-4db2-b5f9-2a1e-679e53000fab@treenet.co.nz>
 <CAL-uOnG3V-izLFxO9Ry-=jNo01_==q0w_9c5ef=wvTx92dknhA@mail.gmail.com>
Message-ID: <012501d67ba1$4c245950$e46d0bf0$@gmail.com>

Hey Kitamura,

 

Technically speaking Openstack admin can create a flavor which has 1 vCPU and 16GB RAM however,
it?s recommended to have 1 vCPU per 4 GB of RAM.

Openstack default vCPU ratio is 16 vCPUs per 1 physical Core.

So for a proxy which use SSL-Bump it?s recommended to have more then 1 vCPU ie at-least 2 if not 4.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of m k
Sent: Monday, August 17, 2020 2:12 PM
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] I would like to know performance sizing aspects.

 

Hi all,

 

I built squid using SSL-bump. In addition, squid also authenticates users with active directory. The hardware is openstack virtual. os is centos8.1. There is one CPU. The memory is 16GB. The hard disk is SSD 200GB.

 

I'm thinking of load testing with Apache Jmeter in this environment.

I don't know the standard, so the test stops and I am in trouble.

How many simultaneous connection sessions?

How many requests per minute?

Help me.

 

thank you,

Kitamura

 

2020?8?7?(?) 8:53 Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> >:

On 6/08/20 5:28 pm, m k wrote:

> Eliezer,

> 

> Squid's default setting is 1 core CPU, 16GB mem.

> How many URLs(Blacklist) will degrade Squid's performance?

> 



Eliezer's answer covers that already, so I will skip here.





> Also, SSL-Bump.

> 



This is "unknown" - as far as I am aware none has published numbers

recently about it. There are a lot of factors in the network traffic and

your servers internal state (eg the RNG engine) that multiply up to

cause varying amounts of delay - plus the volatile nature of this

feature set itself month by month changes the effects or relevance of

each factor.

  So numbers from me today will be wrong in a few weeks, or may be wrong

for your network already. All that we can be sure of is that there is

extra work needed by Squid thus "slower" than plain-text HTTP is to be

expected.



For planning the consideration is just to be aware that the numbers we

can give you (for plain-text) will be over-estimates of capacity for

SSL-Bump traffic and allow some margins. Once you have an install

running you can test and measure the actual numbers for your traffic.





Amos

_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 

http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200826/45be03c3/attachment.htm>

From belle at bazuin.nl  Wed Aug 26 12:11:33 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 26 Aug 2020 14:11:33 +0200
Subject: [squid-users] error:transaction-end-before-headers
In-Reply-To: <0f21b22cd5b8186cab26d51652327da4@loel.fr>
References: <6c28b7c935cb94e3e6e11c70a3b0ec72@loel.fr>
Message-ID: <vmime.5f465175.3132.4100bc331743b845@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

Just something i noticed.. 

> auth_param basic program 
> /usr/local/libexec/squid/basic_ldap_auth -P -R 
> -b dc=lab,dc=local -D cn=squid,cn=users,dc=lab,dc=local -w squid -f 
> "(&(objectClass=person)(sAMAccountName=%s))" -v 3 192.168.0.7:389

Change that to:  
auth_param basic program 
 /usr/local/libexec/squid/basic_ldap_auth -P -R 
 -b dc=lab,dc=local -D cn=squid,cn=users,dc=lab,dc=local  -W /etc/squid/ldap-bind-pwdfile
 -f "(&(objectClass=person)(sAMAccountName=%s))" -v 3 192.168.0.7:389

-w squid is changed to 
-W path/2//etc/squid/ldap-bind-pwdfile_containing_your_password. 

Only add your password in there and only give squid read rights. 

Why, if someone runs ps, they might catch the squid password your using.. 

On your question, see also. 
https://www.mail-archive.com/squid-users at lists.squid-cache.org/msg19734.html 

I cant answer it myself, i dont know. 


Greetz, 

Louis



> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens Eric F.
> Verzonden: woensdag 26 augustus 2020 13:53
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: [squid-users] error:transaction-end-before-headers
> 
> Hi,
> 
> I use squid 4.12 with LDAP (Active Directory).
> All works great except sometimes I have the following errors in my 
> access.log file :
> 
> 1598438527.315      0 192.168.0.50 NONE/000 0 NONE 
> error:transaction-end-before-headers - HIER_NONE/- -
> 
> How can i correct that ? Any suggestions ?
> 
> Below my squid.conf file :
> 
>    --8<--
> 
> acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" 
> network (LAN)
> acl localnet src 10.0.0.0/8             # RFC 1918 local 
> private network 
> (LAN)
> acl localnet src 100.64.0.0/10          # RFC 6598 shared 
> address space 
> (CGN)
> acl localnet src 169.254.0.0/16         # RFC 3927 link-local 
> (directly 
> plugged) machines
> acl localnet src 172.16.0.0/12          # RFC 1918 local 
> private network 
> (LAN)
> acl localnet src 192.168.0.0/16         # RFC 1918 local 
> private network 
> (LAN)
> acl localnet src fc00::/7               # RFC 4193 local 
> private network 
> range
> acl localnet src fe80::/10              # RFC 4291 link-local 
> (directly 
> plugged) machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> 
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> acl bad_urls urlpath_regex -i "/etc/squid/bad_urls"
> acl bad_domains dstdomain "/etc/squid/bad_domains"
> 
> http_access deny bad_urls
> http_access deny bad_domains
> 
> auth_param basic program 
> /usr/local/libexec/squid/basic_ldap_auth -P -R 
> -b dc=lab,dc=local -D cn=squid,cn=users,dc=lab,dc=local -w squid -f 
> "(&(objectClass=person)(sAMAccountName=%s))" -v 3 192.168.0.7:389
> 
> acl ldap-auth proxy_auth REQUIRED
> http_access allow ldap-auth
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all
> 
> http_port 3128
> 
> coredump_dir /var/squid/cache
> 
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> cache_mgr informatique at lab.local
> 
>    -->8--
> 
> Thank you very much !
> 
> Cheers,
> 
> Eric F.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Wed Aug 26 13:13:59 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2020 01:13:59 +1200
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <VI1PR04MB5184538F4A936DB8AAD9BE83DF540@VI1PR04MB5184.eurprd04.prod.outlook.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
 <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>
 <01c346ec-c183-30d0-c2ee-e259ba38e5f4@treenet.co.nz>
 <VI1PR04MB5184538F4A936DB8AAD9BE83DF540@VI1PR04MB5184.eurprd04.prod.outlook.com>
Message-ID: <93472f81-677f-52cf-b075-9c95d9e63ccc@treenet.co.nz>

On 26/08/20 11:03 pm, Mathew Brown wrote:
> Thank you Alex + Amos :) You've really helped clarify things. I had a
> final question regarding this setup. Does this configuration only look
> at the client side part of the SNI request or also the server
> certificate. If it only looks at the client-side, how would I tell it to
> look at the server response as well? Thanks.


SSL-Bump step 1 decides whether to look at the client handshake details.

Step 2 decides whether to look at the server handshake details.

Step 3 decides what to do given all available info from both handshakes.

The process is all described at
<https://wiki.squid-cache.org/Features/SslPeekAndSplice>


Amos


From rousskov at measurement-factory.com  Wed Aug 26 13:54:51 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 26 Aug 2020 09:54:51 -0400
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <93472f81-677f-52cf-b075-9c95d9e63ccc@treenet.co.nz>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
 <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>
 <01c346ec-c183-30d0-c2ee-e259ba38e5f4@treenet.co.nz>
 <VI1PR04MB5184538F4A936DB8AAD9BE83DF540@VI1PR04MB5184.eurprd04.prod.outlook.com>
 <93472f81-677f-52cf-b075-9c95d9e63ccc@treenet.co.nz>
Message-ID: <38a27e1f-91b9-fb2d-ac6c-70e33e26f6ea@measurement-factory.com>

On 8/26/20 9:13 AM, Amos Jeffries wrote:
> On 26/08/20 11:03 pm, Mathew Brown wrote:
>> Thank you Alex + Amos :) You've really helped clarify things. I had a
>> final question regarding this setup. Does this configuration only look
>> at the client side part of the SNI request or also the server
>> certificate.

>> acl whitelist ssl::server_name .httpbin.org
>> 
>> http_access deny CONNECT !SSL_ports
>> http_access allow localnet CONNECT
>> 
>> ssl_bump peek step1
>> ssl_bump splice whitelist
>> ssl_bump terminate all


The above ssl_bump configuration ignores the TCP client information
(during step1) and looks at TLS client information (during the next step
-- step2). With this configuration, Squid will not see the server
certificate at all.


>> If it only looks at the client-side, how would I tell it to
>> look at the server response as well?

If you want Squid to consider the server certificate as well (during
step3), replace "step1" with "all". See ssl::server_name ACL for the
documentation of what "as well" really means in this context. Its
complicated.


> The process is all described at
> https://wiki.squid-cache.org/Features/SslPeekAndSplice

Yes, and also see the documentation for the ssl::server_name ACL. In
modern Squids, you can control what information that ACL is using.


BTW, I just realized that my earlier statements about reverse DNS
lookups were misleading: The ssl::server_name ACL does not do any DNS
lookups. When given an unresolved IP address, that ACL will usually
mismatch .httpbin.org (regardless of whether the reverse lookup would
have returned a matching domain name).


HTH,

Alex.


From mbrown8918 at outlook.com  Wed Aug 26 19:12:11 2020
From: mbrown8918 at outlook.com (Mathew Brown)
Date: Wed, 26 Aug 2020 19:12:11 +0000
Subject: [squid-users] Strange Squid SSL Interception Behavior
In-Reply-To: <38a27e1f-91b9-fb2d-ac6c-70e33e26f6ea@measurement-factory.com>
References: <DB7PR04MB518029A29823D7C129BAE49EDF560@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <9b941bef-08b0-c817-90dd-6efde6a5cdb9@measurement-factory.com>
 <DB7PR04MB51800B1C539C590A42CA5DCEDF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <3286fab7-6f19-6a81-9b11-951faac9613f@measurement-factory.com>
 <DB7PR04MB518016B1A26906912E8DBEA0DF570@DB7PR04MB5180.eurprd04.prod.outlook.com>
 <a2e70c0f-d618-dad4-f5bb-f6f5044b666b@measurement-factory.com>
 <01c346ec-c183-30d0-c2ee-e259ba38e5f4@treenet.co.nz>
 <VI1PR04MB5184538F4A936DB8AAD9BE83DF540@VI1PR04MB5184.eurprd04.prod.outlook.com>
 <93472f81-677f-52cf-b075-9c95d9e63ccc@treenet.co.nz>,
 <38a27e1f-91b9-fb2d-ac6c-70e33e26f6ea@measurement-factory.com>
Message-ID: <VI1PR04MB518473CDF2E370BE03A04544DF540@VI1PR04MB5184.eurprd04.prod.outlook.com>

Thanks Alex
________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Wednesday, August 26, 2020 11:54 PM
To: Mathew Brown <mbrown8918 at outlook.com>; squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Strange Squid SSL Interception Behavior

On 8/26/20 9:13 AM, Amos Jeffries wrote:
> On 26/08/20 11:03 pm, Mathew Brown wrote:
>> Thank you Alex + Amos :) You've really helped clarify things. I had a
>> final question regarding this setup. Does this configuration only look
>> at the client side part of the SNI request or also the server
>> certificate.

>> acl whitelist ssl::server_name .httpbin.org
>>
>> http_access deny CONNECT !SSL_ports
>> http_access allow localnet CONNECT
>>
>> ssl_bump peek step1
>> ssl_bump splice whitelist
>> ssl_bump terminate all


The above ssl_bump configuration ignores the TCP client information
(during step1) and looks at TLS client information (during the next step
-- step2). With this configuration, Squid will not see the server
certificate at all.


>> If it only looks at the client-side, how would I tell it to
>> look at the server response as well?

If you want Squid to consider the server certificate as well (during
step3), replace "step1" with "all". See ssl::server_name ACL for the
documentation of what "as well" really means in this context. Its
complicated.


> The process is all described at
> https://wiki.squid-cache.org/Features/SslPeekAndSplice

Yes, and also see the documentation for the ssl::server_name ACL. In
modern Squids, you can control what information that ACL is using.


BTW, I just realized that my earlier statements about reverse DNS
lookups were misleading: The ssl::server_name ACL does not do any DNS
lookups. When given an unresolved IP address, that ACL will usually
mismatch .httpbin.org (regardless of whether the reverse lookup would
have returned a matching domain name).


HTH,

Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200826/34e31f19/attachment.htm>

From ngtech1ltd at gmail.com  Wed Aug 26 22:24:18 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 27 Aug 2020 01:24:18 +0300
Subject: [squid-users] IPVS/LVS load balancing Squid servers, anyone did it?
Message-ID: <002d01d67bf7$a5084710$ef18d530$@gmail.com>

Hey All,

 

I am reading about LB and tried to find an up-to-date example or tutorial
specific to squid with no luck.

I have seen:
http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS

 

Which makes sense and also is similar or kind of identical to WCCP with gre.

 

Anyone knows about a working Squid setup with IPVS/LVS?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/573a4551/attachment.htm>

From ngtech1ltd at gmail.com  Thu Aug 27 00:05:05 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 27 Aug 2020 03:05:05 +0300
Subject: [squid-users] GENEVE?
In-Reply-To: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
Message-ID: <004801d67c05$b8f3bb70$2adb3250$@gmail.com>

Hey Jonas,

 

What would you expect from Squid to be able to support GENEVE?

Squid works with any tunnel the OS support:

*	GRE
*	IPIP/IP6IP
*	VXLAN
*	Others( https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels/)


>From Squid point of view you on packets and connections there is no need to handle any level of the network stack like tunneling.

 

What would you want to try and use GENEVE for with squid?

The only setup I know about which Squid care about the actual tunnel(GRE) in a way is with WCCP.

 

I would be happy to hear about GENEVE and Squid usage.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Jonas Steinberg
Sent: Tuesday, August 25, 2020 9:43 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] GENEVE?

 

Do recent versions of Squid support GENEVE?

Thank you,

 

Jonas Steinberg

Software Engineer 
3M HIS (remote)
(702) 807-9888 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/38c99c58/attachment.htm>

From ngtech1ltd at gmail.com  Thu Aug 27 00:36:18 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 27 Aug 2020 03:36:18 +0300
Subject: [squid-users] Need squid latest version 4.13 RPM packaged files
 for centos7 and x86_64 architecture
In-Reply-To: <a52fff3a-f03c-7ce7-05f8-67656503d1f6@treenet.co.nz>
References: <14984_1598242014_5F433CDE_14984_258_1_BC8558EDF572F04D9DEAED25CCC290DB5B0C36@OPEXCNORM51.corporate.adroot.infra.ftgroup>
 <CAMwDxM3B5=a-+uQ9rxor_iyWCv57eNNmVCc2BS=Cct6nwmddXQ@mail.gmail.com>
 <14988_1598261144_5F438798_14988_314_2_064671a3-5e76-4f4c-9fd9-d0c43d49b84b@OPEXCNORM52.corporate.adroot.infra.ftgroup>
 <00ba01d67a70$a4eba200$eec2e600$@gmail.com>
 <CAPnyBTNVQk6ESUBK2Kqp_=j7pHAmen4QV2Gv7uHQE3CSpoVNDg@mail.gmail.com>
 <a52fff3a-f03c-7ce7-05f8-67656503d1f6@treenet.co.nz>
Message-ID: <005d01d67c0a$150e8a80$3f2b9f80$@gmail.com>

I have posted in the past a URL:
https://onedrive.live.com/?authkey=%21AFs60Exv3C4B%2DNI&id=6AB28772521B8B88%214385&cid=6AB28772521B8B88

And I am still looking for some sponsorship so it would pay for something.
>From time to time you can expect:
http://www.ngtech.co.il/repo/

to be up.

Cern have been mirroring my repo for many years at:
http://linuxsoft.cern.ch/mirror/www1.ngtech.co.il/repo/centos/

But I haven't seen any updates from their side in the last half a year.
If they have someone that can contact me I think it would be nice to have it hosted there.

For anyone that is willing to build RPM's for his local usage the docker build nodes sources are at:
https://github.com/elico/squid-docker-build-nodes

I did noticed that Amazon Linux 2 is building and maintaining the latest 4 branch with ssl-bump:
https://aws.amazon.com/premiumsupport/knowledge-center/ec2-install-extras-library-software/

Let me know if someone is willing to fund something.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, August 25, 2020 10:36 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need squid latest version 4.13 RPM packaged files for centos7 and x86_64 architecture

On 25/08/20 1:14 pm, Rafa? Stanilewicz wrote:
> 
>>> If for some reason many think that these RPM?s can pop up from 
>>> /dev/null I believe they are wrong.
> 
> Actually, many people do build Squid by themselves successfully, and 
> are willing to share the builds.
> 
> Myself, I have slightly older version than the OP requested, so I 
> cannot help, but I see there is some build available at
> https://cbs.centos.org/koji/buildinfo?buildID=26092 (I cannot tell 
> anything about quality of the build, of course).
> 
> Rafal
> 
> 
> PS. I wish there was some central repo with binary builds of squid for 
> multiple linux distros, verified by community and available for 
> everyone. But now THIS requires some CPU, RAM, food and paying the 
> bills, so we cannot have it easily.
> 


FYI, Eliezer has been providing such a repository at his own expense for some years now. Due to the large number of OS he supports in that repo it takes time to test and verify each package.

I suggest anyone happy to help collaborate with him.

Amos


> On Tue, 25 Aug 2020 at 00:45, Eliezer Croitor wrote:
> 
>     Trying to understand something in the list.____
> 
>     __ __
> 
>     Anyone interested funding the build of these RPM?s?____
> 
>     To power up some CPU, RAM etc requires food and other bills..____
> 
>     If for some reason many think that these RPM?s can pop up from
>     /dev/null I believe they are wrong.____
> 
>     __ __
> 
>     Let Me Know.____
> 
>     __ __
> 
>     Eliezer____
> 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From jsteinberg at mmm.com  Thu Aug 27 01:43:15 2020
From: jsteinberg at mmm.com (Jonas Steinberg)
Date: Thu, 27 Aug 2020 01:43:15 +0000
Subject: [squid-users] GENEVE?
In-Reply-To: <004801d67c05$b8f3bb70$2adb3250$@gmail.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <004801d67c05$b8f3bb70$2adb3250$@gmail.com>
Message-ID: <0C9A94C0-98CE-434F-BD3F-B4EAF043BC75@mmm.com>

Amos:

Your logic to me is very sound and frankly I had no idea that Squid did not handle the tunneling aspect of the network stack and furthermore it makes sense that a router or firewall would be the right appliance to implement such a protocol as the appliance requiring me to have some GENEVE-aware appliance will work with basically any firewall or router (because many support GENEVE).  At this point ?I?m over it? as the kids say because it seems convoluted, tangential and I suppose even anti-patterned to implement such a feature on Squid.  In any event I?m glad to have gotten a thorough and serious response.

Eliezer:

I have no use case.  My cloud provider has written a software-defined ?appliance? meant to integrate with firewalls and routers.  I was complaining that I had no way to integrate it with my DNS filtering workflows (Squid).  They told me ?Hey, if it?ll support GENEVE then you can make it work.?  So I simply came here to ask.

I mean?if anyone has any ideas of how I can get something to work without buying anything expensive I?d certainly be grateful!

Jonas Steinberg
Software Engineer
3M HIS (remote)

From: Eliezer Croitor <ngtech1ltd at gmail.com>
Date: Wednesday, August 26, 2020 at 7:05 PM
To: Jonas Steinberg <jsteinberg at mmm.com>, "squid-users at lists.squid-cache.org" <squid-users at lists.squid-cache.org>
Subject: [EXTERNAL] RE: [squid-users] GENEVE?

Hey Jonas,

What would you expect from Squid to be able to support GENEVE?
Squid works with any tunnel the OS support:

  *   GRE
  *   IPIP/IP6IP
  *   VXLAN
  *   Others( https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels/)

From Squid point of view you on packets and connections there is no need to handle any level of the network stack like tunneling.

What would you want to try and use GENEVE for with squid?
The only setup I know about which Squid care about the actual tunnel(GRE) in a way is with WCCP.

I would be happy to hear about GENEVE and Squid usage.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com<mailto:ngtech1ltd at gmail.com>

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Jonas Steinberg
Sent: Tuesday, August 25, 2020 9:43 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] GENEVE?

Do recent versions of Squid support GENEVE?

Thank you,

Jonas Steinberg
Software Engineer
3M HIS (remote)
(702) 807-9888

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/06ee08b5/attachment.htm>

From mdr at dotforge.ch  Thu Aug 27 04:19:33 2020
From: mdr at dotforge.ch (Marcel de Riedmatten)
Date: Thu, 27 Aug 2020 06:19:33 +0200
Subject: [squid-users] GENEVE?
In-Reply-To: <0C9A94C0-98CE-434F-BD3F-B4EAF043BC75@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <004801d67c05$b8f3bb70$2adb3250$@gmail.com>
 <0C9A94C0-98CE-434F-BD3F-B4EAF043BC75@mmm.com>
Message-ID: <1598501973.27804.4.camel@dotforge.ch>

Le jeudi 27 ao?t 2020 ? 01:43 +0000, Jonas Steinberg a ?crit?:


> I mean?if anyone has any ideas of how I can get something to work
> without buying anything expensive I?d certainly be grateful!

Hi

i haven't ?play with it but man ip-link ?or?

https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-
virtual-interfaces-tunnels/#geneve

should put you on tracks


--?
Marcel de Riedmatten



From bruce.rosenberg.au at gmail.com  Thu Aug 27 04:35:24 2020
From: bruce.rosenberg.au at gmail.com (Bruce Rosenberg)
Date: Thu, 27 Aug 2020 14:35:24 +1000
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
Message-ID: <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>

Hi Eliezer,

We are running a couple of Squid proxies (the real servers) in front of a
pair of LVS servers with keepalived and it works flawlessly.
The 2 x Squid proxies are active / active and the LVS servers are active /
passive.
If a Squid proxy dies the remaining proxy takes all the traffic.
If the active LVS server dies, keepalived running on the backup LVS (via
VRRP) moves the VIP to itself and it takes all the traffic, so the only
difference between the two is one has a higher priority so it gets the VIP
first.
I have included some sanitised snippets from a keepalived.conf file that
should help you.
You could easily scale this out if you need more than 2 Squid proxies.

The config I provided is for LVS/DR (Direct Route) mode.
This method rewrites the MAC address of forwarded packets to that of one of
the real servers and is the most scalable way to run LVS.
It does require the LVS and real servers be on the same L2 network.
If that is not possible then consider LVS/TUN mode or LVS/NAT mode.

As LVS/DR rewrites the MAC address, it requires each real server to have
the VIP address plumbed on an interface and also requires the real
servers to ignore ARP requests for the VIP address as the only device that
should respond to ARP requests for the VIP is the active LVS server.
We do this by configuring the VIP on the loopback interface on each real
but there are other methods as well such as dropping the ARP responses
using arptables, iptables or firewalld.
I think back in the kernel 2.4 and 2.6 days people used the noarp kernel
module which could be configured to ignore ARP requests for a particular IP
address but you don't really need this anymore.

More info on the loopback arp blocking method -
https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/
More info on firewall type arp blocking methods -
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa
More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR

If you are using a RPM based distro then to set up the LVS servers you only
need the ipvsadm and keepalived packages.
Install squid on the reals and configure the VIP on each and disable ARP.
Then build the keepalived.conf on both LVS servers and restart keepalived.

The priority configuration stanza in the vrrp_instance section determines
the primary VRRP node (LVS server) for that virtual router instance.
The secondary LVS server needs a lower priority compared to the primary.
You can configure one as the MASTER and the other as the BACKUP but our
guys make them both BACKUP and let the priority sort the election of the
primary out.
I think this might be to solve a problem of bringing up a BACKUP without a
MASTER but I can't confirm that.


Good luck.


$ cat /etc/keepalived/keepalived.conf

global_defs {

    notification_email {
        # rootmail at example.com
    }
    notification_email_from keepalive-daemon at lvs01.example.com
    smtp_server 10.1.2.3        # mail.example.com
    smtp_connect_timeout 30
    lvs_id lvs01.example.com    # Name to mention in email.
}

vrrp_instance LVS_example {

    state BACKUP
    priority 150
    interface eth0
    lvs_sync_daemon_interface eth0
    virtual_router_id 5
    preempt_delay 20

    virtual_ipaddress_excluded {

        10.10.10.10   # Squid proxy
    }

    notify_master "some command to log or send an alert"
    notify_backup "some command to log or send an alert"
    notify_fault "some command to log or send an alert"
}


# SQUID Proxy
virtual_server 10.10.10.10 3128 {

    delay_loop 5
    lb_algo wrr
    lb_kind DR
    protocol TCP

    real_server 10.10.10.11 3128 {   # proxy01.example.com
        weight 1
        inhibit_on_failure 1
        TCP_CHECK {
            connect_port 3128
            connect_timeout 5
        }
    }

    real_server 10.10.10.12 3128 {   # proxy02.example.com
        weight 1
        inhibit_on_failure 1
        TCP_CHECK {
            connect_port 3128
            connect_timeout 5
        }
    }
}


On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com>
wrote:

> Hey All,
>
>
>
> I am reading about LB and tried to find an up-to-date example or tutorial
> specific to squid with no luck.
>
> I have seen:
> http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS
>
>
>
> Which makes sense and also is similar or kind of identical to WCCP with
> gre.
>
>
>
> Anyone knows about a working Squid setup with IPVS/LVS?
>
>
>
> Thanks,
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/63096c7d/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 27 10:14:35 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2020 22:14:35 +1200
Subject: [squid-users] GENEVE?
In-Reply-To: <0C9A94C0-98CE-434F-BD3F-B4EAF043BC75@mmm.com>
References: <57C6BFE3-44CB-4E48-87E7-4BC878E9C1A0@mmm.com>
 <004801d67c05$b8f3bb70$2adb3250$@gmail.com>
 <0C9A94C0-98CE-434F-BD3F-B4EAF043BC75@mmm.com>
Message-ID: <281b574a-5da8-23bf-274c-71ad5fed538a@treenet.co.nz>

On 27/08/20 1:43 pm, Jonas Steinberg wrote:
> 
> I have no use case.? My cloud provider has written a software-defined
> ?appliance? meant to integrate with firewalls and routers.? I was
> complaining that I had no way to integrate it with my DNS filtering
> workflows (Squid).? They told me ?Hey, if it?ll support GENEVE then you
> can make it work.?? So I simply came here to ask.
> 
> I mean?if anyone has any ideas of how I can get something to work
> without buying anything expensive I?d certainly be grateful!
> 

Hmm. It depends a bit on what this appliance is for and what you want it
doing.

I'm not sure what Squid has to do with your DNS filtering workflows TBH.
Squid is typically just a client for DNS like any other software. It
does not manage or control DNS.


(warning: making some big assumptions here, so this may be way off what
you need).


If you mean Squid managing that new DNS-over-HTTP stuff Browsers are
trying to have happen. Whatever message filtering you have in the HTTP
layer should work no differently with or without any extra appliance
existing in the network.

If you mean Squid ACLs to apply policy to HTTP traffic to/from the
appliance ...

If the appliance is assigned IPs from your LAN or a DMZ range your Squid
ACLs that check IP range can match it in the broad sense. Like the
localnet ACL just checks for existence of a client on LAN vs Internet.

If you need an ACL to identify/match a specific appliance with
dynamically assigned IP you can use its hostname instead of IP. Squid
finds the IP as-needed via rDNS or mDNS depending on the .local TLD
existence in the FQDN.
 NP: This has variable reliability. When the appliance IP changes the
DNS TTL determines how fast Squid can know about the change.


HTH
Amos


From squid3 at treenet.co.nz  Thu Aug 27 10:24:54 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2020 22:24:54 +1200
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
 <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
Message-ID: <d47ff613-7986-57a2-8b25-f3af66357643@treenet.co.nz>

Nice writeup. Do you mind if I add this to the Squid wiki as an example
for high-performance proxying?


Amos



On 27/08/20 4:35 pm, Bruce Rosenberg wrote:
> Hi Eliezer,
> 
> We are running a couple of Squid proxies (the real servers) in front of
> a pair of LVS servers with keepalived and it works flawlessly.
> The 2 x Squid proxies are active / active and the LVS servers are active
> / passive.
> If a Squid proxy dies the remaining proxy takes all the traffic.
> If the active LVS server dies, keepalived running on the backup LVS (via
> VRRP) moves the?VIP to itself and it?takes all the traffic, so the only
> difference between the two is one has a higher priority so it gets the
> VIP first.
> I have included some sanitised snippets from a keepalived.conf file that
> should help you.
> You could easily scale this out if you need more than 2 Squid proxies.
> 
> The config I provided is for LVS/DR (Direct Route) mode.
> This method rewrites the MAC address of forwarded packets to that of one
> of the real servers and is the most scalable way to run LVS.
> It does require the LVS and real servers be on the same L2 network.
> If that is not possible then consider LVS/TUN mode or LVS/NAT mode.
> 
> As LVS/DR rewrites the MAC address, it requires each real server to have
> the VIP address plumbed on an interface and also requires the real
> servers?to ignore ARP requests for the VIP address as the only device
> that should respond?to ARP requests?for the VIP is the active LVS server.
> We do this by configuring?the VIP on the loopback interface on each real
> but there are other methods as well such as dropping the ARP responses
> using arptables, iptables or firewalld.
> I think back in the kernel 2.4 and 2.6 days people used the noarp kernel
> module which could be configured to ignore ARP requests for a particular
> IP address but you don't really need this anymore.
> 
> More info on the loopback arp blocking method -
> https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/
> More info on firewall type arp blocking methods
> -?https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa
> More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR
> 
> If you are using a RPM based distro then to set up the LVS servers you
> only need the?ipvsadm and keepalived packages.
> Install squid on the reals and configure the VIP on each and disable ARP.
> Then build the keepalived.conf on both LVS servers and restart keepalived.
> 
> The priority configuration stanza in the vrrp_instance section
> determines the primary VRRP node (LVS server) for that virtual router
> instance.
> The secondary LVS server?needs a lower priority compared to the primary.
> You can configure one as the MASTER and the other as the BACKUP but our
> guys make them both BACKUP and let the priority sort the election of the
> primary out.
> I think this might be to solve a problem of bringing up a BACKUP without
> a MASTER but I can't confirm that.
> 
> 
> Good luck.
> 
> 
> $ cat?/etc/keepalived/keepalived.conf
> 
> global_defs {
> 
> ? ? notification_email {
> ? ? ? ? # rootmail at example.com <mailto:rootmail at example.com>
> ? ? }
> ? ? notification_email_from keepalive-daemon at lvs01.example.com
> <mailto:keepalive-daemon at lvs01.example.com>
> ? ? smtp_server 10.1.2.3 ? ? ? ?# mail.example.com <http://mail.example.com>
> ? ? smtp_connect_timeout 30
> ? ? lvs_id lvs01.example.com <http://lvs01.example.com> ? ?# Name to
> mention in email.
> }
> 
> vrrp_instance LVS_example {
> 
> ? ? state BACKUP
> ? ? priority 150
> ? ? interface eth0
> ? ? lvs_sync_daemon_interface eth0
> ? ? virtual_router_id 5
> ? ? preempt_delay 20
> 
> ? ? virtual_ipaddress_excluded {
> ? ? ? ?
> ? ? ? ? 10.10.10.10 ? # Squid proxy
> ? ? }
> 
> ? ? notify_master "some command to log or send an alert"
> ? ? notify_backup "some command to log or send an alert"
> ? ? notify_fault "some command to log or send an alert"
> }
> 
> 
> # SQUID Proxy
> virtual_server 10.10.10.10 3128 {
> 
> ? ? delay_loop 5
> ? ? lb_algo wrr
> ? ? lb_kind DR
> ? ? protocol TCP
> 
> ? ? real_server 10.10.10.11 3128 { ? # proxy01.example.com
> <http://proxy01.example.com>
> ? ? ? ? weight 1
> ? ? ? ? inhibit_on_failure 1
> ? ? ? ? TCP_CHECK {
> ? ? ? ? ? ? connect_port 3128
> ? ? ? ? ? ? connect_timeout 5
> ? ? ? ? }
> ? ? }
> 
> ? ? real_server 10.10.10.12 3128 { ? # proxy02.example.com
> <http://proxy02.example.com>
> ? ? ? ? weight 1
> ? ? ? ? inhibit_on_failure 1
> ? ? ? ? TCP_CHECK {
> ? ? ? ? ? ? connect_port 3128
> ? ? ? ? ? ? connect_timeout 5
> ? ? ? ? }
> ? ? }
> }
> 
> 
> On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com
> <mailto:ngtech1ltd at gmail.com>> wrote:
> 
>     Hey All,____
> 
>     __?__
> 
>     I am reading about LB and tried to find an up-to-date example or
>     tutorial specific to squid with no luck.____
> 
>     I have seen:
>     http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS____
> 
>     __?__
> 
>     Which makes sense and also is similar or kind of identical to WCCP
>     with gre.____
> 
>     __?__
> 
>     Anyone knows about a working Squid setup with IPVS/LVS?____
> 
>     __?__
> 
>     Thanks,____
> 
>     Eliezer____
> 
>     __?__
> 
>     ----____
> 
>     Eliezer Croitoru____
> 
>     Tech Support____
> 
>     Mobile: +972-5-28704261____
> 
>     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>____
> 
>     __?__
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From Dave.Lewthwaite at realitymine.com  Thu Aug 27 10:41:38 2020
From: Dave.Lewthwaite at realitymine.com (Dave Lewthwaite)
Date: Thu, 27 Aug 2020 10:41:38 +0000
Subject: [squid-users] Squid Compile with custom OpenSSL
Message-ID: <DBBPR08MB4380706F4AD67E46AFFF7914F0550@DBBPR08MB4380.eurprd08.prod.outlook.com>

Hello,

I am having some trouble compiling squid using a custom build of openssl ? it seems it is unable to find the libraries during run-time.

OpenSSL 1.1.1g is compiled and installed (setting prefix and openssldir) to /home/centos/openssl ? the libraries are present there as are all the headers.

Squid is built using ?with-openssl=/home/centos/openssl ? it builds correctly, references the correct includes, libraries etc. However when the resulting binary is run, it is unable to find the OpenSSL libs ?

squid: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory

ldd /sbin/squid shows that it is unable to find the libs ?

                libssl.so.1.1 => not found
                libcrypto.so.1.1 => not found

If I install the 1.1.1g libs into  /lib64 then squid will run ? have I made an assumption on my part that this isn?t required? I was under the impression that squid would run with an entirely standalone build of OpenSSL.

Any advice would be appreciated ? it certainly feels as though I?m missing something simple, but I can?t figure out what it is.

Versions
CentOS 7 (standard build chain)
Squid 4.13
OpenSSL 1.1.1g

Configure commands ?

Squid -

./configure \
     --prefix=/usr \
     --with-logdir=/var/log/squid \
     --enable-useragent-log \
     --with-filedescriptors=65535 \
     --enable-ssl \
     --enable-http-violations \
     --sysconfdir=/etc/squid \
     --with-default-user="squid" \
     --localstatedir=/var \
     --with-logdir='/var/log/squid' \
     --with-pidfile='/var/run/squid.pid' \
     --enable-stacktraces \
     --with-openssl='/home/centos/openssl' \
     --enable-snmp \
     --disable-arch-native

Openssl ?

./config --prefix=/home/centos/openssl --openssldir=/home/centos/openssl


Thanks






This email and any attachments to it may contain confidential information and are intended solely for the addressee.



If you are not the intended recipient of this email or if you believe you have received this email in error, please contact the sender and remove it from your system.Do not use, copy or disclose the information contained in this email or in any attachment.

RealityMine Limited may monitor email traffic data including the content of email for the purposes of security.

RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/b993210b/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 27 11:07:29 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Aug 2020 23:07:29 +1200
Subject: [squid-users] Squid Compile with custom OpenSSL
In-Reply-To: <DBBPR08MB4380706F4AD67E46AFFF7914F0550@DBBPR08MB4380.eurprd08.prod.outlook.com>
References: <DBBPR08MB4380706F4AD67E46AFFF7914F0550@DBBPR08MB4380.eurprd08.prod.outlook.com>
Message-ID: <27cf01a2-17ca-7542-e38b-fdd6543bd9d7@treenet.co.nz>

On 27/08/20 10:41 pm, Dave Lewthwaite wrote:
> Hello,
> 
> ?
> 
> I am having some trouble compiling squid using a custom build of openssl
> ? it seems it is unable to find the libraries during run-time.
> 
> ?
> 
> OpenSSL 1.1.1g is compiled and installed (setting prefix and openssldir)
> to /home/centos/openssl ? the libraries are present there as are all the
> headers.
> 
> ?
> 
> Squid is built using ?with-openssl=/home/centos/openssl ? it builds
> correctly, references the correct includes, libraries etc. However when
> the resulting binary is run, it is unable to find the OpenSSL libs ??
> 

Looks like you need to set the environment LD_LIBRARY_PATH variable when
starting Squid.

<https://blog.andrewbeacock.com/2007/10/how-to-add-shared-libraries-to-linuxs.html>

...
> If I install the 1.1.1g libs into ?/lib64 then squid will run ? have I
> made an assumption on my part that this isn?t required? I was under the
> impression that squid would run with an entirely standalone build of
> OpenSSL.

Squid should, yes.


Amos


From emmanuel.fuste at thalesgroup.com  Thu Aug 27 11:22:53 2020
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Thu, 27 Aug 2020 11:22:53 +0000
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
 <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
Message-ID: <dabb1f23-7a7e-10ea-931e-845bf466c07b@thalesgroup.com>

Hi,

To complement this, on modern kernel take the opportunity to try nftlb 
instead of LVS too.
https://www.zevenet.com/knowledge-base/nftlb/what-is-nftlb/

Emmanuel.

Le 27/08/2020 ? 06:35, Bruce Rosenberg a ?crit?:
> Hi Eliezer,
>
> We are running a couple of Squid proxies (the real servers) in front 
> of a pair of LVS servers with keepalived and it works flawlessly.
> The 2 x Squid proxies are active / active and the LVS servers are 
> active / passive.
> If a Squid proxy dies the remaining proxy takes all the traffic.
> If the active LVS server dies, keepalived running on the backup LVS 
> (via VRRP) moves the?VIP to itself and it?takes all the traffic, so 
> the only difference between the two is one has a higher priority so it 
> gets the VIP first.
> I have included some sanitised snippets from a keepalived.conf file 
> that should help you.
> You could easily scale this out if you need more than 2 Squid proxies.
>
> The config I provided is for LVS/DR (Direct Route) mode.
> This method rewrites the MAC address of forwarded packets to that of 
> one of the real servers and is the most scalable way to run LVS.
> It does require the LVS and real servers be on the same L2 network.
> If that is not possible then consider LVS/TUN mode or LVS/NAT mode.
>
> As LVS/DR rewrites the MAC address, it requires each real server to 
> have the VIP address plumbed on an interface and also requires the 
> real servers?to ignore ARP requests for the VIP address as the only 
> device that should respond?to ARP requests?for the VIP is the active 
> LVS server.
> We do this by configuring?the VIP on the loopback interface on each 
> real but there are other methods as well such as dropping the ARP 
> responses using arptables, iptables or firewalld.
> I think back in the kernel 2.4 and 2.6 days people used the noarp 
> kernel module which could be configured to ignore ARP requests for a 
> particular IP address but you don't really need this anymore.
>
> More info on the loopback arp blocking method - 
> https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/
> More info on firewall type arp blocking methods - 
> https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa
> More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR
>
> If you are using a RPM based distro then to set up the LVS servers you 
> only need the ipvsadm and keepalived packages.
> Install squid on the reals and configure the VIP on each and disable ARP.
> Then build the keepalived.conf on both LVS servers and restart keepalived.
>
> The priority configuration stanza in the vrrp_instance section 
> determines the primary VRRP node (LVS server) for that virtual router 
> instance.
> The secondary LVS server?needs a lower priority compared to the primary.
> You can configure one as the MASTER and the other as the BACKUP but 
> our guys make them both BACKUP and let the priority sort the election 
> of the primary out.
> I think this might be to solve a problem of bringing up a BACKUP 
> without a MASTER but I can't confirm that.
>
>
> Good luck.
>
>
> $ cat?/etc/keepalived/keepalived.conf
>
> global_defs {
>
> ? ? notification_email {
> ? ? ? ? # rootmail at example.com <mailto:rootmail at example.com>
> ? ? }
> ? ? notification_email_from keepalive-daemon at lvs01.example.com 
> <mailto:keepalive-daemon at lvs01.example.com>
> ? ? smtp_server 10.1.2.3 ? ? ? ?# mail.example.com 
> <http://mail.example.com>
> ? ? smtp_connect_timeout 30
> ? ? lvs_id lvs01.example.com <http://lvs01.example.com> ? ?# Name to 
> mention in email.
> }
>
> vrrp_instance LVS_example {
>
> ? ? state BACKUP
> ? ? priority 150
> ? ? interface eth0
> ? ? lvs_sync_daemon_interface eth0
> ? ? virtual_router_id 5
> ? ? preempt_delay 20
>
> ? ? virtual_ipaddress_excluded {
>
> ? ? ? ? 10.10.10.10 ? # Squid proxy
> ? ? }
>
> ? ? notify_master "some command to log or send an alert"
> ? ? notify_backup "some command to log or send an alert"
> ? ? notify_fault "some command to log or send an alert"
> }
>
>
> # SQUID Proxy
> virtual_server 10.10.10.10 3128 {
>
> ? ? delay_loop 5
> ? ? lb_algo wrr
> ? ? lb_kind DR
> ? ? protocol TCP
>
> ? ? real_server 10.10.10.11 3128 { ? # proxy01.example.com 
> <http://proxy01.example.com>
> ? ? ? ? weight 1
> ? ? ? ? inhibit_on_failure 1
> ? ? ? ? TCP_CHECK {
> ? ? ? ? ? ? connect_port 3128
> ? ? ? ? ? ? connect_timeout 5
> ? ? ? ? }
> ? ? }
>
> ? ? real_server 10.10.10.12 3128 { ? # proxy02.example.com 
> <http://proxy02.example.com>
> ? ? ? ? weight 1
> ? ? ? ? inhibit_on_failure 1
> ? ? ? ? TCP_CHECK {
> ? ? ? ? ? ? connect_port 3128
> ? ? ? ? ? ? connect_timeout 5
> ? ? ? ? }
> ? ? }
> }
>
>
> On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com>> wrote:
>
>     Hey All,
>
>     I am reading about LB and tried to find an up-to-date example or
>     tutorial specific to squid with no luck.
>
>     I have seen:
>     http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS
>
>     Which makes sense and also is similar or kind of identical to WCCP
>     with gre.
>
>     Anyone knows about a working Squid setup with IPVS/LVS?
>
>     Thanks,
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From Dave.Lewthwaite at realitymine.com  Thu Aug 27 11:37:40 2020
From: Dave.Lewthwaite at realitymine.com (Dave Lewthwaite)
Date: Thu, 27 Aug 2020 11:37:40 +0000
Subject: [squid-users] Squid Compile with custom OpenSSL
In-Reply-To: <27cf01a2-17ca-7542-e38b-fdd6543bd9d7@treenet.co.nz>
References: <DBBPR08MB4380706F4AD67E46AFFF7914F0550@DBBPR08MB4380.eurprd08.prod.outlook.com>,
 <27cf01a2-17ca-7542-e38b-fdd6543bd9d7@treenet.co.nz>
Message-ID: <DBBPR08MB438004C1D284C16C76235F9BF0550@DBBPR08MB4380.eurprd08.prod.outlook.com>

Oh that?s excellent ? it worked. I did think by using ?with-openssl=, squid would statically link the libs ? but I can?t see an option for that.

Regardless, I can work with this.

Thanks for your help.



This email and any attachments to it may contain confidential information and are intended solely for the addressee.



If you are not the intended recipient of this email or if you believe you have received this email in error, please contact the sender and remove it from your system.Do not use, copy or disclose the information contained in this email or in any attachment.

RealityMine Limited may monitor email traffic data including the content of email for the purposes of security.

RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/7f5decda/attachment.htm>

From ngtech1ltd at gmail.com  Thu Aug 27 12:03:48 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 27 Aug 2020 15:03:48 +0300
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
 <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
Message-ID: <00b801d67c6a$1fb17730$5f146590$@gmail.com>

Hey Bruce,

 

Thanks for the detailed and beautiful answer.

I am actually trying to understand what ipvs gives and to compare it to nftables.

 

We need a setup structure which will make it more real.

 

I am trying to think about a setup Sketch:

3+ Proxies

2 LB

1 Edge Router VIP(maybe more actual routers)

 

Networks:

PX Internal net: 192.168.100.0/24

Wan Edge Routers net: 192.168.200.0/24

 

R1:

WAN VIP: 192.168.200.200/24

LAN VIP: 192.168.100.254/24

Static route toward 192.168.101.100 via 192.168.200.200

(Another option would be using FRR for ECMP and ACTIVE/ACTIVE LB)

 

Proxies VIP: 192.168.101.100/32

PX1 IP:  192.168.100.101/24 GW 192.168.100.254

PX2 IP:  192.168.100.102/24 GW 192.168.100.254

PX3 IP:  192.168.100.103/24 GW 192.168.100.254

 

LBs VIP: 192.168.200.200/24

LB 1 IP: 192.168.200.201/24

LB 2 IP: 192.168.200.202/24

 

## Things to consider about the setup:

We can use either FWMARK based LB or mac replacement.

It is possible to avoid arp issues with either tunnels or VIP assignment to interfaces.

There are couple tunneling options such as GENEVE\GUE\FUE\GRE\IPIP which can be used with IPVS.

 

Thoughts? 

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: Bruce Rosenberg <bruce.rosenberg.au at gmail.com> 
Sent: Thursday, August 27, 2020 7:35 AM
To: Eliezer Croitor <ngtech1ltd at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPVS/LVS load balancing Squid servers, anyone did it?

 

Hi Eliezer,

 

We are running a couple of Squid proxies (the real servers) in front of a pair of LVS servers with keepalived and it works flawlessly.

The 2 x Squid proxies are active / active and the LVS servers are active / passive.

If a Squid proxy dies the remaining proxy takes all the traffic.

If the active LVS server dies, keepalived running on the backup LVS (via VRRP) moves the VIP to itself and it takes all the traffic, so the only difference between the two is one has a higher priority so it gets the VIP first.

I have included some sanitised snippets from a keepalived.conf file that should help you.

You could easily scale this out if you need more than 2 Squid proxies.

 

The config I provided is for LVS/DR (Direct Route) mode.

This method rewrites the MAC address of forwarded packets to that of one of the real servers and is the most scalable way to run LVS.

It does require the LVS and real servers be on the same L2 network.

If that is not possible then consider LVS/TUN mode or LVS/NAT mode.

 

As LVS/DR rewrites the MAC address, it requires each real server to have the VIP address plumbed on an interface and also requires the real servers to ignore ARP requests for the VIP address as the only device that should respond to ARP requests for the VIP is the active LVS server.

We do this by configuring the VIP on the loopback interface on each real but there are other methods as well such as dropping the ARP responses using arptables, iptables or firewalld.

I think back in the kernel 2.4 and 2.6 days people used the noarp kernel module which could be configured to ignore ARP requests for a particular IP address but you don't really need this anymore.

 

More info on the loopback arp blocking method - https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/

More info on firewall type arp blocking methods - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa

More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR

 

If you are using a RPM based distro then to set up the LVS servers you only need the ipvsadm and keepalived packages.

Install squid on the reals and configure the VIP on each and disable ARP.

Then build the keepalived.conf on both LVS servers and restart keepalived.

 

The priority configuration stanza in the vrrp_instance section determines the primary VRRP node (LVS server) for that virtual router instance.

The secondary LVS server needs a lower priority compared to the primary.

You can configure one as the MASTER and the other as the BACKUP but our guys make them both BACKUP and let the priority sort the election of the primary out.

I think this might be to solve a problem of bringing up a BACKUP without a MASTER but I can't confirm that.

 

 

Good luck.

 

 

$ cat /etc/keepalived/keepalived.conf

global_defs {

    notification_email {
        # rootmail at example.com <mailto:rootmail at example.com> 
    }
    notification_email_from keepalive-daemon at lvs01.example.com <mailto:keepalive-daemon at lvs01.example.com> 
    smtp_server 10.1.2.3        # mail.example.com <http://mail.example.com> 
    smtp_connect_timeout 30
    lvs_id lvs01.example.com <http://lvs01.example.com>     # Name to mention in email.
}

vrrp_instance LVS_example {

    state BACKUP
    priority 150
    interface eth0
    lvs_sync_daemon_interface eth0
    virtual_router_id 5
    preempt_delay 20

    virtual_ipaddress_excluded {
        
        10.10.10.10   # Squid proxy
    }

    notify_master "some command to log or send an alert"
    notify_backup "some command to log or send an alert"
    notify_fault "some command to log or send an alert"
}


# SQUID Proxy
virtual_server 10.10.10.10 3128 { 

    delay_loop 5
    lb_algo wrr
    lb_kind DR
    protocol TCP

    real_server 10.10.10.11 3128 {   # proxy01.example.com <http://proxy01.example.com> 
        weight 1
        inhibit_on_failure 1
        TCP_CHECK {
            connect_port 3128
            connect_timeout 5
        }
    }

    real_server 10.10.10.12 3128 {   # proxy02.example.com <http://proxy02.example.com> 
        weight 1
        inhibit_on_failure 1
        TCP_CHECK {
            connect_port 3128
            connect_timeout 5
        }
    }
}

 

 

On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > wrote:

Hey All,

 

I am reading about LB and tried to find an up-to-date example or tutorial specific to squid with no luck.

I have seen: http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS

 

Which makes sense and also is similar or kind of identical to WCCP with gre.

 

Anyone knows about a working Squid setup with IPVS/LVS?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200827/e1d34cc2/attachment.htm>

From ngtech1ltd at gmail.com  Thu Aug 27 12:14:17 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Thu, 27 Aug 2020 15:14:17 +0300
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <dabb1f23-7a7e-10ea-931e-845bf466c07b@thalesgroup.com>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
 <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
 <dabb1f23-7a7e-10ea-931e-845bf466c07b@thalesgroup.com>
Message-ID: <00c901d67c6b$96d42a00$c47c7e00$@gmail.com>

Hey Emmanuel,

I was just trying to understand if and how nftables is can LB and what exactly makes IPVS that fast.
It seems that IPVS in DR actually converts the linux box into a Switch.
I am still unsure if mac address replacement is better then FWMARK for DR.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of FUSTE Emmanuel
Sent: Thursday, August 27, 2020 2:23 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] IPVS/LVS load balancing Squid servers, anyone did it?

Hi,

To complement this, on modern kernel take the opportunity to try nftlb 
instead of LVS too.
https://www.zevenet.com/knowledge-base/nftlb/what-is-nftlb/

Emmanuel.

Le 27/08/2020 ? 06:35, Bruce Rosenberg a ?crit :
> Hi Eliezer,
>
> We are running a couple of Squid proxies (the real servers) in front 
> of a pair of LVS servers with keepalived and it works flawlessly.
> The 2 x Squid proxies are active / active and the LVS servers are 
> active / passive.
> If a Squid proxy dies the remaining proxy takes all the traffic.
> If the active LVS server dies, keepalived running on the backup LVS 
> (via VRRP) moves the VIP to itself and it takes all the traffic, so 
> the only difference between the two is one has a higher priority so it 
> gets the VIP first.
> I have included some sanitised snippets from a keepalived.conf file 
> that should help you.
> You could easily scale this out if you need more than 2 Squid proxies.
>
> The config I provided is for LVS/DR (Direct Route) mode.
> This method rewrites the MAC address of forwarded packets to that of 
> one of the real servers and is the most scalable way to run LVS.
> It does require the LVS and real servers be on the same L2 network.
> If that is not possible then consider LVS/TUN mode or LVS/NAT mode.
>
> As LVS/DR rewrites the MAC address, it requires each real server to 
> have the VIP address plumbed on an interface and also requires the 
> real servers to ignore ARP requests for the VIP address as the only 
> device that should respond to ARP requests for the VIP is the active 
> LVS server.
> We do this by configuring the VIP on the loopback interface on each 
> real but there are other methods as well such as dropping the ARP 
> responses using arptables, iptables or firewalld.
> I think back in the kernel 2.4 and 2.6 days people used the noarp 
> kernel module which could be configured to ignore ARP requests for a 
> particular IP address but you don't really need this anymore.
>
> More info on the loopback arp blocking method - 
> https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/
> More info on firewall type arp blocking methods - 
> https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa
> More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR
>
> If you are using a RPM based distro then to set up the LVS servers you 
> only need the ipvsadm and keepalived packages.
> Install squid on the reals and configure the VIP on each and disable ARP.
> Then build the keepalived.conf on both LVS servers and restart keepalived.
>
> The priority configuration stanza in the vrrp_instance section 
> determines the primary VRRP node (LVS server) for that virtual router 
> instance.
> The secondary LVS server needs a lower priority compared to the primary.
> You can configure one as the MASTER and the other as the BACKUP but 
> our guys make them both BACKUP and let the priority sort the election 
> of the primary out.
> I think this might be to solve a problem of bringing up a BACKUP 
> without a MASTER but I can't confirm that.
>
>
> Good luck.
>
>
> $ cat /etc/keepalived/keepalived.conf
>
> global_defs {
>
>     notification_email {
>         # rootmail at example.com <mailto:rootmail at example.com>
>     }
>     notification_email_from keepalive-daemon at lvs01.example.com 
> <mailto:keepalive-daemon at lvs01.example.com>
>     smtp_server 10.1.2.3        # mail.example.com 
> <http://mail.example.com>
>     smtp_connect_timeout 30
>     lvs_id lvs01.example.com <http://lvs01.example.com>    # Name to 
> mention in email.
> }
>
> vrrp_instance LVS_example {
>
>     state BACKUP
>     priority 150
>     interface eth0
>     lvs_sync_daemon_interface eth0
>     virtual_router_id 5
>     preempt_delay 20
>
>     virtual_ipaddress_excluded {
>
>         10.10.10.10   # Squid proxy
>     }
>
>     notify_master "some command to log or send an alert"
>     notify_backup "some command to log or send an alert"
>     notify_fault "some command to log or send an alert"
> }
>
>
> # SQUID Proxy
> virtual_server 10.10.10.10 3128 {
>
>     delay_loop 5
>     lb_algo wrr
>     lb_kind DR
>     protocol TCP
>
>     real_server 10.10.10.11 3128 {   # proxy01.example.com 
> <http://proxy01.example.com>
>         weight 1
>         inhibit_on_failure 1
>         TCP_CHECK {
>             connect_port 3128
>             connect_timeout 5
>         }
>     }
>
>     real_server 10.10.10.12 3128 {   # proxy02.example.com 
> <http://proxy02.example.com>
>         weight 1
>         inhibit_on_failure 1
>         TCP_CHECK {
>             connect_port 3128
>             connect_timeout 5
>         }
>     }
> }
>
>
> On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com>> wrote:
>
>     Hey All,
>
>     I am reading about LB and tried to find an up-to-date example or
>     tutorial specific to squid with no luck.
>
>     I have seen:
>     http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS
>
>     Which makes sense and also is similar or kind of identical to WCCP
>     with gre.
>
>     Anyone knows about a working Squid setup with IPVS/LVS?
>
>     Thanks,
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From emmanuel.fuste at thalesgroup.com  Thu Aug 27 13:07:51 2020
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Thu, 27 Aug 2020 13:07:51 +0000
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <00c901d67c6b$96d42a00$c47c7e00$@gmail.com>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
 <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
 <dabb1f23-7a7e-10ea-931e-845bf466c07b@thalesgroup.com>
 <00c901d67c6b$96d42a00$c47c7e00$@gmail.com>
Message-ID: <686fc730-10a6-0a54-8527-9e7b5b6a8f47@thalesgroup.com>

Le 27/08/2020 ? 14:14, Eliezer Croitor a ?crit?:
> Hey Emmanuel,
>
> I was just trying to understand if and how nftables is can LB and what exactly makes IPVS that fast.
nftlb is simply a rulemanager in front of nftables. And IPVS is not as 
fast as nftables, it is the reverse.
All you could do with IPVS is normally do-able with nftable/nftlb and more.
> It seems that IPVS in DR actually converts the linux box into a Switch.
Technically it is purely hw address mangling not full switching.
> I am still unsure if mac address replacement is better then FWMARK for DR.
FWMARK is just for packet selection grouping for mac address 
replacement. It does not replace it.
And with the expressive capability of nftable this FWMARK dance is no 
longer necessary.

Emmanuel.
>
> Thanks,
> Eliezer
>
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
>
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of FUSTE Emmanuel
> Sent: Thursday, August 27, 2020 2:23 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] IPVS/LVS load balancing Squid servers, anyone did it?
>
> Hi,
>
> To complement this, on modern kernel take the opportunity to try nftlb
> instead of LVS too.
> https://www.zevenet.com/knowledge-base/nftlb/what-is-nftlb/
>
> Emmanuel.
>
> Le 27/08/2020 ? 06:35, Bruce Rosenberg a ?crit :
>> Hi Eliezer,
>>
>> We are running a couple of Squid proxies (the real servers) in front
>> of a pair of LVS servers with keepalived and it works flawlessly.
>> The 2 x Squid proxies are active / active and the LVS servers are
>> active / passive.
>> If a Squid proxy dies the remaining proxy takes all the traffic.
>> If the active LVS server dies, keepalived running on the backup LVS
>> (via VRRP) moves the VIP to itself and it takes all the traffic, so
>> the only difference between the two is one has a higher priority so it
>> gets the VIP first.
>> I have included some sanitised snippets from a keepalived.conf file
>> that should help you.
>> You could easily scale this out if you need more than 2 Squid proxies.
>>
>> The config I provided is for LVS/DR (Direct Route) mode.
>> This method rewrites the MAC address of forwarded packets to that of
>> one of the real servers and is the most scalable way to run LVS.
>> It does require the LVS and real servers be on the same L2 network.
>> If that is not possible then consider LVS/TUN mode or LVS/NAT mode.
>>
>> As LVS/DR rewrites the MAC address, it requires each real server to
>> have the VIP address plumbed on an interface and also requires the
>> real servers to ignore ARP requests for the VIP address as the only
>> device that should respond to ARP requests for the VIP is the active
>> LVS server.
>> We do this by configuring the VIP on the loopback interface on each
>> real but there are other methods as well such as dropping the ARP
>> responses using arptables, iptables or firewalld.
>> I think back in the kernel 2.4 and 2.6 days people used the noarp
>> kernel module which could be configured to ignore ARP requests for a
>> particular IP address but you don't really need this anymore.
>>
>> More info on the loopback arp blocking method -
>> https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/
>> More info on firewall type arp blocking methods -
>> https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa
>> More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR
>>
>> If you are using a RPM based distro then to set up the LVS servers you
>> only need the ipvsadm and keepalived packages.
>> Install squid on the reals and configure the VIP on each and disable ARP.
>> Then build the keepalived.conf on both LVS servers and restart keepalived.
>>
>> The priority configuration stanza in the vrrp_instance section
>> determines the primary VRRP node (LVS server) for that virtual router
>> instance.
>> The secondary LVS server needs a lower priority compared to the primary.
>> You can configure one as the MASTER and the other as the BACKUP but
>> our guys make them both BACKUP and let the priority sort the election
>> of the primary out.
>> I think this might be to solve a problem of bringing up a BACKUP
>> without a MASTER but I can't confirm that.
>>
>>
>> Good luck.
>>
>>
>> $ cat /etc/keepalived/keepalived.conf
>>
>> global_defs {
>>
>>      notification_email {
>>          # rootmail at example.com <mailto:rootmail at example.com>
>>      }
>>      notification_email_from keepalive-daemon at lvs01.example.com
>> <mailto:keepalive-daemon at lvs01.example.com>
>>      smtp_server 10.1.2.3        # mail.example.com
>> <http://mail.example.com>
>>      smtp_connect_timeout 30
>>      lvs_id lvs01.example.com <http://lvs01.example.com>    # Name to
>> mention in email.
>> }
>>
>> vrrp_instance LVS_example {
>>
>>      state BACKUP
>>      priority 150
>>      interface eth0
>>      lvs_sync_daemon_interface eth0
>>      virtual_router_id 5
>>      preempt_delay 20
>>
>>      virtual_ipaddress_excluded {
>>
>>          10.10.10.10   # Squid proxy
>>      }
>>
>>      notify_master "some command to log or send an alert"
>>      notify_backup "some command to log or send an alert"
>>      notify_fault "some command to log or send an alert"
>> }
>>
>>
>> # SQUID Proxy
>> virtual_server 10.10.10.10 3128 {
>>
>>      delay_loop 5
>>      lb_algo wrr
>>      lb_kind DR
>>      protocol TCP
>>
>>      real_server 10.10.10.11 3128 {   # proxy01.example.com
>> <http://proxy01.example.com>
>>          weight 1
>>          inhibit_on_failure 1
>>          TCP_CHECK {
>>              connect_port 3128
>>              connect_timeout 5
>>          }
>>      }
>>
>>      real_server 10.10.10.12 3128 {   # proxy02.example.com
>> <http://proxy02.example.com>
>>          weight 1
>>          inhibit_on_failure 1
>>          TCP_CHECK {
>>              connect_port 3128
>>              connect_timeout 5
>>          }
>>      }
>> }
>>
>>
>> On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com
>> <mailto:ngtech1ltd at gmail.com>> wrote:
>>
>>      Hey All,
>>
>>      I am reading about LB and tried to find an up-to-date example or
>>      tutorial specific to squid with no luck.
>>
>>      I have seen:
>>      http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS
>>
>>      Which makes sense and also is similar or kind of identical to WCCP
>>      with gre.
>>
>>      Anyone knows about a working Squid setup with IPVS/LVS?
>>
>>      Thanks,
>>
>>      Eliezer
>>
>>      ----
>>
>>      Eliezer Croitoru
>>
>>      Tech Support
>>
>>      Mobile: +972-5-28704261
>>
>>      Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>>
>>      _______________________________________________
>>

From lmniedas at gmail.com  Thu Aug 27 15:40:41 2020
From: lmniedas at gmail.com (=?UTF-8?Q?Luis_Mario_Niedas_Hern=C3=A1ndez?=)
Date: Thu, 27 Aug 2020 11:40:41 -0400
Subject: [squid-users] ACL-by time- not working. Help!
Message-ID: <CALtjann5OSbx9YOf=p8xY5fXvhcY=Bng6u+MPGz+a6QfDii97w@mail.gmail.com>

Hello. I need restrict some site by time, but i am not doing well.
This is my squid.conf. Please help me to fix the problem. I don't know
why it is not working.


### autenticacion de los usuarios (http
b?sica)############################################
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/usuarios_inet
auth_param basic realm Introduzca su usuario para navegar por la WEB.

### por donde responde el squid ###
####################################################

http_port 192.168.1.3:3128
http_port 127.0.0.1:3128

########## ACL ###########################################################################
#
# Recommended minimum configuration:

acl all src all
acl localhost src 127.0.0.1/32
acl localnet src 192.168.1.0/24
acl manager_proto_cache proto cache_object
acl peticion_identificacion proxy_auth REQUIRED
acl intranet dstdomain intra.xzy

acl ocio dstdomain  .facebook.com .youtube.com

acl ocio_medio_dia time MTWHF 12:00-13:10
acl ocio_tarde time MTWHF 14:00-14:30
acl ocio_mannana time MTWHF 6:00-8:30

http_access allow localnet manager_proto_cache
http_access allow localhost manager_proto_cache
http_access deny manager_proto_cache
http_access allow ocio ocio_tarde
http_access allow ocio ocio_medio_dia
http_access allow ocio ocio_mannana

http_access allow intranet
http_access allow localnet peticion_identificacion

always_direct allow intranet
http_access deny all
never_direct allow all



####### cahce padre #################################################

cache_peer proxy_padre parent 3128 0  proxy-only
#cache_peer_domain  proxy_padre !intra.xzy

##### correo cache manager ####

cache_mgr lmniedas
cachemgr_passwd tu_password all
#### tamanno de la cache ####################################

cache_dir aufs /var/spool/squid 20280 16 256

#### limites para comenzar a limpiar la cache #####################

cache_swap_low 90
cache_swap_high 95

#### tamanno de los objetos en la cache como maximo ####################

maximum_object_size  15 MB

### memoria cache ###########################

cache_mem 500 MB

### idioma de las paginas de error de squid ##########################

error_directory /usr/share/squid/errors/Spanish

##### debug_options cantidad de informaci?n en cache_log #################

debug_options ALL,0 ALL,1 rotate=8760

######### LOGS #######################################

cache_log /var/log/squid/cache.log
access_log stdio:/var/log/squid/access.log  rotate=8760
cache_store_log stdio:/var/log/squid/store.log

##################

#AFECTA LA CANTIDAD ESPECIFICAMENTE A STORE.LOG

logfile_rotate 8760

#### 365 dias * 24 horas es la cantidad de rotaciones de los logs en el crontab


-- 
"El futuro tiene muchos nombres. Para los d?biles es lo inalcanzable.
Para los temerosos, lo desconocido. Para los valientes es la
oportunidad"
Victor Hugo


From bruce.rosenberg.au at gmail.com  Thu Aug 27 21:47:53 2020
From: bruce.rosenberg.au at gmail.com (Bruce Rosenberg)
Date: Fri, 28 Aug 2020 07:47:53 +1000
Subject: [squid-users] IPVS/LVS load balancing Squid servers,
 anyone did it?
In-Reply-To: <d47ff613-7986-57a2-8b25-f3af66357643@treenet.co.nz>
References: <002d01d67bf7$a5084710$ef18d530$@gmail.com>
 <CAHaxnUJTYRs2y8g0LWJQfG_p0VVGW04vJk+rf8mqytUfWv79Yw@mail.gmail.com>
 <d47ff613-7986-57a2-8b25-f3af66357643@treenet.co.nz>
Message-ID: <CAHaxnUJzc_MvO2zQ=pKSqOCegCD0qq88F6Mnso=Epi4L58+A6w@mail.gmail.com>

Hi Amos,

Sure, please add it.
Always nice to contribute a little bit :)

Cheers,
Bruce

On Thu, Aug 27, 2020 at 8:30 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> Nice writeup. Do you mind if I add this to the Squid wiki as an example
> for high-performance proxying?
>
>
> Amos
>
>
>
> On 27/08/20 4:35 pm, Bruce Rosenberg wrote:
> > Hi Eliezer,
> >
> > We are running a couple of Squid proxies (the real servers) in front of
> > a pair of LVS servers with keepalived and it works flawlessly.
> > The 2 x Squid proxies are active / active and the LVS servers are active
> > / passive.
> > If a Squid proxy dies the remaining proxy takes all the traffic.
> > If the active LVS server dies, keepalived running on the backup LVS (via
> > VRRP) moves the VIP to itself and it takes all the traffic, so the only
> > difference between the two is one has a higher priority so it gets the
> > VIP first.
> > I have included some sanitised snippets from a keepalived.conf file that
> > should help you.
> > You could easily scale this out if you need more than 2 Squid proxies.
> >
> > The config I provided is for LVS/DR (Direct Route) mode.
> > This method rewrites the MAC address of forwarded packets to that of one
> > of the real servers and is the most scalable way to run LVS.
> > It does require the LVS and real servers be on the same L2 network.
> > If that is not possible then consider LVS/TUN mode or LVS/NAT mode.
> >
> > As LVS/DR rewrites the MAC address, it requires each real server to have
> > the VIP address plumbed on an interface and also requires the real
> > servers to ignore ARP requests for the VIP address as the only device
> > that should respond to ARP requests for the VIP is the active LVS server.
> > We do this by configuring the VIP on the loopback interface on each real
> > but there are other methods as well such as dropping the ARP responses
> > using arptables, iptables or firewalld.
> > I think back in the kernel 2.4 and 2.6 days people used the noarp kernel
> > module which could be configured to ignore ARP requests for a particular
> > IP address but you don't really need this anymore.
> >
> > More info on the loopback arp blocking method -
> >
> https://www.loadbalancer.org/blog/layer-4-direct-routing-lvs-dr-and-layer-4-tun-lvs-tun-in-aws/
> > More info on firewall type arp blocking methods
> > -
> https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/load_balancer_administration/s1-lvs-direct-vsa
> > More info about LVS/DR - http://kb.linuxvirtualserver.org/wiki/LVS/DR
> >
> > If you are using a RPM based distro then to set up the LVS servers you
> > only need the ipvsadm and keepalived packages.
> > Install squid on the reals and configure the VIP on each and disable ARP.
> > Then build the keepalived.conf on both LVS servers and restart
> keepalived.
> >
> > The priority configuration stanza in the vrrp_instance section
> > determines the primary VRRP node (LVS server) for that virtual router
> > instance.
> > The secondary LVS server needs a lower priority compared to the primary.
> > You can configure one as the MASTER and the other as the BACKUP but our
> > guys make them both BACKUP and let the priority sort the election of the
> > primary out.
> > I think this might be to solve a problem of bringing up a BACKUP without
> > a MASTER but I can't confirm that.
> >
> >
> > Good luck.
> >
> >
> > $ cat /etc/keepalived/keepalived.conf
> >
> > global_defs {
> >
> >     notification_email {
> >         # rootmail at example.com <mailto:rootmail at example.com>
> >     }
> >     notification_email_from keepalive-daemon at lvs01.example.com
> > <mailto:keepalive-daemon at lvs01.example.com>
> >     smtp_server 10.1.2.3        # mail.example.com <
> http://mail.example.com>
> >     smtp_connect_timeout 30
> >     lvs_id lvs01.example.com <http://lvs01.example.com>    # Name to
> > mention in email.
> > }
> >
> > vrrp_instance LVS_example {
> >
> >     state BACKUP
> >     priority 150
> >     interface eth0
> >     lvs_sync_daemon_interface eth0
> >     virtual_router_id 5
> >     preempt_delay 20
> >
> >     virtual_ipaddress_excluded {
> >
> >         10.10.10.10   # Squid proxy
> >     }
> >
> >     notify_master "some command to log or send an alert"
> >     notify_backup "some command to log or send an alert"
> >     notify_fault "some command to log or send an alert"
> > }
> >
> >
> > # SQUID Proxy
> > virtual_server 10.10.10.10 3128 {
> >
> >     delay_loop 5
> >     lb_algo wrr
> >     lb_kind DR
> >     protocol TCP
> >
> >     real_server 10.10.10.11 3128 {   # proxy01.example.com
> > <http://proxy01.example.com>
> >         weight 1
> >         inhibit_on_failure 1
> >         TCP_CHECK {
> >             connect_port 3128
> >             connect_timeout 5
> >         }
> >     }
> >
> >     real_server 10.10.10.12 3128 {   # proxy02.example.com
> > <http://proxy02.example.com>
> >         weight 1
> >         inhibit_on_failure 1
> >         TCP_CHECK {
> >             connect_port 3128
> >             connect_timeout 5
> >         }
> >     }
> > }
> >
> >
> > On Thu, Aug 27, 2020 at 8:24 AM Eliezer Croitor <ngtech1ltd at gmail.com
> > <mailto:ngtech1ltd at gmail.com>> wrote:
> >
> >     Hey All,____
> >
> >     __ __
> >
> >     I am reading about LB and tried to find an up-to-date example or
> >     tutorial specific to squid with no luck.____
> >
> >     I have seen:
> >
> http://kb.linuxvirtualserver.org/wiki/Building_Web_Cache_Cluster_using_LVS____
> >
> >     __ __
> >
> >     Which makes sense and also is similar or kind of identical to WCCP
> >     with gre.____
> >
> >     __ __
> >
> >     Anyone knows about a working Squid setup with IPVS/LVS?____
> >
> >     __ __
> >
> >     Thanks,____
> >
> >     Eliezer____
> >
> >     __ __
> >
> >     ----____
> >
> >     Eliezer Croitoru____
> >
> >     Tech Support____
> >
> >     Mobile: +972-5-28704261____
> >
> >     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>____
> >
> >     __ __
> >
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200828/82991263/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 28 03:57:41 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2020 15:57:41 +1200
Subject: [squid-users] ACL-by time- not working. Help!
In-Reply-To: <CALtjann5OSbx9YOf=p8xY5fXvhcY=Bng6u+MPGz+a6QfDii97w@mail.gmail.com>
References: <CALtjann5OSbx9YOf=p8xY5fXvhcY=Bng6u+MPGz+a6QfDii97w@mail.gmail.com>
Message-ID: <713c45c9-ba43-28d2-f017-7ff1583d2670@treenet.co.nz>

On 28/08/20 3:40 am, Luis Mario Niedas Hern?ndez wrote:
> Hello. I need restrict some site by time, but i am not doing well.
> This is my squid.conf. Please help me to fix the problem. I don't know
> why it is not working.
> 

It is not clear what your problem actually is.

An educated guess tells me that you have missed two important details:

 1) your http_access lines are just a long list of allow, allow, allow.
Squid has no reason to deny.

To resolve this you need to write out your policy(s) in the form of
denials. Allowing only the good traffic that remains.

For best performance sort the lines by ACL checking speed and how much
traffic they can drop. The faster it can identify and deny bad traffic
the more speed can go towards the good traffic.


 2) those FB and YT websites use HTTPS and http_access controls only
apply when an HTTPS connection is established. The TLS connection itself
may remain open and continue to be used indefinitely.

You can use the client_lifetime directive to shorten the time CONNECT
tunnels are allowed to remain in use. For your specific case I would set
it to something like 5 minutes. Browsers can auto-recover so this length
should not be visible to clients, but you will want to test that to
confirm what is good for your needs.


There are several other things about your config file that indicate
extremely outdated practices or Squid version. Below is a free audit
report of things that need fixing.


If you are running a Squid older than 3.5 please update ASAP. Then apply
the changes below.

If you are running a Squid v3.5 or newer then you can fix these issues
now with just a check to confirm the change is okay.


> 
> ### autenticacion de los usuarios (http
> b?sica)############################################
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/usuarios_inet
> auth_param basic realm Introduzca su usuario para navegar por la WEB.
> 
> ### por donde responde el squid ###
> ####################################################
> 
> http_port 192.168.1.3:3128
> http_port 127.0.0.1:3128

Are there other IPs assigned to the machine Squid is running on which
you definitely don't want offering proxy service?

If no, then you can replace both those with this line:
  http_port 3128

If yes, then you should replace just the second one with:
  http_port localhost:3128


> 
> ########## ACL ###########################################################################
> #
> # Recommended minimum configuration:
> 

You are missing the very critical port safety checks. These are to
prevent your proxy being DoS'ed or uses as an attack vector against
other software in your LAN.

At worst, you may need to "open" some specific ports by adding them to
the Safe_ports and/or SSL_ports ACL definitions. But generally this is
not necessary, and should only be done after investigating carefully
what that port is used for, including things *other* than the reason you
are asked to open it.


> acl all src all

Since Squid-3.1 the "all "ACL has been built into Squid. You can remove
this line, it does nothing.


> acl localhost src 127.0.0.1/32

On all modern machines localhost include the ::1/128 address. Even when
the machine is IPv4-only connectivity to the network. Localhost is about
connections within the machine itself and IPv4-only OS no longer exist.


> acl localnet src 192.168.1.0/24

No LAN IPv6 ranges? that is something everyone should be planning for a
decade ago.


> acl manager_proto_cache proto cache_object

Since Squid-3.2 the "manager" ACL has been built into Squid. There are
feature changes to the management URLs that need to be controlled by it
and the built-in definition handles those.

Please remove the above ACL line and convert anything that used it to
use the ACL named "manager" instead.


> acl peticion_identificacion proxy_auth REQUIRED
> acl intranet dstdomain intra.xzy
> 
> acl ocio dstdomain  .facebook.com .youtube.com
> 
> acl ocio_medio_dia time MTWHF 12:00-13:10
> acl ocio_tarde time MTWHF 14:00-14:30
> acl ocio_mannana time MTWHF 6:00-8:30
> 
> http_access allow localnet manager_proto_cache
> http_access allow localhost manager_proto_cache
> http_access deny manager_proto_cache

> http_access allow ocio ocio_tarde
> http_access allow ocio ocio_medio_dia
> http_access allow ocio ocio_mannana

Do you really want your proxy to be allowing anyone anywhere in the
world to access those websites through your proxy?

I think these "ocio" lines should look like:

  http_access deny ocio !ocio_tarde !ocio_medio_dia !ocio_mannana

Or, you can combine the time periods into one ACL check for better speed
and understanding:

  acl ocio_tempo time MTWHF 12:00-13:10
  acl ocio_tempo time MTWHF 14:00-14:30
  acl ocio_tempo time MTWHF 6:00-8:30

  http_access deny ocio !ocio_tempo


> 
> http_access allow intranet

I am guessing here. But I think this means you do not want to require
login to access the intranet website.


> http_access allow localnet peticion_identificacion

For more reliable authentication this should be:

  http_access deny !peticion_identificacion
  http_access allow localnet

Or, assuming the above about intranet:

  http_access deny !intranet !peticion_identificacion
  http_access allow localnet

> 
> always_direct allow intranet
> http_access deny all
> never_direct allow all
> 

In summary, I think this access control section should look like the
below lines:

  acl SSL_ports port 443

  acl Safe_ports port 80		# http
  acl Safe_ports port 21		# ftp
  acl Safe_ports port 443		# https
  acl Safe_ports port 70		# gopher
  acl Safe_ports port 210		# wais
  acl Safe_ports port 1025-65535	# unregistered ports
  acl Safe_ports port 280		# http-mgmt
  acl Safe_ports port 488		# gss-http
  acl Safe_ports port 591		# filemaker
  acl Safe_ports port 777		# multiling http

  acl localhost src 127.0.0.1/32 ::1/128
  acl localnet src 192.168.1.0/24

  acl peticion_identificacion proxy_auth REQUIRED
  acl intranet dstdomain intra.xzy

  acl ocio dstdomain  .facebook.com .youtube.com

  acl ocio_tempo time MTWHF 12:00-13:10
  acl ocio_tempo time MTWHF 14:00-14:30
  acl ocio_tempo time MTWHF 6:00-8:30

  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access deny manager !localnet !localhost

  # Prevent occio domains outside permitted times
  http_access deny ocio !ocio_tempo

  # Login required unless visiting intranet site(s)
  http_access deny !intranet !peticion_identificacion

  http_access allow localnet

  http_access deny all

  always_direct allow intranet
  never_direct allow all


> 
> 
> ####### cahce padre #################################################
> 
> cache_peer proxy_padre parent 3128 0  proxy-only
> #cache_peer_domain  proxy_padre !intra.xzy


NP: if you want to restore that !intra.xyz behaviour with modern Squid
use this:

 cache_peer_access proxy_padre allow !intranet


> 
> ##### correo cache manager ####
> 
> cache_mgr lmniedas

This should be an admin contact email. The documentation is not very
clear, sorry about that. It will receive reports about proxy crashes (if
the feature is built) and is displayed on error pages as the address to
contact about problems using the proxy.

For Example;

 cachemgr  lmniedas at example.local

or the prettier version:

 cache_mgr Luis Mario Niedas Hern?ndez <lmniedas at example.local>


> cachemgr_passwd ***

I hope that was not your actual password. If it was you now need to
change it.


> #### tamanno de la cache ####################################
> 
> cache_dir aufs /var/spool/squid 20280 16 256
> 
> #### limites para comenzar a limpiar la cache #####################
> 
> cache_swap_low 90
> cache_swap_high 95
> 
> #### tamanno de los objetos en la cache como maximo ####################
> 
> maximum_object_size  15 MB
> 
> ### memoria cache ###########################
> 
> cache_mem 500 MB
> 
> ### idioma de las paginas de error de squid ##########################
> 
> error_directory /usr/share/squid/errors/Spanish

Since Squid-3.2 error pages can automatically be delivered in a language
the person receiving it can read.

To allow that to happen, but with Spanish as the default use this
directive instead of error_directory:

 error_default_language es


FYI, you can also apply branding to the pages display by editing
/etc/squid/errorpages.css


> 
> ##### debug_options cantidad de informaci?n en cache_log #################
> 
> debug_options ALL,0 ALL,1 rotate=8760

This directive applies the options configured left-to-right.

The "ALL" setting resets *ALL* debug sections to the level given.

That means you should only use debug section "ALL" once in the whole of
squid.conf and it should be done before any other N,N pairs.

Your config actually means this:

  debug_options ALL,1 rotate=8760


> 
> ######### LOGS #######################################
> 
> cache_log /var/log/squid/cache.log
> access_log stdio:/var/log/squid/access.log  rotate=8760
> cache_store_log stdio:/var/log/squid/store.log

Is there any reason you need this log?
It typically is only useful for debugging and this line could be removed
to speed up your proxy and save disk space.


> 
> ##################
> 
> #AFECTA LA CANTIDAD ESPECIFICAMENTE A STORE.LOG
> 
> logfile_rotate 8760
> 
> #### 365 dias * 24 horas es la cantidad de rotaciones de los logs en el crontab

Does that mean you are running logrotate every hour of every day?

Perhapse there is some better way to do log handling?

Begin with deciding whether you need store.log at all. If that is not
enough and you want assistance with ideas about further improvements
please tell what is the reason why this proxy is rotating to often.


HTH
Amos


From web at 3dresearch.com  Fri Aug 28 04:08:38 2020
From: web at 3dresearch.com (Janos Dohanics)
Date: Fri, 28 Aug 2020 00:08:38 -0400
Subject: [squid-users] deny_info page not shown
Message-ID: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>

Hello,

In my config file I have:

deny_info http://google.com custom

However, Firefox shows the error page "Unable to connect".

Here is the full config file:

acl SSL_ports port 443 4433 8443
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost

acl custom dstdom_regex "/usr/local/share/examples/squidGuard/blacklists/custom/banlist.txt"
http_access deny custom
deny_info http://google.com custom
http_reply_access deny custom

acl ads dstdom_regex "/usr/local/etc/squid/yoyo_ad_block.txt"
http_access deny ads
deny_info TCP_RESET ads

http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128

cache_dir ufs /var/squid/cache 100 16 256

coredump_dir /var/squid/cache

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

logfile_rotate 0

banlist.txt:
.hulu.com
.netflix.com

Would you please point out the problem?


From squid3 at treenet.co.nz  Fri Aug 28 05:08:01 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2020 17:08:01 +1200
Subject: [squid-users] deny_info page not shown
In-Reply-To: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
Message-ID: <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>

On 28/08/20 4:08 pm, Janos Dohanics wrote:
> Hello,
> 
> In my config file I have:
> 
> deny_info http://google.com custom
> 
> However, Firefox shows the error page "Unable to connect".
> 

When? To what type of URL?


> 
> acl custom dstdom_regex "/usr/local/share/examples/squidGuard/blacklists/custom/banlist.txt"
> http_access deny custom

Denies a client access to some traffic ...

> deny_info http://google.com custom

Asks Squid to perform a URL-redirect to http://google.com instead of
delivering error pages when ACL "deny custom" happens.


> http_reply_access deny custom

... denies Squid permission to deliver your custom URL-redirect to the
client.

> 
> Would you please point out the problem?


Two problems. The one mentioned above.

Plus the fact that Browsers refuse to display or do anything for non-200
status responses to CONNECT tunnels. Whenever Browsers access https://
URLs through the proxy they use CONNECT tunnels.

Amos


From web at 3dresearch.com  Fri Aug 28 06:22:54 2020
From: web at 3dresearch.com (Janos Dohanics)
Date: Fri, 28 Aug 2020 02:22:54 -0400
Subject: [squid-users] deny_info page not shown
In-Reply-To: <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
Message-ID: <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>

On Fri, 28 Aug 2020 17:08:01 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> [...]

Amos,

thank you for the quick reply.

> > deny_info http://google.com custom
> 
> Asks Squid to perform a URL-redirect to http://google.com instead of
> delivering error pages when ACL "deny custom" happens.
> 
> 
> > http_reply_access deny custom
> 
> ... denies Squid permission to deliver your custom URL-redirect to the
> client.

I have removed the http_reply_access... line.

> > 
> > Would you please point out the problem?
> 
> 
> Two problems. The one mentioned above.
> 
> Plus the fact that Browsers refuse to display or do anything for
> non-200 status responses to CONNECT tunnels. Whenever Browsers access
> https:// URLs through the proxy they use CONNECT tunnels.

I tried different browsers:

-Firefox79/FreeBSD12: no redirect
-Firefox80/Windows7:  no redirect
-Explorer11/Windows7: sometimes does redirect, sometimes doesn't
-Chrome84/Windows7:   sometimes does redirect, sometimes doesn't

>From the log (10.61.70.68=Win7, 10.61.70.200=FreeBSD):

1598593892.883    342 10.61.70.68 TCP_DENIED/307 403 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598593917.883      0 10.61.70.68 TCP_DENIED/307 403 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598593953.145  61038 10.61.70.68 TCP_TUNNEL/200 4768 CONNECT netflix.com:443 - HIER_DIRECT/34.198.43.9 -
1598593965.273    167 10.61.70.68 TCP_MISS/301 992 GET http://netflix.com/ - HIER_DIRECT/34.198.43.9 -
1598593966.352      0 10.61.70.68 TCP_DENIED/302 390 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598593978.145  60456 10.61.70.68 TCP_TUNNEL/200 4768 CONNECT netflix.com:443 - HIER_DIRECT/34.198.43.9 -
1598593998.290  32918 10.61.70.68 TCP_TUNNEL/200 4610 CONNECT netflix.com:443 - HIER_DIRECT/34.198.43.9 -
1598594045.752      0 10.61.70.68 TCP_DENIED/302 390 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598594086.507  41199 10.61.70.68 TCP_TUNNEL/200 4610 CONNECT netflix.com:443 - HIER_DIRECT/34.198.43.9 -
1598594166.954      0 10.61.70.68 TCP_DENIED/307 403 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598594449.238      0 10.61.70.68 TCP_DENIED/302 390 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598594475.705      0 10.61.70.68 TCP_DENIED/302 390 CONNECT www.netflix.com:443 - HIER_NONE/- text/html
1598594523.052  47644 10.61.70.68 TCP_TUNNEL/200 4610 CONNECT netflix.com:443 - HIER_DIRECT/34.198.43.9 -

1598595287.510      0 10.61.70.200 TCP_DENIED/307 403 CONNECT www.netflix.com:443 - HIER_NONE/- text/html

I think the TCP_DENIED/307 entries are from Firefox.

Is there a way to have deny_info instruct browsers to reliably display
the desired URL/page?



From squid3 at treenet.co.nz  Fri Aug 28 06:59:56 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2020 18:59:56 +1200
Subject: [squid-users] deny_info page not shown
In-Reply-To: <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
Message-ID: <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>

On 28/08/20 6:22 pm, Janos Dohanics wrote:
> 
> Is there a way to have deny_info instruct browsers to reliably display
> the desired URL/page?

No there is not. This is a security feature of Browsers not something
Squid can workaround.

CONNECT is a request to open a TCP connection. Delivering an HTTP page,
or even a URL redirect in response to a TCP connection request is
completely the wrong type of result.

Like asking someone to open a door because you have a load of things
needing to go through it - and they instead throw a basket of apples at
you. Not want you expected, and more harm than good.


Amos


From squid at loel.fr  Fri Aug 28 07:18:09 2020
From: squid at loel.fr (Wesley Mouedine Assaby)
Date: Fri, 28 Aug 2020 11:18:09 +0400
Subject: [squid-users] filter access.log
Message-ID: <7b8594d4d3f332c3a9576f34b8df0875@loel.fr>

Hi,

I have the following logs :

1598547651.549 120818 192.168.100.105 TCP_TUNNEL/200 3234 CONNECT 
dmp.re:443 ericf HIER_DIRECT/213.186.33.2 -
1598547651.549 120726 192.168.100.105 TCP_TUNNEL/200 3234 CONNECT 
www.dmp.re:443 ericf HIER_DIRECT/213.186.33.2 -
1598547652.325      0 192.168.100.109 TCP_DENIED/407 3881 CONNECT 
g.live.com:443 - HIER_NONE/- text/html
1598547654.216     25 192.168.100.109 TCP_MISS/200 4973 GET 
http://192.168.100.89/nagios/cgi-bin/status.cgi? ericf 
HIER_DIRECT/192.168.100.89 text/html
1598547662.424      0 192.168.100.109 TCP_DENIED/407 3881 CONNECT 
g.live.com:443 - HIER_NONE/- text/html
1598547664.937     26 192.168.100.109 TCP_MISS/200 4978 GET 
http://192.168.100.89/nagios/cgi-bin/status.cgi? ericf 
HIER_DIRECT/192.168.100.89 text/html
1598547671.345 110538 192.168.100.116 TCP_TUNNEL/200 55246 CONNECT 
login.live.com:443 ericf HIER_DIRECT/40.90.22.187 -
1598547672.565      0 192.168.100.109 TCP_DENIED/407 4228 CONNECT 
g.live.com:443 - HIER_NONE/- text/html
1598547675.655     25 192.168.100.109 TCP_MISS/200 4974 GET 
http://192.168.100.89/nagios/cgi-bin/status.cgi? ericf 
HIER_DIRECT/192.168.100.89 text/html
1598547676.192      0 192.168.100.109 TCP_DENIED/407 3881 CONNECT 
g.live.com:443 - HIER_NONE/- text/html

Is it possible to remove log that is not authenticated (ldap) ?
I mean these lines :  *- HIER_NONE/- text/html$

Thank's!

-- Eric


From info at schroeffu.ch  Fri Aug 28 08:12:26 2020
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Fri, 28 Aug 2020 08:12:26 +0000
Subject: [squid-users] SSL Bump: I have weekly more sites to whitelist due
 to HTTP Error 403 on opening site content
Message-ID: <1fbaab32147a0f8e5dfd73ef99964e74@schroeffu.ch>

Hi Squid Community,

the last weeks it felt that more and more websites are going to be "incompatible" with Squid SSL bump.
Some Websites are not displayed at all and a "403 Forbidden" from their proxy is displayed, others are displayed very ugly because some CSS is missing due to HTTP Error 403 on CSS resources.

Is there any way to tune SSL Bump for less problems with websites?

Here some example websites which are not loading at all with SSL Bump:

- forcepoint.com (Their Proxy displays: 403 forbidden)
- itsg.de (Squid: Connect reset by peer)
- leica-geosystems.com (Bad Request)

Displayed very ugly because CSS Files gots HTTP Error 403 with SSL bump:

- pyur.com
- help.nextcloud.com
- it feels like all websites with Discourse Forums are having problems with ssl bump - css missing, very ugly
- many more

This are only some examples. Who can reproduce this problems with its own SSL Bump Squid? Am I doing something wrong with SSL Bump? Is Squid 5 alerady better for this? 

Thanks for any help
Schroeffu

My current cump conf is extremely simple, just the default:

http_port proxy03bs.tld.com:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/subca.crt.pem key=/etc/squid/certs/subca.key.ohnersa.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
ssl_bump bump !domains_dont_sslbump
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200828/5d928b11/attachment.htm>

From web at 3dresearch.com  Fri Aug 28 08:23:44 2020
From: web at 3dresearch.com (Janos Dohanics)
Date: Fri, 28 Aug 2020 04:23:44 -0400
Subject: [squid-users] deny_info page not shown
In-Reply-To: <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
 <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
Message-ID: <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>

On Fri, 28 Aug 2020 18:59:56 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 28/08/20 6:22 pm, Janos Dohanics wrote:
> > 
> > Is there a way to have deny_info instruct browsers to reliably
> > display the desired URL/page?
> 
> No there is not. This is a security feature of Browsers not something
> Squid can workaround.
> 
> CONNECT is a request to open a TCP connection. Delivering an HTTP
> page, or even a URL redirect in response to a TCP connection request
> is completely the wrong type of result.
> 
> Like asking someone to open a door because you have a load of things
> needing to go through it - and they instead throw a basket of apples
> at you. Not want you expected, and more harm than good.

Thanks for the explanation - so, the rationale for the http://... acl
value in the deny_info directive is conditioned on "if the browser is
willing"?


From uhlar at fantomas.sk  Fri Aug 28 08:31:41 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 28 Aug 2020 10:31:41 +0200
Subject: [squid-users] deny_info page not shown
In-Reply-To: <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
 <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
 <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>
Message-ID: <20200828083141.GA7917@fantomas.sk>

>> On 28/08/20 6:22 pm, Janos Dohanics wrote:
>> > Is there a way to have deny_info instruct browsers to reliably
>> > display the desired URL/page?

>On Fri, 28 Aug 2020 18:59:56 +1200
>Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> No there is not. This is a security feature of Browsers not something
>> Squid can workaround.
>>
>> CONNECT is a request to open a TCP connection. Delivering an HTTP
>> page, or even a URL redirect in response to a TCP connection request
>> is completely the wrong type of result.
>>
>> Like asking someone to open a door because you have a load of things
>> needing to go through it - and they instead throw a basket of apples
>> at you. Not want you expected, and more harm than good.

On 28.08.20 04:23, Janos Dohanics wrote:
>Thanks for the explanation - so, the rationale for the http://... acl
>value in the deny_info directive is conditioned on "if the browser is
>willing"?

when you ask via HTTP for HTTP page and get HTTP answer, it is different
than asking via HTTP for CONNECT and getting CONNECT denied via HTTP.

in the latter case it is clear that the request was denied by proxy and
since secure content was requested, the insecure response must not be shown.

That's the security provided.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Support bacteria - they're the only culture some people have.


From web at 3dresearch.com  Fri Aug 28 08:49:09 2020
From: web at 3dresearch.com (Janos Dohanics)
Date: Fri, 28 Aug 2020 04:49:09 -0400
Subject: [squid-users] deny_info page not shown
In-Reply-To: <20200828083141.GA7917@fantomas.sk>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
 <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
 <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>
 <20200828083141.GA7917@fantomas.sk>
Message-ID: <20200828044909.46732082e447b049d56d06a4@3dresearch.com>

On Fri, 28 Aug 2020 10:31:41 +0200
Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:

> >> On 28/08/20 6:22 pm, Janos Dohanics wrote:
> >> > Is there a way to have deny_info instruct browsers to reliably
> >> > display the desired URL/page?
> 
> >On Fri, 28 Aug 2020 18:59:56 +1200
> >Amos Jeffries <squid3 at treenet.co.nz> wrote:
> >> No there is not. This is a security feature of Browsers not
> >> something Squid can workaround.
> >>
> >> CONNECT is a request to open a TCP connection. Delivering an HTTP
> >> page, or even a URL redirect in response to a TCP connection
> >> request is completely the wrong type of result.
> >>
> >> Like asking someone to open a door because you have a load of
> >> things needing to go through it - and they instead throw a basket
> >> of apples at you. Not want you expected, and more harm than good.
> 
> On 28.08.20 04:23, Janos Dohanics wrote:
> >Thanks for the explanation - so, the rationale for the http://... acl
> >value in the deny_info directive is conditioned on "if the browser is
> >willing"?
> 
> when you ask via HTTP for HTTP page and get HTTP answer, it is
> different than asking via HTTP for CONNECT and getting CONNECT denied
> via HTTP.
> 
> in the latter case it is clear that the request was denied by proxy
> and since secure content was requested, the insecure response must
> not be shown.

Thanks - would you have an example of using deny_info http://... acl
which actually works?



From squid3 at treenet.co.nz  Fri Aug 28 08:49:33 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2020 20:49:33 +1200
Subject: [squid-users] SSL Bump: I have weekly more sites to whitelist
 due to HTTP Error 403 on opening site content
In-Reply-To: <1fbaab32147a0f8e5dfd73ef99964e74@schroeffu.ch>
References: <1fbaab32147a0f8e5dfd73ef99964e74@schroeffu.ch>
Message-ID: <34579902-83c9-e549-57c1-c5d85d1249a0@treenet.co.nz>

On 28/08/20 8:12 pm, info at schroeffu.ch wrote:
> 
> Hi Squid Community,
> 
> the last weeks it felt that more and more websites are going to be
> "incompatible" with Squid SSL bump.

"feelings" aside, that is exactly the situation. SSL-Bump is literally a
security attack on clients traffic. Exactly the thing TLS is designed to
prevent.

As all our official SSL-Bump documentation says very prominently:
  "when used properly TLS cannot be bumped".

There was a long period where very few websites used TLS properly. The
"HTTPS Everywhere" project has forced a lot of sites admin to get
experience with HTTPS and over time more networks are actually starting
to use it properly.

Which version are you using now?


> Some Websites are not displayed at all and a "403 Forbidden" from their
> proxy is displayed, others are displayed very ugly because some CSS is
> missing due to HTTP Error 403 on CSS resources.

"their"? Whose proxy?

If the problem is coming from a proxy that is not yours, contact its'
sysadmin.

> 
> Is there any way to tune SSL Bump for less problems with websites?
> 

That depends on what you have configured (see below) and whether the
sites you are interested in are capable of being bumped (see above).


> Here some example websites which are not loading at all with SSL Bump:
> 
> - forcepoint.com (Their Proxy displays: 403 forbidden)
> - itsg.de (Squid: Connect reset by peer)
> - leica-geosystems.com (Bad Request)
> 
> Displayed very ugly because CSS Files gots HTTP Error 403 with SSL bump:
> 
> - pyur.com
> - help.nextcloud.com
> - it feels like all websites with Discourse Forums are having problems
> with ssl bump - css missing, very ugly
> - many more
> 
> This are only some examples. Who can reproduce this problems with its
> own SSL Bump Squid? Am I doing something wrong with SSL Bump? Is Squid 5
> alerady better for this?
> 

Which version are you using now?
Exact version, as shown by squid -v, including OpenSSL library version.


> Thanks for any help
> Schroeffu
> 
> My current cump conf is extremely simple, just the default:

FYI; the *default* for SSL-Bump is not to exist. So no your config is
way beyond default, simple as it is.


> 
> http_port proxy03bs.tld.com:8080 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/subca.crt.pem
> key=/etc/squid/certs/subca.key.ohnersa.pem
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db
> -M 4MB
> ssl_bump bump !domains_dont_sslbump
> 

This performs the bump action without any actual info from the TLS
server being "bumped". So of course you can expect a lot of problems
with that.

A "reliable" (as much as it can be) configuration looks at the TLS
handshake from each endpoint before deciding which details to pass on
and which to change:


 acl step1 at_step SslBump1
 ssl_bump peek step1
 ssl_bump splice domains_dont_sslbump
 ssl_bump stare all
 ssl_bump bump all


Adding in your whitelist of non-bump domains would look like:

 acl step1 at_step SslBump1
 ssl_bump peek step1
 ssl_bump splice domains_dont_sslbump
 ssl_bump stare all
 ssl_bump bump all


HTH
Amos


From squid3 at treenet.co.nz  Fri Aug 28 08:54:00 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2020 20:54:00 +1200
Subject: [squid-users] filter access.log
In-Reply-To: <7b8594d4d3f332c3a9576f34b8df0875@loel.fr>
References: <7b8594d4d3f332c3a9576f34b8df0875@loel.fr>
Message-ID: <fe197ed0-c71a-791c-3c3e-36e251576a67@treenet.co.nz>

On 28/08/20 7:18 pm, Wesley Mouedine Assaby wrote:
> 
> Is it possible to remove log that is not authenticated (ldap) ?

Of course.

With the current Squid versions use a "note" type ACL to match any
details produced by helpers. e.g. the "user=" sent by the authentication
helper.

For example:

 acl hasUser note user
 access_log ... logformat=squid hasUser


Or,

 acl hasUser note user
 access_log none !hasUser
 access_log ...


Amos


From squid3 at treenet.co.nz  Fri Aug 28 10:58:00 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Aug 2020 22:58:00 +1200
Subject: [squid-users] deny_info page not shown
In-Reply-To: <20200828044909.46732082e447b049d56d06a4@3dresearch.com>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
 <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
 <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>
 <20200828083141.GA7917@fantomas.sk>
 <20200828044909.46732082e447b049d56d06a4@3dresearch.com>
Message-ID: <b0c1814a-02f4-9c16-a559-417ea8e5d2f9@treenet.co.nz>

On 28/08/20 8:49 pm, Janos Dohanics wrote:
> 
> Thanks - would you have an example of using deny_info http://... acl
> which actually works?
> 

Any HTTP request message where 302 is a valid response status code will
work. Your configuration does that.

The problem is that Browsers only accept 20x status for CONNECT
requests. Everything else is "Cannot Connect".


Amos


From web at 3dresearch.com  Fri Aug 28 15:29:57 2020
From: web at 3dresearch.com (Janos Dohanics)
Date: Fri, 28 Aug 2020 11:29:57 -0400
Subject: [squid-users] deny_info page not shown
In-Reply-To: <b0c1814a-02f4-9c16-a559-417ea8e5d2f9@treenet.co.nz>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
 <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
 <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>
 <20200828083141.GA7917@fantomas.sk>
 <20200828044909.46732082e447b049d56d06a4@3dresearch.com>
 <b0c1814a-02f4-9c16-a559-417ea8e5d2f9@treenet.co.nz>
Message-ID: <20200828112957.042691182212b4f33680f2fb@3dresearch.com>

On Fri, 28 Aug 2020 22:58:00 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 28/08/20 8:49 pm, Janos Dohanics wrote:
> > 
> > Thanks - would you have an example of using deny_info http://... acl
> > which actually works?
> > 
> 
> Any HTTP request message where 302 is a valid response status code
> will work. Your configuration does that.
> 
> The problem is that Browsers only accept 20x status for CONNECT
> requests. Everything else is "Cannot Connect".

Thank you for all your help and patience...


From lmniedas at gmail.com  Fri Aug 28 15:50:38 2020
From: lmniedas at gmail.com (=?UTF-8?Q?Luis_Mario_Niedas_Hern=C3=A1ndez?=)
Date: Fri, 28 Aug 2020 11:50:38 -0400
Subject: [squid-users] ACL-by time- not working. Help!
In-Reply-To: <713c45c9-ba43-28d2-f017-7ff1583d2670@treenet.co.nz>
References: <CALtjann5OSbx9YOf=p8xY5fXvhcY=Bng6u+MPGz+a6QfDii97w@mail.gmail.com>
 <713c45c9-ba43-28d2-f017-7ff1583d2670@treenet.co.nz>
Message-ID: <CALtjann+q=-_OZ5EZ3m3zagiJ+0QnhmTKXhGUTQr7=6-x77TUA@mail.gmail.com>

El vie., 28 de ago. de 2020 a la(s) 00:03, Amos Jeffries
(squid3 at treenet.co.nz) escribi?:
>
> On 28/08/20 3:40 am, Luis Mario Niedas Hern?ndez wrote:
> > Hello. I need restrict some site by time, but i am not doing well.
> > This is my squid.conf. Please help me to fix the problem. I don't know
> > why it is not working.
> >
>
> It is not clear what your problem actually is.

Well. My problem is that  i had to block facebook and youtube but i
did not how to do it. Your correction works fine. Thanks for
explaining me some stuff. I appreciate that.

>
> An educated guess tells me that you have missed two important details:
>
>  1) your http_access lines are just a long list of allow, allow, allow.
> Squid has no reason to deny.

 jajajaja Honestly, I don't understand how squid work. I mean, I don't
know what it is the logic to follow with the acl directives and
http_access allow | deny. I am learning about it. If you can recommend
me a book or a place to look for learning about how to build
adequately my rules in squid. I need it. i don't want copy and paste
acl from some plate and put it in my config, I really want to know how
I have to think. LEARN

>
> To resolve this you need to write out your policy(s) in the form of
> denials. Allowing only the good traffic that remains.
>
I guess that when we put  this:

http_access deny all.

we are telling squid that everything that has not a http_access allow,
it is blocked. So,  why I have to put http_access deny
!peticion_identificacion, instead http_access allow
peticion_identificacion ?


> For best performance sort the lines by ACL checking speed and how much
> traffic they can drop. The faster it can identify and deny bad traffic
> the more speed can go towards the good traffic.
>
>  2) those FB and YT websites use HTTPS and http_access controls only
> apply when an HTTPS connection is established. The TLS connection itself
> may remain open and continue to be used indefinitely.
>
> You can use the client_lifetime directive to shorten the time CONNECT
> tunnels are allowed to remain in use. For your specific case I would set
> it to something like 5 minutes. Browsers can auto-recover so this length
> should not be visible to clients, but you will want to test that to
> confirm what is good for your needs.
>
>
> There are several other things about your config file that indicate
> extremely outdated practices or Squid version. Below is a free audit
> report of things that need fixing.
>
>
> If you are running a Squid older than 3.5 please update ASAP. Then apply
> the changes below.

I am running squid 4.6.

>
> If you are running a Squid v3.5 or newer then you can fix these issues
> now with just a check to confirm the change is okay.
>
>
> >
> > ### autenticacion de los usuarios (http
> > b?sica)############################################
> > auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/usuarios_inet
> > auth_param basic realm Introduzca su usuario para navegar por la WEB.
> >
> > ### por donde responde el squid ###
> > ####################################################
> >
> > http_port 192.168.1.3:3128
> > http_port 127.0.0.1:3128
>
> Are there other IPs assigned to the machine Squid is running on which
> you definitely don't want offering proxy service?

Yes, my machine has two different interfaces with  different ranges
and I want to offer squid service only for one. ;-)

>
> If no, then you can replace both those with this line:
>   http_port 3128
>
> If yes, then you should replace just the second one with:
>   http_port localhost:3128
>
>
> >
> > ########## ACL ###########################################################################
> > #
> > # Recommended minimum configuration:
> >
>
> You are missing the very critical port safety checks. These are to
> prevent your proxy being DoS'ed or uses as an attack vector against
> other software in your LAN.
>
> At worst, you may need to "open" some specific ports by adding them to
> the Safe_ports and/or SSL_ports ACL definitions. But generally this is
> not necessary, and should only be done after investigating carefully
> what that port is used for, including things *other* than the reason you
> are asked to open it.
>
>
> > acl all src all
>
> Since Squid-3.1 the "all "ACL has been built into Squid. You can remove
> this line, it does nothing.
>
>
> > acl localhost src 127.0.0.1/32
>
> On all modern machines localhost include the ::1/128 address. Even when
> the machine is IPv4-only connectivity to the network. Localhost is about
> connections within the machine itself and IPv4-only OS no longer exist.
>
>
> > acl localnet src 192.168.1.0/24
>
> No LAN IPv6 ranges? that is something everyone should be planning for a
> decade ago.

I don't need IPv6. It is a small office.

>
>
> > acl manager_proto_cache proto cache_object
>
> Since Squid-3.2 the "manager" ACL has been built into Squid. There are
> feature changes to the management URLs that need to be controlled by it
> and the built-in definition handles those.
>
> Please remove the above ACL line and convert anything that used it to
> use the ACL named "manager" instead.
>
>
> > acl peticion_identificacion proxy_auth REQUIRED
> > acl intranet dstdomain intra.xzy
> >
> > acl ocio dstdomain  .facebook.com .youtube.com
> >
> > acl ocio_medio_dia time MTWHF 12:00-13:10
> > acl ocio_tarde time MTWHF 14:00-14:30
> > acl ocio_mannana time MTWHF 6:00-8:30
> >
> > http_access allow localnet manager_proto_cache
> > http_access allow localhost manager_proto_cache
> > http_access deny manager_proto_cache
>
> > http_access allow ocio ocio_tarde
> > http_access allow ocio ocio_medio_dia
> > http_access allow ocio ocio_mannana
>
> Do you really want your proxy to be allowing anyone anywhere in the
> world to access those websites through your proxy?
>
> I think these "ocio" lines should look like:
>
>   http_access deny ocio !ocio_tarde !ocio_medio_dia !ocio_mannana
>
> Or, you can combine the time periods into one ACL check for better speed
> and understanding:
>
>   acl ocio_tempo time MTWHF 12:00-13:10
>   acl ocio_tempo time MTWHF 14:00-14:30
>   acl ocio_tempo time MTWHF 6:00-8:30
>
>   http_access deny ocio !ocio_tempo

Thanks for this recommendation. I did not know how to do it fine.

>
> >
> > http_access allow intranet
>
> I am guessing here. But I think this means you do not want to require
> login to access the intranet website.

jajaja No, here i wanted to say: Squid you should allow the connection
to intranet, but with authentication. I thought that we only had to
request authentication only one time and squid will know that every
http request  made for the same user will be checked  again the rest
of ACL.  :-( I am configuring my squid blink, i meant, i read and test
what happens, but it is a little hard. It is the reason that I wrote
to the squid-user list.

>
>
> > http_access allow localnet peticion_identificacion
>
> For more reliable authentication this should be:
>
>   http_access deny !peticion_identificacion
>   http_access allow localnet
>
> Or, assuming the above about intranet:
>
>   http_access deny !intranet !peticion_identificacion
>   http_access allow localnet
>
> >
> > always_direct allow intranet
> > http_access deny all
> > never_direct allow all
> >
>
> In summary, I think this access control section should look like the
> below lines:
>
>   acl SSL_ports port 443
>
>   acl Safe_ports port 80                # http
>   acl Safe_ports port 21                # ftp
>   acl Safe_ports port 443               # https
>   acl Safe_ports port 70                # gopher
>   acl Safe_ports port 210               # wais
>   acl Safe_ports port 1025-65535        # unregistered ports
>   acl Safe_ports port 280               # http-mgmt
>   acl Safe_ports port 488               # gss-http
>   acl Safe_ports port 591               # filemaker
>   acl Safe_ports port 777               # multiling http
>
>   acl localhost src 127.0.0.1/32 ::1/128
>   acl localnet src 192.168.1.0/24
>
>   acl peticion_identificacion proxy_auth REQUIRED
>   acl intranet dstdomain intra.xzy
>
>   acl ocio dstdomain  .facebook.com .youtube.com
>
>   acl ocio_tempo time MTWHF 12:00-13:10
>   acl ocio_tempo time MTWHF 14:00-14:30
>   acl ocio_tempo time MTWHF 6:00-8:30
>
>   http_access deny !Safe_ports
>   http_access deny CONNECT !SSL_ports
>   http_access deny manager !localnet !localhost
>
>   # Prevent occio domains outside permitted times
>   http_access deny ocio !ocio_tempo
>
>   # Login required unless visiting intranet site(s)
>   http_access deny !intranet !peticion_identificacion
>
>   http_access allow localnet
>
>   http_access deny all
>
>   always_direct allow intranet
>   never_direct allow all
>
>
> >
> >
> > ####### cahce padre #################################################
> >
> > cache_peer proxy_padre parent 3128 0  proxy-only
> > #cache_peer_domain  proxy_padre !intra.xzy
>
>
> NP: if you want to restore that !intra.xyz behaviour with modern Squid
> use this:
>
>  cache_peer_access proxy_padre allow !intranet
>
>
> >
> > ##### correo cache manager ####
> >
> > cache_mgr lmniedas
>
> This should be an admin contact email. The documentation is not very
> clear, sorry about that. It will receive reports about proxy crashes (if
> the feature is built) and is displayed on error pages as the address to
> contact about problems using the proxy.
>
> For Example;
>
>  cachemgr  lmniedas at example.local
>
> or the prettier version:
>
>  cache_mgr Luis Mario Niedas Hern?ndez <lmniedas at example.local>
>
>
> > cachemgr_passwd ***
>
> I hope that was not your actual password. If it was you now need to
> change it.
>

jajajaja i changed the information, just to not compromise my security
or at least just to not show all the real information about my
network. ;-)
>
> > #### tamanno de la cache ####################################
> >
> > cache_dir aufs /var/spool/squid 20280 16 256
> >
> > #### limites para comenzar a limpiar la cache #####################
> >
> > cache_swap_low 90
> > cache_swap_high 95
> >
> > #### tamanno de los objetos en la cache como maximo ####################
> >
> > maximum_object_size  15 MB
> >
> > ### memoria cache ###########################
> >
> > cache_mem 500 MB
> >
> > ### idioma de las paginas de error de squid ##########################
> >
> > error_directory /usr/share/squid/errors/Spanish
>
> Since Squid-3.2 error pages can automatically be delivered in a language
> the person receiving it can read.
>
> To allow that to happen, but with Spanish as the default use this
> directive instead of error_directory:
>
>  error_default_language es
>

I didn't know this. thanks again.

> FYI, you can also apply branding to the pages display by editing
> /etc/squid/errorpages.css
>
>
> >
> > ##### debug_options cantidad de informaci?n en cache_log #################
> >
> > debug_options ALL,0 ALL,1 rotate=8760
>
> This directive applies the options configured left-to-right.
>
> The "ALL" setting resets *ALL* debug sections to the level given.
>
> That means you should only use debug section "ALL" once in the whole of
> squid.conf and it should be done before any other N,N pairs.
>
> Your config actually means this:
>
>   debug_options ALL,1 rotate=8760
>
>
> >
> > ######### LOGS #######################################
> >
> > cache_log /var/log/squid/cache.log
> > access_log stdio:/var/log/squid/access.log  rotate=8760
> > cache_store_log stdio:/var/log/squid/store.log
>
> Is there any reason you need this log?
> It typically is only useful for debugging and this line could be removed
> to speed up your proxy and save disk space.

My boss told me that I must save  all my logs, just to check later
what the people are doing and where they are surfing on the internet.

>
> >
> > ##################
> >
> > #AFECTA LA CANTIDAD ESPECIFICAMENTE A STORE.LOG
> >
> > logfile_rotate 8760
> >
> > #### 365 dias * 24 horas es la cantidad de rotaciones de los logs en el crontab
>
> Does that mean you are running logrotate every hour of every day?
>
Yes, I am rotating every one hours all squid's logs. Honestly, because
i thought that it is the more easy way to make analytics work. What
you can recommend me??

> Perhapse there is some better way to do log handling?
>
> Begin with deciding whether you need store.log at all. If that is not
> enough and you want assistance with ideas about further improvements
> please tell what is the reason why this proxy is rotating to often.
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



--
"El futuro tiene muchos nombres. Para los d?biles es lo inalcanzable.
Para los temerosos, lo desconocido. Para los valientes es la
oportunidad"
Victor Hugo


From squid3 at treenet.co.nz  Fri Aug 28 16:25:10 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Aug 2020 04:25:10 +1200
Subject: [squid-users] ACL-by time- not working. Help!
In-Reply-To: <CALtjann+q=-_OZ5EZ3m3zagiJ+0QnhmTKXhGUTQr7=6-x77TUA@mail.gmail.com>
References: <CALtjann5OSbx9YOf=p8xY5fXvhcY=Bng6u+MPGz+a6QfDii97w@mail.gmail.com>
 <713c45c9-ba43-28d2-f017-7ff1583d2670@treenet.co.nz>
 <CALtjann+q=-_OZ5EZ3m3zagiJ+0QnhmTKXhGUTQr7=6-x77TUA@mail.gmail.com>
Message-ID: <bb91242a-60f0-0e05-3b5a-57457e3a363a@treenet.co.nz>

On 29/08/20 3:50 am, Luis Mario Niedas Hern?ndez wrote:
> El vie., 28 de ago. de 2020 a la(s) 00:03, Amos Jeffries
> (squid3 at treenet.co.nz) escribi?:
>>
>> On 28/08/20 3:40 am, Luis Mario Niedas Hern?ndez wrote:
>>> Hello. I need restrict some site by time, but i am not doing well.
>>> This is my squid.conf. Please help me to fix the problem. I don't know
>>> why it is not working.
>>>
>>
>> It is not clear what your problem actually is.
> 
> Well. My problem is that  i had to block facebook and youtube but i
> did not how to do it. Your correction works fine. Thanks for
> explaining me some stuff. I appreciate that.
> 
>>
>> An educated guess tells me that you have missed two important details:
>>
>>  1) your http_access lines are just a long list of allow, allow, allow.
>> Squid has no reason to deny.
> 
>  jajajaja Honestly, I don't understand how squid work. I mean, I don't
> know what it is the logic to follow with the acl directives and
> http_access allow | deny. I am learning about it. If you can recommend
> me a book or a place to look for learning about how to build
> adequately my rules in squid. I need it. i don't want copy and paste
> acl from some plate and put it in my config, I really want to know how
> I have to think. LEARN
> 

Certainly. The details of access controls are all documented at
<http://wiki.squid-cache.org/SquidFaq/SquidAcl>

or if you prefer a physical book the "Squid 3.1: Beginners Guide" is
still a good learning resource to begin with. What it lacks is mostly
detail on new features.


>>
>> To resolve this you need to write out your policy(s) in the form of
>> denials. Allowing only the good traffic that remains.
>>
> I guess that when we put  this:
> 
> http_access deny all.
> 
> we are telling squid that everything that has not a http_access allow,
> it is blocked. So,  why I have to put http_access deny
> !peticion_identificacion, instead http_access allow
> peticion_identificacion ?


ACLs actually have three states: YES, NO, UNKNOWN. Authentication is one
ACL type where the third state is important.

"allow peticion_identificacion" lets all the traffic which is-YES through.
 Meaning it will try to get credentials, but if they do not validate as
correct Squid skips on to checking the next access control line.


"deny !peticion_identificacion" blocks all the traffic which is not-YES.

Meaning Squid will block clients who cannot login with valid credentials.


> 
> 
>> For best performance sort the lines by ACL checking speed and how much
>> traffic they can drop. The faster it can identify and deny bad traffic
>> the more speed can go towards the good traffic.
>>
>>  2) those FB and YT websites use HTTPS and http_access controls only
>> apply when an HTTPS connection is established. The TLS connection itself
>> may remain open and continue to be used indefinitely.
>>
>> You can use the client_lifetime directive to shorten the time CONNECT
>> tunnels are allowed to remain in use. For your specific case I would set
>> it to something like 5 minutes. Browsers can auto-recover so this length
>> should not be visible to clients, but you will want to test that to
>> confirm what is good for your needs.
>>
>>
>> There are several other things about your config file that indicate
>> extremely outdated practices or Squid version. Below is a free audit
>> report of things that need fixing.
>>
>>
>> If you are running a Squid older than 3.5 please update ASAP. Then apply
>> the changes below.
> 
> I am running squid 4.6.
> 

Okay. The changes should all work, but please plan to upgrade ASAP.
There have been quite a few critical security vulnerabilities fixed this
past year.


>>>
>>> ######### LOGS #######################################
>>>
>>> cache_log /var/log/squid/cache.log
>>> access_log stdio:/var/log/squid/access.log  rotate=8760
>>> cache_store_log stdio:/var/log/squid/store.log
>>
>> Is there any reason you need this log?
>> It typically is only useful for debugging and this line could be removed
>> to speed up your proxy and save disk space.
> 
> My boss told me that I must save  all my logs, just to check later
> what the people are doing and where they are surfing on the internet.
> 

Okay. For that you need the access.log. Not the store.log or cache.log.


>>
>>>
>>> ##################
>>>
>>> #AFECTA LA CANTIDAD ESPECIFICAMENTE A STORE.LOG
>>>
>>> logfile_rotate 8760
>>>
>>> #### 365 dias * 24 horas es la cantidad de rotaciones de los logs en el crontab
>>
>> Does that mean you are running logrotate every hour of every day?
>>
> Yes, I am rotating every one hours all squid's logs. Honestly, because
> i thought that it is the more easy way to make analytics work. What
> you can recommend me??

Okay. I would double check that assumption.

Modern Squid have the logging modules for different outputs. Depending
on what analytics system you are using it may not need the rotation at
all or takes input directly somehow other than from the disk file.

Some analytics use "tail" or similar to watch the end of the access.log
and update the reports shortly after a transaction is logged. Less
rotating works better for them.

Some tools can take input from syslog. Squid has a log module to write
access.log records to syslog for those.

Then there is the daemon module. A fairly simple helper can deliver the
log lines to anywhere. For example; any unusual APIs the analytics has.

My CDN analytics dashboard and billing work off an SQL database. So I
co-wrote the helper to drop logs into a database and customers can see
their usage real-time.

Just something to think about when you have time.


HTH
Amos


From rousskov at measurement-factory.com  Fri Aug 28 20:10:36 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 28 Aug 2020 16:10:36 -0400
Subject: [squid-users] deny_info page not shown
In-Reply-To: <20200828083141.GA7917@fantomas.sk>
References: <20200828000838.2b6c089ac5b72a0920059afc@3dresearch.com>
 <a4c90341-395e-7489-f6e2-7e8350e2d973@treenet.co.nz>
 <20200828022254.e448e17cf0af48f1cc1b37b8@3dresearch.com>
 <791e9a90-b60a-9229-13f0-6a7dedf8ad09@treenet.co.nz>
 <20200828042344.ee422fccda905b9b187bd038@3dresearch.com>
 <20200828083141.GA7917@fantomas.sk>
Message-ID: <c9352b1d-7a4c-e3f9-8452-f7c5d1b95e9e@measurement-factory.com>

>> Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>> CONNECT is a request to open a TCP connection. Delivering an HTTP
>>> page, or even a URL redirect in response to a TCP connection request
>>> is completely the wrong type of result.

>>> Like asking someone to open a door because you have a load of things
>>> needing to go through it - and they instead throw a basket of apples
>>> at you. Not want you expected, and more harm than good. 


On 8/28/20 4:31 AM, Matus UHLAR - fantomas wrote:
> when you ask via HTTP for HTTP page and get HTTP answer, it is different
> than asking via HTTP for CONNECT and getting CONNECT denied via HTTP.
> 
> in the latter case it is clear that the request was denied by proxy and
> since secure content was requested, the insecure response must not be
> shown.
> 
> That's the security provided.


I believe the above explanations and analogies are rather misleading!
There are no conceptual or protocol problems with HTTP error responses
to HTTP CONNECT requests. The browser knows where the response is coming
from. The browser knows that the response is an error. The browser
already anticipates and processes some error CONNECT responses specially
(think proxy authentication). There is no confusion, harm,
inappropriateness, or some new insecurity here!

What is actually happening (AFAICT) is that browser folks do not want to
spend their resources on properly informing the user of the error. There
are ways to do it, but they all require non-trivial work in a
controversial area, and browser folks simply do not consider this
specific use case important enough to support. At the end of the day,
you are not their customer. They do not want you as their customer. You
lost.


While opinions on the underlying causes may differ, the end result is
still the same -- a forward proxy cannot display an error page to a user
behind a popular browser in a modern environment (without bumping the
browser connection first).


Cheer,

Alex.


From andreas.voneuw at axa.com  Fri Aug 28 20:29:14 2020
From: andreas.voneuw at axa.com (VON EUW Andreas)
Date: Fri, 28 Aug 2020 20:29:14 +0000
Subject: [squid-users] Squid 3.5 - icap parsing error
Message-ID: <AM6PR04MB56533F4A3507C26D4FF704DA9D520@AM6PR04MB5653.eurprd04.prod.outlook.com>

Hi all,

I'm trying to integrate a Squid Cache version 3.5.20 for x86_64-redhat-linux-gnu with a Symantec Protection Engine 8.1 to do virus scaning in a reverse proxy setup.
I do send all POST requests to our virus scan engine. But icap integration does not work as expected. Squid does send a OPTIONS request to the icap server.
We get a valid answer from Symantec Protection Engine. But squid fails afterwards with a parsing exception:

2020/08/26 10:04:54.590| 58,3| HttpMsg.cc(173) parse: HttpMsg::parse: failed to find end of headers (eof: 0) in 'ICAP/1.0 200 OK
Date: Wed Aug 26 08:04:54 2020 GMT
Methods: REQMOD
Service: Symantec Protection Engine/8.1.0.29
Service-ID: SYMCSCANREQ-AV
ISTag: "0FF01DDE4872272B6F445AED8643888C"
X-Definition-Info: 20200825.022
Max-Connections: 32
X-Allow-Out: X-Outer-Container-Is-Mime, X-Infection-Found, X-Definition-Info, X-AV-License
X-Allow-Out: X-Violations-Found
X-Allow-Out: X-SYMANTEC-URL-Definition-Info, X-CAIC-URL-Definition-Info, X-SYMANTEC-URLReputation-Definition-Info, X-URL-License, X-URL-Reputation-License
Allow: 204
Options-TTL: 3600
Preview: 4
Transfer-Preview: *
X-AV-License: 1
X-URL-License: 1
X-URL-Reputation-License: 1
'

Does somebody has an idea what's going wrong here? Is this a known squid/icap bug?

Attached: log, config and tcpdumps from icap server 1 and 2 (squid does connect thru a loadbalancer to the icap server)


IPs in the tcpdump:

Squid has IP 10.64.7.145

ICAP Server has IP: 10.140.28.144



Relevant Time in squid.log: 2020/08/26 10:04:54 (= 2020/08/26 08:04:54 icap server time)

Thanks and kind regards,
 Andy


Andreas von Euw

Java Dev Support
AXA Group Operations

andreas.voneuw at axa.com<mailto:andreas.voneuw at axa-tech.com>

Ce message est confidentiel; Son contenu ne represente en aucun cas
un engagement de la part de AXA  sous reserve de tout accord conclu
par ecrit  entre vous et  AXA.  Toute publication,  utilisation  ou 
diffusion,  meme partielle,  doit etre autorisee prealablement.  Si
vous  n'etes pas  destinataire  de ce message,  merci  d'en avertir 
immediatement l'expediteur.

This message is  confidential;  its  contents  do not  constitute a
commitment by AXA  except where provided for in a written agreement 
between you and AXA.  Any unauthorised disclosure,  use or dissemi-
nation, either whole or partial,  is prohibited. If you are not the
intended recipient of the message,  please notify  the sender imme-
diately.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200828/8178c8ea/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid-icap.conf
Type: application/octet-stream
Size: 393 bytes
Desc: squid-icap.conf
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200828/8178c8ea/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: dump.rar
Type: application/octet-stream
Size: 1786 bytes
Desc: dump.rar
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200828/8178c8ea/attachment-0001.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.log.rar
Type: application/octet-stream
Size: 7146 bytes
Desc: squid.log.rar
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200828/8178c8ea/attachment-0002.obj>

From squid3 at treenet.co.nz  Sat Aug 29 22:04:58 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Aug 2020 10:04:58 +1200
Subject: [squid-users] Squid 3.5 - icap parsing error
In-Reply-To: <AM6PR04MB56533F4A3507C26D4FF704DA9D520@AM6PR04MB5653.eurprd04.prod.outlook.com>
References: <AM6PR04MB56533F4A3507C26D4FF704DA9D520@AM6PR04MB5653.eurprd04.prod.outlook.com>
Message-ID: <189bad49-98aa-9a9b-8360-394420331ac1@treenet.co.nz>

On 29/08/20 8:29 am, VON EUW Andreas wrote:
> Hi all,
> 
> ?
> 
> I'm trying to integrate a Squid Cache version 3.5.20 for
> x86_64-redhat-linux-gnu with a Symantec Protection Engine 8.1 to do
> virus scaning in a reverse proxy setup.
> 
> I do send all POST requests to our virus scan engine. But icap
> integration does not work as expected. Squid does send a OPTIONS request
> to the icap server.
> 
> We get a valid answer from Symantec Protection Engine. But squid fails
> afterwards with a parsing exception:
> 
> ?
> 
> 2020/08/26 10:04:54.590| 58,3| HttpMsg.cc(173) parse: HttpMsg::parse:
> failed to find end of headers (eof: 0) in 'ICAP/1.0 200 OK
...
> 
> X-URL-Reputation-License: 1
> '
> 

Message headers are supposed to be ended by an empty line. As this log
entry says "failed to find end of headers" and you can see from the
buffer content displayed, there is no empty line. Which would look like:

> X-URL-Reputation-License: 1
>
> '

So Squid is waiting for the rest of the headers to arrive from the server.


Amos


From rousskov at measurement-factory.com  Sun Aug 30 01:48:31 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 29 Aug 2020 21:48:31 -0400
Subject: [squid-users] Squid 3.5 - icap parsing error
In-Reply-To: <189bad49-98aa-9a9b-8360-394420331ac1@treenet.co.nz>
References: <AM6PR04MB56533F4A3507C26D4FF704DA9D520@AM6PR04MB5653.eurprd04.prod.outlook.com>
 <189bad49-98aa-9a9b-8360-394420331ac1@treenet.co.nz>
Message-ID: <dccc62c4-3426-8e4a-bc07-1eece3b6bb3e@measurement-factory.com>

On 8/29/20 6:04 PM, Amos Jeffries wrote:
> On 29/08/20 8:29 am, VON EUW Andreas wrote:
>> Squid does send a OPTIONS request to the icap server.
>>
>> We get a valid answer from Symantec Protection Engine. 

As Amos correctly pointed out, the answer Squid gets is syntactically
invalid. The packet captures you have attached (thank you!) confirm that
the ICAP server forgets to send an empty line after the first 650 bytes
of the ICAP response header.


>> But squid fails afterwards with a parsing exception:

More accurately, the exception is thrown later than the not-yet-failing
log lines you quoted. Here is the exception line:

  2020/08/26 10:04:54.593| 93,3| ...
Adaptation::Icap::Xaction::noteCommRead threw exception: parsed || !error

 ?
> So Squid is waiting for the rest of the headers to arrive from the server.

Yes, and (3ms later) Squid reads the EOF on the ICAP connection and
(correctly) throws the above exception.

Hopefully, you get get a fixed version of the ICAP server from Symantec.


Cheers,

Alex.


From michaelspd at gmail.com  Sun Aug 30 19:51:30 2020
From: michaelspd at gmail.com (Michael Davis)
Date: Sun, 30 Aug 2020 15:51:30 -0400
Subject: [squid-users] seeking assistance for home users wanting to cache
 https contents
Message-ID: <CAO7iuacWfas1+qdtD_Ef8dzNu7E_yTO7ibBH5f0vp2Cbd8CNkA@mail.gmail.com>

okay, so I'm working on making a public github repository for others like
me out there that are having such a hard time with this, given the state of
the web being almost completely run via SSL websites, who want to use squid
for bandwidth easing in these times of everyone being stuck in home
isolation, but given that this literally constitutes making what is by
design a man in the middle attack, I am finding it more difficult than
learning to do brain surgery (I am not a surgeon).

my goal is to set up squid so that it can properly decrypt SSL traffic for
my own local devices, I own everything on this network, so this is not an
ethical problem for me given I am the sole user of everything on my own
network, and I want to PROPERLY be able to cache contents that are
otherwise delivered by SSL (nvidia graphics driver updates, Microsoft
updates (if I can do so without WSUS, verdict on this one still highly
fuzzy) web content, such as for example twitter contents, facebook
posts/videos/pictures, images on image sites like Photobucket and others)
given most web content is delivered by SSL these days, NOT having it work
using MITM setup is kind of impossible to actually cache data in this day
in age.

I have done extensive research, and even after having another member of the
pfsense community join my attempts at this, we both are at a loss on how to
correctly set up peek and splice to do the job were after here, that being
decrypting SSL traffic for local lans (yes we both know the implications,
but they are in both of our cases our own property and networks and we are
both the only people using them individually respectively) and yes we both
have also installed our local certificate authority certificates on our
devices to let it work properly, we just don't seem to understand enough on
how peek and splice is supposed to work, to implement it properly, and
thus, the guy I'm working with on this suggested we reach out to this
mailing list and ask those here that understand it more than we do.

so, could we kindly request some assistance in understanding this and how
to implement it please? I will admit, the guy I'm working with understands
this far better than I do myself, however I figured I'd reach out on my end
given that I'm the guy that's publishing the information into the public
github repo I made for this.

I am NOT the most knowledgeable on networking, I will straight up admit
that, I learn by trial and error and am almost completely self taught on
what I know, so please bear with me if it takes me a little bit to
understand a given term or other item if I'm a little bit slow to grasp it.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200830/36fb0922/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 31 11:04:03 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 31 Aug 2020 23:04:03 +1200
Subject: [squid-users] seeking assistance for home users wanting to
 cache https contents
In-Reply-To: <CAO7iuacWfas1+qdtD_Ef8dzNu7E_yTO7ibBH5f0vp2Cbd8CNkA@mail.gmail.com>
References: <CAO7iuacWfas1+qdtD_Ef8dzNu7E_yTO7ibBH5f0vp2Cbd8CNkA@mail.gmail.com>
Message-ID: <45ddd73d-cb27-13ba-08c5-1feff524a4e0@treenet.co.nz>

On 31/08/20 7:51 am, Michael Davis wrote:
> okay, so I'm working on making a public github repository for others
> like me out there that are having such a hard time with this, given the
> state of the web being almost completely?run via SSL websites, who want
> to use squid for bandwidth easing in these times of everyone being stuck
> in home isolation, but given that this literally constitutes making what
> is by design a man in the middle attack, I am finding it more difficult
> than learning to do brain surgery (I am not a surgeon).

FYI, a github repository is not always the right answer. Everyones needs
are slightly different, so what we have in the Squid Project is a wiki
of examples with enough explanation that people should be able to make
the small changes necessary for their needs.

 <https://wiki.squid-cache.org/ConfigExamples/>

I have updated
<https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA>
to clarify the squid.conf port lines you need.

<https://wiki.squid-cache.org/Features/SslPeekAndSplice> documents the
ssl_bump access controls.


> 
> my goal is to set up squid so that it can properly decrypt SSL traffic
> for my own local devices, I own everything on this network, so this is
> not an ethical problem for me given I am the sole user of everything on
> my own network, and I want to PROPERLY be able to cache contents that
> are otherwise delivered by SSL (nvidia graphics driver updates,

FYI, "proper" caching has nothing to do with SSL-Bump.

Separate any changes you want in relation to caching from the SSL-Bump
changes. Test each set of changes independently to get one feature going
before you move on to the other.


> 
> I have done extensive research, and even after having another member of

Unfortunately TLS is one topic where things have been very volatile. So
the more research you do may expose you to outdated and/or irrelevant
details that just add confusion.

If you have two confusing sources of information (including archived
mailing list replies) go with the official wiki page as authoritative.
Or ask here, that is what this mailing list is for.


> the pfsense community join my attempts at this, we both are at a loss on

FYI, pfsense should not be relevant to SSL-Bump. Like caching the two
features can be used, but are not directly related to each other. So
setup, test and get each working separately.


> how to correctly set up peek and splice to do the job were after here,
> that being decrypting SSL traffic for local lans (yes we both know the
> implications, but they are in both of our cases our own property and
> networks and we are both the only people using them individually
> respectively) and yes we both have also installed our local certificate
> authority certificates on our devices to let it work properly, we just
> don't seem to understand enough on how peek and splice is supposed to
> work, to implement it properly, and thus, the guy I'm working with on
> this suggested we reach out to this mailing list and ask those here that
> understand it more than we do.
> 

So what I am understanding from your description is that you are trying to:
 A) intercept traffic with pfsense
 B) SSL-Bump the TLS which arrives at the proxy
 C) cache the decrypted HTTP messages

Is that correct?


The pfsense portion I cannot help much with right now, it has been too
long since I worked with that software.

All I can say is:

 1) the config examples we have in the wiki for setting up the
interception part should work fine, and

 2) test it *without* worrying about SSL-Bump or caching. Make sure it
works before going on to the other features, and

 3) "working" for the port 443 intercept (no bumping) can take the form
of HTTP error messages from Squid or client rejecting TLS handshake
details from Squid. Both of these mean the traffic is reaching Squid and
client getting whatever Squid produces.


For SSL-Bump when (after pfsense intercept is working) you want to
follow
<https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA>
to get a successful TLS handshake happening.

You can use these bare-bones ssl_bump settings to pass the traffic
through Squid without decrypt to start with:

  acl step1 at_step SslBump1
  ssl_bump peek step1
  ssl_bump splice all


Amos


From anon.amish at gmail.com  Mon Aug 31 13:18:46 2020
From: anon.amish at gmail.com (Amish)
Date: Mon, 31 Aug 2020 18:48:46 +0530
Subject: [squid-users] Does GREASE bug affect squid 4.10 too?
Message-ID: <e2ed12f9-be17-565f-68a6-0d3b8da3a6e0@gmail.com>

Hello,

Recently there has been reports of GREASE bug wrt Google chrome and Squid.

I was under belief that it did not affect squid version 4.10 or below. 
(I am using squid 4.10 on server - OS is Arch Linux.)

But today since morning users have been complaining about sites not 
opening. The server is heavily loaded so it is not easy for me to debug 
as there is continuous stream of logs. Plus its work from home which 
slows things down for me.

After spending lots of time in finding the issue, I disabled SSL bump 
for all sites (added .* to acl file to not monitor any site)

And sites started working again.

I believe this issue was specific to those who used Google chrome but I 
am not 100% sure.

So can someone please confirm if bug can affect squid version 4.10 or 
not? And if I upgrade to squid 4.13 will it definitely solve the GREASE 
issue.

Or can it be that Google chrome has again come up with something new 
which is breaking SSL again?

Thank you in advance,

Amish.



From squid3 at treenet.co.nz  Mon Aug 31 13:35:42 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2020 01:35:42 +1200
Subject: [squid-users] Does GREASE bug affect squid 4.10 too?
In-Reply-To: <e2ed12f9-be17-565f-68a6-0d3b8da3a6e0@gmail.com>
References: <e2ed12f9-be17-565f-68a6-0d3b8da3a6e0@gmail.com>
Message-ID: <f86654d9-16d4-d528-e1f0-f5e5817b1cab@treenet.co.nz>

On 1/09/20 1:18 am, Amish wrote:
> Hello,
> 
> Recently there has been reports of GREASE bug wrt Google chrome and Squid.
> 
...>
> So can someone please confirm if bug can affect squid version 4.10 or
> not? And if I upgrade to squid 4.13 will it definitely solve the GREASE
> issue.

4.13 will definitely solve the GREASE issue. I cannot confirm whether
that is affecting 4.10 or older since those versions pass the values to
libssl for interpretation - which may or may not have handled GREASE.

You need to upgrade regardless of whether this solves the issue. The CVE
issues fixed in that release are critical.

If the issues continue or you discover anything more specific about it
please let us know. :)


> 
> Or can it be that Google chrome has again come up with something new
> which is breaking SSL again?
> 

That is entirely possible as well.

HTH
Amos


From anon.amish at gmail.com  Mon Aug 31 13:59:11 2020
From: anon.amish at gmail.com (Amish)
Date: Mon, 31 Aug 2020 19:29:11 +0530
Subject: [squid-users] Does GREASE bug affect squid 4.10 too?
In-Reply-To: <f86654d9-16d4-d528-e1f0-f5e5817b1cab@treenet.co.nz>
References: <e2ed12f9-be17-565f-68a6-0d3b8da3a6e0@gmail.com>
 <f86654d9-16d4-d528-e1f0-f5e5817b1cab@treenet.co.nz>
Message-ID: <6a7821d9-481a-eb85-e1b0-6cd48f61d67e@gmail.com>


On 31/08/20 7:05 pm, Amos Jeffries wrote:
> On 1/09/20 1:18 am, Amish wrote:
>> Hello,
>>
>> Recently there has been reports of GREASE bug wrt Google chrome and Squid.
>>
> ...>
>> So can someone please confirm if bug can affect squid version 4.10 or
>> not? And if I upgrade to squid 4.13 will it definitely solve the GREASE
>> issue.
> 4.13 will definitely solve the GREASE issue. I cannot confirm whether
> that is affecting 4.10 or older since those versions pass the values to
> libssl for interpretation - which may or may not have handled GREASE.
>
> You need to upgrade regardless of whether this solves the issue. The CVE
> issues fixed in that release are critical.
>
> If the issues continue or you discover anything more specific about it
> please let us know. :)

Thank you very much for a super quick reply. I will definitely update to 
4.13 tomorrow and report back if there is any issue (I hope there is 
none. Do not want one more restless day!! Phew!)

You (Amos) and Alex always amaze me on how you manage replying to 
queries on the list and also do coding with good documentation too. At 
the same time appreciate other developers too.

Thank you again and best regards,

Amish



From m_zouhairy at skno.by  Mon Aug 31 08:24:55 2020
From: m_zouhairy at skno.by (Vacheslav)
Date: Mon, 31 Aug 2020 11:24:55 +0300
Subject: [squid-users] limit bandwidth
Message-ID: <3d7ba9d6-aacb-8907-3440-31c1bd709615@skno.by>

Peace,

been suffering for many hours so i'd rather ask for aid..

i'm trying to limit the flow mainly for the most maximize people


acl slower ??? ?src 10.46.0.74 10.46.0.107
acl localnet src 0.0.0.1-0.255.255.255??? # RFC 1122 "this" network (LAN)
acl localnet src 10.46.0.0/24??? ??? #? local private network (LAN)


acl SSL_ports port 443
acl Safe_ports port 80??? ??? # http
acl Safe_ports port 8080??? # http
acl Safe_ports port 21??? ??? # ftp
acl Safe_ports port 443??? ??? # https
acl Safe_ports port 70??? ??? # gopher
acl Safe_ports port 210??? ??? # wais
acl Safe_ports port 1025-65535??? # unregistered ports
acl Safe_ports port 280??? ??? # http-mgmt
acl Safe_ports port 488??? ??? # gss-http
acl Safe_ports port 591??? ??? # filemaker
acl Safe_ports port 777??? ??? # multiling http
acl CONNECT method CONNECT
acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost
visible_hostname proxy.k

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
error_directory /usr/share/squid/errors/en
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed


delay_pools 1
delay_class 1 3
delay_access 1 allow slower !localnet
delay_access 1 deny all
delay_parameters 1 12800/12800 -1/-1 6400/12800


http_access allow localnet
http_access allow localhost

i tried doing the delay class 1 1

but bandwidth is full

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200831/9ef2d442/attachment.htm>

From squid3 at treenet.co.nz  Mon Aug 31 17:10:38 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Sep 2020 05:10:38 +1200
Subject: [squid-users] limit bandwidth
In-Reply-To: <3d7ba9d6-aacb-8907-3440-31c1bd709615@skno.by>
References: <3d7ba9d6-aacb-8907-3440-31c1bd709615@skno.by>
Message-ID: <7e727b46-dace-eb5f-17ec-dfa705c77ff8@treenet.co.nz>

On 31/08/20 8:24 pm, Vacheslav wrote:
> Peace,
> 
> been suffering for many hours so i'd rather ask for aid..
> 
> i'm trying to limit the flow mainly for the most maximize people
> 

Okay.

What Squid version are you using?


> 
> acl slower src 10.46.0.74 10.46.0.107

One of the reasons this posting git held up for moderation was that the
lines which are supposed to contain ASCII tab characters contained
Unicode characters "\c3\82".

If those Unicode characters are actually present in your squid.conf file
then you need to go through and remove them all.

...
> acl localnet src 10.46.0.0/24		#  local private network (LAN)

...
> acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"
> 
...

> error_directory /usr/share/squid/errors/en

The above is a default value. Remove that line from your config.

> 
> delay_pools 1
> delay_class 1 3
> delay_access 1 allow slower !localnet


All IPs which match "slower" ACL are also matched by "localnet" ACL.

It is impossible for an IP to be both part of slower and not part of
localnet. So this line never matches and all traffic is not-delayed.

To fix, remove the "!localnet" requirement from the above line.


Amos


