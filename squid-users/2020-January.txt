From squid3 at treenet.co.nz  Wed Jan  1 14:19:01 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Jan 2020 03:19:01 +1300
Subject: [squid-users] Switched ISP
In-Reply-To: <ee1bddb5-d8e7-3b5d-c09b-2b9bdf23f988@measurement-factory.com>
References: <1158f876-6043-ed00-b8d2-c19a178364bd@edm-inc.com>
 <ee1bddb5-d8e7-3b5d-c09b-2b9bdf23f988@measurement-factory.com>
Message-ID: <bef671aa-c038-2188-29a2-5683315b29dc@treenet.co.nz>

On 31/12/19 11:51 am, Alex Rousskov wrote:
> On 12/30/19 4:08 PM, Robert A Wooldridge wrote:
>> We recently switched to ATT from Windstream.? After the switch there is
>> a very large difference between accessing sites through squid compared
>> to bypassing squid (squid is slower).? However it is not completely
>> uniform.? Some sites are extremely slow and do not load at all.
>> news.yahoo.com is an example.? Is there anything I should be aware of
>> when switching to a different ISP?? Would the lack of a PTR record on
>> the proxy server have any effect on this? Should I clear squid's cache?
> 
> 0. A lack of PTR record will slow down origin servers that try to
> resolve your proxy IP address and (slowly?) fail. I do not know how
> typical such sites are.
> 
> 1. Are the problems related to sites that have IPv6 addresses (even
> though access.log may show IPv4 transactions)? YMMV, but some ISP
> networks enable IPv6 while not supporting it well, leading to random
> IPv6 connection establishment timeouts.
> 
> 2. Are you using AT&T's DNS resolver? YMMV, but some ISP DNS resolvers
> overwrite NXDOMAIN responses. Such overwrites might have unexpected side
> effects on complex sites.
> 

Of Googles 8.8.8.8 resolver? it produces different types of result based
on where one queries from.

There is also the possibility of route engineering by the two ISP being
different for some destination networks. That could changes some traffic
in ways that are visible on inspection.

Or also on the network layer side of things oddities in the particular
routers each ISP uses may result in differences in MTU, ECN etc handling
that also affects some traffic routes in different ways for each of the
two ISP.

Amos


From robertocarna36 at gmail.com  Wed Jan  1 21:14:40 2020
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Wed, 1 Jan 2020 18:14:40 -0300
Subject: [squid-users] Exclude dstdomain in access.log file
Message-ID: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>

Hi people, I have Debian 9 + Squid 3.5.23.

I'm using squidguard to filter domains and URL's, so in
/etc/squid/squid.conf I have:

url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

I must exclude "hangouts.google.com" domain in /var/log/squid/access.log
file.

So firstly I edited in my /etc/squid/squid.conf file:

acl exclude dstdomain hangouts.google.com
access_log none exclude
access_log /var/log/squid/access.log squid

But it didn't work, when I executed "tail -f /var/log/squid/access.log" I
could see logs from hangouts.google.com.

After that I edited again my /etc/squid/squid.conf file:

acl exclude dstdomain hangouts.google.com
access_log /var/log/squid/access.log squid !exclude

But it didn't work again.

Please can you tell me what I can do in order to deny logs from
hangouts.google.com ???

Thanks a lot, greetings !!!

Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200101/210c1dfb/attachment.htm>

From m_zouhairy at skno.by  Thu Jan  2 05:16:27 2020
From: m_zouhairy at skno.by (Vacheslav)
Date: Thu, 02 Jan 2020 08:16:27 +0300
Subject: [squid-users] Exclude dstdomain in access.log file
In-Reply-To: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
References: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
Message-ID: <d3c2b21a3f6848243807c71939e0f15c30899127.camel@skno.by>

well ufdbguard is better, it's about time to upgrade..
On Wed, 2020-01-01 at 18:14 -0300, Roberto Carna wrote:
> Hi people, I have Debian 9 + Squid 3.5.23.
> 
> I'm using squidguard to filter domains and URL's, so in
> /etc/squid/squid.conf I have:
> 
> url_rewrite_program /usr/bin/squidGuard -c
> /etc/squidguard/squidGuard.conf
> 
> I must exclude "hangouts.google.com" domain in
> /var/log/squid/access.log file.
> 
> So firstly I edited in my /etc/squid/squid.conf file:
> 
> acl exclude dstdomain hangouts.google.com
> access_log none exclude
> access_log /var/log/squid/access.log squid
> 
> But it didn't work, when I executed "tail -f
> /var/log/squid/access.log" I could see logs from hangouts.google.com.
> 
> After that I edited again my /etc/squid/squid.conf file:
> 
> acl exclude dstdomain hangouts.google.com
> access_log /var/log/squid/access.log squid !exclude
> 
> But it didn't work again.
> 
> Please can you tell me what I can do in order to deny logs from
> hangouts.google.com ???
> 
> Thanks a lot, greetings !!!
> 
> Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200102/ab16dfdc/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan  2 05:55:26 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Jan 2020 18:55:26 +1300
Subject: [squid-users] Exclude dstdomain in access.log file
In-Reply-To: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
References: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
Message-ID: <fffbbde4-cd22-a215-5651-d8ac5f1b3ede@treenet.co.nz>

On 2/01/20 10:14 am, Roberto Carna wrote:
> 
> Please can you tell me what I can do in order to deny logs from
> hangouts.google.com <http://hangouts.google.com> ???

Which of the several FQDN entries in the logs are you seeing it appear?

dstdomain only relates to the URL domain.


Amos


From robertocarna36 at gmail.com  Thu Jan  2 18:09:43 2020
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Thu, 2 Jan 2020 15:09:43 -0300
Subject: [squid-users] Exclude dstdomain in access.log file
In-Reply-To: <fffbbde4-cd22-a215-5651-d8ac5f1b3ede@treenet.co.nz>
References: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
 <fffbbde4-cd22-a215-5651-d8ac5f1b3ede@treenet.co.nz>
Message-ID: <CAG2Qp6vT5aNP0PYRUaGbKdLKXT8jhmw0v5bbQp--e9o5wqajaA@mail.gmail.com>

Dear Amos, I have this log entries that I want to disable:

1577988384.248      0 10.88.1.112 TAG_NONE/503 0 CONNECT
hangouts.google.com:443 fchop HIER_NONE/- -
1577988384.435      0 10.88.1.31 TAG_NONE/503 0 CONNECT
hangouts.google.com:443 ccardiff HIER_NONE/- -
1577988384.659      0 10.88.1.26 TAG_NONE/503 0 CONNECT
hangouts.google.com:443 mtowers HIER_NONE/- -
1577988385.069      3 10.88.1.13 TAG_NONE/503 0 CONNECT
hangouts.google.com:443 pmonkey HIER_NONE/- -

Is it possible doing that using "dstdomain" ???

Or maybe squidguard doesn't let do it ?

Thanks a lot again !!!

El jue., 2 ene. 2020 a las 2:55, Amos Jeffries (<squid3 at treenet.co.nz>)
escribi?:

> On 2/01/20 10:14 am, Roberto Carna wrote:
> >
> > Please can you tell me what I can do in order to deny logs from
> > hangouts.google.com <http://hangouts.google.com> ???
>
> Which of the several FQDN entries in the logs are you seeing it appear?
>
> dstdomain only relates to the URL domain.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200102/e36ca1e8/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan  3 06:14:53 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Jan 2020 19:14:53 +1300
Subject: [squid-users] Exclude dstdomain in access.log file
In-Reply-To: <CAG2Qp6vT5aNP0PYRUaGbKdLKXT8jhmw0v5bbQp--e9o5wqajaA@mail.gmail.com>
References: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
 <fffbbde4-cd22-a215-5651-d8ac5f1b3ede@treenet.co.nz>
 <CAG2Qp6vT5aNP0PYRUaGbKdLKXT8jhmw0v5bbQp--e9o5wqajaA@mail.gmail.com>
Message-ID: <b3fe2467-80ba-4b99-d27b-f9fa1922f3a9@treenet.co.nz>

On 3/01/20 7:09 am, Roberto Carna wrote:
> Dear Amos, I have this log entries that I want to disable:
> 
> 1577988384.248 ? ? ?0 10.88.1.112 TAG_NONE/503 0 CONNECT
> hangouts.google.com:443 <http://hangouts.google.com:443> fchop HIER_NONE/- -
> 1577988384.435 ? ? ?0 10.88.1.31 TAG_NONE/503 0 CONNECT
> hangouts.google.com:443 <http://hangouts.google.com:443> ccardiff
> HIER_NONE/- -
> 1577988384.659 ? ? ?0 10.88.1.26 TAG_NONE/503 0 CONNECT
> hangouts.google.com:443 <http://hangouts.google.com:443> mtowers
> HIER_NONE/- -
> 1577988385.069 ? ? ?3 10.88.1.13 TAG_NONE/503 0 CONNECT
> hangouts.google.com:443 <http://hangouts.google.com:443> pmonkey
> HIER_NONE/- -
> 
> Is it possible doing that using "dstdomain" ???

It should be, but yes you have hit a bug. It may be fixed in the latest
Squid version, but I am not certain of that.

> 
> Or maybe squidguard doesn't let do it ?
> 

squidguard does not have anything to do with the logging issue.

It likely does have something to do with those being 503 though. It is
responsible for telling Squid what URL to send the upstream server - and
a CONNECT tunnel has no such URL, squidguard cannot handle that.


Amos


From robertocarna36 at gmail.com  Fri Jan  3 12:53:16 2020
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Fri, 3 Jan 2020 09:53:16 -0300
Subject: [squid-users] Exclude dstdomain in access.log file
In-Reply-To: <b3fe2467-80ba-4b99-d27b-f9fa1922f3a9@treenet.co.nz>
References: <CAG2Qp6vBkoavm6RG=4g-+uKq9cqyT+xeuXexrB6t5SLpeUfO7g@mail.gmail.com>
 <fffbbde4-cd22-a215-5651-d8ac5f1b3ede@treenet.co.nz>
 <CAG2Qp6vT5aNP0PYRUaGbKdLKXT8jhmw0v5bbQp--e9o5wqajaA@mail.gmail.com>
 <b3fe2467-80ba-4b99-d27b-f9fa1922f3a9@treenet.co.nz>
Message-ID: <CAG2Qp6vy=pXDzet21rf2pVWxQZtnmq6aNgYawtyD4UeGJ-B4bQ@mail.gmail.com>

Ok Amos, thanks a lot for your help.

I'll try to update my Squid version i order to block the given domains in
my access.log file.

Greetings !!!

El vie., 3 ene. 2020 a las 3:15, Amos Jeffries (<squid3 at treenet.co.nz>)
escribi?:

> On 3/01/20 7:09 am, Roberto Carna wrote:
> > Dear Amos, I have this log entries that I want to disable:
> >
> > 1577988384.248      0 10.88.1.112 TAG_NONE/503 0 CONNECT
> > hangouts.google.com:443 <http://hangouts.google.com:443> fchop
> HIER_NONE/- -
> > 1577988384.435      0 10.88.1.31 TAG_NONE/503 0 CONNECT
> > hangouts.google.com:443 <http://hangouts.google.com:443> ccardiff
> > HIER_NONE/- -
> > 1577988384.659      0 10.88.1.26 TAG_NONE/503 0 CONNECT
> > hangouts.google.com:443 <http://hangouts.google.com:443> mtowers
> > HIER_NONE/- -
> > 1577988385.069      3 10.88.1.13 TAG_NONE/503 0 CONNECT
> > hangouts.google.com:443 <http://hangouts.google.com:443> pmonkey
> > HIER_NONE/- -
> >
> > Is it possible doing that using "dstdomain" ???
>
> It should be, but yes you have hit a bug. It may be fixed in the latest
> Squid version, but I am not certain of that.
>
> >
> > Or maybe squidguard doesn't let do it ?
> >
>
> squidguard does not have anything to do with the logging issue.
>
> It likely does have something to do with those being 503 though. It is
> responsible for telling Squid what URL to send the upstream server - and
> a CONNECT tunnel has no such URL, squidguard cannot handle that.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200103/549bcc23/attachment.htm>

From yaroslav.pushko at globallogic.com  Fri Jan  3 13:40:52 2020
From: yaroslav.pushko at globallogic.com (Yaroslav Pushko)
Date: Fri, 3 Jan 2020 15:40:52 +0200
Subject: [squid-users] Fwd: Squid 4.8 with OpenSSL 1.1.1d
In-Reply-To: <fc72bea5-0f98-864b-57f2-ae4d8cc9ae92@measurement-factory.com>
References: <CADSgaCTS3xsUY0EBZvJY+fk2_OBqUVNBO1gMDwyxJpYYPH7_Ng@mail.gmail.com>
 <CADSgaCQtQAetQCukyQfR5c6Bq8B22_k76Ff3Y8OONsqm1juiPg@mail.gmail.com>
 <fc72bea5-0f98-864b-57f2-ae4d8cc9ae92@measurement-factory.com>
Message-ID: <CADSgaCSEgoPUorknDxw-CPxVbJ7LfWsS-bTg79L+U-qweuFbzg@mail.gmail.com>

Hi Alex,

Thank you for the reply, we update our patch with provided changes.

One more thing, with TLSv1.3.

There is site https://3frontoffice.tre.se/login with specific behavior in
the Chrome browser OS X El Capitan.

During establishing TLSv1.3 handshake after successfully send our Client
Hello, the server answers us with Hello Retry Request.

In Squid this behavior interprets next Client Hello peek successfully and
Hello Retry Request peeks as Server Hello.
After that, we splice it and send to OpenSSL and fail handshake
establishing.

Did you already notice such behavior?
Did you have some investigation on this issue, any advice will be pleasant?

Best regards,
Yaroslav.


On Tue, Dec 17, 2019 at 4:39 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 12/17/19 9:00 AM, Yaroslav Pushko wrote:
> > Hi All
> >
> > We use Squid 4.8 with OpenSSL 1.1.1d in a transparent mode for peek and
> > splice interception.
> >
> > With this version, we lost the possibility to connect to any HTTPS site.
> >
> > There are a few issues:
> >
> >   * support TLSv1.2 sites (already discussed in
> >     thread
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-with-ssl-choose-client-version-inappropriate-fallback-on-some-sites-when-using-TLS1-2-td4688258.html
>  )
> >   * support TLSv1.3 sites.
>
> Please see
>
> http://lists.squid-cache.org/pipermail/squid-users/2019-December/021435.html
> for several alternative fixes. AFAICT, those fixes are more flexible
> and, after polishing, appropriate for the official inclusion because
> they make fewer assumptions about the values sent via the supported
> versions extension.
>
> It is possible that your SSL_MODE_SEND_FALLBACK_SCSV change needs to be
> integrated with the other fixes. Thank you for sharing that idea!
>
> Alex.
>
>
> > Support TLSv1.2.
> >
> >     OpenSSL 1.1.1d adds support of TLSv1.3. These changes added some
> >     kind of guard if we perform a handshake with a lower version of the
> >     TLS protocol than we support. In this scenario, we receive downgrade
> >     fallback error.
> >     Handshake version TLSv1.2 vs. max support TLSv1.3.
> >
> >     In such case, we have the next error:
> >
> >         ERROR: negotiating TLS on FD 19: error:1425F175:SSL
> >         routines:ssl_choose_client_version:inappropriate fallback
> (1/-1/0)
> >
> >
> >     OpenSSL already provided a fix for it. You can configure SSL session
> >     to use option SSL_MODE_SEND_FALLBACK_SCSV and setting SSL max proto
> >     version for current SSL session, but squid not yet supported these
> >     features.
> >
> >     You can find a patch in the attachments, will be grateful for the
> >     review.
> >
> >
> > The issue with TLS 1.3 support, we are still investigating, any advice
> > will be pleasant.
> >
> > Best regards,
> > Yaroslav Pushko.
> > --
> > Best Regards,
> > Yaroslav Pushko | Senior *Software Engineer*
> > GlobalLogic
> > P +380971842774  M +380634232226 S dithard
> > www.globallogic.com <http://www.globallogic.com/>
> > http://www.globallogic.com/email_disclaimer.txt
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>

-- 
Best Regards,
Yaroslav Pushko | Senior *Software Engineer*
GlobalLogic
P +380971842774  M +380634232226 S dithard
www.globallogic.com
<http://www.globallogic.com/>
http://www.globallogic.com/email_disclaimer.txt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200103/887e7e76/attachment.htm>

From andrei.pozolotin at gmail.com  Fri Jan  3 18:14:29 2020
From: andrei.pozolotin at gmail.com (Andrei Pozolotin)
Date: Fri, 03 Jan 2020 12:14:29 -0600
Subject: [squid-users] Question: Force the caching of 302 responses without
 Expires header and with Strict-Transport-Security max-age header?
Message-ID: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>

Hello. 

1. this question was asked before, but not yet resolved: 

http://www.squid-cache.org/mail-archive/squid-users/200701/0000.html 

2. use case: 

the following url goes though double redirect, both times not providing
"Expires:" header, 

which results in repeated TCP_MISS/302 entries in the squid logs: 

2020-Jan-03 17:45:14    125 192.168.1.106 TCP_MISS/302 565 GET
https://archive.archlinux.org/repos/2020/01/01/community/os/x86_64/python-wheel-0.33.6-3-any.pkg.tar.xz
- HIER_DIRECT/88.198.91.70 text/html                                   

2020-Jan-03 17:45:14     82 192.168.1.106 TCP_MISS/302 461 GET
https://archive.org/download/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz
- HIER_DIRECT/207.241.224.2 text/html                                   
         

2020-Jan-03 17:45:14    215 192.168.1.106 NONE/200 0 CONNECT
ia803100.us.archive.org:443 - HIER_DIRECT/207.241.232.150 -   

2020-Jan-03 17:45:14      1 192.168.1.106 TCP_HIT/200 38605 GET
https://ia803100.us.archive.org/6/items/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz
- HIER_NONE/- application/octet-stream                               

3. here are response details via curl: 

a) 

curl --head
https://archive.archlinux.org/repos/2020/01/01/community/os/x86_64/python-wheel-0.33.6-3-any.pkg.tar.xz

HTTP/2 302  
server: nginx/1.16.1 
date: Fri, 03 Jan 2020 17:56:14 GMT 
content-type: text/html 
content-length: 145 
location:
https://archive.org/download/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz

strict-transport-security: max-age=31536000; includeSubdomains; preload 

b) 

curl --head
https://archive.org/download/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz

HTTP/1.1 302 Found 
Server: nginx/1.14.0 (Ubuntu) 
Date: Fri, 03 Jan 2020 17:56:42 GMT 
Content-Type: text/html; charset=UTF-8 
Connection: keep-alive 
Accept-Ranges: bytes 
Location:
https://ia803100.us.archive.org/6/items/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz

Strict-Transport-Security: max-age=15724800

4. it seems that Strict-Transport-Security: max-age header is ignored
here by squid  

5. any attempt to use any of the refresh_pattern options also has no
effect: 

http://www.squid-cache.org/Doc/config/refresh_pattern/ 

6. full squid.conf is posted here: 

https://github.com/random-python/nspawn/blob/master/src/main/nspawn/app/hatcher/service/image-proxy/etc/squid/squid.conf


Question: how can one force the caching of 302 responses 

without the Expires header and with Strict-Transport-Security max-age
header? 

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200103/b4e2f359/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan  3 20:19:30 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Jan 2020 13:19:30 -0700
Subject: [squid-users] Question: Force the caching of 302 responses
 without Expires header and with Strict-Transport-Security max-age header?
In-Reply-To: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>
References: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>
Message-ID: <631b8834-2aba-0779-b165-91390ba54d79@measurement-factory.com>

On 1/3/20 11:14, Andrei Pozolotin wrote:
> 3. here are response details via curl:
> 
> a)
> 
> curl --head 
> https://archive.archlinux.org/repos/2020/01/01/community/os/x86_64/python-wheel-0.33.6-3-any.pkg.tar.xz
> 
> HTTP/2 302
> server: nginx/1.16.1
> date: Fri, 03 Jan 2020 17:56:14 GMT
> content-type: text/html
> content-length: 145
> location: 
> https://archive.org/download/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz 
> 
> strict-transport-security: max-age=31536000; includeSubdomains; preload
> 
> b)
> 
> curl --head 
> https://archive.org/download/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz
> 
> HTTP/1.1 302 Found
> Server: nginx/1.14.0 (Ubuntu)
> Date: Fri, 03 Jan 2020 17:56:42 GMT
> Content-Type: text/html; charset=UTF-8
> Connection: keep-alive
> Accept-Ranges: bytes
> Location: 
> https://ia803100.us.archive.org/6/items/archlinux_pkg_python-wheel/python-wheel-0.33.6-3-any.pkg.tar.xz 
> 
> Strict-Transport-Security: max-age=15724800
> 
> 4. it seems that?Strict-Transport-Security: max-age header is ignored 
> here by squid


Correct. Squid does not know anything about the 
Strict-Transport-Security header. The header is treated like an 
extension header (i.e. it is usually forwarded without interpreting its 
value).


> 5. any attempt to use any of the refresh_pattern options also has no effect:
> 
> http://www.squid-cache.org/Doc/config/refresh_pattern/

Yes, the decision to avoid caching of 302 responses without Expires is 
hard-coded. It is made before refresh_pattern is consulted AFAICT.


> Question: how can one force the caching of 302 responses
> without the Expires header and with Strict-Transport-Security?max-age 
> header?


You can modify Squid to handle Strict-Transport-Security specially or 
you can write an ICAP or eCAP service that would add a "more standard" 
Cache-Control:max-age header to the response (with even more work, it 
would be possible to drop the added response header before it leaves Squid).


HTH,

Alex.


From robertkwild at gmail.com  Sat Jan  4 01:13:49 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Sat, 4 Jan 2020 01:13:49 +0000
Subject: [squid-users] showing clients ip on whos tried to download virus on
	squid 4
Message-ID: <CAGU_CiLGQP6R+TN+QuRYmxHMuAmzs-wY6yDBPXMxAew3hsNE2g@mail.gmail.com>

 hi all,

this is my config at the end of squid.conf

#ICAP
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=0 icap://
127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=0 icap://
127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all

it works great but when i go on the c-icap logs its just specifies this

Sat Jan  4 00:50:36 2020, 1610/782518016, squidclamav.c(908)
squidclamav_end_of_data_handler: Sat Jan  4 00:50:36 2020, 1610/782518016,
LOG Virus found, sending redirection header / error page.
Sat Jan  4 00:50:39 2020, 1610/782518016, squidclamav.c(866)
squidclamav_end_of_data_handler: Sat Jan  4 00:50:39 2020, 1610/782518016,
LOG Virus found in https://secure.eicar.org/eicar_com.zip ending download
[stream: Eicar-
Test-Signature FOUND]

how come the clients ip is not showing up?

thanks,
rob


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200104/f1b8f558/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan  4 01:41:10 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Jan 2020 14:41:10 +1300
Subject: [squid-users] showing clients ip on whos tried to download
 virus on squid 4
In-Reply-To: <CAGU_CiLGQP6R+TN+QuRYmxHMuAmzs-wY6yDBPXMxAew3hsNE2g@mail.gmail.com>
References: <CAGU_CiLGQP6R+TN+QuRYmxHMuAmzs-wY6yDBPXMxAew3hsNE2g@mail.gmail.com>
Message-ID: <6209e197-5edc-19be-c6ca-fb365c4462c4@treenet.co.nz>

On 4/01/20 2:13 pm, robert k Wild wrote:
> 
> it works great but when i go on the c-icap logs its just specifies this
> 
> Sat Jan ?4 00:50:36 2020, 1610/782518016, squidclamav.c(908)
> squidclamav_end_of_data_handler: Sat Jan ?4 00:50:36 2020,
> 1610/782518016, LOG Virus found, sending redirection header / error page.
> Sat Jan ?4 00:50:39 2020, 1610/782518016, squidclamav.c(866)
> squidclamav_end_of_data_handler: Sat Jan ?4 00:50:39 2020,
> 1610/782518016, LOG Virus found in
> https://secure.eicar.org/eicar_com.zip ending download [stream: Eicar-
> Test-Signature FOUND]
> 
> how come the clients ip is not showing up?

Those are ICAP service issues. Nothing to do with Squid.

For this type of question you will have to contact the squidclamav or
c-icap developers or community.

Amos


From robertkwild at gmail.com  Sat Jan  4 01:43:00 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Sat, 4 Jan 2020 01:43:00 +0000
Subject: [squid-users] showing clients ip on whos tried to download
 virus on squid 4
In-Reply-To: <6209e197-5edc-19be-c6ca-fb365c4462c4@treenet.co.nz>
References: <CAGU_CiLGQP6R+TN+QuRYmxHMuAmzs-wY6yDBPXMxAew3hsNE2g@mail.gmail.com>
 <6209e197-5edc-19be-c6ca-fb365c4462c4@treenet.co.nz>
Message-ID: <CAGU_CiLhLp8iqdUxW1tAC3yTASe-ggAF+C6_iUaX0nyKZ5H1dw@mail.gmail.com>

Thanks Amos,

I will contact c icap users in future with any cicap config lines

On Sat, 4 Jan 2020, 01:41 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 4/01/20 2:13 pm, robert k Wild wrote:
> >
> > it works great but when i go on the c-icap logs its just specifies this
> >
> > Sat Jan  4 00:50:36 2020, 1610/782518016, squidclamav.c(908)
> > squidclamav_end_of_data_handler: Sat Jan  4 00:50:36 2020,
> > 1610/782518016, LOG Virus found, sending redirection header / error page.
> > Sat Jan  4 00:50:39 2020, 1610/782518016, squidclamav.c(866)
> > squidclamav_end_of_data_handler: Sat Jan  4 00:50:39 2020,
> > 1610/782518016, LOG Virus found in
> > https://secure.eicar.org/eicar_com.zip ending download [stream: Eicar-
> > Test-Signature FOUND]
> >
> > how come the clients ip is not showing up?
>
> Those are ICAP service issues. Nothing to do with Squid.
>
> For this type of question you will have to contact the squidclamav or
> c-icap developers or community.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200104/fc86bcbb/attachment.htm>

From andrei.pozolotin at gmail.com  Sat Jan  4 10:49:30 2020
From: andrei.pozolotin at gmail.com (Andrei Pozolotin)
Date: Sat, 04 Jan 2020 04:49:30 -0600
Subject: [squid-users] Question: Force the caching of 302 responses
 without Expires header and with Strict-Transport-Security max-age header?
In-Reply-To: <631b8834-2aba-0779-b165-91390ba54d79@measurement-factory.com>
References: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>
 <631b8834-2aba-0779-b165-91390ba54d79@measurement-factory.com>
Message-ID: <f1a76fe06f090626c68fbc4a6a4fc9b8@gmail.com>

Alex:

On 2020-01-03 14:19, Alex Rousskov wrote:
>> Question: how can one force the caching of 302 responses
>> without the Expires header and with Strict-Transport-Security?max-age 
>> header?
> 
> 
> You can modify Squid to handle Strict-Transport-Security specially or
> you can write an ICAP or eCAP service that would add a "more standard"
> Cache-Control:max-age header to the response (with even more work, it
> would be possible to drop the added response header before it leaves
> Squid).

1. thank you for your suggestions

2. just to confirm I got this right:

there is no way to use any current squid configuration options
or any existing squid plugins to cache 302 responses without Expires 
header,
instead must write some brand new code, correct?

Andrei


From squid3 at treenet.co.nz  Sat Jan  4 11:14:55 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 5 Jan 2020 00:14:55 +1300
Subject: [squid-users] Question: Force the caching of 302 responses
 without Expires header and with Strict-Transport-Security max-age header?
In-Reply-To: <f1a76fe06f090626c68fbc4a6a4fc9b8@gmail.com>
References: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>
 <631b8834-2aba-0779-b165-91390ba54d79@measurement-factory.com>
 <f1a76fe06f090626c68fbc4a6a4fc9b8@gmail.com>
Message-ID: <53b91362-cebd-c9ba-82d9-299041f2bca4@treenet.co.nz>

On 4/01/20 11:49 pm, Andrei Pozolotin wrote:
> Alex:
> 
> On 2020-01-03 14:19, Alex Rousskov wrote:
>>> Question: how can one force the caching of 302 responses
>>> without the Expires header and with Strict-Transport-Security?max-age
>>> header?
>>
>>
>> You can modify Squid to handle Strict-Transport-Security specially or
>> you can write an ICAP or eCAP service that would add a "more standard"
>> Cache-Control:max-age header to the response (with even more work, it
>> would be possible to drop the added response header before it leaves
>> Squid).
> 
> 1. thank you for your suggestions
> 
> 2. just to confirm I got this right:
> 
> there is no way to use any current squid configuration options
> or any existing squid plugins to cache 302 responses without Expires
> header,
> instead must write some brand new code, correct?

Expires header is an HTTP/1.0 protocol feature. Its absence has no meaning.

The 302 response is explicitly defined in HTTP as a *temporary* object
which can change at any time. The *presence* of Cache-Control:max-age or
Expires set a minimum time the response is guaranteed not to change.



Since your use-case is a software archive mirrors you should investigate
whether the objects stored there are truly identical. If they are, the
Store-ID feature can be used to de-duplicate the URLs the 302 are
pointing at so *they* are cached efficiently.
 <https://wiki.squid-cache.org/Features/StoreID>


Amos


From andrei.pozolotin at gmail.com  Sat Jan  4 18:24:01 2020
From: andrei.pozolotin at gmail.com (Andrei Pozolotin)
Date: Sat, 04 Jan 2020 12:24:01 -0600
Subject: [squid-users] Question: Force the caching of 302 responses
 without Expires header and with Strict-Transport-Security max-age header?
In-Reply-To: <53b91362-cebd-c9ba-82d9-299041f2bca4@treenet.co.nz>
References: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>
 <631b8834-2aba-0779-b165-91390ba54d79@measurement-factory.com>
 <f1a76fe06f090626c68fbc4a6a4fc9b8@gmail.com>
 <53b91362-cebd-c9ba-82d9-299041f2bca4@treenet.co.nz>
Message-ID: <9b7288751137e4b6091455d3ae45123d@gmail.com>

Amos, hello:

On 2020-01-04 05:14, Amos Jeffries wrote:
> Expires header is an HTTP/1.0 protocol feature. Its absence has no 
> meaning.
> The 302 response is explicitly defined in HTTP as a *temporary* object
> which can change at any time. The *presence* of Cache-Control:max-age 
> or
> Expires set a minimum time the response is guaranteed not to change.

1. perhaps an argument could be made that these are semantically 
identical:
* Cache-Control: max-age=<expire-time>
* Strict-Transport-Security: max-age=<expire-time>

2. and therefore "Strict-Transport-Security" should be handled
by squid "Cache-Control" related features such as refresh_pattern
http://www.squid-cache.org/Doc/config/refresh_pattern/

> Since your use-case is a software archive mirrors you should 
> investigate
> whether the objects stored there are truly identical. If they are, the
> Store-ID feature can be used to de-duplicate the URLs the 302 are
> pointing at so *they* are cached efficiently.
>  <https://wiki.squid-cache.org/Features/StoreID>

3. thank you for the StoreID idea

4. I have already implemented it:
https://github.com/random-python/nspawn/tree/master/src/main/nspawn/app/hatcher/service/image-proxy/etc/squid

5. it does improve performance, however two preceding TCP_MISS/302 hits
for every archive url hit, do provide major contribution to the overall 
response delay

Thanks again,

Andrei.


From squid3 at treenet.co.nz  Sun Jan  5 01:27:58 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 5 Jan 2020 14:27:58 +1300
Subject: [squid-users] Question: Force the caching of 302 responses
 without Expires header and with Strict-Transport-Security max-age header?
In-Reply-To: <9b7288751137e4b6091455d3ae45123d@gmail.com>
References: <d97c1b48bd3a943fc3656fea169590e9@gmail.com>
 <631b8834-2aba-0779-b165-91390ba54d79@measurement-factory.com>
 <f1a76fe06f090626c68fbc4a6a4fc9b8@gmail.com>
 <53b91362-cebd-c9ba-82d9-299041f2bca4@treenet.co.nz>
 <9b7288751137e4b6091455d3ae45123d@gmail.com>
Message-ID: <3c61bf53-ff1d-8641-9f13-add734f3faa7@treenet.co.nz>

On 5/01/20 7:24 am, Andrei Pozolotin wrote:
> Amos, hello:
> 
> On 2020-01-04 05:14, Amos Jeffries wrote:
>> Expires header is an HTTP/1.0 protocol feature. Its absence has no
>> meaning.
>> The 302 response is explicitly defined in HTTP as a *temporary* object
>> which can change at any time. The *presence* of Cache-Control:max-age or
>> Expires set a minimum time the response is guaranteed not to change.
> 
> 1. perhaps an argument could be made that these are semantically identical:
> * Cache-Control: max-age=<expire-time>
> * Strict-Transport-Security: max-age=<expire-time>
> 

They are not. One relates to hop-by-hop message storage. The other
relates to end-to-end connection setup.


> 2. and therefore "Strict-Transport-Security" should be handled
> by squid "Cache-Control" related features such as refresh_pattern
> http://www.squid-cache.org/Doc/config/refresh_pattern/
> 

As Alex said Squid does nothing with Strict-Transport-Security headers.
They are for the client UA software, irrelevant to middleware like Squid.


>> Since your use-case is a software archive mirrors you should investigate
>> whether the objects stored there are truly identical. If they are, the
>> Store-ID feature can be used to de-duplicate the URLs the 302 are
>> pointing at so *they* are cached efficiently.
>> ?<https://wiki.squid-cache.org/Features/StoreID>
> 
> 3. thank you for the StoreID idea
> 
> 4. I have already implemented it:
> https://github.com/random-python/nspawn/tree/master/src/main/nspawn/app/hatcher/service/image-proxy/etc/squid
> 
> 
> 5. it does improve performance, however two preceding TCP_MISS/302 hits
> for every archive url hit, do provide major contribution to the overall
> response delay


(Warning: I have not tested this idea yet, if it does not work it can
break the downloads completely. Treat with extreme care).

You may be able to improve that a little by adding the original 302 URL
to the Store-ID map. However you MUST then add a store_miss rule to
prevent those URLs being stored in the cache.

The idea being that one one of the real download objects is stored Squid
use it as a substitute for the 302. But the 302 payload can never be
used as a substitute for the real object.

Amos


From rousskov at measurement-factory.com  Mon Jan  6 14:52:45 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 6 Jan 2020 09:52:45 -0500
Subject: [squid-users] Fwd: Squid 4.8 with OpenSSL 1.1.1d
In-Reply-To: <CADSgaCSEgoPUorknDxw-CPxVbJ7LfWsS-bTg79L+U-qweuFbzg@mail.gmail.com>
References: <CADSgaCTS3xsUY0EBZvJY+fk2_OBqUVNBO1gMDwyxJpYYPH7_Ng@mail.gmail.com>
 <CADSgaCQtQAetQCukyQfR5c6Bq8B22_k76Ff3Y8OONsqm1juiPg@mail.gmail.com>
 <fc72bea5-0f98-864b-57f2-ae4d8cc9ae92@measurement-factory.com>
 <CADSgaCSEgoPUorknDxw-CPxVbJ7LfWsS-bTg79L+U-qweuFbzg@mail.gmail.com>
Message-ID: <95c21e62-8918-c872-583b-232ef954e9bd@measurement-factory.com>

On 1/3/20 8:40 AM, Yaroslav Pushko wrote:

> During?establishing TLSv1.3 handshake after?successfully?send our Client
> Hello, the server answers us with Hello Retry Request.

HelloRetryRequest is a TLS v1.3 feature that tells the client to restart
the negotiation (with additional info). Please keep in mind that:

1. If Squid peeks at TLS v1.3 data from the server, bumping the
connection is likely to be impossible. Since plain text TLS v1.3 server
bytes should not contain useful information, and encrypted TLS v1.3
server bytes are useless for a peeking transaction (and deadly for a
bumping transaction), the decision to bump such a transaction has to be
done no later than step2 (i.e. no later than after receiving TLS Client
Hello).

2. Squid TLS parser does not know about TLS v1.3 HelloRetryRequest
messages yet. Thus, if Squid is asked to parse from-server traffic
containing that message, Squid may not do what it should do (whatever
that is).

The changes discussed earlier in this email thread do not include adding
support for HelloRetryRequest, but they may help with the overall
situation by correctly identifying TLS v1.3 traffic. I think we are very
close to submitting an official pull request with these changes.

HTH,

Alex.


> In Squid this behavior interprets next Client Hello peek
> successfully?and Hello Retry Request peeks?as Server Hello.
> After that, we splice it and send to OpenSSL and fail handshake
> establishing.
> 
> Did you already notice such behavior??
> Did you have some investigation on this issue,?any advice will be pleasant?
> 
> Best regards,
> Yaroslav.
> 
> 
> On Tue, Dec 17, 2019 at 4:39 PM Alex Rousskov wrote:
> 
>     On 12/17/19 9:00 AM, Yaroslav Pushko wrote:
>     > Hi All
>     >
>     > We use?Squid 4.8 with OpenSSL 1.1.1d in a transparent mode for
>     peek and
>     > splice interception.
>     >
>     > With this version, we lost the?possibility?to connect to any HTTPS
>     site.
>     >
>     > There are a few issues:?
>     >
>     >? ?* support TLSv1.2 sites (already discussed in
>     >? ?
>     ?thread?http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-with-ssl-choose-client-version-inappropriate-fallback-on-some-sites-when-using-TLS1-2-td4688258.html?)
>     >? ?* support TLSv1.3 sites.
> 
>     Please see
>     http://lists.squid-cache.org/pipermail/squid-users/2019-December/021435.html
>     for several alternative fixes. AFAICT, those fixes are more flexible
>     and, after polishing, appropriate for the official inclusion because
>     they make fewer assumptions about the values sent via the supported
>     versions extension.
> 
>     It is possible that your SSL_MODE_SEND_FALLBACK_SCSV change needs to be
>     integrated with the other fixes. Thank you for sharing that idea!
> 
>     Alex.
> 
> 
>     > Support TLSv1.2.
>     >
>     >? ? ?OpenSSL 1.1.1d adds support of TLSv1.3. These changes added some
>     >? ? ?kind of guard if we perform a handshake with a lower version
>     of the
>     >? ? ?TLS protocol than we support. In this scenario, we receive
>     downgrade
>     >? ? ?fallback?error.
>     >? ? ?Handshake version TLSv1.2 vs. max support TLSv1.3.
>     >
>     >? ? ?In such case, we have the next error:
>     >
>     >? ? ? ? ?ERROR: negotiating TLS on FD 19: error:1425F175:SSL
>     >? ? ? ? ?routines:ssl_choose_client_version:inappropriate fallback
>     (1/-1/0)
>     >
>     >
>     >? ? ?OpenSSL already?provided?a fix for it. You can configure SSL
>     session
>     >? ? ?to use option?SSL_MODE_SEND_FALLBACK_SCSV and setting SSL max
>     proto
>     >? ? ?version for current SSL session, but squid not yet supported these
>     >? ? ?features.
>     >
>     >? ? ?You can find a patch in the attachments, will be grateful for the
>     >? ? ?review.
>     >
>     >
>     > The issue with TLS 1.3 support, we are still investigating, any advice
>     > will be pleasant.
>     >
>     > Best regards,
>     > Yaroslav Pushko.
>     > --?
>     > Best Regards,
>     > Yaroslav Pushko | Senior?*Software Engineer*
>     > GlobalLogic
>     > P +380971842774 ?M +380634232226 S?dithard
>     > www.globallogic.com <http://www.globallogic.com>
>     <http://www.globallogic.com/>
>     > http://www.globallogic.com/email_disclaimer.txt
>     >
>     > _______________________________________________
>     > squid-users mailing list
>     > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     > http://lists.squid-cache.org/listinfo/squid-users
>     >
> 
> 
> 
> -- 
> Best Regards,
> Yaroslav Pushko | Senior?*Software Engineer*
> GlobalLogic
> P +380971842774 ?M +380634232226 S?dithard
> www.globallogic.com <http://www.globallogic.com/>
> <http://www.globallogic.com/>
> http://www.globallogic.com/email_disclaimer.txt



From suprajasridhar95 at gmail.com  Wed Jan  8 04:36:46 2020
From: suprajasridhar95 at gmail.com (supraja sridhar)
Date: Wed, 8 Jan 2020 10:06:46 +0530
Subject: [squid-users] Parent proxy expects Digest based authentication
Message-ID: <CAKL1wB0VU+LoZ24mN7br=JsTNaceK6Xeo7Xb+2id8QSW8VwP2Q@mail.gmail.com>

Hello,

Does squid support digest based authentication to peers?

Thanks
Supraja
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200108/914ad05a/attachment.htm>

From robertkwild at gmail.com  Thu Jan  9 07:34:22 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 9 Jan 2020 07:34:22 +0000
Subject: [squid-users] squid-cache proxy which does it all
Message-ID: <CAGU_CiL2jPDeEpYheTsEV-Tfw+vMEYtOnsEi2uW=J1o-+BC31Q@mail.gmail.com>

hi all,

I have made a script for squid that installs the following ?

Squid ? http proxy server
Squid ssl-bump ? https interception for squid
C-ICAP ? icap server
clamAV ? AV engine to detect trojan viruses malware etc
squidclamav ? to make it all integrated with squid

what do you think?

#!/bin/bash
#squid on DMZ host
#
#first things first lets disable firewalld and SElinux
#
systemctl stop firewalld
systemctl disable firewalld
sed -i -e 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
#
#squid packages
#
yum install -y epel-release swaks sed tar zip unzip curl telnet openssl
openssl-devel bzip2-devel libarchive libarchive-devel perl perl-Data-Dumper
gcc gcc-c++ binutils autoconf automake make sudo wget libxml2-devel
libcap-devel libtool-ltdl-devel
#
#clamAV packages
#
yum install -y clamav-server clamav-data clamav-update clamav-filesystem
clamav clamav-scanner-systemd clamav-devel clamav-lib clamav-server-systemd
#
#download and compile from source
#
cd /tmp
wget http://www.squid-cache.org/Versions/v4/squid-4.9.tar.gz
wget
http://sourceforge.net/projects/c-icap/files/c-icap/0.5.x/c_icap-0.5.6.tar.gz
wget
http://sourceforge.net/projects/c-icap/files/c-icap-modules/0.5.x/c_icap_modules-0.5.4.tar.gz
wget
https://sourceforge.net/projects/squidclamav/files/squidclamav/7.1/squidclamav-7.1.tar.gz
for f in *.tar.gz; do tar xf "$f"; done
cd /tmp/squid-4.9
./configure --with-openssl --enable-ssl-crtd --enable-icap-client && make
&& make install
#
cd /tmp/c_icap-0.5.6
./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe' --without-bdb
--prefix=/usr/local && make && make install
#
cd /tmp/squidclamav-7.1
./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
--with-c-icap=/usr/local --with-libarchive && make && make install
#
cd /tmp/c_icap_modules-0.5.4
./configure 'CFLAGS=-O3 -m64 -pipe' 'CPPFLAGS=-I/usr/local/clamav/include'
'LDFLAGS=-L/usr/local/lib -L/usr/local/clamav/lib/' && make && make install
#
#creating shortcuts and copying files
#
cp -f /usr/local/squid/etc/squid.conf /usr/local/squid/etc/squid.conf.orig
cp -f /usr/local/etc/c-icap.conf /usr/local/etc/c-icap.conf.orig
cp -f /usr/local/etc/squidclamav.conf /usr/local/etc/squidclamav.conf.orig
cp -f /usr/local/etc/clamav_mod.conf /usr/local/etc/clamav_mod.conf.orig
cp -f /usr/local/etc/virus_scan.conf /usr/local/etc/virus_scan.conf.orig
#
ln -s /usr/local/squid/etc/squid.conf /etc
ln -s /usr/local/etc/c-icap.conf /etc
ln -s /usr/local/etc/squidclamav.conf /etc
ln -s /usr/local/etc/clamav_mod.conf /etc
ln -s /usr/local/etc/virus_scan.conf /etc
#
mkdir -p /usr/local/clamav/share/clamav
ln -s /var/lib/clamav /usr/local/clamav/share/clamav
#
#tmpfiles for run files
#
echo "d /var/run/c-icap 0755 root root -" >> /etc/tmpfiles.d/c-icap.conf
echo "d /var/run/clamav 0755 root root -" >> /etc/tmpfiles.d/clamav.conf
#
#delete a few lines in squid
#
sed -i '/http_port 3128/d' /usr/local/squid/etc/squid.conf
sed -i '/http_access deny all/d' /usr/local/squid/etc/squid.conf
#
#whitelist in squid
#
sed -i '50i#HTTP_HTTPS whitelist websites' /usr/local/squid/etc/squid.conf
sed -i '51iacl whitelist ssl::server_name
"/usr/local/squid/etc/urlwhite.txt"' /usr/local/squid/etc/squid.conf
sed -i '52ihttp_access allow whitelist' /usr/local/squid/etc/squid.conf
sed -i '53ihttp_access deny all' /usr/local/squid/etc/squid.conf
echo "#Microsoft" >> /usr/local/squid/etc/urlwhite.txt
echo ".bing.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".msn.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".msedge.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".msftauth.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".msauth.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".msocdn.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".outlook.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".onedrive.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".office.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".office.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".office365.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".microsoft.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".microsoftonline.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".live.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".live.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".akamaized.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".akamaihd.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".svc.ms" >> /usr/local/squid/etc/urlwhite.txt
echo ".lync.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".skype.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".gfx.ms" >> /usr/local/squid/etc/urlwhite.txt
echo ".sharepoint.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".sharepointonline.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".windowsupdate.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".windows.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".edgesuite.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".a-msedge.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".akamaiedge.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".sfx.ms" >> /usr/local/squid/etc/urlwhite.txt
echo ".azureedge.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".trafficmanager.net" >> /usr/local/squid/etc/urlwhite.txt
echo ".azure.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".s-microsoft.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".onestore.ms" >> /usr/local/squid/etc/urlwhite.txt
echo "#Google" >> /usr/local/squid/etc/urlwhite.txt
echo ".google.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".google.co.uk" >> /usr/local/squid/etc/urlwhite.txt
echo ".googleusercontent.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".googleapis.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".withgoogle.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".gstatic.com" >> /usr/local/squid/etc/urlwhite.txt
echo "#Adobe" >> /usr/local/squid/etc/urlwhite.txt
echo ".adobedtm.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".adobe.io" >> /usr/local/squid/etc/urlwhite.txt
echo ".adobe.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".adobelogin.com" >> /usr/local/squid/etc/urlwhite.txt
echo "#others" >> /usr/local/squid/etc/urlwhite.txt
echo ".digicert.com" >> /usr/local/squid/etc/urlwhite.txt
echo ".pixelogicmedia.com" >> /usr/local/squid/etc/urlwhite.txt
#
#ICAP in squid
#
echo "#ICAP" >> /usr/local/squid/etc/squid.conf
echo "icap_enable on" >> /usr/local/squid/etc/squid.conf
echo "adaptation_uses_indirect_client on" >> /usr/local/squid/etc/squid.conf
echo "icap_send_client_ip on" >> /usr/local/squid/etc/squid.conf
echo "icap_send_client_username on" >> /usr/local/squid/etc/squid.conf
echo "icap_client_username_header X-Authenticated-User" >>
/usr/local/squid/etc/squid.conf
echo "icap_service service_req reqmod_precache bypass=0 icap://
127.0.0.1:1344/squidclamav" >> /usr/local/squid/etc/squid.conf
echo "adaptation_access service_req allow all" >>
/usr/local/squid/etc/squid.conf
echo "icap_service service_resp respmod_precache bypass=0 icap://
127.0.0.1:1344/squidclamav" >> /usr/local/squid/etc/squid.conf
echo "adaptation_access service_resp allow all" >>
/usr/local/squid/etc/squid.conf
#
#squid with SSL
#
mkdir -p /usr/local/squid/etc/ssl_cert
cd /usr/local/squid/etc/ssl_cert
adduser squid
chown squid:squid /usr/local/squid/etc/ssl_cert
chmod 700 /usr/local/squid/etc/ssl_cert
openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509
-extensions v3_ca -keyout myCA.pem -out myCA.pem -batch
#must import the below cert on hosts in trusted root cert ie the .der file
openssl x509 -in myCA.pem -outform DER -out myCA.der
/usr/local/squid/libexec/security_file_certgen -c -s /var/lib/ssl_db -M 4MB
chown squid:squid -R /var/lib/ssl_db
chmod -R 777 /usr/local/squid/var/logs
sed -i '1ihttp_port 3128 ssl-bump
cert=/usr/local/squid/etc/ssl_cert/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB' /usr/local/squid/etc/squid.conf
sed -i '2isslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB' /usr/local/squid/etc/squid.conf
sed -i '3iacl step1 at_step SslBump1' /usr/local/squid/etc/squid.conf
sed -i '4issl_bump peek step1' /usr/local/squid/etc/squid.conf
sed -i '5issl_bump bump all' /usr/local/squid/etc/squid.conf
#
#squidclamav conf
#
sed -i -e 's%redirect http://proxy.domain.dom/cgi-bin/clwarn.cgi%#redirect
http://proxy.domain.dom/cgi-bin/clwarn.cgi%g' /etc/squidclamav.conf
#sed -i -e 's%clamd_local /var/run/clamav/clamd.ctl%clamd_local
/run/clamd.scan/clamd.sock%g' /etc/squidclamav.conf
sed -i -e 's%enable_libarchive 0%enable_libarchive 1%g'
/etc/squidclamav.conf
#
#clamav conf
#
sed -i -e 's%#LocalSocket /run/clamd.scan/clamd.sock%LocalSocket
/var/run/clamav/clamd.ctl%g' /etc/clamd.d/scan.conf
sed -i -e 's%Example%#Example%g' /etc/clamd.d/scan.conf
sed -i -e 's%User clamscan%User root%g' /etc/clamd.d/scan.conf
sed -i -e 's%#StreamMaxLength 10M%StreamMaxLength 5M%g'
/etc/clamd.d/scan.conf
freshclam
echo "00 01,13 * * * /usr/bin/freshclam --quiet" >> /var/spool/cron/root
systemctl enable clamd at scan
#
#c-icap and c-icap modules
#
#sed -i -e 's%PidFile /var/run/c-icap/c-icap.pid%PidFile
/run/c-icap/c-icap.pid%g' /etc/c-icap.conf
#sed -i -e 's%CommandsSocket /var/run/c-icap/c-icap.ctl%CommandsSocket
/run/c-icap/c-icap.ctl%g' /etc/c-icap.conf
sed -i -e 's%#.*User wwwrun%User root%g' /etc/c-icap.conf
sed -i -e 's%#.*Group nogroup%Group root%g' /etc/c-icap.conf
sed -i -e 's%#.*Service echo_service srv_echo.so%Service squidclamav
squidclamav.so%g' /etc/c-icap.conf
sed -i -e 's%DebugLevel 1%DebugLevel 0%g' /etc/c-icap.conf
sed -i -e 's%StartServers 3%StartServers 1%g' /etc/c-icap.conf
sed -i -e 's%MaxServers 10%MaxServers 20%g' /etc/c-icap.conf
sed -i -e 's%MaxRequestsPerChild 0%MaxRequestsPerChild 100%g'
/etc/c-icap.conf
sed -i '520iacl localhost src 127.0.0.1/255.255.255.255' /etc/c-icap.conf
sed -i '521iacl PERMIT_REQUESTS type REQMOD RESPMOD' /etc/c-icap.conf
sed -i '522iicap_access allow localhost PERMIT_REQUESTS' /etc/c-icap.conf
sed -i '523iicap_access deny all' /etc/c-icap.conf
echo "clamav_mod.TmpDir /var/tmp" >> /etc/clamav_mod.conf
echo "clamav_mod.MaxFilesInArchive 1000" >> /etc/clamav_mod.conf
echo "clamav_mod.MaxScanSize 5M" >> /etc/clamav_mod.conf
echo "clamav_mod.HeuristicScanPrecedence on" >> /etc/clamav_mod.conf
echo "clamav_mod.OLE2BlockMacros on" >> /etc/clamav_mod.conf
echo "virus_scan.ScanFileTypes TEXT DATA EXECUTABLE ARCHIVE DOCUMENT" >>
/etc/virus_scan.conf
echo "virus_scan.SendPercentData 5" >> /etc/virus_scan.conf
echo "virus_scan.PassOnError on" >> /etc/virus_scan.conf
echo "virus_scan.MaxObjectSize 5M" >> /etc/virus_scan.conf
echo "virus_scan.DefaultEngine clamav" >> /etc/virus_scan.conf
echo "Include clamav_mod.conf" >> /etc/virus_scan.conf
echo "Include virus_scan.conf" >> /etc/c-icap.conf
#
#make c-icap service
#
echo "[Unit]" >> /usr/lib/systemd/system/c-icap.service
echo "Description=c-icap service" >> /usr/lib/systemd/system/c-icap.service
echo "After=network.target" >> /usr/lib/systemd/system/c-icap.service
echo "[Service]" >> /usr/lib/systemd/system/c-icap.service
echo "Type=forking" >> /usr/lib/systemd/system/c-icap.service
echo "PIDFile=/var/run/c-icap/c-icap.pid" >>
/usr/lib/systemd/system/c-icap.service
echo "ExecStart=/usr/local/bin/c-icap -f /etc/c-icap.conf" >>
/usr/lib/systemd/system/c-icap.service
echo "KillMode=process" >> /usr/lib/systemd/system/c-icap.service
echo "[Install]" >> /usr/lib/systemd/system/c-icap.service
echo "WantedBy=multi-user.target" >> /usr/lib/systemd/system/c-icap.service
systemctl enable c-icap
reboot

thanks,

rob


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200109/740bbf6f/attachment.htm>

From netadmin at aicta.ro  Thu Jan  9 09:22:37 2020
From: netadmin at aicta.ro (netadmin)
Date: Thu, 9 Jan 2020 03:22:37 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <564215BD.1080607@mdx.ac.uk>
References: <564215BD.1080607@mdx.ac.uk>
Message-ID: <1578561757370-0.post@n4.nabble.com>

I try to use the configuration given in the previous post with: Squid 4.9 and
Sophos SAVDI 2.6.
If I download a virus file, the Squid sends the file for scanning and is
detected by Sophos SAVDI (I find it in logs) but it is not blocked by Squid
(I can download it).
The problem I think is in the response received by the Squid after the scan
but I do not know where.
Has anyone managed to make this solution functional?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eype69 at gmail.com  Thu Jan  9 13:08:07 2020
From: eype69 at gmail.com (John Sweet-Escott)
Date: Thu, 9 Jan 2020 13:08:07 +0000
Subject: [squid-users] Fwd: Squid 4.8 with OpenSSL 1.1.1d
In-Reply-To: <95c21e62-8918-c872-583b-232ef954e9bd@measurement-factory.com>
References: <95c21e62-8918-c872-583b-232ef954e9bd@measurement-factory.com>
Message-ID: <1A113AB5-AD03-4F7F-A188-2BA0BAF18DA0@gmail.com>

Alex

Really looking forward to this patch being submitted and hopefully accepted. Let me know if it would be helpful for me to do some independent testing of the patch. 

John 

> On 6 Jan 2020, at 14:53, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> ?On 1/3/20 8:40 AM, Yaroslav Pushko wrote:
> 
>> During establishing TLSv1.3 handshake after successfully send our Client
>> Hello, the server answers us with Hello Retry Request.
> 
> HelloRetryRequest is a TLS v1.3 feature that tells the client to restart
> the negotiation (with additional info). Please keep in mind that:
> 
> 1. If Squid peeks at TLS v1.3 data from the server, bumping the
> connection is likely to be impossible. Since plain text TLS v1.3 server
> bytes should not contain useful information, and encrypted TLS v1.3
> server bytes are useless for a peeking transaction (and deadly for a
> bumping transaction), the decision to bump such a transaction has to be
> done no later than step2 (i.e. no later than after receiving TLS Client
> Hello).
> 
> 2. Squid TLS parser does not know about TLS v1.3 HelloRetryRequest
> messages yet. Thus, if Squid is asked to parse from-server traffic
> containing that message, Squid may not do what it should do (whatever
> that is).
> 
> The changes discussed earlier in this email thread do not include adding
> support for HelloRetryRequest, but they may help with the overall
> situation by correctly identifying TLS v1.3 traffic. I think we are very
> close to submitting an official pull request with these changes.
> 
> HTH,
> 
> Alex.
> 
> 
>> In Squid this behavior interprets next Client Hello peek
>> successfully and Hello Retry Request peeks as Server Hello.
>> After that, we splice it and send to OpenSSL and fail handshake
>> establishing.
>> 
>> Did you already notice such behavior? 
>> Did you have some investigation on this issue, any advice will be pleasant?
>> 
>> Best regards,
>> Yaroslav.
>> 
>> 
>> On Tue, Dec 17, 2019 at 4:39 PM Alex Rousskov wrote:
>> 
>>>    On 12/17/19 9:00 AM, Yaroslav Pushko wrote:
>>> Hi All
>>> 
>>> We use Squid 4.8 with OpenSSL 1.1.1d in a transparent mode for
>>    peek and
>>> splice interception.
>>> 
>>> With this version, we lost the possibility to connect to any HTTPS
>>    site.
>>> 
>>> There are a few issues: 
>>> 
>>>    * support TLSv1.2 sites (already discussed in
>>>    
>>     thread http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-with-ssl-choose-client-version-inappropriate-fallback-on-some-sites-when-using-TLS1-2-td4688258.html )
>>>    * support TLSv1.3 sites.
>> 
>>    Please see
>>    http://lists.squid-cache.org/pipermail/squid-users/2019-December/021435.html
>>    for several alternative fixes. AFAICT, those fixes are more flexible
>>    and, after polishing, appropriate for the official inclusion because
>>    they make fewer assumptions about the values sent via the supported
>>    versions extension.
>> 
>>    It is possible that your SSL_MODE_SEND_FALLBACK_SCSV change needs to be
>>    integrated with the other fixes. Thank you for sharing that idea!
>> 
>>    Alex.
>> 
>> 
>>> Support TLSv1.2.
>>> 
>>>      OpenSSL 1.1.1d adds support of TLSv1.3. These changes added some
>>>      kind of guard if we perform a handshake with a lower version
>>    of the
>>>      TLS protocol than we support. In this scenario, we receive
>>    downgrade
>>>      fallback error.
>>>      Handshake version TLSv1.2 vs. max support TLSv1.3.
>>> 
>>>      In such case, we have the next error:
>>> 
>>>          ERROR: negotiating TLS on FD 19: error:1425F175:SSL
>>>          routines:ssl_choose_client_version:inappropriate fallback
>>    (1/-1/0)
>>> 
>>> 
>>>      OpenSSL already provided a fix for it. You can configure SSL
>>    session
>>>      to use option SSL_MODE_SEND_FALLBACK_SCSV and setting SSL max
>>    proto
>>>      version for current SSL session, but squid not yet supported these
>>>      features.
>>> 
>>>      You can find a patch in the attachments, will be grateful for the
>>>      review.
>>> 
>>> 
>>> The issue with TLS 1.3 support, we are still investigating, any advice
>>> will be pleasant.
>>> 
>>> Best regards,
>>> Yaroslav Pushko.
>>> -- 
>>> Best Regards,
>>> Yaroslav Pushko | Senior *Software Engineer*
>>> GlobalLogic
>>> P +380971842774  M +380634232226 S dithard
>>> www.globallogic.com <http://www.globallogic.com>
>>    <http://www.globallogic.com/>
>>> http://www.globallogic.com/email_disclaimer.txt
>>> 
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>    <mailto:squid-users at lists.squid-cache.org>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>> 
>> 
>> 
>> 
>> -- 
>> Best Regards,
>> Yaroslav Pushko | Senior *Software Engineer*
>> GlobalLogic
>> P +380971842774  M +380634232226 S dithard
>> www.globallogic.com <http://www.globallogic.com/>
>> <http://www.globallogic.com/>
>> http://www.globallogic.com/email_disclaimer.txt
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Thu Jan  9 19:00:13 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jan 2020 08:00:13 +1300
Subject: [squid-users] squid-cache proxy which does it all
In-Reply-To: <CAGU_CiL2jPDeEpYheTsEV-Tfw+vMEYtOnsEi2uW=J1o-+BC31Q@mail.gmail.com>
References: <CAGU_CiL2jPDeEpYheTsEV-Tfw+vMEYtOnsEi2uW=J1o-+BC31Q@mail.gmail.com>
Message-ID: <d05fa165-e8f4-2cc0-595c-de76f389a240@treenet.co.nz>

On 9/01/20 8:34 pm, robert k Wild wrote:
> hi all,
> 
> I have made a script for squid that installs the following ?
> 
> Squid ? http proxy server
> Squid ssl-bump ? https interception for squid
> C-ICAP ? icap server
> clamAV ? AV engine to detect trojan viruses malware etc
> squidclamav ? to make it all integrated with squid
> 
> what do you think?
> 
> #!/bin/bash
> #squid on DMZ host
> #
> #first things first lets disable firewalld and SElinux
> #
> systemctl stop firewalld
> systemctl disable firewalld
> sed -i -e 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
> #

Why?



> #squid packages
> #
> yum install -y epel-release swaks sed tar zip unzip curl telnet openssl
> openssl-devel bzip2-devel libarchive libarchive-devel perl
> perl-Data-Dumper gcc gcc-c++ binutils autoconf automake make sudo wget
> libxml2-devel libcap-devel libtool-ltdl-devel
> #
> #clamAV packages
> #
> yum install -y clamav-server clamav-data clamav-update clamav-filesystem
> clamav clamav-scanner-systemd clamav-devel clamav-lib clamav-server-systemd
> #
> #download and compile from source
> #
> cd /tmp
> wget http://www.squid-cache.org/Versions/v4/squid-4.9.tar.gz

Please use rsync for this, and verify against the *.asc file signature
that you got the file correctly.

> wget
> http://sourceforge.net/projects/c-icap/files/c-icap/0.5.x/c_icap-0.5.6.tar.gz
> wget
> http://sourceforge.net/projects/c-icap/files/c-icap-modules/0.5.x/c_icap_modules-0.5.4.tar.gz
> wget
> https://sourceforge.net/projects/squidclamav/files/squidclamav/7.1/squidclamav-7.1.tar.gz
> for f in *.tar.gz; do tar xf "$f"; done
> cd /tmp/squid-4.9
> ./configure --with-openssl --enable-ssl-crtd --enable-icap-client &&
> make && make install
> #

IIRC this was a CentoOS machine right?
If so, see <https://wiki.squid-cache.org/KnowledgeBase/CentOS#Compiling>
otherwise see the equivalent wiki page for your chosen OS compile.

Those settings install Squid as a system application. So no need for the
/usr/local stuff.


> cd /tmp/c_icap-0.5.6
> ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> --without-bdb --prefix=/usr/local && make && make install
> #
> cd /tmp/squidclamav-7.1
> ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> --with-c-icap=/usr/local --with-libarchive && make && make install
> #
> cd /tmp/c_icap_modules-0.5.4
> ./configure 'CFLAGS=-O3 -m64 -pipe'
> 'CPPFLAGS=-I/usr/local/clamav/include' 'LDFLAGS=-L/usr/local/lib
> -L/usr/local/clamav/lib/' && make && make install
> #
> #creating shortcuts and copying files
> #
> cp -f /usr/local/squid/etc/squid.conf /usr/local/squid/etc/squid.conf.orig
> cp -f /usr/local/etc/c-icap.conf /usr/local/etc/c-icap.conf.orig
> cp -f /usr/local/etc/squidclamav.conf /usr/local/etc/squidclamav.conf.orig
> cp -f /usr/local/etc/clamav_mod.conf /usr/local/etc/clamav_mod.conf.orig
> cp -f /usr/local/etc/virus_scan.conf /usr/local/etc/virus_scan.conf.orig
> #
> ln -s /usr/local/squid/etc/squid.conf /etc
> ln -s /usr/local/etc/c-icap.conf /etc
> ln -s /usr/local/etc/squidclamav.conf /etc
> ln -s /usr/local/etc/clamav_mod.conf /etc
> ln -s /usr/local/etc/virus_scan.conf /etc
> #
> mkdir -p /usr/local/clamav/share/clamav
> ln -s /var/lib/clamav /usr/local/clamav/share/clamav
> #
> #tmpfiles for run files
> #
> echo "d /var/run/c-icap 0755 root root -" >> /etc/tmpfiles.d/c-icap.conf
> echo "d /var/run/clamav 0755 root root -" >> /etc/tmpfiles.d/clamav.conf
> #
> #delete a few lines in squid
> #
> sed -i '/http_port 3128/d' /usr/local/squid/etc/squid.conf
> sed -i '/http_access deny all/d' /usr/local/squid/etc/squid.conf

Please do not remove that second line from yoru squid.conf. It will
result in unpredictable default allow/deny behaviour from your proxy.

Instead I recommend (mind the wrap):

 sed -i '/# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR
CLIENTS/include "/etc/squid/squid.conf.d/*"/'
/usr/local/squid/etc/squid.conf

Then you can just drop files into the /etc/squid/squid.conf.d/ directory
and they will be loaded as config on next start or reconfigure.



HTH
Amos


From squid3 at treenet.co.nz  Thu Jan  9 19:00:57 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jan 2020 08:00:57 +1300
Subject: [squid-users] Parent proxy expects Digest based authentication
In-Reply-To: <CAKL1wB0VU+LoZ24mN7br=JsTNaceK6Xeo7Xb+2id8QSW8VwP2Q@mail.gmail.com>
References: <CAKL1wB0VU+LoZ24mN7br=JsTNaceK6Xeo7Xb+2id8QSW8VwP2Q@mail.gmail.com>
Message-ID: <27c26bff-b4b2-ab4a-36af-2ee0c1e9c406@treenet.co.nz>

On 8/01/20 5:36 pm, supraja sridhar wrote:
> Hello,
> 
> Does squid support digest based authentication to peers?
> 

Unfortunately no. Only Basic and Negotiate/Kerberos authentication can
be used to peers.

Amos


From robertkwild at gmail.com  Thu Jan  9 19:42:51 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 9 Jan 2020 19:42:51 +0000
Subject: [squid-users] squid-cache proxy which does it all
In-Reply-To: <d05fa165-e8f4-2cc0-595c-de76f389a240@treenet.co.nz>
References: <CAGU_CiL2jPDeEpYheTsEV-Tfw+vMEYtOnsEi2uW=J1o-+BC31Q@mail.gmail.com>
 <d05fa165-e8f4-2cc0-595c-de76f389a240@treenet.co.nz>
Message-ID: <CAGU_CiJHFJOK4=go53eYbtpB4v-NAz-URy379uE-m-N1pTeQNA@mail.gmail.com>

thanks for this Amos, really appreciate it :)

On Thu, 9 Jan 2020 at 19:00, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 9/01/20 8:34 pm, robert k Wild wrote:
> > hi all,
> >
> > I have made a script for squid that installs the following ?
> >
> > Squid ? http proxy server
> > Squid ssl-bump ? https interception for squid
> > C-ICAP ? icap server
> > clamAV ? AV engine to detect trojan viruses malware etc
> > squidclamav ? to make it all integrated with squid
> >
> > what do you think?
> >
> > #!/bin/bash
> > #squid on DMZ host
> > #
> > #first things first lets disable firewalld and SElinux
> > #
> > systemctl stop firewalld
> > systemctl disable firewalld
> > sed -i -e 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
> > #
>
> Why?
>
>
>
> > #squid packages
> > #
> > yum install -y epel-release swaks sed tar zip unzip curl telnet openssl
> > openssl-devel bzip2-devel libarchive libarchive-devel perl
> > perl-Data-Dumper gcc gcc-c++ binutils autoconf automake make sudo wget
> > libxml2-devel libcap-devel libtool-ltdl-devel
> > #
> > #clamAV packages
> > #
> > yum install -y clamav-server clamav-data clamav-update clamav-filesystem
> > clamav clamav-scanner-systemd clamav-devel clamav-lib
> clamav-server-systemd
> > #
> > #download and compile from source
> > #
> > cd /tmp
> > wget http://www.squid-cache.org/Versions/v4/squid-4.9.tar.gz
>
> Please use rsync for this, and verify against the *.asc file signature
> that you got the file correctly.
>
> > wget
> >
> http://sourceforge.net/projects/c-icap/files/c-icap/0.5.x/c_icap-0.5.6.tar.gz
> > wget
> >
> http://sourceforge.net/projects/c-icap/files/c-icap-modules/0.5.x/c_icap_modules-0.5.4.tar.gz
> > wget
> >
> https://sourceforge.net/projects/squidclamav/files/squidclamav/7.1/squidclamav-7.1.tar.gz
> > for f in *.tar.gz; do tar xf "$f"; done
> > cd /tmp/squid-4.9
> > ./configure --with-openssl --enable-ssl-crtd --enable-icap-client &&
> > make && make install
> > #
>
> IIRC this was a CentoOS machine right?
> If so, see <https://wiki.squid-cache.org/KnowledgeBase/CentOS#Compiling>
> otherwise see the equivalent wiki page for your chosen OS compile.
>
> Those settings install Squid as a system application. So no need for the
> /usr/local stuff.
>
>
> > cd /tmp/c_icap-0.5.6
> > ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> > --without-bdb --prefix=/usr/local && make && make install
> > #
> > cd /tmp/squidclamav-7.1
> > ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> > --with-c-icap=/usr/local --with-libarchive && make && make install
> > #
> > cd /tmp/c_icap_modules-0.5.4
> > ./configure 'CFLAGS=-O3 -m64 -pipe'
> > 'CPPFLAGS=-I/usr/local/clamav/include' 'LDFLAGS=-L/usr/local/lib
> > -L/usr/local/clamav/lib/' && make && make install
> > #
> > #creating shortcuts and copying files
> > #
> > cp -f /usr/local/squid/etc/squid.conf
> /usr/local/squid/etc/squid.conf.orig
> > cp -f /usr/local/etc/c-icap.conf /usr/local/etc/c-icap.conf.orig
> > cp -f /usr/local/etc/squidclamav.conf
> /usr/local/etc/squidclamav.conf.orig
> > cp -f /usr/local/etc/clamav_mod.conf /usr/local/etc/clamav_mod.conf.orig
> > cp -f /usr/local/etc/virus_scan.conf /usr/local/etc/virus_scan.conf.orig
> > #
> > ln -s /usr/local/squid/etc/squid.conf /etc
> > ln -s /usr/local/etc/c-icap.conf /etc
> > ln -s /usr/local/etc/squidclamav.conf /etc
> > ln -s /usr/local/etc/clamav_mod.conf /etc
> > ln -s /usr/local/etc/virus_scan.conf /etc
> > #
> > mkdir -p /usr/local/clamav/share/clamav
> > ln -s /var/lib/clamav /usr/local/clamav/share/clamav
> > #
> > #tmpfiles for run files
> > #
> > echo "d /var/run/c-icap 0755 root root -" >> /etc/tmpfiles.d/c-icap.conf
> > echo "d /var/run/clamav 0755 root root -" >> /etc/tmpfiles.d/clamav.conf
> > #
> > #delete a few lines in squid
> > #
> > sed -i '/http_port 3128/d' /usr/local/squid/etc/squid.conf
> > sed -i '/http_access deny all/d' /usr/local/squid/etc/squid.conf
>
> Please do not remove that second line from yoru squid.conf. It will
> result in unpredictable default allow/deny behaviour from your proxy.
>
> Instead I recommend (mind the wrap):
>
>  sed -i '/# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR
> CLIENTS/include "/etc/squid/squid.conf.d/*"/'
> /usr/local/squid/etc/squid.conf
>
> Then you can just drop files into the /etc/squid/squid.conf.d/ directory
> and they will be loaded as config on next start or reconfigure.
>
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200109/4ef69df3/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 10 05:30:57 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Jan 2020 18:30:57 +1300
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1578561757370-0.post@n4.nabble.com>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
Message-ID: <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>

On 9/01/20 10:22 pm, netadmin wrote:
> I try to use the configuration given in the previous post

FYI: this post started a brand new thread, there is no previous post
visible to us. Please provide a direct reference to the post and/or
config file in question.


> with: Squid 4.9 and
> Sophos SAVDI 2.6.
> If I download a virus file, the Squid sends the file for scanning and is
> detected by Sophos SAVDI (I find it in logs) but it is not blocked by Squid
> (I can download it).
> The problem I think is in the response received by the Squid after the scan
> but I do not know where.
> Has anyone managed to make this solution functional?
> 

I assume the config uses the AV software as an ICAP service?
That has been made working by many AFAIK, with several different AV
software.

It is most likely that the ICAP service is either telling Squid it can
start delivering the response early before it finds the virus payload.
Or, producing the wrong response and thus causing Squid to deliver the
content.


To help we are likely to need your squid.conf details, the access.log
entries that show the transaction(s) you know are wrong, and the
icap.log content.

Amos


From netadmin at aicta.ro  Fri Jan 10 06:11:44 2020
From: netadmin at aicta.ro (netadmin)
Date: Fri, 10 Jan 2020 00:11:44 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
Message-ID: <1578636704817-0.post@n4.nabble.com>

I'm sorry I didn't know how the list I posted works.
Direct reference is in the basement of the post:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html

I will also post the required information as soon as I restore the
configuration on a test server.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From netadmin at aicta.ro  Fri Jan 10 06:18:02 2020
From: netadmin at aicta.ro (netadmin)
Date: Fri, 10 Jan 2020 00:18:02 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1578636704817-0.post@n4.nabble.com>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com>
Message-ID: <1578637082640-0.post@n4.nabble.com>

http://squid-web-proxy-cache.1019090.n4.nabble.com/icap-SOPHOS-SAVDI-and-custom-errorpage-td4674469.html



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From netadmin at aicta.ro  Fri Jan 10 10:37:46 2020
From: netadmin at aicta.ro (netadmin)
Date: Fri, 10 Jan 2020 04:37:46 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1578637082640-0.post@n4.nabble.com>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com> <1578637082640-0.post@n4.nabble.com>
Message-ID: <1578652666905-0.post@n4.nabble.com>

squid.conf
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/squid.conf>  
access.log
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/access.log>  
icap.log
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/icap.log>  
Sophos_SAVDI.log
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/Sophos_SAVDI.log>  



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Jan 10 13:28:24 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jan 2020 02:28:24 +1300
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1578652666905-0.post@n4.nabble.com>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com> <1578637082640-0.post@n4.nabble.com>
 <1578652666905-0.post@n4.nabble.com>
Message-ID: <4f84307d-ffee-fcaa-2ca5-d8dfca40d308@treenet.co.nz>

On 10/01/20 11:37 pm, netadmin wrote:
> squid.conf
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/squid.conf>

Okay, so you have taken the part of David's config which sends traffic
to ICAP, but not the part which generates a custom 403 message for the
client.

That means whatever SAVDI is providing to Squid via ICAP is being
delivered to the end-client.

> access.log
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/access.log>

Notice the "Content-Length: 0" in the response headers delivered to the
client ...

> icap.log
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/icap.log>
> Sophos_SAVDI.log
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377857/Sophos_SAVDI.log>  
> 

 ... and in both these the HTTP response given to SAVDI was 184 bytes long.


SAVDI is truncating infected payloads and telling Squid to deliver a
0-length response instead of the infection. So the setup is working fine
- though not with the log entries you were expecting to see.

Amos


From netadmin at aicta.ro  Fri Jan 10 18:43:37 2020
From: netadmin at aicta.ro (netadmin)
Date: Fri, 10 Jan 2020 12:43:37 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <4f84307d-ffee-fcaa-2ca5-d8dfca40d308@treenet.co.nz>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com> <1578637082640-0.post@n4.nabble.com>
 <1578652666905-0.post@n4.nabble.com>
 <4f84307d-ffee-fcaa-2ca5-d8dfca40d308@treenet.co.nz>
Message-ID: <1578681817198-0.post@n4.nabble.com>

I also tried with the settings from David Webb's post ie:
acl http_status_403 http_status 403
acl virus_found rep_header X-Blocked -i \Virus found during virus scan\.

I tried both options:
http_reply_access deny http_status_403 virus_found
and
adapted_http_access deny http_status_403 virus_found

but something is wrong, I can download the test file (eicar).




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From g2011828 at hotmail.com  Fri Jan 10 22:27:07 2020
From: g2011828 at hotmail.com (GeorgeShen)
Date: Fri, 10 Jan 2020 16:27:07 -0600 (CST)
Subject: [squid-users] Does Squid proxy passes client/server MIME type such
 as application/x-protobuf?
Message-ID: <1578695227552-0.post@n4.nabble.com>


I would like to know in the case of proxy, can be ssl-bump, does the squid
proxy passes the http MIME type to the other side of the connection? such as
application/x-protobuf, application/json, text/plain, etc. What is the
expectation on this for the other HTTP header information?

thanks.
- George



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From robertkwild at gmail.com  Fri Jan 10 22:46:30 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 10 Jan 2020 22:46:30 +0000
Subject: [squid-users] squid to only allow office activation and not windows
	updates
Message-ID: <CAGU_CiLScwym_ue8JNn0rHVp0ugrtjNYRdBFQrk=pYF+vmxQYQ@mail.gmail.com>

hi all,

i have added all these lines to my squid config as it wasnt allowing office
activation

https://wiki.squid-cache.org/SquidFaq/WindowsUpdate

but now its allowing office activation and now windows updates but i dont
want it to do windows updates as this is managed by our WSUS server

what are the corect lines to just do the office activation

as when i comment out all the lines i get this

0 - TCP_DENIED/403 3810 GET
http://www.microsoft.com/pkiops/certs/Microsoft%20ECC%20Product%20Root%20Certificate%20Authority%202018.crt

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200110/8932cf3a/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 11 01:20:37 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jan 2020 14:20:37 +1300
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1578681817198-0.post@n4.nabble.com>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com> <1578637082640-0.post@n4.nabble.com>
 <1578652666905-0.post@n4.nabble.com>
 <4f84307d-ffee-fcaa-2ca5-d8dfca40d308@treenet.co.nz>
 <1578681817198-0.post@n4.nabble.com>
Message-ID: <1665d349-6a35-3eea-1d12-031e9b34e3ec@treenet.co.nz>

On 11/01/20 7:43 am, netadmin wrote:
> I also tried with the settings from David Webb's post ie:
> acl http_status_403 http_status 403
> acl virus_found rep_header X-Blocked -i \Virus found during virus scan\.
> 
> I tried both options:
> http_reply_access deny http_status_403 virus_found
> and
> adapted_http_access deny http_status_403 virus_found
> 
> but something is wrong, I can download the test file (eicar).
> 

There are two problems here.

 *  The string SAVDI adds has no '.' at the end. The regex you have says
(with "\.") that is mandatory.
  - remove that bit of the regex

 * SAVDI is producing status 200. So the 403 status check will not work
for you.
  - remove the http_status_403.

The access.log you showed earlier say that SAVDI is adding both of these
headers which you could use:

 X-Blocked: Virus found during virus scan
 X-Blocked-By: Sophos Anti-Virus



Amos


From squid3 at treenet.co.nz  Sat Jan 11 01:32:05 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jan 2020 14:32:05 +1300
Subject: [squid-users] Does Squid proxy passes client/server MIME type
 such as application/x-protobuf?
In-Reply-To: <1578695227552-0.post@n4.nabble.com>
References: <1578695227552-0.post@n4.nabble.com>
Message-ID: <04c7c6ce-1d7d-e265-1ee5-ecd47368b028@treenet.co.nz>

On 11/01/20 11:27 am, GeorgeShen wrote:
> 
> I would like to know in the case of proxy, can be ssl-bump, does the squid
> proxy passes the http MIME type to the other side of the connection?

SSL-Bump is about the TLS protocol. It has nothing to do with HTTP
messages or headers.

The standard rules for HTTP are applied to plain-text HTTP messages, and
to the decrypted HTTP(S) messages equally.

A proxy passes the mime type appropriate for the message it is
delivering. If that message payload is being relayed the mime type will
be transmitted along with it.
 (There are cases in HTTP when new messages are generated to replace the
received one, or where the message originates at the proxy, or stops at
the proxy).


> such as
> application/x-protobuf, application/json, text/plain, etc. What is the
> expectation on this for the other HTTP header information?

Mime type is a Content-* header - related to the payload itself. Squid
(and all HTTP proxies) do as little with relayed payloads as possible.
Ideally they just send it onwards and/or store for delivery later - the
recipient of the message is responsible for anything which mime header
may be involved with.


Amos


From squid3 at treenet.co.nz  Sat Jan 11 01:40:39 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jan 2020 14:40:39 +1300
Subject: [squid-users] squid to only allow office activation and not
 windows updates
In-Reply-To: <CAGU_CiLScwym_ue8JNn0rHVp0ugrtjNYRdBFQrk=pYF+vmxQYQ@mail.gmail.com>
References: <CAGU_CiLScwym_ue8JNn0rHVp0ugrtjNYRdBFQrk=pYF+vmxQYQ@mail.gmail.com>
Message-ID: <ba67ce5d-fc57-2199-ae96-3943edbba3f7@treenet.co.nz>

On 11/01/20 11:46 am, robert k Wild wrote:
> hi all,
> 
> i have added all these lines to my squid config as it wasnt allowing
> office activation
> 
> https://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> 
> but now its allowing office activation and now windows updates but i
> dont want it to do windows updates as this is managed by our WSUS server
> 

That would be right then. As the wiki page name indicates that config is
all about allowing WindowsUpdate.


> what are the corect lines to just do the office activation
> 

This is a strong indication you still do not understand how ACLs work.

So your reference points are:
 <https://wiki.squid-cache.org/SquidFaq/SquidAcl>
and
 <http://www.squid-cache.org/Doc/config/acl/>


> as when i comment out all the lines i get this
> 
> 0 - TCP_DENIED/403 3810 GET
> http://www.microsoft.com/pkiops/certs/Microsoft%20ECC%20Product%20Root%20Certificate%20Authority%202018.crt
> 

That then is the first URL you need to let clients access.

Once that is accessible the activation process will get further and
there may be others. When you know the whole set there may be some
optimizations your rules can use to simplify the final config.


Amos


From g2011828 at hotmail.com  Sat Jan 11 04:30:19 2020
From: g2011828 at hotmail.com (GeorgeShen)
Date: Fri, 10 Jan 2020 22:30:19 -0600 (CST)
Subject: [squid-users] Does Squid proxy passes client/server MIME type
 such as application/x-protobuf?
In-Reply-To: <04c7c6ce-1d7d-e265-1ee5-ecd47368b028@treenet.co.nz>
References: <1578695227552-0.post@n4.nabble.com>
 <04c7c6ce-1d7d-e265-1ee5-ecd47368b028@treenet.co.nz>
Message-ID: <1578717019626-0.post@n4.nabble.com>


Thanks Amos,

Good to know the MIME types are forwarded if the payload is being relayed.

What will be the expectation on the http custom headers, such as
'X-Request-ID', or 'X-Serial-Number' if they are set from client, during the
proxy relay process, will those also be forwarded unchanged or is it
possible
only some of the well-known ones being honored for squid or other types of
proxies?

thanks.
- George



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sat Jan 11 06:10:55 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jan 2020 19:10:55 +1300
Subject: [squid-users] Does Squid proxy passes client/server MIME type
 such as application/x-protobuf?
In-Reply-To: <1578717019626-0.post@n4.nabble.com>
References: <1578695227552-0.post@n4.nabble.com>
 <04c7c6ce-1d7d-e265-1ee5-ecd47368b028@treenet.co.nz>
 <1578717019626-0.post@n4.nabble.com>
Message-ID: <ab9a806f-8b47-efb0-23e0-99981d63f876@treenet.co.nz>

On 11/01/20 5:30 pm, GeorgeShen wrote:
> 
> Thanks Amos,
> 
> Good to know the MIME types are forwarded if the payload is being relayed.
> 
> What will be the expectation on the http custom headers, such as
> 'X-Request-ID', or 'X-Serial-Number' if they are set from client, during the
> proxy relay process, will those also be forwarded unchanged or is it
> possible
> only some of the well-known ones being honored for squid or other types of
> proxies?
> 

Please see the HTTP specifications. They answer most of your questions.
 <https://tools.ietf.org/html/rfc7230#section-3.2.1>

Amos


From robertkwild at gmail.com  Sat Jan 11 12:15:03 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Sat, 11 Jan 2020 12:15:03 +0000
Subject: [squid-users] squid to only allow office activation and not
 windows updates
In-Reply-To: <ba67ce5d-fc57-2199-ae96-3943edbba3f7@treenet.co.nz>
References: <CAGU_CiLScwym_ue8JNn0rHVp0ugrtjNYRdBFQrk=pYF+vmxQYQ@mail.gmail.com>
 <ba67ce5d-fc57-2199-ae96-3943edbba3f7@treenet.co.nz>
Message-ID: <CAGU_CiJbTBxLTzxcBYoaz8=8i9n2Aqs4E3Sz7UdGwFaTKMujEA@mail.gmail.com>

Hi Amos,

ok, i have found the rule for it

acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name .microsoft.com
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all

but the thing is both windows updates and office activation use the exact
same cert file

.
microsoft.com/pkiops/certs/Microsoft%20ECC%20Product%20Root%20Certificate%20Authority%202018.crt

im stuck

or if i can get squid to block windows updates altogether?

Thanks,

Rob

On Sat, 11 Jan 2020, 01:40 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 11/01/20 11:46 am, robert k Wild wrote:
> > hi all,
> >
> > i have added all these lines to my squid config as it wasnt allowing
> > office activation
> >
> > https://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> >
> > but now its allowing office activation and now windows updates but i
> > dont want it to do windows updates as this is managed by our WSUS server
> >
>
> That would be right then. As the wiki page name indicates that config is
> all about allowing WindowsUpdate.
>
>
> > what are the corect lines to just do the office activation
> >
>
> This is a strong indication you still do not understand how ACLs work.
>
> So your reference points are:
>  <https://wiki.squid-cache.org/SquidFaq/SquidAcl>
> and
>  <http://www.squid-cache.org/Doc/config/acl/>
>
>
> > as when i comment out all the lines i get this
> >
> > 0 - TCP_DENIED/403 3810 GET
> >
> http://www.microsoft.com/pkiops/certs/Microsoft%20ECC%20Product%20Root%20Certificate%20Authority%202018.crt
> >
>
> That then is the first URL you need to let clients access.
>
> Once that is accessible the activation process will get further and
> there may be others. When you know the whole set there may be some
> optimizations your rules can use to simplify the final config.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200111/7f2eeea4/attachment.htm>

From robertkwild at gmail.com  Sat Jan 11 13:19:12 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Sat, 11 Jan 2020 13:19:12 +0000
Subject: [squid-users] squid to only allow office activation and not
 windows updates
In-Reply-To: <CAGU_CiJbTBxLTzxcBYoaz8=8i9n2Aqs4E3Sz7UdGwFaTKMujEA@mail.gmail.com>
References: <CAGU_CiLScwym_ue8JNn0rHVp0ugrtjNYRdBFQrk=pYF+vmxQYQ@mail.gmail.com>
 <ba67ce5d-fc57-2199-ae96-3943edbba3f7@treenet.co.nz>
 <CAGU_CiJbTBxLTzxcBYoaz8=8i9n2Aqs4E3Sz7UdGwFaTKMujEA@mail.gmail.com>
Message-ID: <CAGU_CiJV194R+A40h=NBGJDkVvw65tD-XcJR=YdUoZKPUg04AQ@mail.gmail.com>

ok think i have done it

#
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i .microsoft.com
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all
#
#URL deny MIME types
acl mimetype rep_mime_type application/octet-stream
http_reply_access deny mimetype
#

as now windows can check for updates but it cant download as i have denied
the octet-stream ie cab/exe files

On Sat, 11 Jan 2020 at 12:15, robert k Wild <robertkwild at gmail.com> wrote:

> Hi Amos,
>
> ok, i have found the rule for it
>
> acl DiscoverSNIHost at_step SslBump1
> acl NoSSLIntercept ssl::server_name .microsoft.com
> ssl_bump peek DiscoverSNIHost
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
>
> but the thing is both windows updates and office activation use the exact
> same cert file
>
> .
> microsoft.com/pkiops/certs/Microsoft%20ECC%20Product%20Root%20Certificate%20Authority%202018.crt
>
> im stuck
>
> or if i can get squid to block windows updates altogether?
>
> Thanks,
>
> Rob
>
> On Sat, 11 Jan 2020, 01:40 Amos Jeffries, <squid3 at treenet.co.nz> wrote:
>
>> On 11/01/20 11:46 am, robert k Wild wrote:
>> > hi all,
>> >
>> > i have added all these lines to my squid config as it wasnt allowing
>> > office activation
>> >
>> > https://wiki.squid-cache.org/SquidFaq/WindowsUpdate
>> >
>> > but now its allowing office activation and now windows updates but i
>> > dont want it to do windows updates as this is managed by our WSUS server
>> >
>>
>> That would be right then. As the wiki page name indicates that config is
>> all about allowing WindowsUpdate.
>>
>>
>> > what are the corect lines to just do the office activation
>> >
>>
>> This is a strong indication you still do not understand how ACLs work.
>>
>> So your reference points are:
>>  <https://wiki.squid-cache.org/SquidFaq/SquidAcl>
>> and
>>  <http://www.squid-cache.org/Doc/config/acl/>
>>
>>
>> > as when i comment out all the lines i get this
>> >
>> > 0 - TCP_DENIED/403 3810 GET
>> >
>> http://www.microsoft.com/pkiops/certs/Microsoft%20ECC%20Product%20Root%20Certificate%20Authority%202018.crt
>> >
>>
>> That then is the first URL you need to let clients access.
>>
>> Once that is accessible the activation process will get further and
>> there may be others. When you know the whole set there may be some
>> optimizations your rules can use to simplify the final config.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200111/7336af28/attachment.htm>

From netadmin at aicta.ro  Sat Jan 11 13:55:41 2020
From: netadmin at aicta.ro (netadmin)
Date: Sat, 11 Jan 2020 07:55:41 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1665d349-6a35-3eea-1d12-031e9b34e3ec@treenet.co.nz>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com> <1578637082640-0.post@n4.nabble.com>
 <1578652666905-0.post@n4.nabble.com>
 <4f84307d-ffee-fcaa-2ca5-d8dfca40d308@treenet.co.nz>
 <1578681817198-0.post@n4.nabble.com>
 <1665d349-6a35-3eea-1d12-031e9b34e3ec@treenet.co.nz>
Message-ID: <1578750941810-0.post@n4.nabble.com>

Thank you for your time, patience and lessons learned.
Now it is all functional and I can no longer download the test file neither
by clicking nor with Save link as.
I will come back with a post that includes the necessary settings for both
Sophos SAVDI version 2.6 (I highly recommend it for scanning Squid traffic
and antivirus for e-mail) but also for Squid version 4.9 (I have been using
it for 10 years and it is an extraordinary tool for network traffic
management).

Thanks again Amos!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From 5f787a at i2pmail.org  Sat Jan 11 13:04:10 2020
From: 5f787a at i2pmail.org (user)
Date: Sat, 11 Jan 2020 13:04:10 +0000 (UTC)
Subject: [squid-users] reverse proxy
Message-ID: <20200111130410.1E80D41BA3@smtp.postman.i2p>

Hello.
I have use squid 4.8 as reverse proxy. The problem is remote (or local?)
side close connection every 2-4 minutes with message "TCP_MISS_ABORTED/200"
in log.

Another one problem - downloader report incorrect speed and time.
random.bin 100%[============================>] 2.00M --.-KB/s in 0.02s
                                                                 ^^^^^
(83.0 MB/s) - 'random.bin' saved [2097152/2097152]
 ^^^^^^^^^

Spend time in seconds
real    0m27.864s
user    0m0.010s
sys     0m0.040s
-- 



From netadmin at aicta.ro  Sat Jan 11 19:57:14 2020
From: netadmin at aicta.ro (netadmin)
Date: Sat, 11 Jan 2020 13:57:14 -0600 (CST)
Subject: [squid-users] icap SOPHOS SAVDI and custom errorpage
In-Reply-To: <1578750941810-0.post@n4.nabble.com>
References: <564215BD.1080607@mdx.ac.uk> <1578561757370-0.post@n4.nabble.com>
 <c14c3265-7d60-7a46-c5bb-7d3e02e39c9b@treenet.co.nz>
 <1578636704817-0.post@n4.nabble.com> <1578637082640-0.post@n4.nabble.com>
 <1578652666905-0.post@n4.nabble.com>
 <4f84307d-ffee-fcaa-2ca5-d8dfca40d308@treenet.co.nz>
 <1578681817198-0.post@n4.nabble.com>
 <1665d349-6a35-3eea-1d12-031e9b34e3ec@treenet.co.nz>
 <1578750941810-0.post@n4.nabble.com>
Message-ID: <1578772634733-0.post@n4.nabble.com>

Configurations for Sophos-SAVDI (savdid.conf):
> threadcount: <xy>
Normally it should be at least the maximum of customers.
> loglevel: 0
> address: 127.0.0.1
Configurations for Squid-ICAP (squid.conf):
> acl virus_found rep_header X-Blocked -i \ Virus found during virus scan
> http_reply_access deny virus_found
> access_log daemon: /var/log/access.log virus_found
> icap_log daemon: /var/log/icap.log icap_squid
> deny_info ERR_ACCESS_DENIED virus_found
> icap_enable on
> icap_service sophosicap respmod_precache icap: //127.0.0.1: 4020 / sophos
> adaptation_access sophosicap allow all
Configurations for Squid-ssl-bump (squid.conf):
> http_port <IP>: <port> ssl-bump \
cert = / usr / local / squid / ssl_cert / myCA.pem \
generate-host-certificates = on dynamic_cert_mem_cache_size = 4MB
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
> sslcrtd_program / usr / local / squid / libexec / security_file_certgen -s
> / var / lib / ssl_db -M 4MB



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From robertkwild at gmail.com  Mon Jan 13 07:05:02 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Mon, 13 Jan 2020 07:05:02 +0000
Subject: [squid-users] how to stop MSU files being downloaded
Message-ID: <CAGU_CiK6uQWK=+eMSbFUnJqv2zNTUVdRF6FewLWW-=gC+R72tg@mail.gmail.com>

hi all,

i want to block MSU files from being downloaded via windows updates

i know

application/octet-stream
application/x-msi

but whats the one for MSU files?

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200113/2d74e045/attachment.htm>

From 0xff1f at gmail.com  Mon Jan 13 10:36:00 2020
From: 0xff1f at gmail.com (Ahmad Alzaeem)
Date: Mon, 13 Jan 2020 13:36:00 +0300
Subject: [squid-users] TCP  incoming requests Traffic Normalization
Message-ID: <47E10599-149D-4BFE-9B7A-6C8BB478D9B1@gmail.com>


Hello Folks .

I have about 10x sources or different ip addresses  and sending requests to squid  .

imagine we have 10 servers and sending burst in sometimes due to nature of Traffic  ?.i have a sensitive APP on squid that must be equalized to handle only 50 req/sec . ? No more ?

i just want to equalize all incoming requests which can be in some seconds 60  , 40 , 90 , 100 , 50 to have steady 50 req/sec on squid equally and even if we need to delay some packs  its ok , just keep squid handle 50 req/sec  of those incoming requests no  more .

i know squid can limit connections and Drop connections above threshold , but i need only to discipline and Buffer and try to decrease dropped requests as possible and normalize all incoming requests to be steady 50 req/sec inside squid whatever there is burst outside or higher than 50 .

So again just need to apply that on ? new requests ? not on already ? established ? connections .

Let me know Guys if squid can do something like that or we need 3rd party outside squid .


Kind regards 



From killpilot at gmail.com  Mon Jan 13 11:24:13 2020
From: killpilot at gmail.com (killpilot)
Date: Mon, 13 Jan 2020 12:24:13 +0100
Subject: [squid-users] [TECHNICAL QUESTION] try to resolve 403 error for
	specific website
Message-ID: <CAMBJ0ziBwyrDYAt0x8VQLxGeG7a-dYYSBs0OXf83qAm=v5XJ_w@mail.gmail.com>

hi squid community,

sorry for my bad english, i french, i try do my best for explain
cleary my issue.

i have a pfsense with squid plugin. the plugin contain :
squidclamav-6.16
squid_radius_auth-1.10
squid-3.5.27_3
c-icap-modules-0.5.3_1

my squid is config for transparent proxy for http only.

for on game Star Citizen i have a issue with voip feature, when i try
launch voip connection, she failed.

in my squid log i see this entry
1578684384.329 237 192.168.2.2 TCP_MISS/403 270 GET
http://foip-v02.robertsspaceindustries.com/ -
ORIGINAL_DST/35.153.171.151 text/html
1578684385.507 165 192.168.2.2 TCP_MISS/403 270 GET
http://foip-v02.robertsspaceindustries.com/ -
ORIGINAL_DST/35.153.171.151 text/html

when i disable squid, all working fine.
my squid conf file is :

------ My conf file -------
# This file is automatically generated by pfSense
# Do not edit manually !

http_port 192.168.2.1:3128
http_port 192.168.4.1:3128
http_port 192.168.8.1:3128
http_port 127.0.0.1:3128 intercept
icp_port 0
digest_generation off
dns_v4_first off
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language fr
icon_directory /usr/local/etc/squid/icons
visible_hostname localhost
cache_mgr xxxxxxxxxxxxxxxx
access_log /var/squid/logs/access.log
cache_log /var/squid/logs/cache.log
cache_store_log none
netdb_filename /var/squid/logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger

logfile_rotate 7
debug_options rotate=7
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  192.168.2.0/24 192.168.4.0/24 192.168.8.0/24
forwarded_for on
httpd_suppress_version_string on
uri_whitespace strip


cache_mem 64 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 10 MB
cache_dir ufs /var/squid/cache 1024 16 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
acl donotcache dstdomain "/var/squid/acl/donotcache.acl"
cache deny donotcache
cache allow all
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0  0%  0
refresh_pattern .    0  20%  4320

#Remote proxies

# Setup some default acls
# ACLs all, manager, localhost, and to_localhost are predefined.
acl allsrc src all
acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 9443
3128 3129 1025-65535
acl sslports port 443 563 9443

acl purge method PURGE
acl connect method CONNECT

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS
acl unrestricted_hosts src "/var/squid/acl/unrestricted_hosts.acl"
acl whitelist dstdom_regex -i "/var/squid/acl/whitelist.acl"
http_access allow manager localhost

http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !safeports
http_access deny CONNECT !sslports

# Always allow localhost connections
http_access allow localhost

request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
# Do not throttle unrestricted hosts
delay_access 1 deny unrestricted_hosts
delay_access 1 allow allsrc

# Reverse Proxy settings
acl rvm_uri_proxmox url_regex -i proxmox.killpilot.fr
never_direct allow rvm_uri_proxmox
http_access allow rvm_uri_proxmox


# Custom options before auth
acl voip_rsi dstdomain .robertsspaceindustries.com
always_direct allow voip_rsi
cache deny voip_rsi
http_access allow voip_rsi

# These hosts do not have any restrictions
http_access allow unrestricted_hosts
# Always allow access to whitelist domains
http_access allow whitelist
# Setup allowed ACLs
# Allow local network(s) on interface(s)
http_access allow localnet
# Default block all to be sure
http_access deny allsrc

icap_enable on
icap_send_client_ip off
icap_send_client_username off
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024

icap_service service_avi_req reqmod_precache
icap://127.0.0.1:1344/squid_clamav bypass=off
adaptation_access service_avi_req allow all
icap_service service_avi_resp respmod_precache
icap://127.0.0.1:1344/squid_clamav bypass=on
adaptation_access service_avi_resp allow all
------ END My conf file -------


i try to add this block
acl voip_rsi dstdomain .robertsspaceindustries.com
always_direct allow voip_rsi
cache deny voip_rsi
http_access allow voip_rsi

but, not resolved my issue,

i also try add this conf into this file :
my file /var/squid/acl/donotcache.acl  contain :
robertsspaceindustries.com

my file /var/squid/acl/unrestricted_hosts.acl contain my pc IP
192.168.2.2/32

my file /var/squid/acl/whitelist.acl contain
^.*\.robertsspaceindustries.com

same result, failed.... i don't understand why the request are denied .....
from my pc i try with curl command the result is :

curl -vvv -x http://192.168.2.1:3128 -I
http://foip-v02.robertsspaceindustries.com

Trying 192.168.2.1...
TCP_NODELAY set
Connected to 192.168.2.1 (192.168.2.1) port 3128 (#0)

HEAD http://foip-v02.robertsspaceindustries.com/ HTTP/1.1
Host: foip-v02.robertsspaceindustries.com
User-Agent: curl/7.64.1
Accept: /
Proxy-Connection: Keep-Alive

HTTP/1.1 403 Forbidden
Date: Fri, 10 Jan 2020 19:46:03 GMT
Content-Type: text/html
Content-Length: 38
X-Cache: MISS from localhost
X-Cache-Lookup: MISS from localhost:3128
Via: 1.1 localhost (squid)
Connection: keep-alive

someone can help for fix this issue ? i don't find the right
configuration. i try give some help in pfsense forum, but for the
moment the issue is not solved, i try here, may be i be more lucky  ;)

thank for your help.

have a good day, regards,


From 457636876 at qq.com  Mon Jan 13 14:01:59 2020
From: 457636876 at qq.com (yohan83942)
Date: Mon, 13 Jan 2020 08:01:59 -0600 (CST)
Subject: [squid-users] Squid configuration cache_peer does not take effect?
Message-ID: <1578924119045-0.post@n4.nabble.com>

The configuration is as follows:

```shell
# Squid normally listens to port 3128
always_direct allow all
ssl_bump bump all
sslproxy_cert_error allow all
http_port 3128 ssl-bump cert=/etc/squid/squid.pem key=/etc/squid/squid.pem
generate-host-certificates=on options=NO_SSLv2
#http_port 3128

cache_peer 127.0.0.1 parent 10809 0 no-query
never_direct allow all
```

I tested it and found out that the `127.0.0.1:10809` proxy was not taken?

How to solve? 

However, when I changed it to the following, it worked. But then Squid
cannot cache https.

```shell
http_port 3128

cache_peer 127.0.0.1 parent 10809 0 no-query
never_direct allow all
```

Why is that?

Full configuration

```shell
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
always_direct allow all
ssl_bump bump all
sslproxy_cert_error allow all
http_port 3128 ssl-bump cert=/etc/squid/squid.pem key=/etc/squid/squid.pem
generate-host-certificates=on options=NO_SSLv2
#http_port 3128

cache_peer 127.0.0.1 parent 10809 0 no-query
never_direct allow all

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

dns_nameservers 8.8.8.8
```



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From 457636876 at qq.com  Mon Jan 13 14:36:26 2020
From: 457636876 at qq.com (yohan83942)
Date: Mon, 13 Jan 2020 08:36:26 -0600 (CST)
Subject: [squid-users] Squid configuration cache_peer does not take effect?
Message-ID: <1578926186767-0.post@n4.nabble.com>

The configuration is as follows:

```shell
# Squid normally listens to port 3128
always_direct allow all
ssl_bump bump all
sslproxy_cert_error allow all
http_port 3128 ssl-bump cert=/etc/squid/squid.pem key=/etc/squid/squid.pem
generate-host-certificates=on options=NO_SSLv2
#http_port 3128

cache_peer 127.0.0.1 parent 10809 0 no-query
never_direct allow all
```

I tested it and found out that the `127.0.0.1:10809` proxy was not taken?

How to solve? 

However, when I changed it to the following, it worked. But then Squid
cannot cache https.

```shell
http_port 3128

cache_peer 127.0.0.1 parent 10809 0 no-query
never_direct allow all
```

Why is that?

Full configuration

```shell
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
always_direct allow all
ssl_bump bump all
sslproxy_cert_error allow all
http_port 3128 ssl-bump cert=/etc/squid/squid.pem key=/etc/squid/squid.pem
generate-host-certificates=on options=NO_SSLv2
#http_port 3128

cache_peer 127.0.0.1 parent 10809 0 no-query
never_direct allow all

# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

dns_nameservers 8.8.8.8
```



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From info at schroeffu.ch  Mon Jan 13 15:13:00 2020
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Mon, 13 Jan 2020 15:13:00 +0000
Subject: [squid-users] Squid configuration cache_peer does not take
 effect?
In-Reply-To: <1578926186767-0.post@n4.nabble.com>
References: <1578926186767-0.post@n4.nabble.com>
Message-ID: <82599fa11eeaf0a86cdc90ba5eeb4fbc@schroeffu.ch>

> The configuration is as follows:
> 
> ```shell
> # Squid normally listens to port 3128
> always_direct allow all
> ssl_bump bump all
> sslproxy_cert_error allow all
> http_port 3128 ssl-bump cert=/etc/squid/squid.pem key=/etc/squid/squid.pem
> generate-host-certificates=on options=NO_SSLv2
> #http_port 3128
> 
> cache_peer 127.0.0.1 parent 10809 0 no-query
> never_direct allow all
> ```
> 

Hi,

as far as i know, cache_peer seems to be mostly not supported with ssl_bump before Squid v5 (not stable yet), see here: http://squid-web-proxy-cache.1019090.n4.nabble.com/Non-transparent-proxy-with-cache-peer-and-ssl-bump-tp4687620p4687622.html 

Had the same issue and gave up, will wait for Squid v5 becomes productive usable.

All the best
Schroeffu


From 457636876 at qq.com  Mon Jan 13 15:45:53 2020
From: 457636876 at qq.com (yohan83942)
Date: Mon, 13 Jan 2020 09:45:53 -0600 (CST)
Subject: [squid-users] Squid configuration cache_peer does not take
	effect?
In-Reply-To: <82599fa11eeaf0a86cdc90ba5eeb4fbc@schroeffu.ch>
References: <1578926186767-0.post@n4.nabble.com>
 <82599fa11eeaf0a86cdc90ba5eeb4fbc@schroeffu.ch>
Message-ID: <1578930353782-0.post@n4.nabble.com>

I have read this article before and it has been a long time. The problem
persists and I'm surprised.
Do you know when the v5 version will be released? Already 2020.

Is there a solution?
I just want a server that can cache https and access the Internet through a
proxy like `cache_peer`?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Mon Jan 13 16:42:52 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 13 Jan 2020 11:42:52 -0500
Subject: [squid-users] TCP incoming requests Traffic Normalization
In-Reply-To: <47E10599-149D-4BFE-9B7A-6C8BB478D9B1@gmail.com>
References: <47E10599-149D-4BFE-9B7A-6C8BB478D9B1@gmail.com>
Message-ID: <c15362b7-d466-9d8c-bf79-1bbc1895aefb@measurement-factory.com>

On 1/13/20 5:36 AM, Ahmad Alzaeem wrote:

> just need to apply that on ? new requests ? not on already ? established ? connections

You can probably do this using an external ACL. The program implementing
your external ACL can delay incoming requests to make sure that the
aggregate served request rate is at most 50/s. You will need to write
that program or find something suitable on the web.

For more details, look for "acl aclname external" and perhaps "external
ACL" in squid.conf.documented. The API for the external ACL program
(called "helper") is described at
https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29

N.B. One connection may carry many HTTP requests. To implement the limit
correctly, you will need to clarify whether the limit should apply to
new requests on established connections. The external ACL helper can
support any answer to that question, but its code will be different
depending on your answer.

Alex.


From selltorontu at gmail.com  Tue Jan 14 01:59:06 2020
From: selltorontu at gmail.com (squdbuff)
Date: Mon, 13 Jan 2020 19:59:06 -0600 (CST)
Subject: [squid-users] Squid memory usage increase overtime,
	proxies slows down
Message-ID: <1578967146829-0.post@n4.nabble.com>

Hello guys

Running squid on ubuntu 16

When I first start the program, the proxies are at a good speed (below
100ms)

after about 5 mins then memory usage of squid increases greatly, then the
proxies slow down to 1000ms+

my access log is getting spammed by the follow message ( I have no idea what
this means) I dont have a user named kaiz9n9d9

1578959115.266      0 216.115.184.251 TCP_DENIED/407 4130 CONNECT
www.supremenewyork.com:443 kaiz9n9d9 HIER_NONE/- text/html
1578959115.270      0 216.115.184.251 TCP_DENIED/407 4020 CONNECT
www.supremenewyork.com:443 - HIER_NONE/- text/html
1578959115.272      0 216.115.184.251 TCP_DENIED/407 4130 CONNECT
www.supremenewyork.com:443 kaiz9n9d9 HIER_NONE/- text/html
1578959115.276      0 216.115.184.251 TCP_DENIED/407 4020 CONNECT
www.supremenewyork.com:443 - HIER_NONE/- text/html
1578959115.277      0 216.115.184.251 TCP_DENIED/407 4130 CONNECT
www.supremenewyork.com:443 kaiz9n9d9 HIER_NONE/- text/html
1578959115.281      0 216.115.184.251 TCP_DENIED/407 4020 CONNECT
www.supremenewyork.com:443 - HIER_NONE/- text/html


Currently I am running "service squid reload", every 300 seconds which keeps
the proxies at a good speed

But this is not a permanent fix

Can someone please offer your advise for this issue?

Thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From selltorontu at gmail.com  Tue Jan 14 02:02:10 2020
From: selltorontu at gmail.com (squdbuff)
Date: Mon, 13 Jan 2020 20:02:10 -0600 (CST)
Subject: [squid-users] Squid memory usage increase overtime,
	proxies slows down
In-Reply-To: <1578967146829-0.post@n4.nabble.com>
References: <1578967146829-0.post@n4.nabble.com>
Message-ID: <1578967330452-0.post@n4.nabble.com>

When I first start squid memory usage is around 24mb, then after 5 mins
increased to 1GB!

Also, the access log size increases by 500 kb every second, so I have
disabled access log for now



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Jan 14 05:28:49 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jan 2020 18:28:49 +1300
Subject: [squid-users] Squid configuration cache_peer does not take
 effect?
In-Reply-To: <1578930353782-0.post@n4.nabble.com>
References: <1578926186767-0.post@n4.nabble.com>
 <82599fa11eeaf0a86cdc90ba5eeb4fbc@schroeffu.ch>
 <1578930353782-0.post@n4.nabble.com>
Message-ID: <f30e199c-1a09-530f-89a2-2d62a019598f@treenet.co.nz>

On 14/01/20 4:45 am, yohan83942 wrote:
> I have read this article before and it has been a long time. The problem
> persists and I'm surprised.
> Do you know when the v5 version will be released? Already 2020.

I am bundling 5.0.1 (beta) later today. As for when it's stable that
will depend on what bugs get found and how hard they are to fix.
Hopefully later this year or early next.

You do not need a Squid version to be stable to use it. Just expect some
bugs.

Code for 5.0.0 (alpha) is available right now at
<http://www.squid-cache.org/Versions/v5/> if you are able to self-build.


> 
> Is there a solution?

Only the feature in v5.

There is a hacks, but that requires the peer to be a Squid with SSL-Bump
and requires some tricky config at the peer. It is unlikely to be useful
unless you are admin of both proxies.

Amos


From uhlar at fantomas.sk  Tue Jan 14 08:46:09 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 14 Jan 2020 09:46:09 +0100
Subject: [squid-users] Squid memory usage increase overtime,
 proxies slows down
In-Reply-To: <1578967330452-0.post@n4.nabble.com>
References: <1578967146829-0.post@n4.nabble.com>
 <1578967330452-0.post@n4.nabble.com>
Message-ID: <20200114084609.GA17254@fantomas.sk>

On 13.01.20 20:02, squdbuff wrote:
>When I first start squid memory usage is around 24mb, then after 5 mins
>increased to 1GB!

https://wiki.squid-cache.org/SquidFaq/SquidMemory

>Also, the access log size increases by 500 kb every second, so I have
>disabled access log for now

wow, you have really huge traffic, in such case you should expect high
memory usage. For all data, squif must fetch it from server and pass it to
client. This requires much of buffer space.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
How does cat play with mouse? cat /dev/mouse


From uhlar at fantomas.sk  Tue Jan 14 08:48:07 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 14 Jan 2020 09:48:07 +0100
Subject: [squid-users] Squid memory usage increase overtime,
 proxies slows down
In-Reply-To: <1578967146829-0.post@n4.nabble.com>
References: <1578967146829-0.post@n4.nabble.com>
Message-ID: <20200114084807.GB17254@fantomas.sk>

On 13.01.20 19:59, squdbuff wrote:
>When I first start the program, the proxies are at a good speed (below
>100ms)
>
>after about 5 mins then memory usage of squid increases greatly, then the
>proxies slow down to 1000ms+
>
>my access log is getting spammed by the follow message ( I have no idea what
>this means) I dont have a user named kaiz9n9d9
>
>1578959115.266      0 216.115.184.251 TCP_DENIED/407 4130 CONNECT
>www.supremenewyork.com:443 kaiz9n9d9 HIER_NONE/- text/html
>1578959115.270      0 216.115.184.251 TCP_DENIED/407 4020 CONNECT
>www.supremenewyork.com:443 - HIER_NONE/- text/html
>1578959115.272      0 216.115.184.251 TCP_DENIED/407 4130 CONNECT
>www.supremenewyork.com:443 kaiz9n9d9 HIER_NONE/- text/html
>1578959115.276      0 216.115.184.251 TCP_DENIED/407 4020 CONNECT
>www.supremenewyork.com:443 - HIER_NONE/- text/html
>1578959115.277      0 216.115.184.251 TCP_DENIED/407 4130 CONNECT
>www.supremenewyork.com:443 kaiz9n9d9 HIER_NONE/- text/html
>1578959115.281      0 216.115.184.251 TCP_DENIED/407 4020 CONNECT
>www.supremenewyork.com:443 - HIER_NONE/- text/html
>
>
>Currently I am running "service squid reload", every 300 seconds which keeps
>the proxies at a good speed
>
>But this is not a permanent fix
>
>Can someone please offer your advise for this issue?

do you know that IP, does it belong to you?
If not, it's apparently someone from the internet trying to use your squid
as a proxy.

the easiest way to avoid this is to use firewall for blocking connections
from the world to squid.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
They that can give up essential liberty to obtain a little temporary
safety deserve neither liberty nor safety. -- Benjamin Franklin, 1759


From robertkwild at gmail.com  Wed Jan 15 00:12:09 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 15 Jan 2020 00:12:09 +0000
Subject: [squid-users] ssl::server_name_regex with multiple domains
Message-ID: <CAGU_CiKbLLUHisjDi8TMsbj-ufyLvE2V9OARqhFercJvJ-KM3w@mail.gmail.com>

hi all,

will this work ie multiple domains on one line?

# SSL bump rulesacl DiscoverSNIHost at_step SslBump1acl NoSSLIntercept
ssl::server_name_regex -i .adobe.com .microsoft.comssl_bump peek
DiscoverSNIHostssl_bump splice NoSSLInterceptssl_bump bump all

ie

acl NoSSLIntercept ssl::server_name_regex -i .adobe.com .microsoft.com

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/770287b6/attachment.htm>

From robertkwild at gmail.com  Wed Jan 15 02:53:11 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 15 Jan 2020 02:53:11 +0000
Subject: [squid-users] ssl::server_name_regex with multiple domains
In-Reply-To: <CAGU_CiKbLLUHisjDi8TMsbj-ufyLvE2V9OARqhFercJvJ-KM3w@mail.gmail.com>
References: <CAGU_CiKbLLUHisjDi8TMsbj-ufyLvE2V9OARqhFercJvJ-KM3w@mail.gmail.com>
Message-ID: <CAGU_Ci+_ucP+6XCvBi4nN+LpPs=nVTTyDsBXVSCERJb9wV_CTw@mail.gmail.com>

done

#allow SSL cert
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i
"/usr/local/squid/etc/pubkey.txt"
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

in my pubkey.txt

.microsoft.com
.adobe.com

On Wed, 15 Jan 2020 at 00:12, robert k Wild <robertkwild at gmail.com> wrote:

> hi all,
>
> will this work ie multiple domains on one line?
>
> # SSL bump rulesacl DiscoverSNIHost at_step SslBump1acl NoSSLIntercept ssl::server_name_regex -i .adobe.com .microsoft.comssl_bump peek DiscoverSNIHostssl_bump splice NoSSLInterceptssl_bump bump all
>
> ie
>
> acl NoSSLIntercept ssl::server_name_regex -i .adobe.com .microsoft.com
>
> thanks,
> rob
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/b96b7f42/attachment.htm>

From robertkwild at gmail.com  Wed Jan 15 02:59:02 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 15 Jan 2020 02:59:02 +0000
Subject: [squid-users] unable to get issuer cert locally protocol error
Message-ID: <CAGU_CiJ4j9TpeDzdMOG7pWkuSAymhEgJsZaauf26rt2T04oYzw@mail.gmail.com>

hi all,

when im trying to go on a web page, squid cant connect and gives me an
error page -

The system returned:
(71) Protocol error (TLS code:
X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

SSL Certficate error: certificate issuer (CA) not known: /C=US/O=Let's
Encrypt/CN=Let's Encrypt Authority X3

This proxy and the remote host failed to negotiate a mutually acceptable
security settings for handling your request. It is possible that the remote
host does not support secure connections, or the proxy is not satisfied
with the host security credentials.
does anyone know what the problem is

cheers,

rob


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/7e4c01aa/attachment.htm>

From 457636876 at qq.com  Wed Jan 15 04:29:31 2020
From: 457636876 at qq.com (yohan83942)
Date: Tue, 14 Jan 2020 22:29:31 -0600 (CST)
Subject: [squid-users] Squid configuration cache_peer does not take
	effect?
In-Reply-To: <f30e199c-1a09-530f-89a2-2d62a019598f@treenet.co.nz>
References: <1578926186767-0.post@n4.nabble.com>
 <82599fa11eeaf0a86cdc90ba5eeb4fbc@schroeffu.ch>
 <1578930353782-0.post@n4.nabble.com>
 <f30e199c-1a09-530f-89a2-2d62a019598f@treenet.co.nz>
Message-ID: <1579062571217-0.post@n4.nabble.com>

Just ask, how do I install the v5 version of squid,
 I used to install it with `yum -y install squid`.





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From robertkwild at gmail.com  Wed Jan 15 04:33:25 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 15 Jan 2020 04:33:25 +0000
Subject: [squid-users] unable to get issuer cert locally protocol error
In-Reply-To: <CAGU_CiJ4j9TpeDzdMOG7pWkuSAymhEgJsZaauf26rt2T04oYzw@mail.gmail.com>
References: <CAGU_CiJ4j9TpeDzdMOG7pWkuSAymhEgJsZaauf26rt2T04oYzw@mail.gmail.com>
Message-ID: <CAGU_CiLC-BLrCUgvegANabHBODb87b868rdExRVbFb5ZUNfEwg@mail.gmail.com>

awesome ive done it!!!!!!!!!!!!!!!!!!!!!!

https://docs.diladele.com/faq/squid/fix_unable_to_get_issuer_cert_locally.html

On Wed, 15 Jan 2020 at 02:59, robert k Wild <robertkwild at gmail.com> wrote:

> hi all,
>
> when im trying to go on a web page, squid cant connect and gives me an
> error page -
>
> The system returned:
> (71) Protocol error (TLS code:
> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>
> SSL Certficate error: certificate issuer (CA) not known: /C=US/O=Let's
> Encrypt/CN=Let's Encrypt Authority X3
>
> This proxy and the remote host failed to negotiate a mutually acceptable
> security settings for handling your request. It is possible that the remote
> host does not support secure connections, or the proxy is not satisfied
> with the host security credentials.
> does anyone know what the problem is
>
> cheers,
>
> rob
>
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/32dbf929/attachment.htm>

From rafael.akchurin at diladele.com  Wed Jan 15 06:40:06 2020
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 15 Jan 2020 06:40:06 +0000
Subject: [squid-users] unable to get issuer cert locally protocol error
In-Reply-To: <CAGU_CiJ4j9TpeDzdMOG7pWkuSAymhEgJsZaauf26rt2T04oYzw@mail.gmail.com>
References: <CAGU_CiJ4j9TpeDzdMOG7pWkuSAymhEgJsZaauf26rt2T04oYzw@mail.gmail.com>
Message-ID: <A6DFB949-E820-44C4-83B6-0EBF81234ABF@diladele.com>

Hello Robert,

See why it happens - https://docs.diladele.com/faq/squid/fix_unable_to_get_issuer_cert_locally.html

Best regards,
Rafael Akchurin

On 15 Jan 2020, at 03:59, robert k Wild <robertkwild at gmail.com> wrote:

?
hi all,

when im trying to go on a web page, squid cant connect and gives me an error page -

The system returned:

(71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

SSL Certficate error: certificate issuer (CA) not known: /C=US/O=Let's Encrypt/CN=Let's Encrypt Authority X3

This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.

does anyone know what the problem is

cheers,

rob


--
Regards,

Robert K Wild.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/d4dedac7/attachment.htm>

From fagha at olfeo.com  Wed Jan 15 16:43:34 2020
From: fagha at olfeo.com (Farid Agha)
Date: Wed, 15 Jan 2020 17:43:34 +0100
Subject: [squid-users] Caching content-ype : video/MP2T
Message-ID: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>

Hi all, 

After reading archives and FAQ, I send this email.  

My main
goal is to cache segments of a live video. The content-type is
video/MP2T and the extension is .ts 

I'm actually in explicit proxy and
the cache is working fine for all files except this content-type. 


Here after my configuration : 

# Begin "Cache
Rules"
maximum_object_size_in_memory 2048 KB
memory_replacement_policy
lru
cache_mem 512 MB 

maximum_object_size 32768 KB
minimum_object_size
1 KB
cache_replacement_policy lru
cache_dir ufs
/opt/olfeo/data/proxy/cache 5000 16 256 

cache_swap_low
85
cache_swap_high 95 

delay_pools 1
acl acl_bw_0_categorylist note
urlgroup theme_5
delay_class 1 3
delay_parameters 1 10000/20000 -1/-1
5000/10000
delay_access 1 allow acl_bw_0_categorylist
delay_access 1
deny all 

#refresh_pattern ^ftp: 1440 20% 10080
#refresh_pattern
^gopher: 1440 0% 1440
refresh_pattern . 0 100% 4320 ignore-no-store


request_header_access Cache-Control deny all
cache allow all 

My
squid version is : 4.4 

An access.log portion :  

1579103694.775 42
10.12.0.6 TCP_MISS/200 1154117 GET
http://xxx.xxx.net/i/2_1 at 799489/segment157910368_700_av-p.ts? -
HIER_DIRECT/104.123.50.35 video/MP2T
1579103704.762 42 10.12.0.6
TCP_MISS/200 1223301 GET
http://xxx.xxx.net/i/2_1 at 799489/segment157910369_700_av-p.ts? -
HIER_DIRECT/104.123.50.35 video/MP2T
1579103714.688 70 10.12.0.6
TCP_MISS/200 1205253 GET
http://xxx.xxx.net/i/2_1 at 799489/segment157910370_700_av-p.ts? -
HIER_DIRECT/104.123.50.35 video/MP2T

Somebody has experience on that,
is that possible ? 

Thank you, 

Farid 

Do you see any error, is that
for you possible ? 

Thanks,
BR

  


Farid Agha 
Directeur du Customer Success
t : +33184177173 m : +33636497378
e : fagha at olfeo.com [mailto:%emailaddress%] w : www.olfeo.com [https://www.olfeo.com/]
4 rue de Ventadour, 75001 Paris
[http://www.linkedin.com/company/olfeo?trk=hb_tab_compy_id_234636] [https://twitter.com/olfeo] [http://www.youtube.com/user/OlfeoTV?feature=mhee] [https://www.facebook.com/societeolfeo]
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/034adb5d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: x-disclaimer-649115605-0.png
Type: image/png
Size: 23421 bytes
Desc: x-disclaimer-649115605-0.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/034adb5d/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: x-disclaimer-649115605-1.png
Type: image/png
Size: 6498 bytes
Desc: x-disclaimer-649115605-1.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/034adb5d/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: x-disclaimer-649115605-2.png
Type: image/png
Size: 6249 bytes
Desc: x-disclaimer-649115605-2.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/034adb5d/attachment-0002.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: x-disclaimer-649115605-3.png
Type: image/png
Size: 3936 bytes
Desc: x-disclaimer-649115605-3.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/034adb5d/attachment-0003.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: x-disclaimer-649115605-4.png
Type: image/png
Size: 7179 bytes
Desc: x-disclaimer-649115605-4.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200115/034adb5d/attachment-0004.png>

From Anton.Kornexl at uni-passau.de  Thu Jan 16 08:06:09 2020
From: Anton.Kornexl at uni-passau.de (Kornexl, Anton)
Date: Thu, 16 Jan 2020 08:06:09 +0000
Subject: [squid-users] Squid access.log
Message-ID: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>

Hello 

 

I see many requests with CONNECT https:443 in my access.log

 

How are these entries triggered?

They produce errors in some accounting scripts

 

 

Kind regards

Anton Kornexl

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/b34910d7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 6394 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/b34910d7/attachment.bin>

From robertjm at hockeyhockeyhockey.com  Thu Jan 16 08:30:00 2020
From: robertjm at hockeyhockeyhockey.com (Robert Marshall)
Date: Thu, 16 Jan 2020 00:30:00 -0800
Subject: [squid-users] Need help setting up DD-WRT router to use Squid as a
	transparent proxy
Message-ID: <CABtemdEWzsGTze=cL6WE6PxUOL8_0XmbOZaBt5DHj9Q7trjvCA@mail.gmail.com>

Hi all,

I'm trying to set up a transparent proxy on my network so that all devices
are forced to use Squid/SquidGuard for network traffic, and can filter out
undesirable destinations.

I have Squid/SquidGuard running on a Raspberry Pi 4, running the latest
release of Raspian Buster. The route is a D-Link DIR-860L, flashed with the
01/14/20 build of DD-WRT. I tried using the instructions at DD-WRT. But, am
running into problems.

Squid/SquidGuard works fine if I enter in a manual proxy in my Firefox
browser. However, when I go to configure my router's settings I have
problems. The error message that's coming up says that I'm passing an
invalid URL, and the only thing that shows in the error message is the /.
This is happening on ALL webpages I try and go to, not just the ones which
SquidGuard is set to filter out.

Helpful hints or directions will be greatly appreciated!

Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/818693af/attachment.htm>

From rafael.akchurin at diladele.com  Thu Jan 16 08:34:31 2020
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 16 Jan 2020 08:34:31 +0000
Subject: [squid-users] Need help setting up DD-WRT router to use Squid
 as a	transparent proxy
In-Reply-To: <CABtemdEWzsGTze=cL6WE6PxUOL8_0XmbOZaBt5DHj9Q7trjvCA@mail.gmail.com>
References: <CABtemdEWzsGTze=cL6WE6PxUOL8_0XmbOZaBt5DHj9Q7trjvCA@mail.gmail.com>
Message-ID: <AM0PR04MB4753B71F3F7D7CC2E7F41B228F360@AM0PR04MB4753.eurprd04.prod.outlook.com>

You can try policy based routing if DD-WRT supports that ? see https://docs.diladele.com/tutorials/policy_based_routing_squid/index.html

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Robert Marshall
Sent: Thursday, 16 January 2020 09:30
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Need help setting up DD-WRT router to use Squid as a transparent proxy

Hi all,

I'm trying to set up a transparent proxy on my network so that all devices are forced to use Squid/SquidGuard for network traffic, and can filter out undesirable destinations.

I have Squid/SquidGuard running on a Raspberry Pi 4, running the latest release of Raspian Buster. The route is a D-Link DIR-860L, flashed with the 01/14/20 build of DD-WRT. I tried using the instructions at DD-WRT. But, am running into problems.

Squid/SquidGuard works fine if I enter in a manual proxy in my Firefox browser. However, when I go to configure my router's settings I have problems. The error message that's coming up says that I'm passing an invalid URL, and the only thing that shows in the error message is the /. This is happening on ALL webpages I try and go to, not just the ones which SquidGuard is set to filter out.

Helpful hints or directions will be greatly appreciated!

Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/0476c8a9/attachment.htm>

From uhlar at fantomas.sk  Thu Jan 16 08:42:35 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 16 Jan 2020 09:42:35 +0100
Subject: [squid-users] Squid access.log
In-Reply-To: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
Message-ID: <20200116084235.GB12509@fantomas.sk>

On 16.01.20 08:06, Kornexl, Anton wrote:
>I see many requests with CONNECT https:443 in my access.log
>
>How are these entries triggered?
>
>They produce errors in some accounting scripts

what do you mean, triggered and what accounting scripts?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I intend to live forever - so far so good.


From rousskov at measurement-factory.com  Thu Jan 16 14:08:06 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Jan 2020 09:08:06 -0500
Subject: [squid-users] Squid access.log
In-Reply-To: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
Message-ID: <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>

On 1/16/20 3:06 AM, Kornexl, Anton wrote:
> I see many requests with CONNECT https:443 in my access.log

> How are these entries triggered?

These records are logged when your Squid is done with an HTTP CONNECT
tunnel or after Squid intercepts a TLS connection. In very broad terms,
they are a sign that your Squid participates in HTTPS transactions.
Normally, there should be more than "https:443" in those CONNECT records.


> They produce errors in some accounting scripts

Consider either fixing the scripts or, if losing information about
CONNECT tunnels is acceptable to your accounting, filtering CONNECT
records out before giving the logs to the scripts.

You can also configure Squid to stop logging CONNECT transactions (using
access_log ACLs), but I do not recommend hiding the truth that may be
critical in a triage.


HTH,

Alex.


From robertkwild at gmail.com  Thu Jan 16 14:59:06 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 16 Jan 2020 14:59:06 +0000
Subject: [squid-users] follow_x_forwarded_for to get client ip instead of
	sibling proxy
Message-ID: <CAGU_CiJYaeS+NZ=4opQDvigr7SdZdGBd8s-K-gvg6UD7cDhgGA@mail.gmail.com>

hi all,

i have two proxies (one sibling going to a parent)

when i look at the parent proxy access logs, it just logs the ip address of
the sibling proxy

if i add the lines below in my sibling proxy

acl localhost src 127.0.0.1
acl my_other_proxy srcdomain .proxy.example.com
follow_x_forwarded_for allow localhost
follow_x_forwarded_for allow my_other_proxy

when i next look at the logs, will it show the ip of my clients?

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/5b85e82f/attachment.htm>

From squid.org at bloms.de  Thu Jan 16 16:02:21 2020
From: squid.org at bloms.de (Dieter Bloms)
Date: Thu, 16 Jan 2020 17:02:21 +0100
Subject: [squid-users] sslbump with squid 4.9 and websockets doesn't work
Message-ID: <20200116160221.d6pj6lhmktd5gdhv@bloms.de>

Hello,

I use squid 4.9 with enabled sslbump and it works great for the most
websites.

There are some websites, which use websockets like web.whatsapp.com
and can not be reached with enabled sslbump.
When I exclude this destination from sslbump, I get the qrcode, which
can be scanned with the smartphone.
But if I've enabled sslbump, the qrcode doesn't appear and the browser
seems to hang.

The Debugging window of my chrome browser reports stalled access to the uri
wss://web.whatsapp.com/ws

Does anybody know how to enable wss support in squid, so the website can
be reached even sslbump is enabled ?

I know, that I can disable sslbump for his site, but there are more and
more site, which uses websockets wss://
So I want to use an generic solution, without putting them one by one in
a list.

Thank you very much.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From rousskov at measurement-factory.com  Thu Jan 16 16:47:54 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Jan 2020 11:47:54 -0500
Subject: [squid-users] follow_x_forwarded_for to get client ip instead
 of sibling proxy
In-Reply-To: <CAGU_CiJYaeS+NZ=4opQDvigr7SdZdGBd8s-K-gvg6UD7cDhgGA@mail.gmail.com>
References: <CAGU_CiJYaeS+NZ=4opQDvigr7SdZdGBd8s-K-gvg6UD7cDhgGA@mail.gmail.com>
Message-ID: <b9207af5-5aa6-b939-01a6-de18b4cd1d96@measurement-factory.com>

On 1/16/20 9:59 AM, robert k Wild wrote:

> i have two proxies (one sibling going to a parent)

FYI: "siblings" are proxies that fetch hits from each other. The proxy
"going to the parent" is usually called a "child" proxy:

    clients -> child -> parent -> servers


> when i look at the parent proxy access logs, it just logs the ip address
> of the sibling proxy
> 
> if i add the lines below in my sibling proxy
> 
> acl localhost src 127.0.0.1
> acl my_other_proxy srcdomain .proxy.example.com
> follow_x_forwarded_for allow localhost
> follow_x_forwarded_for allow my_other_proxy
> 
> when i next look at the logs, will it show the ip of my clients?


No, it will not (by default) AFAICT. For the parent proxy logs to
contain IP addresses of the clients,

a) The child proxy must send the X-Forwarded-For header to the parent.
b) The parent proxy must trust X-Forwarded-For received from the child
   (as far as logging is concerned).

Your configuration changes at the child proxy do neither (a) nor (b).

IIRC, (a) will happen by default, while (b) requires
follow_x_forwarded_for and log_uses_indirect_client rules at the parent
proxy.

 I did not review your follow_x_forwarded_for rules.

The follow_x_forwarded_for rules at the child proxy are needed if and
only if you want the child proxy to trust the X-Forwarded-For headers
received by that child proxy (from its clients). That is only necessary
in deeper hierarchies:

    clients -> child1 -> child2 -> parent

Alex.


From rousskov at measurement-factory.com  Thu Jan 16 16:56:04 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Jan 2020 11:56:04 -0500
Subject: [squid-users] sslbump with squid 4.9 and websockets doesn't work
In-Reply-To: <20200116160221.d6pj6lhmktd5gdhv@bloms.de>
References: <20200116160221.d6pj6lhmktd5gdhv@bloms.de>
Message-ID: <ee559b80-950c-03e7-5844-f048b8c9bdf5@measurement-factory.com>

On 1/16/20 11:02 AM, Dieter Bloms wrote:

> Does anybody know how to enable wss support in squid, so the website can
> be reached even sslbump is enabled ?

If the service switches to websocket using an HTTP Upgrade mechanism,
then you need at least https://github.com/squid-cache/squid/pull/481

If the service uses websocket without HTTP Upgrade negotiation, then you
need on_unsupported_protocol (with some way of detecting that service
traffic if you do not want to tunnel everything that is not HTTP(S)).

HTH,

Alex.


From robertkwild at gmail.com  Thu Jan 16 17:05:07 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 16 Jan 2020 17:05:07 +0000
Subject: [squid-users] follow_x_forwarded_for to get client ip instead
 of sibling proxy
In-Reply-To: <b9207af5-5aa6-b939-01a6-de18b4cd1d96@measurement-factory.com>
References: <CAGU_CiJYaeS+NZ=4opQDvigr7SdZdGBd8s-K-gvg6UD7cDhgGA@mail.gmail.com>
 <b9207af5-5aa6-b939-01a6-de18b4cd1d96@measurement-factory.com>
Message-ID: <CAGU_CiJApfNE9rQrPhpsJUNaLFa6FVf=U9CGOk0XWOsaJHCpXA@mail.gmail.com>

hi Alex,

thanks for the notes

so my child proxy, i have added -

#forward clients IP
forwarded_for on

and my parent -

acl my_other_proxy srcdomain 10.110.130.80
follow_x_forwarded_for allow my_other_proxy
log_uses_indirect_client on

but in my parent logs, im still getting the ip of the child proxy?

On Thu, 16 Jan 2020 at 16:47, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/16/20 9:59 AM, robert k Wild wrote:
>
> > i have two proxies (one sibling going to a parent)
>
> FYI: "siblings" are proxies that fetch hits from each other. The proxy
> "going to the parent" is usually called a "child" proxy:
>
>     clients -> child -> parent -> servers
>
>
> > when i look at the parent proxy access logs, it just logs the ip address
> > of the sibling proxy
> >
> > if i add the lines below in my sibling proxy
> >
> > acl localhost src 127.0.0.1
> > acl my_other_proxy srcdomain .proxy.example.com
> > follow_x_forwarded_for allow localhost
> > follow_x_forwarded_for allow my_other_proxy
> >
> > when i next look at the logs, will it show the ip of my clients?
>
>
> No, it will not (by default) AFAICT. For the parent proxy logs to
> contain IP addresses of the clients,
>
> a) The child proxy must send the X-Forwarded-For header to the parent.
> b) The parent proxy must trust X-Forwarded-For received from the child
>    (as far as logging is concerned).
>
> Your configuration changes at the child proxy do neither (a) nor (b).
>
> IIRC, (a) will happen by default, while (b) requires
> follow_x_forwarded_for and log_uses_indirect_client rules at the parent
> proxy.
>
>  I did not review your follow_x_forwarded_for rules.
>
> The follow_x_forwarded_for rules at the child proxy are needed if and
> only if you want the child proxy to trust the X-Forwarded-For headers
> received by that child proxy (from its clients). That is only necessary
> in deeper hierarchies:
>
>     clients -> child1 -> child2 -> parent
>
> Alex.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/7e5d9150/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 16 17:11:22 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Jan 2020 12:11:22 -0500
Subject: [squid-users] follow_x_forwarded_for to get client ip instead
 of sibling proxy
In-Reply-To: <CAGU_CiJApfNE9rQrPhpsJUNaLFa6FVf=U9CGOk0XWOsaJHCpXA@mail.gmail.com>
References: <CAGU_CiJYaeS+NZ=4opQDvigr7SdZdGBd8s-K-gvg6UD7cDhgGA@mail.gmail.com>
 <b9207af5-5aa6-b939-01a6-de18b4cd1d96@measurement-factory.com>
 <CAGU_CiJApfNE9rQrPhpsJUNaLFa6FVf=U9CGOk0XWOsaJHCpXA@mail.gmail.com>
Message-ID: <a71c7bbc-33a5-4cb9-6278-b8f70272fd18@measurement-factory.com>

On 1/16/20 12:05 PM, robert k Wild wrote:
> hi Alex,
> 
> thanks for the notes
> 
> so my child proxy, i have added -
> 
> #forward clients IP
> forwarded_for on


FYI: This is the default.


> and my parent -
> 
> acl my_other_proxy srcdomain 10.110.130.80

If you identify your child proxy by its IP, then use src instead of
srcdomain.

Alex.


> follow_x_forwarded_for allow my_other_proxy
> log_uses_indirect_client on
> 
> but in my parent logs, im still getting the ip of the child proxy?



> On Thu, 16 Jan 2020 at 16:47, Alex Rousskov wrote:
> 
>     On 1/16/20 9:59 AM, robert k Wild wrote:
> 
>     > i have two proxies (one sibling going to a parent)
> 
>     FYI: "siblings" are proxies that fetch hits from each other. The proxy
>     "going to the parent" is usually called a "child" proxy:
> 
>     ? ? clients -> child -> parent -> servers
> 
> 
>     > when i look at the parent proxy access logs, it just logs the ip
>     address
>     > of the sibling proxy
>     >
>     > if i add the lines below in my sibling proxy
>     >
>     > acl localhost src 127.0.0.1
>     > acl my_other_proxy srcdomain .proxy.example.com
>     <http://proxy.example.com>
>     > follow_x_forwarded_for allow localhost
>     > follow_x_forwarded_for allow my_other_proxy
>     >
>     > when i next look at the logs, will it show the ip of my clients?
> 
> 
>     No, it will not (by default) AFAICT. For the parent proxy logs to
>     contain IP addresses of the clients,
> 
>     a) The child proxy must send the X-Forwarded-For header to the parent.
>     b) The parent proxy must trust X-Forwarded-For received from the child
>     ? ?(as far as logging is concerned).
> 
>     Your configuration changes at the child proxy do neither (a) nor (b).
> 
>     IIRC, (a) will happen by default, while (b) requires
>     follow_x_forwarded_for and log_uses_indirect_client rules at the parent
>     proxy.
> 
>     ?I did not review your follow_x_forwarded_for rules.
> 
>     The follow_x_forwarded_for rules at the child proxy are needed if and
>     only if you want the child proxy to trust the X-Forwarded-For headers
>     received by that child proxy (from its clients). That is only necessary
>     in deeper hierarchies:
> 
>     ? ? clients -> child1 -> child2 -> parent
> 
>     Alex.
> 
> 
> 
> -- 
> Regards,
> 
> Robert K Wild.



From robertkwild at gmail.com  Thu Jan 16 17:23:18 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 16 Jan 2020 17:23:18 +0000
Subject: [squid-users] follow_x_forwarded_for to get client ip instead
 of sibling proxy
In-Reply-To: <a71c7bbc-33a5-4cb9-6278-b8f70272fd18@measurement-factory.com>
References: <CAGU_CiJYaeS+NZ=4opQDvigr7SdZdGBd8s-K-gvg6UD7cDhgGA@mail.gmail.com>
 <b9207af5-5aa6-b939-01a6-de18b4cd1d96@measurement-factory.com>
 <CAGU_CiJApfNE9rQrPhpsJUNaLFa6FVf=U9CGOk0XWOsaJHCpXA@mail.gmail.com>
 <a71c7bbc-33a5-4cb9-6278-b8f70272fd18@measurement-factory.com>
Message-ID: <CAGU_CiLxK8zeu+y2H0-+QNUf4Djmy+ow_7iq7YK-0sj99=et4w@mail.gmail.com>

Alex, you are AWESOME!!!!!!!!!!!!!!!!!!

On Thu, 16 Jan 2020 at 17:11, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/16/20 12:05 PM, robert k Wild wrote:
> > hi Alex,
> >
> > thanks for the notes
> >
> > so my child proxy, i have added -
> >
> > #forward clients IP
> > forwarded_for on
>
>
> FYI: This is the default.
>
>
> > and my parent -
> >
> > acl my_other_proxy srcdomain 10.110.130.80
>
> If you identify your child proxy by its IP, then use src instead of
> srcdomain.
>
> Alex.
>
>
> > follow_x_forwarded_for allow my_other_proxy
> > log_uses_indirect_client on
> >
> > but in my parent logs, im still getting the ip of the child proxy?
>
>
>
> > On Thu, 16 Jan 2020 at 16:47, Alex Rousskov wrote:
> >
> >     On 1/16/20 9:59 AM, robert k Wild wrote:
> >
> >     > i have two proxies (one sibling going to a parent)
> >
> >     FYI: "siblings" are proxies that fetch hits from each other. The
> proxy
> >     "going to the parent" is usually called a "child" proxy:
> >
> >         clients -> child -> parent -> servers
> >
> >
> >     > when i look at the parent proxy access logs, it just logs the ip
> >     address
> >     > of the sibling proxy
> >     >
> >     > if i add the lines below in my sibling proxy
> >     >
> >     > acl localhost src 127.0.0.1
> >     > acl my_other_proxy srcdomain .proxy.example.com
> >     <http://proxy.example.com>
> >     > follow_x_forwarded_for allow localhost
> >     > follow_x_forwarded_for allow my_other_proxy
> >     >
> >     > when i next look at the logs, will it show the ip of my clients?
> >
> >
> >     No, it will not (by default) AFAICT. For the parent proxy logs to
> >     contain IP addresses of the clients,
> >
> >     a) The child proxy must send the X-Forwarded-For header to the
> parent.
> >     b) The parent proxy must trust X-Forwarded-For received from the
> child
> >        (as far as logging is concerned).
> >
> >     Your configuration changes at the child proxy do neither (a) nor (b).
> >
> >     IIRC, (a) will happen by default, while (b) requires
> >     follow_x_forwarded_for and log_uses_indirect_client rules at the
> parent
> >     proxy.
> >
> >      I did not review your follow_x_forwarded_for rules.
> >
> >     The follow_x_forwarded_for rules at the child proxy are needed if and
> >     only if you want the child proxy to trust the X-Forwarded-For headers
> >     received by that child proxy (from its clients). That is only
> necessary
> >     in deeper hierarchies:
> >
> >         clients -> child1 -> child2 -> parent
> >
> >     Alex.
> >
> >
> >
> > --
> > Regards,
> >
> > Robert K Wild.
>
>

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200116/6937f29b/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 16 19:59:18 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Jan 2020 08:59:18 +1300
Subject: [squid-users] Squid access.log
In-Reply-To: <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
 <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>
Message-ID: <8d1dddf8-b2ef-78e1-3df1-9c612a28b72c@treenet.co.nz>

On 17/01/20 3:08 am, Alex Rousskov wrote:
> On 1/16/20 3:06 AM, Kornexl, Anton wrote:
>> I see many requests with CONNECT https:443 in my access.log
> 
>> How are these entries triggered?
> 
> These records are logged when your Squid is done with an HTTP CONNECT
> tunnel or after Squid intercepts a TLS connection. In very broad terms,
> they are a sign that your Squid participates in HTTPS transactions.
> Normally, there should be more than "https:443" in those CONNECT records.
> 

This particular "https:443" happens when people use SquidGuard or
similarly broken redirector to tell Squid the *URI* (hostname:443) of a
CONNECT tunnel is a *URL* (https://hostname:443[path])..

If this is your case, fix the redirector or use this:

 uri_rewrite_access deny CONNECT


Amos


From Anton.Kornexl at uni-passau.de  Thu Jan 16 20:39:02 2020
From: Anton.Kornexl at uni-passau.de (Kornexl, Anton)
Date: Thu, 16 Jan 2020 20:39:02 +0000
Subject: [squid-users] Squid access.log
In-Reply-To: <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
 <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>
Message-ID: <c6d05344b0614e38a66788e6b3551747@ads.uni-passau.de>

I use squid 4.9 on  OpenSuse 15.1
Almost all https-Requests are logged with https:443

1579204357.578      1 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204358.623      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204358.672      1 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204358.677      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204358.680      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204359.261      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204360.227   8766 1.2.3.4 TCP_TUNNEL/200 47056 CONNECT 3c.web.de:443 - HIER_DIRECT/217.72.196.68 -
1579204363.236      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204377.895  16489 1.2.3.4 TCP_TUNNEL/200 3851 CONNECT t.uimserv.net:443 - HIER_DIRECT/195.20.250.183 -
1579204381.210      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204381.960      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
1579204383.712   8416 1.2.3.4 TCP_TUNNEL/200 8409 CONNECT 3c.web.de:443 - HIER_DIRECT/217.72.196.68 -
1579204396.847  45930 1.2.3.4 TCP_TUNNEL/200 77063 CONNECT adimg.uimserv.net:443 - HIER_DIRECT/23.210.249.45 -

Only some https-Requests get logged with a useful line
I don't use SSLBump

I have logged the traffic in a haproxy in front of this squid:
These requests
2020-01-16T20:59:28+01:00 Jufi haproxy[1796]: 1.2.3.4:20711 [16/Jan/2020:20:59:28.656] squid squidservers/squidserver1 0/0/0/3/3 503 4252 - - ---- 12/12/11/3/0 0/0 "CONNECT incoming.telemetry.mozilla.org:443 HTTP/1.1"
2020-01-16T20:59:34+01:00 Jufi haproxy[1796]: 1.2.3.4:30065 [16/Jan/2020:20:59:34.226] squid squidservers/squidserver1 0/0/0/1/1 503 4252 - - ---- 13/13/12/3/0 0/0 "CONNECT incoming.telemetry.mozilla.org:443 HTTP/1.1"
2020-01-16T21:01:14+01:00 Jufi haproxy[1796]: 1.2.3.4:19521 [16/Jan/2020:21:01:14.892] squid squidservers/squidserver1 0/0/0/2/2 503 4252 - - ---- 22/22/19/9/0 0/0 "CONNECT incoming.telemetry.mozilla.org:443 HTTP/1.1"
2020-01-16T21:01:15+01:00 Jufi haproxy[1796]: 1.2.3.4:31880 [16/Jan/2020:21:01:15.901] squid squidservers/squidserver1 0/0/0/0/0 503 4252 - - ---- 22/22/19/9/0 0/0 "CONNECT incoming.telemetry.mozilla.org:443 HTTP/1.1"

don't show up in access.log (squid)

These requests are logged (with time at the start of the line converted to human readable)
Thu Jan 16 20:59:28 2020      2 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
Thu Jan 16 20:59:34 2020      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
Thu Jan 16 21:01:14 2020      1 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -
Thu Jan 16 21:01:15 2020      0 1.2.3.4 NONE/503 0 CONNECT https:443 - HIER_NONE/- -

Why are some https-requests logged with the correct hostname and no fake CONNECT https:443 and other requests are logged without  correct domain but with fake CONNECT entries

On another system i have squid 3.5.27 (Ubuntu 18.04) 
There are no CONNECT https:443 log lines and all https-requests are logged with CONNECT <hostname>:443 entries. 

Anton Kornexl

-----Urspr?ngliche Nachricht-----
Von: Alex Rousskov <rousskov at measurement-factory.com> 
Gesendet: Donnerstag, 16. Januar 2020 15:08
An: Kornexl, Anton <KORNEXL at ads.uni-passau.de>; 217.252.117.35
Betreff: Re: [squid-users] Squid access.log

On 1/16/20 3:06 AM, Kornexl, Anton wrote::

> I see many requests with CONNECT https:443 in my access.log

> How are these entries triggered?

These records are logged when your Squid is done with an HTTP CONNECT
tunnel or after Squid intercepts a TLS connection. In very broad terms,
they are a sign that your Squid participates in HTTPS transactions.
Normally, there should be more than "https:443" in those CONNECT records.


> They produce errors in some accounting scripts

Consider either fixing the scripts or, if losing information about
CONNECT tunnels is acceptable to your accounting, filtering CONNECT
records out before giving the logs to the scripts.

You can also configure Squid to stop logging CONNECT transactions (using
access_log ACLs), but I do not recommend hiding the truth that may be
critical in a triage.


HTH,

Alex.


From rousskov at measurement-factory.com  Thu Jan 16 21:04:06 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Jan 2020 16:04:06 -0500
Subject: [squid-users] Squid access.log
In-Reply-To: <c6d05344b0614e38a66788e6b3551747@ads.uni-passau.de>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
 <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>
 <c6d05344b0614e38a66788e6b3551747@ads.uni-passau.de>
Message-ID: <6547a0a7-e4bc-84ff-629c-b7c56ebed007@measurement-factory.com>

On 1/16/20 3:39 PM, Kornexl, Anton wrote:

> Why are some https-requests logged with the correct hostname and no
> fake CONNECT https:443 and other requests are logged without correct
> domain but with fake CONNECT entries

For every specific log record, either there is a Squid bug or you are
dealing with some special traffic that you do not know about (e.g., see
Amos response regarding old redirectors causing such weird entries).

Your best bet may be to find out what exactly Squid receives when it
produces a specific unexpected log entry. You can try to do that using
%>handshake logformat code or, if that does not work, using
tcpdump/wireshark/etc. Once properly collected and shared, the folks
here can help you decode the binary handshake blob and, hopefully,
explain what you are seeing.

Alex.


> -----Urspr?ngliche Nachricht-----
> Von: Alex Rousskov <rousskov at measurement-factory.com> 
> Gesendet: Donnerstag, 16. Januar 2020 15:08
> An: Kornexl, Anton <KORNEXL at ads.uni-passau.de>; 217.252.117.35
> Betreff: Re: [squid-users] Squid access.log
> 
> On 1/16/20 3:06 AM, Kornexl, Anton wrote::
> 
>> I see many requests with CONNECT https:443 in my access.log
> 
>> How are these entries triggered?
> 
> These records are logged when your Squid is done with an HTTP CONNECT
> tunnel or after Squid intercepts a TLS connection. In very broad terms,
> they are a sign that your Squid participates in HTTPS transactions.
> Normally, there should be more than "https:443" in those CONNECT records.
> 
> 
>> They produce errors in some accounting scripts
> 
> Consider either fixing the scripts or, if losing information about
> CONNECT tunnels is acceptable to your accounting, filtering CONNECT
> records out before giving the logs to the scripts.
> 
> You can also configure Squid to stop logging CONNECT transactions (using
> access_log ACLs), but I do not recommend hiding the truth that may be
> critical in a triage.
> 
> 
> HTH,
> 
> Alex.
> 



From Anton.Kornexl at uni-passau.de  Thu Jan 16 21:06:25 2020
From: Anton.Kornexl at uni-passau.de (Kornexl, Anton)
Date: Thu, 16 Jan 2020 21:06:25 +0000
Subject: [squid-users] Squid access.log
In-Reply-To: <8d1dddf8-b2ef-78e1-3df1-9c612a28b72c@treenet.co.nz>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
 <8cbb81b0-915c-670d-faf1-441cc2a04ecd@measurement-factory.com>
 <8d1dddf8-b2ef-78e1-3df1-9c612a28b72c@treenet.co.nz>
Message-ID: <f693fe2d03ed48cb8720eebf51229898@ads.uni-passau.de>

Thank you for this INFO

I use ufdbguard with the line
url_rewrite_program /usr/sbin/sgwrapper_ufdb

I had 
redirect-https "https://www.jug.... in the config file for ufdbguard

Removing https:// from this definition  removed the fake CONNECT https:443 entries

Anton Kornexl

-----Urspr?ngliche Nachricht-----
Von: squid-users <squid-users-bounces at lists.squid-cache.org> Im Auftrag von Amos Jeffries
Gesendet: Donnerstag, 16. Januar 2020 20:59
An: squid-users at lists.squid-cache.org
Betreff: Re: [squid-users] Squid access.log

On 17/01/20 3:08 am, Alex Rousskov wrote:
> On 1/16/20 3:06 AM, Kornexl, Anton wrote:

>> I see many requests with CONNECT https:443 in my access.log
> 
>> How are these entries triggered?
> 
> These records are logged when your Squid is done with an HTTP CONNECT
> tunnel or after Squid intercepts a TLS connection. In very broad terms,
> they are a sign that your Squid participates in HTTPS transactions.
> Normally, there should be more than "https:443" in those CONNECT records.
> 

This particular "https:443" happens when people use SquidGuard or
similarly broken redirector to tell Squid the *URI* (hostname:443) of a
CONNECT tunnel is a *URL* (https://hostname:443[path])..

If this is your case, fix the redirector or use this:

 uri_rewrite_access deny CONNECT


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From robertkwild at gmail.com  Fri Jan 17 14:10:02 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 17 Jan 2020 14:10:02 +0000
Subject: [squid-users] whats wrong with this url regex line
Message-ID: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>

hi all,

really struggling with getting the url regex syntax correct, does it accept
normal url paths ie

acl special_url url_regex http://updater.maxon.net/server_test
http_access allow special_client special_url

but it doesnt work?

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200117/52a6616d/attachment.htm>

From robertkwild at gmail.com  Fri Jan 17 14:11:16 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 17 Jan 2020 14:11:16 +0000
Subject: [squid-users] whats wrong with this url regex line
In-Reply-To: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>
References: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>
Message-ID: <CAGU_CiJHe8=mD13KzKJHPvEDES6L4F5ushD_8RHjcvr0P4rOjg@mail.gmail.com>

sorry i meant this -

acl special_url url_regex http://updater.maxon.net/server_test
http_access allow special_url

On Fri, 17 Jan 2020 at 14:10, robert k Wild <robertkwild at gmail.com> wrote:

> hi all,
>
> really struggling with getting the url regex syntax correct, does it
> accept normal url paths ie
>
> acl special_url url_regex http://updater.maxon.net/server_test
> http_access allow special_client special_url
>
> but it doesnt work?
>
> thanks,
> rob
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200117/8f6622b0/attachment.htm>

From robertkwild at gmail.com  Fri Jan 17 14:51:57 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 17 Jan 2020 14:51:57 +0000
Subject: [squid-users] whats wrong with this url regex line
In-Reply-To: <CAGU_CiJHe8=mD13KzKJHPvEDES6L4F5ushD_8RHjcvr0P4rOjg@mail.gmail.com>
References: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>
 <CAGU_CiJHe8=mD13KzKJHPvEDES6L4F5ushD_8RHjcvr0P4rOjg@mail.gmail.com>
Message-ID: <CAGU_CiJxmQQF-+XU_gbVet--Y4hcDC_Xx2E7n88TMGk+3huJ=Q@mail.gmail.com>

smashed it -

acl special_url url_regex ^http://updater.maxon.net/server_test.*
http_access allow special_url

On Fri, 17 Jan 2020 at 14:11, robert k Wild <robertkwild at gmail.com> wrote:

> sorry i meant this -
>
> acl special_url url_regex http://updater.maxon.net/server_test
> http_access allow special_url
>
> On Fri, 17 Jan 2020 at 14:10, robert k Wild <robertkwild at gmail.com> wrote:
>
>> hi all,
>>
>> really struggling with getting the url regex syntax correct, does it
>> accept normal url paths ie
>>
>> acl special_url url_regex http://updater.maxon.net/server_test
>> http_access allow special_client special_url
>>
>> but it doesnt work?
>>
>> thanks,
>> rob
>>
>> --
>> Regards,
>>
>> Robert K Wild.
>>
>
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200117/2285fb60/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 19 09:22:56 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Jan 2020 22:22:56 +1300
Subject: [squid-users] whats wrong with this url regex line
In-Reply-To: <CAGU_CiJxmQQF-+XU_gbVet--Y4hcDC_Xx2E7n88TMGk+3huJ=Q@mail.gmail.com>
References: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>
 <CAGU_CiJHe8=mD13KzKJHPvEDES6L4F5ushD_8RHjcvr0P4rOjg@mail.gmail.com>
 <CAGU_CiJxmQQF-+XU_gbVet--Y4hcDC_Xx2E7n88TMGk+3huJ=Q@mail.gmail.com>
Message-ID: <11017918-9ea2-1b24-b2f6-48a57f02f0cd@treenet.co.nz>

On 18/01/20 3:51 am, robert k Wild wrote:
> smashed it -
> 
> acl special_url url_regex ^http://updater.maxon.net/server_test.*
> http_access allow special_url
> 

It should work without the anchor and suffix. Perhapse the URL is not
actually that string?


 * any (.*) at beginning or end is assumed, so are stripped away by
Squid config loader.

 * the ^ anchor is only necessary to forbid sub-URL matches. eg URLs like:
  http://hello/q?http://updater.maxon.net/server_test

 * any characters within the URL which are special to regex are handled
as those *regex* commands. Not characters in the URL.

 These characters may clash between regex and URL:

    . ? ( ) [ ] - + * \


Amos


From squid3 at treenet.co.nz  Sun Jan 19 09:33:16 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Jan 2020 22:33:16 +1300
Subject: [squid-users] Need help setting up DD-WRT router to use Squid
 as a transparent proxy
In-Reply-To: <CABtemdEWzsGTze=cL6WE6PxUOL8_0XmbOZaBt5DHj9Q7trjvCA@mail.gmail.com>
References: <CABtemdEWzsGTze=cL6WE6PxUOL8_0XmbOZaBt5DHj9Q7trjvCA@mail.gmail.com>
Message-ID: <c422de0e-305e-b37f-1add-05b5af05ea6d@treenet.co.nz>

On 16/01/20 9:30 pm, Robert Marshall wrote:
> Hi all,
> 
> I'm trying to set up a transparent proxy on my network so that all
> devices are forced to use Squid/SquidGuard for network traffic, and can
> filter out undesirable destinations.
> 
> I have Squid/SquidGuard running on a Raspberry Pi 4, running the latest
> release of Raspian Buster. The route is a D-Link DIR-860L, flashed with
> the 01/14/20 build of DD-WRT. I tried using the instructions at DD-WRT.
> But, am running into problems.

What Instructions? If they are telling you to "port forward" or NAT
traffic to wards a separate Squid *machine* they are outdated and now wrong.


<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat>
Note that traffic MUST only have the NAT performed on the Squid machine.
Any use of DNAT (aka port forwarding) results in the problem you are seeing.

Last time I setup devices like DD-WRT the UI only provided a "DMZ
server" option. If you cannot do Policy Routing for only the port 80 or
443 traffic on the DD-WRT device then the DMZ equivalent may be used
instead.
Either way you need correct and separate routing and NAT rules for
traffic arriving at the Squid machine to send the appropriate
connections to Squid and anything else to its proper destination.


Amos


From robertkwild at gmail.com  Sun Jan 19 13:51:16 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Sun, 19 Jan 2020 13:51:16 +0000
Subject: [squid-users] whats wrong with this url regex line
In-Reply-To: <11017918-9ea2-1b24-b2f6-48a57f02f0cd@treenet.co.nz>
References: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>
 <CAGU_CiJHe8=mD13KzKJHPvEDES6L4F5ushD_8RHjcvr0P4rOjg@mail.gmail.com>
 <CAGU_CiJxmQQF-+XU_gbVet--Y4hcDC_Xx2E7n88TMGk+3huJ=Q@mail.gmail.com>
 <11017918-9ea2-1b24-b2f6-48a57f02f0cd@treenet.co.nz>
Message-ID: <CAGU_Ci+nDrTNa=BHCrh+weS0bPzW58bgFe1Pw6G4-+Db35bf7g@mail.gmail.com>

thanks Amos

#allow special URL paths
acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
http_access allow special_url
#
#deny MIME types
acl mimetype rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
http_reply_access allow special_url
http_reply_access deny mimetype

the reason why i added this line

http_reply_access allow special_url

in my mime lines is because

http://updater.maxon.net/server_test
<http://updater.maxon.net/server_test.*>

wants to download an octet stream and it was keep on denying it


On Sun, 19 Jan 2020 at 09:23, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/01/20 3:51 am, robert k Wild wrote:
> > smashed it -
> >
> > acl special_url url_regex ^http://updater.maxon.net/server_test.*
> > http_access allow special_url
> >
>
> It should work without the anchor and suffix. Perhapse the URL is not
> actually that string?
>
>
>  * any (.*) at beginning or end is assumed, so are stripped away by
> Squid config loader.
>
>  * the ^ anchor is only necessary to forbid sub-URL matches. eg URLs like:
>   http://hello/q?http://updater.maxon.net/server_test
>
>  * any characters within the URL which are special to regex are handled
> as those *regex* commands. Not characters in the URL.
>
>  These characters may clash between regex and URL:
>
>     . ? ( ) [ ] - + * \
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200119/3a6867b4/attachment.htm>

From brett.lymn at baesystems.com  Sun Jan 19 22:38:22 2020
From: brett.lymn at baesystems.com (Brett Lymn)
Date: Mon, 20 Jan 2020 09:08:22 +1030
Subject: [squid-users] whats wrong with this url regex line
In-Reply-To: <11017918-9ea2-1b24-b2f6-48a57f02f0cd@treenet.co.nz>
References: <CAGU_CiJxNc9MxZNLz6uy-KBoMXr3cC7q=yjUz-DODXDKPUx65Q@mail.gmail.com>
 <CAGU_CiJHe8=mD13KzKJHPvEDES6L4F5ushD_8RHjcvr0P4rOjg@mail.gmail.com>
 <CAGU_CiJxmQQF-+XU_gbVet--Y4hcDC_Xx2E7n88TMGk+3huJ=Q@mail.gmail.com>
 <11017918-9ea2-1b24-b2f6-48a57f02f0cd@treenet.co.nz>
Message-ID: <20200119223822.GB8745@baea.com.au>

On Sun, Jan 19, 2020 at 10:22:56PM +1300, Amos Jeffries wrote:
> 
> It should work without the anchor and suffix. Perhapse the URL is not
> actually that string?
> 

Yes it will work without the anchor but using an anchor can make the
regex faster because it then does not have to scan the whole string for
a match.  So if you know that the string you are looking for must be
right at the start of the string then it is much better to use the ^
anchor to let the regex library know that.

-- 
Brett Lymn
This email has been sent on behalf of one of the following companies within the BAE Systems Australia group of companies:

BAE Systems Australia Limited - Australian Company Number 008 423 005
BAE Systems Australia Defence Pty Limited - Australian Company Number 006 870 846
ASC Shipbuilding Pty Limited - Australian Company Number 051 899 864

BAE Systems Australia's registered office is Evans Building, Taranaki Road, Edinburgh Parks, Edindurgh, South Australia, 5111.
ASC Shipbuilding's registered office is Level 2, 80 Flinders Street, Adelaide, South Australia, 5000.
If the identity of the sending company is not clear from the content of this email, please contact the sender.

This email and any attachments may contain confidential and legally privileged information. If you are not the intended recipient, do not copy or disclose its content, but please reply to this email immediately and highlight the error to the sender and then immediately delete the message.



From squid3 at treenet.co.nz  Mon Jan 20 02:15:42 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jan 2020 15:15:42 +1300
Subject: [squid-users] Caching content-ype : video/MP2T
In-Reply-To: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>
References: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>
Message-ID: <e97e7310-917b-ac93-38f5-06a5d237c647@treenet.co.nz>

On 16/01/20 5:43 am, Farid Agha wrote:
> Hi all,
> 
> After reading archives and FAQ, I send this email.?
> 
> My main goal is to cache segments of a live video. The content-type is
> video/MP2T and the extension is .ts
> 
> I'm actually in explicit proxy and the cache is working fine for all
> files except this content-type.

Some content is simply not cacheable. The hint there is the word *live*.
Content from a cache is never "live" - it is *old* / past / previous /
outdated / obsolete.


> 
> Here after my configuration :
> 
...
> 
> #refresh_pattern ^ftp: 1440 20% 10080
> #refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern . 0 100% 4320 ignore-no-store

VERY bad idea. 'no-store' and similar HTTP violations should be
performed as conservatively as possible. Only applied as a last resort
to fix a specific sites problems.

> 
> request_header_access Cache-Control deny all

This breaks *other* caches on machines upstream or downstream of yours.
It has no effect on your proxy. Please remove.


> cache allow all

This is the default behaviour for Squid. You can remove the above line.


> 
> My squid version is : 4.4
> 

Please upgrade.


> An access.log portion :?
> 
> 1579103694.775 42 10.12.0.6 TCP_*MISS*/200 1154117 GET
> http://xxx.xxx.net/i/2_1 at 799489/segment157910368_700_av-p.ts? -
> HIER_DIRECT/104.123.50.35 video/MP2T
> 1579103704.762 42 10.12.0.6 TCP_*MISS*/200 1223301 GET
> http://xxx.xxx.net/i/2_1 at 799489/segment157910369_700_av-p.ts? -
> HIER_DIRECT/104.123.50.35 video/MP2T
> 1579103714.688 70 10.12.0.6 TCP_*MISS*/200 1205253 GET
> http://xxx.xxx.net/i/2_1 at 799489/segment157910370_700_av-p.ts? -
> HIER_DIRECT/104.123.50.35 video/MP2T
> 
> Somebody has experience on that, is that possible ?

Difficult to say without the details you have removed.

Take a visit to redbot.org and paste the full URL - include the domain
name and query-string parameters which are not being logged. It will
tell you whether the resource is cacheable and why or why not.


HTH
Amos


From squid3 at treenet.co.nz  Mon Jan 20 02:26:43 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jan 2020 15:26:43 +1300
Subject: [squid-users] reverse proxy
In-Reply-To: <20200111130410.1E80D41BA3@smtp.postman.i2p>
References: <20200111130410.1E80D41BA3@smtp.postman.i2p>
Message-ID: <97948ac3-bec4-bcee-2b05-a04a8fa86402@treenet.co.nz>

On 12/01/20 2:04 am, user wrote:
> Hello.
> I have use squid 4.8 as reverse proxy. The problem is remote (or local?)
> side close connection every 2-4 minutes with message "TCP_MISS_ABORTED/200"
> in log.

That log tag is normal for traffic with Happy Eyeballs operating.
Without extra information there is no sign of an actual problem in your
description.


> 
> Another one problem - downloader report incorrect speed and time.

That is the tool you are using and how it interprets the data transfer.
Nothing to do with Squid.


Amos


From numsys at free.fr  Mon Jan 20 08:19:06 2020
From: numsys at free.fr (FredB)
Date: Mon, 20 Jan 2020 09:19:06 +0100 (CET)
Subject: [squid-users] unsuscribe
Message-ID: <1168506197.6163113.1579508346389.JavaMail.root@zimbra30-e5.priv.proxad.net>



From squid3 at treenet.co.nz  Mon Jan 20 08:52:30 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 20 Jan 2020 21:52:30 +1300
Subject: [squid-users] unsuscribe
In-Reply-To: <1168506197.6163113.1579508346389.JavaMail.root@zimbra30-e5.priv.proxad.net>
References: <1168506197.6163113.1579508346389.JavaMail.root@zimbra30-e5.priv.proxad.net>
Message-ID: <e6f605c8-cc58-f68a-a1d3-f748112e9d50@treenet.co.nz>

Please use the web interface linked from any list email to unsubscribe.
Mailing everyone on-list is not going to help.

Amos


On 20/01/20 9:19 pm, FredB wrote:
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From fagha at olfeo.com  Mon Jan 20 13:27:27 2020
From: fagha at olfeo.com (wrz)
Date: Mon, 20 Jan 2020 07:27:27 -0600 (CST)
Subject: [squid-users] Caching content-ype : video/MP2T
In-Reply-To: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>
References: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>
Message-ID: <1579526847933-0.post@n4.nabble.com>

Hi Amos,

Thank you very much for your advices. 

Our config was of course dirty. We will clean it up. 

I tried redbot but the result is negative. 

What I try to do is the tell the proxy to cache a ressource that can by
cacheable by default. 

Is that for you possible ? 

Thanks,
BR



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Jan 20 13:39:51 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Jan 2020 02:39:51 +1300
Subject: [squid-users] Caching content-ype : video/MP2T
In-Reply-To: <1579526847933-0.post@n4.nabble.com>
References: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>
 <1579526847933-0.post@n4.nabble.com>
Message-ID: <5d1b0d93-8ba5-9160-9df4-2dcac40c6d96@treenet.co.nz>

On 21/01/20 2:27 am, wrz wrote:
> Hi Amos,
> 
> Thank you very much for your advices. 
> 
> Our config was of course dirty. We will clean it up. 
> 
> I tried redbot but the result is negative. 
> 

By negative you mean it says the resource cannot be cached?


> What I try to do is the tell the proxy to cache a ressource that can by
> cacheable by default. 
> 
> Is that for you possible ? 

If the resource is cacheable there is nothing special to do. Squid
caches as much as it can.


Amos


From anon.amish at gmail.com  Tue Jan 21 04:28:15 2020
From: anon.amish at gmail.com (Amish)
Date: Tue, 21 Jan 2020 09:58:15 +0530
Subject: [squid-users] squid and netdata causes squid to drop SYN?
Message-ID: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>

Hello,

Recently, I started using netdata to monitor various system functions 
(which also monitors squid)

I am using squid (v4.9) with transparent (NAT) as well as Proxy mode (on 
different ports). Network has 10-15 users. Some on transparent proxy 
(redirection to port 3128) and some via proxy setting (port 8080) in 
browser.

Netdata calls squidclient every second to fetch squid statistics. (for 
generating per second graphs / statistics)

After I started using netdata, everything worked fine for a while. But 
then many users started complaining that they are not able to access 
sites. (Sometimes it worked and sometimes not.)

I could see SYN packets coming in but there were no SYN,ACK going back. 
I quickly went through cache.log but did not find anything. (searched 
for descriptors). I believe (but not 100% sure) that this happened only 
with those on transparent proxy. (again not sure)

Then I restarted the squid and all was well. But then issue happened 
again and I disabled netdata's squid module and now all is working fine 
from few days.

So I suspect that netdata calling squidclient every second is not a 
right thing to do. Its probably causing denial-of-service on squid.

So:
1) Is there any squid setting which I can adjust? (File descriptors 
available is 16384)
2) Is calling squidclient so frequently a right thing to do by netdata? 
Its probably over loading squid. (I will report to netdata if not)

Please guide,

Thank you,

Regards,

Amish



From uhlar at fantomas.sk  Tue Jan 21 09:32:34 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 21 Jan 2020 10:32:34 +0100
Subject: [squid-users] squid and netdata causes squid to drop SYN?
In-Reply-To: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
References: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
Message-ID: <20200121093234.GA1480@fantomas.sk>

On 21.01.20 09:58, Amish wrote:
>Recently, I started using netdata to monitor various system functions 
>(which also monitors squid)
>
>I am using squid (v4.9) with transparent (NAT) as well as Proxy mode 
>(on different ports). Network has 10-15 users. Some on transparent 
>proxy (redirection to port 3128) and some via proxy setting (port 
>8080) in browser.
>
>Netdata calls squidclient every second to fetch squid statistics. (for 
>generating per second graphs / statistics)
>
>After I started using netdata, everything worked fine for a while. But 
>then many users started complaining that they are not able to access 
>sites. (Sometimes it worked and sometimes not.)
>
>I could see SYN packets coming in but there were no SYN,ACK going 
>back. I quickly went through cache.log but did not find anything. 
>(searched for descriptors). I believe (but not 100% sure) that this 
>happened only with those on transparent proxy. (again not sure)
>
>Then I restarted the squid and all was well. But then issue happened 
>again and I disabled netdata's squid module and now all is working 
>fine from few days.
>
>So I suspect that netdata calling squidclient every second is not a 
>right thing to do. Its probably causing denial-of-service on squid.

very apparently.

>So:
>1) Is there any squid setting which I can adjust? (File descriptors 
>available is 16384)
>2) Is calling squidclient so frequently a right thing to do by 
>netdata? Its probably over loading squid. (I will report to netdata if 
>not)

I would use snmp for that. You surely don't need every variable available.
netdata seems to support SNMP.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
If Barbie is so popular, why do you have to buy her friends?


From skz169 at outlook.com  Tue Jan 21 09:36:25 2020
From: skz169 at outlook.com (Kuznetsov Sergei)
Date: Tue, 21 Jan 2020 09:36:25 +0000
Subject: [squid-users] Fail and empty response instead cached resource
Message-ID: <AM7PR09MB3798C2374CC6543D287111E4800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>

Hello

I run Squid 4.9 in non transparent mode with OPNsense (FreeBSD 
11.2-RELEASE-p16-HBSD FreeBSD 11.2-RELEASE-p16-HBSD 
87a7fc985c3(stable/19.7)? amd64).

Everything works except access to one website.

Squid startup log is clean:

> 2020/01/21 13:56:43 kid1| Startup: Initializing Authentication Schemes ...
> 2020/01/21 13:56:43 kid1| Startup: Initialized Authentication Scheme 
> 'basic'
> 2020/01/21 13:56:43 kid1| Startup: Initialized Authentication Scheme 
> 'digest'
> 2020/01/21 13:56:43 kid1| Startup: Initialized Authentication Scheme 
> 'negotiate'
> 2020/01/21 13:56:43 kid1| Startup: Initialized Authentication Scheme 
> 'ntlm'
> 2020/01/21 13:56:43 kid1| Startup: Initialized Authentication.
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/squid.conf (depth 0)
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/pre-auth/40-snmp.conf (depth 1)
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/pre-auth/dummy.conf (depth 1)
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/pre-auth/parentproxy.conf (depth 1)
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/auth/ProxyUserACL.conf (depth 1)
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/auth/dummy.conf (depth 1)
> 2020/01/21 13:56:43 kid1| Processing Configuration File: 
> /usr/local/etc/squid/post-auth/dummy.conf (depth 1)
> 2020/01/21 13:56:43 kid1| Initializing https:// proxy context
> 2020/01/21 13:56:43 kid1| Logfile: opening log 
> stdio:/var/log/squid/access.log
> 2020/01/21 13:56:43 kid1| Squid plugin modules loaded: 0
> 2020/01/21 13:56:43 kid1| Adaptation support is off.
> 2020/01/21 13:56:43 kid1| Logfile: opening log 
> stdio:/var/log/squid/store.log
> 2020/01/21 13:56:43 kid1| DNS Socket created at [::], FD 9
> 2020/01/21 13:56:43 kid1| DNS Socket created at 0.0.0.0, FD 11
> 2020/01/21 13:56:43 kid1| Adding nameserver 127.0.0.1 from squid.conf
> 2020/01/21 13:56:43 kid1| HTCP Disabled.
> 2020/01/21 13:56:43 kid1| Pinger socket opened on FD 15
> 2020/01/21 13:56:43 kid1| Finished loading MIME types and icons.
> 2020/01/21 13:56:43 kid1| Accepting HTTP Socket connections at 
> local=192.168.0.4:3128 remote=[::] FD 12 flags=9
> 2020/01/21 13:56:43| pinger: Initialising ICMP pinger ...
> 2020/01/21 13:56:43| pinger: ICMP socket opened.
> 2020/01/21 13:56:43| pinger: ICMPv6 socket opened
> 2020/01/21 13:59:54 kid1| Logfile: opening log 
> stdio:/var/squid/cache/netdb.state
> 2020/01/21 13:59:54 kid1| Logfile: closing log 
> stdio:/var/squid/cache/netdb.state
> 2020/01/21 13:59:54 kid1| NETDB state saved; 680 entries, 7 msec


When I make a request for the first time, everything is fine and I get 
correct result.

> curl --proxy 192.168.0.4:3128 -v -O 
> http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js
> * STATE: INIT => CONNECT handle 0x600092d68; line 1491 (connection #-5000)
> * Added connection 0. The cache now contains 1 members
> *?? Trying 192.168.0.4:3128...
> * TCP_NODELAY set
> * STATE: CONNECT => WAITCONNECT handle 0x600092d68; line 1547 
> (connection #0)
> * Connected to 192.168.0.4 (192.168.0.4) port 3128 (#0)
> * STATE: WAITCONNECT => SENDPROTOCONNECT handle 0x600092d68; line 1665 
> (connection #0)
> * Marked for [keep alive]: HTTP default
> * STATE: SENDPROTOCONNECT => DO handle 0x600092d68; line 1685 
> (connection #0)
> > GET http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js HTTP/1.1
> > Host: tehspb.kodeks.ru
> > User-Agent: curl/7.67.0
> > Accept: */*
> > Proxy-Connection: Keep-Alive
> >
> * STATE: DO => DO_DONE handle 0x600092d68; line 1756 (connection #0)
> * STATE: DO_DONE => PERFORM handle 0x600092d68; line 1877 (connection #0)
> * Mark bundle as not supporting multiuse
> * HTTP 1.1 or later with persistent connection
> < HTTP/1.1 200 OK
> < Server: nginx/1.4.6 (Ubuntu)
> < Date: Tue, 21 Jan 2020 07:10:43 GMT
> < Content-Type: text/javascript; charset=UTF-8
> < Content-Length: 72175
> < X-PaperRoute: Node
> < ETag: "800439-72175-1534779018000"
> < Last-Modified: Mon, 20 Aug 2018 15:30:18 GMT
> < Access-Control-Allow-Origin: *
> < Cache-Control: no-cache
> < X-Cache: MISS from opnsense.lan
> < X-Cache-Lookup: MISS from opnsense.lan:3128
> < Via: 1.1 opnsense.lan (squid/4.9)
> < Connection: keep-alive
> <
> { [1460 bytes data]
> * STATE: PERFORM => DONE handle 0x600092d68; line 2067 (connection #0)
> * multi_done
> 100 72175? 100 72175??? 0???? 0?? 175k????? 0 --:--:-- --:--:-- 
> --:--:--? 176k
> * Connection #0 to host 192.168.0.4 left intact
Squid access log:
> 2020/01/21 12:10:43 ??? 184 192.168.0.5 TCP_MISS/200 72647 GET 
> http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js - 
> HIER_DIRECT/5.61.15.55 text/javascript

However, all subsequent requests fail and return an empty response.

> curl --proxy 192.168.0.4:3128 -v -O 
> http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js
> * STATE: INIT => CONNECT handle 0x600092d68; line 1491 (connection #-5000)
> * Added connection 0. The cache now contains 1 members
> *?? Trying 192.168.0.4:3128...
> * TCP_NODELAY set
> * STATE: CONNECT => WAITCONNECT handle 0x600092d68; line 1547 
> (connection #0)
> * Connected to 192.168.0.4 (192.168.0.4) port 3128 (#0)
> * STATE: WAITCONNECT => SENDPROTOCONNECT handle 0x600092d68; line 1665 
> (connection #0)
> * Marked for [keep alive]: HTTP default
> * STATE: SENDPROTOCONNECT => DO handle 0x600092d68; line 1685 
> (connection #0)
> > GET http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js HTTP/1.1
> > Host: tehspb.kodeks.ru
> > User-Agent: curl/7.67.0
> > Accept: */*
> > Proxy-Connection: Keep-Alive
> >
> * STATE: DO => DO_DONE handle 0x600092d68; line 1756 (connection #0)
> * STATE: DO_DONE => PERFORM handle 0x600092d68; line 1877 (connection #0)
> * Mark bundle as not supporting multiuse
> * HTTP 1.1 or later with persistent connection
> < HTTP/1.1 200 OK
> < Server: nginx/1.4.6 (Ubuntu)
> < Date: Tue, 21 Jan 2020 07:14:20 GMT
> < Content-Type: text/javascript; charset=UTF-8
> < Content-Length: 0
> < X-PaperRoute: Node
> < ETag: "800439-72175-1534779018000"
> < Last-Modified: Mon, 20 Aug 2018 15:30:18 GMT
> < Access-Control-Allow-Origin: *
> < Cache-Control: no-cache
> < Age: 0
> < X-Cache: HIT from opnsense.lan
> < X-Cache-Lookup: HIT from opnsense.lan:3128
> < Via: 1.1 opnsense.lan (squid/4.9)
> < Connection: keep-alive
> <
> * Excess found: excess = 986 url = /js/jquery-1.4.2.min.js 
> (zero-length body)
> * STATE: PERFORM => DONE handle 0x600092d68; line 2067 (connection #0)
> * multi_done
> ? 0???? 0??? 0???? 0??? 0???? 0????? 0????? 0 --:--:-- --:--:-- 
> --:--:--???? 0
> * Connection #0 to host 192.168.0.4 left intact
> * Expire cleared (transfer 0x600092d68)

Squid access log:

> 2020/01/21 12:14:20 ??? 102 192.168.0.5 
> TCP_REFRESH_UNMODIFIED_ABORTED/200 65672 GET 
> http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js - 
> HIER_DIRECT/5.61.15.55 text/javascript


Direct access without proxy works so this is not network issue. I 
thought it was a server setup problem but support guy say they have 
thousands of customers and everything's fine :-) Maybe someone has faced 
similar problems?


From squid3 at treenet.co.nz  Tue Jan 21 11:54:51 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jan 2020 00:54:51 +1300
Subject: [squid-users] Fail and empty response instead cached resource
In-Reply-To: <AM7PR09MB3798C2374CC6543D287111E4800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>
References: <AM7PR09MB3798C2374CC6543D287111E4800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>
Message-ID: <84cd067a-c962-4d37-764e-8b69ef759df2@treenet.co.nz>

On 21/01/20 10:36 pm, Kuznetsov Sergei wrote:
> Hello
> 
> I run Squid 4.9 in non transparent mode with OPNsense (FreeBSD 
> 11.2-RELEASE-p16-HBSD FreeBSD 11.2-RELEASE-p16-HBSD 
> 87a7fc985c3(stable/19.7)? amd64).
> 
> Everything works except access to one website.
> 

Which is broken. More on that below...

> 
> When I make a request for the first time, everything is fine and I get 
> correct result.
> 
...
>> < HTTP/1.1 200 OK
>> < Server: nginx/1.4.6 (Ubuntu)
>> < Date: Tue, 21 Jan 2020 07:10:43 GMT
>> < Content-Type: text/javascript; charset=UTF-8
>> < Content-Length: 72175
>> < X-PaperRoute: Node
>> < ETag: "800439-72175-1534779018000"
>> < Last-Modified: Mon, 20 Aug 2018 15:30:18 GMT
>> < Access-Control-Allow-Origin: *
>> < Cache-Control: no-cache
>> < X-Cache: MISS from opnsense.lan
>> < X-Cache-Lookup: MISS from opnsense.lan:3128
>> < Via: 1.1 opnsense.lan (squid/4.9)
>> < Connection: keep-alive
>> <

The full object contains "Cache-Control: no-cache" which requires a
revalidate/refresh before the object can be used from cache.


> However, all subsequent requests fail and return an empty response.
> 


> Squid access log:
>
>> 2020/01/21 12:14:20     102 192.168.0.5
>> TCP_REFRESH_UNMODIFIED_ABORTED/200 65672 GET
>> http://tehspb.kodeks.ru/js/jquery-1.4.2.min.js -
>> HIER_DIRECT/5.61.15.55 text/javascript
>

... access.log indicates that Squid performed the revalidate (aka
refresh) on the content.


When the revalidation request is sent, the origin server updates the
HTTP headers to say that the object is now 0 bytes in length:

    HTTP/1.1 304 Not Modified
    Server: nginx/1.4.6 (Ubuntu)
    Date: Tue, 21 Jan 2020 11:27:17 GMT
    Content-Type: text/javascript; charset=UTF-8
=>  Content-Length: 0
    Connection: keep-alive
    X-PaperRoute: Node
    ETag: "800439-72175-1534779018000"
    last-modified: Mon, 20 Aug 2018 15:30:18 GMT
    Access-Control-Allow-Origin: *
    Cache-Control: no-cache


So Squid is relaying that change to the clients :

>> < HTTP/1.1 200 OK
>> < Server: nginx/1.4.6 (Ubuntu)
>> < Date: Tue, 21 Jan 2020 07:14:20 GMT
>> < Content-Type: text/javascript; charset=UTF-8
>> < Content-Length: 0

==> ^^

>> < X-PaperRoute: Node
>> < ETag: "800439-72175-1534779018000"
>> < Last-Modified: Mon, 20 Aug 2018 15:30:18 GMT
>> < Access-Control-Allow-Origin: *
>> < Cache-Control: no-cache
>> < Age: 0
>> < X-Cache: HIT from opnsense.lan
>> < X-Cache-Lookup: HIT from opnsense.lan:3128
>> < Via: 1.1 opnsense.lan (squid/4.9)
>> < Connection: keep-alive
>> <
>> * Excess found: excess = 986 url = /js/jquery-1.4.2.min.js 
>> (zero-length body)




Amos


From fagha at olfeo.com  Tue Jan 21 12:38:26 2020
From: fagha at olfeo.com (wrz)
Date: Tue, 21 Jan 2020 06:38:26 -0600 (CST)
Subject: [squid-users] Caching content-ype : video/MP2T
In-Reply-To: <5d1b0d93-8ba5-9160-9df4-2dcac40c6d96@treenet.co.nz>
References: <9ec86a3ad64e58e0a4348110d81725ff@olfeo.bm>
 <1579526847933-0.post@n4.nabble.com>
 <5d1b0d93-8ba5-9160-9df4-2dcac40c6d96@treenet.co.nz>
Message-ID: <1579610306582-0.post@n4.nabble.com>

Hi Amos,

That's right, it says that the resource cannot be cached. 

Is that for you strict or can I enforce caching ?

Thanks,
Farid



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Jan 21 15:39:03 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Jan 2020 10:39:03 -0500
Subject: [squid-users] squid and netdata causes squid to drop SYN?
In-Reply-To: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
References: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
Message-ID: <124e34d2-6c5d-6190-e6a0-21d5d2a69117@measurement-factory.com>

On 1/20/20 11:28 PM, Amish wrote:

> 2) Is calling squidclient so frequently a right thing to do by netdata?

The answer depends on what cache manager query (or queries) your netdata
is sending to Squid. Sending some queries every second is perfectly
fine, but there are other, "heavy" queries that should not be sent so
often and could, if sent with a high enough concurrency level,
effectively DoS a Squid instance. For example, queries that require
iterating all cached objects should not be sent to busy Squids.

If netdata does not document the queries it uses, you can probably use
Squid access.log to figure out what queries netdata is sending (and how
long they take).


N.B. If netdata is killing the previous query when starting a new
would-be-concurrent query, then there should be no DoS conditions -- a
single "heavy" query may slow Squid down a bit but should not stall the
whole Squid instance. Thus, if netdata ensures that the number of
concurrent cache manager queries is small, then there may be a Squid bug
related to terminating an aborted query. Otherwise, one could argue that
the lack of concurrency controls is a netdata bug.


As Matus UHLAR have said, SNMP is a viable alternative to cache manager
queries, but please keep in mind that the two interfaces provide access
to only partially overlapping measurement sets, and that Squid SNMP code
is neglected even more than Squid cache manager code. Pick your poison.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Jan 21 16:33:49 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Jan 2020 11:33:49 -0500
Subject: [squid-users] Fail and empty response instead cached resource
In-Reply-To: <84cd067a-c962-4d37-764e-8b69ef759df2@treenet.co.nz>
References: <AM7PR09MB3798C2374CC6543D287111E4800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>
 <84cd067a-c962-4d37-764e-8b69ef759df2@treenet.co.nz>
Message-ID: <b6e27f31-ae75-5703-6991-a2c9a736105c@measurement-factory.com>

On 1/21/20 6:54 AM, Amos Jeffries wrote:
> When the revalidation request is sent, the origin server updates the
> HTTP headers to say that the object is now 0 bytes in length:
> 
>     HTTP/1.1 304 Not Modified
>     Server: nginx/1.4.6 (Ubuntu)
>     Date: Tue, 21 Jan 2020 11:27:17 GMT
>     Content-Type: text/javascript; charset=UTF-8
> =>  Content-Length: 0
>     Connection: keep-alive
>     X-PaperRoute: Node
>     ETag: "800439-72175-1534779018000"
>     last-modified: Mon, 20 Aug 2018 15:30:18 GMT
>     Access-Control-Allow-Origin: *
>     Cache-Control: no-cache

AFAICT, nobody is working on a workaround for these broken sites right
now, but there is a potentially useful discussion about this kind of
problem at https://bugs.squid-cache.org/show_bug.cgi?id=4882

Alex.


From aashutosh.xyz at gmail.com  Tue Jan 21 18:39:57 2020
From: aashutosh.xyz at gmail.com (aashutosh kalyankar)
Date: Tue, 21 Jan 2020 10:39:57 -0800
Subject: [squid-users] Issues with TLS inspection Intercept Mode.
Message-ID: <CABi+OR+1u2gLOdHuaEu_XTG7=rOOBfsbQ52kcSoFD9AnHuSKvg@mail.gmail.com>

The problem I am seeing is the intercept port initiates HTTP connection to
self-IP instead of the web server IP it gets from the DNS request.
 Filtered Tcpdump screenshot @
https://drive.google.com/open?id=0ByReiwdSAAY_VXBPTjF1M3dYTnBTTnhFVnRocXFveUlNSlNj

Server IP: Eth0: IP: 172.22.22.148/26 (Same eth0 interface reaches the
internet gateway).
Configurations for
1) Nat table:
Chain PREROUTING (policy ACCEPT 23 packets, 1632 bytes)
num   pkts bytes target     prot opt in     out   source
destination
1       66 3960 REDIRECT   tcp -- eth0 *     0.0.0.0/0 0.0.0.0/0
tcp dpt:80 /* Redirect http traffic eth0:80 to eth0:3128 */ redir ports 3128

Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out   source
destination
1    13500  856K MASQUERADE  all -- * *       0.0.0.0/0 0.0.0.0/0
  /* Allows NAT To happen */

2) Mangle table:
Chain PREROUTING (policy ACCEPT 6180 packets, 519K bytes)
pkts bytes target     prot opt in     out   source
destination
1434  148K ACCEPT     tcp -- any any     172.22.22.0/24 anywhere
  tcp dpt:http
    0   0 DROP       tcp -- any   any anywhere             anywhere tcp
dpt:3128

3) Squid.conf

http_port 172.22.22.148:3128 intercept

https_port 172.22.22.148:3129 intercept ssl-bump
cert=/etc/squid/ssl_certs/myCA.pem generate-host-certificates=on
Complete squid.conf file @ https://pastebin.com/gG8pYpLF.

Please let me know if I am missing some conf or the next steps I should try
to get this running.

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200121/83b25a89/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 22 05:36:37 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jan 2020 18:36:37 +1300
Subject: [squid-users] Issues with TLS inspection Intercept Mode.
In-Reply-To: <CABi+OR+1u2gLOdHuaEu_XTG7=rOOBfsbQ52kcSoFD9AnHuSKvg@mail.gmail.com>
References: <CABi+OR+1u2gLOdHuaEu_XTG7=rOOBfsbQ52kcSoFD9AnHuSKvg@mail.gmail.com>
Message-ID: <2fa147c4-e499-9244-d2d1-c52438e2f4f4@treenet.co.nz>

On 22/01/20 7:39 am, aashutosh kalyankar wrote:
> The problem I am seeing is the intercept port initiates HTTP connection
> to self-IP instead of the web server IP it gets from the DNS request.

Neither of those IPs should be used.

Self-IP indicates that the traffic is being delivered to the proxy port
as explicit proxy traffic. Or that the NAT system is broken. We usually
get this complaint from people who are testing their proxy by sending
traffic directly to the proxy port.

DNS is only involved to validate the HTTP Host header against the IP
address to forbid caching dangerous contents.


> ?Filtered?Tcpdump
> screenshot?@?https://drive.google.com/open?id=0ByReiwdSAAY_VXBPTjF1M3dYTnBTTnhFVnRocXFveUlNSlNj
> 

Screenshots are rarely useful. You are logging level 11,3 to cache.log,
so there should be full HTTP message traces with related connection
details and flow direction (client vs server). That would be more useful.


What I do see in the image is several "GET http://" lines. That
absolute-form URL syntax is for explicit proxy traffic. Traffic
intercepted from port 80 or 443 would use origin-form URLs.
 - This reinforces the idea that you are probably testing the proxy
wrong - eg direct curl requests to the proxy?



> Server IP: Eth0: IP: 172.22.22.148/26 (Same
> eth0 interface reaches the internet gateway).
> Configurations for?
> 1) Nat table:?
> Chain PREROUTING(policy ACCEPT 23 packets, 1632 bytes)
> num ? pkts bytes target ? ? prot opt in? ? out ? source? ? ? ? ? ? ?
> destination ? ?
> 1 ? ? ? 66 3960 REDIRECT ? tcp -- eth0 * ? ? 0.0.0.0/0
> 0.0.0.0/0? ? ? ? ? ? tcp dpt:80 /*
> Redirect http traffic eth0:80 to eth0:3128 */ redir ports 3128
> 

You are missing the PREROUTING rule which would ACCEPT traffic outbound
from the proxy to port 80.

This config page details what you need for REDIRECT interception on
iptables:
 <https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>


Note that it does not place the intercept flag on port 3128. That is
because Squid will generate URLs with its hostname and that port for
clients to use directly when needed (error page contents, manager
reports, etc).



> Please let me know if I am missing some conf or the next steps I should
> try to get this running.
>

Firstly, add that missing NAT rule.

Secondly, make sure that your tests are accurately emulating how clients
would "use" the proxy. That means making connections from a test machine
directly to the Internet and seeing if the routing and NAT delivers the
traffic to Squid properly.

 - Use cache.log to view the traffic coming into the proxy. It will be
request messages with a prefix line indicating "Client HTTP request".
Make sure that prefix line says the remote Internet IP address and port
80/443 you were testing with.
 - If you want confirm that access.log has a transaction entry for the
URL you tested with ORIGINAL_DST and the server IP.



Thirdly, some squid.conf improvements for other problems you have either
not noticed or not encountered yet:

> 3) Squid.conf
> 
...
> acl my_machine src 172.22.22.0/24
> http_access allow my_machine
> acl ap_clients src 172.16.10.0/24
> acl local_clients src 172.18.10.0/24
> acl localnet src 10.0.0.0/8	
...
> http_access allow localnet
> http_access allow ap_clients
> http_access allow local_clients

It looks sub-optimal to have these as separate ACLs. May as well use
config comments to document what ranges are which clients and put them
all in localnet (that is what it is for).

eg:
 # AP clients
 acl localnet src 172.16.10.0/24
 # Local clients
 acl localnet src 172.18.10.0/24
 # LAN
 acl localnet src 10.0.0.0/8

 http_access allow localnet



> acl purge method PURGE
> http_access deny purge

PURGE is largely obsoleted (by HTCP CLR feature) and requires Squid to
perform a relatively large amount of processing per-request. Unless you
have tools that use it specifically it is best to remove all mention of
it from the config file to let Squid avoid all that work. If any mention
remains Squid will auto-activate the cache indexing work.

If you do have tools using it. Then it is past time to consider/plan
moving those tools and Squid to using HTCP CLR instead.


> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

 ... this is where all your custom http_access rules are supposed to be.
The Safe_ports and SSL_Ports lines above are DoS and hijack protections.

 * DoS protections need to be first to do their job of minimizing the
CPU impact effectively. It is counter productive to have allow's earlier
than here.

 * It is risky to let clients hijack the proxy and Squid cannot handle
native traffic to or from those ports anyway. If you need a specific
port available



> http_reply_access allow all

Above is the default reply handling. No need to specify.

> http_access deny all

> http_port 8080
> http_port 172.22.22.148:3128 intercept
> https_port 172.22.22.148:3129 intercept ssl-bump cert=/etc/squid/ssl_certs/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

You do need at least one non-intercept port for the direct-to-proxy
communications that are needed occasionally. Port 3128 is reserved for
that. It is best to pick a random other port number for the intercepted
traffic to arrive at and leave 3128 for the normal proxy traffic.



> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all

NP: while this "works" bumping without any details about the server
(obtained at step2) can cause a lot of connection compatibility problems
and undesirable security side effects.  Prefer to stare at step2 and
bump at step 3. Do this step2 bump only as a last-resort (ie restrict it
to sites that are broken and cannot be worked around in other ways).


> host_verify_strict off

The above is the default, no need to specify.

> acl oyster-vpn-test dstdomain .oyster-vpn-test.com
> cache deny oyster-vpn-test
> visible_hostname togo.mtv.corp.google.com

Er ... AFAIK Google does not use that domain for their machine
hostnames. You sure you want to advertise your network as *.google.com ?

Also, you only need a visible_hostname line if Squid is unable to pull
the machines hostname from the OS configuration.


Amos


From anon.amish at gmail.com  Wed Jan 22 05:55:08 2020
From: anon.amish at gmail.com (Amish)
Date: Wed, 22 Jan 2020 11:25:08 +0530
Subject: [squid-users] squid and netdata causes squid to drop SYN?
In-Reply-To: <124e34d2-6c5d-6190-e6a0-21d5d2a69117@measurement-factory.com>
References: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
 <124e34d2-6c5d-6190-e6a0-21d5d2a69117@measurement-factory.com>
Message-ID: <bd855c75-9ac6-a001-c24b-95a53ea0f50a@gmail.com>

On 21/01/20 9:09 pm, Alex Rousskov wrote:
> On 1/20/20 11:28 PM, Amish wrote:
>
>> 2) Is calling squidclient so frequently a right thing to do by netdata?
> The answer depends on what cache manager query (or queries) your netdata
> is sending to Squid. Sending some queries every second is perfectly
> fine, but there are other, "heavy" queries that should not be sent so
> often and could, if sent with a high enough concurrency level,
> effectively DoS a Squid instance. For example, queries that require
> iterating all cached objects should not be sent to busy Squids.
>
> If netdata does not document the queries it uses, you can probably use
> Squid access.log to figure out what queries netdata is sending (and how
> long they take).

Thanks Matus UHLAR and Alex for responses.

I have not gone in detail through netdata sources but here is whatever I 
could find.

Squid python code that runs HTTP query on squid: (I have never coded in 
python)
https://github.com/netdata/netdata/blob/master/collectors/python.d.plugin/squid/squid.chart.py

Configuration that decides what to query. (netdata chooses one of 
options specified)
https://github.com/netdata/netdata/blob/master/collectors/python.d.plugin/squid/squid.conf

It appears that it runs a query on "counters". But I dont know if that 
is counted as a "heavy" query or not.

> N.B. If netdata is killing the previous query when starting a new
> would-be-concurrent query, then there should be no DoS conditions -- a
> single "heavy" query may slow Squid down a bit but should not stall the
> whole Squid instance. Thus, if netdata ensures that the number of
> concurrent cache manager queries is small, then there may be a Squid bug
> related to terminating an aborted query. Otherwise, one could argue that
> the lack of concurrency controls is a netdata bug.

Not sure if netdata terminates previous query or not. But I do see use 
of keep-alive in netdata code.

And also I completely understand that this area needs to be looked upon 
by netdata team. I will follow up with them.

But posting here just case, a quick glance can reveal a squid bug (or 
buggy approach by netdata) somewhere.

Thanks again and regards,

Amish.



From squid3 at treenet.co.nz  Wed Jan 22 06:11:50 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jan 2020 19:11:50 +1300
Subject: [squid-users] Fail and empty response instead cached resource
In-Reply-To: <AM7PR09MB379858962BFD4E406681C2E1800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>
References: <AM7PR09MB3798C2374CC6543D287111E4800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>
 <84cd067a-c962-4d37-764e-8b69ef759df2@treenet.co.nz>
 <AM7PR09MB379858962BFD4E406681C2E1800D0@AM7PR09MB3798.eurprd09.prod.outlook.com>
Message-ID: <f11ca2e0-3e2e-7320-7b76-f7ad16e2970b@treenet.co.nz>

On 22/01/20 1:08 am, Kuznetsov Sergei wrote:
> If I get you right, is this a problem with the web server.
>

Yes, exactly so.

> I tried to explain to them with about the same logic, but they could not
> reproduce the error and accordingly do not recognize the existence of
> the problem.

To reproduce, just deliver a valid If-None-Match header to their server.


Request:
  GET /js/jquery-1.4.2.min.js HTTP/1.1
  Host: tehspb.kodeks.ru
  User-Agent: squidclient/4.9
  Accept: */*
  Connection: close
  If-None-Match: "800439-72175-1534779018000"


Response:
  HTTP/1.1 304 Not Modified
  Server: nginx/1.4.6 (Ubuntu)
  Date: Wed, 22 Jan 2020 05:48:20 GMT
  Content-Type: text/javascript; charset=UTF-8
  Content-Length: 0
  Connection: close
  X-PaperRoute: Node
  ETag: "800439-72175-1534779018000"
  last-modified: Mon, 20 Aug 2018 15:30:18 GMT
  Access-Control-Allow-Origin: *
  Cache-Control: no-cache



Aaron's analysis in comment #11 of that bug report Alex linked to is
exactly correct. The Content-Length header should not be present at all
in these 304. The case where a Content-Length header would be valid here
is also the case where the ETag should have changed - meaning the status
should be 200 and the new content+ETag delivered (not a 304 at all).


IIRC we had a more recent proposal for some work towards identifying
these broken 304 cases and re-fetching the content. But no idea whether
that has made progress past the early brainstorming.


Amos


From squid3 at treenet.co.nz  Wed Jan 22 06:40:48 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jan 2020 19:40:48 +1300
Subject: [squid-users] squid and netdata causes squid to drop SYN?
In-Reply-To: <bd855c75-9ac6-a001-c24b-95a53ea0f50a@gmail.com>
References: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
 <124e34d2-6c5d-6190-e6a0-21d5d2a69117@measurement-factory.com>
 <bd855c75-9ac6-a001-c24b-95a53ea0f50a@gmail.com>
Message-ID: <875781fa-8d5c-2e08-5277-6d0de2006edc@treenet.co.nz>

On 22/01/20 6:55 pm, Amish wrote:
> On 21/01/20 9:09 pm, Alex Rousskov wrote:
>> On 1/20/20 11:28 PM, Amish wrote:
>>
>>> 2) Is calling squidclient so frequently a right thing to do by netdata?
>> The answer depends on what cache manager query (or queries) your netdata
>> is sending to Squid. Sending some queries every second is perfectly
>> fine, but there are other, "heavy" queries that should not be sent so
>> often and could, if sent with a high enough concurrency level,
>> effectively DoS a Squid instance. For example, queries that require
>> iterating all cached objects should not be sent to busy Squids.
>>
>> If netdata does not document the queries it uses, you can probably use
>> Squid access.log to figure out what queries netdata is sending (and how
>> long they take).
> 
> Thanks Matus UHLAR and Alex for responses.
> 
> I have not gone in detail through netdata sources but here is whatever I
> could find.
> 
> Squid python code that runs HTTP query on squid: (I have never coded in
> python)
> https://github.com/netdata/netdata/blob/master/collectors/python.d.plugin/squid/squid.chart.py
> 
> 
> Configuration that decides what to query. (netdata chooses one of
> options specified)
> https://github.com/netdata/netdata/blob/master/collectors/python.d.plugin/squid/squid.conf
> 
> 
> It appears that it runs a query on "counters". But I dont know if that
> is counted as a "heavy" query or not.

It is one of the light ones. So if that were all that is going on I
would not be expecting a problem.

The worst case I have seen with the quick reports is tools not closing
the sockets properly and running out FD numbers. But at 1/sec there
would only be 900 FD held up for the TCP 15min TIMEWAIT, not enough
relative to your 16K available to cause the level of issues you are seeing.


Your mention of intercepting traffic to port 3128 makes me wonder if the
netdata auto-detect is trying to use that port.
 If that is happening there would be some visible effects:
 A) if you have firewall protection DROP'ing direct connections to the
intercept port. That would show up exactly as SYN with no SYN+ACK on any
auto-detect probes the tool used to that port. (This is one reason I am
so vocal about people not using port 3128 to intercept).

 B) if you are missing that mandatory firewall protection, the tool may
be triggering forwarding loops. Which use many more FD than it should
(up to all of them) with the visible effects being clients not able to
connect around peak times and SYN being dropped when limits are hit
(either the loop limit, or interception port protection.
 If the tool is smart enough to detect the error state and move to
another port for a while that might explain the intermittent nature.



> 
>> N.B. If netdata is killing the previous query when starting a new
>> would-be-concurrent query, then there should be no DoS conditions -- a
>> single "heavy" query may slow Squid down a bit but should not stall the
>> whole Squid instance. Thus, if netdata ensures that the number of
>> concurrent cache manager queries is small, then there may be a Squid bug
>> related to terminating an aborted query. Otherwise, one could argue that
>> the lack of concurrency controls is a netdata bug.
> 
> Not sure if netdata terminates previous query or not. But I do see use
> of keep-alive in netdata code.
> 
> And also I completely understand that this area needs to be looked upon
> by netdata team. I will follow up with them.
> 
> But posting here just case, a quick glance can reveal a squid bug (or
> buggy approach by netdata) somewhere.


Since Squid is sending chunked header, the response should be chunked
like they expect and keep-alive pipeline available for use on their
followup requests. If the connection is closed with proper TCP closure
sequence, that is not a problem though.

(NP: Just a "should" because I have run out of time today to confirm
that particular mgr report is acting properly - some do, others not so
nice.)

Amos


From anon.amish at gmail.com  Wed Jan 22 07:54:40 2020
From: anon.amish at gmail.com (Amish)
Date: Wed, 22 Jan 2020 13:24:40 +0530
Subject: [squid-users] squid and netdata causes squid to drop SYN?
In-Reply-To: <875781fa-8d5c-2e08-5277-6d0de2006edc@treenet.co.nz>
References: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
 <124e34d2-6c5d-6190-e6a0-21d5d2a69117@measurement-factory.com>
 <bd855c75-9ac6-a001-c24b-95a53ea0f50a@gmail.com>
 <875781fa-8d5c-2e08-5277-6d0de2006edc@treenet.co.nz>
Message-ID: <e5d4071b-fa86-71b1-67ff-144f50a7efcc@gmail.com>


On 22/01/20 12:10 pm, Amos Jeffries wrote:
> On 22/01/20 6:55 pm, Amish wrote:
>
>>> It appears that it runs a query on "counters". But I dont know if that
>>> is counted as a "heavy" query or not.
> It is one of the light ones. So if that were all that is going on I
> would not be expecting a problem.
>
> ...
>
> Your mention of intercepting traffic to port 3128 makes me wonder if the
> netdata auto-detect is trying to use that port.
>   If that is happening there would be some visible effects:
>   A) if you have firewall protection DROP'ing direct connections to the
> intercept port. That would show up exactly as SYN with no SYN+ACK on any
> auto-detect probes the tool used to that port. (This is one reason I am
> so vocal about people not using port 3128 to intercept).
>
>   B) if you are missing that mandatory firewall protection, the tool may
> be triggering forwarding loops. Which use many more FD than it should
> (up to all of them) with the visible effects being clients not able to
> connect around peak times and SYN being dropped when limits are hit
> (either the loop limit, or interception port protection.
>   If the tool is smart enough to detect the error state and move to
> another port for a while that might explain the intermittent nature.

Connection from 127.0.0.1 (loopback interface) are not blocked. (DROPped)

Also, if it was due to DROP then it should not work from beginning but 
here it works for a while (few hours) and then suddenly starts blocking.

Also I grepped whole cache.log for word "descriptor", it showed me no 
error. Only line I saw is that 16384 descriptors are available (when 
squid starts)

I will try to reproduce the error when I get some time. Currently I have 
disabled netdata's squid module and things are stable.

Thank you very much for your response,

Amish.


From squid3 at treenet.co.nz  Wed Jan 22 10:25:03 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Jan 2020 23:25:03 +1300
Subject: [squid-users] squid and netdata causes squid to drop SYN?
In-Reply-To: <e5d4071b-fa86-71b1-67ff-144f50a7efcc@gmail.com>
References: <d5356e74-53f8-b873-7490-329b05c2ef28@gmail.com>
 <124e34d2-6c5d-6190-e6a0-21d5d2a69117@measurement-factory.com>
 <bd855c75-9ac6-a001-c24b-95a53ea0f50a@gmail.com>
 <875781fa-8d5c-2e08-5277-6d0de2006edc@treenet.co.nz>
 <e5d4071b-fa86-71b1-67ff-144f50a7efcc@gmail.com>
Message-ID: <136226b4-2239-3d32-3d39-0e38882389e2@treenet.co.nz>

On 22/01/20 8:54 pm, Amish wrote:
> 
> On 22/01/20 12:10 pm, Amos Jeffries wrote:
>> On 22/01/20 6:55 pm, Amish wrote:
>>
>>>> It appears that it runs a query on "counters". But I dont know if that
>>>> is counted as a "heavy" query or not.
>> It is one of the light ones. So if that were all that is going on I
>> would not be expecting a problem.
>>
>> ...
>>
>> Your mention of intercepting traffic to port 3128 makes me wonder if the
>> netdata auto-detect is trying to use that port.
>> ? If that is happening there would be some visible effects:
>> ? A) if you have firewall protection DROP'ing direct connections to the
>> intercept port. That would show up exactly as SYN with no SYN+ACK on any
>> auto-detect probes the tool used to that port. (This is one reason I am
>> so vocal about people not using port 3128 to intercept).
>>
>> ? B) if you are missing that mandatory firewall protection, the tool may
>> be triggering forwarding loops. Which use many more FD than it should
>> (up to all of them) with the visible effects being clients not able to
>> connect around peak times and SYN being dropped when limits are hit
>> (either the loop limit, or interception port protection.
>> ? If the tool is smart enough to detect the error state and move to
>> another port for a while that might explain the intermittent nature.
> 
> Connection from 127.0.0.1 (loopback interface) are not blocked. (DROPped)

It should be. *No* traffic can be allowed to connect directly to a Squid
'intercept' port. It is exclusively for use by NAT<->Squid communication.

Anything that does connect directly will start a loop which can consume
up to all of the resources on the server (TCP sockets, TCP ports, NAT
table entries, file descriptors, memory, CPU ... whichever exhausts
first). Squid uses Via header and some other details to try to detect
loops before they become a problem and forbid ASAP. But there may be
ways around the detection - since Via is often disabled and/or stripped
from traffic.

> 
> Also, if it was due to DROP then it should not work from beginning but
> here it works for a while (few hours) and then suddenly starts blocking.
> 

The netdata tool at least has multiple different ways to connect to the
proxy. It has some logic to auto-detect which work and which don't. One
of those ways is port 3128 and another is port 8080 - which on your
proxy are the intercept and explicit proxy ports respectively. It may
try a working port some executions and then a non-working port next
execution.


> Also I grepped whole cache.log for word "descriptor", it showed me no
> error. Only line I saw is that 16384 descriptors are available (when
> squid starts)

That is not what you should be looking at anyway. Look at the HTTP
messages arriving, where they are coming from and what happens to them.

> 
> I will try to reproduce the error when I get some time. Currently I have
> disabled netdata's squid module and things are stable.
> 
> Thank you very much for your response,

Aye, hope this info helps for that troubleshooting.

Amos


From aashutosh.xyz at gmail.com  Thu Jan 23 02:11:55 2020
From: aashutosh.xyz at gmail.com (aashutosh kalyankar)
Date: Wed, 22 Jan 2020 18:11:55 -0800
Subject: [squid-users] Issues with TLS inspection-2
In-Reply-To: <mailman.3889.1579671321.3049.squid-users@lists.squid-cache.org>
References: <mailman.3889.1579671321.3049.squid-users@lists.squid-cache.org>
Message-ID: <CABi+ORKQuZV-tOOU5F7W5dPAetkk+=7ra=iHsGJ_72OveGQ+1A@mail.gmail.com>

>
>
>
>
> Message: 5
> Date: Wed, 22 Jan 2020 18:36:37 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Issues with TLS inspection Intercept Mode.
> Message-ID: <2fa147c4-e499-9244-d2d1-c52438e2f4f4 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 22/01/20 7:39 am, aashutosh kalyankar wrote:
> > The problem I am seeing is the intercept port initiates HTTP connection
> > to self-IP instead of the web server IP it gets from the DNS request.
>
> Neither of those IPs should be used.
>
> Self-IP indicates that the traffic is being delivered to the proxy port
> as explicit proxy traffic. Or that the NAT system is broken. We usually
> get this complaint from people who are testing their proxy by sending
> traffic directly to the proxy port.
>
> DNS is only involved to validate the HTTP Host header against the IP
> address to forbid caching dangerous contents.
>
>
> >  Filtered Tcpdump
> > screenshot @
> https://drive.google.com/open?id=0ByReiwdSAAY_VXBPTjF1M3dYTnBTTnhFVnRocXFveUlNSlNj
> >
>
> Screenshots are rarely useful. You are logging level 11,3 to cache.log,
> so there should be full HTTP message traces with related connection
> details and flow direction (client vs server). That would be more useful.
>
>
> What I do see in the image is several "GET http://" lines. That
> absolute-form URL syntax is for explicit proxy traffic. Traffic
> intercepted from port 80 or 443 would use origin-form URLs.
>  - This reinforces the idea that you are probably testing the proxy
> wrong - eg direct curl requests to the proxy?
>
>
>
> > Server IP: Eth0: IP: 172.22.22.148/26 (Same
> > eth0 interface reaches the internet gateway).
> > Configurations for
> > 1) Nat table:
> > Chain PREROUTING(policy ACCEPT 23 packets, 1632 bytes)
> > num   pkts bytes target     prot opt in    out   source
> > destination
> > 1       66 3960 REDIRECT   tcp -- eth0 *     0.0.0.0/0
> > 0.0.0.0/0            tcp dpt:80 /*
> > Redirect http traffic eth0:80 to eth0:3128 */ redir ports 3128
> >
>
> You are missing the PREROUTING rule which would ACCEPT traffic outbound
> from the proxy to port 80.
>
> As suggested, taking reference from
https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect, I
added the following rules. The only additional part is the Separation of
HTTP and HTTPS traffic rules.
*NAT TABLE*
* Chain PREROUTING*
 1        0     0 ACCEPT     tcp  --  *      *       172.22.22.148
0.0.0.0/0            tcp dpt:80 /* Accept http traffic at 172.22.22.148:80
*/

2       79  4740 REDIRECT   tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 /* Redirect eth0:80 to eth0:3129 */ redir
ports 3129

3        0     0 ACCEPT     tcp  --  *      *       172.22.22.148
0.0.0.0/0            tcp dpt:443 /* Accept https traffic at
172.22.22.148:443 */

4      565 33900 REDIRECT   tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443 /* Redirect eth0:443 to eth0:3130 */ redir
ports 3130

*Chain POSTROUTING*
1     365K   23M MASQUERADE  all  --  *      *       0.0.0.0/0
0.0.0.0/0            /*  Allows NAT To happen */

*Mangle Table *

*Chain PREROUTING*
1        0     0 DROP       tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:3129
2        0     0 DROP       tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:3130

 This config page details what you need for REDIRECT interception on
> iptables:
>  <https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
>
> Note that it does not place the intercept flag on port 3128. That is
> because Squid will generate URLs with its hostname and that port for
> clients to use directly when needed (error page contents, manager
> reports, etc).
>
> Using ports 3129 for http and 3130 for https.
http_port 172.22.22.148:3129 intercept
https_port 172.22.22.148:3130 intercept ssl-bump
cert=/etc/squid/ssl_certs/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB


>
>
> > Please let me know if I am missing some conf or the next steps I should
> > try to get this running.
> >
>
> Firstly, add that missing NAT rule.
>


> Secondly, make sure that your tests are accurately emulating how clients
> would "use" the proxy. That means making connections from a test machine
> directly to the Internet and seeing if the routing and NAT delivers the
> traffic to Squid properly.
>

I am using a chromebook to test. In the configuration section of the
wireless network there is an option to add proxy hostname and proxy port
based on protocols.
Http proxy     :  proxy-tls 80
HTTPS proxy:  proxy-tls 443


>  - Use cache.log to view the traffic coming into the proxy. It will be
> request messages with a prefix line indicating "Client HTTP request".
> Make sure that prefix line says the remote Internet IP address and port
> 80/443 you were testing with.
>  - If you want confirm that access.log has a transaction entry for the
> URL you tested with ORIGINAL_DST and the server IP.
>
> Sample cache.log for a test I did for neverssl.com

2020/01/22 17:08:30.236 kid1| 11,2| client_side.cc(2346) parseHttpRequest:
HTTP Client local=172.22.22.148:80 remote=172.22.22.151:34728 FD 12 flags=33
2020/01/22 17:08:30.236 kid1| 11,2| client_side.cc(2347) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://neverssl.com/ HTTP/1.1
Host: neverssl.com
Proxy-Connection: keep-alive
DNT: 1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (X11; CrOS x86_64 12499.12.0) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/78.0.3904.26 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8,ar;q=0.7


----------
2020/01/22 17:08:30.249 kid1| 20,3| store.cc(774) storeCreatePureEntry:
storeCreateEntry: 'http://neverssl.com/'
2020/01/22 17:08:30.250 kid1| 20,3| MemObject.cc(97) MemObject: new
MemObject 0x7f3055a51990
2020/01/22 17:08:30.250 kid1| 20,3| store.cc(499) setReleaseFlag:
StoreEntry::setReleaseFlag: '[null_store_key]'
2020/01/22 17:08:30.250 kid1| 20,3| store_key_md5.cc(89) storeKeyPrivate:
storeKeyPrivate: GET http://neverssl.com/
2020/01/22 17:08:30.250 kid1| 20,3| store.cc(447) hashInsert:
StoreEntry::hashInsert: Inserting Entry e:=XI/0x7f3055a51910*0 key
'17D405610D6C945224B64DBB3180E9CB'
2020/01/22 17:08:30.250 kid1| 20,3| store.cc(483) lock: storeCreateEntry
locked key 17D405610D6C945224B64DBB3180E9CB e:=XIV/0x7f3055a51910*1
2020/01/22 17:08:30.250 kid1| 20,3| store.cc(483) lock:
clientReplyContext::setReplyToStoreEntry locked key
17D405610D6C945224B64DBB3180E9CB e:=XIV/0x7f3055a51910*2
2020/01/22 17:08:30.258 kid1| 20,3| store.cc(483) lock:
StoreEntry::storeErrorResponse locked key 17D405610D6C945224B64DBB3180E9CB
e:=XIV/0x7f3055a51910*3
2020/01/22 17:08:30.258 kid1| 20,3| store.cc(1862) replaceHttpReply:
StoreEntry::replaceHttpReply: http://neverssl.com/
2020/01/22 17:08:30.258 kid1| 20,2| store.cc(949) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2020/01/22 17:08:30.258 kid1| 20,3| store.cc(1048) complete: storeComplete:
'17D405610D6C945224B64DBB3180E9CB'
2020/01/22 17:08:30.258 kid1| 20,3| store.cc(1337) validLength:
storeEntryValidLength: Checking '17D405610D6C945224B64DBB3180E9CB'
2020/01/22 17:08:30.258 kid1| 20,2| store.cc(949) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2020/01/22 17:08:30.258 kid1| 20,3| store.cc(521) unlock:
StoreEntry::storeErrorResponse unlocking key
17D405610D6C945224B64DBB3180E9CB e:=sXINV/0x7f3055a51910*3
2020/01/22 17:08:30.259 kid1| 20,3| store.cc(483) lock: store_client::copy
locked key 17D405610D6C945224B64DBB3180E9CB e:=sXINV/0x7f3055a51910*3
2020/01/22 17:08:30.259 kid1| 20,3| store.cc(483) lock:
ClientHttpRequest::loggingEntry locked key 17D405610D6C945224B64DBB3180E9CB
e:=sXINV/0x7f3055a51910*4
2020/01/22 17:08:30.259 kid1| 11,2| client_side.cc(1392)
sendStartOfMessage: HTTP Client local=172.22.22.148:80 remote=
172.22.22.151:34728 FD 12 flags=33
2020/01/22 17:08:30.259 kid1| 11,2| client_side.cc(1393)
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 403 Forbidden
Server: squid/3.5.19
Mime-Version: 1.0
Date: Thu, 23 Jan 2020 01:08:30 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3861
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en-us
X-Cache: MISS from proxy-tls
X-Cache-Lookup: NONE from proxy-tls:8080
Via: 1.1 proxy-tls (squid/3.5.19)
Connection: keep-alive

complete file:https://pastebin.com/iBvy9naz
----------

>
>
> Thirdly, some squid.conf improvements for other problems you have either
> not noticed or not encountered yet:
>
> > 3) Squid.conf
> >
> ...
> > acl my_machine src 172.22.22.0/24
> > http_access allow my_machine
> > acl ap_clients src 172.16.10.0/24
> > acl local_clients src 172.18.10.0/24
> > acl localnet src 10.0.0.0/8
> ...
> > http_access allow localnet
> > http_access allow ap_clients
> > http_access allow local_clients
>
> It looks sub-optimal to have these as separate ACLs. May as well use
> config comments to document what ranges are which clients and put them
> all in localnet (that is what it is for).
>
> eg:
>  # AP clients
>  acl localnet src 172.16.10.0/24
>  # Local clients
>  acl localnet src 172.18.10.0/24
>  # LAN
>  acl localnet src 10.0.0.0/8
>
>  http_access allow localnet
>

Updated as suggested:)

>
>
>
> > acl purge method PURGE
> > http_access deny purge
>
> PURGE is largely obsoleted (by HTCP CLR feature) and requires Squid to
> perform a relatively large amount of processing per-request. Unless you
> have tools that use it specifically it is best to remove all mention of
> it from the config file to let Squid avoid all that work. If any mention
> remains Squid will auto-activate the cache indexing work.
>
> If you do have tools using it. Then it is past time to consider/plan
> moving those tools and Squid to using HTCP CLR instead.
>
>
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
>
>  ... this is where all your custom http_access rules are supposed to be.
> The Safe_ports and SSL_Ports lines above are DoS and hijack protections.
>

IIUC, These are not required to be here so I commented out those lines.


>  * DoS protections need to be first to do their job of minimizing the
> CPU impact effectively. It is counter productive to have allow's earlier
> than here.
>
>  * It is risky to let clients hijack the proxy and Squid cannot handle
> native traffic to or from those ports anyway. If you need a specific
> port available
>
>
>
> > http_reply_access allow all
>
> Above is the default reply handling. No need to specify.
>
> > http_access deny all
>
> > http_port 8080
> > http_port 172.22.22.148:3128 intercept
> > https_port 172.22.22.148:3129 intercept ssl-bump
> cert=/etc/squid/ssl_certs/myCA.pem generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> > sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> You do need at least one non-intercept port for the direct-to-proxy
> communications that are needed occasionally. Port 3128 is reserved for
> that. It is best to pick a random other port number for the intercepted
> traffic to arrive at and leave 3128 for the normal proxy traffic.
>
>
>
> > acl step1 at_step SslBump1
> > ssl_bump peek step1
> > ssl_bump bump all
>
> NP: while this "works" bumping without any details about the server
> (obtained at step2) can cause a lot of connection compatibility problems
> and undesirable security side effects.  Prefer to stare at step2 and
> bump at step 3. Do this step2 bump only as a last-resort (ie restrict it
> to sites that are broken and cannot be worked around in other ways).
>

As suggested, updated it to
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump stare step2
ssl_bump bump step3


>
>
> > host_verify_strict off
>
> The above is the default, no need to specify.
>
> > acl oyster-vpn-test dstdomain .oyster-vpn-test.com
> > cache deny oyster-vpn-test
> > visible_hostname togo.mtv.corp.google.com
>
> Er ... AFAIK Google does not use that domain for their machine
> hostnames. You sure you want to advertise your network as *.google.com ?
>
> Also, you only need a visible_hostname line if Squid is unable to pull
> the machines hostname from the OS configuration.
>
>
> Amos
>
> Thanks for your time and help  Amos.

>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 65, Issue 30
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200122/e96b17b7/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 23 11:04:50 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Jan 2020 00:04:50 +1300
Subject: [squid-users] Issues with TLS inspection-2
In-Reply-To: <CABi+ORKQuZV-tOOU5F7W5dPAetkk+=7ra=iHsGJ_72OveGQ+1A@mail.gmail.com>
References: <mailman.3889.1579671321.3049.squid-users@lists.squid-cache.org>
 <CABi+ORKQuZV-tOOU5F7W5dPAetkk+=7ra=iHsGJ_72OveGQ+1A@mail.gmail.com>
Message-ID: <c91678b8-fd3f-2535-8d14-8dbbdcae9324@treenet.co.nz>

On 23/01/20 3:11 pm, aashutosh kalyankar wrote:
>     From: Amos Jeffrie>
>     Secondly, make sure that your tests are accurately emulating how clients
>     would "use" the proxy. That means making connections from a test machine
>     directly to the Internet and seeing if the routing and NAT delivers the
>     traffic to Squid properly.
> 
> 
> I am using a chromebook to test. In the configuration section of the
> wireless network there is an option to add proxy hostname and proxy port
> based on protocols.??
> Http proxy? ? ?:? proxy-tls 80
> HTTPS proxy:? proxy-tls 443
> 

That is part of your problem. Those are settings for explicit proxy.

With intercept the clients knows nothing about any proxy. They are just
connecting to a web server directly (but *NAT* sends it to Squid instead).


> 
>     ?- Use cache.log to view the traffic coming into the proxy. It will be
>     request messages with a prefix line indicating "Client HTTP request".
>     Make sure that prefix line says the remote Internet IP address and port
>     80/443 you were testing with.
>     ?- If you want confirm that access.log has a transaction entry for the
>     URL you tested with ORIGINAL_DST and the server IP.
> 
> Sample cache.log for a test I did for neverssl.com <http://neverssl.com>
> 
> 2020/01/22 17:08:30.236 kid1| 11,2| client_side.cc(2346)
> parseHttpRequest: HTTP Client local=172.22.22.148:80
> <http://172.22.22.148:80> remote=172.22.22.151:34728
> <http://172.22.22.151:34728> FD 12 flags=33
> 2020/01/22 17:08:30.236 kid1| 11,2| client_side.cc(2347)
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET http://neverssl.com/ HTTP/1.1
> Host: neverssl.com <http://neverssl.com>
> Proxy-Connection: keep-alive


...
> 
>     > http_access deny !Safe_ports
>     > http_access deny CONNECT !SSL_ports
> 
>     ?... this is where all your custom http_access rules are supposed to be.
>     The Safe_ports and SSL_Ports lines above are DoS and hijack protections.
> 
> ?
> IIUC, These are not required to be here so I commented out those lines.?
> 

Sorry if I was not clear. They should be the first http_access lines in
your config. Local policy rules follow them. Then the final "deny all"
rule to block anything not allowed by your policy.



Amos


From eperez at quadrianweb.com  Fri Jan 24 15:41:51 2020
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Fri, 24 Jan 2020 10:41:51 -0500
Subject: [squid-users] squid 4.8 cannot connect to Skype for business
Message-ID: <CACXMG+t3wn=rLPhuFP6m6wz+4wNxvWjgMyvLji39OkrmEDAczQ@mail.gmail.com>

good day to all,
my windows clients can connect successfully to regular skype, but cannot
connect to skype for business. With the included extract of client machine
10.231.0.20 is there anything that can tell me why it is not connecting?

ip  squid :  10.230.5.71
tested client machine: 10.231.0.20
skype for business: 16.0.12325.20280 32 bit

*********
squid -v:
[root at s03-prxy squid]# squid -v
Squid Cache: Version 4.8
Service Name: squid

This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions
on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--exec_prefix=/usr'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
'--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake'
'--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp'
'--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi'
'--enable-security-cert-generators' '--enable-security-cert-validators'
'--enable-icmp' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=16384' '--with-dl' '--with-openssl'
'--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl'
'--disable-arch-native' '--without-nettle'
'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu'
'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
-m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
--param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
-fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
--enable-ltdl-convenience
[root at s03-prxy squid]#
*****************

access.log extract:
[root at s03-prxy squid]# tail -f /var/log/squid/access.log | grep 10.231.0.20
1579879519.778   3539 10.231.0.200 TCP_TUNNEL/200 4974 CONNECT
clients1.google.com:443 - HIER_DIRECT/172.217.15.206 -
1579879519.779   4340 10.231.0.200 TCP_TUNNEL/200 3490 CONNECT
www.google.com:443 - HIER_DIRECT/172.217.8.132 -
1579879524.530 114397 10.231.0.200 TCP_TUNNEL_ABORTED/200 2051 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
34.218.169.26 -
1579879541.929 6131038 10.231.0.20 TCP_TUNNEL_ABORTED/200 1544733 CONNECT
outlook.office365.com:443 - HIER_DIRECT/52.96.87.194 -
1579879541.929 4712050 10.231.0.20 TCP_TUNNEL_ABORTED/200 226325 CONNECT
outlook.office365.com:443 - HIER_DIRECT/40.97.190.2 -
1579879541.944 126301 10.231.0.20 TCP_TUNNEL_ABORTED/200 27260 CONNECT
outlook.office365.com:443 - HIER_DIRECT/52.96.33.82 -
1579879541.944 862124 10.231.0.20 TCP_TUNNEL_ABORTED/200 1061225 CONNECT
outlook.office365.com:443 - HIER_DIRECT/52.96.88.18 -
1579879546.091    106 10.231.0.20 TCP_TUNNEL_ABORTED/200 4357 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879546.925    817 10.231.0.20 TCP_TUNNEL/200 17155 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879568.080     11 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879568.432    350 10.231.0.20 TCP_TUNNEL/200 7353 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879571.803 121672 10.231.0.20 TCP_TUNNEL/200 14037 CONNECT
login.windows.net:443 - HIER_DIRECT/40.126.3.35 -
1579879576.554     69 10.231.0.20 TCP_MISS/304 449 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
- HIER_DIRECT/8.252.64.254 -
1579879576.645     69 10.231.0.20 TCP_MISS/304 449 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/authrootstl.cab?
- HIER_DIRECT/8.252.64.254 -
1579879579.790   5275 10.231.0.200 TCP_TUNNEL/200 4088 CONNECT
safebrowsing.googleapis.com:443 - HIER_DIRECT/172.217.8.74 -
1579879580.975     72 10.231.0.202 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879580.976     72 10.231.0.202 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879580.976     72 10.231.0.202 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879580.976     72 10.231.0.202 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879581.049     72 10.231.0.202 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879581.050     71 10.231.0.202 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879584.088  68045 10.231.0.20 TCP_TUNNEL/200 5636 CONNECT
config.edge.skype.com:443 - HIER_DIRECT/13.107.3.128 -
1579879586.050  14003 10.231.0.200 TCP_TUNNEL/200 7371 CONNECT
pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
146.123.20.137 -
1579879589.994     48 10.231.0.20 TCP_MISS/200 1978 GET
http://ocsp.digicert.com/MFEwTzBNMEswSTAJBgUrDgMCGgUABBTBL0V27RVZ7LBduom%2FnYB45SPUEwQU5Z1ZMIJHWMys%2BghUNoZ7OrUETfACEA8sEMlbBsCTf7jUSfg%2BhWk%3D
- HIER_DIRECT/72.21.91.29 application/ocsp-response
1579879590.031    455 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
1579879590.596    563 10.231.0.20 TCP_TUNNEL/200 7556 CONNECT
sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
1579879593.377 118087 10.231.0.20 TCP_TUNNEL_ABORTED/200 1987 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
52.10.0.129 -
1579879611.227     69 10.231.0.202 TCP_MISS/304 449 GET
http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
- HIER_DIRECT/8.252.64.254 -
1579879611.312     69 10.231.0.202 TCP_MISS/304 259 GET
http://crl.pki.goog/GTS1O1.crl - HIER_DIRECT/172.217.3.131 -
1579879612.106    336 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
1579879612.585    477 10.231.0.20 TCP_TUNNEL/200 7700 CONNECT
sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
1579879613.465     48 10.231.0.20 TCP_MISS/200 1978 GET
http://ocsp.digicert.com/MFEwTzBNMEswSTAJBgUrDgMCGgUABBTBL0V27RVZ7LBduom%2FnYB45SPUEwQU5Z1ZMIJHWMys%2BghUNoZ7OrUETfACEAiIzVJfGSRETRSlgpHeuVI%3D
- HIER_DIRECT/72.21.91.29 application/ocsp-response
1579879613.729    190 10.231.0.20 TCP_MISS/200 2597 GET
http://ocsp.msocsp.com/MFQwUjBQME4wTDAJBgUrDgMCGgUABBQphfxhPb4vsBIPXkIOTJ7D1Z79fAQUCP4ln3TqhwTCvLuOqDhfM8bRbGUCEy0ABoRa5qu8lnR3dcQAAAAGhFo%3D
- HIER_DIRECT/104.18.24.243 application/ocsp-response
1579879615.429     73 10.231.0.20 TCP_MISS/200 2597 GET
http://ocsp.msocsp.com/MFQwUjBQME4wTDAJBgUrDgMCGgUABBQirwAcgHViBybgyJMa7KdCHDISOgQUenuMwc%2FnoMoc1Gv6%2B%2BEzww8aop0CExYAAvvgLxYAucV3A0IAAAAC%2B%2BA%3D
- HIER_DIRECT/104.18.24.243 application/ocsp-response
1579879615.601    510 10.231.0.20 TCP_TUNNEL_ABORTED/200 7961 CONNECT
login.microsoftonline.com:443 - HIER_DIRECT/20.190.131.98 -
1579879615.769    151 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
1579879615.921    149 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
1579879616.936 117994 10.231.0.202 TCP_TUNNEL_ABORTED/200 1987 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
52.10.0.129 -
1579879627.471  10590 10.231.0.200 TCP_TUNNEL/200 2135 CONNECT
pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
146.123.20.137 -
1579879629.024  15981 10.231.0.20 TCP_TUNNEL_ABORTED/200 35500 CONNECT
webdir0a.online.lync.com:443 - HIER_DIRECT/52.112.64.14 -
1579879639.086      0 10.231.0.20 NONE/503 0 CONNECT
lyncdiscoverinternal.aaaaair.com:443 - HIER_NONE/- -
1579879639.087      0 10.231.0.20 TCP_MISS/503 4569 GET
http://lyncdiscoverinternal.aaaaair.com/? - HIER_NONE/- text/html
1579879639.211    114 10.231.0.20 TCP_MISS/200 789 GET
http://lyncdiscover.aaaaair.com/? - HIER_DIRECT/200.46.240.179
application/vnd.microsoft.rtc.autodiscover+xml
1579879644.031  29867 10.231.0.20 TCP_TUNNEL_ABORTED/200 23372 CONNECT
webdir0a.online.lync.com:443 - HIER_DIRECT/52.112.64.14 -
1579879644.529 114376 10.231.0.200 TCP_TUNNEL_ABORTED/200 1987 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
52.10.0.129 -
1579879661.924 451876 10.231.0.20 TCP_TUNNEL_ABORTED/200 8663 CONNECT
outlook.office365.com:443 - HIER_DIRECT/52.96.33.82 -
1579879665.721  66079 10.231.0.202 TCP_TUNNEL/200 6277 CONNECT
4.sophosxl.net:443 - HIER_DIRECT/3.227.84.245 -
1579879675.921     72 10.231.0.202 TCP_MISS/200 256 GET
http://http.00.a.sophosxl.net/V3/01/cynl.tbbtyr.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879676.722  65561 10.231.0.202 TCP_TUNNEL/200 7269 CONNECT
4.sophosxl.net:443 - HIER_DIRECT/3.227.84.245 -
1579879681.592  13173 10.231.0.200 TCP_TUNNEL/200 6761 CONNECT
pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
146.123.20.137 -
1579879695.487     15 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879695.599    110 10.231.0.20 TCP_TUNNEL/200 17155 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879699.006  59919 10.231.0.20 NONE/503 0 CONNECT
lyncdiscover.aaaaair.com:443 - HIER_NONE/- -
1579879699.006  59714 10.231.0.20 NONE/503 0 CONNECT abs.aaaaair.com:443 -
HIER_NONE/- -
1579879705.254 212866 10.231.0.20 TCP_TUNNEL/200 10722 CONNECT
4.sophosxl.net:443 - HIER_DIRECT/34.234.127.160 -
1579879705.255 209910 10.231.0.20 TCP_TUNNEL/200 12951 CONNECT
4.sophosxl.net:443 - HIER_DIRECT/34.234.127.160 -
1579879713.373 118074 10.231.0.20 TCP_TUNNEL_ABORTED/200 605 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
52.10.0.129 -
1579879716.652     11 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879716.713     59 10.231.0.20 TCP_TUNNEL/200 7401 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879724.032     72 10.231.0.200 TCP_MISS/200 257 GET
http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
3.222.35.82 -
1579879734.719  66296 10.231.0.200 TCP_TUNNEL/200 6101 CONNECT
4.sophosxl.net:443 - HIER_DIRECT/3.227.84.245 -
1579879736.935 117987 10.231.0.202 TCP_TUNNEL_ABORTED/200 605 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
52.10.0.129 -
1579879737.777     11 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879737.940    161 10.231.0.20 TCP_TUNNEL/200 7353 CONNECT
sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
1579879754.531     68 10.231.0.200 TCP_MISS/304 259 GET
http://crl.pki.goog/gsr2/gsr2.crl - HIER_DIRECT/172.217.3.131 -
1579879759.398    321 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
1579879759.922    522 10.231.0.20 TCP_TUNNEL/200 7556 CONNECT
sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
1579879761.075  16921 10.231.0.200 TCP_TUNNEL/200 7350 CONNECT
pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
146.123.20.137 -
1579879764.529 114383 10.231.0.200 TCP_TUNNEL_ABORTED/200 605 CONNECT
dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
52.10.0.129 -
1579879778.623   8111 10.231.0.200 TCP_TUNNEL/200 774 CONNECT
pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
146.123.20.137 -
1579879781.400    324 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
1579879781.883    481 10.231.0.20 TCP_TUNNEL/200 7700 CONNECT
sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
1579879782.075    142 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
1579879782.225    146 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
1579879800.250      0 10.231.0.20 NONE/503 0 CONNECT
lyncdiscoverinternal.aaaaair.com:443 - HIER_NONE/- -
1579879800.251      0 10.231.0.20 TCP_MISS/503 4569 GET
http://lyncdiscoverinternal.aaaaair.com/? - HIER_NONE/- text/html
1579879800.256      5 10.231.0.20 TCP_MISS/200 789 GET
http://lyncdiscover.aaaaair.com/? - HIER_DIRECT/200.46.240.179
application/vnd.microsoft.rtc.autodiscover+xml
1579879801.905     72 10.231.0.200 TCP_MISS/200 262 GET
http://http.00.a.sophosxl.net/V3/01/jjj.tbbtyr.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
1579879808.540     72 10.231.0.20 TCP_MISS/200 262 GET
http://http.00.a.sophosxl.net/V3/01/jjj.tbbtyr.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
1579879809.335     72 10.231.0.20 TCP_MISS/200 367 GET
http://http.00.a.sophosxl.net/V3/01/jjj.tfgngvp.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
1579879809.400     72 10.231.0.20 TCP_MISS/200 256 GET
http://http.00.a.sophosxl.net/V3/01/sbagf.tbbtyrncvf.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
1579879809.763     72 10.231.0.20 TCP_MISS/200 252 GET
http://http.00.a.sophosxl.net/V3/01/btf.tbbtyr.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
1579879809.835   8004 10.231.0.200 TCP_TUNNEL/200 1205 CONNECT
www.google.com:443 - HIER_DIRECT/172.217.8.132 -
1579879810.279     72 10.231.0.20 TCP_MISS/200 256 GET
http://http.00.a.sophosxl.net/V3/01/rapelcgrq-goa0.tfgngvp.pbz.m/ -
HIER_DIRECT/3.225.217.224 -
1579879814.061     72 10.231.0.20 TCP_MISS/200 252 GET
http://http.00.a.sophosxl.net/V3/01/prageny.fbcubf.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
1579879814.061     72 10.231.0.20 TCP_MISS/200 252 GET
http://http.00.a.sophosxl.net/V3/01/prageny.fbcubf.pbz.m/ - HIER_DIRECT/
3.225.217.224 -
^C
[root at s03-prxy squid]#

-- 

---------------------
Erick Perez
Quadrian Enterprises S.A. - Panama, Republica de Panama
Skype chat: eaperezh
WhatsApp IM: +507-6675-5083
---------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200124/689324ed/attachment.htm>

From sdev36744 at gmail.com  Fri Jan 24 16:08:13 2020
From: sdev36744 at gmail.com (Sattafixjodi)
Date: Fri, 24 Jan 2020 10:08:13 -0600 (CST)
Subject: [squid-users] Squid not coming up with dynamic host certificate
	on ssl bum
In-Reply-To: <CAL0CaX_aZ+QSZV6OO9kUwFY8cKsBRL5ESUbbyFTCpO=t5W5gNw@mail.gmail.com>
References: <CAL0CaX_aZ+QSZV6OO9kUwFY8cKsBRL5ESUbbyFTCpO=t5W5gNw@mail.gmail.com>
Message-ID: <1579882093952-0.post@n4.nabble.com>

squid -vSquid Cache: Version 3.5.28Service Name: squidThis binary uses
OpenSSL 1.0.1e-fips 11 Feb 2013. For legal restrictions on distribution
seeconfigure options:  '--prefix=/usr' '--includedir=/usr/include'
'--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--localstatedir=/var' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-openssl' '--enable-ssl-crtd'
--enable-ltdl-convenience[c5278791 at ban-squid-proxy22 ~]$ cat
/etc/redhat-release CentOS release 6.10 (Final)[c5278791 at ban-squid-proxy22
~]$ 



-----
Satta Matka 
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sdev36744 at gmail.com  Fri Jan 24 16:14:13 2020
From: sdev36744 at gmail.com (Sattafixjodi)
Date: Fri, 24 Jan 2020 10:14:13 -0600 (CST)
Subject: [squid-users] Squid access.log
In-Reply-To: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
Message-ID: <1579882453444-0.post@n4.nabble.com>

Actually I'm newbie in this field so give me some time to answer and I want
learn more in this field thanks for your understanding...I see many requests
with CONNECT https:443 in my access.logHow are these entries triggered?They
produce errors in some accounting scripts 



-----
Satta Matka 
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eperez at quadrianweb.com  Fri Jan 24 17:28:59 2020
From: eperez at quadrianweb.com (Erick Perez - Quadrian Enterprises)
Date: Fri, 24 Jan 2020 12:28:59 -0500
Subject: [squid-users] squid 4.8 cannot connect to Skype for business
In-Reply-To: <CACXMG+t3wn=rLPhuFP6m6wz+4wNxvWjgMyvLji39OkrmEDAczQ@mail.gmail.com>
References: <CACXMG+t3wn=rLPhuFP6m6wz+4wNxvWjgMyvLji39OkrmEDAczQ@mail.gmail.com>
Message-ID: <CACXMG+tq62FWJAw95=XTVmmJTKOz_02zrbtQpD0nvQ2vELonWw@mail.gmail.com>

Perhaps, this additional info is relevant:

squid.conf:
# Squid is configured NOT to cache and only works as allow/deny networks
for browsing the internet.
cache deny all
acl SSL_ports port 443
acl SSL_ports port 8441
acl SSL_ports port 8442
acl SSL_ports port 9090
acl SSL_ports port 10000
http_port 10.230.5.71:8080
tcp_outgoing_address 10.230.5.73
via off
forwarded_for delete
follow_x_forwarded_for allow localhost
follow_x_forwarded_for deny all
request_header_access From deny all
request_header_access Server deny all
request_header_access WWW-Authenticate deny all
request_header_access Link deny all
request_header_access Cache-Control deny all
request_header_access Proxy-Connection deny all
request_header_access X-Cache deny all
request_header_access X-Cache-Lookup deny all
request_header_access Via deny all
request_header_access X-Forwarded-For deny all
request_header_access Pragma deny all
request_header_access Keep-Alive deny all
httpd_suppress_version_string on

On Fri, Jan 24, 2020 at 10:41 AM Erick Perez - Quadrian Enterprises <
eperez at quadrianweb.com> wrote:

> good day to all,
> my windows clients can connect successfully to regular skype, but cannot
> connect to skype for business. With the included extract of client machine
> 10.231.0.20 is there anything that can tell me why it is not connecting?
>
> ip  squid :  10.230.5.71
> tested client machine: 10.231.0.20
> skype for business: 16.0.12325.20280 32 bit
>
> *********
> squid -v:
> [root at s03-prxy squid]# squid -v
> Squid Cache: Version 4.8
> Service Name: squid
>
> This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions
> on distribution see https://www.openssl.org/source/license.html
>
> configure options:  '--build=x86_64-redhat-linux-gnu'
> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--exec_prefix=/usr'
> '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
> '--enable-auth'
> '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake'
> '--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
> '--enable-ident-lookups' '--enable-linux-netfilter'
> '--enable-removal-policies=heap,lru' '--enable-snmp'
> '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi'
> '--enable-security-cert-generators' '--enable-security-cert-validators'
> '--enable-icmp' '--with-aio' '--with-default-user=squid'
> '--with-filedescriptors=16384' '--with-dl' '--with-openssl'
> '--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl'
> '--disable-arch-native' '--without-nettle'
> 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu'
> 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
> -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
> -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
> --enable-ltdl-convenience
> [root at s03-prxy squid]#
> *****************
>
> access.log extract:
> [root at s03-prxy squid]# tail -f /var/log/squid/access.log | grep
> 10.231.0.20
> 1579879519.778   3539 10.231.0.200 TCP_TUNNEL/200 4974 CONNECT
> clients1.google.com:443 - HIER_DIRECT/172.217.15.206 -
> 1579879519.779   4340 10.231.0.200 TCP_TUNNEL/200 3490 CONNECT
> www.google.com:443 - HIER_DIRECT/172.217.8.132 -
> 1579879524.530 114397 10.231.0.200 TCP_TUNNEL_ABORTED/200 2051 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 34.218.169.26 -
> 1579879541.929 6131038 10.231.0.20 TCP_TUNNEL_ABORTED/200 1544733 CONNECT
> outlook.office365.com:443 - HIER_DIRECT/52.96.87.194 -
> 1579879541.929 4712050 10.231.0.20 TCP_TUNNEL_ABORTED/200 226325 CONNECT
> outlook.office365.com:443 - HIER_DIRECT/40.97.190.2 -
> 1579879541.944 126301 10.231.0.20 TCP_TUNNEL_ABORTED/200 27260 CONNECT
> outlook.office365.com:443 - HIER_DIRECT/52.96.33.82 -
> 1579879541.944 862124 10.231.0.20 TCP_TUNNEL_ABORTED/200 1061225 CONNECT
> outlook.office365.com:443 - HIER_DIRECT/52.96.88.18 -
> 1579879546.091    106 10.231.0.20 TCP_TUNNEL_ABORTED/200 4357 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879546.925    817 10.231.0.20 TCP_TUNNEL/200 17155 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879568.080     11 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879568.432    350 10.231.0.20 TCP_TUNNEL/200 7353 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879571.803 121672 10.231.0.20 TCP_TUNNEL/200 14037 CONNECT
> login.windows.net:443 - HIER_DIRECT/40.126.3.35 -
> 1579879576.554     69 10.231.0.20 TCP_MISS/304 449 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> - HIER_DIRECT/8.252.64.254 -
> 1579879576.645     69 10.231.0.20 TCP_MISS/304 449 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/authrootstl.cab?
> - HIER_DIRECT/8.252.64.254 -
> 1579879579.790   5275 10.231.0.200 TCP_TUNNEL/200 4088 CONNECT
> safebrowsing.googleapis.com:443 - HIER_DIRECT/172.217.8.74 -
> 1579879580.975     72 10.231.0.202 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879580.976     72 10.231.0.202 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879580.976     72 10.231.0.202 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879580.976     72 10.231.0.202 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879581.049     72 10.231.0.202 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879581.050     71 10.231.0.202 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879584.088  68045 10.231.0.20 TCP_TUNNEL/200 5636 CONNECT
> config.edge.skype.com:443 - HIER_DIRECT/13.107.3.128 -
> 1579879586.050  14003 10.231.0.200 TCP_TUNNEL/200 7371 CONNECT
> pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
> 146.123.20.137 -
> 1579879589.994     48 10.231.0.20 TCP_MISS/200 1978 GET
> http://ocsp.digicert.com/MFEwTzBNMEswSTAJBgUrDgMCGgUABBTBL0V27RVZ7LBduom%2FnYB45SPUEwQU5Z1ZMIJHWMys%2BghUNoZ7OrUETfACEA8sEMlbBsCTf7jUSfg%2BhWk%3D
> - HIER_DIRECT/72.21.91.29 application/ocsp-response
> 1579879590.031    455 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
> sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
> 1579879590.596    563 10.231.0.20 TCP_TUNNEL/200 7556 CONNECT
> sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
> 1579879593.377 118087 10.231.0.20 TCP_TUNNEL_ABORTED/200 1987 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 52.10.0.129 -
> 1579879611.227     69 10.231.0.202 TCP_MISS/304 449 GET
> http://ctldl.windowsupdate.com/msdownload/update/v3/static/trustedr/en/disallowedcertstl.cab?
> - HIER_DIRECT/8.252.64.254 -
> 1579879611.312     69 10.231.0.202 TCP_MISS/304 259 GET
> http://crl.pki.goog/GTS1O1.crl - HIER_DIRECT/172.217.3.131 -
> 1579879612.106    336 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
> sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
> 1579879612.585    477 10.231.0.20 TCP_TUNNEL/200 7700 CONNECT
> sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
> 1579879613.465     48 10.231.0.20 TCP_MISS/200 1978 GET
> http://ocsp.digicert.com/MFEwTzBNMEswSTAJBgUrDgMCGgUABBTBL0V27RVZ7LBduom%2FnYB45SPUEwQU5Z1ZMIJHWMys%2BghUNoZ7OrUETfACEAiIzVJfGSRETRSlgpHeuVI%3D
> - HIER_DIRECT/72.21.91.29 application/ocsp-response
> 1579879613.729    190 10.231.0.20 TCP_MISS/200 2597 GET
> http://ocsp.msocsp.com/MFQwUjBQME4wTDAJBgUrDgMCGgUABBQphfxhPb4vsBIPXkIOTJ7D1Z79fAQUCP4ln3TqhwTCvLuOqDhfM8bRbGUCEy0ABoRa5qu8lnR3dcQAAAAGhFo%3D
> - HIER_DIRECT/104.18.24.243 application/ocsp-response
> 1579879615.429     73 10.231.0.20 TCP_MISS/200 2597 GET
> http://ocsp.msocsp.com/MFQwUjBQME4wTDAJBgUrDgMCGgUABBQirwAcgHViBybgyJMa7KdCHDISOgQUenuMwc%2FnoMoc1Gv6%2B%2BEzww8aop0CExYAAvvgLxYAucV3A0IAAAAC%2B%2BA%3D
> - HIER_DIRECT/104.18.24.243 application/ocsp-response
> 1579879615.601    510 10.231.0.20 TCP_TUNNEL_ABORTED/200 7961 CONNECT
> login.microsoftonline.com:443 - HIER_DIRECT/20.190.131.98 -
> 1579879615.769    151 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
> fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
> 1579879615.921    149 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
> fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
> 1579879616.936 117994 10.231.0.202 TCP_TUNNEL_ABORTED/200 1987 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 52.10.0.129 -
> 1579879627.471  10590 10.231.0.200 TCP_TUNNEL/200 2135 CONNECT
> pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
> 146.123.20.137 -
> 1579879629.024  15981 10.231.0.20 TCP_TUNNEL_ABORTED/200 35500 CONNECT
> webdir0a.online.lync.com:443 - HIER_DIRECT/52.112.64.14 -
> 1579879639.086      0 10.231.0.20 NONE/503 0 CONNECT
> lyncdiscoverinternal.aaaaair.com:443 - HIER_NONE/- -
> 1579879639.087      0 10.231.0.20 TCP_MISS/503 4569 GET
> http://lyncdiscoverinternal.aaaaair.com/? - HIER_NONE/- text/html
> 1579879639.211    114 10.231.0.20 TCP_MISS/200 789 GET
> http://lyncdiscover.aaaaair.com/? - HIER_DIRECT/200.46.240.179
> application/vnd.microsoft.rtc.autodiscover+xml
> 1579879644.031  29867 10.231.0.20 TCP_TUNNEL_ABORTED/200 23372 CONNECT
> webdir0a.online.lync.com:443 - HIER_DIRECT/52.112.64.14 -
> 1579879644.529 114376 10.231.0.200 TCP_TUNNEL_ABORTED/200 1987 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 52.10.0.129 -
> 1579879661.924 451876 10.231.0.20 TCP_TUNNEL_ABORTED/200 8663 CONNECT
> outlook.office365.com:443 - HIER_DIRECT/52.96.33.82 -
> 1579879665.721  66079 10.231.0.202 TCP_TUNNEL/200 6277 CONNECT
> 4.sophosxl.net:443 - HIER_DIRECT/3.227.84.245 -
> 1579879675.921     72 10.231.0.202 TCP_MISS/200 256 GET
> http://http.00.a.sophosxl.net/V3/01/cynl.tbbtyr.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879676.722  65561 10.231.0.202 TCP_TUNNEL/200 7269 CONNECT
> 4.sophosxl.net:443 - HIER_DIRECT/3.227.84.245 -
> 1579879681.592  13173 10.231.0.200 TCP_TUNNEL/200 6761 CONNECT
> pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
> 146.123.20.137 -
> 1579879695.487     15 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879695.599    110 10.231.0.20 TCP_TUNNEL/200 17155 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879699.006  59919 10.231.0.20 NONE/503 0 CONNECT
> lyncdiscover.aaaaair.com:443 - HIER_NONE/- -
> 1579879699.006  59714 10.231.0.20 NONE/503 0 CONNECT abs.aaaaair.com:443
> - HIER_NONE/- -
> 1579879705.254 212866 10.231.0.20 TCP_TUNNEL/200 10722 CONNECT
> 4.sophosxl.net:443 - HIER_DIRECT/34.234.127.160 -
> 1579879705.255 209910 10.231.0.20 TCP_TUNNEL/200 12951 CONNECT
> 4.sophosxl.net:443 - HIER_DIRECT/34.234.127.160 -
> 1579879713.373 118074 10.231.0.20 TCP_TUNNEL_ABORTED/200 605 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 52.10.0.129 -
> 1579879716.652     11 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879716.713     59 10.231.0.20 TCP_TUNNEL/200 7401 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879724.032     72 10.231.0.200 TCP_MISS/200 257 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.pbcnnve.pbz.m/ - HIER_DIRECT/
> 3.222.35.82 -
> 1579879734.719  66296 10.231.0.200 TCP_TUNNEL/200 6101 CONNECT
> 4.sophosxl.net:443 - HIER_DIRECT/3.227.84.245 -
> 1579879736.935 117987 10.231.0.202 TCP_TUNNEL_ABORTED/200 605 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 52.10.0.129 -
> 1579879737.777     11 10.231.0.20 TCP_TUNNEL_ABORTED/200 188 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879737.940    161 10.231.0.20 TCP_TUNNEL/200 7353 CONNECT
> sip.aaaaair.com:443 - HIER_DIRECT/200.46.240.195 -
> 1579879754.531     68 10.231.0.200 TCP_MISS/304 259 GET
> http://crl.pki.goog/gsr2/gsr2.crl - HIER_DIRECT/172.217.3.131 -
> 1579879759.398    321 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
> sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
> 1579879759.922    522 10.231.0.20 TCP_TUNNEL/200 7556 CONNECT
> sipfed.online.lync.com:443 - HIER_DIRECT/52.112.65.75 -
> 1579879761.075  16921 10.231.0.200 TCP_TUNNEL/200 7350 CONNECT
> pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
> 146.123.20.137 -
> 1579879764.529 114383 10.231.0.200 TCP_TUNNEL_ABORTED/200 605 CONNECT
> dzr-mcs-amzn-us-west-2-fa88.upe.p.hmr.sophos.com:443 - HIER_DIRECT/
> 52.10.0.129 -
> 1579879778.623   8111 10.231.0.200 TCP_TUNNEL/200 774 CONNECT
> pssgui.aaaa.travel.airservices.svcs.entsvcs.com:443 - HIER_DIRECT/
> 146.123.20.137 -
> 1579879781.400    324 10.231.0.20 TCP_TUNNEL_ABORTED/200 6298 CONNECT
> sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
> 1579879781.883    481 10.231.0.20 TCP_TUNNEL/200 7700 CONNECT
> sipfed0a.online.lync.com:443 - HIER_DIRECT/52.112.64.11 -
> 1579879782.075    142 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
> fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
> 1579879782.225    146 10.231.0.20 TCP_TUNNEL/200 39 CONNECT
> fs.aaaaair.com:443 - HIER_DIRECT/94.188.209.80 -
> 1579879800.250      0 10.231.0.20 NONE/503 0 CONNECT
> lyncdiscoverinternal.aaaaair.com:443 - HIER_NONE/- -
> 1579879800.251      0 10.231.0.20 TCP_MISS/503 4569 GET
> http://lyncdiscoverinternal.aaaaair.com/? - HIER_NONE/- text/html
> 1579879800.256      5 10.231.0.20 TCP_MISS/200 789 GET
> http://lyncdiscover.aaaaair.com/? - HIER_DIRECT/200.46.240.179
> application/vnd.microsoft.rtc.autodiscover+xml
> 1579879801.905     72 10.231.0.200 TCP_MISS/200 262 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.tbbtyr.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> 1579879808.540     72 10.231.0.20 TCP_MISS/200 262 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.tbbtyr.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> 1579879809.335     72 10.231.0.20 TCP_MISS/200 367 GET
> http://http.00.a.sophosxl.net/V3/01/jjj.tfgngvp.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> 1579879809.400     72 10.231.0.20 TCP_MISS/200 256 GET
> http://http.00.a.sophosxl.net/V3/01/sbagf.tbbtyrncvf.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> 1579879809.763     72 10.231.0.20 TCP_MISS/200 252 GET
> http://http.00.a.sophosxl.net/V3/01/btf.tbbtyr.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> 1579879809.835   8004 10.231.0.200 TCP_TUNNEL/200 1205 CONNECT
> www.google.com:443 - HIER_DIRECT/172.217.8.132 -
> 1579879810.279     72 10.231.0.20 TCP_MISS/200 256 GET
> http://http.00.a.sophosxl.net/V3/01/rapelcgrq-goa0.tfgngvp.pbz.m/ -
> HIER_DIRECT/3.225.217.224 -
> 1579879814.061     72 10.231.0.20 TCP_MISS/200 252 GET
> http://http.00.a.sophosxl.net/V3/01/prageny.fbcubf.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> 1579879814.061     72 10.231.0.20 TCP_MISS/200 252 GET
> http://http.00.a.sophosxl.net/V3/01/prageny.fbcubf.pbz.m/ - HIER_DIRECT/
> 3.225.217.224 -
> ^C
> [root at s03-prxy squid]#
>
> --
>
> ---------------------
> Erick Perez
> Quadrian Enterprises S.A. - Panama, Republica de Panama
> Skype chat: eaperezh
> WhatsApp IM: +507-6675-5083
> ---------------------
>


-- 

---------------------
Erick Perez
Quadrian Enterprises S.A. - Panama, Republica de Panama
Skype chat: eaperezh
WhatsApp IM: +507-6675-5083
---------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200124/075c1261/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 26 10:25:44 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 26 Jan 2020 23:25:44 +1300
Subject: [squid-users] Squid access.log
In-Reply-To: <1579882453444-0.post@n4.nabble.com>
References: <d9edc7a9ff9e45d4946cfaf3dfb9652a@ads.uni-passau.de>
 <1579882453444-0.post@n4.nabble.com>
Message-ID: <79592f48-2d20-fa43-1c13-878730729cea@treenet.co.nz>

On 25/01/20 5:14 am, Sattafixjodi wrote:
> Actually I'm newbie in this field so give me some time to answer and I want
> learn more in this field thanks for your understanding...I see many requests
> with CONNECT https:443 in my access.logHow are these entries triggered?They
> produce errors in some accounting scripts 
> 

My and Alex answers earlier in the thread explain that.

Amos


From netadmin at aicta.ro  Mon Jan 27 18:52:15 2020
From: netadmin at aicta.ro (netadmin)
Date: Mon, 27 Jan 2020 12:52:15 -0600 (CST)
Subject: [squid-users] Squid ICAP -> Sophos SAVDI -> read_ahead_gap question
Message-ID: <1580151135195-0.post@n4.nabble.com>

My system:
7th generation Intel processor with 32 GB RAM
HDD on SATA without RAID
OS: Linux Slackware 64 bit
Squid version: 4.10 (compiled from sources)
Number of clients using ICAP: 20
Relevant squid.conf options:
reply_body_max_size 20 MB localnet
http_port 192.168.1.1:3128 ssl-bump \
cert=/usr/local/squid/ssl_cert/myCA.pem \
generate-host-certificates=on dynamic_cert_mem_cache_size=16MB
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 16MB
sslcrtd_children 32 startup=10 idle=2
cache_mem 4096 MB
maximum_object_size_in_memory 20 MB
maximum_object_size 200 MB
cache_dir ufs <...> 10240 16 256
quick_abort_min -1
read_ahead_gap 20 MB
icap_enable on
icap_service_failure_limit 20
icap_service sophosicap respmod_precache icap://127.0.0.1:4020/sophos
adaptation_access sophosicap deny CONNECT
adaptation_access sophosicap allow all

If I try to download a 20 MB file on all workstations at the same time,
without the option "read_ahead_gap 20 MB", the download fails on a small
number of workstations.
After about a week of searching and reading the documentation I tried the
above option, the download errors are gone and the processor load is low.
I do not fully understand the transfer mechanism between the ICAP client
(Squid) and the ICAP server (Sophos SAVDI), but I noticed that the download
time is shorter if the file is stored in RAM cache (using cache_mem)
compared to disk storage.
If I use disk storage for the 20 MB file, during the simultaneous download
the processor load reaches 100% - this I think not because of the ICAP
server - and download errors occur.
Is the maximum supported size for a file transmitted to the ICAP server 20
MB?
Is there anything wrong with my settings?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Mon Jan 27 19:28:33 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 27 Jan 2020 14:28:33 -0500
Subject: [squid-users] Squid ICAP -> Sophos SAVDI -> read_ahead_gap
 question
In-Reply-To: <1580151135195-0.post@n4.nabble.com>
References: <1580151135195-0.post@n4.nabble.com>
Message-ID: <7d3fd5e9-07ce-9140-c484-96c80b95c7d9@measurement-factory.com>

On 1/27/20 1:52 PM, netadmin wrote:

> reply_body_max_size 20 MB localnet

I would not set this option unless there is a good reason for setting it.


> maximum_object_size_in_memory 20 MB

If you want to cache objects that approach 20MB in body size, then raise
this to at least 21MB to account for overheads. The overheads (e.g.,
HTTP headers) are not that large, but it is easier and safer than
computing them directly.


> read_ahead_gap 20 MB

FYI: Due to Squid implementation deficiencies/limitations, a huge gap
like this is likely to cause large, unpooled memory allocations on
virtually every received HTTP response with a body, regardless of that
response body size. This should not cause bugs or transaction aborts,
but you should be aware of the high performance cost of such
allocations, including stalling the entire Squid worker for a few
milliseconds (at least).


> If I try to download a 20 MB file on all workstations at the same time,
> without the option "read_ahead_gap 20 MB", the download fails on a small
> number of workstations.

This is an indication of a bug somewhere. The usual suspects are: Squid,
the ICAP service, the HTTP origin server.


> If I use disk storage for the 20 MB file, during the simultaneous download
> the processor load reaches 100% - this I think not because of the ICAP
> server - and download errors occur.

Persistent 100% CPU usage with 20 concurrent transactions on decent
hardware is also a red flag.


> Is the maximum supported size for a file transmitted to the ICAP server 20
> MB?

No. There is virtually no maximum.


> Is there anything wrong with my settings?

What you want should "work" (in principle) without any special settings.
And it should work with/through a memory cache given a large-enough
maximum_object_size_in_memory setting.

Some of your settings might stress Squid because they set very tight
limits for 20MB objects, with no room for overheads (see above for
specifics), but even that should not really break things (especially in
your non-SMP setup) -- it may cause more cache misses and such.

I would suspect a bug (possible, but not necessarily in Squid).
Analyzing a cache.log with debug_options set to ALL,9 will probably be
enough to pin-point the culprit but may require some developer
knowledge/effort because those 20 concurrent transactions will create a
lot of debugging noise.

Alex.


From aashutosh.xyz at gmail.com  Wed Jan 29 03:32:04 2020
From: aashutosh.xyz at gmail.com (aashutosh kalyankar)
Date: Tue, 28 Jan 2020 19:32:04 -0800
Subject: [squid-users] Issues with TLS inspection- 3 Follow up question
In-Reply-To: <mailman.5.1579780801.6989.squid-users@lists.squid-cache.org>
References: <mailman.5.1579780801.6989.squid-users@lists.squid-cache.org>
Message-ID: <CABi+ORKRrorqShjoqv-q9BArB0Ht_CTQ+yZvEWSBEHROBzEHeg@mail.gmail.com>

As suggested, I removed the settings for explicit proxy and have NAT move
the HTTP/HTTPs request to squid intercept ports, and all the web traffic is
now going through the proxy server (I see certs and connection requests in
the cache log file).

I have a follow-up question. Any idea how do we accurately test to make
sure if SSL bump is happening for a connection?
I have doubts as I was expecting, "Your connection is not Private" error
when no CA cert on my browser. CA cert or no CA cert in my cert-manager
does not affect the connection. Also, I read in some articles that dropbox
and apple app store will not work if SSL Bump is active, but it works for
me without any issues.
I was able to verify that websites in the ssl::server_name acl whitelist do
not use squid generated certs for connection, as expected.

Squid file:

acl localnet src 172.22.22.0/24
acl localnet src 172.16.10.0/24
acl localnet src 172.18.10.0/24
acl localnet src 10.0.0.0/8
http_access allow localnet
http_access allow localhost

acl CONNECT method CONNECT

http_access deny all
http_port 3129 intercept
https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_certs/myCA1.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl nobumpSites ssl::server_name "/etc/squid/whitelist.txt"

ssl_bump peek step1 all
ssl_bump splice nobumpSites
ssl_bump stare step2
ssl_bump bump step3
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
debug_options ALL,1 84,9 11,9 33,3 20,3 83,3 85,4 85,9
dns_v4_first on
shutdown_lifetime 5

Thanks for the help!
Aashutosh K




On Thu, Jan 23, 2020 at 4:01 AM <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Issues with TLS inspection-2 (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 24 Jan 2020 00:04:50 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Issues with TLS inspection-2
> Message-ID: <c91678b8-fd3f-2535-8d14-8dbbdcae9324 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 23/01/20 3:11 pm, aashutosh kalyankar wrote:
> >     From: Amos Jeffrie>
> >     Secondly, make sure that your tests are accurately emulating how
> clients
> >     would "use" the proxy. That means making connections from a test
> machine
> >     directly to the Internet and seeing if the routing and NAT delivers
> the
> >     traffic to Squid properly.
> >
> >
> > I am using a chromebook to test. In the configuration section of the
> > wireless network there is an option to add proxy hostname and proxy port
> > based on protocols.
> > Http proxy     :  proxy-tls 80
> > HTTPS proxy:  proxy-tls 443
> >
>
> That is part of your problem. Those are settings for explicit proxy.
>
> With intercept the clients knows nothing about any proxy. They are just
> connecting to a web server directly (but *NAT* sends it to Squid instead).
>
>
> >
> >      - Use cache.log to view the traffic coming into the proxy. It will
> be
> >     request messages with a prefix line indicating "Client HTTP request".
> >     Make sure that prefix line says the remote Internet IP address and
> port
> >     80/443 you were testing with.
> >      - If you want confirm that access.log has a transaction entry for
> the
> >     URL you tested with ORIGINAL_DST and the server IP.
> >
> > Sample cache.log for a test I did for neverssl.com <http://neverssl.com>
> >
> > 2020/01/22 17:08:30.236 kid1| 11,2| client_side.cc(2346)
> > parseHttpRequest: HTTP Client local=172.22.22.148:80
> > <http://172.22.22.148:80> remote=172.22.22.151:34728
> > <http://172.22.22.151:34728> FD 12 flags=33
> > 2020/01/22 17:08:30.236 kid1| 11,2| client_side.cc(2347)
> > parseHttpRequest: HTTP Client REQUEST:
> > ---------
> > GET http://neverssl.com/ HTTP/1.1
> > Host: neverssl.com <http://neverssl.com>
> > Proxy-Connection: keep-alive
>
>
> ...
> >
> >     > http_access deny !Safe_ports
> >     > http_access deny CONNECT !SSL_ports
> >
> >      ... this is where all your custom http_access rules are supposed to
> be.
> >     The Safe_ports and SSL_Ports lines above are DoS and hijack
> protections.
> >
> >
> > IIUC, These are not required to be here so I commented out those lines.
> >
>
> Sorry if I was not clear. They should be the first http_access lines in
> your config. Local policy rules follow them. Then the final "deny all"
> rule to block anything not allowed by your policy.
>
>
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 65, Issue 33
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200128/65af6da0/attachment.htm>

From netadmin at aicta.ro  Wed Jan 29 09:26:04 2020
From: netadmin at aicta.ro (netadmin)
Date: Wed, 29 Jan 2020 03:26:04 -0600 (CST)
Subject: [squid-users] Squid ICAP -> Sophos SAVDI -> read_ahead_gap
	question
In-Reply-To: <7d3fd5e9-07ce-9140-c484-96c80b95c7d9@measurement-factory.com>
References: <1580151135195-0.post@n4.nabble.com>
 <7d3fd5e9-07ce-9140-c484-96c80b95c7d9@measurement-factory.com>
Message-ID: <1580289964479-0.post@n4.nabble.com>

> reply_body_max_size 20 MB localnet
This is the last line for the respective option, before it there are others
that set limits, for example:
reply_body_max_size 200 MB ubuntu_updates
The line has the role of blocking the download of files larger than 20 MB
for which antivirus scanning (with the current settings) takes too long.
> read_ahead_gap 20 MB
The setting was wrong, it works for my particular case, but it creates
problems when downloading the package updates for Ubuntu, which do not go
through the ICAP filter. Now I have reduced the value to 64 KB and the
problem when downloading the Ubuntu updates has disappeared.
Simultaneous download on 20 workstations, with antivirus scanning through
the ICAP server works without problems only for files of up to 10 MB and
anyway a 100% load on the processor appears for a period of several seconds
from the moment the download for customers begins.
For example, if I use wget:
wget
http://mirror.slackware.hr/slackware/slackware-13.1/slackware/kde/oxygen-icons-4.4.3-i486-1.txz
--2020-01-29 10:32:37 -
http://mirror.slackware.hr/slackware/slackware-13.1/slackware/kde/oxygen-icons-4.4.3-i486-1.txz
Connecting to 192.168.1.1:3128 ... connected.
Proxy request sent, awaiting response ... 200 OK
Length: 21117900 (20M)
Saving to: 'oxygen-icons-4.4.3-i486-1.txz'
oxygen-icons-4.4.3- 47% [========>] 9.47M 609KB / s in 18s
2020-01-29 10:33:25 (553 KB / s) - Connection closed at byte 9926425.
Retrying.
--2020-01-29 10: 33: 26-- (try: 2)
http://mirror.slackware.hr/slackware/slackware-13.1/slackware/kde/oxygen-icons-4.4.3-i486-1
.txz
Connecting to 192.168.1.1:3128 ... connected.
Proxy request sent, awaiting response ... 206 Partial Content
Length: 21117900 (20M), 11191475 (11M) remaining
Saving to: 'oxygen-icons-4.4.3-i486-1.txz'
oxygen-icons-4.4.3- 100% [+++++++++ ==========>] 20.14M 2.08MB / s in 7.9s
2020-01-29 10:33:46 (1.34 MB / s) - 'oxygen-icons-4.4.3-i486-1.txz' saved
[21117900/21117900]
I think the "read_ahead_gap 20 MB" option here helps maintain the
connection.
So the problem is not at the client-to-server connection (Squid -> Sophos
SAVDI), nor at the antivirus scan but it can be in the server-to-client
response buffer.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Jan 29 12:45:11 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Jan 2020 01:45:11 +1300
Subject: [squid-users] Issues with TLS inspection- 3 Follow up question
In-Reply-To: <CABi+ORKRrorqShjoqv-q9BArB0Ht_CTQ+yZvEWSBEHROBzEHeg@mail.gmail.com>
References: <mailman.5.1579780801.6989.squid-users@lists.squid-cache.org>
 <CABi+ORKRrorqShjoqv-q9BArB0Ht_CTQ+yZvEWSBEHROBzEHeg@mail.gmail.com>
Message-ID: <1ac8f2c2-c2f3-c134-48dd-1b51adf845cc@treenet.co.nz>

On 29/01/20 4:32 pm, aashutosh kalyankar wrote:
> As suggested, I removed the settings for explicit proxy and have NAT
> move the HTTP/HTTPs request to squid intercept ports, and all the web
> traffic is now going through the proxy server (I see certs and
> connection requests in the cache log file).
> 
> I have a follow-up question. Any idea how do we accurately test to make
> sure if SSL bump is happening for a connection?

Use any tools that you like which can show the TLS server certificate
and CA certificate that signed it.
 * When 'bump' action takes place the certificate will be signed by the
CA cert you configured Squid to use.
 * When 'splice' action takes place the certificate will be its normal one.


> I have doubts as I was expecting, "Your connection is not Private" error
> when no CA cert on my browser.

That message does not mean what the hype claims. It is security theatre
by the Browser folks to force web developers to use TLS / HTTPS.


> CA cert or no CA cert in my cert-manager
> does not affect the connection.

*That* is a worry. You should at least see a difference between those
two cases.


> Also, I read in some articles that
> dropbox and apple app store will not work if SSL Bump is active, but it
> works for me without any issues.

That depends on device type and such details. Squid is also constantly
improving in these areas.


> I was able to verify that websites in the ssl::server_name acl whitelist
> do not use squid generated certs for connection, as expected.
> 
> Squid file:
>> acl localnet src?172.22.22.0/24 <http://172.22.22.0/24>
>> acl localnet src?172.16.10.0/24 <http://172.16.10.0/24>
>> acl localnet src?172.18.10.0/24 <http://172.18.10.0/24>
>> acl localnet src?10.0.0.0/8 <http://10.0.0.0/8>


acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

>> http_access allow localnet
>> http_access allow localhost
>>
>> http_access deny all


>> http_port 3129 intercept
>> https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_certs/myCA1.pem
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> acl nobumpSites ssl::server_name "/etc/squid/whitelist.txt"
>>
>> ssl_bump peek step1 all

No need for that "all".

>> ssl_bump splice nobumpSites
>> ssl_bump stare step2
>> ssl_bump bump step3


Amos


From ml at netfence.it  Thu Jan 30 07:50:34 2020
From: ml at netfence.it (Andrea Venturoli)
Date: Thu, 30 Jan 2020 08:50:34 +0100
Subject: [squid-users] Squid won't download intermediate certificates
Message-ID: <3eb1e57d-0be9-bc82-607d-9d078ea70525@netfence.it>

Hello.

I'm experimenting SSLBump and I've got a problem: when a client visits a 
site which won't provide intermediate SSL certificates, the connection 
will fail.
I read Squid 4 should download such certificates itself, however this 
does not succeed.
I see in the logs something like:
> 1580334345.045      1 - TCP_DENIED/403 3634 GET http://secure.globalsign.com/cacert/gsorganizationvalsha2g2r1.crt - HIER_NONE/- text/html;charset=utf-8

Seems like an ACL problem.
There is no source IP, but a - (dash): I guess this means the connection 
was originated from Squid itself.

Is there a specific keyword I need to use to allow such connections?
"localhost" doesn't seem to do the trink.

Any help appreciated.

  bye & Thanks
	av.


From info at schroeffu.ch  Thu Jan 30 08:15:15 2020
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Thu, 30 Jan 2020 08:15:15 +0000
Subject: [squid-users] Squid won't download intermediate certificates
In-Reply-To: <3eb1e57d-0be9-bc82-607d-9d078ea70525@netfence.it>
References: <3eb1e57d-0be9-bc82-607d-9d078ea70525@netfence.it>
Message-ID: <617928a7d875bd2d95b2b0e62cd04df7@schroeffu.ch>

Hi av,

have had the same issue due to authenticate any user before passing the proxy. Squid couldn't fetch the intermediate certificates.
I added the following in squid.conf before the line "acl Authenticated_Users proxy_auth REQUIRED":

###
#Allow fetch intermediate certs before required authentication
acl fetched_certificate transaction_initiator certificate-fetching
cache allow fetched_certificate
http_access allow fetched_certificate
###

Hope this helps you too.

Lot regards
Schroeffu

PS: DKIM verification failed for sender ml at netfence.it

30. Januar 2020 08:51, "Andrea Venturoli" <ml at netfence.it> schrieb:

> Hello.
> 
> I'm experimenting SSLBump and I've got a problem: when a client visits a
> site which won't provide intermediate SSL certificates, the connection
> will fail.
> I read Squid 4 should download such certificates itself, however this
> does not succeed.
> I see in the logs something like:
> 
>> 1580334345.045 1 - TCP_DENIED/403 3634 GET
>> http://secure.globalsign.com/cacert/gsorganizationvalsha2g2r1.crt - HIER_NONE/-
>> text/html;charset=utf-8
> 
> Seems like an ACL problem.
> There is no source IP, but a - (dash): I guess this means the connection
> was originated from Squid itself.
> 
> Is there a specific keyword I need to use to allow such connections?
> "localhost" doesn't seem to do the trink.
> 
> Any help appreciated.
> 
> bye & Thanks
> av.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ml at netfence.it  Thu Jan 30 08:52:36 2020
From: ml at netfence.it (Andrea Venturoli)
Date: Thu, 30 Jan 2020 09:52:36 +0100
Subject: [squid-users] Squid won't download intermediate certificates
In-Reply-To: <617928a7d875bd2d95b2b0e62cd04df7@schroeffu.ch>
References: <3eb1e57d-0be9-bc82-607d-9d078ea70525@netfence.it>
 <617928a7d875bd2d95b2b0e62cd04df7@schroeffu.ch>
Message-ID: <e730ed2e-fd07-942b-42f0-c54a89ee0eb0@netfence.it>

On 2020-01-30 09:15, info at schroeffu.ch wrote:
> acl fetched_certificate transaction_initiator certificate-fetching
> cache allow fetched_certificate
> http_access allow fetched_certificate

Thanks!
This is exactly it.

  bye
	av.


From alex at esines.cu  Fri Jan 31 16:40:38 2020
From: alex at esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez?=)
Date: Fri, 31 Jan 2020 11:40:38 -0500
Subject: [squid-users] squid4 config file doub
Message-ID: <792dda25-1569-63ed-23b9-f80651b07a40@esines.cu>

Hello community, I am using squid in debian 10 and for security reasons 
in the entity where i work we have as a policy to change the default 
configuration path for squid. I did this in the /etc/init.d/squid file 
and after restarting the proxy the change had no effect. Any idea why 
this happens after changing the value of the CONFIG variable in 
/etc/init.d/squid file?

-- 
Saludos cordiales

Lic. Alex Guti?rrez Mart?nez



