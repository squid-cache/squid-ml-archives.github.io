From alberto.senni at gmail.com  Mon Jun  1 13:45:28 2020
From: alberto.senni at gmail.com (Alberto Senni)
Date: Mon, 1 Jun 2020 15:45:28 +0200
Subject: [squid-users] FATAL: mimeLoadIcon: cannot parse internal URL
Message-ID: <89d00e1a-0718-27dd-72bd-697a23fd3bc2@gmail.com>

Hi to? all,

on my linux Devuan configured as transparent router, squid 4 exit with 
FATAL error (from syslog):

May 31 14:36:02 beofw (squid-1): FATAL: mimeLoadIcon: cannot parse 
internal URL: http://beofw:0/squid-internal-static/icons/silk/image.png
May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001 
exited with status 1
May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001 
will not be restarted for 3600 seconds due to repeated, frequent failures
May 31 14:36:02 beofw squid[3966]: Exiting due to repeated, frequent 
failures

but all is ok without intercept mode.

What is the problem ?

Thanks to all
Alberto Senni

my squid.conf is:

acl reteInterna src 192.168.100.0/24 # RFC 1918 local private network (LAN)
acl SSL_ports port 443 # RFC 4291 link-local (directly plugged) machines
acl Safe_ports port 80
acl Safe_ports port 21 # http
acl Safe_ports port 443 # ftp
acl Safe_ports port 70 # https
acl Safe_ports port 3128 # gopher
acl Safe_ports port 1025-65535 # wais
acl Safe_ports port 280 # unregistered ports
acl Safe_ports port 488 # http-mgmt
acl Safe_ports port 591 # gss-http
acl Safe_ports port 777 # filemaker
acl CONNECT method CONNECT # multiling http
#acl squidInt url_regex? ^/squid-internal-static/
http_access allow reteInterna
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access allow localhost
http_access deny manager
http_access deny all
http_port 192.168.100.254:3128 transparent
host_verify_strict off
cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid
refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
refresh_pattern ^gopher:??? 1440??? 0%??? 1440
refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
refresh_pattern .??? ??? 0??? 20%??? 4320
cache_effective_user proxy
cache_mgr root at studiozocca.com
check_hostnames off
cache_effective_group proxy
debug_options ALL,3

root at beofw:/home/alsen# uname -a
Linux beofw 4.19.0-9-amd64 #1 SMP Debian 4.19.118-2 (2020-04-29) x86_64 
GNU/Linux

root at beofw:/home/alsen# squid -v
Squid Cache: Version 4.6
Service Name: squid
Debian linux
configure options:? '--build=x86_64-linux-gnu' '--prefix=/usr' 
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man' 
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' 
'--localstatedir=/var' '--libexecdir=${prefix}/lib/squid' '--srcdir=.' 
'--disable-maintainer-mode' '--disable-dependency-tracking' 
'--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 
-fdebug-prefix-map=/build/squid-EQa0Xz/squid-4.6=. 
-fstack-protector-strong -Wformat -Werror=format-security -Wdate-time 
-D_FORTIFY_SOURCE=2 -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' 
'BUILDCXX=x86_64-linux-gnu-g++' '--with-build-environment=default' 
'--enable-build-info=Debian linux' '--datadir=/usr/share/squid' 
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' 
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' 
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' 
'--enable-removal-policies=lru,heap' '--enable-delay-pools' 
'--enable-cache-digests' '--enable-icap-client' 
'--enable-follow-x-forwarded-for' 
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' 
'--enable-auth-digest=file,LDAP' 
'--enable-auth-negotiate=kerberos,wrapper' 
'--enable-auth-ntlm=fake,SMB_LM' 
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' 
'--enable-security-cert-validators=fake' 
'--enable-storeid-rewrite-helpers=file' 
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' 
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' 
'--disable-translation' '--with-swapdir=/var/spool/squid' 
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' 
'--with-filedescriptors=65536' '--with-large-files' 
'--with-default-user=proxy' '--with-gnutls' '--enable-linux-netfilter' 
'build_alias=x86_64-linux-gnu' 'CC=x86_64-linux-gnu-gcc' 'CFLAGS=-g -O2 
-fdebug-prefix-map=/build/squid-EQa0Xz/squid-4.6=. 
-fstack-protector-strong -Wformat -Werror=format-security -Wall' 
'LDFLAGS=-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' 
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXX=x86_64-linux-gnu-g++' 
'CXXFLAGS=-g -O2 -fdebug-prefix-map=/build/squid-EQa0Xz/squid-4.6=. 
-fstack-protector-strong -Wformat -Werror=format-security'




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200601/7a4b7bb5/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun  1 14:15:05 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Jun 2020 02:15:05 +1200
Subject: [squid-users] FATAL: mimeLoadIcon: cannot parse internal URL
In-Reply-To: <89d00e1a-0718-27dd-72bd-697a23fd3bc2@gmail.com>
References: <89d00e1a-0718-27dd-72bd-697a23fd3bc2@gmail.com>
Message-ID: <28c47a71-87f2-6152-4fc6-8b97d4bffd53@treenet.co.nz>

On 2/06/20 1:45 am, Alberto Senni wrote:
> Hi to? all,
> 
> on my linux Devuan configured as transparent router, squid 4 exit with
> FATAL error (from syslog):
> 
> May 31 14:36:02 beofw (squid-1): FATAL: mimeLoadIcon: cannot parse
> internal URL: http://beofw:0/squid-internal-static/icons/silk/image.png
> May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001
> exited with status 1
> May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001
> will not be restarted for 3600 seconds due to repeated, frequent failures
> May 31 14:36:02 beofw squid[3966]: Exiting due to repeated, frequent
> failures
> 
> but all is ok without intercept mode.
> 
> What is the problem ?
> 

Your proxy is missing an http_port suitable to receive requests for
those URLs.

Amos


From alberto.senni at gmail.com  Mon Jun  1 19:41:51 2020
From: alberto.senni at gmail.com (Alberto Senni)
Date: Mon, 1 Jun 2020 21:41:51 +0200
Subject: [squid-users] FATAL: mimeLoadIcon: cannot parse internal URL
In-Reply-To: <94bbf9f1-566a-0dc6-b5e4-511c7f2b16df@gmail.com>
References: <89d00e1a-0718-27dd-72bd-697a23fd3bc2@gmail.com>
 <28c47a71-87f2-6152-4fc6-8b97d4bffd53@treenet.co.nz>
 <94bbf9f1-566a-0dc6-b5e4-511c7f2b16df@gmail.com>
Message-ID: <13af1506-4dfd-7032-1a2c-c8b1fae539c9@gmail.com>

Il 01/06/20 17:29, Alberto Senni ha scritto:
> Il 01/06/20 16:15, Amos Jeffries ha scritto:
>> On 2/06/20 1:45 am, Alberto Senni wrote:
>>> Hi to? all,
>>>
>>> on my linux Devuan configured as transparent router, squid 4 exit with
>>> FATAL error (from syslog):
>>>
>>> May 31 14:36:02 beofw (squid-1): FATAL: mimeLoadIcon: cannot parse
>>> internal URL: http://beofw:0/squid-internal-static/icons/silk/image.png
>>> May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001
>>> exited with status 1
>>> May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001
>>> will not be restarted for 3600 seconds due to repeated, frequent 
>>> failures
>>> May 31 14:36:02 beofw squid[3966]: Exiting due to repeated, frequent
>>> failures
>>>
>>> but all is ok without intercept mode.
>>>
>>> What is the problem ?
>>>
>>
>> Your proxy is missing an http_port suitable to receive requests for
>> those URLs.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> Hi Amos,
> 
> but do you mean when my proxy in missing an http_port ...
> 
> in my squid.conf I have
> 
> http_port 192.168.100.254:3128 transparent
> 
> and the IP address of beofw in 192.168.100.254 
> (http://beofw:0/squid-internal-static/icons/silk/image.png)
> 
> which other http_port I have to insert ?
> 
> Thanks a lot
> Alberto Senni



From squid3 at treenet.co.nz  Tue Jun  2 06:06:00 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Jun 2020 18:06:00 +1200
Subject: [squid-users] FATAL: mimeLoadIcon: cannot parse internal URL
In-Reply-To: <94bbf9f1-566a-0dc6-b5e4-511c7f2b16df@gmail.com>
References: <89d00e1a-0718-27dd-72bd-697a23fd3bc2@gmail.com>
 <28c47a71-87f2-6152-4fc6-8b97d4bffd53@treenet.co.nz>
 <94bbf9f1-566a-0dc6-b5e4-511c7f2b16df@gmail.com>
Message-ID: <8791576b-effc-8f68-a24a-72a923f94c85@treenet.co.nz>

On 2/06/20 3:29 am, Alberto Senni wrote:
> Il 01/06/20 16:15, Amos Jeffries ha scritto:
>> On 2/06/20 1:45 am, Alberto Senni wrote:
>>> Hi to? all,
>>>
>>> on my linux Devuan configured as transparent router, squid 4 exit with
>>> FATAL error (from syslog):
>>>
>>> May 31 14:36:02 beofw (squid-1): FATAL: mimeLoadIcon: cannot parse
>>> internal URL: http://beofw:0/squid-internal-static/icons/silk/image.png
>>> May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001
>>> exited with status 1
>>> May 31 14:36:02 beofw squid[3966]: Squid Parent: squid-1 process 4001
>>> will not be restarted for 3600 seconds due to repeated, frequent
>>> failures
>>> May 31 14:36:02 beofw squid[3966]: Exiting due to repeated, frequent
>>> failures
>>>
>>> but all is ok without intercept mode.
>>>
>>> What is the problem ?
>>>
>>
>> Your proxy is missing an http_port suitable to receive requests for
>> those URLs.
>>
>>
> Hi Amos,
> 
> but do you mean when my proxy in missing an http_port ...
> 
> in my squid.conf I have
> 
> http_port 192.168.100.254:3128 transparent
> 

This is a port for receiving NAT intercepted traffic. It should be a
randomly selected port and only ever receive traffic from the kernel's
NAT sub-system. Nothing from any other machine (or process on the same
machine) should ever be accepted there.


> and the IP address of beofw in 192.168.100.254
> (http://beofw:0/squid-internal-static/icons/silk/image.png)
> 
> which other http_port I have to insert ?

One that does not use the "transparent" or "tproxy" mode flags.
Typically port 3128 is used for explicit proxy traffic.

Amos


From ben.maling42 at gmail.com  Tue Jun  2 10:02:45 2020
From: ben.maling42 at gmail.com (ben benml)
Date: Tue, 2 Jun 2020 12:02:45 +0200
Subject: [squid-users] Squid 4.4 https_port and ssl-bump : Fatal bungled
	line
In-Reply-To: <CAF-5T9H6eaAvsm1xFwSqDCOZeMApygk=5oHtmdDTMHeiJ9SCOw@mail.gmail.com>
References: <CACTby66gbiTxsj6Ftv-ZFwzwmTZSavD3CaoWGSxHWeQAWmGcQA@mail.gmail.com>
 <467d0766-1821-8a25-81cc-dab40188e664@treenet.co.nz>
 <CACTby66UXqfEXE8zvD8VmX3q33EzY7VHMqzJcjzgAoT5TQBh+A@mail.gmail.com>
 <CAF-5T9H6eaAvsm1xFwSqDCOZeMApygk=5oHtmdDTMHeiJ9SCOw@mail.gmail.com>
Message-ID: <CACTby67dKOkyr-gkwKD_=KuCck8Xz2j90Z8cTEYTF0qrzR8dyA@mail.gmail.com>

Hello,

Thank you for your answer. And sorry for my late reply .. .busy on multiple
stuff... you know what it is ;)

I'm totally agree that using https is the best way to secure the
authentication.

But, in case, ssl-bump is mandatory what you be the best (or the less
worst) options to secure authentification (or at least the most possibile
secured authent) ?

Thank you in advance.

Regards,



Le mer. 27 mai 2020 ? 02:08, Ronan Lucio <ronanlucio at gmail.com> a ?crit :

> Hi Ben,
>
> I made working just using https_port (without ssl-bump).
>
> I think it's a good way to secure squid authentication.
> You can also use some tool (like certbot) to generate and
> automatically renew certificates, so you can work with a short period
> expiration time.
>
> Hope that helps,
> Ronan
>
> On Tue, May 26, 2020 at 12:10 AM ben benml <ben.maling42 at gmail.com> wrote:
> >
> > Hello,
> >
> > Thank you for your prompt and precise answer.
> >
> > Well I'm permit myself another question, sorry. If you have an opinion
> about securing the authentification without https_port :
> > With a FreeIPA central users directory, what could be the best way to
> secure/protect the  authentication process, the login/password.
> > Or more generally what could be the best options to secure the
> login/password with only the http_port. So no directly encrypted traffic.
> >
> > I was assuming https connection could secure the authentication process
> .. but if ssl-dump  is really wanted, so I need another options to secure
> the login/password.
> >
> > Did you see my point / what I'm trying to talk about ?
> >
> > Thank you in advance.
> >
> > Regards,
> >
> >
> > Le lun. 25 mai 2020 ? 12:26, Amos Jeffries <squid3 at treenet.co.nz> a
> ?crit :
> >>
> >> On 25/05/20 9:59 pm, ben benml wrote:
> >> > Hello,
> >> >
> >> > I'm contacting you for some help.
> >> > I need to deploy a secure proxy based on Squid.
> >> >
> >> > I try to use https_port combined with sslbump. I get an error message
> >> > about a bungled line.
> >> >
> >> > The reasons I want to do this :
> >> > - secure connection between the client browser and the proxy server,
> so
> >> > using https_port to do it. encrypted  traffic in TLS between the
> client
> >> > and the server.
> >>
> >> Fine. Simply using https_port does that.
> >>
> >> > - secure login connection. So I need to use https_port to do this.
> >>
> >> Fine. Simply using https_port does that.
> >>
> >> > - Do ssl inspection of the traffic goeing through the proxy
> >>
> >> Squid does not yet support SSL-Bump decrypt of traffic already being
> >> decrypted for the secure proxy.
> >>
> >>
> >> Please see
> >> <
> http://lists.squid-cache.org/pipermail/squid-users/2020-May/022120.html>
> if
> >> you want details.
> >>
> >> Amos
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200602/1f77df78/attachment.htm>

From biao.wei at eeoa.com  Tue Jun  2 11:44:53 2020
From: biao.wei at eeoa.com (biao.wei at eeoa.com)
Date: Tue, 2 Jun 2020 19:44:53 +0800
Subject: [squid-users] squid-4.9 TCP_MISS_ABORTED  and memory leak
Message-ID: <202006021944529635161@eeoa.com>+61563EA9255EC2BD

hi squid developer:
    we use squid-4.9 meet two questions?
      (1)some request have timeout, not response to user data neither to request upstream from log information.
      (2)memory leak, now we need restart squid to release memory.

       look forward to response.

1. log format
logformat logfmt_cdn [%{%Y-%m-%d:%H:%M:%S}tl.%03tu] %{X-Real-IP}>h %>a:%>p %<a:%<p %rm %03>Hs %<st %>st HTTP/%>rv "%>rs://%>rd:%>rP" "%>rp" "%{Referer}>h" "%{User-Agent}>h" %tr %<pt %03<Hs %Ss %Sh/%<A "%{Range}>h" "%{xxx-ComeFrom}>h" "%{xxx-Origin-Domain}>h"

2. log infomation
[2020-06-01:10:59:40.346] 123.124.197.113 10.8.120.9:40680 -:- GET 206 396 740 HTTP/1.1 "http://download.xx.com:80" "/xx_mac_install_1.0.0.114_s.dmg" "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36" 850826 - - TCP_MISS_ABORTED HIER_NONE/- "bytes=25165824-25165824" "tcdn" "download.xx.com"
[2020-06-01:11:02:12.629] 123.124.197.113 10.8.120.10:49318 -:- GET 206 402 743 HTTP/1.1 "http://download.xx.com:80" "/xx_mac_install_1.0.0.114_s.dmg" "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36" 850299 - - TCP_MISS_ABORTED HIER_NONE/- "bytes=48234496-49283071" "tcdn" "download.xx.com"

3. topology
  origin site <---  [squid <- nginx]  <--- bussiness cdn <--- users

4.  config files

squid.conf
coredump_dir /var/spool/squid
pid_filename /var/run/squid.pid
httpd_suppress_version_string on
via off
cache_effective_user squid
cache_effective_group squid

max_filedescriptors 65535 
cache_swap_low 90
cache_swap_high 95
minimum_object_size 0 KB
maximum_object_size 200 MB

acl urlpath_QUERY urlpath_regex -i cgi-bin \? \.php \.xml
no_cache deny urlpath_QUERY  
# acl urlpath_denyssl urlpath_regex -i ^https:\\
# no_cache deny urlpath_denyssl

logformat logfmt_cdn [%{%Y-%m-%d:%H:%M:%S}tl.%03tu] %{X-Real-IP}>h %>a:%>p %<a:%<p %rm %03>Hs %<st %>st HTTP/%>rv "%>rs://%>rd:%>rP" "%>rp" "%{Referer}>h" "%{User-Agent}>h" %tr %<pt %03<Hs %Ss %Sh/%<A "%{Range}>h" "%{EEO-ComeFrom}>h" "%{xxx-Origin-Domain}>h"
acl nolog_url url_regex -i cache_object

cache_log /data/cache.log
cache_store_log none 
logfile_rotate 1

quick_abort_min 512 KB
quick_abort_max 512 KB
quick_abort_pct 80

range_offset_limit -1
reload_into_ims on
# collapsed_forwarding on

client_db off
dns_v4_first on
dns_nameservers 127.0.0.1
dns_retransmit_interval 0.01 second
dns_timeout 0.01 second
dns_defnames off
ignore_unknown_nameservers on
ipcache_size 10240
ipcache_low 90
ipcache_high 95
fqdncache_size 1024
positive_dns_ttl 365 days
negative_dns_ttl 365 days

peer_connect_timeout 30 seconds

connect_timeout 10 minutes
client_idle_pconn_timeout 10 minutes
server_idle_pconn_timeout 10 minutes

client_persistent_connections on
server_persistent_connections off
half_closed_clients off
request_entities off

reply_header_access Server deny all
reply_header_access X-Cache-Lookup deny all

include /etc/squid/upstream.conf
include /etc/squid/node.conf


upstream.conf
acl dstdomain_www.xxx.com  dstdomain www.xxx.com
cache_peer_access peer2www1 allow dstdomain_ www.xxx.com
cache_peer_access peer2www1 deny all
cache_peer_access peer2www2 allow dstdomain_ www.xxx.com
cache_peer_access peer2www2 deny all
access_log daemon:/data/squid/ www.xxx.com_all.access.log logfmt_cdn dstdomain_ www.xxx.com !nolog_url



node.conf
workers 6
hopeless_kid_revival_delay 1 year
cpu_affinity_map process_numbers=1,2,3,4,5,6 cores=1,3,5,7,9,11
cache_mem 60 GB
maximum_object_size_in_memory 100 MB

cache_dir rock /data1/squid/cache1 400000 max-size=204800000

cache_peer 10.1.1.13 parent 61160 0 no-query no-digest no-netdb-exchange originserver forceddomain=www.xxx.com   name=peer2www1   round-robin weight=1 connect-fail-limit=3
cache_peer 10.1.1.14 parent 61160 0 no-query no-digest no-netdb-exchange originserver forceddomain=www.xxx.com   name=peer2www2   round-robin weight=1 connect-fail-limit=3

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200602/f23768a7/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun  4 08:12:12 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jun 2020 20:12:12 +1200
Subject: [squid-users] squid-4.9 TCP_MISS_ABORTED and memory leak
In-Reply-To: <202006021944529635161@eeoa.com>
References: <202006021944529635161@eeoa.com>
Message-ID: <b8c7acab-d5d8-17f7-2bb1-5108f2903cab@treenet.co.nz>

On 2/06/20 11:44 pm, biao.wei wrote:
> hi squid developer:
> ? ? we use squid-4.9 meet two questions?
> ? ??? (1)some request have timeout, not response to user data neither to
> request upstream from log information.

Your logs are showing just over 14min to deliver 360 bytes to the
client. That smells like a routing MTU issue, probably ICMP packets
being dropped.



> ? ? ? (2)memory leak, now we need restart squid to release memory.

memory leaks are a rarity these days. Note that your large caches
require up to 530 GB of RAM spread over the 8 Squid processes. Plus
whatever the active traffic may be consuming - which may be some GB.
That is not all going to be allocated on startup, it will grow as the
caches fill, particularly the in-memory ones.

If you still think there is a leak - details are required.


> 
> ? ?? ? look forward to response.
> 
> 1. log format
> logformat logfmt_cdn [%{%Y-%m-%d:%H:%M:%S}tl.%03tu] %{X-Real-IP}>h
> %>a:%>p %<a:%<p %rm %03>Hs %<st %>st HTTP/%>rv "%>rs://%>rd:%>rP" "%>rp"
> "%{Referer}>h" "%{User-Agent}>h" %tr %<pt %03<Hs %Ss %Sh/%<A
> "%{Range}>h" "%{xxx-ComeFrom}>h" "%{xxx-Origin-Domain}>h"
> 
> 2. log infomation
> [2020-06-01:10:59:40.346] 123.124.197.113 10.8.120.9:40680 -:- GET 206
> 396 740 HTTP/1.1 "http://download.xx.com:80"
> "/xx_mac_install_1.0.0.114_s.dmg" "-" "Mozilla/5.0 (Windows NT 10.0;
> Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61
> Safari/537.36" 850826 - - TCP_MISS_ABORTED HIER_NONE/-
> "bytes=25165824-25165824" "tcdn" "download.xx.com"
> [2020-06-01:11:02:12.629] 123.124.197.113 10.8.120.10:49318 -:- GET 206
> 402 743 HTTP/1.1 "http://download.xx.com:80"
> "/xx_mac_install_1.0.0.114_s.dmg" "-" "Mozilla/5.0 (Windows NT 10.0;
> Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61
> Safari/537.36" 850299 - - TCP_MISS_ABORTED HIER_NONE/-
> "bytes=48234496-49283071" "tcdn" "download.xx.com"
> 
> 3. topology
> ? origin site <--- ?[squid <- nginx] ?<--- bussiness cdn <--- users
> 
> 4. ?config files
> 
> squid.conf
> coredump_dir /var/spool/squid
> pid_filename /var/run/squid.pid
> httpd_suppress_version_string on
> via off
> cache_effective_user squid
> cache_effective_group squid
> 
> max_filedescriptors 65535?
> cache_swap_low 90
> cache_swap_high 95
> minimum_object_size 0 KB
> maximum_object_size 200 MB
> 
> acl urlpath_QUERY urlpath_regex -i cgi-bin \? \.php \.xml
> no_cache deny urlpath_QUERY ?
> # acl urlpath_denyssl urlpath_regex -i ^https:\\
> # no_cache deny urlpath_denyssl

The name "no_cache" is obsolete since *very*, *very* long ago.

Remove the "no_" characters from the start of those config lines.


> 
> logformat logfmt_cdn [%{%Y-%m-%d:%H:%M:%S}tl.%03tu] %{X-Real-IP}>h
> %>a:%>p %<a:%<p %rm %03>Hs %<st %>st HTTP/%>rv "%>rs://%>rd:%>rP" "%>rp"
> "%{Referer}>h" "%{User-Agent}>h" %tr %<pt %03<Hs %Ss %Sh/%<A
> "%{Range}>h" "%{EEO-ComeFrom}>h" "%{xxx-Origin-Domain}>h"
> acl nolog_url url_regex -i cache_object
> 
> cache_log /data/cache.log
> cache_store_log none
> logfile_rotate 1
> 
> quick_abort_min 512 KB
> quick_abort_max 512 KB
> quick_abort_pct 80
> 
> range_offset_limit -1
> reload_into_ims on
> # collapsed_forwarding on
> 
> client_db off
> dns_v4_first on
> dns_nameservers 127.0.0.1
> dns_retransmit_interval 0.01 second
> dns_timeout 0.01 second
> dns_defnames off
> ignore_unknown_nameservers on
> ipcache_size 10240
> ipcache_low 90
> ipcache_high 95
> fqdncache_size 1024

Many of the above settings are irrelevant. They are setting things to
the default value things things use anyway. You can simplify by removing
those lines entirely from your config file.


> positive_dns_ttl 365 days
> negative_dns_ttl 365 days

This is very bad. negative_dns_ttl is a *minimum* limit for DNS value
storage. There will be up to a years delay before Squid notices any time
servers are added, renumbered, or removed from the network your diagram
labels "origin site".
 negative_dns_ttl should be in the order of seconds or minutes.

 positive_dns_ttl is okay with large values, but also should be in the
order of days or weeks to avoid long-term timeouts trying to connect to
servers that no longer exist.


> 
> peer_connect_timeout 30 seconds

This is the default, no need to configure it to this value.


> 
> connect_timeout 10 minutes

Really 10min for a TCP SYN+ACK packet exchange?

This is a major DoS risk, and not just for Squid. It allows an attacker
to consume the systems networking resources completely. Causing the
entire machine to become non-responsive.


Amos


From frio_cervesa at hotmail.com  Thu Jun  4 20:15:01 2020
From: frio_cervesa at hotmail.com (senor)
Date: Thu, 4 Jun 2020 20:15:01 +0000
Subject: [squid-users] Adaptation error
Message-ID: <BYAPR01MB496825E847BDC3D40D77A229F7890@BYAPR01MB4968.prod.exchangelabs.com>

Hi All,

I occasionally get an assert and core dump on a worker here:
#3  0x0000000000580d84 in ClientHttpRequest::resetRequest (this=this at entry=0x2af0808, newRequest=newRequest at entry=0x2b51f00)
    at client_side_request.cc:1676
#4  0x000000000058ac9e in ClientHttpRequest::handleAdaptedHeader (this=0x2af0808, msg=<optimized out>) at client_side_request.cc:2005
#5  0x0000000000870a39 in JobDialer<Adaptation::Initiator>::dial (this=0x36b6dc0, call=...) at ../../src/base/AsyncJobCalls.h:174

It has occurred with 4.8 & 4.11. I don't think the config is relevant for what I'm trying to accomplish but can provide if important.

As I understand it, the request returned from adaptation is compared with pre-adapted request and it is considered a fail if they match. Since there is no requirement that adaptation make a change, I assume it should not be in that code unless adaptation indicated there was a change. But I'm not on solid ground there. We use c-icap and have had no errors. 

I'm mainly trying to get an understanding of what is occurring here. The URI being handled at this point varies and doesn't seem to be relevant. This probably only occurs when a reconfigure is processed. 

Any guidance or hints are appreciated. Thanks
Senor



From j.dierkse at madeo.nl  Thu Jun  4 20:21:48 2020
From: j.dierkse at madeo.nl (J. Dierkse)
Date: Thu, 04 Jun 2020 22:21:48 +0200
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
Message-ID: <Mailbird-8e50ee46-132b-49f3-81e1-ba56b9d6363c@madeo.nl>

Hi all,

I'm new to this mailing list, and I would start off with saying that I really love the Squid product.
I use it to intercept HTTP and HTTPS traffic in my network, and based on several ACLs forward it to different peer proxies.
This is where the DNS load balancing trickery becomes a hassle for HTTPS connections;

What I would like to do is if the request hostname matches an ACL (dstdomain or ssl::server_name), only do a splice for all ssl_bump steps. Otherwise do a peek for step1 and a splice afterwards.
My thinking is that this would be a nice workaround for the Office365 headache of TTLs of 5 seconds on the outlook.office365.com hostname.
However, this doesn't seem to work; I can't seem to trigger the "only splice" using any of the ACLs, and I keep getting the "Host header forgery" errors. (which on my Squid 4.11 version don't seem to be server, regardless of what I'm reading in various locations)
The only way I'm able to work around this for now is to create an ACL for all the possible IP's, and only splice for these (Blegh).

Is there any way to achieve what I would like to have? (without having to build a version of squid with the host forgery detection turned off...? :))

The relevant portion of my configuration is as follows.

-snip-

acl local dst 192.168.0.0/16


acl microsoft dstdomain .microsoft.com
acl microsoft dstdomain .teams.microsoft.com
acl microsoft dstdomain .office365.com
acl microsoft dstdomain .office.com
acl microsoft dstdomain .office.net
acl microsoft dstdomain .outlook.com

http_port 3128
http_port 3129 intercept
https_port 3130 intercept ssl-bump cert=/etc/certificates/SquidCA.pem key=/etc/certificates/SquidCA.pem

sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 16MB
sslcrtd_children 8 startup=1 idle=1

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek? ?step1 !microsoft !local
ssl_bump splice step2 !microsoft !local
ssl_bump splice step3 !microsoft !local

-snip-

Thanks!


Best Regards,


J. Dierkse
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200604/03d6432b/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun  4 21:55:02 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Jun 2020 17:55:02 -0400
Subject: [squid-users] Adaptation error
In-Reply-To: <BYAPR01MB496825E847BDC3D40D77A229F7890@BYAPR01MB4968.prod.exchangelabs.com>
References: <BYAPR01MB496825E847BDC3D40D77A229F7890@BYAPR01MB4968.prod.exchangelabs.com>
Message-ID: <b54528d4-0e91-51b2-5e9a-5487948ac3db@measurement-factory.com>

On 6/4/20 4:15 PM, senor wrote:

> I occasionally get an assert and core dump on a worker here:

> #3  in ClientHttpRequest::resetRequest () at client_side_request.cc:1676

I assume you are hitting assert(request != newRequest).


> #4  in ClientHttpRequest::handleAdaptedHeader () at client_side_request.cc:2005


> As I understand it, the request returned from adaptation is compared
> with pre-adapted request and it is considered a fail if they match.
> Since there is no requirement that adaptation make a change, I assume
> it should not be in that code unless adaptation indicated there was a
> change.

IIRC, Squid ICAP client code creates a new request structure even if no
adaptation happened at the ICAP service. This is a deficiency of that
old code (some may say a bug, but there could have been good reasons to
make it that way back then because that request structure is "special"
and may have been difficult to reuse -- I am not sure).

Apparently, there is some other adaptation code that does not create a
new request structure. I speculate that it may be the request routing
code that lives outside the ICAP space (it also serves eCAP adapters).
For a specific example, see Adaptation::Iterator::step() -- I see no
obvious/immediate signs there that theMsg is a new message and not the
virgin one. IIRC, theMsg starts as a virgin message structure.

I suspect that the correct fix would be to stop requiring a new request
structure for the akForward Answers. Instead, handleAdaptedHeader()
should accommodate the case where the request structure was preserved
and does not need to be reset. In other words, a Forward(x) answer
should not imply that the original message was (not) adapted.

Other Answer recipients should be checked for a similar problem.


HTH,

Alex.


From rousskov at measurement-factory.com  Thu Jun  4 22:11:34 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Jun 2020 18:11:34 -0400
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
In-Reply-To: <Mailbird-8e50ee46-132b-49f3-81e1-ba56b9d6363c@madeo.nl>
References: <Mailbird-8e50ee46-132b-49f3-81e1-ba56b9d6363c@madeo.nl>
Message-ID: <cab5476f-8883-2896-ab98-bb9c7b573054@measurement-factory.com>

On 6/4/20 4:21 PM, J. Dierkse wrote:

> I use it to intercept HTTP and HTTPS traffic in my network, and based on
> several ACLs forward it to different peer proxies.
> This is where the DNS load balancing trickery becomes a hassle for HTTPS
> connections;

> What I would like to do is if the request hostname matches an ACL
> (dstdomain or ssl::server_name), only do a splice for all ssl_bump
> steps. 

That goal needs polishing or rephrasing -- one cannot splice more than
once -- but I think I know what you mean.


> Otherwise do a peek for step1 and a splice afterwards.


Here is a sketch for v5. Sorry, I do not remember if v4 is equally
capable (but it very well may be):

    # splice as soon as we detect specialHost
    ssl_bump splice specialHost
    # peek to get more info if needed
    ssl_bump peek all
    # optional: splice if we never detect specialHost
    ssl_bump splice all

... where specialHost is an ssl::server_name ACL.


> without having to build a version of squid with the host forgery detection turned
> off...? :))

Those errors are a separate issue. Even the best possible ssl_bump
configuration can trigger them. They have been discussed many times on
this list before but if you have some new questions about them, please ask.


HTH,

Alex.




> 
> The relevant portion of my configuration is as follows.
> 
> -snip-
> 
> acl local dst 192.168.0.0/16
> 
> acl microsoft dstdomain .microsoft.com
> acl microsoft dstdomain .teams.microsoft.com
> acl microsoft dstdomain .office365.com
> acl microsoft dstdomain .office.com
> acl microsoft dstdomain .office.net
> acl microsoft dstdomain .outlook.com
> 
> http_port 3128
> http_port 3129 intercept
> https_port 3130 intercept ssl-bump cert=/etc/certificates/SquidCA.pem
> key=/etc/certificates/SquidCA.pem
> 
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db
> -M 16MB
> sslcrtd_children 8 startup=1 idle=1
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek? ?step1 !microsoft !local
> ssl_bump splice step2 !microsoft !local
> ssl_bump splice step3 !microsoft !local
> 
> -snip-
> 
> Thanks!
> 
> 
> Best Regards,
> 
> 
> J. Dierkse
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From frio_cervesa at hotmail.com  Thu Jun  4 23:56:14 2020
From: frio_cervesa at hotmail.com (senor)
Date: Thu, 4 Jun 2020 23:56:14 +0000
Subject: [squid-users] Adaptation error
In-Reply-To: <b54528d4-0e91-51b2-5e9a-5487948ac3db@measurement-factory.com>
References: <BYAPR01MB496825E847BDC3D40D77A229F7890@BYAPR01MB4968.prod.exchangelabs.com>,
 <b54528d4-0e91-51b2-5e9a-5487948ac3db@measurement-factory.com>
Message-ID: <BYAPR01MB496836B72E6AEB357AC3BE1CF7890@BYAPR01MB4968.prod.exchangelabs.com>

Thanks Alex. It does help. - Sorry for the earlier direct email reply.
The only solution we had come up with was to eliminate the assert because it didn't seem to be of concern. The alternate adaptation handling looks like  a deep dive and I feel guilty I can't help with that.
Unless you think eliminating the assert is not a good idea, I think my question is answered.

Thanks

________________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Thursday, June 4, 2020 2:55 PM
To: senor; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Adaptation error

On 6/4/20 4:15 PM, senor wrote:

> I occasionally get an assert and core dump on a worker here:

> #3  in ClientHttpRequest::resetRequest () at client_side_request.cc:1676

I assume you are hitting assert(request != newRequest).


> #4  in ClientHttpRequest::handleAdaptedHeader () at client_side_request.cc:2005


> As I understand it, the request returned from adaptation is compared
> with pre-adapted request and it is considered a fail if they match.
> Since there is no requirement that adaptation make a change, I assume
> it should not be in that code unless adaptation indicated there was a
> change.

IIRC, Squid ICAP client code creates a new request structure even if no
adaptation happened at the ICAP service. This is a deficiency of that
old code (some may say a bug, but there could have been good reasons to
make it that way back then because that request structure is "special"
and may have been difficult to reuse -- I am not sure).

Apparently, there is some other adaptation code that does not create a
new request structure. I speculate that it may be the request routing
code that lives outside the ICAP space (it also serves eCAP adapters).
For a specific example, see Adaptation::Iterator::step() -- I see no
obvious/immediate signs there that theMsg is a new message and not the
virgin one. IIRC, theMsg starts as a virgin message structure.

I suspect that the correct fix would be to stop requiring a new request
structure for the akForward Answers. Instead, handleAdaptedHeader()
should accommodate the case where the request structure was preserved
and does not need to be reset. In other words, a Forward(x) answer
should not imply that the original message was (not) adapted.

Other Answer recipients should be checked for a similar problem.


HTH,

Alex.


From j.dierkse at madeo.nl  Fri Jun  5 06:55:07 2020
From: j.dierkse at madeo.nl (J. Dierkse)
Date: Fri, 05 Jun 2020 08:55:07 +0200
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
In-Reply-To: <cab5476f-8883-2896-ab98-bb9c7b573054@measurement-factory.com>
References: <Mailbird-8e50ee46-132b-49f3-81e1-ba56b9d6363c@madeo.nl>
 <cab5476f-8883-2896-ab98-bb9c7b573054@measurement-factory.com>
Message-ID: <Mailbird-38e389fb-e59b-4ebc-bb7f-54a0953d44b6@madeo.nl>


On 05-Jun-20 00:11:44, Alex Rousskov <rousskov at measurement-factory.com> wrote:
On 6/4/20 4:21 PM, J. Dierkse wrote:

> I use it to intercept HTTP and HTTPS traffic in my network, and based on
> several ACLs forward it to different peer proxies.
> This is where the DNS load balancing trickery becomes a hassle for HTTPS
> connections;

> What I would like to do is if the request hostname matches an ACL
> (dstdomain or ssl::server_name), only do a splice for all ssl_bump
> steps.

That goal needs polishing or rephrasing -- one cannot splice more than
once -- but I think I know what you mean.


Correct, of course splice is the end of the chain, my bad.
The main thing I want to achieve is to setup the connection through different types of privacy proxies, depending on the target domain.
For this I don't really need bumping, if I understand the protocol correctly, I just need to peek to check what the target domain is.
Unfortunately, peeking has the side effect of detecting false-positive host forgery errors, due to Microsoft's DNS load balancing strategy.


> Otherwise do a peek for step1 and a splice afterwards.


Here is a sketch for v5. Sorry, I do not remember if v4 is equally
capable (but it very well may be):

# splice as soon as we detect specialHost
ssl_bump splice specialHost
# peek to get more info if needed
ssl_bump peek all
# optional: splice if we never detect specialHost
ssl_bump splice all

... where specialHost is an ssl::server_name ACL.


I tried this configuration, but it doesn't give the desired effect.
In 4.11 it doesn't seem to splice at all, but bump for some reason (is it correct not to refer to any bump steps?)


> without having to build a version of squid with the host forgery detection turned
> off...? :))

Those errors are a separate issue. Even the best possible ssl_bump
configuration can trigger them. They have been discussed many times on
this list before but if you have some new questions about them, please ask.


I know these errors have been a hot topic, and from what I can find (also in this mailing list) is that it should not block the connection.
Frankly, I couldn't care less about the errors themselves, as long as the connection is still allowed.
However, this is not what I'm experiencing. What I'm seeing is that when the error occurs, my outlook apps can no longer connect to the server.
What am I doing wrong...?


Thanks!


J. Dierkse


HTH,

Alex.




>
> The relevant portion of my configuration is as follows.
>
> -snip-
>
> acl local dst 192.168.0.0/16
>
> acl microsoft dstdomain .microsoft.com
> acl microsoft dstdomain .teams.microsoft.com
> acl microsoft dstdomain .office365.com
> acl microsoft dstdomain .office.com
> acl microsoft dstdomain .office.net
> acl microsoft dstdomain .outlook.com
>
> http_port 3128
> http_port 3129 intercept
> https_port 3130 intercept ssl-bump cert=/etc/certificates/SquidCA.pem
> key=/etc/certificates/SquidCA.pem
>
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db
> -M 16MB
> sslcrtd_children 8 startup=1 idle=1
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> ssl_bump peek? ?step1 !microsoft !local
> ssl_bump splice step2 !microsoft !local
> ssl_bump splice step3 !microsoft !local
>
> -snip-
>
> Thanks!
>
>
> Best Regards,
>
>
> J. Dierkse
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200605/eb0cd035/attachment.htm>

From rousskov at measurement-factory.com  Fri Jun  5 13:00:31 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Jun 2020 09:00:31 -0400
Subject: [squid-users] Adaptation error
In-Reply-To: <BYAPR01MB496836B72E6AEB357AC3BE1CF7890@BYAPR01MB4968.prod.exchangelabs.com>
References: <BYAPR01MB496825E847BDC3D40D77A229F7890@BYAPR01MB4968.prod.exchangelabs.com>
 <b54528d4-0e91-51b2-5e9a-5487948ac3db@measurement-factory.com>
 <BYAPR01MB496836B72E6AEB357AC3BE1CF7890@BYAPR01MB4968.prod.exchangelabs.com>
Message-ID: <9d453255-5b11-57b6-092f-f63aaee3fb34@measurement-factory.com>

On 6/4/20 7:56 PM, senor wrote:

> The only solution we had come up with was to eliminate the assert
> because it didn't seem to be of concern. Unless you think eliminating
> the assert is not a good idea, I think my question is answered.

AFAICT, the code should continue to work despite the violation of the
asserted condition. Needless to say, that conclusion may change in
future Squid versions, and my superficial analysis itself can be wrong.

I recommend filing a bug report in Squid bugzilla with a stack trace and
a reference to this email thread.


Thank you,

Alex.


> From: Alex Rousskov <rousskov at measurement-factory.com>
> Sent: Thursday, June 4, 2020 2:55 PM
> To: senor; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Adaptation error
> 
> On 6/4/20 4:15 PM, senor wrote:
> 
>> I occasionally get an assert and core dump on a worker here:
> 
>> #3  in ClientHttpRequest::resetRequest () at client_side_request.cc:1676
> 
> I assume you are hitting assert(request != newRequest).
> 
> 
>> #4  in ClientHttpRequest::handleAdaptedHeader () at client_side_request.cc:2005
> 
> 
>> As I understand it, the request returned from adaptation is compared
>> with pre-adapted request and it is considered a fail if they match.
>> Since there is no requirement that adaptation make a change, I assume
>> it should not be in that code unless adaptation indicated there was a
>> change.
> 
> IIRC, Squid ICAP client code creates a new request structure even if no
> adaptation happened at the ICAP service. This is a deficiency of that
> old code (some may say a bug, but there could have been good reasons to
> make it that way back then because that request structure is "special"
> and may have been difficult to reuse -- I am not sure).
> 
> Apparently, there is some other adaptation code that does not create a
> new request structure. I speculate that it may be the request routing
> code that lives outside the ICAP space (it also serves eCAP adapters).
> For a specific example, see Adaptation::Iterator::step() -- I see no
> obvious/immediate signs there that theMsg is a new message and not the
> virgin one. IIRC, theMsg starts as a virgin message structure.
> 
> I suspect that the correct fix would be to stop requiring a new request
> structure for the akForward Answers. Instead, handleAdaptedHeader()
> should accommodate the case where the request structure was preserved
> and does not need to be reset. In other words, a Forward(x) answer
> should not imply that the original message was (not) adapted.
> 
> Other Answer recipients should be checked for a similar problem.
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Fri Jun  5 13:32:25 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Jun 2020 09:32:25 -0400
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
In-Reply-To: <Mailbird-38e389fb-e59b-4ebc-bb7f-54a0953d44b6@madeo.nl>
References: <Mailbird-8e50ee46-132b-49f3-81e1-ba56b9d6363c@madeo.nl>
 <cab5476f-8883-2896-ab98-bb9c7b573054@measurement-factory.com>
 <Mailbird-38e389fb-e59b-4ebc-bb7f-54a0953d44b6@madeo.nl>
Message-ID: <1f21542e-2051-0523-453e-202a4de4d93b@measurement-factory.com>

On 6/5/20 2:55 AM, J. Dierkse wrote:
>> Here is a sketch for v5. Sorry, I do not remember if v4 is equally
>> capable (but it very well may be):
>>
>> # splice as soon as we detect specialHost
>> ssl_bump splice specialHost
>> # peek to get more info if needed
>> ssl_bump peek all
>> # optional: splice if we never detect specialHost
>> ssl_bump splice all
>>
>> ... where specialHost is an ssl::server_name ACL.
> 
> I tried this configuration, but it doesn't give the desired effect.
> In 4.11 it doesn't seem to splice at all, but bump for some reason (is
> it correct not to refer to any bump steps?)

Bugs notwithstanding, the only reason the above configuration could
result in a bumped connection is if Squid wants to report an error to
the client. To make progress, you need to find out what that error is.

* I would start by examining access.log records after adding
%err_code/%err_detail to your custom logformat.

* If that does not help (we are still working on detailing various
errors better), I would look at cache.log after setting debug_options to
ALL,2 or higher.

* If that does not help, I would share a link to a compressed cache.log
after setting debug_options to ALL,9 and reproducing the problem using a
single transaction (to the extent possible).


> I know these errors have been a hot topic, and from what I can find
> (also in this mailing list) is that it should not block the connection.

I agree that host forgery detection should not block intercepted
connections and CONNECT tunnels (by default). I do not know how close
Squid is to that ideal -- there may be some holes in following that rule
of thumb. Also, I do not know whether your connections are bumped due to
these false positives or some other error. See above for the suggested
next steps.


HTH,

Alex.


From j.dierkse at madeo.nl  Sat Jun  6 07:39:41 2020
From: j.dierkse at madeo.nl (j.dierkse at madeo.nl)
Date: Sat, 6 Jun 2020 09:39:41 +0200
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
In-Reply-To: <1f21542e-2051-0523-453e-202a4de4d93b@measurement-factory.com>
References: <1f21542e-2051-0523-453e-202a4de4d93b@measurement-factory.com>
Message-ID: <4DDFA58B-219A-4644-B178-CD28B5EE6B66@madeo.nl>

> On 5 Jun 2020, at 15:32, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> ?On 6/5/20 2:55 AM, J. Dierkse wrote:
>>> Here is a sketch for v5. Sorry, I do not remember if v4 is equally
>>> capable (but it very well may be):
>>> 
>>> # splice as soon as we detect specialHost
>>> ssl_bump splice specialHost
>>> # peek to get more info if needed
>>> ssl_bump peek all
>>> # optional: splice if we never detect specialHost
>>> ssl_bump splice all
>>> 
>>> ... where specialHost is an ssl::server_name ACL.
>> 
>> I tried this configuration, but it doesn't give the desired effect.
>> In 4.11 it doesn't seem to splice at all, but bump for some reason (is
>> it correct not to refer to any bump steps?)
> 
> Bugs notwithstanding, the only reason the above configuration could
> result in a bumped connection is if Squid wants to report an error to
> the client. To make progress, you need to find out what that error is.
> 
> * I would start by examining access.log records after adding
> %err_code/%err_detail to your custom logformat.
> 
> * If that does not help (we are still working on detailing various
> errors better), I would look at cache.log after setting debug_options to
> ALL,2 or higher.
> 
> * If that does not help, I would share a link to a compressed cache.log
> after setting debug_options to ALL,9 and reproducing the problem using a
> single transaction (to the extent possible).
> 
> 
>> I know these errors have been a hot topic, and from what I can find
>> (also in this mailing list) is that it should not block the connection.
> 
> I agree that host forgery detection should not block intercepted
> connections and CONNECT tunnels (by default). I do not know how close
> Squid is to that ideal -- there may be some holes in following that rule
> of thumb. Also, I do not know whether your connections are bumped due to
> these false positives or some other error. See above for the suggested
> next steps.
> 
> 
> HTH,
> 
> Alex.
> 

Thanks!

I will start to dig a bit more into what the underlying errors are. I will report back here when I have more info.


Best,


J. Dierkse


From j.dierkse at madeo.nl  Sun Jun  7 17:53:37 2020
From: j.dierkse at madeo.nl (J. Dierkse)
Date: Sun, 7 Jun 2020 19:53:37 +0200
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
In-Reply-To: <4DDFA58B-219A-4644-B178-CD28B5EE6B66@madeo.nl>
References: <1f21542e-2051-0523-453e-202a4de4d93b@measurement-factory.com>
 <4DDFA58B-219A-4644-B178-CD28B5EE6B66@madeo.nl>
Message-ID: <975C8B16-41E7-4ABE-85E3-AE52D5E2EB0E@madeo.nl>


> On 6 Jun 2020, at 09:39, j.dierkse at madeo.nl wrote:
> 
>> On 5 Jun 2020, at 15:32, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>> 
>> ?On 6/5/20 2:55 AM, J. Dierkse wrote:
>>>> Here is a sketch for v5. Sorry, I do not remember if v4 is equally
>>>> capable (but it very well may be):
>>>> 
>>>> # splice as soon as we detect specialHost
>>>> ssl_bump splice specialHost
>>>> # peek to get more info if needed
>>>> ssl_bump peek all
>>>> # optional: splice if we never detect specialHost
>>>> ssl_bump splice all
>>>> 
>>>> ... where specialHost is an ssl::server_name ACL.
>>> 
>>> I tried this configuration, but it doesn't give the desired effect.
>>> In 4.11 it doesn't seem to splice at all, but bump for some reason (is
>>> it correct not to refer to any bump steps?)
>> 
>> Bugs notwithstanding, the only reason the above configuration could
>> result in a bumped connection is if Squid wants to report an error to
>> the client. To make progress, you need to find out what that error is.
>> 
>> * I would start by examining access.log records after adding
>> %err_code/%err_detail to your custom logformat.
>> 
>> * If that does not help (we are still working on detailing various
>> errors better), I would look at cache.log after setting debug_options to
>> ALL,2 or higher.
>> 
>> * If that does not help, I would share a link to a compressed cache.log
>> after setting debug_options to ALL,9 and reproducing the problem using a
>> single transaction (to the extent possible).
>> 
>> 
>>> I know these errors have been a hot topic, and from what I can find
>>> (also in this mailing list) is that it should not block the connection.
>> 
>> I agree that host forgery detection should not block intercepted
>> connections and CONNECT tunnels (by default). I do not know how close
>> Squid is to that ideal -- there may be some holes in following that rule
>> of thumb. Also, I do not know whether your connections are bumped due to
>> these false positives or some other error. See above for the suggested
>> next steps.
>> 

I think I found the culprit; I?m exclusively using peer routing, and the knowledgebase mentions that this is disabled when host forgery is detected.
I understand the reasoning behind disabling this, but it renders my setup pointless for SSL connections.
Perhaps anyone has any brilliant ideas to do peer routing with Squid with (sorry to say) misbehaving CDN DNS servers..?
For now I?ll just set it to splice in step1.
Thanks for the help!


Best,


J. Dierkse



From rousskov at measurement-factory.com  Tue Jun  9 20:33:01 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Jun 2020 16:33:01 -0400
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>

On 6/5/20 2:38 PM, DIXIT Ankit wrote:

> We are facing memory issues on Squid proxy. We have squid proxy server
> running on AWS Cloud(Amazon Linux 2). Server is having total 8 GB RAM
> and 100 GB hard disk.
> 
> The problem is that squid is eating all of system memory and not freeing
> up the objects. Server memory lasts for maximum 15 days and after that
> squid process start crashing. Please suggest.

I would start by making sure that you are not suffering from Squid bug
#4005: https://bugs.squid-cache.org/show_bug.cgi?id=4005

After that, if you are sure that there is a memory leak, then I suggest
upgrading to v4 (or v5) as the next step. IMHO, it is unlikely that
somebody would volunteer to triage a v3.5 memory leak these days.


HTH,

Alex.


> [root at eaa-lpx-003-p ~]# squid -version
> 
> Squid Cache: Version 3.5.27-20180318-r1330042
> 
> Service Name: squid
> 
> This binary uses OpenSSL 1.0.2k-fips? 26 Jan 2017.



From ngtech1ltd at gmail.com  Wed Jun 10 08:25:57 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 10 Jun 2020 11:25:57 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
Message-ID: <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/15167235/attachment.htm>

From m992493 at gmail.com  Wed Jun 10 09:26:22 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Wed, 10 Jun 2020 14:56:22 +0530
Subject: [squid-users] Problem with squid proxy authentication configuration
Message-ID: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>

Hi Guys,

I am trying to configure squid so as to have user proxy
authentication, below is how my squid.conf file looks like:

-----
acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access deny all
http_port 3128
coredump_dir /var/spool/squid

refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .        0    20%    4320
-----

The above lines were default in squid.conf file.

I have added below lines:

-----
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024

icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/echo
adaptation_access service_req allow all

icap_service service_resp respmod_precache bypass=0 icap://127.0.0.1:1344/echo
adaptation_access service_resp allow all

acl ncsa src 0.0.0.0/0.0.0.0
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic realm proxy
acl ncsa proxy_auth REQUIRED
http access allow ncsa
-----

With this in place, I have configured 127.0.0.1:3128 as manual proxy in firefox.

But when I try to connect to internet it displays "The proxy server is
refusing connections"
(https://i.ytimg.com/vi/-LJAxo9TVig/hqdefault.jpg)

Please point out what I am doing wrong?

Thanks
Amiq


From premchand142 at gmail.com  Wed Jun 10 10:09:44 2020
From: premchand142 at gmail.com (Prem Chand)
Date: Wed, 10 Jun 2020 15:39:44 +0530
Subject: [squid-users] Switch cache peer Parent server for every 30 minutes
Message-ID: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>

Hi ,

My squid cache peer has 3 parent IP?s configured. I need to send HTTPS
requests to the first parent IP for 30 minutes and after to the 2nd parent
IP for 30 minutes and then to 3rd IP for 30 minutes and this switching
needs to happen continuously .I tried round robin but the requests are
equally distributed among 3 parents instead of sending requests to each
parent for 30 mins .Could you please let us know how I can achieve this?


cache_peer first_IP_Address parent 3218 0 no-query  connect-fail-limit=2
cache_peer second_IP_Address parent 3218 0 no-query connect-fail-limit=2
cache_peer third_IP_Address  parent 3218 0 no-query connect-fail-limit=2

Thanks & Regards

Premchand
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/88f9db17/attachment.htm>

From ngtech1ltd at gmail.com  Wed Jun 10 11:25:40 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 10 Jun 2020 14:25:40 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/1848b2b3/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 98855962CDEF4DFE9433D33307C3C229.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/1848b2b3/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/1848b2b3/attachment.jpg>

From ngtech1ltd at gmail.com  Wed Jun 10 12:13:45 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 10 Jun 2020 15:13:45 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/ced2f3eb/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 67EADB7F83724BE6B362AE44C3E91857.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/ced2f3eb/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/ced2f3eb/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/ced2f3eb/attachment-0001.png>

From rousskov at measurement-factory.com  Wed Jun 10 13:18:48 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jun 2020 09:18:48 -0400
Subject: [squid-users] Problem with squid proxy authentication
 configuration
In-Reply-To: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
References: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
Message-ID: <6590aa8a-8769-f292-82da-5d7f549df0d6@measurement-factory.com>

On 6/10/20 5:26 AM, Amiq Nahas wrote:

> http_port 3128


> I have configured 127.0.0.1:3128 as manual proxy in firefox.
> 
> But when I try to connect to internet it displays "The proxy server is
> refusing connections"

Does your Squid run on the same machine as your browser? If yes, are
there any errors or warnings in Squid's cache.log, access.log? Does
"netstat -na" or equivalent (executed on the browser machine) show
something listening for TCP/IPv4 connection on port 3128?

Alex.


From rousskov at measurement-factory.com  Wed Jun 10 13:42:06 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jun 2020 09:42:06 -0400
Subject: [squid-users] Switch cache peer Parent server for every 30
 minutes
In-Reply-To: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
Message-ID: <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>

On 6/10/20 6:09 AM, Prem Chand wrote:

> My squid cache peer has 3 parent IP?s configured. I need to send HTTPS
> requests to the first parent IP for 30 minutes and after to the 2nd
> parent IP for 30 minutes and then to 3rd IP for 30 minutes and this
> switching needs to happen continuously .Could you please let us know how I
> can achieve this?

If you are OK with hard-coded usage time slots for each peer, then I
would use two[1] "time" ACLs and cache_peer_access rules. Look for
"aclname time" in squid.conf.documented. You will have to generate a
list of (24*2/3=16) staggered time slots for each of the two ACLs, but
it should work. This may be the simplest solution.

[1] You need two ACLs for three peers because the third peer should get
the requests that the first two peers were not allowed to get.

----

With a modern Squid, you could also implement this using a more flexible
(and more expensive, on several layers!) architecture with two ACLs:

1. An external ACL that returns the right cache peer name to use via a
keyword=value annotation API. This always-matching ACL should be
attached to http_access or a similar directive that supports slow ACLs.
Its goal is to annotate the request. You will need to write a
script/program that will compute the right annotations based on time or
some other factors. This is where the flexibility of this solution is
coming from.

2. A "note" ACL attached to cache_peer_access directives, allowing
access to peer X if the external ACL in item 1 returned
use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can be
reliably used with cache_peer_access.

If you already have another external ACL, you may be able to piggyback
annotations in item 1 to whatever that ACL is already doing.

For more information, search for "keyword=value" and "acl aclname note"
in your squid.conf.documented and see
https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29


HTH,

Alex.


From squid3 at treenet.co.nz  Wed Jun 10 14:34:40 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2020 02:34:40 +1200
Subject: [squid-users] Problem with squid proxy authentication
 configuration
In-Reply-To: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
References: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
Message-ID: <11cd0d12-38e3-21e0-71f2-4496d83c8d8b@treenet.co.nz>

On 10/06/20 9:26 pm, Amiq Nahas wrote:
> Hi Guys,
> 
> I am trying to configure squid so as to have user proxy
> authentication, below is how my squid.conf file looks like:
> 
> -----
> acl SSL_ports port 443
> acl Safe_ports port 80        # http
> acl Safe_ports port 21        # ftp
> acl Safe_ports port 443        # https
> acl Safe_ports port 70        # gopher
> acl Safe_ports port 210        # wais
> acl Safe_ports port 1025-65535    # unregistered ports
> acl Safe_ports port 280        # http-mgmt
> acl Safe_ports port 488        # gss-http
> acl Safe_ports port 591        # filemaker
> acl Safe_ports port 777        # multiling http
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access deny all
> http_port 3128
> coredump_dir /var/spool/squid
> 
> refresh_pattern ^ftp:        1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern .        0    20%    4320
> -----
> 
> The above lines were default in squid.conf file.
> 
> I have added below lines:
> 

*Where* did you add them? order is important.


> -----
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> 
> icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/echo
> adaptation_access service_req allow all
> 
> icap_service service_resp respmod_precache bypass=0 icap://127.0.0.1:1344/echo
> adaptation_access service_resp allow all
> 
> acl ncsa src 0.0.0.0/0.0.0.0

Don't do that. Use "all" to match any IP address.

If you want to match IPv4-only clients there is a special value "ipv4"
which is used like so:
  acl ipv4_only src ipv4

Be careful with these type of control. Different access behaviours for
IPv4 and IPv6 is how security bypass issues are created.



> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
> auth_param basic realm proxy
> acl ncsa proxy_auth REQUIRED

"ncsa" was already defined as a IP address matching ACL.


> http access allow ncsa


This will only allow clients who are already trying to send credentials.
It will not inform clients that they need to and no sane client will
broadcast its credential secrets unless it has to.

To have HTTP auth work in the usual way it is best to *deny*
non-authenticated traffic and allow based on any other criteria you
have. Like so:

  http_access deny !ncsa
  http_access allow localnet

or

  http_access deny !ncsa
  http_access allow ncsa





Amos


From premchand142 at gmail.com  Wed Jun 10 16:11:03 2020
From: premchand142 at gmail.com (Prem Chand)
Date: Wed, 10 Jun 2020 21:41:03 +0530
Subject: [squid-users] Switch cache peer Parent server for every 30
	minutes
In-Reply-To: <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
Message-ID: <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>

Hi Alex,

Thanks for responding to my issue  . I didn't get how the math was done(why
it's multiplied by 2) to get 16 slots if possible could you please
elaborate with an example.

Regards
Premchand

On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 6/10/20 6:09 AM, Prem Chand wrote:
>
> > My squid cache peer has 3 parent IP?s configured. I need to send HTTPS
> > requests to the first parent IP for 30 minutes and after to the 2nd
> > parent IP for 30 minutes and then to 3rd IP for 30 minutes and this
> > switching needs to happen continuously .Could you please let us know how
> I
> > can achieve this?
>
> If you are OK with hard-coded usage time slots for each peer, then I
> would use two[1] "time" ACLs and cache_peer_access rules. Look for
> "aclname time" in squid.conf.documented. You will have to generate a
> list of (24*2/3=16) staggered time slots for each of the two ACLs, but
> it should work. This may be the simplest solution.
>
> [1] You need two ACLs for three peers because the third peer should get
> the requests that the first two peers were not allowed to get.
>
> ----
>
> With a modern Squid, you could also implement this using a more flexible
> (and more expensive, on several layers!) architecture with two ACLs:
>
> 1. An external ACL that returns the right cache peer name to use via a
> keyword=value annotation API. This always-matching ACL should be
> attached to http_access or a similar directive that supports slow ACLs.
> Its goal is to annotate the request. You will need to write a
> script/program that will compute the right annotations based on time or
> some other factors. This is where the flexibility of this solution is
> coming from.
>
> 2. A "note" ACL attached to cache_peer_access directives, allowing
> access to peer X if the external ACL in item 1 returned
> use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can be
> reliably used with cache_peer_access.
>
> If you already have another external ACL, you may be able to piggyback
> annotations in item 1 to whatever that ACL is already doing.
>
> For more information, search for "keyword=value" and "acl aclname note"
> in your squid.conf.documented and see
> https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.29
>
>
> HTH,
>
> Alex.
>


-- 
prem
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200610/d812a49f/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Jun 10 16:20:46 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 10 Jun 2020 18:20:46 +0200
Subject: [squid-users] Switch cache peer Parent server for every 30
	minutes
In-Reply-To: <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
Message-ID: <202006101820.47163.Antony.Stone@squid.open.source.it>

On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:

> Hi Alex,
> 
> Thanks for responding to my issue  . I didn't get how the math was done(why
> it's multiplied by 2) to get 16 slots if possible could you please elaborate
> with an example.

I believe what Alex meant was:

You want 30 minute timeslots for each of 3 peers, which is 48 half-hour 
timeslots throughout the day.

However, you only need to define 48/3 of these for peer A, and 48/3 of them for 
peer B, and then let peer C deal with anything not already handled (so it 
doesn't need its own definitions).

48/3 = 16, therefore you define 16 half-hour periods when you want peer A to do 
the work, 16 half-hour periods for peer B, and then just say "peer C, handle 
anything left over".


Regards,


Antony.

> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
> > On 6/10/20 6:09 AM, Prem Chand wrote:
> > > My squid cache peer has 3 parent IP?s configured. I need to send HTTPS
> > > requests to the first parent IP for 30 minutes and after to the 2nd
> > > parent IP for 30 minutes and then to 3rd IP for 30 minutes and this
> > > switching needs to happen continuously .Could you please let us know
> > > how I can achieve this?
> > 
> > If you are OK with hard-coded usage time slots for each peer, then I
> > would use two[1] "time" ACLs and cache_peer_access rules. Look for
> > "aclname time" in squid.conf.documented. You will have to generate a
> > list of (24*2/3=16) staggered time slots for each of the two ACLs, but
> > it should work. This may be the simplest solution.
> > 
> > [1] You need two ACLs for three peers because the third peer should get
> > the requests that the first two peers were not allowed to get.
> > 
> > ----
> > 
> > With a modern Squid, you could also implement this using a more flexible
> > (and more expensive, on several layers!) architecture with two ACLs:
> > 
> > 1. An external ACL that returns the right cache peer name to use via a
> > keyword=value annotation API. This always-matching ACL should be
> > attached to http_access or a similar directive that supports slow ACLs.
> > Its goal is to annotate the request. You will need to write a
> > script/program that will compute the right annotations based on time or
> > some other factors. This is where the flexibility of this solution is
> > coming from.
> > 
> > 2. A "note" ACL attached to cache_peer_access directives, allowing
> > access to peer X if the external ACL in item 1 returned
> > use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can be
> > reliably used with cache_peer_access.
> > 
> > If you already have another external ACL, you may be able to piggyback
> > annotations in item 1 to whatever that ACL is already doing.
> > 
> > For more information, search for "keyword=value" and "acl aclname note"
> > in your squid.conf.documented and see
> > https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
> > 29
> > 
> > 
> > HTH,
> > 
> > Alex.

-- 
Neurotics build castles in the sky;
Psychotics live in them;
Psychiatrists collect the rent.


                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Jun 10 17:47:28 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jun 2020 05:47:28 +1200
Subject: [squid-users] Issue with SSL_BUMP and Office365 (for one...)
In-Reply-To: <975C8B16-41E7-4ABE-85E3-AE52D5E2EB0E@madeo.nl>
References: <1f21542e-2051-0523-453e-202a4de4d93b@measurement-factory.com>
 <4DDFA58B-219A-4644-B178-CD28B5EE6B66@madeo.nl>
 <975C8B16-41E7-4ABE-85E3-AE52D5E2EB0E@madeo.nl>
Message-ID: <8a9c017b-9d76-c388-6cfb-7b8aebfbc8bd@treenet.co.nz>

On 8/06/20 5:53 am, J. Dierkse wrote:>
> I think I found the culprit; I?m exclusively using peer routing, and the knowledgebase mentions that this is disabled when host forgery is detected.
> I understand the reasoning behind disabling this, but it renders my setup pointless for SSL connections.
> Perhaps anyone has any brilliant ideas to do peer routing with Squid with (sorry to say) misbehaving CDN DNS servers..?

This is <https://bugs.squid-cache.org/show_bug.cgi?id=4940>.

There is no easy fix. A number of design problems need to be resolved
before there is a chance at avoiding this safely.

Amos


From ronanlucio at gmail.com  Wed Jun 10 19:08:35 2020
From: ronanlucio at gmail.com (Ronan Lucio)
Date: Thu, 11 Jun 2020 07:08:35 +1200
Subject: [squid-users] Server monitoring
Message-ID: <CAF-5T9F=pYdYA7BjeFouhrjL6-BM4ZFMUEuU4NPtWXn+b1ucAA@mail.gmail.com>

Hi guys,

How do you suggest to monitor service availability?
A know that some people use to monitor a few URLs through the proxy,
but, I'd like to know if there is any way to remotly monitor squid
service.

Thanks,
Ronan


From Antony.Stone at squid.open.source.it  Wed Jun 10 19:16:56 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 10 Jun 2020 21:16:56 +0200
Subject: [squid-users] Server monitoring
In-Reply-To: <CAF-5T9F=pYdYA7BjeFouhrjL6-BM4ZFMUEuU4NPtWXn+b1ucAA@mail.gmail.com>
References: <CAF-5T9F=pYdYA7BjeFouhrjL6-BM4ZFMUEuU4NPtWXn+b1ucAA@mail.gmail.com>
Message-ID: <202006102116.56392.Antony.Stone@squid.open.source.it>

On Wednesday 10 June 2020 at 21:08:35, Ronan Lucio wrote:

> Hi guys,
> 
> How do you suggest to monitor service availability?
> A know that some people use to monitor a few URLs through the proxy,
> but, I'd like to know if there is any way to remotly monitor squid service.

Do you mean "is it running?"

Or do you mean "how busy is it?"

Or do you mean "is it working and supplying the content it's expected / 
supposed to ?"

Or... maybe something else?

So, what it is you want to monitor?


Next question: do you already have some monitoring system such as Icinga, 
Zabbix, Nagios, etc., which you use for other systems and services, or is 
Squid the first thing you're thinking of keeping a watchful eye on?


Given that information, we might have some ideas, or else pointers to where 
else it's worth asking the question.


Regards,


Antony.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ronanlucio at gmail.com  Wed Jun 10 19:37:10 2020
From: ronanlucio at gmail.com (Ronan Lucio)
Date: Thu, 11 Jun 2020 07:37:10 +1200
Subject: [squid-users] Server monitoring
In-Reply-To: <202006102116.56392.Antony.Stone@squid.open.source.it>
References: <CAF-5T9F=pYdYA7BjeFouhrjL6-BM4ZFMUEuU4NPtWXn+b1ucAA@mail.gmail.com>
 <202006102116.56392.Antony.Stone@squid.open.source.it>
Message-ID: <CAF-5T9GQDVqoZVCuup7f2YmrButkvH8EwBwF+n+93cumNxHcMA@mail.gmail.com>

Hi Antony,

I mean "is it running"?

Yes, I have a couple of ways to monitor servers and services.
Specifically for this one, I plan to use GCP Stackdriver.

Agent will gather system data for CPU, disk, memory, and some services.
Besides that, I'd like to have a monitor to say "is squid running?",
regardless of CPU, disk, and memory are fine.

Thanks,
Ronan

On Thu, Jun 11, 2020 at 7:17 AM Antony Stone
<Antony.Stone at squid.open.source.it> wrote:
>
> On Wednesday 10 June 2020 at 21:08:35, Ronan Lucio wrote:
>
> > Hi guys,
> >
> > How do you suggest to monitor service availability?
> > A know that some people use to monitor a few URLs through the proxy,
> > but, I'd like to know if there is any way to remotly monitor squid service.
>
> Do you mean "is it running?"
>
> Or do you mean "how busy is it?"
>
> Or do you mean "is it working and supplying the content it's expected /
> supposed to ?"
>
> Or... maybe something else?
>
> So, what it is you want to monitor?
>
>
> Next question: do you already have some monitoring system such as Icinga,
> Zabbix, Nagios, etc., which you use for other systems and services, or is
> Squid the first thing you're thinking of keeping a watchful eye on?
>
>
> Given that information, we might have some ideas, or else pointers to where
> else it's worth asking the question.
>
>
> Regards,
>
>
> Antony.
>
> --
> "In fact I wanted to be John Cleese and it took me some time to realise that
> the job was already taken."
>
>  - Douglas Adams
>
>                                                    Please reply to the list;
>                                                          please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Wed Jun 10 22:37:27 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jun 2020 18:37:27 -0400
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <2c9897e1-fc51-9190-8ba3-1476a62bee2f@measurement-factory.com>

On 6/10/20 7:56 AM, DIXIT Ankit wrote:

> Does it mean, Squid 4 is not tested on Amazon Linux 2, yet?

Officially, no Squid version is tested on Amazon Linux. With enough
Squid Project donations, that may change in the foreseeable future.

Alex.


From rousskov at measurement-factory.com  Wed Jun 10 23:24:35 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jun 2020 19:24:35 -0400
Subject: [squid-users] Switch cache peer Parent server for every 30
 minutes
In-Reply-To: <202006101820.47163.Antony.Stone@squid.open.source.it>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
 <202006101820.47163.Antony.Stone@squid.open.source.it>
Message-ID: <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>

On 6/10/20 12:20 PM, Antony Stone wrote:
> On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:
> 
>> Hi Alex,
>>
>> Thanks for responding to my issue  . I didn't get how the math was done(why
>> it's multiplied by 2) to get 16 slots if possible could you please elaborate
>> with an example.
> 
> I believe what Alex meant was:
> 
> You want 30 minute timeslots for each of 3 peers, which is 48 half-hour 
> timeslots throughout the day.
> 
> However, you only need to define 48/3 of these for peer A, and 48/3 of them for 
> peer B, and then let peer C deal with anything not already handled (so it 
> doesn't need its own definitions).
> 
> 48/3 = 16, therefore you define 16 half-hour periods when you want peer A to do 
> the work, 16 half-hour periods for peer B, and then just say "peer C, handle 
> anything left over".

Thank you, Antony! Here is an untested sketch:

  acl usePeerA time 00:00-00:29
  acl usePeerA time 01:30-01:59
  ... a total of 16 ORed lines for the first peer ...
  ... each line matches a unique 30 minute period ...


  acl usePeerB time 00:30-00:59
  acl usePeerB time 02:00-02:29
  ... a total of 16 ORed lines for the second peer ...
  ... each line matches a unique 30 minute period ...

  # and now match peer to its time slots
  cache_peer_access peerA allow usePeerA
  cache_peer_access peerB allow usePeerB
  cache_peer_access peerC allow !usePeerA !userPeerB


The above may need further adjustments and polishing. For example, I am
not sure how Squid will round these time values. The above assumes that
00:29 limit includes all 60 seconds up to (but excluding) 00:30:00.


HTH,

Alex.


>> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
>>> On 6/10/20 6:09 AM, Prem Chand wrote:
>>>> My squid cache peer has 3 parent IP?s configured. I need to send HTTPS
>>>> requests to the first parent IP for 30 minutes and after to the 2nd
>>>> parent IP for 30 minutes and then to 3rd IP for 30 minutes and this
>>>> switching needs to happen continuously .Could you please let us know
>>>> how I can achieve this?
>>>
>>> If you are OK with hard-coded usage time slots for each peer, then I
>>> would use two[1] "time" ACLs and cache_peer_access rules. Look for
>>> "aclname time" in squid.conf.documented. You will have to generate a
>>> list of (24*2/3=16) staggered time slots for each of the two ACLs, but
>>> it should work. This may be the simplest solution.
>>>
>>> [1] You need two ACLs for three peers because the third peer should get
>>> the requests that the first two peers were not allowed to get.
>>>
>>> ----
>>>
>>> With a modern Squid, you could also implement this using a more flexible
>>> (and more expensive, on several layers!) architecture with two ACLs:
>>>
>>> 1. An external ACL that returns the right cache peer name to use via a
>>> keyword=value annotation API. This always-matching ACL should be
>>> attached to http_access or a similar directive that supports slow ACLs.
>>> Its goal is to annotate the request. You will need to write a
>>> script/program that will compute the right annotations based on time or
>>> some other factors. This is where the flexibility of this solution is
>>> coming from.
>>>
>>> 2. A "note" ACL attached to cache_peer_access directives, allowing
>>> access to peer X if the external ACL in item 1 returned
>>> use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can be
>>> reliably used with cache_peer_access.
>>>
>>> If you already have another external ACL, you may be able to piggyback
>>> annotations in item 1 to whatever that ACL is already doing.
>>>
>>> For more information, search for "keyword=value" and "acl aclname note"
>>> in your squid.conf.documented and see
>>> https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
>>> 29
>>>
>>>
>>> HTH,
>>>
>>> Alex.
> 



From m992493 at gmail.com  Thu Jun 11 12:29:18 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Thu, 11 Jun 2020 17:59:18 +0530
Subject: [squid-users] Problem with squid proxy authentication
	configuration
In-Reply-To: <11cd0d12-38e3-21e0-71f2-4496d83c8d8b@treenet.co.nz>
References: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
 <11cd0d12-38e3-21e0-71f2-4496d83c8d8b@treenet.co.nz>
Message-ID: <CAPicJaE+tz0O_+XJm0oT8Lg9epwJNuvAZ-2xFCo2WqwsM+vvzw@mail.gmail.com>

On Wed, Jun 10, 2020 at 8:07 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 10/06/20 9:26 pm, Amiq Nahas wrote:
> > Hi Guys,
> >
> > I am trying to configure squid so as to have user proxy
> > authentication, below is how my squid.conf file looks like:
> >
> > -----
> > acl SSL_ports port 443
> > acl Safe_ports port 80        # http
> > acl Safe_ports port 21        # ftp
> > acl Safe_ports port 443        # https
> > acl Safe_ports port 70        # gopher
> > acl Safe_ports port 210        # wais
> > acl Safe_ports port 1025-65535    # unregistered ports
> > acl Safe_ports port 280        # http-mgmt
> > acl Safe_ports port 488        # gss-http
> > acl Safe_ports port 591        # filemaker
> > acl Safe_ports port 777        # multiling http
> > acl CONNECT method CONNECT
> >
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > http_access allow localhost manager
> > http_access deny manager
> > http_access allow localhost
> > http_access deny all
> > http_port 3128
> > coredump_dir /var/spool/squid
> >
> > refresh_pattern ^ftp:        1440    20%    10080
> > refresh_pattern ^gopher:    1440    0%    1440
> > refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> > refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> > refresh_pattern .        0    20%    4320
> > -----
> >
> > The above lines were default in squid.conf file.
> >
> > I have added below lines:
> >
>
> *Where* did you add them? order is important.

I have added the below lines exactly in this order at the end of the
file squid.conf.

> > -----
> > icap_enable on
> > icap_send_client_ip on
> > icap_send_client_username on
> > icap_client_username_header X-Authenticated-User
> > icap_preview_enable on
> > icap_preview_size 1024
> >
> > icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/echo
> > adaptation_access service_req allow all
> >
> > icap_service service_resp respmod_precache bypass=0 icap://127.0.0.1:1344/echo
> > adaptation_access service_resp allow all
> >
> > acl ncsa src 0.0.0.0/0.0.0.0
>
> Don't do that. Use "all" to match any IP address.
>
> If you want to match IPv4-only clients there is a special value "ipv4"
> which is used like so:
>   acl ipv4_only src ipv4
>
> Be careful with these type of control. Different access behaviours for
> IPv4 and IPv6 is how security bypass issues are created.
>
>
>
> > auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
> > auth_param basic realm proxy
> > acl ncsa proxy_auth REQUIRED
>
> "ncsa" was already defined as a IP address matching ACL.
>
>
> > http access allow ncsa
>
>
> This will only allow clients who are already trying to send credentials.
> It will not inform clients that they need to and no sane client will
> broadcast its credential secrets unless it has to.
>
> To have HTTP auth work in the usual way it is best to *deny*
> non-authenticated traffic and allow based on any other criteria you
> have. Like so:
>
>   http_access deny !ncsa
>   http_access allow localnet
>
> or
>
>   http_access deny !ncsa
>   http_access allow ncsa

So I changed the configuration according to what you suggested and now
I can access the internet.
Below is how the configuration now looks like:

acl ncsa src all
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic realm proxy
acl authenticated proxy_auth REQUIRED
http_access allow authenticated ncsa

I am able to access the internet now, does this mean that everything
worked fine? I am asking because I will be using this proxy
authentication setup in c-icap for setting up the url_check service.
Also I am not prompted for any password, I am able to access the
internet just like that. Is that how it is supposed to work because if
I don't need to enter the password before browsing the web what would
be the point of it all. Right? or am I missing something here?
I have been using this article for reference
http://hevi.info/do-it-yourself/install-and-setup-squid3-on-ubuntu-14-04-with-authentication/

Thanks
Amiq


From m992493 at gmail.com  Thu Jun 11 12:32:14 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Thu, 11 Jun 2020 18:02:14 +0530
Subject: [squid-users] Problem with squid proxy authentication
	configuration
In-Reply-To: <6590aa8a-8769-f292-82da-5d7f549df0d6@measurement-factory.com>
References: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
 <6590aa8a-8769-f292-82da-5d7f549df0d6@measurement-factory.com>
Message-ID: <CAPicJaEBdy9rsL5Qvm-pU3E5RACbWGHW==u0=A5V10736b7RUA@mail.gmail.com>

On Wed, Jun 10, 2020 at 6:48 PM Alex Rousskov
<rousskov at measurement-factory.com> wrote:
>
> On 6/10/20 5:26 AM, Amiq Nahas wrote:
>
> > http_port 3128
>
>
> > I have configured 127.0.0.1:3128 as manual proxy in firefox.
> >
> > But when I try to connect to internet it displays "The proxy server is
> > refusing connections"
>
> Does your Squid run on the same machine as your browser? If yes, are
> there any errors or warnings in Squid's cache.log, access.log? Does
> "netstat -na" or equivalent (executed on the browser machine) show
> something listening for TCP/IPv4 connection on port 3128?


Thanks for the reply.

Yes, Squid is running on the same machine as the browser.
I have checked cache.log and access.log, there are no errors or
warnings that get generated when the browser shows the proxy refusing
connections error page.
I ran netstat -na and no 3128 port is not listening.

I should mention that before I added the below lines in the
configuration, it was running all running fine. I could browse the net
and was getting logs of websites visited in the access.log file.
So I am guessing these lines are the problem, but the more important
problem at hand is that 3128 port is not listening. Any thoughts?

acl ncsa src 0.0.0.0/0.0.0.0
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic realm proxy
acl ncsa proxy_auth REQUIRED
http access allow ncsa

In case if system environment has something to do with the problem.
The system in question runs Ubuntu 18.04 with Linux-5.6.0-rc7.

Thanks
Amiq


From squid3 at treenet.co.nz  Thu Jun 11 16:55:02 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jun 2020 04:55:02 +1200
Subject: [squid-users] Problem with squid proxy authentication
 configuration
In-Reply-To: <CAPicJaE+tz0O_+XJm0oT8Lg9epwJNuvAZ-2xFCo2WqwsM+vvzw@mail.gmail.com>
References: <CAPicJaGUm3q2Eg1wvOUQuG_OJm9rvMe0Zzx7dk74qg475+gG6g@mail.gmail.com>
 <11cd0d12-38e3-21e0-71f2-4496d83c8d8b@treenet.co.nz>
 <CAPicJaE+tz0O_+XJm0oT8Lg9epwJNuvAZ-2xFCo2WqwsM+vvzw@mail.gmail.com>
Message-ID: <d0cf2d65-dcff-73a5-40d8-ba43b72cea5b@treenet.co.nz>

On 12/06/20 12:29 am, Amiq Nahas wrote:
> On Wed, Jun 10, 2020 at 8:07 PM Amos Jeffries wrote:
>>
>> On 10/06/20 9:26 pm, Amiq Nahas wrote:
>>> Hi Guys,
>>>
>>> I am trying to configure squid so as to have user proxy
>>> authentication, below is how my squid.conf file looks like:
>>>
>>> -----
>>> acl SSL_ports port 443
>>> acl Safe_ports port 80        # http
>>> acl Safe_ports port 21        # ftp
>>> acl Safe_ports port 443        # https
>>> acl Safe_ports port 70        # gopher
>>> acl Safe_ports port 210        # wais
>>> acl Safe_ports port 1025-65535    # unregistered ports
>>> acl Safe_ports port 280        # http-mgmt
>>> acl Safe_ports port 488        # gss-http
>>> acl Safe_ports port 591        # filemaker
>>> acl Safe_ports port 777        # multiling http
>>> acl CONNECT method CONNECT
>>>
>>> http_access deny !Safe_ports
>>> http_access deny CONNECT !SSL_ports
>>> http_access allow localhost manager
>>> http_access deny manager
>>> http_access allow localhost
>>> http_access deny all
>>> http_port 3128
>>> coredump_dir /var/spool/squid
>>>
>>> refresh_pattern ^ftp:        1440    20%    10080
>>> refresh_pattern ^gopher:    1440    0%    1440
>>> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
>>> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>>> refresh_pattern .        0    20%    4320
>>> -----
>>>
>>> The above lines were default in squid.conf file.
>>>
>>> I have added below lines:
>>>
>>
>> *Where* did you add them? order is important.
> 
> I have added the below lines exactly in this order at the end of the
> file squid.conf.
> 

That is the wrong place to be adding the http_access part of your custom
config.


>>> -----
>>> icap_enable on
>>> icap_send_client_ip on
>>> icap_send_client_username on
>>> icap_client_username_header X-Authenticated-User
>>> icap_preview_enable on
>>> icap_preview_size 1024
>>>
>>> icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/echo
>>> adaptation_access service_req allow all
>>>
>>> icap_service service_resp respmod_precache bypass=0 icap://127.0.0.1:1344/echo
>>> adaptation_access service_resp allow all
>>>
>>> acl ncsa src 0.0.0.0/0.0.0.0
>>
>> Don't do that. Use "all" to match any IP address.
>>
>> If you want to match IPv4-only clients there is a special value "ipv4"
>> which is used like so:
>>   acl ipv4_only src ipv4
>>
>> Be careful with these type of control. Different access behaviours for
>> IPv4 and IPv6 is how security bypass issues are created.
>>
>>
>>
>>> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
>>> auth_param basic realm proxy
>>> acl ncsa proxy_auth REQUIRED
>>
>> "ncsa" was already defined as a IP address matching ACL.
>>
>>
>>> http access allow ncsa
>>
>>
>> This will only allow clients who are already trying to send credentials.
>> It will not inform clients that they need to and no sane client will
>> broadcast its credential secrets unless it has to.
>>
>> To have HTTP auth work in the usual way it is best to *deny*
>> non-authenticated traffic and allow based on any other criteria you
>> have. Like so:
>>
>>   http_access deny !ncsa
>>   http_access allow localnet
>>
>> or
>>
>>   http_access deny !ncsa
>>   http_access allow ncsa
> 
> So I changed the configuration according to what you suggested and now
> I can access the internet.
> Below is how the configuration now looks like:
> 
> acl ncsa src all

That is the same as the built-in "all" ACL ...


> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
> auth_param basic realm proxy
> acl authenticated proxy_auth REQUIRED
> http_access allow authenticated ncsa

... which makes the above line same as:

  http_access allow authenticated all

Which actively *prevents* Squid from requesting credentials from clients.

> 
> I am able to access the internet now, does this mean that everything
> worked fine?

No. There are many ways to configure Squid to allow traffic through.
Most of them do not in any way match your policy.


> I am asking because I will be using this proxy
> authentication setup in c-icap for setting up the url_check service.
> Also I am not prompted for any password, I am able to access the
> internet just like that. Is that how it is supposed to work

It is what you currently configured to be happening.
I wrote earlier that you needed something like this:

  http_access deny !ncsa
  http_access allow localnet


That needs to be in sequence with the other http_access rules in your
config:


  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access allow localhost manager
  http_access deny manager

  acl authenticated proxy_auth REQUIRED
  http_access deny !authenticated

  http_access allow localhost
  http_access deny all


> because if
> I don't need to enter the password before browsing the web what would
> be the point of it all. Right? or am I missing something here?

You are missing the order http_access rules are applied.


> I have been using this article for reference
> http://hevi.info/do-it-yourself/install-and-setup-squid3-on-ubuntu-14-04-with-authentication/
> 

Please notice that while the individual steps of the tutorial itself are
correct they omit very important details like where to place the config
settings. Like I said at the beginning order is important.
  And the followup comments are from people with non-working setups or
wrong answers.

The Squid wiki contains the authoritative information on how to use HTTP
authentication in Squid
<https://wiki.squid-cache.org/Features/Authentication>


Amos


From ngtech1ltd at gmail.com  Thu Jun 11 18:18:15 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 11 Jun 2020 21:18:15 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200611/4afdf0ce/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2396E00209874982838EA2114E9AF3AA.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200611/4afdf0ce/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200611/4afdf0ce/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200611/4afdf0ce/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200611/4afdf0ce/attachment-0002.png>

From premchand142 at gmail.com  Fri Jun 12 03:52:05 2020
From: premchand142 at gmail.com (Prem Chand)
Date: Fri, 12 Jun 2020 09:22:05 +0530
Subject: [squid-users] Switch cache peer Parent server for every 30
	minutes
In-Reply-To: <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
 <202006101820.47163.Antony.Stone@squid.open.source.it>
 <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>
Message-ID: <CACbtF4PGhKts-U96Y3nGFZauWduzfwDh3D3UAycGcpXMqeutRA@mail.gmail.com>

Hi Alex,

It's working as expected. I tried to allow only specific domains during the
time by adding below acl but I'm getting HTTP status code 503 in my
access.log, below is my configuration. Could you please let me know what
I'm missing here.

acl usePeerB time 00:30-00:59
acl usePeerB time 02:00-02:29
acl alloweddomains dstdomain google.com facebook.com


cache_peer_access peerA allow usePeerA allowedomains
cache_peer_access peerB allow usePeerB allowedomains
cache_peer_access peerC allow !usePeerA !userPeerB alloweddomains

On Thu, Jun 11, 2020 at 4:54 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 6/10/20 12:20 PM, Antony Stone wrote:
> > On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:
> >
> >> Hi Alex,
> >>
> >> Thanks for responding to my issue  . I didn't get how the math was
> done(why
> >> it's multiplied by 2) to get 16 slots if possible could you please
> elaborate
> >> with an example.
> >
> > I believe what Alex meant was:
> >
> > You want 30 minute timeslots for each of 3 peers, which is 48 half-hour
> > timeslots throughout the day.
> >
> > However, you only need to define 48/3 of these for peer A, and 48/3 of
> them for
> > peer B, and then let peer C deal with anything not already handled (so
> it
> > doesn't need its own definitions).
> >
> > 48/3 = 16, therefore you define 16 half-hour periods when you want peer
> A to do
> > the work, 16 half-hour periods for peer B, and then just say "peer C,
> handle
> > anything left over".
>
> Thank you, Antony! Here is an untested sketch:
>
>   acl usePeerA time 00:00-00:29
>   acl usePeerA time 01:30-01:59
>   ... a total of 16 ORed lines for the first peer ...
>   ... each line matches a unique 30 minute period ...
>
>
>   acl usePeerB time 00:30-00:59
>   acl usePeerB time 02:00-02:29
>   ... a total of 16 ORed lines for the second peer ...
>   ... each line matches a unique 30 minute period ...
>
>   # and now match peer to its time slots
>   cache_peer_access peerA allow usePeerA
>   cache_peer_access peerB allow usePeerB
>   cache_peer_access peerC allow !usePeerA !userPeerB
>
>
> The above may need further adjustments and polishing. For example, I am
> not sure how Squid will round these time values. The above assumes that
> 00:29 limit includes all 60 seconds up to (but excluding) 00:30:00.
>
>
> HTH,
>
> Alex.
>
>
> >> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
> >>> On 6/10/20 6:09 AM, Prem Chand wrote:
> >>>> My squid cache peer has 3 parent IP?s configured. I need to send HTTPS
> >>>> requests to the first parent IP for 30 minutes and after to the 2nd
> >>>> parent IP for 30 minutes and then to 3rd IP for 30 minutes and this
> >>>> switching needs to happen continuously .Could you please let us know
> >>>> how I can achieve this?
> >>>
> >>> If you are OK with hard-coded usage time slots for each peer, then I
> >>> would use two[1] "time" ACLs and cache_peer_access rules. Look for
> >>> "aclname time" in squid.conf.documented. You will have to generate a
> >>> list of (24*2/3=16) staggered time slots for each of the two ACLs, but
> >>> it should work. This may be the simplest solution.
> >>>
> >>> [1] You need two ACLs for three peers because the third peer should get
> >>> the requests that the first two peers were not allowed to get.
> >>>
> >>> ----
> >>>
> >>> With a modern Squid, you could also implement this using a more
> flexible
> >>> (and more expensive, on several layers!) architecture with two ACLs:
> >>>
> >>> 1. An external ACL that returns the right cache peer name to use via a
> >>> keyword=value annotation API. This always-matching ACL should be
> >>> attached to http_access or a similar directive that supports slow ACLs.
> >>> Its goal is to annotate the request. You will need to write a
> >>> script/program that will compute the right annotations based on time or
> >>> some other factors. This is where the flexibility of this solution is
> >>> coming from.
> >>>
> >>> 2. A "note" ACL attached to cache_peer_access directives, allowing
> >>> access to peer X if the external ACL in item 1 returned
> >>> use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can be
> >>> reliably used with cache_peer_access.
> >>>
> >>> If you already have another external ACL, you may be able to piggyback
> >>> annotations in item 1 to whatever that ACL is already doing.
> >>>
> >>> For more information, search for "keyword=value" and "acl aclname note"
> >>> in your squid.conf.documented and see
> >>>
> https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
> >>> 29
> >>>
> >>>
> >>> HTH,
> >>>
> >>> Alex.
> >
>
>

-- 
prem
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200612/f739d4bb/attachment.htm>

From rousskov at measurement-factory.com  Fri Jun 12 13:17:48 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Jun 2020 09:17:48 -0400
Subject: [squid-users] Switch cache peer Parent server for every 30
 minutes
In-Reply-To: <CACbtF4PGhKts-U96Y3nGFZauWduzfwDh3D3UAycGcpXMqeutRA@mail.gmail.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
 <202006101820.47163.Antony.Stone@squid.open.source.it>
 <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>
 <CACbtF4PGhKts-U96Y3nGFZauWduzfwDh3D3UAycGcpXMqeutRA@mail.gmail.com>
Message-ID: <1d44d011-e5fc-d813-4435-eaf17a3616b8@measurement-factory.com>

On 6/11/20 11:52 PM, Prem Chand wrote:

> It's working as expected. I tried to allow only specific domains during
> the time by adding below acl but I'm getting HTTP status code 503

> acl usePeerB time 00:30-00:59
> acl usePeerB time 02:00-02:29
> acl alloweddomains dstdomain google.com facebook.com

> cache_peer_access peerA allow usePeerA allowedomains
> cache_peer_access peerB allow usePeerB allowedomains
> cache_peer_access peerC allow !usePeerA !userPeerB alloweddomains

Assuming there are no other cache peers, the above rules leave no
forwarding path for a request to a banned domain. If you want to ban
such requests, http_access instead of cache_peer_access.


HTH,

Alex.


> On Thu, Jun 11, 2020 at 4:54 AM Alex Rousskov wrote:
> 
>     On 6/10/20 12:20 PM, Antony Stone wrote:
>     > On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:
>     >
>     >> Hi Alex,
>     >>
>     >> Thanks for responding to my issue? . I didn't get how the math
>     was done(why
>     >> it's multiplied by 2) to get 16 slots if possible could you
>     please elaborate
>     >> with an example.
>     >
>     > I believe what Alex meant was:
>     >
>     > You want 30 minute timeslots for each of 3 peers, which is 48
>     half-hour
>     > timeslots throughout the day.
>     >
>     > However, you only need to define 48/3 of these for peer A, and
>     48/3 of them for
>     > peer B, and then let peer C deal with anything not already handled
>     (so it
>     > doesn't need its own definitions).
>     >
>     > 48/3 = 16, therefore you define 16 half-hour periods when you want
>     peer A to do
>     > the work, 16 half-hour periods for peer B, and then just say "peer
>     C, handle
>     > anything left over".
> 
>     Thank you, Antony! Here is an untested sketch:
> 
>     ? acl usePeerA time 00:00-00:29
>     ? acl usePeerA time 01:30-01:59
>     ? ... a total of 16 ORed lines for the first peer ...
>     ? ... each line matches a unique 30 minute period ...
> 
> 
>     ? acl usePeerB time 00:30-00:59
>     ? acl usePeerB time 02:00-02:29
>     ? ... a total of 16 ORed lines for the second peer ...
>     ? ... each line matches a unique 30 minute period ...
> 
>     ? # and now match peer to its time slots
>     ? cache_peer_access peerA allow usePeerA
>     ? cache_peer_access peerB allow usePeerB
>     ? cache_peer_access peerC allow !usePeerA !userPeerB
> 
> 
>     The above may need further adjustments and polishing. For example, I am
>     not sure how Squid will round these time values. The above assumes that
>     00:29 limit includes all 60 seconds up to (but excluding) 00:30:00.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     >> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
>     >>> On 6/10/20 6:09 AM, Prem Chand wrote:
>     >>>> My squid cache peer has 3 parent IP?s configured. I need to
>     send HTTPS
>     >>>> requests to the first parent IP for 30 minutes and after to the 2nd
>     >>>> parent IP for 30 minutes and then to 3rd IP for 30 minutes and this
>     >>>> switching needs to happen continuously .Could you please let us
>     know
>     >>>> how I can achieve this?
>     >>>
>     >>> If you are OK with hard-coded usage time slots for each peer, then I
>     >>> would use two[1] "time" ACLs and cache_peer_access rules. Look for
>     >>> "aclname time" in squid.conf.documented. You will have to generate a
>     >>> list of (24*2/3=16) staggered time slots for each of the two
>     ACLs, but
>     >>> it should work. This may be the simplest solution.
>     >>>
>     >>> [1] You need two ACLs for three peers because the third peer
>     should get
>     >>> the requests that the first two peers were not allowed to get.
>     >>>
>     >>> ----
>     >>>
>     >>> With a modern Squid, you could also implement this using a more
>     flexible
>     >>> (and more expensive, on several layers!) architecture with two ACLs:
>     >>>
>     >>> 1. An external ACL that returns the right cache peer name to use
>     via a
>     >>> keyword=value annotation API. This always-matching ACL should be
>     >>> attached to http_access or a similar directive that supports
>     slow ACLs.
>     >>> Its goal is to annotate the request. You will need to write a
>     >>> script/program that will compute the right annotations based on
>     time or
>     >>> some other factors. This is where the flexibility of this
>     solution is
>     >>> coming from.
>     >>>
>     >>> 2. A "note" ACL attached to cache_peer_access directives, allowing
>     >>> access to peer X if the external ACL in item 1 returned
>     >>> use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can be
>     >>> reliably used with cache_peer_access.
>     >>>
>     >>> If you already have another external ACL, you may be able to
>     piggyback
>     >>> annotations in item 1 to whatever that ACL is already doing.
>     >>>
>     >>> For more information, search for "keyword=value" and "acl
>     aclname note"
>     >>> in your squid.conf.documented and see
>     >>>
>     https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
>     >>> 29
>     >>>
>     >>>
>     >>> HTH,
>     >>>
>     >>> Alex.
>     >
> 
> 
> 
> -- 
> prem



From ngtech1ltd at gmail.com  Sun Jun 14 05:00:22 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 14 Jun 2020 08:00:22 +0300
Subject: [squid-users] PAN OS "External Dynamic List" ,
 anyone knows about a similar helper for squid?
Message-ID: <527C68B6-1A7A-4778-BC17-7E6E899417A7@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200614/f1a899dc/attachment.htm>

From kasim1949 at gmail.com  Mon Jun 15 02:03:22 2020
From: kasim1949 at gmail.com (Peng Luo)
Date: Mon, 15 Jun 2020 10:03:22 +0800
Subject: [squid-users] Squid with different HTTP protocol versions
Message-ID: <CAO43QyuZ6QaD+syu_iX_o1WA0jKmEYsGYCxnH-JK7p9G2xwEuQ@mail.gmail.com>

Hi,

I?ve never used Squid until last week. Squid 4.11 has been set as a proxy
server for my mtk demo board connecting to the internet.


Recently, when I tried to run CTS on a demo board, few network test cases
failed. After tcpdumping and investigating with wireshark, it seems that
the following happens ,

1. demo board performs a 'GET' in HTTP 1.0 to the proxy

2. the proxy then performs this GET to the server, but now it's in HTTP 1.1

3. the server replies in HTTP 1.1 to proxy

4. the proxy replies in HTTP 1.1 to the demo board which *expects response
in HTTP 1.0*


At first I was thinking I chose the wrong server which doesn?t support HTTP
1.0. So I used my laptop to send exactly the same 'GET' to this server. It
replied in HTTP 1.0.

Now the problem is, when my squid proxy communicating to server , it cannot
use the same HTTP protocol version as it communicating to  client. Why?


I tried to find similar problems in the forum. But all I found were more
than ten years ago.

I suppose I must miss something in configuration of squid.

Please help, thanks in advance.


best regards
Frank Law
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200615/30c2f15c/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 15 02:55:24 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 15 Jun 2020 14:55:24 +1200
Subject: [squid-users] Squid with different HTTP protocol versions
In-Reply-To: <CAO43QyuZ6QaD+syu_iX_o1WA0jKmEYsGYCxnH-JK7p9G2xwEuQ@mail.gmail.com>
References: <CAO43QyuZ6QaD+syu_iX_o1WA0jKmEYsGYCxnH-JK7p9G2xwEuQ@mail.gmail.com>
Message-ID: <9aab8a56-1b4e-19cb-5bee-76f200dddd11@treenet.co.nz>

On 15/06/20 2:03 pm, Peng Luo wrote:
> Hi,
> 
> I?ve never used Squid until last week. Squid 4.11 has been set as a
> proxy server for my mtk demo board connecting to the internet.
> 
> 
> Recently, when I tried to run CTS on a demo board, few network test
> cases failed. After tcpdumping and investigating with wireshark, it
> seems that the following happens ,
> 
> 1. demo board performs a 'GET' in HTTP 1.0 to the proxy
> 
> 2. the proxy then performs this GET to the server, but now it's in HTTP 1.1
> 
> 3. the server replies in HTTP 1.1 to proxy
> 

This is correct HTTP behaviour as far as the details you have provided show.


> 4. the proxy replies in HTTP 1.1 to the demo board which *expects
> response in HTTP 1.0*
> 

That is broken. It should be expecting a response in HTTP/1.*


The client is telling Squid that it can only support feature level 0. So
responses from Squid can only use *features* compatible with level-0.

Squid is telling the client it supports feature level 1. So the client
can actually *use* features from 1.1 (if it wants/needs to) in its
messages labeled "HTTP/1.0".


<https://tools.ietf.org/html/rfc7230#section-2.6>
"
   When an HTTP/1.1 message is sent to an HTTP/1.0 recipient [RFC1945]
   or a recipient whose version is unknown, the HTTP/1.1 message is
   constructed such that it can be interpreted as a valid HTTP/1.0
   message if all of the newer features are ignored.

...
   Intermediaries that process HTTP messages (i.e., all intermediaries
   other than those acting as tunnels) MUST send their own HTTP-version
   in forwarded messages.
"



Amos


From premchand142 at gmail.com  Mon Jun 15 07:26:26 2020
From: premchand142 at gmail.com (Prem Chand)
Date: Mon, 15 Jun 2020 12:56:26 +0530
Subject: [squid-users] Switch cache peer Parent server for every 30
	minutes
In-Reply-To: <1d44d011-e5fc-d813-4435-eaf17a3616b8@measurement-factory.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
 <202006101820.47163.Antony.Stone@squid.open.source.it>
 <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>
 <CACbtF4PGhKts-U96Y3nGFZauWduzfwDh3D3UAycGcpXMqeutRA@mail.gmail.com>
 <1d44d011-e5fc-d813-4435-eaf17a3616b8@measurement-factory.com>
Message-ID: <CACbtF4OM=fPiitjZAAkfpT4F7aSu+wAM8b9UTMgy+KpFj7ZUZw@mail.gmail.com>

Hi Alex,

I stopped the peerA(purposefully)  and noticed that requests are failing
for the time slots that are going through peerA. I used
"connect-fail-limit" in cache_peer  but it's not working. Is there any way
we can address this issue using the same solution considering how to handle
the requests if any of the parent  peer goes down?

Thanks & Regards
Premchand .

On Fri, Jun 12, 2020 at 6:47 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 6/11/20 11:52 PM, Prem Chand wrote:
>
> > It's working as expected. I tried to allow only specific domains during
> > the time by adding below acl but I'm getting HTTP status code 503
>
> > acl usePeerB time 00:30-00:59
> > acl usePeerB time 02:00-02:29
> > acl alloweddomains dstdomain google.com facebook.com
>
> > cache_peer_access peerA allow usePeerA allowedomains
> > cache_peer_access peerB allow usePeerB allowedomains
> > cache_peer_access peerC allow !usePeerA !userPeerB alloweddomains
>
> Assuming there are no other cache peers, the above rules leave no
> forwarding path for a request to a banned domain. If you want to ban
> such requests, http_access instead of cache_peer_access.
>
>
> HTH,
>
> Alex.
>
>
> > On Thu, Jun 11, 2020 at 4:54 AM Alex Rousskov wrote:
> >
> >     On 6/10/20 12:20 PM, Antony Stone wrote:
> >     > On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:
> >     >
> >     >> Hi Alex,
> >     >>
> >     >> Thanks for responding to my issue  . I didn't get how the math
> >     was done(why
> >     >> it's multiplied by 2) to get 16 slots if possible could you
> >     please elaborate
> >     >> with an example.
> >     >
> >     > I believe what Alex meant was:
> >     >
> >     > You want 30 minute timeslots for each of 3 peers, which is 48
> >     half-hour
> >     > timeslots throughout the day.
> >     >
> >     > However, you only need to define 48/3 of these for peer A, and
> >     48/3 of them for
> >     > peer B, and then let peer C deal with anything not already handled
> >     (so it
> >     > doesn't need its own definitions).
> >     >
> >     > 48/3 = 16, therefore you define 16 half-hour periods when you want
> >     peer A to do
> >     > the work, 16 half-hour periods for peer B, and then just say "peer
> >     C, handle
> >     > anything left over".
> >
> >     Thank you, Antony! Here is an untested sketch:
> >
> >       acl usePeerA time 00:00-00:29
> >       acl usePeerA time 01:30-01:59
> >       ... a total of 16 ORed lines for the first peer ...
> >       ... each line matches a unique 30 minute period ...
> >
> >
> >       acl usePeerB time 00:30-00:59
> >       acl usePeerB time 02:00-02:29
> >       ... a total of 16 ORed lines for the second peer ...
> >       ... each line matches a unique 30 minute period ...
> >
> >       # and now match peer to its time slots
> >       cache_peer_access peerA allow usePeerA
> >       cache_peer_access peerB allow usePeerB
> >       cache_peer_access peerC allow !usePeerA !userPeerB
> >
> >
> >     The above may need further adjustments and polishing. For example, I
> am
> >     not sure how Squid will round these time values. The above assumes
> that
> >     00:29 limit includes all 60 seconds up to (but excluding) 00:30:00.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >     >> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
> >     >>> On 6/10/20 6:09 AM, Prem Chand wrote:
> >     >>>> My squid cache peer has 3 parent IP?s configured. I need to
> >     send HTTPS
> >     >>>> requests to the first parent IP for 30 minutes and after to the
> 2nd
> >     >>>> parent IP for 30 minutes and then to 3rd IP for 30 minutes and
> this
> >     >>>> switching needs to happen continuously .Could you please let us
> >     know
> >     >>>> how I can achieve this?
> >     >>>
> >     >>> If you are OK with hard-coded usage time slots for each peer,
> then I
> >     >>> would use two[1] "time" ACLs and cache_peer_access rules. Look
> for
> >     >>> "aclname time" in squid.conf.documented. You will have to
> generate a
> >     >>> list of (24*2/3=16) staggered time slots for each of the two
> >     ACLs, but
> >     >>> it should work. This may be the simplest solution.
> >     >>>
> >     >>> [1] You need two ACLs for three peers because the third peer
> >     should get
> >     >>> the requests that the first two peers were not allowed to get.
> >     >>>
> >     >>> ----
> >     >>>
> >     >>> With a modern Squid, you could also implement this using a more
> >     flexible
> >     >>> (and more expensive, on several layers!) architecture with two
> ACLs:
> >     >>>
> >     >>> 1. An external ACL that returns the right cache peer name to use
> >     via a
> >     >>> keyword=value annotation API. This always-matching ACL should be
> >     >>> attached to http_access or a similar directive that supports
> >     slow ACLs.
> >     >>> Its goal is to annotate the request. You will need to write a
> >     >>> script/program that will compute the right annotations based on
> >     time or
> >     >>> some other factors. This is where the flexibility of this
> >     solution is
> >     >>> coming from.
> >     >>>
> >     >>> 2. A "note" ACL attached to cache_peer_access directives,
> allowing
> >     >>> access to peer X if the external ACL in item 1 returned
> >     >>> use_cache_peer_=X. The "note" ACL is a fast ACL and, hence, can
> be
> >     >>> reliably used with cache_peer_access.
> >     >>>
> >     >>> If you already have another external ACL, you may be able to
> >     piggyback
> >     >>> annotations in item 1 to whatever that ACL is already doing.
> >     >>>
> >     >>> For more information, search for "keyword=value" and "acl
> >     aclname note"
> >     >>> in your squid.conf.documented and see
> >     >>>
> >
> https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
> >     >>> 29
> >     >>>
> >     >>>
> >     >>> HTH,
> >     >>>
> >     >>> Alex.
> >     >
> >
> >
> >
> > --
> > prem
>
>

-- 
prem
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200615/b277cc74/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 15 13:44:57 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jun 2020 09:44:57 -0400
Subject: [squid-users] Switch cache peer Parent server for every 30
 minutes
In-Reply-To: <CACbtF4OM=fPiitjZAAkfpT4F7aSu+wAM8b9UTMgy+KpFj7ZUZw@mail.gmail.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
 <202006101820.47163.Antony.Stone@squid.open.source.it>
 <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>
 <CACbtF4PGhKts-U96Y3nGFZauWduzfwDh3D3UAycGcpXMqeutRA@mail.gmail.com>
 <1d44d011-e5fc-d813-4435-eaf17a3616b8@measurement-factory.com>
 <CACbtF4OM=fPiitjZAAkfpT4F7aSu+wAM8b9UTMgy+KpFj7ZUZw@mail.gmail.com>
Message-ID: <02466514-dd1a-95ae-ff54-ac0f37664945@measurement-factory.com>

On 6/15/20 3:26 AM, Prem Chand wrote:

> I stopped the peerA(purposefully)? and noticed that requests are failing
> for the time slots that are going through peerA. I used
> "connect-fail-limit" in cache_peer? but it's not working. Is there any
> way we can address this issue using the same solution considering how to
> handle the requests if any of the parent? peer goes down?

I am not sure, but I think it should be possible to always give Squid
three peers to use, in the right order. There is no peer selection
algorithm that will do that automatically, but I suspect that a clever
combination of annotate_transaction and "note" ACLs in cache_peer_access
rules can be used to force a particular cache peer selection order.

https://wiki.squid-cache.org/Features/LoadBalance#Go_through_a_peer

The trick is to place one "best" peer into the first group (your rules
already do that!), but then stop banning peers so that the other two
peers are added to the "All Alive Parents" group (your rules currently
deny those two peers from being considered). It may be possible to stop
banning peers while the peer selection code is running its second pass
by changing request annotation.

I am sorry that I do not have enough time to sketch an example.

Alex.



> On Fri, Jun 12, 2020 at 6:47 PM Alex Rousskov wrote:
> 
>     On 6/11/20 11:52 PM, Prem Chand wrote:
> 
>     > It's working as expected. I tried to allow only specific domains
>     during
>     > the time by adding below acl but I'm getting HTTP status code 503
> 
>     > acl usePeerB time 00:30-00:59
>     > acl usePeerB time 02:00-02:29
>     > acl alloweddomains dstdomain google.com <http://google.com>
>     facebook.com <http://facebook.com>
> 
>     > cache_peer_access peerA allow usePeerA allowedomains
>     > cache_peer_access peerB allow usePeerB allowedomains
>     > cache_peer_access peerC allow !usePeerA !userPeerB alloweddomains
> 
>     Assuming there are no other cache peers, the above rules leave no
>     forwarding path for a request to a banned domain. If you want to ban
>     such requests, http_access instead of cache_peer_access.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > On Thu, Jun 11, 2020 at 4:54 AM Alex Rousskov wrote:
>     >
>     >? ? ?On 6/10/20 12:20 PM, Antony Stone wrote:
>     >? ? ?> On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:
>     >? ? ?>
>     >? ? ?>> Hi Alex,
>     >? ? ?>>
>     >? ? ?>> Thanks for responding to my issue? . I didn't get how the math
>     >? ? ?was done(why
>     >? ? ?>> it's multiplied by 2) to get 16 slots if possible could you
>     >? ? ?please elaborate
>     >? ? ?>> with an example.
>     >? ? ?>
>     >? ? ?> I believe what Alex meant was:
>     >? ? ?>
>     >? ? ?> You want 30 minute timeslots for each of 3 peers, which is 48
>     >? ? ?half-hour
>     >? ? ?> timeslots throughout the day.
>     >? ? ?>
>     >? ? ?> However, you only need to define 48/3 of these for peer A, and
>     >? ? ?48/3 of them for
>     >? ? ?> peer B, and then let peer C deal with anything not already
>     handled
>     >? ? ?(so it
>     >? ? ?> doesn't need its own definitions).
>     >? ? ?>
>     >? ? ?> 48/3 = 16, therefore you define 16 half-hour periods when
>     you want
>     >? ? ?peer A to do
>     >? ? ?> the work, 16 half-hour periods for peer B, and then just say
>     "peer
>     >? ? ?C, handle
>     >? ? ?> anything left over".
>     >
>     >? ? ?Thank you, Antony! Here is an untested sketch:
>     >
>     >? ? ?? acl usePeerA time 00:00-00:29
>     >? ? ?? acl usePeerA time 01:30-01:59
>     >? ? ?? ... a total of 16 ORed lines for the first peer ...
>     >? ? ?? ... each line matches a unique 30 minute period ...
>     >
>     >
>     >? ? ?? acl usePeerB time 00:30-00:59
>     >? ? ?? acl usePeerB time 02:00-02:29
>     >? ? ?? ... a total of 16 ORed lines for the second peer ...
>     >? ? ?? ... each line matches a unique 30 minute period ...
>     >
>     >? ? ?? # and now match peer to its time slots
>     >? ? ?? cache_peer_access peerA allow usePeerA
>     >? ? ?? cache_peer_access peerB allow usePeerB
>     >? ? ?? cache_peer_access peerC allow !usePeerA !userPeerB
>     >
>     >
>     >? ? ?The above may need further adjustments and polishing. For
>     example, I am
>     >? ? ?not sure how Squid will round these time values. The above
>     assumes that
>     >? ? ?00:29 limit includes all 60 seconds up to (but excluding)
>     00:30:00.
>     >
>     >
>     >? ? ?HTH,
>     >
>     >? ? ?Alex.
>     >
>     >
>     >? ? ?>> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
>     >? ? ?>>> On 6/10/20 6:09 AM, Prem Chand wrote:
>     >? ? ?>>>> My squid cache peer has 3 parent IP?s configured. I need to
>     >? ? ?send HTTPS
>     >? ? ?>>>> requests to the first parent IP for 30 minutes and after
>     to the 2nd
>     >? ? ?>>>> parent IP for 30 minutes and then to 3rd IP for 30
>     minutes and this
>     >? ? ?>>>> switching needs to happen continuously .Could you please
>     let us
>     >? ? ?know
>     >? ? ?>>>> how I can achieve this?
>     >? ? ?>>>
>     >? ? ?>>> If you are OK with hard-coded usage time slots for each
>     peer, then I
>     >? ? ?>>> would use two[1] "time" ACLs and cache_peer_access rules.
>     Look for
>     >? ? ?>>> "aclname time" in squid.conf.documented. You will have to
>     generate a
>     >? ? ?>>> list of (24*2/3=16) staggered time slots for each of the two
>     >? ? ?ACLs, but
>     >? ? ?>>> it should work. This may be the simplest solution.
>     >? ? ?>>>
>     >? ? ?>>> [1] You need two ACLs for three peers because the third peer
>     >? ? ?should get
>     >? ? ?>>> the requests that the first two peers were not allowed to get.
>     >? ? ?>>>
>     >? ? ?>>> ----
>     >? ? ?>>>
>     >? ? ?>>> With a modern Squid, you could also implement this using a
>     more
>     >? ? ?flexible
>     >? ? ?>>> (and more expensive, on several layers!) architecture with
>     two ACLs:
>     >? ? ?>>>
>     >? ? ?>>> 1. An external ACL that returns the right cache peer name
>     to use
>     >? ? ?via a
>     >? ? ?>>> keyword=value annotation API. This always-matching ACL
>     should be
>     >? ? ?>>> attached to http_access or a similar directive that supports
>     >? ? ?slow ACLs.
>     >? ? ?>>> Its goal is to annotate the request. You will need to write a
>     >? ? ?>>> script/program that will compute the right annotations
>     based on
>     >? ? ?time or
>     >? ? ?>>> some other factors. This is where the flexibility of this
>     >? ? ?solution is
>     >? ? ?>>> coming from.
>     >? ? ?>>>
>     >? ? ?>>> 2. A "note" ACL attached to cache_peer_access directives,
>     allowing
>     >? ? ?>>> access to peer X if the external ACL in item 1 returned
>     >? ? ?>>> use_cache_peer_=X. The "note" ACL is a fast ACL and,
>     hence, can be
>     >? ? ?>>> reliably used with cache_peer_access.
>     >? ? ?>>>
>     >? ? ?>>> If you already have another external ACL, you may be able to
>     >? ? ?piggyback
>     >? ? ?>>> annotations in item 1 to whatever that ACL is already doing.
>     >? ? ?>>>
>     >? ? ?>>> For more information, search for "keyword=value" and "acl
>     >? ? ?aclname note"
>     >? ? ?>>> in your squid.conf.documented and see
>     >? ? ?>>>
>     >? ?
>     ?https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
>     >? ? ?>>> 29
>     >? ? ?>>>
>     >? ? ?>>>
>     >? ? ?>>> HTH,
>     >? ? ?>>>
>     >? ? ?>>> Alex.
>     >? ? ?>
>     >
>     >
>     >
>     > --
>     > prem
> 
> 
> 
> -- 
> prem



From m992493 at gmail.com  Mon Jun 15 13:55:35 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Mon, 15 Jun 2020 19:25:35 +0530
Subject: [squid-users] Squid and c-icap's srv_url_check module
Message-ID: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>

Hi Guys,

I am trying to use the srv_url_check module to block websites.
I have configured squid with proxy authentication and followed this
wiki: https://sourceforge.net/p/c-icap/wiki/UrlCheckProfiles/
to configure c-icap and srv_url_check. Now, I am having trouble
configuring squid.conf. Below I have shared my configuration of squid.

I suspect that the last svcBlocker line in squid.conf, is the faulty
one, among other possible faults.
Please point out what is it that I am doing wrong.

/etc/squid/squid.conf
-----
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/squid_passwd
auth_param basic realm proxy
acl authenticated proxy_auth REQUIRED

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

http_access deny !authenticated
http_access allow localhost
http_access deny all

http_port 3128
coredump_dir /var/spool/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%
2880refresh_pattern .        0    20%    4320

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_client_username_encode on
icap_preview_enable on
icap_preview_size 1024

icap_service svcBlocker reqmod_precache
icap://127.0.0.1:1344/srv_url_check bypass=off
-----



Below are c-icap related files in case they are required too.


/usr/local/etc/c-icap.conf
-----
PidFile /var/run/c-icap/c-icap.pid
CommandsSocket /var/run/c-icap/c-icap.ctl
Timeout 300
MaxKeepAliveRequests 100
KeepAliveTimeout 600
StartServers 3
MaxServers 10
MinSpareThreads     10
MaxSpareThreads     20
ThreadsPerChild     10
MaxRequestsPerChild  0
Port 1344
[ciphers=ciph1:ciph2...] [tls_options=[!]Opt1|[!]Opt2|...]
ServerAdmin you at your.address
ServerName YourServerName
TmpDir /var/tmp
MaxMemObject 131072
DebugLevel 1
Pipelining on
SupportBuggyClients off
ModulesDir /usr/local/lib/c_icap
ServicesDir /usr/local/lib/c_icap
TemplateDir /usr/local/share/c_icap/templates/
TemplateDefaultLanguage en
LoadMagicFile /usr/local/etc/c-icap.magic

RemoteProxyUsers on
RemoteProxyUserHeader X-Authenticated-User
RemoteProxyUserHeaderEncoded on
GroupSourceByGroup hash:/usr/local/etc/c-icap-groups.txt
acl all src 0.0.0.0/0.0.0.0

LogFormat myFormat "%a %la %lp %>a %<A %ts %tl %tg %tr %>hi %>ho %huo
%hu %<hi %<ho %Hs %Hso %iu %im %is %>ih %<ih %ipl %Ih %Oh %Ib %Ob %I
%O %bph %un %Sl %Sa"

ServerLog /usr/local/var/log/server.log
AccessLog /usr/local/var/log/access.log myFormat all

Service echo srv_echo.so
Include srv_url_check.conf
-----

/usr/local/etc/c-icap-groups.txt
-----
Users: user1
-----

/usr/local/etc/urls.txt
-----
www.facebook.com/
-----

/usr/local/etc/srv_url_check.conf
-----
%{url_check:action_cat}Sa] [Action: %{url_check:action}Sa]"
Service url_check_module srv_url_check.so
url_check.LookupTableDB urls url hash:/usr/local/etc/urls.txt
url_check.Profile social_media block urls
url_check.Profile default pass ALL
acl facebook group Users
url_check.ProfileAccess social_media facebook
-----

Thanks
Amiq


From Loucansky.Lukas at kjj.cz  Tue Jun 16 07:43:19 2020
From: Loucansky.Lukas at kjj.cz (=?iso-8859-2?B?TG916GFuc2v9IEx1a+G5?=)
Date: Tue, 16 Jun 2020 09:43:19 +0200
Subject: [squid-users] SQUID 4.12 (Debian 10,
	OpenSSL 1.1.1d) - SSL bump no server helllo
Message-ID: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>

Hello,
I was wondering if anyone could take a look at this:
I'm running squid for rather long time, recently I have upgraded my squid box to Debian 10 (from Debian 9) and OpenSSL 1.1.1d 
 
4.19.0-9-amd64 #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07) x86_64 GNU/Linux OpenSSL 1.1.1d  10 Sep 2019

squid -v
Squid Cache: Version 4.12
Service Name: squid

This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid4' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid4' '--sysconfdir=/etc/squid4' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,SMB_LM' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-security-cert-validators=fake' '--enable-storeid-rewrite-helpers=file' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--enable-snmp' '--disable-translation' '--with-swapdir=/var/spool/squid4' '--with-logdir=/var/log/squid4' '--with-pidfile=/var/run/squid4.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-openssl' '--enable-ssl-crtd' '--enable-security-cert-generators' '--enable-security-cert-validators' '--enable-linux-netfilter' 'PKG_CONFIG_PATH=:/usr/local/lib/pkgconfig:/usr/lib64/pkgconfig:/usr/share/pkgconfig' 'CFLAGS=-g -O2 -m64 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -m64 -fPIE -fstack-protector-strong -Wformat -Werror=format-security' 'build_alias=x86_64-linux-gnu'

Before upgrade I was running stock kernel, stock openssl and compiled squid version 4.10 with ssl support to splice (local and excepted webs), peek and terminate ssl connections based on the SNI acl. 
 
Now I run into this problem - my configuration does not work anymore. So I decided to try to bump every connection. The security file certgen is making new certificates based on my CA as usual.
But the client on the intercepted connection (via changed routing table under mikrotik and then prerouted to correct squid ports for http and ssl traffic) running Chrome 83 http://download.kjj.cz/pub/ssl/idnes.cz_chrome.83.0.4103.97.pcapng sends ClientHello - and no ServerHello is received. I've tcpdumped outgoing interface on the squid box - and there was no actual connection to the desired server. 
In the access.log there is something like 1592212170.495      2 10.0.0.40 NONE_ABORTED/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
 
But - same client, same network, same network running Firefox 77 http://download.kjj.cz/pub/ssl/idnes.cz_firefox.77.0.1.pcapng  gets ServerHello after it's ClientHello - they exchange information, exchange ciphers etc. and the web page is loaded. I've checked https certificate details - it's been issued by my CA.


access.log:
 
1592212156.764      8 10.0.0.40 TCP_MISS/301 196 GET http://idnes.cz/ - ORIGINAL_DST/185.17.117.32 -
1592212156.774      2 10.0.0.40 NONE/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
1592212156.825     38 10.0.0.40 TCP_MISS/302 777 GET https://idnes.cz/ - ORIGINAL_DST/185.17.117.32 text/html
1592212156.840      7 10.0.0.40 NONE/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
1592212156.893     28 10.0.0.40 TCP_CLIENT_REFRESH_MISS/200 40086 GET https://www.idnes.cz/ - ORIGINAL_DST/185.17.117.32 text/html


So in Firefox - it seems to be working. I have modified opensll.cnf default configuration to avoid MinProtocol TLS1.2, but no change. I have 2048b SSL DH params specified for prime256v1 curve in the https-port definition like this https_port 3129 intercept ssl-bump  generate-host-certificates=on dynamic_cert_mem_cache_size=8MB cert=/etc/squid4/ssl/CAcert.pem tls-dh=prime256v1:/etc/squid4/ssl/dhparams_2048.pem cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

and

tls_outgoing_options options=NO_SSLv3
tls_outgoing_options cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

At first I thought I have to change my configuration or that I missed something during the compiling so I switched back to 4.10 - no change.I see 2020/06/15 11:21:45 kid2| Error negotiating SSL connection on FD 59: error:00000001:lib(0):func(0):reason(1) (1/-1) in the cache.log here and there - but it was the same before. I've actually turned debug on (by debug_options ALL,9), just to get bunch of information, tracked down connect request to the desired servers and seeing nothing...

Is it something about the patch for older TLS traffic, or is it some misconfiguration - maybe in the ciphers or TLS versions?

Thanks LL


From squid.org at bloms.de  Tue Jun 16 11:51:19 2020
From: squid.org at bloms.de (Dieter Bloms)
Date: Tue, 16 Jun 2020 13:51:19 +0200
Subject: [squid-users] get no content for
	https://wiki.squid-cache.org/SquidFaq/SquidLogs
Message-ID: <20200616115117.szpdu4jhojgni26s@bloms.de>

Hello,

i get no contewnt for https://wiki.squid-cache.org/SquidFaq/SquidLogs.
I get 504 Gaterway Timeout:

--snip--
Gateway Timeout
The gateway did not receive a timely response from the upstream server or application.

Apache/2.4.18 (Ubuntu) Server at wiki.squid-cache.org Port 443
--snip--


-- 
Regards

  Dieter Bloms

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From gkinkie at gmail.com  Tue Jun 16 13:03:52 2020
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 16 Jun 2020 15:03:52 +0200
Subject: [squid-users] get no content for
	https://wiki.squid-cache.org/SquidFaq/SquidLogs
In-Reply-To: <20200616115117.szpdu4jhojgni26s@bloms.de>
References: <20200616115117.szpdu4jhojgni26s@bloms.de>
Message-ID: <CA+Y8hcP7c2EgB4ox-cZSZPqcUQOReNx2GNCrBQCrzLBEz5=Wwg@mail.gmail.com>

Hi Dieter,
  it's now fixed. Thanks for letting us know!

On Tue, Jun 16, 2020 at 1:51 PM Dieter Bloms <squid.org at bloms.de> wrote:

> Hello,
>
> i get no contewnt for https://wiki.squid-cache.org/SquidFaq/SquidLogs.
> I get 504 Gaterway Timeout:
>
> --snip--
> Gateway Timeout
> The gateway did not receive a timely response from the upstream server or
> application.
>
> Apache/2.4.18 (Ubuntu) Server at wiki.squid-cache.org Port 443
> --snip--
>
>
> --
> Regards
>
>   Dieter Bloms
>
> --
> I do not get viruses because I do not use MS software.
> If you use Outlook then please do not put my email address in your
> address-book so that WHEN you get a virus it won't use my address in the
> From field.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200616/4a794268/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 16 13:49:31 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jun 2020 09:49:31 -0400
Subject: [squid-users] SQUID 4.12 (Debian 10,
 OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
Message-ID: <1638f204-e0fb-a893-97ba-ddf88c57c3db@measurement-factory.com>

On 6/16/20 3:43 AM, Lou?ansk? Luk?? wrote:

> I was wondering if anyone could take a look at this:

It might be TLS GREASE[1], but I do not see enough information to
reliably diagnose the problem. This is not your fault -- Squid just does
not log the relevant details by default (we are actively working on
improving that).

If others do not find a solution, I suggest sharing an ALL,9 log of a
problematic transaction.

Alex.

[1]
https://github.com/squid-cache/squid/commit/eec67f04490a477d69891c8b05a94bea05e5efbf


> I'm running squid for rather long time, recently I have upgraded my squid box to Debian 10 (from Debian 9) and OpenSSL 1.1.1d 
>  
> 4.19.0-9-amd64 #1 SMP Debian 4.19.118-2+deb10u1 (2020-06-07) x86_64 GNU/Linux OpenSSL 1.1.1d  10 Sep 2019
> 
> squid -v
> Squid Cache: Version 4.12
> Service Name: squid
> 
> This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal restrictions on distribution see https://www.openssl.org/source/license.html
> 
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid4' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid4' '--sysconfdir=/etc/squid4' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,SMB_LM' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-security-cert-validators=fake' '--enable-storeid-rewrite-helpers=file' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--enable-snmp' '--disable-translation' '--with-swapdir=/var/spool/squid4' '--with-logdir=/var/log/squid4' '--with-pidfile=/var/run/squid4.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-openssl' '--enable-ssl-crtd' '--enable-security-cert-generators' '--enable-security-cert-validators' '--enable-linux-netfilter' 'PKG_CONFIG_PATH=:/usr/local/lib/pkgconfig:/usr/lib64/pkgconfig:/usr/share/pkgconfig' 'CFLAGS=-g -O2 -m64 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -m64 -fPIE -fstack-protector-strong -Wformat -Werror=format-security' 'build_alias=x86_64-linux-gnu'
> 
> Before upgrade I was running stock kernel, stock openssl and compiled squid version 4.10 with ssl support to splice (local and excepted webs), peek and terminate ssl connections based on the SNI acl. 
>  
> Now I run into this problem - my configuration does not work anymore. So I decided to try to bump every connection. The security file certgen is making new certificates based on my CA as usual.
> But the client on the intercepted connection (via changed routing table under mikrotik and then prerouted to correct squid ports for http and ssl traffic) running Chrome 83 http://download.kjj.cz/pub/ssl/idnes.cz_chrome.83.0.4103.97.pcapng sends ClientHello - and no ServerHello is received. I've tcpdumped outgoing interface on the squid box - and there was no actual connection to the desired server. 
> In the access.log there is something like 1592212170.495      2 10.0.0.40 NONE_ABORTED/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
>  
> But - same client, same network, same network running Firefox 77 http://download.kjj.cz/pub/ssl/idnes.cz_firefox.77.0.1.pcapng  gets ServerHello after it's ClientHello - they exchange information, exchange ciphers etc. and the web page is loaded. I've checked https certificate details - it's been issued by my CA.
> 
> 
> access.log:
>  
> 1592212156.764      8 10.0.0.40 TCP_MISS/301 196 GET http://idnes.cz/ - ORIGINAL_DST/185.17.117.32 -
> 1592212156.774      2 10.0.0.40 NONE/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
> 1592212156.825     38 10.0.0.40 TCP_MISS/302 777 GET https://idnes.cz/ - ORIGINAL_DST/185.17.117.32 text/html
> 1592212156.840      7 10.0.0.40 NONE/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
> 1592212156.893     28 10.0.0.40 TCP_CLIENT_REFRESH_MISS/200 40086 GET https://www.idnes.cz/ - ORIGINAL_DST/185.17.117.32 text/html
> 
> 
> So in Firefox - it seems to be working. I have modified opensll.cnf default configuration to avoid MinProtocol TLS1.2, but no change. I have 2048b SSL DH params specified for prime256v1 curve in the https-port definition like this https_port 3129 intercept ssl-bump  generate-host-certificates=on dynamic_cert_mem_cache_size=8MB cert=/etc/squid4/ssl/CAcert.pem tls-dh=prime256v1:/etc/squid4/ssl/dhparams_2048.pem cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 
> and
> 
> tls_outgoing_options options=NO_SSLv3
> tls_outgoing_options cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 
> At first I thought I have to change my configuration or that I missed something during the compiling so I switched back to 4.10 - no change.I see 2020/06/15 11:21:45 kid2| Error negotiating SSL connection on FD 59: error:00000001:lib(0):func(0):reason(1) (1/-1) in the cache.log here and there - but it was the same before. I've actually turned debug on (by debug_options ALL,9), just to get bunch of information, tracked down connect request to the desired servers and seeing nothing...
> 
> Is it something about the patch for older TLS traffic, or is it some misconfiguration - maybe in the ciphers or TLS versions?
> 
> Thanks LL
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From loucansky.lukas at kjj.cz  Tue Jun 16 21:36:27 2020
From: loucansky.lukas at kjj.cz (=?UTF-8?B?THVrw6HFoSBMb3XEjWFuc2vDvQ==?=)
Date: Tue, 16 Jun 2020 23:36:27 +0200
Subject: [squid-users] SQUID 4.12 (Debian 10,
 OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <1638f204-e0fb-a893-97ba-ddf88c57c3db@measurement-factory.com>
References: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
 <1638f204-e0fb-a893-97ba-ddf88c57c3db@measurement-factory.com>
Message-ID: <5aea09cc-66c2-49ab-534d-69898561b51e@kjj.cz>

But - according to 
https://github.com/squid-cache/squid/commit/eec67f04490a477d69891c8b05a94bea05e5efbfGREASE 
- as unknown extensions is meant to be ignored (?). The same said here 
https://groups.google.com/a/chromium.org/forum/#!topic/security-dev/d_f6higCJzcBut 
- these information are years old - so I guess squid already does the 
right thing.

Anyway - with debug_options ALL,1 83,2:

2020/06/16 23:24:34.831 kid2| 83,2| client_side.cc(3180) 
parseTlsHandshake: error on FD 22: check failed: vMajor == 3
 ??? exception location: Handshake.cc(119) ParseProtocolVersion

from chrome

2020/06/16 23:24:37.794 kid2| 83,2| client_side.cc(2684) 
clientNegotiateSSL: New session 0x562491ae96c0 on FD 69 
(192.168.xx.yy:53569)
2020/06/16 23:24:37.845 kid2| 83,2| client_side.cc(2684) 
clientNegotiateSSL: New session 0x562490828890 on FD 70 
(192.168.xx.yy:53570)
2020/06/16 23:24:37.845 kid2| clientProcessHit: URL mismatch, 
'[unknown_URI]' != 'https://www.idnes.cz/'
2020/06/16 23:24:38.007 kid1| 83,2| client_side.cc(2684) 
clientNegotiateSSL: New session 0x55e735276ad0 on FD 57 
(192.168.xx.yy:53572)
2020/06/16 23:24:38.013 kid2| 83,2| client_side.cc(2684) 
clientNegotiateSSL: New session 0x56249117f780 on FD 71 
(192.168.xx.yy:53571)
2020/06/16 23:24:38.018 kid1| 83,2| client_side.cc(2684) 
clientNegotiateSSL: New session 0x55e730cfb640 on FD 59 
(192.168.xx.yy:53573)
2020/06/16 23:24:38.025 kid2| 83,2| client_side.cc(2684) 
clientNegotiateSSL: New session 0x562495f16e00 on FD 73 
(192.168.xx.yy:53574)
2020/06/16 23:24:38.028 kid2| 83,2| client_side.cc(2645) 
clientNegotiateSSL: Session 0x562492e12150 reused on FD 74 
(192.168.xx.yy:53576)
2020/06/16 23:24:38.028 kid2| clientProcessHit: URL mismatch, 
'[unknown_URI]' != 'https://1gr.cz/js/uni/uni.js?rr=37'

from firefox

LL



From squid3 at treenet.co.nz  Wed Jun 17 03:14:23 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jun 2020 15:14:23 +1200
Subject: [squid-users] SQUID 4.12 (Debian 10,
 OpenSSL 1.1.1d) - SSL bump no server helllo
Message-ID: <mailman.8.1736411449.1101126.squid-users@lists.squid-cache.org>



Sent from my alcatel U5
On 17/06/2020 09:36, Luk?? Lou?ansk? wrote:


> But - according to 
> https://github.com/squid-cache/squid/commit/eec67f04490a477d69891c8b05a94bea05e5efbfGREASE 
> - as unknown extensions is meant to be ignored (?). The same said here 
> https://groups.google.com/a/chromium.org/forum/#!topic/security-dev/d_f6higCJzcBut 
> - these information are years old - so I guess squid already does the 
> right thing.
>

This is not a safe assumption. Squid tries to use the TLS library for as much as possible, but there are many bits like extension handling which have to be rewritten for SSL-Bump to work. Those are all recent code additions.


> Anyway - with debug_options ALL,1 83,2:
>
> 2020/06/16 23:24:34.831 kid2| 83,2| client_side.cc(3180) 
> parseTlsHandshake: error on FD 22: check failed: vMajor == 3
> ??? exception location: Handshake.cc(119) ParseProtocolVersion
>

That is somewhat useful. TLS version being received is not valid.


Amos

From squid3 at treenet.co.nz  Wed Jun 17 04:41:34 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jun 2020 16:41:34 +1200
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
Message-ID: <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>

On 16/06/20 1:55 am, Amiq Nahas wrote:
> Hi Guys,
> 
> I am trying to use the srv_url_check module to block websites.
> I have configured squid with proxy authentication and followed this
> wiki: https://sourceforge.net/p/c-icap/wiki/UrlCheckProfiles/
> to configure c-icap and srv_url_check. Now, I am having trouble
> configuring squid.conf. Below I have shared my configuration of squid.
> 


"I am having trouble" is not sufficient details to investigate a problem.

Amos


From premchand142 at gmail.com  Wed Jun 17 06:20:59 2020
From: premchand142 at gmail.com (Prem Chand)
Date: Wed, 17 Jun 2020 11:50:59 +0530
Subject: [squid-users] Switch cache peer Parent server for every 30
	minutes
In-Reply-To: <02466514-dd1a-95ae-ff54-ac0f37664945@measurement-factory.com>
References: <CACbtF4MvjheLRwxi8b-HxWnq5a-duDQ-zOYPe=E2rRrfcXu+Sw@mail.gmail.com>
 <8a8aa28d-dfb1-cbb6-b397-e7412e1e4074@measurement-factory.com>
 <CACbtF4NWvYDRk3pwUPVn8gevVdh7aeuZACcRYKA86jJ0r-HKZA@mail.gmail.com>
 <202006101820.47163.Antony.Stone@squid.open.source.it>
 <54b1c7c7-0237-af38-5434-74215891f190@measurement-factory.com>
 <CACbtF4PGhKts-U96Y3nGFZauWduzfwDh3D3UAycGcpXMqeutRA@mail.gmail.com>
 <1d44d011-e5fc-d813-4435-eaf17a3616b8@measurement-factory.com>
 <CACbtF4OM=fPiitjZAAkfpT4F7aSu+wAM8b9UTMgy+KpFj7ZUZw@mail.gmail.com>
 <02466514-dd1a-95ae-ff54-ac0f37664945@measurement-factory.com>
Message-ID: <CACbtF4PZmAw5NdMhY80Bb_q86Uf0SvPeUjP2nWLjNtgCe1+Law@mail.gmail.com>

Hi Alex,

Could you please share with me a rough sketch example for the below
statement.
"but I suspect that a clever
combination of annotate_transaction and "note" ACLs in cache_peer_access
rules can be used to force a particular cache peer selection order."

On Mon, Jun 15, 2020 at 7:14 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 6/15/20 3:26 AM, Prem Chand wrote:
>
> > I stopped the peerA(purposefully)  and noticed that requests are failing
> > for the time slots that are going through peerA. I used
> > "connect-fail-limit" in cache_peer  but it's not working. Is there any
> > way we can address this issue using the same solution considering how to
> > handle the requests if any of the parent  peer goes down?
>
> I am not sure, but I think it should be possible to always give Squid
> three peers to use, in the right order. There is no peer selection
> algorithm that will do that automatically, but I suspect that a clever
> combination of annotate_transaction and "note" ACLs in cache_peer_access
> rules can be used to force a particular cache peer selection order.
>
> https://wiki.squid-cache.org/Features/LoadBalance#Go_through_a_peer
>
> The trick is to place one "best" peer into the first group (your rules
> already do that!), but then stop banning peers so that the other two
> peers are added to the "All Alive Parents" group (your rules currently
> deny those two peers from being considered). It may be possible to stop
> banning peers while the peer selection code is running its second pass
> by changing request annotation.
>
> I am sorry that I do not have enough time to sketch an example.
>
> Alex.
>
>
>
> > On Fri, Jun 12, 2020 at 6:47 PM Alex Rousskov wrote:
> >
> >     On 6/11/20 11:52 PM, Prem Chand wrote:
> >
> >     > It's working as expected. I tried to allow only specific domains
> >     during
> >     > the time by adding below acl but I'm getting HTTP status code 503
> >
> >     > acl usePeerB time 00:30-00:59
> >     > acl usePeerB time 02:00-02:29
> >     > acl alloweddomains dstdomain google.com <http://google.com>
> >     facebook.com <http://facebook.com>
> >
> >     > cache_peer_access peerA allow usePeerA allowedomains
> >     > cache_peer_access peerB allow usePeerB allowedomains
> >     > cache_peer_access peerC allow !usePeerA !userPeerB alloweddomains
> >
> >     Assuming there are no other cache peers, the above rules leave no
> >     forwarding path for a request to a banned domain. If you want to ban
> >     such requests, http_access instead of cache_peer_access.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >     > On Thu, Jun 11, 2020 at 4:54 AM Alex Rousskov wrote:
> >     >
> >     >     On 6/10/20 12:20 PM, Antony Stone wrote:
> >     >     > On Wednesday 10 June 2020 at 18:11:03, Prem Chand wrote:
> >     >     >
> >     >     >> Hi Alex,
> >     >     >>
> >     >     >> Thanks for responding to my issue  . I didn't get how the
> math
> >     >     was done(why
> >     >     >> it's multiplied by 2) to get 16 slots if possible could you
> >     >     please elaborate
> >     >     >> with an example.
> >     >     >
> >     >     > I believe what Alex meant was:
> >     >     >
> >     >     > You want 30 minute timeslots for each of 3 peers, which is 48
> >     >     half-hour
> >     >     > timeslots throughout the day.
> >     >     >
> >     >     > However, you only need to define 48/3 of these for peer A,
> and
> >     >     48/3 of them for
> >     >     > peer B, and then let peer C deal with anything not already
> >     handled
> >     >     (so it
> >     >     > doesn't need its own definitions).
> >     >     >
> >     >     > 48/3 = 16, therefore you define 16 half-hour periods when
> >     you want
> >     >     peer A to do
> >     >     > the work, 16 half-hour periods for peer B, and then just say
> >     "peer
> >     >     C, handle
> >     >     > anything left over".
> >     >
> >     >     Thank you, Antony! Here is an untested sketch:
> >     >
> >     >       acl usePeerA time 00:00-00:29
> >     >       acl usePeerA time 01:30-01:59
> >     >       ... a total of 16 ORed lines for the first peer ...
> >     >       ... each line matches a unique 30 minute period ...
> >     >
> >     >
> >     >       acl usePeerB time 00:30-00:59
> >     >       acl usePeerB time 02:00-02:29
> >     >       ... a total of 16 ORed lines for the second peer ...
> >     >       ... each line matches a unique 30 minute period ...
> >     >
> >     >       # and now match peer to its time slots
> >     >       cache_peer_access peerA allow usePeerA
> >     >       cache_peer_access peerB allow usePeerB
> >     >       cache_peer_access peerC allow !usePeerA !userPeerB
> >     >
> >     >
> >     >     The above may need further adjustments and polishing. For
> >     example, I am
> >     >     not sure how Squid will round these time values. The above
> >     assumes that
> >     >     00:29 limit includes all 60 seconds up to (but excluding)
> >     00:30:00.
> >     >
> >     >
> >     >     HTH,
> >     >
> >     >     Alex.
> >     >
> >     >
> >     >     >> On Wed, Jun 10, 2020 at 7:12 PM Alex Rousskov wrote:
> >     >     >>> On 6/10/20 6:09 AM, Prem Chand wrote:
> >     >     >>>> My squid cache peer has 3 parent IP?s configured. I need
> to
> >     >     send HTTPS
> >     >     >>>> requests to the first parent IP for 30 minutes and after
> >     to the 2nd
> >     >     >>>> parent IP for 30 minutes and then to 3rd IP for 30
> >     minutes and this
> >     >     >>>> switching needs to happen continuously .Could you please
> >     let us
> >     >     know
> >     >     >>>> how I can achieve this?
> >     >     >>>
> >     >     >>> If you are OK with hard-coded usage time slots for each
> >     peer, then I
> >     >     >>> would use two[1] "time" ACLs and cache_peer_access rules.
> >     Look for
> >     >     >>> "aclname time" in squid.conf.documented. You will have to
> >     generate a
> >     >     >>> list of (24*2/3=16) staggered time slots for each of the
> two
> >     >     ACLs, but
> >     >     >>> it should work. This may be the simplest solution.
> >     >     >>>
> >     >     >>> [1] You need two ACLs for three peers because the third
> peer
> >     >     should get
> >     >     >>> the requests that the first two peers were not allowed to
> get.
> >     >     >>>
> >     >     >>> ----
> >     >     >>>
> >     >     >>> With a modern Squid, you could also implement this using a
> >     more
> >     >     flexible
> >     >     >>> (and more expensive, on several layers!) architecture with
> >     two ACLs:
> >     >     >>>
> >     >     >>> 1. An external ACL that returns the right cache peer name
> >     to use
> >     >     via a
> >     >     >>> keyword=value annotation API. This always-matching ACL
> >     should be
> >     >     >>> attached to http_access or a similar directive that
> supports
> >     >     slow ACLs.
> >     >     >>> Its goal is to annotate the request. You will need to
> write a
> >     >     >>> script/program that will compute the right annotations
> >     based on
> >     >     time or
> >     >     >>> some other factors. This is where the flexibility of this
> >     >     solution is
> >     >     >>> coming from.
> >     >     >>>
> >     >     >>> 2. A "note" ACL attached to cache_peer_access directives,
> >     allowing
> >     >     >>> access to peer X if the external ACL in item 1 returned
> >     >     >>> use_cache_peer_=X. The "note" ACL is a fast ACL and,
> >     hence, can be
> >     >     >>> reliably used with cache_peer_access.
> >     >     >>>
> >     >     >>> If you already have another external ACL, you may be able
> to
> >     >     piggyback
> >     >     >>> annotations in item 1 to whatever that ACL is already
> doing.
> >     >     >>>
> >     >     >>> For more information, search for "keyword=value" and "acl
> >     >     aclname note"
> >     >     >>> in your squid.conf.documented and see
> >     >     >>>
> >     >
> >
> https://wiki.squid-cache.org/Features/AddonHelpers#Access_Control_.28ACL.
> >     >     >>> 29
> >     >     >>>
> >     >     >>>
> >     >     >>> HTH,
> >     >     >>>
> >     >     >>> Alex.
> >     >     >
> >     >
> >     >
> >     >
> >     > --
> >     > prem
> >
> >
> >
> > --
> > prem
>
>

-- 
prem
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200617/3ca84064/attachment.htm>

From Loucansky.Lukas at kjj.cz  Wed Jun 17 07:11:02 2020
From: Loucansky.Lukas at kjj.cz (=?iso-8859-2?B?TG916GFuc2v9IEx1a+G5?=)
Date: Wed, 17 Jun 2020 09:11:02 +0200
Subject: [squid-users] SQUID 4.12 (Debian 10,
	OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <20200617031513.A98FBE09F4@lists.squid-cache.org>
References: <20200617031513.A98FBE09F4@lists.squid-cache.org>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B5301089339@kjj-server.KJJ.local>


> That is somewhat useful. TLS version being received is not valid.

Ok - although this is squid users phorum - this could be even more useful:

Firefox - http://download.kjj.cz/pub/ssl/firefox.txt it goes throught everything to the GET / HTTP/1.1 request

Chrome - http://download.kjj.cz/pub/ssl/chrome.txt - it goes from
2020/06/17 08:06:31.292 kid1| 93,7| HttpRequest.cc(63) ~HttpRequest: destructed, this=0x55e730f38e50
2020/06/17 08:06:31.292 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=202 occupying 1 bytes @0 in 0x7ffd9ba4a0f0.
2020/06/17 08:06:31.292 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf71215602 destructed
2020/06/17 08:06:31.292 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=202 occupying 1 bytes @1 in 0x7ffd9ba4a0f0.
2020/06/17 08:06:31.292 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf71215601 destructed

to
2020/06/17 08:06:31.292 kid2| 0,3| Handshake.cc(119) ParseProtocolVersion: check failed: vMajor == 3
    exception location: Handshake.cc(119) ParseProtocolVersion

It is not working in all chrome based browsers - Edge, Opera... It is working in the MSIE and FF

LL



From Loucansky.Lukas at kjj.cz  Wed Jun 17 09:29:54 2020
From: Loucansky.Lukas at kjj.cz (=?iso-8859-2?B?TG916GFuc2v9IEx1a+G5?=)
Date: Wed, 17 Jun 2020 11:29:54 +0200
Subject: [squid-users] SQUID 4.12 (Debian 10,
	OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B5301089339@kjj-server.KJJ.local>
References: <20200617031513.A98FBE09F4@lists.squid-cache.org>
 <72DD5D5CF661B5459DC08A060BF26B5301089339@kjj-server.KJJ.local>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B530108933A@kjj-server.KJJ.local>

Found this:

2020/06/17 08:06:31.292 kid2| 24,7| BinaryTokenizer.cc(74) got: SupportedVersions.octets= caca0304030303020301 occupying 10 bytes @1 in 0x7ffd9ba4a0b0.
0x0301 - 0x0304 -> TLS versions to TLS1.3

0xcaca = non-existent

(a few lines further:)
BinaryTokenizer.cc(65) got: supported_version.major=202 occupying 1 bytes @0 in 0x7ffd9ba4a0f0. 

Note 0xCA = 202 dec

Another examples:
2020/06/17 08:06:31.312 kid1| 24,7| BinaryTokenizer.cc(74) got: SupportedVersions.octets= 3a3a0304030303020301 occupying 10 bytes @1 in 0x7ffe348a1f30.

2020/06/17 08:06:31.312 kid1| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=58 occupying 1 bytes @0 in 0x7ffe348a1f70.

Note 0x3A = 58 dec

2020/06/17 08:06:31.324 kid1| 24,7| BinaryTokenizer.cc(74) got: SupportedVersions.octets= aaaa0304030303020301 occupying 10 bytes @1 in 0x7ffe348a1f30.
2020/06/17 08:06:31.324 kid1| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=170 occupying 1 bytes @1 in 0x7ffe348a1f70.

Note 0xAA = 170 dec


So - I think this is a) badly pased string in /parser/BinaryTokenizer.cc (not likely), or b) in /security/HandShake.cc (line 526 and beyond)
Security::HandShakeParser does not ignore obviously nonse version

What I see is - that it calls tokenizer to get tkVersions, then asks ParseProtocolVersion to check it. I think that code ParseProtocolVersion checks for version 0.2 OR expects version 3.x - but gets versions 202 or 58 etc.

It seems logical to my limited knowledge to check for and ignore uknown versions (GREASed????). I think this is the while statement involved

while (!tkVersions.atEnd()) {
            const auto version = ParseProtocolVersion(tkVersions, "supported_version");
            if (!supportedVersionMax || TlsVersionEarlierThan(supportedVersionMax, version))
                supportedVersionMax = version;
        }

It calls parser - according to
2020/06/17 08:06:31.312 kid1| 0,3| Handshake.cc(119) ParseProtocolVersion: check failed: vMajor == 3    exception location: Handshake.cc(119) ParseProtocolVersion

It fails while calling it - so the check must be before calling ParseProtocolVersion or while in it - there is statement Must(vMajor==3) on line 119 - so I think this is the breakpoint call. Would simple if (vMajor <= 3)... Statement be  sufficient? What value it should return in case of non-parsable version? Sure not any value or some arbitrary value such as TLS1.something or SSLv3 ... It goes through SSLv2 to SSLv3 (implies vMajor = 3) and for versions >3.0 returns TLS1.vMinor-1 (???). So what it should do if it's called with version 0xCACA or 0x3A3A - I think that there should be check in the mentioned while statement - but it involves parsing major and minor version. This already does ParseProtocolVersion. But I think the goal of this is to find the max supported TLS version - so it should not fail on non-existent versions. So I think the mentioned while statement should sort this out, not calling parser to ask for TLS version for "random" numbers.

LL



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Lou?ansk? Luk??
Sent: Wednesday, June 17, 2020 9:11 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SQUID 4.12 (Debian 10,OpenSSL 1.1.1d) - SSL bump no server helllo


> That is somewhat useful. TLS version being received is not valid.

Ok - although this is squid users phorum - this could be even more useful:

Firefox - http://download.kjj.cz/pub/ssl/firefox.txt it goes throught everything to the GET / HTTP/1.1 request

Chrome - http://download.kjj.cz/pub/ssl/chrome.txt - it goes from
2020/06/17 08:06:31.292 kid1| 93,7| HttpRequest.cc(63) ~HttpRequest: destructed, this=0x55e730f38e50
2020/06/17 08:06:31.292 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=202 occupying 1 bytes @0 in 0x7ffd9ba4a0f0.
2020/06/17 08:06:31.292 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf71215602 destructed
2020/06/17 08:06:31.292 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=202 occupying 1 bytes @1 in 0x7ffd9ba4a0f0.
2020/06/17 08:06:31.292 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf71215601 destructed

to
2020/06/17 08:06:31.292 kid2| 0,3| Handshake.cc(119) ParseProtocolVersion: check failed: vMajor == 3
    exception location: Handshake.cc(119) ParseProtocolVersion

It is not working in all chrome based browsers - Edge, Opera... It is working in the MSIE and FF

LL

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From Loucansky.Lukas at kjj.cz  Wed Jun 17 11:35:51 2020
From: Loucansky.Lukas at kjj.cz (=?UTF-8?B?TG91xI1hbnNrw70gTHVrw6HFoQ==?=)
Date: Wed, 17 Jun 2020 13:35:51 +0200
Subject: [squid-users] SQUID 4.12 (Debian 10,
	OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B530108933A@kjj-server.KJJ.local>
References: <20200617031513.A98FBE09F4@lists.squid-cache.org>
 <72DD5D5CF661B5459DC08A060BF26B5301089339@kjj-server.KJJ.local>
 <72DD5D5CF661B5459DC08A060BF26B530108933A@kjj-server.KJJ.local>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B530108933B@kjj-server.KJJ.local>

This is the most na?ve and dirtiest effort but I don't know where else it's called - not going to check it and fix calling it with nonsense numbers - so I went like this:

/// parse TLS ProtocolVersion (uint16) and convert it to AnyP::ProtocolVersion
static AnyP::ProtocolVersion
ParseProtocolVersion(Parser::BinaryTokenizer &tk, const char *contextLabel = ".version")
{
    Parser::BinaryTokenizerContext context(tk, contextLabel);
    uint8_t vMajor = tk.uint8(".major");
    uint8_t vMinor = tk.uint8(".minor");

if (vMajor>3)      return AnyP::ProtocolVersion(AnyP::PROTO_TLS, 1, 0);

    if (vMajor == 0 && vMinor == 2)
        return AnyP::ProtocolVersion(AnyP::PROTO_SSL, 2, 0);

    Must(vMajor == 3);
    if (vMinor == 0)
        return AnyP::ProtocolVersion(AnyP::PROTO_SSL, 3, 0);

    return AnyP::ProtocolVersion(AnyP::PROTO_TLS, 1, (vMinor - 1));
}


So - if someone tries to fool us with random numbers - rule it out as TLS 1.0. I know it deserves more - this code does what it is not mean to be doing etc. etc. (for every version >3 returns something) But:

2020/06/17 13:02:12.978 kid2| 24,7| BinaryTokenizer.cc(65) got: Extension.type=43 occupying 2 bytes @164 in 0x7ffcd4777170.
2020/06/17 13:02:12.978 kid2| 24,7| BinaryTokenizer.cc(65) got: Extension.data.length=11 occupying 2 bytes @166 in 0x7ffcd4777170.
2020/06/17 13:02:12.978 kid2| 24,8| SBuf.cc(38) SBuf: SBuf15611 created from id SBuf15576
2020/06/17 13:02:12.978 kid2| 24,7| BinaryTokenizer.cc(74) got: Extension.data.octets= 0a7a7a0304030303020301 occupying 11 bytes @168 in 0x7ffcd4777170.
2020/06/17 13:02:12.978 kid2| 24,8| SBuf.cc(70) ~SBuf: SBuf15611 destructed
2020/06/17 13:02:12.978 kid2| 24,7| BinaryTokenizer.cc(57) got: Extension occupying 15 bytes @164 in 0x7ffcd4777170.
2020/06/17 13:02:12.978 kid2| 24,8| SBuf.cc(38) SBuf: SBuf15612 created from id SBuf15610
2020/06/17 13:02:12.978 kid2| 24,7| BinaryTokenizer.cc(65) got: SupportedVersions.length=10 occupying 1 bytes @0 in 0x7ffcd4776fd0.
2020/06/17 13:02:12.978 kid2| 24,8| SBuf.cc(38) SBuf: SBuf15613 created from id SBuf15612
2020/06/17 13:02:12.978 kid2| 24,7| BinaryTokenizer.cc(74) got: SupportedVersions.octets= 7a7a0304030303020301 occupying 10 bytes @1 in 0x7ffcd4776fd0.
2020/06/17 13:02:12.979 kid2| 24,8| SBuf.cc(38) SBuf: SBuf15614 created from id SBuf15613
2020/06/17 13:02:12.979 kid2| 24,8| SBuf.cc(70) ~SBuf: SBuf15613 destructed
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=122 occupying 1 bytes @0 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=122 occupying 1 bytes @1 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=3 occupying 1 bytes @2 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=4 occupying 1 bytes @3 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=3 occupying 1 bytes @4 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=3 occupying 1 bytes @5 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=3 occupying 1 bytes @6 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=2 occupying 1 bytes @7 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.major=3 occupying 1 bytes @8 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,7| BinaryTokenizer.cc(65) got: supported_version.minor=1 occupying 1 bytes @9 in 0x7ffcd4777010.
2020/06/17 13:02:12.979 kid2| 24,8| SBuf.cc(70) ~SBuf: SBuf15614 destructed
2020/06/17 13:02:12.979 kid2| 24,8| SBuf.cc(70) ~SBuf: SBuf15612 destructed
2020/06/17 13:02:12.979 kid2| 83,7| Handshake.cc(594) parseSupportedVersionsExtension: found TLS/1.3
2020/06/17 13:02:12.979 kid2| 24,8| SBuf.cc(70) ~SBuf: SBuf15610 destructed

Note 7a7a0304030303020301, 0x7A = 122

I think fixing it everywhere would involve BinaryTokenizing extension string (like  tkVersions) and check every value sent to  ParseProtocolVersion. In the HandShake.cc file on about six occassions. It seems very likely that *some* vendors will send nonsense values to the other parts as well. So it would be nice to have them all sanitized. For me it looks like Google initiative - but I could be wrong. Anyway - what seemed to be problem with TLS on my box now seems to be problem with additive, random numbers in the supported versions string - waiting for someone to investigate it further...

LL


From Loucansky.Lukas at kjj.cz  Wed Jun 17 13:14:10 2020
From: Loucansky.Lukas at kjj.cz (=?iso-8859-2?B?TG916GFuc2v9IEx1a+G5?=)
Date: Wed, 17 Jun 2020 15:14:10 +0200
Subject: [squid-users] SQUID 4.12 (Debian 10,
	OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B530108933B@kjj-server.KJJ.local>
References: <20200617031513.A98FBE09F4@lists.squid-cache.org>
 <72DD5D5CF661B5459DC08A060BF26B5301089339@kjj-server.KJJ.local>
 <72DD5D5CF661B5459DC08A060BF26B530108933A@kjj-server.KJJ.local>
 <72DD5D5CF661B5459DC08A060BF26B530108933B@kjj-server.KJJ.local>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B530108933C@kjj-server.KJJ.local>

 
Just noticed that github version of HandShake.cc is much better "patched" than my humble,pitty attempt to quick-fix the parser. So in the light of self investigation and the lack of information and experience (I'm sorry for that) I maybe over-reacted. But now it seems both modifications made it working (recompiled with github version of Handshake.cc). As I have no idea what to expect from GREASEd connections - I think it would be better - as I've already wrote - to check and sanitize every TLS header input... (version 4.12 from 9th June doesn't do that)

LL


From squid.org at bloms.de  Wed Jun 17 13:17:25 2020
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 17 Jun 2020 15:17:25 +0200
Subject: [squid-users] print errormessage (like %E in ERR_* pages) in squid
	logfile ?
Message-ID: <20200617131724.m67nxdb6bkj2dkfx@bloms.de>

Hello,

more and more clients aren't browser but are programs, which call a
restapi through our squid proxy.

Those clients aren't able to show the errorpage (ERR_*) from proxy in
case the request wasn't successful for any reason.

I added %err_code and %err_detail, but %err_detail is filled with "-" sign all the
time in the logfiles.

For example:
If the connection to a webserver fails %err_code is filled with ERR_CONNECT_FAIL, but
%err_detail is filled with "-" instead of the messages "(110) Connection %timed out"

Is it possible to log the error message like %E in the error pages ?

Thank you very much.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From m992493 at gmail.com  Wed Jun 17 13:32:30 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Wed, 17 Jun 2020 19:02:30 +0530
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
Message-ID: <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>

On Wed, Jun 17, 2020 at 10:23 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 16/06/20 1:55 am, Amiq Nahas wrote:
> > Hi Guys,
> >
> > I am trying to use the srv_url_check module to block websites.
> > I have configured squid with proxy authentication and followed this
> > wiki: https://sourceforge.net/p/c-icap/wiki/UrlCheckProfiles/
> > to configure c-icap and srv_url_check. Now, I am having trouble
> > configuring squid.conf. Below I have shared my configuration of squid.
> >
>
>
> "I am having trouble" is not sufficient details to investigate a problem.

Sorry my bad.
So After doing all the above configuration. The browser does not block
the websites in the blocklist.
Browser does prompt for user credentials just like the squid.conf is
configured to do, but it is not blocking websites.

However, when I execute c-icap-client from command line it blocks the
blocklisted websites.
To check with c-icap-client I have used the below command:
`c-icap-client -s url_check -x "X-Authenticated-User: dXNlcjE=" -req
"https://www.facebook.com/" -v`

So if the blocking from c-icap client is working but blocking from
browser is not working then
something must be wrong with my squid.conf or the configuration part
responsible for making c-icap and squid work together, right?

So what could be it? Please let me know if any other piece of
information is required, I am not sure what else could be of use.

Thanks
Amiq


From rousskov at measurement-factory.com  Wed Jun 17 14:08:13 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jun 2020 10:08:13 -0400
Subject: [squid-users] SQUID 4.12 (Debian 10,
 OpenSSL 1.1.1d) - SSL bump no server helllo
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B530108933C@kjj-server.KJJ.local>
References: <20200617031513.A98FBE09F4@lists.squid-cache.org>
 <72DD5D5CF661B5459DC08A060BF26B5301089339@kjj-server.KJJ.local>
 <72DD5D5CF661B5459DC08A060BF26B530108933A@kjj-server.KJJ.local>
 <72DD5D5CF661B5459DC08A060BF26B530108933B@kjj-server.KJJ.local>
 <72DD5D5CF661B5459DC08A060BF26B530108933C@kjj-server.KJJ.local>
Message-ID: <9c6aab9a-98ad-90b2-3110-f7cab0e35a28@measurement-factory.com>

On 6/17/20 9:14 AM, Lou?ansk? Luk?? wrote:

> Just noticed that github version of HandShake.cc is much better "patched"

Squid should have proper support for GREASEd TLS version values (and
more!) since master/v6 commit eec67f0. That very recent change has not
been ported to earlier Squid versions yet.

    https://github.com/squid-cache/squid/commit/eec67f0

Alex.


From rousskov at measurement-factory.com  Wed Jun 17 14:24:21 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jun 2020 10:24:21 -0400
Subject: [squid-users] print errormessage (like %E in ERR_* pages) in
 squid logfile ?
In-Reply-To: <20200617131724.m67nxdb6bkj2dkfx@bloms.de>
References: <20200617131724.m67nxdb6bkj2dkfx@bloms.de>
Message-ID: <272c561d-4a9a-18e4-1969-c1f23a179814@measurement-factory.com>

On 6/17/20 9:17 AM, Dieter Bloms wrote:

> more and more clients aren't browser but are programs, which call a
> restapi through our squid proxy.
> 
> Those clients aren't able to show the errorpage (ERR_*) from proxy in
> case the request wasn't successful for any reason.
> 
> I added %err_code and %err_detail, but %err_detail is filled with "-" sign all the
> time in the logfiles.
> 
> For example:
> If the connection to a webserver fails %err_code is filled with ERR_CONNECT_FAIL, but
> %err_detail is filled with "-" instead of the messages "(110) Connection %timed out"
> 
> Is it possible to log the error message like %E in the error pages ?


Hello Dieter,

    No, it is not possible to log %E to access.log, and the errno-based
%E itself is often useless in such situations, especially in the
official code where it is often lost or stale.

However, your use case _is_ valid, and Factory is working on improving
detailing of various connectivity problems similar to the ones you are
talking about. There is no official pull request yet, but we are getting
very close. If you would like, please feel free to test-drive our
master/v6-based https://github.com/measurement-factory/squid/pull/63


HTH,

Alex.


From squid3 at treenet.co.nz  Wed Jun 17 14:55:26 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jun 2020 02:55:26 +1200
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
Message-ID: <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>

On 18/06/20 1:32 am, Amiq Nahas wrote:
> On Wed, Jun 17, 2020 at 10:23 AM Amos Jeffries wrote:
>>
>> On 16/06/20 1:55 am, Amiq Nahas wrote:
>>> Hi Guys,
>>>
>>> I am trying to use the srv_url_check module to block websites.
>>> I have configured squid with proxy authentication and followed this
>>> wiki: https://sourceforge.net/p/c-icap/wiki/UrlCheckProfiles/
>>> to configure c-icap and srv_url_check. Now, I am having trouble
>>> configuring squid.conf. Below I have shared my configuration of squid.
>>>
>>
>>
>> "I am having trouble" is not sufficient details to investigate a problem.
> 
> Sorry my bad.
> So After doing all the above configuration. The browser does not block
> the websites in the blocklist.

If the Browser is doing blocking the request would never reach Squid.


> Browser does prompt for user credentials just like the squid.conf is
> configured to do, but it is not blocking websites.
> 
> However, when I execute c-icap-client from command line it blocks the
> blocklisted websites.
> To check with c-icap-client I have used the below command:
> `c-icap-client -s url_check -x "X-Authenticated-User: dXNlcjE=" -req
> "https://www.facebook.com/" -v`
> 
> So if the blocking from c-icap client is working but blocking from
> browser is not working then

Please define "blocking from c-icap"

Please define "blocking from browser"


> something must be wrong with my squid.conf or the configuration part
> responsible for making c-icap and squid work together, right?

Unknown.

> 
> So what could be it? Please let me know if any other piece of
> information is required, I am not sure what else could be of use.
> 

Log trace(s) from a transaction that you think is failing to start with.
Squid access.log and c-icap log. Maybe a Squid cache.log trace with
debug_options 11,2 or ALL,2


Amos


From ngtech1ltd at gmail.com  Wed Jun 17 15:21:35 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 17 Jun 2020 18:21:35 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <2C09A4F5-AB10-41F2-BC6A-D227B2ABD623@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200617/8d928d77/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: A0B914DE82C24C3FAAE0FD4F62DCE08D.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200617/8d928d77/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200617/8d928d77/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200617/8d928d77/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200617/8d928d77/attachment-0002.png>

From ngtech1ltd at gmail.com  Thu Jun 18 12:17:20 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 18 Jun 2020 15:17:20 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <22186F7E-2BC4-44CD-940A-02072A2D92E6@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200618/821defc3/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 5A03F073458A481D835EA299BA239C9A.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200618/821defc3/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200618/821defc3/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200618/821defc3/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200618/821defc3/attachment-0002.png>

From m992493 at gmail.com  Fri Jun 19 08:46:54 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Fri, 19 Jun 2020 14:16:54 +0530
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
 <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
Message-ID: <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>

On Wed, Jun 17, 2020 at 8:28 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> > Browser does prompt for user credentials just like the squid.conf is
> > configured to do, but it is not blocking websites.
> >
> > However, when I execute c-icap-client from command line it blocks the
> > blocklisted websites.
> > To check with c-icap-client I have used the below command:
> > `c-icap-client -s url_check -x "X-Authenticated-User: dXNlcjE=" -req
> > "https://www.facebook.com/" -v`
> >
> > So if the blocking from c-icap client is working but blocking from
> > browser is not working then
>
> Please define "blocking from c-icap"
> Please define "blocking from browser"

Squid is being used as a manual proxy at localhost:3128 in firefox,
and as a way of authenticating a user (proxy authentication), in my
case I am the sole user.
Squid is configured and is being used by the browser on the same machine.

When I said blocking from browser is not happening,
I meant that when I use the browser to access a website it is not blocked,
where as, when c-icap-client is used to make a request using the command
`c-icap-client -s url_check -x "X-Authenticated-User: dXNlcjE=" -req
"https://www.facebook.com/" -v`
I get the response saying the website is blocked.

ls> > So what could be it? Please let me know if any other piece of
> > information is required, I am not sure what else could be of use.
> >
>
> Log trace(s) from a transaction that you think is failing to start with.
> Squid access.log and c-icap log. Maybe a Squid cache.log trace with
> debug_options 11,2 or ALL,2

Squid logs access.log and cache.log do contain the logs of requested
websites requested via browser.
I enabled debug_options ALL,2 and here are the logs:
access.log: https://paste.debian.net/1152807/
cache.log: https://justpaste.it/45fca


From squid3 at treenet.co.nz  Fri Jun 19 09:29:42 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jun 2020 21:29:42 +1200
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
 <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
 <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>
Message-ID: <b0c68567-9705-aa17-c828-40a799152880@treenet.co.nz>

On 19/06/20 8:46 pm, Amiq Nahas wrote:
> On Wed, Jun 17, 2020 at 8:28 PM Amos Jeffries wrote:
>>
>>> Browser does prompt for user credentials just like the squid.conf is
>>> configured to do, but it is not blocking websites.
>>>
>>> However, when I execute c-icap-client from command line it blocks the
>>> blocklisted websites.
>>> To check with c-icap-client I have used the below command:
>>> `c-icap-client -s url_check -x "X-Authenticated-User: dXNlcjE=" -req
>>> "https://www.facebook.com/" -v`
>>>
>>> So if the blocking from c-icap client is working but blocking from
>>> browser is not working then
>>
>> Please define "blocking from c-icap"
>> Please define "blocking from browser"
> 
> Squid is being used as a manual proxy at localhost:3128 in firefox,
> and as a way of authenticating a user (proxy authentication), in my
> case I am the sole user.
> Squid is configured and is being used by the browser on the same machine.
> 
> When I said blocking from browser is not happening,
> I meant that when I use the browser to access a website it is not blocked,
> where as, when c-icap-client is used to make a request using the command
> `c-icap-client -s url_check -x "X-Authenticated-User: dXNlcjE=" -req
> "https://www.facebook.com/" -v`
> I get the response saying the website is blocked.
> 
> ls> > So what could be it? Please let me know if any other piece of
>>> information is required, I am not sure what else could be of use.
>>>
>>
>> Log trace(s) from a transaction that you think is failing to start with.
>> Squid access.log and c-icap log. Maybe a Squid cache.log trace with
>> debug_options 11,2 or ALL,2
> 
> Squid logs access.log and cache.log do contain the logs of requested
> websites requested via browser.
> I enabled debug_options ALL,2 and here are the logs:


Looks like traffic is fine and Squid operational, but no sign of any
ICAP activity. I think try adding this to your config:

  adaptation_access svcBlocker allow all

Its supposed to be the default action, but just to be sure add it
explicitly.


Amos


From m992493 at gmail.com  Fri Jun 19 10:07:37 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Fri, 19 Jun 2020 15:37:37 +0530
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <b0c68567-9705-aa17-c828-40a799152880@treenet.co.nz>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
 <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
 <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>
 <b0c68567-9705-aa17-c828-40a799152880@treenet.co.nz>
Message-ID: <CAPicJaF=r2Xpp-hN-kTnVUu8BFFZHc-eHksHKgaNgeC_+J1SUQ@mail.gmail.com>

> Looks like traffic is fine and Squid operational, but no sign of any
> ICAP activity. I think try adding this to your config:
>
>   adaptation_access svcBlocker allow all
>
> Its supposed to be the default action, but just to be sure add it
> explicitly.

I added the above line and browser could not open any page, this was
the notice shown: https://ibb.co/HVQYD2c
cache.log: https://justpaste.it/38eyl
access.log: https://paste.debian.net/1152817/


From squid3 at treenet.co.nz  Fri Jun 19 10:47:08 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jun 2020 22:47:08 +1200
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <CAPicJaF=r2Xpp-hN-kTnVUu8BFFZHc-eHksHKgaNgeC_+J1SUQ@mail.gmail.com>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
 <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
 <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>
 <b0c68567-9705-aa17-c828-40a799152880@treenet.co.nz>
 <CAPicJaF=r2Xpp-hN-kTnVUu8BFFZHc-eHksHKgaNgeC_+J1SUQ@mail.gmail.com>
Message-ID: <834e052d-a35c-3110-ddc3-644881b5678e@treenet.co.nz>

On 19/06/20 10:07 pm, Amiq Nahas wrote:
>> Looks like traffic is fine and Squid operational, but no sign of any
>> ICAP activity. I think try adding this to your config:
>>
>>   adaptation_access svcBlocker allow all
>>
>> Its supposed to be the default action, but just to be sure add it
>> explicitly.
> 
> I added the above line and browser could not open any page, this was
> the notice shown: https://ibb.co/HVQYD2c
> cache.log: https://justpaste.it/38eyl
> access.log: https://paste.debian.net/1152817/
> 

The problem is this:

"
2020/06/19 15:11:09.998 kid1| 93,2| Xaction.cc(272)
dieOnConnectionFailure: Adaptation::Icap::OptXact failed to connect to
icap://127.0.0.1:1344/srv_url_check

2020/06/19 15:11:09.998 kid1| essential ICAP service is down after an
options fetch failure: icap://127.0.0.1:1344/srv_url_check [down,!opt]
"

Next step is to debug why the OPTIONS request to the ICAP service causes
it to break.


Amos


From squid3 at treenet.co.nz  Fri Jun 19 12:16:17 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2020 00:16:17 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:5 Denial of
	Service when	using SMP cache
Message-ID: <0893e66d-fb17-bba8-7d3f-633d510fc34d@treenet.co.nz>


__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2020:5
__________________________________________________________________

 Advisory ID:       | SQUID-2020:5
 Date:              | June 19, 2020
 Summary:           | Denial of Service when using SMP cache
 Affected versions: | 5.0.1 -> 5.0.2
 Fixed in version:  | 5.0.3
__________________________________________________________________

  <http://www.squid-cache.org/Advisories/SQUID-2020_5.txt>
  <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14059>

<https://github.com/squid-cache/squid/security/advisories/GHSA-w7pw-2m4p-58hr>
__________________________________________________________________

Problem Description:

 Due to an Incorrect Synchronization, Squid is vulnerable to a
 Denial of Service attack when processing objects in an SMP cache.

__________________________________________________________________

Severity:

 This problem may allow a remote client to trigger a Squid worker
 assertion.

 This attack is limited to SMP Squids using shared memory cache
 and/or an SMP rock disk cache.

 CVSS Score of 7.3
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:H/PR:L/UI:N/S:C/C:N/I:N/A:H/E:U/RL:O/RC:C/CR:X/IR:X/AR:H/MAV:N/MAC:H/MPR:L/MUI:N/MS:C/MC:N/MI:X/MA:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed in Squid version 5.0.3.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 5:
 <http://www.squid-cache.org/Versions/v5/changesets/squid-5-7a5af8db8e0377c06ed9ffbdcb1334389c7cd8ab.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-3.x up to and including 3.5.28 are not vulnerable.

 All Squid-4.x up to and including 4.12 are not vulnerable.

 Squid-5.0.1 and 5.0.2 not using SMP caching are not vulnerable.

 Squid-5.0.1 and 5.0.2 using SMP caching are vulnerable.

__________________________________________________________________

Workarounds:

Disable all SMP caching. For example:

 * If using multiple Squid workers, set 'cache_mem' to '0' and
   remove all `cache_dir` directives from squid.conf.

 * If using a single Squid worker, remove all 'cache_dir rock'
   directives from squid.conf.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the <squid-users at lists.squid-cache.org> mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Jack Zar of Bloomberg.

 Fixed by Alex Rousskov of The Measurement Factory.

__________________________________________________________________

Revision history:

 2020-03-02 16:45:44 UTC Initial Report
 2020-05-13 06:37:31 UTC Patch Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Jun 19 12:16:25 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2020 00:16:25 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:6 Denial of
	Service issue in	TLS Handshake
Message-ID: <762ee5f4-ef01-9f7b-d8f8-1a5b0c259a69@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2020:6
__________________________________________________________________

 Advisory ID:       | SQUID-2020:6
 Date:              | June 19, 2020
 Summary:           | Denial of Service issue in TLS Handshake
 Affected versions: | Squid 3.1 -> 3.5.28
                    | Squid 4.x -> 4.11
                    | Squid 5.x -> 5.0.2
 Fixed in version:  | Squid 4.12 and 5.0.3

__________________________________________________________________

  <http://www.squid-cache.org/Advisories/SQUID-2020_6.txt>
  <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14058>

<https://github.com/squid-cache/squid/security/advisories/GHSA-qvf6-485q-vm57>
__________________________________________________________________

Problem Description:

 Due to use of a potentially dangerous function Squid and the
 default certificate validation helper are vulnerable to a Denial
 of Service attack when processing TLS certificates.

__________________________________________________________________

Severity:

 This problem allows a trusted client to perform Denial of Service
 when opening TLS connections with a server for HTTPS.

 This problem allows a trusted client to perform Denial of Service
 when opening TLS connections to a server for SSL-Bump intercepted
 transactions.

 This attack is limited to Squid built with OpenSSL features and
 opening peer or server connections for HTTPS traffic and SSL-Bump
 server handshakes.

 CVSS Score of 8.3
 <https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:H/E:F/RL:O/RC:C/CR:X/IR:X/AR:H/MAV:N/MAC:H/MPR:N/MUI:X/MS:C/MC:N/MI:N/MA:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid versions 4.12 and 5.0.3.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-93f5fda134a2a010b84ffedbe833d670e63ba4be.patch>

Squid 5:
 <http://www.squid-cache.org/Versions/v5/changesets/squid-5-c6d1a4f6a2cbebceebc8a3fcd8f539ceb7b7f723.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x up to and including 2.7.STABLE9 are not vulnerable.

 All Squid-3.x up to and including 3.4.14 built without
 --enable-openssl are not vulnerable.

 All Squid-3.x up to and including 3.4.14 built with
 --disable-openssl are not vulnerable.

 All Squid-3.x up to and including 3.4.14 built with
 --enable-openssl are vulnerable.

 All Squid-3.5 up to and including 3.5.28 built without
 --with-openssl are not vulnerable.

 All Squid-3.5 up to and including 3.5.28 built with
 --without-openssl are not vulnerable.

 All Squid-3.5 up to and including 3.5.28 built with
 --with-openssl are vulnerable.

 All Squid-4.x up to and including 4.11 built without
 --with-openssl are not vulnerable.

 All Squid-4.x up to and including 4.11 built with
 --without-openssl are not vulnerable.

 All Squid-4.x up to and including 4.11 built with --with-openssl
 are vulnerable.

 Squid-5.0.1 and 5.0.2 built without --with-openssl are not
 vulnerable.

 Squid-5.0.1 and 5.0.2 built with --without-openssl are not
 vulnerable.

 Squid-5.0.1 and 5.0.2 built with --with-openssl are vulnerable.

__________________________________________________________________

Workaround:

 * For interception proxies using SSL-Bump functionality there is
   no workaround.

 * For reverse-proxy the GnuTLS support available in Squid-4 may
   provide sufficient functionality. The vulnerable certificate
   validator helper feature is not supported yet by GnuTLS builds.

 * Other installations just needing to relay HTTPS do not need
   OpenSSL support and should be able to run a build that does not
   have --with-openssl.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the <squid-users at lists.squid-cache.org> mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Dimitra Azariadi and Mario
 Galli of Open Systems AG.

 Fixed by Christos Tsantilas of The Measurement Factory.

__________________________________________________________________

Revision history:

 2020-03-04 09:31:23 UTC Initial Report
 2020-05-15 04:54:54 UTC Patches Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Jun 19 12:16:44 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2020 00:16:44 +1200
Subject: [squid-users] [squid-announce] Squid 4.12 is available
Message-ID: <0d39952a-e93d-5960-00ac-d11547dc9d80@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.12 release!


This release is a security release resolving several issues found in
the prior Squid releases.


The major changes to be aware of:

 * SQUID-2020:5 Denial of Service when using SMP cache
   (CVE-2020-14059)

This problem may allow a remote client to trigger a Squid worker
assertion.

This attack is limited to SMP Squids using shared memory cache
and/or an SMP rock disk cache.


See the advisory for patches:
 <http://www.squid-cache.org/Advisories/SQUID-2020_5.txt>


 * SQUID-2020:6 Denial of Service issue in TLS handshake
   (CVE-2020-14058)

This problem allows a trusted client to perform Denial of Service
when opening TLS connections with a server for HTTPS.

This problem allows a trusted client to perform Denial of Service
when opening TLS connections to a server for SSL-Bump intercepted
transactions.

This attack is limited to Squid built with OpenSSL features and
opening peer or server connections for HTTPS traffic and SSL-Bump
server handshakes.

See the advisory for patches:
 <http://www.squid-cache.org/Advisories/SQUID-2020_6.txt>


 * Bug 5041: Missing Debug::Extra breaks build on hosts with systemd

A regression was introduced with the fix for bug 5016 in Squid-4.11.
Which shows up as build errors when libsystemd dependency is
added to enable the systemd notify feature explicitly. This release
fixes the regression and actually enables the feature.


 * Bug 5030: Negative responses are never cached

This bug shows up as cacheable 4xx and 5xx responses not being
cached despite negative_ttl configuration. This release brings
4xx and 5xx responses inline with the expected caching behaviour.


 * SslBump: Disable OpenSSL TLSv1.3 support for older TLS traffic

Squid SSL-Bump features do not support TLS/1.3 protocol.
Previously client or server attempting to use TLS/1.3 would
result in "inappropriate fallback" errors negotiating handshakes.

This release explicitly detects use of TLS/1.3 and disables it.



  All users of Squid are urged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Fri Jun 19 12:16:53 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2020 00:16:53 +1200
Subject: [squid-users] [squid-announce] Squid 5.0.3 beta is available
Message-ID: <ca033185-c2df-2139-ee9f-efff0e601d37@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-5.0.3 beta release!


This release is a security and feature update release resolving
several issues found in the prior Squid releases.


The major changes to be aware of:

 * SQUID-2020:5 Denial of Service when using SMP cache
   (CVE-2020-14059)

This problem may allow a remote client to trigger a Squid worker
assertion.

This attack is limited to SMP Squids using shared memory cache
and/or an SMP rock disk cache.


See the advisory for patches:
 <http://www.squid-cache.org/Advisories/SQUID-2020_5.txt>


 * SQUID-2020:6 Denial of Service issue in TLS handshake
   (CVE-2020-14058)

This problem allows a trusted client to perform Denial of Service
when opening TLS connections with a server for HTTPS.

This problem allows a trusted client to perform Denial of Service
when opening TLS connections to a server for SSL-Bump intercepted
transactions.

This attack is limited to Squid built with OpenSSL features and
opening peer or server connections for HTTPS traffic and SSL-Bump
server handshakes.

See the advisory for patches:
 <http://www.squid-cache.org/Advisories/SQUID-2020_6.txt>


 * Happy Eyeballs: Do not discard viable reforwarding destinations

When Happ Eyeballs starts opening two connections, both destinations
are removed from the destinations list. As soon as one connection
(X) succeeded, the other destination (Y) was essentially forgotten. If
Squid, after using X, decided to reforward the request, then the request
was never reforwarded to Y. We now return Y to the list of possible
destinations.



  All users of Squid-5 are urged to upgrade as soon as possible.

  All users of Squid-4 and older are encouraged to plan for upgrade.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v5/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From mikio.kishi at gmail.com  Sat Jun 20 06:14:04 2020
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Sat, 20 Jun 2020 15:14:04 +0900
Subject: [squid-users] [squid-announce] Squid 5.0.3 beta is available
In-Reply-To: <ca033185-c2df-2139-ee9f-efff0e601d37@treenet.co.nz>
References: <ca033185-c2df-2139-ee9f-efff0e601d37@treenet.co.nz>
Message-ID: <CAMUMefZ4hgPg0M_bRXhZrH3+DgjCBM8BuxysWKDGmO8ABwqz+w@mail.gmail.com>

Hi, Amos

Do you have any plans to release the official stable version for squid5 ?
I'm strongly interested in squid5 features. So, if you have any schedules,
please let me know.

Regards,
--
Mikio Kishi

On Fri, Jun 19, 2020 at 9:41 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-5.0.3 beta release!
>
>
> This release is a security and feature update release resolving
> several issues found in the prior Squid releases.
>
>
> The major changes to be aware of:
>
>  * SQUID-2020:5 Denial of Service when using SMP cache
>    (CVE-2020-14059)
>
> This problem may allow a remote client to trigger a Squid worker
> assertion.
>
> This attack is limited to SMP Squids using shared memory cache
> and/or an SMP rock disk cache.
>
>
> See the advisory for patches:
>  <http://www.squid-cache.org/Advisories/SQUID-2020_5.txt>
>
>
>  * SQUID-2020:6 Denial of Service issue in TLS handshake
>    (CVE-2020-14058)
>
> This problem allows a trusted client to perform Denial of Service
> when opening TLS connections with a server for HTTPS.
>
> This problem allows a trusted client to perform Denial of Service
> when opening TLS connections to a server for SSL-Bump intercepted
> transactions.
>
> This attack is limited to Squid built with OpenSSL features and
> opening peer or server connections for HTTPS traffic and SSL-Bump
> server handshakes.
>
> See the advisory for patches:
>  <http://www.squid-cache.org/Advisories/SQUID-2020_6.txt>
>
>
>  * Happy Eyeballs: Do not discard viable reforwarding destinations
>
> When Happ Eyeballs starts opening two connections, both destinations
> are removed from the destinations list. As soon as one connection
> (X) succeeded, the other destination (Y) was essentially forgotten. If
> Squid, after using X, decided to reforward the request, then the request
> was never reforwarded to Y. We now return Y to the list of possible
> destinations.
>
>
>
>   All users of Squid-5 are urged to upgrade as soon as possible.
>
>   All users of Squid-4 and older are encouraged to plan for upgrade.
>
>
> See the ChangeLog for the full list of changes in this and earlier
> releases.
>
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
> when you are ready to make the switch to Squid-5
>
> This new release can be downloaded from our HTTP or FTP servers
>
>   http://www.squid-cache.org/Versions/v5/
>   ftp://ftp.squid-cache.org/pub/squid/
>   ftp://ftp.squid-cache.org/pub/archive/5/
>
> or the mirrors. For a list of mirror sites see
>
>   http://www.squid-cache.org/Download/http-mirrors.html
>   http://www.squid-cache.org/Download/mirrors.html
>
> If you encounter any issues with this release please file a bug report.
>   http://bugs.squid-cache.org/
>
>
> Amos Jeffries
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200620/f6bc875d/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 20 07:27:43 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Jun 2020 19:27:43 +1200
Subject: [squid-users] [squid-announce] Squid 5.0.3 beta is available
In-Reply-To: <CAMUMefZ4hgPg0M_bRXhZrH3+DgjCBM8BuxysWKDGmO8ABwqz+w@mail.gmail.com>
References: <ca033185-c2df-2139-ee9f-efff0e601d37@treenet.co.nz>
 <CAMUMefZ4hgPg0M_bRXhZrH3+DgjCBM8BuxysWKDGmO8ABwqz+w@mail.gmail.com>
Message-ID: <2dd9a898-1d06-84dd-d096-501251e2164a@treenet.co.nz>

On 20/06/20 6:14 pm, mikio.kishi wrote:
> Hi, Amos
> 
> Do you have any plans to?release the official stable version for squid5 ?
> I'm strongly interested in squid5 features. So, if you have any
> schedules, please let me know.
> 


The release process is documented at
<https://wiki.squid-cache.org/ReleaseProcess>

"stable" happens when I (as maintainer) am confident that there are no
fixable major bugs in the new code.


You do not have to wait for that status to start using any Squid
version. Squid-5.0.1 came out when the code was determined to be usable
for many installations. We just recommend that you test beta releases
well before use.


Amos


From ngtech1ltd at gmail.com  Sat Jun 20 19:45:36 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 20 Jun 2020 22:45:36 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB2756B10588909C4F4635B23A93980@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <22186F7E-2BC4-44CD-940A-02072A2D92E6@hxcore.ol>,
 <AM5PR0102MB2756B10588909C4F4635B23A93980@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <B2FC1ADC-2403-4FD8-936C-C830DD22AFB3@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200620/ed7006b6/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 36BDBBCA07C54ECE935D7760AE4E990A.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200620/ed7006b6/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200620/ed7006b6/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200620/ed7006b6/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200620/ed7006b6/attachment-0002.png>

From monika.avalur at gmail.com  Sun Jun 21 10:27:21 2020
From: monika.avalur at gmail.com (Monika Avalur)
Date: Sun, 21 Jun 2020 15:57:21 +0530
Subject: [squid-users] Fwd: HTTP X-FORWARDED HEADER
In-Reply-To: <CA+vZrw=S-mC6GspRqTELk_W2RxYi=CB=vMW=B0tANryCMG6gxA@mail.gmail.com>
References: <CA+vZrw=S-mC6GspRqTELk_W2RxYi=CB=vMW=B0tANryCMG6gxA@mail.gmail.com>
Message-ID: <CA+vZrw=9K5G+fLRwqW4S9YxPA-TMapgA0UVfxZUmm99EBQP6KA@mail.gmail.com>

Hello,

I am using squid proxy to test some application in my company.

I have a use cases where I need to use the X-Forwarded-For header from
squid proxy

I tried by editing the squid configuration file and including

acl localhost src 127.0.0.1

 forwarded_for on
follow_x_forwarded_for allow localhost

But still I am unable to see the HTTP header in chrome.

While I looked up the documentation, it said requires
--enable-follow-x-forwarded-for.

Can some one tell me how can I enable it?

It's sort of urgent.

Thanks & Best Regards,
Monika
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200621/eb7fa2ac/attachment.htm>

From squid3 at treenet.co.nz  Sun Jun 21 10:40:26 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jun 2020 22:40:26 +1200
Subject: [squid-users] Fwd: HTTP X-FORWARDED HEADER
In-Reply-To: <CA+vZrw=9K5G+fLRwqW4S9YxPA-TMapgA0UVfxZUmm99EBQP6KA@mail.gmail.com>
References: <CA+vZrw=S-mC6GspRqTELk_W2RxYi=CB=vMW=B0tANryCMG6gxA@mail.gmail.com>
 <CA+vZrw=9K5G+fLRwqW4S9YxPA-TMapgA0UVfxZUmm99EBQP6KA@mail.gmail.com>
Message-ID: <e04860cc-a3d4-6508-6e73-24505072c63a@treenet.co.nz>

On 21/06/20 10:27 pm, Monika Avalur wrote:
> Hello,
> 
> I am using squid proxy to test some application in my company.
> 
> I have a use cases where I need to use the X-Forwarded-For header from
> squid proxy?
> 
> I tried by editing the squid configuration file and including
> 
> acl localhost src 127.0.0.1
> 
> ?forwarded_for on 

Enables the header to be sent to servers. Only relevant on intercepted
traffic.


> follow_x_forwarded_for allow localhost
> 

Processes headers *received* from client.


> But still I am unable to see the HTTP header in chrome.

This is a header sent to *servers*. Browser will never see it.

> 
> While I looked up the documentation, it said requires
> --enable-follow-x-forwarded-for.
> 
> Can some one tell me how can I enable it?> It's sort of urgent.
> 

What use exactly are you needing to use this header for?


Amos


From squid3 at treenet.co.nz  Sun Jun 21 11:08:43 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jun 2020 23:08:43 +1200
Subject: [squid-users] Fwd: HTTP X-FORWARDED HEADER
In-Reply-To: <e04860cc-a3d4-6508-6e73-24505072c63a@treenet.co.nz>
References: <CA+vZrw=S-mC6GspRqTELk_W2RxYi=CB=vMW=B0tANryCMG6gxA@mail.gmail.com>
 <CA+vZrw=9K5G+fLRwqW4S9YxPA-TMapgA0UVfxZUmm99EBQP6KA@mail.gmail.com>
 <e04860cc-a3d4-6508-6e73-24505072c63a@treenet.co.nz>
Message-ID: <ec580a56-2fee-bb32-c457-5be0b314fd65@treenet.co.nz>

On 21/06/20 10:40 pm, Amos Jeffries wrote:
> On 21/06/20 10:27 pm, Monika Avalur wrote:
>> Hello,
>>
>> I am using squid proxy to test some application in my company.
>>
>> I have a use cases where I need to use the X-Forwarded-For header from
>> squid proxy?
>>
>> I tried by editing the squid configuration file and including
>>
>> acl localhost src 127.0.0.1
>>
>> ?forwarded_for on 
> 
> Enables the header to be sent to servers. Only relevant on intercepted
> traffic.

Sorry *indirect* traffic.


Amos


From monika.avalur at gmail.com  Sun Jun 21 13:24:28 2020
From: monika.avalur at gmail.com (Monika Avalur)
Date: Sun, 21 Jun 2020 18:54:28 +0530
Subject: [squid-users] squid-users Digest, Vol 70, Issue 27
In-Reply-To: <mailman.3.1592740801.11532.squid-users@lists.squid-cache.org>
References: <mailman.3.1592740801.11532.squid-users@lists.squid-cache.org>
Message-ID: <CA+vZrwkKbObLdfYj6M4G+k+Adoh_=i+U=s0iLH8O4wa3H27MBQ@mail.gmail.com>

Hello Amos,

Thank you so much for your reply.

So my use case is, in my application I have a switch which when turned ON,
reads the client IP address from the header I configured in the application
(i.e. X-Forwarded-For in Squid) and based on it some rules are triggered.

If the switch is turned off, it uses proxy IP and based on it some other
set of rules are triggered.

I want to establish this scenario using Squid proxy. But not sure whether
Squid is sending the header.

Thanks,
Monika



On Sun, Jun 21, 2020 at 5:30 PM <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Fwd: HTTP X-FORWARDED HEADER (Monika Avalur)
>    2. Re: Fwd: HTTP X-FORWARDED HEADER (Amos Jeffries)
>    3. Re: Fwd: HTTP X-FORWARDED HEADER (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sun, 21 Jun 2020 15:57:21 +0530
> From: Monika Avalur <monika.avalur at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Fwd: HTTP X-FORWARDED HEADER
> Message-ID:
>         <CA+vZrw=
> 9K5G+fLRwqW4S9YxPA-TMapgA0UVfxZUmm99EBQP6KA at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hello,
>
> I am using squid proxy to test some application in my company.
>
> I have a use cases where I need to use the X-Forwarded-For header from
> squid proxy
>
> I tried by editing the squid configuration file and including
>
> acl localhost src 127.0.0.1
>
>  forwarded_for on
> follow_x_forwarded_for allow localhost
>
> But still I am unable to see the HTTP header in chrome.
>
> While I looked up the documentation, it said requires
> --enable-follow-x-forwarded-for.
>
> Can some one tell me how can I enable it?
>
> It's sort of urgent.
>
> Thanks & Best Regards,
> Monika
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20200621/eb7fa2ac/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Sun, 21 Jun 2020 22:40:26 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Fwd: HTTP X-FORWARDED HEADER
> Message-ID: <e04860cc-a3d4-6508-6e73-24505072c63a at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 21/06/20 10:27 pm, Monika Avalur wrote:
> > Hello,
> >
> > I am using squid proxy to test some application in my company.
> >
> > I have a use cases where I need to use the X-Forwarded-For header from
> > squid proxy
> >
> > I tried by editing the squid configuration file and including
> >
> > acl localhost src 127.0.0.1
> >
> >  forwarded_for on
>
> Enables the header to be sent to servers. Only relevant on intercepted
> traffic.
>
>
> > follow_x_forwarded_for allow localhost
> >
>
> Processes headers *received* from client.
>
>
> > But still I am unable to see the HTTP header in chrome.
>
> This is a header sent to *servers*. Browser will never see it.
>
> >
> > While I looked up the documentation, it said requires
> > --enable-follow-x-forwarded-for.
> >
> > Can some one tell me how can I enable it?> It's sort of urgent.
> >
>
> What use exactly are you needing to use this header for?
>
>
> Amos
>
>
> ------------------------------
>
> Message: 3
> Date: Sun, 21 Jun 2020 23:08:43 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Fwd: HTTP X-FORWARDED HEADER
> Message-ID: <ec580a56-2fee-bb32-c457-5be0b314fd65 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 21/06/20 10:40 pm, Amos Jeffries wrote:
> > On 21/06/20 10:27 pm, Monika Avalur wrote:
> >> Hello,
> >>
> >> I am using squid proxy to test some application in my company.
> >>
> >> I have a use cases where I need to use the X-Forwarded-For header from
> >> squid proxy
> >>
> >> I tried by editing the squid configuration file and including
> >>
> >> acl localhost src 127.0.0.1
> >>
> >>  forwarded_for on
> >
> > Enables the header to be sent to servers. Only relevant on intercepted
> > traffic.
>
> Sorry *indirect* traffic.
>
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 70, Issue 27
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200621/c87a823a/attachment.htm>

From squid3 at treenet.co.nz  Sun Jun 21 13:42:48 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2020 01:42:48 +1200
Subject: [squid-users] squid-users Digest, Vol 70, Issue 27
In-Reply-To: <CA+vZrwkKbObLdfYj6M4G+k+Adoh_=i+U=s0iLH8O4wa3H27MBQ@mail.gmail.com>
References: <mailman.3.1592740801.11532.squid-users@lists.squid-cache.org>
 <CA+vZrwkKbObLdfYj6M4G+k+Adoh_=i+U=s0iLH8O4wa3H27MBQ@mail.gmail.com>
Message-ID: <c5aafbfe-116e-3622-618a-b1ddaeb32177@treenet.co.nz>

On 22/06/20 1:24 am, Monika Avalur wrote:
> Hello Amos,
> 
> Thank you so much for your reply.
> 
> So my use case is, in my application I have a switch which when turned
> ON, reads the client IP address from the header I configured in the
> application (i.e. X-Forwarded-For in Squid) and based on it some rules
> are triggered.
> 

You do not have to configure anything to *send* the XFF header. That is
the default behaviour. Config is only needed to receive it from clients,
and to manipulate it.


> If the switch is turned off, it uses proxy IP and based on it some other
> set of rules are triggered.
> 
> I want to establish this scenario using Squid proxy. But not sure
> whether Squid is sending the header.

To see what HTTP headers Squid is sending you can configure
"debug_options 11,2" and look for "HTTP server REQUEST" in cache.log



You may want to look into the standard "Forwarded" header for a better
long-term solution. <https://tools.ietf.org/html/rfc7239>

Currently that will need a bit of configuration for Squid to send, but
easily done:
  request_header_add Forwarded "for=%>a" all


Amos


From ngtech1ltd at gmail.com  Mon Jun 22 05:14:43 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 22 Jun 2020 08:14:43 +0300
Subject: [squid-users] Error: (71) Protocol error (TLS code:
	SQUID_ERR_SSL_HANDSHAKE)
Message-ID: <FCFD9211-D740-447B-B7D3-035E8A4D0684@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200622/8f55a5af/attachment.htm>

From Loucansky.Lukas at kjj.cz  Mon Jun 22 06:10:24 2020
From: Loucansky.Lukas at kjj.cz (=?iso-8859-2?B?TG916GFuc2v9IEx1a+G5?=)
Date: Mon, 22 Jun 2020 08:10:24 +0200
Subject: [squid-users] Error: (71) Protocol error (TLS
	code:SQUID_ERR_SSL_HANDSHAKE)
In-Reply-To: <FCFD9211-D740-447B-B7D3-035E8A4D0684@hxcore.ol>
References: <FCFD9211-D740-447B-B7D3-035E8A4D0684@hxcore.ol>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B530108933F@kjj-server.KJJ.local>

Sorry -  but how is your solution different from:
1) openssl dhparam -outform PEM -out dhparam.pem 2048
2) https_port 3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA.crt key=/usr/local/squid/etc/rootCA.key options=SINGLE_DH_USE,SINGLE_ECDH_USE tls-dh=/usr/local/squid/etc/dhparam.pem

Or tls-dh=prime256v1:/usr/local/squid/etc/dhparam.pem

?

LL

> I have tested 4.12 and with default settings I am getting an error on some local common web pages.
> 
>  
> 
> (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> Handshake with SSL server failed: error:141A318A:SSL routines:tls_process_ske_dhe:dh key too small

 




From m992493 at gmail.com  Mon Jun 22 07:37:47 2020
From: m992493 at gmail.com (Amiq Nahas)
Date: Mon, 22 Jun 2020 13:07:47 +0530
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <834e052d-a35c-3110-ddc3-644881b5678e@treenet.co.nz>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
 <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
 <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>
 <b0c68567-9705-aa17-c828-40a799152880@treenet.co.nz>
 <CAPicJaF=r2Xpp-hN-kTnVUu8BFFZHc-eHksHKgaNgeC_+J1SUQ@mail.gmail.com>
 <834e052d-a35c-3110-ddc3-644881b5678e@treenet.co.nz>
Message-ID: <CAPicJaG5S-iufbC=XouRuV-+awS1r4S7u94e_QuJmsUZjnhshg@mail.gmail.com>

On Fri, Jun 19, 2020 at 4:20 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 19/06/20 10:07 pm, Amiq Nahas wrote:
> >> Looks like traffic is fine and Squid operational, but no sign of any
> >> ICAP activity. I think try adding this to your config:
> >>
> >>   adaptation_access svcBlocker allow all
> >>
> >> Its supposed to be the default action, but just to be sure add it
> >> explicitly.
> >
> > I added the above line and browser could not open any page, this was
> > the notice shown: https://ibb.co/HVQYD2c
> > cache.log: https://justpaste.it/38eyl
> > access.log: https://paste.debian.net/1152817/
> >
>
> The problem is this:
>
> "
> 2020/06/19 15:11:09.998 kid1| 93,2| Xaction.cc(272)
> dieOnConnectionFailure: Adaptation::Icap::OptXact failed to connect to
> icap://127.0.0.1:1344/srv_url_check
>
> 2020/06/19 15:11:09.998 kid1| essential ICAP service is down after an
> options fetch failure: icap://127.0.0.1:1344/srv_url_check [down,!opt]
> "
>
> Next step is to debug why the OPTIONS request to the ICAP service causes
> it to break.

Problem solved. The name of service was wrong.
Service name should be icap://127.0.0.1:1344/url_check instead of
icap://127.0.0.1:1344/srv_url_check.
Thanks for helping in narrowing the problem.


From squid3 at treenet.co.nz  Mon Jun 22 09:10:06 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2020 21:10:06 +1200
Subject: [squid-users] Error: (71) Protocol error (TLS code:
 SQUID_ERR_SSL_HANDSHAKE)
In-Reply-To: <FCFD9211-D740-447B-B7D3-035E8A4D0684@hxcore.ol>
References: <FCFD9211-D740-447B-B7D3-035E8A4D0684@hxcore.ol>
Message-ID: <150ee17c-b037-7087-e0a5-a6c95267b4c6@treenet.co.nz>

On 22/06/20 5:14 pm, Eliezer Croitoru wrote:
> I have tested 4.12 and with default settings I am getting an error on
> some local common web pages.
> 
> 
> (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> Handshake with SSL server failed: error:141A318A:SSL
> routines:tls_process_ske_dhe:dh key too small
...
> 
> But yet I am still confused about the subject.
> 
> Can anyone simplify this specific issue for me?
> 

Just like any other key-pair encryption DHE depends on a secret. Over
time short secrets become easy for attackers to discover.

You may be more familiar with the RSA 1024->2048->4096 migrations. The
same thing is going on here but for the DHE key bit-size.


IIRC, minimum these days for DHE is 1024-bit with 2048-bit secrets being
preferred. Anything under 2048 the clients may warn, under 1024 they are
expected to reject with the above error.

For public domains you should be able to use the QualSys SSL Labs tests
to check a problematic site and see some explanation of the details.

Amos


From squid3 at treenet.co.nz  Mon Jun 22 09:24:25 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jun 2020 21:24:25 +1200
Subject: [squid-users] Squid and c-icap's srv_url_check module
In-Reply-To: <CAPicJaG5S-iufbC=XouRuV-+awS1r4S7u94e_QuJmsUZjnhshg@mail.gmail.com>
References: <CAPicJaGYAY10eeVzhjwTmDqtD2Oc1-wcWhFEkGcUvTjSnD=gNw@mail.gmail.com>
 <494c5685-cf40-1d6a-cd8a-185bf99328c4@treenet.co.nz>
 <CAPicJaHAC3K-zb=rqXTdci8_4mxG7ReWXnU8y859GMAO_1RebQ@mail.gmail.com>
 <d40b9e00-a56a-9057-bc7b-6f6be1c1dd44@treenet.co.nz>
 <CAPicJaFqgM1G-rFHg3R06hq6PJPUoAY7DSiUBaYB90Zqa9NTyA@mail.gmail.com>
 <b0c68567-9705-aa17-c828-40a799152880@treenet.co.nz>
 <CAPicJaF=r2Xpp-hN-kTnVUu8BFFZHc-eHksHKgaNgeC_+J1SUQ@mail.gmail.com>
 <834e052d-a35c-3110-ddc3-644881b5678e@treenet.co.nz>
 <CAPicJaG5S-iufbC=XouRuV-+awS1r4S7u94e_QuJmsUZjnhshg@mail.gmail.com>
Message-ID: <1e381786-88a6-8ec2-41ed-9c47f67589b1@treenet.co.nz>

On 22/06/20 7:37 pm, Amiq Nahas wrote:
> 
> Problem solved. The name of service was wrong.
> Service name should be icap://127.0.0.1:1344/url_check instead of
> icap://127.0.0.1:1344/srv_url_check.
> Thanks for helping in narrowing the problem.
> 

Welcome, thats what we are here for. Sorry for not being familiar enough
with c-icap to pick up that much faster.


FWIW; I have sent a note to Christos the c-icap author about the wiki
docs needing to specify this detail clearly on the page you were working
from.


Amos


From Mate.Javor at walmart.com  Mon Jun 22 12:25:38 2020
From: Mate.Javor at walmart.com (Mate Javor)
Date: Mon, 22 Jun 2020 12:25:38 +0000
Subject: [squid-users] Upload throttling
Message-ID: <1501740C-7408-4C96-9162-F9E415207404@homeoffice.wal-mart.com>

Hi squid-users,
We?re trying to configure squid upload throttling using client_delay_pools . I was reading around the topic and some says that client_delay_pools is not a solution for throttling uploads. Can you pls help me here?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200622/4f0da029/attachment.htm>

From tannmann at gmail.com  Mon Jun 22 14:50:41 2020
From: tannmann at gmail.com (Tanner)
Date: Mon, 22 Jun 2020 08:50:41 -0600
Subject: [squid-users] transparent proxy upgrade 3.5 to 4.12,
 Error parsing SSL Server Hello Message on FD XX
Message-ID: <CAPAcTN2antoDrUWss9Mh_LBuEW4xxvwx+tEJcOVamM0wJnc_9Q@mail.gmail.com>

I have squid set up as a transparent outbound proxy using version 3.5. When
upgrading to 4.12, I am seeing an error "Error parsing SSL Server Hello
Message on FD XX" that did not happen before. Here is my config:

http_port 3129 intercept
cache_effective_user squid
cache_effective_group squid
workers 1
acl CONNECT method CONNECT
acl allowed_http_sites dstdom_regex "/etc/squid/outbound_whitelist.txt"
http_access allow allowed_http_sites
acl allowed_networks src 10.0.0.0/8
acl allowed_networks src 172.0.0.0/8
https_port 3130 intercept ssl-bump cert=/etc/squid/ssl/squid.pem
acl SSL_port port 443
http_access allow SSL_port
acl allowed_https_sites ssl::server_name_regex
"/etc/squid/outbound_whitelist.txt"
acl step3 at_step SslBump3
ssl_bump peek all
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate all
cache deny all
http_access deny all
shutdown_lifetime 0
pid_filename /var/run/squid.pid
log_mime_hdrs on
logfile_rotate 2
access_log stdio:/dev/stdout
cache_log stdio:/dev/stderr

Previous to 4.12, if I tried to upgrade to any v4 or v5 of squid, I would
get an issue with "inappropriate fallback" when going to some sites
supporting TLS 1.3 (but not all). This appears to have been resolved, but
this "Error parsing SSL Server Hello Message" is new. Is there something
that should change in my config? Can anyone tell me what this error means?

Thanks,

Tanner
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200622/77339abf/attachment.htm>

From joshuakronemeyer at gmail.com  Mon Jun 22 21:14:54 2020
From: joshuakronemeyer at gmail.com (Joshua Bazgrim)
Date: Mon, 22 Jun 2020 14:14:54 -0700
Subject: [squid-users] Squid is using ipv4 for non-ssl connections
Message-ID: <CAF1CunQycvC5ZrX0hJ6OJ0shmDcfaH_72UhCWXMRMTdwZYGrTQ@mail.gmail.com>

Hi there,

I'm still fairly new to squid. Hoping someone can help me out.
I have a tcp_outgoing_address for ipv6 that routes to nftables to give me a
rotating ipv6 address.

However, on non-ssl calls, it uses ipv4 instead.
Is there a way to have squid use ipv6 for non-ssl calls?

curl -L -x PROXYIP:3128 http://api6.ipify.org # This returns an ipv4
address through squid
curl -L -x PROXYIP:3128 https://api6.ipify.org   # This returns an ipv6
address through squid

If I don't use the squid proxy, it properly returns an ipv6 for non-ssl
connections.

squid 4.9
The squid.conf file is default besides tcp_outgoing_address

Any ideas?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200622/57763b59/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 23 07:17:15 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Jun 2020 19:17:15 +1200
Subject: [squid-users] Squid is using ipv4 for non-ssl connections
In-Reply-To: <CAF1CunQycvC5ZrX0hJ6OJ0shmDcfaH_72UhCWXMRMTdwZYGrTQ@mail.gmail.com>
References: <CAF1CunQycvC5ZrX0hJ6OJ0shmDcfaH_72UhCWXMRMTdwZYGrTQ@mail.gmail.com>
Message-ID: <0c11b678-eeec-19a4-4b7c-68ab93e98de2@treenet.co.nz>

On 23/06/20 9:14 am, Joshua Bazgrim wrote:
> Hi there,
> 
> I'm still fairly new to squid. Hoping someone can help me out.
> I have a tcp_outgoing_address for ipv6 that routes to nftables to give
> me a rotating ipv6 address.
> 

Hint: you cannot talk to IPv4 servers using IPv6 address.


> However, on non-ssl calls, it uses ipv4 instead.
> Is there a way to have squid use ipv6 for non-ssl calls?
> 
> curl -L -x PROXYIP:3128 http://api6.ipify.org # This returns an ipv4
> address through squid
> curl -L -x PROXYIP:3128 https://api6.ipify.org? ?# This returns an ipv6
> address through squid
> 
> If I don't use the squid proxy, it properly returns an ipv6 for non-ssl
> connections.
> 
> squid 4.9
> The squid.conf file is default besides tcp_outgoing_address
> 
> Any ideas?
> 

Tried investigating yet?
<https://wiki.squid-cache.org/KnowledgeBase/DebugSections>

The domain you mention has a mix of both IPv6 and IPv4 addresses. Squid
should be using whichever it can connect to. There is a small bias
towards IPv6, but no guarantee.


Amos


From ngtech1ltd at gmail.com  Tue Jun 23 08:12:02 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 23 Jun 2020 11:12:02 +0300
Subject: [squid-users] Squid is using ipv4 for non-ssl connections
In-Reply-To: <0c11b678-eeec-19a4-4b7c-68ab93e98de2@treenet.co.nz>
References: <CAF1CunQycvC5ZrX0hJ6OJ0shmDcfaH_72UhCWXMRMTdwZYGrTQ@mail.gmail.com>,
 <0c11b678-eeec-19a4-4b7c-68ab93e98de2@treenet.co.nz>
Message-ID: <4114B290-2128-44C5-910C-DBF8EAAEFA57@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200623/1d3bcef9/attachment.htm>

From mikio.kishi at gmail.com  Tue Jun 23 11:07:12 2020
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Tue, 23 Jun 2020 20:07:12 +0900
Subject: [squid-users] X-Forwaded-for and SSLBump
Message-ID: <CAMUMefa2+BnC6exRCa9x5sVM3w81OdCnP4tFKFX+KW+it8vZVA@mail.gmail.com>

Hi Amos,

Regarding the following bug,
 https://bugs.squid-cache.org/show_bug.cgi?id=5052

I really hope that the bug will be fixed as well. The above mentioned the
attached patch was
still incomplete. However, it's a very useful feature. Could you apply to
the squid main trunk
as an optional feature ?  I would like to use  a forwarded-for header(with
sslbump) between
trusted proxies.

Regards,
--
Mikio Kishi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200623/5502f84b/attachment.htm>

From ml at netfence.it  Tue Jun 23 15:04:56 2020
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 23 Jun 2020 17:04:56 +0200
Subject: [squid-users] (92) Protocol error (TLS code:
	X509_V_ERR_CERT_HAS_EXPIRED)
Message-ID: <7b43530a-7516-22de-ac30-a418612cfac7@netfence.it>

Hello.

Running Squid 4.11 on FreeBSD 11.3 with SSLBump, since a few days, I've 
got several sites (e.g. https://www.kawsaki.it/) failing with:

> The following error was encountered while trying to retrieve the URL: https://www.kawasaki.it/*
> 
>     Failed to establish a secure connection to 54.39.161.167
> 
> The system returned:
> 
>     (92) Protocol error (TLS code: X509_V_ERR_CERT_HAS_EXPIRED)
> 
>     SSL Certificate expired on: May 30 10:48:38 2020 GMT
> 
> This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.



When this happens, in cache.log I see:
> 2020/06/23 15:03:31 kid1| ERROR: negotiating TLS on FD 33: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
> 2020/06/23 15:03:31 kid1| ERROR: negotiating TLS on FD 33: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
> 2020/06/23 15:03:31 kid1| ERROR: negotiating TLS on FD 53: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)



I know an intermediate certificate expired, but a new one should have 
been published.



What I find strange, is that using openssl directly succeeds:

> # openssl s_client -connect www.kawasaki.it:https
> CONNECTED(00000003)
> depth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert Global Root CA
> verify return:1
> depth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert CN RSA CA G1
> verify return:1
> depth=0 C = CN, ST = \E7\A6\8F\E5\BB\BA\E7\9C\81, L = \E5\8E\A6\E9\97\A8\E5\B8\82, O = \E7\BD\91\E5\AE\BF\E7\A7\91\E6\8A\80\E8\82\A1\E4\BB\BD\E6\9C\89\E9\99\90\E5\85\AC\E5\8F\B8\E5\8E\A6\E9\97\A8\E5\88\86\E5\85\AC\E5\8F\B8, OU = IT, CN = webssl.chinanetcenter.com
> verify return:1
> ---
> Certificate chain
>  0 s:/C=CN/ST=\xE7\xA6\x8F\xE5\xBB\xBA\xE7\x9C\x81/L=\xE5\x8E\xA6\xE9\x97\xA8\xE5\xB8\x82/O=\xE7\xBD\x91\xE5\xAE\xBF\xE7\xA7\x91\xE6\x8A\x80\xE8\x82\xA1\xE4\xBB\xBD\xE6\x9C\x89\xE9\x99\x90\xE5\x85\xAC\xE5\x8F\xB8\xE5\x8E\xA6\xE9\x97\xA8\xE5\x88\x86\xE5\x85\xAC\xE5\x8F\xB8/OU=IT/CN=webssl.chinanetcenter.com
>    i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert CN RSA CA G1
>  1 s:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert CN RSA CA G1
>    i:/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert Global Root CA
> ---
> Server certificate
> -----BEGIN CERTIFICATE-----
> MIIWEDCCFPigAwIBAgIQA1RHNwOepXqwyoBuZiYbQTANBgkqhkiG9w0BAQsFADBf
> MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
> d3cuZGlnaWNlcnQuY29tMR4wHAYDVQQDExVEaWdpQ2VydCBDTiBSU0EgQ0EgRzEw
> HhcNMjAwNjE5MDAwMDAwWhcNMjAxMTA5MTIwMDAwWjCBnjELMAkGA1UEBhMCQ04x
> EjAQBgNVBAgMCeemj+W7uuecgTESMBAGA1UEBwwJ5Y6m6Zeo5biCMTYwNAYDVQQK
> DC3nvZHlrr/np5HmioDogqHku73mnInpmZDlhazlj7jljqbpl6jliIblhazlj7gx
> CzAJBgNVBAsTAklUMSIwIAYDVQQDExl3ZWJzc2wuY2hpbmFuZXRjZW50ZXIuY29t
> MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA11rUTXwosZacGiiTO6+o
> Qhm7qZzl8T5fGeNwXsZw/EGtCcySXD8pQ33+IpMdXq8hi5EaBXeHCpUs4UCg4S1S
> WXlfGr3PbP+SwLRiXGGNPOPYywLX8N0SyDy1VOkrMHHDRscbf1x6pSJpSTRkNqXS
> 7+/zFTP26fDpvVlgG3U9VpAf7jpCg+xO2ppCbEyKEd02DGNSzSC0vmBJnsg/vI+j
> E8kpiDLjBXAIl5nSns6rChXgxH9/BO60Vef+R3lA5EMVUp31CzhkvjNrk9pcSVbw
> 6AVKlEU314G5diBe/ju0Vie/rnUHXb9FIIHN8+XiNhLBGK2TrgpYvba7gC+wkvVu
> ZwIDAQABo4IShjCCEoIwHwYDVR0jBBgwFoAU70ULeBWRpbbRc6SSb2NaWdNfPp0w
> HQYDVR0OBBYEFGdOsyhZD+/HmDtCHqpecLdpZvYeMIIPwgYDVR0RBIIPuTCCD7WC
> CiouY2N0di5jb22CCSouMTYzLmNvbYIPKi5jaGluYWxpdmUuY29tgg0qLmNjdHZw
> aWMuY29tggwqLmlzZWV5b28uY26CECouMjAxMGV4cG90di5jb22CCSouY2N0di5j
> boIMKi5zYW1sY3IuY29tggoqLnN5eXguY29tgg8qLnpodW9xdWFwcC5jb22CDSou
> NTA1NDM5OS5jb22CDyouYWl3YW40Mzk5LmNvbYIKKi4zODM5LmNvbYIJKi40Mzk5
> LmNugggqLjU2LmNvbYIJKi5jbnR2LmNugg4qLml3YW40Mzk5LmNvbYIOKi5saXZl
> Y2hpbmEuY26CDyoubGl2ZWNoaW5hLmNvbYIQKi5taXRhZ3Rlbm5pLm5ldIIOKi5v
> dXJkdnNzcy5jb22CDSouZGViZW5jZS5uZXSCDSouMzgzOWFwcC5jb22CDSouZGF5
> amF1eS5uZXSCDSouYm13Z3JvdXAuY26CDCouZm94aWpuLmNvbYIPKi5jaGlkYXJl
> c3MuY29tgg4qLmNvdmluaXlhLmNvbYINKi5pbWc0Mzk5LmNvbYIKKi4xMjM3MS5j
> boIMKi5pcGFuZGEubmV0ggwzMDAwdGVzdC5jb22CCyouaTM4MzkuY29tggsqLmlw
> MTM4LmNvbYIJaXAxMzguY29tggwqLnpoZTgwMC5jb22CDSoudnhpbnlvdS5jb22C
> DCouNDM5OWtlLmNvbYIKKi4zMDAwLmNvbYIQKi40Mzk5eW91cGFpLmNvbYIMKi55
> eGhoZGwuY29tgg0qLjMwMDBhcGkuY29tgg4qLmt1eWlueXVuLmNvbYIOKi5rdXlp
> bjEyMy5jb22CDCouZGl5cmluZy5jY4IOKi4zMDAwdGVzdC5jb22CDCoubWVpcGFp
> LmNvbYISKi5jYW5rYW94aWFveGkuY29tggsqLmNudHZ3Yi5jboIQKi5pYW5ub25l
> a3RtLm5ldIIMKi5pcGFuZGEuY29tggsqLmlwYW5kYS5jboINKi40Mzk5YXBpLm5l
> dIINKi51bmNjb2RvLmNvbYIPKi5tZWl0dWRhdGEuY29tggsqLm1laXR1LmNvbYIK
> Ki40Mzk5LmNvbYIPKi5uZXdzLmNjdHYuY29tghEqLm5ld2VyYS5jY3R2LmNvbYIS
> Ki5vcGVuY2xhLmNjdHYuY29tghYqLm5ld3Njb250ZW50LmNjdHYuY29tgg4qLm5u
> bi5jY3R2LmNvbYIOKi5uZXdzLmNudHYuY26CDyoubGl2ZS5jY3R2LmNvbYIUKi5u
> ZXdjb21tZW50LmNudHYuY26CESouaXR2LmNjdHZwaWMuY29tgg0qLmltZy5jbnR2
> LmNughMqLmltZy5saXZlY2hpbmEuY29tgg8qLmlwYW5kYS5jb20uY26CDiouaXBy
> LmNjdHYuY29tgg0qLmlwci5jbnR2LmNughAqLmlzaG93LmNjdHYuY29tgg4qLml0
> di5jY3R2LmNvbYINKi5pdHYuY250di5jboIWKi5uY3BhLWNsYXNzaWMuY250di5j
> boINKi5vdnAuY250di5jboIOKi5saXZlLmNudHYuY26CDioubWFpbC5jbnR2LmNu
> gg8qLm1pbmkuY2N0di5jb22CESoubW9uZ29sLmNjdHYuY29tghAqLm1vbmdvbC5j
> bnR2LmNuggwqLm15LmNudHYuY26CDSoub3BzLmNudHYuY26CDCoudC5jY3R2LmNv
> bYITKi5wYXNzcG9ydC5jY3R2LmNvbYIRKi53ZWJhcHAuY2N0di5jb22CDioudi5p
> cGFuZGEuY29tghAqLnZpZGVvLmNjdHYuY29tgg0qLnZpcC5jbnR2LmNugg4qLnZt
> cy5jY3R2LmNvbYIPKi52b3RlLmNjdHYuY29tgg4qLnZvdGUuY250di5jboINKi52
> cG4uY250di5jboINKi53eC5jY3R2LmNvbYIMKi52LmNjdHYuY29tggwqLnd4LmNu
> dHYuY26CDyoud3hhcHAuY250di5jboIQKi54aXlvdS5jY3R2LmNvbYIPKi54aXlv
> dS5jbnR2LmNugg0qLnlwLmNjdHYuY29tgg8qLnlzZHguY2N0di5jb22CFW1pY3Jv
> Z2FtZS41MDU0Mzk5Lm5ldIINaW1nLjcxYWNnLm5ldIILKi52LmNudHYuY26CDyou
> dW5pb24uY250di5jboISKi5wYXNzcG9ydC5jbnR2LmNughEqLnNwb3J0cy5jY3R2
> LmNvbYINKi5wYXkuY250di5jboIQKi5wbGF5ZXIuY250di5jboILKi5xLmNudHYu
> Y26CDSoucXIuY2N0di5jb22CDCoucXIuY250di5jboIOKi5zZGMuY2N0di5jb22C
> DSouc21zLmNudHYuY26CECouc3BvcnRzLmNudHYuY26CECoudW5pb24uY2N0di5j
> b22CDyouc3RhZmYuY250di5jboIOKi5pbWcuY2N0di5jb22CDSoudGMuY2N0di5j
> b22CFCoudGVjaGNlbnRlci5jbnR2LmNugg4qLnRlc3QuY250di5jboIQKi50cmF2
> ZWwuY250di5jboINKi50di5jY3R2LmNvbYIMKi50di5jbnR2LmNughEqLmltZy5j
> Y3R2cGljLmNvbYIZd2Vic3NsLmNoaW5hbmV0Y2VudGVyLmNvbYIic2VjdXJlLWlu
> dC13ZWItdGljLWNuLmJtd2dyb3VwLmNvbYIcc2VjdXJlLWluZm9uZXQzLmJtd2dy
> b3VwLmNvbYIOKi5hZHMuY2N0di5jb22CDyouNXBsdXMuY250di5jboIMdy50YW5j
> ZG4uY29tghFjcGcubWVpdHViYXNlLmNvbYIWd3d3Lm1pbml0aGVjb29wZXJzLmNv
> bYITKi5zZXJ2aWNlLmt1Z291LmNvbYIec2VjdXJlLXdlYi10aWMtY24uYm13Z3Jv
> dXAuY29tgiBzZWN1cmUtaW5mb25ldDItaW50LmJtd2dyb3VwLmNvbYInc2VjdXJl
> LWludC13ZWItdGljLW1pbmktY24uYm13Z3JvdXAuY29tgiNzZWN1cmUtd2ViLXRp
> Yy1taW5pLWNuLmJtd2dyb3VwLmNvbYIgc2VjdXJlLWluZm9uZXQzLWludC5ibXdn
> cm91cC5jb22CEmNtc2NuLmJtd2dyb3VwLmNvbYISKi5nZGpoLnZ4aW55b3UuY29t
> ggsqLnFmLjU2LmNvbYINbS40Mzk5YXBpLmNvbYIPYXV0by50YW5jZG4uY29tghBz
> ai5uenNpdGVyZXMuY29tghFmYW54aW5nLmt1Z291LmNvbYIMc3MuM3oyMjIuY29t
> gglzcy45azkuY26CFHl4ZC5mbGFzaGdhbWUxNjMuY29tghAqLnNlcnZ5b3UuY29t
> LmNughJtZmFueGluZy5rdWdvdS5jb22CCnNzby41Ni5jb22CDSouYXBpLmNudHYu
> Y26CDyouMjAwOC5jY3R2LmNvbYIOKi5hcHAuY2N0di5jb22CDiouY2JveC5jbnR2
> LmNughMqLmRvd25sb2FkLmNjdHYuY29tghIqLmRpYW55aW5nLmNudHYuY26CFCou
> ZGVuZ3poZXdvLmNjdHYuY29tgg8qLmRhdGEuY2N0di5jb22CEiouY3BvcnRhbC5j
> Y3R2LmNvbYIPKi5jcGM5MC5jbnR2LmNugg0qLmNudHYuY29tLmNugg0qLmNtcy5j
> bnR2LmNughAqLmNjdHY1LmNjdHYuY29tgg0qLmFwcC5jbnR2LmNugg9taC50aWFu
> Y2l0eS5jb22CDSouY2N0di5jb20uY26CDyouYmxvZy5jY3R2LmNvbYIQKi5iYWln
> ZS5jY3R2LmNvbYIPKi5hcHAuY250dndiLmNugg8qLmFwcHMuY2N0di5jb22CDSou
> YmJzLmNudHYuY26CDyouYXJ0cy5jY3R2LmNvbYIPKi5iYWlkdS5jbnR2LmNugg4q
> LmFwcHMuY250di5jboISZmFueGluZzIua3Vnb3UuY29tgg4qLmJicy5jY3R2LmNv
> bYIUbml0cm9tZS5jb20uNDM5OS5jb22CG2FwaS5iZWF1dHltYXN0ZXIubWVpeWFu
> LmNvbYIVaW9zLmh4ankuaXdhbjQzOTkuY29tghpoNS5iZWF1dHltYXN0ZXIubWVp
> eWFuLmNvbYIYYXBpLnNlbGZpZWNpdHkubWVpdHUuY29tghdoNS5zZWxmaWVjaXR5
> Lm1laXR1LmNvbYIWYXBpLnBob3RvLm1laXR1eXVuLmNvbYISaW0ubGl2ZS5tZWlw
> YWkuY29tghRhcGkueGl1eGl1Lm1laXR1LmNvbYIYeGl1eGl1Lmh1b2RvbmcubWVp
> dHUuY29tghhjZG4uaHhqeWlvcy5pd2FuNDM5OS5jb22CF2RsLmdpdmluZ3RhbGVz
> Lmd4cGFuLmNughBpZC5hcGkubWVpdHUuY29tghRhcGkubWFrZXVwLm1laXR1LmNv
> bYISb3Blbi53ZWIubWVpdHUuY29tghR4aXV4aXUud2ViLm1laXR1LmNvbYIUc3Rh
> dGljLmJzdC5tZWl0dS5jb22CFCouZ3JpZHN1bS52ZC5jbnR2LmNughIqLnZkbi5h
> cHBzLmNudHYuY26CEHVwbG9hZC5xZi41Ni5jb22CEiouZGlhcnkubXkuY250di5j
> boIUKi5pbnRsLjIwMDguY2N0di5jb22CEmRsLmpwaGJway5neHBhbi5jboIVKi5j
> YnMuc3BvcnRzLmNjdHYuY29tghgqLm11c2V1bS5pbWcuY2N0dnBpYy5jb22CFiou
> YXBpLmNwb3J0YWwuY2N0di5jb22CFioubmV3cy5pbWcuY2N0dnBpYy5jb22CH2Rs
> b3lhbHR5cGFydG5lci5jbi5ibXdncm91cC5jb22CHmRsb3lhbHR5ZGVhbGVyLmNu
> LmJtd2dyb3VwLmNvbYIdZGxveWFsdHlhZG1pbi5jbi5ibXdncm91cC5jb22CHmRs
> b3lhbHR5cmVwb3J0LmNuLmJtd2dyb3VwLmNvbYIZcHZtZXNzYWdlLmNuLmJtd2dy
> b3VwLmNvbYIVKi5oZC5jcG9ydGFsLmNjdHYuY29tghgqLnNwb3J0cy5pbWcuY2N0
> dnBpYy5jb22CFiouc3RhdGljLjIwMDguY2N0di5jb22CEyoudHYuY2N0djUuY2N0
> di5jb22CDm0uYmJzLjM4MzkuY29tghVjZG4uc3Nqai5pd2FuNDM5OS5jb22CESou
> di4yMDA4LmNjdHYuY29tghBjZG4uZGFubXUuNTYuY29tghRjZG4uaDV3YW4uNDM5
> OXNqLmNvbYIbd3d3Lm1pbmljbGlwLmNvbS40Mzk5cGsuY29tMA4GA1UdDwEB/wQE
> AwIFoDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwdQYDVR0fBG4wbDA0
> oDKgMIYuaHR0cDovL2NybDMuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0Q05SU0FDQUcx
> LmNybDA0oDKgMIYuaHR0cDovL2NybDQuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0Q05S
> U0FDQUcxLmNybDBMBgNVHSAERTBDMDcGCWCGSAGG/WwBATAqMCgGCCsGAQUFBwIB
> FhxodHRwczovL3d3dy5kaWdpY2VydC5jb20vQ1BTMAgGBmeBDAECAjBvBggrBgEF
> BQcBAQRjMGEwIQYIKwYBBQUHMAGGFWh0dHA6Ly9vY3NwLmRjb2NzcC5jbjA8Bggr
> BgEFBQcwAoYwaHR0cDovL2NybC5kaWdpY2VydC1jbi5jb20vRGlnaUNlcnRDTlJT
> QUNBRzEuY3J0MAwGA1UdEwEB/wQCMAAwggEFBgorBgEEAdZ5AgQCBIH2BIHzAPEA
> dgAHt1wb5X1o//Gwxh0jFce65ld8V5S3au68YToaadOiHAAAAXLKnX81AAAEAwBH
> MEUCICQ38xCx2oi4RdWoHG37l5LfXuX6sYDFziYtIpljwgpVAiEAzSxc3uErukR8
> NgB/v65pyXrBmmZGvoAEyFp7YQcA1qYAdwBep3P531bA57U2SH3QSeAyepGaDISh
> EhKEGHWWgXFFWAAAAXLKnX66AAAEAwBIMEYCIQD2lWe7lQe3TGClTe9fsJ7FyzjF
> eEz15SyKOOdXF9VXxAIhAJZbRlEgVHC+pirpGXg7NjPaavEJm0p6F0TzcUYrnbWB
> MA0GCSqGSIb3DQEBCwUAA4IBAQB8k8PLcRM5n+0gMaqPoCHyWiONOAzo+nUzUwKl
> ZqaglB/s/ARo4tSnAj9cqPxryxw6gHt/waaHKucRMdJIALCiD6KMxaJfdq+RWeNs
> U9sb66G0S13I6viXPZQWRvyvqCnH8+VIeixg8ju68sjAFlYzO1lVTMfb6jZgpoGn
> /hBWMC5Ya4Y37PZjTXtF/3nH47+6n5qJi5d6kY4NPedOHo6ICa1GEroCFnNtmKol
> P4FopEjjCC7OPdWzGMtR+KGqiOUle1g4OwCd7/poKWuz73ae7T1Q0hhMOgs/wxC+
> 9dCRobbSomi61cNd42Y0zhvAws/Yfyw4GVFSJkIB+CqLdVOf
> -----END CERTIFICATE-----
> subject=/C=CN/ST=\xE7\xA6\x8F\xE5\xBB\xBA\xE7\x9C\x81/L=\xE5\x8E\xA6\xE9\x97\xA8\xE5\xB8\x82/O=\xE7\xBD\x91\xE5\xAE\xBF\xE7\xA7\x91\xE6\x8A\x80\xE8\x82\xA1\xE4\xBB\xBD\xE6\x9C\x89\xE9\x99\x90\xE5\x85\xAC\xE5\x8F\xB8\xE5\x8E\xA6\xE9\x97\xA8\xE5\x88\x86\xE5\x85\xAC\xE5\x8F\xB8/OU=IT/CN=webssl.chinanetcenter.com
> issuer=/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert CN RSA CA G1
> ---
> No client certificate CA names sent
> Peer signing digest: SHA256
> Server Temp Key: ECDH, P-256, 256 bits
> ---
> SSL handshake has read 7635 bytes and written 433 bytes
> ---
> New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256
> Server public key is 2048 bit
> Secure Renegotiation IS supported
> Compression: NONE
> Expansion: NONE
> No ALPN negotiated
> SSL-Session:
>     Protocol  : TLSv1.2
>     Cipher    : ECDHE-RSA-AES128-GCM-SHA256
>     Session-ID: 82265A527B8027908036EBB9486CC7A048E484F836AD3250952976969D95E12D
>     Session-ID-ctx: 
>     Master-Key: 0738979C685DE1EFC159C9D21453A069379651D1B28326165A5C0C52265EE4601ED6D01BB44D74FFDEBACF7F73085853
>     Key-Arg   : None
>     PSK identity: None
>     PSK identity hint: None
>     SRP username: None
>     TLS session ticket lifetime hint: 7200 (seconds)
>     TLS session ticket:
>     0000 - f9 b5 c9 ba 56 9e 82 e9-e0 9e d1 09 bd 1e e3 ee   ....V...........
>     0010 - 24 0d 2a a3 ec c9 76 e3-60 b9 03 ff 86 62 e5 f3   $.*...v.`....b..
>     0020 - e4 28 3f e2 1b 3f 9a 42-3e 89 ce 5d b0 5a 78 3a   .(?..?.B>..].Zx:
>     0030 - 27 fa e3 0d f2 e8 72 2f-92 c5 a8 14 cd f3 22 0b   '.....r/......".
>     0040 - bc ec e3 f3 74 95 cf 07-56 b8 37 e1 a0 66 a5 23   ....t...V.7..f.#
>     0050 - 92 03 f3 b4 5b 47 4f f8-a0 11 c2 a2 9a 48 b5 6f   ....[GO......H.o
>     0060 - 6a e0 e6 2d ac f6 dc 23-32 ea b3 1a 92 11 ba f9   j..-...#2.......
>     0070 - 3c 4b 51 c8 3f ff 2d 37-15 89 56 2c 8e 63 ab 08   <KQ.?.-7..V,.c..
>     0080 - 0d 54 be fd f2 7c 3b 3a-2f 58 79 3d f6 58 31 91   .T...|;:/Xy=.X1.
>     0090 - 22 01 9e 2b 9a 62 fd 7b-3a 0b f0 71 f6 56 77 28   "..+.b.{:..q.Vw(
>     00a0 - 39 a3 0e 51 1e 39 fb b9-56 94 85 3c 93 7d e7 e1   9..Q.9..V..<.}..
> 
>     Start Time: 1592924413
>     Timeout   : 300 (sec)
>     Verify return code: 0 (ok)
> ---



Why this?
Does Squid perform something different from OpenSSL?
Does it have some certificate cache I should clear? How?

  bye & Thanks
	av.


From rousskov at measurement-factory.com  Tue Jun 23 15:42:19 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Jun 2020 11:42:19 -0400
Subject: [squid-users] (92) Protocol error (TLS code:
 X509_V_ERR_CERT_HAS_EXPIRED)
In-Reply-To: <7b43530a-7516-22de-ac30-a418612cfac7@netfence.it>
References: <7b43530a-7516-22de-ac30-a418612cfac7@netfence.it>
Message-ID: <19001981-ec9c-4bf7-5e1b-a441dfd33a2d@measurement-factory.com>

On 6/23/20 11:04 AM, Andrea Venturoli wrote:

> Running Squid 4.11 on FreeBSD 11.3 with SSLBump, since a few days, I've
> got several sites (e.g. https://www.kawsaki.it/) failing with:
> 
>> The following error was encountered while trying to retrieve the URL:
>> https://www.kawasaki.it/*
>>
>> ??? Failed to establish a secure connection to 54.39.161.167
>>
>> The system returned:
>>
>> ??? (92) Protocol error (TLS code: X509_V_ERR_CERT_HAS_EXPIRED)
>>
>> ??? SSL Certificate expired on: May 30 10:48:38 2020 GMT

> When this happens, in cache.log I see:
>> 2020/06/23 15:03:31 kid1| ERROR: negotiating TLS on FD 33:
>> error:14090086:SSL routines:ssl3_get_server_certificate:certificate
>> verify failed (1/-1/0)

> I know an intermediate certificate expired, but a new one should have
> been published.


> Does Squid perform something different from OpenSSL?

Yes, Squid has custom TLS-related code, including certificate
validation, generation, and fetching code.


> Does it have some certificate cache

Yes, there can be two or even four caches in play here:

1. The in-RAM cache of generated fake certificates (see
dynamic_cert_mem_cache_size),

2. on-disk cache of generated fake certificates (see sslcrtd_program),

3. a regular HTTP in-RAM cache (see cache_mem) that may keep a copy of
the intermediate certificate downloaded by Squid.

4. a regular HTTP on-disk cache (see cache_dir) that may keep a copy of
the intermediate certificate downloaded by Squid.


> I should clear?

*If* Squid is caching an expired certificate without revalidation, then
this is essentially a Squid bug. There are many unknowns here, so I
cannot confirm or deny the existence of such a bug without spending more
free time which I do not have (unfortunately). I also do not know (did
not check) whether Squid is caching the expired fake certificate and/or
the real intermediate one.

You can try to fix the problem or workaround the Squid bug by clearing
the caches.


> How?

I would begin with a full Squid shutdown and start. This will clear all
in-RAM caches.

If the problem persists, you can remove the entire on-disk certificate
generator cache (or extract the bad certificates from it, but that
requires even more work). See sslcrtd_program for more info on that
cache location. Do not forget to re-initialize it!

If the problem persists, you can remove the entire on-disk HTTP cache
(or extract the bad certificates from it, but that requires even more
work). See cache_dir for more info on that cache location. Do not forget
to re-initialize it!


I cannot give you step-by-step instructions, but others on the list may
pitch in as you make progress in your triage using the above hints.


HTH,

Alex.


From yannick.rousseau at tutanota.com  Tue Jun 23 19:27:07 2020
From: yannick.rousseau at tutanota.com (yannick.rousseau at tutanota.com)
Date: Tue, 23 Jun 2020 21:27:07 +0200 (CEST)
Subject: [squid-users] no response from the proxy squid parent
Message-ID: <MAXYaDB--3-2@tutanota.com>

Hi,?

I'm using squid (4.6) on my server (debianedu buster LTSP), and I'm trying to configure a parent proxy.

At first, when I configure the client's firefox (manual proxy configuration) with the ip and port of the parent proxy, it's ok, I can surf on the internet.?

But I would like to configure my server's Squid Proxy to forward to a parent proxy (172.16.103.254:3128)
-> So I add these two lines at the end of squid.conf:
cache_peer 172.16.103.254 parent 3128 0 no-query no-digest
never_direct allow all

-> And restart squid. It seems to be ok:
# cat /var/log/squid/cache.log
(.....)
2020/06/23 09:51:12 kid1| Configuring Parent 172.16.103.254/3128/0
(....)

-> Then I configure firefox to use system proxy settings, but when I try to google something or visit debian-fr.org, it doesn't work (no reponse from the proxy). But my squid's configuration seems to be ok:
# cat /var/log/squid/access.log
(....)
1592921221.753??? 138 10.0.2.2 TCP_TUNNEL/403 361 CONNECT?www.google.com:443 <http://www.google.com:443/>?- FIRSTUP_PARENT/172.16.103.254 -
1592921275.641??? 521 10.0.2.2 TCP_MISS/403 4289 GET?http://www.debian-fr.org/?- FIRSTUP_PARENT/172.16.103.254 text/html
1592921275.692????? 0 10.0.2.2 TCP_HIT/200 13072 GET
(...)

Is it possible that the squid parent refuse to have "a child" ?

Thanks a lot for your help.




-- 
Envoi s?curis? avec Tutanota. Obtenez votre propre adresse email chiffr?e : 
https://tutanota.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200623/7d1bd5c6/attachment.htm>

From rentorbuy at yahoo.com  Wed Jun 24 14:57:43 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 24 Jun 2020 14:57:43 +0000 (UTC)
Subject: [squid-users] reverse proxy Squid 4
References: <572062950.2433499.1593010663033.ref@mail.yahoo.com>
Message-ID: <572062950.2433499.1593010663033@mail.yahoo.com>

Hi,

Today I just migrated from Squid 3 to Squid 4, and I found that a reverse proxy that was working fine before is now failing. The client browser sees this message:

[No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
Handshake with SSL server failed: [No Error]

This is how I configured the backend:

cache_peer 10.215.144.16 parent 443 0 no-query originserver login=PASS ssl sslcert=/etc/ssl/MY-CA/certs/W1_cert.cer sslkey=/etc/ssl/MY-CA/certs/W1_key_nopassphrase.pem sslcafile=/etc/ssl/MY-CA/cacert.pem ssloptions=NO_SSLv3,NO_SSLv2,NO_TLSv1_2,NO_TLSv1_1 sslflags=DONT_VERIFY_PEER front-end-https=on name=MyServer

The NO_TLSv* options are because the backend server is an old Windows 2003 (which hasn't changed either).

How can I debug this?

Vieri


From robertkwild at gmail.com  Wed Jun 24 15:36:34 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 24 Jun 2020 16:36:34 +0100
Subject: [squid-users] try and reslove domain via local DNS and not squid
Message-ID: <CAGU_CiJf_XSdX_QMzs3v5tUfVvTaAUrXeX-+yYnQL_D2rv75Xw@mail.gmail.com>

hi all,

i want squid not to try and resolve our domain name ie so it resolves
internally on our local DNS server and not go out squid to try and resolve

hope that makes sense

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200624/722a9789/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Jun 24 15:42:36 2020
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 24 Jun 2020 17:42:36 +0200
Subject: [squid-users] try and reslove domain via local DNS and not squid
In-Reply-To: <CAGU_CiJf_XSdX_QMzs3v5tUfVvTaAUrXeX-+yYnQL_D2rv75Xw@mail.gmail.com>
References: <CAGU_CiJf_XSdX_QMzs3v5tUfVvTaAUrXeX-+yYnQL_D2rv75Xw@mail.gmail.com>
Message-ID: <202006241742.36411.Antony.Stone@squid.open.source.it>

On Wednesday 24 June 2020 at 17:36:34, robert k Wild wrote:

> hi all,
> 
> i want squid not to try and resolve our domain name ie so it resolves
> internally on our local DNS server and not go out squid to try and resolve

What is in /etc/resolv.conf on your squid server?

Antony.

-- 
Never write it in Perl if you can do it in Awk.
Never do it in Awk if sed can handle it.
Never use sed when tr can do the job.
Never invoke tr when cat is sufficient.
Avoid using cat whenever possible.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From robertkwild at gmail.com  Wed Jun 24 15:47:19 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 24 Jun 2020 16:47:19 +0100
Subject: [squid-users] try and reslove domain via local DNS and not squid
In-Reply-To: <202006241742.36411.Antony.Stone@squid.open.source.it>
References: <CAGU_CiJf_XSdX_QMzs3v5tUfVvTaAUrXeX-+yYnQL_D2rv75Xw@mail.gmail.com>
 <202006241742.36411.Antony.Stone@squid.open.source.it>
Message-ID: <CAGU_Ci+WtL_bqQpqLqrQ5_g9rzzeT82FhzvESoNTrmkmZRhxSw@mail.gmail.com>

hahahahaha, simples

i havnt added my internal dns to it, i just have it as 8.8.8.8

On Wed, 24 Jun 2020 at 16:42, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 24 June 2020 at 17:36:34, robert k Wild wrote:
>
> > hi all,
> >
> > i want squid not to try and resolve our domain name ie so it resolves
> > internally on our local DNS server and not go out squid to try and
> resolve
>
> What is in /etc/resolv.conf on your squid server?
>
> Antony.
>
> --
> Never write it in Perl if you can do it in Awk.
> Never do it in Awk if sed can handle it.
> Never use sed when tr can do the job.
> Never invoke tr when cat is sufficient.
> Avoid using cat whenever possible.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200624/74942411/attachment.htm>

From rentorbuy at yahoo.com  Thu Jun 25 00:16:21 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 25 Jun 2020 00:16:21 +0000 (UTC)
Subject: [squid-users] reverse proxy Squid 4
References: <2037867409.2681367.1593044181279.ref@mail.yahoo.com>
Message-ID: <2037867409.2681367.1593044181279@mail.yahoo.com>

This is what the squid cache log reports:

2020/06/25 00:29:05.467 kid1| 83,5| NegotiationHistory.cc(81) retrieveNegotiatedInfo: SSL connection info on FD 15 SSL version NONE/0.0 negotiated cipher
2020/06/25 00:29:05.467 kid1| ERROR: negotiating TLS on FD 15: error:00000000:lib(0):func(0):reason(0) (5/-1/0)
2020/06/25 00:29:05.467 kid1| 83,5| BlindPeerConnector.cc(68) noteNegotiationDone: error=0x55cf5c9bb5b8
2020/06/25 00:29:05.467 kid1| TCP connection to 10.215.144.16/443 failed

Same old issue where openssl does not say why the handshake failed.

I'm having the same problem with an Apache reverse proxy, so now I'm falling back to use http on my backend.

Thanks


From squid3 at treenet.co.nz  Thu Jun 25 08:28:52 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jun 2020 20:28:52 +1200
Subject: [squid-users] reverse proxy Squid 4
In-Reply-To: <572062950.2433499.1593010663033@mail.yahoo.com>
References: <572062950.2433499.1593010663033.ref@mail.yahoo.com>
 <572062950.2433499.1593010663033@mail.yahoo.com>
Message-ID: <bcf6fc32-d298-1640-de69-1253be85bd72@treenet.co.nz>

On 25/06/20 2:57 am, Vieri wrote:
> Hi,
> 
> Today I just migrated from Squid 3 to Squid 4, and I found that a reverse proxy that was working fine before is now failing. The client browser sees this message:
> 
> [No Error] (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> Handshake with SSL server failed: [No Error]
> 
> This is how I configured the backend:
> 
> cache_peer 10.215.144.16 parent 443 0 no-query originserver login=PASS ssl sslcert=/etc/ssl/MY-CA/certs/W1_cert.cer sslkey=/etc/ssl/MY-CA/certs/W1_key_nopassphrase.pem sslcafile=/etc/ssl/MY-CA/cacert.pem ssloptions=NO_SSLv3,NO_SSLv2,NO_TLSv1_2,NO_TLSv1_1 sslflags=DONT_VERIFY_PEER front-end-https=on name=MyServer
> 

All options relating to SSLv2 are no longer supported in Squid-4+:
 <http://www.squid-cache.org/Versions/v4/RELEASENOTES.html#ss2.3>


> The NO_TLSv* options are because the backend server is an old Windows 2003 (which hasn't changed either).

Does it obey TLS/1.0 properly?

If so you should only need to configure these for Squid-4+
  tls-options=NO_SSLv3,NO_TLSv1_3 tls-min-version=1.0

If it is so broken that is cannot handle TLS 1.1 or 1.2 numbers in the
handshake (TLSv1.0 requires that it does). Then you will need:
  tls_options=NO_SSLv3,NO_TLSv1_1,NO_TLSv1_2,NO_TLSv1_3


> 
> How can I debug this?
> 

Start with removing the "sslflags=DONT_VERIFY_PEER" so TLS information
gets checked instead of silently ignored.

Then reduce the ssloptions= as much as you can. Remove if possible. A
packet trace of what is being attempted will be useful then.


Amos


From squid3 at treenet.co.nz  Thu Jun 25 08:42:00 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jun 2020 20:42:00 +1200
Subject: [squid-users] Upload throttling
In-Reply-To: <1501740C-7408-4C96-9162-F9E415207404@homeoffice.wal-mart.com>
References: <1501740C-7408-4C96-9162-F9E415207404@homeoffice.wal-mart.com>
Message-ID: <9374a2b5-5218-1760-ec43-734cd12d3be1@treenet.co.nz>

On 23/06/20 12:25 am, Mate Javor wrote:
> Hi squid-users,
> 
> We?re trying to configure squid upload throttling using
> client_delay_pools . I was reading around the topic and some says that
> client_delay_pools is not a solution for throttling uploads. Can you pls
> help me here?
> 

client_delay_* is a rate limit on HTTP traffic delivered to the client
by Squid.

Assuming by "upload" you mean traffic from client to Squid, then yes it
is not going to count the bytes you want counted.


The modern QoS systems built into the OS are almost always better for
the rate limiting jobs. Especially for counting bytes *before* they
arrive at Squid on client connections.


Amos


From rousskov at measurement-factory.com  Thu Jun 25 12:52:21 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Jun 2020 08:52:21 -0400
Subject: [squid-users] reverse proxy Squid 4
In-Reply-To: <2037867409.2681367.1593044181279@mail.yahoo.com>
References: <2037867409.2681367.1593044181279.ref@mail.yahoo.com>
 <2037867409.2681367.1593044181279@mail.yahoo.com>
Message-ID: <2a203bab-d8fe-8793-fb10-404c36c2288e@measurement-factory.com>

On 6/24/20 8:16 PM, Vieri wrote:
> This is what the squid cache log reports:
> 
> 2020/06/25 00:29:05.467 kid1| 83,5| NegotiationHistory.cc(81) retrieveNegotiatedInfo: SSL connection info on FD 15 SSL version NONE/0.0 negotiated cipher
> 2020/06/25 00:29:05.467 kid1| ERROR: negotiating TLS on FD 15: error:00000000:lib(0):func(0):reason(0) (5/-1/0)
> 2020/06/25 00:29:05.467 kid1| 83,5| BlindPeerConnector.cc(68) noteNegotiationDone: error=0x55cf5c9bb5b8
> 2020/06/25 00:29:05.467 kid1| TCP connection to 10.215.144.16/443 failed
> 
> Same old issue where openssl does not say why the handshake failed.

Actually, OpenSSL does say why the handshake failed in this case:
AFAICT, OpenSSL reports that a system call has failed ("5" in "5/-1/0"
is SSL_ERROR_SYSCALL). Squid loses the details of that failure (e.g.,
what kind of system call error Squid has experienced), but we are almost
done improving that.

Alex.


From rentorbuy at yahoo.com  Thu Jun 25 14:25:27 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 25 Jun 2020 14:25:27 +0000 (UTC)
Subject: [squid-users] reverse proxy Squid 4
In-Reply-To: <bcf6fc32-d298-1640-de69-1253be85bd72@treenet.co.nz>
References: <572062950.2433499.1593010663033.ref@mail.yahoo.com>
 <572062950.2433499.1593010663033@mail.yahoo.com>
 <bcf6fc32-d298-1640-de69-1253be85bd72@treenet.co.nz>
Message-ID: <68223973.2895014.1593095127752@mail.yahoo.com>



On Thursday, June 25, 2020, 10:32:46 AM GMT+2, Amos Jeffries <squid3 at treenet.co.nz> wrote: 

>
>? tls-options=NO_SSLv3,NO_TLSv1_3 tls-min-version=1.0
>
>? tls_options=NO_SSLv3,NO_TLSv1_1,NO_TLSv1_2,NO_TLSv1_3
>
> removing the "sslflags=DONT_VERIFY_PEER"
>
> Then reduce the ssloptions= as much as you can. Remove if possible. 

Tried all of that, but still just getting this in the log:

kid1| 83,5| NegotiationHistory.cc(81) retrieveNegotiatedInfo: SSL connection info on FD 13 SSL version NONE/0.0 negotiated cipher
kid1| ERROR: negotiating TLS on FD 13: error:00000000:lib(0):func(0):reason(0) (5/-1/0)

> A packet trace of what is being attempted will be useful then.

Will try to save one.

Thanks,

Vieri


From squid3 at treenet.co.nz  Fri Jun 26 11:11:25 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jun 2020 23:11:25 +1200
Subject: [squid-users] no response from the proxy squid parent
In-Reply-To: <MAXYaDB--3-2@tutanota.com>
References: <MAXYaDB--3-2@tutanota.com>
Message-ID: <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>

On 24/06/20 7:27 am, yannick.rousseau at tutanota.com wrote:
> Hi,?
> 
> I'm using squid (4.6) on my server (debianedu buster LTSP), and I'm
> trying to configure a parent proxy.
> 
> At first, when I configure the client's firefox (manual proxy
> configuration) with the ip and port of the parent proxy, it's ok, I can
> surf on the internet.?
> 
> But I would like to configure my server's Squid Proxy to forward to a
> parent proxy (172.16.103.254:3128)
> -> So I add these two lines at the end of squid.conf:
> cache_peer 172.16.103.254 parent 3128 0 no-query no-digest
> never_direct allow all
> 
> -> And restart squid. It seems to be ok:
> # cat /var/log/squid/cache.log
> (.....)
> 2020/06/23 09:51:12 kid1| Configuring Parent 172.16.103.254/3128/0
> (....)
> 
> -> Then I configure firefox to use system proxy settings, but when I try
> to google something or visit debian-fr.org, it doesn't work (no reponse
> from the proxy).

That is odd. The log shows a 403 response being delivered by the parent
proxy and delivered to Firefox.

Browsers refuse to display proxy responses on CONNECT requests. So the
first is expected. But the second one using http:// should be shown.


> But my squid's configuration seems to be ok:
> # cat /var/log/squid/access.log
> (....)
> 1592921221.753??? 138 10.0.2.2 TCP_TUNNEL/403 361
> CONNECT?www.google.com:443 <http://www.google.com:443/>?-
> FIRSTUP_PARENT/172.16.103.254 -
> 1592921275.641??? 521 10.0.2.2 TCP_MISS/403 4289
> GET?http://www.debian-fr.org/?- FIRSTUP_PARENT/172.16.103.254 text/html
> 1592921275.692????? 0 10.0.2.2 TCP_HIT/200 13072 GET
> (...)
> 
> Is it possible that the squid parent refuse to have "a child" ?

Maybe. You will need to know the parent proxy configuration to tell
that. All that is visible from the detail you have shown is that parent
proxy has forbidden the requests it is receiving.


Amos


From stefano.tabacchiera at IGT.com  Fri Jun 26 12:19:58 2020
From: stefano.tabacchiera at IGT.com (Tabacchiera, Stefano)
Date: Fri, 26 Jun 2020 12:19:58 +0000
Subject: [squid-users] squid stores multiple copies of identical ETags
Message-ID: <DM6PR01MB3817F6D622C17F16BE46CD01E4930@DM6PR01MB3817.prod.exchangelabs.com>

Hello there,
I have the following issue with squid (squid-3.5.20-12.el7_6.1.x86_64  on RHEL 7.6)

A client requests a json with "no-cache" header via proxy.
Squid forwards the request to origin server, which replies with "ETag" header (object is cachable).
Squid stores the object in cache_dir and forwards back to the client.

The client is pushing same request at high rate (~10/sec), regardless its cachable status.
Squid keeps forwarding the request and - here is my issue - keeps storing the same identical object on disk.
I have thousands of copies of the same Etag on disk.

Is there a way to avoid this? I think Squid should store a single copy per-URL/ETAg of the object.
I'd like to avoid an ad-hoc reload-into-ims refresh-pattern.

Here's an example of headers sequence:



  1.  CLIENT --> SQUID
GET http://xxx.xxx.xxx.xxx/blah/FEED.json HTTP/1.1
Accept-Encoding: gzip
User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.4; en-US; rv:1.9.2.2) Gecko/20100316 Firefox/3.6.2
Content-Language: en-US
Cache-Control: no-cache
Pragma: no-cache
Host: xxx.xxx.xxx.xxx
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Proxy-Connection: keep-alive


  1.  SQUID --> ORIGIN SERVER
GET /blah/FEED.json HTTP/1.1
Accept-Encoding: gzip
User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.4; en-US; rv:1.9.2.2) Gecko/20100316 Firefox/3.6.2
Content-Language: en-US
Pragma: no-cache
Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2
Host: xxx.xxx.xxx.xxx
Via: 1.1 RP-PRXSQUID-2 (squid/3.5.20)
X-Forwarded-For: unknown
Cache-Control: no-cache
Connection: keep-alive


  1.  ORIGIN SERVER --> SQUID
HTTP/1.1 200 OK
Server: Apache/2.2.16 (Debian)
Last-Modified: Fri, 26 Jun 2020 11:46:00 GMT
ETag: "62b905-a7cfa-5a8fb40bdee00"
Accept-Ranges: bytes
Content-Length: 687354
Keep-Alive: timeout=4, max=45
Connection: Keep-Alive
Content-Type: text/plain


  1.  SQUID --> CLIENT
HTTP/1.1 200 OK
Date: Fri, 26 Jun 2020 11:52:15 GMT
Server: Apache/2.2.16 (Debian)
Last-Modified: Fri, 26 Jun 2020 11:46:00 GMT
ETag: "62b905-a7cfa-5a8fb40bdee00"
Accept-Ranges: bytes
Content-Length: 687354
Content-Type: text/plain
X-Cache: MISS from RP-PRXSQUID-2
X-Cache-Lookup: HIT from RP-PRXSQUID-2:3128
Via: 1.1 RP-PRXSQUID-2 (squid/3.5.20)
Connection: keep-alive

Thanks
ST
____________________________________________________________________________________ La presente comunicazione ed i suoi allegati e' destinata esclusivamente ai destinatari. Qualsiasi suo utilizzo, comunicazione o diffusione non autorizzata e' proibita. Se ha ricevuto questa comunicazione per errore, la preghiamo di darne immediata comunicazione al mittente e di cancellare tutte le informazioni erroneamente acquisite. Grazie This message and its attachments are intended only for use by the addressees. Any use, re-transmission or dissemination not authorized of it is prohibited. If you received this e-mail in error, please inform the sender immediately and delete all the material. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200626/1d472686/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun 26 12:19:25 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jun 2020 00:19:25 +1200
Subject: [squid-users] transparent proxy upgrade 3.5 to 4.12,
 Error parsing SSL Server Hello Message on FD XX
In-Reply-To: <CAPAcTN2antoDrUWss9Mh_LBuEW4xxvwx+tEJcOVamM0wJnc_9Q@mail.gmail.com>
References: <CAPAcTN2antoDrUWss9Mh_LBuEW4xxvwx+tEJcOVamM0wJnc_9Q@mail.gmail.com>
Message-ID: <27410ff3-1904-3e8d-804f-c94a3eaab7fc@treenet.co.nz>

On 23/06/20 2:50 am, Tanner wrote:
> I have squid set up as a transparent outbound proxy using version 3.5.
> When upgrading to 4.12, I am seeing an error "Error parsing SSL Server
> Hello Message on FD XX" that did not happen before. Here is my config:
> 
...

> 
> Previous to 4.12, if I tried to upgrade to any v4 or v5 of squid, I
> would get an issue with "inappropriate?fallback" when going to some
> sites supporting TLS 1.3 (but not all). This appears to have been
> resolved, but this "Error parsing SSL Server Hello Message" is new. Is
> there something that should change in my config? Can anyone tell me what
> this error means?

It may be resolved with this patch:
 <http://www.squid-cache.org/Versions/v5/changesets/squid-5-8f80586b2137cd6eaacef4e5908d03a0f7f9c7eb.patch>

Otherwise you could try the latest Squid-5.

If neither of those work, v5 should have better debugging to help track
down what the issue actually is.

Amos


From squid3 at treenet.co.nz  Fri Jun 26 12:36:19 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jun 2020 00:36:19 +1200
Subject: [squid-users] squid stores multiple copies of identical ETags
In-Reply-To: <DM6PR01MB3817F6D622C17F16BE46CD01E4930@DM6PR01MB3817.prod.exchangelabs.com>
References: <DM6PR01MB3817F6D622C17F16BE46CD01E4930@DM6PR01MB3817.prod.exchangelabs.com>
Message-ID: <39ba6fcd-394c-62fc-1b8d-657f88682ea3@treenet.co.nz>

On 27/06/20 12:19 am, Tabacchiera, Stefano wrote:
> Hello there,
> 
> I have the following issue with squid (squid-3.5.20-12.el7_6.1.x86_64?
> on RHEL 7.6)
> 
> A client requests a json with ?no-cache? header via proxy.
> 
> Squid forwards the request to origin server, which replies with ?ETag?
> header (object is cachable).
> 
> Squid stores the object in cache_dir and forwards back to the client.
> 
> 
> The client is pushing same request at high rate (~10/sec), regardless
> its cachable status.
> 
> Squid keeps forwarding the request and ? here is my issue ? keeps
> storing the same identical object on disk.
> 
> I have thousands of copies of the same Etag on disk.
> 

Objects are *not* stored by ETag. They are stored by *URL* (or URL+Vary
header).

Also, an object existing on disk does not mean it is considered to be
"latest" version of an object. It only means that no other object has
needed to use that same cache slot/file since the existing object was
stored there.
 Clearing cache slots/files the instant its content become obsolete
would cause up to _double_ the amount of disk writing to happen. Squid
already does a huge amount of writes.

In general, if the same object occuring N times on disk is a problem,
you have issues with misconfigured cache_dir parameters. eg the
cache_dir size is too big for the physical disk it is stored on. Each
type of cache_dir is optimized for different object types on different
OS - selecting which to use can be important for high performance
installations.


> 
> Is there a way to avoid this? I think Squid should store a single copy
> per-URL/ETAg of the object.
> 
> I?d like to avoid an ad-hoc reload-into-ims refresh-pattern.
> 

Squid is doing exactly what the client is demanding with its use of
"no-cache".


There is nothing wrong with that configuration. It is the best way to
make Squid cope with such a nasty client. The alternative is to ignore
*all* Cache-Control headers from all clients on all traffic - which is
much overkill.


Amos


From squid3 at treenet.co.nz  Fri Jun 26 15:48:20 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jun 2020 03:48:20 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:7 Cache
 Poisoning Issue in HTTP Request processing
Message-ID: <6fddb72c-8524-699f-455f-31ad31097585@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2020:7
__________________________________________________________________

 Advisory ID:       | SQUID-2020:7
 Date:              | June 26, 2020
 Summary:           | Cache Poisoning Issue
                    | in HTTP Request processing.
 Affected versions: | Squid 2.x -> 2.7.STABLE9
                    | Squid 3.x -> 3.5.28
                    | Squid 4.x -> 4.11
                    | Squid 5.x -> 5.0.2
 Fixed in version:  | Squid 4.12 and 5.0.3
__________________________________________________________________

 <https://github.com/squid-cache/squid/security/advisories/GHSA-qf3v-rc95-96j5>
 <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15049>
__________________________________________________________________

Problem Description:

 Due to incorrect input validation Squid is vulnerable to a
 Request Smuggling and Poisoning attack against the HTTP cache.

__________________________________________________________________

Severity:

 This problem allows a trusted client to perform request smuggling
 and poison the HTTP cache contents with crafted HTTP(S) request
 messages.

 This attack requires an upstream server to participate in the
 smuggling and generate the poison response sequence. Most popular
 server software are not vulnerable to participation in this
 attack.

CVSS Score of 9.3
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H/E:F/RL:O/RC:C/CR:H/IR:H/AR:H/MAV:N/MAC:L/MPR:L/MUI:N/MS:C/MC:H/MI:H/MA:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid versions 4.12 and 5.0.3.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 4:
 <http://www.squid-cache.org/Versions/v4/changesets/squid-4-ea12a34d338b962707d5078d6d1fc7c6eb119a22.patch>

Squid 5:
 <http://www.squid-cache.org/Versions/v5/changesets/squid-5-485c9a7bb1bba88754e07ad0094647ea57a6eb8d.patch>

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-3.x up to and including 3.5.28 are vulnerable.

 All Squid-4.x up to and including 4.11 are vulnerable.

 Squid-5.0.1 and 5.0.2 are vulnerable.

__________________________________________________________________

Workaround:

 There is no workaround for this vulnerability.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If you install and build Squid from the original Squid sources
 then the <squid-users at lists.squid-cache.org> mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Alex Rousskov of The
 Measurement Factory.

 Independent discovery and replication reported by Amit Klein of
 Safebreach.

 Fixed by Alex Rousskov of The Measurement Factory.

__________________________________________________________________

Revision history:


 2016-09-06 02:45:20 UTC Initial Report
 2020-05-11 12:41:17 UTC Replication Reported
 2020-05-13 14:05:00 UTC Patch Released
 2020-06-25 11:15:10 UTC CVE Allocated
______________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From stefano.tabacchiera at IGT.com  Fri Jun 26 17:39:59 2020
From: stefano.tabacchiera at IGT.com (Tabacchiera, Stefano)
Date: Fri, 26 Jun 2020 17:39:59 +0000
Subject: [squid-users] squid stores multiple copies of identical ETags
Message-ID: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>




>In general, if the same object occuring N times on disk is a problem,

That's the point. There's  a LOT of identical objects on disk.

>you have issues with misconfigured cache_dir parameters. eg the
cache_dir size is too big for the physical disk it is stored on.

I have 2x100gb dedicated disks, ext4 noatime.
Each cache_dir is aufs 80000 16 256.

Where's the issue? I did't even imagine this would lead a multiple copies stores of the same object.

Can you please advise on this?
Thx!

ST

____________________________________________________________________________________ La presente comunicazione ed i suoi allegati e' destinata esclusivamente ai destinatari. Qualsiasi suo utilizzo, comunicazione o diffusione non autorizzata e' proibita. Se ha ricevuto questa comunicazione per errore, la preghiamo di darne immediata comunicazione al mittente e di cancellare tutte le informazioni erroneamente acquisite. Grazie This message and its attachments are intended only for use by the addressees. Any use, re-transmission or dissemination not authorized of it is prohibited. If you received this e-mail in error, please inform the sender immediately and delete all the material. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200626/aec7e197/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 27 00:04:04 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jun 2020 12:04:04 +1200
Subject: [squid-users] squid stores multiple copies of identical ETags
In-Reply-To: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
Message-ID: <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>

On 27/06/20 5:39 am, Tabacchiera, Stefano wrote:
> 
>>In general, if the same object occuring N times on disk is a problem,
> 
> That's the point. There's? a LOT of identical objects on disk.
> 
>>you have issues with misconfigured cache_dir parameters. eg the
> cache_dir size is too big for the physical disk it is stored on.
> 
> I have 2x100gb dedicated disks, ext4 noatime.
> Each cache_dir is aufs 80000 16 256.
> 
> Where's the issue? I did't even imagine this would lead a multiple
> copies stores of the same object.

So far the problem appears to be you not understanding how caching
works. My previous response contains the explanation that should have
resolved that.


> 
> Can you please advise on this?

Only with what I stated already in my previous response.


Amos


From yannick.rousseau at tutanota.com  Sat Jun 27 02:24:04 2020
From: yannick.rousseau at tutanota.com (yannick.rousseau at tutanota.com)
Date: Sat, 27 Jun 2020 04:24:04 +0200 (CEST)
Subject: [squid-users] no response from the proxy squid parent
In-Reply-To: <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>
References: <MAXYaDB--3-2@tutanota.com>
 <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>
Message-ID: <MAn_g0n--3-2@tutanota.com>

Hi, 

Here's one more clue (thank's wireshark):

-> When I try to surf on the Net with client's firefox configued (manual proxy configuration) with the ip and port of the parent proxy, it's ok :
58	5.940721294	172.16.103.101	172.16.103.254	HTTP	255	CONNECT www.google.com:443 <http://www.google.com:443> HTTP/1.1
62	6.046854511	172.16.103.254	172.16.103.101	HTTP	75	HTTP/1.1 200 Connection established

-> When I configure firefox to use system proxy settings , it doesn't work:
35	4.798844976	172.16.103.101	172.16.103.254	HTTP	265	GET http://172.16.103.254:3128/squid-internal-dynamic/netdb HTTP/1.1
47	4.800699191	172.16.103.254	172.16.103.101	HTTP	890	HTTP/1.1 403 Forbidden? (text/html)

I think I'm going to disable? netdb by adding? no-netdb-exchange in my conf. 
And by the way, what's the difference between CONNECT and GET ?

Yannick

 
-- 
 Envoi s?curis? avec Tutanota. Obtenez votre propre adresse email chiffr?e : 
 https://tutanota.com


26 juin 2020 ? 07:11 de squid3 at treenet.co.nz:

> On 24/06/20 7:27 am, yannick.rousseau at tutanota.com wrote:
>
>> Hi,?
>>
>> I'm using squid (4.6) on my server (debianedu buster LTSP), and I'm
>> trying to configure a parent proxy.
>>
>> At first, when I configure the client's firefox (manual proxy
>> configuration) with the ip and port of the parent proxy, it's ok, I can
>> surf on the internet.?
>>
>> But I would like to configure my server's Squid Proxy to forward to a
>> parent proxy (172.16.103.254:3128)
>> -> So I add these two lines at the end of squid.conf:
>> cache_peer 172.16.103.254 parent 3128 0 no-query no-digest
>> never_direct allow all
>>
>> -> And restart squid. It seems to be ok:
>> # cat /var/log/squid/cache.log
>> (.....)
>> 2020/06/23 09:51:12 kid1| Configuring Parent 172.16.103.254/3128/0
>> (....)
>>
>> -> Then I configure firefox to use system proxy settings, but when I try
>> to google something or visit debian-fr.org, it doesn't work (no reponse
>> from the proxy).
>>
>
> That is odd. The log shows a 403 response being delivered by the parent
> proxy and delivered to Firefox.
>
> Browsers refuse to display proxy responses on CONNECT requests. So the
> first is expected. But the second one using http:// should be shown.
>
>
>> But my squid's configuration seems to be ok:
>> # cat /var/log/squid/access.log
>> (....)
>> 1592921221.753??? 138 10.0.2.2 TCP_TUNNEL/403 361
>> CONNECT?www.google.com:443 <http://www.google.com:443/>?-
>> FIRSTUP_PARENT/172.16.103.254 -
>> 1592921275.641??? 521 10.0.2.2 TCP_MISS/403 4289
>> GET?http://www.debian-fr.org/?- FIRSTUP_PARENT/172.16.103.254 text/html
>> 1592921275.692????? 0 10.0.2.2 TCP_HIT/200 13072 GET
>> (...)
>>
>> Is it possible that the squid parent refuse to have "a child" ?
>>
>
> Maybe. You will need to know the parent proxy configuration to tell
> that. All that is visible from the detail you have shown is that parent
> proxy has forbidden the requests it is receiving.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200627/1ddabb15/attachment.htm>

From mikio.kishi at gmail.com  Sat Jun 27 07:07:23 2020
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Sat, 27 Jun 2020 16:07:23 +0900
Subject: [squid-users] Trusted first verification regarding cross root cert
Message-ID: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>

Hi all,

I am currently using sslbump feature. Sometimes, squid failed to verify a
https web site with
cross root cert. On the other hand, the site is accessible directly from
major web browsers,
such as chrome and firefox. I am guessing that the cert verification
handling of the current
sslbump seems to be NOT trusted_first mode. Are there any solutions to
change to trusted_first
verification mode for squid ?

Regards,
--
Mikio Kishi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200627/006fac9b/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 27 12:25:47 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Jun 2020 00:25:47 +1200
Subject: [squid-users] Trusted first verification regarding cross root
 cert
In-Reply-To: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>
References: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>
Message-ID: <0ad55353-cee6-1cb6-be4f-db92a29aad44@treenet.co.nz>

On 27/06/20 7:07 pm, mikio.kishi wrote:
> Hi all,
> 
> I am currently using sslbump feature. Sometimes, squid failed to verify
> a https web site with
> cross root cert. On the other hand, the site is accessible directly from
> major web browsers,
> such as chrome and firefox. I am guessing that the cert verification
> handling of the current
> sslbump seems to be NOT trusted_first mode. Are there any solutions to
> change to trusted_first
> verification mode for squid ?
> 

Solutions based purely on guesswork are unlikely to work.


Missing information:

 * Squid version

 * details of the chain being delivered to Squid

 * details of the expected cross-signing chain(s).

 * by "trusted_first mode" do you mean TOFU or something else?


Squid supports a helper, which can to do any type of validation -
including none. BUT ... you first need to eliminate the guesses to see
if it is a validation or something completely unexpected.


Amos


From uhlar at fantomas.sk  Sat Jun 27 13:29:58 2020
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 27 Jun 2020 15:29:58 +0200
Subject: [squid-users] Help
In-Reply-To: <CABi3FSkGcwg43BYL8ehV2jrHbZV=tOoEW=WYhm08Yw+avKQ-ww@mail.gmail.com>
References: <CABi3FSkGcwg43BYL8ehV2jrHbZV=tOoEW=WYhm08Yw+avKQ-ww@mail.gmail.com>
Message-ID: <20200627132958.GB14658@fantomas.sk>

On 30.05.20 16:35, santosh panchal wrote:
>We have setup outbound proxy in AWS for private infra
>
>We have put required entry in /etc/profile

what "required entry"?

> and try to install package on
>ubuntu machine but getting error as it is not going over the internet
>
>Error
>Connecting to AP-south-1.ec2.archive.ubuntu.com
>
>Also we are unable to curl google.com even after whitelisting the domain in
>Cloudformation template

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
REALITY.SYS corrupted. Press any key to reboot Universe.


From tarotapprentice at yahoo.com  Sat Jun 27 14:27:43 2020
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 28 Jun 2020 00:27:43 +1000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:7 Cache
	Poisoning Issue in HTTP Request processing
In-Reply-To: <6fddb72c-8524-699f-455f-31ad31097585@treenet.co.nz>
References: <6fddb72c-8524-699f-455f-31ad31097585@treenet.co.nz>
Message-ID: <55C1D173-A26F-4341-AEF5-50E783A271F6@yahoo.com>

Any plans to get this into Debian, or if they?ll apply the patch to 4.11?

Cheers
MarkJ

> On 27 Jun 2020, at 2:45 am, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> ?__________________________________________________________________
> 
> Squid Proxy Cache Security Update Advisory SQUID-2020:7
> __________________________________________________________________
> 
> Advisory ID:       | SQUID-2020:7
> Date:              | June 26, 2020
> Summary:           | Cache Poisoning Issue
>                    | in HTTP Request processing.
> Affected versions: | Squid 2.x -> 2.7.STABLE9
>                    | Squid 3.x -> 3.5.28
>                    | Squid 4.x -> 4.11
>                    | Squid 5.x -> 5.0.2
> Fixed in version:  | Squid 4.12 and 5.0.3
> __________________________________________________________________
> 
> <https://github.com/squid-cache/squid/security/advisories/GHSA-qf3v-rc95-96j5>
> <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15049>
> __________________________________________________________________
> 
> Problem Description:
> 
> Due to incorrect input validation Squid is vulnerable to a
> Request Smuggling and Poisoning attack against the HTTP cache.
> 
> __________________________________________________________________
> 
> Severity:
> 
> This problem allows a trusted client to perform request smuggling
> and poison the HTTP cache contents with crafted HTTP(S) request
> messages.
> 
> This attack requires an upstream server to participate in the
> smuggling and generate the poison response sequence. Most popular
> server software are not vulnerable to participation in this
> attack.
> 
> CVSS Score of 9.3
> <https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H/E:F/RL:O/RC:C/CR:H/IR:H/AR:H/MAV:N/MAC:L/MPR:L/MUI:N/MS:C/MC:H/MI:H/MA:H&version=3.1>
> 
> __________________________________________________________________
> 
> Updated Packages:
> 
> This bug is fixed by Squid versions 4.12 and 5.0.3.
> 
> In addition, patches addressing this problem for the stable
> releases can be found in our patch archives:
> 
> Squid 4:
> <http://www.squid-cache.org/Versions/v4/changesets/squid-4-ea12a34d338b962707d5078d6d1fc7c6eb119a22.patch>
> 
> Squid 5:
> <http://www.squid-cache.org/Versions/v5/changesets/squid-5-485c9a7bb1bba88754e07ad0094647ea57a6eb8d.patch>
> 
> If you are using a prepackaged version of Squid then please refer
> to the package vendor for availability information on updated
> packages.
> 
> __________________________________________________________________
> 
> Determining if your version is vulnerable:
> 
> All Squid-3.x up to and including 3.5.28 are vulnerable.
> 
> All Squid-4.x up to and including 4.11 are vulnerable.
> 
> Squid-5.0.1 and 5.0.2 are vulnerable.
> 
> __________________________________________________________________
> 
> Workaround:
> 
> There is no workaround for this vulnerability.
> 
> __________________________________________________________________
> 
> Contact details for the Squid project:
> 
> For installation / upgrade support on binary packaged versions
> of Squid: Your first point of contact should be your binary
> package vendor.
> 
> If you install and build Squid from the original Squid sources
> then the <squid-users at lists.squid-cache.org> mailing list is your
> primary support point. For subscription details see
> <http://www.squid-cache.org/Support/mailing-lists.html>.
> 
> For reporting of non-security bugs in the latest STABLE release
> the squid bugzilla database should be used
> <http://bugs.squid-cache.org/>.
> 
> For reporting of security sensitive bugs send an email to the
> <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
> list (though anyone can post) and security related bug reports
> are treated in confidence until the impact has been established.
> 
> __________________________________________________________________
> 
> Credits:
> 
> This vulnerability was discovered by Alex Rousskov of The
> Measurement Factory.
> 
> Independent discovery and replication reported by Amit Klein of
> Safebreach.
> 
> Fixed by Alex Rousskov of The Measurement Factory.
> 
> __________________________________________________________________
> 
> Revision history:
> 
> 
> 2016-09-06 02:45:20 UTC Initial Report
> 2020-05-11 12:41:17 UTC Replication Reported
> 2020-05-13 14:05:00 UTC Patch Released
> 2020-06-25 11:15:10 UTC CVE Allocated
> ______________________
> END
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat Jun 27 14:54:18 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Jun 2020 02:54:18 +1200
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:7 Cache
 Poisoning Issue in HTTP Request processing
In-Reply-To: <55C1D173-A26F-4341-AEF5-50E783A271F6@yahoo.com>
References: <6fddb72c-8524-699f-455f-31ad31097585@treenet.co.nz>
 <55C1D173-A26F-4341-AEF5-50E783A271F6@yahoo.com>
Message-ID: <bdbf8f6f-01b0-48c0-0446-ae409a4c19dd@treenet.co.nz>

On 28/06/20 2:27 am, TarotApprentice wrote:
> Any plans to get this into Debian, or if they?ll apply the patch to 4.11?
> 

v4.12 package is already being worked on. I'm not sure of ETA though,
its already taken longer than usual.

Can't speak for the security team about the stable Debian packages.


Amos


From yannick.rousseau at tutanota.com  Sat Jun 27 15:09:26 2020
From: yannick.rousseau at tutanota.com (yannick.rousseau at tutanota.com)
Date: Sat, 27 Jun 2020 17:09:26 +0200 (CEST)
Subject: [squid-users] no response from the proxy squid parent
In-Reply-To: <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>
References: <MAXYaDB--3-2@tutanota.com>
 <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>
Message-ID: <MAqIdTJ--3-2@tutanota.com>

Hi, 
I've noticed one more difference between the CONNECT packets (it appears in the HTTP layer):

--> client's firefox configured with the ip and port of the parent proxy (172.16.103.254:3128), surf on the Net ok:

Frame 58: 255 bytes on wire (2040 bits), 255 bytes captured (2040 bits) on interface eth1, id 0
Ethernet II, Src: D-LinkIn_79:24:ed (ac:f1:df:79:24:ed), Dst: VMware_92:8a:f2 (00:0c:29:92:8a:f2)
Internet Protocol Version 4, Src: 172.16.103.101, Dst: 172.16.103.254
Transmission Control Protocol, Src Port: 35604, Dst Port: 3128, Seq: 1, Ack: 1, Len: 201
Hypertext Transfer Protocol
??? CONNECT www.google.com:443 <http://www.google.com:443> HTTP/1.1\r\n
??? User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0\r\n
??? Proxy-Connection: keep-alive\r\n
??? Connection: keep-alive\r\n
??? Host: www.google.com:443\r\n <http://www.google.com:443\r\n>
??? \r\n
??? [Full request URI: www.google.com:443 <http://www.google.com:443>]
??? [HTTP request 1/1]
??? [Response in frame: 62]


--> client's firefox configured to use system proxy settings (can't surf on the Net):

Frame 620: 295 bytes on wire (2360 bits), 295 bytes captured (2360 bits) on interface eth1, id 0
Ethernet II, Src: D-LinkIn_79:24:ed (ac:f1:df:79:24:ed), Dst: VMware_92:8a:f2 (00:0c:29:92:8a:f2)
Internet Protocol Version 4, Src: 172.16.103.101, Dst: 172.16.103.254
Transmission Control Protocol, Src Port: 35528, Dst Port: 3128, Seq: 1, Ack: 1, Len: 241
Hypertext Transfer Protocol
??? CONNECT www.google.com:443 <http://www.google.com:443> HTTP/1.1\r\n
??? User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0\r\n
??? Host: www.google.com:443\r\n <http://www.google.com:443\r\n>
??? Via: 1.1 tjener.intern (squid/4.6)\r\n
??? X-Forwarded-For: 10.0.2.2\r\n????? -> request field introduit par squid
??? Cache-Control: max-age=259200\r\n? -> si on vire le cache dans la config, tjs l? ?
??? \r\n
??? [Full request URI: www.google.com:443 <http://www.google.com:443>]
??? [HTTP request 1/1]
??? [Response in frame: 624]


Remarks: tjener.inter is my server with squid (172.16.103.101) and 172.16.103.254:3128 is the parent.
????????????????? 10.0.2.2 is the IP of the client.

If you have any idea to help me to fix this ....

Thanks for your answer.

Yannick
-- 
Securely sent with Tutanota. Get your own encrypted, ad-free mailbox: 
https://tutanota.com


Jun 26, 2020, 07:11 by squid3 at treenet.co.nz:

> On 24/06/20 7:27 am, yannick.rousseau at tutanota.com wrote:
>
>> Hi,?
>>
>> I'm using squid (4.6) on my server (debianedu buster LTSP), and I'm
>> trying to configure a parent proxy.
>>
>> At first, when I configure the client's firefox (manual proxy
>> configuration) with the ip and port of the parent proxy, it's ok, I can
>> surf on the internet.?
>>
>> But I would like to configure my server's Squid Proxy to forward to a
>> parent proxy (172.16.103.254:3128)
>> -> So I add these two lines at the end of squid.conf:
>> cache_peer 172.16.103.254 parent 3128 0 no-query no-digest
>> never_direct allow all
>>
>> -> And restart squid. It seems to be ok:
>> # cat /var/log/squid/cache.log
>> (.....)
>> 2020/06/23 09:51:12 kid1| Configuring Parent 172.16.103.254/3128/0
>> (....)
>>
>> -> Then I configure firefox to use system proxy settings, but when I try
>> to google something or visit debian-fr.org, it doesn't work (no reponse
>> from the proxy).
>>
>
> That is odd. The log shows a 403 response being delivered by the parent
> proxy and delivered to Firefox.
>
> Browsers refuse to display proxy responses on CONNECT requests. So the
> first is expected. But the second one using http:// should be shown.
>
>
>> But my squid's configuration seems to be ok:
>> # cat /var/log/squid/access.log
>> (....)
>> 1592921221.753??? 138 10.0.2.2 TCP_TUNNEL/403 361
>> CONNECT?www.google.com:443 <http://www.google.com:443/>?-
>> FIRSTUP_PARENT/172.16.103.254 -
>> 1592921275.641??? 521 10.0.2.2 TCP_MISS/403 4289
>> GET?http://www.debian-fr.org/?- FIRSTUP_PARENT/172.16.103.254 text/html
>> 1592921275.692????? 0 10.0.2.2 TCP_HIT/200 13072 GET
>> (...)
>>
>> Is it possible that the squid parent refuse to have "a child" ?
>>
>
> Maybe. You will need to know the parent proxy configuration to tell
> that. All that is visible from the detail you have shown is that parent
> proxy has forbidden the requests it is receiving.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200627/2c538d7b/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 27 15:49:01 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Jun 2020 03:49:01 +1200
Subject: [squid-users] no response from the proxy squid parent
In-Reply-To: <MAqIdTJ--3-2@tutanota.com>
References: <MAXYaDB--3-2@tutanota.com>
 <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>
 <MAqIdTJ--3-2@tutanota.com>
Message-ID: <f92ab89d-f4f6-3cef-ac7b-0ab5adde5070@treenet.co.nz>

On 28/06/20 3:09 am, yannick.rousseau at tutanota.com wrote:
> Hi,
> I've noticed one more difference between the CONNECT packets (it appears
> in the HTTP layer):
> 
...
> 
> If you have any idea to help me to fix this ....
> 

You can try adding this to squid.conf:

 forwarded_for transparent


Amos


From stefano.tabacchiera at IGT.com  Sat Jun 27 18:31:08 2020
From: stefano.tabacchiera at IGT.com (Tabacchiera, Stefano)
Date: Sat, 27 Jun 2020 18:31:08 +0000
Subject: [squid-users] squid stores multiple copies of identical ETags
In-Reply-To: <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
 <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
Message-ID: <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>

>>>In general, if the same object occuring N times on disk is a problem,
>>>you have issues with misconfigured cache_dir parameters. eg the
>>> cache_dir size is too big for the physical disk it is stored on.    >

 >> That's the point. There's  a LOT of identical objects on disk.
 >> I have 2x100gb dedicated disks, ext4 noatime.
 >> Each cache_dir is aufs 80000 16 256.
 >> Where's the issue? I did't even imagine this would lead a multiple
 >> copies stores of the same object.

   >  So far the problem appears to be you not understanding how caching
   >  works. My previous response contains the explanation that should have
   >  resolved that.

Amos, I'm sorry, but I'm still confused.

Please follow me on this:
Consider a cachable object, e.g. a static image, with all its response headers set (content-length/last-modified/etag/expiration/etc).
When the client requests it with "no-cache", it prevents squid from providing the cached on-disk object, and it forces the retrieve of a new copy from the origin server.
So far, so good.
But THIS new copy is the same identical object which is already on disk (same url/size/etc.), 'cause the client is requesting the same object many times per second, all day.

I understand that squid must serve the new copy to the client (no-cache), but what I don't get is why squid is storing every time a new copy of THIS object on disk.
In my (maybe faulty) understandings this could be avoided, by simply look up in the store log and find that this particular object already exists on disk.

Since this doesn't seem to be happening, chances are: squid doesn't care about storing multiple copies on disk OR (more probably) I'm still missing something vital.

In the real case, the object is a JSON which is modified every 5 minutes. Every times it changes, obviously it has a new Etag, new Last-modified, a proper content-length, etc.
Client requests it like 10 times per sec: 10*300 ~ 3000 copies on disk. Consider a mean object size of 500KB: 3000*500KB = 1.4GB.
A single object is wasting 1GB of disk space every 5'. Indeed, during a restart, squid does a lot of purging of duplicate objects.
Is this really necessary? I don't see the point.

You mentioned the cache_dir parameters, like the cache size compared to disk size, or L1/L2 ratio.
Can you please be more specific or point me at the right documentation?
I'd appreciate a lot your help.

Thanks
ST

____________________________________________________________________________________ La presente comunicazione ed i suoi allegati e' destinata esclusivamente ai destinatari. Qualsiasi suo utilizzo, comunicazione o diffusione non autorizzata e' proibita. Se ha ricevuto questa comunicazione per errore, la preghiamo di darne immediata comunicazione al mittente e di cancellare tutte le informazioni erroneamente acquisite. Grazie This message and its attachments are intended only for use by the addressees. Any use, re-transmission or dissemination not authorized of it is prohibited. If you received this e-mail in error, please inform the sender immediately and delete all the material. Thank you.

From rousskov at measurement-factory.com  Sat Jun 27 20:59:45 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Jun 2020 16:59:45 -0400
Subject: [squid-users] squid stores multiple copies of identical ETags
In-Reply-To: <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
 <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
 <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>
Message-ID: <4954f8e0-3b55-ccda-7aa8-2e0a706947ff@measurement-factory.com>

On 6/27/20 2:31 PM, Tabacchiera, Stefano wrote:

> Consider a cachable object ... with all its response headers set
> (content-length/last-modified/etag/expiration/etc). When the client
> requests it with "no-cache", it prevents squid from providing the
> cached on-disk object, and it forces the retrieve of a new copy from
> the origin server.

> But THIS new copy is the same identical object which is already on
> disk (same url/size/etc.)

Squid does not know that the response headers and body have not changed.
Squid could, in theory, trust the URL+Vary+ETag+etc. combination as a
precise response identity, but it is a bit risky to do that by default
because ETags/etc. might lie. There is currently no code implementing
that optimization either.


> In my (maybe faulty) understandings this could be avoided, by simply
> look up in the store log and find that this particular object already
> exists on disk.

Squid could do that if it trusts ETag/etc and updates stored headers.
Squid does not do that (yet?). Even the header update part is not fully
supported yet!


> Since this doesn't seem to be happening, chances are: squid doesn't
> care about storing multiple copies on disk

To be more accurate, Squid does not store multiple copies of (what Squid
considers to be) the same response -- only one object can be indexed per
URL/Vary. Bugs notwithstanding, Squid will overwrite the old response
(for some definition of "overwrite") with the new one.

I do not know much about aufs -- that code has been neglected for a
while -- but perhaps aufs simply does not have enough time to delete its
old/unused files? Try setting cache_swap_low and cache_swap_high to the
same very low value, perhaps even zero (to avoid backgrounding the
cleanup task).



HTH,

Alex.


From ngtech1ltd at gmail.com  Sun Jun 28 11:55:40 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 28 Jun 2020 14:55:40 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB2756A4397BC6A6C95B4CACDC93920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <22186F7E-2BC4-44CD-940A-02072A2D92E6@hxcore.ol>,
 <AM5PR0102MB2756B10588909C4F4635B23A93980@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <B2FC1ADC-2403-4FD8-936C-C830DD22AFB3@hxcore.ol>
 <AM5PR0102MB2756AE61B5D0035049B9BC7993920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <AM5PR0102MB2756A4397BC6A6C95B4CACDC93920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <13040F9E-CAF9-4E99-B0F6-ED8927C72545@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ADB6CA0082E44852A5D9B743B4F0452E.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0002.png>

From ngtech1ltd at gmail.com  Sun Jun 28 11:55:40 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 28 Jun 2020 14:55:40 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB2756A4397BC6A6C95B4CACDC93920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <22186F7E-2BC4-44CD-940A-02072A2D92E6@hxcore.ol>,
 <AM5PR0102MB2756B10588909C4F4635B23A93980@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <B2FC1ADC-2403-4FD8-936C-C830DD22AFB3@hxcore.ol>
 <AM5PR0102MB2756AE61B5D0035049B9BC7993920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <AM5PR0102MB2756A4397BC6A6C95B4CACDC93920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <13040F9E-CAF9-4E99-B0F6-ED8927C72545@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0001.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ADB6CA0082E44852A5D9B743B4F0452E.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0003.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 135 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0004.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 132 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/f3ae6cb1/attachment-0005.png>

From patrick.chemla at performance-managers.com  Sun Jun 28 14:56:45 2020
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Sun, 28 Jun 2020 17:56:45 +0300
Subject: [squid-users] squid+apache+php-fpm How squid could mark DEAD a peer
 where Apache is alive and php-fpm dead?
Message-ID: <a3463986-6dbd-42f8-089f-b88bc63e49f2@performance-managers.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200628/23b18251/attachment.htm>

From rousskov at measurement-factory.com  Sun Jun 28 16:44:14 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 28 Jun 2020 12:44:14 -0400
Subject: [squid-users] squid+apache+php-fpm How squid could mark DEAD a
 peer where Apache is alive and php-fpm dead?
In-Reply-To: <a3463986-6dbd-42f8-089f-b88bc63e49f2@performance-managers.com>
References: <a3463986-6dbd-42f8-089f-b88bc63e49f2@performance-managers.com>
Message-ID: <acb0f91a-854e-f5d7-1f41-658bc2b3ea5c@measurement-factory.com>

On 6/28/20 10:56 AM, Patrick Chemla wrote:

> I got last week a bad "issue" where a squid Version 4.0.23 

Please upgrade to the latest Squid v4 (or v5). While the problem you are
writing about is unlikely to disappear, there are other problems in your
Squid version. My answer is based on Squid v5, but I hope it applies to
the latest v4 as well.


> But how to tell squid to mark dead a peer that returns 504 code ?

I did not check closely, but I believe that Squid's peer viability
decisions are based primarily on transport layer problems -- valid HTTP
responses will not mark a peer dead, regardless of the HTTP status code.

Modern Squids should reforward HTTP 504 responses to other peers (where
possible). See the last bullet in reforwarding _exceptions_ list at
https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F

Please note that your configuration may not have any peers eligible for
reforwarding the request to. For example, if you have no parent caches,
then a sourcehash peer will be the only candidate, with no reforwarding
candidates at all AFAICT. The algorithm that computes the list of
peering candidates (including candidates for reforwarding) is documented
at
https://wiki.squid-cache.org/Features/LoadBalance#Overall_peer_selection_logic

If reforwarding is not good enough in your use case, you should be able
to ban a peer using cache_peer_access ACLs. The trigger for a ban would
probably be an annotate_transaction ACL in http_reply_access. Turning
the ban off would probably require an external ACL (or adding a
counter-like ACL support to Squid itself).

Alternatively, one could enhance Squid with a new ACL-driven
cache_peer_failure directive that would allow the admin to determine
which valid HTTP responses should qualify for marking a cache peer as
dead. Adding this feature would be straightforward AFAICT. It would be a
generally useful option. IIRC, there were requests for a similar feature
in the past. Quality pull requests or development sponsorship should be
welcomed IMO.


HTH,

Alex.


From stefano.tabacchiera at IGT.com  Sun Jun 28 18:41:12 2020
From: stefano.tabacchiera at IGT.com (Tabacchiera, Stefano)
Date: Sun, 28 Jun 2020 18:41:12 +0000
Subject: [squid-users] squid stores multiple copies of identical ETags
In-Reply-To: <4954f8e0-3b55-ccda-7aa8-2e0a706947ff@measurement-factory.com>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
 <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
 <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>
 <4954f8e0-3b55-ccda-7aa8-2e0a706947ff@measurement-factory.com>
Message-ID: <95925593-DC27-4E8E-BE57-A34ABA332523@igt.com>

Alex,
first of all, thank you for your clarification.

>    Squid does not know that the response headers and body have not changed.
>    Squid could, in theory, trust the URL+Vary+ETag+etc. combination as a
>    precise response identity, but it is a bit risky to do that by default
>    because ETags/etc. might lie. There is currently no code implementing
>    that optimization either.

Ok, now I know that only URL/Vary are taken into account when storing an object.

>    To be more accurate, Squid does not store multiple copies of (what Squid
>    considers to be) the same response -- only one object can be indexed per
>    URL/Vary. Bugs notwithstanding, Squid will overwrite the old response
>    (for some definition of "overwrite") with the new one.

Since in my case there's no Vary header and the object Full URL never changes,
I',m starting to think about a bug (?!).

>    I do not know much about aufs -- that code has been neglected for a
>    while -- but perhaps aufs simply does not have enough time to delete its
>    old/unused files? Try setting cache_swap_low and cache_swap_high to the
>    same very low value, perhaps even zero (to avoid backgrounding the
>  cleanup task).

Uhm, are you saying that the the process of an object replacement on disk is not atomic?
I mean: squid would store a new copy of the object while leaving the old copy deletion to cleanup task?
If it's not, I still suspect a bug.
I'm hesitant to to turn the store_log on, 'cause the performance impact.
Btw, is there a specific debug_option?

Thanks a lot.
ST


____________________________________________________________________________________ La presente comunicazione ed i suoi allegati e' destinata esclusivamente ai destinatari. Qualsiasi suo utilizzo, comunicazione o diffusione non autorizzata e' proibita. Se ha ricevuto questa comunicazione per errore, la preghiamo di darne immediata comunicazione al mittente e di cancellare tutte le informazioni erroneamente acquisite. Grazie This message and its attachments are intended only for use by the addressees. Any use, re-transmission or dissemination not authorized of it is prohibited. If you received this e-mail in error, please inform the sender immediately and delete all the material. Thank you.

From rousskov at measurement-factory.com  Sun Jun 28 21:07:21 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 28 Jun 2020 17:07:21 -0400
Subject: [squid-users] squid stores multiple copies of identical ETags
In-Reply-To: <95925593-DC27-4E8E-BE57-A34ABA332523@igt.com>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
 <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
 <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>
 <4954f8e0-3b55-ccda-7aa8-2e0a706947ff@measurement-factory.com>
 <95925593-DC27-4E8E-BE57-A34ABA332523@igt.com>
Message-ID: <1cac51ab-30ba-b196-a96d-87e7bc7cd1bb@measurement-factory.com>

On 6/28/20 2:41 PM, Tabacchiera, Stefano wrote:

>>    Squid does not know that the response headers and body have not changed.
>>    Squid could, in theory, trust the URL+Vary+ETag+etc. combination as a
>>    precise response identity, but it is a bit risky to do that by default
>>    because ETags/etc. might lie. There is currently no code implementing
>>    that optimization either.

> now I know that only URL/Vary are taken into account when storing an object.

For the record: I did not say or imply that only URL/Vary are taken into
account. Other request aspects such as request method also influence the
cache key.


> I mean: squid would store a new copy of the object while leaving the
> old copy deletion to cleanup task?

Some parts of the cleanup process may be delegated. The details depend
on the cache_dir type. I do not know or remember aufs specifics, but I
suspect that all ufs-based cache_dirs, including aufs, use lazy garbage
collection (under normal circumstances). The cache_swap_low and
cache_swap_high directives should determine what is "normal".


> I'm hesitant to to turn the store_log on, 'cause the performance impact.
> Btw, is there a specific debug_option?

FWIW, I would not try to debug this on a live/production cache,
especially if you are not used to navigating debugging cache.logs from
busy proxies. The same bug (if any) should be reproducible in lab settings.

These debugging sections may be relevant (but this is not meant as a
comprehensive list, and I do not know what exactly you need to look at):

> doc/debug-sections.txt:section 20    Memory Cache
> doc/debug-sections.txt:section 20    Storage Manager MD5 Cache Keys
> doc/debug-sections.txt:section 20    Store Controller
> doc/debug-sections.txt:section 32    Asynchronous Disk I/O
> doc/debug-sections.txt:section 43    AIOPS
> doc/debug-sections.txt:section 47    Store Directory Routines
> doc/debug-sections.txt:section 81    Store HEAP Removal Policies

Most of the AUFS/AIO code is in src/DiskIO/DiskThreads


HTH,

Alex.


From mikio.kishi at gmail.com  Mon Jun 29 07:29:43 2020
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Mon, 29 Jun 2020 16:29:43 +0900
Subject: [squid-users] Trusted first verification regarding cross root
	cert
In-Reply-To: <0ad55353-cee6-1cb6-be4f-db92a29aad44@treenet.co.nz>
References: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>
 <0ad55353-cee6-1cb6-be4f-db92a29aad44@treenet.co.nz>
Message-ID: <CAMUMefYPEbzd8+73-oNYHXxFKGt_kTGnRmNWWutOJ8vv0=71Xw@mail.gmail.com>

Hi Amos,

Thank you for your reply and I apologize for the missing information.
The following is the detailed one.

> * Squid version
* squid version 3.5.26 (probably, ver4.X also might have same issue)
* OpenSSL 1.0.2k

> * details of the chain being delivered to Squid
> * details of the expected cross-signing chain(s).

There are so many websites which are facing this issue.
For instance, "sbv.gov.vn:443".

# openssl s_client -connect sbv.gov.vn:443 -servername sbv.gov.vn
-showcerts -verify 5 -state
verify depth is 5
CONNECTED(00000003)
SSL_connect:before/connect initialization
SSL_connect:SSLv2/v3 write client hello A
SSL_connect:SSLv3 read server hello A
depth=3 C = BE, O = GlobalSign nv-sa, OU = Root CA, CN = GlobalSign Root CA
verify return:1
depth=2 OU = GlobalSign Root CA - R3, O = GlobalSign, CN = GlobalSign
verify error:num=10:certificate has expired
notAfter=Mar 18 10:00:00 2019 GMT
verify return:1
depth=2 OU = GlobalSign Root CA - R3, O = GlobalSign, CN = GlobalSign
notAfter=Mar 18 10:00:00 2019 GMT
verify return:1
depth=1 C = BE, O = GlobalSign nv-sa, CN = GlobalSign Extended Validation
CA - SHA256 - G3
notAfter=Sep 21 00:00:00 2026 GMT
verify return:1
depth=0 businessCategory = Government Entity, serialNumber = Government
Entity, jurisdictionC = VN, C = VN, ST = Ha Noi, L = Ha Noi, street =
"47-49 Ly Thai To, Hoan Kiem District", OU = Department of Information
Technology, O = The State Bank of Viet Nam, CN = www.sbv.gov.vn
notAfter=Nov  8 03:31:58 2020 GMT
verify return:1
... snip ...
    Verify return code: 10 (certificate has expired)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The above verification was NG(certificate has expired))
On the other hand, the verification was OK if  the "-trusted_first" option
was given.

# openssl s_client -trusted_first -connect sbv.gov.vn:443 -servername
sbv.gov.vn -showcerts -verify 5 -state
verify depth is 5
CONNECTED(00000003)
SSL_connect:before/connect initialization
SSL_connect:SSLv2/v3 write client hello A
SSL_connect:SSLv3 read server hello A
depth=2 OU = GlobalSign Root CA - R3, O = GlobalSign, CN = GlobalSign
verify return:1
depth=1 C = BE, O = GlobalSign nv-sa, CN = GlobalSign Extended Validation
CA - SHA256 - G3
verify return:1
depth=0 businessCategory = Government Entity, serialNumber = Government
Entity, jurisdictionC = VN, C = VN, ST = Ha Noi, L = Ha Noi, street =
"47-49 Ly Thai To, Hoan Kiem District", OU = Department of Information
Technology, O = The State Bank of Viet Nam, CN = www.sbv.gov.vn
verify return:1
... snip ...
    Verify return code: 0 (ok)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^

In the "Cross-Signed Certificate" case, openssl failed to verify by default
even if  another signed root is available.
Squid's behavior seems to be also the same. That's why I needed the
"trusted_first" feature.
For your information, a major web browser(like chrome/firefox) could access
the site directly because of trusted first mode.

In my opinion, appending the following codes(in ssl/support.cc) will be
effective.

 X509_VERIFY_PARAM_set_flags(ctx->param, X509_V_FLAG_TRUSTED_FIRST);
 (The type of ctx is "X509_STORE_CTX *").

Could you please add the trusted_first option on squid ?

By the way, I think that the following topic is also the same issue.
 [squid-users] (92) Protocol error (TLS code: X509_V_ERR_CERT_HAS_EXPIRED)

Regards,
--
Mikio Kishi

On Sat, Jun 27, 2020 at 9:29 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 27/06/20 7:07 pm, mikio.kishi wrote:
> > Hi all,
> >
> > I am currently using sslbump feature. Sometimes, squid failed to verify
> > a https web site with
> > cross root cert. On the other hand, the site is accessible directly from
> > major web browsers,
> > such as chrome and firefox. I am guessing that the cert verification
> > handling of the current
> > sslbump seems to be NOT trusted_first mode. Are there any solutions to
> > change to trusted_first
> > verification mode for squid ?
> >
>
> Solutions based purely on guesswork are unlikely to work.
>
>
> Missing information:
>
>  * Squid version
>
>  * details of the chain being delivered to Squid
>
>  * details of the expected cross-signing chain(s).
>
>  * by "trusted_first mode" do you mean TOFU or something else?
>
>
> Squid supports a helper, which can to do any type of validation -
> including none. BUT ... you first need to eliminate the guesses to see
> if it is a validation or something completely unexpected.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200629/a57d3e24/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 29 09:51:55 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Jun 2020 21:51:55 +1200
Subject: [squid-users] Trusted first verification regarding cross root
 cert
In-Reply-To: <CAMUMefYPEbzd8+73-oNYHXxFKGt_kTGnRmNWWutOJ8vv0=71Xw@mail.gmail.com>
References: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>
 <0ad55353-cee6-1cb6-be4f-db92a29aad44@treenet.co.nz>
 <CAMUMefYPEbzd8+73-oNYHXxFKGt_kTGnRmNWWutOJ8vv0=71Xw@mail.gmail.com>
Message-ID: <7a3bee09-1a80-180f-7070-a3028655dd9a@treenet.co.nz>

On 29/06/20 7:29 pm, mikio.kishi wrote:
> Hi Amos,
> 
> Thank you for your reply and I apologize for the missing information.
> The following is the detailed?one.
> 
>> * Squid version
> * squid version 3.5.26 (probably, ver4.X also might have same issue)
> * OpenSSL 1.0.2k
> 
>> * details of the chain being delivered to Squid
>> * details of the expected cross-signing chain(s).
> 
> There are so many websites which are facing this issue.
> For instance, "sbv.gov.vn:443 <http://sbv.gov.vn:443>".
> 
> # openssl s_client -connect sbv.gov.vn:443 <http://sbv.gov.vn:443>
> -servername sbv.gov.vn <http://sbv.gov.vn> -showcerts -verify 5 -state
> verify depth is 5

...
> 
> Could you please add the trusted_first option on squid ?
> 

Ah. This is a feature of OpenSSL v1.1. Apparently your OpenSSL v1.0 has
had the feature *partially* backported to it.

I suggest you upgrade to Squid-4 and build against OpenSSL v1.1 where
this "feature" is the default behaviour. Squid-3 is no longer supported
for code updates.


Amos


From mikio.kishi at gmail.com  Mon Jun 29 10:30:34 2020
From: mikio.kishi at gmail.com (mikio.kishi at gmail.com)
Date: Mon, 29 Jun 2020 19:30:34 +0900
Subject: [squid-users] Trusted first verification regarding cross root
	cert
In-Reply-To: <7a3bee09-1a80-180f-7070-a3028655dd9a@treenet.co.nz>
References: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>
 <0ad55353-cee6-1cb6-be4f-db92a29aad44@treenet.co.nz>
 <CAMUMefYPEbzd8+73-oNYHXxFKGt_kTGnRmNWWutOJ8vv0=71Xw@mail.gmail.com>
 <7a3bee09-1a80-180f-7070-a3028655dd9a@treenet.co.nz>
Message-ID: <CAMUMefZbUvL3J2gErBuw4BhbYazJSWLnoU-hmc_WCDzLENb9SA@mail.gmail.com>

Hi Amos,

>Ah. This is a feature of OpenSSL v1.1. Apparently your OpenSSL v1.0 has
>had the feature *partially* backported to it.
>I suggest you upgrade to Squid-4 and build against OpenSSL v1.1 where
>this "feature" is the default behaviour.

Yes, Exactly.  However, currently I am using CentOS7 which openssl package
version is still 1.0.....
Upgrading  openssl to v1.1.1 is challenging for me. Could you please
implement the rusted first option to squid-4 ? ...

Regards,
--
Mikio Kishi


On Mon, Jun 29, 2020 at 7:05 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 29/06/20 7:29 pm, mikio.kishi wrote:
> > Hi Amos,
> >
> > Thank you for your reply and I apologize for the missing information.
> > The following is the detailed one.
> >
> >> * Squid version
> > * squid version 3.5.26 (probably, ver4.X also might have same issue)
> > * OpenSSL 1.0.2k
> >
> >> * details of the chain being delivered to Squid
> >> * details of the expected cross-signing chain(s).
> >
> > There are so many websites which are facing this issue.
> > For instance, "sbv.gov.vn:443 <http://sbv.gov.vn:443>".
> >
> > # openssl s_client -connect sbv.gov.vn:443 <http://sbv.gov.vn:443>
> > -servername sbv.gov.vn <http://sbv.gov.vn> -showcerts -verify 5 -state
> > verify depth is 5
>
> ...
> >
> > Could you please add the trusted_first option on squid ?
> >
>
> Ah. This is a feature of OpenSSL v1.1. Apparently your OpenSSL v1.0 has
> had the feature *partially* backported to it.
>
> I suggest you upgrade to Squid-4 and build against OpenSSL v1.1 where
> this "feature" is the default behaviour. Squid-3 is no longer supported
> for code updates.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200629/c7527fab/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jun 29 12:14:27 2020
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 29 Jun 2020 15:14:27 +0300
Subject: [squid-users] Trusted first verification regarding cross root
	cert
In-Reply-To: <CAMUMefZbUvL3J2gErBuw4BhbYazJSWLnoU-hmc_WCDzLENb9SA@mail.gmail.com>
References: <CAMUMefa-_wQqLefO16bq6+vY0WmcC_CuNYJb1gWqiqQ=vJyTew@mail.gmail.com>
 <0ad55353-cee6-1cb6-be4f-db92a29aad44@treenet.co.nz>
 <CAMUMefYPEbzd8+73-oNYHXxFKGt_kTGnRmNWWutOJ8vv0=71Xw@mail.gmail.com>
 <7a3bee09-1a80-180f-7070-a3028655dd9a@treenet.co.nz>
 <CAMUMefZbUvL3J2gErBuw4BhbYazJSWLnoU-hmc_WCDzLENb9SA@mail.gmail.com>
Message-ID: <CABA8h=TG_TKj6g5HkWXOCiTHa1QVcLGb06ADguwYP1s9v359+Q@mail.gmail.com>

Upgrading to 1.1 on a running os is a challenge for any sysadmin.

Eliezer

On Mon, Jun 29, 2020, 13:30 <mikio.kishi at gmail.com> wrote:

> Hi Amos,
>
> >Ah. This is a feature of OpenSSL v1.1. Apparently your OpenSSL v1.0 has
> >had the feature *partially* backported to it.
> >I suggest you upgrade to Squid-4 and build against OpenSSL v1.1 where
> >this "feature" is the default behaviour.
>
> Yes, Exactly.  However, currently I am using CentOS7 which openssl package
> version is still 1.0.....
> Upgrading  openssl to v1.1.1 is challenging for me. Could you please
> implement the rusted first option to squid-4 ? ...
>
> Regards,
> --
> Mikio Kishi
>
>
> On Mon, Jun 29, 2020 at 7:05 PM Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 29/06/20 7:29 pm, mikio.kishi wrote:
>> > Hi Amos,
>> >
>> > Thank you for your reply and I apologize for the missing information.
>> > The following is the detailed one.
>> >
>> >> * Squid version
>> > * squid version 3.5.26 (probably, ver4.X also might have same issue)
>> > * OpenSSL 1.0.2k
>> >
>> >> * details of the chain being delivered to Squid
>> >> * details of the expected cross-signing chain(s).
>> >
>> > There are so many websites which are facing this issue.
>> > For instance, "sbv.gov.vn:443 <http://sbv.gov.vn:443>".
>> >
>> > # openssl s_client -connect sbv.gov.vn:443 <http://sbv.gov.vn:443>
>> > -servername sbv.gov.vn <http://sbv.gov.vn> -showcerts -verify 5 -state
>> > verify depth is 5
>>
>> ...
>> >
>> > Could you please add the trusted_first option on squid ?
>> >
>>
>> Ah. This is a feature of OpenSSL v1.1. Apparently your OpenSSL v1.0 has
>> had the feature *partially* backported to it.
>>
>> I suggest you upgrade to Squid-4 and build against OpenSSL v1.1 where
>> this "feature" is the default behaviour. Squid-3 is no longer supported
>> for code updates.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200629/2c2e25be/attachment.htm>

From info at schroeffu.ch  Mon Jun 29 15:13:24 2020
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Mon, 29 Jun 2020 15:13:24 +0000
Subject: [squid-users] Squid 4.11 Howto create SSL Bump certificates with
 only 3-12 months date of expiry
Message-ID: <3375f400073c90c55debdd21ab57da60@schroeffu.ch>

Hi Squid Community,

how can I configure Squid to create SSL Bump Certifications with only 3-12 months date of expiry?

Currently, Squid SSL bumped Certifications are valid 20 years in my case, way too long, as Apple & Google & Mozilla will trust only <1 Year SSL certifications in the future.

Thanks for any help!
Schroeffu

my conf:

http_port {{ inventory_hostname }}:{{ squid_port }} ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/(***).pem key=/etc/squid/certs/(***).pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
always_direct allow all
ssl_bump bump !domains_dont_sslbump
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200629/d71526b9/attachment.htm>

From anon.amish at gmail.com  Mon Jun 29 15:18:12 2020
From: anon.amish at gmail.com (Amish)
Date: Mon, 29 Jun 2020 20:48:12 +0530
Subject: [squid-users] Squid 4.12 Arch Linux Google Chrome fails - OpenSSL
 1.1.1g (was Re: SQUID 4.12 (Debian 10,
 OpenSSL 1.1.1d) - SSL bump no server helllo)
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
Message-ID: <17949f91-f48e-2558-bf76-969affa77d03@gmail.com>


On 16/06/20 1:13 pm, Lou?ansk? Luk?? wrote:
> But the client on the intercepted connection (via changed routing table under mikrotik and then prerouted to correct squid ports for http and ssl traffic) running Chrome 83 http://download.kjj.cz/pub/ssl/idnes.cz_chrome.83.0.4103.97.pcapng sends ClientHello - and no ServerHello is received. I've tcpdumped outgoing interface on the squid box - and there was no actual connection to the desired server.
> In the access.log there is something like 1592212170.495      2 10.0.0.40 NONE_ABORTED/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
>   
> But - same client, same network, same network running Firefox 77 http://download.kjj.cz/pub/ssl/idnes.cz_firefox.77.0.1.pcapng  gets ServerHello after it's ClientHello - they exchange information, exchange ciphers etc. and the web page is loaded. I've checked https certificate details - it's been issued by my CA.
>
>
> access.log:
>   
> 1592212156.764      8 10.0.0.40 TCP_MISS/301 196 GET http://idnes.cz/ - ORIGINAL_DST/185.17.117.32 -
> 1592212156.774      2 10.0.0.40 NONE/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
> 1592212156.825     38 10.0.0.40 TCP_MISS/302 777 GET https://idnes.cz/ - ORIGINAL_DST/185.17.117.32 text/html
> 1592212156.840      7 10.0.0.40 NONE/200 0 CONNECT 185.17.117.32:443 - HIER_NONE/- -
> 1592212156.893     28 10.0.0.40 TCP_CLIENT_REFRESH_MISS/200 40086 GET https://www.idnes.cz/ - ORIGINAL_DST/185.17.117.32 text/html
>
>
> So in Firefox - it seems to be working

I am using Arch Linux and today I upgraded squid to 4.12 (from 4.10)

I am observing very similar issue.

Clients make HTTPS request via CONNECT to port 8080.

I have configured SSL bump but it is "effectively" deactivated via 
following ACL

http_port 8080 ssl-bump generate-host-certificates=on 
tls-cert=/etc/squid/ssl_cert/squid.pem 
tls-dh=prime256v1:/etc/squid/ssl_cert/dhparam.pem

tls_outgoing_options cafile=/etc/ssl/cert.pem

tls_outgoing_options 
cipher=ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS

ssl_bump splice ssl_step1 nosslbump_ips # (acl type src)
ssl_bump peek ssl_step1
ssl_bump splice nosslbump_domains # (acl type ssl::server_name_regex)
(more ssl_bump lines not shown)

nosslbump_domains contains ".*" - so effectively nothing is bumped.

Firefox and IE work fine. But in Google chrome - sites dont open.

Access log shows NONE_ABORTED (for google chrome).

And packet sniffer shows FIN, ACK sent by squid. (I have not gone in 
details as I dont understand packet details)

Am I doing anything wrong? If not, then is there any temporary 
workaround without downgrading squid?

Please guide,

Thank you

Amish.



From rentorbuy at yahoo.com  Mon Jun 29 16:14:14 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 29 Jun 2020 16:14:14 +0000 (UTC)
Subject: [squid-users] Squid 4 and on_unsupported_protocol
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>
Message-ID: <1918547998.730780.1593447254694@mail.yahoo.com>

Hi,

I'd like to allow whatsapp web through a transparent tproxy sslbump Squid setup.

The target site is not loading:

wss://web.whatsapp.com/ws

I get TCP_MISS/400 305 GET https://web.whatsapp.com/ws in Squid cache log.

I'm not sure I know how to use the on_unsupported_protocol diective.

I have this in my config file:

acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN ERR_TOO_BIG
acl serverTalksFirstProtocol squid_error ERR_REQUEST_START_TIMEOUT
on_unsupported_protocol tunnel foreignProtocol
on_unsupported_protocol tunnel serverTalksFirstProtocol
on_unsupported_protocol respond all

How can I change this to allow websockets through Squid, but preferably only for a specific SRC IP addr. acl?

Regards,

Vieri


From ngtech1ltd at gmail.com  Mon Jun 29 16:41:33 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 29 Jun 2020 19:41:33 +0300
Subject: [squid-users] Squid 4 and on_unsupported_protocol
In-Reply-To: <1918547998.730780.1593447254694@mail.yahoo.com>
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>,
 <1918547998.730780.1593447254694@mail.yahoo.com>
Message-ID: <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200629/9226825b/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 29 19:52:16 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Jun 2020 15:52:16 -0400
Subject: [squid-users] Squid 4.12 Arch Linux Google Chrome fails -
 OpenSSL 1.1.1g
In-Reply-To: <17949f91-f48e-2558-bf76-969affa77d03@gmail.com>
References: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
 <17949f91-f48e-2558-bf76-969affa77d03@gmail.com>
Message-ID: <c90411ab-f15f-29cd-6e9d-496b8a1ef914@measurement-factory.com>

On 6/29/20 11:18 AM, Amish wrote:
> I am using Arch Linux and today I upgraded squid to 4.12 (from 4.10)
> Firefox and IE work fine. But in Google chrome - sites dont open.

You may need a fix for TLS GREASEd values. The following master/v6 PR
has not been backported to v4 yet AFAICT, but it might work "as is":

    https://github.com/squid-cache/squid/pull/663

Alex.


From yannick.rousseau at tutanota.com  Mon Jun 29 20:29:26 2020
From: yannick.rousseau at tutanota.com (yannick.rousseau at tutanota.com)
Date: Mon, 29 Jun 2020 22:29:26 +0200 (CEST)
Subject: [squid-users] no response from the proxy squid parent
In-Reply-To: <f92ab89d-f4f6-3cef-ac7b-0ab5adde5070@treenet.co.nz>
References: <MAXYaDB--3-2@tutanota.com>
 <909fe308-7c06-88d7-fefb-562e41af317e@treenet.co.nz>
 <MAqIdTJ--3-2@tutanota.com>
 <f92ab89d-f4f6-3cef-ac7b-0ab5adde5070@treenet.co.nz>
Message-ID: <MB0lHkl--3-2@tutanota.com>

Thanks a lot: it works fine now !
The line forwarded_for transparent was the solution.

Thanks again.

Yannick

-- 
 Envoi s?curis? avec Tutanota. Obtenez votre propre adresse email chiffr?e : 
 https://tutanota.com


27 juin 2020 ? 11:49 de squid3 at treenet.co.nz:

> On 28/06/20 3:09 am, yannick.rousseau at tutanota.com wrote:
>
>> Hi,
>> I've noticed one more difference between the CONNECT packets (it appears
>> in the HTTP layer):
>>
> ...
>
>>
>> If you have any idea to help me to fix this ....
>>
>
> You can try adding this to squid.conf:
>
>  forwarded_for transparent
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200629/0ef2ac9a/attachment.htm>

From rentorbuy at yahoo.com  Mon Jun 29 22:08:50 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 29 Jun 2020 22:08:50 +0000 (UTC)
Subject: [squid-users] Squid 4 and on_unsupported_protocol
In-Reply-To: <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>
 <1918547998.730780.1593447254694@mail.yahoo.com>
 <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>
Message-ID: <978069642.935061.1593468530437@mail.yahoo.com>



On Monday, June 29, 2020, 6:41:41 PM GMT+2, Eliezer Croitoru <ngtech1ltd at gmail.com> wrote: 
>
>
> I believe what you are looking for is at:
> https://wiki.squid-cache.org/ConfigExamples/Chat/Whatsapp
?
Thanks, but the article doesn't work for me.
I still see Firefox complaining (console) about not being able to connect to wss://web.whatsapp.com/ws.

Vieri


From anon.amish at gmail.com  Tue Jun 30 00:56:14 2020
From: anon.amish at gmail.com (Amish)
Date: Tue, 30 Jun 2020 06:26:14 +0530
Subject: [squid-users] Squid 4.12 Arch Linux Google Chrome fails -
 OpenSSL 1.1.1g
In-Reply-To: <c90411ab-f15f-29cd-6e9d-496b8a1ef914@measurement-factory.com>
References: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
 <17949f91-f48e-2558-bf76-969affa77d03@gmail.com>
 <c90411ab-f15f-29cd-6e9d-496b8a1ef914@measurement-factory.com>
Message-ID: <6c9233cc-1116-c91a-ef63-f26c239a2dd8@gmail.com>

On 30/06/20 1:22 am, Alex Rousskov wrote:
> On 6/29/20 11:18 AM, Amish wrote:
>> I am using Arch Linux and today I upgraded squid to 4.12 (from 4.10)
>> Firefox and IE work fine. But in Google chrome - sites dont open.
> You may need a fix for TLS GREASEd values. The following master/v6 PR
> has not been backported to v4 yet AFAICT, but it might work "as is":
>
>      https://github.com/squid-cache/squid/pull/663
>
> Alex.

Thank you very much for your response. I will try it soon.

But I am confused that PR has just 1 file changed, but if I read the 
comments in PR, it has many more commits.

So is the single file change enough? OR I need to apply all the commits?

Also just wondering that should we not release 4.13 because chrome is a 
most popular browser and so this kind of needs urgent fix.

Thanks and regards,

Amish.



From ngtech1ltd at gmail.com  Tue Jun 30 06:50:04 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 30 Jun 2020 09:50:04 +0300
Subject: [squid-users] Squid 4 and on_unsupported_protocol
In-Reply-To: <978069642.935061.1593468530437@mail.yahoo.com>
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>
 <1918547998.730780.1593447254694@mail.yahoo.com>
 <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>,
 <978069642.935061.1593468530437@mail.yahoo.com>
Message-ID: <272BD800-8D82-4E27-B966-0AA5582612DA@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200630/5a767fb1/attachment.htm>

From belle at bazuin.nl  Tue Jun 30 07:38:28 2020
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 30 Jun 2020 09:38:28 +0200
Subject: [squid-users] Squid typo fixes for 4.12
Message-ID: <vmime.5efaebf4.2df9.34d1bbac35781934@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 
 
Sorry for not pushing this through git.
If you want some typo fixed, here you go.


Fixed typo's found by Lintian on Debian Buster.
--- a/src/ssl/crtd_message.cc
+++ b/src/ssl/crtd_message.cc
@@ -206,7 +206,7 @@
     i = map.find(Ssl::CrtdMessage::param_Sign);
     if (i != map.end()) {
         if ((certProperties.signAlgorithm = Ssl::certSignAlgorithmId(i->second.c_str())) == Ssl::algSignEnd) {
-            error = "Wrong signing algoritm: ";
+            error = "Wrong signing algorithm: ";
             error += i->second;
             return false;
         }
--- a/CREDITS
+++ b/CREDITS
@@ -1631,7 +1631,7 @@
  * (C) 2000 Antonino Iannella, Stellar-X Pty Ltd
  * Released under GPL, see COPYING-2.0 for details.

- * Released under GNU Public License
+ * Released under GNU General Public License
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
--- a/compat/getaddrinfo.cc
+++ b/compat/getaddrinfo.cc
@@ -8,7 +8,7 @@

 /*
  *  Shamelessly duplicated from the fetchmail public sources
- *  for use by the Squid Project under GNU Public License.
+ *  for use by the Squid Project under GNU General Public License.
  *
  * Update/Maintenance History:
  *
--- a/compat/getaddrinfo.h
+++ b/compat/getaddrinfo.h
@@ -11,7 +11,7 @@

 /*
  *  Shamelessly duplicated from the fetchmail public sources
- *  for use by the Squid Project under GNU Public License.
+ *  for use by the Squid Project under GNU General Public License.
  *
  * Update/Maintenance History:
  *
--- a/compat/getnameinfo.cc
+++ b/compat/getnameinfo.cc
@@ -8,7 +8,7 @@

 /*
  *  Shamelessly duplicated from the fetchmail public sources
- *  for use by the Squid Project under GNU Public License.
+ *  for use by the Squid Project under GNU General Public License.
  *
  * Update/Maintenance History:
  *
--- a/compat/strnstr.cc
+++ b/compat/strnstr.cc
@@ -11,7 +11,7 @@

 /*
  *  Shamelessly duplicated from the FreeBSD public sources
- *  for use by the Squid Project under GNU Public License.
+ *  for use by the Squid Project under GNU General Public License.
  *
  * Update/Maintenance History:
  *
--- a/compat/xstrto.cc
+++ b/compat/xstrto.cc
@@ -11,7 +11,7 @@

 /*
  *  Shamelessly duplicated from the netfilter iptables sources
- *  for use by the Squid Project under GNU Public License.
+ *  for use by the Squid Project under GNU General Public License.
  *
  * Reason for use as explained by Luciano Coelho:
  * "I found that there is a bug in strtoul (and strtoull for
--- a/src/auth/basic/SMB_LM/msntauth.cc
+++ b/src/auth/basic/SMB_LM/msntauth.cc
@@ -21,7 +21,7 @@
  * Bill Welliver 1999
  * Duane Wessels 2000 (wessels at squid-cache.org)
  *
- * Released under GNU Public License
+ * Released under GNU General Public License
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
--- a/src/cf.data.pre
+++ b/src/cf.data.pre
@@ -3247,7 +3247,7 @@
                concurrency=

        The number of requests each certificate validator helper can handle in
-       parallel. A value of 0 indicates the certficate validator does not
+       parallel. A value of 0 indicates the certificate validator does not
        support concurrency. Defaults to 1.

        When this directive is set to a value >= 1 then the protocol
--- a/src/security/ServerOptions.cc
+++ b/src/security/ServerOptions.cc
@@ -215,7 +215,7 @@
         if (certs.size() > 1) {
             // NOTE: calling SSL_CTX_use_certificate() repeatedly _replaces_ the previous cert details.
             //       so we cannot use it and support multiple server certificates with OpenSSL.
-            debugs(83, DBG_CRITICAL, "ERROR: OpenSSL does not support multiple server certificates. Ignoring addional cert= parameters.");
+            debugs(83, DBG_CRITICAL, "ERROR: OpenSSL does not support multiple server certificates. Ignoring additional cert= parameters.");
         }

         const auto &keys = certs.front();
--- a/src/security/cert_generators/file/certificate_db.cc
+++ b/src/security/cert_generators/file/certificate_db.cc
@@ -537,7 +537,7 @@
     subSize(filename);
     int ret = remove(filename.c_str());
     if (ret < 0 && errno != ENOENT)
-        throw std::runtime_error("Failed to remove certficate file " + filename + " from db");
+        throw std::runtime_error("Failed to remove certificate file " + filename + " from db");
 }

 bool Ssl::CertificateDb::deleteInvalidCertificate() {
--- a/src/ssl/support.cc
+++ b/src/ssl/support.cc
@@ -353,7 +353,7 @@
                 filledCheck->serverCert.reset();
             }
             // If the certificate validator is used then we need to allow all errors and
-            // pass them to certficate validator for more processing
+            // pass them to certificate validator for more processing
             else if (Ssl::TheConfig.ssl_crt_validator) {
                 ok = 1;
             }
--- a/src/client_side.cc
+++ b/src/client_side.cc
@@ -2868,7 +2868,7 @@
             if (reply.result != Helper::Okay) {
                 debugs(33, 5, "Certificate for " << tlsConnectHostOrIp << " cannot be generated. ssl_crtd response: " << reply_message.getBody());
             } else {
-                debugs(33, 5, "Certificate for " << tlsConnectHostOrIp << " was successfully recieved from ssl_crtd");
+                debugs(33, 5, "Certificate for " << tlsConnectHostOrIp << " was successfully received from ssl_crtd");
                 if (sslServerBump && (sslServerBump->act.step1 == Ssl::bumpPeek || sslServerBump->act.step1 == Ssl::bumpStare)) {
                     doPeekAndSpliceStep();
                     auto ssl = fd_table[clientConnection->fd].ssl.get();



From ngtech1ltd at gmail.com  Tue Jun 30 08:10:25 2020
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 30 Jun 2020 11:10:25 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <8D009722-1EF8-4648-B77E-A7060F3F7D24@hxcore.ol>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <22186F7E-2BC4-44CD-940A-02072A2D92E6@hxcore.ol>,
 <AM5PR0102MB2756B10588909C4F4635B23A93980@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <B2FC1ADC-2403-4FD8-936C-C830DD22AFB3@hxcore.ol>
 <AM5PR0102MB2756AE61B5D0035049B9BC7993920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <AM5PR0102MB2756A4397BC6A6C95B4CACDC93920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <13040F9E-CAF9-4E99-B0F6-ED8927C72545@hxcore.ol>,
 <VI1PR0102MB2767221111907FD0278DC0E4936F0@VI1PR0102MB2767.eurprd01.prod.exchangelabs.com>,
 <8D009722-1EF8-4648-B77E-A7060F3F7D24@hxcore.ol>
Message-ID: <8B3071BC-5B3C-426B-B38D-688ED3C311B6@hxcore.ol>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200630/ce1b2773/attachment.htm>

From rentorbuy at yahoo.com  Tue Jun 30 08:57:11 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 30 Jun 2020 08:57:11 +0000 (UTC)
Subject: [squid-users] Squid 4 and on_unsupported_protocol
In-Reply-To: <272BD800-8D82-4E27-B966-0AA5582612DA@hxcore.ol>
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>
 <1918547998.730780.1593447254694@mail.yahoo.com>
 <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>
 <978069642.935061.1593468530437@mail.yahoo.com>
 <272BD800-8D82-4E27-B966-0AA5582612DA@hxcore.ol>
Message-ID: <807950326.154229.1593507431528@mail.yahoo.com>



 On Tuesday, June 30, 2020, 8:50:09 AM GMT+2, Eliezer Croitoru <ngtech1ltd at gmail.com> wrote: 

>
> I can try to re-produce this setup locally to make sure that it works as described in the docs.

Thanks!

> So couple details:
> ? * PC Windows(What OS?) client with firefox

Windows 10, Windows 7
Firefox ESR 68.5.0
 
>??? * Are you Intercepting the traffic or using Squid as a simple forward proxy defined in the browser or OS proxy settings?

Intercepting with TPROXY.

> Can you share a basic squid.conf (cleaned of personal details) to make sure where and how these rules should be applied?
?
Here it goes (client traffic is intercepted/bumped):

squid.conf:

acl SSL_ports port 443
acl Safe_ports port 80?? ??? ?# http
acl Safe_ports port 21?? ??? ?# ftp
acl Safe_ports port 443?? ??? ?# https
acl Safe_ports port 70?? ??? ?# gopher
acl Safe_ports port 210?? ??? ?# wais
acl Safe_ports port 1025-65535?? ?# unregistered ports
acl Safe_ports port 280?? ??? ?# http-mgmt
acl Safe_ports port 488?? ??? ?# gss-http
acl Safe_ports port 591?? ??? ?# filemaker
acl Safe_ports port 777?? ??? ?# multiling http
acl Safe_ports port 901?? ??? ?# SWAT
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
include /etc/squid/squid.include
include /etc/squid/squid.include.rules
http_access allow localhost
http_access deny all
coredump_dir /var/cache/squid
refresh_pattern ^ftp:?? ??? ?1440?? ?20%?? ?10080
refresh_pattern ^gopher:?? ?1440?? ?0%?? ?1440
refresh_pattern -i (/cgi-bin/|\?) 0?? ?0%?? ?0
refresh_pattern .?? ??? ?0?? ?20%?? ?4320


squid.include:

acl explicit myportname 3128
acl intercepted myportname 3129
acl interceptedssl myportname 3130
http_port 3128
http_port 3129 tproxy
https_port 3130 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem sslflags=NO_DEFAULT_CA
tls_outgoing_options flags=DONT_VERIFY_PEER
sslcrtd_program /usr/libexec/squid/security_file_certgen -s /var/lib/squid/ssl_db -M 16MB
sslcrtd_children 40 startup=20 idle=10
cache_dir diskd /var/cache/squid 32 16 256

squid.include.common:

cache_mgr admin at domain.org
email_err_data on
error_directory /usr/share/squid/errors/custom
client_lifetime 480 minutes


squid.include.hide:

httpd_suppress_version_string on
dns_v4_first on
via off
forwarded_for transparent


squid.include.rules:

external_acl_type nt_group ttl=0 children-max=50 %LOGIN /usr/libexec/squid/ext_wbinfo_group_acl -K
auth_param negotiate program /usr/libexec/squid/negotiate_kerberos_auth -s HTTP/fwprox.domain.org at DOMAIN.ORG
auth_param negotiate children 60
auth_param negotiate keep_alive on
acl localnet src 10.0.0.0/8
acl localnet src 192.168.0.0/16
acl ORG_all proxy_auth REQUIRED
external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %PROTO %DST %PORT %PATH /opt/custom/scripts/run/scripts/firewall/ext_sql_blwl_acl.pl --table=shallalist_bl --categories=adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
acl privileged_src_ips src "/SAMBA/proxy-settings/allowed.ips"
acl privileged_extra1_src_ips src "/SAMBA/proxy-settings/allowed.extra1.ips"
acl privileged_user_groups external nt_group "/SAMBA/proxy-settings/allowed.groups"
acl direct_dst_domains dstdomain "/SAMBA/proxy-settings/allowed.direct"
acl good_dst_domains dstdomain "/SAMBA/proxy-settings/allowed.domains"
acl good_dst_domains_with_any_filetype dstdomain "/SAMBA/proxy-settings/allowed.domains.filetypes"
acl good_dst_domains_with_any_mimetype dstdomain "/SAMBA/proxy-settings/allowed.domains.mimetypes"
acl good_urls_any_useragent url_regex "/SAMBA/proxy-settings/allowed.useragents.urls"
acl good_urls url_regex "/SAMBA/proxy-settings/allowed.urls"
acl bad_dst_domains dstdomain "/SAMBA/proxy-settings/denied.domains"
acl bad_dst_ccn_domains dstdomain "/SAMBA/proxy-settings/denied.ccn.domains"
acl bad_dst_ccn_ips dst "/SAMBA/proxy-settings/denied.ccn.ips"
acl limited_dst_domains_1 dstdomain "/SAMBA/proxy-settings/denied.extra1.domains"
acl bad_ads url_regex "/SAMBA/proxy-settings/denied.ads"
acl bad_filetypes urlpath_regex -i "/SAMBA/proxy-settings/denied.filetypes"
acl bad_requested_mimetypes req_mime_type -i "/SAMBA/proxy-settings/denied.mimetypes"
acl limited_requested_mimetypes_1 req_mime_type -i "/SAMBA/proxy-settings/denied.extra1.mimetypes"
acl bad_replied_mimetypes rep_mime_type -i "/SAMBA/proxy-settings/denied.mimetypes"
acl limited_replied_mimetypes_1 rep_mime_type -i "/SAMBA/proxy-settings/denied.extra1.mimetypes"
acl restricted_requested_mimetypes_1 req_mime_type -i "/SAMBA/proxy-settings/denied.restricted1.mimetypes"
acl restricted_replied_mimetypes_1 rep_mime_type -i "/SAMBA/proxy-settings/denied.restricted1.mimetypes"
acl restricted_good_dst_domains_1 dstdomain "/SAMBA/proxy-settings/allowed.restricted1.domains"
acl restricted_src_ips_1 dst "/SAMBA/proxy-settings/allowed.restricted1.ips"
acl explicit_only_src_ips src "/SAMBA/proxy-settings/restricted.ips"
acl explicit_only_user_groups external nt_group "/SAMBA/proxy-settings/restricted.groups"
acl explicit_only_dst_domains dstdomain "/SAMBA/proxy-settings/restricted.domains"
acl bl_lookup external bllookup
acl bad_urlshorteners dstdomain "/etc/squidGuard/db/HMANshallalist/urlshortener/domains"
acl redirected_domains_1 dstdomain .some.domain.com .some.other.domain.com
acl redirected_domains_2 dstdomain anotherdomain.com
acl redirected_urls_1 url_regex ^https://domain.com/path/
acl good_useragents req_header User-Agent Firefox/
acl good_useragents req_header User-Agent Edge/
acl good_useragents req_header User-Agent Microsoft-CryptoAPI/
acl src_ips_with_any_useragent src "/SAMBA/proxy-settings/allowed.useragents.ips"
acl dst_domains_with_any_useragent dstdomain "/SAMBA/proxy-settings/allowed.useragents.domains"
acl dst_ips_with_any_useragent dst "/SAMBA/proxy-settings/allowed.useragents.dst.ips"
http_access deny explicit !ORG_all
http_access deny explicit SSL_ports
http_access deny intercepted !localnet
http_access deny interceptedssl !localnet
acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN ERR_TOO_BIG
acl serverTalksFirstProtocol squid_error ERR_REQUEST_START_TIMEOUT
on_unsupported_protocol tunnel foreignProtocol
on_unsupported_protocol tunnel serverTalksFirstProtocol
on_unsupported_protocol respond all
http_access allow CONNECT interceptedssl SSL_ports
http_access deny !good_useragents !src_ips_with_any_useragent !dst_domains_with_any_useragent !dst_ips_with_any_useragent !good_urls_any_useragent
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents good_useragents
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents src_ips_with_any_useragent
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents dst_domains_with_any_useragent
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents dst_ips_with_any_useragent
http_access allow localnet !explicit_only_src_ips good_dst_domains
http_access allow localnet !explicit_only_src_ips good_urls
http_access allow localnet !explicit_only_src_ips good_urls_any_useragent
http_access allow localnet !explicit_only_src_ips privileged_src_ips
http_reply_access allow localnet !explicit_only_src_ips privileged_src_ips
http_reply_access allow localnet !explicit_only_src_ips good_dst_domains
http_reply_access allow localnet !explicit_only_src_ips good_urls
http_access allow explicit_only_src_ips explicit_only_dst_domains
http_access deny explicit_only_src_ips
http_access deny redirected_domains_1
deny_info 302:http://some.domain.com redirected_domains_1
http_access deny redirected_domains_2
deny_info 302:https://anotherdomain.com redirected_domains_2
http_access deny redirected_urls_1
deny_info 302:http://some.domain.com redirected_urls_1
http_access deny !privileged_src_ips bad_urlshorteners
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_urlshorteners bad_urlshorteners
http_access allow restricted_requested_mimetypes_1 restricted_good_dst_domains_1
http_access allow restricted_requested_mimetypes_1 restricted_src_ips_1
http_reply_access allow restricted_replied_mimetypes_1 restricted_good_dst_domains_1
http_reply_access allow restricted_replied_mimetypes_1 restricted_src_ips_1
http_access allow limited_requested_mimetypes_1 privileged_extra1_src_ips limited_dst_domains_1
http_reply_access allow limited_replied_mimetypes_1 privileged_extra1_src_ips limited_dst_domains_1
http_access deny restricted_requested_mimetypes_1
http_reply_access deny restricted_replied_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes restricted_replied_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes restricted_requested_mimetypes_1
http_access deny limited_requested_mimetypes_1
http_reply_access deny limited_replied_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes limited_requested_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes limited_replied_mimetypes_1
http_access deny !privileged_src_ips bad_dst_domains
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_domains bad_dst_domains
http_access deny bad_dst_ccn_domains
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_ccn bad_dst_ccn_domains
http_access deny bad_dst_ccn_ips
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_ccn bad_dst_ccn_ips
http_access allow privileged_extra1_src_ips limited_dst_domains_1
http_access deny limited_dst_domains_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=limited_dst_domains_1 limited_dst_domains_1
http_access deny bad_filetypes !good_dst_domains_with_any_filetype
http_reply_access deny bad_filetypes !good_dst_domains_with_any_filetype
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_filetypes bad_filetypes
http_access deny bad_requested_mimetypes !good_dst_domains_with_any_mimetype
http_reply_access deny bad_replied_mimetypes !good_dst_domains_with_any_mimetype
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes bad_requested_mimetypes
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes bad_replied_mimetypes
http_access allow localnet bl_lookup
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_domains_bl all
debug_options rotate=1 ALL,1
append_domain .domain.org
reply_header_access Alternate-Protocol deny all
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/SAMBA/proxy-settings/allowed.direct"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service antivirus respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access antivirus allow all
include /etc/squid/squid.include.common
include /etc/squid/squid.include.hide
cache_mem 32 MB
max_filedescriptors 65536
icap_service_failure_limit -1
icap_persistent_connections off


Regards,

Vieri


From stefano.tabacchiera at IGT.com  Tue Jun 30 09:10:57 2020
From: stefano.tabacchiera at IGT.com (Tabacchiera, Stefano)
Date: Tue, 30 Jun 2020 09:10:57 +0000
Subject: [squid-users] R:  squid stores multiple copies of identical ETags
In-Reply-To: <1cac51ab-30ba-b196-a96d-87e7bc7cd1bb@measurement-factory.com>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
 <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
 <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>
 <4954f8e0-3b55-ccda-7aa8-2e0a706947ff@measurement-factory.com>
 <95925593-DC27-4E8E-BE57-A34ABA332523@igt.com>
 <1cac51ab-30ba-b196-a96d-87e7bc7cd1bb@measurement-factory.com>
Message-ID: <DM6PR01MB3817CBD17D7EB746D6B28EB2E46F0@DM6PR01MB3817.prod.exchangelabs.com>

>> I mean: squid would store a new copy of the object while leaving the

>> old copy deletion to cleanup task?



>Some parts of the cleanup process may be delegated. The details depend on the cache_dir type. I do not know or remember aufs specifics, but I suspect that all ufs-based cache_dirs, including aufs, use lazy garbage collection >(under normal circumstances). The cache_swap_low and cache_swap_high directives should determine what is "normal".



Alex, you were absolutely right.

I managed to reproduce the case.

On test environment I set up "cache_swap_low 1" and "cache_swap_low 2" and enabled the store_log.

Then I tailed the store_log and watched the evolution of cache_dir, while running squidclient toward origin server every 100ms.



Store.log:

1593504359.704 SWAPOUT 00 000098C5 1865A3A26D411E7C0D8D87770720E405  200 1593504359 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504359.858 SWAPOUT 00 000098C6 1865A3A26D411E7C0D8D87770720E405  200 1593504359 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.015 SWAPOUT 00 000098C7 1865A3A26D411E7C0D8D87770720E405  200 1593504359 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.170 SWAPOUT 00 000098C8 1865A3A26D411E7C0D8D87770720E405  200 1593504359 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.324 SWAPOUT 00 000098C9 1865A3A26D411E7C0D8D87770720E405  200 1593504359 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.476 SWAPOUT 00 000098CA 1865A3A26D411E7C0D8D87770720E405  200 1593504359 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.634 SWAPOUT 00 000098CB 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.788 SWAPOUT 00 000098CC 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504360.941 SWAPOUT 00 000098CD 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504361.096 SWAPOUT 00 000098CE 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504361.249 SWAPOUT 00 000098CF 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504361.403 SWAPOUT 00 000098D0 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504361.556 SWAPOUT 00 000098D1 1865A3A26D411E7C0D8D87770720E405  200 1593504360 1593504061        -1 text/plain 544275/544275 GET http://xxx.xxx.xxx.xxx/blah/FEED.json

1593504361.607 RELEASE 00 000098D1 1865A3A26D411E7C0D8D87770720E405   ?         ?         ?         ? ?/? ?/? ? ?



The cached object was gradually appearing in cache_dir, until the ?RELEASE? line showed up in the store.log.

At this right moment, all copies of the object stored on disk were deleted.



So I?m assuming that only one object on disk (the last one retrieved) is the object referenced as ?active? by squid, all the rest being trashable.

Since the client is forcing a ?no-cache? header, squid does what the client is asking for, and every time it stores the object on disk.

I?m also assuming that IF another client asked the same object without the ?no-cache? header, squid would serve the latest cached object on disk.

If I?m right so far, squid never ?overwrites? the old copy of an object on disk. Instead, it stores a new one, marking it as ?active?, and let the deletion task to (a)ufs threads.



Could this this way?



Thanks!

ST



____________________________________________________________________________________ La presente comunicazione ed i suoi allegati e' destinata esclusivamente ai destinatari. Qualsiasi suo utilizzo, comunicazione o diffusione non autorizzata e' proibita. Se ha ricevuto questa comunicazione per errore, la preghiamo di darne immediata comunicazione al mittente e di cancellare tutte le informazioni erroneamente acquisite. Grazie This message and its attachments are intended only for use by the addressees. Any use, re-transmission or dissemination not authorized of it is prohibited. If you received this e-mail in error, please inform the sender immediately and delete all the material. Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200630/b46ce9c0/attachment.htm>

From ngtech1ltd at gmail.com  Tue Jun 30 10:26:32 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Tue, 30 Jun 2020 13:26:32 +0300
Subject: [squid-users] Squid memory consumption problem
In-Reply-To: <AM5PR0102MB2756C2EB1208B52C586DA670936F0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
References: <AM5PR0102MB27564C95912983EA0E6401E493860@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <7c81730e-64f4-24d7-e540-c027fb2c2b77@measurement-factory.com>
 <5967FC66-09A7-44AE-869B-9C6299D84F48@hxcore.ol>,
 <AM5PR0102MB27563FE044CFA5B57138F1B093830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <D130E594-17F6-4833-BB7A-9622A531441C@hxcore.ol>,
 <AM5PR0102MB27569DC251607D12468B0D2B93830@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <F2702CF4-D5C3-46CE-914E-BD6A5843852C@hxcore.ol>,
 <AM5PR0102MB27560AE240283C7DAC58648C93800@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <4C837D1A-C7A1-44FC-B591-2F404173539A@hxcore.ol>,
 <AM5PR0102MB275689506D68A171E4EF245E939D0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <22186F7E-2BC4-44CD-940A-02072A2D92E6@hxcore.ol>,
 <AM5PR0102MB2756B10588909C4F4635B23A93980@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <B2FC1ADC-2403-4FD8-936C-C830DD22AFB3@hxcore.ol>
 <AM5PR0102MB2756AE61B5D0035049B9BC7993920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>,
 <AM5PR0102MB2
 756A4397BC6A6C95B4CACDC93920@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
 <13040F9E-CAF9-4E99-B0F6-ED8927C72545@hxcore.ol>,
 <VI1PR0102MB2767221111907FD0278DC0E4936F0@VI1PR0102MB2767.eurprd01.prod.exchangelabs.com>,
 <8D009722-1EF8-4648-B77E-A7060F3F7D24@hxcore.ol>
 <8B3071BC-5B3C-426B-B38D-688ED3C311B6@hxcore.ol>
 <AM5PR0102MB2756C2EB1208B52C586DA670936F0@AM5PR0102MB2756.eurprd01.prod.exchangelabs.com>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAEGQtNYT0SBEjroouRDlGygigQAAEAAAAO5fXzXIYIlKqMgnPdDAr9wBAAAAAA==@gmail.com>

Hey Dixit is it your first name?,

 

I want to help you but you are jumping between subjects and issues over and over again.

Try to open a single thread for a subject and follow it.

 

You are missing details on the clients.

Clients can vary from PC to MOBILE etc..

To clear out any doubts about the setup more details are required if you want any public help.

 

I can try to help you specifically with installation level issues.

The topics you are writing about are a more advanced topic rather then basic..

 

Did you had any chance reading the next docs:

?         https://stackoverflow.com/questions/30057104/squid-ssl-bump-3-5-4-error-error-negotiating-ssl-connection-on-fd-10-success

?         http://squid-web-proxy-cache.1019090.n4.nabble.com/Error-negotiating-SSL-connection-on-FD-12-Success-td4671090.html

 

I want to help you but it?s not always so simple.

 

Take into account that some servers/websites use different versions of TLS/SSL and Squid 4 is not compatible with all of them.

 

What version of OpenSSL do you have installed on your OS?

yum list installed

 

might give you the right answer.

 

 

Also, if you have specific clients which are having issues try to get the relevant details on their software and device.

I cannot do this work for you but can try to help you based on what you might send the public list.

 

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com

 

 

 

 

From: DIXIT Ankit [mailto:Ankit.Dixit at eurostar.com] 
Sent: Tuesday, June 30, 2020 12:25 PM
To: Eliezer Croitoru; Squid Users
Cc: SETHI Konica
Subject: RE: [squid-users] Squid memory consumption problem

 

Eliezer,

 

Clients are facing some SSL related issues after upgrade. I could see below error. Please suggest, its little urgent.

 

quid[6706]: Error negotiating SSL connection on FD 167: error:00000001:lib(0):func(0):reason(1) (1/0)
Jun 30 09:17:38 squid[6706]: Error parsing SSL Server Hello Message on FD 77
Jun 30 09:17:38 squid[6706]: Error negotiating SSL connection on FD 75: error:00000001:lib(0):func(0):reason(1) (1/0)

 

 

Regards,

Ankit Dixit|IS Cloud Team

Eurostar International Ltd

Times House | Bravingtons Walk | London N1 9AW

Office: +44 (0)207 84 35550 (Extension? 35530)

 

From: Eliezer Croitoru <ngtech1ltd at gmail.com> 
Sent: Tuesday, June 30, 2020 9:10 AM
To: Squid Users <squid-users at lists.squid-cache.org>; DIXIT Ankit <Ankit.Dixit at eurostar.com>
Subject: RE: [squid-users] Squid memory consumption problem

 




 

The first thing to do is look at:

https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery

 

It should clear couple doubts for you.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com

 

From: DIXIT Ankit <mailto:Ankit.Dixit at eurostar.com> 
Sent: Tuesday, June 30, 2020 10:46 AM
To: Eliezer Croitoru <mailto:ngtech1ltd at gmail.com> ; Alex Rousskov <mailto:rousskov at measurement-factory.com> ; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid memory consumption problem

 

Elizer,

 

We installed Squid 4.12 on production server, amazon Linux 2, successfully but I could see below messages in the logs for SECURITY ALERT: Host header forgery detected. These are getting generated very frequently.

Can we ignore this Or is it advised to suppress these alerts?

 

kid2| SECURITY ALERT: on URL: 5-25-3-app.agent.datadoghq.com:443

2020/06/30 07:41:29 kid1| SECURITY ALERT: Host header forgery detected on local=IP remote=IP FD 97 flags=33 (local IP does not match any domain IP)

 

Regards,

Ankit Dixit|IS Cloud Team

Eurostar International Ltd

Times House | Bravingtons Walk | London N1 9AW

Office: +44 (0)207 84 35550 (Extension? 35530)

 

 

  _____  

This email (including any attachments) is intended only for the addressee(s), is confidential and may be legally privileged. If you are not the intended recipient, do not use, disclose, copy, or forward this email. Please notify the sender immediately and then delete it. Eurostar International Limited and its affiliates ("EIL") do not accept any liability for action taken in reliance on this email. EIL makes no representation that this email is free of viruses and addressees should check this email for viruses. The comments or statements expressed in this email are not necessarily those of EIL. 

Eurostar International Ltd 
Times House, Bravingtons Walk, London N1 9AW Registered in England and Wales No. 2462001 

  _____  

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200630/aff516e7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 19517 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20200630/aff516e7/attachment.jpg>

From ngtech1ltd at gmail.com  Tue Jun 30 11:41:53 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Tue, 30 Jun 2020 14:41:53 +0300
Subject: [squid-users] Squid 4 and on_unsupported_protocol
In-Reply-To: <807950326.154229.1593507431528@mail.yahoo.com>
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>
 <1918547998.730780.1593447254694@mail.yahoo.com>
 <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>
 <978069642.935061.1593468530437@mail.yahoo.com>
 <272BD800-8D82-4E27-B966-0AA5582612DA@hxcore.ol>
 <807950326.154229.1593507431528@mail.yahoo.com>
Message-ID: <764601d64ed3$745371e0$5cfa55a0$@gmail.com>

Hey Vieri,

I have tested the wiki pages again to make sure it's not misleading  and..
I have used the next regex:
## START OF FILE
# Web.whatsapp.com
^(w[0-9]+|[a-z]+\.)?web\.whatsapp\.com$

# Whatsapp CDN issue
.whatsapp\.net$
## EOF 

Which seems a bit more accurate then what's in the wiki.
If it works for your use case the same I think the should be updated.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: Vieri [mailto:rentorbuy at yahoo.com] 
Sent: Tuesday, June 30, 2020 11:57 AM
To: Squid Users; Eliezer Croitoru
Subject: Re: [squid-users] Squid 4 and on_unsupported_protocol



 On Tuesday, June 30, 2020, 8:50:09 AM GMT+2, Eliezer Croitoru <ngtech1ltd at gmail.com> wrote: 

>
> I can try to re-produce this setup locally to make sure that it works as described in the docs.

Thanks!

> So couple details:
>   * PC Windows(What OS?) client with firefox

Windows 10, Windows 7
Firefox ESR 68.5.0
 
>    * Are you Intercepting the traffic or using Squid as a simple forward proxy defined in the browser or OS proxy settings?

Intercepting with TPROXY.

> Can you share a basic squid.conf (cleaned of personal details) to make sure where and how these rules should be applied?
 
Here it goes (client traffic is intercepted/bumped):

squid.conf:

acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl Safe_ports port 901        # SWAT
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
include /etc/squid/squid.include
include /etc/squid/squid.include.rules
http_access allow localhost
http_access deny all
coredump_dir /var/cache/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320


squid.include:

acl explicit myportname 3128
acl intercepted myportname 3129
acl interceptedssl myportname 3130
http_port 3128
http_port 3129 tproxy
https_port 3130 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem sslflags=NO_DEFAULT_CA
tls_outgoing_options flags=DONT_VERIFY_PEER
sslcrtd_program /usr/libexec/squid/security_file_certgen -s /var/lib/squid/ssl_db -M 16MB
sslcrtd_children 40 startup=20 idle=10
cache_dir diskd /var/cache/squid 32 16 256

squid.include.common:

cache_mgr admin at domain.org
email_err_data on
error_directory /usr/share/squid/errors/custom
client_lifetime 480 minutes


squid.include.hide:

httpd_suppress_version_string on
dns_v4_first on
via off
forwarded_for transparent


squid.include.rules:

external_acl_type nt_group ttl=0 children-max=50 %LOGIN /usr/libexec/squid/ext_wbinfo_group_acl -K
auth_param negotiate program /usr/libexec/squid/negotiate_kerberos_auth -s HTTP/fwprox.domain.org at DOMAIN.ORG
auth_param negotiate children 60
auth_param negotiate keep_alive on
acl localnet src 10.0.0.0/8
acl localnet src 192.168.0.0/16
acl ORG_all proxy_auth REQUIRED
external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %PROTO %DST %PORT %PATH /opt/custom/scripts/run/scripts/firewall/ext_sql_blwl_acl.pl --table=shallalist_bl --categories=adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
acl privileged_src_ips src "/SAMBA/proxy-settings/allowed.ips"
acl privileged_extra1_src_ips src "/SAMBA/proxy-settings/allowed.extra1.ips"
acl privileged_user_groups external nt_group "/SAMBA/proxy-settings/allowed.groups"
acl direct_dst_domains dstdomain "/SAMBA/proxy-settings/allowed.direct"
acl good_dst_domains dstdomain "/SAMBA/proxy-settings/allowed.domains"
acl good_dst_domains_with_any_filetype dstdomain "/SAMBA/proxy-settings/allowed.domains.filetypes"
acl good_dst_domains_with_any_mimetype dstdomain "/SAMBA/proxy-settings/allowed.domains.mimetypes"
acl good_urls_any_useragent url_regex "/SAMBA/proxy-settings/allowed.useragents.urls"
acl good_urls url_regex "/SAMBA/proxy-settings/allowed.urls"
acl bad_dst_domains dstdomain "/SAMBA/proxy-settings/denied.domains"
acl bad_dst_ccn_domains dstdomain "/SAMBA/proxy-settings/denied.ccn.domains"
acl bad_dst_ccn_ips dst "/SAMBA/proxy-settings/denied.ccn.ips"
acl limited_dst_domains_1 dstdomain "/SAMBA/proxy-settings/denied.extra1.domains"
acl bad_ads url_regex "/SAMBA/proxy-settings/denied.ads"
acl bad_filetypes urlpath_regex -i "/SAMBA/proxy-settings/denied.filetypes"
acl bad_requested_mimetypes req_mime_type -i "/SAMBA/proxy-settings/denied.mimetypes"
acl limited_requested_mimetypes_1 req_mime_type -i "/SAMBA/proxy-settings/denied.extra1.mimetypes"
acl bad_replied_mimetypes rep_mime_type -i "/SAMBA/proxy-settings/denied.mimetypes"
acl limited_replied_mimetypes_1 rep_mime_type -i "/SAMBA/proxy-settings/denied.extra1.mimetypes"
acl restricted_requested_mimetypes_1 req_mime_type -i "/SAMBA/proxy-settings/denied.restricted1.mimetypes"
acl restricted_replied_mimetypes_1 rep_mime_type -i "/SAMBA/proxy-settings/denied.restricted1.mimetypes"
acl restricted_good_dst_domains_1 dstdomain "/SAMBA/proxy-settings/allowed.restricted1.domains"
acl restricted_src_ips_1 dst "/SAMBA/proxy-settings/allowed.restricted1.ips"
acl explicit_only_src_ips src "/SAMBA/proxy-settings/restricted.ips"
acl explicit_only_user_groups external nt_group "/SAMBA/proxy-settings/restricted.groups"
acl explicit_only_dst_domains dstdomain "/SAMBA/proxy-settings/restricted.domains"
acl bl_lookup external bllookup
acl bad_urlshorteners dstdomain "/etc/squidGuard/db/HMANshallalist/urlshortener/domains"
acl redirected_domains_1 dstdomain .some.domain.com .some.other.domain.com
acl redirected_domains_2 dstdomain anotherdomain.com
acl redirected_urls_1 url_regex ^https://domain.com/path/
acl good_useragents req_header User-Agent Firefox/
acl good_useragents req_header User-Agent Edge/
acl good_useragents req_header User-Agent Microsoft-CryptoAPI/
acl src_ips_with_any_useragent src "/SAMBA/proxy-settings/allowed.useragents.ips"
acl dst_domains_with_any_useragent dstdomain "/SAMBA/proxy-settings/allowed.useragents.domains"
acl dst_ips_with_any_useragent dst "/SAMBA/proxy-settings/allowed.useragents.dst.ips"
http_access deny explicit !ORG_all
http_access deny explicit SSL_ports
http_access deny intercepted !localnet
http_access deny interceptedssl !localnet
acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN ERR_TOO_BIG
acl serverTalksFirstProtocol squid_error ERR_REQUEST_START_TIMEOUT
on_unsupported_protocol tunnel foreignProtocol
on_unsupported_protocol tunnel serverTalksFirstProtocol
on_unsupported_protocol respond all
http_access allow CONNECT interceptedssl SSL_ports
http_access deny !good_useragents !src_ips_with_any_useragent !dst_domains_with_any_useragent !dst_ips_with_any_useragent !good_urls_any_useragent
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents good_useragents
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents src_ips_with_any_useragent
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents dst_domains_with_any_useragent
deny_info http://inf-fw2.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents dst_ips_with_any_useragent
http_access allow localnet !explicit_only_src_ips good_dst_domains
http_access allow localnet !explicit_only_src_ips good_urls
http_access allow localnet !explicit_only_src_ips good_urls_any_useragent
http_access allow localnet !explicit_only_src_ips privileged_src_ips
http_reply_access allow localnet !explicit_only_src_ips privileged_src_ips
http_reply_access allow localnet !explicit_only_src_ips good_dst_domains
http_reply_access allow localnet !explicit_only_src_ips good_urls
http_access allow explicit_only_src_ips explicit_only_dst_domains
http_access deny explicit_only_src_ips
http_access deny redirected_domains_1
deny_info 302:http://some.domain.com redirected_domains_1
http_access deny redirected_domains_2
deny_info 302:https://anotherdomain.com redirected_domains_2
http_access deny redirected_urls_1
deny_info 302:http://some.domain.com redirected_urls_1
http_access deny !privileged_src_ips bad_urlshorteners
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_urlshorteners bad_urlshorteners
http_access allow restricted_requested_mimetypes_1 restricted_good_dst_domains_1
http_access allow restricted_requested_mimetypes_1 restricted_src_ips_1
http_reply_access allow restricted_replied_mimetypes_1 restricted_good_dst_domains_1
http_reply_access allow restricted_replied_mimetypes_1 restricted_src_ips_1
http_access allow limited_requested_mimetypes_1 privileged_extra1_src_ips limited_dst_domains_1
http_reply_access allow limited_replied_mimetypes_1 privileged_extra1_src_ips limited_dst_domains_1
http_access deny restricted_requested_mimetypes_1
http_reply_access deny restricted_replied_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes restricted_replied_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes restricted_requested_mimetypes_1
http_access deny limited_requested_mimetypes_1
http_reply_access deny limited_replied_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes limited_requested_mimetypes_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes limited_replied_mimetypes_1
http_access deny !privileged_src_ips bad_dst_domains
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_domains bad_dst_domains
http_access deny bad_dst_ccn_domains
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_ccn bad_dst_ccn_domains
http_access deny bad_dst_ccn_ips
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_ccn bad_dst_ccn_ips
http_access allow privileged_extra1_src_ips limited_dst_domains_1
http_access deny limited_dst_domains_1
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=limited_dst_domains_1 limited_dst_domains_1
http_access deny bad_filetypes !good_dst_domains_with_any_filetype
http_reply_access deny bad_filetypes !good_dst_domains_with_any_filetype
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_filetypes bad_filetypes
http_access deny bad_requested_mimetypes !good_dst_domains_with_any_mimetype
http_reply_access deny bad_replied_mimetypes !good_dst_domains_with_any_mimetype
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes bad_requested_mimetypes
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes bad_replied_mimetypes
http_access allow localnet bl_lookup
deny_info http://fwprox.domain.org/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_domains_bl all
debug_options rotate=1 ALL,1
append_domain .domain.org
reply_header_access Alternate-Protocol deny all
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/SAMBA/proxy-settings/allowed.direct"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service antivirus respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access antivirus allow all
include /etc/squid/squid.include.common
include /etc/squid/squid.include.hide
cache_mem 32 MB
max_filedescriptors 65536
icap_service_failure_limit -1
icap_persistent_connections off


Regards,

Vieri



From rousskov at measurement-factory.com  Tue Jun 30 13:07:02 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Jun 2020 09:07:02 -0400
Subject: [squid-users] Squid 4.12 Arch Linux Google Chrome fails -
 OpenSSL 1.1.1g
In-Reply-To: <6c9233cc-1116-c91a-ef63-f26c239a2dd8@gmail.com>
References: <72DD5D5CF661B5459DC08A060BF26B5301089333@kjj-server.KJJ.local>
 <17949f91-f48e-2558-bf76-969affa77d03@gmail.com>
 <c90411ab-f15f-29cd-6e9d-496b8a1ef914@measurement-factory.com>
 <6c9233cc-1116-c91a-ef63-f26c239a2dd8@gmail.com>
Message-ID: <1e52e43d-9d69-4ba0-6710-4411a97bb183@measurement-factory.com>

On 6/29/20 8:56 PM, Amish wrote:
> On 30/06/20 1:22 am, Alex Rousskov wrote:
>> On 6/29/20 11:18 AM, Amish wrote:
>>> I am using Arch Linux and today I upgraded squid to 4.12 (from 4.10)
>>> Firefox and IE work fine. But in Google chrome - sites dont open.
>> You may need a fix for TLS GREASEd values. The following master/v6 PR
>> has not been backported to v4 yet AFAICT, but it might work "as is":
>>
>> ???? https://github.com/squid-cache/squid/pull/663

> But I am confused that PR has just 1 file changed, but if I read the
> comments in PR, it has many more commits.

Many commits may modify one file. If you are not familiar with git, the
easiest way to get cumulative PR changes is to add ".diff" to the PR
URL. See the very bottom of any PR page on GitHub for the link/reminder.

    https://github.com/squid-cache/squid/pull/663.diff

An arguably better way is to find the (cumulative) official commit
itself, but that requires navigating sometimes-confusing GitHub PR page
history and/or some knowledge of git and Squid merge procedures. In this
particular case, the official (master) commit is eec67f0:

    https://github.com/squid-cache/squid/commit/eec67f0
    https://github.com/squid-cache/squid/commit/eec67f0.diff


> So is the single file change enough? OR I need to apply all the
> commits?

You do not need to replay individual PR commits. When you use the
PR.diff trick, you will get a patch with all the commits squashed together.


BTW, there is a similar PR.patch trick, but it will give you a patch
with all the individual PR commits replayed in the right order. In most
cases, replaying individual commits may cause more conflicts.


HTH,

Alex.


From rentorbuy at yahoo.com  Tue Jun 30 13:41:51 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 30 Jun 2020 13:41:51 +0000 (UTC)
Subject: [squid-users] Squid 4 and on_unsupported_protocol
In-Reply-To: <764601d64ed3$745371e0$5cfa55a0$@gmail.com>
References: <1918547998.730780.1593447254694.ref@mail.yahoo.com>
 <1918547998.730780.1593447254694@mail.yahoo.com>
 <40E9F3E7-EE7B-41C8-BA4F-756DA91E8358@hxcore.ol>
 <978069642.935061.1593468530437@mail.yahoo.com>
 <272BD800-8D82-4E27-B966-0AA5582612DA@hxcore.ol>
 <807950326.154229.1593507431528@mail.yahoo.com>
 <764601d64ed3$745371e0$5cfa55a0$@gmail.com>
Message-ID: <1862500182.232291.1593524511163@mail.yahoo.com>

On Tuesday, June 30, 2020, 1:41:57 PM GMT+2, Eliezer Croitor <ngtech1ltd at gmail.com> wrote: 

> ^(w[0-9]+|[a-z]+\.)?web\.whatsapp\.com$

Yes, it does. I should have seen that... Thanks for your help!

Vieri


From rousskov at measurement-factory.com  Tue Jun 30 14:16:20 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Jun 2020 10:16:20 -0400
Subject: [squid-users] R: squid stores multiple copies of identical ETags
In-Reply-To: <DM6PR01MB3817CBD17D7EB746D6B28EB2E46F0@DM6PR01MB3817.prod.exchangelabs.com>
References: <DM6PR01MB3817C2346D012386F1C50321E4930@DM6PR01MB3817.prod.exchangelabs.com>
 <b2e23374-10ed-6ecb-99e4-e9b2019f1541@treenet.co.nz>
 <F16E97A2-4F7B-457C-BA39-7BB0E7E4174C@igt.com>
 <4954f8e0-3b55-ccda-7aa8-2e0a706947ff@measurement-factory.com>
 <95925593-DC27-4E8E-BE57-A34ABA332523@igt.com>
 <1cac51ab-30ba-b196-a96d-87e7bc7cd1bb@measurement-factory.com>
 <DM6PR01MB3817CBD17D7EB746D6B28EB2E46F0@DM6PR01MB3817.prod.exchangelabs.com>
Message-ID: <e7a7380a-332e-4a5f-b14e-2771b147c1a3@measurement-factory.com>

On 6/30/20 5:10 AM, Tabacchiera, Stefano wrote:

> So I?m assuming that only one object on disk (the last one retrieved) is
> the object referenced as ?active? by squid, all the rest being trashable.
> 
> Since the client is forcing a ?no-cache? header, squid does what the
> client is asking for, and every time it stores the object on disk.
> 
> I?m also assuming that IF another client asked the same object without
> the ?no-cache? header, squid would serve the latest cached object on disk.
> 
> If I?m right so far, squid never ?overwrites? the old copy of an object
> on disk. Instead, it stores a new one, marking it as ?active?, 

Yes, the above matches my understanding (for some definition of "last
one", "overwrites", and "active"). The actual situation is a bit more
nuanced (e.g., Squid could be storing and using multiple copies of the
same resource concurrently, even though any new request will never see
more than one copy), but those low-level details may not matter to your
investigation.


> and let the deletion task to (a)ufs threads.

I cannot confirm or deny this part -- I do not know whether garbage
collection is delegated to aufs thread(s) -- but it sounds plausible.


HTH,

Alex.


