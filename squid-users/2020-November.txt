From ngtech1ltd at gmail.com  Sun Nov  1 00:40:58 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Sun, 1 Nov 2020 02:40:58 +0200
Subject: [squid-users] Best practice for adding or removing ACLs
 dynamically ?
In-Reply-To: <75C05DB7-2C0E-4B77-88A4-66F581A035ED@gmail.com>
References: <75C05DB7-2C0E-4B77-88A4-66F581A035ED@gmail.com>
Message-ID: <005001d6afe7$ab52ba00$01f82e00$@gmail.com>

Hey Roee,

I am trying to understand what part of squid.conf you want to be able to change without a reconfigure/reload?
If you have many users, ie above 50 you should probably not use a simple ncsa_auth although  it's possible and in more then one case is preferable.
You could probably write your own basic auth helper that will interact with a DB which will probably simplify your whole setup.
(You can use existing basic auth helpers with mysql or ldap)

As for the tcp_outgoing_address, it?s a whole different story.
Since it's a "fast" acl type the options to do something dynamic with it are an issue.
(Maybe eCAP/ICAP service or a "pre-cooked" note or other factor to the acl can be used)

I am pretty sure that if an authentication service can reply with a note ie connection annotation then it can be used for the address selection.
One issue with it is that It will be valid for the next X ttl seconds/minutes/hours.

I do believe that there should be a way to allow something like external_acl helper to affect this squid feature.
I was thinking that an eCAP or an ICAP service or an external_acl helper can add a note for a connection based on couple other factors like:
* src ip
* auth username
* request domain or request sni
* ...

So let say the proxy will have a set of 100 addresses, each will have a single specific matching acl for a request header or connection annotation/note.
This way the selection of a tcp_outgoing_address would be a little less complex the it is today.

I have couple other ideas for implementations which I have experimented with but the proxy admin need to learn how these work which might be
a bit complicated some times.

Eliezer
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of roee klinger
Sent: Saturday, October 31, 2020 2:35 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Best practice for adding or removing ACLs dynamically ?

?
Hey,
I have Squid configured to send users to different outgoing interface like so:

..
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/htpassword
acl acl_for_user3002 proxy_auth user2
tcp_outgoing_address 192.168.8.12 acl_for_user3002
http_port 3002 name=3002
http_access allow authenticated
..

When I wanted to change the username:password for user2, I run a bash script to change it in squid.conf and also in htpassword and then I run "squid -k reconfigure", if I don't reconfigure the old user still has access to the proxy and the new one doesn't for about 30 minutes.

I am expecting to have 100s of users soon that will change credentials often, and also I would like to blacklist websites often and on the fly, so I was searching for a better way to manage this without reconfiguring every time, since sometimes a reconfigure can take up to 10-15 seconds.

I am new to Squid and wasn't able to find any info on this, am I doing this currently or there is a better way to change users/ACLs on the fly without reloading Squid?

Thanks,
Roee Klinger
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Nov  1 08:17:58 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 1 Nov 2020 21:17:58 +1300
Subject: [squid-users] Best practice for adding or removing ACLs
 dynamically ?
In-Reply-To: <CAGCa14qSmFGD3+ercC2JDEa3WKeLmU1d9scEgoWYWCUzztx7ww@mail.gmail.com>
References: <75C05DB7-2C0E-4B77-88A4-66F581A035ED@gmail.com>
 <e7119d05-4ab8-59f3-b616-dd7ce2d48ccb@treenet.co.nz>
 <CAGCa14qSmFGD3+ercC2JDEa3WKeLmU1d9scEgoWYWCUzztx7ww@mail.gmail.com>
Message-ID: <ccb17952-7300-282a-84bf-4c57efa9bc65@treenet.co.nz>

On 1/11/20 12:27 pm, roee klinger wrote:
> Thanks Amos!
> 
> I updated "auth_param basic credentialsttl" according to your advice and 
> it is working great.
> 
> I am still having issues with the "tcp_outgoing_address 192.168.8.12 
> acl_for_user3002" part, you mentioned:
>  > For ACLs with values that are expected to change often it is best to use
>  > an external_acl_type helper that manages the updates or fetches from
>  > somewhere the updates are handled without a reload.
> 
> My script updates the authenticator?successfully, but when I update "acl 
> acl_for_user3002 proxy_auth user2" to the new username I have to 
> reconfigure to take effect.
> I read online for hours but to my best understanding external_acl_type 
> are for auth and access control, but they don't work for my needs I believe.
> 
> Is there any way to use?external_acl_type in a way I don't understand to 
> solve this problem? Do I have to reconfigure every time?I make changes 
> to an ACL in squid.conf?


Some directives have to produce allow/deny result immediately, without 
waiting for a helper to respond. The details are documented here:
  <https://wiki.squid-cache.org/SquidFaq/SquidAcl>

In modern Squid you can use a helper to set annotations which are 
checked with the "note" ACL type in the fast checks.



It sounds a bit like you are trying to tie IPs to individual users. 
Please be aware that breaks the multiplexing and persistence features of 
HTTP, which is a major performance loss.

Amos


From rentorbuy at yahoo.com  Mon Nov  2 08:36:46 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 2 Nov 2020 08:36:46 +0000 (UTC)
Subject: [squid-users] squid restart
In-Reply-To: <45922024-b7e6-be8d-af33-f285ca76b877@treenet.co.nz>
References: <2076232307.120171.1604142162886.ref@mail.yahoo.com>
 <2076232307.120171.1604142162886@mail.yahoo.com>
 <45922024-b7e6-be8d-af33-f285ca76b877@treenet.co.nz>
Message-ID: <947127838.562691.1604306206913@mail.yahoo.com>



On Saturday, October 31, 2020, 4:08:23 PM GMT+1, Amos Jeffries <squid3 at treenet.co.nz> wrote: 

>> However, I set the following directive in squid.conf:
>> 
>> max_filedescriptors 65536
>> 
> Are you using systemd, SysV or another init ?

I'm using SysV on Gentoo Linux.

> It doesn't seem to be honored here unless I stop and restart the squid service again (/etc/init.d/squid restart from command line):
> 
> File descriptor usage for squid:
>? ??????? Maximum number of file descriptors:?? 65535
> 
> It seems that if I run the same command (/etc/init.d/squid restart) from crontab, that ulimit is not honored. I guess that's the root cause of my issue because I am asking cron to restart Squid once daily. I'll try not to, but I was hoping to see if there was a reliable way to fully restart the Squid process.
> 
> Vieri

> 

The init system restart command is the preferred one - it handles any 
system details that need updating. Alternatively, "squid -k restart" can 
be used.

The SysV init script works fine when run from command line or at boot time (and probably from a custom inittab script -- cannot confirm it yet). The problem shows up when running it from cron (I have?cronie-1.5.4).
I'll take a look at the '-k restart' alternative.

Thanks,

Vieri


From rentorbuy at yahoo.com  Mon Nov  2 09:45:57 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 2 Nov 2020 09:45:57 +0000 (UTC)
Subject: [squid-users] squid restart
References: <1119836984.571468.1604310357670.ref@mail.yahoo.com>
Message-ID: <1119836984.571468.1604310357670@mail.yahoo.com>

Just in case anyone else has this problem, or if anyone would like to comment on this, here's the solution I've found.

Running '/etc/init.d/squid restart' from cron (setting it up in crontab) does not honor ulimits.

Configuring /etc/crontab with something like 'bash -l /etc/init.d/squid restart' does not work either (it doesn't seem to run at all).

However, creating a custom.sh script somewhere which calls /etc/init.d/squid restart, and then configuring crontab with 'bash -l -c /somewhere/custom.sh' actually works. I now see:

# squidclient mgr:info
[...]
File descriptor usage for squid:
??????? Maximum number of file descriptors:?? 65535
??????? Largest file desc currently in use:?? 1583
??????? Number of file desc currently in use: 1576
??????? Files queued for open:?????????????????? 0
??????? Available number of file descriptors: 63959
??????? Reserved number of file descriptors:?? 100
??????? Store Disk files open:?????????????????? 0

I'm not sure why, but it works.

Vieri


From ngtech1ltd at gmail.com  Mon Nov  2 13:04:27 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Mon, 2 Nov 2020 15:04:27 +0200
Subject: [squid-users] squid restart
In-Reply-To: <2076232307.120171.1604142162886@mail.yahoo.com>
References: <2076232307.120171.1604142162886.ref@mail.yahoo.com>
 <2076232307.120171.1604142162886@mail.yahoo.com>
Message-ID: <002501d6b118$b34763a0$19d62ae0$@gmail.com>

Hey Vieri,

FD and other limits are a tricky thing in gentoo and couple other systems.
In ubuntu for example to apply a ulimit for squid what they did is add a ulimit command at the beginning of the init script.
Ie 'ulimit -hn 65535;ulimit -sn 65535'

In other init or startup systems like systemd which are not started from a restricted shell like bash you can define these settings.

P.S. You should not restart squid because of a FD limit from a crontab job....
If a specific user abuses the service you should try create some firewall rules to limit clients requests bursts.

All The Best,
Elieser

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Vieri
Sent: Saturday, October 31, 2020 1:03 PM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] squid restart

Hi,

Around every hour or so, the Squid proxy client experience comes to a crawl.
It takes a very long time to load a simple web page.

This is a snapshot taken when this happens:

# squidclient mgr:info
HTTP/1.1 200 OK
Server: squid
Mime-Version: 1.0
Date: Sat, 31 Oct 2020 10:43:21 GMT
Content-Type: text/plain;charset=utf-8
Expires: Sat, 31 Oct 2020 10:43:21 GMT
Last-Modified: Sat, 31 Oct 2020 10:43:21 GMT
X-Cache: MISS from inf-fw1
X-Cache-Lookup: MISS from inf-fw1:3128
Connection: close

Squid Object Cache: Version 5.0.4-20201013-r6b13b73d3
Build Info:
Service Name: squid
Start Time:     Sat, 31 Oct 2020 04:50:45 GMT
Current Time:   Sat, 31 Oct 2020 10:43:21 GMT
Connection information for squid:
        Number of clients accessing cache:      320
        Number of HTTP requests received:       519828
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   1474.3
        Average ICP messages per minute since start:    0.0
        Select loop called: 9044075 times, 2.339 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 2.1%, 60min: 2.5%
        Hits as % of bytes sent:        5min: -68.9%, 60min: -402.5%
        Memory hits as % of hit requests:       5min: 78.3%, 60min: 62.3%
        Disk hits as % of hit requests: 5min: 0.0%, 60min: 1.7%
        Storage Swap size:      29040 KB
        Storage Swap capacity:  88.6% used, 11.4% free
        Storage Mem size:       29212 KB
        Storage Mem capacity:   89.1% used, 10.9% free
        Mean Object Size:       17.31 KB
        Requests given to unlinkd:      11815
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.04519  0.04776
        Cache Misses:          0.06286  0.06286
        Cache Hits:            0.00000  0.00000
        Near Hits:             0.04277  0.02317
        Not-Modified Replies:  0.00000  0.00000
        DNS Lookups:           0.00000  0.00000
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:        21155.513 seconds
        CPU Time:       1334.166 seconds
        CPU Usage:      6.31%
        CPU Usage, 5 minute avg:        8.60%
        CPU Usage, 60 minute avg:       9.88%
        Maximum Resident Size: 4287872 KB
        Page faults with physical i/o: 0
Memory accounted for:
        Total accounted:       744703 KB
        memPoolAlloc calls: 136343652
        memPoolFree calls:  140190831
File descriptor usage for squid:
        Maximum number of file descriptors:   4096
        Largest file desc currently in use:   4009
        Number of file desc currently in use: 3997
        Files queued for open:                   0
        Available number of file descriptors:   99
        Reserved number of file descriptors:   100
        Store Disk files open:                   0
Internal Data Structures:
          1852 StoreEntries
          1849 StoreEntries with MemObjects
          1754 Hot Object Cache Items
          1678 on-disk objects

If I issue the '-k reconfigure' command then the user experience is "great again".

A data snapshot taken right after the latter command shows this:

# squidclient mgr:info
HTTP/1.1 200 OK
Server: squid
Mime-Version: 1.0
Date: Sat, 31 Oct 2020 10:48:40 GMT
Content-Type: text/plain;charset=utf-8
Expires: Sat, 31 Oct 2020 10:48:40 GMT
Last-Modified: Sat, 31 Oct 2020 10:48:40 GMT
X-Cache: MISS from inf-fw1
X-Cache-Lookup: MISS from inf-fw1:3128
Connection: close

Squid Object Cache: Version 5.0.4-20201013-r6b13b73d3
Build Info:
Service Name: squid
Start Time:     Sat, 31 Oct 2020 10:46:51 GMT
Current Time:   Sat, 31 Oct 2020 10:48:40 GMT
Connection information for squid:
        Number of clients accessing cache:      179
        Number of HTTP requests received:       4663
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   2575.3
        Average ICP messages per minute since start:    0.0
        Select loop called: 51220 times, 2.121 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 0.3%, 60min: 0.3%
        Hits as % of bytes sent:        5min: 3.6%, 60min: 3.6%
        Memory hits as % of hit requests:       5min: 60.0%, 60min: 60.0%
        Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
        Storage Swap size:      10292 KB
        Storage Swap capacity:  31.4% used, 68.6% free
        Storage Mem size:       10456 KB
        Storage Mem capacity:   31.9% used, 68.1% free
        Mean Object Size:       15.07 KB
        Requests given to unlinkd:      4657
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.05046  0.05046
        Cache Misses:          0.06286  0.06286
        Cache Hits:            0.00000  0.00000
        Near Hits:             0.15048  0.15048
        Not-Modified Replies:  0.00000  0.00000
        DNS Lookups:           0.00000  0.00000
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:        108.639 seconds
        CPU Time:       10.588 seconds
        CPU Usage:      9.75%
        CPU Usage, 5 minute avg:        12.90%
        CPU Usage, 60 minute avg:       12.90%
        Maximum Resident Size: 462736 KB
        Page faults with physical i/o: 0
Memory accounted for:
        Total accounted:        37879 KB
        memPoolAlloc calls:   1256976
        memPoolFree calls:    1307898
File descriptor usage for squid:
        Maximum number of file descriptors:   4096
        Largest file desc currently in use:    567
        Number of file desc currently in use:  559
        Files queued for open:                   0
        Available number of file descriptors: 3537
        Reserved number of file descriptors:   100
        Store Disk files open:                   0
Internal Data Structures:
           997 StoreEntries
           997 StoreEntries with MemObjects
           683 Hot Object Cache Items
           683 on-disk objects

This did not happen with Squid 4, or maybe it wasn't as obvious.


I guess the reason could be for this:

        Maximum number of file descriptors:   4096
        Largest file desc currently in use:   4009
        Number of file desc currently in use: 3997

However, I set the following directive in squid.conf:

max_filedescriptors 65536

It doesn't seem to be honored here unless I stop and restart the squid service again (/etc/init.d/squid restart from command line):

File descriptor usage for squid:
        Maximum number of file descriptors:   65535

It seems that if I run the same command (/etc/init.d/squid restart) from crontab, that ulimit is not honored. I guess that's the root cause of my issue because I am asking cron to restart Squid once daily. I'll try not to, but I was hoping to see if there was a reliable way to fully restart the Squid process.

Vieri



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Wed Nov  4 02:27:22 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Nov 2020 21:27:22 -0500
Subject: [squid-users] websockets through Squid
In-Reply-To: <1304872121.821246.1603058873508@mail.yahoo.com>
References: <1626472963.180117.1602859307164.ref@mail.yahoo.com>
 <1626472963.180117.1602859307164@mail.yahoo.com>
 <d24b2583-4959-99d2-9d22-ba3141f595cd@measurement-factory.com>
 <1798348484.217408.1602863930213@mail.yahoo.com>
 <3f7b01cd-f71c-6fb0-27bf-74fbda532384@measurement-factory.com>
 <1304872121.821246.1603058873508@mail.yahoo.com>
Message-ID: <1c864fcf-f760-4a07-50f7-2ec107a6da74@measurement-factory.com>

On 10/18/20 6:07 PM, Vieri wrote:
> Please let me know if I should add something to the bug report

If you can retest with the triage patch attached to the bug report,
please do so an post new error messages produced by the patch (if any).
You can post them to the bug report (preferred) or here.

    https://bugs.squid-cache.org/show_bug.cgi?id=5084

FWIW, Christos has tested the site mentioned in Comment #5 of the bug
report and the triage patch did not show any obvious problems (just
connection closures but it is not clear to me whether those closures
were normal/expected). If your tests, with your problematic site, will
not show anything noteworthy while still failing, then we can rule out
bug 5084 as the suspect (the bug is real, but it may not affect your
specific use case).


Thank you,

Alex.




From ngtech1ltd at gmail.com  Wed Nov  4 13:08:02 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Wed, 4 Nov 2020 15:08:02 +0200
Subject: [squid-users] allow certian user ips to access only 2 domains
 and disallow everything
In-Reply-To: <646869700.68648.1602840077695@mail.yahoo.com>
References: <646869700.68648.1602840077695.ref@mail.yahoo.com>
 <646869700.68648.1602840077695@mail.yahoo.com>
Message-ID: <000001d6b2ab$8774aa50$965dfef0$@gmail.com>

Hey Simon,

 

I have seen these websites and it seems that some content which is used in them is from CDNs or other domains.

It?s very important to include specific domains like in the url:

https://code.jquery.com/jquery-3.3.1.min.js

 

For these sites to work properly.

 

You can try to run a more complex config which can or might take into account the Referrer header in the Request

It will probably only work if SSL bump is configured in your setup and it?s not the most secure way to allow sites.

I am only offering this as a it can be limited to specific domains such as cdns or specific hosted services.

 

All The Bests,

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of simon ben
Sent: Friday, October 16, 2020 12:21 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] allow certian user ips to access only 2 domains and disallow everything

 

I have squid running perfectly fine on centos 7 64 bit with no issues

I want to allow certain user ips to access a few sites and block everything else so below is the config

the sites are 

1) paloaltonetworks.com

2) redcloak.secureworks.com

 

in squid.conf

-------------------

acl userlist src "/etc/squid/userlist"

acl sitelist dstdomain "/etc/squid/sitelist"

http_access allow userlist sitelist

 

-------------------

 

user list file has the ips

-----------

192.168.62.128

192.168.62.1

192.168.62.129

192.168.61.1

192.168.62.130

192.168.62.3

192.168.61.128

172.16.120.160

------------------------------

 

site list file has the sites

----------------------------------------

.paloaltonetworks.com

.secureworks.com

https://ch-baladia.traps.paloaltonetworks.com

baladia.xdr.eu.paloaltonetworks.com

identity.paloaltonetworks.com

login.paloaltonetworks.com

assets.adobedtm.com

www.paloaltonetworks.com <http://www.paloaltonetworks.com> 

redcloak.secureworks.com

 

------------------------------------------------

 

I see that the first page and some links are working but some do not . also there is a huge deny logs in squid access logs

appreciate if you can advise me on how i can have the above access list so as to have minimum denies when being accessed from the above ips

 

 

Thanks and Regards

 

 

simon  

 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201104/a2fdf620/attachment.htm>

From ngtech1ltd at gmail.com  Wed Nov  4 13:13:00 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Wed, 4 Nov 2020 15:13:00 +0200
Subject: [squid-users] squid kerberos auth, acl note group
In-Reply-To: <19230339.6rUmX2M3JR@cairon>
References: <9473896.LjNUQkeJut@cairon>
 <704e36b3-4cd8-611c-0643-231c02045db6@treenet.co.nz>
 <rfhjuj$17js$1@ciao.gmane.io> <19230339.6rUmX2M3JR@cairon>
Message-ID: <001101d6b2ac$39543fb0$abfcbf10$@gmail.com>

Hey Klaus,

I tried to follow the thread and understand what went wrong and how it was fixed,
and I didn't manage to understand. (Maybe I am missing some emails in the thread)

Can you please clear out what was done to resolve this issue?

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Klaus Brandl
Sent: Monday, July 27, 2020 7:36 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid kerberos auth, acl note group

Hi Markus and Amos,

thanks for your answers, it is working now, after the group was deleted and 
created new. Most likely it was no security object...

Regards

On Saturday 25 July 2020 16:43:13 Markus Moeller wrote:
> Hi Klaus,
> 
>     Is the group you added a security group ?  Only security groups are part
> of the Kerberos ticket.  Which authorisation helper do you use or is this
> just based on the auth helper output ?
> 
>     What do you see on the client ?  e.g. in powershell run whoami /groups
> 
>     Did you clear the client Kerberos cache e.g. by login out and in again
> or use klist purge ?
> 
> 
> Markus
> 
> "Amos Jeffries"  wrote in message
> news:704e36b3-4cd8-611c-0643-231c02045db6 at treenet.co.nz...
> 
> On 25/07/20 2:48 am, Klaus Brandl wrote:
> > sorry, i did not found this script, and the binary is not available on our
> > product, because i'm no developer...
> 
> Darn. Okay that hinders testing a bit.
> 
> > But i think, we have a caching problem here, i found out, that the group
> > informations are only updated on a squid reconfigure.
> > 
> > And also the acl note group ... seems to be cached as long as squid is
> > restarted completely. I removed the configured group from the user, but i
> > could
> > see this group still maching in the cache.log, also after a reconfigure,
> > when
> > the auth_helper does not tell about this group any more.
> 
> The groups are attached to credentials which are attached to the TCP
> connection (TTL only as long as the connection is open) and a token
> replay cache for up to authenticate_ttl directive time (default 1 hour).
> 
> Setting that TTL to something very short, eg:
> 
>   authenticate_ttl 10 seconds
> 
> ... and disabling connection keep-alive:
> 
>   client_persistent_connections off
> 
> ... should work around the cache for testing. At least on HTTP traffic.
> HTTPS traffic goes through the proxy as a single tunnel request - so the
> entire HTTPS session is just one request/response pair to Squid.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Klaus

---

genua GmbH
Domagkstrasse 7, 85551 Kirchheim bei Muenchen
tel +49 89 991950-0, fax -999, www.genua.de

Geschaeftsfuehrer: Matthias Ochs, Marc Tesch
Amtsgericht Muenchen HRB 98238
genua ist ein Unternehmen der Bundesdruckerei-Gruppe.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From klaus_brandl at genua.de  Thu Nov  5 09:20:32 2020
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Thu, 5 Nov 2020 09:20:32 +0000
Subject: [squid-users] squid kerberos auth, acl note group
In-Reply-To: <001101d6b2ac$39543fb0$abfcbf10$@gmail.com>
References: <9473896.LjNUQkeJut@cairon>
 <704e36b3-4cd8-611c-0643-231c02045db6@treenet.co.nz>
 <rfhjuj$17js$1@ciao.gmane.io> <19230339.6rUmX2M3JR@cairon>
 <001101d6b2ac$39543fb0$abfcbf10$@gmail.com>
Message-ID: <0d3205a115c2f360113f2736b5158b4ca1dc2efd.camel@genua.de>

Hi Eliezer,

we have deleted the group in active directory and created it again.
Not sure, if this was the real problem, because this was done by our
customer.

But we have already this caching problem, if membership of this group
is changed in AD, squid has to be completely restartet to take effekt.

Regards

Klaus

Am Mittwoch, den 04.11.2020, 15:13 +0200 schrieb Eliezer Croitor:
> Hey Klaus,
> 
> I tried to follow the thread and understand what went wrong and how
> it was fixed,
> and I didn't manage to understand. (Maybe I am missing some emails in
> the thread)
> 
> Can you please clear out what was done to resolve this issue?
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On
> Behalf Of Klaus Brandl
> Sent: Monday, July 27, 2020 7:36 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid kerberos auth, acl note group
> 
> Hi Markus and Amos,
> 
> thanks for your answers, it is working now, after the group was
> deleted and 
> created new. Most likely it was no security object...
> 
> Regards
> 
> On Saturday 25 July 2020 16:43:13 Markus Moeller wrote:
> > Hi Klaus,
> > 
> >     Is the group you added a security group ?  Only security groups
> > are part
> > of the Kerberos ticket.  Which authorisation helper do you use or
> > is this
> > just based on the auth helper output ?
> > 
> >     What do you see on the client ?  e.g. in powershell run whoami
> > /groups
> > 
> >     Did you clear the client Kerberos cache e.g. by login out and
> > in again
> > or use klist purge ?
> > 
> > 
> > Markus
> > 
> > "Amos Jeffries"  wrote in message
> > news:704e36b3-4cd8-611c-0643-231c02045db6 at treenet.co.nz...
> > 
> > On 25/07/20 2:48 am, Klaus Brandl wrote:
> > > sorry, i did not found this script, and the binary is not
> > > available on our
> > > product, because i'm no developer...
> > 
> > Darn. Okay that hinders testing a bit.
> > 
> > > But i think, we have a caching problem here, i found out, that
> > > the group
> > > informations are only updated on a squid reconfigure.
> > > 
> > > And also the acl note group ... seems to be cached as long as
> > > squid is
> > > restarted completely. I removed the configured group from the
> > > user, but i
> > > could
> > > see this group still maching in the cache.log, also after a
> > > reconfigure,
> > > when
> > > the auth_helper does not tell about this group any more.
> > 
> > The groups are attached to credentials which are attached to the
> > TCP
> > connection (TTL only as long as the connection is open) and a token
> > replay cache for up to authenticate_ttl directive time (default 1
> > hour).
> > 
> > Setting that TTL to something very short, eg:
> > 
> >   authenticate_ttl 10 seconds
> > 
> > ... and disabling connection keep-alive:
> > 
> >   client_persistent_connections off
> > 
> > ... should work around the cache for testing. At least on HTTP
> > traffic.
> > HTTPS traffic goes through the proxy as a single tunnel request -
> > so the
> > entire HTTPS session is just one request/response pair to Squid.
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> > 
> > 
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> Klaus
> 
> ---
> 
> genua GmbH
> Domagkstrasse 7, 85551 Kirchheim bei Muenchen
> tel +49 89 991950-0, fax -999, www.genua.de
> 
> Geschaeftsfuehrer: Matthias Ochs, Marc Tesch
> Amtsgericht Muenchen HRB 98238
> genua ist ein Unternehmen der Bundesdruckerei-Gruppe.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

From ngtech1ltd at gmail.com  Fri Nov  6 10:52:44 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Fri, 6 Nov 2020 12:52:44 +0200
Subject: [squid-users] squid kerberos auth, acl note group
In-Reply-To: <0d3205a115c2f360113f2736b5158b4ca1dc2efd.camel@genua.de>
References: <9473896.LjNUQkeJut@cairon>	
 <704e36b3-4cd8-611c-0643-231c02045db6@treenet.co.nz>	
 <rfhjuj$17js$1@ciao.gmane.io> <19230339.6rUmX2M3JR@cairon>	
 <001101d6b2ac$39543fb0$abfcbf10$@gmail.com>
 <0d3205a115c2f360113f2736b5158b4ca1dc2efd.camel@genua.de>
Message-ID: <003901d6b42a$f85d0480$e9170d80$@gmail.com>

Thanks.

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: Klaus Brandl <klaus_brandl at genua.de> 
Sent: Thursday, November 5, 2020 11:21 AM
To: ngtech1ltd at gmail.com
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid kerberos auth, acl note group

Hi Eliezer,

we have deleted the group in active directory and created it again.
Not sure, if this was the real problem, because this was done by our
customer.

But we have already this caching problem, if membership of this group
is changed in AD, squid has to be completely restartet to take effekt.

Regards

Klaus

Am Mittwoch, den 04.11.2020, 15:13 +0200 schrieb Eliezer Croitor:
> Hey Klaus,
> 
> I tried to follow the thread and understand what went wrong and how
> it was fixed,
> and I didn't manage to understand. (Maybe I am missing some emails in
> the thread)
> 
> Can you please clear out what was done to resolve this issue?
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On
> Behalf Of Klaus Brandl
> Sent: Monday, July 27, 2020 7:36 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid kerberos auth, acl note group
> 
> Hi Markus and Amos,
> 
> thanks for your answers, it is working now, after the group was
> deleted and 
> created new. Most likely it was no security object...
> 
> Regards
> 
> On Saturday 25 July 2020 16:43:13 Markus Moeller wrote:
> > Hi Klaus,
> > 
> >     Is the group you added a security group ?  Only security groups
> > are part
> > of the Kerberos ticket.  Which authorisation helper do you use or
> > is this
> > just based on the auth helper output ?
> > 
> >     What do you see on the client ?  e.g. in powershell run whoami
> > /groups
> > 
> >     Did you clear the client Kerberos cache e.g. by login out and
> > in again
> > or use klist purge ?
> > 
> > 
> > Markus
> > 
> > "Amos Jeffries"  wrote in message
> > news:704e36b3-4cd8-611c-0643-231c02045db6 at treenet.co.nz...
> > 
> > On 25/07/20 2:48 am, Klaus Brandl wrote:
> > > sorry, i did not found this script, and the binary is not
> > > available on our
> > > product, because i'm no developer...
> > 
> > Darn. Okay that hinders testing a bit.
> > 
> > > But i think, we have a caching problem here, i found out, that
> > > the group
> > > informations are only updated on a squid reconfigure.
> > > 
> > > And also the acl note group ... seems to be cached as long as
> > > squid is
> > > restarted completely. I removed the configured group from the
> > > user, but i
> > > could
> > > see this group still maching in the cache.log, also after a
> > > reconfigure,
> > > when
> > > the auth_helper does not tell about this group any more.
> > 
> > The groups are attached to credentials which are attached to the
> > TCP
> > connection (TTL only as long as the connection is open) and a token
> > replay cache for up to authenticate_ttl directive time (default 1
> > hour).
> > 
> > Setting that TTL to something very short, eg:
> > 
> >   authenticate_ttl 10 seconds
> > 
> > ... and disabling connection keep-alive:
> > 
> >   client_persistent_connections off
> > 
> > ... should work around the cache for testing. At least on HTTP
> > traffic.
> > HTTPS traffic goes through the proxy as a single tunnel request -
> > so the
> > entire HTTPS session is just one request/response pair to Squid.
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> > 
> > 
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> 
> Klaus
> 
> ---
> 
> genua GmbH
> Domagkstrasse 7, 85551 Kirchheim bei Muenchen
> tel +49 89 991950-0, fax -999, www.genua.de
> 
> Geschaeftsfuehrer: Matthias Ochs, Marc Tesch
> Amtsgericht Muenchen HRB 98238
> genua ist ein Unternehmen der Bundesdruckerei-Gruppe.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From leolistas at solutti.com.br  Fri Nov  6 21:18:23 2020
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Fri, 6 Nov 2020 18:18:23 -0300
Subject: [squid-users] issues with sslbump and "Host header forgery
 detected" warnings
Message-ID: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>


 ??? Hello Everyone,

 ??? I'm trying to setup sslbump for the first time (on squid-4.13) and, 
at first, things seems to be working. After taking some time to 
understand the new terms (splice, bump, stare, etc), seems to got things 
somehow working.

 ??? Actually i'm NOT looking for complete bumping (and decrypting) the 
connections. During my lab studies, I found out that simply 'splice' the 
connections is enough for me. I just wanna intercept https connections 
and have them logged, just the hostname, and that seems to be 
acchievable without even installing my certificates on the client, as 
i'm not changing anything, just 'taking a look' on the SNI values of the 
connection. The connection itself remains end-to-end protected, and 
that's fine to me. I just wanna have things logged. And that's working 
just fine.

 ??? However, some connections are failing with the "Host header forgery 
detected" warnings. Example:

2020/11/06 18:04:21 kid1| SECURITY ALERT: Host header forgery detected 
on local=216.58.222.106:443 remote=10.4.1.123:39994 FD 73 flags=33 
(local IP does not match any domain IP)
2020/11/06 18:04:21 kid1| SECURITY ALERT: on URL: 
chromesyncpasswords-pa.googleapis.com:443

 ??? and usually a NONE/409 (Conflict) log entry is generated on those. 
Refreshing once or twice and it will eventually work.

 ??? I have found several discussions on this and I can confirm it 
happens on hostnames that resolvs to several different IPs or hostnames 
that, somehow, keeps changing IPs (CDNs or something like that).

 ??? Clients are already using the same DNS server as the squid box, as 
recommended, but problem is still happening quite a lot. For regular 
hostnames who translates for a single IP address, things are 100% working.

 ??? Questions:

 ??? - without using WPAD or without configuring proxy on the client 
devices, is this somehow "fixable" ? Same DNS already being used ...
 ??? - is there any chance the NONE/409 (Conflict) logs i'm seeing are 
not related to this? Maybe these are just WARNINGs and not ERRORs, or 
these would really cause a fail to the intercepted connection?
 ??? - any other hint on this one without having to set proxy, in any 
way, on the clients? I just wanna have hostnames (and traffic generated) 
logged, no need for full decrypt (bumping) of the connections.


 ??? Thanks !!!






-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From squid3 at treenet.co.nz  Sat Nov  7 11:42:36 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 8 Nov 2020 00:42:36 +1300
Subject: [squid-users] issues with sslbump and "Host header forgery
 detected" warnings
In-Reply-To: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
References: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
Message-ID: <76ad43b4-5f02-1302-a231-b19feca7b1a6@treenet.co.nz>

On 7/11/20 10:18 am, Leonardo Rodrigues wrote>
>  ??? However, some connections are failing with the "Host header forgery 
> detected" warnings. Example:
> 
...
>  ??? Questions:
> 
>  ??? - without using WPAD or without configuring proxy on the client 
> devices, is this somehow "fixable" ? Same DNS already being used ...

All we can do is minimize the occurrences (sometimes not very much). 
This wiki page has all the details of why and workarounds 
<https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>.


>  ??? - is there any chance the NONE/409 (Conflict) logs i'm seeing are 
> not related to this? Maybe these are just WARNINGs and not ERRORs, or 
> these would really cause a fail to the intercepted connection?

No. The only time current Squid produce 409 status is these Host header 
problems.

>  ??? - any other hint on this one without having to set proxy, in any 
> way, on the clients? I just wanna have hostnames (and traffic generated) 
> logged, no need for full decrypt (bumping) of the connections.
> 

No, sorry.


Amos


From rousskov at measurement-factory.com  Sat Nov  7 16:18:04 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 7 Nov 2020 11:18:04 -0500
Subject: [squid-users] issues with sslbump and "Host header forgery
 detected" warnings
In-Reply-To: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
References: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
Message-ID: <bbb0cd4a-50f7-b584-5304-652b2fb58a9c@measurement-factory.com>

On 11/6/20 4:18 PM, Leonardo Rodrigues wrote:

> 2020/11/06 18:04:21 kid1| SECURITY ALERT: Host header forgery detected...
> 2020/11/06 18:04:21 kid1| SECURITY ALERT: on URL:...

> - without using WPAD or without configuring proxy on the client
> devices, is this somehow "fixable" ?

Yes, it is possible to modify Squid to correctly forward the intercepted
TCP connection where it was going despite the host validation failure.
Squid already does that for some use cases, but that code currently
excludes intercepted TLS connections (among other cases).


> ??? - is there any chance the NONE/409 (Conflict) logs i'm seeing are
> not related to this? Maybe these are just WARNINGs and not ERRORs, or
> these would really cause a fail to the intercepted connection?

Bugs notwithstanding, your Squid is terminating the TLS connection after
printing the _second_ ALERT line above:

> debugs(85, DBG_IMPORTANT, "SECURITY ALERT: on URL: " << ...
> // IP address validation for Host: failed. reject the connection.
> repContext->setReplyToError(ERR_CONFLICT_HOST, Http::scConflict,

The documented intent of that code path is to eventually terminate the
TLS connection without contacting the origin server. I suspect that is
what actually happens, but I did not check. You can check this
(indirectly) by reproducing the problem while capturing client<->Squid
and Squid->server traffic.


> ??? - any other hint on this one without having to set proxy, in any
> way, on the clients? I just wanna have hostnames (and traffic generated)
> logged, no need for full decrypt (bumping) of the connections.

You have a use case where connection rejection is not the best handling
option. Implementing and defending proper support for that specific use
case is probably not trivial, but I think it is doable.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


From david at articatech.com  Sat Nov  7 19:08:56 2020
From: david at articatech.com (David Touzeau)
Date: Sat, 7 Nov 2020 20:08:56 +0100
Subject: [squid-users] Squid4/5: Feature request identify access rules.
Message-ID: <792dbb10-b9dc-a0e7-11bc-e34b84394926@articatech.com>

When having several *_access http_access,reply_access...
In a stressed environment, it is difficult to hunt an issue or a wrong rule.

The debug mode is impossible because the proxy in production mode write too many logs..

But if we can identify the rule and add pointer to the log, it is possible to see a wrong rule or to see that a request is correctly passed trough.

Currently we have to do

acl acl1 src 1.2.3.4
http_access deny acl1



We suggest using the same token used in http_port:

acl acl1 src 1.2.3.4
http_access deny acl1 rulename=Rule.access1

And add a token for template eg %RULENAME and a token for logformat %rname that helps to identify the token.


Added in bugtrack

https://bugs.squid-cache.org/show_bug.cgi?id=5087


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201107/fdeeff0e/attachment.htm>

From ngtech1ltd at gmail.com  Sun Nov  8 01:08:03 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Sun, 8 Nov 2020 03:08:03 +0200
Subject: [squid-users] Best practice for adding or removing ACLs
 dynamically ?
In-Reply-To: <005001d6afe7$ab52ba00$01f82e00$@gmail.com>
References: <75C05DB7-2C0E-4B77-88A4-66F581A035ED@gmail.com>
 <005001d6afe7$ab52ba00$01f82e00$@gmail.com>
Message-ID: <006d01d6b56b$9c57b490$d5071db0$@gmail.com>

Following up.

Well, github and google is full of horrible ways of implementing the tcp_outgoing_address selection so..
I have written a sketch of the concept I was writing about based on ncsa_auth which I must admit is far more .. simple then using a DB like mysql or LDAP.

The config I found was using either:
- multiple inbound squid ports -> mapped each to a single tcp_outgoing_addrees
- multiple static usernames mapped to a specific address(based on either username or usernamer+dst-domain.. else ACLs types)
- Others which are not relevant since these break the internet

What I am offering is an example which offloads the actual selection code/algorithm and config into an external_acl_type helper.
The result would be that squid will only listen(when possible and required) only on one port.
(which means that adding more ports can multiply/power up the service config "capacity", like vlans ontop of vlans)
Also the authentication mechanism would be ncsa_auth which is good for up to 1000s++ of users since the helpers store the htaccess file In RAM.
Even if the file will contain 10k users and the auth helpers in use would be let say 40 and up, the proxy would still consume less Memory on the
storage of the htaccess file in RAM compared to a MySQL or LDAP DB.

To change an address for a specific existing user you can use a simple sed command:
sed -i -E "s at user80[\ \t]+[0-9]+ at user80 1 at g" /etc/squid/user-to-ip.txt

and to change password for a user you can use a simple htaccess command:
htpasswd -b /etc/squid/htpasswd  user80 1234


Within my external_acl_helper I have used a similar approach to the one the ncsa_auth helper uses to reload modified files from disk.
(If the mtime stamp is being changed from the old one the helper would reload the mapping config file from disk)
* https://github.com/elico/vagrant-squid-outgoing-addresses/blob/master/shared/note.rb

To demonstrate this piece of code I have used Vagrant on-top of Virtualbox.
(I have verified that on the current Windows 10 PRO installations both Virtualbox and Hyper-v can be used simultaneously with CPU Virtualization support ON)

The example lab is at:
https://github.com/elico/vagrant-squid-outgoing-addresses

The test is composed of two nodes:
* Squid
* Web

The Squid instance will be populated with more then 20 IP addresses on the same subnet.
For simplicity I have added a simple /32 address to the Squid+Web private/closed network interface.
There are other ways which can be used in production when Routing daemons are in the picture.

To start the demo/test you can use:
vagrant up
vagrant ssh squid --command /vagrant/test-random-ips-assignment.sh

The user to ip config mapping file is at:
/etc/squid/user-to-ip.txt

I have used a simple "<user> <tab or space> <ip integer id>" mapping syntax.

Technically speaking this setup can work with scales of 1000s and up of addresses and clients.
I am pretty sure it's good enough for proxy services which uses multiple routing tunnels and/or BGP feeds.

If anyone wants to ask about this setup I will be more than happy to receive emails about it here in the list.

All The Bests,

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: Eliezer Croitor <ngtech1ltd at gmail.com> 
Sent: Sunday, November 1, 2020 2:41 AM
To: squid-users at lists.squid-cache.org
Cc: 'roee klinger' <roeeklinger60 at gmail.com>
Subject: RE: [squid-users] Best practice for adding or removing ACLs dynamically ?

Hey Roee,

I am trying to understand what part of squid.conf you want to be able to change without a reconfigure/reload?
If you have many users, ie above 50 you should probably not use a simple ncsa_auth although  it's possible and in more then one case is preferable.
You could probably write your own basic auth helper that will interact with a DB which will probably simplify your whole setup.
(You can use existing basic auth helpers with mysql or ldap)

As for the tcp_outgoing_address, it?s a whole different story.
Since it's a "fast" acl type the options to do something dynamic with it are an issue.
(Maybe eCAP/ICAP service or a "pre-cooked" note or other factor to the acl can be used)

I am pretty sure that if an authentication service can reply with a note ie connection annotation then it can be used for the address selection.
One issue with it is that It will be valid for the next X ttl seconds/minutes/hours.

I do believe that there should be a way to allow something like external_acl helper to affect this squid feature.
I was thinking that an eCAP or an ICAP service or an external_acl helper can add a note for a connection based on couple other factors like:
* src ip
* auth username
* request domain or request sni
* ...

So let say the proxy will have a set of 100 addresses, each will have a single specific matching acl for a request header or connection annotation/note.
This way the selection of a tcp_outgoing_address would be a little less complex the it is today.

I have couple other ideas for implementations which I have experimented with but the proxy admin need to learn how these work which might be
a bit complicated some times.

Eliezer
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of roee klinger
Sent: Saturday, October 31, 2020 2:35 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Best practice for adding or removing ACLs dynamically ?

?
Hey,
I have Squid configured to send users to different outgoing interface like so:

..
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/htpassword
acl acl_for_user3002 proxy_auth user2
tcp_outgoing_address 192.168.8.12 acl_for_user3002
http_port 3002 name=3002
http_access allow authenticated
..

When I wanted to change the username:password for user2, I run a bash script to change it in squid.conf and also in htpassword and then I run "squid -k reconfigure", if I don't reconfigure the old user still has access to the proxy and the new one doesn't for about 30 minutes.

I am expecting to have 100s of users soon that will change credentials often, and also I would like to blacklist websites often and on the fly, so I was searching for a better way to manage this without reconfiguring every time, since sometimes a reconfigure can take up to 10-15 seconds.

I am new to Squid and wasn't able to find any info on this, am I doing this currently or there is a better way to change users/ACLs on the fly without reloading Squid?

Thanks,
Roee Klinger
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From ngtech1ltd at gmail.com  Sun Nov  8 01:19:09 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Sun, 8 Nov 2020 03:19:09 +0200
Subject: [squid-users] issues with sslbump and "Host header forgery
 detected" warnings
In-Reply-To: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
References: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
Message-ID: <006f01d6b56d$297b0880$7c711980$@gmail.com>

Hey Leonardo,

I assume The best solution for you is a simple SNI proxy.
Squid does also that and you can try to debug this issue to make sure you understand what is wrong.
It clearly states that Squid doesn't see this specific address: local=216.58.222.106:443
As the domain: chromesyncpasswords-pa.googleapis.com:443 "real" destination address.

Maybe Alex or Amos remember the exact and relevant debug_options:
https://wiki.squid-cache.org/KnowledgeBase/DebugSections

I assume section 78 would be of help.
debug_options ALL,1 78,3

Is probably enough to discover what are the DNS responses and from where these are.
On what OS are you running this Squid?

Thanks,

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Leonardo Rodrigues
Sent: Friday, November 6, 2020 11:18 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] issues with sslbump and "Host header forgery detected" warnings


     Hello Everyone,

     I'm trying to setup sslbump for the first time (on squid-4.13) and, 
at first, things seems to be working. After taking some time to 
understand the new terms (splice, bump, stare, etc), seems to got things 
somehow working.

     Actually i'm NOT looking for complete bumping (and decrypting) the 
connections. During my lab studies, I found out that simply 'splice' the 
connections is enough for me. I just wanna intercept https connections 
and have them logged, just the hostname, and that seems to be 
acchievable without even installing my certificates on the client, as 
i'm not changing anything, just 'taking a look' on the SNI values of the 
connection. The connection itself remains end-to-end protected, and 
that's fine to me. I just wanna have things logged. And that's working 
just fine.

     However, some connections are failing with the "Host header forgery 
detected" warnings. Example:

2020/11/06 18:04:21 kid1| SECURITY ALERT: Host header forgery detected 
on local=216.58.222.106:443 remote=10.4.1.123:39994 FD 73 flags=33 
(local IP does not match any domain IP)
2020/11/06 18:04:21 kid1| SECURITY ALERT: on URL: 
chromesyncpasswords-pa.googleapis.com:443

     and usually a NONE/409 (Conflict) log entry is generated on those. 
Refreshing once or twice and it will eventually work.

     I have found several discussions on this and I can confirm it 
happens on hostnames that resolvs to several different IPs or hostnames 
that, somehow, keeps changing IPs (CDNs or something like that).

     Clients are already using the same DNS server as the squid box, as 
recommended, but problem is still happening quite a lot. For regular 
hostnames who translates for a single IP address, things are 100% working.

     Questions:

     - without using WPAD or without configuring proxy on the client 
devices, is this somehow "fixable" ? Same DNS already being used ...
     - is there any chance the NONE/409 (Conflict) logs i'm seeing are 
not related to this? Maybe these are just WARNINGs and not ERRORs, or 
these would really cause a fail to the intercepted connection?
     - any other hint on this one without having to set proxy, in any 
way, on the clients? I just wanna have hostnames (and traffic generated) 
logged, no need for full decrypt (bumping) of the connections.


     Thanks !!!






-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From leolistas at solutti.com.br  Mon Nov  9 13:37:17 2020
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 9 Nov 2020 10:37:17 -0300
Subject: [squid-users] issues with sslbump and "Host header forgery
 detected" warnings
In-Reply-To: <76ad43b4-5f02-1302-a231-b19feca7b1a6@treenet.co.nz>
References: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
 <76ad43b4-5f02-1302-a231-b19feca7b1a6@treenet.co.nz>
Message-ID: <137e5fdb-a54d-d684-fab1-78b941f64e3c@solutti.com.br>

Em 07/11/2020 08:42, Amos Jeffries escreveu:
>
> All we can do is minimize the occurrences (sometimes not very much). 
> This wiki page has all the details of why and workarounds 
> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>.
>

 ??? Thanks Amos, I had already find that and it has very good 
information on the subject. Also found an old thread of you discussing 
the security concerns on bypsasing those checks, very good information, 
thanks so much :)


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From leolistas at solutti.com.br  Mon Nov  9 13:41:20 2020
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 9 Nov 2020 10:41:20 -0300
Subject: [squid-users] issues with sslbump and "Host header forgery
 detected" warnings
In-Reply-To: <006f01d6b56d$297b0880$7c711980$@gmail.com>
References: <e1aa0d21-5ed2-b64e-dbdf-4f500aa8bf37@solutti.com.br>
 <006f01d6b56d$297b0880$7c711980$@gmail.com>
Message-ID: <355e9d18-e781-33ed-c885-6a94f8bb7f14@solutti.com.br>

Em 07/11/2020 22:19, Eliezer Croitor escreveu:
> Hey Leonardo,
>
> I assume The best solution for you is a simple SNI proxy.
> Squid does also that and you can try to debug this issue to make sure you understand what is wrong.
> It clearly states that Squid doesn't see this specific address: local=216.58.222.106:443
> As the domain: chromesyncpasswords-pa.googleapis.com:443 "real" destination address.
>
> Maybe Alex or Amos remember the exact and relevant debug_options:
> https://wiki.squid-cache.org/KnowledgeBase/DebugSections
>
> I assume section 78 would be of help.
> debug_options ALL,1 78,3
>
> Is probably enough to discover what are the DNS responses and from where these are.
> On what OS are you running this Squid?
>
>

 ??? Hi Eliezer,

 ??? I have already tracked the DNS stuff and I could confirm that squid 
is resolving to a different IP address than the client is, despite both 
using the same DNS server. It only happens for hosts with multiple A 
addresses or CDN hostnames that changes IP very often (every 10 seconds 
for example). It's not a bug in that regards, absolutely not, the client 
connecting to a specific IP address and squid seeing another IP to that 
hostname caught on the TLS transaction is real.

 ??? I'm running on CentOS 8 ... and after all, maybe these findings, 
I'm about to realize doing this kind of interception, even without the 
full decrypt part, is not trivial at all, despite it works flawlessly 
(and very easily) for "regular" hostnames which translates to a single 
IP and never changes it.

 ??? Will study this a little more. Thanks for your observations and 
recommendations!



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From roierachamim at gmail.com  Wed Nov 11 11:46:18 2020
From: roierachamim at gmail.com (roie rachamim)
Date: Wed, 11 Nov 2020 13:46:18 +0200
Subject: [squid-users] Using ICAP adaptation with dynamic routing
Message-ID: <CAD=NrcCXMbCah7dm7puFicerCZcM6SHQ4HW7h_pnoKHU9m3jZg@mail.gmail.com>

Hi,

In my setup i'm using two adaptation chains for requests and for responses:
(a) REQ_ICAP1 -> REQ_ICAP2
(b) RESP_ICAP1 -> RESP_ICAP2

I'm using REQ_ICAP1 with routing=1 to be able to change the routing
dynamically
The default flow is REQ_ICAP1->REQ_ICAP2->RESP_ICAP1->RESP_ICAP2

using the dynamic routing i'm able to skip REQ_ICAP2 (by setting
X-Next-Services to 'RESP_ICAP1, RESP_ICAP2')
I'm also able to skip RESP_ICAP1 (by setting X-Next-Services to RESP_ICAP2')
What i didn't manage to do is to stop adaption completely by returning an
empty value for X-Next-Services

According to the documentation: "An empty X-Next-Services value results in
an empty plan which ends the current adaptation."

So i'm not sure what am i missing. Maybe with empty value it simply skips
to the response chain?
Should i use the adaptations in a different configuration?

Any help would be appreciated

Many Thanks,
Roie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201111/693c7825/attachment.htm>

From l.marcantonio at proxind.it  Wed Nov 11 11:56:32 2020
From: l.marcantonio at proxind.it (Lorenzo Marcantonio)
Date: Wed, 11 Nov 2020 12:56:32 +0100
Subject: [squid-users] Troubleshooting certificate issues
Message-ID: <20201111115632.vcsrlcgrua3ij4v2@nekomimi.proxind.it>

I'm using 4.13 with libressl 3.2.2 and SSL bumps. Most of the time
it works (e.g. google). Some other time it get me back a 'fake untrusted'
certificate (like CN=Not trusted by \"proxy.proxind.it\") and this of
course gives user issues.

In the logs I see lines like

2020-11-11 12:47:59.314124500  L   290 192.168.2.102 NONE/200 0 CONNECT www.selcdn.ru:443 - HIER_DIRECT/92.53.68.204 - /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=RapidSSL RSA CA 2018 X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY at depth=2

which suggest something missing in the certificate store.

However testing with openssl verify the certificate from the server
(extracted with a browser *outside* the squid network) it verifies OK.

The certs.pem file is the same (I checked:P) so I don't get why it
should fail. In fact I ensured it with tls_outgoing_options cafile=/var/lib/openssl/certs.pem

Any ideas on how to solve/troubleshoot/workaround the problem?

-- 
Lorenzo Marcantonio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201111/bdbc91a4/attachment.sig>

From l.marcantonio at proxind.it  Wed Nov 11 14:30:12 2020
From: l.marcantonio at proxind.it (Lorenzo Marcantonio)
Date: Wed, 11 Nov 2020 15:30:12 +0100
Subject: [squid-users] Troubleshooting certificate issues
In-Reply-To: <20201111141319.q3negvs7jaufwalk@bloms.de>
References: <20201111115632.vcsrlcgrua3ij4v2@nekomimi.proxind.it>
 <20201111141319.q3negvs7jaufwalk@bloms.de>
Message-ID: <20201111143012.mfgb5uetegpqaa2j@nekomimi.proxind.it>

On Wed, Nov 11, 2020 at 03:13:19PM +0100, Dieter Bloms wrote:
> for me it looks like the server doesn't deliver the intermdeiate
> certificate and your squid proxy doesn't download this certificate
> itself.

Well, squid couln't download even if wanted if it isn't supplied by the
server. AFAIK there is no field in the certificate to hold an url to
download the signer one. In fact in the past I had to put some
intermediates in the cert store (OK, not great, not recommended, but at
least it works).

That aside, if I save the certificate as a PEM from the browser (*only*
the certificate, not the whole chain) and I do an openssl verify on it
it validates, so in the store there are all the certs needed to verify
it. I even tried doing it as the squid user in case of permission
issues.

For some reason squid doesn't like *some* certificates. And I don't
think that so many sites anyway send incomplete chains.

-- 
Lorenzo Marcantonio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201111/d7dfd608/attachment.sig>

From manojwajekar93 at gmail.com  Wed Nov 11 15:19:37 2020
From: manojwajekar93 at gmail.com (Manoj Wajekar)
Date: Wed, 11 Nov 2020 10:19:37 -0500
Subject: [squid-users] TLS renegotiation failing between squids in hierarchy
 in Squid 4.
Message-ID: <CABrhxqvkgMMS3s914kWea5FqM+Ew4D6zu7sE=xY8jFNZBT8-Vg@mail.gmail.com>

Hi,

I am currently squid-cache in hierarchy setup, with TLS enabled throughout.

client --> child Squid --> parent Squid --> web server

Openssl version: 1.0.2k
This setup is working for 3.5.20.

But when I updated to squid 4(tried 4.8, 4.11 and 4.13),
initial HTTP request goes through, but TLS renegotiation is failing between
child and parent squid for the following requests.

>From the logs, it looks like child squid is trying to initialize TLS
renegotiating using old TLS session ID, but parent squid is rejecting
session resumption.

I confirm this behavior using openssl s_client --reconnect option.

I tried to disabled client initialed TLS renegotiating by setting
tls-options=NO_TICKET (on child squid), but it is affecting the behavior.

Are there any changes in default TLS renegotiation behavior between squid
3.5 and 4.x?
Is there a way to disable the client (child squid) initialized TLS
renegotiation in squid 4?

Thanks,
Manoj
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201111/d26ae293/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov 11 15:58:44 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Nov 2020 10:58:44 -0500
Subject: [squid-users] Using ICAP adaptation with dynamic routing
In-Reply-To: <CAD=NrcCXMbCah7dm7puFicerCZcM6SHQ4HW7h_pnoKHU9m3jZg@mail.gmail.com>
References: <CAD=NrcCXMbCah7dm7puFicerCZcM6SHQ4HW7h_pnoKHU9m3jZg@mail.gmail.com>
Message-ID: <cdf10d9f-2585-4f8d-57e9-cde4a861d4f3@measurement-factory.com>

On 11/11/20 6:46 AM, roie rachamim wrote:

> The default flow is REQ_ICAP1->REQ_ICAP2->RESP_ICAP1->RESP_ICAP2

> using the dynamic routing i'm able to skip REQ_ICAP2 (by setting
> X-Next-Services to 'RESP_ICAP1, RESP_ICAP2')
> I'm also able to skip RESP_ICAP1 (by setting X-Next-Services to RESP_ICAP2')
> What i didn't manage to do is to stop adaption completely by returning
> an empty value for X-Next-Services

> According to the documentation: "An empty X-Next-Services value results
> in an empty plan which ends the current adaptation."

> So i'm not sure what am i missing. Maybe with empty value it simply
> skips to the response chain?

If that is what happens, then it is probably a Squid bug. AFAICT, an
empty plan returned by a routing REQMOD service should prevent RESPMOD
adaptation by design -- a routing service is supposed to have full
routing control.

I checked the code that processes X-Next-Services and did not notice any
red flags, but the bug may be located deeper in the code, where Squid
reacts to the remaining adaptation plan. I suspect somewhere an empty
plan is misinterpreted as no plan.

If the bug is confirmed, we may have to preserve it for backward
compatibility sake and add two special X-Next-Services values (e.g.,
"END_ADAPTATION" and "JUMP_TO_RESPMOD") to allow updated services to
distinguish the two use cases instead of returning an empty list with an
arguably ambiguous meaning.


> Should i use the adaptations in a different configuration?

Ideally, the bug should be confirmed and fixed. If you cannot
(facilitate a) fix, then you may be able to work around the bug using
service-set annotations and adaptation_access directives (for all or
just the buggy cases). Unfortunately, I do not remember whether Squid
supports setting transaction annotations using ICAP services. IIRC,
Squid supports that for eCAP services.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Nov 11 16:45:26 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Nov 2020 11:45:26 -0500
Subject: [squid-users] Troubleshooting certificate issues
In-Reply-To: <20201111115632.vcsrlcgrua3ij4v2@nekomimi.proxind.it>
References: <20201111115632.vcsrlcgrua3ij4v2@nekomimi.proxind.it>
Message-ID: <20f7c102-e085-b1e8-81bc-78c65ee65c96@measurement-factory.com>

On 11/11/20 6:56 AM, Lorenzo Marcantonio wrote:
> I'm using 4.13 with libressl 3.2.2 and SSL bumps.

FYI: Libressl-based builds are not officially supported. I do not know
whether libressl is a factor here.


> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY at depth=2

If the connection is using TLS v1.3, then you may be suffering from Bug
5067: https://bugs.squid-cache.org/show_bug.cgi?id=5067

Alex.


From rousskov at measurement-factory.com  Wed Nov 11 17:15:14 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Nov 2020 12:15:14 -0500
Subject: [squid-users] TLS renegotiation failing between squids in
 hierarchy in Squid 4.
In-Reply-To: <CABrhxqvkgMMS3s914kWea5FqM+Ew4D6zu7sE=xY8jFNZBT8-Vg@mail.gmail.com>
References: <CABrhxqvkgMMS3s914kWea5FqM+Ew4D6zu7sE=xY8jFNZBT8-Vg@mail.gmail.com>
Message-ID: <6841aad3-09a1-c0b5-6d7f-35e7b754ee6e@measurement-factory.com>

On 11/11/20 10:19 AM, Manoj Wajekar wrote:

> I am currently squid-cache in hierarchy setup, with TLS enabled throughout.
> 
> client --> child Squid --> parent Squid --> web server

Do you use SslBump anywhere?


> Openssl version: 1.0.2k
> This setup is working for 3.5.20.

> But when I updated to squid 4(tried 4.8, 4.11 and 4.13),

Does all of the above apply to both child and parent Squids? Or just the
child?


> initial HTTP request goes through, but TLS renegotiation is failing
> between child and parent squid for the following requests.
> 
> From the logs, it looks like child squid is trying to initialize TLS
> renegotiating using old TLS session ID, but parent squid is rejecting
> session resumption.
> 
> I confirm this behavior using openssl s_client --reconnect option.
> ?
> I tried to disabled client initialed TLS renegotiating by setting
> tls-options=NO_TICKET (on child squid), but it is affecting the behavior.

Did you mean to say "_not_ affecting the behavior"?


> Are there any changes in default TLS renegotiation behavior between
> squid 3.5 and 4.x?

It is difficult for me to say for sure -- too many changes in the
surrounding code, too long ago. "Maybe" is the best answer I can give.
Hopefully, others can be more specific.


> Is there a way to disable the client (child squid) initialized TLS
> renegotiation in squid 4?

OpenSSL v1.1 docs have the following paragraph:

> By default OpenSSL will use stateless tickets. The SSL_OP_NO_TICKET
> option will cause stateless tickets to not be issued. In TLSv1.2 and
> below this means no ticket gets sent to the client at all. In TLSv1.3
> a stateful ticket will be sent. This is a server-side option only.

The last sentence is interesting. However, OpenSSL v1.0 documentation
does not have that last caveat. It has another somewhat vague or open to
interpretation statement. Perhaps OpenSSL behavior changed with v1.1. In
that case, ignore this caveat.

You can try options discussed in the SECURE RENEGOTIATION section of
https://www.openssl.org/docs/man1.0.2/man3/SSL_CTX_set_options.html
but it is not clear to me whether they apply to your environment.


Alex.


From l.marcantonio at proxind.it  Wed Nov 11 18:17:29 2020
From: l.marcantonio at proxind.it (Lorenzo Marcantonio)
Date: Wed, 11 Nov 2020 19:17:29 +0100
Subject: [squid-users] Troubleshooting certificate issues
In-Reply-To: <20f7c102-e085-b1e8-81bc-78c65ee65c96@measurement-factory.com>
References: <20201111115632.vcsrlcgrua3ij4v2@nekomimi.proxind.it>
 <20f7c102-e085-b1e8-81bc-78c65ee65c96@measurement-factory.com>
Message-ID: <20201111181729.rakr5ehwhliuwltt@aika.discordia.loc>

On Wed, Nov 11, 2020 at 11:45:26AM -0500, Alex Rousskov wrote:
> On 11/11/20 6:56 AM, Lorenzo Marcantonio wrote:
> > I'm using 4.13 with libressl 3.2.2 and SSL bumps.
> 
> FYI: Libressl-based builds are not officially supported. I do not know
> whether libressl is a factor here.

Uhm. That could be. However I think that mixing openssl and libressl
could be an even bigger can of worm, given that they have the same soname.

> > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY at depth=2
> 
> If the connection is using TLS v1.3, then you may be suffering from Bug
> 5067: https://bugs.squid-cache.org/show_bug.cgi?id=5067

Ah. There is some kind of hack in squid to get the missing certificates.
but openssl verify checks ok without going to the net (I did a strace to
check the cafile).

libressl seems to be the most probable issue then. Not an easy fix I fear

Thanks for the advice

-- 
Lorenzo Marcantonio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201111/86ddd594/attachment.sig>

From manojwajekar93 at gmail.com  Wed Nov 11 19:35:15 2020
From: manojwajekar93 at gmail.com (Manoj Wajekar)
Date: Wed, 11 Nov 2020 14:35:15 -0500
Subject: [squid-users] TLS renegotiation failing between squids in
 hierarchy in Squid 4.
In-Reply-To: <6841aad3-09a1-c0b5-6d7f-35e7b754ee6e@measurement-factory.com>
References: <CABrhxqvkgMMS3s914kWea5FqM+Ew4D6zu7sE=xY8jFNZBT8-Vg@mail.gmail.com>
 <6841aad3-09a1-c0b5-6d7f-35e7b754ee6e@measurement-factory.com>
Message-ID: <CABrhxqsBbcnAP5Lpiosfr7GSUmzz-xnmFwC=PwGNdYjJuy2J9w@mail.gmail.com>

   > I am currently squid-cache in hierarchy setup, with TLS enabled
throughout.

> > client --> child Squid --> parent Squid --> web server
>
> >> Do you use SslBump anywhere?
>
       I am not using  SslBump. Part of my child squid config looks like:


https_port 3128\
 accel\
 no-vhost\
 defaultsite=origin\
 cert=/squid/certs/server/cert.pem\
 key=/squid/certs/server/key.pem\
 cafile=/squid/certs/server/ca.pem\
 clientca=/squid/certs/server/ca.pem

cache_peer\
 parentsquid.com\
 parent\
 3128\
 0\
 no-query\
 originserver\
 no-digest\
 no-netdb-exchange\
 login=PASSTHRU\
 tls\
 tls-options=NO_TICKET\
 sslcert=/squid/certs/client/cert.pem\
 sslkey=/squid/certs/client/key.pem\
 tls-cafile=/squid/certs/client/ca.pem


>
>
> > Openssl version: 1.0.2k
> > This setup is working for 3.5.20.
>
> > But when I updated to squid 4(tried 4.8, 4.11 and 4.13),
>
> >> Does all of the above apply to both child and parent Squids? Or just the
> >> child?
>
    Following scenarios are working:
    client --> child Squid 3.5.20 --> parent Squid 3.5.20 --> web server
    client --> child Squid 4 --> parent Squid 3.5.20 --> web server
    client --> Squid 4  --> web server

    But this scenarios is failing:
  client --> child Squid 4 --> parent Squid 4 --> web server

>
> > initial HTTP request goes through, but TLS renegotiation is failing
> > between child and parent squid for the following requests.
> >
> > From the logs, it looks like child squid is trying to initialize TLS
> > renegotiating using old TLS session ID, but parent squid is rejecting
> > session resumption.
> >
> > I confirm this behavior using openssl s_client --reconnect option.
> >
> > I tried to disabled client initialed TLS renegotiating by setting
> > tls-options=NO_TICKET (on child squid), but it is affecting the behavior.
>
> >> Did you mean to say "_not_ affecting the behavior"?
>
      Sorry for typo. Yes, with NO_TICKET set, I am encountering same
issue.

>
>
> > Are there any changes in default TLS renegotiation behavior between
> > squid 3.5 and 4.x?
>
> It is difficult for me to say for sure -- too many changes in the
> surrounding code, too long ago. "Maybe" is the best answer I can give.
> Hopefully, others can be more specific.
>
>
> > Is there a way to disable the client (child squid) initialized TLS
> > renegotiation in squid 4?
>
> >> OpenSSL v1.1 docs have the following paragraph:
>
> > By default OpenSSL will use stateless tickets. The SSL_OP_NO_TICKET
> > option will cause stateless tickets to not be issued. In TLSv1.2 and
> > below this means no ticket gets sent to the client at all. In TLSv1.3
> > a stateful ticket will be sent. This is a server-side option only.
> >> The last sentence is interesting. However, OpenSSL v1.0 documentation
> >> does not have that last caveat. It has another somewhat vague or open to
> >> interpretation statement. Perhaps OpenSSL behavior changed with v1.1. In
> >> that case, ignore this caveat.
>
> >> You can try options discussed in the SECURE RENEGOTIATION section of
> >> https://www.openssl.org/docs/man1.0.2/man3/SSL_CTX_set_options.html
> <https://www.openssl.org/docs/man1.0.2/man3/SSL_CTX_set_options.html>
> >> but it is not clear to me whether they apply to your environment.
>

  I tried SSL_OP_NO_SESSION_RESUMPTION_ON_RENEGOTIATION,
SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION, etc in
 openssl option.
  but it did not changed the behaviour.
  Unfortunately, I can't update to OpenSSL v1.1 because of OS dependency
issues.


> Manoj
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201111/e46ad9b9/attachment.htm>

From robertkwild at gmail.com  Thu Nov 12 18:19:44 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 12 Nov 2020 18:19:44 +0000
Subject: [squid-users] mime deny not working anymore
Message-ID: <CAGU_CiKrTHgD8vKRS_FBu4rojGzWBF+gJOk-ZoS-S2b97+SbFQ@mail.gmail.com>

hi all,

can anyone say why this isnt working anymore, im scratching my head
thinking about it

#deny MIME types
acl mimerep rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
http_reply_access deny mimerep

and in my

/usr/local/squid/etc/mimedeny.txt

application/octet-stream
application/x-msi
application/zip
application/vnd.ms-cab-compressed

but when i go on 7zip website and download an exe, it allows me to download
it

thanks,
rob
-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201112/dd99db80/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 13 07:52:39 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Nov 2020 20:52:39 +1300
Subject: [squid-users] mime deny not working anymore
In-Reply-To: <CAGU_CiKrTHgD8vKRS_FBu4rojGzWBF+gJOk-ZoS-S2b97+SbFQ@mail.gmail.com>
References: <CAGU_CiKrTHgD8vKRS_FBu4rojGzWBF+gJOk-ZoS-S2b97+SbFQ@mail.gmail.com>
Message-ID: <3a770fa4-d143-4319-ea80-03677e8b37d5@treenet.co.nz>

On 13/11/20 7:19 am, robert k Wild wrote:
> hi all,
> 
> can anyone say why this isnt working anymore, im scratching my head 
> thinking about it
> 
> #deny MIME types
> acl mimerep rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
> http_reply_access deny mimerep
> 
> and in my
> 
> /usr/local/squid/etc/mimedeny.txt
> 
> application/octet-stream
> application/x-msi
> application/zip
> application/vnd.ms-cab-compressed
> 
> but when i go on 7zip website and download an exe, it allows me to 
> download it
> 

Have you checked the Content-Type on the reply message is actually on 
that list?
  The one(s) I get by following your described test are, but that is no 
guarantee you see the same.


Any hints in the cache.log ?

For troubleshooting use "debug_options ALL,1 11,2 28,3" to see the 
traffic message headers and what ACLs are applied.


Amos


From robertkwild at gmail.com  Fri Nov 13 08:11:49 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 13 Nov 2020 08:11:49 +0000
Subject: [squid-users] mime deny not working anymore
In-Reply-To: <3a770fa4-d143-4319-ea80-03677e8b37d5@treenet.co.nz>
References: <CAGU_CiKrTHgD8vKRS_FBu4rojGzWBF+gJOk-ZoS-S2b97+SbFQ@mail.gmail.com>
 <3a770fa4-d143-4319-ea80-03677e8b37d5@treenet.co.nz>
Message-ID: <CAGU_CiK_KCHdG1G+uOerZSKzoSTpZG78tR6PhqPwYsPCfjBRBA@mail.gmail.com>

haha, so sorry Amos,  its working, which I thought it should as I havnt
touched the config file in months

I have worked out it works for http traffic but not for https traffic

in my config I have enabled ssl bind ie https interception so I really
don't K ow why its not working.

thanks Amos

On Fri, 13 Nov 2020, 08:00 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 13/11/20 7:19 am, robert k Wild wrote:
> > hi all,
> >
> > can anyone say why this isnt working anymore, im scratching my head
> > thinking about it
> >
> > #deny MIME types
> > acl mimerep rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
> > http_reply_access deny mimerep
> >
> > and in my
> >
> > /usr/local/squid/etc/mimedeny.txt
> >
> > application/octet-stream
> > application/x-msi
> > application/zip
> > application/vnd.ms-cab-compressed
> >
> > but when i go on 7zip website and download an exe, it allows me to
> > download it
> >
>
> Have you checked the Content-Type on the reply message is actually on
> that list?
>   The one(s) I get by following your described test are, but that is no
> guarantee you see the same.
>
>
> Any hints in the cache.log ?
>
> For troubleshooting use "debug_options ALL,1 11,2 28,3" to see the
> traffic message headers and what ACLs are applied.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201113/0fb28931/attachment.htm>

From bobrich at gmail.com  Fri Nov 13 19:30:39 2020
From: bobrich at gmail.com (Bob Rich)
Date: Fri, 13 Nov 2020 14:30:39 -0500
Subject: [squid-users] auth_param tls? limiting proxy access based on client
 TLS authentication
Message-ID: <CAJkLLZ1dheQ+Xx9MSfDUcz+DZpFX75k_=qyk3Xem3x7ECJfuCA@mail.gmail.com>

Hi folks,

Apologies if this is a faq or has treatment elsewhere but I can't find it.

I've got squid configured as an old-school explicit forward proxy.

I would like to limit access through the proxy to only those clients that
authenticate either to an HTTPS proxy listener or via client auth injected
into a CONNECT request to the origin server.  Please note that in this use
case the origin server is not expecting TLS auth in any way.  This is just
being used initially to prevent unauthenticated clients from using the
proxy.

Ideally we would be able to base access control on information derived from
subject DN or other attributes in the certificate, but I'm just aiming for
basic functionality right now.

I built 4.13 locally on Ubuntu and as far as I can tell all of the other
SSL features are working (ssl_bump, generate-host-certificates, etc)

Thanks in advance for any advice!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201113/5094eb24/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov 14 03:24:43 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Nov 2020 16:24:43 +1300
Subject: [squid-users] auth_param tls? limiting proxy access based on
 client TLS authentication
In-Reply-To: <CAJkLLZ1dheQ+Xx9MSfDUcz+DZpFX75k_=qyk3Xem3x7ECJfuCA@mail.gmail.com>
References: <CAJkLLZ1dheQ+Xx9MSfDUcz+DZpFX75k_=qyk3Xem3x7ECJfuCA@mail.gmail.com>
Message-ID: <6c15f9b1-2487-7e90-9831-9033954e00ea@treenet.co.nz>

On 14/11/20 8:30 am, Bob Rich wrote:
> 
> I've got squid configured as an old-school explicit forward proxy.
> 
> I would like to limit access through the proxy to only those clients 
> that authenticate either to an HTTPS proxy listener or via client auth 
> injected into a CONNECT request to the origin server.? Please note that 
> in this use case the origin server is not expecting TLS auth in any 
> way.? This is just being used initially to prevent unauthenticated 
> clients from using the proxy.
> 

You seem to have been confused by the presence of TLS / HTTPS.

 From your description it appears that the clients are talking to Squid 
using HTTP. Any authentication they send to Squid has to be using HTTP 
Authentication. Which is validated by the auth_param helper and 
proxy_auth ACL type.
  <https://wiki.squid-cache.org/Features/Authentication>


To a regular forward-proxy a CONNECT request is an instruction to open a 
TCP tunnel to the origin. There is no way to pass authentication 
credentials in a TCP SYN packet. So the origin will not be aware of 
*which* client authenticated.

However, the way you described your requirement implies that the origin 
does not need the credentials anyway. It is only the proxy which cares 
about auth to decide whether to relay or block a client.



> Ideally we would be able to base access control on information derived 
> from subject DN or other attributes in the certificate, but I'm just 
> aiming for basic functionality right now.
> 

That requires a completely different design for the proxy architecture. 
One which has no relation to HTTP authentication at all.


If you really want this TLS certification to be the primary access for 
clients I think it better to concentrate on getting that design working, 
then add any HTTP auth as a backup later.



> I built 4.13 locally on Ubuntu and as far as I can tell all of the other 
> SSL features are working (ssl_bump, generate-host-certificates, etc)
> 
> Thanks in advance for any advice!
> 


Amos


From bobrich at gmail.com  Sat Nov 14 18:53:40 2020
From: bobrich at gmail.com (Bob Rich)
Date: Sat, 14 Nov 2020 13:53:40 -0500
Subject: [squid-users] auth_param tls? limiting proxy access based on
 client TLS authentication
In-Reply-To: <6c15f9b1-2487-7e90-9831-9033954e00ea@treenet.co.nz>
References: <CAJkLLZ1dheQ+Xx9MSfDUcz+DZpFX75k_=qyk3Xem3x7ECJfuCA@mail.gmail.com>
 <6c15f9b1-2487-7e90-9831-9033954e00ea@treenet.co.nz>
Message-ID: <CAJkLLZ3BTL1BXE2mSaj5kTaR7BBirrFBtnDsqidusJ=iAe7hHQ@mail.gmail.com>

Thanks for the reply Amos.

Couple of quick points

1 - Nothing exists today aside from a little prototyping environment, so
anything that's possible is still in play.
2 - The CA issuing the client certificates and any signing certificates for
the proxy is internal.  Just to reinforce, the web applications that the
clients intend to access have no knowledge of the CA or requirement for
client certificate authentication.

Stating the goal possibly in another way, only clients that possess this
internally issued client certificate should be able to connect to a web
application through the proxy.  If the client does not have a certificate,
there is no requirement around whether or not it can connect to the proxy
and issue requests...the only requirement is that any requests to connect
to applications on the other side of the proxy fail.

On Fri, Nov 13, 2020 at 10:33 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

>  From your description it appears that the clients are talking to Squid
> using HTTP. Any authentication they send to Squid has to be using HTTP
> Authentication. Which is validated by the auth_param helper and
> proxy_auth ACL type.
>   <https://wiki.squid-cache.org/Features/Authentication>
>

Agree, this seems to be a non-starter for the reasons you describe.  We do
not wish to use HTTP authorization headers if it can be avoided.


> However, the way you described your requirement implies that the origin
> does not need the credentials anyway. It is only the proxy which cares
> about auth to decide whether to relay or block a client.
>

This is correct.

To me there are only a few options for introducing TLS client certificate
authentication.

1 - Run TLS on the proxy listener.  This would use https_port directive and
would require that we are able to configure the proxy to mandate client
certificates before allowing the connection to complete.  Clients with
no/invalid certificates wouldn't even get to the point where they can send
a request to the proxy.

2 - Use ssl-bump functionality to modify the TLS handshake that occurs
after a CONNECT request to require a valid client certificate before
completing the request.  No idea if this is possible.

3 - Use either of the above to establish the mutually authenticated TLS
context and then surface that information through ICAP to offload the
authorization decision.

I've been able to get ssl-bump working to generate custom certs and I have
Squid talking to c-icap. I haven't successfully got Squid to prompt the
client to authenticate and I still have quite a bit of learning to do on
the ICAP side.

Thanks in advance for any steers (including 'this is a terrible idea' of
course :)

Lastly I haven't used gmail with a mailing list before.  Let me know if
i've stomping on some etiquette.


>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201114/f3f5aff7/attachment.htm>

From rousskov at measurement-factory.com  Sat Nov 14 22:26:57 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 14 Nov 2020 17:26:57 -0500
Subject: [squid-users] auth_param tls? limiting proxy access based on
 client TLS authentication
In-Reply-To: <CAJkLLZ3BTL1BXE2mSaj5kTaR7BBirrFBtnDsqidusJ=iAe7hHQ@mail.gmail.com>
References: <CAJkLLZ1dheQ+Xx9MSfDUcz+DZpFX75k_=qyk3Xem3x7ECJfuCA@mail.gmail.com>
 <6c15f9b1-2487-7e90-9831-9033954e00ea@treenet.co.nz>
 <CAJkLLZ3BTL1BXE2mSaj5kTaR7BBirrFBtnDsqidusJ=iAe7hHQ@mail.gmail.com>
Message-ID: <c6f71e27-5fd2-ae3b-4547-2bf4ceeb4876@measurement-factory.com>

On 11/14/20 1:53 PM, Bob Rich wrote:

> 1 - Run TLS on the proxy listener.? This would use https_port directive
> and would require that we are able to configure the proxy to mandate
> client certificates before allowing the connection to complete.? Clients
> with no/invalid certificates wouldn't even get to the point where they
> can send a request to the proxy.

Yes, this is how certificate-based authentication is usually done with
Squid. There are large Squid deployments using this mechanism. It is
also the most secure method of using a proxy...

    https_port 3443 clientca=auth.pem tls-cert=squid.pem ...

The biggest problem with this approach is being able to configure
clients to use an HTTPS proxy (as opposed to using an HTTP proxy for
HTTPS traffic). Popular browsers support HTTPS proxies (but may require
PAC-based configuration to enable that support[1]). Many clients do not
support HTTPS proxies.

[1] Look for "HTTPS proxy" at
https://developer.mozilla.org/en-US/docs/Web/HTTP/Proxy_servers_and_tunneling/Proxy_Auto-Configuration_(PAC)_file

Please note that one cannot combine SslBump and certificate-based client
authentication on the same port (yet?).

BTW, the other two options for certificate-based authentication that you
were thinking about will not work out of the box, for various reasons.


HTH,

Alex.


> 2 - Use ssl-bump functionality to modify the TLS handshake that occurs
> after a CONNECT request to require a valid client certificate before
> completing the request.? No idea if this is possible.
> 
> 3 - Use either of the above to establish the mutually authenticated TLS
> context and then surface that information through ICAP to offload the
> authorization decision.
> 
> I've been able to get ssl-bump working to generate custom certs and I
> have Squid talking to c-icap. I haven't successfully got Squid to prompt
> the client to authenticate and I still have quite a bit of learning to
> do on the ICAP side.
> 
> Thanks in advance for any steers (including 'this is a terrible idea' of
> course :)
> 
> Lastly I haven't used gmail with a mailing list before.? Let me know if
> i've stomping on some etiquette.



From roeeklinger60 at gmail.com  Mon Nov 16 23:14:03 2020
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 17 Nov 2020 01:14:03 +0200
Subject: [squid-users] Gather POST request on HTTPS traffic?
Message-ID: <CAGCa14opQ38ULjH5_eP5UpH9Zpv7=GJi4AqjGqdCS8=dZu8pPA@mail.gmail.com>

Hello everyone,

I work at a digital agency that has quite a few machines that are managing
some Instagram accounts, they are all running in the same LAN and we are
using Squid as a proxy to log and analyze some usage statistics and to make
sure the machines are only used for Instagram.

We had an idea to use Squid to capture the POST data of users on the proxy
level, for example, likes, follows, comments, etc so we can log and analyze
everything in a convenient central way, so we can analyze it and even send
out clients a monthly report of all the actions their accounts made (who
they followed, what they liked, etc).

I can easily see the requests that I want to capture inside the "network"
tab in Chrome but the problem is that Instagram uses HTTPS, so I can't seem
to be able to capture this data.


Is there any way for me to log this data via Squid using the POST data or
any other way?


Note: We are aware of the legal issues, all machines connected to the
network are company property, and all the accounts are client accounts that
allow us to gather statistics. No personal account data will be gathered.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201117/31f42c48/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 17 00:09:04 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2020 13:09:04 +1300
Subject: [squid-users] Gather POST request on HTTPS traffic?
In-Reply-To: <CAGCa14opQ38ULjH5_eP5UpH9Zpv7=GJi4AqjGqdCS8=dZu8pPA@mail.gmail.com>
References: <CAGCa14opQ38ULjH5_eP5UpH9Zpv7=GJi4AqjGqdCS8=dZu8pPA@mail.gmail.com>
Message-ID: <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>

On 17/11/20 12:14 pm, roee klinger wrote:
> Hello everyone,
> 
> I work at a digital agency that has quite?a few machines that are 
> managing some Instagram accounts, they are all running in the same LAN 
> and we are using Squid as a proxy to log and analyze some usage 
> statistics and to make sure the?machines are only used for Instagram.
> 
> We had an idea to use Squid to capture the POST data of users on the 
> proxy level, for example, likes, follows, comments, etc so we can log 
> and analyze everything in a convenient central way, so we can analyze it 
> and even send out clients a monthly report of all the actions their 
> accounts made (who they followed, what they liked, etc).
> 
> I can easily see the requests that I want to capture inside the 
> "network" tab in Chrome but the problem is that Instagram uses HTTPS, so 
> I can't seem to be able to capture this data.
> 
> 
> Is there any way for me to log this data via Squid using the POST data 
> or any other way?
> 

Access to HTTPS transactions for a domain you do not own requires the 
SSL-Bump feature to decrypt ("bump") the TLS layer.
  see <https://wiki.squid-cache.org/Features/SslPeekAndSplice>.

You could use cache.log with "debug_options ALL,1 11,2" configured to 
log the transactions. However an ICAP service or eCAP module that does 
both the record and analyze for you is probably better.


> 
> Note: We are aware of the legal issues, all machines connected to the 
> network are company property, and all the accounts are client accounts 
> that allow us to gather statistics. No personal account data?will be 
> gathered.


Please be aware:
   That statement conflicts with the stated purpose(s) of your plan.

Personal data *will* be part of the messages you are decrypting and 
recording for analysis. Further, to perform targeted reports such as 
described you must also associate the data with accounts somehow.


Amos


From david at articatech.com  Tue Nov 17 08:27:09 2020
From: david at articatech.com (David Touzeau)
Date: Tue, 17 Nov 2020 09:27:09 +0100
Subject: [squid-users] squid 4/5 feature request send login informations to
 peers
Message-ID: <8b2ef0ab-c83e-7561-d76a-94f42d115ef9@articatech.com>


Hi,

We a first Squid using Kerberos + Active Directory authentication.
This first squid is used to limit access using ACls and Active Directory 
groups.

This first squid using parents as peer in order to access to internet in 
this way:

 ???????????????????????????? | --------> SQUID B ----------> Internet 1
squid A ------------->
 ???????????????????????????? | ---------> SQUID C ---------> Internet 2

1) We want using ACLs too ( for delegation purpose ) on Squid B and C
2) For legal logs purpose compliance.

In this case,? the username discovered in SQUIDA must be transmitted to 
SQUID B AND C and SQUID B-C must accept the information in order to use 
as login information to parse acls

Is it possible ?

If not: wee have seen that the Proxy protocol accept to transmit the 
source IP/login information to peers that are compliance with proxy 
protocol.
but the peers method in squid did not allow to use Proxy protocol.
Is it possible to add the "Proxy Protocol" support in peers method ?






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201117/0f7ea16f/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 17 10:43:57 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Nov 2020 23:43:57 +1300
Subject: [squid-users] squid 4/5 feature request send login informations
 to peers
In-Reply-To: <8b2ef0ab-c83e-7561-d76a-94f42d115ef9@articatech.com>
References: <8b2ef0ab-c83e-7561-d76a-94f42d115ef9@articatech.com>
Message-ID: <edaf8249-05aa-172c-e618-798d12176dbc@treenet.co.nz>

On 17/11/20 9:27 pm, David Touzeau wrote:
> 
> Hi,
> 
> We a first Squid using Kerberos + Active Directory authentication.
> This first squid is used to limit access using ACls and Active Directory 
> groups.
> 
> This first squid using parents as peer in order to access to internet in 
> this way:
> 
>  ???????????????????????????? | --------> SQUID B ----------> Internet 1
> squid A ------------->
>  ???????????????????????????? | ---------> SQUID C ---------> Internet 2
> 
> 1) We want using ACLs too ( for delegation purpose ) on Squid B and C
> 2) For legal logs purpose compliance.
> 
> In this case,? the username discovered in SQUIDA must be transmitted to 
> SQUID B AND C and SQUID B-C must accept the information in order to use 
> as login information to parse acls
> 
> Is it possible ?

You can send the username. But the security token is tied to the 
client<->SquidA TCP connection - it cannot be validated by other servers 
than SquidA.

This should not matter though. Since Squid A is only permitting 
authenticated traffic you can *authorize* at Squid B and C based only on 
the source being one of your Squid with valid username.


> 
> If not: wee have seen that the Proxy protocol accept to transmit the 
> source IP/login information to peers that are compliance with proxy 
> protocol.
> but the peers method in squid did not allow to use Proxy protocol.
> Is it possible to add the "Proxy Protocol" support in peers method ?
> 

It is possible to implement (for Squid-6 earliest) PROXYv2 for 
cache_peer. But the credentials security token remains tied to SquidA 
service.


Amos


From roeeklinger60 at gmail.com  Tue Nov 17 12:41:53 2020
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 17 Nov 2020 14:41:53 +0200
Subject: [squid-users] Gather POST request on HTTPS traffic?
In-Reply-To: <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>
References: <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>
Message-ID: <CC8E1C68-3B50-43C5-A455-8B9DF3AEF1A8@gmail.com>

Hey Amos,

Thanks for your response, I will try to implement this today and check if I can get the data I am looking for.

I do however have a few questions regarding this approach:
1. If I understand the docu currently, then the server is getting a response which is identical to the client, meaning the server should not detect anything unusual? The last thing I want is for Instagram to detect something unusual and ban our clients Instagram accounts.
2. You said I will need to figure out a way to identify accounts, in Chrome the requests contain the info for both the accounts performing the action and the account receiving the action, should I see the same in these requests?
3. By ?personal? data we are referring to data generated by our clients accounts, which are paying and willing for us to collect it to improve our service, of course it will also contain data on the account which they are performing the actions on, but this is not something that is not visible on the Instagram app, is there anything else I should be aware of that might be a privacy issue?
4. While this is great for my use case, is this something I should be aware of when using outside proxies on our machine? Can any proxy service simply decrypt and log our personal data? Seems like a security  vulnerability I should be aware of.

Thanks again.

> On Nov 17, 2020, at 02:17, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> ?On 17/11/20 12:14 pm, roee klinger wrote:
>> Hello everyone,
>> I work at a digital agency that has quite a few machines that are managing some Instagram accounts, they are all running in the same LAN and we are using Squid as a proxy to log and analyze some usage statistics and to make sure the machines are only used for Instagram.
>> We had an idea to use Squid to capture the POST data of users on the proxy level, for example, likes, follows, comments, etc so we can log and analyze everything in a convenient central way, so we can analyze it and even send out clients a monthly report of all the actions their accounts made (who they followed, what they liked, etc).
>> I can easily see the requests that I want to capture inside the "network" tab in Chrome but the problem is that Instagram uses HTTPS, so I can't seem to be able to capture this data.
>> Is there any way for me to log this data via Squid using the POST data or any other way?
> 
> Access to HTTPS transactions for a domain you do not own requires the SSL-Bump feature to decrypt ("bump") the TLS layer.
> see <https://wiki.squid-cache.org/Features/SslPeekAndSplice>.
> 
> You could use cache.log with "debug_options ALL,1 11,2" configured to log the transactions. However an ICAP service or eCAP module that does both the record and analyze for you is probably better.
> 
> 
>> Note: We are aware of the legal issues, all machines connected to the network are company property, and all the accounts are client accounts that allow us to gather statistics. No personal account data will be gathered.
> 
> 
> Please be aware:
>  That statement conflicts with the stated purpose(s) of your plan.
> 
> Personal data *will* be part of the messages you are decrypting and recording for analysis. Further, to perform targeted reports such as described you must also associate the data with accounts somehow.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201117/44eb8227/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 17 13:14:04 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Nov 2020 02:14:04 +1300
Subject: [squid-users] Gather POST request on HTTPS traffic?
In-Reply-To: <CC8E1C68-3B50-43C5-A455-8B9DF3AEF1A8@gmail.com>
References: <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>
 <CC8E1C68-3B50-43C5-A455-8B9DF3AEF1A8@gmail.com>
Message-ID: <d694cad8-b68d-a7e6-c133-05293d33374d@treenet.co.nz>

On 18/11/20 1:41 am, roee klinger wrote:
> Hey Amos,
> 
> Thanks for your response, I will try to implement this today and check 
> if I can get the data I am looking for.
> 
> I do however have a few questions regarding this approach:
> 1. If I understand the docu currently, then the server is getting a 
> response which is identical to the client, meaning the server should not 
> detect anything unusual? The last thing I want is for Instagram to 
> detect something unusual and ban our clients Instagram accounts.

That depends on the what you configure. Interception is always 
detectable, though most services have limited detection (if they care at 
all).


> 2. You said I will need to figure out a way to identify accounts, in 
> Chrome the requests contain the info for both the accounts performing 
> the action and the account receiving the action, should I see the same 
> in these requests?

Yes. That is what I mean by personal data *will* be gathered.


> 3. By ?personal? data we are referring to data generated by our clients 
> accounts, which are paying and willing for us to collect it to improve 
> our service, of course it will also contain data on the account which 
> they are performing the actions on, but this is not something that is 
> not visible on the Instagram app, is there anything else I should be 
> aware of that might be a privacy issue?


That definition confirms the false nature of "No personal account data 
will be gathered." - having permission to gather does not negate the 
existence of gathering.

Just make sure you have a real lawyers opinion / advice on the situation 
details.


> 4. While this is great for my use case, is this something I should be 
> aware of when using outside proxies on our machine? Can any proxy 
> service simply decrypt and log our personal data? Seems like a security 
>  ?vulnerability I should be aware of.
> 

You will notice when configuring SSL-Bump that you must install signing 
CA certificates used by your proxy into the clients software. Without 
that CA trust you cannot bump.

The possibility of bumping (or lack of) is true for any intermediary 
software.


Amos


From roeeklinger60 at gmail.com  Tue Nov 17 13:37:52 2020
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 17 Nov 2020 15:37:52 +0200
Subject: [squid-users] Gather POST request on HTTPS traffic?
In-Reply-To: <d694cad8-b68d-a7e6-c133-05293d33374d@treenet.co.nz>
References: <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>
 <CC8E1C68-3B50-43C5-A455-8B9DF3AEF1A8@gmail.com>
 <d694cad8-b68d-a7e6-c133-05293d33374d@treenet.co.nz>
Message-ID: <CAGCa14p88+senwLg7cZAr+y_hJB56FEoKncCQmCfVJnjwSZDNA@mail.gmail.com>

Thanks for the replay Amos,

> You will notice when configuring SSL-Bump that you must install signing
> CA certificates used by your proxy into the clients software.
>

I understand, this is something I missed apparently.

Sometimes I am using proxies for scraping which detect if the scraping is
successful and run the request
from a different proxy if it isn't, they even go as far as automatically
solving captcha's for the client or changing
content on the page, I am pretty new to this but these feature seems
impossible to me on HTTPS connections
without having access to the client's machines.

Is there something I am missing or misunderstanding?
I cannot seem to find a good place to start reading about this.

Thanks.




On Tue, Nov 17, 2020 at 3:22 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/11/20 1:41 am, roee klinger wrote:
> > Hey Amos,
> >
> > Thanks for your response, I will try to implement this today and check
> > if I can get the data I am looking for.
> >
> > I do however have a few questions regarding this approach:
> > 1. If I understand the docu currently, then the server is getting a
> > response which is identical to the client, meaning the server should not
> > detect anything unusual? The last thing I want is for Instagram to
> > detect something unusual and ban our clients Instagram accounts.
>
> That depends on the what you configure. Interception is always
> detectable, though most services have limited detection (if they care at
> all).
>
>
> > 2. You said I will need to figure out a way to identify accounts, in
> > Chrome the requests contain the info for both the accounts performing
> > the action and the account receiving the action, should I see the same
> > in these requests?
>
> Yes. That is what I mean by personal data *will* be gathered.
>
>
> > 3. By ?personal? data we are referring to data generated by our clients
> > accounts, which are paying and willing for us to collect it to improve
> > our service, of course it will also contain data on the account which
> > they are performing the actions on, but this is not something that is
> > not visible on the Instagram app, is there anything else I should be
> > aware of that might be a privacy issue?
>
>
> That definition confirms the false nature of "No personal account data
> will be gathered." - having permission to gather does not negate the
> existence of gathering.
>
> Just make sure you have a real lawyers opinion / advice on the situation
> details.
>
>
> > 4. While this is great for my use case, is this something I should be
> > aware of when using outside proxies on our machine? Can any proxy
> > service simply decrypt and log our personal data? Seems like a security
> >   vulnerability I should be aware of.
> >
>
> You will notice when configuring SSL-Bump that you must install signing
> CA certificates used by your proxy into the clients software. Without
> that CA trust you cannot bump.
>
> The possibility of bumping (or lack of) is true for any intermediary
> software.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201117/ce781cc2/attachment.htm>

From hello at ironpeak.be  Wed Nov 18 15:48:09 2020
From: hello at ironpeak.be (Niels Hofmans)
Date: Wed, 18 Nov 2020 16:48:09 +0100
Subject: [squid-users] squid mitm
Message-ID: <4C3E5D29-02A5-4875-8814-64F797D06366@ironpeak.be>

Hi guys,

I am trying to setup squid with TLS intercaption on Docker in an alpine linux image.

My configuration is as follows:

access_log /dev/stdout

https_port 0.0.0.0:3128 \
  intercept \
  ssl-bump \
  cert=/ca.pem \
  generate-host-certificates=on \
  dynamic_cert_mem_cache_size=500MB

sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl.db -M 100MB
sslcrtd_children 5

ssl_bump server-first all
sslproxy_cert_error allow all

And the Dockerfile:

FROM alpine

RUN apk add -U --no-cache squid 
COPY cmd/config/ca.pem cmd/config/squid.conf /
RUN mkdir -p /cache \
    && /usr/lib/squid/security_file_certgen -c -s /cache/ssl.db -M 100MB

EXPOSE 3128
ENTRYPOINT ["/usr/sbin/squid?, ?-f?, "/squid.conf?]

However, this always exits with following error:

proxy_1       | [00] 2020/11/18 15:38:27| WARNING: BCP 177 violation. Detected non-functional IPv6 loopback.
proxy_1       | [00] 2020/11/18 15:38:27| FATAL: No valid signing certificate configured for HTTPS_port 0.0.0.0:3128
proxy_1       | [00] 2020/11/18 15:38:27| Squid Cache (Version 4.13): Terminated abnormally.
proxy_1       | [00] CPU Usage: 0.036 seconds = 0.021 user + 0.014 sys
proxy_1       | [00] Maximum Resident Size: 42256 KB
proxy_1       | [00] Page faults with physical i/o: 0


And cmd/config <http://config.ca/>/ca.pem was created with the command from the wiki on macOS: openssl req -new -newkey rsa:1024 -days 365 -nodes -x509 -keyout ca.pem -out ca.pem
Any ideas? Thanks!

-- 
Met vriendelijke groeten,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201118/ec8d5419/attachment.htm>

From robertkwild at gmail.com  Wed Nov 18 20:20:18 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 18 Nov 2020 20:20:18 +0000
Subject: [squid-users] reply_header to block downloads
Message-ID: <CAGU_CiLfSna9U3rwm7DH6_yocvhoihXEgeAqKrMnaug9-jrW9g@mail.gmail.com>

hi all,

can i use the acl "reply_header_access" to block downloads, like i have
done with the " rep_mime_type " or is this not what its meant for

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201118/4359c8a4/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 19 04:51:16 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Nov 2020 17:51:16 +1300
Subject: [squid-users] squid mitm
In-Reply-To: <4C3E5D29-02A5-4875-8814-64F797D06366@ironpeak.be>
References: <4C3E5D29-02A5-4875-8814-64F797D06366@ironpeak.be>
Message-ID: <11f37ee1-4310-44fb-1c32-4e7b53e89225@treenet.co.nz>

On 19/11/20 4:48 am, Niels Hofmans wrote:
> Hi guys,
> 
> I am trying to setup squid with TLS intercaption on Docker in an alpine 
> linux image.
> 
> My configuration is as follows:
> 
> access_log /dev/stdout
> 

Not a great idea. stdout is process specific ... and Squid is a 
collection of multiple processes that can change over time.

You can use TCP logging if you need to stream the data out of the 
container. see <https://wiki.squid-cache.org/Features/LogModules> for 
details.


> https_port 0.0.0.0:3128 \
>    intercept \
>    ssl-bump \
>    cert=/ca.pem \
>    generate-host-certificates=on \
>    dynamic_cert_mem_cache_size=500MB
> 
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl.db -M 100MB
> sslcrtd_children 5
> 
> ssl_bump server-first all
> sslproxy_cert_error allow all
> 

These indicate that you are using a Squid-3.4 or older. Please upgrade 
to at least Squid-4. Ideally Squid-5 (beta) or even Squid-6 (alpha) for 
best TLS behaviour.


> 
> And the Dockerfile:
> 
> FROM alpine
> 
> RUN apk add -U --no-cache squid
> COPY cmd/config/ca.pem cmd/config/squid.conf /
> RUN mkdir -p /cache \
> && /usr/lib/squid/security_file_certgen -c -s /cache/ssl.db -M 100MB
> 
> EXPOSE 3128
> ENTRYPOINT ["/usr/sbin/squid?, ?-f?, "/squid.conf?]
> 
> 
> However, this always exits with following error:
> 
> proxy_1 ? ? ? | [00] 2020/11/18 15:38:27| WARNING: BCP 177 violation. 
> Detected non-functional IPv6 loopback.
> proxy_1 ? ? ? | [00] 2020/11/18 15:38:27| FATAL: No valid signing 
> certificate configured for HTTPS_port 0.0.0.0:3128
> proxy_1 ? ? ? | [00] 2020/11/18 15:38:27| Squid Cache (Version 4.13): 
> Terminated abnormally.
> proxy_1 ? ? ? | [00] CPU Usage: 0.036 seconds = 0.021 user + 0.014 sys
> proxy_1 ? ? ? | [00] Maximum Resident Size: 42256 KB
> proxy_1 ? ? ? | [00] Page faults with physical i/o: 0
> 
> 
> And cmd/config <http://config.ca>/ca.pem was created with the command 
> from the wiki on macOS: openssl req -new -newkey rsa:1024 -days 365 
> -nodes -x509 -keyout ca.pem -out ca.pem
> Any ideas? Thanks!
> 

That command generates a regular server certificate. SSL-Bump needs a CA 
certificate.

Here are the correct command(s) for generating a root CA certificate for 
Squid and DER file for client install:
 
<https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit#Usage>

Note: most of the rest of that page can be used as instructions for your 
setup. The only notable difference is the http(s)_port line settings, 
your current one is correct for an MITM proxy.


Amos


From squid3 at treenet.co.nz  Thu Nov 19 04:55:12 2020
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Nov 2020 17:55:12 +1300
Subject: [squid-users] reply_header to block downloads
In-Reply-To: <CAGU_CiLfSna9U3rwm7DH6_yocvhoihXEgeAqKrMnaug9-jrW9g@mail.gmail.com>
References: <CAGU_CiLfSna9U3rwm7DH6_yocvhoihXEgeAqKrMnaug9-jrW9g@mail.gmail.com>
Message-ID: <db264429-89b1-9c45-a3d5-01ad43626065@treenet.co.nz>

On 19/11/20 9:20 am, robert k Wild wrote:
> hi all,
> 
> can i use the acl "reply_header_access" to block downloads, like i have 
> done with the " rep_mime_type " or is this not what its meant for
> 

That directive stops matching responses being delivered to clients (they 
get an error page instead).

Note that the full response is still received by the proxy. So it is 
mostly useful for security depending on response details, rather than a 
bandwidth saving measure.


Amos


From rentorbuy at yahoo.com  Thu Nov 19 11:02:01 2020
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 19 Nov 2020 11:02:01 +0000 (UTC)
Subject: [squid-users] websockets through Squid
In-Reply-To: <1c864fcf-f760-4a07-50f7-2ec107a6da74@measurement-factory.com>
References: <1626472963.180117.1602859307164.ref@mail.yahoo.com>
 <1626472963.180117.1602859307164@mail.yahoo.com>
 <d24b2583-4959-99d2-9d22-ba3141f595cd@measurement-factory.com>
 <1798348484.217408.1602863930213@mail.yahoo.com>
 <3f7b01cd-f71c-6fb0-27bf-74fbda532384@measurement-factory.com>
 <1304872121.821246.1603058873508@mail.yahoo.com>
 <1c864fcf-f760-4a07-50f7-2ec107a6da74@measurement-factory.com>
Message-ID: <1848076410.179362.1605783721433@mail.yahoo.com>


On Wednesday, November 4, 2020, 3:27:25 AM GMT+1, Alex Rousskov <rousskov at measurement-factory.com> wrote: 
>?? https://bugs.squid-cache.org/show_bug.cgi?id=5084

Hi,

I added a comment to that bug report.
I cannot reproduce the problem anymore, at least not with the latest version of Squid 5.

Thanks,

Vieri


From hello at ironpeak.be  Thu Nov 19 11:45:28 2020
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 19 Nov 2020 12:45:28 +0100
Subject: [squid-users] squid mitm
In-Reply-To: <mailman.8991.1605783729.1313.squid-users@lists.squid-cache.org>
References: <mailman.8991.1605783729.1313.squid-users@lists.squid-cache.org>
Message-ID: <06870342-73FC-4C0B-A749-FD3106435632@ironpeak.be>

Hello Amos,

I am using the latest squid release on alpine, which is 4.13-r0.
After using the exact command openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout ca.pem  -out ca.pem I still receive this error.

Since it?s a debug cert, I gisted it here: https://gist.githubusercontent.com/hazcod/530ae4ad467d8ed3de6621ba04dddc79/raw/fe62ab6b71f888dd890aded2d61c7c798747a665/ca.pem <https://gist.githubusercontent.com/hazcod/530ae4ad467d8ed3de6621ba04dddc79/raw/fe62ab6b71f888dd890aded2d61c7c798747a665/ca.pem>

strace excerpt:

proxy_1       | [00] brk(0x55e41021f000)                     = 0x55e41021f000
proxy_1       | [00] read(3, "", 1024)                       = 0
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] brk(0x55e410220000)                     = 0x55e410220000
proxy_1       | [00] getuid()                                = 0
proxy_1       | [00] geteuid()                               = 0
proxy_1       | [00] getgid()                                = 0
proxy_1       | [00] getegid()                               = 0
proxy_1       | [00] open("/ca.pem", O_RDONLY)               = -1 EACCES (Permission denied)
proxy_1       | [00] open("/ca.pem", O_RDONLY)               = -1 EACCES (Permission denied)
proxy_1       | [00] geteuid()                               = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setgid(1000)                            = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/var/run/nscd/socket"}, 24) = -1 ENOENT (No such file or directory)
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] open("/etc/group", O_RDONLY|O_CLOEXEC)  = 3
proxy_1       | [00] fcntl(3, F_SETFD, FD_CLOEXEC)           = 0
proxy_1       | [00] fcntl(3, F_SETFD, FD_CLOEXEC)           = 0
proxy_1       | [00] read(3, "root:x:0:root\napp:x:1000:\ndnscac"..., 1024) = 88
proxy_1       | [00] read(3, "", 1024)                       = 0
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] setgroups(1, [1000])                    = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setresuid(1000, 1000, 0)                = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] capget({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, NULL) = 0
proxy_1       | [00] capget({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, {effective=0, permitted=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE, inheritable=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE}) = 0
proxy_1       | [00] capset({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, {effective=1<<CAP_NET_BIND_SERVICE, permitted=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE, inheritable=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE}) = 0
proxy_1       | [00] prctl(PR_SET_DUMPABLE, SUID_DUMP_USER)  = 0
proxy_1       | [00] writev(2, [{iov_base="2020/11/19 11:44:20| ", iov_len=21}, {iov_base="FATAL: No valid signing certific"..., iov_len=73}], 22020/11/19 11:44:20| FATAL: No valid signing certificate configured for HTTP_port 0.0.0.0:3128) = 94
proxy_1       | [00] writev(2, [{iov_base="\n", iov_len=1}, {iov_base=NULL, iov_len=0}], 2
proxy_1       | [00] ) = 1
proxy_1       | [00] socket(AF_UNIX, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 3
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/dev/log"}, 12) = -1 ENOENT (No such file or directory)
proxy_1       | [00] sendto(3, "<9>Nov 19 11:44:20 : FATAL: No v"..., 95, 0, NULL, 0) = -1 ENOTCONN (Socket not connected)
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/dev/log"}, 12) = -1 ENOENT (No such file or directory)
proxy_1       | [00] writev(2, [{iov_base="2020/11/19 11:44:20| Squid Cache"..., iov_len=72}, {iov_base=NULL, iov_len=0}], 22020/11/19 11:44:20| Squid Cache (Version 4.13): Terminated abnormally.
proxy_1       | [00] ) = 72
proxy_1       | [00] getrusage(RUSAGE_SELF, {ru_utime={tv_sec=0, tv_usec=76197}, ru_stime={tv_sec=0, tv_usec=100984}, ...}) = 0
proxy_1       | [00] writev(2, [{iov_base="CPU Usage: 0.177 seconds = 0.076"..., iov_len=50}, {iov_base=NULL, iov_len=0}], 2CPU Usage: 0.177 seconds = 0.076 user + 0.101 sys
proxy_1       | [00] ) = 50
proxy_1       | [00] writev(2, [{iov_base="Maximum Resident Size: 42304 KB\n", iov_len=32}, {iov_base=NULL, iov_len=0}], 2Maximum Resident Size: 42304 KB
proxy_1       | [00] ) = 32
proxy_1       | [00] writev(2, [{iov_base="Page faults with physical i/o: 0"..., iov_len=33}, {iov_base=NULL, iov_len=0}], 2Page faults with physical i/o: 0
proxy_1       | [00] ) = 33
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] exit_group(1)                           = ?
proxy_1       | [00] +++ exited with 1 +++
proxy_1       | [00] (error exit: exit status 1)


-- 
Met vriendelijke groeten,
Niels Hofmans

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201119/f876758b/attachment.htm>

From hello at ironpeak.be  Thu Nov 19 11:57:11 2020
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 19 Nov 2020 12:57:11 +0100
Subject: [squid-users] squid mitm
In-Reply-To: <06870342-73FC-4C0B-A749-FD3106435632@ironpeak.be>
References: <mailman.8991.1605783729.1313.squid-users@lists.squid-cache.org>
 <06870342-73FC-4C0B-A749-FD3106435632@ironpeak.be>
Message-ID: <9A8FAD47-45FE-4D5C-95BE-A1E83EC90154@ironpeak.be>

Hi,

After noticing my permission errors on /ca.pem, I more or less got this:

Is /dev/log/ really required, since we log to /dev/stdout?

proxy_1       | [00] open("/dev/stdout", O_RDWR|O_CREAT|O_APPEND, 0666) = -1 EACCES (Permission denied)
proxy_1       | [00] writev(2, [{iov_base="WARNING: Cannot write log file: "..., iov_len=44}, {iov_base=NULL, iov_len=0}], 2WARNING: Cannot write log file: /dev/stdout
proxy_1       | [00] ) = 44
proxy_1       | [00] writev(2, [{iov_base="", iov_len=0}, {iov_base="/dev/stdout", iov_len=11}], 2/dev/stdout) = 11
proxy_1       | [00] writev(2, [{iov_base="", iov_len=0}, {iov_base=":", iov_len=1}], 2:) = 1
proxy_1       | [00] writev(2, [{iov_base="", iov_len=0}, {iov_base=" ", iov_len=1}], 2 ) = 1
proxy_1       | [00] writev(2, [{iov_base="", iov_len=0}, {iov_base="Permission denied", iov_len=17}], 2Permission denied) = 17
proxy_1       | [00] writev(2, [{iov_base="", iov_len=0}, {iov_base="\n", iov_len=1}], 2
proxy_1       | [00] ) = 1
proxy_1       | [00] writev(2, [{iov_base="", iov_len=0}, {iov_base="         messages will be sent t"..., iov_len=44}], 2         messages will be sent to 'stderr'.
proxy_1       | [00] ) = 44
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setresuid(-1, 0, -1)                    = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] prctl(PR_SET_DUMPABLE, SUID_DUMP_USER)  = 0
proxy_1       | [00] umask(027)                              = 027
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setresuid(-1, 0, -1)                    = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] prctl(PR_SET_DUMPABLE, SUID_DUMP_USER)  = 0
proxy_1       | [00] open("/var/run/squid.pid", O_RDONLY)    = -1 ENOENT (No such file or directory)
proxy_1       | [00] geteuid()                               = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setgid(1000)                            = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/var/run/nscd/socket"}, 24) = -1 ENOENT (No such file or directory)
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] open("/etc/group", O_RDONLY|O_CLOEXEC)  = 3
proxy_1       | [00] fcntl(3, F_SETFD, FD_CLOEXEC)           = 0
proxy_1       | [00] fcntl(3, F_SETFD, FD_CLOEXEC)           = 0
proxy_1       | [00] read(3, "root:x:0:root\napp:x:1000:\ndnscac"..., 1024) = 88
proxy_1       | [00] read(3, "", 1024)                       = 0
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] setgroups(1, [1000])                    = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setresuid(1000, 1000, 0)                = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] capget({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, NULL) = 0
proxy_1       | [00] capget({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, {effective=0, permitted=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE, inheritable=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE}) = 0
proxy_1       | [00] capset({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, {effective=1<<CAP_NET_BIND_SERVICE, permitted=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE, inheritable=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE}) = 0
proxy_1       | [00] prctl(PR_SET_DUMPABLE, SUID_DUMP_USER)  = 0
proxy_1       | [00] brk(0x55dd853ca000)                     = 0x55dd853ca000
proxy_1       | [00] brk(0x55dd853e5000)                     = 0x55dd853e5000
proxy_1       | [00] geteuid()                               = 1000
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setresuid(-1, 0, -1)                    = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] prctl(PR_SET_DUMPABLE, SUID_DUMP_USER)  = 0
proxy_1       | [00] socket(AF_UNIX, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 3
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/dev/log"}, 12) = -1 ENOENT (No such file or directory)
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], [], 8)   = 0
proxy_1       | [00] fork()                                  = 2429
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] exit_group(0)                           = ?
proxy_1       | [00] +++ exited with 0 +++

-- 
Met vriendelijke groeten,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 19 Nov 2020, at 12:45, Niels Hofmans <hello at ironpeak.be> wrote:

Hello Amos,

I am using the latest squid release on alpine, which is 4.13-r0.
After using the exact command openssl req -new -newkey rsa:2048 -sha256 -days 365 -nodes -x509 -extensions v3_ca -keyout ca.pem  -out ca.pem I still receive this error.

Since it?s a debug cert, I gisted it here: https://gist.githubusercontent.com/hazcod/530ae4ad467d8ed3de6621ba04dddc79/raw/fe62ab6b71f888dd890aded2d61c7c798747a665/ca.pem <https://gist.githubusercontent.com/hazcod/530ae4ad467d8ed3de6621ba04dddc79/raw/fe62ab6b71f888dd890aded2d61c7c798747a665/ca.pem>

strace excerpt:

proxy_1       | [00] brk(0x55e41021f000)                     = 0x55e41021f000
proxy_1       | [00] read(3, "", 1024)                       = 0
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] brk(0x55e410220000)                     = 0x55e410220000
proxy_1       | [00] getuid()                                = 0
proxy_1       | [00] geteuid()                               = 0
proxy_1       | [00] getgid()                                = 0
proxy_1       | [00] getegid()                               = 0
proxy_1       | [00] open("/ca.pem", O_RDONLY)               = -1 EACCES (Permission denied)
proxy_1       | [00] open("/ca.pem", O_RDONLY)               = -1 EACCES (Permission denied)
proxy_1       | [00] geteuid()                               = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setgid(1000)                            = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] socket(AF_UNIX, SOCK_STREAM|SOCK_CLOEXEC, 0) = 3
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/var/run/nscd/socket"}, 24) = -1 ENOENT (No such file or directory)
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] open("/etc/group", O_RDONLY|O_CLOEXEC)  = 3
proxy_1       | [00] fcntl(3, F_SETFD, FD_CLOEXEC)           = 0
proxy_1       | [00] fcntl(3, F_SETFD, FD_CLOEXEC)           = 0
proxy_1       | [00] read(3, "root:x:0:root\napp:x:1000:\ndnscac"..., 1024) = 88
proxy_1       | [00] read(3, "", 1024)                       = 0
proxy_1       | [00] close(3)                                = 0
proxy_1       | [00] setgroups(1, [1000])                    = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[], NULL, 8) = 0
proxy_1       | [00] setresuid(1000, 1000, 0)                = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] capget({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, NULL) = 0
proxy_1       | [00] capget({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, {effective=0, permitted=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE, inheritable=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE}) = 0
proxy_1       | [00] capset({version=_LINUX_CAPABILITY_VERSION_3, pid=0}, {effective=1<<CAP_NET_BIND_SERVICE, permitted=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE, inheritable=1<<CAP_SETGID|1<<CAP_SETUID|1<<CAP_NET_BIND_SERVICE|1<<CAP_SYS_PTRACE}) = 0
proxy_1       | [00] prctl(PR_SET_DUMPABLE, SUID_DUMP_USER)  = 0
proxy_1       | [00] writev(2, [{iov_base="2020/11/19 11:44:20| ", iov_len=21}, {iov_base="FATAL: No valid signing certific"..., iov_len=73}], 22020/11/19 11:44:20| FATAL: No valid signing certificate configured for HTTP_port 0.0.0.0:3128) = 94
proxy_1       | [00] writev(2, [{iov_base="\n", iov_len=1}, {iov_base=NULL, iov_len=0}], 2
proxy_1       | [00] ) = 1
proxy_1       | [00] socket(AF_UNIX, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 3
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/dev/log"}, 12) = -1 ENOENT (No such file or directory)
proxy_1       | [00] sendto(3, "<9>Nov 19 11:44:20 : FATAL: No v"..., 95, 0, NULL, 0) = -1 ENOTCONN (Socket not connected)
proxy_1       | [00] connect(3, {sa_family=AF_UNIX, sun_path="/dev/log"}, 12) = -1 ENOENT (No such file or directory)
proxy_1       | [00] writev(2, [{iov_base="2020/11/19 11:44:20| Squid Cache"..., iov_len=72}, {iov_base=NULL, iov_len=0}], 22020/11/19 11:44:20| Squid Cache (Version 4.13): Terminated abnormally.
proxy_1       | [00] ) = 72
proxy_1       | [00] getrusage(RUSAGE_SELF, {ru_utime={tv_sec=0, tv_usec=76197}, ru_stime={tv_sec=0, tv_usec=100984}, ...}) = 0
proxy_1       | [00] writev(2, [{iov_base="CPU Usage: 0.177 seconds = 0.076"..., iov_len=50}, {iov_base=NULL, iov_len=0}], 2CPU Usage: 0.177 seconds = 0.076 user + 0.101 sys
proxy_1       | [00] ) = 50
proxy_1       | [00] writev(2, [{iov_base="Maximum Resident Size: 42304 KB\n", iov_len=32}, {iov_base=NULL, iov_len=0}], 2Maximum Resident Size: 42304 KB
proxy_1       | [00] ) = 32
proxy_1       | [00] writev(2, [{iov_base="Page faults with physical i/o: 0"..., iov_len=33}, {iov_base=NULL, iov_len=0}], 2Page faults with physical i/o: 0
proxy_1       | [00] ) = 33
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_BLOCK, ~[RTMIN RT_1 RT_2], [], 8) = 0
proxy_1       | [00] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
proxy_1       | [00] exit_group(1)                           = ?
proxy_1       | [00] +++ exited with 1 +++
proxy_1       | [00] (error exit: exit status 1)


-- 
Met vriendelijke groeten,
Niels Hofmans


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201119/6e4bc83a/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov 19 15:04:11 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 19 Nov 2020 10:04:11 -0500
Subject: [squid-users] reply_header to block downloads
In-Reply-To: <CAGU_CiLfSna9U3rwm7DH6_yocvhoihXEgeAqKrMnaug9-jrW9g@mail.gmail.com>
References: <CAGU_CiLfSna9U3rwm7DH6_yocvhoihXEgeAqKrMnaug9-jrW9g@mail.gmail.com>
Message-ID: <ceeffec4-bf4a-5b71-2b34-ee00f05b01dd@measurement-factory.com>

On 11/18/20 3:20 PM, robert k Wild wrote:

> can i use the acl "reply_header_access" to block downloads, like i have
> done with the " rep_mime_type " or is this not what its meant for

Roughly speaking, ACL is a boolean function -- something that gives
Squid a yes/no answer to an ACL-specific question. A typical ACL can be
used in many contexts, for many purposes. By itself, an ACL does not
block or allow anything. Unfortunately, folks sometimes misuse the term
"ACL" to mean "an ACL-driven directive".

* rep_mime_type is an ACL. The question this particular ACL answers is
"Does the response have the specified Content-Type header field value?"

* reply_header_access is not an ACL. It is an ACL-driven directive (i.e.
a directive that accepts ACLs as configuration parameters). This
particular directive does not block any responses. Instead, it prevents
individual response header fields from being delivered by Squid to HTTP
clients. It does not affect responses received by Squid -- beyond
sending bytes to clients, Squid does not see the effects of this
directive when processing the response. For example, Squid code
responsible for storing responses in the cache is executed before this
directive is applied.


HTH,

Alex.


From david at articatech.com  Thu Nov 19 17:17:01 2020
From: david at articatech.com (David Touzeau)
Date: Thu, 19 Nov 2020 18:17:01 +0100
Subject: [squid-users] squid 4/5 feature request send login informations to
 peers
In-Reply-To: <bd50fb8d-d7b8-1534-27a1-d26058eb2271@articatech.com>
References: <bd50fb8d-d7b8-1534-27a1-d26058eb2271@articatech.com>
Message-ID: <cef52b91-d1e1-e1d4-301f-819a0d37ba1b@articatech.com>


Thanks Amos

You means using "login=PASS" in peer settings and in Proxy parent B and 
C use the "basic_fake_auth" helper to "simulate" the requested auth ?



Le 17/11/2020 ? 11:43, Amos Jeffries a ?crit?:
> On 17/11/20 9:27 pm, David Touzeau wrote:
>>
>> Hi,
>>
>> We a first Squid using Kerberos + Active Directory authentication.
>> This first squid is used to limit access using ACls and Active 
>> Directory groups.
>>
>> This first squid using parents as peer in order to access to internet 
>> in this way:
>>
>> ????????????????????????????? | --------> SQUID B ----------> Internet 1
>> squid A ------------->
>> ????????????????????????????? | ---------> SQUID C ---------> Internet 2
>>
>> 1) We want using ACLs too ( for delegation purpose ) on Squid B and C
>> 2) For legal logs purpose compliance.
>>
>> In this case,? the username discovered in SQUIDA must be transmitted 
>> to SQUID B AND C and SQUID B-C must accept the information in order 
>> to use as login information to parse acls
>>
>> Is it possible ?
>
> You can send the username. But the security token is tied to the 
> client<->SquidA TCP connection - it cannot be validated by other 
> servers than SquidA.
>
> This should not matter though. Since Squid A is only permitting 
> authenticated traffic you can *authorize* at Squid B and C based only 
> on the source being one of your Squid with valid username.
>
>
>>
>> If not: wee have seen that the Proxy protocol accept to transmit the 
>> source IP/login information to peers that are compliance with proxy 
>> protocol.
>> but the peers method in squid did not allow to use Proxy protocol.
>> Is it possible to add the "Proxy Protocol" support in peers method ?
>>
>
> It is possible to implement (for Squid-6 earliest) PROXYv2 for 
> cache_peer. But the credentials security token remains tied to SquidA 
> service.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201119/8dffa5f1/attachment.htm>

From ngtech1ltd at gmail.com  Mon Nov 23 01:02:44 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Mon, 23 Nov 2020 03:02:44 +0200
Subject: [squid-users] Gather POST request on HTTPS traffic?
In-Reply-To: <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>
References: <CAGCa14opQ38ULjH5_eP5UpH9Zpv7=GJi4AqjGqdCS8=dZu8pPA@mail.gmail.com>
 <733f2b6b-a3ae-637a-2f40-1e54e08ec696@treenet.co.nz>
Message-ID: <00cf01d6c134$5ab50040$101f00c0$@gmail.com>

Hey Roee,

>From what I remember the best solution would be to use an eCAP module in the long term.
You can use the debug_options and it will work good.
The main issue with this is the DISK IO.
If you do have beefy hardware and SSD+RAM on the machine then the debug_options might be good enough for you.

But the most important thing is to test and verify if it works in your specific environment.

All The Bests,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, November 17, 2020 2:09 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Gather POST request on HTTPS traffic?

On 17/11/20 12:14 pm, roee klinger wrote:
> Hello everyone,
> 
> I work at a digital agency that has quite a few machines that are 
> managing some Instagram accounts, they are all running in the same LAN 
> and we are using Squid as a proxy to log and analyze some usage 
> statistics and to make sure the machines are only used for Instagram.
> 
> We had an idea to use Squid to capture the POST data of users on the 
> proxy level, for example, likes, follows, comments, etc so we can log 
> and analyze everything in a convenient central way, so we can analyze it 
> and even send out clients a monthly report of all the actions their 
> accounts made (who they followed, what they liked, etc).
> 
> I can easily see the requests that I want to capture inside the 
> "network" tab in Chrome but the problem is that Instagram uses HTTPS, so 
> I can't seem to be able to capture this data.
> 
> 
> Is there any way for me to log this data via Squid using the POST data 
> or any other way?
> 

Access to HTTPS transactions for a domain you do not own requires the 
SSL-Bump feature to decrypt ("bump") the TLS layer.
  see <https://wiki.squid-cache.org/Features/SslPeekAndSplice>.

You could use cache.log with "debug_options ALL,1 11,2" configured to 
log the transactions. However an ICAP service or eCAP module that does 
both the record and analyze for you is probably better.


> 
> Note: We are aware of the legal issues, all machines connected to the 
> network are company property, and all the accounts are client accounts 
> that allow us to gather statistics. No personal account data will be 
> gathered.


Please be aware:
   That statement conflicts with the stated purpose(s) of your plan.

Personal data *will* be part of the messages you are decrypting and 
recording for analysis. Further, to perform targeted reports such as 
described you must also associate the data with accounts somehow.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From raki7bh at gmail.com  Mon Nov 23 22:19:02 2020
From: raki7bh at gmail.com (raki ben hamouda)
Date: Mon, 23 Nov 2020 23:19:02 +0100
Subject: [squid-users] SQUID src/Tools.cc
Message-ID: <CAJ-5kWa7_xDB=cLce=CsOw+z3JKH7ek==BtX+c_TYtkME60+Yg@mail.gmail.com>

Hello, and good day there,

I'm a higher institute student and while I'm taking a look at
https://github.com/squid-cache/squid/blob/master/src/tools.cc
I did not understand the functions provided by this code tools.cc.

Could you explain it to me in a detailed way?

Cordially,
Raki~BH.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201123/3aa77009/attachment.htm>

From raki7bh at gmail.com  Mon Nov 23 22:22:48 2020
From: raki7bh at gmail.com (raki ben hamouda)
Date: Mon, 23 Nov 2020 23:22:48 +0100
Subject: [squid-users] SQUID src/Tools.cc
In-Reply-To: <CAJ-5kWa7_xDB=cLce=CsOw+z3JKH7ek==BtX+c_TYtkME60+Yg@mail.gmail.com>
References: <CAJ-5kWa7_xDB=cLce=CsOw+z3JKH7ek==BtX+c_TYtkME60+Yg@mail.gmail.com>
Message-ID: <CAJ-5kWb=fKtYeXdFs8a3fR_453Ywm5a6p0z3NGShcAdMDLO6+w@mail.gmail.com>

And especially  mail_warranty(void)

On Mon, Nov 23, 2020 at 11:19 PM raki ben hamouda <raki7bh at gmail.com> wrote:

> Hello, and good day there,
>
> I'm a higher institute student and while I'm taking a look at
> https://github.com/squid-cache/squid/blob/master/src/tools.cc
> I did not understand the functions provided by this code tools.cc.
>
> Could you explain it to me in a detailed way?
>
> Cordially,
> Raki~BH.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201123/c03f5b7d/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov 24 03:51:58 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Nov 2020 22:51:58 -0500
Subject: [squid-users] SQUID src/Tools.cc
In-Reply-To: <CAJ-5kWb=fKtYeXdFs8a3fR_453Ywm5a6p0z3NGShcAdMDLO6+w@mail.gmail.com>
References: <CAJ-5kWa7_xDB=cLce=CsOw+z3JKH7ek==BtX+c_TYtkME60+Yg@mail.gmail.com>
 <CAJ-5kWb=fKtYeXdFs8a3fR_453Ywm5a6p0z3NGShcAdMDLO6+w@mail.gmail.com>
Message-ID: <6d17569e-ac4f-0b3b-562b-5f5c7e9f7d59@measurement-factory.com>

> On Mon, Nov 23, 2020 at 11:19 PM raki ben hamouda wrote:
> 
>     I'm a higher institute student and while I'm taking a look at
>     https://github.com/squid-cache/squid/blob/master/src/tools.cc
>     I did not understand the functions provided by this code tools.cc.
> 
>     Could you explain it to me in a detailed?way?

Development-related questions are more appropriate for the squid-dev
mailing list, but even there, it is very unlikely that a question like
this would motivate somebody to spend the time necessary to detail every
one of the ~40 functions in that file!


> And especially  mail_warranty(void)

To understand what that function is for, try checking how it is _used_.
You will find that mail_warranty() is called only from the function
called death() and only when Config.adminEmail is set. You can search
for Config.adminEmail description in src/cf.data.pre. Its formatted
version (which loses its Config member association) is at
http://www.squid-cache.org/Doc/config/cache_mgr/


HTH,

Alex.


From guillaume-externe.ranquet at edf.fr  Tue Nov 24 15:55:01 2020
From: guillaume-externe.ranquet at edf.fr (RANQUET Guillaume - externe)
Date: Tue, 24 Nov 2020 15:55:01 +0000
Subject: [squid-users] squid won't cache some files with errors "
 storeCreate: no swapdirs for..."
Message-ID: <960fad6d0df547afa3a1c2624e5923a9@PCYINTPEXMU008.NEOPROD.EDF.FR>

Hello,



I'm having some difficulties understanding some errors I'm getting with squid while trying to act as a cache of a yum repository.

Squid works "mostly" fine and cache "most" of the files going through... but some won't get into the cache and this is quite problematic for us.

It doesn't seem to depend on the size of the file or its name... I'm moving a problematic file and it will get cached just fine.

And no matter how many times I "wget" the problematic file through the proxy, the file won't get into the cache.



Config wise, I've tried multiple things.

SMP / non SMP.

Rock / aufs / in memory only...



The machine its running on is quite beefy (200GB of ram, 48 cores) and does nothing else than running squid.



And so far my config looks like this:

debug_options ALL,1 11,3 31,3 65,3 16,5 20,5 cache_mem                     1 GB maximum_object_size_in_memory 10 MB maximum_object_size           512 MB workers 3 access_log                    daemon:/var/log/squid/access.log squid refresh_pattern . 3600 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private ignore-must-revalidate ignore-reload acl authorized_domains dstdomain .hpc.edf.fr http_access allow authorized_domains http_access allow localhost http_port 10.10.10.10:3142 http_port 127.0.0.1:3142 cache_dir rock /data 3000 max-size=500MB





And without further ado, the errors I'm getting in the logs with some of the files...



----------

HTTP/1.1 200 OK^M

Date: Wed, 18 Nov 2020 14:25:56 GMT^M

Server: Apache/2.4.10 (Debian)^M

Last-Modified: Thu, 08 Oct 2020 14:54:47 GMT^M

ETag: "e6c-5b12a0365d88b"^M

Accept-Ranges: bytes^M

Content-Length: 3692^M

Vary: Accept-Encoding^M

Content-Type: application/xml^M

X-Cache: MISS from vipproxy1.mydomain^M

X-Cache-Lookup: MISS from vipproxy1.mydomain:3142^M

Via: 1.1 vipproxy1.mydomain (squid/5.0.1-VCS)^M

Connection: keep-alive^M

^M



----------

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(814) write: storeWrite: writing 1145 bytes for '3BD6C6FADCADCACA042F71576702C21D'

2020/11/18 15:25:56.616 kid3| 20,3| store_swapout.cc(374) mayStartSwapOut: already allowed

2020/11/18 15:25:56.616 kid3| 11,3| http.cc(1087) persistentConnStatus: conn21 local=123.456.789.15:49420 remote=123.456.789.107:80 HIER_DIRECT FD 41 flags=1 eof=0

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(814) write: storeWrite: writing 2547 bytes for '3BD6C6FADCADCACA042F71576702C21D'

2020/11/18 15:25:56.616 kid3| 20,3| store_swapout.cc(374) mayStartSwapOut: already allowed

2020/11/18 15:25:56.616 kid3| 11,3| http.cc(1087) persistentConnStatus: conn21 local=123.456.789.15:49420 remote=123.456.789.107:80 HIER_DIRECT FD 41 flags=1 eof=0

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(1065) complete: storeComplete: '3BD6C6FADCADCACA042F71576702C21D'

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(1258) validLength: storeEntryValidLength: Checking '3BD6C6FADCADCACA042F71576702C21D'

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(1260) validLength: storeEntryValidLength:     object_len = 4007

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(1261) validLength: storeEntryValidLength:         hdr_sz = 315

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(1262) validLength: storeEntryValidLength: content_length = 3692

2020/11/18 15:25:56.616 kid3| 20,3| store_swapout.cc(374) mayStartSwapOut: already allowed

2020/11/18 15:25:56.616 kid3| 20,5| store_swapout.cc(49) storeSwapOutStart: storeSwapOutStart: Begin SwapOut 'http://repos.mydomain/repos/rpm-hpc/el8/repodata/repomd.xml' to dirno -1, fileno FFFFFFFF

2020/11/18 15:25:56.616 kid3| 20,3| store_swapmeta.cc(52) storeSwapMetaBuild: storeSwapMetaBuild URL: http://repos.mydomain/repos/rpm-hpc/el8/repodata/repomd.xml

2020/11/18 15:25:56.616 kid3| 20,2| store_io.cc(38) storeCreate: storeCreate: no swapdirs for e:tw1343=sp2V/0x559eb98619f0*4

2020/11/18 15:25:56.616 kid3| 20,5| Transients.cc(295) evictCached: e:tw1343=sp2V/0x559eb98619f0*4

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(466) unlock: Client unlocking key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*4

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(466) unlock: FwdState unlocking key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*3

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(442) lock: store_client::copy locked key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*3

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(466) unlock: store_client::copy unlocking key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*3

2020/11/18 15:25:56.616 kid3| 20,3| store_swapout.cc(347) mayStartSwapOut:  already rejected

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(442) lock: storeUnregister locked key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*3

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(466) unlock: storeUnregister unlocking key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*3

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(466) unlock: clientReplyContext::removeStoreReference unlocking key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*2

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(466) unlock: ClientHttpRequest::loggingEntry unlocking key 3BD6C6FADCADCACA042F71576702C21D e:tw1343=sp2V/0x559eb98619f0*1

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(482) doAbandon: e:tw1343=sp2V/0x559eb98619f0*0 via ClientHttpRequest::loggingEntry

2020/11/18 15:25:56.616 kid3| 20,5| Controller.cc(654) handleIdleEntry: destroying unlocked entry: 0x559eb98619f0 e:tw1343=sp2V/0x559eb98619f0*0

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(398) destroyStoreEntry: destroyStoreEntry: destroying 0x559eb98619f8

2020/11/18 15:25:56.616 kid3| 20,3| store.cc(380) destroyMemObject: 0x559eb9861a70 in e:tw1343=sp2V/0x559eb98619f0*0

2020/11/18 15:25:56.616 kid3| 20,5| Transients.cc(320) disconnect: e:tw1343=sp2V/0x559eb98619f0*0

2020/11/18 15:25:56.616 kid3| 20,3| MemObject.cc(108) ~MemObject: MemObject destructed, this=0x559eb9861a70

2020/11/18 15:25:56.616 kid3| 20,5| store.cc(353) ~StoreEntry: StoreEntry destructed, this=0x559eb98619f0



I'm running a rather recent version of squid that I recompiled from git with the "enable-http-violations" use flag.

Squid Cache: Version 5.0.1-VCS

Service Name: squid



This binary uses OpenSSL 1.1.1c FIPS  28 May 2019. For legal restrictions on distribution see https://www.openssl.org/source/license.html



configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--localstatedir=/var' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--libexecdir=/usr/lib64/squid' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB,SMB_LM' '--enable-auth-ntlm=SMB_LM,fake' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos' '--enable-storeid-rewrite-helpers=file' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-diskio' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' '--disable-security-cert-validators' '--with-swapdir=/var/spool/squid' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection' 'LDFLAGS=-Wl,-z,relro  -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld' 'CXXFLAGS=-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' '--enable-http-violations'





We have resorted to use apt-cacher-ng instead of squid for the moment as we are heavily impacted "performance wise" by this lack of caching on some of the files.

If anyone has hints or suggestions of what might be the problem ?



Thx,

Guillaume.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201124/92c3fba7/attachment.htm>
-------------- next part --------------



Ce message et toutes les pi?ces jointes (ci-apr?s le 'Message') sont ?tablis ? l'intention exclusive des destinataires et les informations qui y figurent sont strictement confidentielles. Toute utilisation de ce Message non conforme ? sa destination, toute diffusion ou toute publication totale ou partielle, est interdite sauf autorisation expresse.

Si vous n'?tes pas le destinataire de ce Message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d'en utiliser tout ou partie. Si vous avez re?u ce Message par erreur, merci de le supprimer de votre syst?me, ainsi que toutes ses copies, et de n'en garder aucune trace sur quelque support que ce soit. Nous vous remercions ?galement d'en avertir imm?diatement l'exp?diteur par retour du message.

Il est impossible de garantir que les communications par messagerie ?lectronique arrivent en temps utile, sont s?curis?es ou d?nu?es de toute erreur ou virus.
____________________________________________________

This message and any attachments (the 'Message') are intended solely for the addressees. The information contained in this Message is confidential. Any use of information contained in this Message not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval.

If you are not the addressee, you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return message.

E-mail communication cannot be guaranteed to be timely secure, error or virus-free.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201124/92c3fba7/attachment-0001.htm>

From rousskov at measurement-factory.com  Tue Nov 24 19:04:56 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Nov 2020 14:04:56 -0500
Subject: [squid-users] squid won't cache some files with errors "
 storeCreate: no swapdirs for..."
In-Reply-To: <960fad6d0df547afa3a1c2624e5923a9@PCYINTPEXMU008.NEOPROD.EDF.FR>
References: <960fad6d0df547afa3a1c2624e5923a9@PCYINTPEXMU008.NEOPROD.EDF.FR>
Message-ID: <458735f9-346b-737e-32d6-d2834fe088f5@measurement-factory.com>

On 11/24/20 10:55 AM, RANQUET Guillaume - externe wrote:

> cache_dir ... max-size=500MB

Unfortunately, the cache_dir max-size parameter does not understand size
units (a missing feature). Squid also does not tell you that you have
misconfigured it (a bug). I have not tested whether that problem is the
reason behind "no swapdirs for e..." decision, but there is a very good
chance that it is -- 500MB gets misinterpreted as 500 bytes.

The correct way to express a 500MB size limit is

    cache_dir rock /data 3000 max-size=524288000


HTH,

Alex.


From robertkwild at gmail.com  Thu Nov 26 14:05:58 2020
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 26 Nov 2020 14:05:58 +0000
Subject: [squid-users] found out why my "rep_mime_type" wasnt working
Message-ID: <CAGU_CiJbmyQ_Dv5yogncQ0EsFiEraUXPp66oiZNG6e-U-WhCNQ@mail.gmail.com>

i finally figured out why my blocking of mime types wasn't working

this is my good code now

#allow special URL paths
acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
#
#deny MIME types
acl mimetype rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
http_reply_access allow special_url
http_reply_access deny mimetype

in my mime deny its

application/octet-stream
application/x-msi
application/zip
application/x-7z-compressed
application/vnd.ms-cab-compressed

but in my url specials

http://ccmdl.adobe.com/AdobeProducts/KCCC/1/win64/packages/

it contains a mime type that im blocking but now it passes it through as i
have put an allow specials before the deny mime types

and when i go to an adobe website to download an exe ie adobe reader dc, it
detects it and blocks it as its an exe or octet stream

but then this was in my code aswell

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i
"/usr/local/squid/etc/interceptssl.txt"
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

and in my "interceptssl.txt" i stupidly put

.adobe.com

and this was just allowing me to download the exe acrobat adobe reader, i
was going nuts over this but i finally figured it out

thanks all

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201126/272f73e8/attachment.htm>

From roeeklinger60 at gmail.com  Sat Nov 28 12:00:47 2020
From: roeeklinger60 at gmail.com (roee klinger)
Date: Sat, 28 Nov 2020 14:00:47 +0200
Subject: [squid-users] Debugging a slow Squid?
Message-ID: <CAGCa14q8g3B7mF3kTOOzLC+N6We59ZRA5z-75XqsBwKtuG3z_w@mail.gmail.com>

Hey everyone,

I have been having a slow response time on my Squid and I am trying to
figure out why and debug, I am not sure if the server room router is
slowing the response or if Squid is at fault.

I am aware of the <tr> in the Squid log, but does it show the time it took
Squid to process the request or does it also include the time it took the
request to even get to Squid (for example the time from the router to
Squid, from the user to the router, etc)?

What would be the best way to isolate, inspect, and improve Squid
performance, regardless of network performance?

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20201128/160371e4/attachment.htm>

From rousskov at measurement-factory.com  Sat Nov 28 16:23:34 2020
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 28 Nov 2020 11:23:34 -0500
Subject: [squid-users] Debugging a slow Squid?
In-Reply-To: <CAGCa14q8g3B7mF3kTOOzLC+N6We59ZRA5z-75XqsBwKtuG3z_w@mail.gmail.com>
References: <CAGCa14q8g3B7mF3kTOOzLC+N6We59ZRA5z-75XqsBwKtuG3z_w@mail.gmail.com>
Message-ID: <3e594643-4a5a-b2e4-7ecf-cf30ca1d52b0@measurement-factory.com>

On 11/28/20 7:00 AM, roee klinger wrote:

> I am aware of the <tr> in the Squid log, but does it show the time it
> took Squid to process the request or does it also include the time it
> took the request to even get to Squid (for example the time from the
> router to Squid, from the user to the router, etc)?

Overall, do not expect Squid to measure what happens before the HTTP
request header is received. Squid could (and should and eventually will)
measure more than it does now when it comes to the beginning of the
transaction, but often there is just not enough information to measure
the client-Squid propagation/communication delay for the initial TCP
packet(s) or for the initial HTTP request packet on a previously used
persistent connection.

Specifically, %tr logformat code logs the difference between master
transaction start time and master transaction logging time.

* According to %tS documentation, Squid currently considers the master
transaction started when a complete HTTP request header initiating the
transaction is received from the client.

* Currently, Squid logs the master transaction when the client-Squid
transaction that was handling the client HTTP request gets destroyed.
Usually, that destruction happens when Squid receives the entire request
from the client and sends the entire response to that client.



> What would be the best way to isolate, inspect, and improve?Squid
> performance, regardless of network performance?

One could write a book answering this question, but I would start by
running atop to exclude system bottlenecks (memory, CPU, disk) first.
Once system/global bottlenecks are eliminated, you may be able to detect
processing stages that slow down the transaction the most by sending
representative test requests.


HTH,

Alex.


From ngtech1ltd at gmail.com  Sat Nov 28 18:49:49 2020
From: ngtech1ltd at gmail.com (Eliezer Croitor)
Date: Sat, 28 Nov 2020 20:49:49 +0200
Subject: [squid-users] Debugging a slow Squid?
In-Reply-To: <3e594643-4a5a-b2e4-7ecf-cf30ca1d52b0@measurement-factory.com>
References: <CAGCa14q8g3B7mF3kTOOzLC+N6We59ZRA5z-75XqsBwKtuG3z_w@mail.gmail.com>
 <3e594643-4a5a-b2e4-7ecf-cf30ca1d52b0@measurement-factory.com>
Message-ID: <011801d6c5b7$40c68f50$c253adf0$@gmail.com>

I must add that to verify any browsing speed issue there are couple very specific tests which should be tested:
- DNS response speed, response content ie A/AAAA/CNAME etc..
- Basic traceroute tests
- periodic Ping tests
- CPU load(top, htop, others..)
- curl/wget/other specific pages speed download tests
- Another http/socks proxy (tinyproxy/others, see the root folder of the onedrive shared link)
   https://onedrive.live.com/?authkey=%21AFs60Exv3C4B%2DNI&id=6AB28772521B8B88%214385&cid=6AB28772521B8B88

Depend on the up-stream line.
A microwave link can work well for 50-100 Meters however when rain drops pop here and there you would even see 3000ms plus++ delays.
There are ways to compensate on these but these are measurable enough so you would know about them.

I would start with:
How can I make Squid work almost with no DISK?
access_log none ? access_log stdio:/dev/null ?
cache deny all ?
cache_mem 0 MB ?

Adding a local dns cache might resolve many issues(bind, unbound, dnsmasq..)
Without writing a book.
What might cache_mgr interface has to say about this?

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Saturday, November 28, 2020 6:24 PM
To: roee klinger <roeeklinger60 at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Debugging a slow Squid?

On 11/28/20 7:00 AM, roee klinger wrote:

> I am aware of the <tr> in the Squid log, but does it show the time it
> took Squid to process the request or does it also include the time it
> took the request to even get to Squid (for example the time from the
> router to Squid, from the user to the router, etc)?

Overall, do not expect Squid to measure what happens before the HTTP
request header is received. Squid could (and should and eventually will)
measure more than it does now when it comes to the beginning of the
transaction, but often there is just not enough information to measure
the client-Squid propagation/communication delay for the initial TCP
packet(s) or for the initial HTTP request packet on a previously used
persistent connection.

Specifically, %tr logformat code logs the difference between master
transaction start time and master transaction logging time.

* According to %tS documentation, Squid currently considers the master
transaction started when a complete HTTP request header initiating the
transaction is received from the client.

* Currently, Squid logs the master transaction when the client-Squid
transaction that was handling the client HTTP request gets destroyed.
Usually, that destruction happens when Squid receives the entire request
from the client and sends the entire response to that client.



> What would be the best way to isolate, inspect, and improve Squid
> performance, regardless of network performance?

One could write a book answering this question, but I would start by
running atop to exclude system bottlenecks (memory, CPU, disk) first.
Once system/global bottlenecks are eliminated, you may be able to detect
processing stages that slow down the transaction the most by sending
representative test requests.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



