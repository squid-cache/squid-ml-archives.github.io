From squid3 at treenet.co.nz  Thu Apr  1 07:13:14 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 1 Apr 2021 20:13:14 +1300
Subject: [squid-users] Linking Squid Logs
In-Reply-To: <MN2PR06MB5951FD073FE9AE408423FFAD987C9@MN2PR06MB5951.namprd06.prod.outlook.com>
References: <MN2PR06MB5951FD073FE9AE408423FFAD987C9@MN2PR06MB5951.namprd06.prod.outlook.com>
Message-ID: <dab07231-f876-0dc9-a3b8-b3125833185c@treenet.co.nz>

On 1/04/21 6:59 am, Garbacik, Joe wrote:
> In my squid.conf, I have the following logformat which passes all the 
> data from the client via the load balancer to the squid server as headers:
>

...
> 
> This creates the two logs at the end of this message, What I am 
> wondering is:
> 
>  1. Why aren't all the request headers (look between *?** *REQUEST
>     HEADERS and *** RESPONSE HEADERSin each log) seen in the first log
>     present in the second log

They are different transactions.


>  2. I'm assuming since squid is then making the request in the second
>     log, it leaves the items in Flow0 (client ?load balancer) empty but
>     does retain the data for flow1 (load-balancer-> squid)and flow2
>     (squid -> destination). Even the XFF is not passed. It there anyway
>     to included retain this data?



First log entry is an HTTP request to initiate (CONNECT) a tunnel.

Second log entry is an HTTPS request to fetch (GET) data from a server.

What is happening is;

In the beginning there exists a TCP connection between Haproxy and 
Squid. Transferring HTTP messages.

One of those messages is a CONNECT request. Meaning connect this current 
TCP connection to the named server:port and stop performing HTTP - all 
following bytes will be some other protocol.

When Squid acknowledges the tunnel is connected bytes initiating a TLS 
connection start arriving. Squid does its SSL-Bump things to look inside.

The CONNECT message and state related to it are now complete. It gets 
logged and discarded.
   ** this messages is your log line #1.


What is found inside the TLS is a private HTTP communication channel 
with its own *fully separate* HTTP messages going on between the client 
and server. Squid starts acting as an interception proxy for those messages.
   ** one of these messages is your log line #2.


Notice firstly that the CONNECT message is only between the client and 
Squid. There are no HTTP headers or such going to the server for tunnel 
setup - just a TCP CYN packet.

Notice secondly that the intercept-proxy/SSL-bump decrypted HTTP 
messages have no relationship to the CONNECT or any prior forward-proxy 
HTTP messages on the TCP connection. They only thing they have in common 
is that they arrived on the same TCP connection between haproxy and Squid.
  If there is actually a relationship between them it might be visible 
in the fact that haproxy received both from the same client at its end

    ...  or not. Because we don't know whether haproxy can actually see 
the origin client or just another proxy multiplexing traffic into _that_ 
TCP connection.



>  3. Is there a way to generate an unique Id for each flow so, besides
>     the data in flow0, once can easily link these logs together? 
> 

That can only be done reliably by the client itself sending an HTTP 
header in all messages with its flow ID.

Otherwise the closest you can get is to define "flow" as everything from 
a haproxy ingress = { src-IP, src-port, dst-IP, dst-port, squid 
local-IP, squid local-port } to Squids egress = { src-IP, src-port, 
dst-IP, dst-port, dst-domain }.



Amos



From squid3 at treenet.co.nz  Thu Apr  1 07:22:13 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 1 Apr 2021 20:22:13 +1300
Subject: [squid-users] icap adaptation chains with adaptation sets
In-Reply-To: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
References: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
Message-ID: <c4909551-a2a3-4365-7270-58992ba40335@treenet.co.nz>

On 1/04/21 3:02 am, Klaus Brandl wrote:
> Hi,
> 
> is there a way to use more adaptation sets(for redundancy) combined in
> an adaptation chain?
> What we need is something like this:
> 

Have you tried configuring something like exactly what you posted?


Amos


From klaus_brandl at genua.de  Thu Apr  1 08:27:04 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Thu, 1 Apr 2021 08:27:04 +0000
Subject: [squid-users] icap adaptation chains with adaptation sets
In-Reply-To: <c4909551-a2a3-4365-7270-58992ba40335@treenet.co.nz>
References: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
 <c4909551-a2a3-4365-7270-58992ba40335@treenet.co.nz>
Message-ID: <166bae42237335f1e3e210d7a6c0071e7be232ef.camel@genua.de>

yes, then i get:

2021/04/01 10:23:53 kid1| Adaptation support is on
2021/04/01 10:23:53.421 kid3| ERROR: ERROR: Unknown adaptation name
pool1 in adaptation chain 'checks'
2021/04/01 10:23:53.421 kid4| 93,2| src/adaptation/Config.cc(224)
FinalizeEach: Initialized 14 message adaptation services
2021/04/01 10:23:53 kid2| Adaptation support is on
2021/04/01 10:23:53.421 kid3| ERROR: ERROR: Unknown adaptation name
pool2 in adaptation chain 'checks'

(and so on...)

Klaus

Am Donnerstag, den 01.04.2021, 20:22 +1300 schrieb Amos Jeffries:
> On 1/04/21 3:02 am, Klaus Brandl wrote:
> > Hi,
> > 
> > is there a way to use more adaptation sets(for redundancy) combined
> > in
> > an adaptation chain?
> > What we need is something like this:
> > 
> 
> Have you tried configuring something like exactly what you posted?
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From klaus_brandl at genua.de  Thu Apr  1 08:31:29 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Thu, 1 Apr 2021 08:31:29 +0000
Subject: [squid-users] icap adaptation chains with adaptation sets
In-Reply-To: <166bae42237335f1e3e210d7a6c0071e7be232ef.camel@genua.de>
References: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
 <c4909551-a2a3-4365-7270-58992ba40335@treenet.co.nz>
 <166bae42237335f1e3e210d7a6c0071e7be232ef.camel@genua.de>
Message-ID: <6611de9c55c00eb66eaebddb9bd1e2c2204270e0.camel@genua.de>

it only works, if i use icap services directly, like:

icap_service b1 reqmod_precache ...
icap_service m1 reqmod_precache ...

adaptation_service_chain checks b1 m1
adaptation_access checks allow all

But then we have no redundancy...

Klaus

Am Donnerstag, den 01.04.2021, 08:27 +0000 schrieb Klaus Brandl:
> yes, then i get:
> 
> 2021/04/01 10:23:53 kid1| Adaptation support is on
> 2021/04/01 10:23:53.421 kid3| ERROR: ERROR: Unknown adaptation name
> pool1 in adaptation chain 'checks'
> 2021/04/01 10:23:53.421 kid4| 93,2| src/adaptation/Config.cc(224)
> FinalizeEach: Initialized 14 message adaptation services
> 2021/04/01 10:23:53 kid2| Adaptation support is on
> 2021/04/01 10:23:53.421 kid3| ERROR: ERROR: Unknown adaptation name
> pool2 in adaptation chain 'checks'
> 
> (and so on...)
> 
> Klaus
> 
> Am Donnerstag, den 01.04.2021, 20:22 +1300 schrieb Amos Jeffries:
> > On 1/04/21 3:02 am, Klaus Brandl wrote:
> > > Hi,
> > > 
> > > is there a way to use more adaptation sets(for redundancy)
> > > combined
> > > in
> > > an adaptation chain?
> > > What we need is something like this:
> > > 
> > 
> > Have you tried configuring something like exactly what you posted?
> > 
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From m_zouhairy at ckta.by  Thu Apr  1 10:41:53 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Thu, 1 Apr 2021 13:41:53 +0300
Subject: [squid-users] compile squid with tumbleweed
Message-ID: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>

 >Peace,
as part of self developing, we decided that turning on sslbump + splice 
is a good idea, so how to install squid with ssl support on tumbleweed?

answer: it is already compiled with ssl support

but now i followed:

https://medium.com/@steensply/installing-and-configuring-squid-proxy-for-ssl-bumping-or-peek-n-splice-34afd3f69522

to enable ssl bumping.

specifically those commands:

/usr/share/ssl/misc/CA.pl -newca
/usr/share/ssl/misc/CA.pl -newreq
/usr/share/ssl/misc/CA.pl -sign
openssl x509 -in newcert.pem -outform DER -out squidTrusted.der
copied the 3 files to /etc/squid/certs
sudo chown squid:squid -R /etc/squid/certs
sudo /usr/libexec/squid/security_file_certgen -c -s 
/var/lib/squid/ssl_db -M 4MB
sudo chown squid:squid -R /var/lib/squid
sudo chmod 700 /etc/squid/certs/... (newcrt.pem newkey.pem squidTrusted.der)

sudo squid -z

asks for certificate password
then


2021/04/01 13:16:57| WARNING: BCP 177 violation. Detected non-functional 
IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03| Created PID file (/run/squid.pid)
zouhairy at proxy:~> 2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. 
Detected non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.047 seconds = 0.031 user + 0.016 sys
Maximum Resident Size: 62352 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.040 seconds = 0.032 user + 0.008 sys
Maximum Resident Size: 62272 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.042 seconds = 0.008 user + 0.034 sys
Maximum Resident Size: 63360 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.047 seconds = 0.032 user + 0.016 sys
Maximum Resident Size: 62992 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.045 seconds = 0.030 user + 0.015 sys
Maximum Resident Size: 62640 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03| Removing PID file (/run/squid.pid)


squid conf:

acl localnet (network/24)

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 8080	# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"

http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost
visible_hostname proxy.example.vx

dns_v4_first on

http_access allow localnet
http_access allow localhost



# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 8080

#sslproxy_capath /home/zouhairy/demoCA

http_port 8080 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/newcert.pem 
key=/etc/squid/certs/newkey.pem capath=/home/zouhairy/demoCA




#acl step1 at_step SslBump1
#ssl_bump peek step1
#ssl_bump bump all

#sslcrtd_program /usr/libexec/squid/security_file_certgen -s 
/var/lib/squid/ssl_db -M 4MB
#sslcrtd_children 5

ssl_bump peek all
ssl_bump splice all

#ssl_bump server-first all

sslproxy_cert_error allow all


tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS



range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1


cache_dir ufs /var/cache/squid 3000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mem 1024 MB

netdb_filename none

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:				1440	20%	10080
refresh_pattern ^gopher:			1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 	0		0%	0
refresh_pattern .					0		20%	4320

url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode 
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4
#debug_options ALL,1 33,2 28,9

what to change?


From ngtech1ltd at gmail.com  Fri Apr  2 00:24:08 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 2 Apr 2021 03:24:08 +0300
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
Message-ID: <00cf01d72756$7ff91520$7feb3f60$@gmail.com>

Hey,

First try to use the next example:
https://github.com/elico/yt-classification-service-example/blob/master/redwood/init-local-rootca.sh

To create a rootCA key and certificate, which doesn't require you to use a password.
And I have also seen this article you have used and it has two ways to create the rootca.
One with the CA.pl script and the other one is  with the openssl tool.
As long as you don't need the CA.pl specifically I would recommend using openssl.
It's plain simple to just create a rootCA certificate.

All The Bests,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Majed Zouhairy
Sent: Thursday, April 1, 2021 1:42 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] compile squid with tumbleweed

 >Peace,
as part of self developing, we decided that turning on sslbump + splice 
is a good idea, so how to install squid with ssl support on tumbleweed?

answer: it is already compiled with ssl support

but now i followed:

https://medium.com/@steensply/installing-and-configuring-squid-proxy-for-ssl-bumping-or-peek-n-splice-34afd3f69522

to enable ssl bumping.

specifically those commands:

/usr/share/ssl/misc/CA.pl -newca
/usr/share/ssl/misc/CA.pl -newreq
/usr/share/ssl/misc/CA.pl -sign
openssl x509 -in newcert.pem -outform DER -out squidTrusted.der
copied the 3 files to /etc/squid/certs
sudo chown squid:squid -R /etc/squid/certs
sudo /usr/libexec/squid/security_file_certgen -c -s 
/var/lib/squid/ssl_db -M 4MB
sudo chown squid:squid -R /var/lib/squid
sudo chmod 700 /etc/squid/certs/... (newcrt.pem newkey.pem squidTrusted.der)

sudo squid -z

asks for certificate password
then


2021/04/01 13:16:57| WARNING: BCP 177 violation. Detected non-functional 
IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03| Created PID file (/run/squid.pid)
zouhairy at proxy:~> 2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. 
Detected non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.047 seconds = 0.031 user + 0.016 sys
Maximum Resident Size: 62352 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.040 seconds = 0.032 user + 0.008 sys
Maximum Resident Size: 62272 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.042 seconds = 0.008 user + 0.034 sys
Maximum Resident Size: 63360 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.047 seconds = 0.032 user + 0.016 sys
Maximum Resident Size: 62992 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. Detected 
non-functional IPv6 loopback.
Enter PEM pass phrase:
2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
for HTTP_port 0.0.0.0:8080
2021/04/01 13:17:03 kid1| Squid Cache (Version 4.14): Terminated abnormally.
CPU Usage: 0.045 seconds = 0.030 user + 0.015 sys
Maximum Resident Size: 62640 KB
Page faults with physical i/o: 0
2021/04/01 13:17:03| Removing PID file (/run/squid.pid)


squid conf:

acl localnet (network/24)

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 8080	# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"

http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost
visible_hostname proxy.example.vx

dns_v4_first on

http_access allow localnet
http_access allow localhost



# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 8080

#sslproxy_capath /home/zouhairy/demoCA

http_port 8080 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/newcert.pem 
key=/etc/squid/certs/newkey.pem capath=/home/zouhairy/demoCA




#acl step1 at_step SslBump1
#ssl_bump peek step1
#ssl_bump bump all

#sslcrtd_program /usr/libexec/squid/security_file_certgen -s 
/var/lib/squid/ssl_db -M 4MB
#sslcrtd_children 5

ssl_bump peek all
ssl_bump splice all

#ssl_bump server-first all

sslproxy_cert_error allow all


tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS



range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1


cache_dir ufs /var/cache/squid 3000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mem 1024 MB

netdb_filename none

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:				1440	20%	10080
refresh_pattern ^gopher:			1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 	0		0%	0
refresh_pattern .					0		20%	4320

url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode 
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4
#debug_options ALL,1 33,2 28,9

what to change?
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Apr  2 11:02:38 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 3 Apr 2021 00:02:38 +1300
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
Message-ID: <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>

On 1/04/21 11:41 pm, Majed Zouhairy wrote:
> 
> to enable ssl bumping.
> 
> specifically those commands:
> 
> /usr/share/ssl/misc/CA.pl -newca
> /usr/share/ssl/misc/CA.pl -newreq
> /usr/share/ssl/misc/CA.pl -sign
> openssl x509 -in newcert.pem -outform DER -out squidTrusted.der


> sudo squid -z
> 
> asks for certificate password
> then
> 
> Enter PEM pass phrase:
> 2021/04/01 13:17:03| Created PID file (/run/squid.pid)
> zouhairy at proxy:~> 2021/04/01 13:17:03 kid1| WARNING: BCP 177 violation. 
> Detected non-functional IPv6 loopback.
> Enter PEM pass phrase:
> 2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate configured 
> for HTTP_port 0.0.0.0:8080

That says there is no CA certificate found in the file configured for 
that ports tls-cert= option. Squid requires a signing (CA) certificate 
and its private key in order to perform SSL-Bump.

With "squid -k parse" Squid should tell you what it is loading from that 
file.


> 
> squid conf:
> 
...
> 
> http_port 8080 ssl-bump generate-host-certificates=on 
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/newcert.pem 
> key=/etc/squid/certs/newkey.pem capath=/home/zouhairy/demoCA
> 

> 
> ssl_bump peek all
> ssl_bump splice all
> 
> sslproxy_cert_error allow all
> 



Amos


From rousskov at measurement-factory.com  Fri Apr  2 21:15:06 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Apr 2021 17:15:06 -0400
Subject: [squid-users] Squid 5 does not send ICAP request
In-Reply-To: <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
References: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
 <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
Message-ID: <1de29ddd-ad42-14be-408c-e30df6181553@measurement-factory.com>

On 3/12/21 1:42 PM, Alex Rousskov wrote:
> I suspect you are suffering from Bug 4528:
> https://bugs.squid-cache.org/show_bug.cgi?id=4528
> 
> Which has also been discussed earlier as Bug 3621:
> https://bugs.squid-cache.org/show_bug.cgi?id=3621

PR 795 fixes similar problems in my tests:
https://github.com/squid-cache/squid/pull/795

Please test whether the corresponding patch helps in your environment:
https://github.com/squid-cache/squid/commit/61fdbf0.patch


Thank you,

Alex.


> On 3/12/21 2:44 AM, ???? wrote:
>> I made squid and ICAP system using docker-compose.
>>
>> Squid 4 started sending ICAP requests 1 minute after boot.
>>
>> However, squid 5 sends no ICAP request even 10 minutes after boot.
>> Squid continued to mark the ICAP service down.
>>
>> How can I make squid 5 to start ICAP conversation?
>>
>> * squid version
>> 5.0.5-20210223-r4af19cc24
>>
>> * squid.conf
>>
>> ```
>> http_port 3128
>> http_access allow all
>> icap_enable on
>> icap_service icapsvc reqmod_precache icap://icap5:1344 bypass=off
>> adaptation_access icapsvc allow all
>> icap_persistent_connections off
>> icap_service_revival_delay 60
>> debug_options ALL,9
>> ```
>>
>> * This is my environment.
>> https://github.com/hsmtkk/squidicap
>>
>> * I uploaded access.log and cache.log to the GitHub issue.
>> https://github.com/hsmtkk/squidicap/issues/1
>>
>> Best regards,
>> Kouki Hashimoto
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From m_zouhairy at ckta.by  Sat Apr  3 03:13:23 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Sat, 3 Apr 2021 06:13:23 +0300
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
Message-ID: <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>

hmm, thank you both.. i regenerated new certificates using Eliazer's 
method and now squid restarted but it is refusing connections..
i normally configure port 8080 as the proxy port in the browser, and i 
am thinking there needs to be another port for ssl bumping?

now the configuration is like this:

....


# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 8080

##sslproxy_capath /home/zouhairy/demoCA

http_port 8080 ssl-bump? cert=/etc/squid/certs/myCA.pem 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB



ssl_bump peek all
ssl_bump splice all



#tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

# Uncomment and adjust the following to add a disk cache directory.
# Updates: chrome and acrobat
#refresh_pattern -i gvt1.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80% 
129600 reload-into-ims
#refresh_pattern -i adobe.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 
80% 129600 reload-into-ims



range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1



cache_dir ufs /var/cache/squid 3000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mem 1024 MB

netdb_filename none

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:??????????????? 1440??? 20%??? 10080
refresh_pattern ^gopher:??????????? 1440??? 0%??? 1440
refresh_pattern -i (/cgi-bin/|\?) 0??????? 0%??? 0
refresh_pattern .??????????????????? 0??????? 20%??? 4320

url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode 
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4 #debug_options 
ALL,1 33,2 28,9

2.04.21 14:02, Amos Jeffries ?????:
> On 1/04/21 11:41 pm, Majed Zouhairy wrote:
>>
>> to enable ssl bumping.
>>
>> specifically those commands:
>>
>> /usr/share/ssl/misc/CA.pl -newca
>> /usr/share/ssl/misc/CA.pl -newreq
>> /usr/share/ssl/misc/CA.pl -sign
>> openssl x509 -in newcert.pem -outform DER -out squidTrusted.der
>
>
>> sudo squid -z
>>
>> asks for certificate password
>> then
>>
>> Enter PEM pass phrase:
>> 2021/04/01 13:17:03| Created PID file (/run/squid.pid)
>> zouhairy at proxy:~> 2021/04/01 13:17:03 kid1| WARNING: BCP 177 
>> violation. Detected non-functional IPv6 loopback.
>> Enter PEM pass phrase:
>> 2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate 
>> configured for HTTP_port 0.0.0.0:8080
>
> That says there is no CA certificate found in the file configured for 
> that ports tls-cert= option. Squid requires a signing (CA) certificate 
> and its private key in order to perform SSL-Bump.
>
> With "squid -k parse" Squid should tell you what it is loading from 
> that file.
>
>
>>
>> squid conf:
>>
> ...
>>
>> http_port 8080 ssl-bump generate-host-certificates=on 
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/newcert.pem 
>> key=/etc/squid/certs/newkey.pem capath=/home/zouhairy/demoCA
>>
>
>>
>> ssl_bump peek all
>> ssl_bump splice all
>>
>> sslproxy_cert_error allow all
>>
>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Sun Apr  4 01:51:58 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 4 Apr 2021 13:51:58 +1200
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
Message-ID: <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>

On 3/04/21 4:13 pm, Majed Zouhairy wrote:
> hmm, thank you both.. i regenerated new certificates using Eliazer's 
> method and now squid restarted but it is refusing connections..

What is the error happening now?


> i normally configure port 8080 as the proxy port in the browser, and i 
> am thinking there needs to be another port for ssl bumping?
> 

No. SSL-Bump as you have it configured intercepts the CONNECT traffic 
the browser send to normal proxy port.


> now the configuration is like this:
> 
> ....
 >
> http_port 8080 ssl-bump? cert=/etc/squid/certs/myCA.pem 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> 
> 
> ssl_bump peek all
> ssl_bump splice all
> 



> 
> # Uncomment and adjust the following to add a disk cache directory.
> # Updates: chrome and acrobat

NP: the comment above is about the cache_dir line. You can remove it.


Amos


From m_zouhairy at ckta.by  Sun Apr  4 05:09:46 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Sun, 4 Apr 2021 08:09:46 +0300
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
 <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
Message-ID: <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>

the error is:

??????-?????? ???????????? ????????? ??????????

translation: the proxy-server is refusing to accept connections..

might it be some setting in ufdbguard now?

4.04.21 04:51, Amos Jeffries ?????:
> On 3/04/21 4:13 pm, Majed Zouhairy wrote:
>> hmm, thank you both.. i regenerated new certificates using Eliazer's 
>> method and now squid restarted but it is refusing connections..
>
> What is the error happening now?
>
>
>> i normally configure port 8080 as the proxy port in the browser, and 
>> i am thinking there needs to be another port for ssl bumping?
>>
>
> No. SSL-Bump as you have it configured intercepts the CONNECT traffic 
> the browser send to normal proxy port.
>
>
>> now the configuration is like this:
>>
>> ....
> >
>> http_port 8080 ssl-bump cert=/etc/squid/certs/myCA.pem 
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>
>>
>>
>> ssl_bump peek all
>> ssl_bump splice all
>>
>
>
>
>>
>> # Uncomment and adjust the following to add a disk cache directory.
>> # Updates: chrome and acrobat
>
> NP: the comment above is about the cache_dir line. You can remove it.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From andy.frad at gmail.com  Sun Apr  4 10:21:33 2021
From: andy.frad at gmail.com (Andy Frad)
Date: Sun, 4 Apr 2021 06:21:33 -0400
Subject: [squid-users] Bind user ext_file_userip_acl problem
Message-ID: <CAHZe4k-8n7n5HkHPsbf5OtV6b1NeFz34znN6nbmntgXaPsiEfg@mail.gmail.com>

Hello,

I'm currently running squid 4.6 trying to bind users to particular outgoing
ips but it is not working.

In my config:

external_acl_type userip %MYADDR %LOGIN /usr/lib/squid/ext_file_userip_acl
-f /etc/squid/userip.conf

acl password proxy_auth REQUIRED
acl userip external userip

http_access allow localhost
http_access deny !password
http_access allow userip
http_access deny all

It seems as though it's not reading the contents of the userip.conf file.

Default "basic" auth works fine but that is not what I am after.

Any help?

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210404/21272530/attachment.htm>

From squid3 at treenet.co.nz  Sun Apr  4 10:24:14 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 4 Apr 2021 22:24:14 +1200
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
 <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
 <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>
Message-ID: <8487dee7-8f9e-ecda-d9d3-3d30bd2859bb@treenet.co.nz>

On 4/04/21 5:09 pm, Majed Zouhairy wrote:
> the error is:
> 
> ??????-?????? ???????????? ????????? ??????????
> 
> translation: the proxy-server is refusing to accept connections..
> 

That seems like the meaningless text modern Browsers like replacing real 
error with.

Can you check the Squid logs to see what is actually going on?


> might it be some setting in ufdbguard now?
> 

If that text is from the Browser it could be anything.

Amos


From squid3 at treenet.co.nz  Sun Apr  4 11:36:02 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 4 Apr 2021 23:36:02 +1200
Subject: [squid-users] Bind user ext_file_userip_acl problem
In-Reply-To: <CAHZe4k-8n7n5HkHPsbf5OtV6b1NeFz34znN6nbmntgXaPsiEfg@mail.gmail.com>
References: <CAHZe4k-8n7n5HkHPsbf5OtV6b1NeFz34znN6nbmntgXaPsiEfg@mail.gmail.com>
Message-ID: <ff85523e-75ca-22e9-4f6d-ec21c9a38bd9@treenet.co.nz>

On 4/04/21 10:21 pm, Andy Frad wrote:
> Hello,
> 
> I'm currently running squid 4.6 trying to bind users to particular 
> outgoing ips but it is not working.
> 
> In my config:
> 
> external_acl_type userip %MYADDR %LOGIN 
> /usr/lib/squid/ext_file_userip_acl -f /etc/squid/userip.conf
> 

%MYADDR is Squid's IP address. Not the clients.


What you want for Squid-4 is:

   external_acl_type userip %>a %ul \
     /usr/lib/squid/ext_file_userip_acl -f /etc/squid/userip.conf



Amos


From moberger at metanetworks.com  Sun Apr  4 11:44:47 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Sun, 4 Apr 2021 14:44:47 +0300
Subject: [squid-users] Squid 5.0.4 crash
Message-ID: <CAGSk-43mYbsGkOnL_a6Ga2UpKQ0cxha+hf9o8pAZhhPeK40ZeA@mail.gmail.com>

Hi

I noticed Squid sporadically crashes with the following error (taken from
cache.log):

> 2021/04/01 21:58:03| FATAL: check failed: !request->pinnedConnection()
>
>     exception location: FwdState.cc(1055) connectStart
>
>     current master transaction: master4104
>
> 2021/04/01 21:58:03| Removing PID file (/var/run/squid.pid)
>
>     current master transaction: master4104
>
>
>
We run it in a container in a t3.small EC2 instance.
It crashes dozens of times a day, can't tell what triggers it (always with
the same error, beside the master transaction number).
I can attach the logs and squid.conf, around 7MB compressed.
Please advise what's the preferred way of sharing them (or part of them).

Thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210404/ceef5aac/attachment.htm>

From squid3 at treenet.co.nz  Sun Apr  4 13:08:06 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 5 Apr 2021 01:08:06 +1200
Subject: [squid-users] Squid 5.0.4 crash
In-Reply-To: <CAGSk-43mYbsGkOnL_a6Ga2UpKQ0cxha+hf9o8pAZhhPeK40ZeA@mail.gmail.com>
References: <CAGSk-43mYbsGkOnL_a6Ga2UpKQ0cxha+hf9o8pAZhhPeK40ZeA@mail.gmail.com>
Message-ID: <3b9b37f5-9438-218c-365e-7c0a88735b3e@treenet.co.nz>

On 4/04/21 11:44 pm, Moti Berger wrote:
> Hi
> 
> I noticed Squid sporadically crashes with the following error (taken 
> from cache.log):
> 
>         2021/04/01 21:58:03| FATAL: check failed:
>         !request->pinnedConnection()
> 
>          ? ? exception location: FwdState.cc(1055) connectStart
> 
>          ? ? current master transaction: master4104
> 

This is <https://bugs.squid-cache.org/show_bug.cgi?id=5090>


Amos


From squid-user at tlinx.org  Sun Apr  4 20:36:49 2021
From: squid-user at tlinx.org (L A Walsh)
Date: Sun, 04 Apr 2021 13:36:49 -0700
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
 <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
 <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>
Message-ID: <606A2361.3070209@tlinx.org>

On 2021/04/03 22:09, Majed Zouhairy wrote:
> the error is:
>
> ??????-?????? ???????????? ????????? ??????????
>
> translation: the proxy-server is refusing to accept connections..
>   
That most commonly is what I see when squid didn't start, (so it
refuses to accept connections).

Are you sure it started?  Look in the logs for any errors?

I've had problems there where the cert-gen process needed to have
it's database zeroed (among other non-starting issues), so
at boot, squid look liked it ran, but immediately exited when
one of its helpers kept dying...



From hsmtkk at gmail.com  Mon Apr  5 00:10:43 2021
From: hsmtkk at gmail.com (=?UTF-8?B?5qmL5pys57SY5biM?=)
Date: Mon, 5 Apr 2021 09:10:43 +0900
Subject: [squid-users] Squid 5 does not send ICAP request
In-Reply-To: <1de29ddd-ad42-14be-408c-e30df6181553@measurement-factory.com>
References: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
 <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
 <1de29ddd-ad42-14be-408c-e30df6181553@measurement-factory.com>
Message-ID: <CAAif--rp8aTp9r1H=-pW6YYKN2mouyVLOFu=-f6Caq04ReZ8yg@mail.gmail.com>

Dear Alex,

Yes, it solved the problem in my environment.

> Please test whether the corresponding patch helps in your environment:
> https://github.com/squid-cache/squid/commit/61fdbf0.patch

ICAP works fine between squid and the ICAP server.
I hope the patch will be merged to the main line.

Thanks for your help.
Kouki Hashimoto


From m_zouhairy at ckta.by  Mon Apr  5 03:06:17 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 5 Apr 2021 06:06:17 +0300
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <8487dee7-8f9e-ecda-d9d3-3d30bd2859bb@treenet.co.nz>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
 <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
 <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>
 <8487dee7-8f9e-ecda-d9d3-3d30bd2859bb@treenet.co.nz>
Message-ID: <203880a2-1387-5244-2c18-a3a85dca9dd7@ckta.by>

yes, from the browser..

squid cache last showed:

2021/04/02 15:52:47 kid1| Logfile: opening log 
daemon:/var/log/squid/access.log
2021/04/02 15:52:47 kid1| Logfile Daemon: opening log 
/var/log/squid/access.log
2021/04/02 15:52:47 kid1| Unlinkd pipe opened on FD 40
2021/04/02 15:52:47 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2021/04/02 15:52:47 kid1| Store logging disabled
2021/04/02 15:52:47 kid1| Swap maxSize 3072000 + 1048576 KB, estimated 
316967 objects
2021/04/02 15:52:47 kid1| Target number of buckets: 15848
2021/04/02 15:52:47 kid1| Using 16384 Store buckets
2021/04/02 15:52:47 kid1| Max Mem? size: 1048576 KB
2021/04/02 15:52:47 kid1| Max Swap size: 3072000 KB
2021/04/02 15:52:47 kid1| Rebuilding storage in /var/cache/squid (clean log)
2021/04/02 15:52:47 kid1| Using Least Load store dir selection
2021/04/02 15:52:47 kid1| Set Current Directory to /var/cache/squid
2021/04/02 15:52:47 kid1| Finished loading MIME types and icons.
2021/04/02 15:52:47 kid1| HTCP Disabled.
2021/04/02 15:52:47 kid1| Pinger socket opened on FD 45
2021/04/02 15:52:47 kid1| Squid plugin modules loaded: 0
2021/04/02 15:52:47 kid1| Adaptation support is off.
2021/04/02 15:52:47 kid1| Accepting SSL bumped HTTP Socket connections 
at local=0.0.0.0:8080 remote=[::] FD 43 flags=9
2021/04/02 15:52:47| WARNING: BCP 177 violation. Detected non-functional 
IPv6 loopback.
2021/04/02 15:52:47| pinger: Initialising ICMP pinger ...
2021/04/02 15:52:47| pinger: ICMP socket opened.
2021/04/02 15:52:47| pinger: ICMPv6 socket opened
2021/04/02 15:52:47 kid1| Store rebuilding is 19.99% complete
2021/04/02 15:52:47 kid1| Done reading /var/cache/squid swaplog (20010 
entries)
2021/04/02 15:52:47 kid1| Finished rebuilding storage from disk.
2021/04/02 15:52:47 kid1|???? 20010 Entries scanned
2021/04/02 15:52:47 kid1|???????? 0 Invalid entries.
2021/04/02 15:52:47 kid1|???????? 0 With invalid flags.
2021/04/02 15:52:47 kid1|???? 20010 Objects loaded.
2021/04/02 15:52:47 kid1|???????? 0 Objects expired.
2021/04/02 15:52:47 kid1|???????? 0 Objects cancelled.
2021/04/02 15:52:47 kid1|???????? 0 Duplicate URLs purged.
2021/04/02 15:52:47 kid1|???????? 0 Swapfile clashes avoided.
2021/04/02 15:52:47 kid1|?? Took 0.26 seconds (76538.52 objects/sec).
2021/04/02 15:52:47 kid1| Beginning Validation Procedure
2021/04/02 15:52:47 kid1|?? Completed Validation Procedure
2021/04/02 15:52:47 kid1|?? Validated 20010 Entries
2021/04/02 15:52:47 kid1|?? store_swap_size = 1355568.00 KB
2021/04/02 15:52:47 kid1| WARNING: 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 
4MB #Hlpr1 exited
2021/04/02 15:52:47 kid1| Too few 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 
4MB processes are running (need 1/32)
2021/04/02 15:52:47 kid1| Closing HTTP(S) port 0.0.0.0:8080
2021/04/02 15:52:47 kid1| storeDirWriteCleanLogs: Starting...
2021/04/02 15:52:47 kid1|?? Finished.? Wrote 20010 entries.
2021/04/02 15:52:47 kid1|?? Took 0.01 seconds (3978131.21 entries/sec).
2021/04/02 15:52:47 kid1| FATAL: The 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 
4MB helpers are crashing too rapidly, need help!

squid log last showed:


1617367631.100??? 868 10.0.28.26 TCP_REFRESH_MODIFIED_ABORTED/200 13935 
GET http://spastv.ru/ - HIER_DIRECT/84.201.153.140 text/html
1617367725.880????? 0 10.0.28.26 NONE/000 0 NONE 
error:transaction-end-before-headers - HIER_NONE/- -
1617367845.916????? 0 10.0.28.26 NONE/000 0 NONE 
error:transaction-end-before-headers - HIER_NONE/- -

which is an every minute check


sudo systemctl status squid
? squid.service - Squid caching proxy
 ???? Loaded: loaded (/usr/lib/systemd/system/squid.service; enabled; 
vendor preset: disabled)
 ???? Active: failed (Result: exit-code) since Sun 2021-04-04 21:58:13 
+03; 5s ago
 ?????? Docs: man:squid(8)
 ??? Process: 28198 
ExecStartPre=/usr/libexec/squid/initialize_cache_if_needed.sh 
(code=exited, status=0/SUCCESS)
 ??? Process: 28202 ExecStart=/usr/sbin/squid -FC (code=exited, 
status=0/SUCCESS)
 ?? Main PID: 28203 (code=exited, status=1/FAILURE)

Apr 04 21:58:12 proxy squid[28203]: Squid Parent: (squid-1) process 
28355 started
Apr 04 21:58:12 proxy (squid-1)[28355]: FATAL: The 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 4>
Apr 04 21:58:12 proxy squid[28203]: Squid Parent: squid-1 process 28355 
exited with status 1
Apr 04 21:58:12 proxy squid[28203]: Squid Parent: (squid-1) process 
28405 started
Apr 04 21:58:13 proxy (squid-1)[28405]: FATAL: The 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 4>
Apr 04 21:58:13 proxy squid[28203]: Squid Parent: squid-1 process 28405 
exited with status 1
Apr 04 21:58:13 proxy squid[28203]: Squid Parent: squid-1 process 28405 
will not be restarted for 3600 seconds due to repea>
Apr 04 21:58:13 proxy squid[28203]: Exiting due to repeated, frequent 
failures
Apr 04 21:58:13 proxy systemd[1]: squid.service: Main process exited, 
code=exited, status=1/FAILURE
Apr 04 21:58:13 proxy systemd[1]: squid.service: Failed with result 
'exit-code'.

4.04.21 13:24, Amos Jeffries ?????:
> On 4/04/21 5:09 pm, Majed Zouhairy wrote:
>> the error is:
>>
>> ??????-?????? ???????????? ????????? ??????????
>>
>> translation: the proxy-server is refusing to accept connections..
>>
>
> That seems like the meaningless text modern Browsers like replacing 
> real error with.
>
> Can you check the Squid logs to see what is actually going on?
>
>
>> might it be some setting in ufdbguard now?
>>
>
> If that text is from the Browser it could be anything.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Mon Apr  5 03:29:30 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 4 Apr 2021 23:29:30 -0400
Subject: [squid-users] icap adaptation chains with adaptation sets
In-Reply-To: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
References: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
Message-ID: <b6667fc7-4a4b-f23b-71ab-fd98f52b348f@measurement-factory.com>

On 3/31/21 10:02 AM, Klaus Brandl wrote:

> is there a way to use more adaptation sets(for redundancy) combined in
> an adaptation chain?

Squid only supports chains of services and sets of services. There is
currently no support for nesting (e.g., chains of sets). Such support
would be generally useful IMO.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


> What we need is something like this:
> 
> icap_service b1 reqmod_precache ...
> icap_service b2 reqmod_precache ...
> icap_service b3 reqmod_precache ...
> icap_service b4 reqmod_precache ...
> 
> icap_service m1 reqmod_precache ...
> icap_service m2 reqmod_precache ...
> icap_service m3 reqmod_precache ...
> icap_service m4 reqmod_precache ...
> 
> #blacklist
> adaptation_service_set pool1 b1 b2 b3 b4
> #malware scanner
> adaptation_service_set pool2 m1 m2 m3 m4
> 
> adaptation_service_chain checks pool1 pool2
> 
> adaptation_access checks allow all
> 
> Regards
> 
> Klaus



From rousskov at measurement-factory.com  Mon Apr  5 04:02:29 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 5 Apr 2021 00:02:29 -0400
Subject: [squid-users] Linking Squid Logs
In-Reply-To: <MN2PR06MB5951FD073FE9AE408423FFAD987C9@MN2PR06MB5951.namprd06.prod.outlook.com>
References: <MN2PR06MB5951FD073FE9AE408423FFAD987C9@MN2PR06MB5951.namprd06.prod.outlook.com>
Message-ID: <9edfd60c-f170-142b-9cca-e7829d87a37c@measurement-factory.com>

On 3/31/21 1:59 PM, Garbacik, Joe wrote:
>  3. Is there a way to generate an unique Id for each flow so, besides
>     the data in flow0, once can easily link these logs together? ?

I could not spend enough time to grok the true meaning behind all those
logformat %codes and the corresponding logged values in your questions,
but if you are trying to tie CONNECT requests with bumped transactions
inside those CONNECT tunnels, then you can do that using
%transport::>connection_id. Unfortunately, that logformat %code is only
available in master (future v6) branch.

If you cannot use master/v6 code, then you may be able to accomplish the
same effect by annotating the connection during CONNECT request and then
logging that annotation. See the annotate_client ACL and %note logformat
%code. The difficulty with this older solution is what to use for the
annotation value -- the value has to be unique in a worker process scope
(at least).

I have not tested that, but you can try to use %master_xaction as the
annotation _value_. If it is supported (again, I have not checked), then
you will need to "quote" the annotation value string for Squid to start
interpreting %codes in an annotation value.

If you are using SMP Squid, then you will also need to log something
like "kid${process_number}" to make those master transaction IDs unique
across workers OR give each worker a dedicated access.log. You will also
have to move logs across Squid restarts, for similar uniqueness reasons.

You can also generate globally unique IDs using a key=value pair
returned by an external ACL and use those IDs instead of
%master_xaction. That is supported for sure. The clt_conn_tag=value pair
may be especially handy here.

All this requires testing/experimentation, but I suspect that there is a
way to make it work in modern Squids without source code modifications.


HTH,

Alex.



> In my squid.conf, I have the following logformat which passes all the
> data from the client via the load balancer to the squid server as headers:
>
> logformat MyLogFormat  ---> local_time="[%tl]"
> squid_service=%{service}note squid_status=%Ss squid_hierarchy_status=%Sh
> ** haproxy_id=%{X-Request-Id}>h orig_src_ip=%{X-Client-Egress-Ip}>h
> orig_src_port=%{X-Client-Egress-Port}>h
> haproxy_ingress_ip=%{X-Haproxy-Ingress-Ip}>h
> haproxy_ingress_port=%{X-Haproxy-Ingress-Port}>h haproxy_egress_ip=%>a
> haproxy_egress_port=%>p squid_ingress_ip=%>la squid_ingress_port=%>lp
> squid_egress_ip=%<la squid_egress_port=%<lp dst_ip=%<a dst_host=%<A
> dst_port=%<p ident_username=%[ui username=%[un request_method=%rm
> request="%rm %ru HTTP/%rv" status_code_from_server=%>Hs
> status_code_to_client=%<Hs referer="%{Referer}>h"
> user_agent="%{User-Agent}>h" protocol_version=%rv **
> dns_response_time=%dt response_time=%tr mime_type=%mt *XFER*
> total_request_size=%>st total_reply_size=%<st ** %{src_zone}note
> %{dst_zone}note %{method_category}note %{dst_category}note
> %{file_upload}note ** REQUEST HEADERS %>h *** RESPONSE HEADERS %<h ***
> tag_returned=%et tag_string="%ea" previous_hop_mac=%>eui
> peer_response_time=%<pt total_response_time=%<tt *SSL*
> src_ssl_negotiated_version=%ssl::>negotiated_version
> dst_ssl_negotiated_version=%ssl::<negotiated_version
> src_tls_hello_version=%ssl::>received_hello_version
> dst_tls_hello_version=%ssl::<received_hello_version
> src_tls_max_version=%ssl::>received_supported_version
> dst_tls_max_version=%ssl::<received_supported_version
> src_tls_cipher=%ssl::>negotiated_cipher
> dst_tls_cipher=%ssl::<negotiated_cipher ssl_bump=%<bs
> ssl_bump_mode=%ssl::bump_mode ssl_sni=%ssl::>sni
> src_cert_subject="%ssl::>cert_subject"
> src_cert_issuer="%ssl::>cert_issuer"
> dst_cert_subject="%ssl::<cert_subject"
> dst_cert_issuer="%ssl::<cert_issuer" cert_errors="%ssl::<cert_errors"
> *** error_page_presented=%err_code err_detail="%err_detail"
> rule_id=%{ruleid}note rule_type=%{ruletype}note
> XFF=%{X-Forwarded-For}>h dst_app=%{dst_app}note
>
>
>
> This creates the two logs at the end of this message, What I am
> wondering is:
>
>  1. Why aren't all the request headers (look between * ** *REQUEST
>     HEADERS and *** RESPONSE HEADERSin each log) seen in the first log
>     present in the second log
>  2. I'm assuming since squid is then making the request in the second
>     log, it leaves the items in Flow0 (client ?load balancer) empty but
>     does retain the data for flow1 (load-balancer-> squid)and flow2
>     (squid -> destination). Even the XFF is not passed. It there anyway
>     to included retain this data?




> Which generates these two logs when doing SSL intercept
> 
> Log 1-----
> 
> 2021-03-31T12:22:08.402609+00:00 squid1 --->
> local_time="[31/Mar/2021:08:22:08 -0400]" squid_service=explicit
> squid_status=NONE squid_hierarchy_status=HIER_DIRECT **
> haproxy_id=73834348 | **Flow0** src_ip=10.11.63.205 src_port=55624
> haproxy_ingress_ip=192.16.8.1.33 haproxy_ingress_port=3128 | ** Flow1**
> haproxy_egress_ip=192.16.8.1.39 haproxy_egress_port=6079
> squid_ingress_ip=192.16.8.1.36 squid_ingress_port=3128 | ** Flow2*
> squid_egress_ip=192.16.8.1.40 squid_egress_port=55984
> dst_ip=10.51.129.182 dst_host=myhost.foo.com dst_port=443
> ident_username=- username=- request_method=CONNECT request="CONNECT
> myhost.foo.com:443 HTTP/1.1" status_code_from_server=200
> status_code_to_client=- referer="-" user_agent="git/2.7.4"
> protocol_version=1.1 ** dns_response_time=- response_time=174
> mime_type=- *XFER* total_request_size=763 total_reply_size=0 **
> src_zone=CoreLab - method_category=Safe - - ** REQUEST HEADERS
> User-Agent=git/2.7.4 HDR_Proxy-Connection=Keep-Alive
> HDR_X-Client-Environment=SecLab HDR_X-Client-Environment=Corporate
> HDR_X-Client-IP=10.11.63.205 HDR_X-Proxy-Channel=3128
> HDR_X-Haproxy-Role=Squid HDR_X-Correlation-ID=73834348
> ?HDR_X-Client-Egress-Ip=10.11.63.205 HDR_X-Client-Egress-Port=55624
> HDR_X-Haproxy-Ingress-Ip=192.16.8.1.33 HDR_X-Haproxy-Ingress-Port=3128
> HDR_X-Haproxy-Egress-Ip="" HDR_X-Haproxy-Egress-Port=""
> HDR_X-Server-Ingress-Ip="" HDR_X-Server-Ingress-Port=""
> HDR_X-Server-Queue=0 HDR_X-App-Node=%3CNOSRV%3E HDR_X-SSL-Cipher=""
> HDR_X-SSL-Version="" HDR_X-Request-Id=73834348
> HDR_X-Forwarded-For=10.11.63.205 HDR_Connection=close
> HDR_Host=myhost.foo.com:443 HDR_ *** RESPONSE HEADERS - ***
> tag_returned=- tag_string="-" previous_hop_mac=00:50:56:b8:03:73
> peer_response_time=- total_response_time=98 *SSL*
> src_ssl_negotiated_version=- dst_ssl_negotiated_version=TLS/1.2
> src_tls_hello_version=TLS/1.0 dst_tls_hello_version=TLS/1.2
> src_tls_max_version=TLS/1.2 dst_tls_max_version=TLS/1.2 src_tls_cipher=-
> dst_tls_cipher=ECDHE-RSA-AES256-GCM-SHA384 ssl_bump=- ssl_bump_mode=bump
> ssl_sni=myhost.foo.com src_cert_subject="-" src_cert_issuer="-"
> dst_cert_subject="/C=US/postalCode=12345/ST=California/L=Sunnyvale/street=123
> Any Street/O=Demo, Inc./OU=None/CN=foo.com"
> dst_cert_issuer="/C=GB/ST=Greater Manchester/L=Salford/O=Sectigo
> Limited/CN=Sectigo RSA Organization Validation Secure Server CA"
> cert_errors="-" *** error_page_presented=- err_detail="-"
> rule_id=Explicit-45-Rule1.conf_2 rule_type=ALLOW XFF="10.11.63.205"
> squid_dst_app=MyApp
> 
> ?
> 
> Log 2----
> 
> 2021-03-31T12:22:08.495914+00:00 squid1 --->
> local_time="[31/Mar/2021:08:22:08 -0400]" squid_service=explicit
> squid_status=TCP_MISS squid_hierarchy_status=HIER_DIRECT ** haproxy_id=-
> | **Flow0** src_ip=- src_port=- haproxy_ingress_ip=-
> haproxy_ingress_port=- | ** Flow1** haproxy_egress_ip=192.16.8.1.39
> haproxy_egress_port=6079 squid_ingress_ip=192.16.8.1.36
> squid_ingress_port=3128 | ** Flow2** squid_egress_ip=192.16.8.1.40
> squid_egress_port=55984 dst_ip=10.51.129.182 dst_host=myhost.foo.com
> dst_port=443 ident_username=- username=- request_method=GET request="GET
> https://myhost.foo.com/test.js HTTP/1.1" status_code_from_server=401
> status_code_to_client=401 referer="-" user_agent="git/2.7.4"
> protocol_version=1.1 ** dns_response_time=- response_time=33 mime_type=-
> *XFER* total_request_size=231 total_reply_size=434 ** src_zone=CoreLab -
> method_category=Safe - - ** REQUEST HEADERS User-Agent=git/2.7.4
> HDR_Accept=*/* HDR_Accept-Encoding=gzip HDR_Accept-Language=en-US,
> *;q=0.9 HDR_Pragma=no-cache HDR_Host=myhost.foo.com HDR_ *** RESPONSE
> HEADERS HTTP/1.1 401 Unauthorized HDR_Date=Wed, 31 Mar 2021 12:22:07 GMT
> HDR_%0D *** tag_returned=- tag_string="-"
> previous_hop_mac=00:50:56:b8:03:73 peer_response_time=32
> total_response_time=33 *SSL* src_ssl_negotiated_version=TLS/1.2
> dst_ssl_negotiated_version=TLS/1.2 src_tls_hello_version=TLS/1.0
> dst_tls_hello_version=TLS/1.2 src_tls_max_version=TLS/1.2
> dst_tls_max_version=TLS/1.2 src_tls_cipher=ECDHE-RSA-AES256-GCM-SHA384
> dst_tls_cipher=ECDHE-RSA-AES256-GCM-SHA384 ssl_bump=0 ssl_bump_mode=bump
> ssl_sni=myhost.foo.com src_cert_subject="-" src_cert_issuer="-"
> dst_cert_subject="/C=US/postalCode=12345/ST=California/L=Sunnyvale/street=123
> Any Street/O=Demo, Inc./OU=None/CN=foo.com"
> dst_cert_issuer="/C=GB/ST=Greater Manchester/L=Salford/O=Sectigo
> Limited/CN=Sectigo RSA Organization Validation Secure Server CA"
> cert_errors="-" *** error_page_presented=- err_detail="-"
> rule_id=Explicit-45-Rule1.conf_2 rule_type=ALLOW XFF="-" squid_dst_app=MyApp
> 
> ?
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid-user at tlinx.org  Mon Apr  5 08:55:22 2021
From: squid-user at tlinx.org (L A Walsh)
Date: Mon, 05 Apr 2021 01:55:22 -0700
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <0e9ac100-7bb7-1aa5-f980-56ee095625ec@ckta.by>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
 <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
 <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by> <606A2361.3070209@tlinx.org>
 <0e9ac100-7bb7-1aa5-f980-56ee095625ec@ckta.by>
Message-ID: <606AD07A.1090706@tlinx.org>

On 2021/04/04 20:07, Majed Zouhairy wrote:
> you are right, it's not running..
> 
>  >cert-gen process needed to have
> it's database zeroed
> 
> 
> how to do that?
---
	Well that was *my* issue of why it wasn't running -- I
had to explore the logs and try to run the demon interactively
to direct its error messages to the console -- I had to lookup
how to do that in the man pages, and the products "--help" switch
among other things.  

At this point you just know it isn't running, but you don't know
its the same problem as I had -- if it is, the error message 
that tells you that such is the problem also tells you "how to do that"
(i.e. how to fix it).  But first you need to look into why it is
failing.

BTW, please 'Cc' the list so someone else who might better know
about a problem like yours can chime in.  I only related a similar
failure case causing a symptom like that -- but there are lots
of ways to not have squid launch right.

Try building squid from the suse rpm, if you haven't already -- then 
you can try adding in a specific squid-tar-version that can better 
suit you than the suse version, if that's your intent...

Squid is of moderate difficulty to build -- be sure you can do so from
suse's source rpm's, as they'll work for your system, then change things
starting from there...


> 
> 4.04.21 23:36, L A Walsh ?????:
>> On 2021/04/03 22:09, Majed Zouhairy wrote:
>>> the error is:
>>>
>>> ??????-?????? ???????????? ????????? ??????????
>>>
>>> translation: the proxy-server is refusing to accept connections..
>> That most commonly is what I see when squid didn't start, (so it
>> refuses to accept connections).
>>
>> Are you sure it started?  Look in the logs for any errors?
>>
>> I've had problems there where the cert-gen process needed to have
>> it's database zeroed (among other non-starting issues), so
>> at boot, squid look liked it ran, but immediately exited when
>> one of its helpers kept dying...
>>
>>



From m_zouhairy at ckta.by  Mon Apr  5 10:15:52 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 5 Apr 2021 13:15:52 +0300
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
Message-ID: <4f5b2d70-7241-d30c-133b-bb5170dcdb9d@ckta.by>

i solved the problem..

since this was in the squid status:
FATAL: The /usr/libexec/squid/security_file_certgen -s 
/var/cache/squid/ssl_db -M 4MB helpers

and i was creating a dirctory in /var/lib/squid/ssl_db

so instead, i ran:

sudo /usr/libexec/squid/security_file_certgen -c -s 
/var/cache/squid/ssl_db -M 8MB

restarted squid and now it works!

On 4/2/21 2:02 PM, Amos Jeffries wrote:
> On 1/04/21 11:41 pm, Majed Zouhairy wrote:
>>
>> to enable ssl bumping.
>>
>> specifically those commands:
>>
>> /usr/share/ssl/misc/CA.pl -newca
>> /usr/share/ssl/misc/CA.pl -newreq
>> /usr/share/ssl/misc/CA.pl -sign
>> openssl x509 -in newcert.pem -outform DER -out squidTrusted.der
> 
> 
>> sudo squid -z
>>
>> asks for certificate password
>> then
>>
>> Enter PEM pass phrase:
>> 2021/04/01 13:17:03| Created PID file (/run/squid.pid)
>> zouhairy at proxy:~> 2021/04/01 13:17:03 kid1| WARNING: BCP 177 
>> violation. Detected non-functional IPv6 loopback.
>> Enter PEM pass phrase:
>> 2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate 
>> configured for HTTP_port 0.0.0.0:8080
> 
> That says there is no CA certificate found in the file configured for 
> that ports tls-cert= option. Squid requires a signing (CA) certificate 
> and its private key in order to perform SSL-Bump.
> 
> With "squid -k parse" Squid should tell you what it is loading from that 
> file.
> 
> 
>>
>> squid conf:
>>
> ...
>>
>> http_port 8080 ssl-bump generate-host-certificates=on 
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/newcert.pem 
>> key=/etc/squid/certs/newkey.pem capath=/home/zouhairy/demoCA
>>
> 
>>
>> ssl_bump peek all
>> ssl_bump splice all
>>
>> sslproxy_cert_error allow all
>>
> 
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From moberger at metanetworks.com  Tue Apr  6 08:14:32 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Tue, 6 Apr 2021 11:14:32 +0300
Subject: [squid-users] Squid 5.0.4 crash
In-Reply-To: <3b9b37f5-9438-218c-365e-7c0a88735b3e@treenet.co.nz>
References: <CAGSk-43mYbsGkOnL_a6Ga2UpKQ0cxha+hf9o8pAZhhPeK40ZeA@mail.gmail.com>
 <3b9b37f5-9438-218c-365e-7c0a88735b3e@treenet.co.nz>
Message-ID: <CAGSk-411hYUSgDLV31BGiXs+MytXCM+n9RDcndaY8X1EDDSV4w@mail.gmail.com>

Hi

I would like to share also that we haven't encountered this error before,
also when using 5.0.4.
We see that this error is in correlation with an error
from networkd-dispatcher:

> ERROR:Unknown interface index 37 seen even after reload


We looked a month back in our logs for this error from
networkd-dispatcher and we found none.

The changes we made before seeing the error for the first time were:

   1. Add a new ICAP server to RESPMOD (we already had two on REQMOD and
   one on RESPMOD).
   2. Start using the ICAP header X-Next-Services
   3. Start using the adaptation_masterx_shared_names conf.

Maybe you can think of something related to above that might caused it?

Thanks,
Moti

On Sun, Apr 4, 2021 at 4:14 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 4/04/21 11:44 pm, Moti Berger wrote:
> > Hi
> >
> > I noticed Squid sporadically crashes with the following error (taken
> > from cache.log):
> >
> >         2021/04/01 21:58:03| FATAL: check failed:
> >         !request->pinnedConnection()
> >
> >              exception location: FwdState.cc(1055) connectStart
> >
> >              current master transaction: master4104
> >
>
> This is <https://bugs.squid-cache.org/show_bug.cgi?id=5090>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210406/07aa2726/attachment.htm>

From loleary at uic.edu  Thu Apr  8 19:11:26 2021
From: loleary at uic.edu (Elliott Blake, Lisa Marie)
Date: Thu, 8 Apr 2021 19:11:26 +0000
Subject: [squid-users] Can't get squid with whitelist text file to work
 TCP_DENIED/403
Message-ID: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>

I am trying to get squid to work with a text file for a whitelist.  I get TCP_DENIED/403 on every url I try.  I am using curl to test.
acl whitelist dstdomain "/etc/squid/whitelist.txt"
curl -x https://libaux-prod.lib.uic.edu:3128 -I https://arl.org
HTTP/1.1 403 Forbidden
Server: squid/3.5.20
Mime-Version: 1.0
Date: Wed, 07 Apr 2021 17:38:58 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3521
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from libaux-prod.lib.uic.edu
X-Cache-Lookup: NONE from libaux-prod.lib.uic.edu:3128
Via: 1.1 libaux-prod.lib.uic.edu (squid/3.5.20)
Connection: keep-alive
curl: (56) Received HTTP code 403 from proxy after CONNECT

However, if I change my squid.conf to just the url it works.
acl whitelist dstdomain .arl.org
curl -x https://libaux-prod.lib.uic.edu:3128 -I https://arl.org
HTTP/1.1 200 Connection established
HTTP/1.1 301 Moved Permanently
Server: nginx
Date: Wed, 07 Apr 2021 17:40:31 GMT
Content-Type: text/html
Content-Length: 178
Connection: keep-alive
Keep-Alive: timeout=20
Location: https://www.arl.org/
Expires: Wed, 07 Apr 2021 18:40:31 GMT
Cache-Control: max-age=3600

I am running a centos 7 os with squid version 3.5.20, which is the most recent yum version.
This is driving me crazy.  I have tried debugging in squid and cannot find the answer.  I have tried changing the squid.conf file.  I always restart squid after I change the squid.conf file.
Any help would be appreciated.

My Squid.conf file:
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl Safe_ports port 591         # filemaker
acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager

acl whitelist dstdomain "/etc/squid/whitelist.txt"
#acl whitelist dstdomain .arl.org
http_access allow whitelist
#http_access allow CONNECT whitelist

http_access deny !whitelist

http_access allow localnet
http_access allow localhost

http_access deny all

# Squid normally listens to port 3128
http_port 3128

# port 1338 is for Front Desk Machines
http_port 1338

coredump_dir /var/spool/squid

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

Beginning of whitelist.txt
#A Page
.aacrjournals.org
.aai.org
.aaiddjournals.org
.aap.org
.aappublications.orga
.accessanesthesiology.com
.anthropology.org.uk
.archivegrid.org
.arl.org
.arlstatistics.org
.artstor.org

Thank you,
Lisa Blake


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210408/893f135e/attachment.htm>

From ebedsat at gmail.com  Thu Apr  8 23:12:21 2021
From: ebedsat at gmail.com (Ebed)
Date: Fri, 9 Apr 2021 06:12:21 +0700
Subject: [squid-users] How to fix none/200 in access.log
Message-ID: <CAMcTSO31USEgLJq+T31FuHGAEeOZf2J7n6xDcyVjMcdSjcYh4g@mail.gmail.com>

Hi,

I have one apk in my cell phone, which produce 'none/200' instead of
tcp_tunnel in access.log, what i should do to fix this?

Here is access.log,

1617922121.785 94 192.168.20. NONE/200 0 CONNECT 182.0.188.12:443 - ORIGIN


Here is my squid.conf,

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8 # RFC 1918 local private network (LAN)
acl localnet src 100.64.0.0/10 # RFC 6598 shared address space (CGN)
acl localnet src 169.254.0.0/16 # RFC 3927 link-local (directly plugged)
machines
acl localnet src 172.16.0.0/12 # RFC 1918 local private network (LAN)
acl localnet src 192.168.0.0/16 # RFC 1918 local private network (LAN)
acl localnet src fc00::/7        # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

acl adblock urlpath_regex "/etc/squid/adblock.acl"

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

deny_info Transparent.gif adblock
http_access deny adblock

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128
# listen 3129, 3130 for intercepting
http_port 3129 intercept
https_port 3130 intercept ssl-bump \
cert=/etc/squid/cert/myCA.pem \
generate-host-certificates=on \
dynamic_cert_mem_cache_size=4MB

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir aufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

forwarded_for delete
via off
request_header_access Cache-Control deny all
#always_direct allow all

sslcrtd_program /usr/lib64/squid/security_file_certgen -s
/var/spool/squid/ssl_db -M 4MB

acl step1 at_step SslBump1
acl step2 at_step SslBump2
#acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump peek step2
ssl_bump bump all

sslcrtd_children 5
ssl_bump server-first all

sslproxy_cert_error allow all
tls_outgoing_options options=ALL
tls_outgoing_options flags=DONT_VERIFY_PEER

tls_outgoing_options
cafile=/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt
#tls_outgoing_options
cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

#debug_options ALL,2 28,9
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210409/2b8f6ee7/attachment.htm>

From rousskov at measurement-factory.com  Fri Apr  9 03:20:37 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 8 Apr 2021 23:20:37 -0400
Subject: [squid-users] How to fix none/200 in access.log
In-Reply-To: <CAMcTSO31USEgLJq+T31FuHGAEeOZf2J7n6xDcyVjMcdSjcYh4g@mail.gmail.com>
References: <CAMcTSO31USEgLJq+T31FuHGAEeOZf2J7n6xDcyVjMcdSjcYh4g@mail.gmail.com>
Message-ID: <10dc0bbf-f416-bbb4-12b9-646c549d4e70@measurement-factory.com>

On 4/8/21 7:12 PM, Ebed wrote:
> ssl_bump peek step1
> ssl_bump peek step2
> ssl_bump bump all

I cannot answer your original question, but, just FYI: The above
configuration is equivalent to:

  ssl_bump peek all
  ssl_bump splice all

and, as far as traffic on the wire is concerned, it is pretty much
equivalent to having no SslBump at all.

You may want to review how SslBump works if the above conclusion is a
surprise for you: https://wiki.squid-cache.org/Features/SslPeekAndSplice


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Apr  9 14:51:55 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 9 Apr 2021 10:51:55 -0400
Subject: [squid-users] Can't get squid with whitelist text file to work
 TCP_DENIED/403
In-Reply-To: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>
References: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>
Message-ID: <740bc65d-5bea-6526-b25b-c285b23efd05@measurement-factory.com>

On 4/8/21 3:11 PM, Elliott Blake, Lisa Marie wrote:
> I am trying to get squid to work with a text file for a whitelist.? I
> get TCP_DENIED/403 on every url I try.? I am using curl to test.

> curl -x https://libaux-prod.lib.uic.edu:3128 -I https://arl.org

Is that the exact curl command you are using or a typo? The above
command tells curl to use an HTTPS proxy (https://libaux...) and your
squid.conf does not have an https_port so something does not add up.
Perhaps your curl version is as old and buggy as your Squid version and
it just ignores the "s" in "-x https", but I would remove it anyway.


> Server: squid/3.5.20

Could be a bug in that unsupported version, of course. If you share a
link to an debug_options ALL,9 cache.log with a problematic transaction,
somebody may be able to triage this further.

https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

Alex.


> Mime-Version: 1.0
> 
> Date: Wed, 07 Apr 2021 17:38:58 GMT
> 
> Content-Type: text/html;charset=utf-8
> 
> Content-Length: 3521
> 
> X-Squid-Error: ERR_ACCESS_DENIED 0
> 
> Vary: Accept-Language
> 
> Content-Language: en
> 
> X-Cache: MISS from libaux-prod.lib.uic.edu
> 
> X-Cache-Lookup: NONE from libaux-prod.lib.uic.edu:3128
> 
> Via: 1.1 libaux-prod.lib.uic.edu (squid/3.5.20)
> 
> Connection: keep-alive
> 
> curl: (56) Received HTTP code 403 from proxy after CONNECT
> 
> ?
> 
> However, if I change my squid.conf to just the url it works.
> 
> acl whitelist dstdomain .arl.org
> 
> *curl -x https://libaux-prod.lib.uic.edu:3128
> <https://libaux-prod.lib.uic.edu:3128> -I https://arl.org
> <https://arl.org> *
> 
> HTTP/1.1 200 Connection established
> 
> HTTP/1.1 301 Moved Permanently
> 
> Server: nginx
> 
> Date: Wed, 07 Apr 2021 17:40:31 GMT
> 
> Content-Type: text/html
> 
> Content-Length: 178
> 
> Connection: keep-alive
> 
> Keep-Alive: timeout=20
> 
> Location: https://www.arl.org/ <https://www.arl.org/>
> 
> Expires: Wed, 07 Apr 2021 18:40:31 GMT
> 
> Cache-Control: max-age=3600
> 
> ?
> 
> I am running a centos 7 os with squid version 3.5.20, which is the most
> recent yum version.
> 
> This is driving me crazy.? I have tried debugging in squid and cannot
> find the answer.? I have tried changing the squid.conf file.? I always
> restart squid after I change the squid.conf file. ?
> 
> Any help would be appreciated.
> 
> ?
> 
> My Squid.conf file:
> 
> acl localnet src 10.0.0.0/8???? # RFC1918 possible internal network
> 
> acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
> 
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> 
> acl localnet src fc00::/7?????? # RFC 4193 local private network range
> 
> acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged)
> machines
> 
> ?
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80????????? # http
> 
> acl Safe_ports port 443???????? # https
> 
> acl Safe_ports port 591???????? # filemaker
> 
> acl CONNECT method CONNECT
> 
> ?
> 
> http_access deny !Safe_ports
> 
> ?
> 
> http_access deny CONNECT !SSL_ports
> 
> ?
> 
> http_access allow localhost manager
> 
> http_access deny manager
> 
> ?
> 
> acl whitelist dstdomain "/etc/squid/whitelist.txt"
> 
> #acl whitelist dstdomain .arl.org
> 
> http_access allow whitelist
> 
> #http_access allow CONNECT whitelist
> 
> ?
> 
> http_access deny !whitelist
> 
> ?
> 
> http_access allow localnet
> 
> http_access allow localhost
> 
> ?
> 
> http_access deny all
> 
> ?
> 
> # Squid normally listens to port 3128
> 
> http_port 3128
> 
> ?
> 
> # port 1338 is for Front Desk Machines
> 
> http_port 1338
> 
> ?
> 
> coredump_dir /var/spool/squid
> 
> ?
> 
> refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
> 
> refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
> 
> refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
> 
> refresh_pattern .?????????????? 0?????? 20%???? 4320
> 
> ?
> 
> Beginning of whitelist.txt
> 
> #A Page
> 
> .aacrjournals.org
> 
> .aai.org
> 
> .aaiddjournals.org
> 
> .aap.org
> 
> .aappublications.orga
> 
> .accessanesthesiology.com
> 
> .anthropology.org.uk
> 
> .archivegrid.org
> 
> .arl.org
> 
> .arlstatistics.org
> 
> .artstor.org
> 
> ?
> 
> Thank you,
> 
> Lisa Blake
> 
> ?
> 
> ?
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From klaus_brandl at genua.de  Fri Apr  9 15:09:22 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Fri, 9 Apr 2021 15:09:22 +0000
Subject: [squid-users] icap adaptation chains with adaptation sets
In-Reply-To: <b6667fc7-4a4b-f23b-71ab-fd98f52b348f@measurement-factory.com>
References: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>
 <b6667fc7-4a4b-f23b-71ab-fd98f52b348f@measurement-factory.com>
Message-ID: <900437def2aa040c7556379ee11e1d1f951beccf.camel@genua.de>

Thanks!

An other, maybe easier way were to support more then one
adaptation_access lines, so we could do something like this:

adaptation_access pool1 allow all
adaptation_access pool2 allow all

But this is currently not working, only the first line takes effect,
the second is ignored. The config reference says here:

It is currently not possible to apply more than one adaptation
service at the same vectoring point to the same HTTP transaction.

What a shame :)

Klaus

Am Sonntag, den 04.04.2021, 23:29 -0400 schrieb Alex Rousskov:
> On 3/31/21 10:02 AM, Klaus Brandl wrote:
> 
> > is there a way to use more adaptation sets(for redundancy) combined
> > in
> > an adaptation chain?
> 
> Squid only supports chains of services and sets of services. There is
> currently no support for nesting (e.g., chains of sets). Such support
> would be generally useful IMO.
> 
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
> Alex.
> 
> 
> > What we need is something like this:
> > 
> > icap_service b1 reqmod_precache ...
> > icap_service b2 reqmod_precache ...
> > icap_service b3 reqmod_precache ...
> > icap_service b4 reqmod_precache ...
> > 
> > icap_service m1 reqmod_precache ...
> > icap_service m2 reqmod_precache ...
> > icap_service m3 reqmod_precache ...
> > icap_service m4 reqmod_precache ...
> > 
> > #blacklist
> > adaptation_service_set pool1 b1 b2 b3 b4
> > #malware scanner
> > adaptation_service_set pool2 m1 m2 m3 m4
> > 
> > adaptation_service_chain checks pool1 pool2
> > 
> > adaptation_access checks allow all
> > 
> > Regards
> > 
> > Klaus

From rousskov at measurement-factory.com  Sat Apr 10 00:42:46 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 9 Apr 2021 20:42:46 -0400
Subject: [squid-users] Can't get squid with whitelist text file to work
 TCP_DENIED/403
In-Reply-To: <CH2PR13MB38159AA785563CEF25F40B25BF739@CH2PR13MB3815.namprd13.prod.outlook.com>
References: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>
 <740bc65d-5bea-6526-b25b-c285b23efd05@measurement-factory.com>
 <CH2PR13MB38159AA785563CEF25F40B25BF739@CH2PR13MB3815.namprd13.prod.outlook.com>
Message-ID: <7e7ef51c-b4df-5f31-da6b-8903171930b1@measurement-factory.com>

On 4/9/21 4:41 PM, Elliott Blake, Lisa Marie wrote:
> I realized that the whitelist is a symbolic link

Hi Lisa,

    Glad you figured it out! IMO, it is a Squid bug that Squid starts
with broken symbolic links:

> 2021/04/09 20:34:52| ERROR: Can not open file /tmp/link for reading
> 2021/04/09 20:34:52| Warning: empty ACL: acl testLink dstdomain "/tmp/link"
> 2021/04/09 20:34:52| Accepting HTTP Socket connections

The above ERROR should be a fatal (by default).

In fact, I would make the above Warning a fatal configuration error as
well, with a squid.conf option to explicitly allow for empty (hopefully
never matching) ACLs.

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Friday, April 9, 2021 9:52 AM
> To: squid-users at lists.squid-cache.org
> Cc: Elliott Blake, Lisa Marie <loleary at uic.edu>
> Subject: Re: [squid-users] Can't get squid with whitelist text file to work TCP_DENIED/403
> 
> On 4/8/21 3:11 PM, Elliott Blake, Lisa Marie wrote:
>> I am trying to get squid to work with a text file for a whitelist.? I 
>> get TCP_DENIED/403 on every url I try.? I am using curl to test.
> 
>> curl -x https://libaux-prod.lib.uic.edu:3128/ -I 
>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Farl.
>> org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108d8
>> fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C6375357678081347
>> 99%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI
>> 6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=Adwdl4Cdzqutr6%2FmXhn7Dl19qtD
>> rG8%2FZG5G%2BYdCC0cA%3D&amp;reserved=0
> 
> Is that the exact curl command you are using or a typo? The above command tells curl to use an HTTPS proxy (https://libaux...) and your squid.conf does not have an https_port so something does not add up.
> Perhaps your curl version is as old and buggy as your Squid version and it just ignores the "s" in "-x https", but I would remove it anyway.
> 
> 
>> Server: squid/3.5.20
> 
> Could be a bug in that unsupported version, of course. If you share a link to an debug_options ALL,9 cache.log with a problematic transaction, somebody may be able to triage this further.
> 
> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwiki.squid-cache.org%2FSquidFaq%2FBugReporting%23Debugging_a_single_transaction&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108d8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C637535767808134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=VAmMhhAY6jdzY%2FK0tMsxbbEcS%2BB6dihOG5BWBJ%2BFAvw%3D&amp;reserved=0
> 
> Alex.
> 
> 
>> Mime-Version: 1.0
>>
>> Date: Wed, 07 Apr 2021 17:38:58 GMT
>>
>> Content-Type: text/html;charset=utf-8
>>
>> Content-Length: 3521
>>
>> X-Squid-Error: ERR_ACCESS_DENIED 0
>>
>> Vary: Accept-Language
>>
>> Content-Language: en
>>
>> X-Cache: MISS from libaux-prod.lib.uic.edu
>>
>> X-Cache-Lookup: NONE from libaux-prod.lib.uic.edu:3128
>>
>> Via: 1.1 libaux-prod.lib.uic.edu (squid/3.5.20)
>>
>> Connection: keep-alive
>>
>> curl: (56) Received HTTP code 403 from proxy after CONNECT
>>
>> ?
>>
>> However, if I change my squid.conf to just the url it works.
>>
>> acl whitelist dstdomain .arl.org
>>
>> *curl -x https://libaux-prod.lib.uic.edu:3128/
>> <https://libaux-prod.lib.uic.edu:3128/> -I 
>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Farl.
>> org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108d8
>> fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C6375357678081347
>> 99%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI
>> 6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=Adwdl4Cdzqutr6%2FmXhn7Dl19qtD
>> rG8%2FZG5G%2BYdCC0cA%3D&amp;reserved=0
>> <https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Farl
>> .org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108d
>> 8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C637535767808134
>> 799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTi
>> I6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=Adwdl4Cdzqutr6%2FmXhn7Dl19qt
>> DrG8%2FZG5G%2BYdCC0cA%3D&amp;reserved=0> *
>>
>> HTTP/1.1 200 Connection established
>>
>> HTTP/1.1 301 Moved Permanently
>>
>> Server: nginx
>>
>> Date: Wed, 07 Apr 2021 17:40:31 GMT
>>
>> Content-Type: text/html
>>
>> Content-Length: 178
>>
>> Connection: keep-alive
>>
>> Keep-Alive: timeout=20
>>
>> Location: 
>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
>> arl.org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e91
>> 08d8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C637535767808
>> 134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJ
>> BTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=obQUl59%2FNceepVKW4YMlCSF
>> rOobHRl8LtnVZaAV23ks%3D&amp;reserved=0 
>> <https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww
>> .arl.org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9
>> 108d8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C63753576780
>> 8134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLC
>> JBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=obQUl59%2FNceepVKW4YMlCS
>> FrOobHRl8LtnVZaAV23ks%3D&amp;reserved=0>
>>
>> Expires: Wed, 07 Apr 2021 18:40:31 GMT
>>
>> Cache-Control: max-age=3600
>>
>> ?
>>
>> I am running a centos 7 os with squid version 3.5.20, which is the 
>> most recent yum version.
>>
>> This is driving me crazy.? I have tried debugging in squid and cannot 
>> find the answer.? I have tried changing the squid.conf file.? I always 
>> restart squid after I change the squid.conf file.
>>
>> Any help would be appreciated.
>>
>> ?
>>
>> My Squid.conf file:
>>
>> acl localnet src 10.0.0.0/8???? # RFC1918 possible internal network
>>
>> acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
>>
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>
>> acl localnet src fc00::/7?????? # RFC 4193 local private network range
>>
>> acl localnet src fe80::/10????? # RFC 4291 link-local (directly 
>> plugged) machines
>>
>> ?
>>
>> acl SSL_ports port 443
>>
>> acl Safe_ports port 80????????? # http
>>
>> acl Safe_ports port 443???????? # https
>>
>> acl Safe_ports port 591???????? # filemaker
>>
>> acl CONNECT method CONNECT
>>
>> ?
>>
>> http_access deny !Safe_ports
>>
>> ?
>>
>> http_access deny CONNECT !SSL_ports
>>
>> ?
>>
>> http_access allow localhost manager
>>
>> http_access deny manager
>>
>> ?
>>
>> acl whitelist dstdomain "/etc/squid/whitelist.txt"
>>
>> #acl whitelist dstdomain .arl.org
>>
>> http_access allow whitelist
>>
>> #http_access allow CONNECT whitelist
>>
>> ?
>>
>> http_access deny !whitelist
>>
>> ?
>>
>> http_access allow localnet
>>
>> http_access allow localhost
>>
>> ?
>>
>> http_access deny all
>>
>> ?
>>
>> # Squid normally listens to port 3128
>>
>> http_port 3128
>>
>> ?
>>
>> # port 1338 is for Front Desk Machines
>>
>> http_port 1338
>>
>> ?
>>
>> coredump_dir /var/spool/squid
>>
>> ?
>>
>> refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
>>
>> refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
>>
>> refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
>>
>> refresh_pattern .?????????????? 0?????? 20%???? 4320
>>
>> ?
>>
>> Beginning of whitelist.txt
>>
>> #A Page
>>
>> .aacrjournals.org
>>
>> .aai.org
>>
>> .aaiddjournals.org
>>
>> .aap.org
>>
>> .aappublications.orga
>>
>> .accessanesthesiology.com
>>
>> .anthropology.org.uk
>>
>> .archivegrid.org
>>
>> .arl.org
>>
>> .arlstatistics.org
>>
>> .artstor.org
>>
>> ?
>>
>> Thank you,
>>
>> Lisa Blake
>>
>> ?
>>
>> ?
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://nam04.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists
>> .squid-cache.org%2Flistinfo%2Fsquid-users&amp;data=04%7C01%7Cloleary%4
>> 0uic.edu%7Cd7cfe4dfe984430c6e9108d8fb6706c8%7Ce202cd477a564baa99e3e3b7
>> 1a7c77dd%7C0%7C0%7C637535767808134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoi
>> MC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;
>> sdata=xR28PqxDa3d3aQhOqB9b142qoY2x8rSNTZOGTACIMLg%3D&amp;reserved=0
>>



From koshikmoshik at gmail.com  Sat Apr 10 21:03:38 2021
From: koshikmoshik at gmail.com (koshik moshik)
Date: Sat, 10 Apr 2021 23:03:38 +0200
Subject: [squid-users] Cache Peers and traffic handling
Message-ID: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>

Hello,


I am trying to run a Squid proxy Server witth about 5000 cache peers. I am
running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16.


Could you tell me what else is needed / not needed in my squid.config? I am
encountering a high CPU usage and would like to create a very efficient
proxy server.


Down below you can find my squid.config(I deleted the other cache_peer
lines):

-----------

http_port 3128

dns_v4_first on

acl SSL_ports port 1-65535

acl Safe_ports port 1-65535

acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/.htpasswd

auth_param basic children 5

auth_param basic realm Squid Basic Authentication

auth_param basic credentialsttl 5 hours

acl password proxy_auth REQUIRED

http_access allow password

#http_access deny all

cache allow all

never_direct allow all

ident_access deny all





cache_mem 1 GB

maximum_object_size_in_memory 16 MB





# Leave coredumps in the first cache dir

coredump_dir /var/spool/squid


#Rules to anonymize http headers

forwarded_for off

request_header_access Allow allow all

request_header_access Authorization allow all

request_header_access WWW-Authenticate allow all

request_header_access Proxy-Authorization allow all

request_header_access Proxy-Authenticate allow all

request_header_access Cache-Control allow all

request_header_access Content-Encoding allow all

request_header_access Content-Length allow all

request_header_access Content-Type allow all

request_header_access Date allow all

request_header_access Expires allow all

request_header_access Host allow all

request_header_access If-Modified-Since allow all

request_header_access Last-Modified allow all

request_header_access Location allow all

request_header_access Pragma allow all

request_header_access Accept allow all

request_header_access Accept-Charset allow all

request_header_access Accept-Encoding allow all

request_header_access Accept-Language allow all

request_header_access Content-Language allow all

request_header_access Mime-Version allow all

request_header_access Retry-After allow all

request_header_access Title allow all

request_header_access Connection allow all

request_header_access Proxy-Connection allow all

request_header_access User-Agent allow all

request_header_access Cookie allow all

request_header_access All deny all





#

# Add any of your own refresh_pattern entries above these.

#

#refresh_pattern ^ftp:           1440    20%     10080

#refresh_pattern ^gopher:        1440    0%      1440

#refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

#refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880

#refresh_pattern .               0       20%     4320


################################

acl me proxy_auth ye-1

cache_peer my.proxy.com parent 31280 login=user1:password1 no-query name=a1

cache_peer_access a1 allow me

cache_peer_access a1 deny all
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210410/8393358f/attachment.htm>

From rigault.francois at gmail.com  Sun Apr 11 16:46:41 2021
From: rigault.francois at gmail.com (Francois)
Date: Sun, 11 Apr 2021 18:46:41 +0200
Subject: [squid-users] Squid within a network namespace
Message-ID: <CAMc2VtTujVm1H9pq+ctLSw+mjiY8nbSESHqC52W6fNy9cBb0pQ@mail.gmail.com>

Hi Squid
I am running my development tools and VMs in a dedicated network
namespace on my laptop (through Linux "netns"), so they are fully
isolated from the rest of my network. I would like to set-up a proxy
so that if there is a need to connect to the outside, I could set-up
some fine grained ACL to open some very specific HTTP traffic. For
this to work with Squid, there must be a socket opened within the
namespace, while Squid is still running on the default namespace.

This can be achieved without modifying the code by using socat for
example, where a socat running within the namespace sends traffic to a
Unix socket, and another socat outside the namespace, reads from the
Unix socket, and sends the traffic to Squid... it's quite some
plumbing effort, and Squid won't be able to know from which VM the
traffic originates (the X-Forwarded-For is localhost)

Seeing that HAProxy implemented something
(https://github.com/haproxy/haproxy/commit/b3e54fe387c7c1ea750f39d3029672d640c499f9)
so that the process moves into the namespace just for the time of the
socket creation, I came up with a similar change for Squid
(https://github.com/freedge/squid/commit/a778666d8f4760448e29e4a0cc75dcd305b40d02).

As this is a Linux only change, and also the community lived without
it so far, I am sending this mail to see if there is any interest in
this feature, if there was ever any request for it in the past?

Cheers!


From cary.lewis at gmail.com  Sun Apr 11 21:58:09 2021
From: cary.lewis at gmail.com (Cary Lewis)
Date: Sun, 11 Apr 2021 17:58:09 -0400
Subject: [squid-users] Is there a way to bind squid's outbound traffice to a
 specific network interface
Message-ID: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>

I want to be able to bypass a vpn while using a web browser, so I need to
be able to configure squid to always use a specific outbound interface.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210411/4b5a2177/attachment.htm>

From klaus_brandl at genua.de  Mon Apr 12 08:46:25 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Mon, 12 Apr 2021 08:46:25 +0000
Subject: [squid-users] Is there a way to bind squid's outbound traffice
 to a specific network interface
In-Reply-To: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>
References: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>
Message-ID: <ee04804bbe88475929168ddb1708a078d0bc8991.camel@genua.de>

http://www.squid-cache.org/Doc/config/tcp_outgoing_address/

Simply use for any connection:

tcp_outgoing_address 10.1.0.2 all

Regards

Klaus

Am Sonntag, den 11.04.2021, 17:58 -0400 schrieb Cary Lewis:
> I want to be able to bypass a vpn while using a web browser, so I
> need to be able to configure squid to always use a specific outbound
> interface. 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From uhlar at fantomas.sk  Mon Apr 12 09:09:08 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 12 Apr 2021 11:09:08 +0200
Subject: [squid-users] Is there a way to bind squid's outbound traffice
 to a specific network interface
In-Reply-To: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>
References: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>
Message-ID: <20210412090908.GA28534@fantomas.sk>

On 11.04.21 17:58, Cary Lewis wrote:
>I want to be able to bypass a vpn while using a web browser, so I need to
>be able to configure squid to always use a specific outbound interface.

FYI, squid can not decide which interface to use - OS kernel (or, more
precisely, IP stack) does that.

squid can use source IP address and the system can be configured to send
packets from that address via partitular interface.

Thus, in addition to configuring squid's otgoing IP address, you must
configure OS routing table to take source IP Address into account
- normally only destination IP is considered.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
M$ Win's are shit, do not use it !


From roierachamim at gmail.com  Mon Apr 12 09:53:31 2021
From: roierachamim at gmail.com (roie rachamim)
Date: Mon, 12 Apr 2021 12:53:31 +0300
Subject: [squid-users] All Adaptation ICAPs go down at the same time
Message-ID: <CAD=NrcALoEGqctFUpuTZcfkpEbsS=KU4QqXG+LAVd-XtJvfnXg@mail.gmail.com>

Hi,

Our setup includes squid that runs in docker container with several ICAP
servers in additional containers.

>From time to time we see in cache.log the following messages:
2021/04/12 00:22:39| optional ICAP service is down after an options fetch
failure: icap://icap1.proxy:14590/censor [down,!opt]
2021/04/12 00:22:39| optional ICAP service is down after an options fetch
failure: icap://icap2.proxy:1344/request [down,!opt]
2021/04/12 00:22:39| optional ICAP service is down after an options fetch
failure: icap://icap3.proxy:14590/response [down,!opt]

2021/04/12 06:10:45| optional ICAP service is down after an options fetch
failure: icap://icap1.proxy:14590/censor [down,!opt]
2021/04/12 06:10:45| optional ICAP service is down after an options fetch
failure: icap://icap2.proxy:1344/request [down,!opt]
2021/04/12 06:10:45| optional ICAP service is down after an options fetch
failure: icap://icap3.proxy:14590/response [down,!opt]

We're trying to understand why it happens to all ICAPs at once. This
happens in 4.14 and in 5.0.4

Any thoughts about what might cause this ?

Many Thanks,
Roie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210412/3de28aec/attachment.htm>

From rousskov at measurement-factory.com  Mon Apr 12 14:02:18 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Apr 2021 10:02:18 -0400
Subject: [squid-users] All Adaptation ICAPs go down at the same time
In-Reply-To: <CAD=NrcALoEGqctFUpuTZcfkpEbsS=KU4QqXG+LAVd-XtJvfnXg@mail.gmail.com>
References: <CAD=NrcALoEGqctFUpuTZcfkpEbsS=KU4QqXG+LAVd-XtJvfnXg@mail.gmail.com>
Message-ID: <d7c1c4b6-965c-8785-daf7-cfeafebeae02@measurement-factory.com>

On 4/12/21 5:53 AM, roie rachamim wrote:
> Hi,
> 
> Our setup includes squid that runs in docker container with several ICAP
> servers in additional containers.
> 
> From time to time we see in cache.log the following messages:
> 2021/04/12 00:22:39| optional ICAP service is down after an options
> fetch failure: icap://icap1.proxy:14590/censor [down,!opt]

> We're trying to understand why it happens to all ICAPs at once. This
> happens in 4.14 and in 5.0.4
> 
> Any thoughts about what might cause this ?

I can imagine several causes, but I would recommend to find out the
exact reason by enabling debugging (preferably shortly before the next
OPTIONS request) and sharing your (compressed) cache.log with somebody
who can interpret it for you (or this mailing list).

https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction


HTH,

Alex.


From rousskov at measurement-factory.com  Mon Apr 12 14:57:25 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Apr 2021 10:57:25 -0400
Subject: [squid-users] Can't get squid with whitelist text file to work
 TCP_DENIED/403
In-Reply-To: <CH2PR13MB3815A498B7E078875BF73524BF719@CH2PR13MB3815.namprd13.prod.outlook.com>
References: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>
 <740bc65d-5bea-6526-b25b-c285b23efd05@measurement-factory.com>
 <CH2PR13MB38159AA785563CEF25F40B25BF739@CH2PR13MB3815.namprd13.prod.outlook.com>
 <7e7ef51c-b4df-5f31-da6b-8903171930b1@measurement-factory.com>
 <CH2PR13MB3815A498B7E078875BF73524BF719@CH2PR13MB3815.namprd13.prod.outlook.com>
Message-ID: <c036556f-278f-91bf-3403-8861923461a3@measurement-factory.com>

On 4/11/21 12:10 AM, Elliott Blake, Lisa Marie wrote:

> I never got that error.  Wish I had.  It would have made figuring it
> out much easier.

That can be another Squid UX problem (e.g., Squid reported the error but
not where you normally look for Squid errors) OR a sign that there is
something else going on (i.e. something other than a bad link, bad link
permissions, SE Linux policy violation, etc.). Feel free to investigate
further and report, of course, but without more information, we would
not be able to understand what actually went wrong.

Needless to say, any such investigation is unlikely to benefit you at
this point -- you will only be helping future others in your situation.
I am glad you have a working setup now!


Cheers,

Alex.

> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Friday, April 9, 2021 7:43 PM
> To: squid-users at lists.squid-cache.org
> Cc: Elliott Blake, Lisa Marie <loleary at uic.edu>
> Subject: Re: [squid-users] Can't get squid with whitelist text file to work TCP_DENIED/403
> 
> On 4/9/21 4:41 PM, Elliott Blake, Lisa Marie wrote:
>> I realized that the whitelist is a symbolic link
> 
> Hi Lisa,
> 
>     Glad you figured it out! IMO, it is a Squid bug that Squid starts with broken symbolic links:
> 
>> 2021/04/09 20:34:52| ERROR: Can not open file /tmp/link for reading
>> 2021/04/09 20:34:52| Warning: empty ACL: acl testLink dstdomain "/tmp/link"
>> 2021/04/09 20:34:52| Accepting HTTP Socket connections
> 
> The above ERROR should be a fatal (by default).
> 
> In fact, I would make the above Warning a fatal configuration error as well, with a squid.conf option to explicitly allow for empty (hopefully never matching) ACLs.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Friday, April 9, 2021 9:52 AM
>> To: squid-users at lists.squid-cache.org
>> Cc: Elliott Blake, Lisa Marie <loleary at uic.edu>
>> Subject: Re: [squid-users] Can't get squid with whitelist text file to 
>> work TCP_DENIED/403
>>
>> On 4/8/21 3:11 PM, Elliott Blake, Lisa Marie wrote:
>>> I am trying to get squid to work with a text file for a whitelist.? I 
>>> get TCP_DENIED/403 on every url I try.? I am using curl to test.
>>
>>> curl -x https://libaux-prod.lib.uic.edu:3128/ -I 
>>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Farl.
>>> org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108d
>>> 8
>>> fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C637535767808134
>>> 7 
>>> 99%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTi
>>> I 
>>> 6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=Adwdl4Cdzqutr6%2FmXhn7Dl19qt
>>> D
>>> rG8%2FZG5G%2BYdCC0cA%3D&amp;reserved=0
>>
>> Is that the exact curl command you are using or a typo? The above command tells curl to use an HTTPS proxy (https://libaux...) and your squid.conf does not have an https_port so something does not add up.
>> Perhaps your curl version is as old and buggy as your Squid version and it just ignores the "s" in "-x https", but I would remove it anyway.
>>
>>
>>> Server: squid/3.5.20
>>
>> Could be a bug in that unsupported version, of course. If you share a link to an debug_options ALL,9 cache.log with a problematic transaction, somebody may be able to triage this further.
>>
>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwiki
>> .squid-cache.org%2FSquidFaq%2FBugReporting%23Debugging_a_single_transa
>> ction&amp;data=04%7C01%7Cloleary%40uic.edu%7C35e72a5ccd6f4f3e4ff908d8f
>> bb99128%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C63753612172678777
>> 3%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6
>> Ik1haWwiLCJXVCI6Mn0%3D%7C1000&amp;sdata=J0q3V%2F3VqxVLw6C7QBY4eCrs69BH
>> pHhCLOfJErWev90%3D&amp;reserved=0
>>
>> Alex.
>>
>>
>>> Mime-Version: 1.0
>>>
>>> Date: Wed, 07 Apr 2021 17:38:58 GMT
>>>
>>> Content-Type: text/html;charset=utf-8
>>>
>>> Content-Length: 3521
>>>
>>> X-Squid-Error: ERR_ACCESS_DENIED 0
>>>
>>> Vary: Accept-Language
>>>
>>> Content-Language: en
>>>
>>> X-Cache: MISS from libaux-prod.lib.uic.edu
>>>
>>> X-Cache-Lookup: NONE from libaux-prod.lib.uic.edu:3128
>>>
>>> Via: 1.1 libaux-prod.lib.uic.edu (squid/3.5.20)
>>>
>>> Connection: keep-alive
>>>
>>> curl: (56) Received HTTP code 403 from proxy after CONNECT
>>>
>>> ?
>>>
>>> However, if I change my squid.conf to just the url it works.
>>>
>>> acl whitelist dstdomain .arl.org
>>>
>>> *curl -x https://libaux-prod.lib.uic.edu:3128/
>>> <https://libaux-prod.lib.uic.edu:3128/> -I 
>>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Farl.
>>> org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108d
>>> 8
>>> fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C637535767808134
>>> 7 
>>> 99%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTi
>>> I 
>>> 6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=Adwdl4Cdzqutr6%2FmXhn7Dl19qt
>>> D
>>> rG8%2FZG5G%2BYdCC0cA%3D&amp;reserved=0
>>> <https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Far
>>> l 
>>> .org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9108
>>> d
>>> 8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C63753576780813
>>> 4 
>>> 799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
>>> i 
>>> I6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=Adwdl4Cdzqutr6%2FmXhn7Dl19q
>>> t DrG8%2FZG5G%2BYdCC0cA%3D&amp;reserved=0> *
>>>
>>> HTTP/1.1 200 Connection established
>>>
>>> HTTP/1.1 301 Moved Permanently
>>>
>>> Server: nginx
>>>
>>> Date: Wed, 07 Apr 2021 17:40:31 GMT
>>>
>>> Content-Type: text/html
>>>
>>> Content-Length: 178
>>>
>>> Connection: keep-alive
>>>
>>> Keep-Alive: timeout=20
>>>
>>> Location: 
>>> https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
>>> arl.org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e9
>>> 1
>>> 08d8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C63753576780
>>> 8 
>>> 134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLC
>>> J 
>>> BTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=obQUl59%2FNceepVKW4YMlCS
>>> F
>>> rOobHRl8LtnVZaAV23ks%3D&amp;reserved=0
>>> <https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fww
>>> w
>>> .arl.org%2F&amp;data=04%7C01%7Cloleary%40uic.edu%7Cd7cfe4dfe984430c6e
>>> 9
>>> 108d8fb6706c8%7Ce202cd477a564baa99e3e3b71a7c77dd%7C0%7C0%7C6375357678
>>> 0 
>>> 8134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiL
>>> C 
>>> JBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=obQUl59%2FNceepVKW4YMlC
>>> S FrOobHRl8LtnVZaAV23ks%3D&amp;reserved=0>
>>>
>>> Expires: Wed, 07 Apr 2021 18:40:31 GMT
>>>
>>> Cache-Control: max-age=3600
>>>
>>> ?
>>>
>>> I am running a centos 7 os with squid version 3.5.20, which is the 
>>> most recent yum version.
>>>
>>> This is driving me crazy.? I have tried debugging in squid and cannot 
>>> find the answer.? I have tried changing the squid.conf file.? I 
>>> always restart squid after I change the squid.conf file.
>>>
>>> Any help would be appreciated.
>>>
>>> ?
>>>
>>> My Squid.conf file:
>>>
>>> acl localnet src 10.0.0.0/8???? # RFC1918 possible internal network
>>>
>>> acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
>>>
>>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>>
>>> acl localnet src fc00::/7?????? # RFC 4193 local private network 
>>> range
>>>
>>> acl localnet src fe80::/10????? # RFC 4291 link-local (directly
>>> plugged) machines
>>>
>>> ?
>>>
>>> acl SSL_ports port 443
>>>
>>> acl Safe_ports port 80????????? # http
>>>
>>> acl Safe_ports port 443???????? # https
>>>
>>> acl Safe_ports port 591???????? # filemaker
>>>
>>> acl CONNECT method CONNECT
>>>
>>> ?
>>>
>>> http_access deny !Safe_ports
>>>
>>> ?
>>>
>>> http_access deny CONNECT !SSL_ports
>>>
>>> ?
>>>
>>> http_access allow localhost manager
>>>
>>> http_access deny manager
>>>
>>> ?
>>>
>>> acl whitelist dstdomain "/etc/squid/whitelist.txt"
>>>
>>> #acl whitelist dstdomain .arl.org
>>>
>>> http_access allow whitelist
>>>
>>> #http_access allow CONNECT whitelist
>>>
>>> ?
>>>
>>> http_access deny !whitelist
>>>
>>> ?
>>>
>>> http_access allow localnet
>>>
>>> http_access allow localhost
>>>
>>> ?
>>>
>>> http_access deny all
>>>
>>> ?
>>>
>>> # Squid normally listens to port 3128
>>>
>>> http_port 3128
>>>
>>> ?
>>>
>>> # port 1338 is for Front Desk Machines
>>>
>>> http_port 1338
>>>
>>> ?
>>>
>>> coredump_dir /var/spool/squid
>>>
>>> ?
>>>
>>> refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
>>>
>>> refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
>>>
>>> refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
>>>
>>> refresh_pattern .?????????????? 0?????? 20%???? 4320
>>>
>>> ?
>>>
>>> Beginning of whitelist.txt
>>>
>>> #A Page
>>>
>>> .aacrjournals.org
>>>
>>> .aai.org
>>>
>>> .aaiddjournals.org
>>>
>>> .aap.org
>>>
>>> .aappublications.orga
>>>
>>> .accessanesthesiology.com
>>>
>>> .anthropology.org.uk
>>>
>>> .archivegrid.org
>>>
>>> .arl.org
>>>
>>> .arlstatistics.org
>>>
>>> .artstor.org
>>>
>>> ?
>>>
>>> Thank you,
>>>
>>> Lisa Blake
>>>
>>> ?
>>>
>>> ?
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://nam04.safelinks.protection.outlook.com/?url=http%3A%2F%2Flist
>>> s
>>> .squid-cache.org%2Flistinfo%2Fsquid-users&amp;data=04%7C01%7Cloleary%
>>> 4
>>> 0uic.edu%7Cd7cfe4dfe984430c6e9108d8fb6706c8%7Ce202cd477a564baa99e3e3b
>>> 7 
>>> 1a7c77dd%7C0%7C0%7C637535767808134799%7CUnknown%7CTWFpbGZsb3d8eyJWIjo
>>> i 
>>> MC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp
>>> ;
>>> sdata=xR28PqxDa3d3aQhOqB9b142qoY2x8rSNTZOGTACIMLg%3D&amp;reserved=0
>>>



From rousskov at measurement-factory.com  Mon Apr 12 16:03:48 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Apr 2021 12:03:48 -0400
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
Message-ID: <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>

On 4/10/21 5:03 PM, koshik moshik wrote:

> I am trying to run a Squid proxy Server witth about 5000 cache peers. I
> am running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16.?
> 
> 
> Could you tell me what else is needed / not needed in my squid.config? I
> am encountering a high CPU usage and would like to create a very
> efficient proxy server. 

IIRC, Squid code is not optimized for handling a large number of
cache_peers: Several cache peer selection steps involve linear searches.

I do not know what exactly causes high CPU usage in your environment but
it could be those linear searches. You can test that (indirectly) by
decreasing the number of cache_peers from 5000 to, say, 5. That is a
weak test, of course, because other cache_peer-related overheads could
be to blame, but I would start there.


HTH,

Alex.



> Down below you can find my squid.config(I deleted the other cache_peer
> lines):
> 
> -----------
> 
> http_port 3128
> 
> dns_v4_first on
> 
> acl SSL_ports port 1-65535
> 
> acl Safe_ports port 1-65535
> 
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/.htpasswd
> 
> auth_param basic children 5
> 
> auth_param basic realm Squid Basic Authentication
> 
> auth_param basic credentialsttl 5 hours
> 
> acl password proxy_auth REQUIRED
> 
> http_access allow password
> 
> #http_access deny all
> 
> cache allow all
> 
> never_direct allow all
> 
> ident_access deny all
> 
> 
> 
> 
> 
> cache_mem 1 GB
> 
> maximum_object_size_in_memory 16 MB
> 
> 
> 
> 
> 
> # Leave coredumps in the first cache dir
> 
> coredump_dir /var/spool/squid
> 
> 
> #Rules to anonymize http headers
> 
> forwarded_for off
> 
> request_header_access Allow allow all
> 
> request_header_access Authorization allow all
> 
> request_header_access WWW-Authenticate allow all
> 
> request_header_access Proxy-Authorization allow all
> 
> request_header_access Proxy-Authenticate allow all
> 
> request_header_access Cache-Control allow all
> 
> request_header_access Content-Encoding allow all
> 
> request_header_access Content-Length allow all
> 
> request_header_access Content-Type allow all
> 
> request_header_access Date allow all
> 
> request_header_access Expires allow all
> 
> request_header_access Host allow all
> 
> request_header_access If-Modified-Since allow all
> 
> request_header_access Last-Modified allow all
> 
> request_header_access Location allow all
> 
> request_header_access Pragma allow all
> 
> request_header_access Accept allow all
> 
> request_header_access Accept-Charset allow all
> 
> request_header_access Accept-Encoding allow all
> 
> request_header_access Accept-Language allow all
> 
> request_header_access Content-Language allow all
> 
> request_header_access Mime-Version allow all
> 
> request_header_access Retry-After allow all
> 
> request_header_access Title allow all
> 
> request_header_access Connection allow all
> 
> request_header_access Proxy-Connection allow all
> 
> request_header_access User-Agent allow all
> 
> request_header_access Cookie allow all
> 
> request_header_access All deny all
> 
> 
> 
> 
> 
> #
> 
> # Add any of your own refresh_pattern entries above these.
> 
> #
> 
> #refresh_pattern ^ftp: ? ? ? ? ? 1440 ? ?20% ? ? 10080
> 
> #refresh_pattern ^gopher: ? ? ? ?1440 ? ?0% ? ? ?1440
> 
> #refresh_pattern -i (/cgi-bin/|\?) 0 ? ? 0% ? ? ?0
> 
> #refresh_pattern (Release|Packages(.gz)*)$ ? ? ?0 ? ? ? 20% ? ? 2880
> 
> #refresh_pattern . ? ? ? ? ? ? ? 0 ? ? ? 20% ? ? 4320
> 
> 
> ################################
> 
> acl me proxy_auth ye-1
> 
> cache_peer?my.proxy.com <http://my.proxy.com/>?parent 31280
> login=user1:password1 no-query name=a1
> 
> cache_peer_access a1 allow me
> 
> cache_peer_access a1 deny all
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Mon Apr 12 16:13:14 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Apr 2021 12:13:14 -0400
Subject: [squid-users] Squid within a network namespace
In-Reply-To: <CAMc2VtTujVm1H9pq+ctLSw+mjiY8nbSESHqC52W6fNy9cBb0pQ@mail.gmail.com>
References: <CAMc2VtTujVm1H9pq+ctLSw+mjiY8nbSESHqC52W6fNy9cBb0pQ@mail.gmail.com>
Message-ID: <b472eb7d-2ed6-45a5-0f77-3f88fa3de0da@measurement-factory.com>

On 4/11/21 12:46 PM, Francois wrote:

> I am running my development tools and VMs in a dedicated network
> namespace on my laptop (through Linux "netns"), so they are fully
> isolated from the rest of my network. I would like to set-up a proxy
> so that if there is a need to connect to the outside, I could set-up
> some fine grained ACL to open some very specific HTTP traffic. For
> this to work with Squid, there must be a socket opened within the
> namespace, while Squid is still running on the default namespace.
> 
> This can be achieved without modifying the code by using socat for
> example, where a socat running within the namespace sends traffic to a
> Unix socket, and another socat outside the namespace, reads from the
> Unix socket, and sends the traffic to Squid... it's quite some
> plumbing effort, and Squid won't be able to know from which VM the
> traffic originates (the X-Forwarded-For is localhost)
> 
> Seeing that HAProxy implemented something
> (https://github.com/haproxy/haproxy/commit/b3e54fe387c7c1ea750f39d3029672d640c499f9)
> so that the process moves into the namespace just for the time of the
> socket creation, I came up with a similar change for Squid
> (https://github.com/freedge/squid/commit/a778666d8f4760448e29e4a0cc75dcd305b40d02).

Thank you for a detailed explanation of your use case and sample code.


> As this is a Linux only change, and also the community lived without
> it so far, I am sending this mail to see if there is any interest in
> this feature, if there was ever any request for it in the past?

I cannot answer your questions, but I can tell you that, IMO, quality
namespace support should be accepted by the Squid Project. I hope others
will chime in regarding its usefulness to them.

The feature should probably be configured at least on a listening port
basis (rather than globally) and implementation would have to to meet
modern Squid requirements (failing on error, C++, etc.). Your sample
code could be a good starting point.


Cheers,

Alex.


From koshikmoshik at gmail.com  Wed Apr 14 06:29:25 2021
From: koshikmoshik at gmail.com (koshik moshik)
Date: Wed, 14 Apr 2021 08:29:25 +0200
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
Message-ID: <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>

Thank you! Yes, it works fine with 5 peers. So, what would be the best
solution to handle 5000 peers?

On Mon, Apr 12, 2021 at 6:03 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 4/10/21 5:03 PM, koshik moshik wrote:
>
> > I am trying to run a Squid proxy Server witth about 5000 cache peers. I
> > am running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16.
> >
> >
> > Could you tell me what else is needed / not needed in my squid.config? I
> > am encountering a high CPU usage and would like to create a very
> > efficient proxy server.
>
> IIRC, Squid code is not optimized for handling a large number of
> cache_peers: Several cache peer selection steps involve linear searches.
>
> I do not know what exactly causes high CPU usage in your environment but
> it could be those linear searches. You can test that (indirectly) by
> decreasing the number of cache_peers from 5000 to, say, 5. That is a
> weak test, of course, because other cache_peer-related overheads could
> be to blame, but I would start there.
>
>
> HTH,
>
> Alex.
>
>
>
> > Down below you can find my squid.config(I deleted the other cache_peer
> > lines):
> >
> > -----------
> >
> > http_port 3128
> >
> > dns_v4_first on
> >
> > acl SSL_ports port 1-65535
> >
> > acl Safe_ports port 1-65535
> >
> > acl CONNECT method CONNECT
> >
> > http_access deny !Safe_ports
> >
> > http_access deny CONNECT !SSL_ports
> >
> > auth_param basic program /usr/lib/squid/basic_ncsa_auth
> /etc/squid/.htpasswd
> >
> > auth_param basic children 5
> >
> > auth_param basic realm Squid Basic Authentication
> >
> > auth_param basic credentialsttl 5 hours
> >
> > acl password proxy_auth REQUIRED
> >
> > http_access allow password
> >
> > #http_access deny all
> >
> > cache allow all
> >
> > never_direct allow all
> >
> > ident_access deny all
> >
> >
> >
> >
> >
> > cache_mem 1 GB
> >
> > maximum_object_size_in_memory 16 MB
> >
> >
> >
> >
> >
> > # Leave coredumps in the first cache dir
> >
> > coredump_dir /var/spool/squid
> >
> >
> > #Rules to anonymize http headers
> >
> > forwarded_for off
> >
> > request_header_access Allow allow all
> >
> > request_header_access Authorization allow all
> >
> > request_header_access WWW-Authenticate allow all
> >
> > request_header_access Proxy-Authorization allow all
> >
> > request_header_access Proxy-Authenticate allow all
> >
> > request_header_access Cache-Control allow all
> >
> > request_header_access Content-Encoding allow all
> >
> > request_header_access Content-Length allow all
> >
> > request_header_access Content-Type allow all
> >
> > request_header_access Date allow all
> >
> > request_header_access Expires allow all
> >
> > request_header_access Host allow all
> >
> > request_header_access If-Modified-Since allow all
> >
> > request_header_access Last-Modified allow all
> >
> > request_header_access Location allow all
> >
> > request_header_access Pragma allow all
> >
> > request_header_access Accept allow all
> >
> > request_header_access Accept-Charset allow all
> >
> > request_header_access Accept-Encoding allow all
> >
> > request_header_access Accept-Language allow all
> >
> > request_header_access Content-Language allow all
> >
> > request_header_access Mime-Version allow all
> >
> > request_header_access Retry-After allow all
> >
> > request_header_access Title allow all
> >
> > request_header_access Connection allow all
> >
> > request_header_access Proxy-Connection allow all
> >
> > request_header_access User-Agent allow all
> >
> > request_header_access Cookie allow all
> >
> > request_header_access All deny all
> >
> >
> >
> >
> >
> > #
> >
> > # Add any of your own refresh_pattern entries above these.
> >
> > #
> >
> > #refresh_pattern ^ftp:           1440    20%     10080
> >
> > #refresh_pattern ^gopher:        1440    0%      1440
> >
> > #refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> >
> > #refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> >
> > #refresh_pattern .               0       20%     4320
> >
> >
> > ################################
> >
> > acl me proxy_auth ye-1
> >
> > cache_peer my.proxy.com <http://my.proxy.com/> parent 31280
> > login=user1:password1 no-query name=a1
> >
> > cache_peer_access a1 allow me
> >
> > cache_peer_access a1 deny all
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210414/79611294/attachment.htm>

From ngtech1ltd at gmail.com  Wed Apr 14 07:09:17 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Wed, 14 Apr 2021 10:09:17 +0300
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
 <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
Message-ID: <CABA8h=QpPQ=aZ71nGu7u0JL_h9TCn1=Cp7jFbHi=-xQ7u01TTw@mail.gmail.com>

Its not clear what is the factor for a specific cache peer selection.
This will affect any advice.
Is it only baesd on username?

Eliezer

?????? ??? ??, 14 ????? 2021, 9:29, ??? koshik moshik ?<
koshikmoshik at gmail.com>:

> Thank you! Yes, it works fine with 5 peers. So, what would be the best
> solution to handle 5000 peers?
>
> On Mon, Apr 12, 2021 at 6:03 PM Alex Rousskov <
> rousskov at measurement-factory.com> wrote:
>
>> On 4/10/21 5:03 PM, koshik moshik wrote:
>>
>> > I am trying to run a Squid proxy Server witth about 5000 cache peers. I
>> > am running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16.
>> >
>> >
>> > Could you tell me what else is needed / not needed in my squid.config? I
>> > am encountering a high CPU usage and would like to create a very
>> > efficient proxy server.
>>
>> IIRC, Squid code is not optimized for handling a large number of
>> cache_peers: Several cache peer selection steps involve linear searches.
>>
>> I do not know what exactly causes high CPU usage in your environment but
>> it could be those linear searches. You can test that (indirectly) by
>> decreasing the number of cache_peers from 5000 to, say, 5. That is a
>> weak test, of course, because other cache_peer-related overheads could
>> be to blame, but I would start there.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>> > Down below you can find my squid.config(I deleted the other cache_peer
>> > lines):
>> >
>> > -----------
>> >
>> > http_port 3128
>> >
>> > dns_v4_first on
>> >
>> > acl SSL_ports port 1-65535
>> >
>> > acl Safe_ports port 1-65535
>> >
>> > acl CONNECT method CONNECT
>> >
>> > http_access deny !Safe_ports
>> >
>> > http_access deny CONNECT !SSL_ports
>> >
>> > auth_param basic program /usr/lib/squid/basic_ncsa_auth
>> /etc/squid/.htpasswd
>> >
>> > auth_param basic children 5
>> >
>> > auth_param basic realm Squid Basic Authentication
>> >
>> > auth_param basic credentialsttl 5 hours
>> >
>> > acl password proxy_auth REQUIRED
>> >
>> > http_access allow password
>> >
>> > #http_access deny all
>> >
>> > cache allow all
>> >
>> > never_direct allow all
>> >
>> > ident_access deny all
>> >
>> >
>> >
>> >
>> >
>> > cache_mem 1 GB
>> >
>> > maximum_object_size_in_memory 16 MB
>> >
>> >
>> >
>> >
>> >
>> > # Leave coredumps in the first cache dir
>> >
>> > coredump_dir /var/spool/squid
>> >
>> >
>> > #Rules to anonymize http headers
>> >
>> > forwarded_for off
>> >
>> > request_header_access Allow allow all
>> >
>> > request_header_access Authorization allow all
>> >
>> > request_header_access WWW-Authenticate allow all
>> >
>> > request_header_access Proxy-Authorization allow all
>> >
>> > request_header_access Proxy-Authenticate allow all
>> >
>> > request_header_access Cache-Control allow all
>> >
>> > request_header_access Content-Encoding allow all
>> >
>> > request_header_access Content-Length allow all
>> >
>> > request_header_access Content-Type allow all
>> >
>> > request_header_access Date allow all
>> >
>> > request_header_access Expires allow all
>> >
>> > request_header_access Host allow all
>> >
>> > request_header_access If-Modified-Since allow all
>> >
>> > request_header_access Last-Modified allow all
>> >
>> > request_header_access Location allow all
>> >
>> > request_header_access Pragma allow all
>> >
>> > request_header_access Accept allow all
>> >
>> > request_header_access Accept-Charset allow all
>> >
>> > request_header_access Accept-Encoding allow all
>> >
>> > request_header_access Accept-Language allow all
>> >
>> > request_header_access Content-Language allow all
>> >
>> > request_header_access Mime-Version allow all
>> >
>> > request_header_access Retry-After allow all
>> >
>> > request_header_access Title allow all
>> >
>> > request_header_access Connection allow all
>> >
>> > request_header_access Proxy-Connection allow all
>> >
>> > request_header_access User-Agent allow all
>> >
>> > request_header_access Cookie allow all
>> >
>> > request_header_access All deny all
>> >
>> >
>> >
>> >
>> >
>> > #
>> >
>> > # Add any of your own refresh_pattern entries above these.
>> >
>> > #
>> >
>> > #refresh_pattern ^ftp:           1440    20%     10080
>> >
>> > #refresh_pattern ^gopher:        1440    0%      1440
>> >
>> > #refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> >
>> > #refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>> >
>> > #refresh_pattern .               0       20%     4320
>> >
>> >
>> > ################################
>> >
>> > acl me proxy_auth ye-1
>> >
>> > cache_peer my.proxy.com <http://my.proxy.com/> parent 31280
>> > login=user1:password1 no-query name=a1
>> >
>> > cache_peer_access a1 allow me
>> >
>> > cache_peer_access a1 deny all
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>>
>> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210414/49312bf6/attachment.htm>

From uhlar at fantomas.sk  Wed Apr 14 08:50:52 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 14 Apr 2021 10:50:52 +0200
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
 <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
Message-ID: <20210414085052.GA7078@fantomas.sk>

On 14.04.21 08:29, koshik moshik wrote:
>Thank you! Yes, it works fine with 5 peers. So, what would be the best
>solution to handle 5000 peers?

maybe not doing that?
why do you need so many peers?

if you really do, you apparently could set them up all as no-query to only
fetch cache digests, so each request to your squid doesn't get forwarded to
5000 neighbour proxies - that would cause huge traffic and slowdown 

... I assume have digest_generation enabled.


>> On 4/10/21 5:03 PM, koshik moshik wrote:
>> > Down below you can find my squid.config(I deleted the other cache_peer
>> > lines):
>> >
>> > -----------
>> >
>> > http_port 3128
>> >
>> > dns_v4_first on
>> >
>> > acl SSL_ports port 1-65535
>> >
>> > acl Safe_ports port 1-65535
>> >
>> > acl CONNECT method CONNECT
>> >
>> > http_access deny !Safe_ports
>> >
>> > http_access deny CONNECT !SSL_ports

are you aware that Safe_ports and SSL_ports are designed to protect your
squid server from participating in DoS attacks and from DoS attacks against
your squid?


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fighting for peace is like fucking for virginity...


From ngtech1ltd at gmail.com  Wed Apr 14 09:34:10 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 14 Apr 2021 12:34:10 +0300
Subject: [squid-users] Can't get squid with whitelist text file to work
 TCP_DENIED/403
In-Reply-To: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>
References: <CH2PR13MB3815AA59E2E5E79B567D3314BF749@CH2PR13MB3815.namprd13.prod.outlook.com>
Message-ID: <001201d73111$53bba350$fb32e9f0$@gmail.com>

Did you got it working eventually?

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Elliott Blake, Lisa Marie
Sent: Thursday, April 8, 2021 10:11 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Can't get squid with whitelist text file to work
TCP_DENIED/403

 

I am trying to get squid to work with a text file for a whitelist.  I get
TCP_DENIED/403 on every url I try.  I am using curl to test.

acl whitelist dstdomain "/etc/squid/whitelist.txt"

curl -x https://libaux-prod.lib.uic.edu:3128 -I https://arl.org 

HTTP/1.1 403 Forbidden

Server: squid/3.5.20

Mime-Version: 1.0

Date: Wed, 07 Apr 2021 17:38:58 GMT

Content-Type: text/html;charset=utf-8

Content-Length: 3521

X-Squid-Error: ERR_ACCESS_DENIED 0

Vary: Accept-Language

Content-Language: en

X-Cache: MISS from libaux-prod.lib.uic.edu

X-Cache-Lookup: NONE from libaux-prod.lib.uic.edu:3128

Via: 1.1 libaux-prod.lib.uic.edu (squid/3.5.20)

Connection: keep-alive

curl: (56) Received HTTP code 403 from proxy after CONNECT

 

However, if I change my squid.conf to just the url it works.

acl whitelist dstdomain .arl.org

curl -x https://libaux-prod.lib.uic.edu:3128 -I https://arl.org 

HTTP/1.1 200 Connection established

HTTP/1.1 301 Moved Permanently

Server: nginx

Date: Wed, 07 Apr 2021 17:40:31 GMT

Content-Type: text/html

Content-Length: 178

Connection: keep-alive

Keep-Alive: timeout=20

Location: https://www.arl.org/

Expires: Wed, 07 Apr 2021 18:40:31 GMT

Cache-Control: max-age=3600

 

I am running a centos 7 os with squid version 3.5.20, which is the most
recent yum version.

This is driving me crazy.  I have tried debugging in squid and cannot find
the answer.  I have tried changing the squid.conf file.  I always restart
squid after I change the squid.conf file.  

Any help would be appreciated.

 

My Squid.conf file:

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl localnet src 172.16.0.0/12  # RFC1918 possible internal network

acl localnet src 192.168.0.0/16 # RFC1918 possible internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

 

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 443         # https

acl Safe_ports port 591         # filemaker

acl CONNECT method CONNECT

 

http_access deny !Safe_ports

 

http_access deny CONNECT !SSL_ports

 

http_access allow localhost manager

http_access deny manager

 

acl whitelist dstdomain "/etc/squid/whitelist.txt"

#acl whitelist dstdomain .arl.org

http_access allow whitelist

#http_access allow CONNECT whitelist

 

http_access deny !whitelist

 

http_access allow localnet

http_access allow localhost

 

http_access deny all

 

# Squid normally listens to port 3128

http_port 3128

 

# port 1338 is for Front Desk Machines

http_port 1338

 

coredump_dir /var/spool/squid

 

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

 

Beginning of whitelist.txt

#A Page

.aacrjournals.org

.aai.org

.aaiddjournals.org

.aap.org

.aappublications.orga

.accessanesthesiology.com

.anthropology.org.uk

.archivegrid.org

.arl.org

.arlstatistics.org

.artstor.org

 

Thank you,

Lisa Blake

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210414/5a13e630/attachment.htm>

From rousskov at measurement-factory.com  Wed Apr 14 18:37:53 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 14 Apr 2021 14:37:53 -0400
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
 <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
Message-ID: <735670ef-4a71-a3c2-8b5f-534aec8cf866@measurement-factory.com>

On 4/14/21 2:29 AM, koshik moshik wrote:
> Thank you! Yes, it works fine with 5 peers. So, what would be the best
> solution to handle 5000 peers??

As you can tell by other responses, you might be asking the wrong
question. However, I will still try to answer your question. The best
option may be to add support for a new Squid configuration parameter
that tells Squid to limit cache_peer candidate accumulation to N peers,
effectively making all those linear searches fast.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


> On Mon, Apr 12, 2021 at 6:03 PM Alex Rousskov wrote:
> 
>     On 4/10/21 5:03 PM, koshik moshik wrote:
> 
>     > I am trying to run a Squid proxy Server witth about 5000 cache
>     peers. I
>     > am running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16.?
>     >
>     >
>     > Could you tell me what else is needed / not needed in my
>     squid.config? I
>     > am encountering a high CPU usage and would like to create a very
>     > efficient proxy server.
> 
>     IIRC, Squid code is not optimized for handling a large number of
>     cache_peers: Several cache peer selection steps involve linear searches.
> 
>     I do not know what exactly causes high CPU usage in your environment but
>     it could be those linear searches. You can test that (indirectly) by
>     decreasing the number of cache_peers from 5000 to, say, 5. That is a
>     weak test, of course, because other cache_peer-related overheads could
>     be to blame, but I would start there.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
>     > Down below you can find my squid.config(I deleted the other cache_peer
>     > lines):
>     >
>     > -----------
>     >
>     > http_port 3128
>     >
>     > dns_v4_first on
>     >
>     > acl SSL_ports port 1-65535
>     >
>     > acl Safe_ports port 1-65535
>     >
>     > acl CONNECT method CONNECT
>     >
>     > http_access deny !Safe_ports
>     >
>     > http_access deny CONNECT !SSL_ports
>     >
>     > auth_param basic program /usr/lib/squid/basic_ncsa_auth
>     /etc/squid/.htpasswd
>     >
>     > auth_param basic children 5
>     >
>     > auth_param basic realm Squid Basic Authentication
>     >
>     > auth_param basic credentialsttl 5 hours
>     >
>     > acl password proxy_auth REQUIRED
>     >
>     > http_access allow password
>     >
>     > #http_access deny all
>     >
>     > cache allow all
>     >
>     > never_direct allow all
>     >
>     > ident_access deny all
>     >
>     >
>     >
>     >
>     >
>     > cache_mem 1 GB
>     >
>     > maximum_object_size_in_memory 16 MB
>     >
>     >
>     >
>     >
>     >
>     > # Leave coredumps in the first cache dir
>     >
>     > coredump_dir /var/spool/squid
>     >
>     >
>     > #Rules to anonymize http headers
>     >
>     > forwarded_for off
>     >
>     > request_header_access Allow allow all
>     >
>     > request_header_access Authorization allow all
>     >
>     > request_header_access WWW-Authenticate allow all
>     >
>     > request_header_access Proxy-Authorization allow all
>     >
>     > request_header_access Proxy-Authenticate allow all
>     >
>     > request_header_access Cache-Control allow all
>     >
>     > request_header_access Content-Encoding allow all
>     >
>     > request_header_access Content-Length allow all
>     >
>     > request_header_access Content-Type allow all
>     >
>     > request_header_access Date allow all
>     >
>     > request_header_access Expires allow all
>     >
>     > request_header_access Host allow all
>     >
>     > request_header_access If-Modified-Since allow all
>     >
>     > request_header_access Last-Modified allow all
>     >
>     > request_header_access Location allow all
>     >
>     > request_header_access Pragma allow all
>     >
>     > request_header_access Accept allow all
>     >
>     > request_header_access Accept-Charset allow all
>     >
>     > request_header_access Accept-Encoding allow all
>     >
>     > request_header_access Accept-Language allow all
>     >
>     > request_header_access Content-Language allow all
>     >
>     > request_header_access Mime-Version allow all
>     >
>     > request_header_access Retry-After allow all
>     >
>     > request_header_access Title allow all
>     >
>     > request_header_access Connection allow all
>     >
>     > request_header_access Proxy-Connection allow all
>     >
>     > request_header_access User-Agent allow all
>     >
>     > request_header_access Cookie allow all
>     >
>     > request_header_access All deny all
>     >
>     >
>     >
>     >
>     >
>     > #
>     >
>     > # Add any of your own refresh_pattern entries above these.
>     >
>     > #
>     >
>     > #refresh_pattern ^ftp: ? ? ? ? ? 1440 ? ?20% ? ? 10080
>     >
>     > #refresh_pattern ^gopher: ? ? ? ?1440 ? ?0% ? ? ?1440
>     >
>     > #refresh_pattern -i (/cgi-bin/|\?) 0 ? ? 0% ? ? ?0
>     >
>     > #refresh_pattern (Release|Packages(.gz)*)$ ? ? ?0 ? ? ? 20% ? ? 2880
>     >
>     > #refresh_pattern . ? ? ? ? ? ? ? 0 ? ? ? 20% ? ? 4320
>     >
>     >
>     > ################################
>     >
>     > acl me proxy_auth ye-1
>     >
>     > cache_peer?my.proxy.com <http://my.proxy.com>
>     <http://my.proxy.com/ <http://my.proxy.com/>>?parent 31280
>     > login=user1:password1 no-query name=a1
>     >
>     > cache_peer_access a1 allow me
>     >
>     > cache_peer_access a1 deny all
>     >
>     >
>     > _______________________________________________
>     > squid-users mailing list
>     > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     >
> 



From koshikmoshik at gmail.com  Wed Apr 14 18:49:13 2021
From: koshikmoshik at gmail.com (koshik moshik)
Date: Wed, 14 Apr 2021 20:49:13 +0200
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <735670ef-4a71-a3c2-8b5f-534aec8cf866@measurement-factory.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
 <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
 <735670ef-4a71-a3c2-8b5f-534aec8cf866@measurement-factory.com>
Message-ID: <CAPeL7PGmK-Kfqvb-U=uU+DcXMpsjtGAeSRpM_jt0e+ShNJGVEQ@mail.gmail.com>

First of all thank you for trying to help me. Let me describe my current
issue: I have 5000 proxies and would like to hide them. My plan was using
another proxy server with 5000 cache peers and 5000 users. Each user would
get one peer and one proxy attached to that peer. So basically the outer
world would not see my "main proxy" and only the one from the new proxy
server.

Is there any better solution than cache peers for that?

On Wed, Apr 14, 2021 at 8:37 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 4/14/21 2:29 AM, koshik moshik wrote:
> > Thank you! Yes, it works fine with 5 peers. So, what would be the best
> > solution to handle 5000 peers?
>
> As you can tell by other responses, you might be asking the wrong
> question. However, I will still try to answer your question. The best
> option may be to add support for a new Squid configuration parameter
> that tells Squid to limit cache_peer candidate accumulation to N peers,
> effectively making all those linear searches fast.
>
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> Alex.
>
>
> > On Mon, Apr 12, 2021 at 6:03 PM Alex Rousskov wrote:
> >
> >     On 4/10/21 5:03 PM, koshik moshik wrote:
> >
> >     > I am trying to run a Squid proxy Server witth about 5000 cache
> >     peers. I
> >     > am running a dedicated server with 6 cores and 32GB RAM on Ubuntu
> 16.
> >     >
> >     >
> >     > Could you tell me what else is needed / not needed in my
> >     squid.config? I
> >     > am encountering a high CPU usage and would like to create a very
> >     > efficient proxy server.
> >
> >     IIRC, Squid code is not optimized for handling a large number of
> >     cache_peers: Several cache peer selection steps involve linear
> searches.
> >
> >     I do not know what exactly causes high CPU usage in your environment
> but
> >     it could be those linear searches. You can test that (indirectly) by
> >     decreasing the number of cache_peers from 5000 to, say, 5. That is a
> >     weak test, of course, because other cache_peer-related overheads
> could
> >     be to blame, but I would start there.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >
> >     > Down below you can find my squid.config(I deleted the other
> cache_peer
> >     > lines):
> >     >
> >     > -----------
> >     >
> >     > http_port 3128
> >     >
> >     > dns_v4_first on
> >     >
> >     > acl SSL_ports port 1-65535
> >     >
> >     > acl Safe_ports port 1-65535
> >     >
> >     > acl CONNECT method CONNECT
> >     >
> >     > http_access deny !Safe_ports
> >     >
> >     > http_access deny CONNECT !SSL_ports
> >     >
> >     > auth_param basic program /usr/lib/squid/basic_ncsa_auth
> >     /etc/squid/.htpasswd
> >     >
> >     > auth_param basic children 5
> >     >
> >     > auth_param basic realm Squid Basic Authentication
> >     >
> >     > auth_param basic credentialsttl 5 hours
> >     >
> >     > acl password proxy_auth REQUIRED
> >     >
> >     > http_access allow password
> >     >
> >     > #http_access deny all
> >     >
> >     > cache allow all
> >     >
> >     > never_direct allow all
> >     >
> >     > ident_access deny all
> >     >
> >     >
> >     >
> >     >
> >     >
> >     > cache_mem 1 GB
> >     >
> >     > maximum_object_size_in_memory 16 MB
> >     >
> >     >
> >     >
> >     >
> >     >
> >     > # Leave coredumps in the first cache dir
> >     >
> >     > coredump_dir /var/spool/squid
> >     >
> >     >
> >     > #Rules to anonymize http headers
> >     >
> >     > forwarded_for off
> >     >
> >     > request_header_access Allow allow all
> >     >
> >     > request_header_access Authorization allow all
> >     >
> >     > request_header_access WWW-Authenticate allow all
> >     >
> >     > request_header_access Proxy-Authorization allow all
> >     >
> >     > request_header_access Proxy-Authenticate allow all
> >     >
> >     > request_header_access Cache-Control allow all
> >     >
> >     > request_header_access Content-Encoding allow all
> >     >
> >     > request_header_access Content-Length allow all
> >     >
> >     > request_header_access Content-Type allow all
> >     >
> >     > request_header_access Date allow all
> >     >
> >     > request_header_access Expires allow all
> >     >
> >     > request_header_access Host allow all
> >     >
> >     > request_header_access If-Modified-Since allow all
> >     >
> >     > request_header_access Last-Modified allow all
> >     >
> >     > request_header_access Location allow all
> >     >
> >     > request_header_access Pragma allow all
> >     >
> >     > request_header_access Accept allow all
> >     >
> >     > request_header_access Accept-Charset allow all
> >     >
> >     > request_header_access Accept-Encoding allow all
> >     >
> >     > request_header_access Accept-Language allow all
> >     >
> >     > request_header_access Content-Language allow all
> >     >
> >     > request_header_access Mime-Version allow all
> >     >
> >     > request_header_access Retry-After allow all
> >     >
> >     > request_header_access Title allow all
> >     >
> >     > request_header_access Connection allow all
> >     >
> >     > request_header_access Proxy-Connection allow all
> >     >
> >     > request_header_access User-Agent allow all
> >     >
> >     > request_header_access Cookie allow all
> >     >
> >     > request_header_access All deny all
> >     >
> >     >
> >     >
> >     >
> >     >
> >     > #
> >     >
> >     > # Add any of your own refresh_pattern entries above these.
> >     >
> >     > #
> >     >
> >     > #refresh_pattern ^ftp:           1440    20%     10080
> >     >
> >     > #refresh_pattern ^gopher:        1440    0%      1440
> >     >
> >     > #refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> >     >
> >     > #refresh_pattern (Release|Packages(.gz)*)$      0       20%
> 2880
> >     >
> >     > #refresh_pattern .               0       20%     4320
> >     >
> >     >
> >     > ################################
> >     >
> >     > acl me proxy_auth ye-1
> >     >
> >     > cache_peer my.proxy.com <http://my.proxy.com>
> >     <http://my.proxy.com/ <http://my.proxy.com/>> parent 31280
> >     > login=user1:password1 no-query name=a1
> >     >
> >     > cache_peer_access a1 allow me
> >     >
> >     > cache_peer_access a1 deny all
> >     >
> >     >
> >     > _______________________________________________
> >     > squid-users mailing list
> >     > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     > http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210414/846b874f/attachment.htm>

From rousskov at measurement-factory.com  Wed Apr 14 21:09:07 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 14 Apr 2021 17:09:07 -0400
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PGmK-Kfqvb-U=uU+DcXMpsjtGAeSRpM_jt0e+ShNJGVEQ@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
 <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
 <735670ef-4a71-a3c2-8b5f-534aec8cf866@measurement-factory.com>
 <CAPeL7PGmK-Kfqvb-U=uU+DcXMpsjtGAeSRpM_jt0e+ShNJGVEQ@mail.gmail.com>
Message-ID: <233ad492-4f51-56e0-53e7-95232e64ae67@measurement-factory.com>

On 4/14/21 2:49 PM, koshik moshik wrote:
> First of all thank you for trying to help me. Let me describe my current
> issue: I have 5000 proxies and would like to hide them. My plan was
> using another proxy server with 5000 cache peers and 5000 users. Each
> user would get one peer and one proxy attached to that peer. So
> basically the outer world would not see my "main proxy" and only the one
> from the new proxy server.?

> Is there any better solution than cache peers for that??

Probably. It sounds like you need dynamic HTTP proxy routing (where the
first proxy computes the address of the second proxy/hop from the user
name rather than selects from a huge set of fixed cache_peers using a
huge set of fixed ACLs). AFAIK, Squid does not support such routing
without source code modifications. Even ICAP/eCAP cannot do that alone.

Alex.


> On Wed, Apr 14, 2021 at 8:37 PM Alex Rousskov wrote:
> 
>     On 4/14/21 2:29 AM, koshik moshik wrote:
>     > Thank you! Yes, it works fine with 5 peers. So, what would be the best
>     > solution to handle 5000 peers??
> 
>     As you can tell by other responses, you might be asking the wrong
>     question. However, I will still try to answer your question. The best
>     option may be to add support for a new Squid configuration parameter
>     that tells Squid to limit cache_peer candidate accumulation to N peers,
>     effectively making all those linear searches fast.
> 
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>     <https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F>
> 
>     Alex.
> 
> 
>     > On Mon, Apr 12, 2021 at 6:03 PM Alex Rousskov wrote:
>     >
>     >? ? ?On 4/10/21 5:03 PM, koshik moshik wrote:
>     >
>     >? ? ?> I am trying to run a Squid proxy Server witth about 5000 cache
>     >? ? ?peers. I
>     >? ? ?> am running a dedicated server with 6 cores and 32GB RAM on
>     Ubuntu 16.?
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> Could you tell me what else is needed / not needed in my
>     >? ? ?squid.config? I
>     >? ? ?> am encountering a high CPU usage and would like to create a very
>     >? ? ?> efficient proxy server.
>     >
>     >? ? ?IIRC, Squid code is not optimized for handling a large number of
>     >? ? ?cache_peers: Several cache peer selection steps involve linear
>     searches.
>     >
>     >? ? ?I do not know what exactly causes high CPU usage in your
>     environment but
>     >? ? ?it could be those linear searches. You can test that
>     (indirectly) by
>     >? ? ?decreasing the number of cache_peers from 5000 to, say, 5.
>     That is a
>     >? ? ?weak test, of course, because other cache_peer-related
>     overheads could
>     >? ? ?be to blame, but I would start there.
>     >
>     >
>     >? ? ?HTH,
>     >
>     >? ? ?Alex.
>     >
>     >
>     >
>     >? ? ?> Down below you can find my squid.config(I deleted the other
>     cache_peer
>     >? ? ?> lines):
>     >? ? ?>
>     >? ? ?> -----------
>     >? ? ?>
>     >? ? ?> http_port 3128
>     >? ? ?>
>     >? ? ?> dns_v4_first on
>     >? ? ?>
>     >? ? ?> acl SSL_ports port 1-65535
>     >? ? ?>
>     >? ? ?> acl Safe_ports port 1-65535
>     >? ? ?>
>     >? ? ?> acl CONNECT method CONNECT
>     >? ? ?>
>     >? ? ?> http_access deny !Safe_ports
>     >? ? ?>
>     >? ? ?> http_access deny CONNECT !SSL_ports
>     >? ? ?>
>     >? ? ?> auth_param basic program /usr/lib/squid/basic_ncsa_auth
>     >? ? ?/etc/squid/.htpasswd
>     >? ? ?>
>     >? ? ?> auth_param basic children 5
>     >? ? ?>
>     >? ? ?> auth_param basic realm Squid Basic Authentication
>     >? ? ?>
>     >? ? ?> auth_param basic credentialsttl 5 hours
>     >? ? ?>
>     >? ? ?> acl password proxy_auth REQUIRED
>     >? ? ?>
>     >? ? ?> http_access allow password
>     >? ? ?>
>     >? ? ?> #http_access deny all
>     >? ? ?>
>     >? ? ?> cache allow all
>     >? ? ?>
>     >? ? ?> never_direct allow all
>     >? ? ?>
>     >? ? ?> ident_access deny all
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> cache_mem 1 GB
>     >? ? ?>
>     >? ? ?> maximum_object_size_in_memory 16 MB
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> # Leave coredumps in the first cache dir
>     >? ? ?>
>     >? ? ?> coredump_dir /var/spool/squid
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> #Rules to anonymize http headers
>     >? ? ?>
>     >? ? ?> forwarded_for off
>     >? ? ?>
>     >? ? ?> request_header_access Allow allow all
>     >? ? ?>
>     >? ? ?> request_header_access Authorization allow all
>     >? ? ?>
>     >? ? ?> request_header_access WWW-Authenticate allow all
>     >? ? ?>
>     >? ? ?> request_header_access Proxy-Authorization allow all
>     >? ? ?>
>     >? ? ?> request_header_access Proxy-Authenticate allow all
>     >? ? ?>
>     >? ? ?> request_header_access Cache-Control allow all
>     >? ? ?>
>     >? ? ?> request_header_access Content-Encoding allow all
>     >? ? ?>
>     >? ? ?> request_header_access Content-Length allow all
>     >? ? ?>
>     >? ? ?> request_header_access Content-Type allow all
>     >? ? ?>
>     >? ? ?> request_header_access Date allow all
>     >? ? ?>
>     >? ? ?> request_header_access Expires allow all
>     >? ? ?>
>     >? ? ?> request_header_access Host allow all
>     >? ? ?>
>     >? ? ?> request_header_access If-Modified-Since allow all
>     >? ? ?>
>     >? ? ?> request_header_access Last-Modified allow all
>     >? ? ?>
>     >? ? ?> request_header_access Location allow all
>     >? ? ?>
>     >? ? ?> request_header_access Pragma allow all
>     >? ? ?>
>     >? ? ?> request_header_access Accept allow all
>     >? ? ?>
>     >? ? ?> request_header_access Accept-Charset allow all
>     >? ? ?>
>     >? ? ?> request_header_access Accept-Encoding allow all
>     >? ? ?>
>     >? ? ?> request_header_access Accept-Language allow all
>     >? ? ?>
>     >? ? ?> request_header_access Content-Language allow all
>     >? ? ?>
>     >? ? ?> request_header_access Mime-Version allow all
>     >? ? ?>
>     >? ? ?> request_header_access Retry-After allow all
>     >? ? ?>
>     >? ? ?> request_header_access Title allow all
>     >? ? ?>
>     >? ? ?> request_header_access Connection allow all
>     >? ? ?>
>     >? ? ?> request_header_access Proxy-Connection allow all
>     >? ? ?>
>     >? ? ?> request_header_access User-Agent allow all
>     >? ? ?>
>     >? ? ?> request_header_access Cookie allow all
>     >? ? ?>
>     >? ? ?> request_header_access All deny all
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> #
>     >? ? ?>
>     >? ? ?> # Add any of your own refresh_pattern entries above these.
>     >? ? ?>
>     >? ? ?> #
>     >? ? ?>
>     >? ? ?> #refresh_pattern ^ftp: ? ? ? ? ? 1440 ? ?20% ? ? 10080
>     >? ? ?>
>     >? ? ?> #refresh_pattern ^gopher: ? ? ? ?1440 ? ?0% ? ? ?1440
>     >? ? ?>
>     >? ? ?> #refresh_pattern -i (/cgi-bin/|\?) 0 ? ? 0% ? ? ?0
>     >? ? ?>
>     >? ? ?> #refresh_pattern (Release|Packages(.gz)*)$ ? ? ?0 ? ? ? 20%
>     ? ? 2880
>     >? ? ?>
>     >? ? ?> #refresh_pattern . ? ? ? ? ? ? ? 0 ? ? ? 20% ? ? 4320
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> ################################
>     >? ? ?>
>     >? ? ?> acl me proxy_auth ye-1
>     >? ? ?>
>     >? ? ?> cache_peer?my.proxy.com <http://my.proxy.com>
>     <http://my.proxy.com <http://my.proxy.com>>
>     >? ? ?<http://my.proxy.com/ <http://my.proxy.com/>
>     <http://my.proxy.com/ <http://my.proxy.com/>>>?parent 31280
>     >? ? ?> login=user1:password1 no-query name=a1
>     >? ? ?>
>     >? ? ?> cache_peer_access a1 allow me
>     >? ? ?>
>     >? ? ?> cache_peer_access a1 deny all
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> _______________________________________________
>     >? ? ?> squid-users mailing list
>     >? ? ?> squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>     >? ? ?> http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>     >? ? ?>
>     >
> 



From uhlar at fantomas.sk  Thu Apr 15 08:25:31 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 15 Apr 2021 10:25:31 +0200
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PGmK-Kfqvb-U=uU+DcXMpsjtGAeSRpM_jt0e+ShNJGVEQ@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <ff4d3148-cf07-8972-8ca7-469dd3233b3d@measurement-factory.com>
 <CAPeL7PF07+Dcz4Wb9UkK241J9H4RwJJwxjDMigXOLEvHkZ8_Dw@mail.gmail.com>
 <735670ef-4a71-a3c2-8b5f-534aec8cf866@measurement-factory.com>
 <CAPeL7PGmK-Kfqvb-U=uU+DcXMpsjtGAeSRpM_jt0e+ShNJGVEQ@mail.gmail.com>
Message-ID: <20210415082531.GA8811@fantomas.sk>

On 14.04.21 20:49, koshik moshik wrote:
>First of all thank you for trying to help me. Let me describe my current
>issue: I have 5000 proxies and would like to hide them. My plan was using
>another proxy server with 5000 cache peers and 5000 users.

do you have 5000 users with one proxy for each user?
 
> Each user would get one peer and one proxy attached to that peer.

User connects to a proxy, and the proxy can have multiple peers set up.

> So
>basically the outer world would not see my "main proxy" and only the one
>from the new proxy server.

the world will see IP of whichever proxy those users use, or their parent
proxied.

>Is there any better solution than cache peers for that?

maybe try to rephrase your status and requirements.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Holmes, what kind of school did you study to be a detective?
- Elementary, Watkins.  -- Daffy Duck & Porky Pig


From ngtech1ltd at gmail.com  Thu Apr 15 08:37:23 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 15 Apr 2021 11:37:23 +0300
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
Message-ID: <002301d731d2$8f0d5db0$ad281910$@gmail.com>

I don?t know your use case that well but maybe another proxy can do that for you.
I wrote a haproxy routing config by username sometime ago:
https://gist.github.com/elico/405f0608e60910fc9ea119e22e1ffd07

It's very simple and worth a shot.
Let me know if it might be good for you.

All The Bests,
Eliezer


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of koshik moshik
Sent: Sunday, April 11, 2021 12:04 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Cache Peers and traffic handling

Hello, 

I am trying to run a Squid proxy Server witth about 5000 cache peers. I am running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16. 

Could you tell me what else is needed / not needed in my squid.config? I am encountering a high CPU usage and would like to create a very efficient proxy server. 

Down below you can find my squid.config(I deleted the other cache_peer lines):
-----------
http_port 3128
dns_v4_first on
acl SSL_ports port 1-65535
acl Safe_ports port 1-65535
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/.htpasswd
auth_param basic children 5
auth_param basic realm Squid Basic Authentication
auth_param basic credentialsttl 5 hours
acl password proxy_auth REQUIRED
http_access allow password
#http_access deny all
cache allow all
never_direct allow all
ident_access deny all




cache_mem 1 GB
maximum_object_size_in_memory 16 MB




# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#Rules to anonymize http headers
forwarded_for off
request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access User-Agent allow all
request_header_access Cookie allow all
request_header_access All deny all




#
# Add any of your own refresh_pattern entries above these.
#
#refresh_pattern ^ftp:           1440    20%     10080
#refresh_pattern ^gopher:        1440    0%      1440
#refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
#refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
#refresh_pattern .               0       20%     4320

################################
acl me proxy_auth ye-1
cache_peer http://my.proxy.com/ parent 31280 login=user1:password1 no-query name=a1
cache_peer_access a1 allow me
cache_peer_access a1 deny all



From kbvz at ymail.com  Thu Apr 15 10:25:36 2021
From: kbvz at ymail.com (Stephane Simon)
Date: Thu, 15 Apr 2021 10:25:36 +0000 (UTC)
Subject: [squid-users] ssl bump Cannot create /var/lib/squid/ssl_db
References: <748416255.3679640.1618482336183.ref@mail.yahoo.com>
Message-ID: <748416255.3679640.1618482336183@mail.yahoo.com>

Hello,
I'm trying to configure?Intercept HTTPS CONNECT messages with SSL-Bump?in redhat 8
with help of:
https://blog.microlinux.fr/squid-https-centos-7/

https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit#Intercept_HTTPS_CONNECT_messages_with_SSL-Bump

https://support.kaspersky.com/KWTS/6.1/en-US/166244.htm


But when i write command "/usr/lib64/squid/security_file_certgen -c -s /var/lib/squid/ssl_db -M 20MB"
i receive "/usr/lib64/squid/security_file_certgen: Cannot create /var/lib/squid/ssl_db"i don't find answers..
before i whrite this commande:semanage fcontext -a -e /var/spool/squid /var/lib/squid

and? when i check"matchpathcon /var/lib/squid/
/var/lib/squid? system_u:object_r:squid_cache_t:s0"

Anyone have an idea perhaps ?
thks a lot
kbvz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210415/7ffdb901/attachment.htm>

From mr.miroslaw.malinowski at gmail.com  Thu Apr 15 12:03:46 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Thu, 15 Apr 2021 13:03:46 +0100
Subject: [squid-users] allow update domain and block everything else
Message-ID: <CAJtdwus1rtEkaW1xrTfwRrU_MQKfHstUuObrBh4jitMrVAYK9w@mail.gmail.com>

Hi,

I'm trying to use Opnsense built-in squid config to set up a transparent
proxy for server updates and block everything else.
In GUI they use url_regex for whitelist and blacklist, when I simple per
domain whitelist and blacklist it's working as expected, e.g.
# ACL - Whitelist - User defined (whiteList)
acl whiteList url_regex archive\.ubuntu\.com
# ACL - Blacklist - User defined (blackList)
acl blackList url_regex packages\.gitlab\.com
# ACL list (Allow) whitelist
http_access allow whiteList
# ACL list (Deny) blacklist
http_access deny blackList

However, when I do wildcard in blacklist I also get all https domain
blocked even when I've tried to explicitly allow it with
https://archive\.ubuntu\.com
, e.g.
# ACL - Whitelist - User defined (whiteList)
acl whiteList url_regex archive\.ubuntu\.com
# ACL - Blacklist - User defined (blackList)
acl blackList url_regex .*
# ACL list (Allow) whitelist
http_access allow whiteList
# ACL list (Deny) blacklist
http_access deny blackList

I get:
Err:7 https://repos.influxdata.com/ubuntu focal InRelease
 403  Forbidden [IP: 52.84.95.46 443]

What I'm trying to say is with blacklist as . is blocking all https traffic
even if whitelisted, is this an expected behaviour or I'm doing something
wrong or it can't be done with url_regex and I should do it at backend
manually.

My config:
#
# Automatic generated configuration for Squid.
# Do not edit this file manually.
#


# Setup transparent mode listeners on loopback interfaces
http_port 127.0.0.1:3128 intercept ssl-bump cert=/var/squid/ssl/ca.pem
dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
http_port [::1]:3128 intercept ssl-bump cert=/var/squid/ssl/ca.pem
dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
https_port 127.0.0.1:3129 intercept ssl-bump cert=/var/squid/ssl/ca.pem
dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
https_port [::1]:3129 intercept ssl-bump cert=/var/squid/ssl/ca.pem
dynamic_cert_mem_cache_size=10MB generate-host-certificates=on

# Setup regular listeners configuration
http_port 172.16.230.252:3128  ssl-bump cert=/var/squid/ssl/ca.pem
dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
http_port 172.16.230.254:3128  ssl-bump cert=/var/squid/ssl/ca.pem
dynamic_cert_mem_cache_size=10MB generate-host-certificates=on

# setup ssl re-cert
sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s
/var/squid/ssl_crtd -M 10MB
sslcrtd_children 5

tls_outgoing_options options=NO_TLSv1
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

# setup ssl bump acl's
acl bump_step1 at_step SslBump1
acl bump_step2 at_step SslBump2
acl bump_step3 at_step SslBump3
acl bump_nobumpsites ssl::server_name
"/usr/local/etc/squid/nobumpsites.acl"

# configure bump
ssl_bump peek bump_step1 all
ssl_bump peek bump_step2 bump_nobumpsites
ssl_bump splice bump_step3 bump_nobumpsites
ssl_bump stare bump_step2
ssl_bump bump bump_step3

sslproxy_cert_error deny all

acl ftp proto FTP
http_access allow ftp


# Setup ftp proxy

# Rules allowing access from your local networks.
# Generated list of (internal) IP networks from where browsing
# should be allowed. (Allow interface subnets).
acl localnet src <net>/24 # Possible internal network (interfaces v4)
# Default allow for local-link and private networks
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

# ACL - Allow localhost for PURGE cache if enabled
acl PURGE method PURGE
http_access allow localhost PURGE
http_access deny PURGE

# ACL lists
# ACL - Whitelist - User defined (whiteList)
acl whiteList url_regex packages\.wazuh\.com
acl whiteList url_regex archive\.ubuntu\.com
acl whiteList url_regex security\.ubuntu\.com
acl whiteList url_regex repos\.influxdata\.com

# ACL - Blacklist - User defined (blackList)
acl blackList url_regex .*

# ACL - Remote fetched Blacklist (remoteblacklist)

# ACL - Block browser/user-agent - User defined (browser)

# ACL - SSL ports, default are configured in config.xml
# Configured SSL ports (if defaults are not listed, then they have been
removed from the configuration!):
acl SSL_ports port 443 # https

# Default Safe ports are now defined in config.xml
# Configured Safe ports (if defaults are not listed, then they have been
removed from the configuration!):
# ACL - Safe_ports
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

# ICAP SETTINGS
# disable icap
icap_enable off

# Pre-auth plugins
include /usr/local/etc/squid/pre-auth/*.conf

# Authentication Settings

# ACL list (Allow) whitelist
http_access allow whiteList

#
# ACL list (Deny) blacklist
http_access deny blackList

# Google Suite Filter

# YouTube Filter

# Deny requests to certain unsafe ports

http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
http_access deny to_localhost

# Auth plugins
include /usr/local/etc/squid/auth/*.conf

#
# Access Permission configuration:
#
# Deny request from unauthorized clients

#
# ACL - localnet - default these include ranges from selected interfaces
(Allow local subnets)
http_access allow localnet

# ACL - localhost
http_access allow localhost

# Deny all other access to this proxy
http_access deny all
# Post-auth plugins
include /usr/local/etc/squid/post-auth/*.conf

# Caching settings
cache_mem 1000 MB
maximum_object_size 200 MB
cache_replacement_policy heap LFUDA
cache_dir ufs /var/squid/cache 100000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

#
# Add any of your own refresh_pattern entries above these.
#

# Linux package cache:
refresh_pattern pkg\.tar\.xz$   0       20%     4320 refresh-ims
refresh_pattern d?rpm$          0       20%     4320 refresh-ims
refresh_pattern deb$            0       20%     4320 refresh-ims
refresh_pattern udeb$           0       20%     4320 refresh-ims
refresh_pattern Packages\.bz2$  0       20%     4320 refresh-ims
refresh_pattern Sources\.bz2$   0       20%     4320 refresh-ims
refresh_pattern Release\.gpg$   0       20%     4320 refresh-ims
refresh_pattern Release$        0       20%     4320 refresh-ims
# http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)     4320
80% 129600 reload-into-ims
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd) 4320
80% 129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)       4320
80% 129600 reload-into-ims

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

# Squid Options
# dns_v4_first reverses the order of preference to make Squid contact
dual-stack websites over IPv4 first
dns_v4_first on
pinger_enable off
access_log stdio:/var/log/squid/access.log squid
cache_store_log stdio:/var/log/squid/store.log
# URI hanlding with Whitespaces (default=strip)
uri_whitespace strip
# X-Forwarded header handling (default=on)
forwarded_for on
# Disable squid logfile rotate to use system defaults
logfile_rotate 0
# Define visible email
cache_mgr admin at localhost.local
error_directory /usr/local/etc/squid/errors/local

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210415/2777313d/attachment.htm>

From rousskov at measurement-factory.com  Thu Apr 15 17:23:51 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 15 Apr 2021 13:23:51 -0400
Subject: [squid-users] Cache Peers and traffic handling
In-Reply-To: <002301d731d2$8f0d5db0$ad281910$@gmail.com>
References: <CAPeL7PFYQ+wnaFBt0xsKTsDi71zMuhuAhiQ+AnHiy_S4ov3_+w@mail.gmail.com>
 <002301d731d2$8f0d5db0$ad281910$@gmail.com>
Message-ID: <f732de0a-3a55-ed90-3e3e-d65798a2862a@measurement-factory.com>

On 4/15/21 4:37 AM, Eliezer Croitoru wrote:
> I don?t know your use case that well but maybe another proxy can do that for you.
> I wrote a haproxy routing config by username sometime ago:
> https://gist.github.com/elico/405f0608e60910fc9ea119e22e1ffd07

Just to clarify: The above haproxy configuration is conceptually the
same as the problematic Squid configuration -- static lists of users,
ACLs, and cache peers. I do not know whether haproxy scales better than
Squid as far as handling 5000 cache peers are concerned.

Alex.



> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of koshik moshik
> Sent: Sunday, April 11, 2021 12:04 AM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Cache Peers and traffic handling
> 
> Hello, 
> 
> I am trying to run a Squid proxy Server witth about 5000 cache peers. I am running a dedicated server with 6 cores and 32GB RAM on Ubuntu 16. 
> 
> Could you tell me what else is needed / not needed in my squid.config? I am encountering a high CPU usage and would like to create a very efficient proxy server. 
> 
> Down below you can find my squid.config(I deleted the other cache_peer lines):
> -----------
> http_port 3128
> dns_v4_first on
> acl SSL_ports port 1-65535
> acl Safe_ports port 1-65535
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/.htpasswd
> auth_param basic children 5
> auth_param basic realm Squid Basic Authentication
> auth_param basic credentialsttl 5 hours
> acl password proxy_auth REQUIRED
> http_access allow password
> #http_access deny all
> cache allow all
> never_direct allow all
> ident_access deny all
> 
> 
> 
> 
> cache_mem 1 GB
> maximum_object_size_in_memory 16 MB
> 
> 
> 
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> #Rules to anonymize http headers
> forwarded_for off
> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access WWW-Authenticate allow all
> request_header_access Proxy-Authorization allow all
> request_header_access Proxy-Authenticate allow all
> request_header_access Cache-Control allow all
> request_header_access Content-Encoding allow all
> request_header_access Content-Length allow all
> request_header_access Content-Type allow all
> request_header_access Date allow all
> request_header_access Expires allow all
> request_header_access Host allow all
> request_header_access If-Modified-Since allow all
> request_header_access Last-Modified allow all
> request_header_access Location allow all
> request_header_access Pragma allow all
> request_header_access Accept allow all
> request_header_access Accept-Charset allow all
> request_header_access Accept-Encoding allow all
> request_header_access Accept-Language allow all
> request_header_access Content-Language allow all
> request_header_access Mime-Version allow all
> request_header_access Retry-After allow all
> request_header_access Title allow all
> request_header_access Connection allow all
> request_header_access Proxy-Connection allow all
> request_header_access User-Agent allow all
> request_header_access Cookie allow all
> request_header_access All deny all
> 
> 
> 
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> #refresh_pattern ^ftp:           1440    20%     10080
> #refresh_pattern ^gopher:        1440    0%      1440
> #refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> #refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> #refresh_pattern .               0       20%     4320
> 
> ################################
> acl me proxy_auth ye-1
> cache_peer http://my.proxy.com/ parent 31280 login=user1:password1 no-query name=a1
> cache_peer_access a1 allow me
> cache_peer_access a1 deny all
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ygreenfield at kewsystems.com  Thu Apr 15 18:40:25 2021
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Thu, 15 Apr 2021 14:40:25 -0400
Subject: [squid-users] How to see running configuration paramters
Message-ID: <C78E7BBAD2A44D989DA13F722F1A89E3@OhrSomayach>

Hi,
 
How can one view the current values of configuration paramters in a running
squid?  Is there a way to do so? Thanks!
 
Yosi
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210415/ec69efb4/attachment.htm>

From rousskov at measurement-factory.com  Thu Apr 15 19:57:08 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 15 Apr 2021 15:57:08 -0400
Subject: [squid-users] How to see running configuration paramters
In-Reply-To: <C78E7BBAD2A44D989DA13F722F1A89E3@OhrSomayach>
References: <C78E7BBAD2A44D989DA13F722F1A89E3@OhrSomayach>
Message-ID: <32722463-7413-a8ef-6dc8-27d4c592e89a@measurement-factory.com>

On 4/15/21 2:40 PM, Yosi Greenfield wrote:
 ?
> How can one view the current values of configuration paramters in a
> running squid?? Is there a way to do so? Thanks!

The closest you can get is probably via the Cache Manager interface:

  squidclient mgr:config
 ?
Alex.


From mr.miroslaw.malinowski at gmail.com  Thu Apr 15 20:52:03 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Thu, 15 Apr 2021 21:52:03 +0100
Subject: [squid-users] allow update domain and block everything else
In-Reply-To: <CAJtdwus1rtEkaW1xrTfwRrU_MQKfHstUuObrBh4jitMrVAYK9w@mail.gmail.com>
References: <CAJtdwus1rtEkaW1xrTfwRrU_MQKfHstUuObrBh4jitMrVAYK9w@mail.gmail.com>
Message-ID: <CAJtdwuuDBTacmqtoOC7=Ytn=rmuKJGvWWLqHFmWqhb+6qu9Txw@mail.gmail.com>

I've found a resolution using a bit better regex:

acl blackList url_regex ^https?:\/\/.*$

looking at the debug it doing exactly what I wanted, however, I now have a
different issue how to handle a 302 MOVED when the move is to a different
domain, e.g. packages.gitlab.com are moved to d20rj4el6vkp4c.cloudfront.net.
Is squid stateful in a way that it's able to remember those packets are
coming from the same session? What would be the best way to resolve the
issue other than just keep adding domain if a thing like this happens.


Thanks


On Thu, Apr 15, 2021 at 1:03 PM Miroslaw Malinowski <
mr.miroslaw.malinowski at gmail.com> wrote:

> Hi,
>
> I'm trying to use Opnsense built-in squid config to set up a transparent
> proxy for server updates and block everything else.
> In GUI they use url_regex for whitelist and blacklist, when I simple per
> domain whitelist and blacklist it's working as expected, e.g.
> # ACL - Whitelist - User defined (whiteList)
> acl whiteList url_regex archive\.ubuntu\.com
> # ACL - Blacklist - User defined (blackList)
> acl blackList url_regex packages\.gitlab\.com
> # ACL list (Allow) whitelist
> http_access allow whiteList
> # ACL list (Deny) blacklist
> http_access deny blackList
>
> However, when I do wildcard in blacklist I also get all https domain
> blocked even when I've tried to explicitly allow it with https://archive\.ubuntu\.com
> , e.g.
> # ACL - Whitelist - User defined (whiteList)
> acl whiteList url_regex archive\.ubuntu\.com
> # ACL - Blacklist - User defined (blackList)
> acl blackList url_regex .*
> # ACL list (Allow) whitelist
> http_access allow whiteList
> # ACL list (Deny) blacklist
> http_access deny blackList
>
> I get:
> Err:7 https://repos.influxdata.com/ubuntu focal InRelease
>  403  Forbidden [IP: 52.84.95.46 443]
>
> What I'm trying to say is with blacklist as . is blocking all https
> traffic even if whitelisted, is this an expected behaviour or I'm doing
> something wrong or it can't be done with url_regex and I should do it at
> backend manually.
>
> My config:
> #
> # Automatic generated configuration for Squid.
> # Do not edit this file manually.
> #
>
>
> # Setup transparent mode listeners on loopback interfaces
> http_port 127.0.0.1:3128 intercept ssl-bump cert=/var/squid/ssl/ca.pem
> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
> http_port [::1]:3128 intercept ssl-bump cert=/var/squid/ssl/ca.pem
> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
> https_port 127.0.0.1:3129 intercept ssl-bump cert=/var/squid/ssl/ca.pem
> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
> https_port [::1]:3129 intercept ssl-bump cert=/var/squid/ssl/ca.pem
> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>
> # Setup regular listeners configuration
> http_port 172.16.230.252:3128  ssl-bump cert=/var/squid/ssl/ca.pem
> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
> http_port 172.16.230.254:3128  ssl-bump cert=/var/squid/ssl/ca.pem
> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>
> # setup ssl re-cert
> sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s
> /var/squid/ssl_crtd -M 10MB
> sslcrtd_children 5
>
> tls_outgoing_options options=NO_TLSv1
> cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>
> # setup ssl bump acl's
> acl bump_step1 at_step SslBump1
> acl bump_step2 at_step SslBump2
> acl bump_step3 at_step SslBump3
> acl bump_nobumpsites ssl::server_name
> "/usr/local/etc/squid/nobumpsites.acl"
>
> # configure bump
> ssl_bump peek bump_step1 all
> ssl_bump peek bump_step2 bump_nobumpsites
> ssl_bump splice bump_step3 bump_nobumpsites
> ssl_bump stare bump_step2
> ssl_bump bump bump_step3
>
> sslproxy_cert_error deny all
>
> acl ftp proto FTP
> http_access allow ftp
>
>
> # Setup ftp proxy
>
> # Rules allowing access from your local networks.
> # Generated list of (internal) IP networks from where browsing
> # should be allowed. (Allow interface subnets).
> acl localnet src <net>/24 # Possible internal network (interfaces v4)
> # Default allow for local-link and private networks
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
>
> # ACL - Allow localhost for PURGE cache if enabled
> acl PURGE method PURGE
> http_access allow localhost PURGE
> http_access deny PURGE
>
> # ACL lists
> # ACL - Whitelist - User defined (whiteList)
> acl whiteList url_regex packages\.wazuh\.com
> acl whiteList url_regex archive\.ubuntu\.com
> acl whiteList url_regex security\.ubuntu\.com
> acl whiteList url_regex repos\.influxdata\.com
>
> # ACL - Blacklist - User defined (blackList)
> acl blackList url_regex .*
>
> # ACL - Remote fetched Blacklist (remoteblacklist)
>
> # ACL - Block browser/user-agent - User defined (browser)
>
> # ACL - SSL ports, default are configured in config.xml
> # Configured SSL ports (if defaults are not listed, then they have been
> removed from the configuration!):
> acl SSL_ports port 443 # https
>
> # Default Safe ports are now defined in config.xml
> # Configured Safe ports (if defaults are not listed, then they have been
> removed from the configuration!):
> # ACL - Safe_ports
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
>
> # ICAP SETTINGS
> # disable icap
> icap_enable off
>
> # Pre-auth plugins
> include /usr/local/etc/squid/pre-auth/*.conf
>
> # Authentication Settings
>
> # ACL list (Allow) whitelist
> http_access allow whiteList
>
> #
> # ACL list (Deny) blacklist
> http_access deny blackList
>
> # Google Suite Filter
>
> # YouTube Filter
>
> # Deny requests to certain unsafe ports
>
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
>
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> http_access deny to_localhost
>
> # Auth plugins
> include /usr/local/etc/squid/auth/*.conf
>
> #
> # Access Permission configuration:
> #
> # Deny request from unauthorized clients
>
> #
> # ACL - localnet - default these include ranges from selected interfaces
> (Allow local subnets)
> http_access allow localnet
>
> # ACL - localhost
> http_access allow localhost
>
> # Deny all other access to this proxy
> http_access deny all
> # Post-auth plugins
> include /usr/local/etc/squid/post-auth/*.conf
>
> # Caching settings
> cache_mem 1000 MB
> maximum_object_size 200 MB
> cache_replacement_policy heap LFUDA
> cache_dir ufs /var/squid/cache 100000 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/squid/cache
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
>
> # Linux package cache:
> refresh_pattern pkg\.tar\.xz$   0       20%     4320 refresh-ims
> refresh_pattern d?rpm$          0       20%     4320 refresh-ims
> refresh_pattern deb$            0       20%     4320 refresh-ims
> refresh_pattern udeb$           0       20%     4320 refresh-ims
> refresh_pattern Packages\.bz2$  0       20%     4320 refresh-ims
> refresh_pattern Sources\.bz2$   0       20%     4320 refresh-ims
> refresh_pattern Release\.gpg$   0       20%     4320 refresh-ims
> refresh_pattern Release$        0       20%     4320 refresh-ims
> # http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)
> <http://microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cesd)>
>     4320 80% 129600 reload-into-ims
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)
> <http://windowsupdate.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cesd)>
> 4320 80% 129600 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)
> <http://windows.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cesd)>
>       4320 80% 129600 reload-into-ims
>
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
> # Squid Options
> # dns_v4_first reverses the order of preference to make Squid contact
> dual-stack websites over IPv4 first
> dns_v4_first on
> pinger_enable off
> access_log stdio:/var/log/squid/access.log squid
> cache_store_log stdio:/var/log/squid/store.log
> # URI hanlding with Whitespaces (default=strip)
> uri_whitespace strip
> # X-Forwarded header handling (default=on)
> forwarded_for on
> # Disable squid logfile rotate to use system defaults
> logfile_rotate 0
> # Define visible email
> cache_mgr admin at localhost.local
> error_directory /usr/local/etc/squid/errors/local
>
> Thanks
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210415/de47b848/attachment.htm>

From squid3 at treenet.co.nz  Sat Apr 17 22:28:44 2021
From: squid3 at treenet.co.nz (=?UTF-8?B?4oCqQW1vcyBKZWZmcmllc+KArA==?=)
Date: Sun, 18 Apr 2021 10:28:44 +1200
Subject: [squid-users] allow update domain and block everything else
References: <CAJtdwuuDBTacmqtoOC7=Ytn=rmuKJGvWWLqHFmWqhb+6qu9Txw@mail.gmail.com>
Message-ID: <-5igzfvordg39-z1drws74hy16fa2b7jcvsjq7-p7ic6vm92z0ufn5qgla9gvn5-oo44boqeb5f9-mhicam-61t5ys-ms2k4xhnbxgp-rmvcjy-c178ug-h02y0abuk36o28o3o2-e3qrb3-v9uoqpne51ta.1618698524908@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210418/be1750f8/attachment.htm>

From ngtech1ltd at gmail.com  Mon Apr 19 09:07:08 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 19 Apr 2021 12:07:08 +0300
Subject: [squid-users] All Adaptation ICAPs go down at the same time
In-Reply-To: <CAD=NrcALoEGqctFUpuTZcfkpEbsS=KU4QqXG+LAVd-XtJvfnXg@mail.gmail.com>
References: <CAD=NrcALoEGqctFUpuTZcfkpEbsS=KU4QqXG+LAVd-XtJvfnXg@mail.gmail.com>
Message-ID: <000001d734fb$6081bd70$21853850$@gmail.com>

Hey Roie,

 

>From the output I assume it?s a dns resolution issue.

In the past I remember that Docker was updating the hosts file with the relevant names but  it?s not working the same way now.

Currently Docker is using a local network dns service which is being accessed via 127.0.0.53.

>From I remember Squid is resolving the icap service name only at startup or reload.

Lately Alex published a testable patch that might fix specific issues with icap services which are resolved by dns. ( sorry I don?t remember the bug report)

I assume you can try to test this patch first.

If these services are static to some degree you might be able to create a script that updates the hosts file and reload squid on each change.

When using the hosts file it?s possible that some issues will disappear.


There is also another possibility which is a malformed ICAP response or wrong sessions handling which cause this issue.

You might be able to use tcpdump from either the host or the container side to capture traffic when these goes down.

Depends on your preference of debug level you might even be able to debug specific debug_options like for ICAP services
and/or requests to the degree you might be able to see what happens on the basic level of the ICAP encapsulation.

If you really need help with a diagnosis and a solution you might be able to use Alex and the measurement factory.



All The Bests,

Eliezer

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of roie rachamim
Sent: Monday, April 12, 2021 12:54 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] All Adaptation ICAPs go down at the same time

 

Hi,

 

Our setup includes squid that runs in docker container with several ICAP servers in additional containers.

>From time to time we see in cache.log the following messages:
2021/04/12 00:22:39| optional ICAP service is down after an options fetch failure: icap://icap1.proxy:14590/censor [down,!opt]
2021/04/12 00:22:39| optional ICAP service is down after an options fetch failure: icap://icap2.proxy:1344/request [down,!opt]
2021/04/12 00:22:39| optional ICAP service is down after an options fetch failure: icap://icap3.proxy:14590/response [down,!opt]

2021/04/12 06:10:45| optional ICAP service is down after an options fetch failure: icap://icap1.proxy:14590/censor [down,!opt]
2021/04/12 06:10:45| optional ICAP service is down after an options fetch failure: icap://icap2.proxy:1344/request [down,!opt]
2021/04/12 06:10:45| optional ICAP service is down after an options fetch failure: icap://icap3.proxy:14590/response [down,!opt]

 

We're trying to understand why it happens to all ICAPs at once. This happens in 4.14 and in 5.0.4

Any thoughts about what might cause this ?

Many Thanks,

Roie

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210419/4513191b/attachment.htm>

From ngtech1ltd at gmail.com  Mon Apr 19 09:41:47 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 19 Apr 2021 12:41:47 +0300
Subject: [squid-users] Is there a way to bind squid's outbound traffice
 to a specific network interface
In-Reply-To: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>
References: <CAEF1h+VrD=0FUJ9JBYEH+ryfCKn-9upDtfG9r-42b5dvrQagnQ@mail.gmail.com>
Message-ID: <000b01d73500$37c10df0$a74329d0$@gmail.com>

It might be possible to use the tcp_outgoing_address for this purpose but it?s not clear

What your setup technically look like and what is preventing the browser to do as you please.


Eliezer

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cary Lewis
Sent: Monday, April 12, 2021 12:58 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Is there a way to bind squid's outbound traffice to a specific network interface

 

I want to be able to bypass a vpn while using a web browser, so I need to be able to configure squid to always use a specific outbound interface. 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210419/e91d0d91/attachment.htm>

From mr.miroslaw.malinowski at gmail.com  Wed Apr 21 16:48:53 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Wed, 21 Apr 2021 17:48:53 +0100
Subject: [squid-users] allow request to cloudfront after 302 redirection.
Message-ID: <CAJtdwusCo2m=yrg6eBfw-fqR3YPN16CjAceZ-CxgXvz4h_6kwQ@mail.gmail.com>

Is it possible to create a whitelist that allows cloudfront 302
redirections, e.g. gitlab is using cloudfront as CDN and when we whitelist
package.gitlab.com the URL is redirected (302) to
https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb?t=1619023239_a63698472b6bebeaee980e7c030632d97a29c15d.
I could whitelist a whole .cloudfront.net domain or url_regex, but what I
would like to achieve, I don't know if possible, is a chain of events like:
If packages.gitlab.com return 302 Location .cloudfront, then allow
https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb?t=1619023239_a63698472b6bebeaee980e7c030632d97a29c
request.
I've been playing around with http_reply_access and rep_headers, but I can
only go as far as allow replay of the first request to package.gitlab.com,
but then a GET to cloudfront is blocked anyway as it's not on our whitelist.
e.g.
1619022938.916   423 172.16.230.237 NONE/200 0 CONNECT 54.153.54.194:443 -
ORIGINAL_DST/54.153.54.194 -
1619022939.074   153 172.16.230.237 TCP_MISS/302 758 GET
https://packages.gitlab.com/gitlab/gitlab-ee/packages/ubuntu/bionic/gitlab-ee_11.0.1-ee.0_amd64.deb/download.deb
- ORIGINAL_DST/54.153.54.194 text/html
1619022939.108    20 172.16.230.237 NONE/200 0 CONNECT 52.84.90.34:443 -
ORIGINAL_DST/52.84.90.34 -
1619022939.114     2 172.16.230.237 TCP_DENIED/403 19053 GET
https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb?
- HIER_NONE/- text/html

Thanks,
Mirek
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210421/9650a8db/attachment.htm>

From rousskov at measurement-factory.com  Wed Apr 21 17:48:37 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 21 Apr 2021 13:48:37 -0400
Subject: [squid-users] allow request to cloudfront after 302 redirection.
In-Reply-To: <CAJtdwusCo2m=yrg6eBfw-fqR3YPN16CjAceZ-CxgXvz4h_6kwQ@mail.gmail.com>
References: <CAJtdwusCo2m=yrg6eBfw-fqR3YPN16CjAceZ-CxgXvz4h_6kwQ@mail.gmail.com>
Message-ID: <00a2f0e6-0acb-0f8a-b867-74efe1fff59f@measurement-factory.com>

On 4/21/21 12:48 PM, Miroslaw Malinowski wrote:
> Is it possible to create a whitelist that allows cloudfront 302
> redirections, e.g. gitlab is using cloudfront as CDN and when we
> whitelist package.gitlab.com the URL is redirected (302) to
> https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb?t=1619023239_a63698472b6bebeaee980e7c030632d97a29c15d


Yes, it is possible to allow future requests to Location-listed URLs,
but since we are talking about two (or more) independent HTTP
transactions, on two (or more) TCP connections, you will need to store
the allowed Location values (at least) somewhere, maintain that storage
(e.g., remove stale entries), and (optionally) determine whether the
request for an allowed cloudfront URL came from the same user agent as
the gitlab request that was redirected to that URL.

Storing, maintenance, and checking of allowed Locations/etc. can be done
using external ACLs and/or eCAP/ICAP adaptation services. It cannot be
reliably done using built-in ACLs alone AFAICT.


HTH,

Alex.


> I could whitelist a whole .cloudfront.net <http://cloudfront.net> domain
> or url_regex, but what I would like to achieve, I don't know if
> possible, is a chain of events like:
> If packages.gitlab.com <http://packages.gitlab.com> return 302 Location
> .cloudfront, then allow
> https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb?t=1619023239_a63698472b6bebeaee980e7c030632d97a29c
> <https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb?t=1619023239_a63698472b6bebeaee980e7c030632d97a29c>
> request.
> I've been playing around with http_reply_access and rep_headers, but I
> can only go as far as allow replay of the first request to
> package.gitlab.com <http://package.gitlab.com>, but then a GET to
> cloudfront is blocked anyway as it's not on our whitelist.
> e.g.
> 1619022938.916 ? 423 172.16.230.237 NONE/200 0 CONNECT 54.153.54.194:443
> <http://54.153.54.194:443> - ORIGINAL_DST/54.153.54.194
> <http://54.153.54.194> -
> 1619022939.074 ? 153 172.16.230.237 TCP_MISS/302 758 GET
> https://packages.gitlab.com/gitlab/gitlab-ee/packages/ubuntu/bionic/gitlab-ee_11.0.1-ee.0_amd64.deb/download.deb
> <https://packages.gitlab.com/gitlab/gitlab-ee/packages/ubuntu/bionic/gitlab-ee_11.0.1-ee.0_amd64.deb/download.deb>
> - ORIGINAL_DST/54.153.54.194 <http://54.153.54.194> text/html
> 1619022939.108 ? ?20 172.16.230.237 NONE/200 0 CONNECT 52.84.90.34:443
> <http://52.84.90.34:443> - ORIGINAL_DST/52.84.90.34 <http://52.84.90.34> -
> 1619022939.114 ? ? 2 172.16.230.237 TCP_DENIED/403 19053 GET
> https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb <https://d20rj4el6vkp4c.cloudfront.net/7/11/ubuntu/package_files/35938.deb>?
> - HIER_NONE/- text/html



From mr.miroslaw.malinowski at gmail.com  Wed Apr 21 18:15:15 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Wed, 21 Apr 2021 19:15:15 +0100
Subject: [squid-users] allow update domain and block everything else
In-Reply-To: <CAJtdwuuDBTacmqtoOC7=Ytn=rmuKJGvWWLqHFmWqhb+6qu9Txw@mail.gmail.com>
References: <CAJtdwus1rtEkaW1xrTfwRrU_MQKfHstUuObrBh4jitMrVAYK9w@mail.gmail.com>
 <CAJtdwuuDBTacmqtoOC7=Ytn=rmuKJGvWWLqHFmWqhb+6qu9Txw@mail.gmail.com>
Message-ID: <CAJtdwutjDWYN19QHk2GcEBTmvqnx9DCAC1gbXGrLQeqqRzCmKQ@mail.gmail.com>

Sorry, I haven't seen the reply as it's been attached as an attachment and
posted a similar question about 302, but probably I know the answer already
as it's not stateful inspection any redirection to a new domain is a new
request that has to go over ACL again. I thought about using
http_reply_access but again it's creating a whole new ACL that every reply
would have to go over not just that one. Is there a way to use http_access
& http_reply_access together, like http_access allow whitelist & http_reply
allow whitelist?
Mirek

> There is a built-in ACL called "all" which does what you defined for the
regex "blacklist" to do.
> As for sessions. No Squid follows HTTP which is stateless. You can
configure it though. setup an ext_session_acl helper for active mode
sessions that start when a 302 response comes back. you should have some
other ACL to separately whitelist the sites normally blocked, but can open
with a session.
> Amos

On Thu, Apr 15, 2021 at 9:52 PM Miroslaw Malinowski <
mr.miroslaw.malinowski at gmail.com> wrote:

> I've found a resolution using a bit better regex:
>
> acl blackList url_regex ^https?:\/\/.*$
>
> looking at the debug it doing exactly what I wanted, however, I now have a
> different issue how to handle a 302 MOVED when the move is to a different
> domain, e.g. packages.gitlab.com are moved to
> d20rj4el6vkp4c.cloudfront.net. Is squid stateful in a way that it's able
> to remember those packets are coming from the same session? What would be
> the best way to resolve the issue other than just keep adding domain if a
> thing like this happens.
>
>
> Thanks
>
>
> On Thu, Apr 15, 2021 at 1:03 PM Miroslaw Malinowski <
> mr.miroslaw.malinowski at gmail.com> wrote:
>
>> Hi,
>>
>> I'm trying to use Opnsense built-in squid config to set up a transparent
>> proxy for server updates and block everything else.
>> In GUI they use url_regex for whitelist and blacklist, when I simple per
>> domain whitelist and blacklist it's working as expected, e.g.
>> # ACL - Whitelist - User defined (whiteList)
>> acl whiteList url_regex archive\.ubuntu\.com
>> # ACL - Blacklist - User defined (blackList)
>> acl blackList url_regex packages\.gitlab\.com
>> # ACL list (Allow) whitelist
>> http_access allow whiteList
>> # ACL list (Deny) blacklist
>> http_access deny blackList
>>
>> However, when I do wildcard in blacklist I also get all https domain
>> blocked even when I've tried to explicitly allow it with https://archive\.ubuntu\.com
>> , e.g.
>> # ACL - Whitelist - User defined (whiteList)
>> acl whiteList url_regex archive\.ubuntu\.com
>> # ACL - Blacklist - User defined (blackList)
>> acl blackList url_regex .*
>> # ACL list (Allow) whitelist
>> http_access allow whiteList
>> # ACL list (Deny) blacklist
>> http_access deny blackList
>>
>> I get:
>> Err:7 https://repos.influxdata.com/ubuntu focal InRelease
>>  403  Forbidden [IP: 52.84.95.46 443]
>>
>> What I'm trying to say is with blacklist as . is blocking all https
>> traffic even if whitelisted, is this an expected behaviour or I'm doing
>> something wrong or it can't be done with url_regex and I should do it at
>> backend manually.
>>
>> My config:
>> #
>> # Automatic generated configuration for Squid.
>> # Do not edit this file manually.
>> #
>>
>>
>> # Setup transparent mode listeners on loopback interfaces
>> http_port 127.0.0.1:3128 intercept ssl-bump cert=/var/squid/ssl/ca.pem
>> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>> http_port [::1]:3128 intercept ssl-bump cert=/var/squid/ssl/ca.pem
>> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>> https_port 127.0.0.1:3129 intercept ssl-bump cert=/var/squid/ssl/ca.pem
>> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>> https_port [::1]:3129 intercept ssl-bump cert=/var/squid/ssl/ca.pem
>> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>>
>> # Setup regular listeners configuration
>> http_port 172.16.230.252:3128  ssl-bump cert=/var/squid/ssl/ca.pem
>> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>> http_port 172.16.230.254:3128  ssl-bump cert=/var/squid/ssl/ca.pem
>> dynamic_cert_mem_cache_size=10MB generate-host-certificates=on
>>
>> # setup ssl re-cert
>> sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s
>> /var/squid/ssl_crtd -M 10MB
>> sslcrtd_children 5
>>
>> tls_outgoing_options options=NO_TLSv1
>> cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>>
>> # setup ssl bump acl's
>> acl bump_step1 at_step SslBump1
>> acl bump_step2 at_step SslBump2
>> acl bump_step3 at_step SslBump3
>> acl bump_nobumpsites ssl::server_name
>> "/usr/local/etc/squid/nobumpsites.acl"
>>
>> # configure bump
>> ssl_bump peek bump_step1 all
>> ssl_bump peek bump_step2 bump_nobumpsites
>> ssl_bump splice bump_step3 bump_nobumpsites
>> ssl_bump stare bump_step2
>> ssl_bump bump bump_step3
>>
>> sslproxy_cert_error deny all
>>
>> acl ftp proto FTP
>> http_access allow ftp
>>
>>
>> # Setup ftp proxy
>>
>> # Rules allowing access from your local networks.
>> # Generated list of (internal) IP networks from where browsing
>> # should be allowed. (Allow interface subnets).
>> acl localnet src <net>/24 # Possible internal network (interfaces v4)
>> # Default allow for local-link and private networks
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
>> machines
>>
>> # ACL - Allow localhost for PURGE cache if enabled
>> acl PURGE method PURGE
>> http_access allow localhost PURGE
>> http_access deny PURGE
>>
>> # ACL lists
>> # ACL - Whitelist - User defined (whiteList)
>> acl whiteList url_regex packages\.wazuh\.com
>> acl whiteList url_regex archive\.ubuntu\.com
>> acl whiteList url_regex security\.ubuntu\.com
>> acl whiteList url_regex repos\.influxdata\.com
>>
>> # ACL - Blacklist - User defined (blackList)
>> acl blackList url_regex .*
>>
>> # ACL - Remote fetched Blacklist (remoteblacklist)
>>
>> # ACL - Block browser/user-agent - User defined (browser)
>>
>> # ACL - SSL ports, default are configured in config.xml
>> # Configured SSL ports (if defaults are not listed, then they have been
>> removed from the configuration!):
>> acl SSL_ports port 443 # https
>>
>> # Default Safe ports are now defined in config.xml
>> # Configured Safe ports (if defaults are not listed, then they have been
>> removed from the configuration!):
>> # ACL - Safe_ports
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 21 # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70 # gopher
>> acl Safe_ports port 210 # wais
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl Safe_ports port 280 # http-mgmt
>> acl Safe_ports port 488 # gss-http
>> acl Safe_ports port 591 # filemaker
>> acl Safe_ports port 777 # multiling http
>> acl CONNECT method CONNECT
>>
>> # ICAP SETTINGS
>> # disable icap
>> icap_enable off
>>
>> # Pre-auth plugins
>> include /usr/local/etc/squid/pre-auth/*.conf
>>
>> # Authentication Settings
>>
>> # ACL list (Allow) whitelist
>> http_access allow whiteList
>>
>> #
>> # ACL list (Deny) blacklist
>> http_access deny blackList
>>
>> # Google Suite Filter
>>
>> # YouTube Filter
>>
>> # Deny requests to certain unsafe ports
>>
>> http_access deny !Safe_ports
>> # Deny CONNECT to other than secure SSL ports
>>
>> http_access deny CONNECT !SSL_ports
>>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> http_access deny to_localhost
>>
>> # Auth plugins
>> include /usr/local/etc/squid/auth/*.conf
>>
>> #
>> # Access Permission configuration:
>> #
>> # Deny request from unauthorized clients
>>
>> #
>> # ACL - localnet - default these include ranges from selected interfaces
>> (Allow local subnets)
>> http_access allow localnet
>>
>> # ACL - localhost
>> http_access allow localhost
>>
>> # Deny all other access to this proxy
>> http_access deny all
>> # Post-auth plugins
>> include /usr/local/etc/squid/post-auth/*.conf
>>
>> # Caching settings
>> cache_mem 1000 MB
>> maximum_object_size 200 MB
>> cache_replacement_policy heap LFUDA
>> cache_dir ufs /var/squid/cache 100000 16 256
>>
>> # Leave coredumps in the first cache dir
>> coredump_dir /var/squid/cache
>>
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>>
>> # Linux package cache:
>> refresh_pattern pkg\.tar\.xz$   0       20%     4320 refresh-ims
>> refresh_pattern d?rpm$          0       20%     4320 refresh-ims
>> refresh_pattern deb$            0       20%     4320 refresh-ims
>> refresh_pattern udeb$           0       20%     4320 refresh-ims
>> refresh_pattern Packages\.bz2$  0       20%     4320 refresh-ims
>> refresh_pattern Sources\.bz2$   0       20%     4320 refresh-ims
>> refresh_pattern Release\.gpg$   0       20%     4320 refresh-ims
>> refresh_pattern Release$        0       20%     4320 refresh-ims
>> # http://wiki.squid-cache.org/SquidFaq/WindowsUpdate
>> refresh_pattern -i
>> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)
>> <http://microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cesd)>
>>     4320 80% 129600 reload-into-ims
>> refresh_pattern -i
>> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)
>> <http://windowsupdate.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cesd)>
>> 4320 80% 129600 reload-into-ims
>> refresh_pattern -i
>> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip|esd)
>> <http://windows.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cesd)>
>>       4320 80% 129600 reload-into-ims
>>
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>> # Squid Options
>> # dns_v4_first reverses the order of preference to make Squid contact
>> dual-stack websites over IPv4 first
>> dns_v4_first on
>> pinger_enable off
>> access_log stdio:/var/log/squid/access.log squid
>> cache_store_log stdio:/var/log/squid/store.log
>> # URI hanlding with Whitespaces (default=strip)
>> uri_whitespace strip
>> # X-Forwarded header handling (default=on)
>> forwarded_for on
>> # Disable squid logfile rotate to use system defaults
>> logfile_rotate 0
>> # Define visible email
>> cache_mgr admin at localhost.local
>> error_directory /usr/local/etc/squid/errors/local
>>
>> Thanks
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210421/932a095e/attachment.htm>

From neven.vrenko at gmail.com  Thu Apr 22 09:24:22 2021
From: neven.vrenko at gmail.com (Neven Vrenko)
Date: Thu, 22 Apr 2021 11:24:22 +0200
Subject: [squid-users] Client certificate authentication problem
Message-ID: <CAF7xqN7d3YybfQCoduTcawnCOW8CLazb3Bv9uVh08H_53mBO_Q@mail.gmail.com>

Hello community,

I have a problem which I'm coping with for some time now.
I would like to use client certificate authentication with http_port
command.

As far as I understand the parameter "clientca" should be enough to request
the browser to send/offer client certificate.

In my case this line is fairly simple and looks like this:

> http_port 443 clientca=/path/to/the/CA.pem

However the "clientca" parameter seems to be ignored. Even when I put some
non existing path as "clientca" value, Squid starts normally without error.

Squid version which I'm using is 5.0.5 and output of squid -v can be found
below:

Squid Cache: Version 5.0.5
Service Name: squid

This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions
on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--bindir=/usr/bin'
'--sbindir=/usr/sbin' '--sysconfdir=/etc/squid' '--datadir=/usr/share'
'--includedir=/usr/include' '--libexecdir=/usr/lib/squid'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--localstatedir=/var'
'--disable-dependency-tracking' '--disable-arch-native'
'--enable-ltdl-install' '--enable-storeio=aufs,diskd,ufs,rock'
'--enable-removal-policies=heap,lru' '--enable-icmp' '--enable-delay-pools'
'--enable-esi' '--enable-icap-client'
'--enable-cachemgr-hostname=localhost' '--enable-follow-x-forwarded-for'
'--enable-forw-via-db' '--enable-cache-digests' '--enable-linux-netfilter'
'--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake'
'--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake'
'--with-logdir=/var/log/squid' '--enable-log-daemon-helpers=DB,file'
'--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota'
'--enable-url-rewrite-helpers=fake,LFS' '--enable-security-cert-validators'
'--enable-security-cert-generators' *'--with-openssl'* '--enable-ssl-crtd'
'--enable-snmp' '--enable-stacktraces' '--enable-gnuregex'
'--with-pidfile=/var/run/squid.pid' '--with-default-user=squid'
'--with-aio' '--with-dl' '--with-pthreads' '--with-filedescriptors=16384'
'--enable-wccpv2' '--enable-epoll'

I tried to put user_cert acl in hope that this would trigger certificate
request, but it didn't happen.

> acl cert_authentication user_cert CN MY_CN
> http_access allow cert_authentication

If somebody could point me in the right direction I would be very thankful.

Regards,
Neven

P.S
SELinux is disabled and CA certificate is in PEM format
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210422/106ec5e5/attachment.htm>

From rousskov at measurement-factory.com  Thu Apr 22 13:54:29 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 22 Apr 2021 09:54:29 -0400
Subject: [squid-users] Client certificate authentication problem
In-Reply-To: <CAF7xqN7d3YybfQCoduTcawnCOW8CLazb3Bv9uVh08H_53mBO_Q@mail.gmail.com>
References: <CAF7xqN7d3YybfQCoduTcawnCOW8CLazb3Bv9uVh08H_53mBO_Q@mail.gmail.com>
Message-ID: <d8e4df4c-de9f-0775-4fc4-7db9274eb81b@measurement-factory.com>

On 4/22/21 5:24 AM, Neven Vrenko wrote:
> Hello community,
> 
> I have a problem which I'm coping with for some time now.
> I would like to use client certificate authentication with http_port
> command.
> 
> As far as I understand the parameter "clientca" should be enough to
> request the browser to send/offer client certificate.
> 
> In my case this line is fairly simple and looks like this:
> 
>> http_port 443 clientca=/path/to/the/CA.pem

Today, client certificate authentication only works with TLS. That
implies that you need https_port instead of http_port (and your client
should use TLS to connect to the Squid proxy on that port!).


> However the "clientca" parameter seems to be ignored. Even when I put
> some non existing path as "clientca" value, Squid starts normally
> without error.

That is, IMO, a Squid configuration validation bug. Squid should quit
with a fatal configuration error instead.


HTH,

Alex.


> Squid version which I'm using is 5.0.5 and output of squid -v can be
> found below:
> 
> Squid Cache: Version 5.0.5
> Service Name: squid
> 
> This binary uses OpenSSL 1.0.2k-fips ?26 Jan 2017. For legal
> restrictions on distribution see
> https://www.openssl.org/source/license.html
> <https://www.openssl.org/source/license.html>
> 
> configure options: ?'--prefix=/usr' '--bindir=/usr/bin'
> '--sbindir=/usr/sbin' '--sysconfdir=/etc/squid' '--datadir=/usr/share'
> '--includedir=/usr/include' '--libexecdir=/usr/lib/squid'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--localstatedir=/var'
> '--disable-dependency-tracking' '--disable-arch-native'
> '--enable-ltdl-install' '--enable-storeio=aufs,diskd,ufs,rock'
> '--enable-removal-policies=heap,lru' '--enable-icmp'
> '--enable-delay-pools' '--enable-esi' '--enable-icap-client'
> '--enable-cachemgr-hostname=localhost' '--enable-follow-x-forwarded-for'
> '--enable-forw-via-db' '--enable-cache-digests'
> '--enable-linux-netfilter' '--enable-auth'
> '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake'
> '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake'
> '--with-logdir=/var/log/squid' '--enable-log-daemon-helpers=DB,file'
> '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota'
> '--enable-url-rewrite-helpers=fake,LFS'
> '--enable-security-cert-validators' '--enable-security-cert-generators'
> _'--with-openssl'_ '--enable-ssl-crtd' '--enable-snmp'
> '--enable-stacktraces' '--enable-gnuregex'
> '--with-pidfile=/var/run/squid.pid' '--with-default-user=squid'
> '--with-aio' '--with-dl' '--with-pthreads'
> '--with-filedescriptors=16384' '--enable-wccpv2' '--enable-epoll'
> 
> I tried to put user_cert acl in hope that this would trigger certificate
> request, but it didn't happen.
> 
>> acl cert_authentication user_cert CN MY_CN
>> http_access allow cert_authentication
> 
> If somebody could point me in the right direction I would be very thankful.
> 
> Regards,
> Neven
> 
> P.S
> SELinux is disabled and CA certificate is in PEM format
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From bhall at geosolinc.com  Fri Apr 23 17:28:56 2021
From: bhall at geosolinc.com (Brian Hall)
Date: Fri, 23 Apr 2021 17:28:56 +0000
Subject: [squid-users] Errors Compiling Squid on Windows Server using Cygwin
Message-ID: <BL1PR12MB50481DF21266E09E28EDADC4A7459@BL1PR12MB5048.namprd12.prod.outlook.com>

Hello,

I have searched the previous posts and I can only find a mention of this issue from back in 2018 where the solution was to use 3.5. I would like to use 4.12 instead of older versions because of the security concerns.

The following is the errors the compiler gave me:

In file included from /usr/include/w32api/lm.h:18,

from ext_lm_group_acl.cc:96:
/usr/include/w32api/lmserver.h:29:33: error: 'SERVICE_STATUS_HANDLE' was not declared in this scope; did you mean 'SERVICE_PAUSABLE'?
   29 |   WINBOOL WINAPI SetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwServiceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);
      |                                 ^~~~~~~~~~~~~~~~~~~~~
      |                                 SERVICE_PAUSABLE
/usr/include/w32api/lmserver.h:29:76: error: expected primary-expression before 'dwServiceBits'
   29 |   WINBOOL WINAPI SetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwServiceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);
      |                                                                            ^~~~~~~~~~~~~
/usr/include/w32api/lmserver.h:29:98: error: expected primary-expression before 'bSetBitsOn'
   29 |   WINBOOL WINAPI SetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwServiceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);
      |                                                                                                  ^~~~~~~~~~
/usr/include/w32api/lmserver.h:29:117: error: expected primary-expression before 'bUpdateImmediately'
   29 |   WINBOOL WINAPI SetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwServiceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);
      |                                                                                                                     ^~~~~~~~~~~~~~~~~~
/usr/include/w32api/lmserver.h:29:135: error: expression list treated as compound expression in initializer [-fpermissive]
   29 |   WINBOOL WINAPI SetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwServiceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);
      |                                                                                                                                       ^
In file included from ../../../../compat/compat.h:105,
                 from ../../../../include/squid.h:43,
                 from ext_lm_group_acl.cc:78:
ext_lm_group_acl.cc: In function 'char* GetDomainName()':
ext_lm_group_acl.cc:173:19: error: format '%ld' expects argument of type 'long int', but argument 3 has type 'NTSTATUS' {aka 'int'} [-Werror=format=]
  173 |             debug("OpenPolicy Error: %ld\n", status);
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~
      |                                              |
      |                                              NTSTATUS {aka int}
../../../../compat/debug.h:31:41: note: in definition of macro 'debug'
   31 |                          fprintf(stderr,X); \
      |                                         ^
ext_lm_group_acl.cc:173:40: note: format string is defined here
  173 |             debug("OpenPolicy Error: %ld\n", status);
      |                                      ~~^
      |                                        |
      |                                        long int
      |                                      %d
In file included from ../../../../compat/compat.h:105,
                 from ../../../../include/squid.h:43,
                 from ext_lm_group_acl.cc:78:
ext_lm_group_acl.cc:184:23: error: format '%ld' expects argument of type 'long int', but argument 3 has type 'NTSTATUS' {aka 'int'} [-Werror=format=]
  184 |                 debug("LsaQueryInformationPolicy Error: %ld\n", status);
      |                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~
      |                                                                 |
      |                                                                 NTSTATUS {aka int}
../../../../compat/debug.h:31:41: note: in definition of macro 'debug'
   31 |                          fprintf(stderr,X); \
      |                                         ^
ext_lm_group_acl.cc:184:59: note: format string is defined here
  184 |                 debug("LsaQueryInformationPolicy Error: %ld\n", status);
      |                                                         ~~^
      |                                                           |
      |                                                           long int
      |                                                         %d
In file included from ../../../../compat/compat.h:105,
                 from ../../../../include/squid.h:43,
                 from ext_lm_group_acl.cc:78:
ext_lm_group_acl.cc:214:15: error: format '%ld' expects argument of type 'long int', but argument 3 has type 'DWORD' {aka 'unsigned int'} [-Werror=format=]
  214 |         debug("NetWkstaGetInfo Error: %ld\n", netret);
      |               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  ~~~~~~
      |                                               |
      |                                               DWORD {aka unsigned int}
../../../../compat/debug.h:31:41: note: in definition of macro 'debug'
   31 |                          fprintf(stderr,X); \
      |                                         ^
ext_lm_group_acl.cc:214:41: note: format string is defined here
  214 |         debug("NetWkstaGetInfo Error: %ld\n", netret);
      |                                       ~~^
      |                                         |
      |                                         long int
      |                                       %d
In file included from ../../../../compat/compat.h:105,
                 from ../../../../include/squid.h:43,
                 from ext_lm_group_acl.cc:78:
ext_lm_group_acl.cc: In function 'int main(int, char**)':
ext_lm_group_acl.cc:572:15: error: format '%d' expects argument of type 'int', but argument 4 has type 'size_t' {aka 'long unsigned int'} [-Werror=format=]
  572 |         debug("Got '%s' from Squid (length: %d).\n", buf, strlen(buf));
      |               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~       ~~~~~~~~~~~
      |                                                                 |
      |                                                                 size_t {aka long unsigned int}
../../../../compat/debug.h:31:41: note: in definition of macro 'debug'
   31 |                          fprintf(stderr,X); \
      |                                         ^
ext_lm_group_acl.cc:572:46: note: format string is defined here
  572 |         debug("Got '%s' from Squid (length: %d).\n", buf, strlen(buf));
      |                                             ~^
      |                                              |
      |                                              int
      |                                             %ld
cc1plus: all warnings being treated as errors
make[5]: *** [Makefile:840: ext_lm_group_acl.o] Error 1
make[5]: Leaving directory '/cygdrive/c/new/squid-4.14/src/acl/external/LM_group'
make[4]: *** [Makefile:532: install-recursive] Error 1
make[4]: Leaving directory '/cygdrive/c/new/squid-4.14/src/acl/external'
make[3]: *** [Makefile:1125: install-recursive] Error 1
make[3]: Leaving directory '/cygdrive/c/new/squid-4.14/src/acl'
make[2]: *** [Makefile:7129: install-recursive] Error 1
make[2]: Leaving directory '/cygdrive/c/new/squid-4.14/src'
make[1]: *** [Makefile:7657: install] Error 2
make[1]: Leaving directory '/cygdrive/c/new/squid-4.14/src'
make: *** [Makefile:588: install-recursive] Error 1



Brian Hall
Job Spider Team Lead
Geographic Solutions, Inc.
727.786.7955 phone | 727.786.5871 fax | bhall at geosolinc.com<mailto:bhall at geosolinc.com>
1001 Omaha Circle, Palm Harbor, FL 34683 | geographicsolutions.com<http://www.geographicsolutions.com/>
[2016 Email Signature]<http://www.geographicsolutions.com/>

The information contained in this message may be privileged, confidential
and protected from disclosure. If the reader of this message is not the intended
recipient, or an employee or agent responsible for delivering this message to the
intended recipient, you are hereby notified that any dissemination, distribution
or copying of this communication is strictly prohibited. If you have received this
communication in error, please notify us immediately by replying to the message
and deleting it from your computer.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210423/35352dae/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 15980 bytes
Desc: image001.gif
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210423/35352dae/attachment.gif>

From andy.frad at gmail.com  Sat Apr 24 01:28:21 2021
From: andy.frad at gmail.com (Andy Frad)
Date: Fri, 23 Apr 2021 19:28:21 -0600
Subject: [squid-users] Whitelist Src IP and Tie it to specific ip outgoing ip
Message-ID: <CAHZe4k8-a+AcnV4Q--UTdmiZQ0vThBWmpvBXU_Qsbivvh1eHNg@mail.gmail.com>

Hello,

I would like to know if there is a way to whitelist a users src address and
tie it to a specific outgoing ip?

Right now, I can easily add a users src ip to the following:

acl allowedips src " /etc/squid/allowedips.txt "
http_access allow allowedips

but I'd like to go a step further and make it so a persons src ip can only
get access to a specific ip bound to the server.

Thanks

*Andy*
*CCIE #56533*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210423/cb2df394/attachment.htm>

From rousskov at measurement-factory.com  Sat Apr 24 02:58:21 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 23 Apr 2021 22:58:21 -0400
Subject: [squid-users] Whitelist Src IP and Tie it to specific ip
 outgoing ip
In-Reply-To: <CAHZe4k8-a+AcnV4Q--UTdmiZQ0vThBWmpvBXU_Qsbivvh1eHNg@mail.gmail.com>
References: <CAHZe4k8-a+AcnV4Q--UTdmiZQ0vThBWmpvBXU_Qsbivvh1eHNg@mail.gmail.com>
Message-ID: <1960f404-4402-fa13-0eb2-5002f8ebd24f@measurement-factory.com>

On 4/23/21 9:28 PM, Andy Frad wrote:

> I would like to know if there is a way to whitelist a users src address
> and tie it to a specific outgoing ip?

The two parts of the question are completely unrelated AFAICT. Since you
already know how to allow traffic, I will focus on the second part.


> I'd like to ... make it so a persons src ip can
> only get access to a specific ip bound to the server.


To tell Squid to use local source IP address X for Squid-server
transactions matching a specialTransaction ACL, consider using

  tcp_outgoing_address X specialTransaction

Your call how to define the specialTransaction ACL (e.g. it could be a
src ACL). IIRC, tcp_outgoing_address supports fast ACLs only.

Please note that if the transaction is going to an IPv6 address but your
X address is an IPv4 address, then Squid will _ignore_ the
"tcp_outgoing_address X" rule(s) for that transaction. Whether that is a
good thing depends on your (unstated) requirements. If needed, you can,
of course, have two rules for each specialTransaction, one for IPv6 and
one for IPv4 addresses.

You cannot block outgoing traffic using tcp_outgoing_address.

Please see tcp_outgoing_address documentation for caveats. Some of them
sound odd to me so I recommend testing before jumping to conclusions.


HTH,

Alex.


From andy.frad at gmail.com  Sat Apr 24 11:58:01 2021
From: andy.frad at gmail.com (Andy Frad)
Date: Sat, 24 Apr 2021 05:58:01 -0600
Subject: [squid-users] IP Whitelist still getting user:pass prompt
Message-ID: <CAHZe4k8QTdbk3hzj4KQbjKJOYb-fP06nuVUh_nBydXjw9Y2Z1g@mail.gmail.com>

Morning,

I'm having an issue where even though I have my acl higher up in the
squid.conf, the user still gets prompted for a user:pass.

Here is a snippet of the config:

acl allowedips src " /etc/squid/allowedips.txt "
http_access allow allowedips

auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd

auth_param basic children 10
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl password proxy_auth REQUIRED
external_acl_type userip %MYADDR %LOGIN /usr/lib/squid/ext_file_userip_acl
-f /etc/squid/userip.conf

acl userip external userip

http_access allow localhost
http_access allow userip
#http_access allow password
http_access deny all

Anything I'm doing wrong?


*Andy*
*CCIE #56533*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210424/2e110620/attachment.htm>

From mgresko8 at gmail.com  Sun Apr 25 11:52:43 2021
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Sun, 25 Apr 2021 13:52:43 +0200
Subject: [squid-users] IP Whitelist still getting user:pass prompt
In-Reply-To: <CAHZe4k8QTdbk3hzj4KQbjKJOYb-fP06nuVUh_nBydXjw9Y2Z1g@mail.gmail.com>
References: <CAHZe4k8QTdbk3hzj4KQbjKJOYb-fP06nuVUh_nBydXjw9Y2Z1g@mail.gmail.com>
Message-ID: <CAChjPdQeNCUrYyLMsMt2LZ_6QwuBK-zwPC7K833u==ifs2D_Fw@mail.gmail.com>

Hello,

I see extra spaces in
acl allowedips src " /etc/squid/allowedips.txt "

It should be
acl allowedips src "/etc/squid/allowedips.txt"

What is you format of the file itself?

Are you seeing the password prompt also when only http protocol is
used in communicaton?

Marek


2021-04-24 13:58 GMT+02:00, Andy Frad <andy.frad at gmail.com>:
> Morning,
>
> I'm having an issue where even though I have my acl higher up in the
> squid.conf, the user still gets prompted for a user:pass.
>
> Here is a snippet of the config:
>
> acl allowedips src " /etc/squid/allowedips.txt "
> http_access allow allowedips
>
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd
>
> auth_param basic children 10
> auth_param basic realm Squid proxy-caching web server
> auth_param basic credentialsttl 2 hours
> acl password proxy_auth REQUIRED
> external_acl_type userip %MYADDR %LOGIN /usr/lib/squid/ext_file_userip_acl
> -f /etc/squid/userip.conf
>
> acl userip external userip
>
> http_access allow localhost
> http_access allow userip
> #http_access allow password
> http_access deny all
>
> Anything I'm doing wrong?
>
>
> *Andy*
> *CCIE #56533*
>


From moberger at metanetworks.com  Sun Apr 25 18:43:58 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Sun, 25 Apr 2021 21:43:58 +0300
Subject: [squid-users] Sporadic FATAL on squid 4.14
Message-ID: <CAGSk-40kSQLERnb8M=c5qsEhKG3YPr73+NVOTBjz-sV-5_FFug@mail.gmail.com>

Hi

I'm occasionally having the following error (entire compressed cache.log is
150K, I'll attach it if required).

> 2021/04/25 09:42:36.226| 33,2| AsyncCallQueue.cc(55) fireNext: entering clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] flags=9, err=0, HTTP Socket port=0x55ba5f6e3a50)
> 2021/04/25 09:42:36.226| 33,2| AsyncCall.cc(38) make: make call clientListenerConnectionOpened [call19]
> 2021/04/25 09:42:36.226| 0,3| client_side.cc(3563) OpenedHttpSocket: check failed: NHttpSockets > 0
>     exception location: client_side.cc(3563) OpenedHttpSocket
>
> 2021/04/25 09:42:36.226| FATAL: check failed: NHttpSockets > 0
>     exception location: client_side.cc(3563) OpenedHttpSocket
>
> 2021/04/25 09:42:36.226| 24,8| SBuf.cc(70) ~SBuf: SBuf1801 destructed
> 2021/04/25 09:42:36.226| 24,9| MemBlob.cc(82) ~MemBlob: destructed, this=0x55ba5f668a10 id=blob1050 capacity=40 size=2
> 2021/04/25 09:42:36.226| 24,8| SBuf.cc(30) SBuf: SBuf3438 created
> 2021/04/25 09:42:36.226| 24,8| SBuf.cc(38) SBuf: SBuf3439 created from id SBuf3438
> 2021/04/25 09:42:36.226| 24,8| SBuf.cc(70) ~SBuf: SBuf3438 destructed
>
> Could not figure out what may cause it.
Can you please advise me on a set of actions I can perform to gather more
information?

 Output of 'squid --version':

> Squid Cache: Version 4.14
> Service Name: squid
>
> This binary uses OpenSSL 1.1.1d  10 Sep 2019. For legal restrictions on
> distribution see https://www.openssl.org/source/license.html
>
> configure options:  '--prefix=/usr' '--localstatedir=/var'
> '--libexecdir=/usr/lib/squid' '--datadir=/usr/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,SMB_LM'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-zph-qos' '--enable-ecap' '--disable-translation'
> '--with-swapdir=/var/spool/squid' '--with-filedescriptors=65536'
> '--with-large-files' '--enable-linux-netfilter' '--enable-ssl'
> '--enable-ssl-crtd' '--with-openssl'
>

 Many thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210425/39afcc4d/attachment.htm>

From rousskov at measurement-factory.com  Mon Apr 26 02:26:20 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 25 Apr 2021 22:26:20 -0400
Subject: [squid-users] Sporadic FATAL on squid 4.14
In-Reply-To: <CAGSk-40kSQLERnb8M=c5qsEhKG3YPr73+NVOTBjz-sV-5_FFug@mail.gmail.com>
References: <CAGSk-40kSQLERnb8M=c5qsEhKG3YPr73+NVOTBjz-sV-5_FFug@mail.gmail.com>
Message-ID: <4f2d3dd1-d45a-5798-601c-6b3d1ff4331f@measurement-factory.com>

On 4/25/21 2:43 PM, Moti Berger wrote:
> Hi
> 
> I'm occasionally having the following error (entire compressed cache.log
> is 150K, I'll attach it if required).
> 
>     2021/04/25 09:42:36.226| 33,2| AsyncCallQueue.cc(55) fireNext: entering clientListenerConnectionOpened(local=0.0.0.0:3128 <http://0.0.0.0:3128> remote=[::] flags=9, err=0, HTTP Socket port=0x55ba5f6e3a50)
>     2021/04/25 09:42:36.226| 33,2| AsyncCall.cc(38) make: make call clientListenerConnectionOpened [call19]
>     2021/04/25 09:42:36.226| 0,3| client_side.cc(3563) OpenedHttpSocket: check failed: NHttpSockets > 0
>         exception location: client_side.cc(3563) OpenedHttpSocket
> 
>     2021/04/25 09:42:36.226| FATAL: check failed: NHttpSockets > 0
>         exception location: client_side.cc(3563) OpenedHttpSocket
> 
>     2021/04/25 09:42:36.226| 24,8| SBuf.cc(70) ~SBuf: SBuf1801 destructed
>     2021/04/25 09:42:36.226| 24,9| MemBlob.cc(82) ~MemBlob: destructed, this=0x55ba5f668a10 id=blob1050 capacity=40 size=2
>     2021/04/25 09:42:36.226| 24,8| SBuf.cc(30) SBuf: SBuf3438 created
>     2021/04/25 09:42:36.226| 24,8| SBuf.cc(38) SBuf: SBuf3439 created from id SBuf3438
>     2021/04/25 09:42:36.226| 24,8| SBuf.cc(70) ~SBuf: SBuf3438 destructed
> 
> Could not figure out what may cause it.

It could be Bug 4478. You can find more info at
https://bugs.squid-cache.org/show_bug.cgi?id=4478#c2

> Can you please advise me on a set of actions I can perform to gather
> more information?

Do you know whether the problems are correlated to Squid startup,
reconfiguration, or shutdown?

If not, it may be a good idea to share a pointer to the compressed ALL,9
cache.log, ideally in a comment for that bug report.


Thank you,

Alex.


> ?Output?of 'squid --version':
> 
>     Squid Cache: Version 4.14
>     Service Name: squid
> 
>     This binary uses OpenSSL 1.1.1d ?10 Sep 2019. For legal restrictions
>     on distribution see https://www.openssl.org/source/license.html
>     <https://www.openssl.org/source/license.html>
> 
>     configure options: ?'--prefix=/usr' '--localstatedir=/var'
>     '--libexecdir=/usr/lib/squid' '--datadir=/usr/share/squid'
>     '--sysconfdir=/etc/squid' '--with-default-user=proxy'
>     '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
>     '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
>     '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
>     '--enable-removal-policies=lru,heap' '--enable-delay-pools'
>     '--enable-cache-digests' '--enable-icap-client'
>     '--enable-follow-x-forwarded-for'
>     '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
>     '--enable-auth-digest=file,LDAP'
>     '--enable-auth-negotiate=kerberos,wrapper'
>     '--enable-auth-ntlm=fake,SMB_LM'
>     '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>     '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
>     '--enable-zph-qos' '--enable-ecap' '--disable-translation'
>     '--with-swapdir=/var/spool/squid' '--with-filedescriptors=65536'
>     '--with-large-files' '--enable-linux-netfilter' '--enable-ssl'
>     '--enable-ssl-crtd' '--with-openssl'
> 
> 
> ?Many thanks,
> Moti
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From justinglencook at gmail.com  Tue Apr 27 17:33:03 2021
From: justinglencook at gmail.com (Justin Cook)
Date: Tue, 27 Apr 2021 10:33:03 -0700
Subject: [squid-users] Allowing User Certificate Authentication with SSL Bump
Message-ID: <CAH+vZy_9VWCNSCPu2AZhL5xJXW0kJjgLTn447qwCzhO7_TdDaQ@mail.gmail.com>

We are running into a situation where we are unable to fully authenticate
our users to an internal tooling service that requires certificate
authentication as part of its login process, when going through squid
forward proxy with SSL bump enabled.  The problem, however, is that it
shares a domain name with all of our other internal tooling services so we
cannot just enable splice for the domain, since we then lose access to ssl
bump and the url logging that it allows.

In this implementation, the main thing that we are using Squid for is
allowance-only access for a locked down subset of machines, where specific
URLs are allowed for different machine sets.  We need the source machine
and URLs accessed to be logged in case they need to be reviewed at a later
date.

So I am trying to find some way to accomplish one of the following:
1.  Log full URL paths for spliced domains (www.domain.com/auth/tool
instead of just www.domain.com)
2.  Allow user certificate based authentication for SSL Bumped URLs
3.  Splice only a specific subdirectory instead of a whole domain (which is
impossible, if I understand correctly)

Are any of these even possible with squid or have we hit a brick wall in
terms of available functionality?  I haven't been able to find any working
solutions for enabling user certificate authentication with squid forward
proxies.  The most I have been able to do is have the squid proxy itself
present a static user certificate, but that won't be deemed acceptable by
the security teams that manage the authentication services.
Thanks in advance for your help!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210427/03170ad7/attachment.htm>

From rousskov at measurement-factory.com  Tue Apr 27 17:57:31 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Apr 2021 13:57:31 -0400
Subject: [squid-users] Allowing User Certificate Authentication with SSL
 Bump
In-Reply-To: <CAH+vZy_9VWCNSCPu2AZhL5xJXW0kJjgLTn447qwCzhO7_TdDaQ@mail.gmail.com>
References: <CAH+vZy_9VWCNSCPu2AZhL5xJXW0kJjgLTn447qwCzhO7_TdDaQ@mail.gmail.com>
Message-ID: <511eb52c-6e7f-d995-0288-a0defb2d3c88@measurement-factory.com>

On 4/27/21 1:33 PM, Justin Cook wrote:
> We are running into a situation where we are unable to fully
> authenticate our users to an internal tooling service that requires
> certificate authentication as part of its login process, when going
> through squid forward proxy with SSL bump enabled.

SslBump does not support "TLS inside TLS" configurations, which is what
you get when you combine certificate-based proxy authentication (which
requires an https_port working in a forward proxy mode) with SslBump
(which, for an https_port, currently requires an interception proxy mode).

It is possible to add support for "TLS inside TLS", but it requires a
serious development effort.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.


From justinglencook at gmail.com  Tue Apr 27 22:23:53 2021
From: justinglencook at gmail.com (Justin Cook)
Date: Tue, 27 Apr 2021 15:23:53 -0700
Subject: [squid-users] Allowing User Certificate Authentication with SSL
 Bump
In-Reply-To: <511eb52c-6e7f-d995-0288-a0defb2d3c88@measurement-factory.com>
References: <CAH+vZy_9VWCNSCPu2AZhL5xJXW0kJjgLTn447qwCzhO7_TdDaQ@mail.gmail.com>
 <511eb52c-6e7f-d995-0288-a0defb2d3c88@measurement-factory.com>
Message-ID: <CAH+vZy_+yuOC-eg8hg_DxTjiWj-LTA-mstPg73jO0VxC3VnANQ@mail.gmail.com>

In this case we're not looking to authenticate the user themselves with the
squid server but with the destination web server, does that change the
scope?

On Tue, Apr 27, 2021 at 10:57 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 4/27/21 1:33 PM, Justin Cook wrote:
> > We are running into a situation where we are unable to fully
> > authenticate our users to an internal tooling service that requires
> > certificate authentication as part of its login process, when going
> > through squid forward proxy with SSL bump enabled.
>
> SslBump does not support "TLS inside TLS" configurations, which is what
> you get when you combine certificate-based proxy authentication (which
> requires an https_port working in a forward proxy mode) with SslBump
> (which, for an https_port, currently requires an interception proxy mode).
>
> It is possible to add support for "TLS inside TLS", but it requires a
> serious development effort.
>
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210427/c7a34b96/attachment.htm>

From rousskov at measurement-factory.com  Wed Apr 28 19:40:54 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 28 Apr 2021 15:40:54 -0400
Subject: [squid-users] Allowing User Certificate Authentication with SSL
 Bump
In-Reply-To: <CAH+vZy_+yuOC-eg8hg_DxTjiWj-LTA-mstPg73jO0VxC3VnANQ@mail.gmail.com>
References: <CAH+vZy_9VWCNSCPu2AZhL5xJXW0kJjgLTn447qwCzhO7_TdDaQ@mail.gmail.com>
 <511eb52c-6e7f-d995-0288-a0defb2d3c88@measurement-factory.com>
 <CAH+vZy_+yuOC-eg8hg_DxTjiWj-LTA-mstPg73jO0VxC3VnANQ@mail.gmail.com>
Message-ID: <ebf4fea1-146f-fef5-f741-315e6c75ea34@measurement-factory.com>

On 4/27/21 6:23 PM, Justin Cook wrote:
> In this case we're not looking to authenticate the user themselves with
> the squid server but with the destination web server, does that change
> the scope?

* If you do need to bump TLS connections:

Yes, certificate authentication with an origin server is a different
problem. If Squid does not possess the client certificate key, then
Squid cannot both bump the TLS client connection (i.e. become the client
from the origin server point of view) and keep the old client from the
origin server point of view.

In this case, this is not a technical limitation of the current Squid
implementation like "TLS inside TLS"; it is a protocol-level conflict
that no implementation can resolve. TLS design makes
faking/impersonating the authenticating client impossible without
leaking the client key to the proxy.

If you can refactor so that the origin server trusts Squid instead of
the client, and Squid authenticates the TLS client, then we will be back
to the earlier "TLS inside TLS" problem (not to mention client
changes/complications), so this kind of refactoring is unlikely to be
the right way forward.


* If you only need to peek at TLS connections:

You should be able to keep client certificate authentication. If Squid
cannot keep that while peeking at the TLS client or the origin server,
then there is a Squid bug somewhere.


HTH,

Alex.


> On Tue, Apr 27, 2021 at 10:57 AM Alex Rousskov wrote:
> 
>     On 4/27/21 1:33 PM, Justin Cook wrote:
>     > We are running into a situation where we are unable to fully
>     > authenticate our users to an internal tooling service that requires
>     > certificate authentication as part of its login process, when going
>     > through squid forward proxy with SSL bump enabled.
> 
>     SslBump does not support "TLS inside TLS" configurations, which is what
>     you get when you combine certificate-based proxy authentication (which
>     requires an https_port working in a forward proxy mode) with SslBump
>     (which, for an https_port, currently requires an interception proxy
>     mode).
> 
>     It is possible to add support for "TLS inside TLS", but it requires a
>     serious development effort.
> 
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
> 
>     HTH,
> 
>     Alex.
> 



From justinglencook at gmail.com  Wed Apr 28 20:36:05 2021
From: justinglencook at gmail.com (Justin Cook)
Date: Wed, 28 Apr 2021 13:36:05 -0700
Subject: [squid-users] Allowing User Certificate Authentication with SSL
 Bump
In-Reply-To: <ebf4fea1-146f-fef5-f741-315e6c75ea34@measurement-factory.com>
References: <CAH+vZy_9VWCNSCPu2AZhL5xJXW0kJjgLTn447qwCzhO7_TdDaQ@mail.gmail.com>
 <511eb52c-6e7f-d995-0288-a0defb2d3c88@measurement-factory.com>
 <CAH+vZy_+yuOC-eg8hg_DxTjiWj-LTA-mstPg73jO0VxC3VnANQ@mail.gmail.com>
 <ebf4fea1-146f-fef5-f741-315e6c75ea34@measurement-factory.com>
Message-ID: <CAH+vZy-hRtquf1HatEpEtrPBGk55ioVvt3NLDOqMQ+kQv=6uOg@mail.gmail.com>

Unfortunately the peeking only logs the fqdn and no subdirectories, which
doesnt meet our logging requirements for security :(.  It sounds like there
isn't a way to have squid do both currently, I do appreciate the
information though!

On Wed, Apr 28, 2021 at 12:40 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 4/27/21 6:23 PM, Justin Cook wrote:
> > In this case we're not looking to authenticate the user themselves with
> > the squid server but with the destination web server, does that change
> > the scope?
>
> * If you do need to bump TLS connections:
>
> Yes, certificate authentication with an origin server is a different
> problem. If Squid does not possess the client certificate key, then
> Squid cannot both bump the TLS client connection (i.e. become the client
> from the origin server point of view) and keep the old client from the
> origin server point of view.
>
> In this case, this is not a technical limitation of the current Squid
> implementation like "TLS inside TLS"; it is a protocol-level conflict
> that no implementation can resolve. TLS design makes
> faking/impersonating the authenticating client impossible without
> leaking the client key to the proxy.
>
> If you can refactor so that the origin server trusts Squid instead of
> the client, and Squid authenticates the TLS client, then we will be back
> to the earlier "TLS inside TLS" problem (not to mention client
> changes/complications), so this kind of refactoring is unlikely to be
> the right way forward.
>
>
> * If you only need to peek at TLS connections:
>
> You should be able to keep client certificate authentication. If Squid
> cannot keep that while peeking at the TLS client or the origin server,
> then there is a Squid bug somewhere.
>
>
> HTH,
>
> Alex.
>
>
> > On Tue, Apr 27, 2021 at 10:57 AM Alex Rousskov wrote:
> >
> >     On 4/27/21 1:33 PM, Justin Cook wrote:
> >     > We are running into a situation where we are unable to fully
> >     > authenticate our users to an internal tooling service that requires
> >     > certificate authentication as part of its login process, when going
> >     > through squid forward proxy with SSL bump enabled.
> >
> >     SslBump does not support "TLS inside TLS" configurations, which is
> what
> >     you get when you combine certificate-based proxy authentication
> (which
> >     requires an https_port working in a forward proxy mode) with SslBump
> >     (which, for an https_port, currently requires an interception proxy
> >     mode).
> >
> >     It is possible to add support for "TLS inside TLS", but it requires a
> >     serious development effort.
> >
> >
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210428/c2caa5fe/attachment.htm>

From neven.vrenko at gmail.com  Fri Apr 30 08:40:50 2021
From: neven.vrenko at gmail.com (Neven Vrenko)
Date: Fri, 30 Apr 2021 10:40:50 +0200
Subject: [squid-users] Client certificate authentication problem
Message-ID: <CAF7xqN7TiWhdZS_WJjkROWrL_0SYYGvta0V6dGdNDs+_csZH+w@mail.gmail.com>

Hello Alex,

thank you for your answer. I was little bit puzzled since I haven't got any
error when using "clientca" with "http_port". I thought, maybe it was
somehow possible, beyond my understanding. :)

The reason why I didn't respond immediately, because I wanted to test
everything with the "https_port" configuration.

Now my configuration looks like this:

> https_port 443 tls-cert=/etc/squid/certs/new-squid.cert
tls-key=/etc/squid/certs/new-squid.key clientca=/etc/squid/certs/autn.pem
capath=/etc/squid/certs/CAs sslcontext=id

This works...

What I'm trying to do is access control with *user_cert* ACL based on CN
information.

My ACL configuration is super minimal:

> acl ssl_auth user_cert CN "/etc/squid/allowed.cn"
> http_access allow ssl_auth
>
> http_access deny all

File "/etc/squid/allowed.cn" contains one row/entry: "Doe John PKI
1234567890" (without quotes)

However this doesn't work.

>From the cache.log, it is visible that client certificate information is
fetched:

> clientNegotiateSSL: New session 0x11415a0 on FD 12 (10.x.x.x:60308)
> retrieveNegotiatedInfo: SSL connection info on FD 12 SSL version TLS/1.2
negotiated cipher AES128-GCM-SHA256
> clientNegotiateSSL: FD 12 client certificate: subject: /DC=tst/CN=Doe
John PKI 1234567890
> clientNegotiateSSL: FD 12 client certificate: issuer:
/DC=com/DC=tst/DC=PKI/CN=CA-AUTH-01
> Server.cc(90) readSomeData: conn7 local=10.x.x.x:443
remote=10.x.x.x:60308 FD 12 flags=1: reading request...

>From the cache.log is as well visible that ssl_auth ACL is checked, but
there is NO MATCH:

> Acl.cc(124) matches: checking http_access
> Checklist.cc(398) bannedAction: Action 'ALLOWED/0' is not banned
> Acl.cc(124) matches: checking http_access#1
> Acl.cc(124) matches: checking ssl_auth     < --- Access list
>
> CertificateData.cc(68) match: CN=Doe John PKI 1234567890   < --- Client
certificate CN
>
> MemBlob.cc(56) MemBlob: constructed, this=0x14dccc0 id=blob1388
reserveSize=35
> MemBlob.cc(101) memAlloc: blob1388 memAlloc: requested=35, received=40
> SBuf.cc(866) reAlloc: SBuf5096 new store capacity: 40
> StringData.cc(33) match: aclMatchStringList: checking 'Doe John PKI
1234567890'
>
> StringData.cc(36) match: aclMatchStringList: 'Doe John PKI 1234567890'
NOT found   < --- doesn't match
>
> Acl.cc(151) matches: checked: ssl_auth = 0
> Acl.cc(151) matches: checked: http_access#1 = 0


I'm really not sure what I have missed...
I tried to put CN directly in the ACL, so with no reference to thefile.
I tried to put single and double quotes around CN in allowed.cn file, as
well.

Could you please help me further? What am I doing wrong here?

Thank you and regards,
Neven
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210430/f6af83eb/attachment.htm>

From rousskov at measurement-factory.com  Fri Apr 30 15:22:03 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 30 Apr 2021 11:22:03 -0400
Subject: [squid-users] Client certificate authentication problem
In-Reply-To: <CAF7xqN7TiWhdZS_WJjkROWrL_0SYYGvta0V6dGdNDs+_csZH+w@mail.gmail.com>
References: <CAF7xqN7TiWhdZS_WJjkROWrL_0SYYGvta0V6dGdNDs+_csZH+w@mail.gmail.com>
Message-ID: <248ca98d-1862-0ead-87c0-67a0aa408ea8@measurement-factory.com>

On 4/30/21 4:40 AM, Neven Vrenko wrote:
> Hello Alex,
> 
> thank you for your answer. I was little bit puzzled since I haven't got
> any error when using "clientca" with "http_port". I thought, maybe it
> was somehow possible, beyond my understanding. :)
> 
> The reason why I didn't respond immediately, because I wanted to test
> everything with the "https_port" configuration.
> 
> Now my configuration looks like this:
> 
>> https_port 443 tls-cert=/etc/squid/certs/new-squid.cert
> tls-key=/etc/squid/certs/new-squid.key
> clientca=/etc/squid/certs/autn.pem capath=/etc/squid/certs/CAs sslcontext=id
> 
> This works...
> 
> What I'm trying to do is access control with *user_cert* ACL based on CN
> information.
> 
> My ACL configuration is super minimal:
> 
>> acl ssl_auth user_cert CN "/etc/squid/allowed.cn <http://allowed.cn>"
>> http_access allow ssl_auth
>>
>> http_access deny all
> 
> File "/etc/squid/allowed.cn" contains one row/entry:
> "Doe John PKI 1234567890" (without quotes)
> 
> However this doesn't work.
> 
> From the cache.log, it is visible that client certificate information is
> fetched:
> 
>> clientNegotiateSSL: New session 0x11415a0 on FD 12 (10.x.x.x:60308)
>> retrieveNegotiatedInfo: SSL connection info on FD 12 SSL version
> TLS/1.2 negotiated cipher AES128-GCM-SHA256
>> clientNegotiateSSL: FD 12 client certificate: subject: /DC=tst/CN=Doe
> John PKI 1234567890
>> clientNegotiateSSL: FD 12 client certificate: issuer:
> /DC=com/DC=tst/DC=PKI/CN=CA-AUTH-01
>> Server.cc(90) readSomeData: conn7 local=10.x.x.x:443
> remote=10.x.x.x:60308 FD 12 flags=1: reading request...
> 
> From the cache.log is as well visible that ssl_auth ACL is checked, but
> there is NO MATCH:
> 
>> Acl.cc(124) matches: checking http_access
>> Checklist.cc(398) bannedAction: Action 'ALLOWED/0' is not banned
>> Acl.cc(124) matches: checking http_access#1
>> Acl.cc(124) matches: checking ssl_auth???? < --- Access list
>>
>> CertificateData.cc(68) match: CN=Doe John PKI 1234567890?? < ---
> Client certificate CN
>>
>> MemBlob.cc(56) MemBlob: constructed, this=0x14dccc0 id=blob1388
> reserveSize=35
>> MemBlob.cc(101) memAlloc: blob1388 memAlloc: requested=35, received=40
>> SBuf.cc(866) reAlloc: SBuf5096 new store capacity: 40
>> StringData.cc(33) match: aclMatchStringList: checking 'Doe John PKI
> 1234567890'
>>
>> StringData.cc(36) match: aclMatchStringList: 'Doe John PKI 1234567890'
> NOT found?? < --- doesn't match
>>
>> Acl.cc(151) matches: checked: ssl_auth = 0
>> Acl.cc(151) matches: checked: http_access#1 = 0
> 
> 
> I'm really not sure what I have missed...
> I tried to put CN directly in the ACL, so with no reference to thefile.
> I tried to put single and double quotes around CN in allowed.cn
> <http://allowed.cn> file, as well.
> 
> Could you please help me further? What am I doing wrong here?

I see no red flags in what you are describing and in the provided logs.
Unfortunately, ACLStringData::match() does not log the strings it is
comparing the configured ACL value against, and I lack the free time
necessary to triage this further for you right now. Hopefully, somebody
else will volunteer to triage this. It should not be very difficult to
get to the bottom of it.


Sorry,

Alex.


From m_zouhairy at skno.by  Fri Apr  2 13:15:21 2021
From: m_zouhairy at skno.by (Vacheslav)
Date: Fri, 02 Apr 2021 13:15:21 -0000
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
Message-ID: <60eefa5d-51ed-a197-11ab-4a8b9505aab8@skno.by>

hmm, thanks for both of you.. i regenerated new certificates using 
Eliazer's method and now squid restarted but it is refusing connections..
i normally configure port 8080 as the proxy port in the browser, and i 
am thinking there needs to be another port for ssl bumping?

now the configuration is like this:

....


# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 8080

##sslproxy_capath /home/zouhairy/demoCA

http_port 8080 ssl-bump  cert=/etc/squid/certs/myCA.pem 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB



ssl_bump peek all
ssl_bump splice all



#tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

# Uncomment and adjust the following to add a disk cache directory.
# Updates: chrome and acrobat
#refresh_pattern -i gvt1.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80% 
129600 reload-into-ims
#refresh_pattern -i adobe.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 
80% 129600 reload-into-ims



range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1



cache_dir ufs /var/cache/squid 3000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mem 1024 MB

netdb_filename none

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:				1440	20%	10080
refresh_pattern ^gopher:			1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 	0		0%	0
refresh_pattern .					0		20%	4320

url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode 
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4
#debug_options ALL,1 33,2 28,9


On 4/2/21 2:02 PM, Amos Jeffries wrote:
> On 1/04/21 11:41 pm, Majed Zouhairy wrote:
>>
>> to enable ssl bumping.
>>
>> specifically those commands:
>>
>> /usr/share/ssl/misc/CA.pl -newca
>> /usr/share/ssl/misc/CA.pl -newreq
>> /usr/share/ssl/misc/CA.pl -sign
>> openssl x509 -in newcert.pem -outform DER -out squidTrusted.der
> 
> 
>> sudo squid -z
>>
>> asks for certificate password
>> then
>>
>> Enter PEM pass phrase:
>> 2021/04/01 13:17:03| Created PID file (/run/squid.pid)
>> zouhairy at proxy:~> 2021/04/01 13:17:03 kid1| WARNING: BCP 177 
>> violation. Detected non-functional IPv6 loopback.
>> Enter PEM pass phrase:
>> 2021/04/01 13:17:03 kid1| FATAL: No valid signing certificate 
>> configured for HTTP_port 0.0.0.0:8080
> 
> That says there is no CA certificate found in the file configured for 
> that ports tls-cert= option. Squid requires a signing (CA) certificate 
> and its private key in order to perform SSL-Bump.
> 
> With "squid -k parse" Squid should tell you what it is loading from that 
> file.
> 
> 
>>
>> squid conf:
>>
> ...
>>
>> http_port 8080 ssl-bump generate-host-certificates=on 
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/newcert.pem 
>> key=/etc/squid/certs/newkey.pem capath=/home/zouhairy/demoCA
>>
> 
>>
>> ssl_bump peek all
>> ssl_bump splice all
>>
>> sslproxy_cert_error allow all
>>
> 
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From m_zouhairy at skno.by  Sun Apr  4 19:01:39 2021
From: m_zouhairy at skno.by (vacheslav)
Date: Sun, 04 Apr 2021 19:01:39 -0000
Subject: [squid-users] compile squid with tumbleweed
In-Reply-To: <8487dee7-8f9e-ecda-d9d3-3d30bd2859bb@treenet.co.nz>
References: <89a4f103-fd93-49f8-0f80-ffc2c7b62735@ckta.by>
 <e2b028eb-cb4e-8583-b097-7c7a9354db5f@treenet.co.nz>
 <4f54bd64-0e20-a3a5-372e-e273fa3daa81@ckta.by>
 <1648c942-e91c-c54e-a1cf-f6aeb200d0d9@treenet.co.nz>
 <7b2e4118-7701-0892-55b8-1354baad8ea9@ckta.by>
 <8487dee7-8f9e-ecda-d9d3-3d30bd2859bb@treenet.co.nz>
Message-ID: <bfa569db-17f0-279a-8849-fda8f11ea46c@skno.by>

yes, from the browser..

squid cache last showed:

2021/04/02 15:52:47 kid1| Logfile: opening log 
daemon:/var/log/squid/access.log
2021/04/02 15:52:47 kid1| Logfile Daemon: opening log 
/var/log/squid/access.log
2021/04/02 15:52:47 kid1| Unlinkd pipe opened on FD 40
2021/04/02 15:52:47 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2021/04/02 15:52:47 kid1| Store logging disabled
2021/04/02 15:52:47 kid1| Swap maxSize 3072000 + 1048576 KB, estimated 
316967 objects
2021/04/02 15:52:47 kid1| Target number of buckets: 15848
2021/04/02 15:52:47 kid1| Using 16384 Store buckets
2021/04/02 15:52:47 kid1| Max Mem? size: 1048576 KB
2021/04/02 15:52:47 kid1| Max Swap size: 3072000 KB
2021/04/02 15:52:47 kid1| Rebuilding storage in /var/cache/squid (clean log)
2021/04/02 15:52:47 kid1| Using Least Load store dir selection
2021/04/02 15:52:47 kid1| Set Current Directory to /var/cache/squid
2021/04/02 15:52:47 kid1| Finished loading MIME types and icons.
2021/04/02 15:52:47 kid1| HTCP Disabled.
2021/04/02 15:52:47 kid1| Pinger socket opened on FD 45
2021/04/02 15:52:47 kid1| Squid plugin modules loaded: 0
2021/04/02 15:52:47 kid1| Adaptation support is off.
2021/04/02 15:52:47 kid1| Accepting SSL bumped HTTP Socket connections 
at local=0.0.0.0:8080 remote=[::] FD 43 flags=9
2021/04/02 15:52:47| WARNING: BCP 177 violation. Detected non-functional 
IPv6 loopback.
2021/04/02 15:52:47| pinger: Initialising ICMP pinger ...
2021/04/02 15:52:47| pinger: ICMP socket opened.
2021/04/02 15:52:47| pinger: ICMPv6 socket opened
2021/04/02 15:52:47 kid1| Store rebuilding is 19.99% complete
2021/04/02 15:52:47 kid1| Done reading /var/cache/squid swaplog (20010 
entries)
2021/04/02 15:52:47 kid1| Finished rebuilding storage from disk.
2021/04/02 15:52:47 kid1|???? 20010 Entries scanned
2021/04/02 15:52:47 kid1|???????? 0 Invalid entries.
2021/04/02 15:52:47 kid1|???????? 0 With invalid flags.
2021/04/02 15:52:47 kid1|???? 20010 Objects loaded.
2021/04/02 15:52:47 kid1|???????? 0 Objects expired.
2021/04/02 15:52:47 kid1|???????? 0 Objects cancelled.
2021/04/02 15:52:47 kid1|???????? 0 Duplicate URLs purged.
2021/04/02 15:52:47 kid1|???????? 0 Swapfile clashes avoided.
2021/04/02 15:52:47 kid1|?? Took 0.26 seconds (76538.52 objects/sec).
2021/04/02 15:52:47 kid1| Beginning Validation Procedure
2021/04/02 15:52:47 kid1|?? Completed Validation Procedure
2021/04/02 15:52:47 kid1|?? Validated 20010 Entries
2021/04/02 15:52:47 kid1|?? store_swap_size = 1355568.00 KB
2021/04/02 15:52:47 kid1| WARNING: 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 
4MB #Hlpr1 exited
2021/04/02 15:52:47 kid1| Too few 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 
4MB processes are running (need 1/32)
2021/04/02 15:52:47 kid1| Closing HTTP(S) port 0.0.0.0:8080
2021/04/02 15:52:47 kid1| storeDirWriteCleanLogs: Starting...
2021/04/02 15:52:47 kid1|?? Finished.? Wrote 20010 entries.
2021/04/02 15:52:47 kid1|?? Took 0.01 seconds (3978131.21 entries/sec).
2021/04/02 15:52:47 kid1| FATAL: The 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 
4MB helpers are crashing too rapidly, need help!

squid log last showed:


1617367631.100??? 868 10.0.28.26 TCP_REFRESH_MODIFIED_ABORTED/200 13935 
GET http://spastv.ru/ - HIER_DIRECT/84.201.153.140 text/html
1617367725.880????? 0 10.0.28.26 NONE/000 0 NONE 
error:transaction-end-before-headers - HIER_NONE/- -
1617367845.916????? 0 10.0.28.26 NONE/000 0 NONE 
error:transaction-end-before-headers - HIER_NONE/- -

which is an every minute check


sudo systemctl status squid
? squid.service - Squid caching proxy
 ???? Loaded: loaded (/usr/lib/systemd/system/squid.service; enabled; 
vendor preset: disabled)
 ???? Active: failed (Result: exit-code) since Sun 2021-04-04 21:58:13 
+03; 5s ago
 ?????? Docs: man:squid(8)
 ??? Process: 28198 
ExecStartPre=/usr/libexec/squid/initialize_cache_if_needed.sh 
(code=exited, status=0/SUCCESS)
 ??? Process: 28202 ExecStart=/usr/sbin/squid -FC (code=exited, 
status=0/SUCCESS)
 ?? Main PID: 28203 (code=exited, status=1/FAILURE)

Apr 04 21:58:12 proxy squid[28203]: Squid Parent: (squid-1) process 
28355 started
Apr 04 21:58:12 proxy (squid-1)[28355]: FATAL: The 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 4>
Apr 04 21:58:12 proxy squid[28203]: Squid Parent: squid-1 process 28355 
exited with status 1
Apr 04 21:58:12 proxy squid[28203]: Squid Parent: (squid-1) process 
28405 started
Apr 04 21:58:13 proxy (squid-1)[28405]: FATAL: The 
/usr/libexec/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 4>
Apr 04 21:58:13 proxy squid[28203]: Squid Parent: squid-1 process 28405 
exited with status 1
Apr 04 21:58:13 proxy squid[28203]: Squid Parent: squid-1 process 28405 
will not be restarted for 3600 seconds due to repea>
Apr 04 21:58:13 proxy squid[28203]: Exiting due to repeated, frequent 
failures
Apr 04 21:58:13 proxy systemd[1]: squid.service: Main process exited, 
code=exited, status=1/FAILURE
Apr 04 21:58:13 proxy systemd[1]: squid.service: Failed with result 
'exit-code'.

4.04.21 13:24, Amos Jeffries ?????:
> On 4/04/21 5:09 pm, Majed Zouhairy wrote:
>> the error is:
>>
>> ??????-?????? ???????????? ????????? ??????????
>>
>> translation: the proxy-server is refusing to accept connections..
>>
>
> That seems like the meaningless text modern Browsers like replacing 
> real error with.
>
> Can you check the Squid logs to see what is actually going on?
>
>
>> might it be some setting in ufdbguard now?
>>
>
> If that text is from the Browser it could be anything.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



