From ml at netfence.it  Mon Feb  1 07:56:41 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Mon, 1 Feb 2021 08:56:41 +0100
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <7cd48afa-cecf-99b5-f2bd-5ff7de4282f5@treenet.co.nz>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
 <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
 <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>
 <ff0b198a-672a-541b-95d5-c0283aa6ed45@measurement-factory.com>
 <fb6e47bb-a225-6788-5b1e-0e34e745ba8f@netfence.it>
 <7cd48afa-cecf-99b5-f2bd-5ff7de4282f5@treenet.co.nz>
Message-ID: <27612dc1-8ee9-875e-2e9c-8c1851aaa1dc@netfence.it>

On 1/31/21 1:11 AM, Amos Jeffries wrote:

>> As I said, they live on the same host, so it can't be a network problem.
>>
> 
> FYI, that conclusion does not follow. Even on the same host there is a 
> full TCP/IP networking stack between Squid and ICAP server doing things 
> to the packets. All localhost removes is the potential problems due to 
> differences in machine networking stacks.
> 
> Network config, firewall rules, packet handling, and/or protocol 
> negotiation activities between the software are all still happening that 
> may affect the outcome.

Right.
It could be a network problem.
However, I think that's unlikely (also given the host is monitored and I 
don't see alerts or other signs of such troubles).
While I cannot exclude that completely, I think I should first 
investigate in other directions.

  bye & Thanks
	av.


From ml at netfence.it  Mon Feb  1 16:01:55 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Mon, 1 Feb 2021 17:01:55 +0100
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <27612dc1-8ee9-875e-2e9c-8c1851aaa1dc@netfence.it>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
 <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
 <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>
 <ff0b198a-672a-541b-95d5-c0283aa6ed45@measurement-factory.com>
 <fb6e47bb-a225-6788-5b1e-0e34e745ba8f@netfence.it>
 <7cd48afa-cecf-99b5-f2bd-5ff7de4282f5@treenet.co.nz>
 <27612dc1-8ee9-875e-2e9c-8c1851aaa1dc@netfence.it>
Message-ID: <7f1b2ff3-dcc5-67aa-1635-90f60a3faecf@netfence.it>

On 2/1/21 8:56 AM, Andrea Venturoli wrote:

> It could be a network problem.
> However, I think that's unlikely (also given the host is monitored and I 
> don't see alerts or other signs of such troubles).
> While I cannot exclude that completely, I think I should first 
> investigate in other directions.

Finally I have some insight: this happens when ClamAV receives a new 
virus definitions database and so reloads.

Notice I'm using 0.103, which "reloads the signature database without 
blocking scanning" (and no I didn't disable this).
So probably, while it works in theory, this slows the system and hence 
the timeouts.

I'm now trying with increased timeouts or with disabling ICAP failure 
limits.

Thanks to all who helped.

  bye
	av.


From ngtech1ltd at gmail.com  Mon Feb  1 20:37:23 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 1 Feb 2021 22:37:23 +0200
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
 <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
 <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
Message-ID: <006101d6f8da$0c58bf30$250a3d90$@gmail.com>

Hey Milos,

I suggest to try and test this against:
http://s3.amazonaws.com/awsdocs/S3/latest/s3-gsg.pdf
http://s3.amazonaws.com/awsdocs/gettingstarted/latest/awsgsg-freetier.pdf

https://s3.amazonaws.com/awsdocs/gettingstarted/latest/awsgsg-freetier.pdf
https://s3.amazonaws.com/awsdocs/S3/latest/s3-gsg.pdf

What version of squid are you using?
squid -v
output should give something.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: mailto:ngtech1ltd at gmail.com
Zoom: Coming soon


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Milos Dodic
Sent: Friday, January 29, 2021 7:57 PM
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Fixing Squid configuration for caching proxy?

Alex, thanks for the swift response. Your help is very much appreciated!

Here are the logs, but first to mention, from the server that is going through the Squid, I am using curl -k (-k to ignore SSL insecure warnings atm). From the Squid iself, I use squidclient, as using curl from Squid doesn't do much.

So when I curl the newly uploaded test file from the server that has Squid as default gateway, the access logs shows:
------------------------------------------------------------------------------------------------------------------
1611941462.501     13 10.10.1.249 NONE/200 0 CONNECT http://52.217.88.134:443 - ORIGINAL_DST/http://52.217.88.134 -
1611941462.537     22 10.10.1.249 TCP_MISS/200 488 GET https://s3.amazonaws.com/test.XXXXX.com/testFile - ORIGINAL_DST/http://52.217.88.134 binary/octet-stream
------------------------------------------------------------------------------------------------------------------

Cache log is quite long, but won't truncate in order to not omit something potentially important:
--------------------------------------------------------------------------------------------------------------------------------
2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New connection on FD 30
2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(312) acceptNext: connection on local=[::]:3130 remote=[::] FD 30 flags=41
2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(2748) httpsSslBumpAccessCheckDone: sslBump action stareneeded for local=http://52.217.88.134:443 remote=http://10.10.1.249:43538 FD 13 flags=33
2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(3424) fakeAConnectRequest: fake a CONNECT request to force connState to tunnel for ssl-bump
2021/01/29 17:31:02.491 kid1| 85,2| client_side_request.cc(753) clientAccessCheckDone: The request CONNECT http://52.217.88.134:443 is ALLOWED; last ACL checked: allowed_http_sites
2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(729) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(753) clientAccessCheckDone: The request CONNECT http://52.217.88.134:443 is ALLOWED; last ACL checked: allowed_http_sites
2021/01/29 17:31:02.494 kid1| 17,2| FwdState.cc(142) FwdState: Forwarding client request local=http://52.217.88.134:443 remote=http://10.10.1.249:43538 FD 13 flags=33, url=http://52.217.88.134:443
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths: Found sources for 'http://52.217.88.134:443'
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:   always_direct = DENIED
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:    never_direct = DENIED
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(310) peerSelectDnsPaths:    ORIGINAL_DST = local=0.0.0.0 remote=http://52.217.88.134:443 flags=1
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:        timedout = 0
2021/01/29 17:31:02.496 kid1| 83,2| bio.cc(316) readAndParse: parsing error on FD 15: check failed: state < atHelloReceived
    exception location: Handshake.cc(324) parseHandshakeMessage

2021/01/29 17:31:02.496 kid1| Error parsing SSL Server Hello Message on FD 15
2021/01/29 17:31:02.501 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to 52.217.88.134, opcode 3, len 13
2021/01/29 17:31:02.501| 42,2| IcmpPinger.cc(205) Recv:  Pass 52.217.88.134 off to ICMPv4 module.
2021/01/29 17:31:02.501| 42,2| Icmp.cc(95) Log: pingerLog: 1611941462.501640 52.217.88.134                                 32
2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.502| 42,2| IcmpPinger.cc(218) SendResult: return result to squid. len=7994
2021/01/29 17:31:02.502| 42,2| Icmp.cc(95) Log: pingerLog: 1611941462.502816 52.217.88.134                                 0 Echo Reply      1ms 6 hops
2021/01/29 17:31:02.514 kid1| 83,2| client_side.cc(2683) clientNegotiateSSL: New session 0x19d4690 on FD 13 (http://10.10.1.249:43538)
2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1306) parseHttpRequest: HTTP Client local=http://52.217.88.134:443 remote=http://10.10.1.249:43538 FD 13 flags=33
2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1310) parseHttpRequest: HTTP Client REQUEST:
---------
GET /http://test.XXXXX.com/testFile HTTP/1.1
Host: http://s3.amazonaws.com
User-Agent: curl/7.61.1
Accept: */*


----------
2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753) clientAccessCheckDone: The request GET https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL checked: allowed_http_sites
2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(729) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753) clientAccessCheckDone: The request GET https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL checked: allowed_http_sites
2021/01/29 17:31:02.520 kid1| 17,2| FwdState.cc(142) FwdState: Forwarding client request local=http://52.217.88.134:443 remote=http://10.10.1.249:43538 FD 13 flags=33, url=https://s3.amazonaws.com/test.XXXXX.com/testFile
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths: Find IP destination for: https://s3.amazonaws.com/test.XXXXX.com/testFile' via http://s3.amazonaws.com
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths: Found sources for 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:   always_direct = DENIED
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:    never_direct = DENIED
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(312) peerSelectDnsPaths:          PINNED = local=0.0.0.0 remote=http://52.216.80.75:443 flags=1
2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(310) peerSelectDnsPaths:    ORIGINAL_DST = local=0.0.0.0 remote=http://52.217.88.134:443 flags=1
2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:        timedout = 0
2021/01/29 17:31:02.521 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to 52.216.80.75, opcode 3, len 16
2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(205) Recv:  Pass 52.216.80.75 off to ICMPv4 module.
2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog: 1611941462.521215 52.216.80.75                                  32
2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2260) sendRequest: HTTP Server local=http://10.10.0.135:36120 remote=http://52.217.88.134:443 FD 15 flags=1
2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2261) sendRequest: HTTP Server REQUEST:
---------
GET /http://test.XXXXX.com/testFile HTTP/1.1
User-Agent: curl/7.61.1
Accept: */*
Host: http://s3.amazonaws.com
Via: 1.1 squid (squid/4.9)
X-Forwarded-For: 10.10.1.249
Cache-Control: max-age=259200
Connection: keep-alive


----------
2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(218) SendResult: return result to squid. len=7997
2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog: 1611941462.521561 52.216.80.75                                  0 Echo Reply      0ms 5 hops
2021/01/29 17:31:02.536 kid1| ctx: enter level  0: 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:31:02.536 kid1| 11,2| http.cc(719) processReplyHeader: HTTP Server local=http://10.10.0.135:36120 remote=http://52.217.88.134:443 FD 15 flags=1
2021/01/29 17:31:02.536 kid1| 11,2| http.cc(723) processReplyHeader: HTTP Server RESPONSE:
---------
HTTP/1.1 200 OK
x-amz-id-2: hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
x-amz-request-id: CD6D86AAA3FDA43F
Date: Fri, 29 Jan 2021 17:31:03 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3

----------
2021/01/29 17:31:02.536 kid1| ctx: exit level  0
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 88,2| client_side_reply.cc(2061) processReplyAccessResult: The reply for GET https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it matched allowed_http_sites
2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=http://52.217.88.134:443 remote=http://10.10.1.249:43538 FD 13 flags=33
2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 200 OK
x-amz-id-2: hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
x-amz-request-id: CD6D86AAA3FDA43F
Date: Fri, 29 Jan 2021 17:31:03 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/4.9)
Connection: keep-alive


----------
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.538 kid1| 33,2| client_side.cc(582) swanSong: local=http://52.217.88.134:443 remote=http://10.10.1.249:43538 flags=33
2021/01/29 17:31:02.538 kid1| 20,2| store.cc(986) checkCachable: StoreEntry::checkCachable: NO: not cachable
--------------------------------------------------------------------------------------------------------------------------------




On the other hand, with squidclient from the Squid itself, access log (the first run, when nothing is cached for the new test file yet):

------------------------------------------------------------------------------------------------------------------
1611942152.986     29 127.0.0.1 TCP_MISS/200 483 GET https://s3.amazonaws.com/test.dvabearqloza.com/testFile - HIER_DIRECT/http://52.216.226.131 binary/octet-stream
------------------------------------------------------------------------------------------------------------------

And cache log:
------------------------------------------------------------------------------------------------------------------
2021/01/29 17:42:32.956 kid1| 5,2| TcpAcceptor.cc(312) acceptNext: connection on local=[::]:3128 remote=[::] FD 28 flags=9
2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1306) parseHttpRequest: HTTP Client local=http://127.0.0.1:3128 remote=http://127.0.0.1:50584 FD 13 flags=1
2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1310) parseHttpRequest: HTTP Client REQUEST:
---------
GET https://s3.amazonaws.com/test.XXXXX.com/testFile HTTP/1.0
Host: http://s3.amazonaws.com
User-Agent: squidclient/4.9
Accept: */*
Connection: close

----------
2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753) clientAccessCheckDone: The request GET https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL checked: allowed_http_sites
2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(729) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753) clientAccessCheckDone: The request GET https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL checked: allowed_http_sites
2021/01/29 17:42:32.957 kid1| 17,2| FwdState.cc(142) FwdState: Forwarding client request local=http://127.0.0.1:3128 remote=http://127.0.0.1:50584 FD 13 flags=1, url=https://s3.amazonaws.com/test.XXXXX.com/testFile
2021/01/29 17:42:32.957 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths: Find IP destination for: https://s3.amazonaws.com/test.XXXXX.com/testFile' via http://s3.amazonaws.com
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths: Found sources for 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:   always_direct = DENIED
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:    never_direct = DENIED
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(308) peerSelectDnsPaths:          DIRECT = local=0.0.0.0 remote=http://52.216.226.131:443 flags=1
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:        timedout = 0
2021/01/29 17:42:32.961 kid1| 83,2| bio.cc(316) readAndParse: parsing error on FD 15: check failed: state < atHelloReceived
    exception location: Handshake.cc(324) parseHandshakeMessage

2021/01/29 17:42:32.961 kid1| Error parsing SSL Server Hello Message on FD 15
2021/01/29 17:42:32.965 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to 52.216.226.131, opcode 3, len 16
2021/01/29 17:42:32.965| 42,2| IcmpPinger.cc(205) Recv:  Pass 52.216.226.131 off to ICMPv4 module.
2021/01/29 17:42:32.965| 42,2| Icmp.cc(95) Log: pingerLog: 1611942152.965403 52.216.226.131                                32
2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2260) sendRequest: HTTP Server local=http://10.10.0.135:33004 remote=http://52.216.226.131:443 FD 15 flags=1
2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2261) sendRequest: HTTP Server REQUEST:
---------
GET /http://test.XXXXX.com/testFile HTTP/1.1
User-Agent: squidclient/4.9
Accept: */*
Host: http://s3.amazonaws.com
Via: 1.0 squid (squid/4.9)
X-Forwarded-For: 127.0.0.1
Cache-Control: max-age=259200
Connection: keep-alive


----------
2021/01/29 17:42:32.966| 42,2| IcmpPinger.cc(218) SendResult: return result to squid. len=7997
2021/01/29 17:42:32.966| 42,2| Icmp.cc(95) Log: pingerLog: 1611942152.966514 52.216.226.131                                0 Echo Reply      1ms 6 hops
2021/01/29 17:42:32.985 kid1| ctx: enter level  0: 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:42:32.985 kid1| 11,2| http.cc(719) processReplyHeader: HTTP Server local=http://10.10.0.135:33004 remote=http://52.216.226.131:443 FD 15 flags=1
2021/01/29 17:42:32.985 kid1| 11,2| http.cc(723) processReplyHeader: HTTP Server RESPONSE:
---------
HTTP/1.1 200 OK
x-amz-id-2: z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
x-amz-request-id: A6E14CC59FE63894
Date: Fri, 29 Jan 2021 17:42:33 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3

----------
2021/01/29 17:42:32.986 kid1| ctx: exit level  0
2021/01/29 17:42:32.986 kid1| 88,2| client_side_reply.cc(2061) processReplyAccessResult: The reply for GET https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it matched allowed_http_sites
2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=http://127.0.0.1:3128 remote=http://127.0.0.1:50584 FD 13 flags=1
2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 200 OK
x-amz-id-2: z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
x-amz-request-id: A6E14CC59FE63894
Date: Fri, 29 Jan 2021 17:42:33 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/4.9)
Connection: close


----------
2021/01/29 17:42:32.986 kid1| 20,2| store_io.cc(43) storeCreate: storeCreate: Selected dir 0 for e:=sp2V/0x1f582b0*4
2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(891) kick: local=http://127.0.0.1:3128 remote=http://127.0.0.1:50584 flags=1 Connection was closed
2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(582) swanSong: local=http://127.0.0.1:3128 remote=http://127.0.0.1:50584 flags=1
------------------------------------------------------------------------------------------------------------------

The first thing that caught my attention was the line:
"checkCachable: StoreEntry::checkCachable: NO: not cachable", that appears in the logs when server tries to go through proxy.

Any idea what might be the issue overall?

Thanks again!!!




On Fri, Jan 29, 2021 at 5:40 PM Alex Rousskov <mailto:rousskov at measurement-factory.com> wrote:
On 1/28/21 1:34 PM, Milos Dodic wrote:

> I have noticed that the test server also doesn't cache anything
> So if I try to go for a file in S3, it says MISS, and after that, MISS
> again, and I see no new objects in cache being created.

> If I try the same thing from the proxy itself, I get the MISS, and the
> object gets cached, as it should.
> When I go back to the test server, and try again, it sees the object in
> cache and returns TCP_MEM_HIT/200 instead.

Without more details, I can only speculate that the client running on
the test server sends different HTTP request headers than the client
running on the proxy itself. You can see the headers in cache.log if you
set debug_options to ALL,2. If you are not sure whether they are the
same, please share those logs. They will also contain response headers
and other potentially useful details.

If the request headers are the same in both tests, then I would
recommend sharing compressed ALL,7 or ALL,9 debugging logs of both
successful and unsuccessful tests (four transactions, two logs) for
analysis. Do not use sensitive data for these tests.

https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

Alex.



> This is the entire config file:
> 
> 
> visible_hostname squid
> cache_dir ufs /test/cache/squid 10000 16 256
> 
> http_access allow localhost
> http_access alow all
> 
> http_port 3128
> http_port 3129 intercept
> acl allowed_http_sites dstdomain .http://amazonaws.com <http://amazonaws.com>
> http_access allow allowed_http_sites
> 
> https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> acl SSL_port port 443
> http_access allow SSL_port
> acl allowed_https_sites ssl::server_name .http://amazonaws.com
> <http://amazonaws.com>
> 
> ssl_bump stare all
> ssl_bump bump allowed_https_sites
> ssl_bump terminate all


> On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov wrote:
> 
>     On 1/26/21 1:54 PM, Milos Dodic wrote:
> 
>     > when the test server goes for a picture I have stored somewhere in
>     > the cloud, the squid access log shows "TCP_TUNNEL/200". But when I
>     > try from the proxy itself with squidclient tool, I get
>     > "TCP_MEM_HIT/200"
> 
> 
>     Given the very limited information you have provided, I am guessing that
> 
>     * the primary tests opens a CONNECT tunnel through Squid
>     * the squidclient test sends a plain text HTTP request to Squid
> 
>     The final origin server destination may be the same in both tests, but
>     the two transactions are completely different from Squid point of view.
> 
> 
>     > ssl_bump peek step1 all
>     > ssl_bump peek step2 allowed_https_sites
>     > ssl_bump splice step3 allowed_https_sites
>     > ssl_bump terminate step3 all
> 
> 
>     AFAICT, this configuration is splicing or terminating all TLS traffic.
>     No bumping at all. If you want your Squid to bump TLS tunnels, then you
>     have to have at least one "bump" rule!
> 
>     I do not know what your overall SslBump needs are, but perhaps you meant
>     something like the following?
> 
>         acl shouldBeBumped ssl::server_name .http://amazonaws.com
>     <http://amazonaws.com>
> 
>         ssl_bump stare all
>         ssl_bump bump shouldBeBumped
>         ssl_bump terminate all
> 
>     Please do not use the configuration above until you understand what it
>     does. Please see https://wiki.squid-cache.org/Features/SslPeekAndSplice
>     for details.
> 
>     Depending on your environment, the http_access rules may need to be
>     adjusted to allow CONNECT requests (to TLS-safe ports) to IP addresses
>     that do not result in .http://amazonaws.com <http://amazonaws.com> in
>     reverse DNS lookups.
> 
> 
>     HTH,
> 
>     Alex.
> 



From robertkwild at gmail.com  Tue Feb  2 17:42:23 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 2 Feb 2021 17:42:23 +0000
Subject: [squid-users] Wildcard for url domain
Message-ID: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>

Hi all,

I know the .(dot) is a wild card for url domains ie

.onmicrosoft.com

But how would I do this

.autodiscover.*.onmicrosoft.com


Would it be


.autodiscover.onmicrosoft.com


Thanks,
Rob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210202/4fd24b7f/attachment.htm>

From bruno.larini at riosoft.com.br  Tue Feb  2 20:02:46 2021
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Tue, 2 Feb 2021 17:02:46 -0300
Subject: [squid-users] Wildcard for url domain
In-Reply-To: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>
References: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>
Message-ID: <140c6899-0d4b-a452-a9c9-d3dd847861ad@riosoft.com.br>

Em 02/02/2021 14:42, robert k Wild escreveu:
> Hi all,
>
> I know the .(dot) is a wild card for url domains ie
>
> .onmicrosoft.com <http://onmicrosoft.com>
>
> But how would I do this
>
> .autodiscover.*.onmicrosoft.com  <http://onmicrosoft.com>
> Would it be
> .autodiscover.onmicrosoft.com  <http://autodiscover.onmicrosoft.com>
>
> Thanks,
> Rob
>

Maybe

\.autodiscover\..*\.onmicrosoft\.com

but you should use dstdom_regex or ssl::server_name_regex for this one.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210202/bb96af01/attachment.htm>

From huaraz at moeller.plus.com  Tue Feb  2 20:02:50 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Tue, 2 Feb 2021 20:02:50 -0000
Subject: [squid-users] Squid for Windows: negotiate_kerberos_auth helper
 seems to leak(?) handles
In-Reply-To: <8251c91f-1b08-82f2-f6ec-46ef92fe9573@westkamp.net>
References: <dbd232bc-9dcf-4e4f-28b7-54c91dd85f5f@westkamp.net>
 <1cbdf070-04b3-5bf4-c987-8c5e7f8222de@treenet.co.nz>
 <aabdd2ba-5a58-1127-1097-9d5dcfe143db@westkamp.net>
 <8251c91f-1b08-82f2-f6ec-46ef92fe9573@westkamp.net>
Message-ID: <rvcb5e$n9$1@ciao.gmane.io>

Hi Klaus,

   The negotiate_kerberos_auth helper is not intended to run on Windows. 
How did you compile it ?

Markus



"Klaus Westkamp"  wrote in message 
news:8251c91f-1b08-82f2-f6ec-46ef92fe9573 at westkamp.net...

Hi,

i digged a little further (but i'm no exert in WinDBG):

Attachimng to the process with the most handles (currently 323 shown by
Windows Process Manager, as newly started)

!handles gives me:

277 Handles (weired, shows less than process manager)
Type               Count
None               4
Event              199
Section            7
File               18
Directory          3
SymbolicLink       1
Mutant             9
Semaphore          5
Key                8
Token              2
Thread             5
IoCompletion       2
TpWorkerFactory    2
ALPC Port          5
WaitCompletionPacket    7

Asking for Handle Details:

0:003> !handle 5e8 f
Handle 5e8
   Type             Event
   Attributes       0
   GrantedAccess    0x1f0003:
          Delete,ReadControl,WriteDac,WriteOwner,Synch
          QueryState,ModifyState
   HandleCount      2
   PointerCount     32769
   Name             <none>
   Object Specific Information
     Event Type Auto Reset
     Event is Waiting

0:003> !handle 5e0 f
Handle 5e0
   Type             Event
   Attributes       0
   GrantedAccess    0x1f0003:
          Delete,ReadControl,WriteDac,WriteOwner,Synch
          QueryState,ModifyState
   HandleCount      2
   PointerCount     32769
   Name             <none>
   Object Specific Information
     Event Type Auto Reset
     Event is Waiting

0:003> !handle 374 f
Handle 374
   Type             Event
   Attributes       0
   GrantedAccess    0x1f0003:
          Delete,ReadControl,WriteDac,WriteOwner,Synch
          QueryState,ModifyState
   HandleCount      2
   PointerCount     32769
   Name             <none>
   Object Specific Information
     Event Type Auto Reset
     Event is Waiting

These events seem to increase, but only one process gets to the limit of
3x00 handles and then the other processes seem to hang ...


On 15/12/2020 12:18, Klaus Westkamp wrote:
> Hi,
>
>
> yes this is Dildale's last available package. Output of squid -v is as 
> follows:
>
> squid -v
>
> Squid Cache: Version 3.5.28
> Service Name: squid
>
> This binary uses OpenSSL 1.0.2j  26 Sep 2016. For legal restrictions on 
> distribution see https://www.openssl.org/source/license.html
>
> configure options:  '--bindir=/bin/squid' '--sbindir=/usr/sbin/squid' 
> '--sysconfdir=/etc/squid' '--datadir=/usr/share/squid' 
> '--libexecdir=/usr/lib/squid'
> '--disable-strict-error-checking' '--with-logdir=/var/log/squid' 
> '--with-swapdir=/var/cache/squid' '--with-pidfile=/var/run/squid.pid' 
> '--enable-ssl'
> '--enable-delay-pools' '--enable-ssl-crtd' '--enable-icap-client' 
> '--disable-eui' '--localstatedir=/var/run/squid' 
> '--sharedstatedir=/var/run/squid'
> '--datarootdir=/usr/share/squid' 
> '--enable-disk-io=AIO,Blocking,DiskThreads,IpcIo,Mmapped' 
> '--enable-auth-basic=DB,LDAP,NCSA,POP3,RADIUS,SASL,SMB,fake,getpwnam'
> '--enable-auth-ntlm=fake' '--enable-auth-negotiate=kerberos,wrapper' 
> '--enable-external-acl-helpers=LDAP_group,SQL_session,eDirectory_userip,file_userip,kerberos_ldap_group,session,time_quota,unix_group,wbinfo_group'
> '--with-openssl' '--with-filedescriptors=65536' 
> '--enable-removal-policies=lru,heap'
>
> The helper negotiate_kerberos_auth.exe doesn't produce a Version output.
>
>
> Best regards,
>
> Klaus Westkamp
>
>
> On 15/12/2020 09:10, Amos Jeffries wrote:
>> On 15/12/20 4:03 am, Klaus Westkamp wrote:
>>> Hi,
>>>
>>> i'm uncertain, wether this mailing list is the correct one to ask, but i 
>>> have the disputable honor to make a squid running on a Windows Server 
>>> (if possible). Whilst squid.exe seems to run fine, i constantly run into 
>>> an unresponsive system, when i enable Kerberos authentication via 
>>> auth_param and the negotiate_kerberos_auth.exe helper.
>>>
>>> For a while authentication works fine, but all at the sudden the system 
>>> hangs at 100% CPU usage. My Observation is that one of the 
>>> negotiate_kerberos_auth.exe processes has a constantly increasing number 
>>> of handles (Files and events). If i understand the Sysinternals handle 
>>> tool correctly, most handles are event corrolated.
>>>
>>> The setting:
>>>
>>> Windows 2012 R2 AD Controllers with Windows 2008R2 Domain Level. A 
>>> Windows Server 2016 running Squid 3.5 for Windows.
>>
>> Is Squid the package built by Diladele or a custom build?
>>
>> Which exact version number is it? (output of "squid -v" please)
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From ngtech1ltd at gmail.com  Tue Feb  2 20:33:57 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 2 Feb 2021 22:33:57 +0200
Subject: [squid-users] Wildcard for url domain
In-Reply-To: <140c6899-0d4b-a452-a9c9-d3dd847861ad@riosoft.com.br>
References: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>
 <140c6899-0d4b-a452-a9c9-d3dd847861ad@riosoft.com.br>
Message-ID: <006701d6f9a2$bb8a4eb0$329eec10$@gmail.com>

I would use:

 

\.autodiscover\.[a-z0-9\-]+\.onmicrosoft\.com$

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Bruno de Paula Larini
Sent: Tuesday, February 2, 2021 10:03 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Wildcard for url domain

 

Em 02/02/2021 14:42, robert k Wild escreveu:



Hi all, 

 

I know the .(dot) is a wild card for url domains ie

 

.onmicrosoft.com <http://onmicrosoft.com> 

 

But how would I do this

 

.autodiscover.*.onmicrosoft.com <http://onmicrosoft.com> 
 
Would it be
 
.autodiscover.onmicrosoft.com <http://autodiscover.onmicrosoft.com> 

 

Thanks,

Rob

 


Maybe

\.autodiscover\..*\.onmicrosoft\.com

but you should use dstdom_regex or ssl::server_name_regex for this one.



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210202/46898b40/attachment.htm>

From robertkwild at gmail.com  Thu Feb  4 13:20:15 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 4 Feb 2021 13:20:15 +0000
Subject: [squid-users] Wildcard for url domain
In-Reply-To: <006701d6f9a2$bb8a4eb0$329eec10$@gmail.com>
References: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>
 <140c6899-0d4b-a452-a9c9-d3dd847861ad@riosoft.com.br>
 <006701d6f9a2$bb8a4eb0$329eec10$@gmail.com>
Message-ID: <CAGU_Ci+a=ZshPFrXQ4o-eua8nAek8aUJRX+6KCK482oRP7-XLw@mail.gmail.com>

thanks guys, i will have a play!!!!!!!!!!!!!

On Tue, 2 Feb 2021 at 20:34, Eliezer Croitoru <ngtech1ltd at gmail.com> wrote:

> I would use:
>
>
>
> \.autodiscover\.[a-z0-9\-]+\.onmicrosoft\.com$
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> Zoom: Coming soon
>
>
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *Bruno de Paula Larini
> *Sent:* Tuesday, February 2, 2021 10:03 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] Wildcard for url domain
>
>
>
> Em 02/02/2021 14:42, robert k Wild escreveu:
>
> Hi all,
>
>
>
> I know the .(dot) is a wild card for url domains ie
>
>
>
> .onmicrosoft.com
>
>
>
> But how would I do this
>
>
>
> .autodiscover.*.onmicrosoft.com
>
>
>
> Would it be
>
>
>
> .autodiscover.onmicrosoft.com
>
>
>
> Thanks,
>
> Rob
>
>
>
>
> Maybe
>
> \.autodiscover\..*\.onmicrosoft\.com
>
> but you should use dstdom_regex or ssl::server_name_regex for this one.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210204/ca33811d/attachment.htm>

From robertkwild at gmail.com  Thu Feb  4 14:59:54 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 4 Feb 2021 14:59:54 +0000
Subject: [squid-users] Wildcard for url domain
In-Reply-To: <CAGU_Ci+a=ZshPFrXQ4o-eua8nAek8aUJRX+6KCK482oRP7-XLw@mail.gmail.com>
References: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>
 <140c6899-0d4b-a452-a9c9-d3dd847861ad@riosoft.com.br>
 <006701d6f9a2$bb8a4eb0$329eec10$@gmail.com>
 <CAGU_Ci+a=ZshPFrXQ4o-eua8nAek8aUJRX+6KCK482oRP7-XLw@mail.gmail.com>
Message-ID: <CAGU_CiKCuhqsgu8NDSOyC9YCZyc7b0OyXQ6Y1bNqfYpKX50QYg@mail.gmail.com>

just out of interest, could i do this, or do i need to do backslashes for
the acl "ssl::server_name_regex"

.autodiscover.[[:alnum:]].onmicrosoft.com


On Thu, 4 Feb 2021 at 13:20, robert k Wild <robertkwild at gmail.com> wrote:

> thanks guys, i will have a play!!!!!!!!!!!!!
>
> On Tue, 2 Feb 2021 at 20:34, Eliezer Croitoru <ngtech1ltd at gmail.com>
> wrote:
>
>> I would use:
>>
>>
>>
>> \.autodiscover\.[a-z0-9\-]+\.onmicrosoft\.com$
>>
>>
>>
>> Eliezer
>>
>>
>>
>> ----
>>
>> Eliezer Croitoru
>>
>> Tech Support
>>
>> Mobile: +972-5-28704261
>>
>> Email: ngtech1ltd at gmail.com
>>
>> Zoom: Coming soon
>>
>>
>>
>>
>>
>> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
>> Behalf Of *Bruno de Paula Larini
>> *Sent:* Tuesday, February 2, 2021 10:03 PM
>> *To:* squid-users at lists.squid-cache.org
>> *Subject:* Re: [squid-users] Wildcard for url domain
>>
>>
>>
>> Em 02/02/2021 14:42, robert k Wild escreveu:
>>
>> Hi all,
>>
>>
>>
>> I know the .(dot) is a wild card for url domains ie
>>
>>
>>
>> .onmicrosoft.com
>>
>>
>>
>> But how would I do this
>>
>>
>>
>> .autodiscover.*.onmicrosoft.com
>>
>>
>>
>> Would it be
>>
>>
>>
>> .autodiscover.onmicrosoft.com
>>
>>
>>
>> Thanks,
>>
>> Rob
>>
>>
>>
>>
>> Maybe
>>
>> \.autodiscover\..*\.onmicrosoft\.com
>>
>> but you should use dstdom_regex or ssl::server_name_regex for this one.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210204/36020eca/attachment.htm>

From premchand142 at gmail.com  Thu Feb  4 15:32:40 2021
From: premchand142 at gmail.com (Prem Chand)
Date: Thu, 4 Feb 2021 21:02:40 +0530
Subject: [squid-users] SSL Squid 5 Cipher suite ordering issue
Message-ID: <CACbtF4NNm-_mcjJtGJ=Z7AuhTmM-sJNcyFGmy7fSVYqgJJ4w+Q@mail.gmail.com>

Hi Team,

I'm running SSL squid 5 on Centos 8 and I could see Cipher Suites order
changes when I access the below website through Squid and without using
squid I'm getting correct order.

https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html

I want to know why and how Squid is changing the cipher suite order and how
to stop squid from doing it. Please advise.

Thank you
Premchand Naidu.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210204/9e41967f/attachment.htm>

From rousskov at measurement-factory.com  Thu Feb  4 16:39:11 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Feb 2021 11:39:11 -0500
Subject: [squid-users] SSL Squid 5 Cipher suite ordering issue
In-Reply-To: <CACbtF4NNm-_mcjJtGJ=Z7AuhTmM-sJNcyFGmy7fSVYqgJJ4w+Q@mail.gmail.com>
References: <CACbtF4NNm-_mcjJtGJ=Z7AuhTmM-sJNcyFGmy7fSVYqgJJ4w+Q@mail.gmail.com>
Message-ID: <7151e716-55d4-1156-2cca-7cbee93f2841@measurement-factory.com>

On 2/4/21 10:32 AM, Prem Chand wrote:

> I'm running SSL squid 5 on Centos 8 and I could see Cipher Suites order
> changes when I access the below website through Squid and without using
> squid I'm getting correct order.
> 
> https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html
> 
> I want to know why and how Squid is changing the cipher suite order and
> how to stop squid from doing it. Please advise.

There are several different use cases related to cipher order in
Squid-server connections, including these TLS v1.2 (and earlier) cases:

1. You are specifying "tls_outgoing_options cihper", and you are either
not using SslBump or bumping the TLS client during SslBump step1. In
this case, Squid should pass your tls_outgoing_options cipher
configuration to OpenSSL. What happens next is up to OpenSSL.

2. You are not specifying "tls_outgoing_options cihper", and you are
either not using SslBump or bumping the TLS client during SslBump step1.
In this case, Squid does not tell what ciphers to use. What happens next
is up to OpenSSL.

3. You are bumping the TLS client during SslBump step2. In this case,
Squid should give TLS client ciphers to OpenSSL. What happens next is up
to OpenSSL.

4. You are bumping the TLS client during SslBump step3. I am not sure
what should happen here, but perhaps Squid should, during step2, forward
TLS client ciphers that Squid supports, in TLS client order. I do not
know whether Squid actually does that.

To understand why ciphers are reordered, you need to figure out which
use case applies to your test and, if applicable, investigate whether
OpenSSL honors the cipher order specified in SSL_set_cipher_list() and
SSL_CTX_set_cipher_list() calls.

If you confirm that Squid should give the cipher list to OpenSSL in your
use case, and OpenSSL should honor the cipher order, then you can look
for Squid bugs, but that will probably require debugging log analysis
and developer-level expertise.


See also:
http://lists.squid-cache.org/pipermail/squid-users/2021-January/023155.html


HTH,

Alex.


From robertkwild at gmail.com  Thu Feb  4 16:48:15 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 4 Feb 2021 16:48:15 +0000
Subject: [squid-users] Wildcard for url domain
In-Reply-To: <CAGU_CiKCuhqsgu8NDSOyC9YCZyc7b0OyXQ6Y1bNqfYpKX50QYg@mail.gmail.com>
References: <CAGU_CiLynPXxjRUkVWu8CutD5ySop_O2mM+hstZKjU3o3Dnjew@mail.gmail.com>
 <140c6899-0d4b-a452-a9c9-d3dd847861ad@riosoft.com.br>
 <006701d6f9a2$bb8a4eb0$329eec10$@gmail.com>
 <CAGU_Ci+a=ZshPFrXQ4o-eua8nAek8aUJRX+6KCK482oRP7-XLw@mail.gmail.com>
 <CAGU_CiKCuhqsgu8NDSOyC9YCZyc7b0OyXQ6Y1bNqfYpKX50QYg@mail.gmail.com>
Message-ID: <CAGU_CiKZsGeS0epRvq8eJ7EByModORiCNun7sENTja-LK_jbuQ@mail.gmail.com>

scrap this im being such a donut

i will just do as its so much easier

.onmicrosoft.com



On Thu, 4 Feb 2021 at 14:59, robert k Wild <robertkwild at gmail.com> wrote:

> just out of interest, could i do this, or do i need to do backslashes for
> the acl "ssl::server_name_regex"
>
> .autodiscover.[[:alnum:]].onmicrosoft.com
>
>
> On Thu, 4 Feb 2021 at 13:20, robert k Wild <robertkwild at gmail.com> wrote:
>
>> thanks guys, i will have a play!!!!!!!!!!!!!
>>
>> On Tue, 2 Feb 2021 at 20:34, Eliezer Croitoru <ngtech1ltd at gmail.com>
>> wrote:
>>
>>> I would use:
>>>
>>>
>>>
>>> \.autodiscover\.[a-z0-9\-]+\.onmicrosoft\.com$
>>>
>>>
>>>
>>> Eliezer
>>>
>>>
>>>
>>> ----
>>>
>>> Eliezer Croitoru
>>>
>>> Tech Support
>>>
>>> Mobile: +972-5-28704261
>>>
>>> Email: ngtech1ltd at gmail.com
>>>
>>> Zoom: Coming soon
>>>
>>>
>>>
>>>
>>>
>>> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
>>> Behalf Of *Bruno de Paula Larini
>>> *Sent:* Tuesday, February 2, 2021 10:03 PM
>>> *To:* squid-users at lists.squid-cache.org
>>> *Subject:* Re: [squid-users] Wildcard for url domain
>>>
>>>
>>>
>>> Em 02/02/2021 14:42, robert k Wild escreveu:
>>>
>>> Hi all,
>>>
>>>
>>>
>>> I know the .(dot) is a wild card for url domains ie
>>>
>>>
>>>
>>> .onmicrosoft.com
>>>
>>>
>>>
>>> But how would I do this
>>>
>>>
>>>
>>> .autodiscover.*.onmicrosoft.com
>>>
>>>
>>>
>>> Would it be
>>>
>>>
>>>
>>> .autodiscover.onmicrosoft.com
>>>
>>>
>>>
>>> Thanks,
>>>
>>> Rob
>>>
>>>
>>>
>>>
>>> Maybe
>>>
>>> \.autodiscover\..*\.onmicrosoft\.com
>>>
>>> but you should use dstdom_regex or ssl::server_name_regex for this one.
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>> --
>> Regards,
>>
>> Robert K Wild.
>>
>
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210204/59597b20/attachment.htm>

From alex at esines.cu  Thu Feb  4 20:49:43 2021
From: alex at esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez?=)
Date: Thu, 4 Feb 2021 15:49:43 -0500
Subject: [squid-users] problen whith authentication
Message-ID: <acd33a78-c0dc-d539-1028-ed1c700dbc59@esines.cu>

HI community, reciently I install an old UBT 18.04 with squid 3. I use 
to authenticate my users kerberos.

Everithing seem?s great, but my all my users are able to use the proxy, 
instead of the few in the *conexion* group.

Can anyone be so nice to tell me what?s wrong on my config?

Thanks in advance.


httpd_suppress_version_string on
visible_hostname Proxy
via off
forwarded_for off
follow_x_forwarded_for deny all
error_directory /usr/share/squid_error
acl SSL_ports port 443
acl Safe_ports port 21 # ftp
acl Safe_ports port 80 # http
acl Safe_ports port 81 # http_cubaindustria
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 443 # https
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 1025-65535 # unregistered ports
acl CONNECT method CONNECT
#################
#sqstat
#################
acl webserver src proxy.esines.cu
http_access allow manager webserver
http_access deny manager
##########################################
# Logs:
access_log /var/log/squid/access.log squid !manager
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
log_uses_indirect_client on
##########################################
#No Permitimos los puertos inseguros
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
##########################################
#permitir todo lo que sea de cuba
##########################################
acl cuba dstdomain .cu
http_access allow cuba all
########################################################
#auth kerberos de windows#
########################################################
auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth -d 
-s HTTP/proxy.esines.cu at ESINES.CU -k /etc/squid/proxy.keytab
external_acl_type Group ipv4 children-startup=10 children-max=15 ttl=300 
negative_ttl=60 %LOGIN /usr/lib/squid/ext_kerberos_ldap_group_acl -a -D 
ESINES.CU
external_acl_type Group %LOGIN 
/usr/lib/squid/ext_kerberos_ldap_group_acl -g conexion at ESINES.CU
acl auth proxy_auth REQUIRED
acl GrupoInternet external Group
http_access allow auth GrupoInternet
http_access deny !auth
http_access allow auth
authenticate_ip_ttl 600 seconds
acl multilogin max_user_ip -s 1
http_access deny multilogin
########################################################
#definicion de horarios#
########################################################
acl horafb time MTWHF 09:00-12:00 14:00-16:00
########################################################
#No permitir navegacion por ip
########################################################
acl bloquear_ip url_regex [0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}
http_access deny bloquear_ip
########################################################
#declaracion de acl no b?sicas#
########################################################
acl lista-negra dstdomain -n "/etc/squid/lista-negra"
acl sociales dstdomain -n "/etc/squid/sociales"
acl pcinternet src "/etc/squid/ip-internet"
http_access deny lista-negra
http_access deny sociales horafb
#########################################################################
#proxy padre
#########################################################################
cache_peer proxyservicio.etecsa.cu parent 3040 0
#########################################################################
#nunca permitimos conexiones directas, siempre a traves del proxy
#########################################################################
never_direct allow all
#######################################################################
# puerto en que el proxy escuchara a los clientes
http_port 8569
#########################################################################
#########################################################################
#Cache #
#########################################################################
delay_initial_bucket_level 75
maximum_object_size 32 MB
#cache_dir aufs /var/cache/squid 10240 16 256
cache_dir aufs /var/squid 1024 16 256
cache_mem 256 MB
cache_store_log /var/squid/cache_store.log
coredump_dir /var/squid/dump
minimum_expiry_time 550 seconds
############################
#uso cache
###########################
client_db off
offline_mode off
cache_swap_low 93
cache_swap_high 97
cache_replacement_policy heap LUDFA
memory_replacement_policy heap GDSF
maximum_object_size_in_memory 512 KB
half_closed_clients off
###############################################################################
# establecemos los archivos de volcado en /var/cache/squid/
coredump_dir /var/squid/
###############################################################################
#Establecemos los patrones de refrescamiento de la cache #
#patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de 
refresc amiento -- tiempo #
#1440 minutos equivalen a 24 horas #
################################
#Refrescamiento de la cache
################################
refresh_pattern ^ftp: 1440 20% 4320
refresh_pattern ^gopher: 1440 0% 4320
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 4320 
override-expire ignor???????????????????????????????????????????????? 
e-no-store ignore-private
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 1440 90% 
43200 ov erride-expire ignore-no-store ignore-private
refresh_pattern -i 
\.(deb|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff|pdf|xls|docx| 
xlsx|pptx)$ 1440 90% 4320 override-expire ignore-no-store ignore-private
refresh_pattern -i \.index.(html|htm)$ 1440 70% 4320
refresh_pattern -i \.(html|htm|css|js)$ 1440 70% 4320
refresh_pattern . 1440 40% 4320
######################################################
##cuanto el squid intenta cachear en mi nombre
read_ahead_gap 200 KB
quick_abort_min 1024 KB
quick_abort_max 16 KB
quick_abort_pct 95
###############################################################################
#defino las piscinas de retardo
###############################################################################
delay_pools 2

#Canal 1 advertising
delay_class 1 2
delay_access 1 allow sociales !GrupoInternet
delay_access 1 deny all
delay_parameters 1 32768/16348 16348/16348

#Canal 1 sociales
delay_class 2 1
delay_access 2 allow pcinternet
delay_access 2 deny all
delay_parameters 2 65536/32768

-- 
Saludos cordiales

Lic. Alex Guti?rrez Mart?nez

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210204/f6558e4c/attachment.htm>

From huaraz at moeller.plus.com  Fri Feb  5 00:28:18 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Fri, 5 Feb 2021 00:28:18 -0000
Subject: [squid-users] problen whith authentication
In-Reply-To: <acd33a78-c0dc-d539-1028-ed1c700dbc59@esines.cu>
References: <acd33a78-c0dc-d539-1028-ed1c700dbc59@esines.cu>
Message-ID: <rvi3f4$va4$1@ciao.gmane.io>

What does he cache log show  ?

Markus

"Alex Guti?rrez" <alex at esines.cu> wrote in message news:acd33a78-c0dc-d539-1028-ed1c700dbc59 at esines.cu...
HI community, reciently I install an old UBT 18.04 with squid 3. I use to authenticate my users kerberos.

Everithing seem?s great, but my all my users are able to use the proxy, instead of the few in the conexion group.

Can anyone be so nice to tell me what?s wrong on my config?

Thanks in advance.



httpd_suppress_version_string on
visible_hostname Proxy
via off
forwarded_for off
follow_x_forwarded_for deny all
error_directory /usr/share/squid_error
acl SSL_ports port 443
acl Safe_ports port 21 # ftp
acl Safe_ports port 80 # http
acl Safe_ports port 81 # http_cubaindustria
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 443 # https
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 1025-65535 # unregistered ports
acl CONNECT method CONNECT
#################
#sqstat
#################
acl webserver src proxy.esines.cu
http_access allow manager webserver
http_access deny manager
##########################################
# Logs:
access_log /var/log/squid/access.log squid !manager
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
log_uses_indirect_client on
##########################################
#No Permitimos los puertos inseguros
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
##########################################
#permitir todo lo que sea de cuba
##########################################
acl cuba dstdomain .cu
http_access allow cuba all
########################################################
#auth kerberos de windows#
########################################################
auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth -d -s HTTP/proxy.esines.cu at ESINES.CU -k /etc/squid/proxy.keytab
external_acl_type Group ipv4 children-startup=10 children-max=15 ttl=300 negative_ttl=60 %LOGIN /usr/lib/squid/ext_kerberos_ldap_group_acl -a -D ESINES.CU
external_acl_type Group %LOGIN /usr/lib/squid/ext_kerberos_ldap_group_acl -g conexion at ESINES.CU
acl auth proxy_auth REQUIRED
acl GrupoInternet external Group
http_access allow auth GrupoInternet
http_access deny !auth
http_access allow auth
authenticate_ip_ttl 600 seconds
acl multilogin max_user_ip -s 1
http_access deny multilogin
########################################################
#definicion de horarios#
########################################################
acl horafb time MTWHF 09:00-12:00 14:00-16:00
########################################################
#No permitir navegacion por ip
########################################################
acl bloquear_ip url_regex [0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}
http_access deny bloquear_ip
########################################################
#declaracion de acl no b?sicas#
########################################################
acl lista-negra dstdomain -n "/etc/squid/lista-negra"
acl sociales dstdomain -n "/etc/squid/sociales"
acl pcinternet src "/etc/squid/ip-internet"
http_access deny lista-negra
http_access deny sociales horafb
#########################################################################
#proxy padre
#########################################################################
cache_peer proxyservicio.etecsa.cu parent 3040 0
#########################################################################
#nunca permitimos conexiones directas, siempre a traves del proxy
#########################################################################
never_direct allow all
#######################################################################
# puerto en que el proxy escuchara a los clientes
http_port 8569
#########################################################################
#########################################################################
#Cache #
#########################################################################
delay_initial_bucket_level 75
maximum_object_size 32 MB
#cache_dir aufs /var/cache/squid 10240 16 256
cache_dir aufs /var/squid 1024 16 256
cache_mem 256 MB
cache_store_log /var/squid/cache_store.log
coredump_dir /var/squid/dump
minimum_expiry_time 550 seconds
############################
#uso cache
###########################
client_db off
offline_mode off
cache_swap_low 93
cache_swap_high 97
cache_replacement_policy heap LUDFA
memory_replacement_policy heap GDSF
maximum_object_size_in_memory 512 KB
half_closed_clients off
###############################################################################
# establecemos los archivos de volcado en /var/cache/squid/
coredump_dir /var/squid/
###############################################################################
#Establecemos los patrones de refrescamiento de la cache #
#patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de refresc                                                 amiento -- tiempo #
#1440 minutos equivalen a 24 horas #
################################
#Refrescamiento de la cache
################################
refresh_pattern ^ftp: 1440 20% 4320
refresh_pattern ^gopher: 1440 0% 4320
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 4320 override-expire ignor                                                 e-no-store ignore-private
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 1440 90% 43200 ov                                                 erride-expire ignore-no-store ignore-private
refresh_pattern -i \.(deb|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff|pdf|xls|docx|                                                 xlsx|pptx)$ 1440 90% 4320 override-expire ignore-no-store ignore-private
refresh_pattern -i \.index.(html|htm)$ 1440 70% 4320
refresh_pattern -i \.(html|htm|css|js)$ 1440 70% 4320
refresh_pattern . 1440 40% 4320
######################################################
##cuanto el squid intenta cachear en mi nombre
read_ahead_gap 200 KB
quick_abort_min 1024 KB
quick_abort_max 16 KB
quick_abort_pct 95
###############################################################################
#defino las piscinas de retardo
###############################################################################
delay_pools 2

#Canal 1 advertising
delay_class 1 2
delay_access 1 allow sociales !GrupoInternet
delay_access 1 deny all
delay_parameters 1 32768/16348 16348/16348

#Canal 1 sociales
delay_class 2 1
delay_access 2 allow pcinternet
delay_access 2 deny all
delay_parameters 2 65536/32768



-- 
Saludos cordiales

Lic. Alex Guti?rrez Mart?nez



--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210205/c934db7b/attachment.htm>

From ngtech1ltd at gmail.com  Sun Feb  7 17:47:07 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 7 Feb 2021 19:47:07 +0200
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
Message-ID: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>

Since 5.0.4 tests had mixed results of success and failure I move on to
testing squid-6.0.0-20210204-r5f37a71ac

Most of the issues I see are related to Host header forgery detection.

I do see that the main issue with TLS is similar to:

2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS connection on
conn2195 local=216.58.198.67:443 remote=192.168.189.94:41724 FD 104
flags=33: 0x55cf6a6debe0*1

    current master transaction: master78

 

which is a google host related issue.

 

Alex and Amos,

Can the project do something about this?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210207/a2b9eb33/attachment.htm>

From ngtech1ltd at gmail.com  Mon Feb  8 09:48:06 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 8 Feb 2021 11:48:06 +0200
Subject: [squid-users] Port or switch level authorization
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAMjhNBnuf5JHvzSznQ5DJvMBAAAAAA==@gmail.com>

I have a Mikrotik PPPOE server and I would like to register the logged in
user on PPPOE Tunnel creation.
In the mikroitk device I have a code which can run a curl/fetch request with
the login details ie IP and username towards any server.
I was thinking about creating a PHP api that will be allowed access only
from the Mikrotik devices.
On every login the user+IP pairs will be written to a small DB.
Squid in it's turn will use an external helper to run queries against the DB
per request with small cache of 3-10 seconds.

What's the best way to pass a username so with the ip it will be logged.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon





From ml+squidusers at kisswebdev.com  Mon Feb  8 14:40:39 2021
From: ml+squidusers at kisswebdev.com (Chris)
Date: Mon, 8 Feb 2021 15:40:39 +0100
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
Message-ID: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>

Hi all,

I'm trying to figure out the best way to use squid (version 3.5.27) in 
reverse proxy mode in regard to originserver health checks and load 
balancing.

So far I had been using the round-robin originserver cache peer 
selection algorithm while using weight to favor originservers with 
closer proximity/lower latency.

The problem: if one cache_peer is dead it takes ages for squid to choose 
the second originserver. It does look as if (e.g. if one originserver 
has a weight of 32, the other of 2) squid tries the dead server several 
times before accessing the other one.

Now instead of using round-robin plus weight it would be best to use 
weighted-round-robin. But as I understand it, this wouldn't work with 
originserver if (as it's normally the case) the originserver won't 
handle icp or htcp requests. Did I miss sth. here? Would background-ping 
work?

I tried weighted-round-robin and background-ping on originservers but 
got only an evenly distributed request handling even if ones 
originservers rtt would be less than half of the others. But then again, 
those originservers won't handle icp requests.

So what's the best solution to a) choose the originserver with the 
lowest rtt and b) still have a fast switch if one of the originservers 
switches into dead state?

Would I have to span another proxy (like e.g. HAProxy) between Squid and 
originserver or better install Squid on those originservers as well 
(only for serving icp requests from the squid fellows)?

Is there a better way to update the dead state of an originserver?

How do you handle this?

Thanks a lot,

Chris



From ngtech1ltd at gmail.com  Mon Feb  8 23:14:18 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 9 Feb 2021 01:14:18 +0200
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
Message-ID: <003301d6fe70$20f46c50$62dd44f0$@gmail.com>

Hey Chris,

The main question is " what do you need squid for?"
If you need squid for caching it one thing.
RFC compliance is another thing..
Anyway Haproxy is better in Load Balancing and traffic control/management.
If you need load balancer use haproxy.
If you need caching for very specific known use cases then use it.
For general purpose these days it might not work as you might expect.

Take into account that browsers cache lots of things, even these who shouldn't so the gain/profit
should be tested first.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Chris
Sent: Monday, February 8, 2021 4:41 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Originserver load balancing and health checks in Squid reverse proxy mode

Hi all,

I'm trying to figure out the best way to use squid (version 3.5.27) in 
reverse proxy mode in regard to originserver health checks and load 
balancing.

So far I had been using the round-robin originserver cache peer 
selection algorithm while using weight to favor originservers with 
closer proximity/lower latency.

The problem: if one cache_peer is dead it takes ages for squid to choose 
the second originserver. It does look as if (e.g. if one originserver 
has a weight of 32, the other of 2) squid tries the dead server several 
times before accessing the other one.

Now instead of using round-robin plus weight it would be best to use 
weighted-round-robin. But as I understand it, this wouldn't work with 
originserver if (as it's normally the case) the originserver won't 
handle icp or htcp requests. Did I miss sth. here? Would background-ping 
work?

I tried weighted-round-robin and background-ping on originservers but 
got only an evenly distributed request handling even if ones 
originservers rtt would be less than half of the others. But then again, 
those originservers won't handle icp requests.

So what's the best solution to a) choose the originserver with the 
lowest rtt and b) still have a fast switch if one of the originservers 
switches into dead state?

Would I have to span another proxy (like e.g. HAProxy) between Squid and 
originserver or better install Squid on those originservers as well 
(only for serving icp requests from the squid fellows)?

Is there a better way to update the dead state of an originserver?

How do you handle this?

Thanks a lot,

Chris

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Feb  9 03:23:44 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Feb 2021 16:23:44 +1300
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
Message-ID: <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>

On 9/02/21 3:40 am, Chris wrote:
> Hi all,
> 
> I'm trying to figure out the best way to use squid (version 3.5.27) in 
> reverse proxy mode in regard to originserver health checks and load 
> balancing.
> 
> So far I had been using the round-robin originserver cache peer 
> selection algorithm while using weight to favor originservers with 
> closer proximity/lower latency.
> 

Ok.


> The problem: if one cache_peer is dead it takes ages for squid to choose 
> the second originserver. It does look as if (e.g. if one originserver 
> has a weight of 32, the other of 2) squid tries the dead server several 
> times before accessing the other one.
> 

The DEAD check by default requires 10 failures in a row to trigger. This 
is configurable with the connect-fail-limit=N option.


> Now instead of using round-robin plus weight it would be best to use 
> weighted-round-robin. But as I understand it, this wouldn't work with 
> originserver if (as it's normally the case) the originserver won't 
> handle icp or htcp requests. Did I miss sth. here? Would background-ping 
> work?

Well, kind of.

ICP/HTCP is just a protocol. Most origin servers do not support them, 
but some do. Especially if the server is not a true origin but a 
reverse-proxy.


> 
> I tried weighted-round-robin and background-ping on originservers but 
> got only an evenly distributed request handling even if ones 
> originservers rtt would be less than half of the others. But then again, 
> those originservers won't handle icp requests.

RTT is retrieved from ICMP data primarily. Check your Squid is built 
with --enable-icmp, the pinger helper is operational, and that ICMP Echo 
traffic is working on all possible network routes between your Squid and 
the peer server(s).


> 
> So what's the best solution to a) choose the originserver with the 
> lowest rtt and b) still have a fast switch if one of the originservers 
> switches into dead state?


Check whether the RTT is actually being measured properly by Squid 
(debug_options ALL,1 44,3 15,8). If the peers are fast enough responding 
or close enough in the network RTT could come out as a 0 value or some N 
value equal for both peer. ie. neither being "closer".


Amos


From squid3 at treenet.co.nz  Tue Feb  9 03:29:53 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Feb 2021 16:29:53 +1300
Subject: [squid-users] Port or switch level authorization
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAMjhNBnuf5JHvzSznQ5DJvMBAAAAAA==@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAMjhNBnuf5JHvzSznQ5DJvMBAAAAAA==@gmail.com>
Message-ID: <6a3d522e-3970-e4ac-5d87-ce1b9f4095d3@treenet.co.nz>

On 8/02/21 10:48 pm, Eliezer Croitoru wrote:
> I have a Mikrotik PPPOE server and I would like to register the logged in
> user on PPPOE Tunnel creation.
> In the mikroitk device I have a code which can run a curl/fetch request with
> the login details ie IP and username towards any server.
> I was thinking about creating a PHP api that will be allowed access only
> from the Mikrotik devices.
> On every login the user+IP pairs will be written to a small DB.
> Squid in it's turn will use an external helper to run queries against the DB
> per request with small cache of 3-10 seconds.

Do you mean the ext_session_sql_acl helper?

> 
> What's the best way to pass a username so with the ip it will be logged.
> 

The helper needs to return user= kv-pair to Squid for this to be an 
"authentication" rather than just authorization. That username will be 
logged without anything special having to be done.

Amos


From ngtech1ltd at gmail.com  Tue Feb  9 09:15:59 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 9 Feb 2021 11:15:59 +0200
Subject: [squid-users] Port or switch level authorization
In-Reply-To: <6a3d522e-3970-e4ac-5d87-ce1b9f4095d3@treenet.co.nz>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAMjhNBnuf5JHvzSznQ5DJvMBAAAAAA==@gmail.com>
 <6a3d522e-3970-e4ac-5d87-ce1b9f4095d3@treenet.co.nz>
Message-ID: <000601d6fec4$2f006b10$8d014130$@gmail.com>

Thanks Amos,

OK this seems to answer my question.
A session helper with ttl=3 should be enough if it will return the username associated by the helper.

The next thing is to block traffic if there is no username.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, February 9, 2021 5:30 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Port or switch level authorization

On 8/02/21 10:48 pm, Eliezer Croitoru wrote:
> I have a Mikrotik PPPOE server and I would like to register the logged in
> user on PPPOE Tunnel creation.
> In the mikroitk device I have a code which can run a curl/fetch request with
> the login details ie IP and username towards any server.
> I was thinking about creating a PHP api that will be allowed access only
> from the Mikrotik devices.
> On every login the user+IP pairs will be written to a small DB.
> Squid in it's turn will use an external helper to run queries against the DB
> per request with small cache of 3-10 seconds.

Do you mean the ext_session_sql_acl helper?

> 
> What's the best way to pass a username so with the ip it will be logged.
> 

The helper needs to return user= kv-pair to Squid for this to be an 
"authentication" rather than just authorization. That username will be 
logged without anything special having to be done.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ml+squidusers at kisswebdev.com  Tue Feb  9 15:03:10 2021
From: ml+squidusers at kisswebdev.com (Chris)
Date: Tue, 9 Feb 2021 16:03:10 +0100
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
Message-ID: <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>

Hi,

thank you Amos, this is bringing me into the right direction.

Now I know what I'll have to debug: the pinger.

Cache.log shows:

2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
2021/02/09 14:49:27| pinger: ICMP socket opened.
2021/02/09 14:49:27| pinger: ICMPv6 socket opened
2021/02/09 14:49:27| Pinger exiting.

and that last line "pinger exiting" looks like a problem here.

Squid is used as a package from ubuntu bionic, it's configured with 
"--enable-icmp" as stated by squid -v.

Now I explicitly wrote a "pinger_enable on" and the pinger_program path 
(in this case: "/usr/lib/squid/pinger" ) into the squid.conf? (as well 
as icmp_query on) and reconfigured but the cache.log still shows:

"Pinger exiting"

So I don't understand why the pinger is exiting. The pinger_program is 
owned by root and has 0755 execution rights. Normal ping commands do 
work and show the one originserver at ttl=53 and time=50 while the other 
is at ttl=56 and time=155 - so a RTT comparison for weighted-round-robin 
should work here.

Any hints on how I can find out why the pinger is exiting? Right now I'm 
debuging with debug_options ALL,1 44,3 15,8 but don't see a reason why 
the pinger exits.

The Originservers are defined by (with icp/htcp disabled):

cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest 
no-netdb-exchange weighted-round-robin originserver name=srv1 
forceddomain=[domainname]

cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest 
no-netdb-exchange weighted-round-robin originserver name=srv2 
forceddomain=[domainname]


Thank you for your help,

Chris





On 09.02.21 04:23, Amos Jeffries wrote:
> On 9/02/21 3:40 am, Chris wrote:
>> Hi all,
>>
>> I'm trying to figure out the best way to use squid (version 3.5.27) 
>> in reverse proxy mode in regard to originserver health checks and 
>> load balancing.
>>
>> So far I had been using the round-robin originserver cache peer 
>> selection algorithm while using weight to favor originservers with 
>> closer proximity/lower latency.
>>
>
> Ok.
>
>
>> The problem: if one cache_peer is dead it takes ages for squid to 
>> choose the second originserver. It does look as if (e.g. if one 
>> originserver has a weight of 32, the other of 2) squid tries the dead 
>> server several times before accessing the other one.
>>
>
> The DEAD check by default requires 10 failures in a row to trigger. 
> This is configurable with the connect-fail-limit=N option.
>
>
>> Now instead of using round-robin plus weight it would be best to use 
>> weighted-round-robin. But as I understand it, this wouldn't work with 
>> originserver if (as it's normally the case) the originserver won't 
>> handle icp or htcp requests. Did I miss sth. here? Would 
>> background-ping work?
>
> Well, kind of.
>
> ICP/HTCP is just a protocol. Most origin servers do not support them, 
> but some do. Especially if the server is not a true origin but a 
> reverse-proxy.
>
>
>>
>> I tried weighted-round-robin and background-ping on originservers but 
>> got only an evenly distributed request handling even if ones 
>> originservers rtt would be less than half of the others. But then 
>> again, those originservers won't handle icp requests.
>
> RTT is retrieved from ICMP data primarily. Check your Squid is built 
> with --enable-icmp, the pinger helper is operational, and that ICMP 
> Echo traffic is working on all possible network routes between your 
> Squid and the peer server(s).
>
>
>>
>> So what's the best solution to a) choose the originserver with the 
>> lowest rtt and b) still have a fast switch if one of the 
>> originservers switches into dead state?
>
>
> Check whether the RTT is actually being measured properly by Squid 
> (debug_options ALL,1 44,3 15,8). If the peers are fast enough 
> responding or close enough in the network RTT could come out as a 0 
> value or some N value equal for both peer. ie. neither being "closer".
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ml+squidusers at kisswebdev.com  Tue Feb  9 15:17:15 2021
From: ml+squidusers at kisswebdev.com (Chris)
Date: Tue, 9 Feb 2021 16:17:15 +0100
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
Message-ID: <873017da-ca19-3f17-9113-e0cc0d85bcd5@kisswebdev.com>

Oh, that lib won't help, sorry, forget about my pinger_program path

So do I have to recompile squid myself and than install the pinger as 
described here: 
https://wiki.squid-cache.org/SquidFaq/OperatingSquid#Using_ICMP_to_Measure_the_Network 
?

On 09.02.21 16:03, Chris wrote:
> Hi,
>
> thank you Amos, this is bringing me into the right direction.
>
> Now I know what I'll have to debug: the pinger.
>
> Cache.log shows:
>
> 2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
> 2021/02/09 14:49:27| pinger: ICMP socket opened.
> 2021/02/09 14:49:27| pinger: ICMPv6 socket opened
> 2021/02/09 14:49:27| Pinger exiting.
>
> and that last line "pinger exiting" looks like a problem here.
>
> Squid is used as a package from ubuntu bionic, it's configured with 
> "--enable-icmp" as stated by squid -v.
>
> Now I explicitly wrote a "pinger_enable on" and the pinger_program 
> path (in this case: "/usr/lib/squid/pinger" ) into the squid.conf (as 
> well as icmp_query on) and reconfigured but the cache.log still shows:
>
> "Pinger exiting"
>
> So I don't understand why the pinger is exiting. The pinger_program is 
> owned by root and has 0755 execution rights. Normal ping commands do 
> work and show the one originserver at ttl=53 and time=50 while the 
> other is at ttl=56 and time=155 - so a RTT comparison for 
> weighted-round-robin should work here.
>
> Any hints on how I can find out why the pinger is exiting? Right now 
> I'm debuging with debug_options ALL,1 44,3 15,8 but don't see a reason 
> why the pinger exits.
>
> The Originservers are defined by (with icp/htcp disabled):
>
> cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest 
> no-netdb-exchange weighted-round-robin originserver name=srv1 
> forceddomain=[domainname]
>
> cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest 
> no-netdb-exchange weighted-round-robin originserver name=srv2 
> forceddomain=[domainname]
>
>
> Thank you for your help,
>
> Chris
>
>
>
>
>
> On 09.02.21 04:23, Amos Jeffries wrote:
>> On 9/02/21 3:40 am, Chris wrote:
>>> Hi all,
>>>
>>> I'm trying to figure out the best way to use squid (version 3.5.27) 
>>> in reverse proxy mode in regard to originserver health checks and 
>>> load balancing.
>>>
>>> So far I had been using the round-robin originserver cache peer 
>>> selection algorithm while using weight to favor originservers with 
>>> closer proximity/lower latency.
>>>
>>
>> Ok.
>>
>>
>>> The problem: if one cache_peer is dead it takes ages for squid to 
>>> choose the second originserver. It does look as if (e.g. if one 
>>> originserver has a weight of 32, the other of 2) squid tries the 
>>> dead server several times before accessing the other one.
>>>
>>
>> The DEAD check by default requires 10 failures in a row to trigger. 
>> This is configurable with the connect-fail-limit=N option.
>>
>>
>>> Now instead of using round-robin plus weight it would be best to use 
>>> weighted-round-robin. But as I understand it, this wouldn't work 
>>> with originserver if (as it's normally the case) the originserver 
>>> won't handle icp or htcp requests. Did I miss sth. here? Would 
>>> background-ping work?
>>
>> Well, kind of.
>>
>> ICP/HTCP is just a protocol. Most origin servers do not support them, 
>> but some do. Especially if the server is not a true origin but a 
>> reverse-proxy.
>>
>>
>>>
>>> I tried weighted-round-robin and background-ping on originservers 
>>> but got only an evenly distributed request handling even if ones 
>>> originservers rtt would be less than half of the others. But then 
>>> again, those originservers won't handle icp requests.
>>
>> RTT is retrieved from ICMP data primarily. Check your Squid is built 
>> with --enable-icmp, the pinger helper is operational, and that ICMP 
>> Echo traffic is working on all possible network routes between your 
>> Squid and the peer server(s).
>>
>>
>>>
>>> So what's the best solution to a) choose the originserver with the 
>>> lowest rtt and b) still have a fast switch if one of the 
>>> originservers switches into dead state?
>>
>>
>> Check whether the RTT is actually being measured properly by Squid 
>> (debug_options ALL,1 44,3 15,8). If the peers are fast enough 
>> responding or close enough in the network RTT could come out as a 0 
>> value or some N value equal for both peer. ie. neither being "closer".
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ngtech1ltd at gmail.com  Tue Feb  9 15:19:33 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Tue, 9 Feb 2021 17:19:33 +0200
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
Message-ID: <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>

Maybe its apparmor.
pinger needs to have a setuid permission as root.
its a pinger and needs root privleges as far as i remember.

Eliezer


On Tue, Feb 9, 2021, 17:03 Chris <ml+squidusers at kisswebdev.com> wrote:

> Hi,
>
> thank you Amos, this is bringing me into the right direction.
>
> Now I know what I'll have to debug: the pinger.
>
> Cache.log shows:
>
> 2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
> 2021/02/09 14:49:27| pinger: ICMP socket opened.
> 2021/02/09 14:49:27| pinger: ICMPv6 socket opened
> 2021/02/09 14:49:27| Pinger exiting.
>
> and that last line "pinger exiting" looks like a problem here.
>
> Squid is used as a package from ubuntu bionic, it's configured with
> "--enable-icmp" as stated by squid -v.
>
> Now I explicitly wrote a "pinger_enable on" and the pinger_program path
> (in this case: "/usr/lib/squid/pinger" ) into the squid.conf  (as well
> as icmp_query on) and reconfigured but the cache.log still shows:
>
> "Pinger exiting"
>
> So I don't understand why the pinger is exiting. The pinger_program is
> owned by root and has 0755 execution rights. Normal ping commands do
> work and show the one originserver at ttl=53 and time=50 while the other
> is at ttl=56 and time=155 - so a RTT comparison for weighted-round-robin
> should work here.
>
> Any hints on how I can find out why the pinger is exiting? Right now I'm
> debuging with debug_options ALL,1 44,3 15,8 but don't see a reason why
> the pinger exits.
>
> The Originservers are defined by (with icp/htcp disabled):
>
> cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest
> no-netdb-exchange weighted-round-robin originserver name=srv1
> forceddomain=[domainname]
>
> cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest
> no-netdb-exchange weighted-round-robin originserver name=srv2
> forceddomain=[domainname]
>
>
> Thank you for your help,
>
> Chris
>
>
>
>
>
> On 09.02.21 04:23, Amos Jeffries wrote:
> > On 9/02/21 3:40 am, Chris wrote:
> >> Hi all,
> >>
> >> I'm trying to figure out the best way to use squid (version 3.5.27)
> >> in reverse proxy mode in regard to originserver health checks and
> >> load balancing.
> >>
> >> So far I had been using the round-robin originserver cache peer
> >> selection algorithm while using weight to favor originservers with
> >> closer proximity/lower latency.
> >>
> >
> > Ok.
> >
> >
> >> The problem: if one cache_peer is dead it takes ages for squid to
> >> choose the second originserver. It does look as if (e.g. if one
> >> originserver has a weight of 32, the other of 2) squid tries the dead
> >> server several times before accessing the other one.
> >>
> >
> > The DEAD check by default requires 10 failures in a row to trigger.
> > This is configurable with the connect-fail-limit=N option.
> >
> >
> >> Now instead of using round-robin plus weight it would be best to use
> >> weighted-round-robin. But as I understand it, this wouldn't work with
> >> originserver if (as it's normally the case) the originserver won't
> >> handle icp or htcp requests. Did I miss sth. here? Would
> >> background-ping work?
> >
> > Well, kind of.
> >
> > ICP/HTCP is just a protocol. Most origin servers do not support them,
> > but some do. Especially if the server is not a true origin but a
> > reverse-proxy.
> >
> >
> >>
> >> I tried weighted-round-robin and background-ping on originservers but
> >> got only an evenly distributed request handling even if ones
> >> originservers rtt would be less than half of the others. But then
> >> again, those originservers won't handle icp requests.
> >
> > RTT is retrieved from ICMP data primarily. Check your Squid is built
> > with --enable-icmp, the pinger helper is operational, and that ICMP
> > Echo traffic is working on all possible network routes between your
> > Squid and the peer server(s).
> >
> >
> >>
> >> So what's the best solution to a) choose the originserver with the
> >> lowest rtt and b) still have a fast switch if one of the
> >> originservers switches into dead state?
> >
> >
> > Check whether the RTT is actually being measured properly by Squid
> > (debug_options ALL,1 44,3 15,8). If the peers are fast enough
> > responding or close enough in the network RTT could come out as a 0
> > value or some N value equal for both peer. ie. neither being "closer".
> >
> >
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210209/59c690b4/attachment.htm>

From ml+squidusers at kisswebdev.com  Tue Feb  9 16:09:27 2021
From: ml+squidusers at kisswebdev.com (Chris)
Date: Tue, 9 Feb 2021 17:09:27 +0100
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
 <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>
Message-ID: <40e36fef-ec2c-e949-b62b-262e6d01cb56@kisswebdev.com>

Hi Elizer, this helped, it seems as if I got the pinger working.

It's now owned by root in the same group as the squid user and the 
setuid set.

So I used chown root:squidusergroup and chmod u+s on the pinger (and in 
ubuntu it is actually found under /usr/lib/squid/pinger ).

Now with debug 42,3 I get some values as:

Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv2] 
 ??????????????????????????????? 0 Echo Reply????? 155ms 7 hops

and

Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv1] 
 ??????????????????????????????? 0 Echo Reply????? 11ms 9 hops

but squid is still allocating the requests evenly and not using those 
ping times in weighted-round-robin.

Does the weighted-round-robin need some time to use those rtt values?

Best Regards,

Chris


On 09.02.21 16:19, NgTech LTD wrote:
> Maybe its apparmor.
> pinger needs to have a setuid permission as root.
> its a pinger and needs root privleges as far as i remember.
>
> Eliezer
>
>
> On Tue, Feb 9, 2021, 17:03 Chris <ml+squidusers at kisswebdev.com> wrote:
>
>> Hi,
>>
>> thank you Amos, this is bringing me into the right direction.
>>
>> Now I know what I'll have to debug: the pinger.
>>
>> Cache.log shows:
>>
>> 2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
>> 2021/02/09 14:49:27| pinger: ICMP socket opened.
>> 2021/02/09 14:49:27| pinger: ICMPv6 socket opened
>> 2021/02/09 14:49:27| Pinger exiting.
>>
>> and that last line "pinger exiting" looks like a problem here.
>>
>> Squid is used as a package from ubuntu bionic, it's configured with
>> "--enable-icmp" as stated by squid -v.
>>
>> Now I explicitly wrote a "pinger_enable on" and the pinger_program path
>> (in this case: "/usr/lib/squid/pinger" ) into the squid.conf  (as well
>> as icmp_query on) and reconfigured but the cache.log still shows:
>>
>> "Pinger exiting"
>>
>> So I don't understand why the pinger is exiting. The pinger_program is
>> owned by root and has 0755 execution rights. Normal ping commands do
>> work and show the one originserver at ttl=53 and time=50 while the other
>> is at ttl=56 and time=155 - so a RTT comparison for weighted-round-robin
>> should work here.
>>
>> Any hints on how I can find out why the pinger is exiting? Right now I'm
>> debuging with debug_options ALL,1 44,3 15,8 but don't see a reason why
>> the pinger exits.
>>
>> The Originservers are defined by (with icp/htcp disabled):
>>
>> cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest
>> no-netdb-exchange weighted-round-robin originserver name=srv1
>> forceddomain=[domainname]
>>
>> cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest
>> no-netdb-exchange weighted-round-robin originserver name=srv2
>> forceddomain=[domainname]
>>
>>
>> Thank you for your help,
>>
>> Chris
>>
>>
>>
>>
>>
>> On 09.02.21 04:23, Amos Jeffries wrote:
>>> On 9/02/21 3:40 am, Chris wrote:
>>>> Hi all,
>>>>
>>>> I'm trying to figure out the best way to use squid (version 3.5.27)
>>>> in reverse proxy mode in regard to originserver health checks and
>>>> load balancing.
>>>>
>>>> So far I had been using the round-robin originserver cache peer
>>>> selection algorithm while using weight to favor originservers with
>>>> closer proximity/lower latency.
>>>>
>>> Ok.
>>>
>>>
>>>> The problem: if one cache_peer is dead it takes ages for squid to
>>>> choose the second originserver. It does look as if (e.g. if one
>>>> originserver has a weight of 32, the other of 2) squid tries the dead
>>>> server several times before accessing the other one.
>>>>
>>> The DEAD check by default requires 10 failures in a row to trigger.
>>> This is configurable with the connect-fail-limit=N option.
>>>
>>>
>>>> Now instead of using round-robin plus weight it would be best to use
>>>> weighted-round-robin. But as I understand it, this wouldn't work with
>>>> originserver if (as it's normally the case) the originserver won't
>>>> handle icp or htcp requests. Did I miss sth. here? Would
>>>> background-ping work?
>>> Well, kind of.
>>>
>>> ICP/HTCP is just a protocol. Most origin servers do not support them,
>>> but some do. Especially if the server is not a true origin but a
>>> reverse-proxy.
>>>
>>>
>>>> I tried weighted-round-robin and background-ping on originservers but
>>>> got only an evenly distributed request handling even if ones
>>>> originservers rtt would be less than half of the others. But then
>>>> again, those originservers won't handle icp requests.
>>> RTT is retrieved from ICMP data primarily. Check your Squid is built
>>> with --enable-icmp, the pinger helper is operational, and that ICMP
>>> Echo traffic is working on all possible network routes between your
>>> Squid and the peer server(s).
>>>
>>>
>>>> So what's the best solution to a) choose the originserver with the
>>>> lowest rtt and b) still have a fast switch if one of the
>>>> originservers switches into dead state?
>>>
>>> Check whether the RTT is actually being measured properly by Squid
>>> (debug_options ALL,1 44,3 15,8). If the peers are fast enough
>>> responding or close enough in the network RTT could come out as a 0
>>> value or some N value equal for both peer. ie. neither being "closer".
>>>
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>


From ml+squidusers at kisswebdev.com  Tue Feb  9 16:35:47 2021
From: ml+squidusers at kisswebdev.com (Chris)
Date: Tue, 9 Feb 2021 17:35:47 +0100
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <40e36fef-ec2c-e949-b62b-262e6d01cb56@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
 <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>
 <40e36fef-ec2c-e949-b62b-262e6d01cb56@kisswebdev.com>
Message-ID: <38d59624-9837-d224-b2da-d9187c920e76@kisswebdev.com>

This is what I'm seeing in peer_select in cache_log with 44,3 debug options:

2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(258) 
peerSelectDnsPaths: Find IP destination for: '[the_request]' via 
[ip_cache_peer_srv1]
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(280) 
peerSelectDnsPaths: Found sources for '[the_request]'
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(281) 
peerSelectDnsPaths:?? always_direct = DENIED
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(282) 
peerSelectDnsPaths:??? never_direct = DENIED
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:????? cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv1]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:????? cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv2]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:????? cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv3]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:????? cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv1]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(295) 
peerSelectDnsPaths:??????? timedout = 0
2021/02/09 16:25:11.588 kid1| 44,3| peer_select.cc(79) ~ps_state: 
[the_request]

and than in access.log I have:

TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv1]

TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv2]

TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv3]

evenly distributed.

So it's not using the weighted-round-robin that should have srv1 at 
11ms, while srv2 and srv3 are at about 150ms in regard to pinger.

What did I miss in configuring weighted-round-robin?

Best Regards,

Chris







On 09.02.21 17:09, Chris wrote:
> Hi Elizer, this helped, it seems as if I got the pinger working.
>
> It's now owned by root in the same group as the squid user and the 
> setuid set.
>
> So I used chown root:squidusergroup and chmod u+s on the pinger (and 
> in ubuntu it is actually found under /usr/lib/squid/pinger ).
>
> Now with debug 42,3 I get some values as:
>
> Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv2] 
> ??????????????????????????????? 0 Echo Reply????? 155ms 7 hops
>
> and
>
> Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv1] 
> ??????????????????????????????? 0 Echo Reply????? 11ms 9 hops
>
> but squid is still allocating the requests evenly and not using those 
> ping times in weighted-round-robin.
>
> Does the weighted-round-robin need some time to use those rtt values?
>
> Best Regards,
>
> Chris
>
>
> On 09.02.21 16:19, NgTech LTD wrote:
>> Maybe its apparmor.
>> pinger needs to have a setuid permission as root.
>> its a pinger and needs root privleges as far as i remember.
>>
>> Eliezer
>>
>>
>> On Tue, Feb 9, 2021, 17:03 Chris <ml+squidusers at kisswebdev.com> wrote:
>>
>>> Hi,
>>>
>>> thank you Amos, this is bringing me into the right direction.
>>>
>>> Now I know what I'll have to debug: the pinger.
>>>
>>> Cache.log shows:
>>>
>>> 2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
>>> 2021/02/09 14:49:27| pinger: ICMP socket opened.
>>> 2021/02/09 14:49:27| pinger: ICMPv6 socket opened
>>> 2021/02/09 14:49:27| Pinger exiting.
>>>
>>> and that last line "pinger exiting" looks like a problem here.
>>>
>>> Squid is used as a package from ubuntu bionic, it's configured with
>>> "--enable-icmp" as stated by squid -v.
>>>
>>> Now I explicitly wrote a "pinger_enable on" and the pinger_program path
>>> (in this case: "/usr/lib/squid/pinger" ) into the squid.conf (as well
>>> as icmp_query on) and reconfigured but the cache.log still shows:
>>>
>>> "Pinger exiting"
>>>
>>> So I don't understand why the pinger is exiting. The pinger_program is
>>> owned by root and has 0755 execution rights. Normal ping commands do
>>> work and show the one originserver at ttl=53 and time=50 while the 
>>> other
>>> is at ttl=56 and time=155 - so a RTT comparison for 
>>> weighted-round-robin
>>> should work here.
>>>
>>> Any hints on how I can find out why the pinger is exiting? Right now 
>>> I'm
>>> debuging with debug_options ALL,1 44,3 15,8 but don't see a reason why
>>> the pinger exits.
>>>
>>> The Originservers are defined by (with icp/htcp disabled):
>>>
>>> cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest
>>> no-netdb-exchange weighted-round-robin originserver name=srv1
>>> forceddomain=[domainname]
>>>
>>> cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest
>>> no-netdb-exchange weighted-round-robin originserver name=srv2
>>> forceddomain=[domainname]
>>>
>>>
>>> Thank you for your help,
>>>
>>> Chris
>>>
>>>
>>>
>>>
>>>
>>> On 09.02.21 04:23, Amos Jeffries wrote:
>>>> On 9/02/21 3:40 am, Chris wrote:
>>>>> Hi all,
>>>>>
>>>>> I'm trying to figure out the best way to use squid (version 3.5.27)
>>>>> in reverse proxy mode in regard to originserver health checks and
>>>>> load balancing.
>>>>>
>>>>> So far I had been using the round-robin originserver cache peer
>>>>> selection algorithm while using weight to favor originservers with
>>>>> closer proximity/lower latency.
>>>>>
>>>> Ok.
>>>>
>>>>
>>>>> The problem: if one cache_peer is dead it takes ages for squid to
>>>>> choose the second originserver. It does look as if (e.g. if one
>>>>> originserver has a weight of 32, the other of 2) squid tries the dead
>>>>> server several times before accessing the other one.
>>>>>
>>>> The DEAD check by default requires 10 failures in a row to trigger.
>>>> This is configurable with the connect-fail-limit=N option.
>>>>
>>>>
>>>>> Now instead of using round-robin plus weight it would be best to use
>>>>> weighted-round-robin. But as I understand it, this wouldn't work with
>>>>> originserver if (as it's normally the case) the originserver won't
>>>>> handle icp or htcp requests. Did I miss sth. here? Would
>>>>> background-ping work?
>>>> Well, kind of.
>>>>
>>>> ICP/HTCP is just a protocol. Most origin servers do not support them,
>>>> but some do. Especially if the server is not a true origin but a
>>>> reverse-proxy.
>>>>
>>>>
>>>>> I tried weighted-round-robin and background-ping on originservers but
>>>>> got only an evenly distributed request handling even if ones
>>>>> originservers rtt would be less than half of the others. But then
>>>>> again, those originservers won't handle icp requests.
>>>> RTT is retrieved from ICMP data primarily. Check your Squid is built
>>>> with --enable-icmp, the pinger helper is operational, and that ICMP
>>>> Echo traffic is working on all possible network routes between your
>>>> Squid and the peer server(s).
>>>>
>>>>
>>>>> So what's the best solution to a) choose the originserver with the
>>>>> lowest rtt and b) still have a fast switch if one of the
>>>>> originservers switches into dead state?
>>>>
>>>> Check whether the RTT is actually being measured properly by Squid
>>>> (debug_options ALL,1 44,3 15,8). If the peers are fast enough
>>>> responding or close enough in the network RTT could come out as a 0
>>>> value or some N value equal for both peer. ie. neither being "closer".
>>>>
>>>>
>>>> Amos
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ngtech1ltd at gmail.com  Tue Feb  9 17:41:22 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 9 Feb 2021 19:41:22 +0200
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <38d59624-9837-d224-b2da-d9187c920e76@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
 <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>
 <40e36fef-ec2c-e949-b62b-262e6d01cb56@kisswebdev.com>
 <38d59624-9837-d224-b2da-d9187c920e76@kisswebdev.com>
Message-ID: <000f01d6ff0a$c8fe8e90$5afbabb0$@gmail.com>

This is more of Amos and Alex area.
In general I think that haproxy does load balancing much more efficiently then squid.
It is being used in production for years so I'm not sure why you should use Squid for LB.
If you want to resolve this issue then be my guest I can only offer so QA and advice here and there.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Chris
Sent: Tuesday, February 9, 2021 6:36 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Originserver load balancing and health checks in Squid reverse proxy mode

This is what I'm seeing in peer_select in cache_log with 44,3 debug options:

2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(258) 
peerSelectDnsPaths: Find IP destination for: '[the_request]' via 
[ip_cache_peer_srv1]
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(280) 
peerSelectDnsPaths: Found sources for '[the_request]'
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(281) 
peerSelectDnsPaths:   always_direct = DENIED
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(282) 
peerSelectDnsPaths:    never_direct = DENIED
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:      cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv1]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:      cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv2]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:      cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv3]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292) 
peerSelectDnsPaths:      cache_peer = local=0.0.0.0 
remote=[ip_cache_peer_srv1]:[port] flags=1
2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(295) 
peerSelectDnsPaths:        timedout = 0
2021/02/09 16:25:11.588 kid1| 44,3| peer_select.cc(79) ~ps_state: 
[the_request]

and than in access.log I have:

TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv1]

TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv2]

TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv3]

evenly distributed.

So it's not using the weighted-round-robin that should have srv1 at 
11ms, while srv2 and srv3 are at about 150ms in regard to pinger.

What did I miss in configuring weighted-round-robin?

Best Regards,

Chris







On 09.02.21 17:09, Chris wrote:
> Hi Elizer, this helped, it seems as if I got the pinger working.
>
> It's now owned by root in the same group as the squid user and the 
> setuid set.
>
> So I used chown root:squidusergroup and chmod u+s on the pinger (and 
> in ubuntu it is actually found under /usr/lib/squid/pinger ).
>
> Now with debug 42,3 I get some values as:
>
> Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv2] 
>                                 0 Echo Reply      155ms 7 hops
>
> and
>
> Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv1] 
>                                 0 Echo Reply      11ms 9 hops
>
> but squid is still allocating the requests evenly and not using those 
> ping times in weighted-round-robin.
>
> Does the weighted-round-robin need some time to use those rtt values?
>
> Best Regards,
>
> Chris
>
>
> On 09.02.21 16:19, NgTech LTD wrote:
>> Maybe its apparmor.
>> pinger needs to have a setuid permission as root.
>> its a pinger and needs root privleges as far as i remember.
>>
>> Eliezer
>>
>>
>> On Tue, Feb 9, 2021, 17:03 Chris <ml+squidusers at kisswebdev.com> wrote:
>>
>>> Hi,
>>>
>>> thank you Amos, this is bringing me into the right direction.
>>>
>>> Now I know what I'll have to debug: the pinger.
>>>
>>> Cache.log shows:
>>>
>>> 2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
>>> 2021/02/09 14:49:27| pinger: ICMP socket opened.
>>> 2021/02/09 14:49:27| pinger: ICMPv6 socket opened
>>> 2021/02/09 14:49:27| Pinger exiting.
>>>
>>> and that last line "pinger exiting" looks like a problem here.
>>>
>>> Squid is used as a package from ubuntu bionic, it's configured with
>>> "--enable-icmp" as stated by squid -v.
>>>
>>> Now I explicitly wrote a "pinger_enable on" and the pinger_program path
>>> (in this case: "/usr/lib/squid/pinger" ) into the squid.conf (as well
>>> as icmp_query on) and reconfigured but the cache.log still shows:
>>>
>>> "Pinger exiting"
>>>
>>> So I don't understand why the pinger is exiting. The pinger_program is
>>> owned by root and has 0755 execution rights. Normal ping commands do
>>> work and show the one originserver at ttl=53 and time=50 while the 
>>> other
>>> is at ttl=56 and time=155 - so a RTT comparison for 
>>> weighted-round-robin
>>> should work here.
>>>
>>> Any hints on how I can find out why the pinger is exiting? Right now 
>>> I'm
>>> debuging with debug_options ALL,1 44,3 15,8 but don't see a reason why
>>> the pinger exits.
>>>
>>> The Originservers are defined by (with icp/htcp disabled):
>>>
>>> cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest
>>> no-netdb-exchange weighted-round-robin originserver name=srv1
>>> forceddomain=[domainname]
>>>
>>> cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest
>>> no-netdb-exchange weighted-round-robin originserver name=srv2
>>> forceddomain=[domainname]
>>>
>>>
>>> Thank you for your help,
>>>
>>> Chris
>>>
>>>
>>>
>>>
>>>
>>> On 09.02.21 04:23, Amos Jeffries wrote:
>>>> On 9/02/21 3:40 am, Chris wrote:
>>>>> Hi all,
>>>>>
>>>>> I'm trying to figure out the best way to use squid (version 3.5.27)
>>>>> in reverse proxy mode in regard to originserver health checks and
>>>>> load balancing.
>>>>>
>>>>> So far I had been using the round-robin originserver cache peer
>>>>> selection algorithm while using weight to favor originservers with
>>>>> closer proximity/lower latency.
>>>>>
>>>> Ok.
>>>>
>>>>
>>>>> The problem: if one cache_peer is dead it takes ages for squid to
>>>>> choose the second originserver. It does look as if (e.g. if one
>>>>> originserver has a weight of 32, the other of 2) squid tries the dead
>>>>> server several times before accessing the other one.
>>>>>
>>>> The DEAD check by default requires 10 failures in a row to trigger.
>>>> This is configurable with the connect-fail-limit=N option.
>>>>
>>>>
>>>>> Now instead of using round-robin plus weight it would be best to use
>>>>> weighted-round-robin. But as I understand it, this wouldn't work with
>>>>> originserver if (as it's normally the case) the originserver won't
>>>>> handle icp or htcp requests. Did I miss sth. here? Would
>>>>> background-ping work?
>>>> Well, kind of.
>>>>
>>>> ICP/HTCP is just a protocol. Most origin servers do not support them,
>>>> but some do. Especially if the server is not a true origin but a
>>>> reverse-proxy.
>>>>
>>>>
>>>>> I tried weighted-round-robin and background-ping on originservers but
>>>>> got only an evenly distributed request handling even if ones
>>>>> originservers rtt would be less than half of the others. But then
>>>>> again, those originservers won't handle icp requests.
>>>> RTT is retrieved from ICMP data primarily. Check your Squid is built
>>>> with --enable-icmp, the pinger helper is operational, and that ICMP
>>>> Echo traffic is working on all possible network routes between your
>>>> Squid and the peer server(s).
>>>>
>>>>
>>>>> So what's the best solution to a) choose the originserver with the
>>>>> lowest rtt and b) still have a fast switch if one of the
>>>>> originservers switches into dead state?
>>>>
>>>> Check whether the RTT is actually being measured properly by Squid
>>>> (debug_options ALL,1 44,3 15,8). If the peers are fast enough
>>>> responding or close enough in the network RTT could come out as a 0
>>>> value or some N value equal for both peer. ie. neither being "closer".
>>>>
>>>>
>>>> Amos
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Feb  9 20:59:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Feb 2021 15:59:00 -0500
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <38d59624-9837-d224-b2da-d9187c920e76@kisswebdev.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
 <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>
 <40e36fef-ec2c-e949-b62b-262e6d01cb56@kisswebdev.com>
 <38d59624-9837-d224-b2da-d9187c920e76@kisswebdev.com>
Message-ID: <4a53ffb0-654a-808b-7d1e-f023c658b6e3@measurement-factory.com>

On 2/9/21 11:35 AM, Chris wrote:
> This is what I'm seeing in peer_select in cache_log with 44,3 debug
> options:

Add (at least) "15,3" to your debug_options and then look for
getWeightedRoundRobinParent lines. Looking at mgr:server_list Cache
Manager page may also be useful.


> Does the weighted-round-robin need some time to use those rtt values? 

I am not 100% sure, but I think the answer to that question is "no".

If you want to see the details of that peer selection algorithm, look
for the Squid function with that name. I bet it has _some_ undocumented
surprises, but I do not know whether they are relevant to your specific
use case.


HTH,

Alex.


> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(258)
> peerSelectDnsPaths: Find IP destination for: '[the_request]' via
> [ip_cache_peer_srv1]
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(280)
> peerSelectDnsPaths: Found sources for '[the_request]'
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(281)
> peerSelectDnsPaths:?? always_direct = DENIED
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(282)
> peerSelectDnsPaths:??? never_direct = DENIED
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292)
> peerSelectDnsPaths:????? cache_peer = local=0.0.0.0
> remote=[ip_cache_peer_srv1]:[port] flags=1
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292)
> peerSelectDnsPaths:????? cache_peer = local=0.0.0.0
> remote=[ip_cache_peer_srv2]:[port] flags=1
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292)
> peerSelectDnsPaths:????? cache_peer = local=0.0.0.0
> remote=[ip_cache_peer_srv3]:[port] flags=1
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(292)
> peerSelectDnsPaths:????? cache_peer = local=0.0.0.0
> remote=[ip_cache_peer_srv1]:[port] flags=1
> 2021/02/09 16:25:11.588 kid1| 44,2| peer_select.cc(295)
> peerSelectDnsPaths:??????? timedout = 0
> 2021/02/09 16:25:11.588 kid1| 44,3| peer_select.cc(79) ~ps_state:
> [the_request]
> 
> and than in access.log I have:
> 
> TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv1]
> 
> TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv2]
> 
> TCP_MISS/200 [the_request] ROUND_ROBIN_PARENT/[ip_cache_peer_srv3]
> 
> evenly distributed.
> 
> So it's not using the weighted-round-robin that should have srv1 at
> 11ms, while srv2 and srv3 are at about 150ms in regard to pinger.
> 
> What did I miss in configuring weighted-round-robin?
> 
> Best Regards,
> 
> Chris
> 
> 
> 
> 
> 
> 
> 
> On 09.02.21 17:09, Chris wrote:
>> Hi Elizer, this helped, it seems as if I got the pinger working.
>>
>> It's now owned by root in the same group as the squid user and the
>> setuid set.
>>
>> So I used chown root:squidusergroup and chmod u+s on the pinger (and
>> in ubuntu it is actually found under /usr/lib/squid/pinger ).
>>
>> Now with debug 42,3 I get some values as:
>>
>> Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv2]
>> ??????????????????????????????? 0 Echo Reply????? 155ms 7 hops
>>
>> and
>>
>> Icmp.cc(95) Log: pingerLog: [timestamp] [ip_srv1]
>> ??????????????????????????????? 0 Echo Reply????? 11ms 9 hops
>>
>> but squid is still allocating the requests evenly and not using those
>> ping times in weighted-round-robin.
>>
>> Does the weighted-round-robin need some time to use those rtt values?
>>
>> Best Regards,
>>
>> Chris
>>
>>
>> On 09.02.21 16:19, NgTech LTD wrote:
>>> Maybe its apparmor.
>>> pinger needs to have a setuid permission as root.
>>> its a pinger and needs root privleges as far as i remember.
>>>
>>> Eliezer
>>>
>>>
>>> On Tue, Feb 9, 2021, 17:03 Chris <ml+squidusers at kisswebdev.com> wrote:
>>>
>>>> Hi,
>>>>
>>>> thank you Amos, this is bringing me into the right direction.
>>>>
>>>> Now I know what I'll have to debug: the pinger.
>>>>
>>>> Cache.log shows:
>>>>
>>>> 2021/02/09 14:49:27| pinger: Initialising ICMP pinger ...
>>>> 2021/02/09 14:49:27| pinger: ICMP socket opened.
>>>> 2021/02/09 14:49:27| pinger: ICMPv6 socket opened
>>>> 2021/02/09 14:49:27| Pinger exiting.
>>>>
>>>> and that last line "pinger exiting" looks like a problem here.
>>>>
>>>> Squid is used as a package from ubuntu bionic, it's configured with
>>>> "--enable-icmp" as stated by squid -v.
>>>>
>>>> Now I explicitly wrote a "pinger_enable on" and the pinger_program path
>>>> (in this case: "/usr/lib/squid/pinger" ) into the squid.conf (as well
>>>> as icmp_query on) and reconfigured but the cache.log still shows:
>>>>
>>>> "Pinger exiting"
>>>>
>>>> So I don't understand why the pinger is exiting. The pinger_program is
>>>> owned by root and has 0755 execution rights. Normal ping commands do
>>>> work and show the one originserver at ttl=53 and time=50 while the
>>>> other
>>>> is at ttl=56 and time=155 - so a RTT comparison for
>>>> weighted-round-robin
>>>> should work here.
>>>>
>>>> Any hints on how I can find out why the pinger is exiting? Right now
>>>> I'm
>>>> debuging with debug_options ALL,1 44,3 15,8 but don't see a reason why
>>>> the pinger exits.
>>>>
>>>> The Originservers are defined by (with icp/htcp disabled):
>>>>
>>>> cache_peer [ipv4_address_srv1] parent [http_port] 0 no-digest
>>>> no-netdb-exchange weighted-round-robin originserver name=srv1
>>>> forceddomain=[domainname]
>>>>
>>>> cache_peer [ipv4_address_srv2] parent [http_port] 0 no-digest
>>>> no-netdb-exchange weighted-round-robin originserver name=srv2
>>>> forceddomain=[domainname]
>>>>
>>>>
>>>> Thank you for your help,
>>>>
>>>> Chris
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> On 09.02.21 04:23, Amos Jeffries wrote:
>>>>> On 9/02/21 3:40 am, Chris wrote:
>>>>>> Hi all,
>>>>>>
>>>>>> I'm trying to figure out the best way to use squid (version 3.5.27)
>>>>>> in reverse proxy mode in regard to originserver health checks and
>>>>>> load balancing.
>>>>>>
>>>>>> So far I had been using the round-robin originserver cache peer
>>>>>> selection algorithm while using weight to favor originservers with
>>>>>> closer proximity/lower latency.
>>>>>>
>>>>> Ok.
>>>>>
>>>>>
>>>>>> The problem: if one cache_peer is dead it takes ages for squid to
>>>>>> choose the second originserver. It does look as if (e.g. if one
>>>>>> originserver has a weight of 32, the other of 2) squid tries the dead
>>>>>> server several times before accessing the other one.
>>>>>>
>>>>> The DEAD check by default requires 10 failures in a row to trigger.
>>>>> This is configurable with the connect-fail-limit=N option.
>>>>>
>>>>>
>>>>>> Now instead of using round-robin plus weight it would be best to use
>>>>>> weighted-round-robin. But as I understand it, this wouldn't work with
>>>>>> originserver if (as it's normally the case) the originserver won't
>>>>>> handle icp or htcp requests. Did I miss sth. here? Would
>>>>>> background-ping work?
>>>>> Well, kind of.
>>>>>
>>>>> ICP/HTCP is just a protocol. Most origin servers do not support them,
>>>>> but some do. Especially if the server is not a true origin but a
>>>>> reverse-proxy.
>>>>>
>>>>>
>>>>>> I tried weighted-round-robin and background-ping on originservers but
>>>>>> got only an evenly distributed request handling even if ones
>>>>>> originservers rtt would be less than half of the others. But then
>>>>>> again, those originservers won't handle icp requests.
>>>>> RTT is retrieved from ICMP data primarily. Check your Squid is built
>>>>> with --enable-icmp, the pinger helper is operational, and that ICMP
>>>>> Echo traffic is working on all possible network routes between your
>>>>> Squid and the peer server(s).
>>>>>
>>>>>
>>>>>> So what's the best solution to a) choose the originserver with the
>>>>>> lowest rtt and b) still have a fast switch if one of the
>>>>>> originservers switches into dead state?
>>>>>
>>>>> Check whether the RTT is actually being measured properly by Squid
>>>>> (debug_options ALL,1 44,3 15,8). If the peers are fast enough
>>>>> responding or close enough in the network RTT could come out as a 0
>>>>> value or some N value equal for both peer. ie. neither being "closer".
>>>>>
>>>>>
>>>>> Amos
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Feb  9 21:03:04 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Feb 2021 16:03:04 -0500
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
Message-ID: <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>

On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
> I move on to testing squid-6.0.0-20210204-r5f37a71ac
> 
> Most of the issues I see are related to Host header forgery detection.
> 
> I do see that the main issue with TLS is similar to:
> 
> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
> connection on conn2195 local=216.58.198.67:443
> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
> 
> ??? current master transaction: master78
> 
> which is a google host related issue.


> Alex and Amos,
> 
> Can the project do something about this?
 FWIW, I do not understand what you are asking about -- it is not clear
to me what "this" is in the context of your question. As you know, there
have been several recent discussions about host header forgery detection
problems. It is not clear to me whether you are asking about some
specific new case or want to revisit some specific aspects of those
discussions.

Alex.


From squid3 at treenet.co.nz  Wed Feb 10 00:25:30 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Feb 2021 13:25:30 +1300
Subject: [squid-users] Originserver load balancing and health checks in
 Squid reverse proxy mode
In-Reply-To: <4a53ffb0-654a-808b-7d1e-f023c658b6e3@measurement-factory.com>
References: <f89a5cb9-88f0-aec6-f146-c7ee896d3cf7@kisswebdev.com>
 <c3c0ff1c-f8ca-ac08-df48-2e999bc6bfe3@treenet.co.nz>
 <b9f12a2a-fa31-417a-3210-6c9e9052e146@kisswebdev.com>
 <CABA8h=TPM=By_F+xgCQDg-MYFw_WmeNzRTQZKnZnnnDjNaJk=A@mail.gmail.com>
 <40e36fef-ec2c-e949-b62b-262e6d01cb56@kisswebdev.com>
 <38d59624-9837-d224-b2da-d9187c920e76@kisswebdev.com>
 <4a53ffb0-654a-808b-7d1e-f023c658b6e3@measurement-factory.com>
Message-ID: <525fc74b-48e0-b3ae-ef64-388ce69e4afc@treenet.co.nz>

On 10/02/21 9:59 am, Alex Rousskov wrote:
> On 2/9/21 11:35 AM, Chris wrote:
>> This is what I'm seeing in peer_select in cache_log with 44,3 debug
>> options:
> 
> Add (at least) "15,3" to your debug_options and then look for
> getWeightedRoundRobinParent lines. Looking at mgr:server_list Cache
> Manager page may also be useful.
> 
> 
>> Does the weighted-round-robin need some time to use those rtt values?
> 
> I am not 100% sure, but I think the answer to that question is "no".
> 

Well, only the time it takes to find out what the RTT actually is. As 
soon as RTT is known it is used.

This algorithm works like round-robin in that each selection cycle the 
peer with fewest "uses" gets selected. However, where normal round-robin 
adds "(1 * weight) uses" to a peer each time it gets selected, this 
algorithm adds "(RTT * weight) uses".

The debug lines Alex mentioned will tell you what weighted_rtt your 
proxy is using as its RTT count per use. If that shows "1", then the RTT 
is too small to calculate speed difference (math code improvements 
needed to cope with modern fast networks).

Amos


From ngtech1ltd at gmail.com  Thu Feb 11 15:41:59 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 11 Feb 2021 17:41:59 +0200
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
Message-ID: <000001d7008c$717e4100$547ac300$@gmail.com>

Hey Alex,

I am talking/writing about the actual issue..
The issue that makes it's impossible to surf not to cache.
The 
> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
> connection on conn2195 local=216.58.198.67:443
> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
> 
>     current master transaction: master78
> 
> which is a google host related issue.

The access to google hosts seems to be the main issue here.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Tuesday, February 9, 2021 11:03 PM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac

On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
> I move on to testing squid-6.0.0-20210204-r5f37a71ac
> 
> Most of the issues I see are related to Host header forgery detection.
> 
> I do see that the main issue with TLS is similar to:
> 
> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
> connection on conn2195 local=216.58.198.67:443
> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
> 
> ??? current master transaction: master78
> 
> which is a google host related issue.


> Alex and Amos,
> 
> Can the project do something about this?
 FWIW, I do not understand what you are asking about -- it is not clear
to me what "this" is in the context of your question. As you know, there
have been several recent discussions about host header forgery detection
problems. It is not clear to me whether you are asking about some
specific new case or want to revisit some specific aspects of those
discussions.

Alex.



From rousskov at measurement-factory.com  Thu Feb 11 17:01:31 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Feb 2021 12:01:31 -0500
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <000001d7008c$717e4100$547ac300$@gmail.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
 <000001d7008c$717e4100$547ac300$@gmail.com>
Message-ID: <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>

On 2/11/21 10:41 AM, Eliezer Croitoru wrote:

> The issue that makes it's impossible to surf not to cache.
> The 
>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>> connection on conn2195 local=216.58.198.67:443
>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>
>>     current master transaction: master78
>>
>> which is a google host related issue.
> 
> The access to google hosts seems to be the main issue here.

How is this different from the host forgery related discussions we
recently had? I consider the general "What can we do about host forgery
errors?"  question answered already. If you disagree with those answers,
we can discuss further, but, to make progress, you need to say
explicitly which answer you disagree with and why.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Tuesday, February 9, 2021 11:03 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
> squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
> 
> On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
>> I move on to testing squid-6.0.0-20210204-r5f37a71ac
>>
>> Most of the issues I see are related to Host header forgery detection.
>>
>> I do see that the main issue with TLS is similar to:
>>
>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>> connection on conn2195 local=216.58.198.67:443
>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>
>> ??? current master transaction: master78
>>
>> which is a google host related issue.
> 
> 
>> Alex and Amos,
>>
>> Can the project do something about this?
>  FWIW, I do not understand what you are asking about -- it is not clear
> to me what "this" is in the context of your question. As you know, there
> have been several recent discussions about host header forgery detection
> problems. It is not clear to me whether you are asking about some
> specific new case or want to revisit some specific aspects of those
> discussions.
> 
> Alex.
> 



From rentorbuy at yahoo.com  Fri Feb 12 09:31:05 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 12 Feb 2021 09:31:05 +0000 (UTC)
Subject: [squid-users] c-icap, clamav and squid
References: <1836028856.969741.1613122265880.ref@mail.yahoo.com>
Message-ID: <1836028856.969741.1613122265880@mail.yahoo.com>

Hi,

I don't know whether this question should be asked here or on the c-icap or clamav lists.

I've had a c-icap/squid failure and noticed that it was because my tmpfs on /var/tmp was full (12 GB).

It was filled with files such as these:

# lsof +D /var/tmp/
COMMAND??? PID USER?? FD?? TYPE DEVICE SIZE/OFF?????? NODE NAME
c-icap???? 773 root?? 31u?? REG?? 0,48???? 1204 2169779504 /var/tmp/CI_TMP_xqWE8?????????????????????????????????????????????????????? B
c-icap??? 3080 root?? 29u?? REG?? 0,48???? 1204 2169784571 /var/tmp/CI_TMP_pE6B7?????????????????????????????????????????????????????? 6

The fact that these files build up and are not deleted might be a side-effect of something that's failing.

Do you think that the c-icap process is the only one responsible for cleaning these files up?
Or is there some Squid configuration option or a cache log event I should check regarding this?

Thanks,

Vieri



From roeeklinger60 at gmail.com  Fri Feb 12 10:44:50 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Fri, 12 Feb 2021 12:44:50 +0200
Subject: [squid-users] How to serve custom error pages with images in Squid?
Message-ID: <CAGCa14rPbbKY5-9hwV0SsOfpy+ZYm-z6kPL-cE_QGH8af93O=w@mail.gmail.com>

Hey,
I am trying to serve custom error pages in Squid 4.10, this is my
squid.comf:

error_directory /etc/squid/pages/
icon_directory /etc/squid/pages/images/


The custom error page loads, but the images are missing. In my HTML file I
simply put:

<img src="image.png" alt="..." class="img-fluid">


but if I inspect the image in the browser, I can see it is adding the
website URL before the image path, like so:

http://ipinfo.io/image.png


I tried to revert back the default error pages, however, I noticed that
apparently even there, the images never loaded, it had the exact same issue.

Does anyone know about this issue?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210212/3f4cffe3/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 12 15:16:17 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Feb 2021 10:16:17 -0500
Subject: [squid-users] c-icap, clamav and squid
In-Reply-To: <1836028856.969741.1613122265880@mail.yahoo.com>
References: <1836028856.969741.1613122265880.ref@mail.yahoo.com>
 <1836028856.969741.1613122265880@mail.yahoo.com>
Message-ID: <ed420c83-9fee-54b7-8d41-624a51dab5a2@measurement-factory.com>

On 2/12/21 4:31 AM, Vieri wrote:

> I've had a c-icap/squid failure and noticed that it was because my tmpfs on /var/tmp was full (12 GB).
> 
> It was filled with files such as these:
> 
> # lsof +D /var/tmp/
> COMMAND??? PID USER?? FD?? TYPE DEVICE SIZE/OFF?????? NODE NAME
> c-icap???? 773 root?? 31u?? REG?? 0,48???? 1204 2169779504 /var/tmp/CI_TMP_xqWE8?????????????????????????????????????????????????????? B
> c-icap??? 3080 root?? 29u?? REG?? 0,48???? 1204 2169784571 /var/tmp/CI_TMP_pE6B7?????????????????????????????????????????????????????? 6
> 
> The fact that these files build up and are not deleted might be a side-effect of something that's failing.
> 
> Do you think that the c-icap process is the only one responsible for cleaning these files up?
> Or is there some Squid configuration option or a cache log event I should check regarding this?


Definitely not Squid configuration. Squid does not know anything about
these files or even about c-icap/clamav purposes. The ICAP protocol does
not have a notion of a disk file.

If there are no live ICAP transactions using those files, then somebody
(c-icap and/or clamav) is not deleting unused files when they should be
deleting them. The latter could mean a misconfigured lazy garbage
collection, overload conditions, or bugs, but it will be all on the ICAP
service side. I do not know enough about c-icap and do not remember
enough about clamav to guide you to a solution, unfortunately.

Alex.


From rousskov at measurement-factory.com  Fri Feb 12 15:37:23 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Feb 2021 10:37:23 -0500
Subject: [squid-users] How to serve custom error pages with images in
 Squid?
In-Reply-To: <CAGCa14rPbbKY5-9hwV0SsOfpy+ZYm-z6kPL-cE_QGH8af93O=w@mail.gmail.com>
References: <CAGCa14rPbbKY5-9hwV0SsOfpy+ZYm-z6kPL-cE_QGH8af93O=w@mail.gmail.com>
Message-ID: <9faa6028-259c-02b8-513d-b6c8837a2bb4@measurement-factory.com>

On 2/12/21 5:44 AM, roee klinger wrote:

> I am trying to serve custom error pages in Squid 4.10, this is my
> squid.comf:
> 
>     error_directory /etc/squid/pages/
>     icon_directory /etc/squid/pages/images/

> The custom error page loads, but the images are missing. In my HTML file
> I simply put:
> 
>     <img src="image.png" alt="..." class="img-fluid">
> 
> 
> but if I inspect the image in the browser, I can see it is adding the
> website URL before the image path, like so:
> 
>     http://ipinfo.io/image.png

The browser thinks the error page was generated by the origin server.
Popular browsers essentially do not recognize forward proxies as
legitimate content sources so there is no way for Squid to tell the
browser that the error page is coming from Squid itself.

If images in your error pages use a relative image path (including an
empty path like in your example above) and do not define their own base
URL using HTML, then the browser will use the _request_ URL path as the
base URL, as if the error page came from the origin server that
experienced the error.


> I tried to revert back the default error pages, however, I noticed that
> apparently even there, the images never loaded, it had the exact same issue.

Squid-generated content, including error pages, may use a
/squid-internal-static/ path that Squid can then recognize in image
requests and recognize that the user wants Squid-generated content. See
global_internal_static and short_icon_urls directives.


HTH,

Alex.


From yhdezalvarez at gmail.com  Fri Feb 12 20:29:31 2021
From: yhdezalvarez at gmail.com (=?UTF-8?Q?Yanko_Hern=C3=A1ndez_=C3=81lvarez?=)
Date: Fri, 12 Feb 2021 15:29:31 -0500
Subject: [squid-users] The user/password pair is correct,
 yet squid keeps sending me TCP_DENIED/407
In-Reply-To: <CAM9HHif2j1Siy+JYG0Eg+y5ZZtYJ8GpBZp_d5C7701b21R5+hQ@mail.gmail.com>
References: <CAM9HHif2j1Siy+JYG0Eg+y5ZZtYJ8GpBZp_d5C7701b21R5+hQ@mail.gmail.com>
Message-ID: <CAM9HHieHT7DVSQyUtD5X8vbDJkwDP8y8i1j6xEg2+vC_xBpXmA@mail.gmail.com>

Hello :-)

How is it possible that some user tried to log in with the correct
password and squid response was a TCP_DENIED/407?

This is my squid log format
----------------------------
logformat mysquidlog %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un
%Sh/%<A %mt %>A [%>h] [%<h]
access_log daemon:/var/log/squid/access.log mysquidlog
----------------------------

Please notice it includes Request headers([%>h]) and Response headers ([%<h]).

This is the first (of many) relevant squid log entry. (Empty
user/password combination filtered)
----------------------------
# grep TCP_DENIED/407 /var/log/squid/access.log | grep
"Proxy-Authorization: Basic" | grep -v Og== | head -n1
1613138245.113     28 10.128.141.38 TCP_DENIED/407 2609 GET
http://detectportal.firefox.com/success.txt o.suarez HIER_NONE/-
text/html pcmtto.example.com [User-Agent: Mozilla/5.0 (Windows NT 6.1;
Win64; x64; rv:85.0) Gecko/20100101 Firefox/85.0\r\nAccept:
*/*\r\nAccept-Language:
es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3\r\nAccept-Encoding: gzip,
deflate\r\nCache-Control: no-cache\r\nPragma: no-cache\r\nConnection:
keep-alive\r\nProxy-Authorization: Basic
by5zdWFyZXo6TWFudGVuaW1pZW50bzIwMjEr\r\nHost:
detectportal.firefox.com\r\n] [HTTP/1.1 407 Proxy Authentication
Required\r\nServer: squid/4.6\r\nMime-Version: 1.0\r\nDate: Fri, 12
Feb 2021 13:57:25 GMT\r\nContent-Type:
text/html;charset=utf-8\r\nContent-Length: 2110\r\nX-Squid-Error:
ERR_CACHE_ACCESS_DENIED 0\r\nVary:
Accept-Language\r\nContent-Language: es-es\r\n\r]
----------------------------

Same squid log entry (pretty printed)
----------------------------
1613138245.113     28 10.128.141.38 TCP_DENIED/407 2609 GET
http://detectportal.firefox.com/success.txt o.suarez HIER_NONE/-
text/html pcmtto.example.com

Request headers (sent by firefox):
User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:85.0)
Gecko/20100101 Firefox/85.0
Accept: */*
Accept-Language: es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Cache-Control: no-cache
Pragma: no-cache
Connection: keep-alive
Proxy-Authorization: Basic by5zdWFyZXo6TWFudGVuaW1pZW50bzIwMjEr
Host: detectportal.firefox.com


Response headers (sent by squid)
HTTP/1.1 407 Proxy Authentication Required
Server: squid/4.6
Mime-Version: 1.0
Date: Fri, 12 Feb 2021 13:57:25 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 2110
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: es-es
----------------------------

This is my squid configuration regarding ACLs (redacted for brevity
and relevance)
----------------------------
auth_param basic program /usr/lib/squid/basic_ldap_auth -b
"OU=UsersOU,DC=example,DC=com" -D ldapquery at example.com -W
/etc/squid/Other/Password -f
"(&(objectclass=person)(sAMAccountName=%s)(!(userAccountControl:1.2.840.113556.1.4.803:=2)))"
ads.example.com
auth_param basic children 5 startup=5 idle=1
auth_param basic realm Servidor Squid (HTTP-Proxy) example.com
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off

http_access deny !Safe_ports # Safe_ports = default config ports: 80,
21, 443, 70, 210, 1025-65535, 280, 488, 591, 777
http_access deny CONNECT !SSL_ports # CONNECT = method CONNECT,
SSL_ports = 443, 8006, 8443
http_access allow localhost manager
http_access deny manager
http_access allow InternalServers # InternalServers = arp
'/etc/squid/PCs/MACInternalServers'
http_access deny REPorn # REPorn = dstdom_regex -i
'/etc/squid/Sites/Forbbiden/REPorn'
http_access deny FQPornDN # FQPornDN = dstdomain -n
'/etc/squid/Sites/Forbbiden/FQPornDN'
http_access allow localhost
http_access allow MySite # MySite = dstdomain -n .example.com
acl RestrictedPCsGroup1         arp     '/etc/squid/PCs/MACPCsGrp1'
acl RestrictedPCsGroup2         arp     '/etc/squid/PCs/MACPCsGrp2'
acl RestrictedPCsGroup21        arp     '/etc/squid/PCs/MACPCsGrp21'
http_access deny !RestrictedPCsGroup1 !RestrictedPCsGroup2 !RestrictedPCsGroup21
http_access allow AutoConnections # AutoConnections = dstdomain -n
'/etc/squid/Sites/Allowed/AutoConnections'
http_access deny !LoggedIn # LoggedIn = proxy_auth REQUIRED

#
# Some more rules here, but not relevant to that problematic request
as squid stops processing rules on this one.
#
----------------------------

The rule failing should be "http_access deny !LoggedIn". Its the only
one that generates a TCP_DENIED/407. All the other "deny" rules
generate a TCP_DENIED/403.

My auth is configured to use an Active Directory DC and as seen on the
request header, the auth data is
----------------------------
$ echo by5zdWFyZXo6TWFudGVuaW1pZW50bzIwMjEr | base64 -d
o.suarez:Mantenimiento2021+
----------------------------

And it is correct:
----------------------------
# echo o.suarez Mantenimiento2021+ | /usr/lib/squid/basic_ldap_auth -b
"OU=UsersOU,DC=example,DC=com" -D ldapquery at example.com -W
/etc/squid/Other/Password -f
'(&(objectclass=person)(sAMAccountName=%s)(!(userAccountControl:1.2.840.113556.1.4.803:=2)))'
ads.example.com
OK
----------------------------

So... is it a bug? Is there something I misunderstood? I'm using
debian's squid (4.6-1+deb10u4)

I won't be back until monday.


From squid3 at treenet.co.nz  Fri Feb 12 22:32:32 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Feb 2021 11:32:32 +1300
Subject: [squid-users] The user/password pair is correct,
 yet squid keeps sending me TCP_DENIED/407
In-Reply-To: <CAM9HHieHT7DVSQyUtD5X8vbDJkwDP8y8i1j6xEg2+vC_xBpXmA@mail.gmail.com>
References: <CAM9HHif2j1Siy+JYG0Eg+y5ZZtYJ8GpBZp_d5C7701b21R5+hQ@mail.gmail.com>
 <CAM9HHieHT7DVSQyUtD5X8vbDJkwDP8y8i1j6xEg2+vC_xBpXmA@mail.gmail.com>
Message-ID: <b03ddec6-a38c-5e86-197b-073446e4bd72@treenet.co.nz>

On 13/02/21 9:29 am, Yanko Hern?ndez ?lvarez wrote:
> Hello :-)
> 
> How is it possible that some user tried to log in with the correct
> password and squid response was a TCP_DENIED/407?
> 
...
> http_access deny !LoggedIn # LoggedIn = proxy_auth REQUIRED
> 


What rules follow this one? and what ACL types are they?



> #
> # Some more rules here, but not relevant to that problematic request
> as squid stops processing rules on this one.
> #

That sounds like you have done a cache.log trace to verify. But you have 
not shown details of that. Does the trace show same output from the 
helper as seen in your working manual test?


Amos


From ngtech1ltd at gmail.com  Mon Feb 15 11:32:54 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 15 Feb 2021 13:32:54 +0200
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
 <000001d7008c$717e4100$547ac300$@gmail.com>
 <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>
Message-ID: <002501d7038e$4da5a660$e8f0f320$@gmail.com>

Hey Alex,

Where exactly do you see Host Header Forgery in my last email?

Eliezer

* I wrote my own proxy for now.

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Thursday, February 11, 2021 7:02 PM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac

On 2/11/21 10:41 AM, Eliezer Croitoru wrote:

> The issue that makes it's impossible to surf not to cache.
> The 
>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>> connection on conn2195 local=216.58.198.67:443
>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>
>>     current master transaction: master78
>>
>> which is a google host related issue.
> 
> The access to google hosts seems to be the main issue here.

How is this different from the host forgery related discussions we
recently had? I consider the general "What can we do about host forgery
errors?"  question answered already. If you disagree with those answers,
we can discuss further, but, to make progress, you need to say
explicitly which answer you disagree with and why.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Tuesday, February 9, 2021 11:03 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
> squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
> 
> On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
>> I move on to testing squid-6.0.0-20210204-r5f37a71ac
>>
>> Most of the issues I see are related to Host header forgery detection.
>>
>> I do see that the main issue with TLS is similar to:
>>
>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>> connection on conn2195 local=216.58.198.67:443
>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>
>>     current master transaction: master78
>>
>> which is a google host related issue.
> 
> 
>> Alex and Amos,
>>
>> Can the project do something about this?
>  FWIW, I do not understand what you are asking about -- it is not clear
> to me what "this" is in the context of your question. As you know, there
> have been several recent discussions about host header forgery detection
> problems. It is not clear to me whether you are asking about some
> specific new case or want to revisit some specific aspects of those
> discussions.
> 
> Alex.
> 



From mgresko8 at gmail.com  Mon Feb 15 18:56:17 2021
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Mon, 15 Feb 2021 19:56:17 +0100
Subject: [squid-users] dh key too small
Message-ID: <CAChjPdSxevNbiBOojyiMRfKvabor=+icjNRTHCnU2ENZ4zH-nA@mail.gmail.com>

Hello,

I am struggling with "ERROR: negotiating TLS on FD 53:
error:141A318A:SSL routines:tls_process_ske_dhe:dh key too small
(1/-1/0)" error when ssl bumping.

I cannot find out where the problem liesand why is the key too small.
I regenerated my dhparams with openssl dhparam -outform PEM -out
dhparam.pem 4096.

http_port 3128 ssl-bump \
        generate-host-certificates=on \
        dynamic_cert_mem_cache_size=4MB \
        cert=/**********************/bump-ca.crt \
        key=/**********************/bump-ca.key \
        tls-dh=/etc/squid/dhparam.pem

ssl_bump peek step1
ssl_bump bump bumped_group !bank_dom
ssl_bump splice all

I use recent Fedora 33 packages.

I observe the issue when connecting to https://www.p-mat.sk as a bumped user.

Thanks for any help.

Marek


From rousskov at measurement-factory.com  Mon Feb 15 19:02:32 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Feb 2021 14:02:32 -0500
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <002501d7038e$4da5a660$e8f0f320$@gmail.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
 <000001d7008c$717e4100$547ac300$@gmail.com>
 <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>
 <002501d7038e$4da5a660$e8f0f320$@gmail.com>
Message-ID: <ba5d6e1d-d24e-6c4e-18cd-001c29b62ef9@measurement-factory.com>

On 2/15/21 6:32 AM, Eliezer Croitoru wrote:

> Where exactly do you see Host Header Forgery in my last email?

Your last email says "google hosts". The previous email from you (in the
same thread) said "Most of the issues I see are related to Host header
forgery detection" and then named "google host related issue" to be "the
main issue". I naturally assumed that you are talking about a set of
Host forgery related issues with one specific Host forgery detection
issue being the prevalent/major one.

If my assumption was wrong, then you have not addressed the problem I
stated in my very first response -- I still do not know what "google
host related issue" is. The cache.log lines you have posted do not
answer that question for me. You seem to know what the problem actually
is, so, if you want answers, perhaps you can detail/explain the problem
you are asking about.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Thursday, February 11, 2021 7:02 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
> 
> On 2/11/21 10:41 AM, Eliezer Croitoru wrote:
> 
>> The issue that makes it's impossible to surf not to cache.
>> The 
>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>> connection on conn2195 local=216.58.198.67:443
>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>
>>>     current master transaction: master78
>>>
>>> which is a google host related issue.
>>
>> The access to google hosts seems to be the main issue here.
> 
> How is this different from the host forgery related discussions we
> recently had? I consider the general "What can we do about host forgery
> errors?"  question answered already. If you disagree with those answers,
> we can discuss further, but, to make progress, you need to say
> explicitly which answer you disagree with and why.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Tuesday, February 9, 2021 11:03 PM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
>> squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>
>> On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
>>> I move on to testing squid-6.0.0-20210204-r5f37a71ac
>>>
>>> Most of the issues I see are related to Host header forgery detection.
>>>
>>> I do see that the main issue with TLS is similar to:
>>>
>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>> connection on conn2195 local=216.58.198.67:443
>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>
>>>     current master transaction: master78
>>>
>>> which is a google host related issue.
>>
>>
>>> Alex and Amos,
>>>
>>> Can the project do something about this?
>>  FWIW, I do not understand what you are asking about -- it is not clear
>> to me what "this" is in the context of your question. As you know, there
>> have been several recent discussions about host header forgery detection
>> problems. It is not clear to me whether you are asking about some
>> specific new case or want to revisit some specific aspects of those
>> discussions.
>>
>> Alex.
>>



From mgresko8 at gmail.com  Mon Feb 15 21:42:00 2021
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Mon, 15 Feb 2021 22:42:00 +0100
Subject: [squid-users] dh key too small
In-Reply-To: <CAChjPdSxevNbiBOojyiMRfKvabor=+icjNRTHCnU2ENZ4zH-nA@mail.gmail.com>
References: <CAChjPdSxevNbiBOojyiMRfKvabor=+icjNRTHCnU2ENZ4zH-nA@mail.gmail.com>
Message-ID: <CAChjPdRPznxNbBF3DgqxkfMgzVtZ8Amho2DLOrx74dtKfJY9Lg@mail.gmail.com>

Hello,

most probably the problem is on the server side:

openssl s_client -connect www.p-mat.sk:443 -tls1
CONNECTED(00000003)
depth=2 O = Digital Signature Trust Co., CN = DST Root CA X3
verify return:1
depth=1 C = US, O = Let's Encrypt, CN = R3
verify return:1
depth=0 CN = p-mat.sk
verify return:1
139797750867776:error:141A318A:SSL routines:tls_process_ske_dhe:dh key
too small:ssl/statem/statem_clnt.c:2157:

It seems their DH params are too small. What are the possibilities to
overcome the problem on squid side? The only one I am currently aware
of is making exception on ssl bump.

Thanks

Marek



2021-02-15 19:56 GMT+01:00, Marek Gre?ko <mgresko8 at gmail.com>:
> Hello,
>
> I am struggling with "ERROR: negotiating TLS on FD 53:
> error:141A318A:SSL routines:tls_process_ske_dhe:dh key too small
> (1/-1/0)" error when ssl bumping.
>
> I cannot find out where the problem liesand why is the key too small.
> I regenerated my dhparams with openssl dhparam -outform PEM -out
> dhparam.pem 4096.
>
> http_port 3128 ssl-bump \
>         generate-host-certificates=on \
>         dynamic_cert_mem_cache_size=4MB \
>         cert=/**********************/bump-ca.crt \
>         key=/**********************/bump-ca.key \
>         tls-dh=/etc/squid/dhparam.pem
>
> ssl_bump peek step1
> ssl_bump bump bumped_group !bank_dom
> ssl_bump splice all
>
> I use recent Fedora 33 packages.
>
> I observe the issue when connecting to https://www.p-mat.sk as a bumped
> user.
>
> Thanks for any help.
>
> Marek
>


From squid3 at treenet.co.nz  Tue Feb 16 04:28:51 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Feb 2021 17:28:51 +1300
Subject: [squid-users] The user/password pair is correct,
 yet squid keeps sending me TCP_DENIED/407
In-Reply-To: <CAM9HHieHT7DVSQyUtD5X8vbDJkwDP8y8i1j6xEg2+vC_xBpXmA@mail.gmail.com>
References: <CAM9HHif2j1Siy+JYG0Eg+y5ZZtYJ8GpBZp_d5C7701b21R5+hQ@mail.gmail.com>
 <CAM9HHieHT7DVSQyUtD5X8vbDJkwDP8y8i1j6xEg2+vC_xBpXmA@mail.gmail.com>
Message-ID: <b6797943-8fec-55ad-e937-bc79de02d87f@treenet.co.nz>

On 16/02/21 4:16 am, Yanko Hern?ndez ?lvarez wrote:
 > On Fri, Feb 12, 2021 at 5:36 PM Amos Jeffries wrote:
 >>
 >> On 13/02/21 9:29 am, Yanko Hern?ndez ?lvarez wrote:
 >>> Hello :-)
 >>>
 >>> How is it possible that some user tried to log in with the correct
 >>> password and squid response was a TCP_DENIED/407?
 >>>
 >> ...
 >>> http_access deny !LoggedIn # LoggedIn = proxy_auth REQUIRED
 >>>
 >>
 >> What rules follow this one? and what ACL types are they?
 >>
 >
 > "Normal" http_access access/deny rules (TCP_DENIED/403). None Auth
 > related (no TCP_DENIED/407 possible):
 >
 > acl TooManyIPs max_user_ip -s 1
 > acl GRP1 external ADGroup CN=GRP1,OU=Roles,OU=UsersOU,DC=example,DC=com
 > http_access deny TooManyIPs !GRP1
 > acl GRP2 external ADGroup 
CN=UsuariosInternet,OU=UsersOU,DC=example,DC=com
 > acl GRP3 external ADGroup CN=GRP3,OU=UsersOU,DC=example,DC=com
 > acl GRP4 external ADGroup CN=GRP4,OU=UsersOU,DC=example,DC=com

All these group checks will trigger re-authenticate if the user is not a 
member of the group(s) being checked - in case a different login would work.

This issue is where the "all hack" comes from.  Put "all" at the end of 
the deny lines which need to end with a group check. Or where possible 
rearrange the ACL checks to put some other ACL type after the group check.


For example:  ...

 > http_access deny !GRP3 !GRP2 !GRP4

... here:

   http_access deny !GRP3 !GRP2 !GRP4 all


 > http_access deny !InternalSites GRP3 !GRP2

... here:
   http_access deny GRP3 !GRP2 !InternalSites


 > http_access allow SocialNetworks GRP4

... here:
   http_access allow GRP4 SocialNetworks


 > http_access deny SocialNetworks
 > acl BlackListedDomains1 dstdomain -n
 > '/etc/squid/Sites/Forbidden/BlackListedDomains1'
 > http_access deny BlackListedDomains1
 > acl BlackListedDomains2 dstdomain -n
 > '/etc/squid/Sites/Forbidden/BlackListedDomains2'
 > http_access deny BlackListedDomains2
 > acl BlackListedDomains3 dstdomain -n
 > '/etc/squid/Sites/Forbidden/BlackListedDomains3'
 > http_access deny BlackListedDomains3
 > acl BlackListedDomains4 dstdomain -n
 > '/etc/squid/Sites/Forbidden/BlackListedDomains4'
 > http_access deny BlackListedDomains4

Any particular reason for some many different blacklists?

It is a faster check and simpler config file to either have one 
blacklist file, or to load all the files as one ACL name.



 > acl REBlackListedDomains1 dstdom_regex -i
 > '/etc/squid/Sites/Forbidden/REBlackListedDomains1'
 > http_access deny REBlackListedDomains1
 > acl REBlackListedDomains2 dstdom_regex -i
 > '/etc/squid/Sites/Forbidden/REBlackListedDomains2'
 > http_access deny REBlackListedDomains2
 > acl REBlackListedDomains3 dstdom_regex -i
 > '/etc/squid/Sites/Forbidden/REBlackListedDomains3'
 > http_access deny REBlackListedDomains3

Same for the regex blacklists.


Amos


From kshell at gmx.com  Tue Feb 16 07:29:16 2021
From: kshell at gmx.com (Kevin Shell)
Date: Tue, 16 Feb 2021 15:29:16 +0800
Subject: [squid-users] squid http CONNECT
Message-ID: <20210216072916.th2blx7nyi44tszk@apollo.fd012.me>

Hello squid users.

I have configured squid's option SSL_ports to include
smtps(465) imaps(993) pop3s(995) nntps(563)

What requirements are needed for smtps imaps pop3s nntps client programs
to tunnel thru squid proxy?

-- 
kevin



From ngtech1ltd at gmail.com  Tue Feb 16 07:40:42 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 16 Feb 2021 09:40:42 +0200
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <ba5d6e1d-d24e-6c4e-18cd-001c29b62ef9@measurement-factory.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
 <000001d7008c$717e4100$547ac300$@gmail.com>
 <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>
 <002501d7038e$4da5a660$e8f0f320$@gmail.com>
 <ba5d6e1d-d24e-6c4e-18cd-001c29b62ef9@measurement-factory.com>
Message-ID: <000b01d70437$07d39c60$177ad520$@gmail.com>

Google host means that the host that squid couldn't connect ie :
>>> connection on conn2195 local=216.58.198.67:443
>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>

216.58.198.67:443

The issue can be teste against this host.( the above)
There is an issue with ssl bump and this specific host is a re-producible issue/case/problem.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Monday, February 15, 2021 9:03 PM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac

On 2/15/21 6:32 AM, Eliezer Croitoru wrote:

> Where exactly do you see Host Header Forgery in my last email?

Your last email says "google hosts". The previous email from you (in the
same thread) said "Most of the issues I see are related to Host header
forgery detection" and then named "google host related issue" to be "the
main issue". I naturally assumed that you are talking about a set of
Host forgery related issues with one specific Host forgery detection
issue being the prevalent/major one.

If my assumption was wrong, then you have not addressed the problem I
stated in my very first response -- I still do not know what "google
host related issue" is. The cache.log lines you have posted do not
answer that question for me. You seem to know what the problem actually
is, so, if you want answers, perhaps you can detail/explain the problem
you are asking about.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Thursday, February 11, 2021 7:02 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
> 
> On 2/11/21 10:41 AM, Eliezer Croitoru wrote:
> 
>> The issue that makes it's impossible to surf not to cache.
>> The 
>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>> connection on conn2195 local=216.58.198.67:443
>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>
>>>     current master transaction: master78
>>>
>>> which is a google host related issue.
>>
>> The access to google hosts seems to be the main issue here.
> 
> How is this different from the host forgery related discussions we
> recently had? I consider the general "What can we do about host forgery
> errors?"  question answered already. If you disagree with those answers,
> we can discuss further, but, to make progress, you need to say
> explicitly which answer you disagree with and why.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Tuesday, February 9, 2021 11:03 PM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
>> squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>
>> On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
>>> I move on to testing squid-6.0.0-20210204-r5f37a71ac
>>>
>>> Most of the issues I see are related to Host header forgery detection.
>>>
>>> I do see that the main issue with TLS is similar to:
>>>
>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>> connection on conn2195 local=216.58.198.67:443
>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>
>>>     current master transaction: master78
>>>
>>> which is a google host related issue.
>>
>>
>>> Alex and Amos,
>>>
>>> Can the project do something about this?
>>  FWIW, I do not understand what you are asking about -- it is not clear
>> to me what "this" is in the context of your question. As you know, there
>> have been several recent discussions about host header forgery detection
>> problems. It is not clear to me whether you are asking about some
>> specific new case or want to revisit some specific aspects of those
>> discussions.
>>
>> Alex.
>>



From rentorbuy at yahoo.com  Tue Feb 16 10:09:04 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 16 Feb 2021 10:09:04 +0000 (UTC)
Subject: [squid-users] Why some traffic is TCP_DENIED
References: <798605793.2170355.1613470144973.ref@mail.yahoo.com>
Message-ID: <798605793.2170355.1613470144973@mail.yahoo.com>

Hi,

I'm trying to understand why Squid denies access to some sites, eg:

[Tue Feb 16 10:15:36 2021].044????? 0 - TCP_DENIED/302 0 GET http://www.microsoft.com/pki/certs/MicRooCerAut2011_2011_03_22.crt - HIER_NONE/- text/html
[Tue Feb 16 10:15:36 2021].050???? 46 10.215.248.160 TCP_DENIED/403 3352 - 52.109.12.25:443 - HIER_NONE/- text/html
[Tue Feb 16 10:15:36 2021].050????? 0 10.215.248.160 NONE_NONE/000 0 - error:transaction-end-before-headers - HIER_NONE/- -
[Tue Feb 16 10:15:36 2021].052??? 140 10.215.246.144 TCP_MISS/200 193311 GET https://outlook.office.com/mail/ - ORIGINAL_DST/52.97.168.210 text/html
[Tue Feb 16 10:15:36 2021].053???? 49 10.215.248.74 TCP_MISS/200 2037 GET https://puk1-collabhubrtc.officeapps.live.com/rtc2/signalr/negotiate? - ORIGINAL_DST/52.108.88.1 application/json
[Tue Feb 16 10:15:36 2021].057????? 0 10.215.247.159 NONE_NONE/000 0 - error:invalid-request - HIER_NONE/- -
[Tue Feb 16 10:15:36 2021].057????? 0 10.215.247.159 TCP_DENIED/403 3353 - 40.67.251.132:443 - HIER_NONE/- text/html
[Tue Feb 16 10:15:36 2021].057????? 0 10.215.247.159 NONE_NONE/000 0 - error:transaction-end-before-headers - HIER_NONE/- -


If I take the first line in the log and I open the URL from a client I use then the site opens as expected, and the corresponding Squid log is:

[Tue Feb 16 10:45:50 2021].546??? 628 10.215.111.210 TCP_MISS/200 2134 GET https://www.microsoft.com/pki/certs/MicRooCerAut2011_2011_03_22.crt - ORIGINAL_DST/23.210.36.30 application/octet-stream
[Tue Feb 16 10:45:52 2021].668???? 49 10.215.111.210 NONE_NONE/000 0 CONNECT 216.58.215.138:443 - ORIGINAL_DST/216.58.215.138 -

In this log I see my host's IP addr. 10.215.111.210.
However, in the first log I do not see a source IP address. Why?

Other clients seem to be denied access with errors in the log such as "NONE_NONE/000"? followed by error:invalid-request or error:transaction-end-before-headers. How can I find out why I get "invalid requests"? Would a tcpdump on the server or client help? Or should I enable verbose debugging in Squid?

BTW this might be irrelevant but these messages seem to come up when accessing office 365 sites.

Thanks,

Vieri



From yhdezalvarez at gmail.com  Tue Feb 16 15:31:30 2021
From: yhdezalvarez at gmail.com (=?UTF-8?Q?Yanko_Hern=C3=A1ndez_=C3=81lvarez?=)
Date: Tue, 16 Feb 2021 10:31:30 -0500
Subject: [squid-users] Fwd:  The user/password pair is correct,
 yet squid keeps sending me TCP_DENIED/407
In-Reply-To: <CAM9HHice2wHZE+2vR2aaYBG+X-LNCYXGNa1v=RFki5v3D_cH3g@mail.gmail.com>
References: <CAM9HHif2j1Siy+JYG0Eg+y5ZZtYJ8GpBZp_d5C7701b21R5+hQ@mail.gmail.com>
 <CAM9HHieHT7DVSQyUtD5X8vbDJkwDP8y8i1j6xEg2+vC_xBpXmA@mail.gmail.com>
 <b6797943-8fec-55ad-e937-bc79de02d87f@treenet.co.nz>
 <CAM9HHice2wHZE+2vR2aaYBG+X-LNCYXGNa1v=RFki5v3D_cH3g@mail.gmail.com>
Message-ID: <CAM9HHie5GDcNJfgRdyMpKzBGbEXqo1=t_egfOrvdL5Y4XkxNUA@mail.gmail.com>

I just realized gmail was using the wrong reply address. Sorry about that.

>  > acl GRP2 external ADGroup CN=UsuariosInternet,OU=UsersOU,DC=example,DC=com
>  > acl GRP3 external ADGroup CN=GRP3,OU=UsersOU,DC=example,DC=com
>  > acl GRP4 external ADGroup CN=GRP4,OU=UsersOU,DC=example,DC=com
>
> All these group checks will trigger re-authenticate if the user is not a
> member of the group(s) being checked - in case a different login would work.
>
> This issue is where the "all hack" comes from.  Put "all" at the end of
> the deny lines which need to end with a group check. Or where possible
> rearrange the ACL checks to put some other ACL type after the group check.
>
>
> For example:  ...
>
>  > http_access deny !GRP3 !GRP2 !GRP4
>
> ... here:
>
>    http_access deny !GRP3 !GRP2 !GRP4 all
>
>
>  > http_access deny !InternalSites GRP3 !GRP2
>
> ... here:
>    http_access deny GRP3 !GRP2 !InternalSites
>
>
>  > http_access allow SocialNetworks GRP4
>
> ... here:
>    http_access allow GRP4 SocialNetworks

holly ..., that is a tricky detail!!!!

I just read https://wiki.squid-cache.org/action/show/Features/Authentication.

The squid team should put some warning on the config file or something
to bring this detail to prominence.

THANK YOU VERY MUCH!!!!

>
>  > http_access deny SocialNetworks
>  > acl BlackListedDomains1 dstdomain -n
>  > '/etc/squid/Sites/Forbidden/BlackListedDomains1'
>  > http_access deny BlackListedDomains1
>  > acl BlackListedDomains2 dstdomain -n
>  > '/etc/squid/Sites/Forbidden/BlackListedDomains2'
>  > http_access deny BlackListedDomains2
>  > acl BlackListedDomains3 dstdomain -n
>  > '/etc/squid/Sites/Forbidden/BlackListedDomains3'
>  > http_access deny BlackListedDomains3
>  > acl BlackListedDomains4 dstdomain -n
>  > '/etc/squid/Sites/Forbidden/BlackListedDomains4'
>  > http_access deny BlackListedDomains4
>
> Any particular reason for some many different blacklists?
>
> It is a faster check and simpler config file to either have one
> blacklist file, or to load all the files as one ACL name.

Easy maintenance. I want to know/remember why I blacklisted some
specific domain. Keep in mind I "anonymised" the config file before
posting, so the generic names, the example.com domain, etc.

>  > acl REBlackListedDomains1 dstdom_regex -i
>  > '/etc/squid/Sites/Forbidden/REBlackListedDomains1'
>  > http_access deny REBlackListedDomains1
>  > acl REBlackListedDomains2 dstdom_regex -i
>  > '/etc/squid/Sites/Forbidden/REBlackListedDomains2'
>  > http_access deny REBlackListedDomains2
>  > acl REBlackListedDomains3 dstdom_regex -i
>  > '/etc/squid/Sites/Forbidden/REBlackListedDomains3'
>  > http_access deny REBlackListedDomains3
>
> Same for the regex blacklists.
>

Same for the regex blacklists. ;-)

>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Tue Feb 16 16:28:48 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Feb 2021 11:28:48 -0500
Subject: [squid-users] squid http CONNECT
In-Reply-To: <20210216072916.th2blx7nyi44tszk@apollo.fd012.me>
References: <20210216072916.th2blx7nyi44tszk@apollo.fd012.me>
Message-ID: <909eac0b-4874-50bc-5c29-ce7b5d8f37c3@measurement-factory.com>

On 2/16/21 2:29 AM, Kevin Shell wrote:

> What requirements are needed for smtps imaps pop3s nntps client programs
> to tunnel thru squid proxy?

If your Squid is a forward proxy, then those clients have to support
HTTP (and/or HTTPS) forward proxies. In other words, they should
establish a standard HTTP CONNECT tunnel through Squid.

If you are intercepting their traffic, then there are no special
requirements for those clients. You will have to configure Squid to
splice the intercepted connection before getting to unencrypted bytes so
your Squid will be limited to very basic checks at or below the TLS layer.


HTH,

Alex.


From uhlar at fantomas.sk  Tue Feb 16 17:05:21 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 16 Feb 2021 18:05:21 +0100
Subject: [squid-users] squid http CONNECT
In-Reply-To: <909eac0b-4874-50bc-5c29-ce7b5d8f37c3@measurement-factory.com>
References: <20210216072916.th2blx7nyi44tszk@apollo.fd012.me>
 <909eac0b-4874-50bc-5c29-ce7b5d8f37c3@measurement-factory.com>
Message-ID: <20210216170521.GA26944@fantomas.sk>

>On 2/16/21 2:29 AM, Kevin Shell wrote:
>> What requirements are needed for smtps imaps pop3s nntps client programs
>> to tunnel thru squid proxy?

On 16.02.21 11:28, Alex Rousskov wrote:
>If your Squid is a forward proxy, then those clients have to support
>HTTP (and/or HTTPS) forward proxies. In other words, they should
>establish a standard HTTP CONNECT tunnel through Squid.
>
>If you are intercepting their traffic, then there are no special
>requirements for those clients. You will have to configure Squid to
>splice the intercepted connection before getting to unencrypted bytes so
>your Squid will be limited to very basic checks at or below the TLS layer.

also, squid must allow CONNECT to smtps, imaps, pop3s and nntps ports.

which usually means, they have to be added to ssl_ports ACL.
 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
He who laughs last thinks slowest.


From squid3 at treenet.co.nz  Tue Feb 16 18:09:37 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Feb 2021 07:09:37 +1300
Subject: [squid-users] Why some traffic is TCP_DENIED
In-Reply-To: <798605793.2170355.1613470144973@mail.yahoo.com>
References: <798605793.2170355.1613470144973.ref@mail.yahoo.com>
 <798605793.2170355.1613470144973@mail.yahoo.com>
Message-ID: <ad83ae77-d7b9-d494-abf3-482559803f9c@treenet.co.nz>

On 16/02/21 11:09 pm, Vieri wrote:
> Hi,
> 
> I'm trying to understand why Squid denies access to some sites, eg:
> 
> [Tue Feb 16 10:15:36 2021].044????? 0 - TCP_DENIED/302 0 GET http://www.microsoft.com/pki/certs/MicRooCerAut2011_2011_03_22.crt - HIER_NONE/- text/html
> [Tue Feb 16 10:15:36 2021].050???? 46 10.215.248.160 TCP_DENIED/403 3352 - 52.109.12.25:443 - HIER_NONE/- text/html
> [Tue Feb 16 10:15:36 2021].050????? 0 10.215.248.160 NONE_NONE/000 0 - error:transaction-end-before-headers - HIER_NONE/- -
> [Tue Feb 16 10:15:36 2021].052??? 140 10.215.246.144 TCP_MISS/200 193311 GET https://outlook.office.com/mail/ - ORIGINAL_DST/52.97.168.210 text/html
> [Tue Feb 16 10:15:36 2021].053???? 49 10.215.248.74 TCP_MISS/200 2037 GET https://puk1-collabhubrtc.officeapps.live.com/rtc2/signalr/negotiate? - ORIGINAL_DST/52.108.88.1 application/json
> [Tue Feb 16 10:15:36 2021].057????? 0 10.215.247.159 NONE_NONE/000 0 - error:invalid-request - HIER_NONE/- -
> [Tue Feb 16 10:15:36 2021].057????? 0 10.215.247.159 TCP_DENIED/403 3353 - 40.67.251.132:443 - HIER_NONE/- text/html
> [Tue Feb 16 10:15:36 2021].057????? 0 10.215.247.159 NONE_NONE/000 0 - error:transaction-end-before-headers - HIER_NONE/- -
> 
> 
> If I take the first line in the log and I open the URL from a client I use then the site opens as expected, and the corresponding Squid log is:
> 
> [Tue Feb 16 10:45:50 2021].546??? 628 10.215.111.210 TCP_MISS/200 2134 GET https://www.microsoft.com/pki/certs/MicRooCerAut2011_2011_03_22.crt - ORIGINAL_DST/23.210.36.30 application/octet-stream
> [Tue Feb 16 10:45:52 2021].668???? 49 10.215.111.210 NONE_NONE/000 0 CONNECT 216.58.215.138:443 - ORIGINAL_DST/216.58.215.138 -
> 
> In this log I see my host's IP addr. 10.215.111.210.
> However, in the first log I do not see a source IP address. Why?


Because this is Squid downloading the cert for its own use. For example 
SSL-Bump needing it to complete a TLS cert chain.


> 
> Other clients seem to be denied access with errors in the log such as "NONE_NONE/000"? followed by error:invalid-request or error:transaction-end-before-headers. How can I find out why I get "invalid requests"? Would a tcpdump on the server or client help? Or should I enable verbose debugging in Squid?

Looking at all these lines together I see;

  * a client TLS connection being intercepted, the server cert chain in 
incomplete.
  * Squid attempts to download the missing cert(s).
  * squid.conf rules force the cert download to get a 302 instead of a 
valid cert.
  * which leaves Squid unable to send the TLS connection client a valid 
cert chain.
  * the client rejects the TLS handshake and disconnects before any HTTP 
happens.


To avoid these, you need to prevent your squid.conf rules generating 
that 302 when Squid is initiating the request. The ACL type 
"transaction_initiator" can be used for that.


Amos


From rousskov at measurement-factory.com  Tue Feb 16 19:56:54 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Feb 2021 14:56:54 -0500
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <000b01d70437$07d39c60$177ad520$@gmail.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
 <000001d7008c$717e4100$547ac300$@gmail.com>
 <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>
 <002501d7038e$4da5a660$e8f0f320$@gmail.com>
 <ba5d6e1d-d24e-6c4e-18cd-001c29b62ef9@measurement-factory.com>
 <000b01d70437$07d39c60$177ad520$@gmail.com>
Message-ID: <8b2ceac2-e585-7b34-3538-3758466a10e0@measurement-factory.com>

On 2/16/21 2:40 AM, Eliezer Croitoru wrote:
> Google host means that the host that squid couldn't connect ie :
>>>> connection on conn2195 local=216.58.198.67:443
>>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>>
> 
> 216.58.198.67:443
> 
> The issue can be teste against this host.( the above)
> There is an issue with ssl bump and this specific host is a re-producible issue/case/problem.

Thank you for clarifying that.

FWIW, in my tests, a v6-based Squid successfully bumps the connection to
(a result of the reverse DNS lookup of) 216.58.198.67 IP address:

> ... NONE_NONE/200 0 CONNECT dub08s02-in-f3.1e100.net:443 ...
> ... TCP_MISS/404 1960 GET https://dub08s02-in-f3.1e100.net/ ...

Also, the ERROR message you shared earlier suggests that the problem
happens when accepting a TLS client connection ("failure while accepting
a TLS"), but your summary above says "squid couldn't connect" as if the
problem happens when establishing a TLS connection with the server.

The information you have provided so far does not tell me what goes
wrong in your tests. Hopefully, somebody will volunteer to reproduce
this and discover the cause of these connection establishment problems.
Alternatively, you can try sharing an ALL,9 cache.log of the isolated
failed transaction[1]. After that, we will probably know how to address
those problems.

[1]
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Monday, February 15, 2021 9:03 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
> 
> On 2/15/21 6:32 AM, Eliezer Croitoru wrote:
> 
>> Where exactly do you see Host Header Forgery in my last email?
> 
> Your last email says "google hosts". The previous email from you (in the
> same thread) said "Most of the issues I see are related to Host header
> forgery detection" and then named "google host related issue" to be "the
> main issue". I naturally assumed that you are talking about a set of
> Host forgery related issues with one specific Host forgery detection
> issue being the prevalent/major one.
> 
> If my assumption was wrong, then you have not addressed the problem I
> stated in my very first response -- I still do not know what "google
> host related issue" is. The cache.log lines you have posted do not
> answer that question for me. You seem to know what the problem actually
> is, so, if you want answers, perhaps you can detail/explain the problem
> you are asking about.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Thursday, February 11, 2021 7:02 PM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>
>> On 2/11/21 10:41 AM, Eliezer Croitoru wrote:
>>
>>> The issue that makes it's impossible to surf not to cache.
>>> The 
>>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>>> connection on conn2195 local=216.58.198.67:443
>>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>>
>>>>     current master transaction: master78
>>>>
>>>> which is a google host related issue.
>>>
>>> The access to google hosts seems to be the main issue here.
>>
>> How is this different from the host forgery related discussions we
>> recently had? I consider the general "What can we do about host forgery
>> errors?"  question answered already. If you disagree with those answers,
>> we can discuss further, but, to make progress, you need to say
>> explicitly which answer you disagree with and why.
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>>> Sent: Tuesday, February 9, 2021 11:03 PM
>>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
>>> squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>>
>>> On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
>>>> I move on to testing squid-6.0.0-20210204-r5f37a71ac
>>>>
>>>> Most of the issues I see are related to Host header forgery detection.
>>>>
>>>> I do see that the main issue with TLS is similar to:
>>>>
>>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>>> connection on conn2195 local=216.58.198.67:443
>>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>>
>>>>     current master transaction: master78
>>>>
>>>> which is a google host related issue.
>>>
>>>
>>>> Alex and Amos,
>>>>
>>>> Can the project do something about this?
>>>  FWIW, I do not understand what you are asking about -- it is not clear
>>> to me what "this" is in the context of your question. As you know, there
>>> have been several recent discussions about host header forgery detection
>>> problems. It is not clear to me whether you are asking about some
>>> specific new case or want to revisit some specific aspects of those
>>> discussions.
>>>
>>> Alex.
>>>



From rousskov at measurement-factory.com  Tue Feb 16 20:24:31 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Feb 2021 15:24:31 -0500
Subject: [squid-users] dh key too small
In-Reply-To: <CAChjPdRPznxNbBF3DgqxkfMgzVtZ8Amho2DLOrx74dtKfJY9Lg@mail.gmail.com>
References: <CAChjPdSxevNbiBOojyiMRfKvabor=+icjNRTHCnU2ENZ4zH-nA@mail.gmail.com>
 <CAChjPdRPznxNbBF3DgqxkfMgzVtZ8Amho2DLOrx74dtKfJY9Lg@mail.gmail.com>
Message-ID: <4ef0943f-9304-5eeb-95a8-d2f0bd2a53e8@measurement-factory.com>

On 2/15/21 4:42 PM, Marek Gre?ko wrote:
> Hello,
> 
> most probably the problem is on the server side:
> 
> openssl s_client -connect www.p-mat.sk:443 -tls1
> CONNECTED(00000003)
> depth=2 O = Digital Signature Trust Co., CN = DST Root CA X3
> verify return:1
> depth=1 C = US, O = Let's Encrypt, CN = R3
> verify return:1
> depth=0 CN = p-mat.sk
> verify return:1
> 139797750867776:error:141A318A:SSL routines:tls_process_ske_dhe:dh key
> too small:ssl/statem/statem_clnt.c:2157:
> 
> It seems their DH params are too small. What are the possibilities to
> overcome the problem on squid side?

Unfortunately, I can only answer with a question: Does OpenSSL have a
runtime option to allow too-small keys? If yes, you may be able to use
that option with tls_outgoing_options.

Alex.


> 2021-02-15 19:56 GMT+01:00, Marek Gre?ko <mgresko8 at gmail.com>:
>> Hello,
>>
>> I am struggling with "ERROR: negotiating TLS on FD 53:
>> error:141A318A:SSL routines:tls_process_ske_dhe:dh key too small
>> (1/-1/0)" error when ssl bumping.
>>
>> I cannot find out where the problem liesand why is the key too small.
>> I regenerated my dhparams with openssl dhparam -outform PEM -out
>> dhparam.pem 4096.
>>
>> http_port 3128 ssl-bump \
>>         generate-host-certificates=on \
>>         dynamic_cert_mem_cache_size=4MB \
>>         cert=/**********************/bump-ca.crt \
>>         key=/**********************/bump-ca.key \
>>         tls-dh=/etc/squid/dhparam.pem
>>
>> ssl_bump peek step1
>> ssl_bump bump bumped_group !bank_dom
>> ssl_bump splice all
>>
>> I use recent Fedora 33 packages.
>>
>> I observe the issue when connecting to https://www.p-mat.sk as a bumped
>> user.
>>
>> Thanks for any help.
>>
>> Marek
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jzhu at proofpoint.com  Thu Feb 18 05:36:34 2021
From: jzhu at proofpoint.com (John Zhu)
Date: Thu, 18 Feb 2021 05:36:34 +0000
Subject: [squid-users] Data tricking implementation is on ICAP side or
 Squid side?
In-Reply-To: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
References: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
Message-ID: <FFF3A220-33A2-4CEA-99F8-7C0DA11DC722@proofpoint.com>

Hi, All,

I have a wired issue. I setup the  Squid and ICAP.   When ICAP (in RespMod) sends response body (any file types, most of time are large size files) in a relative slow speed to squid,  if the time elapses longer than 1 minute, the browser will close the session and fail the downloading, the squid log shows the error of TCP_MISS_ABORTED/206

Here are the configuration.  I am new to squid.

==> /usr/local/squid/var/logs/access.log <==
1613593651.769  59962 172.90.1.1 TCP_MISS_ABORTED/206 3635 GET https://pfpt-my.sharepoint.com/personal/jzhu_company _com/_layouts/15/download.aspx? - HIER_DIRECT/13.107.136.9 application/pdf


acl SSL_ports port 443
acl Safe_ports port 80    # http
acl Safe_ports port 21    # ftp
acl Safe_ports port 443       # https
acl Safe_ports port 70    # gopher
acl Safe_ports port 210       # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280       # http-mgmt
acl Safe_ports port 488       # gss-http
acl Safe_ports port 591       # filemaker
acl Safe_ports port 777       # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow all

# This is to help with the development process only
#cache deny all


cache_mem 1024 MB
maximum_object_size 200 MB
cache_swap_low 90
cache_swap_high 95
quick_abort_min -1

refresh_pattern ^ftp:     1440   20%    10080
refresh_pattern ^gopher:   1440   0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0    0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .     0  20%    4320

# Docker-compose setup
icap_enable on
icap_io_timeout 600 seconds
icap_connect_timeout 600 seconds

icap_service service_req reqmod_precache bypass=1 icap://icapserver:1344/req
icap_service service_resp respmod_precache bypass=1 icap://icapserver:1344/resp
adaptation_access service_req allow all
adaptation_access service_resp allow all


Thank you all,

John Zhu

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210218/82e357fe/attachment.htm>

From rousskov at measurement-factory.com  Thu Feb 18 06:27:11 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Feb 2021 01:27:11 -0500
Subject: [squid-users] Data tricking implementation is on ICAP side or
 Squid side?
In-Reply-To: <FFF3A220-33A2-4CEA-99F8-7C0DA11DC722@proofpoint.com>
References: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
 <FFF3A220-33A2-4CEA-99F8-7C0DA11DC722@proofpoint.com>
Message-ID: <4054ac33-daab-b8ae-35db-12fdbb627c05@measurement-factory.com>

On 2/18/21 12:36 AM, John Zhu wrote:

> I have a wired issue. I setup the ?Squid and ICAP.?? When ICAP (in
> RespMod) sends response body (any file types, most of time are large
> size files) in a relative slow speed to squid, ?if the time elapses
> longer than 1 minute, the browser will close the session and fail the
> downloading,

During that minute, does the browser actually get any HTTP response
bytes that the ICAP service sent to Squid? Just the response headers?
Nothing at all?


> the squid log shows the error of TCP_MISS_ABORTED/206

HTTP 206 is Partial Content response to a Range request. Did the origin
server respond with an HTTP 206 response? Or did the ICAP server
converted an HTTP 200 response into an HTTP 206 response? Or did Squid
do that conversion?

If Squid does the conversion, then perhaps Squid just has no data to
send to the client because the requested range has not come from the
data trickling ICAP service yet?


HTH,

Alex.



> Here are the configuration.? I am new to squid. ?
> 
> ?
> 
> ==> /usr/local/squid/var/logs/access.log <==
> 
> 1613593651.769? 59962 172.90.1.1 TCP_MISS_ABORTED/206 3635 GET
> https://pfpt-my.sharepoint.com/personal/jzhu_company
> _com/_layouts/15/download.aspx? - HIER_DIRECT/13.107.136.9 application/pdf
> 
> ?
> 
> ?
> 
> acl SSL_ports port 443
> acl Safe_ports port 80??? # http
> acl Safe_ports port 21??? # ftp
> acl Safe_ports port 443?????? # https
> acl Safe_ports port 70??? # gopher
> acl Safe_ports port 210?????? # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280?????? # http-mgmt
> acl Safe_ports port 488?????? # gss-http
> acl Safe_ports port 591?????? # filemaker
> acl Safe_ports port 777?????? # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow all
> 
> # This is to help with the development process only
> #cache deny all
> 
> 
> cache_mem 1024 MB
> maximum_object_size 200 MB
> cache_swap_low 90
> cache_swap_high 95
> quick_abort_min -1
> 
> refresh_pattern ^ftp:???? 1440?? 20%??? 10080
> refresh_pattern ^gopher:?? 1440?? 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0??? 0% 0
> refresh_pattern (Release|Packages(.gz)*)$????? 0?????? 20%???? 2880
> refresh_pattern .???? 0? 20%??? 4320
> 
> # Docker-compose setup
> icap_enable on
> icap_io_timeout 600 seconds
> icap_connect_timeout 600 seconds
> 
> icap_service service_req reqmod_precache bypass=1 icap://icapserver:1344/req
> icap_service service_resp respmod_precache bypass=1
> icap://icapserver:1344/resp
> adaptation_access service_req allow all
> adaptation_access service_resp allow all
> 
> ?
> 
> ?
> 
> Thank you all,
> 
> ?
> 
> John Zhu
> 
> ?
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jzhu at proofpoint.com  Thu Feb 18 06:52:32 2021
From: jzhu at proofpoint.com (John Zhu)
Date: Thu, 18 Feb 2021 06:52:32 +0000
Subject: [squid-users] Data tricking implementation is on ICAP side or
 Squid side?
In-Reply-To: <4054ac33-daab-b8ae-35db-12fdbb627c05@measurement-factory.com>
References: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
 <FFF3A220-33A2-4CEA-99F8-7C0DA11DC722@proofpoint.com>
 <4054ac33-daab-b8ae-35db-12fdbb627c05@measurement-factory.com>
Message-ID: <9D996C66-BA3E-4571-9EC8-CC882C4534E8@proofpoint.com>



?On 2/17/21, 10:28 PM, "Alex Rousskov" <rousskov at measurement-factory.com> wrote:

    On 2/18/21 12:36 AM, John Zhu wrote:

    > I have a wired issue. I setup the  Squid and ICAP.   When ICAP (in
    > RespMod) sends response body (any file types, most of time are large
    > size files) in a relative slow speed to squid,  if the time elapses
    > longer than 1 minute, the browser will close the session and fail the
    > downloading,

    During that minute, does the browser actually get any HTTP response
    bytes that the ICAP service sent to Squid? Just the response headers?
    Nothing at all?

    --- Yes, I implemented the data trickling feature at ICAP server side. I can see in the Firefox browser.:
1)  prompt to save or open file
2)  the progress bar is receiving a few bytes every seconds  


    > the squid log shows the error of TCP_MISS_ABORTED/206

    HTTP 206 is Partial Content response to a Range request. Did the origin
    server respond with an HTTP 206 response? Or did the ICAP server
    converted an HTTP 200 response into an HTTP 206 response? Or did Squid
    do that conversion?

 -- Yes, sending back to squid from ICAP server 206 partial body data, not the original server.

    If Squid does the conversion, then perhaps Squid just has no data to
    send to the client because the requested range has not come from the
    data trickling ICAP service yet?

--- What is request range?  Does it put in the header of icap response header?  This is the ICAP header back to squid
ICAP/1.0 200 OK



Date: Mon, 15 Feb 2021 04:35:37 +0000


    HTH,

    Alex.



    > Here are the configuration.  I am new to squid.  
    > 
    >  
    > 
    > ==> /usr/local/squid/var/logs/access.log <==
    > 
    > 1613593651.769  59962 172.90.1.1 TCP_MISS_ABORTED/206 3635 GET
    > https://urldefense.com/v3/__https://pfpt-my.sharepoint.com/personal/jzhu_company__;!!ORgEfCBsr282Fw!-GAUSOBJG8F9UUMSLCJJWioLebLx-daFRj1qtCC8n3lXrg-1bD6s1AF2-2wMthz3$ 
    > _com/_layouts/15/download.aspx? - HIER_DIRECT/13.107.136.9 application/pdf
    > 
    >  
    > 
    >  
    > 
    > acl SSL_ports port 443
    > acl Safe_ports port 80    # http
    > acl Safe_ports port 21    # ftp
    > acl Safe_ports port 443       # https
    > acl Safe_ports port 70    # gopher
    > acl Safe_ports port 210       # wais
    > acl Safe_ports port 1025-65535 # unregistered ports
    > acl Safe_ports port 280       # http-mgmt
    > acl Safe_ports port 488       # gss-http
    > acl Safe_ports port 591       # filemaker
    > acl Safe_ports port 777       # multiling http
    > acl CONNECT method CONNECT
    > http_access deny !Safe_ports
    > http_access deny CONNECT !SSL_ports
    > http_access allow localhost manager
    > http_access deny manager
    > http_access allow all
    > 
    > # This is to help with the development process only
    > #cache deny all
    > 
    > 
    > cache_mem 1024 MB
    > maximum_object_size 200 MB
    > cache_swap_low 90
    > cache_swap_high 95
    > quick_abort_min -1
    > 
    > refresh_pattern ^ftp:     1440   20%    10080
    > refresh_pattern ^gopher:   1440   0% 1440
    > refresh_pattern -i (/cgi-bin/|\?) 0    0% 0
    > refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
    > refresh_pattern .     0  20%    4320
    > 
    > # Docker-compose setup
    > icap_enable on
    > icap_io_timeout 600 seconds
    > icap_connect_timeout 600 seconds
    > 
    > icap_service service_req reqmod_precache bypass=1 icap://icapserver:1344/req
    > icap_service service_resp respmod_precache bypass=1
    > icap://icapserver:1344/resp
    > adaptation_access service_req allow all
    > adaptation_access service_resp allow all
    > 
    >  
    > 
    >  
    > 
    > Thank you all,
    > 
    >  
    > 
    > John Zhu
    > 
    >  
    > 
    > 
    > _______________________________________________
    > squid-users mailing list
    > squid-users at lists.squid-cache.org
    > https://urldefense.com/v3/__http://lists.squid-cache.org/listinfo/squid-users__;!!ORgEfCBsr282Fw!-GAUSOBJG8F9UUMSLCJJWioLebLx-daFRj1qtCC8n3lXrg-1bD6s1AF2-5jdkNI7$ 
    > 



From rousskov at measurement-factory.com  Thu Feb 18 14:58:27 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Feb 2021 09:58:27 -0500
Subject: [squid-users] Data tricking implementation is on ICAP side or
 Squid side?
In-Reply-To: <9D996C66-BA3E-4571-9EC8-CC882C4534E8@proofpoint.com>
References: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
 <FFF3A220-33A2-4CEA-99F8-7C0DA11DC722@proofpoint.com>
 <4054ac33-daab-b8ae-35db-12fdbb627c05@measurement-factory.com>
 <9D996C66-BA3E-4571-9EC8-CC882C4534E8@proofpoint.com>
Message-ID: <148f433c-ca13-7ed1-f202-01cce657d601@measurement-factory.com>

On 2/18/21 1:52 AM, John Zhu wrote:

> ?On 2/17/21, 10:28 PM, "Alex Rousskov" wrote:
> 
>     On 2/18/21 12:36 AM, John Zhu wrote:
> 
>     > I have a wired issue. I setup the  Squid and ICAP.   When ICAP (in
>     > RespMod) sends response body (any file types, most of time are large
>     > size files) in a relative slow speed to squid,  if the time elapses
>     > longer than 1 minute, the browser will close the session and fail the
>     > downloading,
> 
>     During that minute, does the browser actually get any HTTP response
>     bytes that the ICAP service sent to Squid? Just the response headers?
>     Nothing at all?


>     --- Yes, I implemented the data trickling feature at ICAP server side. I can see in the Firefox browser.:
> 1)  prompt to save or open file
> 2)  the progress bar is receiving a few bytes every seconds  

Please note that my questions were not about the functionality in
general, but the problematic transaction specifically.

If the browser is constantly receiving data during that minute, then it
probably does not timeout. What changes after that minute? You may get
more information from the browser developer console or equivalent. The
browser should log the reason for transaction termination somewhere.


>     > the squid log shows the error of TCP_MISS_ABORTED/206
> 
>     HTTP 206 is Partial Content response to a Range request. Did the origin
>     server respond with an HTTP 206 response? Or did the ICAP server
>     converted an HTTP 200 response into an HTTP 206 response? Or did Squid
>     do that conversion?

>  -- Yes, sending back to squid from ICAP server 206 partial body data, not the original server.

Are you sure that the origin server did not send an HTTP 206 response to
Squid? An ICAP server cannot replace an HTTP 200 OK response with an
HTTP 206 response _unless_ the request had a Range header, and even that
theoretically-possible 200->206 rewrite may not be supported by Squid
today (I have not checked that it is supported).


>     If Squid does the conversion, then perhaps Squid just has no data to
>     send to the client because the requested range has not come from the
>     data trickling ICAP service yet?
> 
> --- What is request range?  Does it put in the header of icap response header?

To learn about Range requests (and 206 responses to them), please see
RFC 7233: https://tools.ietf.org/html/rfc7233


> This is the ICAP header back to squid
> ICAP/1.0 200 OK

We are talking about HTTP headers, not ICAP headers. What HTTP headers
does your service receive and what HTTP headers does your service send back?


Cheers,

Alex.


> Date: Mon, 15 Feb 2021 04:35:37 +0000
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
>     > Here are the configuration.  I am new to squid.  
>     > 
>     >  
>     > 
>     > ==> /usr/local/squid/var/logs/access.log <==
>     > 
>     > 1613593651.769  59962 172.90.1.1 TCP_MISS_ABORTED/206 3635 GET
>     > https://urldefense.com/v3/__https://pfpt-my.sharepoint.com/personal/jzhu_company__;!!ORgEfCBsr282Fw!-GAUSOBJG8F9UUMSLCJJWioLebLx-daFRj1qtCC8n3lXrg-1bD6s1AF2-2wMthz3$ 
>     > _com/_layouts/15/download.aspx? - HIER_DIRECT/13.107.136.9 application/pdf
>     > 
>     >  
>     > 
>     >  
>     > 
>     > acl SSL_ports port 443
>     > acl Safe_ports port 80    # http
>     > acl Safe_ports port 21    # ftp
>     > acl Safe_ports port 443       # https
>     > acl Safe_ports port 70    # gopher
>     > acl Safe_ports port 210       # wais
>     > acl Safe_ports port 1025-65535 # unregistered ports
>     > acl Safe_ports port 280       # http-mgmt
>     > acl Safe_ports port 488       # gss-http
>     > acl Safe_ports port 591       # filemaker
>     > acl Safe_ports port 777       # multiling http
>     > acl CONNECT method CONNECT
>     > http_access deny !Safe_ports
>     > http_access deny CONNECT !SSL_ports
>     > http_access allow localhost manager
>     > http_access deny manager
>     > http_access allow all
>     > 
>     > # This is to help with the development process only
>     > #cache deny all
>     > 
>     > 
>     > cache_mem 1024 MB
>     > maximum_object_size 200 MB
>     > cache_swap_low 90
>     > cache_swap_high 95
>     > quick_abort_min -1
>     > 
>     > refresh_pattern ^ftp:     1440   20%    10080
>     > refresh_pattern ^gopher:   1440   0% 1440
>     > refresh_pattern -i (/cgi-bin/|\?) 0    0% 0
>     > refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>     > refresh_pattern .     0  20%    4320
>     > 
>     > # Docker-compose setup
>     > icap_enable on
>     > icap_io_timeout 600 seconds
>     > icap_connect_timeout 600 seconds
>     > 
>     > icap_service service_req reqmod_precache bypass=1 icap://icapserver:1344/req
>     > icap_service service_resp respmod_precache bypass=1
>     > icap://icapserver:1344/resp
>     > adaptation_access service_req allow all
>     > adaptation_access service_resp allow all
>     > 
>     >  
>     > 
>     >  
>     > 
>     > Thank you all,
>     > 
>     >  
>     > 
>     > John Zhu
>     > 
>     >  
>     > 
>     > 
>     > _______________________________________________
>     > squid-users mailing list
>     > squid-users at lists.squid-cache.org
>     > https://urldefense.com/v3/__http://lists.squid-cache.org/listinfo/squid-users__;!!ORgEfCBsr282Fw!-GAUSOBJG8F9UUMSLCJJWioLebLx-daFRj1qtCC8n3lXrg-1bD6s1AF2-5jdkNI7$ 
>     > 
> 
> 



From rousskov at measurement-factory.com  Thu Feb 18 17:18:38 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Feb 2021 12:18:38 -0500
Subject: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
In-Reply-To: <004101d705e4$7a9b00e0$6fd102a0$@gmail.com>
References: <000401d6fd79$41474bf0$c3d5e3d0$@gmail.com>
 <95f12702-4a8e-9414-8dcd-348a91910d0a@measurement-factory.com>
 <000001d7008c$717e4100$547ac300$@gmail.com>
 <4b1c0cb6-c721-3492-25eb-a63b7ada0050@measurement-factory.com>
 <002501d7038e$4da5a660$e8f0f320$@gmail.com>
 <ba5d6e1d-d24e-6c4e-18cd-001c29b62ef9@measurement-factory.com>
 <000b01d70437$07d39c60$177ad520$@gmail.com>
 <8b2ceac2-e585-7b34-3538-3758466a10e0@measurement-factory.com>
 <004101d705e4$7a9b00e0$6fd102a0$@gmail.com>
Message-ID: <9c9f0b77-8cac-7bda-8ff0-078abecd5eda@measurement-factory.com>

On 2/18/21 5:54 AM, Eliezer Croitoru wrote:

> I  am waiting for these who want to debug this issue.

Understood. If nobody volunteers to do the initial legwork, I would
recommend collecting and sharing a transaction log as detailed in my
previous response.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Tuesday, February 16, 2021 9:57 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
> 
> On 2/16/21 2:40 AM, Eliezer Croitoru wrote:
>> Google host means that the host that squid couldn't connect ie :
>>>>> connection on conn2195 local=216.58.198.67:443
>>>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>>>
>>
>> 216.58.198.67:443
>>
>> The issue can be teste against this host.( the above)
>> There is an issue with ssl bump and this specific host is a re-producible issue/case/problem.
> 
> Thank you for clarifying that.
> 
> FWIW, in my tests, a v6-based Squid successfully bumps the connection to
> (a result of the reverse DNS lookup of) 216.58.198.67 IP address:
> 
>> ... NONE_NONE/200 0 CONNECT dub08s02-in-f3.1e100.net:443 ...
>> ... TCP_MISS/404 1960 GET https://dub08s02-in-f3.1e100.net/ ...
> 
> Also, the ERROR message you shared earlier suggests that the problem
> happens when accepting a TLS client connection ("failure while accepting
> a TLS"), but your summary above says "squid couldn't connect" as if the
> problem happens when establishing a TLS connection with the server.
> 
> The information you have provided so far does not tell me what goes
> wrong in your tests. Hopefully, somebody will volunteer to reproduce
> this and discover the cause of these connection establishment problems.
> Alternatively, you can try sharing an ALL,9 cache.log of the isolated
> failed transaction[1]. After that, we will probably know how to address
> those problems.
> 
> [1]
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Monday, February 15, 2021 9:03 PM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>
>> On 2/15/21 6:32 AM, Eliezer Croitoru wrote:
>>
>>> Where exactly do you see Host Header Forgery in my last email?
>>
>> Your last email says "google hosts". The previous email from you (in the
>> same thread) said "Most of the issues I see are related to Host header
>> forgery detection" and then named "google host related issue" to be "the
>> main issue". I naturally assumed that you are talking about a set of
>> Host forgery related issues with one specific Host forgery detection
>> issue being the prevalent/major one.
>>
>> If my assumption was wrong, then you have not addressed the problem I
>> stated in my very first response -- I still do not know what "google
>> host related issue" is. The cache.log lines you have posted do not
>> answer that question for me. You seem to know what the problem actually
>> is, so, if you want answers, perhaps you can detail/explain the problem
>> you are asking about.
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>>> Sent: Thursday, February 11, 2021 7:02 PM
>>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>>
>>> On 2/11/21 10:41 AM, Eliezer Croitoru wrote:
>>>
>>>> The issue that makes it's impossible to surf not to cache.
>>>> The 
>>>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>>>> connection on conn2195 local=216.58.198.67:443
>>>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>>>
>>>>>     current master transaction: master78
>>>>>
>>>>> which is a google host related issue.
>>>>
>>>> The access to google hosts seems to be the main issue here.
>>>
>>> How is this different from the host forgery related discussions we
>>> recently had? I consider the general "What can we do about host forgery
>>> errors?"  question answered already. If you disagree with those answers,
>>> we can discuss further, but, to make progress, you need to say
>>> explicitly which answer you disagree with and why.
>>>
>>> Alex.
>>>
>>>
>>>> -----Original Message-----
>>>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>>>> Sent: Tuesday, February 9, 2021 11:03 PM
>>>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>;
>>>> squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] Started testing squid-6.0.0-20210204-r5f37a71ac
>>>>
>>>> On 2/7/21 12:47 PM, Eliezer Croitoru wrote:
>>>>> I move on to testing squid-6.0.0-20210204-r5f37a71ac
>>>>>
>>>>> Most of the issues I see are related to Host header forgery detection.
>>>>>
>>>>> I do see that the main issue with TLS is similar to:
>>>>>
>>>>> 2021/02/07 19:46:07 kid1| ERROR: failure while accepting a TLS
>>>>> connection on conn2195 local=216.58.198.67:443
>>>>> remote=192.168.189.94:41724 FD 104 flags=33: 0x55cf6a6debe0*1
>>>>>
>>>>>     current master transaction: master78
>>>>>
>>>>> which is a google host related issue.
>>>>
>>>>
>>>>> Alex and Amos,
>>>>>
>>>>> Can the project do something about this?
>>>>  FWIW, I do not understand what you are asking about -- it is not clear
>>>> to me what "this" is in the context of your question. As you know, there
>>>> have been several recent discussions about host header forgery detection
>>>> problems. It is not clear to me whether you are asking about some
>>>> specific new case or want to revisit some specific aspects of those
>>>> discussions.
>>>>
>>>> Alex.
>>>>



From justinmschw at gmail.com  Sun Feb 21 20:39:58 2021
From: justinmschw at gmail.com (Justin Michael Schwartzbeck)
Date: Sun, 21 Feb 2021 14:39:58 -0600
Subject: [squid-users] icap server name lookup
Message-ID: <CAGEc96nZ=yaZsOka74k52GMa5fygDYsm94j1Sry5+SZU1vh0Bg@mail.gmail.com>

Hello,

I am a little puzzled by something.
I am using squid 4.11 in a docker container.
So I am setting things up so that I have two docker containers, a squid
container and an e2guardian container, where the e2guardian is an ICAP
service. I am connecting these two containers to the same network and
giving them names squid and e2guardian, respectively.

I added the following lines to my squid config:

*icap_service service_req reqmod_precache bypass=0
icap://e2guardian:1344/request*
*icap_service service_resp respmod_precache bypass=0
icap://e2guardian:1344/response*

But when I try to access I get an ICAP error, and the following log message:
*essential ICAP service is down after an options fetch failure:
icap://e2guardian:1344/request [down,!opt]*

If I exec into the docker container and ping e2guardian, it resolves fine.
If I take the IP address of the docker container and add it to that ICAP
line in my squid.conf, it works just fine. But I don't want to have to do
this obviously because I don't know what the IP of the docker container
will be when it comes up.

I should also mention that if I manually add the docker IP to my /etc/hosts
file on the squid container and give it the hostname of e2guardian, it
*still* doesn't work. This inclines me to believe that, for some reason, no
name resolution of any kind is performed when resolving the ICAP service.

My problem looks very similar to the following problem:
https://squid-users.squid-cache.narkive.com/YZLtJNiW/dns-lookup-fails-initially-for-fqdn-in-squid
Except that in my case it never comes up even after the three minutes, and
it doesn't work either if I add e2guardian to my /etc/hosts file.

I should also mention that I am waiting at least three seconds after the
e2guardian container comes up before I start the squid container.

Can someone help me diagnose this?

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210221/488559c8/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb 22 00:18:55 2021
From: squid3 at treenet.co.nz (=?UTF-8?B?4oCqQW1vcyBKZWZmcmllc+KArA==?=)
Date: Mon, 22 Feb 2021 13:18:55 +1300
Subject: [squid-users] icap server name lookup
References: <CAGEc96nZ=yaZsOka74k52GMa5fygDYsm94j1Sry5+SZU1vh0Bg@mail.gmail.com>
Message-ID: <-yi7p4i-k6cgqo-ufpsaj3terd6-7t02my-wpt5qg-lz24i39r9yi4-w8fjrb-xoa6dt4ccfmawn0j19-ouutic7a24a53dahb5crjcop-72oo9br9uepn25x2zmh201l4-dl2nxz-76nnqacm1uu8-pw6163.1613953135387@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210222/d8046664/attachment.htm>

From heimarbeit123.99 at web.de  Mon Feb 22 08:26:22 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Mon, 22 Feb 2021 09:26:22 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
Message-ID: <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210222/6adb2163/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb 22 09:24:36 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Feb 2021 22:24:36 +1300
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
 <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
Message-ID: <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>

On 22/02/21 9:26 pm, heimarbeit123.99 wrote:
> So I finally tried it on my Squid Proxy.
> I edited the squid like this:
> external_acl_type ad_group_member_check ttl=120 %LOGIN 
> /usr/lib/squid/ext_ldap_group_acl -d -R -K -S -b "dc=domain,dc=com" -D 
> ProxyUser at domain.com -W /etc/squid/ldappass.txt -f 
> "(&(sAMAccountName=%u)(memberOf=CN=%g,OU=Groups,DC=domain,DC=com))" -h 
> my.domain.com



> But now I have the problem, that in the squid cache.log is written:
> ext_ldap_group_acl: WARNING: LDAP search error 'Referral'
> So it seems like LDAP can not check the groups but I have no clue why.. 
> Can someone help?

Please read the documentation for that helper. Specifically pay 
attention to what all those command line options do.
  <http://www.squid-cache.org/Versions/v4/manuals/ext_ldap_group_acl.html>


Amos


From heimarbeit123.99 at web.de  Mon Feb 22 09:42:01 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Mon, 22 Feb 2021 10:42:01 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
 <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
 <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>
Message-ID: <trinity-34a274c1-cf50-435e-a7db-405a05122e8a-1613986921889@3c-app-webde-bs47>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210222/2551edec/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb 22 10:05:08 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Feb 2021 23:05:08 +1300
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-34a274c1-cf50-435e-a7db-405a05122e8a-1613986921889@3c-app-webde-bs47>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
 <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
 <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>
 <trinity-34a274c1-cf50-435e-a7db-405a05122e8a-1613986921889@3c-app-webde-bs47>
Message-ID: <375d61e9-4f6f-0ade-0d7e-6a4f3f58f87c@treenet.co.nz>

On 22/02/21 10:42 pm, heimarbeit123.99 wrote:
> of course I did read the documentation. Otherwise I would not have asked 
> here. I would not ask for your time if the solution would be available 
> for myself.
> I am asking right here -after some weeks- because I do not know what is 
> finally wrong.

You used the -R option to forbid "Referral" when your LDAP system 
contains referrals. That is the direct cause of the *WARNING*, and 
possibly why the lookup fails if the user/group details are contained in 
that alternate LDAP database.

Amos


From heimarbeit123.99 at web.de  Mon Feb 22 10:41:45 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Mon, 22 Feb 2021 11:41:45 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <375d61e9-4f6f-0ade-0d7e-6a4f3f58f87c@treenet.co.nz>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
 <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
 <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>
 <trinity-34a274c1-cf50-435e-a7db-405a05122e8a-1613986921889@3c-app-webde-bs47>
 <375d61e9-4f6f-0ade-0d7e-6a4f3f58f87c@treenet.co.nz>
Message-ID: <trinity-2dd9bc7c-e656-47f1-8210-57dad5db480a-1613990505078@3c-app-webde-bap02>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210222/ba49989d/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb 22 11:22:39 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Feb 2021 00:22:39 +1300
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-2dd9bc7c-e656-47f1-8210-57dad5db480a-1613990505078@3c-app-webde-bap02>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
 <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
 <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>
 <trinity-34a274c1-cf50-435e-a7db-405a05122e8a-1613986921889@3c-app-webde-bs47>
 <375d61e9-4f6f-0ade-0d7e-6a4f3f58f87c@treenet.co.nz>
 <trinity-2dd9bc7c-e656-47f1-8210-57dad5db480a-1613990505078@3c-app-webde-bap02>
Message-ID: <dde88a55-9be0-1a7a-f1e7-68bd659187e8@treenet.co.nz>

On 22/02/21 11:41 pm, heimarbeit123.99 wrote:
> You were right! I realy don't know how I was able to miss this..
> I removed "-R" and don't get the error anymore. I did read the 
> documentation again and -K and -S should be fine. -d of course too.
> But now I get the error "WARNING: LDAP search error 'Operations error'". 
> I found out that many people got that because they wrote "DN" instead of 
> "DC", but in my squid conf I wrote "DC".
> Maybe some syntax error?

I'm not seeing any syntax issues. Though I do not use LDAP myself and 
not very familiar with its syntax requirements.


> I don't see what I am missing here,because connection is OK in cache.log.
> 

As far as I understand there is no issues with the connection to LDAP 
service. There are issues with the things it is being required to do.

Since you have used -d to enable debug, there should be some lines added 
to cache.log by the helper about what it is doing and the results of 
each action. If none of those lines give you a hint, please try pasting 
them in a mail here and someone else might be able to spot something.


Amos


From heimarbeit123.99 at web.de  Tue Feb 23 06:43:40 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Tue, 23 Feb 2021 07:43:40 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <dde88a55-9be0-1a7a-f1e7-68bd659187e8@treenet.co.nz>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
 <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>
 <trinity-3573049f-1eb2-46e3-ac99-a8384c6a4e18-1613982382293@3c-app-webde-bs47>
 <fb6775ce-00fb-bbd4-d531-a822f9a87f9a@treenet.co.nz>
 <trinity-34a274c1-cf50-435e-a7db-405a05122e8a-1613986921889@3c-app-webde-bs47>
 <375d61e9-4f6f-0ade-0d7e-6a4f3f58f87c@treenet.co.nz>
 <trinity-2dd9bc7c-e656-47f1-8210-57dad5db480a-1613990505078@3c-app-webde-bap02>
 <dde88a55-9be0-1a7a-f1e7-68bd659187e8@treenet.co.nz>
Message-ID: <trinity-698be38c-bc46-4b80-a917-5a6920c1da79-1614062620438@3c-app-webde-bap11>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210223/606b0894/attachment.htm>

From email_arjun at yahoo.com  Tue Feb 23 18:09:07 2021
From: email_arjun at yahoo.com (Arjun K)
Date: Tue, 23 Feb 2021 18:09:07 +0000 (UTC)
Subject: [squid-users] Allow specific set of IP to access a specific set of
 URL
References: <1647957146.1399922.1614103747474.ref@mail.yahoo.com>
Message-ID: <1647957146.1399922.1614103747474@mail.yahoo.com>

Hi Team
Could you please let me know how to define an acl so that a specific set of IPs can access alone a specific set of URLs.
How to define such ACL.
acl <aclname> src "iplist.txt"
acl allowedurl src "url.txt"http_access <aclname> allowedurl
The above configuration results in ERROR 403 in the logs.
Also, I have an existing configuration which should not get affected and it should not interfere in the rules which were allowed for the servers previously.

Kindly help me in getting the acl so that a specific set of IP can access set of URLs without losing access to other sites which are present already in the configuration.

RegardsArjun K.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210223/d044fe99/attachment.htm>

From wangangelo at hotmail.com  Tue Feb 23 19:56:13 2021
From: wangangelo at hotmail.com (Angelo Wang)
Date: Tue, 23 Feb 2021 19:56:13 +0000
Subject: [squid-users] Squid Proxy Dashboard?
Message-ID: <SJ0PR04MB7277068AF776B8176A6EAC34DE809@SJ0PR04MB7277.namprd04.prod.outlook.com>

Looking for a already made squid proxy/dashboard

Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210223/fab3f61d/attachment.htm>

From justinmschw at gmail.com  Wed Feb 24 02:14:49 2021
From: justinmschw at gmail.com (Justin Michael Schwartzbeck)
Date: Tue, 23 Feb 2021 20:14:49 -0600
Subject: [squid-users] transparently proxy squid in a docker container
Message-ID: <CAGEc96mM2Dh3HSQvGQDS=VrmSFsR8v3AfxX1S9pzj=6MjytBWw@mail.gmail.com>

Hi all,

For some years I have used squid 3.5 with SSL bump and transparent proxy
locally on my laptop. I have been using the following in my squid.conf:


ssl_bump server-first all
http_port 3128
http_port 3129 intercept
http_port 3130 ssl-bump intercept generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl/bluestar.crt
key=/etc/squid/ssl/bluestar.pem


So if I want to manually set the proxy on the client side, I use port 3128,
but by default all http/https traffic is redirected to port 3129 and 3130,
respectively. Here are my iptables rules:


iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner
root -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner
dockeruser -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -j REDIRECT --to-ports
3129
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner
root -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner
dockeruser -j RETURN
iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -j REDIRECT --to-ports
3130


dockeruser is the user that starts the docker container, and proxy is the
actual squid user. I didn't know which one I needed a rule for, so I just
chose both.

As I said before, this worked great when I was running squid 3.5 on bare
metal. Now I am running squid 4 in a docker container. I am seeing the
following error many times in the squid logs when I try to use the
transparent proxy:


2021/02/24 01:45:17| WARNING: Forwarding loop detected for:

GET /success.txt HTTP/1.1

User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101
Firefox/78.0

Accept: */*

Accept-Language: en-US,en;q=0.5

Accept-Encoding: identity,gzip,deflate

Pragma: no-cache

Via: 1.1 19deb96addda (squid/4.11)

X-Forwarded-For: 172.18.0.1

Cache-Control: no-cache

Host: detectportal.firefox.com


And from firefox I see this:

WARNING: Forwarding loop detected for

SSL_ERROR_RX_RECORD_TOO_LONG


I feel like I am very close, but I'm not sure what I am missing. Does
someone else know of a better way to do this? I had assumed that since I
publish the ports, I should be able to redirect to them the same way I
would if squid were running locally.


I would appreciate any help in figuring this out.

Thanks,

-Justin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210223/e18c9904/attachment.htm>

From klaus_brandl at genua.de  Wed Feb 24 09:14:30 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Wed, 24 Feb 2021 09:14:30 +0000
Subject: [squid-users] Allow specific set of IP to access a specific set
 of URL
In-Reply-To: <1647957146.1399922.1614103747474@mail.yahoo.com>
References: <1647957146.1399922.1614103747474.ref@mail.yahoo.com>
 <1647957146.1399922.1614103747474@mail.yahoo.com>
Message-ID: <5333674b494217f5157b01430bba5977af970b36.camel@genua.de>

The acl for the url must be of type url_regex, or something else:

acl allowedurl url_regex "url.txt"

Regards

Klaus

Am Dienstag, den 23.02.2021, 18:09 +0000 schrieb Arjun K:
> Hi Team
> 
> Could you please let me know how to define an acl so that a specific
> set of IPs can access alone a specific set of URLs.
> How to define such ACL.
> 
> acl <aclname> src "iplist.txt"
> acl allowedurl src "url.txt"
> http_access <aclname> allowedurl
> 
> The above configuration results in ERROR 403 in the logs.
> 
> Also, I have an existing configuration which should not get affected
> and it should not interfere in the rules which were allowed for the
> servers previously.
> 
> 
> Kindly help me in getting the acl so that a specific set of IP can
> access set of URLs without losing access to other sites which are
> present already in the configuration.
> 
> 
> Regards
> Arjun K.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Wed Feb 24 10:52:48 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Feb 2021 23:52:48 +1300
Subject: [squid-users] transparently proxy squid in a docker container
In-Reply-To: <CAGEc96mM2Dh3HSQvGQDS=VrmSFsR8v3AfxX1S9pzj=6MjytBWw@mail.gmail.com>
References: <CAGEc96mM2Dh3HSQvGQDS=VrmSFsR8v3AfxX1S9pzj=6MjytBWw@mail.gmail.com>
Message-ID: <9357fcb7-efbd-efa7-2f56-059b5b6187a7@treenet.co.nz>

On 24/02/21 3:14 pm, Justin Michael Schwartzbeck wrote:
> Hi all,
> 
> For some years I have used squid 3.5 with SSL bump and transparent proxy 
> locally on my laptop. I have been using the following in my squid.conf:
> 
> 
> ssl_bump server-first all
> http_port 3128
> http_port 3129 intercept
> http_port 3130 ssl-bump intercept generate-host-certificates=on 
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl/bluestar.crt 
> key=/etc/squid/ssl/bluestar.pem
> 
> 
> So if I want to manually set the proxy on the client side, I use port 
> 3128, but by default all http/https traffic is redirected to port 3129 
> and 3130, respectively. Here are my iptables rules:
> 
> 
> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner 
> root -j RETURN
> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner 
> dockeruser -j RETURN
> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -j REDIRECT 
> --to-ports 3129
> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner 
> root -j RETURN
> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner 
> dockeruser -j RETURN
> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -j REDIRECT 
> --to-ports 3130
> 

These rules are inside the container, yes?


> 
> dockeruser is the user that starts the docker container, and proxy is 
> the actual squid user. I didn't know which one I needed a rule for, so I 
> just chose both.
> 

Should be the "effective user" Squid runs as. Apparently "proxy" from 
that description.


> As I said before, this worked great when I was running squid 3.5 on bare 
> metal. Now I am running squid 4 in a docker container. I am seeing the 
> following error many times in the squid logs when I try to use the 
> transparent proxy:
> 
> 
> 2021/02/24 01:45:17| WARNING: Forwarding loop detected for:
> 

Something on the network is routing traffic back to Squid. The most 
common cause is missing or broken policy routing rules on a router.

Be aware that for containers or virtual systems the host OS may be 
acting as a router for the container. As such it needs policy routing 
like any other.
  see 
<https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute> 
has details of rules needed, assuming your host OS is a Linux.


Amos


From squid3 at treenet.co.nz  Wed Feb 24 11:29:37 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2021 00:29:37 +1300
Subject: [squid-users] Allow specific set of IP to access a specific set
 of URL
In-Reply-To: <5333674b494217f5157b01430bba5977af970b36.camel@genua.de>
References: <1647957146.1399922.1614103747474.ref@mail.yahoo.com>
 <1647957146.1399922.1614103747474@mail.yahoo.com>
 <5333674b494217f5157b01430bba5977af970b36.camel@genua.de>
Message-ID: <770396db-631d-e095-a60b-36957937dcf9@treenet.co.nz>

On 24/02/21 10:14 pm, Klaus Brandl wrote:
> The acl for the url must be of type url_regex, or something else:
> 
> acl allowedurl url_regex "url.txt"
> 

This line tells Squid to load a file full of regex patterns. Nothing more.


The http_access line is the list of rules that determines when those 
loaded values are tested against a URL, and what happens when the URL 
matches (or not) any of the patterns.


> 
> Am Dienstag, den 23.02.2021, 18:09 +0000 schrieb Arjun K:
>> Hi Team
>>
>> Could you please let me know how to define an acl so that a specific
>> set of IPs can access alone a specific set of URLs.


Have you read the docs on how Squid does access control?
 
<https://wiki.squid-cache.org/SquidFaq/SquidAcl#The_Basics:_How_the_parts_fit_together>

The example config you showed indicates a lack of understanding the 
syntax. That understanding is key to these things actually working the 
way you want.


Lets look at your stated requirements:

 >  define an acl

acl okay_urls url_regex "/etc/squid/url.txt"


 > a specific set of IPs can access alone a specific set of URLs.

That sentence is an access policy. It has three parts:

   1) "set of IPs"

  I have assumed from the "can access" words that you mean client IPs. 
Which in networking are the TCP src-IP value.

So ...

    acl set_of_IPs src 192.0.2.1 192.0.2.45 192.0.2.156

   2) "set of URLs"

So ...

     acl set_of_urls url_regex "/etc/squid/url.txt"


   3) "can access alone"

I assume that means you want these IPs to access the URLs. But no others 
to be able to access those same URLs.

So ...
   # permit set_of_ips
   http_access allow set_of_ips set_of_urls
   http_access deny set_of_urls



 > Also, I have an existing configuration which should not get affected
 > and it should not interfere in the rules which were allowed for the
 > servers previously.

The most important word there is "previously".

This is where *you* understanding how Squid access controls fit together 
matters *a lot*.

The first http_access line that matches entirely will _end_ processing 
of the 'http_access' sequence. The action (allow/deny) specified on that 
matched line will be done.

So where you place the above http_access lines determine which 
transactions will be able to reach and be tested by them.


Amos


From senor.j.onion at gmail.com  Wed Feb 24 11:50:13 2021
From: senor.j.onion at gmail.com (=?utf-8?Q?Se=C3=B1or_J_Onion?=)
Date: Wed, 24 Feb 2021 13:50:13 +0200
Subject: [squid-users] My cache gived me a content-length of 0,
 and a 200 TCP_REFRESH_UNMODIFIED_ABORTED
Message-ID: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>

I am new to Squid, I have been trying to get this to work for almost two weeks now, and have found nothing in the archives.

This is my curl command (you will get a 403 forbidden by the time you run this dear reader):

curl -s -D - -o /dev/null -G -d "alt=media" -x "http://localhost:3128" http://storage.googleapis.com/omgimg.appspot.com/test.jpeg -H "host:storage.googleapis.com" -H "x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855" -H "x-amz-date:20210224T111631Z" -H "authorization:AWS4-HMAC-SHA256 Credential=GOOG1EGG4VCQ2EVRCJ2JCIO7ZDSZ3CY45Q72ATYZU2P32HITBFUOVQ6TEBWXI/20210224/auto/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=77a60480e47dda2b65ef3ebcd72a032458685e74e2560bb9083dbb03c3f6c13d?


These are the HTTP response headers:


FIRST RUN:

HTTP/1.1 200 OK
X-GUploader-UploadID: ABg5-UwzmWjdWRPkonOxhLl3rUUik6wN3MB_ME1w1pxS5Rtmp_Cl1AAiP5G3tA9oXpFfAMnLCn5Pb8VW1mioc6GI-wJDun1S_g
Expires: Wed, 24 Feb 2021 11:20:08 GMT
Date: Wed, 24 Feb 2021 11:20:08 GMT
Cache-Control: private, max-age=0
Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
ETag: "d5b65c332fb6f80a0eade692b40b4afd"
Content-Type: image/jpeg
x-goog-hash: crc32c=6ijxaQ==
x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
x-goog-storage-class: STANDARD
Accept-Ranges: bytes
Content-Length: 2296040
Server: UploadServer
X-Cache: MISS from 80396e157a13
X-Cache-Lookup: MISS from 80396e157a13:3128
Via: 1.1 80396e157a13 (squid/3.5.27)
Connection: keep-alive

Squid log: 200 2296721 TCP_MISS:HIER_DIRECT


SECOND RUN:

HTTP/1.1 200 OK
Content-Type: image/jpeg
x-goog-hash: crc32c=6ijxaQ==
x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
x-goog-storage-class: STANDARD
Accept-Ranges: bytes
X-GUploader-UploadID: ABg5-UzooDZGnVTXxGIWQ2i25EasnR3glFz41FfUFvclACmZb3iDccpSXsGbRH0cr-8lofOc6Wb3knUzYMTgj_wdrzo
Expires: Wed, 24 Feb 2021 11:20:17 GMT
Date: Wed, 24 Feb 2021 11:20:17 GMT
Cache-Control: private, max-age=0
Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
ETag: "d5b65c332fb6f80a0eade692b40b4afd"
Content-Length: 0
Server: UploadServer
Age: 0
X-Cache: HIT from 80396e157a13
X-Cache-Lookup: HIT from 80396e157a13:3128
Via: 1.1 80396e157a13 (squid/3.5.27)
Connection: keep-alive

Squid log: 200 651397 TCP_REFRESH_UNMODIFIED_ABORTED:HIER_DIRECT


I don?t know why the second time round, I a) don?t get a HIT, and b) why my content-length is 0.

I think - this is the reason why - when I make these same calls in nodejs I end up with an HTTP Parse error (HPE_INVALID_CONSTANT).


My squid.conf file looks like this:

acl localnet src 0.0.0.1-0.255.255.255
acl localnet src 10.0.0.0/8
acl localnet src 100.64.0.0/10
acl localnet src 169.254.0.0/16
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

acl Connect_ports port 80
acl Connect_ports port 443

acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !Connect_ports

http_access allow localhost manager
http_access deny manager

http_access deny to_localhost

http_access allow localnet
http_access allow localhost

http_access deny all

strip_query_terms off
refresh_pattern . 525600 100% 525600 override-expire override-lastmod ignore-reload ignore-no-cache ignore-no-store reload-into-ims ignore-must-revalidate ignore-private ignore-auth store-stale

cache_mem 2500 MB
maximum_object_size_in_memory 100 MB
memory_cache_mode always




Any direction would be greatly appreciated!






From justinmschw at gmail.com  Wed Feb 24 13:21:46 2021
From: justinmschw at gmail.com (Justin Schwartzbeck)
Date: Wed, 24 Feb 2021 07:21:46 -0600
Subject: [squid-users] transparently proxy squid in a docker container
In-Reply-To: <CAGEc96mM2Dh3HSQvGQDS=VrmSFsR8v3AfxX1S9pzj=6MjytBWw@mail.gmail.com>
References: <CAGEc96mM2Dh3HSQvGQDS=VrmSFsR8v3AfxX1S9pzj=6MjytBWw@mail.gmail.com>
Message-ID: <4d670de8-d2d3-446a-a6a3-2dfe5c9d9bcb@gmail.com>

I believe I have solved the forwarding loop issue by adding a preceding rule to -j ACCEPT all traffic originating from the docker network. Now I still have the SSL_ERROR_RX_RECORD_TOO_LONG issue, which seems to be unrelated. I will set logging to debug and do a wireshark session to see what might be going on.

?Get BlueMail for Android ?

On Feb 23, 2021, 8:14 PM, at 8:14 PM, Justin Michael Schwartzbeck <justinmschw at gmail.com> wrote:
>Hi all,
>
>For some years I have used squid 3.5 with SSL bump and transparent
>proxy
>locally on my laptop. I have been using the following in my squid.conf:
>
>
>ssl_bump server-first all
>http_port 3128
>http_port 3129 intercept
>http_port 3130 ssl-bump intercept generate-host-certificates=on
>dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl/bluestar.crt
>key=/etc/squid/ssl/bluestar.pem
>
>
>So if I want to manually set the proxy on the client side, I use port
>3128,
>but by default all http/https traffic is redirected to port 3129 and
>3130,
>respectively. Here are my iptables rules:
>
>
>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner
>root -j RETURN
>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner
>dockeruser -j RETURN
>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -j REDIRECT
>--to-ports
>3129
>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner
>--uid-owner
>root -j RETURN
>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner
>--uid-owner
>dockeruser -j RETURN
>iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -j REDIRECT
>--to-ports
>3130
>
>
>dockeruser is the user that starts the docker container, and proxy is
>the
>actual squid user. I didn't know which one I needed a rule for, so I
>just
>chose both.
>
>As I said before, this worked great when I was running squid 3.5 on
>bare
>metal. Now I am running squid 4 in a docker container. I am seeing the
>following error many times in the squid logs when I try to use the
>transparent proxy:
>
>
>2021/02/24 01:45:17| WARNING: Forwarding loop detected for:
>
>GET /success.txt HTTP/1.1
>
>User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101
>Firefox/78.0
>
>Accept: */*
>
>Accept-Language: en-US,en;q=0.5
>
>Accept-Encoding: identity,gzip,deflate
>
>Pragma: no-cache
>
>Via: 1.1 19deb96addda (squid/4.11)
>
>X-Forwarded-For: 172.18.0.1
>
>Cache-Control: no-cache
>
>Host: detectportal.firefox.com
>
>
>And from firefox I see this:
>
>WARNING: Forwarding loop detected for
>
>SSL_ERROR_RX_RECORD_TOO_LONG
>
>
>I feel like I am very close, but I'm not sure what I am missing. Does
>someone else know of a better way to do this? I had assumed that since
>I
>publish the ports, I should be able to redirect to them the same way I
>would if squid were running locally.
>
>
>I would appreciate any help in figuring this out.
>
>Thanks,
>
>-Justin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210224/0423d0bf/attachment.htm>

From squid3 at treenet.co.nz  Wed Feb 24 14:43:43 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Feb 2021 03:43:43 +1300
Subject: [squid-users] My cache gived me a content-length of 0,
 and a 200 TCP_REFRESH_UNMODIFIED_ABORTED
In-Reply-To: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>
References: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>
Message-ID: <75628d37-14c8-629a-2c87-13d0030824fb@treenet.co.nz>

On 25/02/21 12:50 am, Se?or J Onion wrote:
> I am new to Squid, I have been trying to get this to work for almost two weeks now, and have found nothing in the archives.
> 
> This is my curl command (you will get a 403 forbidden by the time you run this dear reader):
> 
> curl -s -D - -o /dev/null -G -d "alt=media" -x "http://localhost:3128" http://storage.googleapis.com/omgimg.appspot.com/test.jpeg -H "host:storage.googleapis.com" -H "x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855" -H "x-amz-date:20210224T111631Z" -H "authorization:AWS4-HMAC-SHA256 Credential=GOOG1EGG4VCQ2EVRCJ2JCIO7ZDSZ3CY45Q72ATYZU2P32HITBFUOVQ6TEBWXI/20210224/auto/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=77a60480e47dda2b65ef3ebcd72a032458685e74e2560bb9083dbb03c3f6c13d?
> 
> 
> These are the HTTP response headers:
> 

Actually these are *half* the response headers. There are also the 
response headers received y Squid from the server.

The key details to explain your problem are in those other 
request-response headers going between Squid and server.


> 
> FIRST RUN:
> 
> HTTP/1.1 200 OK
> X-GUploader-UploadID: ABg5-UwzmWjdWRPkonOxhLl3rUUik6wN3MB_ME1w1pxS5Rtmp_Cl1AAiP5G3tA9oXpFfAMnLCn5Pb8VW1mioc6GI-wJDun1S_g
> Expires: Wed, 24 Feb 2021 11:20:08 GMT
> Date: Wed, 24 Feb 2021 11:20:08 GMT
> Cache-Control: private, max-age=0
> Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT

First clue:
   This object is not allowed to be cached. If a modern Squid is forced 
to cache anyway, the server will be checked for updates before it is 
delivered anywhere.


> ETag: "d5b65c332fb6f80a0eade692b40b4afd"
> Content-Type: image/jpeg
> x-goog-hash: crc32c=6ijxaQ==
> x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
> x-goog-storage-class: STANDARD
> Accept-Ranges: bytes
> Content-Length: 2296040
> Server: UploadServer
> X-Cache: MISS from 80396e157a13
> X-Cache-Lookup: MISS from 80396e157a13:3128
> Via: 1.1 80396e157a13 (squid/3.5.27)
> Connection: keep-alive
> 
> Squid log: 200 2296721 TCP_MISS:HIER_DIRECT
> 
> 
> SECOND RUN:
> 
> HTTP/1.1 200 OK
> Content-Type: image/jpeg
> x-goog-hash: crc32c=6ijxaQ==
> x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
> x-goog-storage-class: STANDARD
> Accept-Ranges: bytes
> X-GUploader-UploadID: ABg5-UzooDZGnVTXxGIWQ2i25EasnR3glFz41FfUFvclACmZb3iDccpSXsGbRH0cr-8lofOc6Wb3knUzYMTgj_wdrzo
> Expires: Wed, 24 Feb 2021 11:20:17 GMT
> Date: Wed, 24 Feb 2021 11:20:17 GMT
> Cache-Control: private, max-age=0
> Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
> ETag: "d5b65c332fb6f80a0eade692b40b4afd"
> Content-Length: 0
> Server: UploadServer
> Age: 0
> X-Cache: HIT from 80396e157a13
> X-Cache-Lookup: HIT from 80396e157a13:3128
> Via: 1.1 80396e157a13 (squid/3.5.27)
> Connection: keep-alive
> 
> Squid log: 200 651397 TCP_REFRESH_UNMODIFIED_ABORTED:HIER_DIRECT
> 

Second clue is the REFRESH and UNMODIFIED tags. Squid revalidated with 
the server. The server said no changes happened beyond some headers.

Third clue(s) are the differences in headers between first and second 
responses to the client.

X-Cache, Via, Age, and Connection are generated by Squid. So ignore those.

The remainder of headers are from the server. The second reply is the 
HIT object with header changes provided in the servers 304 message.
  Expires, Date, X-GUploader-UploadID, and Content-Length changed.


> 
> I don?t know why the second time round, I a) don?t get a HIT, and b) why my content-length is 0.
> 


Because the server told Squid the objects length has become 0 bytes.

<https://bugs.squid-cache.org/show_bug.cgi?id=4882#c11> has some more 
details.

The very latest HTTP specification text has made it even clearer:

"
    For messages that do not include content, the
    Content-Length indicates the size of the selected representation
"

"
   Any response ... 304 (Not Modified) status
   code is always terminated by the first empty line after the
   header fields, regardless of the header fields present in the
   message, and thus cannot contain a message body or trailer
   section(s).
"


The 'selected representation' is being the object that was cached.

Amos


From senor.j.onion at gmail.com  Wed Feb 24 15:45:43 2021
From: senor.j.onion at gmail.com (=?utf-8?Q?Se=C3=B1or_J_Onion?=)
Date: Wed, 24 Feb 2021 17:45:43 +0200
Subject: [squid-users] My cache gived me a content-length of 0,
 and a 200 TCP_REFRESH_UNMODIFIED_ABORTED
In-Reply-To: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>
References: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>
Message-ID: <18BA9F8E-8C4E-4F4B-9799-6394299DD40A@gmail.com>

Thanks Amos - that?s a very kind and thorough explanation.

Ok - I understand why the content-length is 0 as the server responded with a 304. Gotcha. Squid in response responds with a 200.

And I also understand why it is a REFRESH because the server responded saying that it should not cache. (As a side note - I don?t understand what these directives do then: ignore-no-cache ignore-no-store ignore-must-revalidate ignore-private)

What isn?t clear to me then is whether Squid actually returns the cached image? (content-length being 0).

The reason being, is that I am using Squid between my server and AWS S3 (or Google Storage)

The first time I call my nodejs code to fetch the image from S3 (via the proxy) it works.
The second time I call my nodejs code to fetch the image from S3, it fails. (Same Squid logs - instead of curl I am using nodejs code)

My nodejs code fails with an HTTP Parse error (HPE_INVALID_CONSTANT). My guess is that the response from Squid  is malformed. My guess is that the response is empty. Or that the image is in the response, but the content-length 0 causes the nodejs HTTP response parser to fail.

I don?t understand why my code behaves differently when it is receiving the image for the first time, and when it is receiving the cached image.

> On 24 Feb 2021, at 13:50, Se?or J Onion <senor.j.onion at gmail.com> wrote:
> 
> I am new to Squid, I have been trying to get this to work for almost two weeks now, and have found nothing in the archives.
> 
> This is my curl command (you will get a 403 forbidden by the time you run this dear reader):
> 
> curl -s -D - -o /dev/null -G -d "alt=media" -x "http://localhost:3128" http://storage.googleapis.com/omgimg.appspot.com/test.jpeg -H "host:storage.googleapis.com" -H "x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855" -H "x-amz-date:20210224T111631Z" -H "authorization:AWS4-HMAC-SHA256 Credential=GOOG1EGG4VCQ2EVRCJ2JCIO7ZDSZ3CY45Q72ATYZU2P32HITBFUOVQ6TEBWXI/20210224/auto/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=77a60480e47dda2b65ef3ebcd72a032458685e74e2560bb9083dbb03c3f6c13d?
> 
> 
> These are the HTTP response headers:
> 
> 
> FIRST RUN:
> 
> HTTP/1.1 200 OK
> X-GUploader-UploadID: ABg5-UwzmWjdWRPkonOxhLl3rUUik6wN3MB_ME1w1pxS5Rtmp_Cl1AAiP5G3tA9oXpFfAMnLCn5Pb8VW1mioc6GI-wJDun1S_g
> Expires: Wed, 24 Feb 2021 11:20:08 GMT
> Date: Wed, 24 Feb 2021 11:20:08 GMT
> Cache-Control: private, max-age=0
> Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
> ETag: "d5b65c332fb6f80a0eade692b40b4afd"
> Content-Type: image/jpeg
> x-goog-hash: crc32c=6ijxaQ==
> x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
> x-goog-storage-class: STANDARD
> Accept-Ranges: bytes
> Content-Length: 2296040
> Server: UploadServer
> X-Cache: MISS from 80396e157a13
> X-Cache-Lookup: MISS from 80396e157a13:3128
> Via: 1.1 80396e157a13 (squid/3.5.27)
> Connection: keep-alive
> 
> Squid log: 200 2296721 TCP_MISS:HIER_DIRECT
> 
> 
> SECOND RUN:
> 
> HTTP/1.1 200 OK
> Content-Type: image/jpeg
> x-goog-hash: crc32c=6ijxaQ==
> x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
> x-goog-storage-class: STANDARD
> Accept-Ranges: bytes
> X-GUploader-UploadID: ABg5-UzooDZGnVTXxGIWQ2i25EasnR3glFz41FfUFvclACmZb3iDccpSXsGbRH0cr-8lofOc6Wb3knUzYMTgj_wdrzo
> Expires: Wed, 24 Feb 2021 11:20:17 GMT
> Date: Wed, 24 Feb 2021 11:20:17 GMT
> Cache-Control: private, max-age=0
> Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
> ETag: "d5b65c332fb6f80a0eade692b40b4afd"
> Content-Length: 0
> Server: UploadServer
> Age: 0
> X-Cache: HIT from 80396e157a13
> X-Cache-Lookup: HIT from 80396e157a13:3128
> Via: 1.1 80396e157a13 (squid/3.5.27)
> Connection: keep-alive
> 
> Squid log: 200 651397 TCP_REFRESH_UNMODIFIED_ABORTED:HIER_DIRECT
> 
> 
> I don?t know why the second time round, I a) don?t get a HIT, and b) why my content-length is 0.
> 
> I think - this is the reason why - when I make these same calls in nodejs I end up with an HTTP Parse error (HPE_INVALID_CONSTANT).
> 
> 
> My squid.conf file looks like this:
> 
> acl localnet src 0.0.0.1-0.255.255.255
> acl localnet src 10.0.0.0/8
> acl localnet src 100.64.0.0/10
> acl localnet src 169.254.0.0/16
> acl localnet src 172.16.0.0/12
> acl localnet src 192.168.0.0/16
> acl localnet src fc00::/7
> acl localnet src fe80::/10
> 
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 443
> 
> acl Connect_ports port 80
> acl Connect_ports port 443
> 
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !Connect_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> http_access deny to_localhost
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all
> 
> strip_query_terms off
> refresh_pattern . 525600 100% 525600 override-expire override-lastmod ignore-reload ignore-no-cache ignore-no-store reload-into-ims ignore-must-revalidate ignore-private ignore-auth store-stale
> 
> cache_mem 2500 MB
> maximum_object_size_in_memory 100 MB
> memory_cache_mode always
> 
> 
> 
> 
> Any direction would be greatly appreciated!
> 
> 
> 
> 



From rajjnagar at gmail.com  Thu Feb 25 04:51:53 2021
From: rajjnagar at gmail.com (Raj Nagar)
Date: Thu, 25 Feb 2021 10:21:53 +0530
Subject: [squid-users] Problem with upload size limit in squid
Message-ID: <CAF=YtiBE9GO0H7BUew=35P+AHxTc5qW3kKPq-xtfhXpwO_JvgA@mail.gmail.com>

Hi,

I am using squid as forward proxy and want to restrict upload of files
larger than 1 MB. I have used following configuration for same:
*request_body_max_size
1 MB*.
But this is not working for me and I am able to upload larger files.
Can someone please help for same. Thanks in advance

-- 
Regards,
Raj Nagar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210225/209c4bf9/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb 25 11:00:04 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Feb 2021 00:00:04 +1300
Subject: [squid-users] My cache gived me a content-length of 0,
 and a 200 TCP_REFRESH_UNMODIFIED_ABORTED
In-Reply-To: <18BA9F8E-8C4E-4F4B-9799-6394299DD40A@gmail.com>
References: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>
 <18BA9F8E-8C4E-4F4B-9799-6394299DD40A@gmail.com>
Message-ID: <73780f0c-9bde-07ea-6432-e3e36a6ee227@treenet.co.nz>

On 25/02/21 4:45 am, Se?or J Onion wrote:
> Thanks Amos - that?s a very kind and thorough explanation.
> 
> Ok - I understand why the content-length is 0 as the server responded with a 304. Gotcha. Squid in response responds with a 200.
> 
> And I also understand why it is a REFRESH because the server responded saying that it should not cache. (As a side note - I don?t understand what these directives do then: ignore-no-cache ignore-no-store ignore-must-revalidate ignore-private)
> 

ignore-no-cache no longer exists, even in your old Squid version.

ignore-must-revalidate and ignore-auth were intended to help admin work 
around caching issues in HTTP/1.0 traffic and during the early days of 
the web's transition to HTTP/1.1. In latest Squid they no longer exist. 
In your old version they are buggy to the point of being somewhat 
dangerous. The ignore-auth is actively *preventing* things caching in 
Squid-3.

ignore-no-store and ignore-private are for admin who think the bandwidth 
saving of not having to transfer response bodies outweigh the risks of 
hitting protocol issues - including the one you hit.



> What isn?t clear to me then is whether Squid actually returns the cached image? (content-length being 0).
> 

There should be 0 bytes of payload/body/object in the 200 response Squid 
delivers.  But I'm not certain Squid is working correctly there 
(dropping the cached object).


> The reason being, is that I am using Squid between my server and AWS S3 (or Google Storage)
> 
> The first time I call my nodejs code to fetch the image from S3 (via the proxy) it works.
> The second time I call my nodejs code to fetch the image from S3, it fails. (Same Squid logs - instead of curl I am using nodejs code)
> 
> My nodejs code fails with an HTTP Parse error (HPE_INVALID_CONSTANT). My guess is that the response from Squid  is malformed. My guess is that the response is empty. Or that the image is in the response, but the content-length 0 causes the nodejs HTTP response parser to fail.
> 

The response you presented earlier is fine. It is simply a 0-byte object 
with the 200 status.


*IF* Squid is delivering the cached object, or any part of it along with 
that Content-Length:0 header. Then the software receiving will be seeing 
those bytes as if they were a second HTTP response message received - 
and failing to parse it because actually an image.


Amos


From rousskov at measurement-factory.com  Thu Feb 25 17:49:55 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Feb 2021 12:49:55 -0500
Subject: [squid-users] My cache gived me a content-length of 0,
 and a 200 TCP_REFRESH_UNMODIFIED_ABORTED
In-Reply-To: <18BA9F8E-8C4E-4F4B-9799-6394299DD40A@gmail.com>
References: <F3F6FBAA-1D6B-4BEA-B62D-808021745F03@gmail.com>
 <18BA9F8E-8C4E-4F4B-9799-6394299DD40A@gmail.com>
Message-ID: <a41d7491-3fe7-57f9-5465-de3bca7df24d@measurement-factory.com>

On 2/24/21 10:45 AM, Se?or J Onion wrote:

> I don?t understand why my code behaves differently when it is
> receiving the image for the first time, and when it is receiving the
> cached image.

What you see is a result of two bugs.

* An origin server bug: During the second transaction, when Squid asks
the server whether there are any updates for the object cached during
the first transaction, the origin server lies to Squid about the changed
size of the object. The origin server gives Squid two contradictory
statements: "the object you cached has not changed at all (its strong
ETag remains the same)" and "the size of the object you cached has
changed (to zero)".

* A Squid bug or deficiency: Squid should detect an inconsistent server
response and, instead of serving the cached object with lying headers,
Squid should remove the cached object and request a fresh one (a cache
miss due to revalidation failure). Optionally, as an admin-authorized
"optimization", Squid can ignore the Content-Length field in the buggy
origin server response and serve a hit with other updated headers.

This is a known problem but nobody has volunteered to address it yet.

For workarounds, see https://bugs.squid-cache.org/show_bug.cgi?id=4882#c14


HTH,

Alex.


>> On 24 Feb 2021, at 13:50, Se?or J Onion <senor.j.onion at gmail.com> wrote:
>>
>> I am new to Squid, I have been trying to get this to work for almost two weeks now, and have found nothing in the archives.
>>
>> This is my curl command (you will get a 403 forbidden by the time you run this dear reader):
>>
>> curl -s -D - -o /dev/null -G -d "alt=media" -x "http://localhost:3128" http://storage.googleapis.com/omgimg.appspot.com/test.jpeg -H "host:storage.googleapis.com" -H "x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855" -H "x-amz-date:20210224T111631Z" -H "authorization:AWS4-HMAC-SHA256 Credential=GOOG1EGG4VCQ2EVRCJ2JCIO7ZDSZ3CY45Q72ATYZU2P32HITBFUOVQ6TEBWXI/20210224/auto/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=77a60480e47dda2b65ef3ebcd72a032458685e74e2560bb9083dbb03c3f6c13d?
>>
>>
>> These are the HTTP response headers:
>>
>>
>> FIRST RUN:
>>
>> HTTP/1.1 200 OK
>> X-GUploader-UploadID: ABg5-UwzmWjdWRPkonOxhLl3rUUik6wN3MB_ME1w1pxS5Rtmp_Cl1AAiP5G3tA9oXpFfAMnLCn5Pb8VW1mioc6GI-wJDun1S_g
>> Expires: Wed, 24 Feb 2021 11:20:08 GMT
>> Date: Wed, 24 Feb 2021 11:20:08 GMT
>> Cache-Control: private, max-age=0
>> Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
>> ETag: "d5b65c332fb6f80a0eade692b40b4afd"
>> Content-Type: image/jpeg
>> x-goog-hash: crc32c=6ijxaQ==
>> x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
>> x-goog-storage-class: STANDARD
>> Accept-Ranges: bytes
>> Content-Length: 2296040
>> Server: UploadServer
>> X-Cache: MISS from 80396e157a13
>> X-Cache-Lookup: MISS from 80396e157a13:3128
>> Via: 1.1 80396e157a13 (squid/3.5.27)
>> Connection: keep-alive
>>
>> Squid log: 200 2296721 TCP_MISS:HIER_DIRECT
>>
>>
>> SECOND RUN:
>>
>> HTTP/1.1 200 OK
>> Content-Type: image/jpeg
>> x-goog-hash: crc32c=6ijxaQ==
>> x-goog-hash: md5=1bZcMy+2+AoOreaStAtK/Q==
>> x-goog-storage-class: STANDARD
>> Accept-Ranges: bytes
>> X-GUploader-UploadID: ABg5-UzooDZGnVTXxGIWQ2i25EasnR3glFz41FfUFvclACmZb3iDccpSXsGbRH0cr-8lofOc6Wb3knUzYMTgj_wdrzo
>> Expires: Wed, 24 Feb 2021 11:20:17 GMT
>> Date: Wed, 24 Feb 2021 11:20:17 GMT
>> Cache-Control: private, max-age=0
>> Last-Modified: Tue, 04 Aug 2020 12:09:00 GMT
>> ETag: "d5b65c332fb6f80a0eade692b40b4afd"
>> Content-Length: 0
>> Server: UploadServer
>> Age: 0
>> X-Cache: HIT from 80396e157a13
>> X-Cache-Lookup: HIT from 80396e157a13:3128
>> Via: 1.1 80396e157a13 (squid/3.5.27)
>> Connection: keep-alive
>>
>> Squid log: 200 651397 TCP_REFRESH_UNMODIFIED_ABORTED:HIER_DIRECT
>>
>>
>> I don?t know why the second time round, I a) don?t get a HIT, and b) why my content-length is 0.
>>
>> I think - this is the reason why - when I make these same calls in nodejs I end up with an HTTP Parse error (HPE_INVALID_CONSTANT).
>>
>>
>> My squid.conf file looks like this:
>>
>> acl localnet src 0.0.0.1-0.255.255.255
>> acl localnet src 10.0.0.0/8
>> acl localnet src 100.64.0.0/10
>> acl localnet src 169.254.0.0/16
>> acl localnet src 172.16.0.0/12
>> acl localnet src 192.168.0.0/16
>> acl localnet src fc00::/7
>> acl localnet src fe80::/10
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80
>> acl Safe_ports port 443
>>
>> acl Connect_ports port 80
>> acl Connect_ports port 443
>>
>> acl CONNECT method CONNECT
>>
>> http_access deny !Safe_ports
>>
>> http_access deny CONNECT !Connect_ports
>>
>> http_access allow localhost manager
>> http_access deny manager
>>
>> http_access deny to_localhost
>>
>> http_access allow localnet
>> http_access allow localhost
>>
>> http_access deny all
>>
>> strip_query_terms off
>> refresh_pattern . 525600 100% 525600 override-expire override-lastmod ignore-reload ignore-no-cache ignore-no-store reload-into-ims ignore-must-revalidate ignore-private ignore-auth store-stale
>>
>> cache_mem 2500 MB
>> maximum_object_size_in_memory 100 MB
>> memory_cache_mode always
>>
>>
>>
>>
>> Any direction would be greatly appreciated!
>>
>>
>>
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Thu Feb 25 18:03:34 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Feb 2021 13:03:34 -0500
Subject: [squid-users] Problem with upload size limit in squid
In-Reply-To: <CAF=YtiBE9GO0H7BUew=35P+AHxTc5qW3kKPq-xtfhXpwO_JvgA@mail.gmail.com>
References: <CAF=YtiBE9GO0H7BUew=35P+AHxTc5qW3kKPq-xtfhXpwO_JvgA@mail.gmail.com>
Message-ID: <10c484a4-5781-2d42-d7f0-1437880f0b1b@measurement-factory.com>

On 2/24/21 11:51 PM, Raj Nagar wrote:

> I am using squid as forward proxy and want to restrict upload of files
> larger than 1 MB. I have used following configuration for
> same:?*request_body_max_size 1 MB*.
> But this is not working for me and I am able to upload larger files.
> Can someone please help for same. Thanks in advance

Does your Squid have access to the HTTP request information? For
example, if it is an HTTPS request, and you are not bumping the
corresponding TLS connection, then Squid would not be working at HTTP
level and, hence, would not be able to limit individual HTTP request sizes.

The corresponding access.log record may tell us more about the
problematic transaction.


HTH,

Alex.


From justinmschw at gmail.com  Thu Feb 25 19:01:10 2021
From: justinmschw at gmail.com (Justin Michael Schwartzbeck)
Date: Thu, 25 Feb 2021 13:01:10 -0600
Subject: [squid-users] transparently proxy squid in a docker container
In-Reply-To: <4d670de8-d2d3-446a-a6a3-2dfe5c9d9bcb@gmail.com>
References: <CAGEc96mM2Dh3HSQvGQDS=VrmSFsR8v3AfxX1S9pzj=6MjytBWw@mail.gmail.com>
 <4d670de8-d2d3-446a-a6a3-2dfe5c9d9bcb@gmail.com>
Message-ID: <CAGEc96mH=nbSs9AHCXfRLE30ntEuURUYkhiFgX90tnm+HsccTQ@mail.gmail.com>

I ended up using redsocks for doing the transparent proxy, that is working
perfectly for me now. I don't need to configure squid for this after all.

On Wed, Feb 24, 2021 at 7:21 AM Justin Schwartzbeck <justinmschw at gmail.com>
wrote:

> I believe I have solved the forwarding loop issue by adding a preceding
> rule to -j ACCEPT all traffic originating from the docker network. Now I
> still have the SSL_ERROR_RX_RECORD_TOO_LONG issue, which seems to be
> unrelated. I will set logging to debug and do a wireshark session to see
> what might be going on.
>
> Get BlueMail for Android <http://www.bluemail.me/r?b=16470>
> On Feb 23, 2021, at 8:14 PM, Justin Michael Schwartzbeck <
> justinmschw at gmail.com> wrote:
>>
>> Hi all,
>>
>> For some years I have used squid 3.5 with SSL bump and transparent proxy
>> locally on my laptop. I have been using the following in my squid.conf:
>>
>>
>> ssl_bump server-first all
>> http_port 3128
>> http_port 3129 intercept
>> http_port 3130 ssl-bump intercept generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl/bluestar.crt
>> key=/etc/squid/ssl/bluestar.pem
>>
>>
>> So if I want to manually set the proxy on the client side, I use port
>> 3128, but by default all http/https traffic is redirected to port 3129 and
>> 3130, respectively. Here are my iptables rules:
>>
>>
>> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner
>> root -j RETURN
>> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -m owner --uid-owner
>> dockeruser -j RETURN
>> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 80 -j REDIRECT --to-ports
>> 3129
>> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner
>> root -j RETURN
>> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -m owner --uid-owner
>> dockeruser -j RETURN
>> iptables -t nat -A OUTPUT -p tcp -m tcp --dport 443 -j REDIRECT
>> --to-ports 3130
>>
>>
>> dockeruser is the user that starts the docker container, and proxy is the
>> actual squid user. I didn't know which one I needed a rule for, so I just
>> chose both.
>>
>> As I said before, this worked great when I was running squid 3.5 on bare
>> metal. Now I am running squid 4 in a docker container. I am seeing the
>> following error many times in the squid logs when I try to use the
>> transparent proxy:
>>
>>
>> 2021/02/24 01:45:17| WARNING: Forwarding loop detected for:
>>
>> GET /success.txt HTTP/1.1
>>
>> User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101
>> Firefox/78.0
>>
>> Accept: */*
>>
>> Accept-Language: en-US,en;q=0.5
>>
>> Accept-Encoding: identity,gzip,deflate
>>
>> Pragma: no-cache
>>
>> Via: 1.1 19deb96addda (squid/4.11)
>>
>> X-Forwarded-For: 172.18.0.1
>>
>> Cache-Control: no-cache
>>
>> Host: detectportal.firefox.com
>>
>>
>> And from firefox I see this:
>>
>> WARNING: Forwarding loop detected for
>>
>> SSL_ERROR_RX_RECORD_TOO_LONG
>>
>>
>> I feel like I am very close, but I'm not sure what I am missing. Does
>> someone else know of a better way to do this? I had assumed that since I
>> publish the ports, I should be able to redirect to them the same way I
>> would if squid were running locally.
>>
>>
>> I would appreciate any help in figuring this out.
>>
>> Thanks,
>>
>> -Justin
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210225/ab871558/attachment.htm>

From justinmschw at gmail.com  Thu Feb 25 19:07:51 2021
From: justinmschw at gmail.com (Justin Michael Schwartzbeck)
Date: Thu, 25 Feb 2021 13:07:51 -0600
Subject: [squid-users] Squid ACL for bypassing ssl-bump
Message-ID: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>

Hi all,

I have thus far used dstdomain acl for bypassing ssl bump on sites that we
don't want to decrypt, like banking sites. It seems to work for some sites,
but not for others.

I see the following post on this from some years back:
http://www.squid-cache.org/mail-archive/squid-users/201303/0046.html

It seems like people there are recommending use of an IP based approach to
doing this. In this case you would need a static list of IP addresses to
the sites in question.

I was thinking about this, and it seems to me that if we are using the
squid proxy with a dns server, we should be able to check the dns cache for
that IP, and find the associated hostname, and then match against that.

Does squid support this kind of a thing? If not, I was going to write an
external acl helper that does a query on a DNS cache to see if it matches a
particular domain. However, I don't want to reinvent the wheel.

Thanks,
-Justin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210225/d9c917d7/attachment.htm>

From rousskov at measurement-factory.com  Thu Feb 25 20:57:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Feb 2021 15:57:00 -0500
Subject: [squid-users] Squid ACL for bypassing ssl-bump
In-Reply-To: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
Message-ID: <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>

On 2/25/21 2:07 PM, Justin Michael Schwartzbeck wrote:

> I have thus far used dstdomain acl for bypassing ssl bump on sites that
> we don't want to decrypt, like banking sites. It seems to work for some
> sites, but not for others.

Yes, many HTTPS transactions do not expose destination domain until it
is too late to decide whether to bump them, and reverse DNS lookups are
often unreliable.


> I was thinking about this, and it seems to me that if we are using the
> squid proxy with a dns server, we should be able to check the dns cache
> for that IP, and find the associated hostname, and then match against that.

When you use dstdomain, Squid will do a (reverse) DNS query for you as
necessary (including DNS cache lookups) unless you specify a -n option
that is documented to disable all such operations.


In many cases, you should be using ssl::server_name instead of dstdomain
or dst ACL, but you may have to use a combination of various ACLs to
cover all the cases you care about.


HTH,

Alex.



From rajjnagar at gmail.com  Fri Feb 26 01:44:04 2021
From: rajjnagar at gmail.com (Raj Nagar)
Date: Fri, 26 Feb 2021 07:14:04 +0530
Subject: [squid-users] Problem with upload size limit in squid
In-Reply-To: <10c484a4-5781-2d42-d7f0-1437880f0b1b@measurement-factory.com>
References: <CAF=YtiBE9GO0H7BUew=35P+AHxTc5qW3kKPq-xtfhXpwO_JvgA@mail.gmail.com>
 <10c484a4-5781-2d42-d7f0-1437880f0b1b@measurement-factory.com>
Message-ID: <CAF=YtiAQmXz+qVHKs0Pq7XWp8onS0vseHngMgX1zWH7vHk_jfg@mail.gmail.com>

Hi Alex,

Thanks for your response. Is there any way by which I can enforce these
limits on other protocols as https ?

On Thu, Feb 25, 2021, 23:33 Alex Rousskov <rousskov at measurement-factory.com>
wrote:

> On 2/24/21 11:51 PM, Raj Nagar wrote:
>
> > I am using squid as forward proxy and want to restrict upload of files
> > larger than 1 MB. I have used following configuration for
> > same: *request_body_max_size 1 MB*.
> > But this is not working for me and I am able to upload larger files.
> > Can someone please help for same. Thanks in advance
>
> Does your Squid have access to the HTTP request information? For
> example, if it is an HTTPS request, and you are not bumping the
> corresponding TLS connection, then Squid would not be working at HTTP
> level and, hence, would not be able to limit individual HTTP request sizes.
>
> The corresponding access.log record may tell us more about the
> problematic transaction.
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210226/8357f989/attachment.htm>

From m_zouhairy at ckta.by  Fri Feb 26 07:22:14 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Fri, 26 Feb 2021 10:22:14 +0300
Subject: [squid-users] squid cache
In-Reply-To: <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
Message-ID: <39cb4439-afd4-64c2-1d8f-f65bf088e717@ckta.by>


Health be Upon you,

i want to cache certain files, let's say exe, msi... above 20MB and 
below 300MB, limit the cache directory to 3GB
i have no ssl bump not configured
version 4.14
how to do that?


From jmpatagonia at gmail.com  Fri Feb 26 07:47:35 2021
From: jmpatagonia at gmail.com (jmpatagonia)
Date: Fri, 26 Feb 2021 07:47:35 +0000
Subject: [squid-users] help to redirect http request to another squid proxy
Message-ID: <CADZCxsvGj9FswUe4bMWz_QV_DtfiPwn7KV02z2fGozsKpYEZ4g@mail.gmail.com>

Hello I need help to redirect request http/https from a specific domain to
another squid proxy server.

Like a domain for example microsoft.com redirect o transfer all request to
another squid proxy server.

I try to use this:
#################################################
http_port  xx.xx.xx..xx:8080 accel
acl microsoft_acl dstdomain microsoft.com
cache_peer yy.yy.yy.yy  parent 8080  0  name=proxy60 default
cache_peer_access proxy60 allow  microsoft_acl
cache_peer_access proxy60 deny all
#################################################
but not wok, error :
26/Feb/2021:07:23:27 -0300 || - || xx.xx.xx.xx || TAG_NONE/405|| CONNECT ||
error:method-not-allowed || text/html

regards.
Juan Manuel.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210226/ca76c34d/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 26 14:59:24 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2021 09:59:24 -0500
Subject: [squid-users] Problem with upload size limit in squid
In-Reply-To: <CAF=YtiAQmXz+qVHKs0Pq7XWp8onS0vseHngMgX1zWH7vHk_jfg@mail.gmail.com>
References: <CAF=YtiBE9GO0H7BUew=35P+AHxTc5qW3kKPq-xtfhXpwO_JvgA@mail.gmail.com>
 <10c484a4-5781-2d42-d7f0-1437880f0b1b@measurement-factory.com>
 <CAF=YtiAQmXz+qVHKs0Pq7XWp8onS0vseHngMgX1zWH7vHk_jfg@mail.gmail.com>
Message-ID: <75db0d54-1277-859e-1c35-19bb30b2c9b5@measurement-factory.com>

On 2/25/21 8:44 PM, Raj Nagar wrote:

> Is there any way by which I can enforce these
> limits on other protocols as https?

If you want to enforce HTTP request size limits for HTTPS transactions,
then you have to bump TLS connections (to see HTTP inside TLS).

If you want to enforce connection limits for TLS (and other non-HTTP)
connections, then you can either enhance Squid or perhaps use some
TCP-level software that can track individual TCP connection usage. Since
this option deals with TCP connections, it cannot limit individual HTTP
uploads, only the total amount of bytes sent by the client. One TLS/TCP
connection could carry one (encrypted) request or thousands of
(encrypted) requests or even non HTTP traffic -- Squid cannot tell
without bumping that TLS connection.

Both options have ugly drawbacks. There is no good solution for what you
want to do (if you do not control the browser).

Alex.


> On Thu, Feb 25, 2021, 23:33 Alex Rousskov wrote:
> 
>     On 2/24/21 11:51 PM, Raj Nagar wrote:
> 
>     > I am using squid as forward proxy and want to restrict upload of files
>     > larger than 1 MB. I have used following configuration for
>     > same:?*request_body_max_size 1 MB*.
>     > But this is not working for me and I am able to upload larger files.
>     > Can someone please help for same. Thanks in advance
> 
>     Does your Squid have access to the HTTP request information? For
>     example, if it is an HTTPS request, and you are not bumping the
>     corresponding TLS connection, then Squid would not be working at HTTP
>     level and, hence, would not be able to limit individual HTTP request
>     sizes.
> 
>     The corresponding access.log record may tell us more about the
>     problematic transaction.
> 
> 
>     HTH,
> 
>     Alex.
> 



From rousskov at measurement-factory.com  Fri Feb 26 15:44:09 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2021 10:44:09 -0500
Subject: [squid-users] Squid ACL for bypassing ssl-bump
In-Reply-To: <CAGEc96nEHSFgQhdV-fVyF9hK4=wLgTKgMF7s5oxKNx-WQXyh6Q@mail.gmail.com>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
 <CAGEc96nEHSFgQhdV-fVyF9hK4=wLgTKgMF7s5oxKNx-WQXyh6Q@mail.gmail.com>
Message-ID: <4e83330f-3b95-7707-310a-3e4bd7705f51@measurement-factory.com>

On 2/26/21 7:35 AM, Justin Michael Schwartzbeck wrote:
>> Yes, many HTTPS transactions do not expose destination domain until it
>> is too late to decide whether to bump them, and reverse DNS lookups are
>> often unreliable.

> I wonder why this would be. 

I suspect you assume that a forward DNS lookup (A or AAAA query) answer
is always the "opposite" of a reverse DSN lookup (PTR query) answer.
AFAIK, that is not how DNS is defined. From DNS point of view, each of
those answers is totally independent -- there is no 1:1 or even 1:N
mapping between them; the answers even come from different DNS zones!  A
caching DNS resolver would probably violate the DNS protocol if it uses
a cached A or AAAA record to answer a PTR query. Disclaimer: I am not a
DNS expert.


> From my understanding, when you open a
> browser and browse to www.google.com, the very
> first thing that happens is you do a DNS resolution so that you know
> what IP to send the CONNECT request and subsequent HTTPS records in the
> first place.

What happens depends on the browser and the proxy port:

1. For forward proxies: Some browsers will not do DNS lookups. They will
send a CONNECT request to example.com, allowing Squid to do the DNS
lookup. In this case, Squid dstdomain configured with a host name will
work well.

2. For forward proxies: Some browsers do DNS lookups. They will send a
CONNECT request to one of the returned IP addresses.

3. For interception proxies: All browsers do DNS lookups. They open a
TCP connection to one of the returned IP addresses.

In cases 2 and 3, Squid dstdomain will have to do a reverse DNS lookup
(PTR query). In many cases, that lookup either fails or returns a
different domain name than the domain the browser started with.


HTH,

Alex.


> So we would have the IP already, and the hostname that was
> looked up already in the DNS cache, right? Why wouldn't squid just be
> able to reach in there, match the IP that DNS returned, and then pull
> that hostname out to compare against the ACLs?
> 
> On Thu, Feb 25, 2021 at 2:57 PM Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     On 2/25/21 2:07 PM, Justin Michael Schwartzbeck wrote:
> 
>     > I have thus far used dstdomain acl for bypassing ssl bump on sites
>     that
>     > we don't want to decrypt, like banking sites. It seems to work for
>     some
>     > sites, but not for others.
> 
>     Yes, many HTTPS transactions do not expose destination domain until it
>     is too late to decide whether to bump them, and reverse DNS lookups are
>     often unreliable.
> 
> 
>     > I was thinking about this, and it seems to me that if we are using the
>     > squid proxy with a dns server, we should be able to check the dns
>     cache
>     > for that IP, and find the associated hostname, and then match
>     against that.
> 
>     When you use dstdomain, Squid will do a (reverse) DNS query for you as
>     necessary (including DNS cache lookups) unless you specify a -n option
>     that is documented to disable all such operations.
> 
> 
>     In many cases, you should be using ssl::server_name instead of dstdomain
>     or dst ACL, but you may have to use a combination of various ACLs to
>     cover all the cases you care about.
> 
> 
>     HTH,
> 
>     Alex.
> 



From service.mv at gmail.com  Fri Feb 26 17:36:56 2021
From: service.mv at gmail.com (Service MV)
Date: Fri, 26 Feb 2021 14:36:56 -0300
Subject: [squid-users] Exclude transaction from being logged
Message-ID: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>

Hi,
I recently compiled SQUID 4.14 with SNMP support. I use Zabbix to monitor
SQUID by SNMP and it really works fine.
One thing I see is that because Zabbix checks status of SQUID port then
SQUID generates a lot of logs like this:
NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -

I know that this is not an error. But I want to exclude this log when the
Zabbix server checks the port status of SQUID. Zabbix server does not use
the SQUID as a proxy.

Some relevant configs (my setup is very basic and similar to defaults SQUID
config):
# SNMP monitoring with Zabbix
acl zabbix snmp_community zabbix_public
acl zabbix_proxy src x.x.x.x
snmp_port 3401
snmp_access allow zabbix zabbix_proxy
snmp_access deny all
[...]
# default SQUID rules
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
http_access allow localhost

# Allow users to use internet and deny to all others
http_access allow localnet
http_access deny all
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210226/fe042afe/attachment.htm>

From justinmschw at gmail.com  Fri Feb 26 17:45:01 2021
From: justinmschw at gmail.com (Justin Michael Schwartzbeck)
Date: Fri, 26 Feb 2021 11:45:01 -0600
Subject: [squid-users] Squid ACL for bypassing ssl-bump
In-Reply-To: <4e83330f-3b95-7707-310a-3e4bd7705f51@measurement-factory.com>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
 <CAGEc96nEHSFgQhdV-fVyF9hK4=wLgTKgMF7s5oxKNx-WQXyh6Q@mail.gmail.com>
 <4e83330f-3b95-7707-310a-3e4bd7705f51@measurement-factory.com>
Message-ID: <CAGEc96k5jF3y9_caFNVMDnZYmO3MRw7Og9rbDFH2xoK1_yRgDA@mail.gmail.com>

Thanks for your answers Alex.

For case 1, I understand that should not be a problem, since squid is the
one asking for DNS resolution.
For case 2 and 3, what you are saying is that the browser is requesting the
DNS lookup first, correct? Hence the need for a reverse DNS from squid,
since squid does not know at that point what domain the IP belongs to. But
they still had to query the DNS server, so that entry is in that DNS cache,
and it should have the same domain as the lookup that the user entered.

So if I have a local dns (maybe dnsmasq) that both squid and the user use,
from what I understand I should be able to use squid's dns_nameservers
directive to point to that DNS, and it should return fine since it is
stored right there in the cache.

If the user is trying to use a different DNS server other than the local
one, then fine, I will just decrypt their traffic as punishment. ?

On Fri, Feb 26, 2021 at 9:44 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2/26/21 7:35 AM, Justin Michael Schwartzbeck wrote:
> >> Yes, many HTTPS transactions do not expose destination domain until it
> >> is too late to decide whether to bump them, and reverse DNS lookups are
> >> often unreliable.
>
> > I wonder why this would be.
>
> I suspect you assume that a forward DNS lookup (A or AAAA query) answer
> is always the "opposite" of a reverse DSN lookup (PTR query) answer.
> AFAIK, that is not how DNS is defined. From DNS point of view, each of
> those answers is totally independent -- there is no 1:1 or even 1:N
> mapping between them; the answers even come from different DNS zones!  A
> caching DNS resolver would probably violate the DNS protocol if it uses
> a cached A or AAAA record to answer a PTR query. Disclaimer: I am not a
> DNS expert.
>
>
> > From my understanding, when you open a
> > browser and browse to www.google.com, the very
> > first thing that happens is you do a DNS resolution so that you know
> > what IP to send the CONNECT request and subsequent HTTPS records in the
> > first place.
>
> What happens depends on the browser and the proxy port:
>
> 1. For forward proxies: Some browsers will not do DNS lookups. They will
> send a CONNECT request to example.com, allowing Squid to do the DNS
> lookup. In this case, Squid dstdomain configured with a host name will
> work well.
>
> 2. For forward proxies: Some browsers do DNS lookups. They will send a
> CONNECT request to one of the returned IP addresses.
>
> 3. For interception proxies: All browsers do DNS lookups. They open a
> TCP connection to one of the returned IP addresses.
>
> In cases 2 and 3, Squid dstdomain will have to do a reverse DNS lookup
> (PTR query). In many cases, that lookup either fails or returns a
> different domain name than the domain the browser started with.
>
>
> HTH,
>
> Alex.
>
>
> > So we would have the IP already, and the hostname that was
> > looked up already in the DNS cache, right? Why wouldn't squid just be
> > able to reach in there, match the IP that DNS returned, and then pull
> > that hostname out to compare against the ACLs?
> >
> > On Thu, Feb 25, 2021 at 2:57 PM Alex Rousskov
> > <rousskov at measurement-factory.com
> > <mailto:rousskov at measurement-factory.com>> wrote:
> >
> >     On 2/25/21 2:07 PM, Justin Michael Schwartzbeck wrote:
> >
> >     > I have thus far used dstdomain acl for bypassing ssl bump on sites
> >     that
> >     > we don't want to decrypt, like banking sites. It seems to work for
> >     some
> >     > sites, but not for others.
> >
> >     Yes, many HTTPS transactions do not expose destination domain until
> it
> >     is too late to decide whether to bump them, and reverse DNS lookups
> are
> >     often unreliable.
> >
> >
> >     > I was thinking about this, and it seems to me that if we are using
> the
> >     > squid proxy with a dns server, we should be able to check the dns
> >     cache
> >     > for that IP, and find the associated hostname, and then match
> >     against that.
> >
> >     When you use dstdomain, Squid will do a (reverse) DNS query for you
> as
> >     necessary (including DNS cache lookups) unless you specify a -n
> option
> >     that is documented to disable all such operations.
> >
> >
> >     In many cases, you should be using ssl::server_name instead of
> dstdomain
> >     or dst ACL, but you may have to use a combination of various ACLs to
> >     cover all the cases you care about.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210226/743eb2cf/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 26 19:21:42 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2021 14:21:42 -0500
Subject: [squid-users] Exclude transaction from being logged
In-Reply-To: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>
References: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>
Message-ID: <4f7fadbe-1a53-3d06-0d29-c8ec237a4da9@measurement-factory.com>

On 2/26/21 12:36 PM, Service MV wrote:

> NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
> 
> I know that this is not an error. But I want to exclude this log when
> the Zabbix server checks the port status of SQUID. Zabbix server does
> not use the SQUID as a proxy.

If you do not want to see what Zabbix is doing to your Squid, you can
exclude its requests from access.log. For example, if you do not want to
see any requests from Zabbix IP address:

  acl zabbix_proxy src x.x.x.x
  access_log none zabbix_proxy


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Feb 26 19:43:11 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2021 14:43:11 -0500
Subject: [squid-users] Squid ACL for bypassing ssl-bump
In-Reply-To: <CAGEc96k5jF3y9_caFNVMDnZYmO3MRw7Og9rbDFH2xoK1_yRgDA@mail.gmail.com>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
 <CAGEc96nEHSFgQhdV-fVyF9hK4=wLgTKgMF7s5oxKNx-WQXyh6Q@mail.gmail.com>
 <4e83330f-3b95-7707-310a-3e4bd7705f51@measurement-factory.com>
 <CAGEc96k5jF3y9_caFNVMDnZYmO3MRw7Og9rbDFH2xoK1_yRgDA@mail.gmail.com>
Message-ID: <e5b39544-df17-298d-2602-b0af0a8b4005@measurement-factory.com>

On 2/26/21 12:45 PM, Justin Michael Schwartzbeck wrote:

> For case 2 and 3, what you are saying is that the browser is requesting
> the DNS lookup first, correct?

Correct, but that does not really matter.


> Hence the need for a reverse DNS from
> squid, since squid does not know at that point what domain the IP
> belongs to.

Squid "does not know" because all Squid gets is an IP address (in those
two cases).


> But they still had to query the DNS server, so that entry is
> in that DNS cache, and it should have the same domain as the lookup that
> the user entered.

DNS does not support what feels natural to you. You are thinking of a
name:IP cache entry that can be looked up by IP. That is a natural
model, but it does not match reality. DNS simply does not have an
interface that says "find me a name that maps to IP Y". DNS essentially
has only one interface: "find me what maps to name X". That is it! There
is just no way to ask a DNS server what name in its cache maps to an IP
address.

For reverse DNS lookups, the DNS client constructs an IP-based _name_ in
a special in-addr.arpa DNS zone and uses that name to query the DNS
server. For example, a "reverse" lookup for 127.0.0.1 is really a lookup
for the "1.0.0.127.in-addr.arpa" name. And that lookup follows all the
DNS rules about contacting authoritative servers for the zone, etc.; the
DNS server does not really "know" that what you really want is a cached
domain name for that 127.0.0.1 IP address.


> So if I have a local dns (maybe dnsmasq) that both squid and the user
> use, from what I understand I should be able to use squid's
> dns_nameservers directive to point to that DNS, and it should return
> fine since it is stored right there in the cache.

The IP may be stored, but it cannot be looked up using DNS.

Alex.


> On Fri, Feb 26, 2021 at 9:44 AM Alex Rousskov wrote:
> 
>     On 2/26/21 7:35 AM, Justin Michael Schwartzbeck wrote:
>     >> Yes, many HTTPS transactions do not expose destination domain
>     until it
>     >> is too late to decide whether to bump them, and reverse DNS
>     lookups are
>     >> often unreliable.
> 
>     > I wonder why this would be.
> 
>     I suspect you assume that a forward DNS lookup (A or AAAA query) answer
>     is always the "opposite" of a reverse DSN lookup (PTR query) answer.
>     AFAIK, that is not how DNS is defined. From DNS point of view, each of
>     those answers is totally independent -- there is no 1:1 or even 1:N
>     mapping between them; the answers even come from different DNS zones!? A
>     caching DNS resolver would probably violate the DNS protocol if it uses
>     a cached A or AAAA record to answer a PTR query. Disclaimer: I am not a
>     DNS expert.
> 
> 
>     > From my understanding, when you open a
>     > browser and browse to www.google.com <http://www.google.com>, the very
>     > first thing that happens is you do a DNS resolution so that you know
>     > what IP to send the CONNECT request and subsequent HTTPS records
>     in the
>     > first place.
> 
>     What happens depends on the browser and the proxy port:
> 
>     1. For forward proxies: Some browsers will not do DNS lookups. They will
>     send a CONNECT request to example.com <http://example.com>, allowing
>     Squid to do the DNS
>     lookup. In this case, Squid dstdomain configured with a host name will
>     work well.
> 
>     2. For forward proxies: Some browsers do DNS lookups. They will send a
>     CONNECT request to one of the returned IP addresses.
> 
>     3. For interception proxies: All browsers do DNS lookups. They open a
>     TCP connection to one of the returned IP addresses.
> 
>     In cases 2 and 3, Squid dstdomain will have to do a reverse DNS lookup
>     (PTR query). In many cases, that lookup either fails or returns a
>     different domain name than the domain the browser started with.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > So we would have the IP already, and the hostname that was
>     > looked up already in the DNS cache, right? Why wouldn't squid just be
>     > able to reach in there, match the IP that DNS returned, and then pull
>     > that hostname out to compare against the ACLs?
>     >
>     > On Thu, Feb 25, 2021 at 2:57 PM Alex Rousskov
>     > <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>     > <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>     >
>     >? ? ?On 2/25/21 2:07 PM, Justin Michael Schwartzbeck wrote:
>     >
>     >? ? ?> I have thus far used dstdomain acl for bypassing ssl bump on
>     sites
>     >? ? ?that
>     >? ? ?> we don't want to decrypt, like banking sites. It seems to
>     work for
>     >? ? ?some
>     >? ? ?> sites, but not for others.
>     >
>     >? ? ?Yes, many HTTPS transactions do not expose destination domain
>     until it
>     >? ? ?is too late to decide whether to bump them, and reverse DNS
>     lookups are
>     >? ? ?often unreliable.
>     >
>     >
>     >? ? ?> I was thinking about this, and it seems to me that if we are
>     using the
>     >? ? ?> squid proxy with a dns server, we should be able to check
>     the dns
>     >? ? ?cache
>     >? ? ?> for that IP, and find the associated hostname, and then match
>     >? ? ?against that.
>     >
>     >? ? ?When you use dstdomain, Squid will do a (reverse) DNS query
>     for you as
>     >? ? ?necessary (including DNS cache lookups) unless you specify a
>     -n option
>     >? ? ?that is documented to disable all such operations.
>     >
>     >
>     >? ? ?In many cases, you should be using ssl::server_name instead of
>     dstdomain
>     >? ? ?or dst ACL, but you may have to use a combination of various
>     ACLs to
>     >? ? ?cover all the cases you care about.
>     >
>     >
>     >? ? ?HTH,
>     >
>     >? ? ?Alex.
>     >
> 



From service.mv at gmail.com  Fri Feb 26 19:58:32 2021
From: service.mv at gmail.com (Service MV)
Date: Fri, 26 Feb 2021 16:58:32 -0300
Subject: [squid-users] Exclude transaction from being logged
In-Reply-To: <4f7fadbe-1a53-3d06-0d29-c8ec237a4da9@measurement-factory.com>
References: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>
 <4f7fadbe-1a53-3d06-0d29-c8ec237a4da9@measurement-factory.com>
Message-ID: <CA+d==oG7k5+LG-LrkOAgV8fPusqMaJPXGsq8n8hX7ZVMeRu-CQ@mail.gmail.com>

Ok, but whit this two lines disabled completely access logs. I've no more
access_log directives configured.

I need disabled only zabbix_proxy IP logs.

Thanks in advance

Gabriel

El vie., 26 feb. 2021 16:21, Alex Rousskov <rousskov at measurement-factory.com>
escribi?:

> On 2/26/21 12:36 PM, Service MV wrote:
>
> > NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
> >
> > I know that this is not an error. But I want to exclude this log when
> > the Zabbix server checks the port status of SQUID. Zabbix server does
> > not use the SQUID as a proxy.
>
> If you do not want to see what Zabbix is doing to your Squid, you can
> exclude its requests from access.log. For example, if you do not want to
> see any requests from Zabbix IP address:
>
>   acl zabbix_proxy src x.x.x.x
>   access_log none zabbix_proxy
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210226/81b6d311/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 26 20:14:26 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Feb 2021 15:14:26 -0500
Subject: [squid-users] Exclude transaction from being logged
In-Reply-To: <CA+d==oG7k5+LG-LrkOAgV8fPusqMaJPXGsq8n8hX7ZVMeRu-CQ@mail.gmail.com>
References: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>
 <4f7fadbe-1a53-3d06-0d29-c8ec237a4da9@measurement-factory.com>
 <CA+d==oG7k5+LG-LrkOAgV8fPusqMaJPXGsq8n8hX7ZVMeRu-CQ@mail.gmail.com>
Message-ID: <05ca2db3-d2a3-ece6-88dc-8e5946decbba@measurement-factory.com>

On 2/26/21 2:58 PM, Service MV wrote:
> Ok, but whit this two lines disabled completely access logs. I've no
> more access_log directives configured.

Yeah, this outcome is surprising to many admins (and there are also
related bugs in Squid code).

To see all other records, add access_log lines that define how you want
the rest of the records to be logged. For example, if you want Squid to
use the default record format and other defaults, then try this untested
sketch:

   acl zabbix_proxy src x.x.x.x
   access_log none zabbix_proxy
   access_log daemon:/specify/exact/log/file/location/here

You can see the default access_log configuration specific to your Squid
build in squid.conf.documented.


HTH,

Alex.



> El vie., 26 feb. 2021 16:21, Alex Rousskov escribi?:
> 
>     On 2/26/21 12:36 PM, Service MV wrote:
> 
>     > NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/- -
>     >
>     > I know that this is not an error. But I want to exclude this log when
>     > the Zabbix server checks the port status of SQUID. Zabbix server does
>     > not use the SQUID as a proxy.
> 
>     If you do not want to see what Zabbix is doing to your Squid, you can
>     exclude its requests from access.log. For example, if you do not want to
>     see any requests from Zabbix IP address:
> 
>     ? acl zabbix_proxy src x.x.x.x
>     ? access_log none zabbix_proxy
> 
> 
>     HTH,
> 
>     Alex.
> 



From david at articatech.com  Sun Feb 28 00:22:56 2021
From: david at articatech.com (David Touzeau)
Date: Sun, 28 Feb 2021 01:22:56 +0100
Subject: [squid-users] Squid 4.14 : no_suid: setuid(0): (1) Operation not
 permitted
Message-ID: <3fc57beb-a81d-9564-b614-38dba07203c8@articatech.com>


Hi, regulary i have this error :

2021/02/28 01:18:43 kid1| helperOpenServers: Starting 5/32 
'security_file_certgen' processes
2021/02/28 01:18:43 kid1| WARNING: no_suid: setuid(0): (1) Operation not 
permitted

i have set the setuid permission

chown root:squid security_file_certgen
chmod 04755 security_file_certgen

or
chown squid:squid security_file_certgen
chmod 0755 security_file_certgen

in both cases, squid always claim with "the no_suid: setuid(0): (1) 
Operation not permitted"

How can i fix it ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210228/1832134e/attachment.htm>

From rousskov at measurement-factory.com  Sun Feb 28 00:58:18 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Feb 2021 19:58:18 -0500
Subject: [squid-users] Squid 4.14 : no_suid: setuid(0): (1) Operation
 not permitted
In-Reply-To: <3fc57beb-a81d-9564-b614-38dba07203c8@articatech.com>
References: <3fc57beb-a81d-9564-b614-38dba07203c8@articatech.com>
Message-ID: <b8b5c70a-93e1-475d-8b2c-c92b8bfc49f2@measurement-factory.com>

On 2/27/21 7:22 PM, David Touzeau wrote:

> Hi, regulary i have this error :
> 
> 2021/02/28 01:18:43 kid1| helperOpenServers: Starting 5/32
> 'security_file_certgen' processes
> 2021/02/28 01:18:43 kid1| WARNING: no_suid: setuid(0): (1) Operation not
> permitted
> 
> i have set the setuid permission
> 
> chown root:squid security_file_certgen
> chmod 04755 security_file_certgen
> 
> or
> chown squid:squid security_file_certgen
> chmod 0755 security_file_certgen
> 
> in both cases, squid always claim with "the no_suid: setuid(0): (1)
> Operation not permitted"

Sounds like bug 3785: https://bugs.squid-cache.org/show_bug.cgi?id=3785
That bug was filed many years ago and for a different helper/OS, but I
suspect it applies to your situation as well.


> How can i fix it ?

Unfortunately, I do not know the answer to that question. If it is
indeed bug 3785, then its current status is reflected by comment #5 at
https://bugs.squid-cache.org/show_bug.cgi?id=3785#c5


HTH,

Alex.


From squid3 at treenet.co.nz  Sun Feb 28 08:21:35 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 28 Feb 2021 21:21:35 +1300
Subject: [squid-users] help to redirect http request to another squid
 proxy
In-Reply-To: <CADZCxsvGj9FswUe4bMWz_QV_DtfiPwn7KV02z2fGozsKpYEZ4g@mail.gmail.com>
References: <CADZCxsvGj9FswUe4bMWz_QV_DtfiPwn7KV02z2fGozsKpYEZ4g@mail.gmail.com>
Message-ID: <4beddfc3-9ed0-e3af-e673-a3f165c8bceb@treenet.co.nz>

On 26/02/21 8:47 pm, jmpatagonia wrote:
> Hello I need help to redirect request http/https from a specific domain 
> to another squid proxy server.
> 
> Like a domain for example microsoft.com <http://microsoft.com> redirect 
> o transfer all request to another squid proxy server.
> 

Firstly, "redirect" has a meaning in HTTP and it has nothing to do with 
what you seem to be wanting to do.

You are using cache_peer, which is the directive you need to be looking 
at for the "HTTP routing" that meets your need.

However, there are some things that look odd in the details you 
provided. I suspect they are related to other things not mentioned that 
you have dome to the system setup thinking "redirect" was needed.



> I try to use this:
> #################################################
> http_port? xx.xx.xx..xx:8080 accel


This is a reverse-proxy. The CONNECT method is not meant to be sent to 
reverse-proxies.


> acl microsoft_acl dstdomain microsoft.com <http://microsoft.com>
> cache_peer yy.yy.yy.yy? parent 8080 ?0 ?name=proxy60 default

"default" indicates the peer is able to handle any traffic and can be 
used as a backup route when DIRECT fails.

You should not use that option on a peer where only certain site/domain 
are serviced.


> cache_peer_access proxy60 allow? microsoft_acl
> cache_peer_access proxy60 deny all
> #################################################
> but not wok, error :
> 26/Feb/2021:07:23:27 -0300 || - || xx.xx.xx.xx || TAG_NONE/405|| CONNECT 
> || error:method-not-allowed || text/html
> 

What does your logformat define all those log fields to mean ?
   There are 4 IP addresses involved in a proxy transaction and it is 
unclear why the xx.xx.xx.xx is the one being logged.


Why are you using port 8080?
   Last I saw Microsoft do not host their website using port 8080.

Hint: for "just an example name" use the domains which are registered 
specially for that purpose: example.com, example.net, example.org. It 
avoids confusing us into thinking microsoft.com is *actually* the domain 
you are hosting - there are implications to hosting their site which 
change the answers you could get.



Amos


From david at articatech.com  Sun Feb 28 14:50:39 2021
From: david at articatech.com (David Touzeau)
Date: Sun, 28 Feb 2021 15:50:39 +0100
Subject: [squid-users] Squid 4.14 : no_suid: setuid(0): (1) Operation
 not permitted
In-Reply-To: <b8b5c70a-93e1-475d-8b2c-c92b8bfc49f2@measurement-factory.com>
References: <3fc57beb-a81d-9564-b614-38dba07203c8@articatech.com>
 <b8b5c70a-93e1-475d-8b2c-c92b8bfc49f2@measurement-factory.com>
Message-ID: <7877e9c7-439d-bb66-ff6b-14df52b1c2fa@articatech.com>

Thanks Alex

This bug is a really "fog"? while i'm using Debian 10.x

https://superuser.com/questions/731104/squid-proxy-cache-server-no-suid-setuid0-1-operation-not-permitted
https://forum.netgate.com/topic/67220/squid3-dev-transparente-con-clamav-64-bit-1a-prueba/2

Your answers since several years:
http://www.squid-cache.org/mail-archive/squid-users/201301/0399.html
https://www.mail-archive.com/search?l=squid-users at squid-cache.org&q=subject:"\[squid\-users\]+Warning+in+cache.log"&o=newest&f=1

My last discuss on squid 4.13
https://www.spinics.net/lists/squid/msg93659.html


Many users says there is no impact on helpers and performance as it is 
just a warning...

Did you confirm it ?


Le 28/02/2021 ? 01:58, Alex Rousskov a ?crit?:
> On 2/27/21 7:22 PM, David Touzeau wrote:
>
>> Hi, regulary i have this error :
>>
>> 2021/02/28 01:18:43 kid1| helperOpenServers: Starting 5/32
>> 'security_file_certgen' processes
>> 2021/02/28 01:18:43 kid1| WARNING: no_suid: setuid(0): (1) Operation not
>> permitted
>>
>> i have set the setuid permission
>>
>> chown root:squid security_file_certgen
>> chmod 04755 security_file_certgen
>>
>> or
>> chown squid:squid security_file_certgen
>> chmod 0755 security_file_certgen
>>
>> in both cases, squid always claim with "the no_suid: setuid(0): (1)
>> Operation not permitted"
> Sounds like bug 3785: https://bugs.squid-cache.org/show_bug.cgi?id=3785
> That bug was filed many years ago and for a different helper/OS, but I
> suspect it applies to your situation as well.
>
>
>> How can i fix it ?
> Unfortunately, I do not know the answer to that question. If it is
> indeed bug 3785, then its current status is reflected by comment #5 at
> https://bugs.squid-cache.org/show_bug.cgi?id=3785#c5
>
>
> HTH,
>
> Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210228/48338f1a/attachment.htm>

From rousskov at measurement-factory.com  Sun Feb 28 23:41:30 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 28 Feb 2021 18:41:30 -0500
Subject: [squid-users] Squid 4.14 : no_suid: setuid(0): (1) Operation
 not permitted
In-Reply-To: <7877e9c7-439d-bb66-ff6b-14df52b1c2fa@articatech.com>
References: <3fc57beb-a81d-9564-b614-38dba07203c8@articatech.com>
 <b8b5c70a-93e1-475d-8b2c-c92b8bfc49f2@measurement-factory.com>
 <7877e9c7-439d-bb66-ff6b-14df52b1c2fa@articatech.com>
Message-ID: <d650c735-0b40-537e-8ba7-5cf4f98f1741@measurement-factory.com>

On 2/28/21 9:50 AM, David Touzeau wrote:
> Thanks Alex
> 
> This bug is a really "fog"? while i'm using Debian 10.x
> 
> https://superuser.com/questions/731104/squid-proxy-cache-server-no-suid-setuid0-1-operation-not-permitted
> https://forum.netgate.com/topic/67220/squid3-dev-transparente-con-clamav-64-bit-1a-prueba/2
> 
> Your answers since several years:
> http://www.squid-cache.org/mail-archive/squid-users/201301/0399.html
> https://www.mail-archive.com/search?l=squid-users at squid-cache.org&q=subject:"\[squid\-users\]+Warning+in+cache.log"&o=newest&f=1
> 
> My last discuss on squid 4.13
> https://www.spinics.net/lists/squid/msg93659.html

Thank you for locating these emails. I added some of them to the bug report.


> Many users says there is no impact on helpers and performance as it is
> just a warning...
> 
> Did you confirm it ?

The problem is considered "confirmed" from Bugzilla point of view. I am
not aware of any volunteers addressing the suspected FreeBSD-specific
"warnings due to double drop" problems or triaging non-FreeBSD warnings.

Alex.



> Le 28/02/2021 ? 01:58, Alex Rousskov a ?crit?:
>> On 2/27/21 7:22 PM, David Touzeau wrote:
>>
>>> Hi, regulary i have this error :
>>>
>>> 2021/02/28 01:18:43 kid1| helperOpenServers: Starting 5/32
>>> 'security_file_certgen' processes
>>> 2021/02/28 01:18:43 kid1| WARNING: no_suid: setuid(0): (1) Operation not
>>> permitted
>>>
>>> i have set the setuid permission
>>>
>>> chown root:squid security_file_certgen
>>> chmod 04755 security_file_certgen
>>>
>>> or
>>> chown squid:squid security_file_certgen
>>> chmod 0755 security_file_certgen
>>>
>>> in both cases, squid always claim with "the no_suid: setuid(0): (1)
>>> Operation not permitted"
>> Sounds like bug 3785: https://bugs.squid-cache.org/show_bug.cgi?id=3785
>> That bug was filed many years ago and for a different helper/OS, but I
>> suspect it applies to your situation as well.
>>
>>
>>> How can i fix it ?
>> Unfortunately, I do not know the answer to that question. If it is
>> indeed bug 3785, then its current status is reflected by comment #5 at
>> https://bugs.squid-cache.org/show_bug.cgi?id=3785#c5
>>
>>
>> HTH,
>>
>> Alex.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



