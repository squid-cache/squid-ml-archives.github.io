From squid3 at treenet.co.nz  Fri Jan  1 03:58:06 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Jan 2021 16:58:06 +1300
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <202012311014.45914.Antony.Stone@squid.open.source.it>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <202012311014.45914.Antony.Stone@squid.open.source.it>
Message-ID: <4f0b1bca-7190-2c56-d25b-ef63ba5e489c@treenet.co.nz>

On 31/12/20 10:14 pm, Antony Stone wrote:
> On Thursday 31 December 2020 at 10:10:11, jean francois hasson wrote:
> 
>> If I set up on a device connected to the access point a proxy manually
>> ie 10.3.141.1 on port 8080, I can access the internet. If I put the
>> following rules for iptables to use in files rules.v4 :
>>
>> *nat
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination
>> 10.3.141.1:3128
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination
>> 10.3.141.1:3129
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129
>> -A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
> 
> Try removing the DNAT rules above.  You should be using REDIRECT for intercept
> mode to work correctly.
> 

Also missing half the iptables rules needed. See the official How-To 
documentation at 
<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>


Amos


From jfhasson at club-internet.fr  Sat Jan  2 10:26:03 2021
From: jfhasson at club-internet.fr (jean francois hasson)
Date: Sat, 2 Jan 2021 11:26:03 +0100
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <202012311014.45914.Antony.Stone@squid.open.source.it>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <202012311014.45914.Antony.Stone@squid.open.source.it>
Message-ID: <da84475b-c08e-d377-78ae-c80d116bbc19@club-internet.fr>

Hi,

Thank you Amos Jeffries and Antony Stone. It seems the configuration I 
have provides the functionality of filtering I am looking for.

There is a strange behavior I can see when accessing some legitimate 
sites which I see traces of in cache.log :

    2021/01/02 10:55:48 kid1| helperOpenServers: Starting 1/20
    'squidGuard' processes
    2021/01/02 10:57:31 kid1| ERROR: negotiating TLS on FD 39:
    error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
    inappropriate fallback (1/-1/0)
    2021/01/02 10:57:31 kid1| Error negotiating SSL connection on FD 38:
    error:00000001:lib(0):func(0):reason(1) (1/-1)
    2021/01/02 10:57:32 kid1| ERROR: negotiating TLS on FD 38:
    error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
    inappropriate fallback (1/-1/0)
    2021/01/02 10:57:32 kid1| Error negotiating SSL connection on FD 35:
    error:00000001:lib(0):func(0):reason(1) (1/-1)
    2021/01/02 10:57:40 kid1| Starting new redirector helpers...
    2021/01/02 10:57:40 kid1| helperOpenServers: Starting 1/20
    'squidGuard' processes
    2021/01/02 10:58:09 kid1| ERROR: negotiating TLS on FD 51:
    error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
    inappropriate fallback (1/-1/0)
    2021/01/02 10:58:09 kid1| Error negotiating SSL connection on FD 40:
    error:00000001:lib(0):func(0):reason(1) (1/-1)
    2021/01/02 10:58:10 kid1| ERROR: negotiating TLS on FD 51:
    error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
    inappropriate fallback (1/-1/0)
    2021/01/02 10:58:10 kid1| Error negotiating SSL connection on FD 40:
    error:00000001:lib(0):func(0):reason(1) (1/-1)

I noticed other users of squid encountered similar issues but I did not 
find a clear answer to the issue. Is there a problem with my setup ? I 
am not sure to be able to solve it on my own ! Any help would be 
appreciated.

Best regards,

JF Hasson

Le 31/12/2020 ? 10:14, Antony Stone a ?crit?:
> On Thursday 31 December 2020 at 10:10:11, jean francois hasson wrote:
>
>> If I set up on a device connected to the access point a proxy manually
>> ie 10.3.141.1 on port 8080, I can access the internet. If I put the
>> following rules for iptables to use in files rules.v4 :
>>
>> *nat
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination
>> 10.3.141.1:3128
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination
>> 10.3.141.1:3129
>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129
>> -A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
> Try removing the DNAT rules above.  You should be using REDIRECT for intercept
> mode to work correctly.
>
>
> Antony.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210102/8d8fe785/attachment.htm>

From ngtech1ltd at gmail.com  Sat Jan  2 16:23:35 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sat, 2 Jan 2021 18:23:35 +0200
Subject: [squid-users] Anyone has experience with Windows clients DNS
 timeout
In-Reply-To: <a0fa06ea-040d-0993-12a8-1c402176e623@treenet.co.nz>
References: <CABA8h=SW8SSgMa0Xyks=aJfw2k3B2kZ_mWBgtJ75b5v1fc+M6Q@mail.gmail.com>
 <a0fa06ea-040d-0993-12a8-1c402176e623@treenet.co.nz>
Message-ID: <316601d6e123$9f46f1a0$ddd4d4e0$@gmail.com>

Hey Amos,

For an INTERCEPT setup we still need to resolve before squid is touching the packets.
There are registry keys for this purpose however we first need to identify this issue.
The basic way to verify this is using the "set debug" on nslookup and use a fully "cold" DNS recurser.

I was thinking about writing some PowerShell script that will do that but for now it's not really important.
More important then that is a good sysadmin.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon




-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, December 30, 2020 6:15 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Anyone has experience with Windows clients DNS timeout

On 30/12/20 9:02 am, NgTech LTD wrote:
> I have seen this issue on Windows clients over the past.
> Windows nslookup shows that the query has timed out after 2 seconds.
> On Linux and xBSD I have researched this issue and have seen that:
> the DNS server is doing a recursive lookup and it takes from 7 to 10++
> seconds sometimes.
> When I pre-warn the DNS cache and the results are cached it takes
> lower then 500 ms for a response to be on the client side and then
> everything works fine.
> 
> I understand that Windows DNS client times out..
> When using froward proxy with squid or any other it works as expected
> since the DNS resolution is done on the proxy server.
> However for this issue I believe that this timeout should be increased
> instead of moving to DNS over HTTPS.


The DNS timeout in Squid is 30sec for exactly this type of reason. 2 
seconds is far too short to *guarantee* a recursive resolver is able to 
perform all the work and many round-trip lookups that are needed.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Sat Jan  2 20:08:55 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sat, 2 Jan 2021 22:08:55 +0200
Subject: [squid-users] SSL-BUMP 5.0.4 not working as expected
Message-ID: <000001d6e143$19bddce0$4d3996a0$@gmail.com>

I am trying to configure 5.0.4 with sslbump to bump only a set of domains.

I am unsure about the right way it should be done.

The basic constrains are POLICY vs a set of rules.

*	Should I bump all connections with exceptions? 
*	Should I bump non else then the exceptions?
*	Based on server_name regex and/or server_name domains

 

 

Squid Cache: Version 5.0.4-20201125-r5fadc09ee

Service Name: squid

 

This binary uses OpenSSL 1.1.1g FIPS  21 Apr 2020. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

 

configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--exec_prefix=/usr'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--disable-dependency-tracking' '--enable-follow-x-forwarded-for'
'--enable-auth'
'--enable-auth-basic=DB,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake'
'--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,d
elayer,file_userip,SQL_session,unix_group,session,time_quota'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-icap-client'
'--enable-ident-lookups' '--enable-linux-netfilter'
'--enable-removal-policies=heap,lru' '--enable-snmp'
'--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi'
'--enable-security-cert-generators' '--enable-security-cert-validators'
'--enable-icmp' '--with-aio' '--with-default-user=squid'
'--with-filedescriptors=16384' '--with-dl' '--with-openssl'
'--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl'
'--disable-arch-native' '--without-nettle'
'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu'
'CC=gcc' 'CFLAGS=-O2  -fexceptions -g -grecord-gcc-switches -pipe -Wall
-Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS
-specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong
-specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -mtune=generic
-fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection'
'LDFLAGS=-Wl,-z,relro -Wl,--as-needed  -Wl,-z,now
-specs=/usr/lib/rpm/redhat/redhat-hardened-ld ' 'CXX=g++' 'CXXFLAGS=-O2
-fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security
-Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS
-specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong
-specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -mtune=generic
-fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fPIC'
'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
'LT_SYS_LIBRARY_PATH=/usr/lib64:' --enable-ltdl-convenience

 

 

I have tried the next set of rules:

## START

acl step1 at_step SslBump1

acl step2 at_step SslBump2

acl step3 at_step SslBump3

 

acl NoBump_server_regex ssl::server_name_regex -i
/etc/squid/server-regex.nobump

acl NoBump_server_name ssl::server_name /etc/squid/server-name.nobump

 

acl NoBump_ALL_regex ssl::server_name_regex -i
/etc/squid/all_server-regex.nobump

 

acl MustBump_server_regex ssl::server_name_regex -i
/etc/squid/must_server-regex.bump

acl MustBump_server_name ssl::server_name /etc/squid/must_server-name.bump

 

 

ssl_bump peek step1

 

ssl_bump splice NoBump_server_regex

ssl_bump splice NoBump_server_name

 

ssl_bump bump MustBump_server_regex

ssl_bump bump MustBump_server_name

 

ssl_bump splice NoBump_ALL_regex

 

ssl_bump bump all

##END

 

 

 

But the BoBump are not applied.

I tried to understand why squid is bumping despite the explicit splice
action.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210102/5396e3e8/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan  3 07:12:02 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 3 Jan 2021 20:12:02 +1300
Subject: [squid-users] SSL-BUMP 5.0.4 not working as expected
In-Reply-To: <000001d6e143$19bddce0$4d3996a0$@gmail.com>
References: <000001d6e143$19bddce0$4d3996a0$@gmail.com>
Message-ID: <f2f33c39-7c40-b8f8-7fa2-3362e7430ac6@treenet.co.nz>

On 3/01/21 9:08 am, ngtech1ltd wrote:
> I am trying to configure 5.0.4 with sslbump to bump only a set of domains.
> 
> I am unsure about the right way it should be done.
> 
> The basic constrains are POLICY vs a set of rules.
> 
>   * Should I bump all connections with exceptions?
>   * Should I bump non else then the exceptions?
>   * Based on server_name regex and/or server_name domains
>

In regards to policy:

Security best-practice is to reject as early as possible. So for 
transactions that early bump steps are indicating going to forbidden 
places should reject immediately on that detection.

For transactions which appear to be not-bad, there is no "best" way. 
That depends on your specific setup needs and the side-effects of making 
a wrong deision.

I prefer to advise bump'ing at step 3 where the most information is 
available for checks and correction of client claims.


...
> I have tried the next set of rules:
> 
> ## START
> 
> acl step1 at_step SslBump1
> 
> acl step2 at_step SslBump2
> 
> acl step3 at_step SslBump3
> 
> acl NoBump_server_regex ssl::server_name_regex -i 
> /etc/squid/server-regex.nobump
> 
> acl NoBump_server_name ssl::server_name /etc/squid/server-name.nobump
> 
> acl NoBump_ALL_regex ssl::server_name_regex -i 
> /etc/squid/all_server-regex.nobump
> 
> acl MustBump_server_regex ssl::server_name_regex -i 
> /etc/squid/must_server-regex.bump
> 
> acl MustBump_server_name ssl::server_name /etc/squid/must_server-name.bump
> 
> ssl_bump peek step1
> 
> ssl_bump splice NoBump_server_regex
> 
> ssl_bump splice NoBump_server_name
> 
> ssl_bump bump MustBump_server_regex
> 
> ssl_bump bump MustBump_server_name
> 
> ssl_bump splice NoBump_ALL_regex
> 
> ssl_bump bump all
> 
> ##END
> 
> But the BoBump are not applied.
> 
> I tried to understand why squid is bumping despite the explicit splice 
> action.

Note that all these splice/bump rules are being applied at step2. There 
is no step3 taking place.


Does your actual config have the required "" marks around those filenames?

Without that all your ACLs will non-match (SNI vs name of the file) and 
the last "bump all" will be applied below.


Amos


From ngtech1ltd at gmail.com  Sun Jan  3 10:49:51 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sun, 3 Jan 2021 12:49:51 +0200
Subject: [squid-users] SSL-BUMP 5.0.4 not working as expected
Message-ID: <000401d6e1be$2a6ab380$7f401a80$@gmail.com>

Hey Amos,

I forgot about the "".
I am attaching /etc/squid/ and inside a txt log dump from cache.log of the minute which 2 or more transactions happening.

I think I'm doing something wrong in the config but not 100% sure.

Link to config and output:
https://1drv.ms/u/s!AoiLG1Jyh7JqqEmrmgzPM5dRFUvK?e=adVJOe

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Sunday, January 3, 2021 9:12 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SSL-BUMP 5.0.4 not working as expected

On 3/01/21 9:08 am, ngtech1ltd wrote:
> I am trying to configure 5.0.4 with sslbump to bump only a set of domains.
> 
> I am unsure about the right way it should be done.
> 
> The basic constrains are POLICY vs a set of rules.
> 
>   * Should I bump all connections with exceptions?
>   * Should I bump non else then the exceptions?
>   * Based on server_name regex and/or server_name domains
>

In regards to policy:

Security best-practice is to reject as early as possible. So for 
transactions that early bump steps are indicating going to forbidden 
places should reject immediately on that detection.

For transactions which appear to be not-bad, there is no "best" way. 
That depends on your specific setup needs and the side-effects of making 
a wrong deision.

I prefer to advise bump'ing at step 3 where the most information is 
available for checks and correction of client claims.


...
> I have tried the next set of rules:
> 
> ## START
> 
> acl step1 at_step SslBump1
> 
> acl step2 at_step SslBump2
> 
> acl step3 at_step SslBump3
> 
> acl NoBump_server_regex ssl::server_name_regex -i 
> /etc/squid/server-regex.nobump
> 
> acl NoBump_server_name ssl::server_name /etc/squid/server-name.nobump
> 
> acl NoBump_ALL_regex ssl::server_name_regex -i 
> /etc/squid/all_server-regex.nobump
> 
> acl MustBump_server_regex ssl::server_name_regex -i 
> /etc/squid/must_server-regex.bump
> 
> acl MustBump_server_name ssl::server_name /etc/squid/must_server-name.bump
> 
> ssl_bump peek step1
> 
> ssl_bump splice NoBump_server_regex
> 
> ssl_bump splice NoBump_server_name
> 
> ssl_bump bump MustBump_server_regex
> 
> ssl_bump bump MustBump_server_name
> 
> ssl_bump splice NoBump_ALL_regex
> 
> ssl_bump bump all
> 
> ##END
> 
> But the BoBump are not applied.
> 
> I tried to understand why squid is bumping despite the explicit splice 
> action.

Note that all these splice/bump rules are being applied at step2. There 
is no step3 taking place.


Does your actual config have the required "" marks around those filenames?

Without that all your ACLs will non-match (SNI vs name of the file) and 
the last "bump all" will be applied below.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Sun Jan  3 12:19:32 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sun, 3 Jan 2021 14:19:32 +0200
Subject: [squid-users] SSL-BUMP 5.0.4 not working as expected
In-Reply-To: <f2f33c39-7c40-b8f8-7fa2-3362e7430ac6@treenet.co.nz>
References: <000001d6e143$19bddce0$4d3996a0$@gmail.com>
 <f2f33c39-7c40-b8f8-7fa2-3362e7430ac6@treenet.co.nz>
Message-ID: <000501d6e1ca$b1c9bcc0$155d3640$@gmail.com>

Comments bellow

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Sunday, January 3, 2021 9:12 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SSL-BUMP 5.0.4 not working as expected

On 3/01/21 9:08 am, ngtech1ltd wrote:
> I am trying to configure 5.0.4 with sslbump to bump only a set of domains.
> 
> I am unsure about the right way it should be done.
> 
> The basic constrains are POLICY vs a set of rules.
> 
>   * Should I bump all connections with exceptions?
>   * Should I bump non else then the exceptions?
>   * Based on server_name regex and/or server_name domains
>

In regards to policy:

Security best-practice is to reject as early as possible. So for 
transactions that early bump steps are indicating going to forbidden 
places should reject immediately on that detection.

For transactions which appear to be not-bad, there is no "best" way. 
That depends on your specific setup needs and the side-effects of making 
a wrong deision.

I prefer to advise bump'ing at step 3 where the most information is 
available for checks and correction of client claims.


# How to do that? I tried to read the docs at:
https://wiki.squid-cache.org/Features/SslPeekAndSplice

But couldn't understand or grasp how to implement what you are talking about.
#

...
> I have tried the next set of rules:
> 
> ## START
> 
> acl step1 at_step SslBump1
> 
> acl step2 at_step SslBump2
> 
> acl step3 at_step SslBump3
> 
> acl NoBump_server_regex ssl::server_name_regex -i 
> /etc/squid/server-regex.nobump
> 
> acl NoBump_server_name ssl::server_name /etc/squid/server-name.nobump
> 
> acl NoBump_ALL_regex ssl::server_name_regex -i 
> /etc/squid/all_server-regex.nobump
> 
> acl MustBump_server_regex ssl::server_name_regex -i 
> /etc/squid/must_server-regex.bump
> 
> acl MustBump_server_name ssl::server_name /etc/squid/must_server-name.bump
> 
> ssl_bump peek step1
> 
> ssl_bump splice NoBump_server_regex
> 
> ssl_bump splice NoBump_server_name
> 
> ssl_bump bump MustBump_server_regex
> 
> ssl_bump bump MustBump_server_name
> 
> ssl_bump splice NoBump_ALL_regex
> 
> ssl_bump bump all
> 
> ##END
> 
> But the BoBump are not applied.
> 
> I tried to understand why squid is bumping despite the explicit splice 
> action.

Note that all these splice/bump rules are being applied at step2. There 
is no step3 taking place.


Does your actual config have the required "" marks around those filenames?

Without that all your ACLs will non-match (SNI vs name of the file) and 
the last "bump all" will be applied below.

# I didn't understood how to separate the different steps and to make the right config which will either allow me bump or splice.
I want to be able to bump or splice by my acls and I couldn't make this happen.
Either I'm really confused or didn't understood how to do that.
With another software I was able to do that and more and this is why it's probably so hard for me.

Thanks,
Eliezer


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Sun Jan  3 14:12:24 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sun, 3 Jan 2021 16:12:24 +0200
Subject: [squid-users] PCI Certification compliance lists
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>

I am looking for domains lists that can be used for squid to be PCI
Certified.

I have read this article:
https://www.imperva.com/learn/data-security/pci-dss-certification/

And couple others to try and understand what might a Squid proxy ssl-bump
exception rules should contain.
So technically we need:
- Banks
- Health care
- Credit Cards(Visa, Mastercard, others)
- Payments sites
- Antivirus(updates and portals)
- OS and software Updates signatures(ASC, MD5, SHAx etc..)

* https://support.kaspersky.com/common/start/6105
*
https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
set-product-with-a-third-party-firewall
*
https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9


If someone has the documents which instructs what domains to not inspect it
would also help a lot.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon





From squid3 at treenet.co.nz  Sun Jan  3 15:04:29 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Jan 2021 04:04:29 +1300
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
Message-ID: <0c89007d-a58f-b9a2-cea9-a592d77bfeb2@treenet.co.nz>

On 4/01/21 3:12 am, ngtech1ltd wrote:
> I am looking for domains lists that can be used for squid to be PCI
> Certified.
> 
> I have read this article:
> https://www.imperva.com/learn/data-security/pci-dss-certification/
> 
> And couple others to try and understand what might a Squid proxy ssl-bump
> exception rules should contain.
> So technically we need:
> - Banks
> - Health care
> - Credit Cards(Visa, Mastercard, others)
> - Payments sites
> - Antivirus(updates and portals)
> - OS and software Updates signatures(ASC, MD5, SHAx etc..)
> 
> * https://support.kaspersky.com/common/start/6105
> *
> https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
> set-product-with-a-third-party-firewall
> *
> https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
> 55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
> p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
> e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
> Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
> 525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
> D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
> 
> 
> If someone has the documents which instructs what domains to not inspect it
> would also help a lot.



Are you trying to get Squid certified as a PCI WAF agent?
  or as security infrastructure agent?
  or as general networking agent?

These roles matter in regards to the PCI requirement to detect malicious 
transactions.


Amos


From ngtech1ltd at gmail.com  Sun Jan  3 15:17:06 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Sun, 3 Jan 2021 17:17:06 +0200
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <0c89007d-a58f-b9a2-cea9-a592d77bfeb2@treenet.co.nz>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <0c89007d-a58f-b9a2-cea9-a592d77bfeb2@treenet.co.nz>
Message-ID: <CABA8h=QxhS_n=3LQECqYSgnUE0ZJeUDgxvF-s4NEfOeG6VgnOA@mail.gmail.com>

I'm trying to figure out what can be done with 5.0.4.
I believe there is either a bug or misunderstanding by me what and how
things should be done or configured.

The first thing is to be able to bump all and add exceptions.
The second would be to bump specific sites.
As i noticed in the past it seems that for a good splice and or bump I need
the any-of acl to be used.

Its a bit different then the way squid acls work in general.

Eliezer

On Sun, Jan 3, 2021, 17:06 Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 4/01/21 3:12 am, ngtech1ltd wrote:
> > I am looking for domains lists that can be used for squid to be PCI
> > Certified.
> >
> > I have read this article:
> > https://www.imperva.com/learn/data-security/pci-dss-certification/
> >
> > And couple others to try and understand what might a Squid proxy ssl-bump
> > exception rules should contain.
> > So technically we need:
> > - Banks
> > - Health care
> > - Credit Cards(Visa, Mastercard, others)
> > - Payments sites
> > - Antivirus(updates and portals)
> > - OS and software Updates signatures(ASC, MD5, SHAx etc..)
> >
> > * https://support.kaspersky.com/common/start/6105
> > *
> >
> https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
> > set-product-with-a-third-party-firewall
> > *
> >
> https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
> >
> 55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
> >
> p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
> >
> e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
> >
> Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
> >
> 525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
> > D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
> >
> >
> > If someone has the documents which instructs what domains to not inspect
> it
> > would also help a lot.
>
>
>
> Are you trying to get Squid certified as a PCI WAF agent?
>   or as security infrastructure agent?
>   or as general networking agent?
>
> These roles matter in regards to the PCI requirement to detect malicious
> transactions.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210103/26d3ccdf/attachment.htm>

From jfhasson at club-internet.fr  Sun Jan  3 18:15:00 2021
From: jfhasson at club-internet.fr (jean francois hasson)
Date: Sun, 3 Jan 2021 19:15:00 +0100
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <da84475b-c08e-d377-78ae-c80d116bbc19@club-internet.fr>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <202012311014.45914.Antony.Stone@squid.open.source.it>
 <da84475b-c08e-d377-78ae-c80d116bbc19@club-internet.fr>
Message-ID: <6568b326-4ba5-6e07-25a5-009c56416f4a@club-internet.fr>

Hi,

After reading more information on this kind of error I captured a few 
transactions with Wireshark running on the raspberry pi hosting squid 
4.6 and opensll 1.1.1d. I captured some transactions when trying to 
access ebay.fr which is currently not successful with the setup I have 
with the error of inappropriate fallback mentioned below.

I am not familiar with TLS transactions so I will try to present a high 
level view of the transactions between the raspberry pi and the ebay.fr 
server. I hope you can guide me as to what I should focus on to 
understand, if possible, the issue I have.

A bird's eye view of the transactions from Wireshark over time is :

 ???? 23 0.175795327??? 192.168.1.32 192.168.1.1?????????? DNS????? 
71???? Standard query 0x057e A www.ebay.fr
 ???? 24 0.214678299??? 192.168.1.1?????????? 192.168.1.32 DNS????? 
165??? Standard query response 0x057e A www.ebay.fr CNAME 
slot11847.ebay.com.edgekey.net CNAME e11847.g.akamaiedge.net A 23.57.6.166
 ???? 25 0.301067317??? 192.168.1.32????????? 23.57.6.166 TCP????? 
74???? 53934 ? 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 
TSval=365186690 TSecr=0 WS=128
 ???? 26 0.302488046??? 192.168.1.32????????? 23.57.6.166 TCP????? 
74???? 53936 ? 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 
TSval=365186691 TSecr=0 WS=128
 ???? 27 0.328959454??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
74???? 443 ? 53934 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 
SACK_PERM=1 TSval=3470404062 TSecr=365186690 WS=128
 ???? 28 0.329115340??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186718 
TSecr=3470404062
 ???? 29 0.329752684??? 192.168.1.32????????? 23.57.6.166 TLSv1.2? 
583??? Client Hello
 ???? 30 0.330530288??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
74???? 443 ? 53936 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 
SACK_PERM=1 TSval=3470404064 TSecr=365186691 WS=128
 ???? 31 0.330644819??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186719 
TSecr=3470404064
 ???? 32 0.331192579??? 192.168.1.32????????? 23.57.6.166 TLSv1.2? 
583??? Client Hello
 ???? 35 0.351054404??? 192.168.1.32????????? 192.168.1.98 TCP????? 
54???? 5900 ? 49903 [ACK] Seq=14256 Ack=97 Win=501 Len=0
 ???? 36 0.363323884??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
66???? 443 ? 53934 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404096 
TSecr=365186719
 ???? 37 0.364291801??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
1514?? Server Hello
 ???? 38 0.364347270??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 
TSval=365186753 TSecr=3470404096
 ???? 39 0.365482999??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1514?? 443 ? 53934 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 
TSval=3470404096 TSecr=365186719 [TCP segment of a reassembled PDU]
 ???? 40 0.365535030??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 
TSval=365186754 TSecr=3470404096
 ???? 41 0.366217999??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1266?? 443 ? 53934 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 
TSval=3470404096 TSecr=365186719 [TCP segment of a reassembled PDU]
 ???? 42 0.366279041??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 
TSval=365186755 TSecr=3470404096
 ???? 43 0.366321697??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
74???? [TCP Retransmission] 443 ? 53936 [SYN, ACK] Seq=0 Ack=1 Win=65160 
Len=0 MSS=1460 SACK_PERM=1 TSval=3470404096 TSecr=365186691 WS=128
 ???? 44 0.366410135??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? [TCP Dup ACK 31#1] 53936 ? 443 [ACK] Seq=518 Ack=1 Win=64256 
Len=0 TSval=365186755 TSecr=3470404064
 ???? 45 0.366709770??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
991??? Certificate, Certificate Status, Server Key Exchange, Server 
Hello Done
 ???? 46 0.366754978??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186756 TSecr=3470404097
 ???? 47 0.369138676??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
66???? 443 ? 53936 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404102 
TSecr=365186720
 ???? 48 0.370432739??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
1514?? Server Hello
 ???? 49 0.370506906??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 
TSval=365186759 TSecr=3470404102
 ???? 50 0.371401125??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1514?? 443 ? 53936 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 
TSval=3470404102 TSecr=365186720 [TCP segment of a reassembled PDU]
 ???? 51 0.371449250??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 
TSval=365186760 TSecr=3470404102
 ???? 52 0.372385968??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1266?? 443 ? 53936 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 
TSval=3470404102 TSecr=365186720 [TCP segment of a reassembled PDU]
 ???? 53 0.372438156??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 
TSval=365186761 TSecr=3470404102
 ???? 54 0.372859562??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
991??? Certificate, Certificate Status, Server Key Exchange, Server 
Hello Done
 ???? 55 0.372905395??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186762 TSecr=3470404103
 ???? 56 0.374064614??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [FIN, ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186763 TSecr=3470404097
 ???? 57 0.382856646??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [FIN, ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186772 TSecr=3470404103
 ???? 58 0.387044251??? 192.168.1.32????????? 23.57.6.166 TCP????? 
74???? 53938 ? 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 
TSval=365186776 TSecr=0 WS=128
 ???? 59 0.401877325??? 192.168.1.32????????? 23.57.6.166 TCP????? 
74???? 53940 ? 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 
TSval=365186791 TSecr=0 WS=128
 ???? 60 0.402472117??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
66???? 443 ? 53934 [FIN, ACK] Seq=5022 Ack=519 Win=64768 Len=0 
TSval=3470404136 TSecr=365186763
 ???? 61 0.402574981??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53934 ? 443 [ACK] Seq=519 Ack=5023 Win=64128 Len=0 
TSval=365186791 TSecr=3470404136
 ???? 62 0.410122326??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
66???? 443 ? 53936 [FIN, ACK] Seq=5022 Ack=519 Win=64768 Len=0 
TSval=3470404143 TSecr=365186772
 ???? 63 0.410185971??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53936 ? 443 [ACK] Seq=519 Ack=5023 Win=64128 Len=0 
TSval=365186799 TSecr=3470404143
 ???? 64 0.415533941??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
74???? 443 ? 53938 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 
SACK_PERM=1 TSval=3470404148 TSecr=365186776 WS=128
 ???? 65 0.415615607??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53938 ? 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186804 
TSecr=3470404148
 ???? 66 0.416199514??? 192.168.1.32????????? 23.57.6.166 TLSv1.2? 
583??? Client Hello
 ???? 67 0.429629098??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
74???? 443 ? 53940 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 
SACK_PERM=1 TSval=3470404163 TSecr=365186791 WS=128
 ???? 68 0.429722796??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53940 ? 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186819 
TSecr=3470404163
 ???? 69 0.430195036??? 192.168.1.32????????? 23.57.6.166 TLSv1.2? 
583??? Client Hello
 ???? 70 0.449937225??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
66???? 443 ? 53938 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404182 
TSecr=365186805
 ???? 71 0.451000037??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
1514?? Server Hello
 ???? 72 0.451064100??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53938 ? 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 
TSval=365186840 TSecr=3470404183
 ???? 73 0.451980194??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1514?? 443 ? 53938 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 
TSval=3470404183 TSecr=365186805 [TCP segment of a reassembled PDU]
 ???? 74 0.452031756??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53938 ? 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 
TSval=365186841 TSecr=3470404183
 ???? 75 0.452935767??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1266?? 443 ? 53938 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 
TSval=3470404183 TSecr=365186805 [TCP segment of a reassembled PDU]
 ???? 76 0.452991027??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53938 ? 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 
TSval=365186842 TSecr=3470404183
 ???? 77 0.453443475??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
991??? Certificate, Certificate Status, Server Key Exchange, Server 
Hello Done
 ???? 78 0.453498215??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53938 ? 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186842 TSecr=3470404184
 ???? 79 0.461625715??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53938 ? 443 [FIN, ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186850 TSecr=3470404184
 ???? 80 0.463463320??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
66???? 443 ? 53940 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404196 
TSecr=365186819
 ???? 81 0.464344413??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
1514?? Server Hello
 ???? 82 0.464433476??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53940 ? 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 
TSval=365186853 TSecr=3470404197
 ???? 83 0.465538632??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1514?? 443 ? 53940 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 
TSval=3470404197 TSecr=365186819 [TCP segment of a reassembled PDU]
 ???? 84 0.465628789??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53940 ? 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 
TSval=365186854 TSecr=3470404197
 ???? 85 0.466298945??? 23.57.6.166?????????? 192.168.1.32 TCP????? 
1266?? 443 ? 53940 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 
TSval=3470404197 TSecr=365186819 [TCP segment of a reassembled PDU]
 ???? 86 0.466437851??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53940 ? 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 
TSval=365186855 TSecr=3470404197
 ???? 87 0.467042591??? 23.57.6.166?????????? 192.168.1.32 TLSv1.2? 
991??? Certificate, Certificate Status, Server Key Exchange, Server 
Hello Done
 ???? 88 0.467190976??? 192.168.1.32????????? 23.57.6.166 TCP????? 
66???? 53940 ? 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 
TSval=365186856 TSecr=3470404197

I start my description with a Client Hello step from the raspberry pi to 
the ebay.fr server :

    No.???? Time?????????? Source Destination?????????? Protocol Length Info
     ???? 29 0.329752684??? 192.168.1.32 23.57.6.166?????????? TLSv1.2?
    583??? Client Hello

    ...

    Transport Layer Security
     ??? TLSv1.2 Record Layer: Handshake Protocol: Client Hello
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.0 (0x0301)
     ??????? Length: 512
     ??????? Handshake Protocol: Client Hello
     ??????????? Handshake Type: Client Hello (1)
     ??????????? Length: 508
     ??????????? Version: TLS 1.2 (0x0303)

Then, there is another Client Hello step which seems quite similar to 
the previous one :

    No.???? Time?????????? Source Destination?????????? Protocol Length Info
     ???? 32 0.331192579??? 192.168.1.32 23.57.6.166?????????? TLSv1.2?
    583??? Client Hello

    ...

    Transport Layer Security
     ??? TLSv1.2 Record Layer: Handshake Protocol: Client Hello
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.0 (0x0301)
     ??????? Length: 512
     ??????? Handshake Protocol: Client Hello
     ??????????? Handshake Type: Client Hello (1)
     ??????????? Length: 508
     ??????????? Version: TLS 1.2 (0x0303)

Then a Server Hello :

    No.???? Time?????????? Source Destination?????????? Protocol Length Info
     ???? 37 0.364291801??? 23.57.6.166 192.168.1.32????????? TLSv1.2?
    1514?? Server Hello

    ...

    Transport Layer Security
     ??? TLSv1.2 Record Layer: Handshake Protocol: Server Hello
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.2 (0x0303)
     ??????? Length: 78
     ??????? Handshake Protocol: Server Hello
     ??????????? Handshake Type: Server Hello (2)
     ??????????? Length: 74
     ??????????? Version: TLS 1.2 (0x0303)

     ??????????? Random:
    08f25b54bfe62d98736a4e5e8cc5a3f4ab97c040c1a892a26110e4d704b2fd9e
     ??????????????? GMT Unix Time: Oct? 4, 1974 08:40:20.000000000
    Paris, Madrid (heure d??t?)
     ??????????????? Random Bytes:
    bfe62d98736a4e5e8cc5a3f4ab97c040c1a892a26110e4d704b2fd9e
     ??????????? Session ID Length: 0
     ??????????? Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
    (0xc02f)

    ...

So it seems the server found a common cipher with the client. I am not 
sure then what to look for. Frames 43 and 44 are detected by Wireshark 
as retransmissions but I am not sure it is a problem.

I noticed frame 45 which is about the Certificate, Certificate Status, 
Server Key Exchange and Server Hello Done

    No.???? Time?????????? Source Destination?????????? Protocol Length Info
     ???? 45 0.366709770??? 23.57.6.166 192.168.1.32????????? TLSv1.2?
    991??? Certificate, Certificate Status, Server Key Exchange, Server
    Hello Done

    Transport Layer Security
     ??? TLSv1.2 Record Layer: Handshake Protocol: Certificate
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.2 (0x0303)
     ??????? Length: 4102
     ??????? Handshake Protocol: Certificate
     ??????????? Handshake Type: Certificate (11)
     ???? ...
    Transport Layer Security
     ??? TLSv1.2 Record Layer: Handshake Protocol: Certificate Status
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.2 (0x0303)
     ??????? Length: 479
     ??????? Handshake Protocol: Certificate Status
     ??????????? Handshake Type: Certificate Status (22)
     ??????????? Length: 475
     ??????????? Certificate Status Type: OCSP (1)
     ??????????? OCSP Response Length: 471
     ??????????? OCSP Response
    ...
     ??? TLSv1.2 Record Layer: Handshake Protocol: Server Key Exchange
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.2 (0x0303)
     ??????? Length: 333
     ??????? Handshake Protocol: Server Key Exchange
     ??????????? Handshake Type: Server Key Exchange (12)
     ??????????? Length: 329
     ??????????? EC Diffie-Hellman Server Params
    ...
     ??? TLSv1.2 Record Layer: Handshake Protocol: Server Hello Done
     ??????? Content Type: Handshake (22)
     ??????? Version: TLS 1.2 (0x0303)
     ??????? Length: 4
     ??????? Handshake Protocol: Server Hello Done
     ??????????? Handshake Type: Server Hello Done (14)
     ??????????? Length: 0

    ...

I noticed there is a mention of Diffie-Hellman which may require some 
attention but I am not sure.

I am sorry for all this information but I really look forward to knowing 
more and managing to sort this issue out. Is there anything in this 
information that is relevant to understanding the issue I have ? Where 
should I focus ?

Best regards,

JF

Le 02/01/2021 ? 11:26, jean francois hasson a ?crit?:
>
> Hi,
>
> Thank you Amos Jeffries and Antony Stone. It seems the configuration I 
> have provides the functionality of filtering I am looking for.
>
> There is a strange behavior I can see when accessing some legitimate 
> sites which I see traces of in cache.log :
>
>     2021/01/02 10:55:48 kid1| helperOpenServers: Starting 1/20
>     'squidGuard' processes
>     2021/01/02 10:57:31 kid1| ERROR: negotiating TLS on FD 39:
>     error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
>     inappropriate fallback (1/-1/0)
>     2021/01/02 10:57:31 kid1| Error negotiating SSL connection on FD
>     38: error:00000001:lib(0):func(0):reason(1) (1/-1)
>     2021/01/02 10:57:32 kid1| ERROR: negotiating TLS on FD 38:
>     error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
>     inappropriate fallback (1/-1/0)
>     2021/01/02 10:57:32 kid1| Error negotiating SSL connection on FD
>     35: error:00000001:lib(0):func(0):reason(1) (1/-1)
>     2021/01/02 10:57:40 kid1| Starting new redirector helpers...
>     2021/01/02 10:57:40 kid1| helperOpenServers: Starting 1/20
>     'squidGuard' processes
>     2021/01/02 10:58:09 kid1| ERROR: negotiating TLS on FD 51:
>     error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
>     inappropriate fallback (1/-1/0)
>     2021/01/02 10:58:09 kid1| Error negotiating SSL connection on FD
>     40: error:00000001:lib(0):func(0):reason(1) (1/-1)
>     2021/01/02 10:58:10 kid1| ERROR: negotiating TLS on FD 51:
>     error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert
>     inappropriate fallback (1/-1/0)
>     2021/01/02 10:58:10 kid1| Error negotiating SSL connection on FD
>     40: error:00000001:lib(0):func(0):reason(1) (1/-1)
>
> I noticed other users of squid encountered similar issues but I did 
> not find a clear answer to the issue. Is there a problem with my setup 
> ? I am not sure to be able to solve it on my own ! Any help would be 
> appreciated.
>
> Best regards,
>
> JF Hasson
>
> Le 31/12/2020 ? 10:14, Antony Stone a ?crit?:
>> On Thursday 31 December 2020 at 10:10:11, jean francois hasson wrote:
>>
>>> If I set up on a device connected to the access point a proxy manually
>>> ie 10.3.141.1 on port 8080, I can access the internet. If I put the
>>> following rules for iptables to use in files rules.v4 :
>>>
>>> *nat
>>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination
>>> 10.3.141.1:3128
>>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
>>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination
>>> 10.3.141.1:3129
>>> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129
>>> -A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
>> Try removing the DNAT rules above.  You should be using REDIRECT for intercept
>> mode to work correctly.
>>
>>
>> Antony.
>>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210103/edcce258/attachment.htm>

From ngtech1ltd at gmail.com  Sun Jan  3 20:55:27 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sun, 3 Jan 2021 22:55:27 +0200
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
Message-ID: <000b01d6e212$c4300b60$4c902220$@gmail.com>

Hey,

 

I am missing a bit of the context, like:

Did you self compiled squid? Is it from the OS repository?

Squid -v might help a bit to understand what you do have enabled in your Squid.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of jean francois hasson
Sent: Thursday, December 31, 2020 11:10 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Setting up a transparent http and https proxy server using squid 4.6

 

Hi,

I am trying to create for my home network a transparent proxy to implement filtering rules based on website names mainly.

I have been looking at using a Raspberry pi 3B+ running pi OS. I configured it to be a Wifi access point using RaspAP quick install. The Wifi network on which the filtering option is to be implemented is with IP 10.3.141.xxx. The router is at address 10.3.141.1.

I have the following squid.conf file which I tried to create based on different mails, websites and blogs I read :

acl SSL_ports port 443 #https
acl SSL_ports port 563 # snews
acl SSL_ports port 873 # rsync
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http

#Le r?seau local
acl LocalNet src 10.3.141.0/24

acl bump_step1 at_step SslBump1
acl bump_step2 at_step SslBump2
acl bump_step3 at_step SslBump3

#D?finition des autorisations
http_access deny !Safe_ports
#http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow LocalNet
http_access deny all

#D?finition des ports d'?coute
http_port 8080
http_port 3128 intercept
https_port 3129 intercept ssl-bump \
  tls-cert=/etc/squid/cert/example.crt \
  tls-key=/etc/squid/cert/example.key \
  generate-host-certificates=on  dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

ssl_bump peek all
acl tls_whitelist ssl::server_name .example.com
ssl_bump splice tls_whitelist
ssl_bump terminate all

coredump_dir /var/spool/squid

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

cache_dir ufs /cache 400 16 256
cache_access_log /var/log/squid/access.log
cache_effective_user proxy

If I set up on a device connected to the access point a proxy manually ie 10.3.141.1 on port 8080, I can access the internet. If I put the following rules for iptables to use in files rules.v4 :

*nat
-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 10.3.141.1:3128
-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination 10.3.141.1:3129
-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129
-A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
COMMIT
Now, if I remove the manual proxy configuration of the device connected to the access point, I can't connect to the internet. If I leave the manual proxy configuration it does work and there is activity logged in /var/log/squid/access.log.

Please let me know what might be wrong in my configuration if possible.

Best regards,

JF

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210103/36055b89/attachment.htm>

From rousskov at measurement-factory.com  Sun Jan  3 22:44:58 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 3 Jan 2021 17:44:58 -0500
Subject: [squid-users] SSL-BUMP 5.0.4 not working as expected
In-Reply-To: <000001d6e143$19bddce0$4d3996a0$@gmail.com>
References: <000001d6e143$19bddce0$4d3996a0$@gmail.com>
Message-ID: <ddcffc48-6f9f-e45b-6828-acfe2b5a18b5@measurement-factory.com>

On 1/2/21 3:08 PM, ngtech1ltd at gmail.com wrote:
> I am trying to configure 5.0.4 with sslbump to bump only a set of domains.

>   * Should I bump all connections with exceptions?
>   * Should I bump non else then the exceptions?
>   * Based on server_name regex and/or server_name domains

Policy-wise, you should bump as little as possible. The rest depends on
your local specifics/goals.

As for implementing any policy, here is a rule of thumb: Workarounds and
exceptions aside, make the splicing-vs-bumping _decision_ during step2:
stare if the transaction matches your bumping policy, and peek
otherwise. Trigger the final splice/bump action during step3 based on
the decision made during step2 (modern Squids will do that for you by
default).

Rationale:

* It is not possible to properly bump at step1 -- Squid usually does not
have enough details (e.g., SNI) to do it properly so early. Thus, it is
usually best to just peek at step1.

* It is not possible to make the splicing-vs-bumping _decision_ during
step3 -- Squid has to know your intent at the end of step2 because the
TLS Hello Squid sends at the beginning of step3 depends on that intent.
Thus, the decision has to be made during the only remaining step -- step2.

* Bumping may work better when Squid mimics the server certificate and
that can only happen during step3. Splicing works well at earlier steps,
but splicing later gives Squid access to the TLS server Hello details
that can be useful for logging/triage. Thus, it may be a good idea to
delay the splice/bump action until step3. Please keep in mind that the
step3 action itself is fully determined by your decision during step2.


> I tried to understand why squid is bumping despite the explicit splice
> action.

Squid bumps either when a bump rule matches OR when Squid decides to
serve an error response to the client. The latter often happens when
your http_access rules deny CONNECT requests, especially during step1.
Examine your http_access rules and study the response to the first
bumped request to confirm that it is a Squid error page.


HTH,

Alex.


From rousskov at measurement-factory.com  Sun Jan  3 23:06:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 3 Jan 2021 18:06:00 -0500
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <CABA8h=QxhS_n=3LQECqYSgnUE0ZJeUDgxvF-s4NEfOeG6VgnOA@mail.gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <0c89007d-a58f-b9a2-cea9-a592d77bfeb2@treenet.co.nz>
 <CABA8h=QxhS_n=3LQECqYSgnUE0ZJeUDgxvF-s4NEfOeG6VgnOA@mail.gmail.com>
Message-ID: <5d1c9a18-7f43-aee8-6c6e-96adad8f3ace@measurement-factory.com>

On 1/3/21 10:17 AM, NgTech LTD wrote:

> As i noticed in the past it seems that for a good splice and or bump I
> need the any-of acl to be used.

> Its a bit different then the way squid acls work in general.

The ACLs in ssl_bump rules work exactly the same as ACLs in other
directives. The any-of ACL is not required for ssl_bump or any other
directive. That ACL can indeed be helpful in writing good ssl_bump and
many other rules.

Side note: While bumping is often required for blocking traffic, and
splicing often implies allowing traffic, those actions/decisions are
often quite distinct. Do not ignore http_access rules while working on
ssl_bump rules -- Squid consults _both_ sets of rules,  first during
step1 and then again during step2!


HTH,

Alex.


> On Sun, Jan 3, 2021, 17:06 Amos Jeffries wrote:
> 
>     On 4/01/21 3:12 am, ngtech1ltd wrote:
>     > I am looking for domains lists that can be used for squid to be PCI
>     > Certified.
>     >
>     > I have read this article:
>     > https://www.imperva.com/learn/data-security/pci-dss-certification/
>     >
>     > And couple others to try and understand what might a Squid proxy
>     ssl-bump
>     > exception rules should contain.
>     > So technically we need:
>     > - Banks
>     > - Health care
>     > - Credit Cards(Visa, Mastercard, others)
>     > - Payments sites
>     > - Antivirus(updates and portals)
>     > - OS and software Updates signatures(ASC, MD5, SHAx etc..)
>     >
>     > * https://support.kaspersky.com/common/start/6105
>     > *
>     >
>     https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
>     > set-product-with-a-third-party-firewall
>     > *
>     >
>     https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
>     >
>     55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
>     >
>     p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
>     >
>     e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
>     >
>     Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
>     >
>     525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
>     > D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
>     >
>     >
>     > If someone has the documents which instructs what domains to not
>     inspect it
>     > would also help a lot.
> 
> 
> 
>     Are you trying to get Squid certified as a PCI WAF agent?
>     ? or as security infrastructure agent?
>     ? or as general networking agent?
> 
>     These roles matter in regards to the PCI requirement to detect
>     malicious
>     transactions.
> 
> 
>     Amos
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jfhasson at club-internet.fr  Mon Jan  4 06:51:01 2021
From: jfhasson at club-internet.fr (jean francois hasson)
Date: Mon, 4 Jan 2021 07:51:01 +0100
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <000b01d6e212$c4300b60$4c902220$@gmail.com>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <000b01d6e212$c4300b60$4c902220$@gmail.com>
Message-ID: <53628536-04d7-361b-84e5-8d56b84dc0a8@club-internet.fr>

Hi,

Thank you for looking at my question.

I dowloaded the squid 4.6 source code from 
http://ftp.debian.org/debian/pool/main/s/squid/ and selected 
squid_4.6.orig.tar.gz, squid_4.6-1+deb10u4.debian.tar.xz and 
squid_4.6-1+deb10u4.dsc. I modified the debian/rules file by adding to 
DEB_CONFIGURE_EXTRA_FLAGS the following --with-openssl, --enable-ssl and 
--enable-ssl-crtd.

The squid -v output is :

    Squid Cache: Version 4.6
    Service Name: squid
    Raspbian linux

    This binary uses OpenSSL 1.0.2q? 20 Nov 2018. For legal restrictions
    on distribution see https://www.openssl.org/source/license.html

    configure options:? '--build=arm-linux-gnueabihf' '--prefix=/usr'
    '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
    '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
    '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid'
    '--srcdir=.' '--disable-maintainer-mode'
    '--disable-dependency-tracking' '--disable-silent-rules'
    'BUILDCXXFLAGS=-g -O2
    -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=.
    -fstack-protector-strong -Wformat -Werror=format-security
    -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-z,relro -Wl,-z,now
    -Wl,--as-needed -latomic' 'BUILDCXX=arm-linux-gnueabihf-g++'
    '--with-build-environment=default' '--enable-build-info=Raspbian
    linux' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
    '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man'
    '--enable-inline' '--disable-arch-native' '--enable-async-io=8'
    '--enable-storeio=ufs,aufs,diskd,rock'
    '--enable-removal-policies=lru,heap' '--enable-delay-pools'
    '--enable-cache-digests' '--enable-icap-client'
    '--enable-follow-x-forwarded-for'
    '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
    '--enable-auth-digest=file,LDAP'
    '--enable-auth-negotiate=kerberos,wrapper'
    '--enable-auth-ntlm=fake,SMB_LM'
    '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
    '--enable-security-cert-validators=fake'
    '--enable-storeid-rewrite-helpers=file'
    '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
    '--enable-icmp' '--enable-zph-qos' '--enable-ecap'
    '--disable-translation' '--with-swapdir=/var/spool/squid'
    '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
    '--with-filedescriptors=65536' '--with-large-files'
    '--with-default-user=proxy' '--with-gnutls' '--with-openssl'
    '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter'
    'build_alias=arm-linux-gnueabihf' 'CC=arm-linux-gnueabihf-gcc'
    'CFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=.
    -fstack-protector-strong -Wformat -Werror=format-security -Wall'
    'LDFLAGS=-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic'
    'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
    'CXX=arm-linux-gnueabihf-g++' 'CXXFLAGS=-g -O2
    -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=.
    -fstack-protector-strong -Wformat -Werror=format-security'

When I run openssl version I get 1.1.1d.

I hope it helps.

Best regards,

JF

Le 03/01/2021 ? 21:55, ngtech1ltd at gmail.com a ?crit?:
>
> Hey,
>
> I am missing a bit of the context, like:
>
> Did you self compiled squid? Is it from the OS repository?
>
> Squid -v might help a bit to understand what you do have enabled in 
> your Squid.
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
> Zoom: Coming soon
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *jean francois hasson
> *Sent:* Thursday, December 31, 2020 11:10 AM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Setting up a transparent http and https proxy 
> server using squid 4.6
>
> Hi,
>
> I am trying to create for my home network a transparent proxy to 
> implement filtering rules based on website names mainly.
>
> I have been looking at using a Raspberry pi 3B+ running pi OS. I 
> configured it to be a Wifi access point using RaspAP quick install. 
> The Wifi network on which the filtering option is to be implemented is 
> with IP 10.3.141.xxx. The router is at address 10.3.141.1.
>
> I have the following squid.conf file which I tried to create based on 
> different mails, websites and blogs I read :
>
>     acl SSL_ports port 443 #https
>     acl SSL_ports port 563 # snews
>     acl SSL_ports port 873 # rsync
>     acl Safe_ports port 80 # http
>     acl Safe_ports port 21 # ftp
>     acl Safe_ports port 443 # https
>     acl Safe_ports port 70 # gopher
>     acl Safe_ports port 210 # wais
>     acl Safe_ports port 1025-65535 # unregistered ports
>     acl Safe_ports port 280 # http-mgmt
>     acl Safe_ports port 488 # gss-http
>     acl Safe_ports port 591 # filemaker
>     acl Safe_ports port 777 # multiling http
>
>     #Le r?seau local
>     acl LocalNet src 10.3.141.0/24
>
>     acl bump_step1 at_step SslBump1
>     acl bump_step2 at_step SslBump2
>     acl bump_step3 at_step SslBump3
>
>     #D?finition des autorisations
>     http_access deny !Safe_ports
>     #http_access deny CONNECT !SSL_ports
>     http_access allow localhost manager
>     http_access deny manager
>     http_access allow localhost
>     http_access allow LocalNet
>     http_access deny all
>
>     #D?finition des ports d'?coute
>     http_port 8080
>     http_port 3128 intercept
>     https_port 3129 intercept ssl-bump \
>     ? tls-cert=/etc/squid/cert/example.crt \
>     ? tls-key=/etc/squid/cert/example.key \
>     ? generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
>     sslcrtd_program /usr/lib/squid/security_file_certgen -s
>     /var/lib/ssl_db -M 4MB
>     sslcrtd_children 5
>
>     ssl_bump peek all
>     acl tls_whitelist ssl::server_name .example.com
>     ssl_bump splice tls_whitelist
>     ssl_bump terminate all
>
>     coredump_dir /var/spool/squid
>
>     refresh_pattern ^ftp: 1440 20% 10080
>     refresh_pattern ^gopher: 1440 0% 1440
>     refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>     refresh_pattern . 0 20% 4320
>
>     cache_dir ufs /cache 400 16 256
>     cache_access_log /var/log/squid/access.log
>     cache_effective_user proxy
>
> If I set up on a device connected to the access point a proxy manually 
> ie 10.3.141.1 on port 8080, I can access the internet. If I put the 
> following rules for iptables to use in files rules.v4 :
>
> *nat
> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT 
> --to-destination 10.3.141.1:3128
> -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT 
> --to-destination 10.3.141.1:3129
> -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 
> 3129
> -A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
> COMMIT
> Now, if I remove the manual proxy configuration of the device 
> connected to the access point, I can't connect to the internet. If I 
> leave the manual proxy configuration it does work and there is 
> activity logged in /var/log/squid/access.log.
>
> Please let me know what might be wrong in my configuration if possible.
>
> Best regards,
>
> JF
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/1cb0c669/attachment.htm>

From david at articatech.com  Mon Jan  4 08:22:49 2021
From: david at articatech.com (David Touzeau)
Date: Mon, 4 Jan 2021 09:22:49 +0100
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
Message-ID: <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>

Hi Eiezer,

I can help you by giving a list but

Just by using "main domains":

  * Banking/transcations : 27 646 websites.
  * AV sofwtare and updates sites (fw, routers...) : 133 295 websites


I can give it to you the lists , they are incomplete and it should 
decrease squid performance by loading huge databases.
Perhaps it is better for the Squid administrator to fill it's own list 
according it's country or company activity.




Le 03/01/2021 ? 15:12, ngtech1ltd at gmail.com a ?crit?:
> I am looking for domains lists that can be used for squid to be PCI
> Certified.
>
> I have read this article:
> https://www.imperva.com/learn/data-security/pci-dss-certification/
>
> And couple others to try and understand what might a Squid proxy ssl-bump
> exception rules should contain.
> So technically we need:
> - Banks
> - Health care
> - Credit Cards(Visa, Mastercard, others)
> - Payments sites
> - Antivirus(updates and portals)
> - OS and software Updates signatures(ASC, MD5, SHAx etc..)
>
> * https://support.kaspersky.com/common/start/6105
> *
> https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
> set-product-with-a-third-party-firewall
> *
> https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
> 55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
> p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
> e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
> Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
> 525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
> D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
>
>
> If someone has the documents which instructs what domains to not inspect it
> would also help a lot.
>
> Thanks,
> Eliezer
>
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> Zoom: Coming soon
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/f2eb085a/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan  4 09:27:51 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 4 Jan 2021 11:27:51 +0200
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
Message-ID: <002001d6e27b$e03ae820$a0b0b860$@gmail.com>

Hey David.

 

Indeed it should be done with the local websites however, These sites are pretty static.

Would it be OK to publish theses lists online as a file/files?

 

The main issue is that ssl-bump requires couple ?fast? acls.

I believe it should be a ?fast? acl but we also need the option to use an external helper like for many other function.

If I can choose between ?fast? as default and the ability to run a ?slow? external acl helper I can
choose what is right for/in my environment.

 

Currently I cannot program a helper that will decide if a CONNECT connection should be spliced or bumped programmatically.

It forces me to reload this list manually which might take couple seconds.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Monday, January 4, 2021 10:23 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] PCI Certification compliance lists

 

Hi Eiezer, 

I can help you by giving a list but 

Just by using "main domains": 

*	Banking/transcations : 27 646 websites.
*	AV sofwtare and updates sites (fw, routers...) :  133 295 websites


I can give it to you the lists , they are incomplete and it should decrease squid performance by loading huge databases.
Perhaps it is better for the Squid administrator to fill it's own list according it's country or company activity.





Le 03/01/2021 ? 15:12, ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>  a ?crit :

I am looking for domains lists that can be used for squid to be PCI
Certified.
 
I have read this article:
https://www.imperva.com/learn/data-security/pci-dss-certification/
 
And couple others to try and understand what might a Squid proxy ssl-bump
exception rules should contain.
So technically we need:
- Banks
- Health care
- Credit Cards(Visa, Mastercard, others)
- Payments sites
- Antivirus(updates and portals)
- OS and software Updates signatures(ASC, MD5, SHAx etc..)
 
* https://support.kaspersky.com/common/start/6105
*
https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
set-product-with-a-third-party-firewall
*
https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
 
 
If someone has the documents which instructs what domains to not inspect it
would also help a lot.
 
Thanks,
Eliezer
 
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 
Zoom: Coming soon
 
 
 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/633e3ddd/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan  4 12:04:09 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 4 Jan 2021 14:04:09 +0200
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <53628536-04d7-361b-84e5-8d56b84dc0a8@club-internet.fr>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <000b01d6e212$c4300b60$4c902220$@gmail.com>
 <53628536-04d7-361b-84e5-8d56b84dc0a8@club-internet.fr>
Message-ID: <000801d6e291$b5f42520$21dc6f60$@gmail.com>

Try as test to remove:

ssl_bump terminate all

 

Ie use only the next bump rules:

### START

# TLS/SSL bumping definitions

acl tls_s1_connect at_step SslBump1

acl tls_s2_client_hello at_step SslBump2

acl tls_s3_server_hello at_step SslBump3

 

ssl_bump peek tls_s1_connect

ssl_bump splice all
### END

The above is from an example at ufdbguard manual.

 

Let me know if you are still having issues in full splice mode.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: jean francois hasson <jfhasson at club-internet.fr> 
Sent: Monday, January 4, 2021 8:51 AM
To: ngtech1ltd at gmail.com
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Setting up a transparent http and https proxy server using squid 4.6

 

Hi,

Thank you for looking at my question.

I dowloaded the squid 4.6 source code from http://ftp.debian.org/debian/pool/main/s/squid/ and selected squid_4.6.orig.tar.gz, squid_4.6-1+deb10u4.debian.tar.xz and squid_4.6-1+deb10u4.dsc. I modified the debian/rules file by adding to DEB_CONFIGURE_EXTRA_FLAGS the following --with-openssl, --enable-ssl and --enable-ssl-crtd.

The squid -v output is :

Squid Cache: Version 4.6
Service Name: squid
Raspbian linux

This binary uses OpenSSL 1.0.2q  20 Nov 2018. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=arm-linux-gnueabihf' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' 'BUILDCXX=arm-linux-gnueabihf-g++' '--with-build-environment=default' '--enable-build-info=Raspbian linux' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,SMB_LM' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-security-cert-validators=fake' '--enable-storeid-rewrite-helpers=file' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-gnutls' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' 'build_alias=arm-linux-gnueabihf' 'CC=arm-linux-gnueabihf-gcc' 'CFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=. -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXX=arm-linux-gnueabihf-g++' 'CXXFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=. -fstack-protector-strong -Wformat -Werror=format-security'

When I run openssl version I get 1.1.1d.

I hope it helps.

Best regards,

JF

Le 03/01/2021 ? 21:55, ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>  a ?crit :

Hey,

 

I am missing a bit of the context, like:

Did you self compiled squid? Is it from the OS repository?

Squid -v might help a bit to understand what you do have enabled in your Squid.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of jean francois hasson
Sent: Thursday, December 31, 2020 11:10 AM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Setting up a transparent http and https proxy server using squid 4.6

 

Hi,

I am trying to create for my home network a transparent proxy to implement filtering rules based on website names mainly.

I have been looking at using a Raspberry pi 3B+ running pi OS. I configured it to be a Wifi access point using RaspAP quick install. The Wifi network on which the filtering option is to be implemented is with IP 10.3.141.xxx. The router is at address 10.3.141.1.

I have the following squid.conf file which I tried to create based on different mails, websites and blogs I read :

acl SSL_ports port 443 #https
acl SSL_ports port 563 # snews
acl SSL_ports port 873 # rsync
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http

#Le r?seau local
acl LocalNet src 10.3.141.0/24

acl bump_step1 at_step SslBump1
acl bump_step2 at_step SslBump2
acl bump_step3 at_step SslBump3

#D?finition des autorisations
http_access deny !Safe_ports
#http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow LocalNet
http_access deny all

#D?finition des ports d'?coute
http_port 8080
http_port 3128 intercept
https_port 3129 intercept ssl-bump \
  tls-cert=/etc/squid/cert/example.crt \
  tls-key=/etc/squid/cert/example.key \
  generate-host-certificates=on  dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

ssl_bump peek all
acl tls_whitelist ssl::server_name .example.com
ssl_bump splice tls_whitelist
ssl_bump terminate all

coredump_dir /var/spool/squid

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

cache_dir ufs /cache 400 16 256
cache_access_log /var/log/squid/access.log
cache_effective_user proxy

If I set up on a device connected to the access point a proxy manually ie 10.3.141.1 on port 8080, I can access the internet. If I put the following rules for iptables to use in files rules.v4 :

*nat
-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 10.3.141.1:3128
-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination 10.3.141.1:3129
-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129
-A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
COMMIT
Now, if I remove the manual proxy configuration of the device connected to the access point, I can't connect to the internet. If I leave the manual proxy configuration it does work and there is activity logged in /var/log/squid/access.log.

Please let me know what might be wrong in my configuration if possible.

Best regards,

JF

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/5e6f111c/attachment.htm>

From david at articatech.com  Mon Jan  4 13:25:17 2021
From: david at articatech.com (David Touzeau)
Date: Mon, 4 Jan 2021 14:25:17 +0100
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
 <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
Message-ID: <b5390ff4-9f46-f5ed-c55c-6f22ac38ba59@articatech.com>


Hi Eliezer:

http://articatech.net/tmpf/categories/banking.gz
http://articatech.net/tmpf/categories/cleaning.gz



Le 04/01/2021 ? 10:27, ngtech1ltd at gmail.com a ?crit?:
>
> Hey David.
>
> Indeed it should be done with the local websites however, These sites 
> are pretty static.
>
> Would it be OK to publish theses lists online as a file/files?
>
> The main issue is that ssl-bump requires couple ?fast? acls.
>
> I believe it should be a ?fast? acl but we also need the option to use 
> an external helper like for many other function.
>
> If I can choose between ?fast? as default and the ability to run a 
> ?slow? external acl helper I can
> choose what is right for/in my environment.
>
> Currently I cannot program a helper that will decide if a CONNECT 
> connection should be spliced or bumped programmatically.
>
> It forces me to reload this list manually which might take couple seconds.
>
> Thanks,
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
> Zoom: Coming soon
>
> *From:*squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *David Touzeau
> *Sent:* Monday, January 4, 2021 10:23 AM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] PCI Certification compliance lists
>
> Hi Eiezer,
>
> I can help you by giving a list but
>
> Just by using "main domains":
>
>   * Banking/transcations : 27 646 websites.
>   * AV sofwtare and updates sites (fw, routers...) :? 133 295 websites
>
>
> I can give it to you the lists , they are incomplete and it should 
> decrease squid performance by loading huge databases.
> Perhaps it is better for the Squid administrator to fill it's own list 
> according it's country or company activity.
>
>
>
> Le 03/01/2021 ? 15:12, ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com> a ?crit?:
>
>     I am looking for domains lists that can be used for squid to be PCI
>
>     Certified.
>
>     I have read this article:
>
>     https://www.imperva.com/learn/data-security/pci-dss-certification/  <https://www.imperva.com/learn/data-security/pci-dss-certification/>
>
>     And couple others to try and understand what might a Squid proxy ssl-bump
>
>     exception rules should contain.
>
>     So technically we need:
>
>     - Banks
>
>     - Health care
>
>     - Credit Cards(Visa, Mastercard, others)
>
>     - Payments sites
>
>     - Antivirus(updates and portals)
>
>     - OS and software Updates signatures(ASC, MD5, SHAx etc..)
>
>     *https://support.kaspersky.com/common/start/6105  <https://support.kaspersky.com/common/start/6105>
>
>     *
>
>     https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e  <https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e>
>
>     set-product-with-a-third-party-firewall
>
>     *
>
>     https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s  <https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s>
>
>     55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
>
>     p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
>
>     e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
>
>     Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
>
>     525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
>
>     D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
>
>     If someone has the documents which instructs what domains to not inspect it
>
>     would also help a lot.
>
>     Thanks,
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email:ngtech1ltd at gmail.com  <mailto:ngtech1ltd at gmail.com>
>
>     Zoom: Coming soon
>
>     _______________________________________________
>
>     squid-users mailing list
>
>     squid-users at lists.squid-cache.org  <mailto:squid-users at lists.squid-cache.org>
>
>     http://lists.squid-cache.org/listinfo/squid-users  <http://lists.squid-cache.org/listinfo/squid-users>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/971fee74/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan  4 13:56:24 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 4 Jan 2021 15:56:24 +0200
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <b5390ff4-9f46-f5ed-c55c-6f22ac38ba59@articatech.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
 <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
 <b5390ff4-9f46-f5ed-c55c-6f22ac38ba59@articatech.com>
Message-ID: <001c01d6e2a1$647a0a60$2d6e1f20$@gmail.com>

Thanks David,

 

I don?t understand something:

1490677018.addr

 

Are these integers representing of ip addresses? 

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: David Touzeau <david at articatech.com> 
Sent: Monday, January 4, 2021 3:25 PM
To: ngtech1ltd at gmail.com; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] PCI Certification compliance lists

 


Hi Eliezer:

http://articatech.net/tmpf/categories/banking.gz
http://articatech.net/tmpf/categories/cleaning.gz




Le 04/01/2021 ? 10:27, ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>  a ?crit :

Hey David.

 

Indeed it should be done with the local websites however, These sites are pretty static.

Would it be OK to publish theses lists online as a file/files?

 

The main issue is that ssl-bump requires couple ?fast? acls.

I believe it should be a ?fast? acl but we also need the option to use an external helper like for many other function.

If I can choose between ?fast? as default and the ability to run a ?slow? external acl helper I can
choose what is right for/in my environment.

 

Currently I cannot program a helper that will decide if a CONNECT connection should be spliced or bumped programmatically.

It forces me to reload this list manually which might take couple seconds.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Monday, January 4, 2021 10:23 AM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] PCI Certification compliance lists

 

Hi Eiezer, 

I can help you by giving a list but 

Just by using "main domains": 

1.	Banking/transcations : 27 646 websites.
2.	AV sofwtare and updates sites (fw, routers...) :  133 295 websites


I can give it to you the lists , they are incomplete and it should decrease squid performance by loading huge databases.
Perhaps it is better for the Squid administrator to fill it's own list according it's country or company activity.






Le 03/01/2021 ? 15:12, ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>  a ?crit :

I am looking for domains lists that can be used for squid to be PCI
Certified.
 
I have read this article:
https://www.imperva.com/learn/data-security/pci-dss-certification/
 
And couple others to try and understand what might a Squid proxy ssl-bump
exception rules should contain.
So technically we need:
- Banks
- Health care
- Credit Cards(Visa, Mastercard, others)
- Payments sites
- Antivirus(updates and portals)
- OS and software Updates signatures(ASC, MD5, SHAx etc..)
 
* https://support.kaspersky.com/common/start/6105
*
https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e
set-product-with-a-third-party-firewall
*
https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s
55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
 
 
If someone has the documents which instructs what domains to not inspect it
would also help a lot.
 
Thanks,
Eliezer
 
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 
Zoom: Coming soon
 
 
 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/bcc38182/attachment.htm>

From jfhasson at club-internet.fr  Mon Jan  4 14:18:41 2021
From: jfhasson at club-internet.fr (jean francois hasson)
Date: Mon, 4 Jan 2021 15:18:41 +0100
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <000801d6e291$b5f42520$21dc6f60$@gmail.com>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <000b01d6e212$c4300b60$4c902220$@gmail.com>
 <53628536-04d7-361b-84e5-8d56b84dc0a8@club-internet.fr>
 <000801d6e291$b5f42520$21dc6f60$@gmail.com>
Message-ID: <bde447d3-0b9e-41d5-7c1f-2532a8ca853b@club-internet.fr>

Hi,

Doing the change below works. I can now access ebay.fr through the 
raspberry pi.

Best regards,

JF

Le 04/01/2021 ? 13:04, ngtech1ltd at gmail.com a ?crit?:
>
> Try as test to remove:
>
> ssl_bump terminate all
>
> Ie use only the next bump rules:
>
> ### START
>
> # TLS/SSL bumping definitions
>
> acl tls_s1_connect at_step SslBump1
>
> acl tls_s2_client_hello at_step SslBump2
>
> acl tls_s3_server_hello at_step SslBump3
>
> ssl_bump peek tls_s1_connect
>
> ssl_bump splice all
> ### END
>
> The above is from an example at ufdbguard manual.
>
> Let me know if you are still having issues in full splice mode.
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
> Zoom: Coming soon
>
> *From:* jean francois hasson <jfhasson at club-internet.fr>
> *Sent:* Monday, January 4, 2021 8:51 AM
> *To:* ngtech1ltd at gmail.com
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] Setting up a transparent http and https 
> proxy server using squid 4.6
>
> Hi,
>
> Thank you for looking at my question.
>
> I dowloaded the squid 4.6 source code from 
> http://ftp.debian.org/debian/pool/main/s/squid/ 
> <http://ftp.debian.org/debian/pool/main/s/squid/> and selected 
> squid_4.6.orig.tar.gz, squid_4.6-1+deb10u4.debian.tar.xz and 
> squid_4.6-1+deb10u4.dsc. I modified the debian/rules file by adding to 
> DEB_CONFIGURE_EXTRA_FLAGS the following --with-openssl, --enable-ssl 
> and --enable-ssl-crtd.
>
> The squid -v output is :
>
>     Squid Cache: Version 4.6
>     Service Name: squid
>     Raspbian linux
>
>     This binary uses OpenSSL 1.0.2q? 20 Nov 2018. For legal
>     restrictions on distribution see
>     https://www.openssl.org/source/license.html
>     <https://www.openssl.org/source/license.html>
>
>     configure options:? '--build=arm-linux-gnueabihf' '--prefix=/usr'
>     '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
>     '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
>     '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid'
>     '--srcdir=.' '--disable-maintainer-mode'
>     '--disable-dependency-tracking' '--disable-silent-rules'
>     'BUILDCXXFLAGS=-g -O2
>     -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=.
>     -fstack-protector-strong -Wformat -Werror=format-security
>     -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-z,relro -Wl,-z,now
>     -Wl,--as-needed -latomic' 'BUILDCXX=arm-linux-gnueabihf-g++'
>     '--with-build-environment=default' '--enable-build-info=Raspbian
>     linux' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
>     '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man'
>     '--enable-inline' '--disable-arch-native' '--enable-async-io=8'
>     '--enable-storeio=ufs,aufs,diskd,rock'
>     '--enable-removal-policies=lru,heap' '--enable-delay-pools'
>     '--enable-cache-digests' '--enable-icap-client'
>     '--enable-follow-x-forwarded-for'
>     '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
>     '--enable-auth-digest=file,LDAP'
>     '--enable-auth-negotiate=kerberos,wrapper'
>     '--enable-auth-ntlm=fake,SMB_LM'
>     '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
>     '--enable-security-cert-validators=fake'
>     '--enable-storeid-rewrite-helpers=file'
>     '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
>     '--enable-icmp' '--enable-zph-qos' '--enable-ecap'
>     '--disable-translation' '--with-swapdir=/var/spool/squid'
>     '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
>     '--with-filedescriptors=65536' '--with-large-files'
>     '--with-default-user=proxy' '--with-gnutls' '--with-openssl'
>     '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter'
>     'build_alias=arm-linux-gnueabihf' 'CC=arm-linux-gnueabihf-gcc'
>     'CFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=.
>     -fstack-protector-strong -Wformat -Werror=format-security -Wall'
>     'LDFLAGS=-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic'
>     'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'
>     'CXX=arm-linux-gnueabihf-g++' 'CXXFLAGS=-g -O2
>     -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=.
>     -fstack-protector-strong -Wformat -Werror=format-security'
>
> When I run openssl version I get 1.1.1d.
>
> I hope it helps.
>
> Best regards,
>
> JF
>
> Le 03/01/2021 ? 21:55, ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com> a ?crit?:
>
>     Hey,
>
>     I am missing a bit of the context, like:
>
>     Did you self compiled squid? Is it from the OS repository?
>
>     Squid -v might help a bit to understand what you do have enabled
>     in your Squid.
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>     Zoom: Coming soon
>
>     *From:* squid-users <squid-users-bounces at lists.squid-cache.org>
>     <mailto:squid-users-bounces at lists.squid-cache.org> *On Behalf Of
>     *jean francois hasson
>     *Sent:* Thursday, December 31, 2020 11:10 AM
>     *To:* squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     *Subject:* [squid-users] Setting up a transparent http and https
>     proxy server using squid 4.6
>
>     Hi,
>
>     I am trying to create for my home network a transparent proxy to
>     implement filtering rules based on website names mainly.
>
>     I have been looking at using a Raspberry pi 3B+ running pi OS. I
>     configured it to be a Wifi access point using RaspAP quick
>     install. The Wifi network on which the filtering option is to be
>     implemented is with IP 10.3.141.xxx. The router is at address
>     10.3.141.1.
>
>     I have the following squid.conf file which I tried to create based
>     on different mails, websites and blogs I read :
>
>         acl SSL_ports port 443 #https
>         acl SSL_ports port 563 # snews
>         acl SSL_ports port 873 # rsync
>         acl Safe_ports port 80 # http
>         acl Safe_ports port 21 # ftp
>         acl Safe_ports port 443 # https
>         acl Safe_ports port 70 # gopher
>         acl Safe_ports port 210 # wais
>         acl Safe_ports port 1025-65535 # unregistered ports
>         acl Safe_ports port 280 # http-mgmt
>         acl Safe_ports port 488 # gss-http
>         acl Safe_ports port 591 # filemaker
>         acl Safe_ports port 777 # multiling http
>
>         #Le r?seau local
>         acl LocalNet src 10.3.141.0/24
>
>         acl bump_step1 at_step SslBump1
>         acl bump_step2 at_step SslBump2
>         acl bump_step3 at_step SslBump3
>
>         #D?finition des autorisations
>         http_access deny !Safe_ports
>         #http_access deny CONNECT !SSL_ports
>         http_access allow localhost manager
>         http_access deny manager
>         http_access allow localhost
>         http_access allow LocalNet
>         http_access deny all
>
>         #D?finition des ports d'?coute
>         http_port 8080
>         http_port 3128 intercept
>         https_port 3129 intercept ssl-bump \
>         ? tls-cert=/etc/squid/cert/example.crt \
>         ? tls-key=/etc/squid/cert/example.key \
>         ? generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
>         sslcrtd_program /usr/lib/squid/security_file_certgen -s
>         /var/lib/ssl_db -M 4MB
>         sslcrtd_children 5
>
>         ssl_bump peek all
>         acl tls_whitelist ssl::server_name .example.com
>         ssl_bump splice tls_whitelist
>         ssl_bump terminate all
>
>         coredump_dir /var/spool/squid
>
>         refresh_pattern ^ftp: 1440 20% 10080
>         refresh_pattern ^gopher: 1440 0% 1440
>         refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>         refresh_pattern . 0 20% 4320
>
>         cache_dir ufs /cache 400 16 256
>         cache_access_log /var/log/squid/access.log
>         cache_effective_user proxy
>
>     If I set up on a device connected to the access point a proxy
>     manually ie 10.3.141.1 on port 8080, I can access the internet. If
>     I put the following rules for iptables to use in files rules.v4 :
>
>     *nat
>     -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT
>     --to-destination 10.3.141.1:3128
>     -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT
>     --to-ports 3128
>     -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT
>     --to-destination 10.3.141.1:3129
>     -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT
>     --to-ports 3129
>     -A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
>     COMMIT
>     Now, if I remove the manual proxy configuration of the device
>     connected to the access point, I can't connect to the internet. If
>     I leave the manual proxy configuration it does work and there is
>     activity logged in /var/log/squid/access.log.
>
>     Please let me know what might be wrong in my configuration if
>     possible.
>
>     Best regards,
>
>     JF
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/c60a3581/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan  4 14:26:23 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 4 Jan 2021 16:26:23 +0200
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <bde447d3-0b9e-41d5-7c1f-2532a8ca853b@club-internet.fr>
References: <c4c855ec-64af-548e-1b07-ed376ba17737@club-internet.fr>
 <000b01d6e212$c4300b60$4c902220$@gmail.com>
 <53628536-04d7-361b-84e5-8d56b84dc0a8@club-internet.fr>
 <000801d6e291$b5f42520$21dc6f60$@gmail.com>
 <bde447d3-0b9e-41d5-7c1f-2532a8ca853b@club-internet.fr>
Message-ID: <002501d6e2a5$9475aef0$bd610cd0$@gmail.com>

Just take into account that it will not filter any https/ssl sites this way.
You will need to create an acl to allow only exceptions to be spliced.

Try to look at the ufdbguard manual at:
https://www.urlfilterdb.com/files/downloads/ReferenceManual.pdf

at section: 3.3.2Squid Example Configuration, SSL-Bump peek+splice

All The Bests,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: mailto:ngtech1ltd at gmail.com
Zoom: Coming soon


From: jean francois hasson <jfhasson at club-internet.fr> 
Sent: Monday, January 4, 2021 4:19 PM
To: ngtech1ltd at gmail.com
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Setting up a transparent http and https proxy server using squid 4.6

Hi,
Doing the change below works. I can now access ebay.fr through the raspberry pi.
Best regards,
JF
Le 04/01/2021 ? 13:04, mailto:ngtech1ltd at gmail.com a ?crit :
Try as test to remove:
ssl_bump terminate all
 
Ie use only the next bump rules:
### START
# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3
 
ssl_bump peek tls_s1_connect
ssl_bump splice all
### END
The above is from an example at ufdbguard manual.
 
Let me know if you are still having issues in full splice mode.
 
Eliezer
 
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: mailto:ngtech1ltd at gmail.com
Zoom: Coming soon
 
 
From: jean francois hasson mailto:jfhasson at club-internet.fr 
Sent: Monday, January 4, 2021 8:51 AM
To: mailto:ngtech1ltd at gmail.com
Cc: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Setting up a transparent http and https proxy server using squid 4.6
 
Hi,
Thank you for looking at my question.
I dowloaded the squid 4.6 source code from http://ftp.debian.org/debian/pool/main/s/squid/ and selected squid_4.6.orig.tar.gz, squid_4.6-1+deb10u4.debian.tar.xz and squid_4.6-1+deb10u4.dsc. I modified the debian/rules file by adding to DEB_CONFIGURE_EXTRA_FLAGS the following --with-openssl, --enable-ssl and --enable-ssl-crtd.
The squid -v output is :
Squid Cache: Version 4.6
Service Name: squid
Raspbian linux

This binary uses OpenSSL 1.0.2q  20 Nov 2018. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=arm-linux-gnueabihf' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' 'BUILDCXX=arm-linux-gnueabihf-g++' '--with-build-environment=default' '--enable-build-info=Raspbian linux' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,SMB_LM' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-security-cert-validators=fake' '--enable-storeid-rewrite-helpers=file' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-gnutls' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-linux-netfilter' 'build_alias=arm-linux-gnueabihf' 'CC=arm-linux-gnueabihf-gcc' 'CFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=. -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-z,relro -Wl,-z,now -Wl,--as-needed -latomic' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXX=arm-linux-gnueabihf-g++' 'CXXFLAGS=-g -O2 -fdebug-prefix-map=/home/pi/build/squid/squid-4.6=. -fstack-protector-strong -Wformat -Werror=format-security'
When I run openssl version I get 1.1.1d.
I hope it helps.
Best regards,
JF
Le 03/01/2021 ? 21:55, mailto:ngtech1ltd at gmail.com a ?crit :
Hey,
 
I am missing a bit of the context, like:
Did you self compiled squid? Is it from the OS repository?
Squid -v might help a bit to understand what you do have enabled in your Squid.
 
Eliezer
 
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: mailto:ngtech1ltd at gmail.com
Zoom: Coming soon
 
 
From: squid-users mailto:squid-users-bounces at lists.squid-cache.org On Behalf Of jean francois hasson
Sent: Thursday, December 31, 2020 11:10 AM
To: mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] Setting up a transparent http and https proxy server using squid 4.6
 
Hi,
I am trying to create for my home network a transparent proxy to implement filtering rules based on website names mainly.
I have been looking at using a Raspberry pi 3B+ running pi OS. I configured it to be a Wifi access point using RaspAP quick install. The Wifi network on which the filtering option is to be implemented is with IP 10.3.141.xxx. The router is at address 10.3.141.1.
I have the following squid.conf file which I tried to create based on different mails, websites and blogs I read :
acl SSL_ports port 443 #https
acl SSL_ports port 563 # snews
acl SSL_ports port 873 # rsync
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http

#Le r?seau local
acl LocalNet src 10.3.141.0/24

acl bump_step1 at_step SslBump1
acl bump_step2 at_step SslBump2
acl bump_step3 at_step SslBump3

#D?finition des autorisations
http_access deny !Safe_ports
#http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow LocalNet
http_access deny all

#D?finition des ports d'?coute
http_port 8080
http_port 3128 intercept
https_port 3129 intercept ssl-bump \
  tls-cert=/etc/squid/cert/example.crt \
  tls-key=/etc/squid/cert/example.key \
  generate-host-certificates=on  dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5

ssl_bump peek all
acl tls_whitelist ssl::server_name .example.com
ssl_bump splice tls_whitelist
ssl_bump terminate all

coredump_dir /var/spool/squid

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

cache_dir ufs /cache 400 16 256
cache_access_log /var/log/squid/access.log
cache_effective_user proxy
If I set up on a device connected to the access point a proxy manually ie 10.3.141.1 on port 8080, I can access the internet. If I put the following rules for iptables to use in files rules.v4 :
*nat
-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 10.3.141.1:3128
-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128
-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination 10.3.141.1:3129
-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129
-A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE
COMMIT
Now, if I remove the manual proxy configuration of the device connected to the access point, I can't connect to the internet. If I leave the manual proxy configuration it does work and there is activity logged in /var/log/squid/access.log.
Please let me know what might be wrong in my configuration if possible.
Best regards,
JF
 



From rousskov at measurement-factory.com  Mon Jan  4 14:48:09 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Jan 2021 09:48:09 -0500
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
 <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
Message-ID: <04fbe741-24e9-d780-02e3-c9071e7fbb65@measurement-factory.com>

On 1/4/21 4:27 AM, ngtech1ltd at gmail.com wrote:
> The main issue is that ssl-bump requires couple ?fast? acls.

It does not: The ssl_bump directive supports both fast and slow ACLs.

Alex.


From ngtech1ltd at gmail.com  Mon Jan  4 15:22:21 2021
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 4 Jan 2021 17:22:21 +0200
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <04fbe741-24e9-d780-02e3-c9071e7fbb65@measurement-factory.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
 <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
 <04fbe741-24e9-d780-02e3-c9071e7fbb65@measurement-factory.com>
Message-ID: <002601d6e2ad$65fdcdc0$31f96940$@gmail.com>

Thanks Alex,

So for now the next should work by the docs at:
http://www.squid-cache.org/Versions/v5/cfgman/ssl_bump.html

I just noticed that I didn't put helper in the right context as you wrote in another email.
This way we can reload automatically lists on a change without reloading the whole squid.
So for it to work we just need a single server which supports threading and concurrency.
To overcome updates related issues we can use either a lock/mutex/other.

Thanks Again,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Monday, January 4, 2021 4:48 PM
To: squid-users at lists.squid-cache.org
Cc: ngtech1ltd at gmail.com
Subject: Re: [squid-users] PCI Certification compliance lists

On 1/4/21 4:27 AM, ngtech1ltd at gmail.com wrote:
> The main issue is that ssl-bump requires couple ?fast? acls.

It does not: The ssl_bump directive supports both fast and slow ACLs.

Alex.



From david at articatech.com  Tue Jan  5 00:07:47 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 5 Jan 2021 01:07:47 +0100
Subject: [squid-users] PCI Certification compliance lists
In-Reply-To: <001c01d6e2a1$647a0a60$2d6e1f20$@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAHVxLs3Ln3tOttU0mXa/ZswBAAAAAA==@gmail.com>
 <73df541b-2146-ff37-b717-87e60fa3f8eb@articatech.com>
 <002001d6e27b$e03ae820$a0b0b860$@gmail.com>
 <b5390ff4-9f46-f5ed-c55c-6f22ac38ba59@articatech.com>
 <001c01d6e2a1$647a0a60$2d6e1f20$@gmail.com>
Message-ID: <4ebe4238-5fbc-c1b3-0d46-fe89e082c26d@articatech.com>

Yes this an hton of the IP address (ip2long) , remove the .addr and 
switch to long2ip

Le 04/01/2021 ? 14:56, ngtech1ltd at gmail.com a ?crit?:
>
> Thanks David,
>
> I don?t understand something:
>
> 1490677018.addr
>
> Are these integers representing of ip addresses?
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
> Zoom: Coming soon
>
> *From:*David Touzeau <david at articatech.com>
> *Sent:* Monday, January 4, 2021 3:25 PM
> *To:* ngtech1ltd at gmail.com; squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] PCI Certification compliance lists
>
>
> Hi Eliezer:
>
> http://articatech.net/tmpf/categories/banking.gz 
> <http://articatech.net/tmpf/categories/banking.gz>
> http://articatech.net/tmpf/categories/cleaning.gz 
> <http://articatech.net/tmpf/categories/cleaning.gz>
>
>
> Le 04/01/2021 ? 10:27, ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com> a ?crit?:
>
>     Hey David.
>
>     Indeed it should be done with the local websites however, These
>     sites are pretty static.
>
>     Would it be OK to publish theses lists online as a file/files?
>
>     The main issue is that ssl-bump requires couple ?fast? acls.
>
>     I believe it should be a ?fast? acl but we also need the option to
>     use an external helper like for many other function.
>
>     If I can choose between ?fast? as default and the ability to run a
>     ?slow? external acl helper I can
>     choose what is right for/in my environment.
>
>     Currently I cannot program a helper that will decide if a CONNECT
>     connection should be spliced or bumped programmatically.
>
>     It forces me to reload this list manually which might take couple
>     seconds.
>
>     Thanks,
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>     Zoom: Coming soon
>
>     *From:*squid-users <squid-users-bounces at lists.squid-cache.org>
>     <mailto:squid-users-bounces at lists.squid-cache.org> *On Behalf Of
>     *David Touzeau
>     *Sent:* Monday, January 4, 2021 10:23 AM
>     *To:* squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     *Subject:* Re: [squid-users] PCI Certification compliance lists
>
>     Hi Eiezer,
>
>     I can help you by giving a list but
>
>     Just by using "main domains":
>
>      1. Banking/transcations : 27 646 websites.
>      2. AV sofwtare and updates sites (fw, routers...) :? 133 295 websites
>
>
>     I can give it to you the lists , they are incomplete and it should
>     decrease squid performance by loading huge databases.
>     Perhaps it is better for the Squid administrator to fill it's own
>     list according it's country or company activity.
>
>
>
>
>     Le 03/01/2021 ? 15:12, ngtech1ltd at gmail.com
>     <mailto:ngtech1ltd at gmail.com> a ?crit?:
>
>         I am looking for domains lists that can be used for squid to be PCI
>
>         Certified.
>
>           
>
>         I have read this article:
>
>         https://www.imperva.com/learn/data-security/pci-dss-certification/  <https://www.imperva.com/learn/data-security/pci-dss-certification/>
>
>           
>
>         And couple others to try and understand what might a Squid proxy ssl-bump
>
>         exception rules should contain.
>
>         So technically we need:
>
>         - Banks
>
>         - Health care
>
>         - Credit Cards(Visa, Mastercard, others)
>
>         - Payments sites
>
>         - Antivirus(updates and portals)
>
>         - OS and software Updates signatures(ASC, MD5, SHAx etc..)
>
>           
>
>         *https://support.kaspersky.com/common/start/6105  <https://support.kaspersky.com/common/start/6105>
>
>         *
>
>         https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e  <https://support.eset.com/en/kb332-ports-and-addresses-required-to-use-your-e>
>
>         set-product-with-a-third-party-firewall
>
>         *
>
>         https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s  <https://service.mcafee.com/webcenter/portal/oracle/webcenter/page/scopedMD/s>
>
>         55728c97_466d_4ddb_952d_05484ea932c6/Page29.jspx?wc.contextURL=%2Fspaces%2Fc
>
>         p&articleId=TS100291&_afrLoop=641093247174514&leftWidth=0%25&showFooter=fals
>
>         e&showHeader=false&rightWidth=0%25&centerWidth=100%25#!%40%40%3FshowFooter%3
>
>         Dfalse%26_afrLoop%3D641093247174514%26articleId%3DTS100291%26leftWidth%3D0%2
>
>         525%26showHeader%3Dfalse%26wc.contextURL%3D%252Fspaces%252Fcp%26rightWidth%3
>
>         D0%2525%26centerWidth%3D100%2525%26_adf.ctrl-state%3D3wmxkd4vc_9
>
>           
>
>           
>
>         If someone has the documents which instructs what domains to not inspect it
>
>         would also help a lot.
>
>           
>
>         Thanks,
>
>         Eliezer
>
>           
>
>         ----
>
>         Eliezer Croitoru
>
>         Tech Support
>
>         Mobile: +972-5-28704261
>
>         Email:ngtech1ltd at gmail.com  <mailto:ngtech1ltd at gmail.com>
>
>         Zoom: Coming soon
>
>           
>
>           
>
>           
>
>         _______________________________________________
>
>         squid-users mailing list
>
>         squid-users at lists.squid-cache.org  <mailto:squid-users at lists.squid-cache.org>
>
>         http://lists.squid-cache.org/listinfo/squid-users  <http://lists.squid-cache.org/listinfo/squid-users>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210105/65adaa19/attachment.htm>

From krista.pavskaya at gmail.com  Tue Jan  5 07:34:30 2021
From: krista.pavskaya at gmail.com (=?UTF-8?B?0JrRgNC40YHRgtC40L3QsCDQn9Cw0LLRgdC60LDRjw==?=)
Date: Tue, 5 Jan 2021 09:34:30 +0200
Subject: [squid-users] Uploaded data size log
Message-ID: <CAFG1vAOqqm+=MZiPr+3XOU=ApTnDcPfgvg8wMs3By_CVeDPAjQ@mail.gmail.com>

Hi,
?an I configure squid log to display the size of uploaded data transmitted
from user to server?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210105/040cc0b6/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan  5 08:07:34 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jan 2021 21:07:34 +1300
Subject: [squid-users] Uploaded data size log
In-Reply-To: <CAFG1vAOqqm+=MZiPr+3XOU=ApTnDcPfgvg8wMs3By_CVeDPAjQ@mail.gmail.com>
References: <CAFG1vAOqqm+=MZiPr+3XOU=ApTnDcPfgvg8wMs3By_CVeDPAjQ@mail.gmail.com>
Message-ID: <ed585c2a-946e-90cb-2ffd-48cf69a065d6@treenet.co.nz>

On 5/01/21 8:34 pm, ???????? ??????? wrote:
> Hi,
> ?an I configure squid log to display the size of uploaded data 
> transmitted from user to server?
> 

Yes you can create a custom log format.
see <https://wiki.squid-cache.org/Features/LogFormat>

Amos


From wp.rauchholz at gmail.com  Tue Jan  5 17:21:42 2021
From: wp.rauchholz at gmail.com (Wolfgang Paul Rauchholz)
Date: Tue, 5 Jan 2021 18:21:42 +0100
Subject: [squid-users] TCP_DENIED/403 3954 CONNECT www.welt.de:443 -
 HIER_NONE/- text/html
Message-ID: <CAETVtpSsdRpjP5YPc0ef7s8GP4BU3oB3tdXY2Reh-hSdLs7qSg@mail.gmail.com>

I run a home server under Centos 7 and squid 3.5.20. The config is still
work in progress as I started only today. Any tipps&tricks are welcomed
The function is as expected when working from my LAN. But when I tested
today from my cell phone from outside I received a few TCP_DENIED for web
pages that are not part of the blacklists and I can access from my LAN.
I also tried a few apps (e.g. Twitter, Linkedin, etc..) and none worked.
Underneath the config as-is.

Any idea why I cannot connect?
I have two more question I fiddled with and has no success:

   - Is there a possibility to re-direct certain MACs from the LAN through
   Squid and others can go direct?
   - Is there a good howto that describes in detail how autdetect proxy
   works with wpad.dat. I want that when users are on LAN they go through
   squid, but when they are travelling they can go direct.

Thanks for your help,

Wolfgang



#acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
#acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
# Add wo-lar LAN IP
acl lan src 10.5.2.0/24

# Basic user auth
#auth_param basic program /usr/lib64/squid/basic_ncsa_auth
/etc/squid/.htpasswd
#auth_param basic children 5
#auth_param basic realm Squid Basic Authentication
#auth_param basic credentialsttl 5 hours
#acl password proxy_auth REQUIRED
#http_access allow password

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
#http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
#http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
acl allowdomains dstdomain "/etc/squid/whitelists/domains"
acl porn         dstdomain "/etc/squid/blacklists/porn/domains"
acl drugs        dstdomain "/etc/squid/blacklists/drugs/domains"
http_access allow allowdomains
http_access deny porn
http_access deny drugs

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localhost
http_access allow lan

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

request_header_access Referer deny all
request_header_access X-Forwarded-For deny all
request_header_access Via deny all
request_header_access Cache-Control deny all

# do not display IP address
forwarded_for off

# Virus check for downloads
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=1 icap://
127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1 icap://
127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all


Wolfgang Rauchholz
+34 627 994 977
https://www.linkedin.com/in/wolfgangrauchholz/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210105/19f1b08e/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan  6 05:57:23 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jan 2021 18:57:23 +1300
Subject: [squid-users] TCP_DENIED/403 3954 CONNECT www.welt.de:443 -
 HIER_NONE/- text/html
In-Reply-To: <CAETVtpSsdRpjP5YPc0ef7s8GP4BU3oB3tdXY2Reh-hSdLs7qSg@mail.gmail.com>
References: <CAETVtpSsdRpjP5YPc0ef7s8GP4BU3oB3tdXY2Reh-hSdLs7qSg@mail.gmail.com>
Message-ID: <ff5d4c94-0e62-1960-dfb3-88ae209b514e@treenet.co.nz>

On 6/01/21 6:21 am, Wolfgang Paul Rauchholz wrote:
> I run a home server under Centos 7 and squid 3.5.20. The config is still 
> work in progress as I started only today. Any tipps&tricks are welcomed
> The function is as expected when working from my LAN. But when I tested 
> today from my cell phone from?outside I received a few TCP_DENIED for 
> web pages that are not part of the blacklists and I can access from my LAN.
> I also tried a few apps (e.g. Twitter, Linkedin, etc..) and none worked. 
> Underneath the config as-is.
> 
> Any idea why I cannot connect?


The provided squid.conf has two types of access permitted through this 
proxy:

1) non-LAN traffic is allowed to anything on the whitelist.

2) LAN traffic is allowed to anything on the whitelist AND anything not 
on the blacklist(s).


So the key question is whether the domains you tried are (or were) on 
the whitelist when you access them from outside the LAN.



> I have two more question?I fiddled with and has no success:
> 
>   * Is there a possibility to re-direct certain MACs from the LAN
>     through Squid and others can go direct?

Getting traffic to the proxy is entirely the OS duty. Most OS can route 
or NAT IPv4 packets based on MAC.

Note that if the client is not opening connections to the proxy by 
itself then you need to follow one of the interception configurations on 
the Squid machine. see 
<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
Squid needs separate http_port's for the intercepted traffic and the 
normal proxy traffic.


>   * Is there a good howto that describes in detail how autdetect?proxy
>     works with wpad.dat. I want that when users are on LAN they go
>     through squid, but when they are travelling they can go direct.
> 


I suggest you have a read of the FAQs 
(<https://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>) about how 
proxies can be used by Browsers. In particular the section "Recommended 
network configuration" and links from there.



> Thanks for your help,
> 
> Wolfgang
> 
> 
> 
> #acl localnet src 172.16.0.0/12 <http://172.16.0.0/12> ?# RFC1918 
> possible internal network
> #acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> # RFC1918 
> possible internal network
> #acl localnet src fc00::/7 ? ? ? # RFC 4193 local private network range
> #acl localnet src fe80::/10 ? ? ?# RFC 4291 link-local (directly 
> plugged) machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80 ? ? ? ? ?# http
> acl Safe_ports port 21 ? ? ? ? ?# ftp
> acl Safe_ports port 443 ? ? ? ? # https
> acl Safe_ports port 70 ? ? ? ? ?# gopher
> acl Safe_ports port 210 ? ? ? ? # wais
> acl Safe_ports port 1025-65535 ?# unregistered ports
> acl Safe_ports port 280 ? ? ? ? # http-mgmt
> acl Safe_ports port 488 ? ? ? ? # gss-http
> acl Safe_ports port 591 ? ? ? ? # filemaker
> acl Safe_ports port 777 ? ? ? ? # multiling http
> acl CONNECT method CONNECT
> # Add wo-lar LAN IP
> acl lan src 10.5.2.0/24 <http://10.5.2.0/24>
> 

FYI: this is what "localnet" ACL is supposed to be set to.

As the squid.conf file itself says:
  # Adapt localnet in the ACL section to list your (internal) IP networks


> # Basic user auth
> #auth_param basic program /usr/lib64/squid/basic_ncsa_auth 
> /etc/squid/.htpasswd
> #auth_param basic children 5
> #auth_param basic realm Squid Basic Authentication
> #auth_param basic credentialsttl 5 hours
> #acl password proxy_auth REQUIRED
> #http_access allow password
> 

FYI: if you ever want to enable auth this should all be down below the 
blacklist http_access lines.


> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> #http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> #http_access deny CONNECT !SSL_ports
> 

Please re-enable the above lines. They are protecting against whole 
groups of security attacks.


> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> acl allowdomains dstdomain "/etc/squid/whitelists/domains"
> acl porn ? ? ? ? dstdomain "/etc/squid/blacklists/porn/domains"
> acl drugs ? ? ? ?dstdomain "/etc/squid/blacklists/drugs/domains"
> http_access allow allowdomains
> http_access deny porn
> http_access deny drugs
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localhost
> http_access allow lan
> 
> # And finally deny all other access to this proxy
> http_access deny all


> request_header_access Referer deny all
> request_header_access X-Forwarded-For deny all

You have configured "forwarded_for off" and are not using the clients 
X-Forwarded-For header. Which means the above setting does nothing useful.

> request_header_access Via deny all
> request_header_access Cache-Control deny all
> 

The above breaks HTTP in bad ways. Your clients will not be having a 
good experience without Cache-Control.


Amos


From ngtech1ltd at gmail.com  Wed Jan  6 19:49:57 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 6 Jan 2021 21:49:57 +0200
Subject: [squid-users] Host header forgery detected on domain:
 mobile.pipe.aria.microsoft.com
Message-ID: <00a401d6e465$26cd31e0$746795a0$@gmail.com>

I'm testing SSL BUMP in 5.0.4 and it's working pretty well despite some
hiccups.

I am trying to think about the right solution for the next issue:
SECURITY ALERT: Host header forgery detected on conn18767
local=52.114.32.24:443 remote=192.168.189.52:65107 FD 15 flags=33 (local IP
does not match any domain IP)
                                                   current master
transaction: master12927

The main issue is that the DNS service changes address every 10 ~ seconds.
An example:
### DRILL START
# drill mobile.pipe.aria.microsoft.com
;; ->>HEADER<<- opcode: QUERY, rcode: NOERROR, id: 23399
;; flags: qr rd ra ; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0
;; QUESTION SECTION:
;; mobile.pipe.aria.microsoft.com.      IN      A

;; ANSWER SECTION:
mobile.pipe.aria.microsoft.com. 3066    IN      CNAME
mobile.events.data.trafficmanager.net.
mobile.events.data.trafficmanager.net.  43      IN      CNAME
skypedataprdcolcus06.cloudapp.net.
skypedataprdcolcus06.cloudapp.net.      1       IN      A
52.114.128.69

;; AUTHORITY SECTION:

;; ADDITIONAL SECTION:

;; Query time: 3 msec
;; SERVER: 192.168.200.1
;; WHEN: Wed Jan  6 20:22:28 2021
;; MSG SIZE  rcvd: 159
### DRILL END

### DRILL START
# drill mobile.pipe.aria.microsoft.com
;; ->>HEADER<<- opcode: QUERY, rcode: NOERROR, id: 15462
;; flags: qr rd ra ; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0
;; QUESTION SECTION:
;; mobile.pipe.aria.microsoft.com.      IN      A

;; ANSWER SECTION:
mobile.pipe.aria.microsoft.com. 3065    IN      CNAME
mobile.events.data.trafficmanager.net.
mobile.events.data.trafficmanager.net.  42      IN      CNAME
skypedataprdcolcus06.cloudapp.net.
skypedataprdcolcus06.cloudapp.net.      0       IN      A
52.114.128.69

;; AUTHORITY SECTION:

;; ADDITIONAL SECTION:

;; Query time: 23 msec
;; SERVER: 192.168.200.1
;; WHEN: Wed Jan  6 20:22:29 2021
;; MSG SIZE  rcvd: 159
[root at px1 bin]# drill mobile.pipe.aria.microsoft.com
;; ->>HEADER<<- opcode: QUERY, rcode: NOERROR, id: 31545
;; flags: qr rd ra ; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0
;; QUESTION SECTION:
;; mobile.pipe.aria.microsoft.com.      IN      A

;; ANSWER SECTION:
mobile.pipe.aria.microsoft.com. 2993    IN      CNAME
mobile.events.data.trafficmanager.net.
mobile.events.data.trafficmanager.net.  22      IN      CNAME
skypedataprdcoleus14.cloudapp.net.
skypedataprdcoleus14.cloudapp.net.      4       IN      A       52.170.57.27

;; AUTHORITY SECTION:

;; ADDITIONAL SECTION:

;; Query time: 13 msec
;; SERVER: 192.168.200.1
;; WHEN: Wed Jan  6 20:22:30 2021
;; MSG SIZE  rcvd: 159
### DRILL END

All of the hosts use the same DNS service in the LAN however for some reason
both squid and the client are resolving different addresses
in a period of  10  Seconds.

The solution I am thinking is to force a minimum of 60 seconds caching using
dnsmasq or another caching service.
* https://unix.stackexchange.com/a/287908

Can we teach (theoretically) squid a way to look at these short TTLs as
something to decide by an ACL?

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon





From conner.bean at icloud.com  Wed Jan  6 20:24:16 2021
From: conner.bean at icloud.com (Conner Bean)
Date: Wed, 6 Jan 2021 15:24:16 -0500
Subject: [squid-users] Squid 4 Migration - balance_on_multiple_ip
Message-ID: <5FF887B2-74C1-4B4A-922B-F50036AB0DB1@icloud.com>

Hi squid users!

Hope you are all well. I'm attempting to migrate from Squid 3.5 to 4, and in my conf file I used to have balance_on_multiple_ip toggled on as to reduce chance of brownouts on endpoints. I noticed this is not available in Squid 4, is it on by default? Or is there some alternative?

Thank you all very much!

Conner Bean

From squid3 at treenet.co.nz  Wed Jan  6 20:27:38 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Thu, 07 Jan 2021 09:27:38 +1300
Subject: [squid-users] Host header forgery detected on domain:
 mobile.pipe.aria.microsoft.com
References: <00a401d6e465$26cd31e0$746795a0$@gmail.com>
Message-ID: <-gmr3n49qm434-4kue3w-wkqg25w8gy5m-3pkwtyhwvm12l0moczhi8tyab90egc-nvedznmgrog4-v7f0zj-tfeoffz97lrs1q7kvov3q1pq-k0v4qygxlubg5u6gtn-t9oh57lrsjqirng96u-m9wxh2.1609964858839@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210107/234023d7/attachment.htm>

From rousskov at measurement-factory.com  Wed Jan  6 20:42:10 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Jan 2021 15:42:10 -0500
Subject: [squid-users] Host header forgery detected on domain:
 mobile.pipe.aria.microsoft.com
In-Reply-To: <00a401d6e465$26cd31e0$746795a0$@gmail.com>
References: <00a401d6e465$26cd31e0$746795a0$@gmail.com>
Message-ID: <450c138a-042c-a08d-a104-710d706b521c@measurement-factory.com>

On 1/6/21 2:49 PM, Eliezer Croitoru wrote:

> I am trying to think about the right solution for the next issue:
> SECURITY ALERT: Host header forgery detected on conn18767
> local=52.114.32.24:443 remote=192.168.189.52:65107 FD 15 flags=33 (local IP
> does not match any domain IP)

As you know, this has been discussed many times on this list before,
including recently[1]. I doubt anything has changed since then.

[1]
http://lists.squid-cache.org/pipermail/squid-users/2020-November/022912.html


> All of the hosts use the same DNS service in the LAN however for some reason
> both squid and the client are resolving different addresses
> in a period of  10  Seconds.

The "however for some reason" part feels misleading to me -- what you
observe is the direct, expected consequence of the low-TTL environment
you have described. There is no contradiction or uncertainty here AFAICT.


> The solution I am thinking is to force a minimum of 60 seconds caching using
> dnsmasq or another caching service.

FTR: Increasing DNS response TTL will reduce the number/probability of
false positives in forged Host header detection. No more. No less.


> Can we teach (theoretically) squid a way to look at these short TTLs as
> something to decide by an ACL?

Yes, it is possible. There is positive_dns_ttl already which specifies
an upper bound. One could add a similar positive_dns_ttl_min option that
would specify the lower bound. Like virtually any Squid directive, it
can be made conditional on ACLs.

IMO, violating DNS TTLs is not the right solution for this problem
though. See my response on the [1] thread for a better medium-term solution.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jan  6 21:17:34 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 6 Jan 2021 16:17:34 -0500
Subject: [squid-users] Squid 4 Migration - balance_on_multiple_ip
In-Reply-To: <5FF887B2-74C1-4B4A-922B-F50036AB0DB1@icloud.com>
References: <5FF887B2-74C1-4B4A-922B-F50036AB0DB1@icloud.com>
Message-ID: <7fd7134f-2aac-e0b5-6ba5-fc1913b2766a@measurement-factory.com>

On 1/6/21 3:24 PM, Conner Bean wrote:

> Hope you are all well. I'm attempting to migrate from Squid 3.5 to 4,
> and in my conf file I used to have balance_on_multiple_ip toggled on
> as to reduce chance of brownouts on endpoints.

FYI: Enabling balance_on_multiple_ip does nothing in Squid v3.5.


> I noticed this is not available in Squid 4, is it on by default? Or
> is there some alternative?

Squid is no longer capable of rotating host IPs on every access.

* AFAICT, the code associated with that directive was silently and, I
bet, accidentally removed in v3.2 (commit 9a6476c). The release notes
for that version were updated seven years later (commit 635c161).

* Squid v3.2 release notes associated with balance_on_multiple_ip
removal say that such rotation is no longer possible.

* Current squid.conf.documented says that such rotation is no longer
relevant.

FWIW, I disagree with the last two assertions: The Happy Eyeballs
principle is compatible and largely orthogonal to the IP rotation idea.
Squid just lacks the code to properly support that rotation.


HTH,

Alex.


From ngtech1ltd at gmail.com  Thu Jan  7 03:26:32 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 7 Jan 2021 05:26:32 +0200
Subject: [squid-users] Host header forgery detected on domain:
 mobile.pipe.aria.microsoft.com
In-Reply-To: <450c138a-042c-a08d-a104-710d706b521c@measurement-factory.com>
References: <00a401d6e465$26cd31e0$746795a0$@gmail.com>
 <450c138a-042c-a08d-a104-710d706b521c@measurement-factory.com>
Message-ID: <00b101d6e4a4$e5be45f0$b13ad1d0$@gmail.com>

Hey Alex,

The main issue now is the extensive logging.
For a tiny server with a single desktop client the cache.log  are expending a *lot*.
I have a problem with discarding these logs but for this specific case where the ttl is very low ie: lower < 30/20/10 seconds
We can expect for this to happen so we can disable the logs since the service continue to work with this low ttl.
The only and main issue is the extensive logging which is wrong.

Should we continue this on Squid-dev?

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Wednesday, January 6, 2021 10:42 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
Subject: Re: [squid-users] Host header forgery detected on domain: mobile.pipe.aria.microsoft.com

On 1/6/21 2:49 PM, Eliezer Croitoru wrote:

> I am trying to think about the right solution for the next issue:
> SECURITY ALERT: Host header forgery detected on conn18767
> local=52.114.32.24:443 remote=192.168.189.52:65107 FD 15 flags=33 (local IP
> does not match any domain IP)

As you know, this has been discussed many times on this list before,
including recently[1]. I doubt anything has changed since then.

[1]
http://lists.squid-cache.org/pipermail/squid-users/2020-November/022912.html


> All of the hosts use the same DNS service in the LAN however for some reason
> both squid and the client are resolving different addresses
> in a period of  10  Seconds.

The "however for some reason" part feels misleading to me -- what you
observe is the direct, expected consequence of the low-TTL environment
you have described. There is no contradiction or uncertainty here AFAICT.


> The solution I am thinking is to force a minimum of 60 seconds caching using
> dnsmasq or another caching service.

FTR: Increasing DNS response TTL will reduce the number/probability of
false positives in forged Host header detection. No more. No less.


> Can we teach (theoretically) squid a way to look at these short TTLs as
> something to decide by an ACL?

Yes, it is possible. There is positive_dns_ttl already which specifies
an upper bound. One could add a similar positive_dns_ttl_min option that
would specify the lower bound. Like virtually any Squid directive, it
can be made conditional on ACLs.

IMO, violating DNS TTLs is not the right solution for this problem
though. See my response on the [1] thread for a better medium-term solution.


HTH,

Alex.



From ngtech1ltd at gmail.com  Thu Jan  7 08:09:11 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 7 Jan 2021 10:09:11 +0200
Subject: [squid-users] Squid 4 Migration - balance_on_multiple_ip
In-Reply-To: <5FF887B2-74C1-4B4A-922B-F50036AB0DB1@icloud.com>
References: <5FF887B2-74C1-4B4A-922B-F50036AB0DB1@icloud.com>
Message-ID: <00b601d6e4cc$626e37a0$274aa6e0$@gmail.com>

Depends on the number of IP addresses there is a possibility to use an external ACL helper that will do that for you.
An example for LB over wan links can be seen with the next examples which I wrote:
* https://github.com/elico/mwan-nft-lb-example
* https://github.com/elico/vagrant-squid-outgoing-addresses

How many IP addresses are we talking about ?

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Conner Bean
Sent: Wednesday, January 6, 2021 10:24 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid 4 Migration - balance_on_multiple_ip

Hi squid users!

Hope you are all well. I'm attempting to migrate from Squid 3.5 to 4, and in my conf file I used to have balance_on_multiple_ip toggled on as to reduce chance of brownouts on endpoints. I noticed this is not available in Squid 4, is it on by default? Or is there some alternative?

Thank you all very much!

Conner Bean
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Jan  7 16:08:04 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jan 2021 11:08:04 -0500
Subject: [squid-users] Host header forgery detected on domain:
 mobile.pipe.aria.microsoft.com
In-Reply-To: <00b101d6e4a4$e5be45f0$b13ad1d0$@gmail.com>
References: <00a401d6e465$26cd31e0$746795a0$@gmail.com>
 <450c138a-042c-a08d-a104-710d706b521c@measurement-factory.com>
 <00b101d6e4a4$e5be45f0$b13ad1d0$@gmail.com>
Message-ID: <05fbb740-83f3-9f55-67ae-1a5d5b03e7a8@measurement-factory.com>

On 1/6/21 10:26 PM, Eliezer Croitoru wrote:

> The main issue now is the extensive logging.

Please note that excessive logging may be the main issue for you and
_some_ Squid admins. For many, the main issue is denied transactions.
For some, it is cache misses. For some, it is a combination of things.
These facts make any single simple solution inapplicable to some use cases.

For example, you can trivially decrease the number (or, with a few more
code lines, frequency) of these messages, but it will not help with the
other problems I mentioned. FWIW, Factory is working on making all
level-0/1 messages configurable that way, but we need more time to
finish that project.

This does not mean that any simple partial solution is going to be
rejected, but please be very careful/specific in defining the problem
when proposing (or asking for) a solution.


> Should we continue this on Squid-dev?

IMO, if you are going to discuss the problem and possible
functionality-level solutions, then squid-users may be the best place
for that. If you are going to discuss code changes and similar
developer-level issues, use squid-dev.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Wednesday, January 6, 2021 10:42 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
> Subject: Re: [squid-users] Host header forgery detected on domain: mobile.pipe.aria.microsoft.com
> 
> On 1/6/21 2:49 PM, Eliezer Croitoru wrote:
> 
>> I am trying to think about the right solution for the next issue:
>> SECURITY ALERT: Host header forgery detected on conn18767
>> local=52.114.32.24:443 remote=192.168.189.52:65107 FD 15 flags=33 (local IP
>> does not match any domain IP)
> 
> As you know, this has been discussed many times on this list before,
> including recently[1]. I doubt anything has changed since then.
> 
> [1]
> http://lists.squid-cache.org/pipermail/squid-users/2020-November/022912.html
> 
> 
>> All of the hosts use the same DNS service in the LAN however for some reason
>> both squid and the client are resolving different addresses
>> in a period of  10  Seconds.
> 
> The "however for some reason" part feels misleading to me -- what you
> observe is the direct, expected consequence of the low-TTL environment
> you have described. There is no contradiction or uncertainty here AFAICT.
> 
> 
>> The solution I am thinking is to force a minimum of 60 seconds caching using
>> dnsmasq or another caching service.
> 
> FTR: Increasing DNS response TTL will reduce the number/probability of
> false positives in forged Host header detection. No more. No less.
> 
> 
>> Can we teach (theoretically) squid a way to look at these short TTLs as
>> something to decide by an ACL?
> 
> Yes, it is possible. There is positive_dns_ttl already which specifies
> an upper bound. One could add a similar positive_dns_ttl_min option that
> would specify the lower bound. Like virtually any Squid directive, it
> can be made conditional on ACLs.
> 
> IMO, violating DNS TTLs is not the right solution for this problem
> though. See my response on the [1] thread for a better medium-term solution.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ngtech1ltd at gmail.com  Thu Jan  7 17:47:38 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 7 Jan 2021 19:47:38 +0200
Subject: [squid-users] Host header forgery detected on domain:
 mobile.pipe.aria.microsoft.com
In-Reply-To: <05fbb740-83f3-9f55-67ae-1a5d5b03e7a8@measurement-factory.com>
References: <00a401d6e465$26cd31e0$746795a0$@gmail.com>
 <450c138a-042c-a08d-a104-710d706b521c@measurement-factory.com>
 <00b101d6e4a4$e5be45f0$b13ad1d0$@gmail.com>
 <05fbb740-83f3-9f55-67ae-1a5d5b03e7a8@measurement-factory.com>
Message-ID: <000901d6e51d$31065840$931308c0$@gmail.com>

Thanks Alex,

For now staying on the admin level.
I understand that the issues are affecting and that a solution will affect functionality.
I will try to take two scenarios:
- short ttl
- nxdomain

With the short ttl it's probably ok in many of the setups to have different resolution between the client and the proxy.
It might affect some connections (which is bad from the service provider side..) but else then cache misses it's acceptable
for application level services.
For content services it's a whole new story.

About the nxdomain case, there are couple options:
- What happen when there is no domain at all, will squid allow it? A default allow/deny and ACL based might be a good solution.
- When the client is using an internal DNS via ssl VPN while the actual end point is on the internet on the public WWW.

I have seen a scenario which the proxy resolves to another domain and IP and for this it would be smart to allow by ACLS 
an override for the host header forgery.

I do not expect things to be done in a second since these in most of the cases are a "patch" not a solution.

I would like to hear what do you think about these real world scenarios which are far more important
then basic caching while not lowering the caching as important as the service itself.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Thursday, January 7, 2021 6:08 PM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
Cc: squid-dev at lists.squid-cache.org
Subject: Re: [squid-users] Host header forgery detected on domain: mobile.pipe.aria.microsoft.com

On 1/6/21 10:26 PM, Eliezer Croitoru wrote:

> The main issue now is the extensive logging.

Please note that excessive logging may be the main issue for you and
_some_ Squid admins. For many, the main issue is denied transactions.
For some, it is cache misses. For some, it is a combination of things.
These facts make any single simple solution inapplicable to some use cases.

For example, you can trivially decrease the number (or, with a few more
code lines, frequency) of these messages, but it will not help with the
other problems I mentioned. FWIW, Factory is working on making all
level-0/1 messages configurable that way, but we need more time to
finish that project.

This does not mean that any simple partial solution is going to be
rejected, but please be very careful/specific in defining the problem
when proposing (or asking for) a solution.


> Should we continue this on Squid-dev?

IMO, if you are going to discuss the problem and possible
functionality-level solutions, then squid-users may be the best place
for that. If you are going to discuss code changes and similar
developer-level issues, use squid-dev.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Wednesday, January 6, 2021 10:42 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
> Subject: Re: [squid-users] Host header forgery detected on domain: mobile.pipe.aria.microsoft.com
> 
> On 1/6/21 2:49 PM, Eliezer Croitoru wrote:
> 
>> I am trying to think about the right solution for the next issue:
>> SECURITY ALERT: Host header forgery detected on conn18767
>> local=52.114.32.24:443 remote=192.168.189.52:65107 FD 15 flags=33 (local IP
>> does not match any domain IP)
> 
> As you know, this has been discussed many times on this list before,
> including recently[1]. I doubt anything has changed since then.
> 
> [1]
> http://lists.squid-cache.org/pipermail/squid-users/2020-November/022912.html
> 
> 
>> All of the hosts use the same DNS service in the LAN however for some reason
>> both squid and the client are resolving different addresses
>> in a period of  10  Seconds.
> 
> The "however for some reason" part feels misleading to me -- what you
> observe is the direct, expected consequence of the low-TTL environment
> you have described. There is no contradiction or uncertainty here AFAICT.
> 
> 
>> The solution I am thinking is to force a minimum of 60 seconds caching using
>> dnsmasq or another caching service.
> 
> FTR: Increasing DNS response TTL will reduce the number/probability of
> false positives in forged Host header detection. No more. No less.
> 
> 
>> Can we teach (theoretically) squid a way to look at these short TTLs as
>> something to decide by an ACL?
> 
> Yes, it is possible. There is positive_dns_ttl already which specifies
> an upper bound. One could add a similar positive_dns_ttl_min option that
> would specify the lower bound. Like virtually any Squid directive, it
> can be made conditional on ACLs.
> 
> IMO, violating DNS TTLs is not the right solution for this problem
> though. See my response on the [1] thread for a better medium-term solution.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From Paul at pjb.org.uk  Thu Jan  7 19:43:17 2021
From: Paul at pjb.org.uk (Paul at pjb.org.uk)
Date: Thu, 07 Jan 2021 19:43:17 -0000
Subject: [squid-users] Squid 5.0.3 Cache_Peer Authentication Issue
Message-ID: <5FF76455.8039.284FDE69@Paul.pjb.org.uk>

Hello,

I am currently using Squid 5.0.3 but have an issue when using a cache_peer (non-squid &  
outside my control) that requires authentication.  My Squid server doesn't require 
authentication and reading the documentation indicated that I need to set 
'login=PASSTHRU' on my cache_peer line, which I have done.  This has enabled GET 
methods to work as expected, but CONNECT methods are failing.  The response from the 
peer is a '407' with both methods. 

I am controlling access to the peer via an acl:
---------------
acl localClients src 10.10.1.0/24
http_access allow localClients

acl aclREDIRECT dstdomain "/etc/squid/redirect.txt"
cache_peer 10.10.10.167 parent 8080 0 no-query name=peerREDIRECT login=PASSTHRU 
connection-auth=on
cache_peer_access peerREDIRECT allow aclREDIRECT
cache_peer_access peerREDIRECT deny !aclREDIRECT
never_direct allow aclREDIRECT
always_direct deny aclREDIRECT
always_direct allow all

http_port 80 connection-auth=on
---------------


An extract from my logs showing the failure:
---------
kid1| 5,3| IoCallback.cc(112) finish: called for conn30 local=10.10.10.60:41270 
remote=10.10.10.167:8080 FIRSTUP_PARENT FD 17 flags=1 (0, 0)
kid1| 5,3| Read.cc(93) ReadNow: conn30 local=10.10.10.60:41270 
remote=10.10.10.167:8080 FIRSTUP_PARENT FD 17 flags=1, size 65535, retval 978, 
errno 0
kid1| 11,2| HttpTunneler.cc(323) handleResponse: Tunnel Server conn30 
local=10.10.10.60:41270 remote=10.10.10.167:8080 FIRSTUP_PARENT FD 17 flags=1
kid1| 11,2| HttpTunneler.cc(326) handleResponse: Tunnel Server RESPONSE:
---------
<HEAD><TITLE>Proxy Authorization Required</TITLE></HEAD>
<BODY BGCOLOR="white" FGCOLOR="black"><H1>Proxy Authorization 
Required</H1><HR>
<FONT FACE="Helvetica,Arial"><B>
Description: Authorization is required for access to this proxy</B></FONT>
<HR>
<!-- default "Proxy Authorization Required" response (407) -->----------
kid1| 83,3| HttpTunneler.cc(345) bailOnResponseError: unsupported CONNECT response 
status code [state:w FD 17 job22]
kid1| TCP connection to 10.10.10.167/8080 failed
    current master transaction: master57
kid1| 83,5| HttpTunneler.cc(404) callBack: conn30 local=10.10.10.60:41270 
remote=10.10.10.167:8080 FIRSTUP_PARENT FD 17 flags=1 [state:w FD 17 job22]
--------------

Is this a mis-configuration? or have I mis-understood how cache_peer works?

regards,
Paul




From rousskov at measurement-factory.com  Thu Jan  7 20:18:14 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jan 2021 15:18:14 -0500
Subject: [squid-users] Squid 5.0.3 Cache_Peer Authentication Issue
In-Reply-To: <5FF76455.8039.284FDE69@Paul.pjb.org.uk>
References: <5FF76455.8039.284FDE69@Paul.pjb.org.uk>
Message-ID: <b02bff7c-43ec-3028-52b7-fcce018c7e02@measurement-factory.com>

On 1/7/21 2:43 PM, Paul at pjb.org.uk wrote:

> I am currently using Squid 5.0.3 but have an issue when using a cache_peer (non-squid &  
> outside my control) that requires authentication.  My Squid server doesn't require 
> authentication and reading the documentation indicated that I need to set 
> 'login=PASSTHRU' on my cache_peer line, which I have done.  This has enabled GET 
> methods to work as expected, but CONNECT methods are failing.  The response from the 
> peer is a '407' with both methods. 

> I am controlling access to the peer via an acl:

> ---------------
> acl localClients src 10.10.1.0/24
> http_access allow localClients
> 
> acl aclREDIRECT dstdomain "/etc/squid/redirect.txt"
> cache_peer 10.10.10.167 parent 8080 0 no-query name=peerREDIRECT login=PASSTHRU 
> connection-auth=on
> cache_peer_access peerREDIRECT allow aclREDIRECT
> cache_peer_access peerREDIRECT deny !aclREDIRECT
> never_direct allow aclREDIRECT
> always_direct deny aclREDIRECT
> always_direct allow all
> 
> http_port 80 connection-auth=on
> ---------------


> An extract from my logs showing the failure:
> kid1| 11,2| HttpTunneler.cc(326) handleResponse: Tunnel Server RESPONSE:
> <!-- default "Proxy Authorization Required" response (407) -->----------
> kid1| 83,3| HttpTunneler.cc(345) bailOnResponseError: unsupported CONNECT response 
> status code [state:w FD 17 job22]

> Is this a mis-configuration? or have I mis-understood how cache_peer works?

N.B. I assume you do not use SslBump -- the configuration snippet above
does not show SslBump being used. SslBump does not support what you want
per commit f5e1794 message.

What kind of HTTP authentication does your client/peer use/expect?

The 407 response from the peer may be normal/expected (a part of HTTP
authentication) or indicate a problem. If you do not get better
suggestions, please show us the CONNECT request and response headers,
exchanged between the client and your Squid and between your Squid and
cache_peer (i.e. 4 headers total). You can use tools like
tcpdump/wireshark to collect/render those plain text headers.

Alex.


From Paul at pjb.org.uk  Fri Jan  8 17:18:07 2021
From: Paul at pjb.org.uk (Paul at pjb.org.uk)
Date: Fri, 08 Jan 2021 17:18:07 -0000
Subject: [squid-users] Squid 5.0.3 Cache_Peer Authentication Issue
In-Reply-To: <b02bff7c-43ec-3028-52b7-fcce018c7e02@measurement-factory.com>
References: <5FF76455.8039.284FDE69@Paul.pjb.org.uk>,
 <b02bff7c-43ec-3028-52b7-fcce018c7e02@measurement-factory.com>
Message-ID: <5FF893CF.26686.2CF14D2D@Paul.pjb.org.uk>

Thank you for responding.

> N.B. I assume you do not use SslBump -- the configuration snippet above
> does not show SslBump being used. SslBump does not support what you want
> per commit f5e1794 message.

I am using Ssl-bump, but on a different inbound port and it will never be for traffic heading 
out to this peer.  

There is an entry in my logs much earlier during the process (just before it does 
peer_select) which states:
kid1| 85,5| client_side_request.cc(1444) sslBumpAccessCheck: cannot SslBump this 
request 

Does this use of ssl-bump at any stage preclude the use of cache_peer authentication with 
CONNECT?  My http_port, various acls and ssl_bump statements are all after the config 
extract shown below.

> What kind of HTTP authentication does your client/peer use/expect?

>From the successful GET connections, I can see that the peer requests both a Negotiate 
and NTLM, my clients respond with a Negotiate.

regards,
Paul


On 7 Jan 2021 at 15:18, Alex Rousskov wrote:

> On 1/7/21 2:43 PM, Paul at pjb.org.uk wrote:
> 
> > I am currently using Squid 5.0.3 but have an issue when using a cache_peer (non-squid &  
> > outside my control) that requires authentication.  My Squid server doesn't require 
> > authentication and reading the documentation indicated that I need to set 
> > 'login=PASSTHRU' on my cache_peer line, which I have done.  This has enabled GET 
> > methods to work as expected, but CONNECT methods are failing.  The response from the 
> > peer is a '407' with both methods. 
> 
> > I am controlling access to the peer via an acl:
> 
> > ---------------
> > acl localClients src 10.10.1.0/24
> > http_access allow localClients
> > 
> > acl aclREDIRECT dstdomain "/etc/squid/redirect.txt"
> > cache_peer 10.10.10.167 parent 8080 0 no-query name=peerREDIRECT login=PASSTHRU 
> > connection-auth=on
> > cache_peer_access peerREDIRECT allow aclREDIRECT
> > cache_peer_access peerREDIRECT deny !aclREDIRECT
> > never_direct allow aclREDIRECT
> > always_direct deny aclREDIRECT
> > always_direct allow all
> > 
> > http_port 80 connection-auth=on
> > ---------------
> 
> 
> > An extract from my logs showing the failure:
> > kid1| 11,2| HttpTunneler.cc(326) handleResponse: Tunnel Server RESPONSE:
> > <!-- default "Proxy Authorization Required" response (407) -->----------
> > kid1| 83,3| HttpTunneler.cc(345) bailOnResponseError: unsupported CONNECT response 
> > status code [state:w FD 17 job22]
> 
> > Is this a mis-configuration? or have I mis-understood how cache_peer works?
> 
> N.B. I assume you do not use SslBump -- the configuration snippet above
> does not show SslBump being used. SslBump does not support what you want
> per commit f5e1794 message.
> 
> What kind of HTTP authentication does your client/peer use/expect?
> 
> The 407 response from the peer may be normal/expected (a part of HTTP
> authentication) or indicate a problem. If you do not get better
> suggestions, please show us the CONNECT request and response headers,
> exchanged between the client and your Squid and between your Squid and
> cache_peer (i.e. 4 headers total). You can use tools like
> tcpdump/wireshark to collect/render those plain text headers.
> 
> Alex.




From rousskov at measurement-factory.com  Fri Jan  8 19:27:36 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 8 Jan 2021 14:27:36 -0500
Subject: [squid-users] Squid 5.0.3 Cache_Peer Authentication Issue
In-Reply-To: <5FF893CF.26686.2CF14D2D@Paul.pjb.org.uk>
References: <5FF76455.8039.284FDE69@Paul.pjb.org.uk>
 <b02bff7c-43ec-3028-52b7-fcce018c7e02@measurement-factory.com>
 <5FF893CF.26686.2CF14D2D@Paul.pjb.org.uk>
Message-ID: <b5c52495-0a09-a279-fa21-b89b98dc7ef1@measurement-factory.com>

On 1/8/21 12:18 PM, Paul at pjb.org.uk wrote:

> I am using Ssl-bump, but on a different inbound port and it will never be for traffic heading 
> out to this peer.

My assumption is correct then -- bugs notwithstanding, we can ignore
SslBump limitations here because they do not apply to your use case.


> From the successful GET connections, I can see that the peer requests both a Negotiate 
> and NTLM, my clients respond with a Negotiate.

AFAICT, Squid documentation implies that Negotiate PASSTHRU
authentication should work in combination with connection-auth=on (which
you do have). This is not my area of expertise, but my previous
recommendation regarding sharing four CONNECT-related headers stands.

Alex.


> On 7 Jan 2021 at 15:18, Alex Rousskov wrote:
> 
>> On 1/7/21 2:43 PM, Paul at pjb.org.uk wrote:
>>
>>> I am currently using Squid 5.0.3 but have an issue when using a cache_peer (non-squid &  
>>> outside my control) that requires authentication.  My Squid server doesn't require 
>>> authentication and reading the documentation indicated that I need to set 
>>> 'login=PASSTHRU' on my cache_peer line, which I have done.  This has enabled GET 
>>> methods to work as expected, but CONNECT methods are failing.  The response from the 
>>> peer is a '407' with both methods. 
>>
>>> I am controlling access to the peer via an acl:
>>
>>> ---------------
>>> acl localClients src 10.10.1.0/24
>>> http_access allow localClients
>>>
>>> acl aclREDIRECT dstdomain "/etc/squid/redirect.txt"
>>> cache_peer 10.10.10.167 parent 8080 0 no-query name=peerREDIRECT login=PASSTHRU 
>>> connection-auth=on
>>> cache_peer_access peerREDIRECT allow aclREDIRECT
>>> cache_peer_access peerREDIRECT deny !aclREDIRECT
>>> never_direct allow aclREDIRECT
>>> always_direct deny aclREDIRECT
>>> always_direct allow all
>>>
>>> http_port 80 connection-auth=on
>>> ---------------
>>
>>
>>> An extract from my logs showing the failure:
>>> kid1| 11,2| HttpTunneler.cc(326) handleResponse: Tunnel Server RESPONSE:
>>> <!-- default "Proxy Authorization Required" response (407) -->----------
>>> kid1| 83,3| HttpTunneler.cc(345) bailOnResponseError: unsupported CONNECT response 
>>> status code [state:w FD 17 job22]
>>
>>> Is this a mis-configuration? or have I mis-understood how cache_peer works?
>>
>> N.B. I assume you do not use SslBump -- the configuration snippet above
>> does not show SslBump being used. SslBump does not support what you want
>> per commit f5e1794 message.
>>
>> What kind of HTTP authentication does your client/peer use/expect?
>>
>> The 407 response from the peer may be normal/expected (a part of HTTP
>> authentication) or indicate a problem. If you do not get better
>> suggestions, please show us the CONNECT request and response headers,
>> exchanged between the client and your Squid and between your Squid and
>> cache_peer (i.e. 4 headers total). You can use tools like
>> tcpdump/wireshark to collect/render those plain text headers.
>>
>> Alex.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From roeeklinger60 at gmail.com  Sun Jan 10 15:24:50 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Sun, 10 Jan 2021 17:24:50 +0200
Subject: [squid-users] How do I rotate access.log?
In-Reply-To: <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
References: <CAGCa14roPU4h0n=Z_f8VXFWKAU2s7wL53LUnS_qMmFJ9pOOVkg@mail.gmail.com>
 <19017c1c-ab07-1693-d5a0-3af88afc13f7@measurement-factory.com>
 <CAGCa14rfbj0CgavkbstQNYE0zp519x1J65aaFGCPsD=LjdR98A@mail.gmail.com>
 <32d4ef6f-583a-40b1-80a0-148f945d0788@measurement-factory.com>
 <CAGCa14o_pNEPRUkY2-GwzfW0KGpszCeuNyrOw84JhNQLzTyi8g@mail.gmail.com>
 <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
Message-ID: <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>

Hey,

I just wanted to give an update in case anyone is interested, I was not
able to find a solution,
Instead, I set "logfile_rotate 0" and wrote my own custom script to rotate
the logs and I am running it as a cron, works just fine.

Thanks for trying to help.



On Fri, Jan 1, 2021 at 1:38 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 12/31/20 1:39 PM, roee klinger wrote:
>
> >     2020/12/31 20:33:49 kid1| Logfile: opening log
> daemon:/var/log/squid/access.log
> >     2020/12/31 20:33:49 kid1| Logfile Daemon: opening log
> /var/log/squid/access.log
> >     2020/12/31 20:33:49 kid1| Store logging disabled
> >     2020/12/31 20:33:57 kid1| logfileRotate:
> daemon:/var/log/squid/access.log
> >     2020/12/31 20:33:57 kid1| logfileRotate:
> daemon:/var/log/squid/access.log
> >     2020/12/31 20:33:57 kid1| Logfile: opening log
> daemon:/var/log/squid/access.log
> >     2020/12/31 20:33:57 kid1| Logfile Daemon: opening log
> /var/log/squid/access.log
> >     2020/12/31 20:33:57 kid1| Store logging disabled
>
> The second set of the "opening log" lines at 20:33:57 concern me -- why
> would somebody start opening those files when you are asking Squid to
> rotate the logs. However, this could be a red herring. Do you get the
> same kind of output when you send USR1 signal to the process identifier
> in the PID file (instead of running "squid -k rotate")?
>
>
> > Any tips?
>
> I have not looked at v4.6 code, but I do not see anything in the more
> recent code that would make the visible effects of access.log rotation
> conditional except setting logfile_rotate to zero. I also do not see any
> obviously relevant changes in v4 change.log (although there was one
> access-logging bug fixed).
>
> A few thing could go wrong. If you do not get better advice, I can
> suggest the following:
>
> * If you are a developer, I would recommend attaching a debugger to the
> logging daemon process to (a) make sure it gets the rotation command
> from Squid and (b) to understand why it ignores that command.
>
> * If you are a sysadmin, you may be able to attach strace to the logging
> daemon process and share its output. This is best done without user
> traffic going through Squid to avoid accidentally sharing user info.
> Here are rough steps:
>
> 1. Attach strace to the running daemon process (-p). Configure strace to
> log at least 100 bytes of system call data (-s 100). Tell strace to
> write the output into a file.
>
> 2. Rotate.
>
> 3. Wait a few seconds.
>
> 4. Stop strace. Compress and share a link to its output file.
>
>
> Cheers,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210110/3ab004e8/attachment.htm>

From roeeklinger60 at gmail.com  Sun Jan 10 15:32:54 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Sun, 10 Jan 2021 17:32:54 +0200
Subject: [squid-users] cache_peer selection based on username
Message-ID: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>

Hey,

I am trying to figure out the best way to select cache peers based on the
client username, I have read extensively but I cannot figure out the best
way to do it.

so far I have:

external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a
script.sh
acl whitelisted_users external user_whitelist_external
http_access allow whitelisted_users


and:

nonhierarchical_direct off
never_direct allow all
cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
cache_peer_access proxy1 allow whitelisted_users
cache_peer_access proxy0.2 deny all
cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
cache_peer_access proxy2 allow whitelisted_users
cache_peer_access proxy0.3 deny all

ideally, script.sh checks if the request is authinticated and if it is, it
selects the cache peer to use, is there some kind of way to achieve this
with "Defined keywords" to select which cache peer to use or am I looking
at this the wrong way?

What would be the best way to accomplish this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210110/7a5fc519/attachment.htm>

From ngtech1ltd at gmail.com  Sun Jan 10 15:35:36 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 10 Jan 2021 17:35:36 +0200
Subject: [squid-users] How do I rotate access.log?
In-Reply-To: <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
References: <CAGCa14roPU4h0n=Z_f8VXFWKAU2s7wL53LUnS_qMmFJ9pOOVkg@mail.gmail.com>
 <19017c1c-ab07-1693-d5a0-3af88afc13f7@measurement-factory.com>
 <CAGCa14rfbj0CgavkbstQNYE0zp519x1J65aaFGCPsD=LjdR98A@mail.gmail.com>
 <32d4ef6f-583a-40b1-80a0-148f945d0788@measurement-factory.com>
 <CAGCa14o_pNEPRUkY2-GwzfW0KGpszCeuNyrOw84JhNQLzTyi8g@mail.gmail.com>
 <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
 <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
Message-ID: <001901d6e766$3e502ad0$baf08070$@gmail.com>

Have you tried to use the OS logrotate?

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of roee klinger
Sent: Sunday, January 10, 2021 5:25 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] How do I rotate access.log?

 

Hey,

 

I just wanted to give an update in case anyone is interested, I was not able to find a solution, 

Instead, I set "logfile_rotate 0" and wrote my own custom script to rotate the logs and I am running it as a cron, works just fine.

 

Thanks for trying to help.

 

 

 

On Fri, Jan 1, 2021 at 1:38 AM Alex Rousskov <rousskov at measurement-factory.com <mailto:rousskov at measurement-factory.com> > wrote:

On 12/31/20 1:39 PM, roee klinger wrote:

>     2020/12/31 20:33:49 kid1| Logfile: opening log daemon:/var/log/squid/access.log
>     2020/12/31 20:33:49 kid1| Logfile Daemon: opening log /var/log/squid/access.log
>     2020/12/31 20:33:49 kid1| Store logging disabled
>     2020/12/31 20:33:57 kid1| logfileRotate: daemon:/var/log/squid/access.log
>     2020/12/31 20:33:57 kid1| logfileRotate: daemon:/var/log/squid/access.log
>     2020/12/31 20:33:57 kid1| Logfile: opening log daemon:/var/log/squid/access.log
>     2020/12/31 20:33:57 kid1| Logfile Daemon: opening log /var/log/squid/access.log
>     2020/12/31 20:33:57 kid1| Store logging disabled

The second set of the "opening log" lines at 20:33:57 concern me -- why
would somebody start opening those files when you are asking Squid to
rotate the logs. However, this could be a red herring. Do you get the
same kind of output when you send USR1 signal to the process identifier
in the PID file (instead of running "squid -k rotate")?


> Any tips?

I have not looked at v4.6 code, but I do not see anything in the more
recent code that would make the visible effects of access.log rotation
conditional except setting logfile_rotate to zero. I also do not see any
obviously relevant changes in v4 change.log (although there was one
access-logging bug fixed).

A few thing could go wrong. If you do not get better advice, I can
suggest the following:

* If you are a developer, I would recommend attaching a debugger to the
logging daemon process to (a) make sure it gets the rotation command
from Squid and (b) to understand why it ignores that command.

* If you are a sysadmin, you may be able to attach strace to the logging
daemon process and share its output. This is best done without user
traffic going through Squid to avoid accidentally sharing user info.
Here are rough steps:

1. Attach strace to the running daemon process (-p). Configure strace to
log at least 100 bytes of system call data (-s 100). Tell strace to
write the output into a file.

2. Rotate.

3. Wait a few seconds.

4. Stop strace. Compress and share a link to its output file.


Cheers,

Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210110/1828deb5/attachment.htm>

From ngtech1ltd at gmail.com  Sun Jan 10 15:36:41 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 10 Jan 2021 17:36:41 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
Message-ID: <001e01d6e766$656ab220$30401660$@gmail.com>

You should use a note acl for that.

When you return the whitelisted client you should add a note which can be 1-100 or any other static string.

 

It works just out of the box.

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of roee klinger
Sent: Sunday, January 10, 2021 5:33 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] cache_peer selection based on username

 

Hey,

 

I am trying to figure out the best way to select cache peers based on the client username, I have read extensively but I cannot figure out the best way to do it.

 

so far I have:

external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a script.sh

acl whitelisted_users external user_whitelist_external

http_access allow whitelisted_users

 

and:

nonhierarchical_direct off

never_direct allow all

cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1

cache_peer_access proxy1 allow whitelisted_users

cache_peer_access proxy0.2 deny all

cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2

cache_peer_access proxy2 allow whitelisted_users

cache_peer_access proxy0.3 deny all

 

ideally, script.sh checks if the request is authinticated and if it is, it selects the cache peer to use, is there some kind of way to achieve this with "Defined keywords" to select which cache peer to use or am I looking at this the wrong way?

 

What would be the best way to accomplish this?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210110/c66aede4/attachment.htm>

From roeeklinger60 at gmail.com  Sun Jan 10 15:51:05 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Sun, 10 Jan 2021 17:51:05 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <001e01d6e766$656ab220$30401660$@gmail.com>
References: <001e01d6e766$656ab220$30401660$@gmail.com>
Message-ID: <803EF177-F354-4AAB-943E-2E32CCF82BC7@gmail.com>

So basically I return a note with the ?OK? response, which can be any string, for example ?100?.

Then, I can use ?100? as a normal ACL in squid.conf?

Thanks



> On Jan 10, 2021, at 17:36, Eliezer Croitoru <ngtech1ltd at gmail.com> wrote:
> 
> ?
> You should use a note acl for that.
> When you return the whitelisted client you should add a note which can be 1-100 or any other static string.
>  
> It works just out of the box.
>  
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> Zoom: Coming soon
>  
>  
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of roee klinger
> Sent: Sunday, January 10, 2021 5:33 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] cache_peer selection based on username
>  
> Hey,
>  
> I am trying to figure out the best way to select cache peers based on the client username, I have read extensively but I cannot figure out the best way to do it.
>  
> so far I have:
> external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a script.sh
> acl whitelisted_users external user_whitelist_external
> http_access allow whitelisted_users
>  
> and:
> nonhierarchical_direct off
> never_direct allow all
> cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
> cache_peer_access proxy1 allow whitelisted_users
> cache_peer_access proxy0.2 deny all
> cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
> cache_peer_access proxy2 allow whitelisted_users
> cache_peer_access proxy0.3 deny all
>  
> ideally, script.sh checks if the request is authinticated and if it is, it selects the cache peer to use, is there some kind of way to achieve this with "Defined keywords" to select which cache peer to use or am I looking at this the wrong way?
>  
> What would be the best way to accomplish this?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210110/0b20d81e/attachment.htm>

From roeeklinger60 at gmail.com  Sun Jan 10 19:06:40 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Sun, 10 Jan 2021 21:06:40 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <001e01d6e766$656ab220$30401660$@gmail.com>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
 <001e01d6e766$656ab220$30401660$@gmail.com>
Message-ID: <CAGCa14qA6Sdjrbf1ZkcFnbP9fPTjZYWmiwAs1J2Yn6uqCh9bfA@mail.gmail.com>

Thanks, Eliezer, I was able to get it working.
Here is an example in case anybody runs into this in the future:

acl mynote1 note mykey note1
acl mynote2 note mykey note2

external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a
script.sh
acl whitelisted_users external user_whitelist_external
http_access allow whitelisted_users

nonhierarchical_direct off
never_direct allow all
cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
cache_peer_access proxy1 allow mynote1
cache_peer_access proxy0.2 deny all
cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
cache_peer_access proxy2 allow mynote2
cache_peer_access proxy0.3 deny all


Then, on the external helper, I return one of these two:

OK mykey=note1
OK mykey=note2


On Sun, Jan 10, 2021 at 5:36 PM Eliezer Croitoru <ngtech1ltd at gmail.com>
wrote:

> You should use a note acl for that.
>
> When you return the whitelisted client you should add a note which can be
> 1-100 or any other static string.
>
>
>
> It works just out of the box.
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> Zoom: Coming soon
>
>
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *roee klinger
> *Sent:* Sunday, January 10, 2021 5:33 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] cache_peer selection based on username
>
>
>
> Hey,
>
>
>
> I am trying to figure out the best way to select cache peers based on the
> client username, I have read extensively but I cannot figure out the best
> way to do it.
>
>
>
> so far I have:
>
> external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a
> script.sh
>
> acl whitelisted_users external user_whitelist_external
>
> http_access allow whitelisted_users
>
>
>
> and:
>
> nonhierarchical_direct off
>
> never_direct allow all
>
> cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
>
> cache_peer_access proxy1 allow whitelisted_users
>
> cache_peer_access proxy0.2 deny all
>
> cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
>
> cache_peer_access proxy2 allow whitelisted_users
>
> cache_peer_access proxy0.3 deny all
>
>
>
> ideally, script.sh checks if the request is authinticated and if it is, it
> selects the cache peer to use, is there some kind of way to achieve this
> with "Defined keywords" to select which cache peer to use or am I looking
> at this the wrong way?
>
>
>
> What would be the best way to accomplish this?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210110/58dbed5a/attachment.htm>

From uhlar at fantomas.sk  Sun Jan 10 19:53:41 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 10 Jan 2021 20:53:41 +0100
Subject: [squid-users] How do I rotate access.log?
In-Reply-To: <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
References: <CAGCa14roPU4h0n=Z_f8VXFWKAU2s7wL53LUnS_qMmFJ9pOOVkg@mail.gmail.com>
 <19017c1c-ab07-1693-d5a0-3af88afc13f7@measurement-factory.com>
 <CAGCa14rfbj0CgavkbstQNYE0zp519x1J65aaFGCPsD=LjdR98A@mail.gmail.com>
 <32d4ef6f-583a-40b1-80a0-148f945d0788@measurement-factory.com>
 <CAGCa14o_pNEPRUkY2-GwzfW0KGpszCeuNyrOw84JhNQLzTyi8g@mail.gmail.com>
 <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
 <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
Message-ID: <20210110195341.GB17841@fantomas.sk>

On 10.01.21 17:24, roee klinger wrote:
>I just wanted to give an update in case anyone is interested, I was not
>able to find a solution,

it was posted here:
http://lists.squid-cache.org/pipermail/squid-users/2020-December/023074.html

>> 	rotate=N		Specifies the number of log file rotations to
>> 				make when you run 'squid -k rotate'. [...]
>> 				Only supported by the stdio module
>
>You are not using an "stdio" module. You are using a "daemon" module.

simply said, it could not work in your case, 

>Instead, I set "logfile_rotate 0" and wrote my own custom script to rotate
>the logs and I am running it as a cron, works just fine.

isn't this the default in raspbian? Afaik it comes from debian, where this
is the default.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Support bacteria - they're the only culture some people have.


From ngtech1ltd at gmail.com  Mon Jan 11 07:05:50 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 11 Jan 2021 09:05:50 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
Message-ID: <CABA8h=Rcmpn61+=P-HP0Wa39H6wf4N3gAvZ-=jhJmKptf1-wdw@mail.gmail.com>

Squid provides the acl login or username.
http://www.squid-cache.org/Doc/config/acl/

should have maybe ident.
you will need to include a usernames file which contains them.

I believe a note in a helper should do that better.

Eliezer

On Sun, Jan 10, 2021, 17:33 roee klinger <roeeklinger60 at gmail.com> wrote:

> Hey,
>
> I am trying to figure out the best way to select cache peers based on the
> client username, I have read extensively but I cannot figure out the best
> way to do it.
>
> so far I have:
>
> external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a
> script.sh
> acl whitelisted_users external user_whitelist_external
> http_access allow whitelisted_users
>
>
> and:
>
> nonhierarchical_direct off
> never_direct allow all
> cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
> cache_peer_access proxy1 allow whitelisted_users
> cache_peer_access proxy0.2 deny all
> cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
> cache_peer_access proxy2 allow whitelisted_users
> cache_peer_access proxy0.3 deny all
>
> ideally, script.sh checks if the request is authinticated and if it is, it
> selects the cache peer to use, is there some kind of way to achieve this
> with "Defined keywords" to select which cache peer to use or am I looking
> at this the wrong way?
>
> What would be the best way to accomplish this?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210111/6a367389/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan 11 09:03:06 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 11 Jan 2021 11:03:06 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <803EF177-F354-4AAB-943E-2E32CCF82BC7@gmail.com>
References: <001e01d6e766$656ab220$30401660$@gmail.com>
 <803EF177-F354-4AAB-943E-2E32CCF82BC7@gmail.com>
Message-ID: <003701d6e7f8$93d90f70$bb8b2e50$@gmail.com>

In the next example I wrote a whole setup:

https://github.com/elico/vagrant-squid-outgoing-addresses

 

Specifically it would look something like:

https://github.com/elico/vagrant-squid-outgoing-addresses/blob/master/shared/note.rb#L82

 

it?s as a line like:

echo ?OK x_note=100 ip=100?

 

The in squid use an acl like this:

https://github.com/elico/vagrant-squid-outgoing-addresses/blob/9221a73394ced582fec84bc42abfaae3c9a364b3/shared/collect-32-subnet-addresses.rb#L17

 

ie:

echo "acl #{ip_map[key]} note ip #{acl_name.match(/([0-9]+)/)[1]}" |tee -a /etc/squid/conf.d/acl-to-ip.conf

 

It?s better to run the lab and see the content of the conf files to understand it.

You will need VirtualBox and Vagrant to power up this lab.

 

Later I might be able to record a video of this but not sure yet about this.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: roee klinger <roeeklinger60 at gmail.com> 
Sent: Sunday, January 10, 2021 5:51 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
Subject: Re: [squid-users] cache_peer selection based on username

 

So basically I return a note with the ?OK? response, which can be any string, for example ?100?.

 

Then, I can use ?100? as a normal ACL in squid.conf?

 

Thanks

 

 





On Jan 10, 2021, at 17:36, Eliezer Croitoru <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > wrote:

?

You should use a note acl for that.

When you return the whitelisted client you should add a note which can be 1-100 or any other static string.

 

It works just out of the box.

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of roee klinger
Sent: Sunday, January 10, 2021 5:33 PM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] cache_peer selection based on username

 

Hey,

 

I am trying to figure out the best way to select cache peers based on the client username, I have read extensively but I cannot figure out the best way to do it.

 

so far I have:

external_acl_type user_whitelist_external children-max=20 ttl=300 %>lp %>a script.sh

acl whitelisted_users external user_whitelist_external

http_access allow whitelisted_users

 

and:

nonhierarchical_direct off

never_direct allow all

cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1

cache_peer_access proxy1 allow whitelisted_users

cache_peer_access proxy0.2 deny all

cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2

cache_peer_access proxy2 allow whitelisted_users

cache_peer_access proxy0.3 deny all

 

ideally, script.sh checks if the request is authinticated and if it is, it selects the cache peer to use, is there some kind of way to achieve this with "Defined keywords" to select which cache peer to use or am I looking at this the wrong way?

 

What would be the best way to accomplish this?

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210111/5d67ad6e/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan 11 13:46:01 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 11 Jan 2021 15:46:01 +0200
Subject: [squid-users] What is the state of V5 branch? Can I try to
 publish some RPMS?
In-Reply-To: <f971c5f5-0c07-bb95-235c-41365a145ac0@treenet.co.nz>
References: <!&!AAAAAAAAAAAuAAAAAAAAAGg7dk9mYqJGhbBt3m/ghGwBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC83OVyby6rRYk1AIQfIcCyAQAAAAA=@gmail.com>
 <f971c5f5-0c07-bb95-235c-41365a145ac0@treenet.co.nz>
Message-ID: <003e01d6e820$19de60d0$4d9b2270$@gmail.com>

We are now less one bug then before, just 3 to go:
  <https://bugs.squid-cache.org/show_bug.cgi?id=4806>
  <https://bugs.squid-cache.org/show_bug.cgi?id=4832>

  <http://bugs.squid-cache.org/show_bug.cgi?id=5055>

Eliezer
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Saturday, December 19, 2020 11:11 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] What is the state of V5 branch? Can I try to publish some RPMS?

On 17/12/20 10:28 pm, Eliezer Croitor wrote:
> Hey,
> 
> Next year I will start publishing RPMs for Squid again.
> 
> What is the state of V5? What should be verified or tested with V5?
> 


Status of Squid-5 is that there are three major bugs to be resolved or 
proven not to be as important as they seem now (limited impact or 
existence in older Squid versions):

  <https://bugs.squid-cache.org/show_bug.cgi?id=4806>
  <https://bugs.squid-cache.org/show_bug.cgi?id=4832>
  <https://bugs.squid-cache.org/show_bug.cgi?id=4872>


When those are resolved it will be reviewed for production release.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Mon Jan 11 14:16:15 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 11 Jan 2021 16:16:15 +0200
Subject: [squid-users] no src IP in access log for locally generated requests
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAANXJRMoPSZBAnE3xIcS5teABAAAAAA==@gmail.com>

I have in my logs:

1610372657.529      0 - TCP_DENIED/403 3638 GET
http://crl.kaspersky.com/aia/KSNGlobalRootCAECC.crt - HIER_NONE/- text/html
-

 

And it means probably that squid is generating these requests.

What ACL can I use to allow this?

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210111/17f9703f/attachment.htm>

From Walter.H at mathemainzel.info  Mon Jan 11 14:19:53 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 11 Jan 2021 15:19:53 +0100
Subject: [squid-users] distinguish between IPv4 and IPv6
Message-ID: <c96d58ce-3251-4d98-31b2-61e3619f6827@mathemainzel.info>

Hello,

is there a way, that I can do something like

if ( dst is IPv4 ) go direct
if ( dst is IPv6 ) use parent proxy xxx

The reason for my question, I'm using a IPv6-in-IPv4 tunnel,
and it would make sense to forward all traffic going to IPv6 to squid 
running on tunnel end;

Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3511 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210111/cb926a69/attachment.bin>

From rousskov at measurement-factory.com  Mon Jan 11 15:11:46 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 11 Jan 2021 10:11:46 -0500
Subject: [squid-users] no src IP in access log for locally generated
 requests
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAANXJRMoPSZBAnE3xIcS5teABAAAAAA==@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAANXJRMoPSZBAnE3xIcS5teABAAAAAA==@gmail.com>
Message-ID: <06f6468f-9140-fa83-6ba9-4a188c845bb5@measurement-factory.com>

On 1/11/21 9:16 AM, Eliezer Croitoru wrote:
> I have in my logs:

> 1610372657.529????? 0 - TCP_DENIED/403 3638 GET
> http://crl.kaspersky.com/aia/KSNGlobalRootCAECC.crt - HIER_NONE/-
> text/html ?

> And it means probably that squid is generating these requests.

> What ACL can I use to allow this?

To match internal requests, consider using the "internal" initiator with
the transaction_initiator ACL.

Alex.


From squid3 at treenet.co.nz  Mon Jan 11 20:10:18 2021
From: squid3 at treenet.co.nz (=?UTF-8?B?4oCqQW1vcyBKZWZmcmllc+KArA==?=)
Date: Tue, 12 Jan 2021 09:10:18 +1300
Subject: [squid-users] distinguish between IPv4 and IPv6
References: <c96d58ce-3251-4d98-31b2-61e3619f6827@mathemainzel.info>
Message-ID: <yvc1n6htd3xj-6eodnf29tt45vao1akq32o5-wjz1w61o5845-eg4r2dona1dz9zt4mv-5l909r-beatalb88jpusooh5f-ox8r71-1xqalp0f5eh5a0526-1rhgxo-ekl6jx-5hjc6j-5x67yz45yx8j.1610395802292@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/69360d28/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan 11 20:29:53 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 11 Jan 2021 22:29:53 +0200
Subject: [squid-users] distinguish between IPv4 and IPv6
In-Reply-To: <yvc1n6htd3xj-6eodnf29tt45vao1akq32o5-wjz1w61o5845-eg4r2dona1dz9zt4mv-5l909r-beatalb88jpusooh5f-ox8r71-1xqalp0f5eh5a0526-1rhgxo-ekl6jx-5hjc6j-5x67yz45yx8j.1610395802292@email.android.com>
References: <c96d58ce-3251-4d98-31b2-61e3619f6827@mathemainzel.info>
 <yvc1n6htd3xj-6eodnf29tt45vao1akq32o5-wjz1w61o5845-eg4r2dona1dz9zt4mv-5l909r-beatalb88jpusooh5f-ox8r71-1xqalp0f5eh5a0526-1rhgxo-ekl6jx-5hjc6j-5x67yz45yx8j.1610395802292@email.android.com>
Message-ID: <000601d6e858$852f3d40$8f8db7c0$@gmail.com>

The detection of an IPV6 available DST can be determined by DNS and external ACL helper.

It will ?slow? down the first couple bytes of the connection but can be much more reliable then the basic ?dst? acl.

The basic test would be something like:

nslookup -type=aaaa www.squid-cache.org -timeout=10 |grep -v '#53'|grep Address:|wc -l

 

if the wc -l gt 0 then try to use IPV6.

 

I believe it?s pretty simple and the main issue is that if a service advertises unreachable IPV6 address.

It can be either because of network misconfiguration or FW or misconfigured DNS.

I have seen all of the above happen in production services in the last year.

 

I can write a helper for this if required.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of ?Amos Jeffries?
Sent: Monday, January 11, 2021 10:10 PM
To: Walter H. <Walter.H at mathemainzel.info>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] distinguish between IPv4 and IPv6

 

The dst ACL type accepts the special value of "ipv4". You can use that and the "!" operator to split traffic.

 

However, please be aware dst is not very reliable until *after* the outgoing connection has been created, and we are still finding some access checks that do not use it correctly. YMMV.

 

Amos


-------- Original message --------
From: "Walter H."
Date: Tue, 12 Jan 2021, 03:19

Hello,

is there a way, that I can do something like

if ( dst is IPv4 ) go direct
if ( dst is IPv6 ) use parent proxy xxx

The reason for my question, I'm using a IPv6-in-IPv4 tunnel,
and it would make sense to forward all traffic going to IPv6 to squid 
running on tunnel end;

Thanks,
Walter



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210111/93d098e0/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan 12 01:23:22 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2021 14:23:22 +1300
Subject: [squid-users] How do I rotate access.log?
In-Reply-To: <20210110195341.GB17841@fantomas.sk>
References: <CAGCa14roPU4h0n=Z_f8VXFWKAU2s7wL53LUnS_qMmFJ9pOOVkg@mail.gmail.com>
 <19017c1c-ab07-1693-d5a0-3af88afc13f7@measurement-factory.com>
 <CAGCa14rfbj0CgavkbstQNYE0zp519x1J65aaFGCPsD=LjdR98A@mail.gmail.com>
 <32d4ef6f-583a-40b1-80a0-148f945d0788@measurement-factory.com>
 <CAGCa14o_pNEPRUkY2-GwzfW0KGpszCeuNyrOw84JhNQLzTyi8g@mail.gmail.com>
 <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
 <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
 <20210110195341.GB17841@fantomas.sk>
Message-ID: <42b8c88c-b799-5baa-f2b9-d010e9db1776@treenet.co.nz>

On 11/01/21 8:53 am, Matus UHLAR - fantomas wrote:
> On 10.01.21 17:24, roee klinger wrote:
>> I just wanted to give an update in case anyone is interested, I was not
>> able to find a solution,
> 
> it was posted here:
> http://lists.squid-cache.org/pipermail/squid-users/2020-December/023074.html 
> 
> 
>>> ????rotate=N??????? Specifies the number of log file rotations to
>>> ??????????????? make when you run 'squid -k rotate'. [...]
>>> ??????????????? Only supported by the stdio module
>>
>> You are not using an "stdio" module. You are using a "daemon" module.
> 
> simply said, it could not work in your case,
>> Instead, I set "logfile_rotate 0" and wrote my own custom script to 
>> rotate
>> the logs and I am running it as a cron, works just fine.
> 
> isn't this the default in raspbian? Afaik it comes from debian, where this
> is the default.
> 

Exactly so. The Debian default is to offload log handling to logrotated. 
Nothing needs to be done by the admin in squid.conf. Raspbian uses the 
Debian package, rebuilt to run on the Pi hardware.

Amos


From squid3 at treenet.co.nz  Tue Jan 12 01:46:01 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2021 14:46:01 +1300
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <CAGCa14qA6Sdjrbf1ZkcFnbP9fPTjZYWmiwAs1J2Yn6uqCh9bfA@mail.gmail.com>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
 <001e01d6e766$656ab220$30401660$@gmail.com>
 <CAGCa14qA6Sdjrbf1ZkcFnbP9fPTjZYWmiwAs1J2Yn6uqCh9bfA@mail.gmail.com>
Message-ID: <be15dfda-2de9-2143-34d9-445f80a7ffcc@treenet.co.nz>

On 11/01/21 8:06 am, roee klinger wrote:
> Thanks, Eliezer, I was able to get it working.
> Here is an example in case anybody?runs into this in the?future:
> 
>     acl mynote1 note mykey note1
>     acl mynote2 note mykey note2
> 

FYI, key names ending with "_" character are reserved for custom keys 
like this.


>     external_acl_type user_whitelist_external children-max=20 ttl=300
>     %>lp %>a script.sh

NP: this does not check for users or authenticated traffic at all. It is 
only using the client-IP and Squid receiving port number.

To meet the earlier stated requirement about authenticated traffic the 
helper format should contain %un. The lines below should follow the 
http_access rules doing authentication checks.


You could also have the helper doing authentication send the notes to 
Squid. eg as a group name.



>     acl whitelisted_users external user_whitelist_external
>     http_access allow whitelisted_users
> 
>     nonhierarchical_direct off
>     never_direct allow all
>     cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
>     cache_peer_access proxy1 allow?mynote1
>     cache_peer_access proxy0.2 deny all
>     cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
>     cache_peer_access proxy2 allow?mynote2
>     cache_peer_access proxy0.3 deny all
> 

NP: there is no peer named "proxy0.2" or "proxy0.3" so those deny lines 
are not doing anything. The only reason this config does what it appears 
at first glance to do, is that the inverted default for the prox1 and 
proxy2 peer access rules default is deny.


> 
> Then, on the external helper, I return one of these two:
> 
>     OK mykey=note1
>     OK mykey=note2
> 
> 


Amos



From vinod9987 at gmail.com  Tue Jan 12 04:44:49 2021
From: vinod9987 at gmail.com (vinod mg)
Date: Tue, 12 Jan 2021 10:14:49 +0530
Subject: [squid-users] Change cipher suite ordering
Message-ID: <CALLSHhyhjwb8Z8wvY6aRC7SHy-va_KRXgtxRwtBV_tn6hQ+_og@mail.gmail.com>

Hello Team,

I need some help in configuring cipher suite ordering. I am using squid
with SSL configs and trying to configure the cipher order but not able to
do so, I am using below sites to check my chipher ordering and its showing
different ordering then what I have configured.

https://www.howsmyssl.com
https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html

Below is my compiled squid details -

squid -v
Squid Cache: Version 5.0.4
Service Name: squid

This binary uses OpenSSL 1.1.1g FIPS  21 Apr 2020. For legal restrictions
on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/app/squid' '--with-openssl'
'--enable-ssl-crtd' '--with-filedescriptors=5000'
'--enable-storeio=diskd,aufs,ufs' '--with-large-files'
'--enable-useragent-log' '--enable-ltdl-convenience' '--with-tls'
'--enable-http-violations'


OS I am using - CentOS Linux release 8.3.2011

I have tried changing the ordering as with below parameters but with no
luck -

http_port 443 tcpkeepalive=60,30,3 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=512MB
tls-cert=/app/squid/etc/certs/ProxyBump.crt
tls-key=/app/squid/etc/certs/ProxyBump.key
cipher=TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:!RC4:!aNULL:!eNULL:!LOW:!MEDIUM:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
options=NO_SSLv3,SINGLE_DH_USE
tls-dh=prime256v1:/app/squid/etc/certs/ProxyBump.pem

tls_outgoing_options min-version=1.2
cipher=TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:EE
CDHE-RSA-CHACHA20-POLY1305:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:!RC4:!aNULL:!eNULL:!LOW:!MEDIUM:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
options=NO_SSLv3,SINGLE_DH_USE

Below is the cipher list order I am expecting but it is not the case.

TLS_AES_128_GCM_SHA256
TLS_AES_256_GCM_SHA384
TLS_CHACHA20_POLY1305_SHA256
TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
TLS_RSA_WITH_AES_128_GCM_SHA256
TLS_RSA_WITH_AES_256_GCM_SHA384
TLS_RSA_WITH_AES_128_CBC_SHA
TLS_RSA_WITH_AES_256_CBC_SHA

Below is my full config file -

#
# Recommended minimum configuration:
#
acl manager proto cache_object
#acl localhost src 127.0.0.1/32
#acl to_localhost dst 127.0.0.0/8

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
#acl localnet src fc00::/7       # RFC 4193 local private network range
#acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl SSL_ports port 8443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl Safe_ports port 8443 # multiling http
acl CONNECT method CONNECT
acl intermediate_fetching transaction_initiator certificate-fetching

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow intermediate_fetching
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access allow all

# Squid normally listens to port 3128
http_port 443 tcpkeepalive=60,30,3 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=512MB
tls-cert=/app/squid/etc/certs/ProxyBump.crt
tls-key=/app/squid/etc/certs/ProxyBump.key
cipher=TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:!RC4:!aNULL:!eNULL:!LOW:!MEDIUM:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
options=NO_SSLv3,SINGLE_DH_USE
tls-dh=prime256v1:/app/squid/etc/certs/ProxyBump.pem

sslcrtd_program /app/squid/etc/libexec/security_file_certgen -s
/var/lib/squid/ssl_db -M 512MB
sslproxy_cert_error allow all
ssl_bump stare all
tls_outgoing_options min-version=1.2
cipher=TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:EE
CDHE-RSA-CHACHA20-POLY1305:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:!RC4:!aNULL:!eNULL:!LOW:!MEDIUM:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
options=NO_SSLv3,SINGLE_DH_USE
cache_mem 1024 MB
# Uncomment and adjust the following to add a disk cache directory.
cache_dir aufs /app/squid/var/cache/squid 1024 16 256
shutdown_lifetime 10 seconds

# Leave coredumps in the first cache dir
coredump_dir /app/squid/var/cache/squid

logformat squid      %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un
%Sh/%<A %mt %>h %Se %>sh
logformat extended %tl %6tr %>a %Ss/%03Hs %<st %rm %ru %un %Sh/%<A %mt %Hs
%<st "%{Referer}>h" "%{User-agent}>h"
server_persistent_connections off
logfile_rotate 30

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

cache_peer 1.2.3.4 parent 8080 0 no-query default login=abc:xyz
never_direct allow all

cache_log /app/squid/var/logs/cache.log
access_log /app/squid/var/logs/access.log
access_log /app/squid/var/logs/access.log.mitm extended
pid_filename /app/squid/var/run/squid.pid
max_filedescriptors 5000

Please let me know, If I am missing anything.

Thanks,
Vinod
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/63d355d0/attachment.htm>

From reskumar at in.ibm.com  Tue Jan 12 05:30:25 2021
From: reskumar at in.ibm.com (Reshma V Kumar)
Date: Tue, 12 Jan 2021 05:30:25 +0000
Subject: [squid-users] ERROR connecting to squid proxy server
In-Reply-To: <ef961681-2df4-4733-1eb7-4dce61a6df33@treenet.co.nz>
References: <ef961681-2df4-4733-1eb7-4dce61a6df33@treenet.co.nz>,
 <OF9BF640F1.DE7933D6-ON00258631.002172E2-00258631.002855EE@notes.na.collabserv.com>
Message-ID: <OF3011B29D.ACC63CA5-ON0025865B.001DCB43-0025865B.001E4057@notes.na.collabserv.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/0c1bb279/attachment.htm>

From ngtech1ltd at gmail.com  Tue Jan 12 08:17:44 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 10:17:44 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <be15dfda-2de9-2143-34d9-445f80a7ffcc@treenet.co.nz>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
 <001e01d6e766$656ab220$30401660$@gmail.com>
 <CAGCa14qA6Sdjrbf1ZkcFnbP9fPTjZYWmiwAs1J2Yn6uqCh9bfA@mail.gmail.com>
 <be15dfda-2de9-2143-34d9-445f80a7ffcc@treenet.co.nz>
Message-ID: <000d01d6e8bb$6840bd90$38c238b0$@gmail.com>

Hey Amos,

One thing that the auth helper cannot do with this note is the ttl.
The auth ttl is different then the request IP binding/routing.
With separated auth and external_acl helper you can change/apply a note/rule/acl in a lower ttl
ie 3 seconds which can be critical to some applications.
If one ip goes down for any reason you can change the routing.
I would have expected for the note to stick if the ttl is either 0 or 1 for the relevant session.
This so we would rely on the helper to be "live" helper per request.

I know that 0-3 is almost the same like 0-5 but some prefer to use 0-1.

Eliezer 

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, January 12, 2021 3:46 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cache_peer selection based on username

On 11/01/21 8:06 am, roee klinger wrote:
> Thanks, Eliezer, I was able to get it working.
> Here is an example in case anybody runs into this in the future:
> 
>     acl mynote1 note mykey note1
>     acl mynote2 note mykey note2
> 

FYI, key names ending with "_" character are reserved for custom keys 
like this.


>     external_acl_type user_whitelist_external children-max=20 ttl=300
>     %>lp %>a script.sh

NP: this does not check for users or authenticated traffic at all. It is 
only using the client-IP and Squid receiving port number.

To meet the earlier stated requirement about authenticated traffic the 
helper format should contain %un. The lines below should follow the 
http_access rules doing authentication checks.


You could also have the helper doing authentication send the notes to 
Squid. eg as a group name.



>     acl whitelisted_users external user_whitelist_external
>     http_access allow whitelisted_users
> 
>     nonhierarchical_direct off
>     never_direct allow all
>     cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
>     cache_peer_access proxy1 allow mynote1
>     cache_peer_access proxy0.2 deny all
>     cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
>     cache_peer_access proxy2 allow mynote2
>     cache_peer_access proxy0.3 deny all
> 

NP: there is no peer named "proxy0.2" or "proxy0.3" so those deny lines 
are not doing anything. The only reason this config does what it appears 
at first glance to do, is that the inverted default for the prox1 and 
proxy2 peer access rules default is deny.


> 
> Then, on the external helper, I return one of these two:
> 
>     OK mykey=note1
>     OK mykey=note2
> 
> 


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Tue Jan 12 08:33:00 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 10:33:00 +0200
Subject: [squid-users] Microsoft store issues with ssl-bump
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>

I am trying to implement a full SSL-BUMP and I am having trouble with MS
Store.

The Windows 10 MS Store tries to connect the domains:

storeedgefd.dsx.mp.microsoft.com

 

which is bypassed from SSL BUMP with a regex and server-name.

For some reason the store claims that there is an issue with the connection.

To resolve this issue what I am dong is resolving the domain:
storeedgefd.dsx.mp.microsoft.com

And bypassing it from squid interception which is done with an
nftables/iptables REDIRECT.

 

I believe I am not the first to encounter such an issue and looking for a
way to resolve this issue without
the dns+fw level bypass.

 

Any hints might help to find and resolve this issue

 

Thanks,

Eliezer

 

*	Squid 5.0.4 on Fedora 33.
*	I can generate a "support file" which contains all squid conf for
reproduction of the issue by the dev team.

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/e6fcfb90/attachment.htm>

From l.marcantonio at proxind.it  Tue Jan 12 08:57:44 2021
From: l.marcantonio at proxind.it (Lorenzo Marcantonio)
Date: Tue, 12 Jan 2021 09:57:44 +0100
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
Message-ID: <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>

On Tue, Jan 12, 2021 at 10:33:00AM +0200, Eliezer Croitoru wrote:
>
> Any hints might help to find and resolve this issue

From my experience MS Update and probably the store too use custom root
certificates; check if that's the case. It's also possible that that
connection is so hardwired that it doesn't accept a redirect. So it sees
that and become suspicious (Windows Update is extremely suspicious :D)

For some antivirus (avast maybe? I don't remember) the updater actually
checks the server certificate fingerprint so you can't bump it and you
need a special NAT rule for all the fscking IPs it uses (if you set a
proxy it does a connect BY IP and not by name, and the IPs are hardcoded
and not resolved by DNS).

So it is possible you can't bump a store connection (remember that
technically a bump is a MITM intrusion that TLS is explicitely design to
detect!)

-- 
Lorenzo Marcantonio
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/b8031d33/attachment.sig>

From ngtech1ltd at gmail.com  Tue Jan 12 09:09:17 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 11:09:17 +0200
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
Message-ID: <026e01d6e8c2$9bb7c5e0$d32751a0$@gmail.com>

Even with splice???
This is a weird way of MS Store of handling things

I was sure that when I am using SPLICE it is expected to work.
Maybe there is a way to handle these IP addresses before even peeking, which
should work.
I think that there is some level of a BUMP happening when it shouldn't.
I will try to test it with another proxy which only looks at the SNI.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Lorenzo Marcantonio
Sent: Tuesday, January 12, 2021 10:58 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Microsoft store issues with ssl-bump

On Tue, Jan 12, 2021 at 10:33:00AM +0200, Eliezer Croitoru wrote:
>
> Any hints might help to find and resolve this issue

>From my experience MS Update and probably the store too use custom root
certificates; check if that's the case. It's also possible that that
connection is so hardwired that it doesn't accept a redirect. So it sees
that and become suspicious (Windows Update is extremely suspicious :D)

For some antivirus (avast maybe? I don't remember) the updater actually
checks the server certificate fingerprint so you can't bump it and you
need a special NAT rule for all the fscking IPs it uses (if you set a
proxy it does a connect BY IP and not by name, and the IPs are hardcoded
and not resolved by DNS).

So it is possible you can't bump a store connection (remember that
technically a bump is a MITM intrusion that TLS is explicitely design to
detect!)

-- 
Lorenzo Marcantonio



From ngtech1ltd at gmail.com  Tue Jan 12 09:15:36 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 11:15:36 +0200
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
Message-ID: <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>

This works in another proxy which looks at the SNI only without any bump
involved.
Remember that Squid should splice the connection based on regex and
server-name dst.

On the other proxy this is what I have:
Jan 12 11:12:46 ndpi-fw proxy[497]: 2021/01/12 11:12:46 conn
192.168.189.X:64632 - 104.79.221.20:443 released
[storeedgefd.dsx.mp.microsoft.com:443]
Jan 12 11:12:46 ndpi-fw proxy[497]: 2021/01/12 11:12:46 conn
192.168.189.X:64633 - 104.79.221.20:443 released
[storeedgefd.dsx.mp.microsoft.com:443]
Jan 12 11:12:46 ndpi-fw proxy[497]: 2021/01/12 11:12:46 conn
192.168.189.X:64634 - 104.79.221.20:443 released
[storeedgefd.dsx.mp.microsoft.com:443]
Jan 12 11:12:46 ndpi-fw proxy[497]: 2021/01/12 11:12:46 conn
192.168.189.X:64630 - 104.79.221.20:443 released
[storeedgefd.dsx.mp.microsoft.com:443]
Jan 12 11:12:46 ndpi-fw proxy[497]: 2021/01/12 11:12:46 conn
192.168.189.X:64631 - 104.79.221.20:443 released
[storeedgefd.dsx.mp.microsoft.com:443]
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54
SNI:https://storeedgefd.dsx.mp.microsoft.com:443
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54 use parent : false,
storeedgefd.dsx.mp.microsoft.com:443
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54 ip 192.168.189.X
rate, current: 1/s, max: 20/s
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54 conn
192.168.189.X:64667 - 104.79.221.20:443 connected
[storeedgefd.dsx.mp.microsoft.com:443]
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54
SNI:https://storeedgefd.dsx.mp.microsoft.com:443
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54 use parent : false,
storeedgefd.dsx.mp.microsoft.com:443
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54 ip 192.168.189.X
rate, current: 2/s, max: 20/s
Jan 12 11:12:54 ndpi-fw proxy[497]: 2021/01/12 11:12:54 conn
192.168.189.X:64669 - 104.79.221.20:443 connected
[storeedgefd.dsx.mp.microsoft.com:443]

So the regex:
storeedgefd\.dsx\.mp\.microsoft\.com

should work.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Lorenzo Marcantonio
Sent: Tuesday, January 12, 2021 10:58 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Microsoft store issues with ssl-bump

On Tue, Jan 12, 2021 at 10:33:00AM +0200, Eliezer Croitoru wrote:
>
> Any hints might help to find and resolve this issue

>From my experience MS Update and probably the store too use custom root
certificates; check if that's the case. It's also possible that that
connection is so hardwired that it doesn't accept a redirect. So it sees
that and become suspicious (Windows Update is extremely suspicious :D)

For some antivirus (avast maybe? I don't remember) the updater actually
checks the server certificate fingerprint so you can't bump it and you
need a special NAT rule for all the fscking IPs it uses (if you set a
proxy it does a connect BY IP and not by name, and the IPs are hardcoded
and not resolved by DNS).

So it is possible you can't bump a store connection (remember that
technically a bump is a MITM intrusion that TLS is explicitely design to
detect!)

-- 
Lorenzo Marcantonio



From squid3 at treenet.co.nz  Tue Jan 12 09:47:10 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2021 22:47:10 +1300
Subject: [squid-users] Change cipher suite ordering
In-Reply-To: <CALLSHhyhjwb8Z8wvY6aRC7SHy-va_KRXgtxRwtBV_tn6hQ+_og@mail.gmail.com>
References: <CALLSHhyhjwb8Z8wvY6aRC7SHy-va_KRXgtxRwtBV_tn6hQ+_og@mail.gmail.com>
Message-ID: <34306cbe-c404-0fad-d331-0d63fc47f896@treenet.co.nz>

On 12/01/21 5:44 pm, vinod mg wrote:
> Hello Team,
> 
> I need some help in configuring cipher suite ordering. I am using squid 
> with SSL configs and trying to configure the cipher order but not able 
> to do so, I am using below sites to check my chipher ordering and its 
> showing different ordering then what I have configured.
> 
> https://www.howsmyssl.com <https://www.howsmyssl.com>
> https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html 
> <https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html>
> 

These sites show what the client is sending. Modern Squid mimic what the 
Browser sends in as closely as possible to avoid issues being added.


Amos


From squid3 at treenet.co.nz  Tue Jan 12 09:52:57 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2021 22:52:57 +1300
Subject: [squid-users] ERROR connecting to squid proxy server
In-Reply-To: <OF3011B29D.ACC63CA5-ON0025865B.001DCB43-0025865B.001E4057@notes.na.collabserv.com>
References: <ef961681-2df4-4733-1eb7-4dce61a6df33@treenet.co.nz>
 <OF9BF640F1.DE7933D6-ON00258631.002172E2-00258631.002855EE@notes.na.collabserv.com>
 <OF3011B29D.ACC63CA5-ON0025865B.001DCB43-0025865B.001E4057@notes.na.collabserv.com>
Message-ID: <648d25e9-b0da-fccb-ce03-2b59c093ea44@treenet.co.nz>

On 12/01/21 6:30 pm, Reshma V Kumar wrote:
> Hi !
> This is the error from cache.log file
> 2021/01/11 23:21:07 kid1| idnsSendQuery FD -1: sendto: (0) No error.


  "-1" is a closed socket. It looks like there is no UDP port open for 
sending traffic to your DNS server(s).

You are starting Squid with root privileges right?

Amos


From squid3 at treenet.co.nz  Tue Jan 12 09:57:19 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2021 22:57:19 +1300
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <000d01d6e8bb$6840bd90$38c238b0$@gmail.com>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
 <001e01d6e766$656ab220$30401660$@gmail.com>
 <CAGCa14qA6Sdjrbf1ZkcFnbP9fPTjZYWmiwAs1J2Yn6uqCh9bfA@mail.gmail.com>
 <be15dfda-2de9-2143-34d9-445f80a7ffcc@treenet.co.nz>
 <000d01d6e8bb$6840bd90$38c238b0$@gmail.com>
Message-ID: <fd74b48d-ba95-3191-af8e-410aac363ddb@treenet.co.nz>

On 12/01/21 9:17 pm, Eliezer Croitoru wrote:
> Hey Amos,
> 
> One thing that the auth helper cannot do with this note is the ttl.
> The auth ttl is different then the request IP binding/routing.

That can be added in via the the key_extras detail.

Though I am still worried that the OP *only* asked about routing by 
"username" then their apparently working solution has nothing to do with 
users or usernames at all.


Amos


From squid3 at treenet.co.nz  Tue Jan 12 10:13:02 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Jan 2021 23:13:02 +1300
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
Message-ID: <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>

On 12/01/21 10:15 pm, Eliezer Croitoru wrote:
> This works in another proxy which looks at the SNI only without any bump
> involved.

So you are saying you find a bug with Squid?
   or .. ??


Amos


From ngtech1ltd at gmail.com  Tue Jan 12 10:32:41 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Tue, 12 Jan 2021 12:32:41 +0200
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
 <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
Message-ID: <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>

Im saying that my config might be wrong and I will send you a full config
save which can show you the whole setup like most vendors has.
I have upgraded squid in production.

Let me verify first before shouting "bug".

Eliezer

On Tue, Jan 12, 2021, 12:15 Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 12/01/21 10:15 pm, Eliezer Croitoru wrote:
> > This works in another proxy which looks at the SNI only without any bump
> > involved.
>
> So you are saying you find a bug with Squid?
>    or .. ??
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/29defef0/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan 12 12:42:16 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Jan 2021 01:42:16 +1300
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
 <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
 <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>
Message-ID: <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>

On 12/01/21 11:32 pm, NgTech LTD wrote:
> Im saying that my config might be wrong and I will send you a full 
> config save which can show you the whole setup like most vendors has.
> I have upgraded squid in production.
> 
> Let me verify first before shouting "bug".
> 
> Eliezer
> 

Okay. I see a few things to follow up on.


The other proxy logs show SNI as being 
"https://storeedgefd.dsx.mp.microsoft.com:443". SNI should be only a 
name, not a full URL. So if we assume that log is correct the client is 
producing invalid SNI. This may be an issue for Squid, causing it to 
ignore the SNI value entirely.

The openssl tool connecting to the same IP address the other proxy 
claims to be going to gets "sfdataservice.microsoft.com" as the server 
name. In absence of valid SNI to work with that is the name your Squid 
will be trying to match against to decide splice vs bump.


The server prefers to use TLS/1.3 unless explicitly connected to with 
TLS/1.2 immediately. IIRC latest Squid force the client to TLS/1.2 when 
preparing to bump, but may not for spliceand stare. So YMMV.


Amos


From roeeklinger60 at gmail.com  Tue Jan 12 13:06:05 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 12 Jan 2021 15:06:05 +0200
Subject: [squid-users] cache_peer selection based on username
In-Reply-To: <fd74b48d-ba95-3191-af8e-410aac363ddb@treenet.co.nz>
References: <CAGCa14oeRjjkr1O_-NTHJ_ioG=q_cJHhHcoF8LenWXNTESQHfA@mail.gmail.com>
 <001e01d6e766$656ab220$30401660$@gmail.com>
 <CAGCa14qA6Sdjrbf1ZkcFnbP9fPTjZYWmiwAs1J2Yn6uqCh9bfA@mail.gmail.com>
 <be15dfda-2de9-2143-34d9-445f80a7ffcc@treenet.co.nz>
 <000d01d6e8bb$6840bd90$38c238b0$@gmail.com>
 <fd74b48d-ba95-3191-af8e-410aac363ddb@treenet.co.nz>
Message-ID: <CAGCa14q8uPHdUcOx6CrBcymXmusegQ2ZHdf6RWbzbN9SxX1hnw@mail.gmail.com>

Hey Amos,
Thanks, I fixed the keys with the proper "_" character.
Seems like I was in a hurry and did some config mistakes, "proxy0.2" and
"proxy0.3" are supposed to be "proxy1" and "proxy2".
Regarding the helper, I also forgot to mention, I am using 2 helpers, one
for IP whitelisting and one for username authentication,
in the example I provided I am using IP whitelisting, the naming is wrong,
please see the fixed config.

acl mynote1 note mykey_ note1
acl mynote2 note mykey_ note2

external_acl_type IP_whitelist_external children-max=20 ttl=300 %>lp %>a
script.sh
acl whitelisted_IP external IP_whitelist_external
http_access allow whitelisted_IP

nonhierarchical_direct off
never_direct allow all
cache_peer 192.168.8.1 parent 101 0 proxy-only default name=proxy1
cache_peer_access proxy1 allow mynote1
cache_peer_access proxy1 deny all
cache_peer 192.168.8.2 parent 102 0 proxy-only default name=proxy2
cache_peer_access proxy2 allow mynote2
cache_peer_access proxy2 deny all

Then, on the external helper, I return one of these two:

OK mykey=note1
OK mykey=note2

For the authentication helper, I did not look into it but contrary to my
belief it seems auth_param does not support defined keywords,
so I guess I will have to follow your advice by adding %un to
my user_whitelist_external helper, is there any way to do this with
auth_param?
what exactly do you mean to send it as a group name?

Roee.



On Tue, Jan 12, 2021 at 11:59 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 12/01/21 9:17 pm, Eliezer Croitoru wrote:
> > Hey Amos,
> >
> > One thing that the auth helper cannot do with this note is the ttl.
> > The auth ttl is different then the request IP binding/routing.
>
> That can be added in via the the key_extras detail.
>
> Though I am still worried that the OP *only* asked about routing by
> "username" then their apparently working solution has nothing to do with
> users or usernames at all.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/5cb6d614/attachment.htm>

From roeeklinger60 at gmail.com  Tue Jan 12 13:09:47 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 12 Jan 2021 15:09:47 +0200
Subject: [squid-users] How do I rotate access.log?
In-Reply-To: <42b8c88c-b799-5baa-f2b9-d010e9db1776@treenet.co.nz>
References: <CAGCa14roPU4h0n=Z_f8VXFWKAU2s7wL53LUnS_qMmFJ9pOOVkg@mail.gmail.com>
 <19017c1c-ab07-1693-d5a0-3af88afc13f7@measurement-factory.com>
 <CAGCa14rfbj0CgavkbstQNYE0zp519x1J65aaFGCPsD=LjdR98A@mail.gmail.com>
 <32d4ef6f-583a-40b1-80a0-148f945d0788@measurement-factory.com>
 <CAGCa14o_pNEPRUkY2-GwzfW0KGpszCeuNyrOw84JhNQLzTyi8g@mail.gmail.com>
 <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
 <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
 <20210110195341.GB17841@fantomas.sk>
 <42b8c88c-b799-5baa-f2b9-d010e9db1776@treenet.co.nz>
Message-ID: <CAGCa14qE7df_gT_AzOo=oF5FPH5OQaVEZmGz8KppCUphq-BmCQ@mail.gmail.com>

Thanks, everyone for making it clear, I will investigate how to do it using
logrotated.

Roee.

On Tue, Jan 12, 2021 at 3:26 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 11/01/21 8:53 am, Matus UHLAR - fantomas wrote:
> > On 10.01.21 17:24, roee klinger wrote:
> >> I just wanted to give an update in case anyone is interested, I was not
> >> able to find a solution,
> >
> > it was posted here:
> >
> http://lists.squid-cache.org/pipermail/squid-users/2020-December/023074.html
> >
> >
> >>>     rotate=N        Specifies the number of log file rotations to
> >>>                 make when you run 'squid -k rotate'. [...]
> >>>                 Only supported by the stdio module
> >>
> >> You are not using an "stdio" module. You are using a "daemon" module.
> >
> > simply said, it could not work in your case,
> >> Instead, I set "logfile_rotate 0" and wrote my own custom script to
> >> rotate
> >> the logs and I am running it as a cron, works just fine.
> >
> > isn't this the default in raspbian? Afaik it comes from debian, where
> this
> > is the default.
> >
>
> Exactly so. The Debian default is to offload log handling to logrotated.
> Nothing needs to be done by the admin in squid.conf. Raspbian uses the
> Debian package, rebuilt to run on the Pi hardware.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/13b10dc7/attachment.htm>

From ngtech1ltd at gmail.com  Tue Jan 12 14:10:42 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 16:10:42 +0200
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
 <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
 <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>
 <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>
Message-ID: <000e01d6e8ec$b7221b80$25665280$@gmail.com>

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, January 12, 2021 2:42 PM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Microsoft store issues with ssl-bump

On 12/01/21 11:32 pm, NgTech LTD wrote:
> Im saying that my config might be wrong and I will send you a full 
> config save which can show you the whole setup like most vendors has.
> I have upgraded squid in production.
> 
> Let me verify first before shouting "bug".
> 
> Eliezer
> 

> The other proxy logs show SNI as being 
> "https://storeedgefd.dsx.mp.microsoft.com:443". SNI should be only a 
>name, not a full URL. So if we assume that log is correct the client is 
>producing invalid SNI. This may be an issue for Squid, causing it to 
> ignore the SNI value entirely.

It?s only fprint the does this with https://XYZ:port
It sees only the ip + domain(plain SNI) + port


> The openssl tool connecting to the same IP address the other proxy 
> claims to be going to gets "sfdataservice.microsoft.com" as the server 
> name. In absence of valid SNI to work with that is the name your Squid 
> will be trying to match against to decide splice vs bump.

So squid tried to match only the certificate and not the SNI?
>From what I see the SNI is ok with the certificate version 3 extensions ie DNS=XYZ
(it should, I cannot verify this against the server at the moment.)


> The server prefers to use TLS/1.3 unless explicitly connected to with 
> TLS/1.2 immediately. IIRC latest Squid force the client to TLS/1.2 when 
> preparing to bump, but may not for spliceand stare. So YMMV.
OK

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon






From uhlar at fantomas.sk  Tue Jan 12 14:22:06 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 12 Jan 2021 15:22:06 +0100
Subject: [squid-users] How do I rotate access.log?
In-Reply-To: <CAGCa14qE7df_gT_AzOo=oF5FPH5OQaVEZmGz8KppCUphq-BmCQ@mail.gmail.com>
References: <CAGCa14roPU4h0n=Z_f8VXFWKAU2s7wL53LUnS_qMmFJ9pOOVkg@mail.gmail.com>
 <19017c1c-ab07-1693-d5a0-3af88afc13f7@measurement-factory.com>
 <CAGCa14rfbj0CgavkbstQNYE0zp519x1J65aaFGCPsD=LjdR98A@mail.gmail.com>
 <32d4ef6f-583a-40b1-80a0-148f945d0788@measurement-factory.com>
 <CAGCa14o_pNEPRUkY2-GwzfW0KGpszCeuNyrOw84JhNQLzTyi8g@mail.gmail.com>
 <12630169-3781-0f11-487b-b0c7f9cd4f7a@measurement-factory.com>
 <CAGCa14pFOtU4H4jO2z4TGsxWR9QiqTGWc-h=UBpzy6KtUdLaKw@mail.gmail.com>
 <20210110195341.GB17841@fantomas.sk>
 <42b8c88c-b799-5baa-f2b9-d010e9db1776@treenet.co.nz>
 <CAGCa14qE7df_gT_AzOo=oF5FPH5OQaVEZmGz8KppCUphq-BmCQ@mail.gmail.com>
Message-ID: <20210112142206.GA20893@fantomas.sk>

On 12.01.21 15:09, roee klinger wrote:
>Thanks, everyone for making it clear, I will investigate how to do it using
>logrotated.

do you have squid installed from raspbian? squid 4.6 is in debian 10 thus
should be in raspbian too.

it comes with /etc/logrotate.d/squid and if you have logrotate package it
should care about rotating.


>On Tue, Jan 12, 2021 at 3:26 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>> On 11/01/21 8:53 am, Matus UHLAR - fantomas wrote:
>> > On 10.01.21 17:24, roee klinger wrote:
>> >> I just wanted to give an update in case anyone is interested, I was not
>> >> able to find a solution,
>> >
>> > it was posted here:
>> >
>> http://lists.squid-cache.org/pipermail/squid-users/2020-December/023074.html
>> >
>> >
>> >>>     rotate=N        Specifies the number of log file rotations to
>> >>>                 make when you run 'squid -k rotate'. [...]
>> >>>                 Only supported by the stdio module
>> >>
>> >> You are not using an "stdio" module. You are using a "daemon" module.
>> >
>> > simply said, it could not work in your case,
>> >> Instead, I set "logfile_rotate 0" and wrote my own custom script to
>> >> rotate
>> >> the logs and I am running it as a cron, works just fine.
>> >
>> > isn't this the default in raspbian? Afaik it comes from debian, where
>> this
>> > is the default.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
   One OS to rule them all, One OS to find them,
One OS to bring them all and into darkness bind them


From rousskov at measurement-factory.com  Tue Jan 12 15:14:59 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Jan 2021 10:14:59 -0500
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
 <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
 <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>
 <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>
Message-ID: <7b05e737-818f-9114-8440-1dab8287a6e1@measurement-factory.com>

On 1/12/21 7:42 AM, Amos Jeffries wrote:
> IIRC latest Squid force the client to TLS/1.2 when
> preparing to bump, but may not for spliceand stare. So YMMV.

FTR: Bugs notwithstanding, modern Squid changes nothing on TLS level
when peeking, splicing, and/or terminating. Squid changes TLS bytes when
staring and/or bumping.

Alex.


From rousskov at measurement-factory.com  Tue Jan 12 15:30:18 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Jan 2021 10:30:18 -0500
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
Message-ID: <c55218c9-b2f4-5166-cc3e-e555142c2213@measurement-factory.com>

On 1/12/21 3:33 AM, Eliezer Croitoru wrote:

> The Windows 10 MS Store tries to connect the domains:
> storeedgefd.dsx.mp.microsoft.com

> which is bypassed from SSL BUMP with a regex and server-name.

>   * Squid 5.0.4 on Fedora 33.

It sounds like you have tried to configure Squid to splice traffic
matching some criteria. So does Squid actually splice traffic matching
those criteria? That is the first question I would ask myself when
trying to triage this problem.

Assuming you can create test traffic, there are many ways to answer that
question, including:

1. Checking whether Squid signs Squid-to-client traffic with its own
certificate.

2. After skipping any CONNECT exchanges, comparing to-Squid TCP payload
with from-Squid TCP payload. If the answer to the question is "yes",
then that payload should be identical, in both client-server and
server-client directions.

3. Sharing Squid debugging logs containing an isolated test transaction.

Testing with other proxies and speculating about the magical possibility
of client detection of TLS splicing is a waste of time _if_ your Squid
configuration is incorrect (i.e. if Squid correctly follows its
configuration, but that configuration contradicts your goals). Thus, I
recommend starting by validating that splicing is happening, as
discussed above.


HTH,

Alex.


From ngtech1ltd at gmail.com  Tue Jan 12 15:46:44 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 17:46:44 +0200
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <7b05e737-818f-9114-8440-1dab8287a6e1@measurement-factory.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
 <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
 <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>
 <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>
 <7b05e737-818f-9114-8440-1dab8287a6e1@measurement-factory.com>
Message-ID: <001301d6e8fa$21525440$63f6fcc0$@gmail.com>

Alex,

I am using the next stare rule:
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3
ssl_bump stare tls_s2_client_hello

Which I am not sure about.
For now this issue seems to be gone.
I don't know why or how but it seems that some IP rotation is happening as we speak/write.
The IP address my service was accessing is different then the one now so I think what Amos
wrote is probably the real reason, ie that the service certificate was for another service CN/DNS Name.
While it's ok for the windows client it's not OK for Squid and any other SNI based certificate validator.

Thanks Helped and Helps,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Tuesday, January 12, 2021 5:15 PM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Microsoft store issues with ssl-bump

On 1/12/21 7:42 AM, Amos Jeffries wrote:
> IIRC latest Squid force the client to TLS/1.2 when
> preparing to bump, but may not for spliceand stare. So YMMV.

FTR: Bugs notwithstanding, modern Squid changes nothing on TLS level
when peeking, splicing, and/or terminating. Squid changes TLS bytes when
staring and/or bumping.

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Jan 12 16:57:54 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Jan 2021 11:57:54 -0500
Subject: [squid-users] Microsoft store issues with ssl-bump
In-Reply-To: <001301d6e8fa$21525440$63f6fcc0$@gmail.com>
References: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAFJ7pNDqRUlEq+OuaIAORAkBAAAAAA==@gmail.com>
 <20210112085744.bj2wolrcyex4m4qo@nekomimi.proxind.it>
 <026f01d6e8c3$7d4292b0$77c7b810$@gmail.com>
 <cf528f45-aa8b-d728-4e07-c8b558fa70f9@treenet.co.nz>
 <CABA8h=T0Qcar9UXgMusEMGckhSQ+t5Ns6a9tyaXZXPa3DbN+sQ@mail.gmail.com>
 <f1395415-7be1-b93e-1d69-140ad8938a52@treenet.co.nz>
 <7b05e737-818f-9114-8440-1dab8287a6e1@measurement-factory.com>
 <001301d6e8fa$21525440$63f6fcc0$@gmail.com>
Message-ID: <4c39c120-d630-8546-0cc2-ef0b1bd08fd2@measurement-factory.com>

On 1/12/21 10:46 AM, Eliezer Croitoru wrote:

> I am using the next stare rule:
> acl tls_s1_connect at_step SslBump1
> acl tls_s2_client_hello at_step SslBump2
> acl tls_s3_server_hello at_step SslBump3
> ssl_bump stare tls_s2_client_hello

I do not know what you are trying to acheive, but if the above is your
entire ssl_bump configuration, then, bugs notwithstanding, it should be
equivalent to a much simpler one:

  # splice at step1, without looking at SNI
  ssl_bump splice all

Alex.


> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
> Sent: Tuesday, January 12, 2021 5:15 PM
> To: Squid Users <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Microsoft store issues with ssl-bump
> 
> On 1/12/21 7:42 AM, Amos Jeffries wrote:
>> IIRC latest Squid force the client to TLS/1.2 when
>> preparing to bump, but may not for spliceand stare. So YMMV.
> 
> FTR: Bugs notwithstanding, modern Squid changes nothing on TLS level
> when peeking, splicing, and/or terminating. Squid changes TLS bytes when
> staring and/or bumping.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From Walter.H at mathemainzel.info  Tue Jan 12 17:24:05 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Tue, 12 Jan 2021 18:24:05 +0100
Subject: [squid-users] distinguish between IPv4 and IPv6
In-Reply-To: <000601d6e858$852f3d40$8f8db7c0$@gmail.com>
References: <c96d58ce-3251-4d98-31b2-61e3619f6827@mathemainzel.info>
 <yvc1n6htd3xj-6eodnf29tt45vao1akq32o5-wjz1w61o5845-eg4r2dona1dz9zt4mv-5l909r-beatalb88jpusooh5f-ox8r71-1xqalp0f5eh5a0526-1rhgxo-ekl6jx-5hjc6j-5x67yz45yx8j.1610395802292@email.android.com>
 <000601d6e858$852f3d40$8f8db7c0$@gmail.com>
Message-ID: <3804a149-2a7f-f5b3-4825-92d6c8b1462f@mathemainzel.info>

Hello,

I did something different, that prevents using the IPv6 of the tunnel 
device als source address;
(a general solution not just squid)

Walter

On 11.01.2021 21:29, Eliezer Croitoru wrote:
>
> The detection of an IPV6 available DST can be determined by DNS and 
> external ACL helper.
>
> It will ?slow? down the first couple bytes of the connection but can 
> be much more reliable then the basic ?dst? acl.
>
> The basic test would be something like:
>
> nslookup -type=aaaa www.squid-cache.org -timeout=10 |grep -v 
> '#53'|grep Address:|wc -l
>
> if the wc -l gt 0 then try to use IPV6.
>
> I believe it?s pretty simple and the main issue is that if a service 
> advertises unreachable IPV6 address.
>
> It can be either because of network misconfiguration or FW or 
> misconfigured DNS.
>
> I have seen all of the above happen in production services in the last 
> year.
>
> I can write a helper for this if required.
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
> Zoom: Coming soon
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *?Amos Jeffries?
> *Sent:* Monday, January 11, 2021 10:10 PM
> *To:* Walter H. <Walter.H at mathemainzel.info>; 
> squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] distinguish between IPv4 and IPv6
>
> The dst ACL type accepts the special value of "ipv4". You can use that 
> and the "!" operator to split traffic.
>
> However, please be aware dst is not very reliable until *after* the 
> outgoing connection has been created, and we are still finding some 
> access checks that do not use it correctly. YMMV.
>
> Amos
>
>
> -------- Original message --------
> From: "Walter H."
> Date: Tue, 12 Jan 2021, 03:19
>
>     Hello,
>
>     is there a way, that I can do something like
>
>     if ( dst is IPv4 ) go direct
>     if ( dst is IPv6 ) use parent proxy xxx
>
>     The reason for my question, I'm using a IPv6-in-IPv4 tunnel,
>     and it would make sense to forward all traffic going to IPv6 to squid
>     running on tunnel end;
>
>     Thanks,
>     Walter
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/b108ecc4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3511 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/b108ecc4/attachment.bin>

From ngtech1ltd at gmail.com  Tue Jan 12 17:53:19 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 12 Jan 2021 19:53:19 +0200
Subject: [squid-users] distinguish between IPv4 and IPv6
In-Reply-To: <3804a149-2a7f-f5b3-4825-92d6c8b1462f@mathemainzel.info>
References: <c96d58ce-3251-4d98-31b2-61e3619f6827@mathemainzel.info>
 <yvc1n6htd3xj-6eodnf29tt45vao1akq32o5-wjz1w61o5845-eg4r2dona1dz9zt4mv-5l909r-beatalb88jpusooh5f-ox8r71-1xqalp0f5eh5a0526-1rhgxo-ekl6jx-5hjc6j-5x67yz45yx8j.1610395802292@email.android.com>
 <000601d6e858$852f3d40$8f8db7c0$@gmail.com>
 <3804a149-2a7f-f5b3-4825-92d6c8b1462f@mathemainzel.info>
Message-ID: <000801d6e90b$d0a0cec0$71e26c40$@gmail.com>

Can you share this solution of yours?

These days it?s good to know about any piece of IPv4 vs/with IPv6 stack solutions.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Walter H.
Sent: Tuesday, January 12, 2021 7:24 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] distinguish between IPv4 and IPv6

 

Hello,

 

I did something different, that prevents using the IPv6 of the tunnel device als source address;

(a general solution not just squid)

 

Walter

 

On 11.01.2021 21:29, Eliezer Croitoru wrote:

The detection of an IPV6 available DST can be determined by DNS and external ACL helper.

It will ?slow? down the first couple bytes of the connection but can be much more reliable then the basic ?dst? acl.

The basic test would be something like:

nslookup -type=aaaa www.squid-cache.org <http://www.squid-cache.org>  -timeout=10 |grep -v '#53'|grep Address:|wc -l

 

if the wc -l gt 0 then try to use IPV6.

 

I believe it?s pretty simple and the main issue is that if a service advertises unreachable IPV6 address.

It can be either because of network misconfiguration or FW or misconfigured DNS.

I have seen all of the above happen in production services in the last year.

 

I can write a helper for this if required.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of ?Amos Jeffries?
Sent: Monday, January 11, 2021 10:10 PM
To: Walter H.  <mailto:Walter.H at mathemainzel.info> <Walter.H at mathemainzel.info>; squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] distinguish between IPv4 and IPv6

 

The dst ACL type accepts the special value of "ipv4". You can use that and the "!" operator to split traffic.

 

However, please be aware dst is not very reliable until *after* the outgoing connection has been created, and we are still finding some access checks that do not use it correctly. YMMV.

 

Amos


-------- Original message --------
From: "Walter H."
Date: Tue, 12 Jan 2021, 03:19

Hello,

is there a way, that I can do something like

if ( dst is IPv4 ) go direct
if ( dst is IPv6 ) use parent proxy xxx

The reason for my question, I'm using a IPv6-in-IPv4 tunnel,
and it would make sense to forward all traffic going to IPv6 to squid 
running on tunnel end;

Thanks,
Walter

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210112/06b7bea3/attachment.htm>

From squid.org at bloms.de  Wed Jan 13 10:27:58 2021
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 13 Jan 2021 11:27:58 +0100
Subject: [squid-users] Incomplete Certificate Chain for wiki.squid-cache.org
Message-ID: <20210113102758.s64u5edzswv2bxhd@bloms.de>

Hello,

the wiki of squid cache project (wiki.squid-cache.org) has an incomplete
certificate chain.
I can't access the website with enabled sslbump and tlsv1.3 support,
because squid isn't able to download the missing intermediate
certificate on its own.

The administrator of that website should add the intermediate
certificate.

More infos can be see here: https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid%2dcache.org


-- 
Regards

  Dieter Bloms

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From david at articatech.com  Wed Jan 13 14:17:26 2021
From: david at articatech.com (David Touzeau)
Date: Wed, 13 Jan 2021 15:17:26 +0100
Subject: [squid-users] WARNING: no_suid: setuid(0): (1) Operation not
 permitted
Message-ID: <1d36fffa-f732-e3d6-d9c3-361877edcb5b@articatech.com>


Hi

This error is generated every 15 minutes when using any authenticator 
helper (ntlm, kerberos...)

Is there a way to investigate on this issue ?

kidxx| WARNING: no_suid: setuid(0): (1) Operation not permitted

Sometimes, after rebooting the system, issue is fixed for an 
undetermined period.

Using squid 4.13 on Debian 10 Intel 64 bits.

regards


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210113/3251e797/attachment.htm>

From Ralf.Hildebrandt at charite.de  Wed Jan 13 14:29:15 2021
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 13 Jan 2021 15:29:15 +0100
Subject: [squid-users] Squid vs. Telegram
Message-ID: <20210113142915.t7zp7w4itwdyzdwf@charite.de>

Recently I'm having issues connecting to Telegram (or rather keeping
a connection).

My log is filled with lots of

TCP_MISS/502 3925 POST http://149.154.167.92/api - HIER_DIRECT/149.154.167.92 text/html

and

TCP_MISS_ABORTED/000 0 POST http://149.154.167.92/api - HIER_DIRECT/149.154.167.92

alike. I know Telegram has a huge influx of new users, probably due
to the recent changes in WhatsApp. But is what I'm seeing normal?

---
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From ngtech1ltd at gmail.com  Wed Jan 13 19:16:56 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 13 Jan 2021 21:16:56 +0200
Subject: [squid-users] WARNING: no_suid: setuid(0): (1) Operation not
 permitted
In-Reply-To: <1d36fffa-f732-e3d6-d9c3-361877edcb5b@articatech.com>
References: <1d36fffa-f732-e3d6-d9c3-361877edcb5b@articatech.com>
Message-ID: <000001d6e9e0$a97547b0$fc5fd710$@gmail.com>

squid -v

 

output might help to understand a bit.

I do not know if the helper is the one that does that but it?s a matter of permissions or FD limits.
This is as far as I know.

If you or anyone have the option to create a full kerberos lab it might help to re-create this issue.

 

I can create the basic Debian machine but not the whole Kerberos setup in a sec.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Wednesday, January 13, 2021 4:17 PM
To: 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: [squid-users] WARNING: no_suid: setuid(0): (1) Operation not permitted

 


Hi 

This error is generated every 15 minutes when using any authenticator helper (ntlm, kerberos...) 

Is there a way to investigate on this issue ?

kidxx| WARNING: no_suid: setuid(0): (1) Operation not permitted

Sometimes, after rebooting the system, issue is fixed for an undetermined period.

Using squid 4.13 on Debian 10 Intel 64 bits.

regards



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210113/2328fc4b/attachment.htm>

From ghulands at me.com  Wed Jan 13 21:33:05 2021
From: ghulands at me.com (Greg Hulands)
Date: Wed, 13 Jan 2021 13:33:05 -0800
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
Message-ID: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>

Hi,
I am setting up squid 5.0.3 and during testing I have found some websites fail to have their certificates generated correctly. I am able to go to sites like YouTube.com <http://youtube.com/> and have the certificates for that be generated correctly, but when I try to go to some others, like arstechnica.com <http://arstechnica.com/>, they fail to generate and return the CA cert that squid is using to sign certificates with.

I turned the logging up on certificate stuff to 5 and have the cache log from trying to make a request here: https://gist.github.com/ghulands/f89b49bf180bfac86c98c46c4260f1eb <https://gist.github.com/ghulands/f89b49bf180bfac86c98c46c4260f1eb>

My ssl-bump config is 

ssl_bump peek step1
ssl_bump bump all

Does anyone have any suggestions or insight on what might the problem be?

Thanks,
Greg


$ squid --version
Squid Cache: Version 5.0.3
Service Name: squid

This binary uses OpenSSL 1.1.1i  8 Dec 2020. For legal restrictions on distribution see https://www.openssl.org/source/license.html <https://www.openssl.org/source/license.html>

configure options:  '--with-default-user=squid' '--bindir=/usr/local/sbin' '--sbindir=/usr/local/sbin' '--datadir=/usr/local/etc/squid' '--libexecdir=/usr/local/libexec/squid' '--localstatedir=/var' '--sysconfdir=/usr/local/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid/squid.pid' '--with-swapdir=/var/squid/cache' '--without-gnutls' '--with-included-ltdl' '--enable-auth' '--enable-zph-qos' '--enable-build-info' '--enable-loadable-modules' '--enable-removal-policies=lru heap' '--disable-epoll' '--disable-linux-netfilter' '--disable-linux-tproxy' '--disable-translation' '--disable-arch-native' '--disable-strict-error-checking' '--enable-eui' '--enable-cache-digests' '--enable-delay-pools' '--disable-ecap' '--disable-esi' '--enable-follow-x-forwarded-for' '--with-mit-krb5=/usr/local' 'CFLAGS=-I/usr/local/include -O2 -pipe  -fstack-protector-strong -fno-strict-aliasing ' 'LDFLAGS=-L/usr/local/lib  -pthread -L/usr/local/lib -lpcreposix -lpcre -Wl,-rpath,/usr/local/lib:/usr/lib -Wl,-rpath,/usr/local/lib -fstack-protector-strong ' 'LIBS=-lkrb5 -lgssapi_krb5 ' 'KRB5CONFIG=/usr/local/bin/krb5-config' 'krb5_config=/usr/local/bin/krb5-config' '--enable-htcp' '--enable-icap-client' '--enable-icmp' '--enable-ident-lookups' '--enable-ipv6' '--enable-kqueue' '--with-large-files' '--enable-http-violations' '--without-nettle' '--enable-snmp' '--enable-ssl' '--with-openssl=/usr/local' '--enable-security-cert-generators=file' 'LIBOPENSSL_CFLAGS=-I/usr/local/include' 'LIBOPENSSL_LIBS=-lcrypto -lssl' '--enable-ssl-crtd' '--disable-stacktraces' '--disable-tdb' '--disable-ipf-transparent' '--enable-ipfw-transparent' '--disable-pf-transparent' '--without-nat-devpf' '--enable-forw-via-db' '--enable-wccp' '--enable-wccpv2' '--enable-auth-basic=DB SMB_LM NCSA PAM POP3 RADIUS fake getpwnam NIS' '--enable-auth-digest=file' '--enable-external-acl-helpers=file_userip unix_group delayer' '--enable-auth-negotiate=kerberos wrapper' '--enable-auth-ntlm=fake SMB_LM' '--enable-storeio=aufs diskd rock ufs' '--enable-disk-io=DiskThreads DiskDaemon AIO Blocking IpcIo Mmapped' '--enable-log-daemon-helpers=file DB' '--enable-url-rewrite-helpers=fake LFS' '--enable-storeid-rewrite-helpers=file' '--enable-security-cert-validators=fake' '--prefix=/usr/local' '--mandir=/usr/local/man' '--disable-silent-rules' '--infodir=/usr/local/share/info/' '--build=amd64-portbld-freebsd12.2' 'build_alias=amd64-portbld-freebsd12.2' 'CC=cc' 'CPPFLAGS=-I/usr/local/include' 'CXX=c++' 'CXXFLAGS=-O2 -pipe -fstack-protector-strong -fno-strict-aliasing  ' 'CPP=cpp' --enable-ltdl-convenience
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210113/4ccca489/attachment.htm>

From rousskov at measurement-factory.com  Wed Jan 13 22:23:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 13 Jan 2021 17:23:00 -0500
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
In-Reply-To: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
References: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
Message-ID: <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>

On 1/13/21 4:33 PM, Greg Hulands wrote:

> I am setting up squid 5.0.3 and during testing I have found some 
> websites fail to have their certificates generated correctly. I am
> able to go to sites like YouTube.com and have the certificates for
> that be generated correctly, but when I try to go to some others,
> like arstechnica.com, they fail to generate and return the CA cert
> that squid is using to sign certificates with.

Just to double check: Are you sure that the certificate the client gets
is the configured CA certificate? For example, do the two certificates
have the same fingerprint?


> I turned the logging up on certificate stuff to 5 and have the cache log
> from trying to make a request
> here:?https://gist.github.com/ghulands/f89b49bf180bfac86c98c46c4260f1eb

The posted snippet shows successful TLS negotiation with the origin
server (FD 23) and a subsequently failed negotiation with the client (FD
21). The latter may have failed because the client did not like the
certificate generated by Squid, but I did not check the exact failure
reason carefully.

The snippet has no information about Squid sending the (generated)
certificates to the client, but Squid appears to receive some generated
certificate from the helper (crtGenRq3180846).

* If you are sure that the client gets a wrong certificate from Squid,
then I recommend posting an ALL,9 log of the problematic transaction.
With some luck, we may be able to see what went wrong with certificate
generation (or virgin certificate validation??).

* Otherwise, I recommend double checking what certificate the client
gets. If the client gets the correct generated certificate, then the
problem is not in certificate validation or generation.

Posting the certificate that the client actually gets may help a lot
with the triage as well.


HTH,

Alex.


From ghulands at me.com  Thu Jan 14 02:47:36 2021
From: ghulands at me.com (Greg Hulands)
Date: Wed, 13 Jan 2021 18:47:36 -0800
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
In-Reply-To: <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>
References: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
 <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>
Message-ID: <52DB1218-9541-4467-BB44-F6ACFE5D4BC8@me.com>

Hi Alex,
Thanks for the help. Comments inline.


> On Jan 13, 2021, at 2:23 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 1/13/21 4:33 PM, Greg Hulands wrote:
> 
>> I am setting up squid 5.0.3 and during testing I have found some 
>> websites fail to have their certificates generated correctly. I am
>> able to go to sites like YouTube.com and have the certificates for
>> that be generated correctly, but when I try to go to some others,
>> like arstechnica.com, they fail to generate and return the CA cert
>> that squid is using to sign certificates with.
> 
> Just to double check: Are you sure that the certificate the client gets
> is the configured CA certificate? For example, do the two certificates
> have the same fingerprint?

Yes, I verified it?s the same certificate - fingerprints are a match.

> 
>> I turned the logging up on certificate stuff to 5 and have the cache log
>> from trying to make a request
>> here: https://gist.github.com/ghulands/f89b49bf180bfac86c98c46c4260f1eb
> 
> The posted snippet shows successful TLS negotiation with the origin
> server (FD 23) and a subsequently failed negotiation with the client (FD
> 21). The latter may have failed because the client did not like the
> certificate generated by Squid, but I did not check the exact failure
> reason carefully.
> 
> The snippet has no information about Squid sending the (generated)
> certificates to the client, but Squid appears to receive some generated
> certificate from the helper (crtGenRq3180846).
> 
> * If you are sure that the client gets a wrong certificate from Squid,
> then I recommend posting an ALL,9 log of the problematic transaction.
> With some luck, we may be able to see what went wrong with certificate
> generation (or virgin certificate validation??).

I have put the ALL,9 log here https://gist.github.com/ghulands/4a689db93fc87f9e7f69174f292f1914 <https://gist.github.com/ghulands/4a689db93fc87f9e7f69174f292f1914>

I can see it generates the certificate correctly, but couldn?t identify why it didn?t return the cert to the client.

> 
> * Otherwise, I recommend double checking what certificate the client
> gets. If the client gets the correct generated certificate, then the
> problem is not in certificate validation or generation.
> 
> Posting the certificate that the client actually gets may help a lot
> with the triage as well.

The certificate that gets returned is in the logs as it?s the CA cert.

Thanks,
Greg

> 
> 
> HTH,
> 
> Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210113/c9aa387c/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 14 04:44:28 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 13 Jan 2021 23:44:28 -0500
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
In-Reply-To: <52DB1218-9541-4467-BB44-F6ACFE5D4BC8@me.com>
References: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
 <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>
 <52DB1218-9541-4467-BB44-F6ACFE5D4BC8@me.com>
Message-ID: <ba51b826-3979-78e4-4f70-43e40f7e532c@measurement-factory.com>

On 1/13/21 9:47 PM, Greg Hulands wrote:
> I have put the ALL,9 log
> here?https://gist.github.com/ghulands/4a689db93fc87f9e7f69174f292f1914

> I can see it generates the certificate correctly,

Agreed. Squid receives (from the helper) a generated certificate with
the right wildcard CN, not a CA certificate.


> but couldn?t identify why it didn?t return the cert to the client.

Yeah... Squid is calling the code that should set the certificate for
the client connection. Unfortunately, I cannot easily tell whether that
code is using the right certificate -- the existing debugging may not
even reveal that detail.

If you see a different certificate received by the client -- something I
cannot verify from the logs -- then perhaps Squid incorrectly switched
the right certificate to a different one or Squid failed to set the
right certificate but forgot to report the problem (and the CA
certificate from the related context was used?). These are just wild
guesses.

If you do not get better suggestions for going forward, consider these
last-straw ideas:

* Testing with a client like openssl, try disabling TLS v1.3. It is
being used by the client in your logs. Perhaps there is something in TLS
v1.3 that requires special handing when talking to the client. I know
that Squid has problems with TLS v1.3 on the Squid-to-server
connections... (In your case, the Squid-to-server connection is TLS v1.2
AFAICT).

* Upgrade to the latest v5 or even v6. I see no relevant fixes in v5 but
I could miss them.

* If you are a developer, add more debugging or use gdb to find out what
happens with the Squid-to-client certificate. Otherwise, find a
developer who can do that for you.

Sorry I cannot think of any good options here.

Alex.


From squid3 at treenet.co.nz  Thu Jan 14 04:41:44 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2021 17:41:44 +1300
Subject: [squid-users] Incomplete Certificate Chain for
 wiki.squid-cache.org
In-Reply-To: <20210113102758.s64u5edzswv2bxhd@bloms.de>
References: <20210113102758.s64u5edzswv2bxhd@bloms.de>
Message-ID: <55754d22-cbc1-0dbb-a2f7-8d5f9320ea2c@treenet.co.nz>

On 13/01/21 11:27 pm, Dieter Bloms wrote:
> Hello,
> 
> the wiki of squid cache project (wiki.squid-cache.org) has an incomplete
> certificate chain.
> I can't access the website with enabled sslbump and tlsv1.3 support,
> because squid isn't able to download the missing intermediate
> certificate on its own.
> 

What version of Squid are you using?

These certificates generated by LetsEncrypt use the AIA mechanism which 
latest Squid versions should be downloading intermediate certs as-needed.

Amos


From squid3 at treenet.co.nz  Thu Jan 14 04:43:25 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jan 2021 17:43:25 +1300
Subject: [squid-users] WARNING: no_suid: setuid(0): (1) Operation not
 permitted
In-Reply-To: <1d36fffa-f732-e3d6-d9c3-361877edcb5b@articatech.com>
References: <1d36fffa-f732-e3d6-d9c3-361877edcb5b@articatech.com>
Message-ID: <dcd3aeda-fb3d-6392-ea99-4799262c8abb@treenet.co.nz>

On 14/01/21 3:17 am, David Touzeau wrote:
> 
> Hi
> 
> This error is generated every 15 minutes when using any authenticator 
> helper (ntlm, kerberos...)
> 
> Is there a way to investigate on this issue ?
> 
> kidxx| WARNING: no_suid: setuid(0): (1) Operation not permitted
> 

This looks like <https://bugs.squid-cache.org/show_bug.cgi?id=3785>


Amos


From ghulands at me.com  Thu Jan 14 06:22:11 2021
From: ghulands at me.com (Greg Hulands)
Date: Wed, 13 Jan 2021 22:22:11 -0800
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
In-Reply-To: <ba51b826-3979-78e4-4f70-43e40f7e532c@measurement-factory.com>
References: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
 <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>
 <52DB1218-9541-4467-BB44-F6ACFE5D4BC8@me.com>
 <ba51b826-3979-78e4-4f70-43e40f7e532c@measurement-factory.com>
Message-ID: <7FD9CCFF-4DCC-4D59-9EA0-186E976B3318@me.com>

Hey Alex,
Can you point me to the rough location in code where the certs are sent to the client.

I tried with TLS 1.2 with openssl s_client and it returned the certs the same.

Thanks,
Greg

> On Jan 13, 2021, at 8:44 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 1/13/21 9:47 PM, Greg Hulands wrote:
>> I have put the ALL,9 log
>> here https://gist.github.com/ghulands/4a689db93fc87f9e7f69174f292f1914
> 
>> I can see it generates the certificate correctly,
> 
> Agreed. Squid receives (from the helper) a generated certificate with
> the right wildcard CN, not a CA certificate.
> 
> 
>> but couldn?t identify why it didn?t return the cert to the client.
> 
> Yeah... Squid is calling the code that should set the certificate for
> the client connection. Unfortunately, I cannot easily tell whether that
> code is using the right certificate -- the existing debugging may not
> even reveal that detail.
> 
> If you see a different certificate received by the client -- something I
> cannot verify from the logs -- then perhaps Squid incorrectly switched
> the right certificate to a different one or Squid failed to set the
> right certificate but forgot to report the problem (and the CA
> certificate from the related context was used?). These are just wild
> guesses.
> 
> If you do not get better suggestions for going forward, consider these
> last-straw ideas:
> 
> * Testing with a client like openssl, try disabling TLS v1.3. It is
> being used by the client in your logs. Perhaps there is something in TLS
> v1.3 that requires special handing when talking to the client. I know
> that Squid has problems with TLS v1.3 on the Squid-to-server
> connections... (In your case, the Squid-to-server connection is TLS v1.2
> AFAICT).
> 
> * Upgrade to the latest v5 or even v6. I see no relevant fixes in v5 but
> I could miss them.
> 
> * If you are a developer, add more debugging or use gdb to find out what
> happens with the Squid-to-client certificate. Otherwise, find a
> developer who can do that for you.
> 
> Sorry I cannot think of any good options here.
> 
> Alex.



From david at articatech.com  Thu Jan 14 08:13:40 2021
From: david at articatech.com (David Touzeau)
Date: Thu, 14 Jan 2021 09:13:40 +0100
Subject: [squid-users] WARNING: no_suid: setuid(0): (1) Operation not
 permitted
In-Reply-To: <dcd3aeda-fb3d-6392-ea99-4799262c8abb@treenet.co.nz>
References: <1d36fffa-f732-e3d6-d9c3-361877edcb5b@articatech.com>
 <dcd3aeda-fb3d-6392-ea99-4799262c8abb@treenet.co.nz>
Message-ID: <3ab3bf78-8fcf-257d-d6a8-040d3a8521b8@articatech.com>

Yes it seems the same bug but the ticket is not relevant (FreeBSD) as 
i'm on Debian and on a modern kernel

The main incomprehensible behavior that is issue occurs sometimes as 
setuid is a sticky bit permission..

squid -v output:

Squid Cache: Version 4.13
Service Name: squid

This binary uses OpenSSL 1.1.1d? 10 Sep 2019. For legal restrictions on 
distribution see https://www.openssl.org/source/license.html

configure options:? '--prefix=/usr' '--build=x86_64-linux-gnu' 
'--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' 
'--localstatedir=/var' '--libexecdir=/lib/squid3' 
'--disable-maintainer-mode' '--disable-dependency-tracking' 
'--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' 
'--enable-gnuregex' '--enable-removal-policy=heap' 
'--enable-follow-x-forwarded-for' '--enable-removal-policies=lru,heap' 
'--enable-arp-acl' '--enable-truncate' '--with-large-files' 
'--with-pthreads' '--enable-esi' '--enable-storeio=aufs,diskd,ufs,rock' 
'--enable-x-accelerator-vary' '--with-dl' '--enable-linux-netfilter' 
'--with-netfilter-conntrack' '--enable-wccpv2' '--enable-eui' 
'--enable-auth' '--enable-auth-basic' '--enable-snmp' '--enable-icmp' 
'--enable-auth-digest' '--enable-log-daemon-helpers' 
'--enable-url-rewrite-helpers' '--enable-auth-ntlm' 
'--with-default-user=squid' '--enable-icap-client' 
'--disable-cache-digests' '--enable-poll' '--enable-epoll' 
'--enable-async-io=128' '--enable-zph-qos' '--enable-delay-pools' 
'--enable-http-violations' '--enable-url-maps' '--enable-ecap' 
'--enable-ssl' '--with-openssl' '--enable-ssl-crtd' 
'--enable-xmalloc-statistics' '--enable-ident-lookups' 
'--with-filedescriptors=65536' '--with-aufs-threads=128' 
'--disable-arch-native' '--with-logdir=/var/log/squid' 
'--with-pidfile=/var/run/squid/squid.pid' 
'--with-swapdir=/var/cache/squid' 'build_alias=x86_64-linux-gnu'


Le 14/01/2021 ? 05:43, Amos Jeffries a ?crit?:
> On 14/01/21 3:17 am, David Touzeau wrote:
>>
>> Hi
>>
>> This error is generated every 15 minutes when using any authenticator 
>> helper (ntlm, kerberos...)
>>
>> Is there a way to investigate on this issue ?
>>
>> kidxx| WARNING: no_suid: setuid(0): (1) Operation not permitted
>>
>
> This looks like <https://bugs.squid-cache.org/show_bug.cgi?id=3785>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210114/12f97035/attachment.htm>

From uhlar at fantomas.sk  Thu Jan 14 09:22:37 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 14 Jan 2021 10:22:37 +0100
Subject: [squid-users] Incomplete Certificate Chain for
 wiki.squid-cache.org
In-Reply-To: <55754d22-cbc1-0dbb-a2f7-8d5f9320ea2c@treenet.co.nz>
References: <20210113102758.s64u5edzswv2bxhd@bloms.de>
 <55754d22-cbc1-0dbb-a2f7-8d5f9320ea2c@treenet.co.nz>
Message-ID: <20210114092237.GA19910@fantomas.sk>

>On 13/01/21 11:27 pm, Dieter Bloms wrote:
>>the wiki of squid cache project (wiki.squid-cache.org) has an incomplete
>>certificate chain.
>>I can't access the website with enabled sslbump and tlsv1.3 support,
>>because squid isn't able to download the missing intermediate
>>certificate on its own.

On 14.01.21 17:41, Amos Jeffries wrote:
>These certificates generated by LetsEncrypt use the AIA mechanism 
>which latest Squid versions should be downloading intermediate certs 
>as-needed.

invalid intermediate certifiate is provided:


 0 s:CN = master.squid-cache.org
   i:C = US, O = Let's Encrypt, CN = R3
 1 s:C = US, O = Let's Encrypt, CN = Let's Encrypt Authority X3
   i:O = Digital Signature Trust Co., CN = DST Root CA X3



-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Holmes, what kind of school did you study to be a detective?
- Elementary, Watkins.  -- Daffy Duck & Porky Pig


From squid.org at bloms.de  Thu Jan 14 13:26:26 2021
From: squid.org at bloms.de (Dieter Bloms)
Date: Thu, 14 Jan 2021 14:26:26 +0100
Subject: [squid-users] Incomplete Certificate Chain for
 wiki.squid-cache.org
In-Reply-To: <55754d22-cbc1-0dbb-a2f7-8d5f9320ea2c@treenet.co.nz>
References: <20210113102758.s64u5edzswv2bxhd@bloms.de>
 <55754d22-cbc1-0dbb-a2f7-8d5f9320ea2c@treenet.co.nz>
Message-ID: <20210114132626.nfyjnwvycsrwy3e5@bloms.de>

Hello Amos,

On Thu, Jan 14, Amos Jeffries wrote:

> On 13/01/21 11:27 pm, Dieter Bloms wrote:
> > Hello,
> > 
> > the wiki of squid cache project (wiki.squid-cache.org) has an incomplete
> > certificate chain.
> > I can't access the website with enabled sslbump and tlsv1.3 support,
> > because squid isn't able to download the missing intermediate
> > certificate on its own.
> 
> What version of Squid are you using?

we use squid 4.13 and it works for tls version <1.3
 
> These certificates generated by LetsEncrypt use the AIA mechanism which
> latest Squid versions should be downloading intermediate certs as-needed.

but for tls1.3 it doesn't work, because the certificate is encrypted.
Please have a look at the bugreport https://bugs.squid-cache.org/show_bug.cgi?id=5067


-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From ngtech1ltd at gmail.com  Thu Jan 14 13:44:06 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 14 Jan 2021 15:44:06 +0200
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
In-Reply-To: <7FD9CCFF-4DCC-4D59-9EA0-186E976B3318@me.com>
References: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
 <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>
 <52DB1218-9541-4467-BB44-F6ACFE5D4BC8@me.com>
 <ba51b826-3979-78e4-4f70-43e40f7e532c@measurement-factory.com>
 <7FD9CCFF-4DCC-4D59-9EA0-186E976B3318@me.com>
Message-ID: <000201d6ea7b$546069d0$fd213d70$@gmail.com>

Hey Greg,

I am trying to test it with 5.0.4 and it seems that this site works for me with SSL BUMP.
The CN and the SAN are the same so it makes sense that it should work the same on your proxy.
However I do see that this domain has 2 IP addresses which might affect what you see.
I am trying to verify this issue locally.

I wrote the next ruby script to help others with some insights.
https://github.com/elico/tls-check-script

Both ip addresses seem to give the same certificate.
I am using openssl to see the certificate:
openssl s_client -showcerts -servername arstechnica.com -connect arstechnica.com:443 </dev/null 2>/dev/null | openssl x509 -noout -text

Let me know if something specific is seen in your environment.

It shouldn't matter too much but, what OS are you running squid ontop and what is "squid -v" output?

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Greg Hulands
Sent: Thursday, January 14, 2021 8:22 AM
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] generate-host-certificates=on fails to generate certificates for _some_ hosts

Hey Alex,
Can you point me to the rough location in code where the certs are sent to the client.

I tried with TLS 1.2 with openssl s_client and it returned the certs the same.

Thanks,
Greg

> On Jan 13, 2021, at 8:44 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 1/13/21 9:47 PM, Greg Hulands wrote:
>> I have put the ALL,9 log
>> here https://gist.github.com/ghulands/4a689db93fc87f9e7f69174f292f1914
> 
>> I can see it generates the certificate correctly,
> 
> Agreed. Squid receives (from the helper) a generated certificate with
> the right wildcard CN, not a CA certificate.
> 
> 
>> but couldn?t identify why it didn?t return the cert to the client.
> 
> Yeah... Squid is calling the code that should set the certificate for
> the client connection. Unfortunately, I cannot easily tell whether that
> code is using the right certificate -- the existing debugging may not
> even reveal that detail.
> 
> If you see a different certificate received by the client -- something I
> cannot verify from the logs -- then perhaps Squid incorrectly switched
> the right certificate to a different one or Squid failed to set the
> right certificate but forgot to report the problem (and the CA
> certificate from the related context was used?). These are just wild
> guesses.
> 
> If you do not get better suggestions for going forward, consider these
> last-straw ideas:
> 
> * Testing with a client like openssl, try disabling TLS v1.3. It is
> being used by the client in your logs. Perhaps there is something in TLS
> v1.3 that requires special handing when talking to the client. I know
> that Squid has problems with TLS v1.3 on the Squid-to-server
> connections... (In your case, the Squid-to-server connection is TLS v1.2
> AFAICT).
> 
> * Upgrade to the latest v5 or even v6. I see no relevant fixes in v5 but
> I could miss them.
> 
> * If you are a developer, add more debugging or use gdb to find out what
> happens with the Squid-to-client certificate. Otherwise, find a
> developer who can do that for you.
> 
> Sorry I cannot think of any good options here.
> 
> Alex.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Jan 14 14:53:10 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Jan 2021 09:53:10 -0500
Subject: [squid-users] generate-host-certificates=on fails to generate
 certificates for _some_ hosts
In-Reply-To: <7FD9CCFF-4DCC-4D59-9EA0-186E976B3318@me.com>
References: <36205A40-0297-47CE-BB95-2E75C0AC549B@me.com>
 <6a1bf364-9dd8-fc73-22de-71b71974b51d@measurement-factory.com>
 <52DB1218-9541-4467-BB44-F6ACFE5D4BC8@me.com>
 <ba51b826-3979-78e4-4f70-43e40f7e532c@measurement-factory.com>
 <7FD9CCFF-4DCC-4D59-9EA0-186E976B3318@me.com>
Message-ID: <9e539460-7d9b-221c-980e-2c725e62ceb0@measurement-factory.com>

On 1/14/21 1:22 AM, Greg Hulands wrote:

> Can you point me to the rough location in code where the certs are sent to the client.

I would start with the following log line:

> 2021/01/13 18:09:11.655 kid1| 33,5| client_side.cc(2700) sslCrtdHandleReply: Certificate for arstechnica.com was successfully recieved from ssl_crtd

You can see the exact code location of each low-level debug statement.
Unfortunately, Squid uses a somewhat unusual filename(linenumber)
notation for that, but the information is there -- client_side.cc line
2700. After the source code location, Squid prints the function name
where the message was generated -- sslCrtdHandleReply.

The above line points to a method that receives the (parsed) certificate
from the code that talks to the helper. IIRC, Squid then calls
Ssl::configureSSLUsingPkeyAndCertFromMemory() to apply the certificate
to the client-Squid TLS connection. It would be good to confirm that the
certificate Squid applies is the right one and that all the application
functions were successful (e.g., Squid may forget to check some OpenSSL
function result). This will require some development skills.


> I tried with TLS 1.2 with openssl s_client and it returned the certs the same.

Noted. You may prefer to test with TLS v1.2 (first) because it may
produce simpler debugging logs and create fewer uncertainties. The code
mentioned above is used for all TLS versions.


Good luck,

Alex.


>> On Jan 13, 2021, at 8:44 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>>
>> On 1/13/21 9:47 PM, Greg Hulands wrote:
>>> I have put the ALL,9 log
>>> here https://gist.github.com/ghulands/4a689db93fc87f9e7f69174f292f1914
>>
>>> I can see it generates the certificate correctly,
>>
>> Agreed. Squid receives (from the helper) a generated certificate with
>> the right wildcard CN, not a CA certificate.
>>
>>
>>> but couldn?t identify why it didn?t return the cert to the client.
>>
>> Yeah... Squid is calling the code that should set the certificate for
>> the client connection. Unfortunately, I cannot easily tell whether that
>> code is using the right certificate -- the existing debugging may not
>> even reveal that detail.
>>
>> If you see a different certificate received by the client -- something I
>> cannot verify from the logs -- then perhaps Squid incorrectly switched
>> the right certificate to a different one or Squid failed to set the
>> right certificate but forgot to report the problem (and the CA
>> certificate from the related context was used?). These are just wild
>> guesses.
>>
>> If you do not get better suggestions for going forward, consider these
>> last-straw ideas:
>>
>> * Testing with a client like openssl, try disabling TLS v1.3. It is
>> being used by the client in your logs. Perhaps there is something in TLS
>> v1.3 that requires special handing when talking to the client. I know
>> that Squid has problems with TLS v1.3 on the Squid-to-server
>> connections... (In your case, the Squid-to-server connection is TLS v1.2
>> AFAICT).
>>
>> * Upgrade to the latest v5 or even v6. I see no relevant fixes in v5 but
>> I could miss them.
>>
>> * If you are a developer, add more debugging or use gdb to find out what
>> happens with the Squid-to-client certificate. Otherwise, find a
>> developer who can do that for you.
>>
>> Sorry I cannot think of any good options here.
>>
>> Alex.



From evolvah at gmail.com  Thu Jan 14 19:41:25 2021
From: evolvah at gmail.com (Sergey Maslyakov)
Date: Thu, 14 Jan 2021 13:41:25 -0600
Subject: [squid-users] Mutual TLS for the upstream example
Message-ID: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>

Folks,

Is the CONNECT tunnel designed in a way that enables it to "enrich" the
outgoing connection with mTLS authentication? "tls_outgoing_options" does
not seem to work the way I was hoping it does.

My destination server requires mTLS authentication of the client. I have a
valid key-cert pair and I can successfully execute a "curl" command to
fetch a document from that server using the key-cert pair at hand.

I want to put Squid between my clients (Maven, Gradle, Docker Engine, etc)
and the server so that clients would be configured to use the instance of
Squid as an HTTPS proxy but would not have to be configured with the mTLS
key-cert pair.

Here is how I see it:

Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert pair) ---
(mTLS/SSL) ---> Server

Is this doable within Squid architecture?

I got it working using NGINX with some minor hiccups and I was hoping I can
do it more elegantly with Squid.


Thank you,
/Sergey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210114/5f99857d/attachment.htm>

From ngtech1ltd at gmail.com  Thu Jan 14 20:39:49 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 14 Jan 2021 22:39:49 +0200
Subject: [squid-users] Mutual TLS for the upstream example
In-Reply-To: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
References: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
Message-ID: <003301d6eab5$679f4680$36ddd380$@gmail.com>

I don?t know about Squid but I assume varnish has this feature:

https://docs.varnish-software.com/varnish-cache-plus/features/backend-ssl/

 

If you just need a GW without caching it should work as expected.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Sergey Maslyakov
Sent: Thursday, January 14, 2021 9:41 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Mutual TLS for the upstream example

 

Folks,

 

Is the CONNECT tunnel designed in a way that enables it to "enrich" the outgoing connection with mTLS authentication? "tls_outgoing_options" does not seem to work the way I was hoping it does.

 

My destination server requires mTLS authentication of the client. I have a valid key-cert pair and I can successfully execute a "curl" command to fetch a document from that server using the key-cert pair at hand.

 

I want to put Squid between my clients (Maven, Gradle, Docker Engine, etc) and the server so that clients would be configured to use the instance of Squid as an HTTPS proxy but would not have to be configured with the mTLS key-cert pair.

 

Here is how I see it:

 

Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert pair) --- (mTLS/SSL) ---> Server

 

Is this doable within Squid architecture?

 

I got it working using NGINX with some minor hiccups and I was hoping I can do it more elegantly with Squid.

 

 

Thank you,

/Sergey

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210114/be8e634d/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 14 20:42:56 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Jan 2021 15:42:56 -0500
Subject: [squid-users] Mutual TLS for the upstream example
In-Reply-To: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
References: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
Message-ID: <948920b3-df53-6dbb-6f61-047bc413d7c4@measurement-factory.com>

On 1/14/21 2:41 PM, Sergey Maslyakov wrote:

> Is the CONNECT tunnel designed in a way that enables it to "enrich" the
> outgoing connection with mTLS authentication?

If, by "designed", you are asking about HTTP protocol design, then the
CONNECT tunnel is a blind TCP tunnel by that design. Hence, by design, a
proxy dealing with CONNECT tunnels does not assume they are related to TLS.

Squid SslBump feature allows the admin to disregard the general design
principles above and interpret the CONNECT tunnel bytes as TLS. Usually,
when folks ask about "enriching" TLS, they talk about SslBump. It is not
yet clear to me whether you need SslBump.

Neither Squid nor any other proxy can enrich a client TLS connection.
TLS is designed to protect from such fiddling by proxies. I suspect that
what you need is for Squid to use mTLS when forwarding a client request
to the origin server. This is supported in principle, but the devil is
in the details.


> My destination server requires mTLS authentication of the client. I have
> a valid key-cert pair and I can successfully execute a "curl" command to
> fetch a document from that server using the key-cert pair at hand.

Squid supports mTLS authentication with HTTPS cache_peers (including
origin servers) via the cache_peers sslcert option.


> I want to put Squid between my clients (Maven, Gradle, Docker Engine,
> etc) and the server so that clients would be configured to use the
> instance of Squid as an HTTPS proxy but would not have to be configured
> with the mTLS key-cert pair.

1. "HTTPS proxy" is a proxy that accepts TLS connections opened by
clients that want to explicitly ask that proxy to forward their request
to some origin server. I suspect that is not what you meant by "HTTPS
proxy". Whether there are CONNECT requests in this case depends on the
client, but all popular clients I know about do send CONNECT requests
inside that secure TLS connection to the proxy (unfortunately). For
those clients, SslBump is required.

2. It is possible that you are talking about an HTTPS surrogate or HTTPS
reverse proxy/accelerator. In this configuration, clients talk to Squid
thinking that Squid is an HTTPS origin server. There are no CONNECT
requests in this case. Clients receive Squid certificates. No SslBump.

3. If you meant that Squid should intercept/hijack client TCP
connections to HTTPS origin servers, then it is a third kind of
configuration. There are no CONNECT requests in this case. Clients
receive Squid certificates. SslBump is required.

4. If you meant that Squid, as a regular HTTP forward proxy, should
hijack client CONNECT tunnels to HTTPS origin servers, then it is a yet
another kind of configuration. There are CONNECT requests in this case.
Clients receive Squid certificates. SslBump is required.

Each scenario above would present its own Squid challenges. Please
clarify what you are dealing with.


> Here is how I see it:

> Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert pair) ---
> (mTLS/SSL) ---> Server

> Is this doable within?Squid architecture?
 The "HTTPS/CONNECT" part can be misinterpreted as any of the four cases
above. Please clarify what it means to you. If this is all too
overwhelming, we can start with a simple question: Do your clients
establish TCP connections to a Squid port (cases 1, 2, 4)? Or do they
open TCP connections to origin servers instead (case 3)?


Cheers,

Alex.


From evolvah at gmail.com  Thu Jan 14 23:36:29 2021
From: evolvah at gmail.com (Sergey Maslyakov)
Date: Thu, 14 Jan 2021 17:36:29 -0600
Subject: [squid-users] Mutual TLS for the upstream example
In-Reply-To: <948920b3-df53-6dbb-6f61-047bc413d7c4@measurement-factory.com>
References: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
 <948920b3-df53-6dbb-6f61-047bc413d7c4@measurement-factory.com>
Message-ID: <CAB3mbkRsuzx5RALkXDQn0eWwMnhL1jVfY2J6NQxrNVJHBbDRpA@mail.gmail.com>

Thank you for the very detailed explanation, Alex!

Having looked at the sequence diagram of the mTLS handshake, I now see why
I cannot get what I want.

I can either get plain-text HTTP to mTLS-secured forwarding, or I have to
have two independent legs of communication when the authenticity of
client-to-Squid connection is ensured using its (Squid's) own key-cert
pair, which is different from authentication credentials of the handshake
between Squid and my target server.

I cannot afford to have unencrypted the client-to-Squid leg, otherwise, I
could do it in a single "socat" invocation.

I don't have the server key-cert pair, which makes the sslbump impractical
too.

In NGINX, I implemented the reverse proxy model (option 2 on your list),
when the proxy terminated SSL with its own cert, received the request,
rewrote some headers, and forwarded it over an mTLS-secured connection. The
problem is that under some circumstances I also need to rewrite the body of
the HTTP request and this was taking the whole solution to a whole new
level of complexity...

I guess I need to start looking into automation that would allow me to
manage mTLS configuration of clients at scale instead of trying to defeat
the whole purpose of the mTLS integrity mechanism :)

On Thu, Jan 14, 2021 at 2:42 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/14/21 2:41 PM, Sergey Maslyakov wrote:
>
> > Is the CONNECT tunnel designed in a way that enables it to "enrich" the
> > outgoing connection with mTLS authentication?
>
> If, by "designed", you are asking about HTTP protocol design, then the
> CONNECT tunnel is a blind TCP tunnel by that design. Hence, by design, a
> proxy dealing with CONNECT tunnels does not assume they are related to TLS.
>
> Squid SslBump feature allows the admin to disregard the general design
> principles above and interpret the CONNECT tunnel bytes as TLS. Usually,
> when folks ask about "enriching" TLS, they talk about SslBump. It is not
> yet clear to me whether you need SslBump.
>
> Neither Squid nor any other proxy can enrich a client TLS connection.
> TLS is designed to protect from such fiddling by proxies. I suspect that
> what you need is for Squid to use mTLS when forwarding a client request
> to the origin server. This is supported in principle, but the devil is
> in the details.
>
>
> > My destination server requires mTLS authentication of the client. I have
> > a valid key-cert pair and I can successfully execute a "curl" command to
> > fetch a document from that server using the key-cert pair at hand.
>
> Squid supports mTLS authentication with HTTPS cache_peers (including
> origin servers) via the cache_peers sslcert option.
>
>
> > I want to put Squid between my clients (Maven, Gradle, Docker Engine,
> > etc) and the server so that clients would be configured to use the
> > instance of Squid as an HTTPS proxy but would not have to be configured
> > with the mTLS key-cert pair.
>
> 1. "HTTPS proxy" is a proxy that accepts TLS connections opened by
> clients that want to explicitly ask that proxy to forward their request
> to some origin server. I suspect that is not what you meant by "HTTPS
> proxy". Whether there are CONNECT requests in this case depends on the
> client, but all popular clients I know about do send CONNECT requests
> inside that secure TLS connection to the proxy (unfortunately). For
> those clients, SslBump is required.
>
> 2. It is possible that you are talking about an HTTPS surrogate or HTTPS
> reverse proxy/accelerator. In this configuration, clients talk to Squid
> thinking that Squid is an HTTPS origin server. There are no CONNECT
> requests in this case. Clients receive Squid certificates. No SslBump.
>
> 3. If you meant that Squid should intercept/hijack client TCP
> connections to HTTPS origin servers, then it is a third kind of
> configuration. There are no CONNECT requests in this case. Clients
> receive Squid certificates. SslBump is required.
>
> 4. If you meant that Squid, as a regular HTTP forward proxy, should
> hijack client CONNECT tunnels to HTTPS origin servers, then it is a yet
> another kind of configuration. There are CONNECT requests in this case.
> Clients receive Squid certificates. SslBump is required.
>
> Each scenario above would present its own Squid challenges. Please
> clarify what you are dealing with.
>
>
> > Here is how I see it:
>
> > Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert pair) ---
> > (mTLS/SSL) ---> Server
>
> > Is this doable within Squid architecture?
>  The "HTTPS/CONNECT" part can be misinterpreted as any of the four cases
> above. Please clarify what it means to you. If this is all too
> overwhelming, we can start with a simple question: Do your clients
> establish TCP connections to a Squid port (cases 1, 2, 4)? Or do they
> open TCP connections to origin servers instead (case 3)?
>
>
> Cheers,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210114/ddeb617b/attachment.htm>

From evolvah at gmail.com  Thu Jan 14 23:38:11 2021
From: evolvah at gmail.com (Sergey Maslyakov)
Date: Thu, 14 Jan 2021 17:38:11 -0600
Subject: [squid-users] Mutual TLS for the upstream example
In-Reply-To: <003301d6eab5$679f4680$36ddd380$@gmail.com>
References: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
 <003301d6eab5$679f4680$36ddd380$@gmail.com>
Message-ID: <CAB3mbkRJw9ft8LsHOWxcpE-V5HjUSrsOin+jq3LFzq142HxJCQ@mail.gmail.com>

Thank you, Eliezer! I will look into it but it appears that the underlying
problem is not solvable by design of the mTLS handshake... There are corner
cases that can be solved but not the original issue.


On Thu, Jan 14, 2021 at 2:39 PM Eliezer Croitoru <ngtech1ltd at gmail.com>
wrote:

> I don?t know about Squid but I assume varnish has this feature:
>
> https://docs.varnish-software.com/varnish-cache-plus/features/backend-ssl/
>
>
>
> If you just need a GW without caching it should work as expected.
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> Zoom: Coming soon
>
>
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *Sergey Maslyakov
> *Sent:* Thursday, January 14, 2021 9:41 PM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Mutual TLS for the upstream example
>
>
>
> Folks,
>
>
>
> Is the CONNECT tunnel designed in a way that enables it to "enrich" the
> outgoing connection with mTLS authentication? "tls_outgoing_options" does
> not seem to work the way I was hoping it does.
>
>
>
> My destination server requires mTLS authentication of the client. I have a
> valid key-cert pair and I can successfully execute a "curl" command to
> fetch a document from that server using the key-cert pair at hand.
>
>
>
> I want to put Squid between my clients (Maven, Gradle, Docker Engine, etc)
> and the server so that clients would be configured to use the instance of
> Squid as an HTTPS proxy but would not have to be configured with the mTLS
> key-cert pair.
>
>
>
> Here is how I see it:
>
>
>
> Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert pair) ---
> (mTLS/SSL) ---> Server
>
>
>
> Is this doable within Squid architecture?
>
>
>
> I got it working using NGINX with some minor hiccups and I was hoping I
> can do it more elegantly with Squid.
>
>
>
>
>
> Thank you,
>
> /Sergey
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210114/7b8a679b/attachment.htm>

From ngtech1ltd at gmail.com  Fri Jan 15 07:56:06 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 15 Jan 2021 09:56:06 +0200
Subject: [squid-users] Mutual TLS for the upstream example
In-Reply-To: <CAB3mbkRJw9ft8LsHOWxcpE-V5HjUSrsOin+jq3LFzq142HxJCQ@mail.gmail.com>
References: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
 <003301d6eab5$679f4680$36ddd380$@gmail.com>
 <CAB3mbkRJw9ft8LsHOWxcpE-V5HjUSrsOin+jq3LFzq142HxJCQ@mail.gmail.com>
Message-ID: <001001d6eb13$e17670b0$a4635210$@gmail.com>

Can we try to understand the issue again?

 

In this setup, should squid know about the client certificate and pass it to the service  backend 

Or maybe just terminate the clients certificate?

 

I am not sure I understood what you need/want to do with squid.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: Sergey Maslyakov <evolvah at gmail.com> 
Sent: Friday, January 15, 2021 1:38 AM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Mutual TLS for the upstream example

 

Thank you, Eliezer! I will look into it but it appears that the underlying problem is not solvable by design of the mTLS handshake... There are corner cases that can be solved but not the original issue.

 

 

On Thu, Jan 14, 2021 at 2:39 PM Eliezer Croitoru <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > wrote:

I don?t know about Squid but I assume varnish has this feature:

https://docs.varnish-software.com/varnish-cache-plus/features/backend-ssl/

 

If you just need a GW without caching it should work as expected.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of Sergey Maslyakov
Sent: Thursday, January 14, 2021 9:41 PM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Mutual TLS for the upstream example

 

Folks,

 

Is the CONNECT tunnel designed in a way that enables it to "enrich" the outgoing connection with mTLS authentication? "tls_outgoing_options" does not seem to work the way I was hoping it does.

 

My destination server requires mTLS authentication of the client. I have a valid key-cert pair and I can successfully execute a "curl" command to fetch a document from that server using the key-cert pair at hand.

 

I want to put Squid between my clients (Maven, Gradle, Docker Engine, etc) and the server so that clients would be configured to use the instance of Squid as an HTTPS proxy but would not have to be configured with the mTLS key-cert pair.

 

Here is how I see it:

 

Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert pair) --- (mTLS/SSL) ---> Server

 

Is this doable within Squid architecture?

 

I got it working using NGINX with some minor hiccups and I was hoping I can do it more elegantly with Squid.

 

 

Thank you,

/Sergey

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210115/d9317920/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 15 17:37:03 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 15 Jan 2021 12:37:03 -0500
Subject: [squid-users] Mutual TLS for the upstream example
In-Reply-To: <CAB3mbkRsuzx5RALkXDQn0eWwMnhL1jVfY2J6NQxrNVJHBbDRpA@mail.gmail.com>
References: <CAB3mbkSohTFJHWDzEYxWMue6LA79v1-80wbpbLvdmpD7_8RY6g@mail.gmail.com>
 <948920b3-df53-6dbb-6f61-047bc413d7c4@measurement-factory.com>
 <CAB3mbkRsuzx5RALkXDQn0eWwMnhL1jVfY2J6NQxrNVJHBbDRpA@mail.gmail.com>
Message-ID: <31d76444-b291-1bf7-03b9-bff389821325@measurement-factory.com>

On 1/14/21 6:36 PM, Sergey Maslyakov wrote:

> I can either get plain-text HTTP to mTLS-secured forwarding, or I have
> to have two independent legs of communication when the authenticity of
> client-to-Squid connection is ensured using its (Squid's) own key-cert
> pair, which is different from authentication credentials of the
> handshake between Squid and my target server.

I doubt you can simply do plain-text HTTP to the proxy in this case
(without SslBump) even if you were OK with the risks because most
clients will not send "GET https://" requests to the proxy. They will
send CONNECT requests instead. The proxy would have to decrypt their
CONNECT tunnels with SslBump in order to add mTLS.

Thus, the second part of your summary essentially covers all practical
cases, including reverse proxy and SslBump-based ones.


> I don't have the server key-cert pair, which makes the
> sslbump?impractical too.

FYI: SslBump is meant for cases where you do _not_ have the origin
server key. Squid generates/fakes server certificates and signs them
with its own CA key. The clients have to be configured to trust that CA
key. It is not a "nice" solution by any means, but, sometimes, it is the
best one available.


> In NGINX, I implemented the reverse proxy model (option 2 on your list),
> when the proxy terminated SSL with its own cert, received the request,
> rewrote some headers, and forwarded it over an mTLS-secured connection.
> The problem is that under some?circumstances I also need to rewrite the
> body of the HTTP request and this was taking the whole solution to a
> whole new level of complexity...

FWIW, Squid should be able to do that as well. Body rewrites are
supported via eCAP/ICAP adaptations, but the result will be quite
complex indeed! Technically, this setup is pretty much as (in)secure as
a setup based on SslBump -- it is based on you securing the client-proxy
leg with your own certificate and forcing the clients to trust your
certificate.


Cheers,

Alex.


> On Thu, Jan 14, 2021 at 2:42 PM Alex Rousskov wrote:
> 
>     On 1/14/21 2:41 PM, Sergey Maslyakov wrote:
> 
>     > Is the CONNECT tunnel designed in a way that enables it to
>     "enrich" the
>     > outgoing connection with mTLS authentication?
> 
>     If, by "designed", you are asking about HTTP protocol design, then the
>     CONNECT tunnel is a blind TCP tunnel by that design. Hence, by design, a
>     proxy dealing with CONNECT tunnels does not assume they are related
>     to TLS.
> 
>     Squid SslBump feature allows the admin to disregard the general design
>     principles above and interpret the CONNECT tunnel bytes as TLS. Usually,
>     when folks ask about "enriching" TLS, they talk about SslBump. It is not
>     yet clear to me whether you need SslBump.
> 
>     Neither Squid nor any other proxy can enrich a client TLS connection.
>     TLS is designed to protect from such fiddling by proxies. I suspect that
>     what you need is for Squid to use mTLS when forwarding a client request
>     to the origin server. This is supported in principle, but the devil is
>     in the details.
> 
> 
>     > My destination server requires mTLS authentication of the client.
>     I have
>     > a valid key-cert pair and I can successfully execute a "curl"
>     command to
>     > fetch a document from that server using the key-cert pair at hand.
> 
>     Squid supports mTLS authentication with HTTPS cache_peers (including
>     origin servers) via the cache_peers sslcert option.
> 
> 
>     > I want to put Squid between my clients (Maven, Gradle, Docker Engine,
>     > etc) and the server so that clients would be configured to use the
>     > instance of Squid as an HTTPS proxy but would not have to be
>     configured
>     > with the mTLS key-cert pair.
> 
>     1. "HTTPS proxy" is a proxy that accepts TLS connections opened by
>     clients that want to explicitly ask that proxy to forward their request
>     to some origin server. I suspect that is not what you meant by "HTTPS
>     proxy". Whether there are CONNECT requests in this case depends on the
>     client, but all popular clients I know about do send CONNECT requests
>     inside that secure TLS connection to the proxy (unfortunately). For
>     those clients, SslBump is required.
> 
>     2. It is possible that you are talking about an HTTPS surrogate or HTTPS
>     reverse proxy/accelerator. In this configuration, clients talk to Squid
>     thinking that Squid is an HTTPS origin server. There are no CONNECT
>     requests in this case. Clients receive Squid certificates. No SslBump.
> 
>     3. If you meant that Squid should intercept/hijack client TCP
>     connections to HTTPS origin servers, then it is a third kind of
>     configuration. There are no CONNECT requests in this case. Clients
>     receive Squid certificates. SslBump is required.
> 
>     4. If you meant that Squid, as a regular HTTP forward proxy, should
>     hijack client CONNECT tunnels to HTTPS origin servers, then it is a yet
>     another kind of configuration. There are CONNECT requests in this case.
>     Clients receive Squid certificates. SslBump is required.
> 
>     Each scenario above would present its own Squid challenges. Please
>     clarify what you are dealing with.
> 
> 
>     > Here is how I see it:
> 
>     > Maven --- (HTTPS/CONNECT) ---> Squid (stores my mTLS key-cert
>     pair) ---
>     > (mTLS/SSL) ---> Server
> 
>     > Is this doable within?Squid architecture?
>     ?The "HTTPS/CONNECT" part can be misinterpreted as any of the four cases
>     above. Please clarify what it means to you. If this is all too
>     overwhelming, we can start with a simple question: Do your clients
>     establish TCP connections to a Squid port (cases 1, 2, 4)? Or do they
>     open TCP connections to origin servers instead (case 3)?
> 
> 
>     Cheers,
> 
>     Alex.
> 



From roeeklinger60 at gmail.com  Sat Jan 16 12:52:13 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Sat, 16 Jan 2021 14:52:13 +0200
Subject: [squid-users] Peer selection based on IP with multiple ports?
Message-ID: <CAGCa14rx6qdh=WnS81Fs2f=6N5iu-9jgLeeNEwrm5khQ4hjXAQ@mail.gmail.com>

Hey,

I am using Squid to route users to different peers based on their
usernames, I was asked to add support for IP whitelisting recently but I
ran into an issue.

If one IP wants to access to different peers, I will have to do it based on
on the listening port number, as there is no other way to differentiate the
traffic, however, Squid is limited to 128 listening ports, which means I
will have to use a max of 128 peers.

I know I can increase the maximum to more than 128 listening ports, but
apparently, this will result in performance issues.

I was wondering what is the accepted way to do this?

Thanks,
Roee
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210116/5682b0e8/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 17 02:19:13 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Jan 2021 15:19:13 +1300
Subject: [squid-users] Peer selection based on IP with multiple ports?
In-Reply-To: <CAGCa14rx6qdh=WnS81Fs2f=6N5iu-9jgLeeNEwrm5khQ4hjXAQ@mail.gmail.com>
References: <CAGCa14rx6qdh=WnS81Fs2f=6N5iu-9jgLeeNEwrm5khQ4hjXAQ@mail.gmail.com>
Message-ID: <b60efd64-8f19-c062-77d9-c5c859cad992@treenet.co.nz>

On 17/01/21 1:52 am, roee klinger wrote:
> Hey,
> 
> I am using Squid to route users to different peers based on their 
> usernames, I was asked to add support for IP whitelisting recently but I 
> ran into an issue.
> 
> If one IP wants to access to different peers, I will have to do it based 
> on on the listening port number, as there is no other way to 
> differentiate?the traffic, however, Squid is limited to 128 listening 
> ports, which means I will have to use a max of 128 peers.
> 

What about using the ext_SQL_session_acl helper?

You would have a URL/page that these clients visit to select which peer 
they are going to use. That updates the database to start/change their 
session with username that allows access to the relevant peer.


Amos


From moberger at metanetworks.com  Sun Jan 17 20:08:54 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Sun, 17 Jan 2021 22:08:54 +0200
Subject: [squid-users] Adding headers in ICAP server with no preview
Message-ID: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>

Hi

I have an environment with squid version 5.0.4 with ICAP server adapting
requests by adding an header.
When I'm trying to send a POST request with a body I'm having an issue of a
stuck connection.
What should the ICAP response look like?

What I do is to reply like this:

> (dI./M..ICAP/1.0 200 OK
> ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
> Date: Sun, 17 Jan 2021 19:34:12 GMT
> Server: BaseICAP/1.0 Python/3.6.12
> Encapsulated: req-hdr=0, req-body=360
>
> POST http://www.dst-server.com:22222/v1/test HTTP/1.1
> x-new-header: {"key": "value"}
> user-agent: python-requests/2.25.1
> accept-encoding: gzip, deflate
> accept: */*
> content-length: 16
> content-type: application/json
> host: www.dst-server.com:22222
>

 Please assume the number in req-body=360 is correct (I trimmed here the
content of the new header).
As I said, I use 'Preview: 0' since I don't mind the body. The question is
whether declaring the body starts at X (req-body=X) is OK even though I
don't have a body to send? I think having req-null=X is bad since it
probably tells squid that I decided the adapted request should have no
body, but that's only a guess.

When the ICAP doesn't adapt the request, everything looks fine.
When it adapts the request I see that the POST request squid sends to
www.dst-server.com doesn't contain the body.

On the logs of the server behind www.dst-server.com I see an entry for the
API request only after I abort the request I sent.

I use python's requests module to make the request:

import requests
> s = requests.Session()
> s.proxies = {'http': 'localhost:8000', 'https': 'localhost:8000'}

resp = s.post('http://www.dst-server.com:22222/v1/test',
> allow_redirects=False, json={'key': 'value'})
>

I'll highly appreciate any help.
Thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210117/04f81a05/attachment.htm>

From rousskov at measurement-factory.com  Sun Jan 17 21:33:59 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 17 Jan 2021 16:33:59 -0500
Subject: [squid-users] Adding headers in ICAP server with no preview
In-Reply-To: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
References: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
Message-ID: <81302836-3c5f-fb18-8a09-0539540b2702@measurement-factory.com>

On 1/17/21 3:08 PM, Moti Berger wrote:
> What should the ICAP response look like?

The vast majority off ICAP responses containing an HTTP POST message
will look like ICAP header + HTTP header + HTTP body. Please see RFC
3507 and its errata for examples of and discussion about those three
components. It should help avoid guessing and developing by examples
(which usually leads to bugs, especially where ICAP is involved).


> What I do is to reply like this:
> 
>     (dI./M..ICAP/1.0 200 OK
>     ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
>     Date: Sun, 17 Jan 2021 19:34:12 GMT
>     Server: BaseICAP/1.0 Python/3.6.12
>     Encapsulated: req-hdr=0, req-body=360
> 
>     POST http://www.dst-server.com:22222/v1/test HTTP/1.1
>     x-new-header: {"key": "value"}
>     user-agent: python-requests/2.25.1
>     accept-encoding: gzip, deflate
>     accept: */*
>     content-length: 16
>     content-type: application/json
>     host: www.dst-server.com:22222 <http://www.dst-server.com:22222>


FYI: The above incomplete ICAP response promises an HTTP request body,
both on the ICAP level (req-body) and on the HTTP level (content-length:
16).


> As I said, I use 'Preview: 0' since I don't mind the body. The question
> is whether declaring the body starts at X (req-body=X) is OK even though
> I don't have a body to send?

It is not OK not to send the body. Encapsulated:req-body does more than
declaring where the encapsulated headers end. It also promises an
embedded HTTP body after those headers. You must encapsulate the body if
the HTTP message should have one. You cannot adapt the header of an HTTP
message with a body without also sending the HTTP body (virgin or adapted).

Preview is pretty much irrelevant in this context -- the ICAP protocol
does not care how the ICAP service gets the HTTP body to include in the
ICAP response.

There are unofficial ICAP extensions that make it possible to tell the
ICAP client to reuse the body it has buffered while adapting the header,
but you should get the baseline case working before bothering with those
extensions -- they are optimizations that are not applicable to some
transactions.


> I think having req-null=X is bad since it
> probably tells squid that I decided the adapted request?should have no
> body, but that's only a guess.

If you meant to say "null-body", then you guessed correctly -- null-body
means the adapted HTTP message has no body. That is not what you want to
say when adapting most HTTP POST messages.


HTH,

Alex.


From moberger at metanetworks.com  Sun Jan 17 22:28:38 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Mon, 18 Jan 2021 00:28:38 +0200
Subject: [squid-users] Making destination IP available in ICAP REQMOD request
Message-ID: <CAGSk-42s3gSjoHfuhVsh+MRY44kDejkXdRjhvw6DjDmGK8Vq-Q@mail.gmail.com>

Hi

My goal is to obtain the destination IP when sending an HTTP request for my
ICAP server so it would be able to decide the kind of adaptation required
based on it.

Looking at squid (5.0.4) code I discovered the following:

It seems that "everything" starts at ClientRequestContext.
I've noticed that noteAdaptationAclCheckDone calls startAdaptation which
calls more methods, eventually getting
to Adaptation::Icap::ModXact::makeRequestHeaders where it iterates over
headers defined by the adaptation_meta configurations in squid.conf.
For each, it calls the 'match' method where it tries to format (and
assemble) it. There it seems that the value is taken from an AccessLogEntry:

>    case LFT_SERVER_IP_ADDRESS:
>             if (al->hier.tcpServer)
>                 out = al->hier.tcpServer->remote.toStr(tmp, sizeof(tmp));
>             break;
>

So the AccessLogEntry object seems to be the key.
At REQMOD time, I don't get the value of the destination IP.
Looking further I found that the DNS resolving happens when it's decided
that the request should be forwarded to the destination server.

So I tracked the flow and it seems to start from FwdState::Start method
which gets an AccessLogEntryPointer.
Then it calls methods that eventually do the DNS resolving
(Dns::nbgethostbyname) and ending in (FwdState::connectStart) which have
the IP to connect to.
So it seems that this flow will populate the AccessLogEntry.
This seems right since during RESPMOD, the same code above
(in Adaptation::Icap::ModXact::makeRequestHeaders) is running and this time
the `match` method eventually gets the destination IP.
I added logs that prints the AccessLogEntryPointer and in the FwdState.cc
the log says address 0x5592ab521e30*12 and in the Notes.cc the log says
address 0x5592ab521e30*25.

Two things that I haven't found yet:
1. The place where the AccessLogEntry is populated
2. Where after the adaptation, the forwarding to the destination server
occured (assuming it should be forwarded)

I couldn't figure out a way to start the DNS resolving just before
the startAdaptation starts as it requires all sorts of objects that seem to
be unavailable there.
I wonder if you can help me to find a way to do it.

Thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/cd7a2889/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan 18 00:34:01 2021
From: squid3 at treenet.co.nz (=?UTF-8?B?4oCqQW1vcyBKZWZmcmllc+KArA==?=)
Date: Mon, 18 Jan 2021 13:34:01 +1300
Subject: [squid-users] Making destination IP available in ICAP REQMOD
 request
References: <CAGSk-42s3gSjoHfuhVsh+MRY44kDejkXdRjhvw6DjDmGK8Vq-Q@mail.gmail.com>
Message-ID: <ydmvdc3afv5nljaba4-3xvogj-fy4uatuxmp2k-s5ne2i85z4l-a64ylw-2q72y8-agqa39d1j254-mpuzle-lwp66h8bciv0-8b88tp-62u1ex-ak630ub1q1cd-7wg0y66gzo9u-2j3jsq-pvjhir-e0b6cv.1610929850287@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/b32a5498/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan 18 07:47:16 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 18 Jan 2021 09:47:16 +0200
Subject: [squid-users] Making destination IP available in ICAP REQMOD
 request
In-Reply-To: <CAGSk-42s3gSjoHfuhVsh+MRY44kDejkXdRjhvw6DjDmGK8Vq-Q@mail.gmail.com>
References: <CAGSk-42s3gSjoHfuhVsh+MRY44kDejkXdRjhvw6DjDmGK8Vq-Q@mail.gmail.com>
Message-ID: <CABA8h=SOsA=LMUk0DLaeFQQNCW-4ds=gGwa_y_RO5eh4cfeEaA@mail.gmail.com>

Hey Moti,

It is a good assumption that the same caching dns server (not 8.8.8.8 or
1.1.1.1) that the client use will return the relevant destination ip for
the domain.
Its possible to do such a query in the icap service with low timeout(2-3)
seconds.
can this be good enough for your use case?

Eliezer

On Mon, Jan 18, 2021, 00:28 Moti Berger <moberger at metanetworks.com> wrote:

> Hi
>
> My goal is to obtain the destination IP when sending an HTTP request for
> my ICAP server so it would be able to decide the kind of adaptation
> required based on it.
>
> Looking at squid (5.0.4) code I discovered the following:
>
> It seems that "everything" starts at ClientRequestContext.
> I've noticed that noteAdaptationAclCheckDone calls startAdaptation which
> calls more methods, eventually getting
> to Adaptation::Icap::ModXact::makeRequestHeaders where it iterates over
> headers defined by the adaptation_meta configurations in squid.conf.
> For each, it calls the 'match' method where it tries to format (and
> assemble) it. There it seems that the value is taken from an AccessLogEntry:
>
>>    case LFT_SERVER_IP_ADDRESS:
>>             if (al->hier.tcpServer)
>>                 out = al->hier.tcpServer->remote.toStr(tmp, sizeof(tmp));
>>             break;
>>
>
> So the AccessLogEntry object seems to be the key.
> At REQMOD time, I don't get the value of the destination IP.
> Looking further I found that the DNS resolving happens when it's decided
> that the request should be forwarded to the destination server.
>
> So I tracked the flow and it seems to start from FwdState::Start method
> which gets an AccessLogEntryPointer.
> Then it calls methods that eventually do the DNS resolving
> (Dns::nbgethostbyname) and ending in (FwdState::connectStart) which have
> the IP to connect to.
> So it seems that this flow will populate the AccessLogEntry.
> This seems right since during RESPMOD, the same code above
> (in Adaptation::Icap::ModXact::makeRequestHeaders) is running and this time
> the `match` method eventually gets the destination IP.
> I added logs that prints the AccessLogEntryPointer and in the FwdState.cc
> the log says address 0x5592ab521e30*12 and in the Notes.cc the log says
> address 0x5592ab521e30*25.
>
> Two things that I haven't found yet:
> 1. The place where the AccessLogEntry is populated
> 2. Where after the adaptation, the forwarding to the destination server
> occured (assuming it should be forwarded)
>
> I couldn't figure out a way to start the DNS resolving just before
> the startAdaptation starts as it requires all sorts of objects that seem to
> be unavailable there.
> I wonder if you can help me to find a way to do it.
>
> Thanks,
> Moti
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/30fa32c7/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan 18 09:49:41 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 18 Jan 2021 11:49:41 +0200
Subject: [squid-users] Adding headers in ICAP server with no preview
In-Reply-To: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
References: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
Message-ID: <001201d6ed7f$3f032ad0$bd098070$@gmail.com>

Hey Moti,

 

I had an example on my local git which also works with gzip and other stuff for BGU however I cannot find it now.

>From what I remember this worked with POST but only like an external acl helper.

Ie blocking or allowing OK/ERR:

https://github.com/elico/drbl-icap-service

 

Any modification of the headers is a bit complicated.

 

I can try to check/test it but it will take time.

>From what I see 5.0.4 is pretty stable however there are specific issues related to TLS 1.3.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Moti Berger
Sent: Sunday, January 17, 2021 10:09 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Adding headers in ICAP server with no preview

 

Hi

 

I have an environment with squid version 5.0.4 with ICAP server adapting requests by adding an header.

When I'm trying to send a POST request with a body I'm having an issue of a stuck connection.

What should the ICAP response look like?

 

What I do is to reply like this:

(dI./M..ICAP/1.0 200 OK
ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
Date: Sun, 17 Jan 2021 19:34:12 GMT
Server: BaseICAP/1.0 Python/3.6.12
Encapsulated: req-hdr=0, req-body=360

POST http://www.dst-server.com:22222/v1/test HTTP/1.1
x-new-header: {"key": "value"}
user-agent: python-requests/2.25.1
accept-encoding: gzip, deflate
accept: */*
content-length: 16
content-type: application/json
host: www.dst-server.com:22222 <http://www.dst-server.com:22222> 

 

 Please assume the number in req-body=360 is correct (I trimmed here the content of the new header).

As I said, I use 'Preview: 0' since I don't mind the body. The question is whether declaring the body starts at X (req-body=X) is OK even though I don't have a body to send? I think having req-null=X is bad since it probably tells squid that I decided the adapted request should have no body, but that's only a guess.

 

When the ICAP doesn't adapt the request, everything looks fine.

When it adapts the request I see that the POST request squid sends to www.dst-server.com <http://www.dst-server.com>  doesn't contain the body.

 

On the logs of the server behind www.dst-server.com <http://www.dst-server.com>  I see an entry for the API request only after I abort the request I sent.

 

I use python's requests module to make the request:

 

import requests
s = requests.Session()
s.proxies = {'http': 'localhost:8000', 'https': 'localhost:8000'} 

resp = s.post <http://s.post> ('http://www.dst-server.com:22222/v1/test', allow_redirects=False, json={'key': 'value'})

 

I'll highly appreciate any help.

Thanks,

Moti

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/33ea1cf7/attachment.htm>

From moberger at metanetworks.com  Mon Jan 18 11:45:44 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Mon, 18 Jan 2021 13:45:44 +0200
Subject: [squid-users] Adding headers in ICAP server with no preview
In-Reply-To: <81302836-3c5f-fb18-8a09-0539540b2702@measurement-factory.com>
References: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
 <81302836-3c5f-fb18-8a09-0539540b2702@measurement-factory.com>
Message-ID: <CAGSk-42Fqie5b3MFDwzjWL92-e+yZAjt8_9kMia0v9Q1V1atDw@mail.gmail.com>

Hi

If the ICAP server sets 'Preview: 0' in the OPTIONS it means that when the
ICAP client sends a request, it should not contain the body.
This is the REQMOD request:

> F..n...DREQMOD icap://censor-req.proxy:14590/request ICAP/1.0
> Host: censor-req.proxy:14590
> Date: Mon, 18 Jan 2021 11:34:54 GMT
> Encapsulated: req-hdr=0, req-body=222
> Preview: 0
> Allow: 204, trailers
> X-custom-header: data
>
> POST http://www.dst-server.com:22222/v1/test HTTP/1.1
> User-Agent: python-requests/2.25.1
> Accept-Encoding: gzip, deflate
> Accept: */*
> Content-Length: 10
> Content-Type: application/json
> Host: www.dst-server.com:22222
>

The ICAP 'Encapsulated' header has a req-body even though no 'body' should
be in this request.
I wonder why in this case the 'Encapsulated' header doesn't contain
null-body.
I could not find any reference to this case in the RFC3507.
The ICAP server has no way to encapsulate the HTTP request body if it
didn't get it.

I want to avoid sending the body because the adaptation is body agnostic.


On Sun, Jan 17, 2021 at 11:34 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/17/21 3:08 PM, Moti Berger wrote:
> > What should the ICAP response look like?
>
> The vast majority off ICAP responses containing an HTTP POST message
> will look like ICAP header + HTTP header + HTTP body. Please see RFC
> 3507 and its errata for examples of and discussion about those three
> components. It should help avoid guessing and developing by examples
> (which usually leads to bugs, especially where ICAP is involved).
>
>
> > What I do is to reply like this:
> >
> >     (dI./M..ICAP/1.0 200 OK
> >     ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
> >     Date: Sun, 17 Jan 2021 19:34:12 GMT
> >     Server: BaseICAP/1.0 Python/3.6.12
> >     Encapsulated: req-hdr=0, req-body=360
> >
> >     POST http://www.dst-server.com:22222/v1/test HTTP/1.1
> >     x-new-header: {"key": "value"}
> >     user-agent: python-requests/2.25.1
> >     accept-encoding: gzip, deflate
> >     accept: */*
> >     content-length: 16
> >     content-type: application/json
> >     host: www.dst-server.com:22222 <http://www.dst-server.com:22222>
>
>
> FYI: The above incomplete ICAP response promises an HTTP request body,
> both on the ICAP level (req-body) and on the HTTP level (content-length:
> 16).
>
>
> > As I said, I use 'Preview: 0' since I don't mind the body. The question
> > is whether declaring the body starts at X (req-body=X) is OK even though
> > I don't have a body to send?
>
> It is not OK not to send the body. Encapsulated:req-body does more than
> declaring where the encapsulated headers end. It also promises an
> embedded HTTP body after those headers. You must encapsulate the body if
> the HTTP message should have one. You cannot adapt the header of an HTTP
> message with a body without also sending the HTTP body (virgin or adapted).
>
> Preview is pretty much irrelevant in this context -- the ICAP protocol
> does not care how the ICAP service gets the HTTP body to include in the
> ICAP response.
>
> There are unofficial ICAP extensions that make it possible to tell the
> ICAP client to reuse the body it has buffered while adapting the header,
> but you should get the baseline case working before bothering with those
> extensions -- they are optimizations that are not applicable to some
> transactions.
>
>
> > I think having req-null=X is bad since it
> > probably tells squid that I decided the adapted request should have no
> > body, but that's only a guess.
>
> If you meant to say "null-body", then you guessed correctly -- null-body
> means the adapted HTTP message has no body. That is not what you want to
> say when adapting most HTTP POST messages.
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/1a63b352/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan 18 13:33:56 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 18 Jan 2021 15:33:56 +0200
Subject: [squid-users] Adding headers in ICAP server with no preview
In-Reply-To: <CAGSk-42Fqie5b3MFDwzjWL92-e+yZAjt8_9kMia0v9Q1V1atDw@mail.gmail.com>
References: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
 <81302836-3c5f-fb18-8a09-0539540b2702@measurement-factory.com>
 <CAGSk-42Fqie5b3MFDwzjWL92-e+yZAjt8_9kMia0v9Q1V1atDw@mail.gmail.com>
Message-ID: <CABA8h=T++gKPg-Ruu9fhozq1qFS4SSDab35AJt8w85S4M3AV-g@mail.gmail.com>

I assume that a null body is based on the logic that the icap client knows
the progress and the icap details enough to only modify the headers.
it should be tested.
I tried to test it but im busy to test it right now.

Eliezer

On Mon, Jan 18, 2021, 13:46 Moti Berger <moberger at metanetworks.com> wrote:

> Hi
>
> If the ICAP server sets 'Preview: 0' in the OPTIONS it means that when the
> ICAP client sends a request, it should not contain the body.
> This is the REQMOD request:
>
>> F..n...DREQMOD icap://censor-req.proxy:14590/request ICAP/1.0
>> Host: censor-req.proxy:14590
>> Date: Mon, 18 Jan 2021 11:34:54 GMT
>> Encapsulated: req-hdr=0, req-body=222
>> Preview: 0
>> Allow: 204, trailers
>> X-custom-header: data
>>
>> POST http://www.dst-server.com:22222/v1/test HTTP/1.1
>> User-Agent: python-requests/2.25.1
>> Accept-Encoding: gzip, deflate
>> Accept: */*
>> Content-Length: 10
>> Content-Type: application/json
>> Host: www.dst-server.com:22222
>>
>
> The ICAP 'Encapsulated' header has a req-body even though no 'body' should
> be in this request.
> I wonder why in this case the 'Encapsulated' header doesn't contain
> null-body.
> I could not find any reference to this case in the RFC3507.
> The ICAP server has no way to encapsulate the HTTP request body if it
> didn't get it.
>
> I want to avoid sending the body because the adaptation is body agnostic.
>
>
> On Sun, Jan 17, 2021 at 11:34 PM Alex Rousskov <
> rousskov at measurement-factory.com> wrote:
>
>> On 1/17/21 3:08 PM, Moti Berger wrote:
>> > What should the ICAP response look like?
>>
>> The vast majority off ICAP responses containing an HTTP POST message
>> will look like ICAP header + HTTP header + HTTP body. Please see RFC
>> 3507 and its errata for examples of and discussion about those three
>> components. It should help avoid guessing and developing by examples
>> (which usually leads to bugs, especially where ICAP is involved).
>>
>>
>> > What I do is to reply like this:
>> >
>> >     (dI./M..ICAP/1.0 200 OK
>> >     ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
>> >     Date: Sun, 17 Jan 2021 19:34:12 GMT
>> >     Server: BaseICAP/1.0 Python/3.6.12
>> >     Encapsulated: req-hdr=0, req-body=360
>> >
>> >     POST http://www.dst-server.com:22222/v1/test HTTP/1.1
>> >     x-new-header: {"key": "value"}
>> >     user-agent: python-requests/2.25.1
>> >     accept-encoding: gzip, deflate
>> >     accept: */*
>> >     content-length: 16
>> >     content-type: application/json
>> >     host: www.dst-server.com:22222 <http://www.dst-server.com:22222>
>>
>>
>> FYI: The above incomplete ICAP response promises an HTTP request body,
>> both on the ICAP level (req-body) and on the HTTP level (content-length:
>> 16).
>>
>>
>> > As I said, I use 'Preview: 0' since I don't mind the body. The question
>> > is whether declaring the body starts at X (req-body=X) is OK even though
>> > I don't have a body to send?
>>
>> It is not OK not to send the body. Encapsulated:req-body does more than
>> declaring where the encapsulated headers end. It also promises an
>> embedded HTTP body after those headers. You must encapsulate the body if
>> the HTTP message should have one. You cannot adapt the header of an HTTP
>> message with a body without also sending the HTTP body (virgin or
>> adapted).
>>
>> Preview is pretty much irrelevant in this context -- the ICAP protocol
>> does not care how the ICAP service gets the HTTP body to include in the
>> ICAP response.
>>
>> There are unofficial ICAP extensions that make it possible to tell the
>> ICAP client to reuse the body it has buffered while adapting the header,
>> but you should get the baseline case working before bothering with those
>> extensions -- they are optimizations that are not applicable to some
>> transactions.
>>
>>
>> > I think having req-null=X is bad since it
>> > probably tells squid that I decided the adapted request should have no
>> > body, but that's only a guess.
>>
>> If you meant to say "null-body", then you guessed correctly -- null-body
>> means the adapted HTTP message has no body. That is not what you want to
>> say when adapting most HTTP POST messages.
>>
>>
>> HTH,
>>
>> Alex.
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/a189e775/attachment.htm>

From rousskov at measurement-factory.com  Mon Jan 18 16:48:40 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jan 2021 11:48:40 -0500
Subject: [squid-users] Adding headers in ICAP server with no preview
In-Reply-To: <CAGSk-42Fqie5b3MFDwzjWL92-e+yZAjt8_9kMia0v9Q1V1atDw@mail.gmail.com>
References: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
 <81302836-3c5f-fb18-8a09-0539540b2702@measurement-factory.com>
 <CAGSk-42Fqie5b3MFDwzjWL92-e+yZAjt8_9kMia0v9Q1V1atDw@mail.gmail.com>
Message-ID: <2dbee277-e239-0759-9b79-cf857db25088@measurement-factory.com>

On 1/18/21 6:45 AM, Moti Berger wrote:

> If the ICAP server sets 'Preview: 0' in the OPTIONS it means that when
> the ICAP client sends a request, it should not contain the body.

The above summary may mislead many readers. I would describe the
protocol operation differently:

* Preview in an OPTIONS response indicates that the server supports
Preview in general and specifies the maximum Preview size the client
should use (e.g., Preview:0 limits Preview to HTTP headers).

* The Preview mode for a specific REQMOD or RESPMOD transaction is
signaled in the corresponding REQMOD or RESPMOD request (not a previous
OPTIONS response) by adding a Preview:N ICAP request header (Preview:0
specifies a headers-only Preview for the current transaction).

* The REQMOD or RESPMOD transaction with a Preview:0 request header is
split into two phases. During the first phase, the client must not send
the virgin body. During the second phase, if any, the client must send
the virgin body. Both phases comprise a single ICAP transaction, with a
single ICAP request and a single ICAP response. Thus, one cannot say
that this transaction (as a whole) "should not contain a body".


> This is the REQMOD request:
> 
>     F..n...DREQMOD icap://censor-req.proxy:14590/request ICAP/1.0
>     Host: censor-req.proxy:14590
>     Date: Mon, 18 Jan 2021 11:34:54 GMT
>     Encapsulated: req-hdr=0, req-body=222
>     Preview: 0
>     Allow: 204, trailers
>     X-custom-header: data
> 
>     POST http://www.dst-server.com:22222/v1/test HTTP/1.1
>     User-Agent: python-requests/2.25.1
>     Accept-Encoding: gzip, deflate
>     Accept: */*
>     Content-Length: 10
>     Content-Type: application/json
>     Host: www.dst-server.com:22222 <http://www.dst-server.com:22222>

> The ICAP 'Encapsulated' header has a req-body even though no 'body'
> should be in this request.

Not exactly. The request may not be over at this point. Please see my
third bullet above for details.


> The ICAP server has no way to encapsulate the HTTP request body if it
> didn't get it.

To get the request body in Preview:0 mode, the ICAP server must respond
with ICAP 100 (Continue).


> I want to avoid sending the body because the adaptation is body agnostic.

Yes, I know, but you have to work within the ICAP protocol boundaries.
ICAP simply does not optimize your use case! After you have the basics
working well, you can invest in implementing a use-original-body ICAP
extension[1] that, in _some_ cases, can prevent the body exchange while
adapting HTTP headers.

Alternatively, you can use an existing (extendible) ICAP server to do
the legwork for you [2]. Many individuals and companies have learned the
hard way that implementing an ICAP service correctly from scratch is
very difficult and often prohibitively expensive.

[1]
http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt

[2] https://wiki.squid-cache.org/Features/ICAP#ICAP_Servers


HTH,

Alex.



> On Sun, Jan 17, 2021 at 11:34 PM Alex Rousskov wrote:
> 
>     On 1/17/21 3:08 PM, Moti Berger wrote:
>     > What should the ICAP response look like?
> 
>     The vast majority off ICAP responses containing an HTTP POST message
>     will look like ICAP header + HTTP header + HTTP body. Please see RFC
>     3507 and its errata for examples of and discussion about those three
>     components. It should help avoid guessing and developing by examples
>     (which usually leads to bugs, especially where ICAP is involved).
> 
> 
>     > What I do is to reply like this:
>     >
>     >? ? ?(dI./M..ICAP/1.0 200 OK
>     >? ? ?ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
>     >? ? ?Date: Sun, 17 Jan 2021 19:34:12 GMT
>     >? ? ?Server: BaseICAP/1.0 Python/3.6.12
>     >? ? ?Encapsulated: req-hdr=0, req-body=360
>     >
>     >? ? ?POST http://www.dst-server.com:22222/v1/test HTTP/1.1
>     >? ? ?x-new-header: {"key": "value"}
>     >? ? ?user-agent: python-requests/2.25.1
>     >? ? ?accept-encoding: gzip, deflate
>     >? ? ?accept: */*
>     >? ? ?content-length: 16
>     >? ? ?content-type: application/json
>     >? ? ?host: www.dst-server.com:22222
>     <http://www.dst-server.com:22222> <http://www.dst-server.com:22222>
> 
> 
>     FYI: The above incomplete ICAP response promises an HTTP request body,
>     both on the ICAP level (req-body) and on the HTTP level (content-length:
>     16).
> 
> 
>     > As I said, I use 'Preview: 0' since I don't mind the body. The
>     question
>     > is whether declaring the body starts at X (req-body=X) is OK even
>     though
>     > I don't have a body to send?
> 
>     It is not OK not to send the body. Encapsulated:req-body does more than
>     declaring where the encapsulated headers end. It also promises an
>     embedded HTTP body after those headers. You must encapsulate the body if
>     the HTTP message should have one. You cannot adapt the header of an HTTP
>     message with a body without also sending the HTTP body (virgin or
>     adapted).
> 
>     Preview is pretty much irrelevant in this context -- the ICAP protocol
>     does not care how the ICAP service gets the HTTP body to include in the
>     ICAP response.
> 
>     There are unofficial ICAP extensions that make it possible to tell the
>     ICAP client to reuse the body it has buffered while adapting the header,
>     but you should get the baseline case working before bothering with those
>     extensions -- they are optimizations that are not applicable to some
>     transactions.
> 
> 
>     > I think having req-null=X is bad since it
>     > probably tells squid that I decided the adapted request?should have no
>     > body, but that's only a guess.
> 
>     If you meant to say "null-body", then you guessed correctly -- null-body
>     means the adapted HTTP message has no body. That is not what you want to
>     say when adapting most HTTP POST messages.
> 
> 
>     HTH,
> 
>     Alex.
> 



From ngtech1ltd at gmail.com  Mon Jan 18 16:53:43 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 18 Jan 2021 18:53:43 +0200
Subject: [squid-users] sslcrtvalidator_program
In-Reply-To: <518aa67f-dcb7-e822-e54f-f2392a03ee5e@measurement-factory.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAGg7dk9mYqJGhbBt3m/ghGwBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACjNZPYF6/+Qof0xJIlu+dJAQAAAAA=@gmail.com>
 <e1f6a657-5f5f-3c17-c9da-b28e03d50a42@treenet.co.nz>
 <02e201d6d1fb$41d06060$c5712120$@gmail.com>
 <23c4a121-c464-e276-b359-f0b5a7262e2f@measurement-factory.com>
 <005701d6d24a$c194dde0$44be99a0$@gmail.com>
 <518aa67f-dcb7-e822-e54f-f2392a03ee5e@measurement-factory.com>
Message-ID: <002201d6edba$7b44ba50$71ce2ef0$@gmail.com>

Hey Alex,

I have tried to read the documentation and to compose a single certificate validation "call" or "request".
The issue with this is that I am unable to do that.
It would help a lot if a single verification request would be public and available to me and maybe others.
The example shows:
0 cert_validate 1519 host=dmz.example-domain.com
cert_0=-----BEGIN CERTIFICATE-----
MIID+DCCA2GgAwIBAgIJAIDcHRUxB2O4MA0GCSqGSIb3DQEBBAUAMIGvMQswCQYD
...
YpVJGt5CJuNfCcB/
-----END CERTIFICATE-----
error_name_0=X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
error_cert_0=cert0

so where ix the 0x01 byte and where are the new lines?
Maybe it's written but I do not see it like in the examples of the external_acl helpres.
My assumption for now is that:
## START
0 cert_validate 1519 host=dmz.example-domain.com0x01
cert_0=-----BEGIN CERTIFICATE-----0x01
MIID+DCCA2GgAwIBAgIJAIDcHRUxB2O4MA0GCSqGSIb3DQEBBAUAMIGvMQswCQYD0x01
...
YpVJGt5CJuNfCcB/0x01
-----END CERTIFICATE-----0x01
error_name_0=X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT0x01
error_cert_0=cert0\n
## END

I am pretty sure I am wrong since the helper I wrote doesn't work.

In bash I thing I can use the next echo:
echo -n -e 'test\x01'

to emulate it but I still don't get it right.

Hope for a hint about the subject.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Monday, December 14, 2020 9:05 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitor <ngtech1ltd at gmail.com>
Subject: Re: [squid-users] sslcrtvalidator_program

On 12/14/20 1:55 PM, Eliezer Croitor wrote:

> We can use this as an example for a single transaction in the wiki:
> https://gist.githubusercontent.com/elico/a0397c879776336eeae569317015edc1/raw/b34dff8ece76e480007a950655efff3564afcccc/cache.log

> Let me know if it's enough to document this subject.

I am not sure I understand your question -- the format is already
documented. If you think that attaching an example of a raw helper
request to that wiki page would help others, please feel free to do so!
Just avoid the implication that all helper requests would have the same
set of fields.

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Monday, December 14, 2020 6:42 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitor <ngtech1ltd at gmail.com>
> Subject: Re: [squid-users] sslcrtvalidator_program
> 
> On 12/14/20 4:26 AM, Eliezer Croitor wrote:
>> So starts with:
>> 0 cert_validate... line
> 
>> And ends with?:
>> error_name_0=X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
>> error_cert_0=cert0
>> ?
> 
> No. The size of the key=value block is specified on the first request
> line. Please try to follow documentation that Amos has pointed you to:
> https://wiki.squid-cache.org/Features/AddonHelpers#SSL_server_certificate_validator
> 
> If that documentation is missing some details, we should fix it.
> 
> 
> 
>> I am unsure, let me try to re-read this section.
>> I am missing a fake helper for this..
>> And a "real world" full example.
> 
>> Can someone simulate it for me?
> 
> Glad you found
> src/security/cert_validators/fake/security_fake_certverify.pl.in. I hope
> it still works!
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
>> Sent: Monday, December 14, 2020 10:15 AM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] sslcrtvalidator_program
>>
>> On 14/12/20 9:11 am, Eliezer Croitor wrote:
>>> I am trying to understand the way the sslcrtvalidator_program  works.
>>> I am pretty sure I have asked this in the past but didn?t found it for some
>>> reason.
>>>
>>> I want to read line by line so.
>>> /^-----BEGIN CERTIFICATE-----$/
>>> ***
>>> /^-----END CERTIFICATE-----$/
>>>
>>> What else should I look for? I was thinking about validating with some extra
>>> values in the request, for example ip/domain:port and sni.
>>> Are these available in some way?
>>
>>
>> The details you need are all here:
>>
>>  
>> <https://wiki.squid-cache.org/Features/AddonHelpers#SSL_server_certificate_validator>
>>
>> Notice that it receives chains of certificates - maybe several, and/or 
>> out of order. Whatever the client sends.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From rousskov at measurement-factory.com  Mon Jan 18 17:04:29 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jan 2021 12:04:29 -0500
Subject: [squid-users] Making destination IP available in ICAP REQMOD
 request
In-Reply-To: <CAGSk-42s3gSjoHfuhVsh+MRY44kDejkXdRjhvw6DjDmGK8Vq-Q@mail.gmail.com>
References: <CAGSk-42s3gSjoHfuhVsh+MRY44kDejkXdRjhvw6DjDmGK8Vq-Q@mail.gmail.com>
Message-ID: <373f2d94-e3d6-a2fa-e0fe-f524a80371d5@measurement-factory.com>

On 1/17/21 5:28 PM, Moti Berger wrote:
> I couldn't figure out a way to start the DNS resolving just before
> the?startAdaptation starts as it requires all sorts of objects that seem
> to be unavailable there.

Please ask development questions on squid-dev:
http://www.squid-cache.org/Support/mailing-lists.html#squid-dev

Triggering a DNS lookup before REQMOD is easy, but waiting for its
asynchronous result requires a bit of development work outside this
mailing list scope.

More importantly, a DNS lookup alone is probably not enough. If your
REQMOD service relies on the destination address that Squid will use
when/if forwarding the request, then Squid should be modified to
(optionally) decide on the forwarding destination first, _before_
adapting the request and then, to the extent possible, stick with that
original decision after REQMOD. This may be equivalent to implementing
post-cache REQMOD!

It is possible to implement all that, and there are legitimate use cases
that would benefit from such functionality, but a quality implementation
will require serious development work. If you are sure that you need
this functionality, and you want to implement it yourself, then I
recommend starting with an RFC on squid-dev:
https://wiki.squid-cache.org/MergeProcedure#Before_you_start_coding


HTH,

Alex.


From ngtech1ltd at gmail.com  Mon Jan 18 17:04:36 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 18 Jan 2021 19:04:36 +0200
Subject: [squid-users] Trying to verify couple tls issues
Message-ID: <002301d6edbc$00852a50$018f7ef0$@gmail.com>

I wrote the next "helping/helper/testing scripts":
https://github.com/elico/tls-check-script/blob/master/tls-check.rb
https://github.com/elico/tls-check-script/blob/master/check-dns-san.sh

Now I am trying to verify what issues exists that causes squid to this
result:
2021/01/18 18:54:47 kid1| Error negotiating SSL connection on FD 46:
error:00000001:lib(0):func(0):reason(1) (1/-1)
    connection: conn407043 local=161.117.96.220:443 remote=192.16.XYZ
flags=33

So the output of: bash check-dns-san.sh 161.117.96.220 443 is:
## START
Can't use SSL_get_servername
depth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert
Global Root CA
verify return:1
depth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = RapidSSL RSA
CA 2018
verify return:1
depth=0 CN = data.mistat.intl.xiaomi.com
verify return:1
DONE
X509v3 Subject Alternative Name:
    DNS:data.mistat.intl.xiaomi.com
## END

And then I am testing with the next command: ruby tls-check.rb
161.117.96.220 443 and the output is:
## START
### Number of Ciphers to be tested: 66
### Timeout per test: 3
### Delay between tests: 1
Testing TLS_AES_256_GCM_SHA384...  NO, SSL_CTX_set_cipher_list
Testing TLS_CHACHA20_POLY1305_SHA256...  NO, SSL_CTX_set_cipher_list
Testing TLS_AES_128_GCM_SHA256...  NO, SSL_CTX_set_cipher_list
Testing TLS_AES_128_CCM_SHA256...  NO, SSL_CTX_set_cipher_list
Testing ECDHE-ECDSA-AES256-GCM-SHA384...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-AES256-GCM-SHA384...  CONNECTED:
ECDHE-RSA-AES256-GCM-SHA384, YES, Secure Renegotiation IS supported
Testing DHE-RSA-AES256-GCM-SHA384...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-CHACHA20-POLY1305...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-CHACHA20-POLY1305...  NO, sslv3 alert handshake failure
Testing DHE-RSA-CHACHA20-POLY1305...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES256-CCM8...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES256-CCM...  NO, sslv3 alert handshake failure
Testing DHE-RSA-AES256-CCM8...  NO, sslv3 alert handshake failure
Testing DHE-RSA-AES256-CCM...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
Testing ECDHE-ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
Testing DHE-RSA-ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES128-GCM-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-AES128-GCM-SHA256...  CONNECTED:
ECDHE-RSA-AES128-GCM-SHA256, YES, Secure Renegotiation IS supported
Testing DHE-RSA-AES128-GCM-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES128-CCM8...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES128-CCM...  NO, sslv3 alert handshake failure
Testing DHE-RSA-AES128-CCM8...  NO, sslv3 alert handshake failure
Testing DHE-RSA-AES128-CCM...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
Testing DHE-RSA-ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES256-SHA384...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-AES256-SHA384...  CONNECTED: ECDHE-RSA-AES256-SHA384, YES,
Secure Renegotiation IS supported
Testing DHE-RSA-AES256-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-CAMELLIA256-SHA384...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-CAMELLIA256-SHA384...  NO, sslv3 alert handshake failure
Testing DHE-RSA-CAMELLIA256-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES128-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-AES128-SHA256...  CONNECTED: ECDHE-RSA-AES128-SHA256, YES,
Secure Renegotiation IS supported
Testing DHE-RSA-AES128-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
Testing DHE-RSA-CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES256-SHA...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-AES256-SHA...  CONNECTED: ECDHE-RSA-AES256-SHA, YES,
Secure Renegotiation IS supported
Testing DHE-RSA-AES256-SHA...  NO, sslv3 alert handshake failure
Testing DHE-RSA-CAMELLIA256-SHA...  NO, sslv3 alert handshake failure
Testing ECDHE-ECDSA-AES128-SHA...  NO, sslv3 alert handshake failure
Testing ECDHE-RSA-AES128-SHA...  CONNECTED: ECDHE-RSA-AES128-SHA, YES,
Secure Renegotiation IS supported
Testing DHE-RSA-AES128-SHA...  NO, sslv3 alert handshake failure
Testing DHE-RSA-CAMELLIA128-SHA...  NO, sslv3 alert handshake failure
Testing AES256-GCM-SHA384...  CONNECTED: AES256-GCM-SHA384, YES, Secure
Renegotiation IS supported
Testing AES256-CCM8...  NO, sslv3 alert handshake failure
Testing AES256-CCM...  NO, sslv3 alert handshake failure
Testing ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
Testing AES128-GCM-SHA256...  CONNECTED: AES128-GCM-SHA256, YES, Secure
Renegotiation IS supported
Testing AES128-CCM8...  NO, sslv3 alert handshake failure
Testing AES128-CCM...  NO, sslv3 alert handshake failure
Testing ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
Testing AES256-SHA256...  CONNECTED: AES256-SHA256, YES, Secure
Renegotiation IS supported
Testing CAMELLIA256-SHA256...  NO, sslv3 alert handshake failure
Testing AES128-SHA256...  CONNECTED: AES128-SHA256, YES, Secure
Renegotiation IS supported
Testing CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
Testing AES256-SHA...  CONNECTED: AES256-SHA, YES, Secure Renegotiation IS
supported
Testing CAMELLIA256-SHA...  NO, sslv3 alert handshake failure
Testing AES128-SHA...  CONNECTED: AES128-SHA, YES, Secure Renegotiation IS
supported
Testing CAMELLIA128-SHA...  NO, sslv3 alert handshake failure
Testing DHE-RSA-SEED-SHA...  NO, sslv3 alert handshake failure
Testing SEED-SHA...  NO, sslv3 alert handshake failure
Testing IDEA-CBC-SHA...  NO, ssl_cipher_process_rulestr
## END

I assume that the above results might give a clue why mentioned error line:
2021/01/18 18:54:47 kid1| Error negotiating SSL connection on FD 46:
error:00000001:lib(0):func(0):reason(1) (1/-1)
    connection: conn407043 local=161.117.96.220:443 remote=192.16.XYZ
flags=33

happens. However I am not sure.
Are there any config that might affect this negotiation in squid?

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon





From ngtech1ltd at gmail.com  Mon Jan 18 17:45:28 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 18 Jan 2021 19:45:28 +0200
Subject: [squid-users] What is this access.log line?
Message-ID: <002401d6edc1$b6442a80$22cc7f80$@gmail.com>

While testing 5.0.4 I am seeing this line:

1610991736.039      0 192.168.189.48 NONE_NONE/400 3798 CNT
error:invalid-request - HIER_NONE/- text/html -

 

I was thinking about adding to the log line the incoming port ie either
intercept plain http port 80 or the ssl-bump port.

 

What is the CNT means?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210118/b2cd53a2/attachment.htm>

From rousskov at measurement-factory.com  Mon Jan 18 18:39:53 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jan 2021 13:39:53 -0500
Subject: [squid-users] sslcrtvalidator_program
In-Reply-To: <002201d6edba$7b44ba50$71ce2ef0$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAGg7dk9mYqJGhbBt3m/ghGwBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACjNZPYF6/+Qof0xJIlu+dJAQAAAAA=@gmail.com>
 <e1f6a657-5f5f-3c17-c9da-b28e03d50a42@treenet.co.nz>
 <02e201d6d1fb$41d06060$c5712120$@gmail.com>
 <23c4a121-c464-e276-b359-f0b5a7262e2f@measurement-factory.com>
 <005701d6d24a$c194dde0$44be99a0$@gmail.com>
 <518aa67f-dcb7-e822-e54f-f2392a03ee5e@measurement-factory.com>
 <002201d6edba$7b44ba50$71ce2ef0$@gmail.com>
Message-ID: <4659fb94-7d2c-69cd-c1c2-11ad990a8d99@measurement-factory.com>

On 1/18/21 11:53 AM, Eliezer Croitoru wrote:

> I have tried to read the documentation and to compose a single certificate validation "call" or "request".

> It would help a lot if a single verification request would be public and available to me and maybe others.

As I said, please feel free to add that example to the wiki. I do not
have one, but you should be able to collect a sample using strace or
helper debugging.


> The example shows:

> 0 cert_validate 1519 host=dmz.example-domain.com
> cert_0=-----BEGIN CERTIFICATE-----
> MIID+DCCA2GgAwIBAgIJAIDcHRUxB2O4MA0GCSqGSIb3DQEBBAUAMIGvMQswCQYD
> ...
> YpVJGt5CJuNfCcB/
> -----END CERTIFICATE-----
> error_name_0=X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
> error_cert_0=cert0

> so where ix the 0x01 byte 

I have not checked carefully, but I do not think the 0x01 delimiter is
used for certificate generation or validation requests. Their framing
should be size-based, not EOM-delimiter based -- it does not make sense
to use both at once! If you can confirm that suspicion, you should fix
Squid wiki accordingly.


> and where are the new lines?

Probably where you see them in the sample.


> Hope for a hint about the subject.

You should be able to collect it using strace or by adding debugging to
a test helper that simply prints everything it receives, using, say,
c-string escapes or URL encoding for any special character.


HTH,

Alex.



> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Monday, December 14, 2020 9:05 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitor <ngtech1ltd at gmail.com>
> Subject: Re: [squid-users] sslcrtvalidator_program
> 
> On 12/14/20 1:55 PM, Eliezer Croitor wrote:
> 
>> We can use this as an example for a single transaction in the wiki:
>> https://gist.githubusercontent.com/elico/a0397c879776336eeae569317015edc1/raw/b34dff8ece76e480007a950655efff3564afcccc/cache.log
> 
>> Let me know if it's enough to document this subject.
> 
> I am not sure I understand your question -- the format is already
> documented. If you think that attaching an example of a raw helper
> request to that wiki page would help others, please feel free to do so!
> Just avoid the implication that all helper requests would have the same
> set of fields.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Monday, December 14, 2020 6:42 PM
>> To: squid-users at lists.squid-cache.org
>> Cc: Eliezer Croitor <ngtech1ltd at gmail.com>
>> Subject: Re: [squid-users] sslcrtvalidator_program
>>
>> On 12/14/20 4:26 AM, Eliezer Croitor wrote:
>>> So starts with:
>>> 0 cert_validate... line
>>
>>> And ends with?:
>>> error_name_0=X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
>>> error_cert_0=cert0
>>> ?
>>
>> No. The size of the key=value block is specified on the first request
>> line. Please try to follow documentation that Amos has pointed you to:
>> https://wiki.squid-cache.org/Features/AddonHelpers#SSL_server_certificate_validator
>>
>> If that documentation is missing some details, we should fix it.
>>
>>
>>
>>> I am unsure, let me try to re-read this section.
>>> I am missing a fake helper for this..
>>> And a "real world" full example.
>>
>>> Can someone simulate it for me?
>>
>> Glad you found
>> src/security/cert_validators/fake/security_fake_certverify.pl.in. I hope
>> it still works!
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
>>> Sent: Monday, December 14, 2020 10:15 AM
>>> To: squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] sslcrtvalidator_program
>>>
>>> On 14/12/20 9:11 am, Eliezer Croitor wrote:
>>>> I am trying to understand the way the sslcrtvalidator_program  works.
>>>> I am pretty sure I have asked this in the past but didn?t found it for some
>>>> reason.
>>>>
>>>> I want to read line by line so.
>>>> /^-----BEGIN CERTIFICATE-----$/
>>>> ***
>>>> /^-----END CERTIFICATE-----$/
>>>>
>>>> What else should I look for? I was thinking about validating with some extra
>>>> values in the request, for example ip/domain:port and sni.
>>>> Are these available in some way?
>>>
>>>
>>> The details you need are all here:
>>>
>>>  
>>> <https://wiki.squid-cache.org/Features/AddonHelpers#SSL_server_certificate_validator>
>>>
>>> Notice that it receives chains of certificates - maybe several, and/or 
>>> out of order. Whatever the client sends.
>>>
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Mon Jan 18 18:44:51 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jan 2021 13:44:51 -0500
Subject: [squid-users] What is this access.log line?
In-Reply-To: <002401d6edc1$b6442a80$22cc7f80$@gmail.com>
References: <002401d6edc1$b6442a80$22cc7f80$@gmail.com>
Message-ID: <521687ab-7670-5b05-2c52-a9ef1d3e1179@measurement-factory.com>

On 1/18/21 12:45 PM, Eliezer Croitoru wrote:
> While testing 5.0.4 I am seeing this line:
> 
> 1610991736.039????? 0 192.168.189.48 NONE_NONE/400 3798 CNT
> error:invalid-request - HIER_NONE/- text/html ?
> 
> What is the CNT means?

If you see CNT where the request method usually is, then these are
probably the first three characters in a (malformed from Squid point of
view) request that Squid has received. A packet trace should give you
the exact answer.

HTH,

Alex.


From squid3 at treenet.co.nz  Mon Jan 18 18:45:31 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2021 07:45:31 +1300
Subject: [squid-users] Trying to verify couple tls issues
In-Reply-To: <002301d6edbc$00852a50$018f7ef0$@gmail.com>
References: <002301d6edbc$00852a50$018f7ef0$@gmail.com>
Message-ID: <8116b402-9606-f189-1359-c1b4673c179e@treenet.co.nz>

On 19/01/21 6:04 am, Eliezer Croitoru wrote:
> I wrote the next "helping/helper/testing scripts":
> https://github.com/elico/tls-check-script/blob/master/tls-check.rb
> https://github.com/elico/tls-check-script/blob/master/check-dns-san.sh
> 
> Now I am trying to verify what issues exists that causes squid to this
> result:
> 2021/01/18 18:54:47 kid1| Error negotiating SSL connection on FD 46:
> error:00000001:lib(0):func(0):reason(1) (1/-1)
>      connection: conn407043 local=161.117.96.220:443 remote=192.16.XYZ
> flags=33
> 
> So the output of: bash check-dns-san.sh 161.117.96.220 443 is:
> ## START
> Can't use SSL_get_servername
> depth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert
> Global Root CA
> verify return:1
> depth=1 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = RapidSSL RSA
> CA 2018
> verify return:1
> depth=0 CN = data.mistat.intl.xiaomi.com
> verify return:1
> DONE
> X509v3 Subject Alternative Name:
>      DNS:data.mistat.intl.xiaomi.com
> ## END
> 
> And then I am testing with the next command: ruby tls-check.rb
> 161.117.96.220 443 and the output is:
> ## START
> ### Number of Ciphers to be tested: 66
> ### Timeout per test: 3
> ### Delay between tests: 1
> Testing TLS_AES_256_GCM_SHA384...  NO, SSL_CTX_set_cipher_list
> Testing TLS_CHACHA20_POLY1305_SHA256...  NO, SSL_CTX_set_cipher_list
> Testing TLS_AES_128_GCM_SHA256...  NO, SSL_CTX_set_cipher_list
> Testing TLS_AES_128_CCM_SHA256...  NO, SSL_CTX_set_cipher_list
> Testing ECDHE-ECDSA-AES256-GCM-SHA384...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-AES256-GCM-SHA384...  CONNECTED:
> ECDHE-RSA-AES256-GCM-SHA384, YES, Secure Renegotiation IS supported
> Testing DHE-RSA-AES256-GCM-SHA384...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-CHACHA20-POLY1305...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-CHACHA20-POLY1305...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-CHACHA20-POLY1305...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES256-CCM8...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES256-CCM...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-AES256-CCM8...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-AES256-CCM...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
> Testing ECDHE-ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES128-GCM-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-AES128-GCM-SHA256...  CONNECTED:
> ECDHE-RSA-AES128-GCM-SHA256, YES, Secure Renegotiation IS supported
> Testing DHE-RSA-AES128-GCM-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES128-CCM8...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES128-CCM...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-AES128-CCM8...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-AES128-CCM...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES256-SHA384...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-AES256-SHA384...  CONNECTED: ECDHE-RSA-AES256-SHA384, YES,
> Secure Renegotiation IS supported
> Testing DHE-RSA-AES256-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-CAMELLIA256-SHA384...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-CAMELLIA256-SHA384...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-CAMELLIA256-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES128-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-AES128-SHA256...  CONNECTED: ECDHE-RSA-AES128-SHA256, YES,
> Secure Renegotiation IS supported
> Testing DHE-RSA-AES128-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES256-SHA...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-AES256-SHA...  CONNECTED: ECDHE-RSA-AES256-SHA, YES,
> Secure Renegotiation IS supported
> Testing DHE-RSA-AES256-SHA...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-CAMELLIA256-SHA...  NO, sslv3 alert handshake failure
> Testing ECDHE-ECDSA-AES128-SHA...  NO, sslv3 alert handshake failure
> Testing ECDHE-RSA-AES128-SHA...  CONNECTED: ECDHE-RSA-AES128-SHA, YES,
> Secure Renegotiation IS supported
> Testing DHE-RSA-AES128-SHA...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-CAMELLIA128-SHA...  NO, sslv3 alert handshake failure
> Testing AES256-GCM-SHA384...  CONNECTED: AES256-GCM-SHA384, YES, Secure
> Renegotiation IS supported
> Testing AES256-CCM8...  NO, sslv3 alert handshake failure
> Testing AES256-CCM...  NO, sslv3 alert handshake failure
> Testing ARIA256-GCM-SHA384...  NO, sslv3 alert handshake failure
> Testing AES128-GCM-SHA256...  CONNECTED: AES128-GCM-SHA256, YES, Secure
> Renegotiation IS supported
> Testing AES128-CCM8...  NO, sslv3 alert handshake failure
> Testing AES128-CCM...  NO, sslv3 alert handshake failure
> Testing ARIA128-GCM-SHA256...  NO, sslv3 alert handshake failure
> Testing AES256-SHA256...  CONNECTED: AES256-SHA256, YES, Secure
> Renegotiation IS supported
> Testing CAMELLIA256-SHA256...  NO, sslv3 alert handshake failure
> Testing AES128-SHA256...  CONNECTED: AES128-SHA256, YES, Secure
> Renegotiation IS supported
> Testing CAMELLIA128-SHA256...  NO, sslv3 alert handshake failure
> Testing AES256-SHA...  CONNECTED: AES256-SHA, YES, Secure Renegotiation IS
> supported
> Testing CAMELLIA256-SHA...  NO, sslv3 alert handshake failure
> Testing AES128-SHA...  CONNECTED: AES128-SHA, YES, Secure Renegotiation IS
> supported
> Testing CAMELLIA128-SHA...  NO, sslv3 alert handshake failure
> Testing DHE-RSA-SEED-SHA...  NO, sslv3 alert handshake failure
> Testing SEED-SHA...  NO, sslv3 alert handshake failure
> Testing IDEA-CBC-SHA...  NO, ssl_cipher_process_rulestr
> ## END
> 
> I assume that the above results might give a clue why mentioned error line:
> 2021/01/18 18:54:47 kid1| Error negotiating SSL connection on FD 46:
> error:00000001:lib(0):func(0):reason(1) (1/-1)
>      connection: conn407043 local=161.117.96.220:443 remote=192.16.XYZ
> flags=33

Take the output above and grep "CONNECTED: ". If the client or Squid do 
not support those combinations, the above error will result when 
connecting to that server.


 > Testing ECDHE-RSA-AES256-GCM-SHA384...  CONNECTED:
 > ECDHE-RSA-AES256-GCM-SHA384, YES, Secure Renegotiation IS supported

 > Testing ECDHE-RSA-AES128-GCM-SHA256...CONNECTED:
 > ECDHE-RSA-AES128-GCM-SHA256, YES, Secure Renegotiation IS supported

 > Testing ECDHE-RSA-AES256-SHA384...CONNECTED: ECDHE-RSA-AES256-SHA384,
 > Testing ECDHE-RSA-AES128-SHA256...CONNECTED: ECDHE-RSA-AES128-SHA256,
 > Testing ECDHE-RSA-AES256-SHA...  CONNECTED: ECDHE-RSA-AES256-SHA, YES,
 > Testing AES256-GCM-SHA384...  CONNECTED: AES256-GCM-SHA384, YES,
 > Testing AES128-GCM-SHA256...  CONNECTED: AES128-GCM-SHA256, YES,
 > Testing AES256-SHA256...  CONNECTED: AES256-SHA256, YES, Secure
 > Testing AES128-SHA256...  CONNECTED: AES128-SHA256, YES, Secure
 > Testing AES256-SHA...  CONNECTED: AES256-SHA, YES, Secure
 > Testing AES128-SHA...  CONNECTED: AES128-SHA, YES, Secure
 > ## END


> 
> happens. However I am not sure.
> Are there any config that might affect this negotiation in squid?


When either SHA or AES are not possible for Squid to use it will happen. 
Depending on whether your Squid is doing bumping or not will will 
determine whether it is possible to affect with a configuration change 
or if the issue is the client software.


Amos


From squid3 at treenet.co.nz  Mon Jan 18 19:06:01 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Jan 2021 08:06:01 +1300
Subject: [squid-users] sslcrtvalidator_program
In-Reply-To: <002201d6edba$7b44ba50$71ce2ef0$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAGg7dk9mYqJGhbBt3m/ghGwBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACjNZPYF6/+Qof0xJIlu+dJAQAAAAA=@gmail.com>
 <e1f6a657-5f5f-3c17-c9da-b28e03d50a42@treenet.co.nz>
 <02e201d6d1fb$41d06060$c5712120$@gmail.com>
 <23c4a121-c464-e276-b359-f0b5a7262e2f@measurement-factory.com>
 <005701d6d24a$c194dde0$44be99a0$@gmail.com>
 <518aa67f-dcb7-e822-e54f-f2392a03ee5e@measurement-factory.com>
 <002201d6edba$7b44ba50$71ce2ef0$@gmail.com>
Message-ID: <65ebf6ba-359d-f57d-655d-fa5ef0f7082c@treenet.co.nz>

On 19/01/21 5:53 am, Eliezer Croitoru wrote:
> Hey Alex,
> 
> I have tried to read the documentation and to compose a single certificate validation "call" or "request".
> The issue with this is that I am unable to do that.
> It would help a lot if a single verification request would be public and available to me and maybe others.
> The example shows:
> 0 cert_validate 1519 host=dmz.example-domain.com
> cert_0=-----BEGIN CERTIFICATE-----
> MIID+DCCA2GgAwIBAgIJAIDcHRUxB2O4MA0GCSqGSIb3DQEBBAUAMIGvMQswCQYD
> ...
> YpVJGt5CJuNfCcB/
> -----END CERTIFICATE-----
> error_name_0=X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT
> error_cert_0=cert0
> 
> so where ix the 0x01 byte and where are the new lines?


The \0x1 is the "logical line" terminator for the helper query. Which 
means it goes last. \n are used by the PEM format for certificates.


Amos


From rousskov at measurement-factory.com  Mon Jan 18 21:04:49 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jan 2021 16:04:49 -0500
Subject: [squid-users] Peer selection based on IP with multiple ports?
In-Reply-To: <CAGCa14rx6qdh=WnS81Fs2f=6N5iu-9jgLeeNEwrm5khQ4hjXAQ@mail.gmail.com>
References: <CAGCa14rx6qdh=WnS81Fs2f=6N5iu-9jgLeeNEwrm5khQ4hjXAQ@mail.gmail.com>
Message-ID: <ab889689-20a4-f85a-b682-a6167ab4d22b@measurement-factory.com>

On 1/16/21 7:52 AM, roee klinger wrote:

> I am using Squid to route users to different peers based on their
> usernames, I was asked to add support for IP whitelisting recently but I
> ran into an issue.?
> 
> If one IP wants to access to different peers, I will have to do it based
> on on the listening port number, as there is no other way to
> differentiate?the traffic,

If Amos suggestion does not apply, please note that I do not understand
the last part of your assertion above. You are already differentiating
and routing traffic by user names. Why do you feel there is a need to
add some listening port-based rules to your configuration?

Alex.


From vinod9987 at gmail.com  Tue Jan 19 04:43:25 2021
From: vinod9987 at gmail.com (vinod mg)
Date: Tue, 19 Jan 2021 10:13:25 +0530
Subject: [squid-users] Change cipher suite ordering
In-Reply-To: <CALLSHhxHZEPTe2PRa-jX_HYtm8nKXqgVsu1EaoK3+Zn2rOFh6Q@mail.gmail.com>
References: <CALLSHhyhjwb8Z8wvY6aRC7SHy-va_KRXgtxRwtBV_tn6hQ+_og@mail.gmail.com>
 <34306cbe-c404-0fad-d331-0d63fc47f896@treenet.co.nz>
 <CALLSHhxHZEPTe2PRa-jX_HYtm8nKXqgVsu1EaoK3+Zn2rOFh6Q@mail.gmail.com>
Message-ID: <CALLSHhzEazNdUpxfZnQ2o60+-nHAwQA+77bxED3QKGrp=hGBCg@mail.gmail.com>

I have been trying to make this work but still no luck, Any help
is appreciated.

Thanks,
Vinod

On Tue, Jan 12, 2021 at 4:34 PM vinod mg <vinod9987 at gmail.com> wrote:

> Hi Amos,
>
> Thanks for responding, really appreciate the quick response.
>
> So yes if squid can mimic exactly what client is sending that all I am
> looking for, but here its not the case, as you can see below example squid
> is re-arranging the cipher list which I do not want.
>
> Below is the default cipher list order I got with plain firefox browsing
> howsmyssl.com <https://www.howsmyssl.com/> without proxy -
>
>    - TLS_AES_128_GCM_SHA256
>    - TLS_CHACHA20_POLY1305_SHA256
>    - TLS_AES_256_GCM_SHA384
>    - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
>    - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
>    - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
>    - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
>    - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
>    - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
>    - TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA
>    - TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA
>    - TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
>    - TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
>    - TLS_RSA_WITH_AES_128_GCM_SHA256
>    - TLS_RSA_WITH_AES_256_GCM_SHA384
>    - TLS_RSA_WITH_AES_128_CBC_SHA
>    - TLS_RSA_WITH_AES_256_CBC_SHA
>    - TLS_RSA_WITH_3DES_EDE_CBC_SHA
>
> Below is the cipher list order I got with same firefox browsing
> howsmyssl.com <https://www.howsmyssl.com/> with explicit squid
> proxy configured -
>
>    - TLS_AES_256_GCM_SHA384
>    - TLS_CHACHA20_POLY1305_SHA256
>    - TLS_AES_128_GCM_SHA256
>    - TLS_AES_128_CCM_SHA256
>    - TLS_RSA_WITH_AES_256_CBC_SHA
>    - TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
>    - TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
>    - TLS_RSA_WITH_AES_256_GCM_SHA384
>    - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
>    - TLS_RSA_WITH_AES_128_CBC_SHA
>    - TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA
>    - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
>    - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
>    - TLS_RSA_WITH_3DES_EDE_CBC_SHA
>    - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
>    - TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA
>    - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
>    - TLS_RSA_WITH_AES_128_GCM_SHA256
>    - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
>    - TLS_EMPTY_RENEGOTIATION_INFO_SCSV
>
> I have tried removing "cipher=" from both "tls_outgoing_options" and
> "http_port" but still cipher list sent by client is changed
> while its passing via squid, Please let me know if I am missing anything.
>
> Thanks,
> Vinod
>
> On Tue, Jan 12, 2021 at 3:20 PM Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 12/01/21 5:44 pm, vinod mg wrote:
>> > Hello Team,
>> >
>> > I need some help in configuring cipher suite ordering. I am using squid
>> > with SSL configs and trying to configure the cipher order but not able
>> > to do so, I am using below sites to check my chipher ordering and its
>> > showing different ordering then what I have configured.
>> >
>> > https://www.howsmyssl.com <https://www.howsmyssl.com>
>> > https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html
>> > <https://clienttest.ssllabs.com:8443/ssltest/viewMyClient.html>
>> >
>>
>> These sites show what the client is sending. Modern Squid mimic what the
>> Browser sends in as closely as possible to avoid issues being added.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210119/53747df5/attachment.htm>

From moberger at metanetworks.com  Wed Jan 20 07:52:10 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Wed, 20 Jan 2021 09:52:10 +0200
Subject: [squid-users] Adding headers in ICAP server with no preview
In-Reply-To: <2dbee277-e239-0759-9b79-cf857db25088@measurement-factory.com>
References: <CAGSk-42+=OHw=BoJkbBjb7HinNAHjm-uZQ-mJEC5JRzVpfADQg@mail.gmail.com>
 <81302836-3c5f-fb18-8a09-0539540b2702@measurement-factory.com>
 <CAGSk-42Fqie5b3MFDwzjWL92-e+yZAjt8_9kMia0v9Q1V1atDw@mail.gmail.com>
 <2dbee277-e239-0759-9b79-cf857db25088@measurement-factory.com>
Message-ID: <CAGSk-43FNwXK0giAyhbA8HHwRatee1KrCo+=nSsXX8xEDoREmw@mail.gmail.com>

Thank you very much, that really helped!

On Mon, Jan 18, 2021 at 6:48 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/18/21 6:45 AM, Moti Berger wrote:
>
> > If the ICAP server sets 'Preview: 0' in the OPTIONS it means that when
> > the ICAP client sends a request, it should not contain the body.
>
> The above summary may mislead many readers. I would describe the
> protocol operation differently:
>
> * Preview in an OPTIONS response indicates that the server supports
> Preview in general and specifies the maximum Preview size the client
> should use (e.g., Preview:0 limits Preview to HTTP headers).
>
> * The Preview mode for a specific REQMOD or RESPMOD transaction is
> signaled in the corresponding REQMOD or RESPMOD request (not a previous
> OPTIONS response) by adding a Preview:N ICAP request header (Preview:0
> specifies a headers-only Preview for the current transaction).
>
> * The REQMOD or RESPMOD transaction with a Preview:0 request header is
> split into two phases. During the first phase, the client must not send
> the virgin body. During the second phase, if any, the client must send
> the virgin body. Both phases comprise a single ICAP transaction, with a
> single ICAP request and a single ICAP response. Thus, one cannot say
> that this transaction (as a whole) "should not contain a body".
>
>
> > This is the REQMOD request:
> >
> >     F..n...DREQMOD icap://censor-req.proxy:14590/request ICAP/1.0
> >     Host: censor-req.proxy:14590
> >     Date: Mon, 18 Jan 2021 11:34:54 GMT
> >     Encapsulated: req-hdr=0, req-body=222
> >     Preview: 0
> >     Allow: 204, trailers
> >     X-custom-header: data
> >
> >     POST http://www.dst-server.com:22222/v1/test HTTP/1.1
> >     User-Agent: python-requests/2.25.1
> >     Accept-Encoding: gzip, deflate
> >     Accept: */*
> >     Content-Length: 10
> >     Content-Type: application/json
> >     Host: www.dst-server.com:22222 <http://www.dst-server.com:22222>
>
> > The ICAP 'Encapsulated' header has a req-body even though no 'body'
> > should be in this request.
>
> Not exactly. The request may not be over at this point. Please see my
> third bullet above for details.
>
>
> > The ICAP server has no way to encapsulate the HTTP request body if it
> > didn't get it.
>
> To get the request body in Preview:0 mode, the ICAP server must respond
> with ICAP 100 (Continue).
>
>
> > I want to avoid sending the body because the adaptation is body agnostic.
>
> Yes, I know, but you have to work within the ICAP protocol boundaries.
> ICAP simply does not optimize your use case! After you have the basics
> working well, you can invest in implementing a use-original-body ICAP
> extension[1] that, in _some_ cases, can prevent the body exchange while
> adapting HTTP headers.
>
> Alternatively, you can use an existing (extendible) ICAP server to do
> the legwork for you [2]. Many individuals and companies have learned the
> hard way that implementing an ICAP service correctly from scratch is
> very difficult and often prohibitively expensive.
>
> [1]
>
> http://www.icap-forum.org/documents/specification/draft-icap-ext-partial-content-07.txt
>
> [2] https://wiki.squid-cache.org/Features/ICAP#ICAP_Servers
>
>
> HTH,
>
> Alex.
>
>
>
> > On Sun, Jan 17, 2021 at 11:34 PM Alex Rousskov wrote:
> >
> >     On 1/17/21 3:08 PM, Moti Berger wrote:
> >     > What should the ICAP response look like?
> >
> >     The vast majority off ICAP responses containing an HTTP POST message
> >     will look like ICAP header + HTTP header + HTTP body. Please see RFC
> >     3507 and its errata for examples of and discussion about those three
> >     components. It should help avoid guessing and developing by examples
> >     (which usually leads to bugs, especially where ICAP is involved).
> >
> >
> >     > What I do is to reply like this:
> >     >
> >     >     (dI./M..ICAP/1.0 200 OK
> >     >     ISTag: "SjIzlRA4te41axxcDOoiSl6rBRg4ZK"
> >     >     Date: Sun, 17 Jan 2021 19:34:12 GMT
> >     >     Server: BaseICAP/1.0 Python/3.6.12
> >     >     Encapsulated: req-hdr=0, req-body=360
> >     >
> >     >     POST http://www.dst-server.com:22222/v1/test HTTP/1.1
> >     >     x-new-header: {"key": "value"}
> >     >     user-agent: python-requests/2.25.1
> >     >     accept-encoding: gzip, deflate
> >     >     accept: */*
> >     >     content-length: 16
> >     >     content-type: application/json
> >     >     host: www.dst-server.com:22222
> >     <http://www.dst-server.com:22222> <http://www.dst-server.com:22222>
> >
> >
> >     FYI: The above incomplete ICAP response promises an HTTP request
> body,
> >     both on the ICAP level (req-body) and on the HTTP level
> (content-length:
> >     16).
> >
> >
> >     > As I said, I use 'Preview: 0' since I don't mind the body. The
> >     question
> >     > is whether declaring the body starts at X (req-body=X) is OK even
> >     though
> >     > I don't have a body to send?
> >
> >     It is not OK not to send the body. Encapsulated:req-body does more
> than
> >     declaring where the encapsulated headers end. It also promises an
> >     embedded HTTP body after those headers. You must encapsulate the
> body if
> >     the HTTP message should have one. You cannot adapt the header of an
> HTTP
> >     message with a body without also sending the HTTP body (virgin or
> >     adapted).
> >
> >     Preview is pretty much irrelevant in this context -- the ICAP
> protocol
> >     does not care how the ICAP service gets the HTTP body to include in
> the
> >     ICAP response.
> >
> >     There are unofficial ICAP extensions that make it possible to tell
> the
> >     ICAP client to reuse the body it has buffered while adapting the
> header,
> >     but you should get the baseline case working before bothering with
> those
> >     extensions -- they are optimizations that are not applicable to some
> >     transactions.
> >
> >
> >     > I think having req-null=X is bad since it
> >     > probably tells squid that I decided the adapted request should
> have no
> >     > body, but that's only a guess.
> >
> >     If you meant to say "null-body", then you guessed correctly --
> null-body
> >     means the adapted HTTP message has no body. That is not what you
> want to
> >     say when adapting most HTTP POST messages.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210120/9c446f8a/attachment.htm>

From squid.org at bloms.de  Wed Jan 20 11:25:46 2021
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 20 Jan 2021 12:25:46 +0100
Subject: [squid-users] chromium based browsers don't play a video,
 when sslbump is enabled
Message-ID: <20210120112546.eo7pw22bbebqbzws@bloms.de>

Hello,

I use squid 4.13 with enabled sslbump.
Chromium based browsers like chrome and edge don't play this video
https://admin.wissen-ad.de/storage/TEST/Big_Buck_Bunny_1080_10s_30MB.mp4
The firefox browser and the old internet explorer have no problems.

When I disable sslbumping for this destination the chromium based
browsers work as well.

Here are some parts of my config:

--snip--
http_port MYIP:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=32MB cert=/etc/squid/cert.pem key=/etc/squid/key.pem tls-dh=/etc/squid/dhparams.pem
sslcrtd_program /usr/sbin/security_file_certgen -s /var/cache/squid/sslcert_db -M 32MB
sslcrtd_children 32 startup=10 idle=3
tls_outgoing_options capath=/etc/ssl/certs min-version=1.2
tls_outgoing_options cipher=TLSv1.2:+aRSA:+SHA384:+SHA256:+DH:-kRSA:!PSK:!eNULL:!aNULL:!DSS:!AESCCM:!CAMELLIA:!ARIA:AES256-SHA:AES128-SHA:@SECLEVEL=1

acl nobumping dstdomain "/etc/squid/nohttpsscan.domains"
ssl_bump splice nobumping
ssl_bump bump all
--snip--

with wget or curl I can download the mp4 file in both cases (with and without sslbump)

Can anybody try to view the video in a chromium based browser with enabled sslbump ?

Thank you very much.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From heimarbeit123.99 at web.de  Wed Jan 20 13:50:57 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Wed, 20 Jan 2021 14:50:57 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
Message-ID: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210120/78061c24/attachment.htm>

From ngtech1ltd at gmail.com  Wed Jan 20 14:51:03 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 20 Jan 2021 16:51:03 +0200
Subject: [squid-users] chromium based browsers don't play a video,
 when sslbump is enabled
In-Reply-To: <20210120112546.eo7pw22bbebqbzws@bloms.de>
References: <20210120112546.eo7pw22bbebqbzws@bloms.de>
Message-ID: <000401d6ef3b$ad4fded0$07ef9c70$@gmail.com>

It's not clear if only Chromium or also a simple Chrome.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Dieter Bloms
Sent: Wednesday, January 20, 2021 1:26 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] chromium based browsers don't play a video, when sslbump is enabled

Hello,

I use squid 4.13 with enabled sslbump.
Chromium based browsers like chrome and edge don't play this video
https://admin.wissen-ad.de/storage/TEST/Big_Buck_Bunny_1080_10s_30MB.mp4
The firefox browser and the old internet explorer have no problems.

When I disable sslbumping for this destination the chromium based
browsers work as well.

Here are some parts of my config:

--snip--
http_port MYIP:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=32MB cert=/etc/squid/cert.pem key=/etc/squid/key.pem tls-dh=/etc/squid/dhparams.pem
sslcrtd_program /usr/sbin/security_file_certgen -s /var/cache/squid/sslcert_db -M 32MB
sslcrtd_children 32 startup=10 idle=3
tls_outgoing_options capath=/etc/ssl/certs min-version=1.2
tls_outgoing_options cipher=TLSv1.2:+aRSA:+SHA384:+SHA256:+DH:-kRSA:!PSK:!eNULL:!aNULL:!DSS:!AESCCM:!CAMELLIA:!ARIA:AES256-SHA:AES128-SHA:@SECLEVEL=1

acl nobumping dstdomain "/etc/squid/nohttpsscan.domains"
ssl_bump splice nobumping
ssl_bump bump all
--snip--

with wget or curl I can download the mp4 file in both cases (with and without sslbump)

Can anybody try to view the video in a chromium based browser with enabled sslbump ?

Thank you very much.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid.org at bloms.de  Wed Jan 20 16:01:13 2021
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 20 Jan 2021 17:01:13 +0100
Subject: [squid-users] chromium based browsers don't play a video,
 when sslbump is enabled
In-Reply-To: <000401d6ef3b$ad4fded0$07ef9c70$@gmail.com>
References: <20210120112546.eo7pw22bbebqbzws@bloms.de>
 <000401d6ef3b$ad4fded0$07ef9c70$@gmail.com>
Message-ID: <20210120160113.pjme33eilxhgmlxk@bloms.de>

Hello Eliezer,

I've tested with chrome 87.0.4280.141 and Edge 87.0.664.75.

On Wed, Jan 20, Eliezer Croitoru wrote:

> It's not clear if only Chromium or also a simple Chrome.
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> Zoom: Coming soon
> 
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Dieter Bloms
> Sent: Wednesday, January 20, 2021 1:26 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] chromium based browsers don't play a video, when sslbump is enabled
> 
> Hello,
> 
> I use squid 4.13 with enabled sslbump.
> Chromium based browsers like chrome and edge don't play this video
> https://admin.wissen-ad.de/storage/TEST/Big_Buck_Bunny_1080_10s_30MB.mp4
> The firefox browser and the old internet explorer have no problems.
> 
> When I disable sslbumping for this destination the chromium based
> browsers work as well.
> 
> Here are some parts of my config:
> 
> --snip--
> http_port MYIP:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=32MB cert=/etc/squid/cert.pem key=/etc/squid/key.pem tls-dh=/etc/squid/dhparams.pem
> sslcrtd_program /usr/sbin/security_file_certgen -s /var/cache/squid/sslcert_db -M 32MB
> sslcrtd_children 32 startup=10 idle=3
> tls_outgoing_options capath=/etc/ssl/certs min-version=1.2
> tls_outgoing_options cipher=TLSv1.2:+aRSA:+SHA384:+SHA256:+DH:-kRSA:!PSK:!eNULL:!aNULL:!DSS:!AESCCM:!CAMELLIA:!ARIA:AES256-SHA:AES128-SHA:@SECLEVEL=1
> 
> acl nobumping dstdomain "/etc/squid/nohttpsscan.domains"
> ssl_bump splice nobumping
> ssl_bump bump all
> --snip--
> 
> with wget or curl I can download the mp4 file in both cases (with and without sslbump)
> 
> Can anybody try to view the video in a chromium based browser with enabled sslbump ?
> 
> Thank you very much.
> 
> 
> -- 
> Regards
> 
>   Dieter
> 
> --
> I do not get viruses because I do not use MS software.
> If you use Outlook then please do not put my email address in your
> address-book so that WHEN you get a virus it won't use my address in the
> From field.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From klaus_brandl at genua.de  Wed Jan 20 16:21:18 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Wed, 20 Jan 2021 16:21:18 +0000
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
References: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
Message-ID: <8298afbba8b8bd6e9646750dc31f7d3a536dcb88.camel@genua.de>

some similar problem here...

What type of acl do you use for the group selection? Could you please
post the related config lines?

Remember, the client caches also the group informations, i have to
logout/login to let this take effect.
(check with "whoami /groups")

Regards

Klaus

Am Mittwoch, den 20.01.2021, 14:50 +0100 schrieb
heimarbeit123.99 at web.de:
> Hello all! :)
>  
> I am running squid 4.1 on the newest Linux Mint with Kerberos
> SSO(connected to my AD), so I can check for AD groups and therefore
> block websites and so on. Thanks to the very good documentation
> everything looks good so far!
> But there is one realy big problem: Squid does not recognize AD group
> membership changes.
> What does that mean?
>  
> Imagine I have TestUser1 and TestGroup1 and Testgroup2 in my AD. If I
> join TestUser1 to Testgroup1 everything is working(the first time
> ever, this specific user is getting member of one of these two
> groups). SSO works and the forbidden websites get blocked. So far so
> good ;)
> But if I remove TestUser1 from TestGroup1 and make him a member of
> Testgroup2, shit is about to hit the fan!
> After some seconds(winbind cache time = 30 in smb.conf) winbind
> recognizes, that TestUser1 is not member of TestGroup1 anymore, but
> now is a member of Testgroup2. But Squid doesn't!! Squid further
> treats TestUser1 as he would still be in TestGroup1.
> But if I now add a completly new user TestUser2 to the AD and then to
> Testgroup2, squid will treat this user corretly. If I then remove
> TestUser2 from Testgroup2 and add this user to TestGroup1, same shit
> again: winbind recognizes the change, but squid still treats
> TestUser2 like he would be member of TestGroup2.
>  
> What I tried:
> -remove cache (net cache flush, "cache deny all", "no_cache deny
> all")
> -remove squid with "purge" and reinstall it, still same problem
>  
> Can anyone help???
>  
> remember: Everything works with a new user, so I dont think kerberos
> is the problem. And winbind recognizes the change, so I think winbind
> is well configured too. Maybe squid is caching something(only
> explanation for me) but I don't see any caching.. Maybe someone had
> the same issue. Would be awesome, if someone could help me!
>  
> Regards
> Philipp
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From ngtech1ltd at gmail.com  Wed Jan 20 16:58:16 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 20 Jan 2021 18:58:16 +0200
Subject: [squid-users] chromium based browsers don't play a video,
 when sslbump is enabled
In-Reply-To: <20210120160113.pjme33eilxhgmlxk@bloms.de>
References: <20210120112546.eo7pw22bbebqbzws@bloms.de>
 <000401d6ef3b$ad4fded0$07ef9c70$@gmail.com>
 <20210120160113.pjme33eilxhgmlxk@bloms.de>
Message-ID: <000901d6ef4d$72ff69f0$58fe3dd0$@gmail.com>

I can watch both Edge and Chromium here with a "naked" sslbump:
1611161802.690  16176 192.168.189.48 TCP_MISS/206 30704989 GET https://admin.wissen-ad.de/storage/TEST/Big_Buck_Bunny_1080_10s_30MB.mp4 - ORIGINAL_DST/85.214.58.228 video/mp4 admin.wissen-ad.de

# squid -v
Squid Cache: Version 5.0.4-20201125-r5fadc09ee
Service Name: squid

This binary uses OpenSSL 1.1.1g FIPS  21 Apr 2020. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-security-cert-generators' '--enable-security-cert-validators' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CC=gcc' 'CFLAGS=-O2  -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection' 'LDFLAGS=-Wl,-z,relro -Wl,--as-needed  -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld ' 'CXX=g++' 'CXXFLAGS=-O2  -fexceptions -g -grecord-gcc-switches -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -fstack-protector-strong -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1  -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' 'LT_SYS_LIBRARY_PATH=/usr/lib64:' --enable-ltdl-convenience


Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Dieter Bloms
Sent: Wednesday, January 20, 2021 6:01 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] chromium based browsers don't play a video, when sslbump is enabled

Hello Eliezer,

I've tested with chrome 87.0.4280.141 and Edge 87.0.664.75.

On Wed, Jan 20, Eliezer Croitoru wrote:

> It's not clear if only Chromium or also a simple Chrome.
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> Zoom: Coming soon
> 
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Dieter Bloms
> Sent: Wednesday, January 20, 2021 1:26 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] chromium based browsers don't play a video, when sslbump is enabled
> 
> Hello,
> 
> I use squid 4.13 with enabled sslbump.
> Chromium based browsers like chrome and edge don't play this video
> https://admin.wissen-ad.de/storage/TEST/Big_Buck_Bunny_1080_10s_30MB.mp4
> The firefox browser and the old internet explorer have no problems.
> 
> When I disable sslbumping for this destination the chromium based
> browsers work as well.
> 
> Here are some parts of my config:
> 
> --snip--
> http_port MYIP:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=32MB cert=/etc/squid/cert.pem key=/etc/squid/key.pem tls-dh=/etc/squid/dhparams.pem
> sslcrtd_program /usr/sbin/security_file_certgen -s /var/cache/squid/sslcert_db -M 32MB
> sslcrtd_children 32 startup=10 idle=3
> tls_outgoing_options capath=/etc/ssl/certs min-version=1.2
> tls_outgoing_options cipher=TLSv1.2:+aRSA:+SHA384:+SHA256:+DH:-kRSA:!PSK:!eNULL:!aNULL:!DSS:!AESCCM:!CAMELLIA:!ARIA:AES256-SHA:AES128-SHA:@SECLEVEL=1
> 
> acl nobumping dstdomain "/etc/squid/nohttpsscan.domains"
> ssl_bump splice nobumping
> ssl_bump bump all
> --snip--
> 
> with wget or curl I can download the mp4 file in both cases (with and without sslbump)
> 
> Can anybody try to view the video in a chromium based browser with enabled sslbump ?
> 
> Thank you very much.
> 
> 
> -- 
> Regards
> 
>   Dieter
> 
> --
> I do not get viruses because I do not use MS software.
> If you use Outlook then please do not put my email address in your
> address-book so that WHEN you get a virus it won't use my address in the
> From field.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From jzhu at proofpoint.com  Wed Jan 20 20:21:46 2021
From: jzhu at proofpoint.com (John Zhu)
Date: Wed, 20 Jan 2021 20:21:46 +0000
Subject: [squid-users] Data tricking implementation is on ICAP side or Squid
 side?
Message-ID: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>

Hi, All,

We have similar project idea of content scanning which is built on Squid and ICAP. While ICAP content scanner takes very long time for  large file (> 10MB) scanning, the session to client browser will be disconnected.  As people suggested in this community, for better user experience, we would like to add the ?data trickling? feature ? slow the transmit speed to client while waiting for ICAP scanning completion.

I implemented ICAP in java. I have questions regarding the ?data trickling? to handle slow response for large file scanning from ICAP.  Could you please give your advices?
1) Java libraries available for data trickling at ICAP side, if any?
2) Need any configuration change for trickling feature on Squid side?
3) Need any code change on Squid side?
3) The trickling (in a very slow speed data send to Squid) is implemented only ICAP server side, correct?

Thank you all,

John Zhu

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210120/e2d893c7/attachment.htm>

From ngtech1ltd at gmail.com  Thu Jan 21 06:56:57 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 21 Jan 2021 08:56:57 +0200
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
References: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
Message-ID: <001101d6efc2$9cc21fb0$d6465f10$@gmail.com>

I am not sure but, I am pretty sure that the group membership is better handled in the LDAP level.

The Kerberos side is for handling the password between the client and the server.

A LDAP search/lookup for a user group membership seems more reasonable to me.

 

I have not implemented this with AD but when I have implemented it with LDAP it worked as expected.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of heimarbeit123.99 at web.de
Sent: Wednesday, January 20, 2021 3:51 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid doesn't notice AD group changes

 

Hello all! :)

 

I am running squid 4.1 on the newest Linux Mint with Kerberos SSO(connected to my AD), so I can check for AD groups and therefore block websites and so on. Thanks to the very good documentation everything looks good so far!

But there is one realy big problem: Squid does not recognize AD group membership changes.

What does that mean?

 

Imagine I have TestUser1 and TestGroup1 and Testgroup2 in my AD. If I join TestUser1 to Testgroup1 everything is working(the first time ever, this specific user is getting member of one of these two groups). SSO works and the forbidden websites get blocked. So far so good ;)

But if I remove TestUser1 from TestGroup1 and make him a member of Testgroup2, shit is about to hit the fan!

After some seconds(winbind cache time = 30 in smb.conf) winbind recognizes, that TestUser1 is not member of TestGroup1 anymore, but now is a member of Testgroup2. But Squid doesn't!! Squid further treats TestUser1 as he would still be in TestGroup1.

But if I now add a completly new user TestUser2 to the AD and then to Testgroup2, squid will treat this user corretly. If I then remove TestUser2 from Testgroup2 and add this user to TestGroup1, same shit again: winbind recognizes the change, but squid still treats TestUser2 like he would be member of TestGroup2.

 

What I tried:

-remove cache (net cache flush, "cache deny all", "no_cache deny all")

-remove squid with "purge" and reinstall it, still same problem

 

Can anyone help???

 

remember: Everything works with a new user, so I dont think kerberos is the problem. And winbind recognizes the change, so I think winbind is well configured too. Maybe squid is caching something(only explanation for me) but I don't see any caching.. Maybe someone had the same issue. Would be awesome, if someone could help me!

 

Regards

Philipp

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210121/c63836ce/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 21 18:18:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Jan 2021 13:18:00 -0500
Subject: [squid-users] Data tricking implementation is on ICAP side or
 Squid side?
In-Reply-To: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
References: <E7C78DBB-5150-4958-8B0A-041FD6A2860C@proofpoint.com>
Message-ID: <b15925cc-1078-970d-2d8e-b4bfdd86b4ee@measurement-factory.com>

On 1/20/21 3:21 PM, John Zhu wrote:

> I implemented ICAP in java. I have questions regarding the ?data
> trickling? to handle slow response for large file scanning from ICAP.

> 1) Java libraries available for data trickling at ICAP side, if any?

FWIW, implementing a production ICAP server from scratch (in any
language) is usually a bad idea -- there are too many poorly documented
and barely understood protocol areas. There are production ICAP servers
that support data trickling.

Unfortunately, I am not familiar with Java libraries, but there were two
ICAP projects in Java:
https://wiki.squid-cache.org/Features/ICAP#ICAP_Servers


> 2) Need any configuration change for trickling feature on Squid side?
> 3) Need any code change on Squid side?

I do not recall any required changes. Squid itself can be unaware that
data trickling is going on. However, it is possible that, in some
extreme cases (e.g., trickling one byte at a time), some configuration
or code adjustments would be needed to force "flushing" of that data
through Squid or to fix Squid metadata parsing bugs.


> 3) The trickling (in a very slow speed data send to Squid) is
> implemented only ICAP server side, correct?

The ICAP client side must not buffer (as in "delay to aggregate") data
trickled by the ICAP server.


HTH,

Alex.


From ngtech1ltd at gmail.com  Thu Jan 21 23:14:37 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 22 Jan 2021 01:14:37 +0200
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
References: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
Message-ID: <000201d6f04b$30e9cb10$92bd6130$@gmail.com>

Have You tried to use external_acl_type for group membership checks?

 

Something like this should do the trick:

external_acl_type ad_group_member_check ttl=120 %LOGIN /usr/lib/squid/ext_ldap_group_acl -d -R -K -S -b "dc=ng,dc=tech" -D squid at ng.tech -W /etc/squid/ldappass.txt  -f "(&(sAMAccountName=%u)(memberOf=CN=%g,OU=Groups,DC=ng,DC=tech))" -h ngtech-dc.ng.tech

 


Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of heimarbeit123.99 at web.de
Sent: Wednesday, January 20, 2021 3:51 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid doesn't notice AD group changes

 

Hello all! :)

 

I am running squid 4.1 on the newest Linux Mint with Kerberos SSO(connected to my AD), so I can check for AD groups and therefore block websites and so on. Thanks to the very good documentation everything looks good so far!

But there is one realy big problem: Squid does not recognize AD group membership changes.

What does that mean?

 

Imagine I have TestUser1 and TestGroup1 and Testgroup2 in my AD. If I join TestUser1 to Testgroup1 everything is working(the first time ever, this specific user is getting member of one of these two groups). SSO works and the forbidden websites get blocked. So far so good ;)

But if I remove TestUser1 from TestGroup1 and make him a member of Testgroup2, shit is about to hit the fan!

After some seconds(winbind cache time = 30 in smb.conf) winbind recognizes, that TestUser1 is not member of TestGroup1 anymore, but now is a member of Testgroup2. But Squid doesn't!! Squid further treats TestUser1 as he would still be in TestGroup1.

But if I now add a completly new user TestUser2 to the AD and then to Testgroup2, squid will treat this user corretly. If I then remove TestUser2 from Testgroup2 and add this user to TestGroup1, same shit again: winbind recognizes the change, but squid still treats TestUser2 like he would be member of TestGroup2.

 

What I tried:

-remove cache (net cache flush, "cache deny all", "no_cache deny all")

-remove squid with "purge" and reinstall it, still same problem

 

Can anyone help???

 

remember: Everything works with a new user, so I dont think kerberos is the problem. And winbind recognizes the change, so I think winbind is well configured too. Maybe squid is caching something(only explanation for me) but I don't see any caching.. Maybe someone had the same issue. Would be awesome, if someone could help me!

 

Regards

Philipp

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210122/a1c8a95b/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 22 03:18:13 2021
From: squid3 at treenet.co.nz (=?UTF-8?B?4oCqQW1vcyBKZWZmcmllc+KArA==?=)
Date: Fri, 22 Jan 2021 16:18:13 +1300
Subject: [squid-users] Squid doesn't notice AD group changes
References: <trinity-e80df5b4-c500-49af-8bdb-a90d09b912a2-1611150657312@3c-app-webde-bs60>
Message-ID: <kcfsh8-oooqg6bzy6kp-3cpj6fm7rw2m-65eqi9czsv0culjmmu-ginu89ydgip6afhj0xwykvkj-ksdm34-4majcwuwc0en-4vyg5dqjv0kb-yet84w-x8yet9exjjb6-800dpb6p5jpd5fzffm-cdibhw.1611283870587@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210122/3816a241/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan 22 03:57:54 2021
From: squid3 at treenet.co.nz (=?UTF-8?B?4oCqQW1vcyBKZWZmcmllc+KArA==?=)
Date: Fri, 22 Jan 2021 16:57:54 +1300
Subject: [squid-users] chromium based browsers don't play a video,
 when sslbump is enabled
References: <20210120112546.eo7pw22bbebqbzws@bloms.de>
Message-ID: <orghwg-jyh58o-ftw2uh-yerfgpwp2mywlctyihshzsee-dolj2d5e4ge5xeykfay7k43n76otld-qjxotyft8lf-7bs06rbqhhu0-f3m803-sm3sdo3qod0e1uucgo-vrb83grdlyah-5rpwe3j3lhv9.1611287266684@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210122/b8476375/attachment.htm>

From h.kawai at ntt.com  Fri Jan 22 09:23:25 2021
From: h.kawai at ntt.com (Hideyuki Kawai)
Date: Fri, 22 Jan 2021 09:23:25 +0000
Subject: [squid-users] effective acl for tcp_outgoing_address
Message-ID: <OSBPR01MB2776898F780C5E53E37E8384E8A00@OSBPR01MB2776.jpnprd01.prod.outlook.com>

Hi, this is Kawai.



Now, I'm trying to set up squid4.x on centOS, but, have one issue.

Please let me send inquiry as followings.



### Requirement ###

The squid is required as follows.

1. Kerberos auth with Active Directory : auth_param .....       <- Success

2. "Security group" check which is gotten from AD : external_acl_type ...(using ext_kerberos_ldap_group_acl)   <- success

3. Using different outgoing IP based on "Security group" : tcp_outgoing_address + external_acl  <- fail (can not work)



=== sample configuration which I tested. (but, it did not work?) ===

external_acl_type kerberos_ldap_group1 ttl=3600 negative_ttl=3600 %LOGIN /path/to/ext_kerberos_ldap_group_acl ?g GROUP1

external_acl_type kerberos_ldap_group2 ttl=3600 negative_ttl=3600 %LOGIN /path/to/ext_kerberos_ldap_group_acl ?g GROUP2

acl group1 external kerberos_ldap_group1

acl group2 external kerberos_ldap_group2

tcp_outgoing_address 10.1.0.1 group1

tcp_outgoing_address 10.1.0.2 group2





### Inquiry ###

Based on the web site, ?tcp_outgoing_address? is NOT support "external_acl". Because the external_acl type is slow.

In this case, how to configure the squid.conf to satisfy my requirement?



Please let me inform your comment and knowledge.

Thanks in advance.



-------------------------------------

h.kawai at ntt.com

-------------------------------------


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210122/77d93bc7/attachment.htm>

From hello at ironpeak.be  Fri Jan 22 17:17:46 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Fri, 22 Jan 2021 18:17:46 +0100
Subject: [squid-users] Statically compiling
Message-ID: <4568DE30-A411-4272-B02A-55F75DE2AE8B@ironpeak.be>

Hi guys,

I am trying to compile squid with openssl into a static binary.
Any idea what arguments I need to provide during the build steps?

Currently in Docker:

RUN apt-get source squid

RUN apt-get build-dep --yes squid

RUN mv squid-*/ squid/

WORKDIR /squid/squid/

RUN sed 's/--with-gnutls/--with-gnutls --with-default-user=squid --enable-ssl --enable-ssl-crtd --with-openssl --disable-ipv6/' debian/rules

RUN debuild -us -uc

Thank you!
Regards,
Niels Hofmans

SITE   https://ironpeak.be <https://ironpeak.be/>
BTW   BE0694785660
BANK BE76068909740795
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210122/1ec60965/attachment.htm>

From Walter.H at mathemainzel.info  Fri Jan 22 20:10:09 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Fri, 22 Jan 2021 21:10:09 +0100
Subject: [squid-users] wiki.squid-cache.org has invalid SSL certificate
Message-ID: <4ce26e63-c1ec-3643-0e83-cbb55c944583@mathemainzel.info>

Hello,

look here

https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org

there is an invalid certificate as the intermediate

Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3511 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210122/3c3c2dd8/attachment.bin>

From rousskov at measurement-factory.com  Fri Jan 22 20:32:14 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 22 Jan 2021 15:32:14 -0500
Subject: [squid-users] wiki.squid-cache.org has invalid SSL certificate
In-Reply-To: <4ce26e63-c1ec-3643-0e83-cbb55c944583@mathemainzel.info>
References: <4ce26e63-c1ec-3643-0e83-cbb55c944583@mathemainzel.info>
Message-ID: <2ac49a67-8ec3-bea4-a8ae-9cd3de4132e4@measurement-factory.com>

On 1/22/21 3:10 PM, Walter H. wrote:

> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
> there is an invalid certificate as the intermediate

FWIW, I see nothing marked as "invalid" on that page, even after
clicking on one of the two servers and expanding the "Certification
Paths" group. The "certificate" score is 100%/Green.

The service does show one missing intermediate certificate ("certificate
chain is incomplete" and "extra download" annotations), which the
service was able to successfully download and validated. This extra work
reduced our overall score from A to B AFAICT. This is expected per Squid
Project NOC AFAICT.

It may help if you provide more details about the "invalid" annotations
that _you_ see on that report.

Alex.


From uhlar at fantomas.sk  Sat Jan 23 12:07:04 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 23 Jan 2021 13:07:04 +0100
Subject: [squid-users] wiki.squid-cache.org has invalid SSL certificate
In-Reply-To: <2ac49a67-8ec3-bea4-a8ae-9cd3de4132e4@measurement-factory.com>
References: <4ce26e63-c1ec-3643-0e83-cbb55c944583@mathemainzel.info>
 <2ac49a67-8ec3-bea4-a8ae-9cd3de4132e4@measurement-factory.com>
Message-ID: <20210123120704.GA21727@fantomas.sk>

On 22.01.21 15:32, Alex Rousskov wrote:
>On 1/22/21 3:10 PM, Walter H. wrote:
>
>> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
>> there is an invalid certificate as the intermediate
>
>FWIW, I see nothing marked as "invalid" on that page, even after
>clicking on one of the two servers and expanding the "Certification
>Paths" group. The "certificate" score is 100%/Green.
>
>The service does show one missing intermediate certificate ("certificate
>chain is incomplete" and "extra download" annotations), which the
>service was able to successfully download and validated. This extra work
>reduced our overall score from A to B AFAICT. This is expected per Squid
>Project NOC AFAICT.
>
>It may help if you provide more details about the "invalid" annotations
>that _you_ see on that report.

this may be obsolete info, both server certificate and intermediate were
signes last synday (Jan 17).

I have noticed similar problems for some letsencrypt certificates last
month. 
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
WinError #98652: Operation completed successfully.


From beat.zahnd at gmail.com  Sat Jan 23 12:45:39 2021
From: beat.zahnd at gmail.com (Beat Zahnd)
Date: Sat, 23 Jan 2021 13:45:39 +0100
Subject: [squid-users] Page not accessible when bumped (Cloudflare
 challenge?)
Message-ID: <A94AC6C3-3C8C-467C-84D0-047C009A2017@gmail.com>

Hi all,

I have Squid 5.0.4 (same behaviour on old 4.x) running a simple SSL-bump setup as described in https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit. 

http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB tls-dh=/etc/squid/ssl_cert/dhparam.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
acl step1 at_step SslBump1
acl nobump ssl::server_name "/etc/squid/nobump"
ssl_bump peek step1
ssl_bump splice nobump
ssl_bump bump all

Recently some pages started to not work anymore when bumped one example is https://www.ricardo.ch/de/a/alinghi-reconditionnee-rarete-1155873766/

All browser seem to get stuck wich enormous memory consumption. Seems that some challenge-response is driving them crazy. Seems to be from cloudflare...


What is happening here? Any ideas to get such pages still bumped?

Cheers

From Walter.H at mathemainzel.info  Sat Jan 23 13:15:29 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 23 Jan 2021 14:15:29 +0100
Subject: [squid-users] wiki.squid-cache.org has invalid SSL certificate
In-Reply-To: <20210123120704.GA21727@fantomas.sk>
References: <4ce26e63-c1ec-3643-0e83-cbb55c944583@mathemainzel.info>
 <2ac49a67-8ec3-bea4-a8ae-9cd3de4132e4@measurement-factory.com>
 <20210123120704.GA21727@fantomas.sk>
Message-ID: <caf5fc2e-daae-a32d-b79d-a16f508352f8@mathemainzel.info>

On 23.01.2021 13:07, Matus UHLAR - fantomas wrote:
> On 22.01.21 15:32, Alex Rousskov wrote:
>> On 1/22/21 3:10 PM, Walter H. wrote:
>>
>>> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
>>> there is an invalid certificate as the intermediate
>>
>> FWIW, I see nothing marked as "invalid" on that page, even after
>> clicking on one of the two servers and expanding the "Certification
>> Paths" group. The "certificate" score is 100%/Green.
>>
>> The service does show one missing intermediate certificate ("certificate
>> chain is incomplete" and "extra download" annotations), which the
>> service was able to successfully download and validated. This extra work
>> reduced our overall score from A to B AFAICT. This is expected per Squid
>> Project NOC AFAICT.
>>
>> It may help if you provide more details about the "invalid" annotations
>> that _you_ see on that report.
>
> this may be obsolete info, both server certificate and intermediate were
> signes last synday (Jan 17).
>
> I have noticed similar problems for some letsencrypt certificates last
> month. 

the reason:? Let's encrypt changed the interediate, see here: 
https://letsencrypt.org/certificates/

https://wiki.squid-cache.org/

got a new SSL certificate but the chain still has the old X3 instead of 
R3 ...

Thanks

Walter



-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3511 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210123/9a25ea13/attachment.bin>

From gkinkie at gmail.com  Sat Jan 23 13:19:35 2021
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Sat, 23 Jan 2021 13:19:35 +0000
Subject: [squid-users] wiki.squid-cache.org has invalid SSL certificate
In-Reply-To: <caf5fc2e-daae-a32d-b79d-a16f508352f8@mathemainzel.info>
References: <4ce26e63-c1ec-3643-0e83-cbb55c944583@mathemainzel.info>
 <2ac49a67-8ec3-bea4-a8ae-9cd3de4132e4@measurement-factory.com>
 <20210123120704.GA21727@fantomas.sk>
 <caf5fc2e-daae-a32d-b79d-a16f508352f8@mathemainzel.info>
Message-ID: <CA+Y8hcOW6cjZb3RQDmuX=e7AsGA=XXE6sHg=VNNYz0-JezM7vg@mail.gmail.com>

Thanks for letting me know.
We use letsencrypt and there's an automated renewal mechanism at play, but
apparently it doesn't keep up with changing intermediates.


On Sat, Jan 23, 2021 at 1:15 PM Walter H. <Walter.H at mathemainzel.info>
wrote:

> On 23.01.2021 13:07, Matus UHLAR - fantomas wrote:
> > On 22.01.21 15:32, Alex Rousskov wrote:
> >> On 1/22/21 3:10 PM, Walter H. wrote:
> >>
> >>> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
> >>> there is an invalid certificate as the intermediate
> >>
> >> FWIW, I see nothing marked as "invalid" on that page, even after
> >> clicking on one of the two servers and expanding the "Certification
> >> Paths" group. The "certificate" score is 100%/Green.
> >>
> >> The service does show one missing intermediate certificate ("certificate
> >> chain is incomplete" and "extra download" annotations), which the
> >> service was able to successfully download and validated. This extra work
> >> reduced our overall score from A to B AFAICT. This is expected per Squid
> >> Project NOC AFAICT.
> >>
> >> It may help if you provide more details about the "invalid" annotations
> >> that _you_ see on that report.
> >
> > this may be obsolete info, both server certificate and intermediate were
> > signes last synday (Jan 17).
> >
> > I have noticed similar problems for some letsencrypt certificates last
> > month.
>
> the reason:  Let's encrypt changed the interediate, see here:
> https://letsencrypt.org/certificates/
>
> https://wiki.squid-cache.org/
>
> got a new SSL certificate but the chain still has the old X3 instead of
> R3 ...
>
> Thanks
>
> Walter
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210123/ecf0f69d/attachment.htm>

From hlager at web.de  Sat Jan 23 16:40:01 2021
From: hlager at web.de (hlager at web.de)
Date: Sat, 23 Jan 2021 17:40:01 +0100
Subject: [squid-users] Problem with tcp_outgoing_address
Message-ID: <trinity-033b5a16-dd7f-439d-9b3e-6e2744412e0f-1611420001402@3c-app-webde-bs52>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210123/2f219536/attachment.htm>

From ngtech1ltd at gmail.com  Sun Jan 24 09:03:28 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 24 Jan 2021 11:03:28 +0200
Subject: [squid-users] effective acl for tcp_outgoing_address
In-Reply-To: <OSBPR01MB2776898F780C5E53E37E8384E8A00@OSBPR01MB2776.jpnprd01.prod.outlook.com>
References: <OSBPR01MB2776898F780C5E53E37E8384E8A00@OSBPR01MB2776.jpnprd01.prod.outlook.com>
Message-ID: <003401d6f22f$c87e3f70$597abe50$@gmail.com>

Hey,

 

I can try to test/check this but I am missing the basic Kerberos auth with AD setup.

I have a working setup but the transparent authentication is not working for me.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Hideyuki Kawai
Sent: Friday, January 22, 2021 11:23 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] effective acl for tcp_outgoing_address

 

Hi, this is Kawai.

 

Now, I'm trying to set up squid4.x on centOS, but, have one issue.

Please let me send inquiry as followings.

 

### Requirement ###

The squid is required as follows.

1. Kerberos auth with Active Directory : auth_param .....       <- Success

2. "Security group" check which is gotten from AD : external_acl_type ...(using ext_kerberos_ldap_group_acl)   <- success

3. Using different outgoing IP based on "Security group" : tcp_outgoing_address + external_acl  <- fail (can not work)

 

=== sample configuration which I tested. (but, it did not work?) ===

external_acl_type kerberos_ldap_group1 ttl=3600 negative_ttl=3600 %LOGIN /path/to/ext_kerberos_ldap_group_acl ?g GROUP1

external_acl_type kerberos_ldap_group2 ttl=3600 negative_ttl=3600 %LOGIN /path/to/ext_kerberos_ldap_group_acl ?g GROUP2

acl group1 external kerberos_ldap_group1

acl group2 external kerberos_ldap_group2

tcp_outgoing_address 10.1.0.1 group1

tcp_outgoing_address 10.1.0.2 group2

 

 

### Inquiry ###

Based on the web site, ?tcp_outgoing_address? is NOT support "external_acl". Because the external_acl type is slow.

In this case, how to configure the squid.conf to satisfy my requirement?

 

Please let me inform your comment and knowledge.

Thanks in advance.

 

-------------------------------------

h.kawai at ntt.com <mailto:h.kawai at ntt.com> 

-------------------------------------

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210124/dd806609/attachment.htm>

From heimarbeit123.99 at web.de  Sun Jan 24 09:06:21 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Sun, 24 Jan 2021 10:06:21 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
Message-ID: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210124/1191cd3e/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 24 12:42:43 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jan 2021 01:42:43 +1300
Subject: [squid-users] Problem with tcp_outgoing_address
In-Reply-To: <trinity-033b5a16-dd7f-439d-9b3e-6e2744412e0f-1611420001402@3c-app-webde-bs52>
References: <trinity-033b5a16-dd7f-439d-9b3e-6e2744412e0f-1611420001402@3c-app-webde-bs52>
Message-ID: <0c90969d-352f-0fb0-3721-3f85c69d26d6@treenet.co.nz>

On 24/01/21 5:40 am, hlager wrote:
> Hey guys,
> i'm trying to get squid working with two outgoing ip adresses, but only 
> one will work, i hope someone can help me.
> I'm using an ESXI with a Ubuntu VM, i got 3 NICs on it. One Local were 
> the traffic comes in and two which are for the outgoing traffic. The 
> booth outgoing nics are in the same subnet and have the same gateway but 
> of course different ips.

> i used the following config:

> http_port 10.5.5.3:3128 name=3128
> http_port 10.5.5.3:3129?name=3129
> http_port 10.5.5.3:3130?name=3130

> acl test1 myportname 3128 src 1.0.0.0/8

You do not have a port named "src" or a port named "1.0.0.0/8"

> http_access allow test1
> tcp_outoing_address 132.15.115.14 test1

> acl test2?myportname 3129?src 1.0.0.0/8
> http_access allow test2
> tcp_outoing_address 132.15.115.15?test2


> test1 and test3 (only for testing purpose)?works fine, but test2 wont work.

Please explain how you are testing and what you are seeing happening 
that makes you say that.


Amos


From mgresko8 at gmail.com  Sun Jan 24 16:02:14 2021
From: mgresko8 at gmail.com (=?UTF-8?Q?Marek_Gre=C5=A1ko?=)
Date: Sun, 24 Jan 2021 17:02:14 +0100
Subject: [squid-users] Squid doesn't notice AD group changes
In-Reply-To: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
References: <trinity-a9860a68-6749-4491-b85f-2ca0307c5877-1611479181311@msvc-mesg-web106>
Message-ID: <CAChjPdT2YJTPx41AoOkLR8Z_Q2H8FSAXsTWyBenrcEAT5GVK=Q@mail.gmail.com>

Hello,

that looks correct. Maybe I would add -B option to the
ext_ldap_group_acl helper to specify basedn for users.

Marek


2021-01-24 10:06 GMT+01:00, heimarbeit123.99 at web.de <heimarbeit123.99 at web.de>:
> Thanks for your replies!
>
> Yes, I did try "external_acl_type wbinfocheck %LOGIN
> /usr/lib/squid/ext_wbinfo_group_acl -K".
>
> So if my fqdn would be "my.domain.com" it would be:
>
> external_acl_type ad_group_member_check ttl=120 %LOGIN
> /usr/lib/squid/ext_ldap_group_acl -d -R -K -S -b "dc=domain,dc=com"
> -D?192.168.1.250 at domain.com?-W /etc/squid/ldappass.txt -f
> "(&(sAMAccountName=%u)(memberOf=CN=%g,OU=Groups,DC=domain,DC=com))" -h
> my.domain.com
>
> for 192.168.1.250 being the IP from my Squid Proxy Server, right?
>
> So I could ask for specific groups like this:
> acl Group1 ad_group_member_check TestGroup1
> acl Group2 ad_group_member_check TestGroup2
> and so on.. Am I right?
>
> Thank you so far for your help!
>
> Regads,
> Philipp
>
> --
> Diese Nachricht wurde von meinem Android Mobiltelefon mit WEB.DE Mail
> gesendet.


From rentorbuy at yahoo.com  Sun Jan 24 21:42:24 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Sun, 24 Jan 2021 21:42:24 +0000 (UTC)
Subject: [squid-users] Squid 5 service stops after assertion failure
References: <1536859800.465798.1611524544513.ref@mail.yahoo.com>
Message-ID: <1536859800.465798.1611524544513@mail.yahoo.com>

Hi,

My Squid web proxy crashed as shown in this log:

2021/01/24 13:18:13 kid1| helperHandleRead: unexpected reply on channel 0 from bllookup #Hlpr21 '43 ERR message=[...]
??? current master transaction: master65
2021/01/24 13:18:13 kid1| assertion failed: helper.cc:1066: "skip == 0 && eom == NULL"
??? current master transaction: master65
2021/01/24 13:18:13 kid1| Set Current Directory to /var/cache/squid
2021/01/24 13:18:13 kid1| Starting Squid Cache version 5.0.4-20201125-r5fadc09ee for x86_64-pc-linux-gnu...
2021/01/24 13:18:13 kid1| Service Name: squid
[...]
REPEATS (assertion failure & squid restart)
[...]
2021/01/24 13:18:27 kid1| helperHandleRead: unexpected reply on channel 0 from bllookup #Hlpr21 '2 ERR message=[...]
??? current master transaction: master76
2021/01/24 13:18:27 kid1| assertion failed: helper.cc:1066: "skip == 0 && eom == NULL"
??? current master transaction: master76
2021/01/24 13:18:27| Removing PID file (/run/squid.pid)
2021/01/24 13:18:34| Pinger exiting.
2021/01/24 13:18:37| Pinger exiting.

After the assertion failure Squid tries to restart a few times (assertion failures seen again) and finally exits.
A manual restart works, but I don't know for how long.

The external script "bllookup" is probably responsible for bad output, but maybe Squid could handle it without crashing.

Regards,

Vieri


From squid3 at treenet.co.nz  Sun Jan 24 22:00:20 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jan 2021 11:00:20 +1300
Subject: [squid-users] Squid 5 service stops after assertion failure
In-Reply-To: <1536859800.465798.1611524544513@mail.yahoo.com>
References: <1536859800.465798.1611524544513.ref@mail.yahoo.com>
 <1536859800.465798.1611524544513@mail.yahoo.com>
Message-ID: <1540b6ca-0639-17e0-df07-01cfb605d169@treenet.co.nz>

On 25/01/21 10:42 am, Vieri wrote:
> 
> After the assertion failure Squid tries to restart a few times (assertion failures seen again) and finally exits.
> A manual restart works, but I don't know for how long.
> 
> The external script "bllookup" is probably responsible for bad output,

That is a certainty.


> but maybe Squid could handle it without crashing.
> 

As you noticed, Squid halts service only after the helper fails 10 
multiple times in a row. Before that Squid is restarting the helper to 
see if it was a temporary issue.


Would you prefer Squid sucks up all the TCP/IP resources on the machine 
for clients waiting on this helper to start work?

Or Squid to start mixing up the results so each client A lookup is 
determining the output of some other client B's transaction?


Amos


From rousskov at measurement-factory.com  Sun Jan 24 22:08:48 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 24 Jan 2021 17:08:48 -0500
Subject: [squid-users] Squid 5 service stops after assertion failure
In-Reply-To: <1536859800.465798.1611524544513@mail.yahoo.com>
References: <1536859800.465798.1611524544513.ref@mail.yahoo.com>
 <1536859800.465798.1611524544513@mail.yahoo.com>
Message-ID: <2b29dda4-84a1-e74e-a167-ae652cd94b9e@measurement-factory.com>

On 1/24/21 4:42 PM, Vieri wrote:

> 2021/01/24 13:18:13 kid1| helperHandleRead: unexpected reply on channel 0 from bllookup #Hlpr21 '43 ERR message=[...]
> ??? current master transaction: master65
> 2021/01/24 13:18:13 kid1| assertion failed: helper.cc:1066: "skip == 0 && eom == NULL"
> ??? current master transaction: master65

> The external script "bllookup" is probably responsible for bad
> output, but maybe Squid could handle it without crashing.

You are right. The corresponding code needs some work, hopefully driven
by some well-established error handling principles rather than
semi-random ad hoc decisions. Filing a bug report with Squid Bugzilla
may increase chances of this problem getting fixed.

Alex.


From rousskov at measurement-factory.com  Sun Jan 24 22:21:04 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 24 Jan 2021 17:21:04 -0500
Subject: [squid-users] Squid 5 service stops after assertion failure
In-Reply-To: <1540b6ca-0639-17e0-df07-01cfb605d169@treenet.co.nz>
References: <1536859800.465798.1611524544513.ref@mail.yahoo.com>
 <1536859800.465798.1611524544513@mail.yahoo.com>
 <1540b6ca-0639-17e0-df07-01cfb605d169@treenet.co.nz>
Message-ID: <df17d054-38c7-8d71-0860-92113c506df2@measurement-factory.com>

On 1/24/21 5:00 PM, Amos Jeffries wrote:
> On 25/01/21 10:42 am, Vieri wrote:
>>
>> After the assertion failure Squid tries to restart a few times
>> (assertion failures seen again) and finally exits.
>> A manual restart works, but I don't know for how long.
>>
>> The external script "bllookup" is probably responsible for bad output,

> That is a certainty.

>> but maybe Squid could handle it without crashing.


> As you noticed, Squid halts service only after the helper fails 10
> multiple times in a row. Before that Squid is restarting the helper to
> see if it was a temporary issue.

AFAICT, this Squid worker (rather than the helper) crashes/asserts. The
master process restarts the worker 10 times (because the worker keeps
crashing/asserting) and then gives up and kills the entire instance.


> Would you prefer Squid sucks up all the TCP/IP resources on the machine
> for clients waiting on this helper to start work?

> Or Squid to start mixing up the results so each client A lookup is
> determining the output of some other client B's transaction?


FWIW, I would prefer if Squid worker did not assert. Whether the worker
should quit when its helper fails is debatable and perhaps should be
configurable. In most environments/cases, killing the affected Squid
_transaction_ is much better than killing the Squid worker (and then
killing the entire Squid instance), but there may be exception worth
supporting (via configuration).


HTH,

Alex.


From hello at ironpeak.be  Mon Jan 25 07:53:12 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Mon, 25 Jan 2021 08:53:12 +0100
Subject: [squid-users] Statically compiling
In-Reply-To: <4568DE30-A411-4272-B02A-55F75DE2AE8B@ironpeak.be>
References: <4568DE30-A411-4272-B02A-55F75DE2AE8B@ironpeak.be>
Message-ID: <ACC3350C-9C18-4999-B66C-1E916E15B796@ironpeak.be>

Hi guys,

Apology for asking, but was this already answered?
I?m not really comfortable using mailing lists.
Thanks & stay safe.

Regards,
Niels 

On 22 Jan 2021, at 18:17, Niels Hofmans <hello at ironpeak.be> wrote:

Hi guys,

I am trying to compile squid with openssl into a static binary.
Any idea what arguments I need to provide during the build steps?

Currently in Docker:

RUN apt-get source squid

RUN apt-get build-dep --yes squid

RUN mv squid-*/ squid/

WORKDIR /squid/squid/

RUN sed 's/--with-gnutls/--with-gnutls --with-default-user=squid --enable-ssl --enable-ssl-crtd --with-openssl --disable-ipv6/' debian/rules

RUN debuild -us -uc

Thank you!
Regards,
Niels Hofmans

SITE   https://ironpeak.be <https://ironpeak.be/>
BTW   BE0694785660
BANK BE76068909740795

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210125/47559339/attachment.htm>

From rentorbuy at yahoo.com  Mon Jan 25 08:37:04 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 25 Jan 2021 08:37:04 +0000 (UTC)
Subject: [squid-users] Squid 5 service stops after assertion failure
In-Reply-To: <1540b6ca-0639-17e0-df07-01cfb605d169@treenet.co.nz>
References: <1536859800.465798.1611524544513.ref@mail.yahoo.com>
 <1536859800.465798.1611524544513@mail.yahoo.com>
 <1540b6ca-0639-17e0-df07-01cfb605d169@treenet.co.nz>
Message-ID: <1371127244.979447.1611563824483@mail.yahoo.com>



On Sunday, January 24, 2021, 11:03:19 PM GMT+1, Amos Jeffries <squid3 at treenet.co.nz> wrote: 

>> The external script "bllookup" is probably responsible for bad output,
>
> That is a certainty.
>
>> but maybe Squid could handle it without crashing.
> 
> As you noticed, Squid halts service only after the helper fails 10 
> multiple times in a row. Before that Squid is restarting the helper to 
> see if it was a temporary issue.

OK, the external script is definitely guilty. However, it is buggy and triggers the Squid assertion failure only in specific circumstances. So it's trasaction-specific. In my use case I would definitely prefer that only a few transactions were "killed", and that the whole of the proxy service would keep working.
Of course, I would still need to identify these cases and fix them, but in the meantime I would not get a general crash.
On the other hand a general failure forces me to look into this issue with greater celerity. ;-) 

Thanks,

Vieri



From rentorbuy at yahoo.com  Mon Jan 25 09:09:53 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 25 Jan 2021 09:09:53 +0000 (UTC)
Subject: [squid-users] Squid 5 service stops after assertion failure
In-Reply-To: <2b29dda4-84a1-e74e-a167-ae652cd94b9e@measurement-factory.com>
References: <1536859800.465798.1611524544513.ref@mail.yahoo.com>
 <1536859800.465798.1611524544513@mail.yahoo.com>
 <2b29dda4-84a1-e74e-a167-ae652cd94b9e@measurement-factory.com>
Message-ID: <653652477.978905.1611565793795@mail.yahoo.com>


On Sunday, January 24, 2021, 11:08:49 PM GMT+1, Alex Rousskov <rousskov at measurement-factory.com> wrote: 

> Filing a bug report with Squid Bugzilla may increase chances of this problem getting fixed.

Done here:

https://bugs.squid-cache.org/show_bug.cgi?id=5100

Thanks,

Vieri


From ngtech1ltd at gmail.com  Mon Jan 25 11:03:58 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 25 Jan 2021 13:03:58 +0200
Subject: [squid-users] acl aclname server_cert_fingerprint
Message-ID: <000001d6f309$c8996680$59cc3380$@gmail.com>

I'm trying to use:
acl aclname server_cert_fingerprint [-sha1] fingerprint


I have cerated the next file:
/etc/squid/no-ssl-bump-server-fingerprint.list

And trying to use the next line:
acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
"/etc/squid/no-ssl-bump-server-fingerprint.list"

To be explicit despite that only sha1 is a valid checksum.
Squid doesn't accept the above line but this one yes:
acl NoBump_certificate_fingerprint server_cert_fingerprint
"/etc/squid/no-ssl-bump-server-fingerprint.list"


Is there a reason for that?

Thanks,
Eliezer


----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon





From robertkwild at gmail.com  Mon Jan 25 12:24:30 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Mon, 25 Jan 2021 12:24:30 +0000
Subject: [squid-users] reply_header_access vs rep_mime_type to deny mime
 types
Message-ID: <CAGU_CiLv2B8bKu0HRcbnJ2GkAUcs5a5DitHzcF2wwBcFAdV_oQ@mail.gmail.com>

hi all,

just want your thoughts on what the best acl is to deny mime types

atm i have this and it works really well

#deny MIME types
acl mimetype rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
http_reply_access deny mimetype

and in my mime file i have this

cat /usr/local/squid/etc/mimedeny.txt
application/octet-stream
application/x-msi
application/zip
application/x-7z-compressed
application/vnd.ms-cab-compressed

would you say i should use the acl "reply_header_access" instead?

thanks,
rob
-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210125/11ae953b/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan 25 12:34:32 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jan 2021 01:34:32 +1300
Subject: [squid-users] reply_header_access vs rep_mime_type to deny mime
 types
In-Reply-To: <CAGU_CiLv2B8bKu0HRcbnJ2GkAUcs5a5DitHzcF2wwBcFAdV_oQ@mail.gmail.com>
References: <CAGU_CiLv2B8bKu0HRcbnJ2GkAUcs5a5DitHzcF2wwBcFAdV_oQ@mail.gmail.com>
Message-ID: <af21fcc5-8fec-c2b2-a488-0a7f571c6214@treenet.co.nz>

On 26/01/21 1:24 am, robert k Wild wrote:
> hi all,
> 
> just want your thoughts on what the best acl is to deny mime types
> 

Please explain what you mean by "deny mime types" ...


  Deliver the servers response but without telling the client what data 
format it is using ?

  Prevent the servers response being delivered to the client ?


Amos


From robertkwild at gmail.com  Mon Jan 25 12:47:57 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Mon, 25 Jan 2021 12:47:57 +0000
Subject: [squid-users] reply_header_access vs rep_mime_type to deny mime
 types
In-Reply-To: <af21fcc5-8fec-c2b2-a488-0a7f571c6214@treenet.co.nz>
References: <CAGU_CiLv2B8bKu0HRcbnJ2GkAUcs5a5DitHzcF2wwBcFAdV_oQ@mail.gmail.com>
 <af21fcc5-8fec-c2b2-a488-0a7f571c6214@treenet.co.nz>
Message-ID: <CAGU_CiKVDgy_dApC4JxKcAGqxtwHeZqh8b3fH_yvjziGBEWttQ@mail.gmail.com>

sorry Amos, i will explain why i use the "rep_mime_type"

so when users go to a website and click on a link to download and if that
download is an .exe/.zip etc etc (on my mimedeny.txt ), squid will
stop/block the download and instead they will get an access denied error
displayed on the web page

im guessing by your reply this is not what the "reply_header_access"is used
for

rob

On Mon, 25 Jan 2021 at 12:37, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 26/01/21 1:24 am, robert k Wild wrote:
> > hi all,
> >
> > just want your thoughts on what the best acl is to deny mime types
> >
>
> Please explain what you mean by "deny mime types" ...
>
>
>   Deliver the servers response but without telling the client what data
> format it is using ?
>
>   Prevent the servers response being delivered to the client ?
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210125/84cef246/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan 25 13:08:54 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jan 2021 02:08:54 +1300
Subject: [squid-users] reply_header_access vs rep_mime_type to deny mime
 types
In-Reply-To: <CAGU_CiKVDgy_dApC4JxKcAGqxtwHeZqh8b3fH_yvjziGBEWttQ@mail.gmail.com>
References: <CAGU_CiLv2B8bKu0HRcbnJ2GkAUcs5a5DitHzcF2wwBcFAdV_oQ@mail.gmail.com>
 <af21fcc5-8fec-c2b2-a488-0a7f571c6214@treenet.co.nz>
 <CAGU_CiKVDgy_dApC4JxKcAGqxtwHeZqh8b3fH_yvjziGBEWttQ@mail.gmail.com>
Message-ID: <15cf0908-c1df-2640-ec3b-8b13292b265d@treenet.co.nz>

On 26/01/21 1:47 am, robert k Wild wrote:
> sorry Amos, i will explain why i use the "rep_mime_type"
> 
> so when users go to a website and click on a link to download and if 
> that download is an .exe/.zip etc etc (on my mimedeny.txt ), squid will 
> stop/block the download and instead they will get an access denied error 
> displayed on the web page
> 
> im guessing by your reply this is not what the "reply_header_access"is 
> used for
> 

Correct. "reply_header_access deny " is for removing specific headers 
from sent responses. It will not do what you are wanting.

To forbid a whole reply you need "http_reply_access deny ".


Amos


From robertkwild at gmail.com  Mon Jan 25 13:22:20 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Mon, 25 Jan 2021 13:22:20 +0000
Subject: [squid-users] reply_header_access vs rep_mime_type to deny mime
 types
In-Reply-To: <15cf0908-c1df-2640-ec3b-8b13292b265d@treenet.co.nz>
References: <CAGU_CiLv2B8bKu0HRcbnJ2GkAUcs5a5DitHzcF2wwBcFAdV_oQ@mail.gmail.com>
 <af21fcc5-8fec-c2b2-a488-0a7f571c6214@treenet.co.nz>
 <CAGU_CiKVDgy_dApC4JxKcAGqxtwHeZqh8b3fH_yvjziGBEWttQ@mail.gmail.com>
 <15cf0908-c1df-2640-ec3b-8b13292b265d@treenet.co.nz>
Message-ID: <CAGU_CiLVyEcauGcufb1GpX0pKBv3n39Dyc8qtc6e41TaA5MigQ@mail.gmail.com>

Thanks Amos much appreciated

On Mon, 25 Jan 2021, 13:12 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 26/01/21 1:47 am, robert k Wild wrote:
> > sorry Amos, i will explain why i use the "rep_mime_type"
> >
> > so when users go to a website and click on a link to download and if
> > that download is an .exe/.zip etc etc (on my mimedeny.txt ), squid will
> > stop/block the download and instead they will get an access denied error
> > displayed on the web page
> >
> > im guessing by your reply this is not what the "reply_header_access"is
> > used for
> >
>
> Correct. "reply_header_access deny " is for removing specific headers
> from sent responses. It will not do what you are wanting.
>
> To forbid a whole reply you need "http_reply_access deny ".
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210125/65a3f626/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan 26 04:21:53 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Jan 2021 23:21:53 -0500
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <000001d6f309$c8996680$59cc3380$@gmail.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
Message-ID: <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>

On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
> I'm trying to use:
> acl aclname server_cert_fingerprint [-sha1] fingerprint
> 
> 
> I have cerated the next file:
> /etc/squid/no-ssl-bump-server-fingerprint.list
> 
> And trying to use the next line:
> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
> "/etc/squid/no-ssl-bump-server-fingerprint.list"
> 
> To be explicit despite that only sha1 is a valid checksum.
> Squid doesn't accept the above line 


Does not accept how? What is the error message?


> but this one yes:
> acl NoBump_certificate_fingerprint server_cert_fingerprint
> "/etc/squid/no-ssl-bump-server-fingerprint.list"

> Is there a reason for that?


The use of ACL options and ACL parameter options is poorly documented.

Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
says that server_cert_fingerprint is still broken. Not sure whether that
was true, whether some other commit has fixed that ACL, and whether the
problem mentioned in the commit message is related to your troubles.
https://bugs.squid-cache.org/show_bug.cgi?id=4847
https://github.com/squid-cache/squid/pull/191

Also, according to my 2015 notes, server_cert_fingerprint happens to be
case sensitive. I consider that a bug. I am not sure, but I think Squid
expects uppercase hex letters (if any). I do not know whether that has
been fixed.


Finally, it is dangerous to list ACL parameter options like -sha1 in
front of parameter filename when that parameter file may contain its own
parameter options. A reader may think that -sha1 in squid.conf
overwrites, say, -sha256 in the parameter file, but that is not what
probably will happen when Squid starts supporting both options.

That consideration may actually be the reason why Squid rejects your
first configuration sample (or perhaps it should be the reason even if
it does not).

I am sure there are use cases where the admin wants to apply one
parameter option to the whole file, but the ambiguity is too dangerous
to allow IMO. We should make the choice explicit.


HTH,

Alex.






From ngtech1ltd at gmail.com  Tue Jan 26 07:09:46 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 26 Jan 2021 09:09:46 +0200
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
Message-ID: <000201d6f3b2$3b626240$b22726c0$@gmail.com>

I will try to test it when users are not on the proxy later.

I have another issue with the "server_cert_fingerprint" directive.
I have a working setup which I am unable to make it work with "server_cert_fingerprint".
I'm not sure how and in what step or place in the config it should be used.

My squid conf attached and one server cert fingerprint is:
1C:8C:EC:C8:C4:7F:DF:36:62:69:B1:6A:92:5A:AE:4A:F2:06:E6:B2

Which is in the file:
no-ssl-bump-server-fingerprint.list

I'm trying to understand what I'm doing wrong in the config that stil lets edition.cnn.com be decrypted instead of spliced?

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Tuesday, January 26, 2021 6:22 AM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] acl aclname server_cert_fingerprint

On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
> I'm trying to use:
> acl aclname server_cert_fingerprint [-sha1] fingerprint
> 
> 
> I have cerated the next file:
> /etc/squid/no-ssl-bump-server-fingerprint.list
> 
> And trying to use the next line:
> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
> "/etc/squid/no-ssl-bump-server-fingerprint.list"
> 
> To be explicit despite that only sha1 is a valid checksum.
> Squid doesn't accept the above line 


Does not accept how? What is the error message?


> but this one yes:
> acl NoBump_certificate_fingerprint server_cert_fingerprint
> "/etc/squid/no-ssl-bump-server-fingerprint.list"

> Is there a reason for that?


The use of ACL options and ACL parameter options is poorly documented.

Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
says that server_cert_fingerprint is still broken. Not sure whether that
was true, whether some other commit has fixed that ACL, and whether the
problem mentioned in the commit message is related to your troubles.
https://bugs.squid-cache.org/show_bug.cgi?id=4847
https://github.com/squid-cache/squid/pull/191

Also, according to my 2015 notes, server_cert_fingerprint happens to be
case sensitive. I consider that a bug. I am not sure, but I think Squid
expects uppercase hex letters (if any). I do not know whether that has
been fixed.


Finally, it is dangerous to list ACL parameter options like -sha1 in
front of parameter filename when that parameter file may contain its own
parameter options. A reader may think that -sha1 in squid.conf
overwrites, say, -sha256 in the parameter file, but that is not what
probably will happen when Squid starts supporting both options.

That consideration may actually be the reason why Squid rejects your
first configuration sample (or perhaps it should be the reason even if
it does not).

I am sure there are use cases where the admin wants to apply one
parameter option to the whole file, but the ambiguity is too dangerous
to allow IMO. We should make the choice explicit.


HTH,

Alex.



-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 7195 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210126/6e4f94c4/attachment.obj>

From 2bearqloza at gmail.com  Tue Jan 26 18:54:50 2021
From: 2bearqloza at gmail.com (Milos Dodic)
Date: Tue, 26 Jan 2021 19:54:50 +0100
Subject: [squid-users] Fixing Squid configuration for caching proxy?
Message-ID: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>

I am trying to have a setup where Squid is going to act as a transparent
forward proxy, with caching enabled. I am leaning on a setup like here:
https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/
(though I've tried a few similar ones as well).

The requirement is to have a transparent caching proxy on AWS, that will be
used to reduce the traffic that is being pulled from a remote company (the
data is being pulled on a schedule, while there are not many changes that
often). This traffic causes additional cost, and caching that on our proxy
on AWS could help us out a lot.


The issue I have is, when I configure everything, the test server that is
going through proxy is not actually caching anything, while if I try a test
from the proxy itself (using squidclient) it does. So when the test server
goes for a picture I have stored somewhere in the cloud, the squid access
log shows "TCP_TUNNEL/200". But when I try from the proxy itself with
squidclient tool, I get "TCP_MEM_HIT/200" (the first time it was MISS,
before it was cached), so caching works properly - I even see the new
folder being created or the cached content.

I have the rerouting rules added to IP tables, source and destination check
is disabled (AWS setup), and overall, traffic is going as it should. I
assume I need to make some changes in the configuration, as that part is
where I had to copy most of the stuff, and have least experience with. I've
generated the certificate as per the doc, and my config is mostly the same
as the one there:

(note, I have tried with multiple changes to this config, without success)

visible_hostname squid
cache_dir ufs /squid/cache 10000 16 256

# Handle HTTP requests
http_port 3128
http_port 3129 intercept
acl allowed_http_sites dstdomain .amazonaws.com
http_access allow allowed_http_sites

# Handle HTTPS requests
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
acl SSL_port port 443
http_access allow SSL_port
acl allowed_https_sites ssl::server_name .amazonaws.com
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate step3 all

http_access deny all


The cert and everything else is generated as per the guide (and a few
guides are very similar here when it comes to this part).
The whitelisting works, mostly everything else works too, so the only thing
missing is squid not caching things that are requested by the test server
and is instead only passing it through. Any idea what configuration changes
I need in order to fix this? I guess I am doing something wrong with ssl
bump.

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210126/c6681740/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan 26 20:14:43 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 26 Jan 2021 15:14:43 -0500
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
Message-ID: <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>

On 1/26/21 1:54 PM, Milos Dodic wrote:

> when the test server goes for a picture I have stored somewhere in
> the cloud, the squid access log shows "TCP_TUNNEL/200". But when I
> try from the proxy itself with squidclient tool, I get
> "TCP_MEM_HIT/200"


Given the very limited information you have provided, I am guessing that

* the primary tests opens a CONNECT tunnel through Squid
* the squidclient test sends a plain text HTTP request to Squid

The final origin server destination may be the same in both tests, but
the two transactions are completely different from Squid point of view.


> ssl_bump peek step1 all
> ssl_bump peek step2 allowed_https_sites
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate step3 all


AFAICT, this configuration is splicing or terminating all TLS traffic.
No bumping at all. If you want your Squid to bump TLS tunnels, then you
have to have at least one "bump" rule!

I do not know what your overall SslBump needs are, but perhaps you meant
something like the following?

    acl shouldBeBumped ssl::server_name .amazonaws.com

    ssl_bump stare all
    ssl_bump bump shouldBeBumped
    ssl_bump terminate all

Please do not use the configuration above until you understand what it
does. Please see https://wiki.squid-cache.org/Features/SslPeekAndSplice
for details.

Depending on your environment, the http_access rules may need to be
adjusted to allow CONNECT requests (to TLS-safe ports) to IP addresses
that do not result in .amazonaws.com in reverse DNS lookups.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jan 27 15:12:20 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jan 2021 10:12:20 -0500
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <000201d6f3b2$3b626240$b22726c0$@gmail.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
 <000201d6f3b2$3b626240$b22726c0$@gmail.com>
Message-ID: <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>

On 1/26/21 2:09 AM, Eliezer Croitoru wrote:

> I'm trying to understand what I'm doing wrong in the config that stil
> lets edition.cnn.com be decrypted instead of spliced?

If you still need help, please share the relevant parts of your
configuration and logs. I would start with ssl_bump rules and access log
records containing additional %error_code/%err_detail fields.

Alex.



> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Tuesday, January 26, 2021 6:22 AM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
> 
> On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
>> I'm trying to use:
>> acl aclname server_cert_fingerprint [-sha1] fingerprint
>>
>>
>> I have cerated the next file:
>> /etc/squid/no-ssl-bump-server-fingerprint.list
>>
>> And trying to use the next line:
>> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>
>> To be explicit despite that only sha1 is a valid checksum.
>> Squid doesn't accept the above line 
> 
> 
> Does not accept how? What is the error message?
> 
> 
>> but this one yes:
>> acl NoBump_certificate_fingerprint server_cert_fingerprint
>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
> 
>> Is there a reason for that?
> 
> 
> The use of ACL options and ACL parameter options is poorly documented.
> 
> Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
> says that server_cert_fingerprint is still broken. Not sure whether that
> was true, whether some other commit has fixed that ACL, and whether the
> problem mentioned in the commit message is related to your troubles.
> https://bugs.squid-cache.org/show_bug.cgi?id=4847
> https://github.com/squid-cache/squid/pull/191
> 
> Also, according to my 2015 notes, server_cert_fingerprint happens to be
> case sensitive. I consider that a bug. I am not sure, but I think Squid
> expects uppercase hex letters (if any). I do not know whether that has
> been fixed.
> 
> 
> Finally, it is dangerous to list ACL parameter options like -sha1 in
> front of parameter filename when that parameter file may contain its own
> parameter options. A reader may think that -sha1 in squid.conf
> overwrites, say, -sha256 in the parameter file, but that is not what
> probably will happen when Squid starts supporting both options.
> 
> That consideration may actually be the reason why Squid rejects your
> first configuration sample (or perhaps it should be the reason even if
> it does not).
> 
> I am sure there are use cases where the admin wants to apply one
> parameter option to the whole file, but the ambiguity is too dangerous
> to allow IMO. We should make the choice explicit.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ml at netfence.it  Wed Jan 27 16:01:10 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Wed, 27 Jan 2021 17:01:10 +0100
Subject: [squid-users] Squid "suspending ICAP service for too many failures"
Message-ID: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>

Hello.

On a box I manage, Squids occasionally stops for a few minutes, blaming 
a communication error with C-ICAP (running SquidClamAV).

In cache.log I see:
> 2021/01/04 14:24:24 kid1| suspending ICAP service for too many failures
> 2021/01/04 14:24:24 kid1| essential ICAP service is suspended: icap://127.0.0.1:1344/squidclamav [down,susp,fail11]

This happens usually once a day, always at the same time.
AFAIK there's no particular job running on the server at that time; I 
analyzed squid.log to see whether some client accesses something 
specific at that hour of the day, but came up empty.

Obviously I looked into C-ICAP logs, but, again, found no hint of any 
error or trouble.


Any suggestion on what to do to investigate this?

  bye & Thanks
	av.


From ngtech1ltd at gmail.com  Wed Jan 27 16:45:20 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 27 Jan 2021 18:45:20 +0200
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
 <000201d6f3b2$3b626240$b22726c0$@gmail.com>
 <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>
Message-ID: <000001d6f4cb$ce1242d0$6a36c870$@gmail.com>

Hey Alex,

I'm not sure I understood hat these errorcde and error detai.
I assume that there is a relevant debug_options for parsing the fingerprint.
The next thing I was thinking about was the fingerprint validation related debug.
To verify what might make squid to compare wrongly the SHA1 signature.

 I have attached the full squid.conf
The more relevant parts are:
## START
acl NoBump_server_regex ssl::server_name_regex -i "/etc/squid/no-ssl-bump-regex.list"
acl NoBump_server_regex_by_urls_domain ssl::server_name_regex -i "/etc/squid/no-ssl-bump-urls-domains-regex.list"
acl NoBump_server_name ssl::server_name "/etc/squid/no-ssl-bump-server-name.list"
acl NoBump_dst dst "/etc/squid/no-ssl-bump-server-dst-addresses.list"
acl NoBump_certificate_fingerprint server_cert_fingerprint "/etc/squid/no-ssl-bump-server-fingerprint.list"
acl NoBump_src src "/etc/squid/no-ssl-bump-client-src.list"

acl tls_to_splice any-of NoBump_src NoBump_server_name NoBump_server_regex_by_urls_domain NoBump_server_regex NoBump_dst NoBump_certificate_fingerprint bypass_src_helper


acl Bump_server_regex ssl::server_name_regex -i "/etc/squid/ssl-bump-regex.list"
acl Bump_server_regex_by_urls_domain ssl::server_name_regex -i "/etc/squid/ssl-bump-urls-domains-regex.list"
acl Bump_server_name ssl::server_name "/etc/squid/ssl-bump-server-name.list"
acl Bump_dst dst "/etc/squid/ssl-bump-server-dst-addresses.list"

acl tls_to_bump any-of Bump_server_name Bump_server_regex_by_urls_domain Bump_server_regex Bump_dst sni_matcher_helper yandex_bl_checker_helper


# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

ssl_bump peek tls_s1_connect
ssl_bump splice tls_to_splice
ssl_bump stare tls_s2_client_hello
ssl_bump bump tls_to_bump
## END

The fingerprint contains only 3 signatures and one of them is (Quoted as is):
1C:8C:EC:C8:C4:7F:DF:36:62:69:B1:6A:92:5A:AE:4A:F2:06:E6:B2

My setup is working fine except this fingerprint part which is not always the best way to splice.
However it seems like it should be pretty straight forward.

I can dump the whole config into a tar file to try and understand better the setup if required.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Wednesday, January 27, 2021 5:12 PM
To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] acl aclname server_cert_fingerprint

On 1/26/21 2:09 AM, Eliezer Croitoru wrote:

> I'm trying to understand what I'm doing wrong in the config that stil
> lets edition.cnn.com be decrypted instead of spliced?

If you still need help, please share the relevant parts of your
configuration and logs. I would start with ssl_bump rules and access log
records containing additional %error_code/%err_detail fields.

Alex.



> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Tuesday, January 26, 2021 6:22 AM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
> 
> On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
>> I'm trying to use:
>> acl aclname server_cert_fingerprint [-sha1] fingerprint
>>
>>
>> I have cerated the next file:
>> /etc/squid/no-ssl-bump-server-fingerprint.list
>>
>> And trying to use the next line:
>> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>
>> To be explicit despite that only sha1 is a valid checksum.
>> Squid doesn't accept the above line 
> 
> 
> Does not accept how? What is the error message?
> 
> 
>> but this one yes:
>> acl NoBump_certificate_fingerprint server_cert_fingerprint
>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
> 
>> Is there a reason for that?
> 
> 
> The use of ACL options and ACL parameter options is poorly documented.
> 
> Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
> says that server_cert_fingerprint is still broken. Not sure whether that
> was true, whether some other commit has fixed that ACL, and whether the
> problem mentioned in the commit message is related to your troubles.
> https://bugs.squid-cache.org/show_bug.cgi?id=4847
> https://github.com/squid-cache/squid/pull/191
> 
> Also, according to my 2015 notes, server_cert_fingerprint happens to be
> case sensitive. I consider that a bug. I am not sure, but I think Squid
> expects uppercase hex letters (if any). I do not know whether that has
> been fixed.
> 
> 
> Finally, it is dangerous to list ACL parameter options like -sha1 in
> front of parameter filename when that parameter file may contain its own
> parameter options. A reader may think that -sha1 in squid.conf
> overwrites, say, -sha256 in the parameter file, but that is not what
> probably will happen when Squid starts supporting both options.
> 
> That consideration may actually be the reason why Squid rejects your
> first configuration sample (or perhaps it should be the reason even if
> it does not).
> 
> I am sure there are use cases where the admin wants to apply one
> parameter option to the whole file, but the ambiguity is too dangerous
> to allow IMO. We should make the choice explicit.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 7195 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210127/63b93806/attachment.obj>

From rousskov at measurement-factory.com  Wed Jan 27 17:11:43 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jan 2021 12:11:43 -0500
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
Message-ID: <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>

On 1/27/21 11:01 AM, Andrea Venturoli wrote:

>> 2021/01/04 14:24:24 kid1| suspending ICAP service for too many failures
>> 2021/01/04 14:24:24 kid1| essential ICAP service is suspended:
>> icap://127.0.0.1:1344/squidclamav [down,susp,fail11]

> This happens usually once a day, always at the same time.
> AFAIK there's no particular job running on the server at that time; I
> analyzed squid.log to see whether some client accesses something
> specific at that hour of the day, but came up empty.

Unfortunately, Squid ICAP client does not log some of the failures at
debugging level 0 or 1.


> Any suggestion on what to do to investigate this?

Enable ICAP debugging and study cache.log for relevant messages,
especially just before the "suspending ICAP service" message shown above.

    debug_options ALL,1 93,7

Debugging will produce a lot of irrelevant to you cache.log lines If
necessary, you can enable debugging an hour (or even a minute!) before
the regular failure. This will allow you to detail the last failure (at
least). It is possible that all the 11 failures are the same.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jan 27 18:43:19 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jan 2021 13:43:19 -0500
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <000001d6f4cb$ce1242d0$6a36c870$@gmail.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
 <000201d6f3b2$3b626240$b22726c0$@gmail.com>
 <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>
 <000001d6f4cb$ce1242d0$6a36c870$@gmail.com>
Message-ID: <198a0cb8-a7db-1cbb-b410-e5c067765d59@measurement-factory.com>

On 1/27/21 11:45 AM, Eliezer Croitoru wrote:

> I'm not sure I understood hat these errorcde and error detai.

FWIW, access log fields are configured using logformat %codes. Search
squid.conf.documented for the words "err_code" and "err_detail" (no quotes).


> acl tls_to_splice any-of ... NoBump_certificate_fingerprint

> acl tls_s1_connect at_step SslBump1
> acl tls_s2_client_hello at_step SslBump2

> ssl_bump peek tls_s1_connect
> ssl_bump splice tls_to_splice
> ssl_bump stare tls_s2_client_hello
> ssl_bump bump tls_to_bump

Bugs notwithstanding, the NoBump_certificate_fingerprint ACL will never
match in the above configuration AFAICT:

* step1 is excluded by the earlier "peek if tls_s1_connect" rule. The
server certificate is not yet available during that step anyway.

* step2 is reachable for a "splice" action, but the server certificate
is still not yet available during that step.

* step3 is unreachable for a "splice" action because the only non-final
action during step2 is "stare". Starting precludes splicing.


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Wednesday, January 27, 2021 5:12 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
> 
> On 1/26/21 2:09 AM, Eliezer Croitoru wrote:
> 
>> I'm trying to understand what I'm doing wrong in the config that stil
>> lets edition.cnn.com be decrypted instead of spliced?
> 
> If you still need help, please share the relevant parts of your
> configuration and logs. I would start with ssl_bump rules and access log
> records containing additional %error_code/%err_detail fields.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Tuesday, January 26, 2021 6:22 AM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
>>
>> On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
>>> I'm trying to use:
>>> acl aclname server_cert_fingerprint [-sha1] fingerprint
>>>
>>>
>>> I have cerated the next file:
>>> /etc/squid/no-ssl-bump-server-fingerprint.list
>>>
>>> And trying to use the next line:
>>> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>>
>>> To be explicit despite that only sha1 is a valid checksum.
>>> Squid doesn't accept the above line 
>>
>>
>> Does not accept how? What is the error message?
>>
>>
>>> but this one yes:
>>> acl NoBump_certificate_fingerprint server_cert_fingerprint
>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>
>>> Is there a reason for that?
>>
>>
>> The use of ACL options and ACL parameter options is poorly documented.
>>
>> Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
>> says that server_cert_fingerprint is still broken. Not sure whether that
>> was true, whether some other commit has fixed that ACL, and whether the
>> problem mentioned in the commit message is related to your troubles.
>> https://bugs.squid-cache.org/show_bug.cgi?id=4847
>> https://github.com/squid-cache/squid/pull/191
>>
>> Also, according to my 2015 notes, server_cert_fingerprint happens to be
>> case sensitive. I consider that a bug. I am not sure, but I think Squid
>> expects uppercase hex letters (if any). I do not know whether that has
>> been fixed.
>>
>>
>> Finally, it is dangerous to list ACL parameter options like -sha1 in
>> front of parameter filename when that parameter file may contain its own
>> parameter options. A reader may think that -sha1 in squid.conf
>> overwrites, say, -sha256 in the parameter file, but that is not what
>> probably will happen when Squid starts supporting both options.
>>
>> That consideration may actually be the reason why Squid rejects your
>> first configuration sample (or perhaps it should be the reason even if
>> it does not).
>>
>> I am sure there are use cases where the admin wants to apply one
>> parameter option to the whole file, but the ambiguity is too dangerous
>> to allow IMO. We should make the choice explicit.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>



From ngtech1ltd at gmail.com  Wed Jan 27 18:50:14 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 27 Jan 2021 20:50:14 +0200
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <198a0cb8-a7db-1cbb-b410-e5c067765d59@measurement-factory.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
 <000201d6f3b2$3b626240$b22726c0$@gmail.com>
 <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>
 <000001d6f4cb$ce1242d0$6a36c870$@gmail.com>
 <198a0cb8-a7db-1cbb-b410-e5c067765d59@measurement-factory.com>
Message-ID: <000001d6f4dd$40289700$c079c500$@gmail.com>

First thanks it helps.
The next thing is that I am still missing a way to make this work with the fingerprint.
We first need to know the fingerprint but when squid "knows" about it, it's already too late.
In what config scenario can it work?

Eliezer

* The error code are irrelevant since there aren't any errors.

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Wednesday, January 27, 2021 8:43 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
Subject: Re: [squid-users] acl aclname server_cert_fingerprint

On 1/27/21 11:45 AM, Eliezer Croitoru wrote:

> I'm not sure I understood hat these errorcde and error detai.

FWIW, access log fields are configured using logformat %codes. Search
squid.conf.documented for the words "err_code" and "err_detail" (no quotes).


> acl tls_to_splice any-of ... NoBump_certificate_fingerprint

> acl tls_s1_connect at_step SslBump1
> acl tls_s2_client_hello at_step SslBump2

> ssl_bump peek tls_s1_connect
> ssl_bump splice tls_to_splice
> ssl_bump stare tls_s2_client_hello
> ssl_bump bump tls_to_bump

Bugs notwithstanding, the NoBump_certificate_fingerprint ACL will never
match in the above configuration AFAICT:

* step1 is excluded by the earlier "peek if tls_s1_connect" rule. The
server certificate is not yet available during that step anyway.

* step2 is reachable for a "splice" action, but the server certificate
is still not yet available during that step.

* step3 is unreachable for a "splice" action because the only non-final
action during step2 is "stare". Starting precludes splicing.


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Wednesday, January 27, 2021 5:12 PM
> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
> 
> On 1/26/21 2:09 AM, Eliezer Croitoru wrote:
> 
>> I'm trying to understand what I'm doing wrong in the config that stil
>> lets edition.cnn.com be decrypted instead of spliced?
> 
> If you still need help, please share the relevant parts of your
> configuration and logs. I would start with ssl_bump rules and access log
> records containing additional %error_code/%err_detail fields.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Tuesday, January 26, 2021 6:22 AM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
>>
>> On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
>>> I'm trying to use:
>>> acl aclname server_cert_fingerprint [-sha1] fingerprint
>>>
>>>
>>> I have cerated the next file:
>>> /etc/squid/no-ssl-bump-server-fingerprint.list
>>>
>>> And trying to use the next line:
>>> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>>
>>> To be explicit despite that only sha1 is a valid checksum.
>>> Squid doesn't accept the above line 
>>
>>
>> Does not accept how? What is the error message?
>>
>>
>>> but this one yes:
>>> acl NoBump_certificate_fingerprint server_cert_fingerprint
>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>
>>> Is there a reason for that?
>>
>>
>> The use of ACL options and ACL parameter options is poorly documented.
>>
>> Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
>> says that server_cert_fingerprint is still broken. Not sure whether that
>> was true, whether some other commit has fixed that ACL, and whether the
>> problem mentioned in the commit message is related to your troubles.
>> https://bugs.squid-cache.org/show_bug.cgi?id=4847
>> https://github.com/squid-cache/squid/pull/191
>>
>> Also, according to my 2015 notes, server_cert_fingerprint happens to be
>> case sensitive. I consider that a bug. I am not sure, but I think Squid
>> expects uppercase hex letters (if any). I do not know whether that has
>> been fixed.
>>
>>
>> Finally, it is dangerous to list ACL parameter options like -sha1 in
>> front of parameter filename when that parameter file may contain its own
>> parameter options. A reader may think that -sha1 in squid.conf
>> overwrites, say, -sha256 in the parameter file, but that is not what
>> probably will happen when Squid starts supporting both options.
>>
>> That consideration may actually be the reason why Squid rejects your
>> first configuration sample (or perhaps it should be the reason even if
>> it does not).
>>
>> I am sure there are use cases where the admin wants to apply one
>> parameter option to the whole file, but the ambiguity is too dangerous
>> to allow IMO. We should make the choice explicit.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>



From rousskov at measurement-factory.com  Wed Jan 27 20:06:51 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jan 2021 15:06:51 -0500
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <000001d6f4dd$40289700$c079c500$@gmail.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
 <000201d6f3b2$3b626240$b22726c0$@gmail.com>
 <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>
 <000001d6f4cb$ce1242d0$6a36c870$@gmail.com>
 <198a0cb8-a7db-1cbb-b410-e5c067765d59@measurement-factory.com>
 <000001d6f4dd$40289700$c079c500$@gmail.com>
Message-ID: <0713c44d-1790-42c6-ad94-57c749661a72@measurement-factory.com>

On 1/27/21 1:50 PM, Eliezer Croitoru wrote:

> I am still missing a way to make this work with the fingerprint.

I do not know what you are trying to accomplish (i.e. what "this" is).


> We first need to know the fingerprint but when squid "knows" about
> it, it's already too late. In what config scenario can it work?

Knowing the fingerprint (or any other server-sent detail!) is indeed not
useful for making bump-vs-splice decisions. Fingerprint knowledge can be
useful for many other decisions, including whether to allow an HTTP
request, whether to cache an HTTP response, and whether to terminate a
TLS connection.


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Wednesday, January 27, 2021 8:43 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
> 
> On 1/27/21 11:45 AM, Eliezer Croitoru wrote:
> 
>> I'm not sure I understood hat these errorcde and error detai.
> 
> FWIW, access log fields are configured using logformat %codes. Search
> squid.conf.documented for the words "err_code" and "err_detail" (no quotes).
> 
> 
>> acl tls_to_splice any-of ... NoBump_certificate_fingerprint
> 
>> acl tls_s1_connect at_step SslBump1
>> acl tls_s2_client_hello at_step SslBump2
> 
>> ssl_bump peek tls_s1_connect
>> ssl_bump splice tls_to_splice
>> ssl_bump stare tls_s2_client_hello
>> ssl_bump bump tls_to_bump
> 
> Bugs notwithstanding, the NoBump_certificate_fingerprint ACL will never
> match in the above configuration AFAICT:
> 
> * step1 is excluded by the earlier "peek if tls_s1_connect" rule. The
> server certificate is not yet available during that step anyway.
> 
> * step2 is reachable for a "splice" action, but the server certificate
> is still not yet available during that step.
> 
> * step3 is unreachable for a "splice" action because the only non-final
> action during step2 is "stare". Starting precludes splicing.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Wednesday, January 27, 2021 5:12 PM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
>>
>> On 1/26/21 2:09 AM, Eliezer Croitoru wrote:
>>
>>> I'm trying to understand what I'm doing wrong in the config that stil
>>> lets edition.cnn.com be decrypted instead of spliced?
>>
>> If you still need help, please share the relevant parts of your
>> configuration and logs. I would start with ssl_bump rules and access log
>> records containing additional %error_code/%err_detail fields.
>>
>> Alex.
>>
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>>> Sent: Tuesday, January 26, 2021 6:22 AM
>>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
>>>
>>> On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
>>>> I'm trying to use:
>>>> acl aclname server_cert_fingerprint [-sha1] fingerprint
>>>>
>>>>
>>>> I have cerated the next file:
>>>> /etc/squid/no-ssl-bump-server-fingerprint.list
>>>>
>>>> And trying to use the next line:
>>>> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
>>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>>>
>>>> To be explicit despite that only sha1 is a valid checksum.
>>>> Squid doesn't accept the above line 
>>>
>>>
>>> Does not accept how? What is the error message?
>>>
>>>
>>>> but this one yes:
>>>> acl NoBump_certificate_fingerprint server_cert_fingerprint
>>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>>
>>>> Is there a reason for that?
>>>
>>>
>>> The use of ACL options and ACL parameter options is poorly documented.
>>>
>>> Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
>>> says that server_cert_fingerprint is still broken. Not sure whether that
>>> was true, whether some other commit has fixed that ACL, and whether the
>>> problem mentioned in the commit message is related to your troubles.
>>> https://bugs.squid-cache.org/show_bug.cgi?id=4847
>>> https://github.com/squid-cache/squid/pull/191
>>>
>>> Also, according to my 2015 notes, server_cert_fingerprint happens to be
>>> case sensitive. I consider that a bug. I am not sure, but I think Squid
>>> expects uppercase hex letters (if any). I do not know whether that has
>>> been fixed.
>>>
>>>
>>> Finally, it is dangerous to list ACL parameter options like -sha1 in
>>> front of parameter filename when that parameter file may contain its own
>>> parameter options. A reader may think that -sha1 in squid.conf
>>> overwrites, say, -sha256 in the parameter file, but that is not what
>>> probably will happen when Squid starts supporting both options.
>>>
>>> That consideration may actually be the reason why Squid rejects your
>>> first configuration sample (or perhaps it should be the reason even if
>>> it does not).
>>>
>>> I am sure there are use cases where the admin wants to apply one
>>> parameter option to the whole file, but the ambiguity is too dangerous
>>> to allow IMO. We should make the choice explicit.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>



From ngtech1ltd at gmail.com  Thu Jan 28 08:02:12 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 28 Jan 2021 10:02:12 +0200
Subject: [squid-users] acl aclname server_cert_fingerprint
In-Reply-To: <0713c44d-1790-42c6-ad94-57c749661a72@measurement-factory.com>
References: <000001d6f309$c8996680$59cc3380$@gmail.com>
 <4140a6a8-2555-eb5c-b082-6723c78a739d@measurement-factory.com>
 <000201d6f3b2$3b626240$b22726c0$@gmail.com>
 <2ec9e90e-580c-d37d-68f6-3d1b3d27bb02@measurement-factory.com>
 <000001d6f4cb$ce1242d0$6a36c870$@gmail.com>
 <198a0cb8-a7db-1cbb-b410-e5c067765d59@measurement-factory.com>
 <000001d6f4dd$40289700$c079c500$@gmail.com>
 <0713c44d-1790-42c6-ad94-57c749661a72@measurement-factory.com>
Message-ID: <000701d6f54b$e3724f00$aa56ed00$@gmail.com>

Thanks Alex,

I had a bit trouble understanding while now it seems more clear.
I was a bit confused with the certificate validator.
I got the terminology a bit confused but now it's much clear to me.

Not directly related I hope I can write the relevant validator code.

Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Wednesday, January 27, 2021 10:07 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
Subject: Re: [squid-users] acl aclname server_cert_fingerprint

On 1/27/21 1:50 PM, Eliezer Croitoru wrote:

> I am still missing a way to make this work with the fingerprint.

I do not know what you are trying to accomplish (i.e. what "this" is).


> We first need to know the fingerprint but when squid "knows" about
> it, it's already too late. In what config scenario can it work?

Knowing the fingerprint (or any other server-sent detail!) is indeed not
useful for making bump-vs-splice decisions. Fingerprint knowledge can be
useful for many other decisions, including whether to allow an HTTP
request, whether to cache an HTTP response, and whether to terminate a
TLS connection.


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Wednesday, January 27, 2021 8:43 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <ngtech1ltd at gmail.com>
> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
> 
> On 1/27/21 11:45 AM, Eliezer Croitoru wrote:
> 
>> I'm not sure I understood hat these errorcde and error detai.
> 
> FWIW, access log fields are configured using logformat %codes. Search
> squid.conf.documented for the words "err_code" and "err_detail" (no quotes).
> 
> 
>> acl tls_to_splice any-of ... NoBump_certificate_fingerprint
> 
>> acl tls_s1_connect at_step SslBump1
>> acl tls_s2_client_hello at_step SslBump2
> 
>> ssl_bump peek tls_s1_connect
>> ssl_bump splice tls_to_splice
>> ssl_bump stare tls_s2_client_hello
>> ssl_bump bump tls_to_bump
> 
> Bugs notwithstanding, the NoBump_certificate_fingerprint ACL will never
> match in the above configuration AFAICT:
> 
> * step1 is excluded by the earlier "peek if tls_s1_connect" rule. The
> server certificate is not yet available during that step anyway.
> 
> * step2 is reachable for a "splice" action, but the server certificate
> is still not yet available during that step.
> 
> * step3 is unreachable for a "splice" action because the only non-final
> action during step2 is "stare". Starting precludes splicing.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>> Sent: Wednesday, January 27, 2021 5:12 PM
>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
>>
>> On 1/26/21 2:09 AM, Eliezer Croitoru wrote:
>>
>>> I'm trying to understand what I'm doing wrong in the config that stil
>>> lets edition.cnn.com be decrypted instead of spliced?
>>
>> If you still need help, please share the relevant parts of your
>> configuration and logs. I would start with ssl_bump rules and access log
>> records containing additional %error_code/%err_detail fields.
>>
>> Alex.
>>
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov <rousskov at measurement-factory.com> 
>>> Sent: Tuesday, January 26, 2021 6:22 AM
>>> To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] acl aclname server_cert_fingerprint
>>>
>>> On 1/25/21 6:03 AM, Eliezer Croitoru wrote:
>>>> I'm trying to use:
>>>> acl aclname server_cert_fingerprint [-sha1] fingerprint
>>>>
>>>>
>>>> I have cerated the next file:
>>>> /etc/squid/no-ssl-bump-server-fingerprint.list
>>>>
>>>> And trying to use the next line:
>>>> acl NoBump_certificate_fingerprint server_cert_fingerprint -sha1
>>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>>>
>>>> To be explicit despite that only sha1 is a valid checksum.
>>>> Squid doesn't accept the above line 
>>>
>>>
>>> Does not accept how? What is the error message?
>>>
>>>
>>>> but this one yes:
>>>> acl NoBump_certificate_fingerprint server_cert_fingerprint
>>>> "/etc/squid/no-ssl-bump-server-fingerprint.list"
>>>
>>>> Is there a reason for that?
>>>
>>>
>>> The use of ACL options and ACL parameter options is poorly documented.
>>>
>>> Squid Bug 4847 is marked as fixed, but the corresponding commit d4c6aca
>>> says that server_cert_fingerprint is still broken. Not sure whether that
>>> was true, whether some other commit has fixed that ACL, and whether the
>>> problem mentioned in the commit message is related to your troubles.
>>> https://bugs.squid-cache.org/show_bug.cgi?id=4847
>>> https://github.com/squid-cache/squid/pull/191
>>>
>>> Also, according to my 2015 notes, server_cert_fingerprint happens to be
>>> case sensitive. I consider that a bug. I am not sure, but I think Squid
>>> expects uppercase hex letters (if any). I do not know whether that has
>>> been fixed.
>>>
>>>
>>> Finally, it is dangerous to list ACL parameter options like -sha1 in
>>> front of parameter filename when that parameter file may contain its own
>>> parameter options. A reader may think that -sha1 in squid.conf
>>> overwrites, say, -sha256 in the parameter file, but that is not what
>>> probably will happen when Squid starts supporting both options.
>>>
>>> That consideration may actually be the reason why Squid rejects your
>>> first configuration sample (or perhaps it should be the reason even if
>>> it does not).
>>>
>>> I am sure there are use cases where the admin wants to apply one
>>> parameter option to the whole file, but the ambiguity is too dangerous
>>> to allow IMO. We should make the choice explicit.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>



From ngtech1ltd at gmail.com  Thu Jan 28 15:02:32 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 28 Jan 2021 17:02:32 +0200
Subject: [squid-users] CentOS Changes, anyone noticed?
Message-ID: <000201d6f586$9cadd8b0$d6098a10$@gmail.com>

Since I am providing the CentOS RPM's I noticed that there was a change on
the OS.

Has anyone noticed the latest changes with CentOS 8 and Stream?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210128/4de4a92b/attachment.htm>

From 2bearqloza at gmail.com  Thu Jan 28 18:34:34 2021
From: 2bearqloza at gmail.com (Milos Dodic)
Date: Thu, 28 Jan 2021 19:34:34 +0100
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
Message-ID: <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>

I have redeployed everything, with most basic configuration, and use the
proposed config for ssl_bump.
The test server that goes through Squid now doesn't get tunneled, and
instead checks the cache. I get something like this
NONE/200
TCP_MISS/200

But I have noticed that the test server also doesn't cache anything, and
instead only looks at the cache.
So if I try to go for a file in S3, it says MISS, and after that, MISS
again, and I see no new objects in cache being created.
If I try the same thing from the proxy itself, I get the MISS, and the
object gets cached, as it should.
When I go back to the test server, and try again, it sees the object in
cache and returns TCP_MEM_HIT/200 instead.

Is there a specific configuration that I need to add/enable, in order to
have the server cache the objects, or am I making a mistake elsewhere
perhaps?
This is the entire config file:


visible_hostname squid
cache_dir ufs /test/cache/squid 10000 16 256

http_access allow localhost
http_access alow all

http_port 3128
http_port 3129 intercept
acl allowed_http_sites dstdomain .amazonaws.com
http_access allow allowed_http_sites

https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
acl SSL_port port 443
http_access allow SSL_port
acl allowed_https_sites ssl::server_name .amazonaws.com

ssl_bump stare all
ssl_bump bump allowed_https_sites
ssl_bump terminate all




Thanks!

On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/26/21 1:54 PM, Milos Dodic wrote:
>
> > when the test server goes for a picture I have stored somewhere in
> > the cloud, the squid access log shows "TCP_TUNNEL/200". But when I
> > try from the proxy itself with squidclient tool, I get
> > "TCP_MEM_HIT/200"
>
>
> Given the very limited information you have provided, I am guessing that
>
> * the primary tests opens a CONNECT tunnel through Squid
> * the squidclient test sends a plain text HTTP request to Squid
>
> The final origin server destination may be the same in both tests, but
> the two transactions are completely different from Squid point of view.
>
>
> > ssl_bump peek step1 all
> > ssl_bump peek step2 allowed_https_sites
> > ssl_bump splice step3 allowed_https_sites
> > ssl_bump terminate step3 all
>
>
> AFAICT, this configuration is splicing or terminating all TLS traffic.
> No bumping at all. If you want your Squid to bump TLS tunnels, then you
> have to have at least one "bump" rule!
>
> I do not know what your overall SslBump needs are, but perhaps you meant
> something like the following?
>
>     acl shouldBeBumped ssl::server_name .amazonaws.com
>
>     ssl_bump stare all
>     ssl_bump bump shouldBeBumped
>     ssl_bump terminate all
>
> Please do not use the configuration above until you understand what it
> does. Please see https://wiki.squid-cache.org/Features/SslPeekAndSplice
> for details.
>
> Depending on your environment, the http_access rules may need to be
> adjusted to allow CONNECT requests (to TLS-safe ports) to IP addresses
> that do not result in .amazonaws.com in reverse DNS lookups.
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210128/b9ac4629/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 29 16:40:12 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 29 Jan 2021 11:40:12 -0500
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
Message-ID: <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>

On 1/28/21 1:34 PM, Milos Dodic wrote:

> I have noticed that the test server also doesn't cache anything
> So if I try to go for a file in S3, it says MISS, and after that, MISS
> again, and I see no new objects in cache being created.

> If I try the same thing from the proxy itself, I get the MISS, and the
> object gets cached, as it should.
> When I go back to the test server, and try again, it sees the object in
> cache and returns TCP_MEM_HIT/200 instead.

Without more details, I can only speculate that the client running on
the test server sends different HTTP request headers than the client
running on the proxy itself. You can see the headers in cache.log if you
set debug_options to ALL,2. If you are not sure whether they are the
same, please share those logs. They will also contain response headers
and other potentially useful details.

If the request headers are the same in both tests, then I would
recommend sharing compressed ALL,7 or ALL,9 debugging logs of both
successful and unsuccessful tests (four transactions, two logs) for
analysis. Do not use sensitive data for these tests.

https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

Alex.



> This is the entire config file:
> 
> 
> visible_hostname squid
> cache_dir ufs /test/cache/squid 10000 16 256
> 
> http_access allow localhost
> http_access alow all
> 
> http_port 3128
> http_port 3129 intercept
> acl allowed_http_sites dstdomain .amazonaws.com <http://amazonaws.com>
> http_access allow allowed_http_sites
> 
> https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> acl SSL_port port 443
> http_access allow SSL_port
> acl allowed_https_sites ssl::server_name .amazonaws.com
> <http://amazonaws.com>
> 
> ssl_bump stare all
> ssl_bump bump allowed_https_sites
> ssl_bump terminate all


> On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov wrote:
> 
>     On 1/26/21 1:54 PM, Milos Dodic wrote:
> 
>     > when the test server goes for a picture I have stored somewhere in
>     > the cloud, the squid access log shows "TCP_TUNNEL/200". But when I
>     > try from the proxy itself with squidclient tool, I get
>     > "TCP_MEM_HIT/200"
> 
> 
>     Given the very limited information you have provided, I am guessing that
> 
>     * the primary tests opens a CONNECT tunnel through Squid
>     * the squidclient test sends a plain text HTTP request to Squid
> 
>     The final origin server destination may be the same in both tests, but
>     the two transactions are completely different from Squid point of view.
> 
> 
>     > ssl_bump peek step1 all
>     > ssl_bump peek step2 allowed_https_sites
>     > ssl_bump splice step3 allowed_https_sites
>     > ssl_bump terminate step3 all
> 
> 
>     AFAICT, this configuration is splicing or terminating all TLS traffic.
>     No bumping at all. If you want your Squid to bump TLS tunnels, then you
>     have to have at least one "bump" rule!
> 
>     I do not know what your overall SslBump needs are, but perhaps you meant
>     something like the following?
> 
>     ? ? acl shouldBeBumped ssl::server_name .amazonaws.com
>     <http://amazonaws.com>
> 
>     ? ? ssl_bump stare all
>     ? ? ssl_bump bump shouldBeBumped
>     ? ? ssl_bump terminate all
> 
>     Please do not use the configuration above until you understand what it
>     does. Please see https://wiki.squid-cache.org/Features/SslPeekAndSplice
>     for details.
> 
>     Depending on your environment, the http_access rules may need to be
>     adjusted to allow CONNECT requests (to TLS-safe ports) to IP addresses
>     that do not result in .amazonaws.com <http://amazonaws.com> in
>     reverse DNS lookups.
> 
> 
>     HTH,
> 
>     Alex.
> 



From ml at netfence.it  Fri Jan 29 16:55:49 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Fri, 29 Jan 2021 17:55:49 +0100
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
 <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
Message-ID: <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>

On 1/27/21 6:11 PM, Alex Rousskov wrote:

> Enable ICAP debugging and study cache.log for relevant messages,
> especially just before the "suspending ICAP service" message shown above.
> 
>      debug_options ALL,1 93,7

Thanks a lot.

As expected, I see Squid connections to C-ICAP starting to time out: 
when the number of errors reach 10, Squid marks squidclamav service as 
"suspended".

No big surprise. Still I don't get any more insight (Is C-ICAP choking? 
Why? What data triggers this?).



Is it a really bad idea to raise icap_connect_timeout?
Same for disabling icap_service_failure_limit?

Other hints?

  bye & Thanks
	av.


From 2bearqloza at gmail.com  Fri Jan 29 17:56:39 2021
From: 2bearqloza at gmail.com (Milos Dodic)
Date: Fri, 29 Jan 2021 18:56:39 +0100
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
 <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
Message-ID: <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>

Alex, thanks for the swift response. Your help is very much appreciated!

Here are the logs, but first to mention, from the server that is going
through the Squid, I am using curl -k (-k to ignore SSL insecure warnings
atm). From the Squid iself, I use squidclient, as using curl from Squid
doesn't do much.

So when I curl the newly uploaded test file from the server that has Squid
as default gateway, the access logs shows:
------------------------------------------------------------------------------------------------------------------
1611941462.501     13 10.10.1.249 NONE/200 0 CONNECT 52.217.88.134:443 -
ORIGINAL_DST/52.217.88.134 -
1611941462.537     22 10.10.1.249 TCP_MISS/200 488 GET
https://s3.amazonaws.com/test.XXXXX.com/testFile - ORIGINAL_DST/
52.217.88.134 binary/octet-stream
------------------------------------------------------------------------------------------------------------------

Cache log is quite long, but won't truncate in order to not omit something
potentially important:
--------------------------------------------------------------------------------------------------------------------------------
2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
connection on FD 30
2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
connection on local=[::]:3130 remote=[::] FD 30 flags=41
2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(2748)
httpsSslBumpAccessCheckDone: sslBump action stareneeded for local=
52.217.88.134:443 remote=10.10.1.249:43538 FD 13 flags=33
2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(3424)
fakeAConnectRequest: fake a CONNECT request to force connState to tunnel
for ssl-bump
2021/01/29 17:31:02.491 kid1| 85,2| client_side_request.cc(753)
clientAccessCheckDone: The request CONNECT 52.217.88.134:443 is ALLOWED;
last ACL checked: allowed_http_sites
2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(729)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(753)
clientAccessCheckDone: The request CONNECT 52.217.88.134:443 is ALLOWED;
last ACL checked: allowed_http_sites
2021/01/29 17:31:02.494 kid1| 17,2| FwdState.cc(142) FwdState: Forwarding
client request local=52.217.88.134:443 remote=10.10.1.249:43538 FD 13
flags=33, url=52.217.88.134:443
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths:
Found sources for '52.217.88.134:443'
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:
  always_direct = DENIED
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:
   never_direct = DENIED
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(310) peerSelectDnsPaths:
   ORIGINAL_DST = local=0.0.0.0 remote=52.217.88.134:443 flags=1
2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:
       timedout = 0
2021/01/29 17:31:02.496 kid1| 83,2| bio.cc(316) readAndParse: parsing error
on FD 15: check failed: state < atHelloReceived
    exception location: Handshake.cc(324) parseHandshakeMessage

2021/01/29 17:31:02.496 kid1| Error parsing SSL Server Hello Message on FD
15
2021/01/29 17:31:02.501 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
52.217.88.134, opcode 3, len 13
2021/01/29 17:31:02.501| 42,2| IcmpPinger.cc(205) Recv:  Pass 52.217.88.134
off to ICMPv4 module.
2021/01/29 17:31:02.501| 42,2| Icmp.cc(95) Log: pingerLog:
1611941462.501640 52.217.88.134                                 32
2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.502| 42,2| IcmpPinger.cc(218) SendResult: return result
to squid. len=7994
2021/01/29 17:31:02.502| 42,2| Icmp.cc(95) Log: pingerLog:
1611941462.502816 52.217.88.134                                 0 Echo
Reply      1ms 6 hops
2021/01/29 17:31:02.514 kid1| 83,2| client_side.cc(2683)
clientNegotiateSSL: New session 0x19d4690 on FD 13 (10.10.1.249:43538)
2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1306) parseHttpRequest:
HTTP Client local=52.217.88.134:443 remote=10.10.1.249:43538 FD 13 flags=33
2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1310) parseHttpRequest:
HTTP Client REQUEST:
---------
GET /test.XXXXX.com/testFile HTTP/1.1
Host: s3.amazonaws.com
User-Agent: curl/7.61.1
Accept: */*


----------
2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
clientAccessCheckDone: The request GET
https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
checked: allowed_http_sites
2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(729)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
clientAccessCheckDone: The request GET
https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
checked: allowed_http_sites
2021/01/29 17:31:02.520 kid1| 17,2| FwdState.cc(142) FwdState: Forwarding
client request local=52.217.88.134:443 remote=10.10.1.249:43538 FD 13
flags=33, url=https://s3.amazonaws.com/test.XXXXX.com/testFile
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
Find IP destination for: https://s3.amazonaws.com/test.XXXXX.com/testFile'
via s3.amazonaws.com
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths:
Found sources for 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:
  always_direct = DENIED
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:
   never_direct = DENIED
2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(312) peerSelectDnsPaths:
         PINNED = local=0.0.0.0 remote=52.216.80.75:443 flags=1
2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(310) peerSelectDnsPaths:
   ORIGINAL_DST = local=0.0.0.0 remote=52.217.88.134:443 flags=1
2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:
       timedout = 0
2021/01/29 17:31:02.521 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
52.216.80.75, opcode 3, len 16
2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(205) Recv:  Pass 52.216.80.75
off to ICMPv4 module.
2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
1611941462.521215 52.216.80.75                                  32
2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2260) sendRequest: HTTP Server
local=10.10.0.135:36120 remote=52.217.88.134:443 FD 15 flags=1
2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2261) sendRequest: HTTP Server
REQUEST:
---------
GET /test.XXXXX.com/testFile HTTP/1.1
User-Agent: curl/7.61.1
Accept: */*
Host: s3.amazonaws.com
Via: 1.1 squid (squid/4.9)
X-Forwarded-For: 10.10.1.249
Cache-Control: max-age=259200
Connection: keep-alive


----------
2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(218) SendResult: return result
to squid. len=7997
2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
1611941462.521561 52.216.80.75                                  0 Echo
Reply      0ms 5 hops
2021/01/29 17:31:02.536 kid1| ctx: enter level  0: '
https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:31:02.536 kid1| 11,2| http.cc(719) processReplyHeader: HTTP
Server local=10.10.0.135:36120 remote=52.217.88.134:443 FD 15 flags=1
2021/01/29 17:31:02.536 kid1| 11,2| http.cc(723) processReplyHeader: HTTP
Server RESPONSE:
---------
HTTP/1.1 200 OK
x-amz-id-2:
hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
x-amz-request-id: CD6D86AAA3FDA43F
Date: Fri, 29 Jan 2021 17:31:03 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3

----------
2021/01/29 17:31:02.536 kid1| ctx: exit level  0
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 88,2| client_side_reply.cc(2061)
processReplyAccessResult: The reply for GET
https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it
matched allowed_http_sites
2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(266) sendStartOfMessage: HTTP
Client local=52.217.88.134:443 remote=10.10.1.249:43538 FD 13 flags=33
2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(267) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 200 OK
x-amz-id-2:
hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
x-amz-request-id: CD6D86AAA3FDA43F
Date: Fri, 29 Jan 2021 17:31:03 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/4.9)
Connection: keep-alive


----------
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 17:31:02.538 kid1| 33,2| client_side.cc(582) swanSong: local=
52.217.88.134:443 remote=10.10.1.249:43538 flags=33
2021/01/29 17:31:02.538 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
--------------------------------------------------------------------------------------------------------------------------------




On the other hand, with squidclient from the Squid itself, access log (the
first run, when nothing is cached for the new test file yet):

------------------------------------------------------------------------------------------------------------------
1611942152.986     29 127.0.0.1 TCP_MISS/200 483 GET
https://s3.amazonaws.com/test.dvabearqloza.com/testFile - HIER_DIRECT/
52.216.226.131 binary/octet-stream
------------------------------------------------------------------------------------------------------------------

And cache log:
------------------------------------------------------------------------------------------------------------------
2021/01/29 17:42:32.956 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
connection on local=[::]:3128 remote=[::] FD 28 flags=9
2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1306) parseHttpRequest:
HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:50584 FD 13 flags=1
2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1310) parseHttpRequest:
HTTP Client REQUEST:
---------
GET https://s3.amazonaws.com/test.XXXXX.com/testFile HTTP/1.0
Host: s3.amazonaws.com
User-Agent: squidclient/4.9
Accept: */*
Connection: close

----------
2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
clientAccessCheckDone: The request GET
https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
checked: allowed_http_sites
2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(729)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
clientAccessCheckDone: The request GET
https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
checked: allowed_http_sites
2021/01/29 17:42:32.957 kid1| 17,2| FwdState.cc(142) FwdState: Forwarding
client request local=127.0.0.1:3128 remote=127.0.0.1:50584 FD 13 flags=1,
url=https://s3.amazonaws.com/test.XXXXX.com/testFile
2021/01/29 17:42:32.957 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
Find IP destination for: https://s3.amazonaws.com/test.XXXXX.com/testFile'
via s3.amazonaws.com
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths:
Found sources for 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(303) peerSelectDnsPaths:
  always_direct = DENIED
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(304) peerSelectDnsPaths:
   never_direct = DENIED
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(308) peerSelectDnsPaths:
         DIRECT = local=0.0.0.0 remote=52.216.226.131:443 flags=1
2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(317) peerSelectDnsPaths:
       timedout = 0
2021/01/29 17:42:32.961 kid1| 83,2| bio.cc(316) readAndParse: parsing error
on FD 15: check failed: state < atHelloReceived
    exception location: Handshake.cc(324) parseHandshakeMessage

2021/01/29 17:42:32.961 kid1| Error parsing SSL Server Hello Message on FD
15
2021/01/29 17:42:32.965 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
52.216.226.131, opcode 3, len 16
2021/01/29 17:42:32.965| 42,2| IcmpPinger.cc(205) Recv:  Pass
52.216.226.131 off to ICMPv4 module.
2021/01/29 17:42:32.965| 42,2| Icmp.cc(95) Log: pingerLog:
1611942152.965403 52.216.226.131                                32
2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2260) sendRequest: HTTP Server
local=10.10.0.135:33004 remote=52.216.226.131:443 FD 15 flags=1
2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2261) sendRequest: HTTP Server
REQUEST:
---------
GET /test.XXXXX.com/testFile HTTP/1.1
User-Agent: squidclient/4.9
Accept: */*
Host: s3.amazonaws.com
Via: 1.0 squid (squid/4.9)
X-Forwarded-For: 127.0.0.1
Cache-Control: max-age=259200
Connection: keep-alive


----------
2021/01/29 17:42:32.966| 42,2| IcmpPinger.cc(218) SendResult: return result
to squid. len=7997
2021/01/29 17:42:32.966| 42,2| Icmp.cc(95) Log: pingerLog:
1611942152.966514 52.216.226.131                                0 Echo
Reply      1ms 6 hops
2021/01/29 17:42:32.985 kid1| ctx: enter level  0: '
https://s3.amazonaws.com/test.XXXXX.com/testFile'
2021/01/29 17:42:32.985 kid1| 11,2| http.cc(719) processReplyHeader: HTTP
Server local=10.10.0.135:33004 remote=52.216.226.131:443 FD 15 flags=1
2021/01/29 17:42:32.985 kid1| 11,2| http.cc(723) processReplyHeader: HTTP
Server RESPONSE:
---------
HTTP/1.1 200 OK
x-amz-id-2:
z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
x-amz-request-id: A6E14CC59FE63894
Date: Fri, 29 Jan 2021 17:42:33 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3

----------
2021/01/29 17:42:32.986 kid1| ctx: exit level  0
2021/01/29 17:42:32.986 kid1| 88,2| client_side_reply.cc(2061)
processReplyAccessResult: The reply for GET
https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it
matched allowed_http_sites
2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(266) sendStartOfMessage: HTTP
Client local=127.0.0.1:3128 remote=127.0.0.1:50584 FD 13 flags=1
2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(267) sendStartOfMessage: HTTP
Client REPLY:
---------
HTTP/1.1 200 OK
x-amz-id-2:
z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
x-amz-request-id: A6E14CC59FE63894
Date: Fri, 29 Jan 2021 17:42:33 GMT
Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
Accept-Ranges: bytes
Content-Type: binary/octet-stream
Content-Length: 8
Server: AmazonS3
X-Cache: MISS from squid
X-Cache-Lookup: MISS from squid:3128
Via: 1.1 squid (squid/4.9)
Connection: close


----------
2021/01/29 17:42:32.986 kid1| 20,2| store_io.cc(43) storeCreate:
storeCreate: Selected dir 0 for e:=sp2V/0x1f582b0*4
2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(891) kick: local=
127.0.0.1:3128 remote=127.0.0.1:50584 flags=1 Connection was closed
2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(582) swanSong: local=
127.0.0.1:3128 remote=127.0.0.1:50584 flags=1
------------------------------------------------------------------------------------------------------------------

The first thing that caught my attention was the line:
"checkCachable: StoreEntry::checkCachable: NO: not cachable", that appears
in the logs when server tries to go through proxy.

Any idea what might be the issue overall?

Thanks again!!!




On Fri, Jan 29, 2021 at 5:40 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/28/21 1:34 PM, Milos Dodic wrote:
>
> > I have noticed that the test server also doesn't cache anything
> > So if I try to go for a file in S3, it says MISS, and after that, MISS
> > again, and I see no new objects in cache being created.
>
> > If I try the same thing from the proxy itself, I get the MISS, and the
> > object gets cached, as it should.
> > When I go back to the test server, and try again, it sees the object in
> > cache and returns TCP_MEM_HIT/200 instead.
>
> Without more details, I can only speculate that the client running on
> the test server sends different HTTP request headers than the client
> running on the proxy itself. You can see the headers in cache.log if you
> set debug_options to ALL,2. If you are not sure whether they are the
> same, please share those logs. They will also contain response headers
> and other potentially useful details.
>
> If the request headers are the same in both tests, then I would
> recommend sharing compressed ALL,7 or ALL,9 debugging logs of both
> successful and unsuccessful tests (four transactions, two logs) for
> analysis. Do not use sensitive data for these tests.
>
>
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>
> Alex.
>
>
>
> > This is the entire config file:
> >
> >
> > visible_hostname squid
> > cache_dir ufs /test/cache/squid 10000 16 256
> >
> > http_access allow localhost
> > http_access alow all
> >
> > http_port 3128
> > http_port 3129 intercept
> > acl allowed_http_sites dstdomain .amazonaws.com <http://amazonaws.com>
> > http_access allow allowed_http_sites
> >
> > https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> > acl SSL_port port 443
> > http_access allow SSL_port
> > acl allowed_https_sites ssl::server_name .amazonaws.com
> > <http://amazonaws.com>
> >
> > ssl_bump stare all
> > ssl_bump bump allowed_https_sites
> > ssl_bump terminate all
>
>
> > On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov wrote:
> >
> >     On 1/26/21 1:54 PM, Milos Dodic wrote:
> >
> >     > when the test server goes for a picture I have stored somewhere in
> >     > the cloud, the squid access log shows "TCP_TUNNEL/200". But when I
> >     > try from the proxy itself with squidclient tool, I get
> >     > "TCP_MEM_HIT/200"
> >
> >
> >     Given the very limited information you have provided, I am guessing
> that
> >
> >     * the primary tests opens a CONNECT tunnel through Squid
> >     * the squidclient test sends a plain text HTTP request to Squid
> >
> >     The final origin server destination may be the same in both tests,
> but
> >     the two transactions are completely different from Squid point of
> view.
> >
> >
> >     > ssl_bump peek step1 all
> >     > ssl_bump peek step2 allowed_https_sites
> >     > ssl_bump splice step3 allowed_https_sites
> >     > ssl_bump terminate step3 all
> >
> >
> >     AFAICT, this configuration is splicing or terminating all TLS
> traffic.
> >     No bumping at all. If you want your Squid to bump TLS tunnels, then
> you
> >     have to have at least one "bump" rule!
> >
> >     I do not know what your overall SslBump needs are, but perhaps you
> meant
> >     something like the following?
> >
> >         acl shouldBeBumped ssl::server_name .amazonaws.com
> >     <http://amazonaws.com>
> >
> >         ssl_bump stare all
> >         ssl_bump bump shouldBeBumped
> >         ssl_bump terminate all
> >
> >     Please do not use the configuration above until you understand what
> it
> >     does. Please see
> https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >     for details.
> >
> >     Depending on your environment, the http_access rules may need to be
> >     adjusted to allow CONNECT requests (to TLS-safe ports) to IP
> addresses
> >     that do not result in .amazonaws.com <http://amazonaws.com> in
> >     reverse DNS lookups.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210129/aaf6d87e/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 29 19:38:25 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 29 Jan 2021 14:38:25 -0500
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
 <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
 <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>
Message-ID: <ff0b198a-672a-541b-95d5-c0283aa6ed45@measurement-factory.com>

On 1/29/21 11:55 AM, Andrea Venturoli wrote:

> I see Squid connections to C-ICAP starting to time out:
> when the number of errors reach 10, Squid marks squidclamav service as
> "suspended".

> No big surprise.

IIRC, you did not disclose timeout suspicions before. This explanation
is news to me, and it eliminates several suspects.


> Still I don't get any more insight (Is C-ICAP choking?
> Why? What data triggers this?).

If you are talking about Squid timing out when attempting to establish a
TCP connection with the ICAP server, then this may by as much insight as
you can get from the Squid side. There is no ICAP "data" at that
connection establishment stage. It is a fairly low-level operation that
Squid and c-icap have little control over. The problem is probably
outside Squid.

I do not know much about c-icap, but I would check whether its
configuration or something like crontab results in hourly restarts and
associated loss of connectivity. The network interface or the routing
tables might also be reset hourly for some reason. The ICAP
server/service might be running out of descriptors or memory.

One potentially useful test is to try to connect to the ICAP server
_while the problem is happening_ using telnet or netcat. When Squid
cannot establish a connection, can you? If the ICAP service is not
running on the Squid box, then try this test both from the Squid box and
from the ICAP box.

Packet captures can tell you whether other Squid-ICAP server connections
were active at the time, whether from-Squid SYN packets were able to
reach the ICAP server, etc.

In other words, basic network troubleshooting steps...


> Is it a really bad idea to raise icap_connect_timeout?

Higher timeout will delay HTTP client transactions for longer periods of
time, of course. If you want to go down the road of finding workarounds,
then check whether raising that timeout actually helps. It is not yet
clear (to me) whether the connections just need more time to be
established or are simply doomed.


> Same for disabling icap_service_failure_limit?

This is an essential ICAP service (icap_service bypass=off). I assume
there is no backup service -- no adaptation_service_set in play here. If
so, disabling the limit means that fewer HTTP transactions will be
inconvenienced in the long run than if the service were to be suspended.
 Hence, fewer ICAP errors will be delivered to Squid clients.

You can also enable bypass.

Fixing the problem would be a much better solution, of course.


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Jan 29 19:57:15 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 29 Jan 2021 14:57:15 -0500
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
 <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
 <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
Message-ID: <22bd979a-131a-cc08-666b-aa927a0df0cc@measurement-factory.com>

On 1/29/21 12:56 PM, Milos Dodic wrote:

> Here are the logs, but first to mention, from the server that is going
> through the Squid, I am using curl -k (-k to ignore SSL insecure
> warnings atm). From the Squid iself, I use squidclient, as using curl
> from Squid doesn't do much.

It is possible that SslBump (your first/"server" test) side effects
somehow disable caching, but I cannot tell that from the logs you have
shared. IIRC, SslBump does not prevent caching by default.

I see nothing special in the HTTP headers. TLS ServerHello parsing
errors look suspicious, but they happen in both tests, and I suspect
that they are unrelated to the transactions we are talking about -- I
could not quickly tell for sure from the logs you shared.


>> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>> StoreEntry::checkCachable: NO: not cachable

If this log entry is about the transaction in question, then it tells us
that Squid marked the response for deletion some time ago. We need to
figure out why Squid is releasing this response. I do not see the answer
in your logs.

Sorry, I cannot offer more help without ALL,9 logs [for the first
transaction in the first/failing test].


HTH,

Alex.



> So when I curl the newly uploaded test file from the server that has
> Squid as default gateway, the access logs shows:
> ------------------------------------------------------------------------------------------------------------------
> 1611941462.501 ? ? 13 10.10.1.249 NONE/200 0 CONNECT 52.217.88.134:443
> <http://52.217.88.134:443> - ORIGINAL_DST/52.217.88.134
> <http://52.217.88.134> -
> 1611941462.537 ? ? 22 10.10.1.249 TCP_MISS/200 488 GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile -
> ORIGINAL_DST/52.217.88.134 <http://52.217.88.134> binary/octet-stream
> ------------------------------------------------------------------------------------------------------------------
> 
> Cache log is quite long, but won't truncate in order to not omit
> something potentially important:
> --------------------------------------------------------------------------------------------------------------------------------
> 2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
> connection on FD 30
> 2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
> connection on local=[::]:3130 remote=[::] FD 30 flags=41
> 2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(2748)
> httpsSslBumpAccessCheckDone: sslBump action stareneeded for
> local=52.217.88.134:443 <http://52.217.88.134:443>
> remote=10.10.1.249:43538 <http://10.10.1.249:43538> FD 13 flags=33
> 2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(3424)
> fakeAConnectRequest: fake a CONNECT request to force connState to tunnel
> for ssl-bump
> 2021/01/29 17:31:02.491 kid1| 85,2| client_side_request.cc(753)
> clientAccessCheckDone: The request CONNECT 52.217.88.134:443
> <http://52.217.88.134:443> is ALLOWED; last ACL checked: allowed_http_sites
> 2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(729)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(753)
> clientAccessCheckDone: The request CONNECT 52.217.88.134:443
> <http://52.217.88.134:443> is ALLOWED; last ACL checked: allowed_http_sites
> 2021/01/29 17:31:02.494 kid1| 17,2| FwdState.cc(142) FwdState:
> Forwarding client request local=52.217.88.134:443
> <http://52.217.88.134:443> remote=10.10.1.249:43538
> <http://10.10.1.249:43538> FD 13 flags=33, url=52.217.88.134:443
> <http://52.217.88.134:443>
> 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(302)
> peerSelectDnsPaths: Found sources for '52.217.88.134:443
> <http://52.217.88.134:443>'
> 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(303)
> peerSelectDnsPaths: ? always_direct = DENIED
> 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(304)
> peerSelectDnsPaths: ? ?never_direct = DENIED
> 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(310)
> peerSelectDnsPaths: ? ?ORIGINAL_DST = local=0.0.0.0
> remote=52.217.88.134:443 <http://52.217.88.134:443> flags=1
> 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(317)
> peerSelectDnsPaths: ? ? ? ?timedout = 0
> 2021/01/29 17:31:02.496 kid1| 83,2| bio.cc(316) readAndParse: parsing
> error on FD 15: check failed: state < atHelloReceived
> ? ? exception location: Handshake.cc(324) parseHandshakeMessage
> 
> 2021/01/29 17:31:02.496 kid1| Error parsing SSL Server Hello Message on
> FD 15
> 2021/01/29 17:31:02.501 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
> 52.217.88.134, opcode 3, len 13
> 2021/01/29 17:31:02.501| 42,2| IcmpPinger.cc(205) Recv: ?Pass
> 52.217.88.134 off to ICMPv4 module.
> 2021/01/29 17:31:02.501| 42,2| Icmp.cc(95) Log: pingerLog:
> 1611941462.501640 52.217.88.134 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 32
> 2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.502| 42,2| IcmpPinger.cc(218) SendResult: return
> result to squid. len=7994
> 2021/01/29 17:31:02.502| 42,2| Icmp.cc(95) Log: pingerLog:
> 1611941462.502816 52.217.88.134 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0 Echo
> Reply ? ? ?1ms 6 hops
> 2021/01/29 17:31:02.514 kid1| 83,2| client_side.cc(2683)
> clientNegotiateSSL: New session 0x19d4690 on FD 13 (10.10.1.249:43538
> <http://10.10.1.249:43538>)
> 2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1306)
> parseHttpRequest: HTTP Client local=52.217.88.134:443
> <http://52.217.88.134:443> remote=10.10.1.249:43538
> <http://10.10.1.249:43538> FD 13 flags=33
> 2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1310)
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile> HTTP/1.1
> Host: s3.amazonaws.com <http://s3.amazonaws.com>
> User-Agent: curl/7.61.1
> Accept: */*
> 
> 
> ----------
> 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
> clientAccessCheckDone: The request GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> checked: allowed_http_sites
> 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(729)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
> clientAccessCheckDone: The request GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> checked: allowed_http_sites
> 2021/01/29 17:31:02.520 kid1| 17,2| FwdState.cc(142) FwdState:
> Forwarding client request local=52.217.88.134:443
> <http://52.217.88.134:443> remote=10.10.1.249:43538
> <http://10.10.1.249:43538> FD 13 flags=33,
> url=https://s3.amazonaws.com/test.XXXXX.com/testFile
> 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(281)
> peerSelectDnsPaths: Find IP destination for:
> https://s3.amazonaws.com/test.XXXXX.com/testFile' via s3.amazonaws.com
> <http://s3.amazonaws.com>
> 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(302)
> peerSelectDnsPaths: Found sources for
> 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(303)
> peerSelectDnsPaths: ? always_direct = DENIED
> 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(304)
> peerSelectDnsPaths: ? ?never_direct = DENIED
> 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(312)
> peerSelectDnsPaths: ? ? ? ? ?PINNED = local=0.0.0.0
> remote=52.216.80.75:443 <http://52.216.80.75:443> flags=1
> 2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(310)
> peerSelectDnsPaths: ? ?ORIGINAL_DST = local=0.0.0.0
> remote=52.217.88.134:443 <http://52.217.88.134:443> flags=1
> 2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(317)
> peerSelectDnsPaths: ? ? ? ?timedout = 0
> 2021/01/29 17:31:02.521 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
> 52.216.80.75, opcode 3, len 16
> 2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(205) Recv: ?Pass
> 52.216.80.75 off to ICMPv4 module.
> 2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
> 1611941462.521215 52.216.80.75 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?32
> 2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2260) sendRequest: HTTP
> Server local=10.10.0.135:36120 <http://10.10.0.135:36120>
> remote=52.217.88.134:443 <http://52.217.88.134:443> FD 15 flags=1
> 2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2261) sendRequest: HTTP
> Server REQUEST:
> ---------
> GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile> HTTP/1.1
> User-Agent: curl/7.61.1
> Accept: */*
> Host: s3.amazonaws.com <http://s3.amazonaws.com>
> Via: 1.1 squid (squid/4.9)
> X-Forwarded-For: 10.10.1.249
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> 2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(218) SendResult: return
> result to squid. len=7997
> 2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
> 1611941462.521561 52.216.80.75 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0 Echo
> Reply ? ? ?0ms 5 hops
> 2021/01/29 17:31:02.536 kid1| ctx: enter level ?0:
> 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> 2021/01/29 17:31:02.536 kid1| 11,2| http.cc(719) processReplyHeader:
> HTTP Server local=10.10.0.135:36120 <http://10.10.0.135:36120>
> remote=52.217.88.134:443 <http://52.217.88.134:443> FD 15 flags=1
> 2021/01/29 17:31:02.536 kid1| 11,2| http.cc(723) processReplyHeader:
> HTTP Server RESPONSE:
> ---------
> HTTP/1.1 200 OK
> x-amz-id-2:
> hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
> x-amz-request-id: CD6D86AAA3FDA43F
> Date: Fri, 29 Jan 2021 17:31:03 GMT
> Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> Accept-Ranges: bytes
> Content-Type: binary/octet-stream
> Content-Length: 8
> Server: AmazonS3
> 
> ----------
> 2021/01/29 17:31:02.536 kid1| ctx: exit level ?0
> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.537 kid1| 88,2| client_side_reply.cc(2061)
> processReplyAccessResult: The reply for GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it
> matched allowed_http_sites
> 2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(266) sendStartOfMessage:
> HTTP Client local=52.217.88.134:443 <http://52.217.88.134:443>
> remote=10.10.1.249:43538 <http://10.10.1.249:43538> FD 13 flags=33
> 2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(267) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 200 OK
> x-amz-id-2:
> hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
> x-amz-request-id: CD6D86AAA3FDA43F
> Date: Fri, 29 Jan 2021 17:31:03 GMT
> Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> Accept-Ranges: bytes
> Content-Type: binary/octet-stream
> Content-Length: 8
> Server: AmazonS3
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/4.9)
> Connection: keep-alive
> 
> 
> ----------
> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2021/01/29 17:31:02.538 kid1| 33,2| client_side.cc(582) swanSong:
> local=52.217.88.134:443 <http://52.217.88.134:443>
> remote=10.10.1.249:43538 <http://10.10.1.249:43538> flags=33
> 2021/01/29 17:31:02.538 kid1| 20,2| store.cc(986) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> --------------------------------------------------------------------------------------------------------------------------------
> 
> 
> 
> 
> On the other hand, with squidclient from the Squid itself, access log
> (the first run, when nothing is cached for the new test file yet):
> 
> ------------------------------------------------------------------------------------------------------------------
> 1611942152.986 ? ? 29 127.0.0.1 TCP_MISS/200 483 GET
> https://s3.amazonaws.com/test.dvabearqloza.com/testFile -
> HIER_DIRECT/52.216.226.131 <http://52.216.226.131> binary/octet-stream
> ------------------------------------------------------------------------------------------------------------------
> 
> And cache log:
> ------------------------------------------------------------------------------------------------------------------
> 2021/01/29 17:42:32.956 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
> connection on local=[::]:3128 remote=[::] FD 28 flags=9
> 2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1306)
> parseHttpRequest: HTTP Client local=127.0.0.1:3128
> <http://127.0.0.1:3128> remote=127.0.0.1:50584 <http://127.0.0.1:50584>
> FD 13 flags=1
> 2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1310)
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET https://s3.amazonaws.com/test.XXXXX.com/testFile HTTP/1.0
> Host: s3.amazonaws.com <http://s3.amazonaws.com>
> User-Agent: squidclient/4.9
> Accept: */*
> Connection: close
> 
> ----------
> 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
> clientAccessCheckDone: The request GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> checked: allowed_http_sites
> 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(729)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
> clientAccessCheckDone: The request GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> checked: allowed_http_sites
> 2021/01/29 17:42:32.957 kid1| 17,2| FwdState.cc(142) FwdState:
> Forwarding client request local=127.0.0.1:3128 <http://127.0.0.1:3128>
> remote=127.0.0.1:50584 <http://127.0.0.1:50584> FD 13 flags=1,
> url=https://s3.amazonaws.com/test.XXXXX.com/testFile
> 2021/01/29 17:42:32.957 kid1| 44,2| peer_select.cc(281)
> peerSelectDnsPaths: Find IP destination for:
> https://s3.amazonaws.com/test.XXXXX.com/testFile' via s3.amazonaws.com
> <http://s3.amazonaws.com>
> 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(302)
> peerSelectDnsPaths: Found sources for
> 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(303)
> peerSelectDnsPaths: ? always_direct = DENIED
> 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(304)
> peerSelectDnsPaths: ? ?never_direct = DENIED
> 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(308)
> peerSelectDnsPaths: ? ? ? ? ?DIRECT = local=0.0.0.0
> remote=52.216.226.131:443 <http://52.216.226.131:443> flags=1
> 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(317)
> peerSelectDnsPaths: ? ? ? ?timedout = 0
> 2021/01/29 17:42:32.961 kid1| 83,2| bio.cc(316) readAndParse: parsing
> error on FD 15: check failed: state < atHelloReceived
> ? ? exception location: Handshake.cc(324) parseHandshakeMessage
> 
> 2021/01/29 17:42:32.961 kid1| Error parsing SSL Server Hello Message on
> FD 15
> 2021/01/29 17:42:32.965 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
> 52.216.226.131, opcode 3, len 16
> 2021/01/29 17:42:32.965| 42,2| IcmpPinger.cc(205) Recv: ?Pass
> 52.216.226.131 off to ICMPv4 module.
> 2021/01/29 17:42:32.965| 42,2| Icmp.cc(95) Log: pingerLog:
> 1611942152.965403 52.216.226.131 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?32
> 2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2260) sendRequest: HTTP
> Server local=10.10.0.135:33004 <http://10.10.0.135:33004>
> remote=52.216.226.131:443 <http://52.216.226.131:443> FD 15 flags=1
> 2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2261) sendRequest: HTTP
> Server REQUEST:
> ---------
> GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile> HTTP/1.1
> User-Agent: squidclient/4.9
> Accept: */*
> Host: s3.amazonaws.com <http://s3.amazonaws.com>
> Via: 1.0 squid (squid/4.9)
> X-Forwarded-For: 127.0.0.1
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> 2021/01/29 17:42:32.966| 42,2| IcmpPinger.cc(218) SendResult: return
> result to squid. len=7997
> 2021/01/29 17:42:32.966| 42,2| Icmp.cc(95) Log: pingerLog:
> 1611942152.966514 52.216.226.131 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0 Echo
> Reply ? ? ?1ms 6 hops
> 2021/01/29 17:42:32.985 kid1| ctx: enter level ?0:
> 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> 2021/01/29 17:42:32.985 kid1| 11,2| http.cc(719) processReplyHeader:
> HTTP Server local=10.10.0.135:33004 <http://10.10.0.135:33004>
> remote=52.216.226.131:443 <http://52.216.226.131:443> FD 15 flags=1
> 2021/01/29 17:42:32.985 kid1| 11,2| http.cc(723) processReplyHeader:
> HTTP Server RESPONSE:
> ---------
> HTTP/1.1 200 OK
> x-amz-id-2:
> z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
> x-amz-request-id: A6E14CC59FE63894
> Date: Fri, 29 Jan 2021 17:42:33 GMT
> Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> Accept-Ranges: bytes
> Content-Type: binary/octet-stream
> Content-Length: 8
> Server: AmazonS3
> 
> ----------
> 2021/01/29 17:42:32.986 kid1| ctx: exit level ?0
> 2021/01/29 17:42:32.986 kid1| 88,2| client_side_reply.cc(2061)
> processReplyAccessResult: The reply for GET
> https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it
> matched allowed_http_sites
> 2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(266) sendStartOfMessage:
> HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> remote=127.0.0.1:50584 <http://127.0.0.1:50584> FD 13 flags=1
> 2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(267) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 200 OK
> x-amz-id-2:
> z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
> x-amz-request-id: A6E14CC59FE63894
> Date: Fri, 29 Jan 2021 17:42:33 GMT
> Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> Accept-Ranges: bytes
> Content-Type: binary/octet-stream
> Content-Length: 8
> Server: AmazonS3
> X-Cache: MISS from squid
> X-Cache-Lookup: MISS from squid:3128
> Via: 1.1 squid (squid/4.9)
> Connection: close
> 
> 
> ----------
> 2021/01/29 17:42:32.986 kid1| 20,2| store_io.cc(43) storeCreate:
> storeCreate: Selected dir 0 for e:=sp2V/0x1f582b0*4
> 2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(891) kick:
> local=127.0.0.1:3128 <http://127.0.0.1:3128> remote=127.0.0.1:50584
> <http://127.0.0.1:50584> flags=1 Connection was closed
> 2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(582) swanSong:
> local=127.0.0.1:3128 <http://127.0.0.1:3128> remote=127.0.0.1:50584
> <http://127.0.0.1:50584> flags=1
> ------------------------------------------------------------------------------------------------------------------
> 
> The first thing that caught my attention was the line:
> "checkCachable: StoreEntry::checkCachable: NO: not cachable", that
> appears in the logs when server tries to go through proxy.
> 
> Any idea what might be the issue overall?
> 
> Thanks again!!!
> 
> 
> 
> 
> On Fri, Jan 29, 2021 at 5:40 PM Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     On 1/28/21 1:34 PM, Milos Dodic wrote:
> 
>     > I have noticed that the test server also doesn't cache anything
>     > So if I try to go for a file in S3, it says MISS, and after that, MISS
>     > again, and I see no new objects in cache being created.
> 
>     > If I try the same thing from the proxy itself, I get the MISS, and the
>     > object gets cached, as it should.
>     > When I go back to the test server, and try again, it sees the
>     object in
>     > cache and returns TCP_MEM_HIT/200 instead.
> 
>     Without more details, I can only speculate that the client running on
>     the test server sends different HTTP request headers than the client
>     running on the proxy itself. You can see the headers in cache.log if you
>     set debug_options to ALL,2. If you are not sure whether they are the
>     same, please share those logs. They will also contain response headers
>     and other potentially useful details.
> 
>     If the request headers are the same in both tests, then I would
>     recommend sharing compressed ALL,7 or ALL,9 debugging logs of both
>     successful and unsuccessful tests (four transactions, two logs) for
>     analysis. Do not use sensitive data for these tests.
> 
>     https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
> 
>     Alex.
> 
> 
> 
>     > This is the entire config file:
>     >
>     >
>     > visible_hostname squid
>     > cache_dir ufs /test/cache/squid 10000 16 256
>     >
>     > http_access allow localhost
>     > http_access alow all
>     >
>     > http_port 3128
>     > http_port 3129 intercept
>     > acl allowed_http_sites dstdomain .amazonaws.com
>     <http://amazonaws.com> <http://amazonaws.com>
>     > http_access allow allowed_http_sites
>     >
>     > https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
>     > acl SSL_port port 443
>     > http_access allow SSL_port
>     > acl allowed_https_sites ssl::server_name .amazonaws.com
>     <http://amazonaws.com>
>     > <http://amazonaws.com>
>     >
>     > ssl_bump stare all
>     > ssl_bump bump allowed_https_sites
>     > ssl_bump terminate all
> 
> 
>     > On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov wrote:
>     >
>     >? ? ?On 1/26/21 1:54 PM, Milos Dodic wrote:
>     >
>     >? ? ?> when the test server goes for a picture I have stored
>     somewhere in
>     >? ? ?> the cloud, the squid access log shows "TCP_TUNNEL/200". But
>     when I
>     >? ? ?> try from the proxy itself with squidclient tool, I get
>     >? ? ?> "TCP_MEM_HIT/200"
>     >
>     >
>     >? ? ?Given the very limited information you have provided, I am
>     guessing that
>     >
>     >? ? ?* the primary tests opens a CONNECT tunnel through Squid
>     >? ? ?* the squidclient test sends a plain text HTTP request to Squid
>     >
>     >? ? ?The final origin server destination may be the same in both
>     tests, but
>     >? ? ?the two transactions are completely different from Squid point
>     of view.
>     >
>     >
>     >? ? ?> ssl_bump peek step1 all
>     >? ? ?> ssl_bump peek step2 allowed_https_sites
>     >? ? ?> ssl_bump splice step3 allowed_https_sites
>     >? ? ?> ssl_bump terminate step3 all
>     >
>     >
>     >? ? ?AFAICT, this configuration is splicing or terminating all TLS
>     traffic.
>     >? ? ?No bumping at all. If you want your Squid to bump TLS tunnels,
>     then you
>     >? ? ?have to have at least one "bump" rule!
>     >
>     >? ? ?I do not know what your overall SslBump needs are, but perhaps
>     you meant
>     >? ? ?something like the following?
>     >
>     >? ? ?? ? acl shouldBeBumped ssl::server_name .amazonaws.com
>     <http://amazonaws.com>
>     >? ? ?<http://amazonaws.com>
>     >
>     >? ? ?? ? ssl_bump stare all
>     >? ? ?? ? ssl_bump bump shouldBeBumped
>     >? ? ?? ? ssl_bump terminate all
>     >
>     >? ? ?Please do not use the configuration above until you understand
>     what it
>     >? ? ?does. Please see
>     https://wiki.squid-cache.org/Features/SslPeekAndSplice
>     >? ? ?for details.
>     >
>     >? ? ?Depending on your environment, the http_access rules may need
>     to be
>     >? ? ?adjusted to allow CONNECT requests (to TLS-safe ports) to IP
>     addresses
>     >? ? ?that do not result in .amazonaws.com <http://amazonaws.com>
>     <http://amazonaws.com> in
>     >? ? ?reverse DNS lookups.
>     >
>     >
>     >? ? ?HTH,
>     >
>     >? ? ?Alex.
>     >
> 



From 2bearqloza at gmail.com  Fri Jan 29 23:06:43 2021
From: 2bearqloza at gmail.com (Milos Dodic)
Date: Sat, 30 Jan 2021 00:06:43 +0100
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <22bd979a-131a-cc08-666b-aa927a0df0cc@measurement-factory.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
 <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
 <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
 <22bd979a-131a-cc08-666b-aa927a0df0cc@measurement-factory.com>
Message-ID: <CAPGQvVQMsMUci9ycHpS8aUOBYSsbAZ+VVS3_ao7iqKP+Rbf8Tg@mail.gmail.com>

Hi Alex,

The log is very long :)
I think I found the important part though, but you might need more than
this.
This is the cache log (ALL,9), that triggered when the server tried to curl
a new file, going through Squid:
---------------------------------------------------------------------------------------------------------------------------------------------

2021/01/29 22:48:53.673 kid1| 20,6| Disks.cc(239) get: none of 1 cache_dirs
have E577E409BDF0410C8077DCAFF4A234F2
2021/01/29 22:48:53.673 kid1| 20,4| Controller.cc(420) peek: cannot locate
E577E409BDF0410C8077DCAFF4A234F2
2021/01/29 22:48:53.673 kid1| 55,9| HttpHeader.cc(956) has: 0x148f118
lookup for Vary[67]
2021/01/29 22:48:53.673 kid1| 20,3| store.cc(456) releaseRequest: 1
e:=p2XIV/0x14a3e80*3
2021/01/29 22:48:53.673 kid1| 11,3| http.cc(982) haveParsedReplyHeaders:
decided: do not cache but share because the entry has been released; HTTP
status 200 e:=p2XIV/0x14a3e80*3

and a few lines later

2021/01/29 22:48:53.674 kid1| 20,2| store.cc(986) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2021/01/29 22:48:53.674 kid1| 20,3| store_swapout.cc(379) mayStartSwapOut:
not cachable
---------------------------------------------------------------------------------------------------------------------------------------------

This part "decided: do not cache but share because the entry has been
released" could be what we are looking for.
If you need more, could you tell me what to parse for exactly, as I don't
think I can paste the whole log :)

And again, thank you very much for helping!


On Fri, Jan 29, 2021 at 8:57 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 1/29/21 12:56 PM, Milos Dodic wrote:
>
> > Here are the logs, but first to mention, from the server that is going
> > through the Squid, I am using curl -k (-k to ignore SSL insecure
> > warnings atm). From the Squid iself, I use squidclient, as using curl
> > from Squid doesn't do much.
>
> It is possible that SslBump (your first/"server" test) side effects
> somehow disable caching, but I cannot tell that from the logs you have
> shared. IIRC, SslBump does not prevent caching by default.
>
> I see nothing special in the HTTP headers. TLS ServerHello parsing
> errors look suspicious, but they happen in both tests, and I suspect
> that they are unrelated to the transactions we are talking about -- I
> could not quickly tell for sure from the logs you shared.
>
>
> >> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> >> StoreEntry::checkCachable: NO: not cachable
>
> If this log entry is about the transaction in question, then it tells us
> that Squid marked the response for deletion some time ago. We need to
> figure out why Squid is releasing this response. I do not see the answer
> in your logs.
>
> Sorry, I cannot offer more help without ALL,9 logs [for the first
> transaction in the first/failing test].
>
>
> HTH,
>
> Alex.
>
>
>
> > So when I curl the newly uploaded test file from the server that has
> > Squid as default gateway, the access logs shows:
> >
> ------------------------------------------------------------------------------------------------------------------
> > 1611941462.501     13 10.10.1.249 NONE/200 0 CONNECT 52.217.88.134:443
> > <http://52.217.88.134:443> - ORIGINAL_DST/52.217.88.134
> > <http://52.217.88.134> -
> > 1611941462.537     22 10.10.1.249 TCP_MISS/200 488 GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile -
> > ORIGINAL_DST/52.217.88.134 <http://52.217.88.134> binary/octet-stream
> >
> ------------------------------------------------------------------------------------------------------------------
> >
> > Cache log is quite long, but won't truncate in order to not omit
> > something potentially important:
> >
> --------------------------------------------------------------------------------------------------------------------------------
> > 2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
> > connection on FD 30
> > 2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
> > connection on local=[::]:3130 remote=[::] FD 30 flags=41
> > 2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(2748)
> > httpsSslBumpAccessCheckDone: sslBump action stareneeded for
> > local=52.217.88.134:443 <http://52.217.88.134:443>
> > remote=10.10.1.249:43538 <http://10.10.1.249:43538> FD 13 flags=33
> > 2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(3424)
> > fakeAConnectRequest: fake a CONNECT request to force connState to tunnel
> > for ssl-bump
> > 2021/01/29 17:31:02.491 kid1| 85,2| client_side_request.cc(753)
> > clientAccessCheckDone: The request CONNECT 52.217.88.134:443
> > <http://52.217.88.134:443> is ALLOWED; last ACL checked:
> allowed_http_sites
> > 2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(729)
> > clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> > 2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(753)
> > clientAccessCheckDone: The request CONNECT 52.217.88.134:443
> > <http://52.217.88.134:443> is ALLOWED; last ACL checked:
> allowed_http_sites
> > 2021/01/29 17:31:02.494 kid1| 17,2| FwdState.cc(142) FwdState:
> > Forwarding client request local=52.217.88.134:443
> > <http://52.217.88.134:443> remote=10.10.1.249:43538
> > <http://10.10.1.249:43538> FD 13 flags=33, url=52.217.88.134:443
> > <http://52.217.88.134:443>
> > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(302)
> > peerSelectDnsPaths: Found sources for '52.217.88.134:443
> > <http://52.217.88.134:443>'
> > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(303)
> > peerSelectDnsPaths:   always_direct = DENIED
> > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(304)
> > peerSelectDnsPaths:    never_direct = DENIED
> > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(310)
> > peerSelectDnsPaths:    ORIGINAL_DST = local=0.0.0.0
> > remote=52.217.88.134:443 <http://52.217.88.134:443> flags=1
> > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(317)
> > peerSelectDnsPaths:        timedout = 0
> > 2021/01/29 17:31:02.496 kid1| 83,2| bio.cc(316) readAndParse: parsing
> > error on FD 15: check failed: state < atHelloReceived
> >     exception location: Handshake.cc(324) parseHandshakeMessage
> >
> > 2021/01/29 17:31:02.496 kid1| Error parsing SSL Server Hello Message on
> > FD 15
> > 2021/01/29 17:31:02.501 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
> > 52.217.88.134, opcode 3, len 13
> > 2021/01/29 17:31:02.501| 42,2| IcmpPinger.cc(205) Recv:  Pass
> > 52.217.88.134 off to ICMPv4 module.
> > 2021/01/29 17:31:02.501| 42,2| Icmp.cc(95) Log: pingerLog:
> > 1611941462.501640 52.217.88.134                                 32
> > 2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.502| 42,2| IcmpPinger.cc(218) SendResult: return
> > result to squid. len=7994
> > 2021/01/29 17:31:02.502| 42,2| Icmp.cc(95) Log: pingerLog:
> > 1611941462.502816 52.217.88.134                                 0 Echo
> > Reply      1ms 6 hops
> > 2021/01/29 17:31:02.514 kid1| 83,2| client_side.cc(2683)
> > clientNegotiateSSL: New session 0x19d4690 on FD 13 (10.10.1.249:43538
> > <http://10.10.1.249:43538>)
> > 2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1306)
> > parseHttpRequest: HTTP Client local=52.217.88.134:443
> > <http://52.217.88.134:443> remote=10.10.1.249:43538
> > <http://10.10.1.249:43538> FD 13 flags=33
> > 2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1310)
> > parseHttpRequest: HTTP Client REQUEST:
> > ---------
> > GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile> HTTP/1.1
> > Host: s3.amazonaws.com <http://s3.amazonaws.com>
> > User-Agent: curl/7.61.1
> > Accept: */*
> >
> >
> > ----------
> > 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
> > clientAccessCheckDone: The request GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> > checked: allowed_http_sites
> > 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(729)
> > clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> > 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
> > clientAccessCheckDone: The request GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> > checked: allowed_http_sites
> > 2021/01/29 17:31:02.520 kid1| 17,2| FwdState.cc(142) FwdState:
> > Forwarding client request local=52.217.88.134:443
> > <http://52.217.88.134:443> remote=10.10.1.249:43538
> > <http://10.10.1.249:43538> FD 13 flags=33,
> > url=https://s3.amazonaws.com/test.XXXXX.com/testFile
> > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(281)
> > peerSelectDnsPaths: Find IP destination for:
> > https://s3.amazonaws.com/test.XXXXX.com/testFile' via s3.amazonaws.com
> > <http://s3.amazonaws.com>
> > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(302)
> > peerSelectDnsPaths: Found sources for
> > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(303)
> > peerSelectDnsPaths:   always_direct = DENIED
> > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(304)
> > peerSelectDnsPaths:    never_direct = DENIED
> > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(312)
> > peerSelectDnsPaths:          PINNED = local=0.0.0.0
> > remote=52.216.80.75:443 <http://52.216.80.75:443> flags=1
> > 2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(310)
> > peerSelectDnsPaths:    ORIGINAL_DST = local=0.0.0.0
> > remote=52.217.88.134:443 <http://52.217.88.134:443> flags=1
> > 2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(317)
> > peerSelectDnsPaths:        timedout = 0
> > 2021/01/29 17:31:02.521 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
> > 52.216.80.75, opcode 3, len 16
> > 2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(205) Recv:  Pass
> > 52.216.80.75 off to ICMPv4 module.
> > 2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
> > 1611941462.521215 52.216.80.75                                  32
> > 2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2260) sendRequest: HTTP
> > Server local=10.10.0.135:36120 <http://10.10.0.135:36120>
> > remote=52.217.88.134:443 <http://52.217.88.134:443> FD 15 flags=1
> > 2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2261) sendRequest: HTTP
> > Server REQUEST:
> > ---------
> > GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile> HTTP/1.1
> > User-Agent: curl/7.61.1
> > Accept: */*
> > Host: s3.amazonaws.com <http://s3.amazonaws.com>
> > Via: 1.1 squid (squid/4.9)
> > X-Forwarded-For: 10.10.1.249
> > Cache-Control: max-age=259200
> > Connection: keep-alive
> >
> >
> > ----------
> > 2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(218) SendResult: return
> > result to squid. len=7997
> > 2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
> > 1611941462.521561 52.216.80.75                                  0 Echo
> > Reply      0ms 5 hops
> > 2021/01/29 17:31:02.536 kid1| ctx: enter level  0:
> > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> > 2021/01/29 17:31:02.536 kid1| 11,2| http.cc(719) processReplyHeader:
> > HTTP Server local=10.10.0.135:36120 <http://10.10.0.135:36120>
> > remote=52.217.88.134:443 <http://52.217.88.134:443> FD 15 flags=1
> > 2021/01/29 17:31:02.536 kid1| 11,2| http.cc(723) processReplyHeader:
> > HTTP Server RESPONSE:
> > ---------
> > HTTP/1.1 200 OK
> > x-amz-id-2:
> >
> hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
> > x-amz-request-id: CD6D86AAA3FDA43F
> > Date: Fri, 29 Jan 2021 17:31:03 GMT
> > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> > Accept-Ranges: bytes
> > Content-Type: binary/octet-stream
> > Content-Length: 8
> > Server: AmazonS3
> >
> > ----------
> > 2021/01/29 17:31:02.536 kid1| ctx: exit level  0
> > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.537 kid1| 88,2| client_side_reply.cc(2061)
> > processReplyAccessResult: The reply for GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it
> > matched allowed_http_sites
> > 2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(266) sendStartOfMessage:
> > HTTP Client local=52.217.88.134:443 <http://52.217.88.134:443>
> > remote=10.10.1.249:43538 <http://10.10.1.249:43538> FD 13 flags=33
> > 2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(267) sendStartOfMessage:
> > HTTP Client REPLY:
> > ---------
> > HTTP/1.1 200 OK
> > x-amz-id-2:
> >
> hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
> > x-amz-request-id: CD6D86AAA3FDA43F
> > Date: Fri, 29 Jan 2021 17:31:03 GMT
> > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> > Accept-Ranges: bytes
> > Content-Type: binary/octet-stream
> > Content-Length: 8
> > Server: AmazonS3
> > X-Cache: MISS from squid
> > X-Cache-Lookup: MISS from squid:3128
> > Via: 1.1 squid (squid/4.9)
> > Connection: keep-alive
> >
> >
> > ----------
> > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2021/01/29 17:31:02.538 kid1| 33,2| client_side.cc(582) swanSong:
> > local=52.217.88.134:443 <http://52.217.88.134:443>
> > remote=10.10.1.249:43538 <http://10.10.1.249:43538> flags=33
> > 2021/01/29 17:31:02.538 kid1| 20,2| store.cc(986) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> >
> --------------------------------------------------------------------------------------------------------------------------------
> >
> >
> >
> >
> > On the other hand, with squidclient from the Squid itself, access log
> > (the first run, when nothing is cached for the new test file yet):
> >
> >
> ------------------------------------------------------------------------------------------------------------------
> > 1611942152.986     29 127.0.0.1 TCP_MISS/200 483 GET
> > https://s3.amazonaws.com/test.dvabearqloza.com/testFile -
> > HIER_DIRECT/52.216.226.131 <http://52.216.226.131> binary/octet-stream
> >
> ------------------------------------------------------------------------------------------------------------------
> >
> > And cache log:
> >
> ------------------------------------------------------------------------------------------------------------------
> > 2021/01/29 17:42:32.956 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
> > connection on local=[::]:3128 remote=[::] FD 28 flags=9
> > 2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1306)
> > parseHttpRequest: HTTP Client local=127.0.0.1:3128
> > <http://127.0.0.1:3128> remote=127.0.0.1:50584 <http://127.0.0.1:50584>
> > FD 13 flags=1
> > 2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1310)
> > parseHttpRequest: HTTP Client REQUEST:
> > ---------
> > GET https://s3.amazonaws.com/test.XXXXX.com/testFile HTTP/1.0
> > Host: s3.amazonaws.com <http://s3.amazonaws.com>
> > User-Agent: squidclient/4.9
> > Accept: */*
> > Connection: close
> >
> > ----------
> > 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
> > clientAccessCheckDone: The request GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> > checked: allowed_http_sites
> > 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(729)
> > clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> > 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
> > clientAccessCheckDone: The request GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
> > checked: allowed_http_sites
> > 2021/01/29 17:42:32.957 kid1| 17,2| FwdState.cc(142) FwdState:
> > Forwarding client request local=127.0.0.1:3128 <http://127.0.0.1:3128>
> > remote=127.0.0.1:50584 <http://127.0.0.1:50584> FD 13 flags=1,
> > url=https://s3.amazonaws.com/test.XXXXX.com/testFile
> > 2021/01/29 17:42:32.957 kid1| 44,2| peer_select.cc(281)
> > peerSelectDnsPaths: Find IP destination for:
> > https://s3.amazonaws.com/test.XXXXX.com/testFile' via s3.amazonaws.com
> > <http://s3.amazonaws.com>
> > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(302)
> > peerSelectDnsPaths: Found sources for
> > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(303)
> > peerSelectDnsPaths:   always_direct = DENIED
> > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(304)
> > peerSelectDnsPaths:    never_direct = DENIED
> > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(308)
> > peerSelectDnsPaths:          DIRECT = local=0.0.0.0
> > remote=52.216.226.131:443 <http://52.216.226.131:443> flags=1
> > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(317)
> > peerSelectDnsPaths:        timedout = 0
> > 2021/01/29 17:42:32.961 kid1| 83,2| bio.cc(316) readAndParse: parsing
> > error on FD 15: check failed: state < atHelloReceived
> >     exception location: Handshake.cc(324) parseHandshakeMessage
> >
> > 2021/01/29 17:42:32.961 kid1| Error parsing SSL Server Hello Message on
> > FD 15
> > 2021/01/29 17:42:32.965 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
> > 52.216.226.131, opcode 3, len 16
> > 2021/01/29 17:42:32.965| 42,2| IcmpPinger.cc(205) Recv:  Pass
> > 52.216.226.131 off to ICMPv4 module.
> > 2021/01/29 17:42:32.965| 42,2| Icmp.cc(95) Log: pingerLog:
> > 1611942152.965403 52.216.226.131                                32
> > 2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2260) sendRequest: HTTP
> > Server local=10.10.0.135:33004 <http://10.10.0.135:33004>
> > remote=52.216.226.131:443 <http://52.216.226.131:443> FD 15 flags=1
> > 2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2261) sendRequest: HTTP
> > Server REQUEST:
> > ---------
> > GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile> HTTP/1.1
> > User-Agent: squidclient/4.9
> > Accept: */*
> > Host: s3.amazonaws.com <http://s3.amazonaws.com>
> > Via: 1.0 squid (squid/4.9)
> > X-Forwarded-For: 127.0.0.1
> > Cache-Control: max-age=259200
> > Connection: keep-alive
> >
> >
> > ----------
> > 2021/01/29 17:42:32.966| 42,2| IcmpPinger.cc(218) SendResult: return
> > result to squid. len=7997
> > 2021/01/29 17:42:32.966| 42,2| Icmp.cc(95) Log: pingerLog:
> > 1611942152.966514 52.216.226.131                                0 Echo
> > Reply      1ms 6 hops
> > 2021/01/29 17:42:32.985 kid1| ctx: enter level  0:
> > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
> > 2021/01/29 17:42:32.985 kid1| 11,2| http.cc(719) processReplyHeader:
> > HTTP Server local=10.10.0.135:33004 <http://10.10.0.135:33004>
> > remote=52.216.226.131:443 <http://52.216.226.131:443> FD 15 flags=1
> > 2021/01/29 17:42:32.985 kid1| 11,2| http.cc(723) processReplyHeader:
> > HTTP Server RESPONSE:
> > ---------
> > HTTP/1.1 200 OK
> > x-amz-id-2:
> >
> z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
> > x-amz-request-id: A6E14CC59FE63894
> > Date: Fri, 29 Jan 2021 17:42:33 GMT
> > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> > Accept-Ranges: bytes
> > Content-Type: binary/octet-stream
> > Content-Length: 8
> > Server: AmazonS3
> >
> > ----------
> > 2021/01/29 17:42:32.986 kid1| ctx: exit level  0
> > 2021/01/29 17:42:32.986 kid1| 88,2| client_side_reply.cc(2061)
> > processReplyAccessResult: The reply for GET
> > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED, because it
> > matched allowed_http_sites
> > 2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(266) sendStartOfMessage:
> > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> > remote=127.0.0.1:50584 <http://127.0.0.1:50584> FD 13 flags=1
> > 2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(267) sendStartOfMessage:
> > HTTP Client REPLY:
> > ---------
> > HTTP/1.1 200 OK
> > x-amz-id-2:
> >
> z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
> > x-amz-request-id: A6E14CC59FE63894
> > Date: Fri, 29 Jan 2021 17:42:33 GMT
> > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
> > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
> > Accept-Ranges: bytes
> > Content-Type: binary/octet-stream
> > Content-Length: 8
> > Server: AmazonS3
> > X-Cache: MISS from squid
> > X-Cache-Lookup: MISS from squid:3128
> > Via: 1.1 squid (squid/4.9)
> > Connection: close
> >
> >
> > ----------
> > 2021/01/29 17:42:32.986 kid1| 20,2| store_io.cc(43) storeCreate:
> > storeCreate: Selected dir 0 for e:=sp2V/0x1f582b0*4
> > 2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(891) kick:
> > local=127.0.0.1:3128 <http://127.0.0.1:3128> remote=127.0.0.1:50584
> > <http://127.0.0.1:50584> flags=1 Connection was closed
> > 2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(582) swanSong:
> > local=127.0.0.1:3128 <http://127.0.0.1:3128> remote=127.0.0.1:50584
> > <http://127.0.0.1:50584> flags=1
> >
> ------------------------------------------------------------------------------------------------------------------
> >
> > The first thing that caught my attention was the line:
> > "checkCachable: StoreEntry::checkCachable: NO: not cachable", that
> > appears in the logs when server tries to go through proxy.
> >
> > Any idea what might be the issue overall?
> >
> > Thanks again!!!
> >
> >
> >
> >
> > On Fri, Jan 29, 2021 at 5:40 PM Alex Rousskov
> > <rousskov at measurement-factory.com
> > <mailto:rousskov at measurement-factory.com>> wrote:
> >
> >     On 1/28/21 1:34 PM, Milos Dodic wrote:
> >
> >     > I have noticed that the test server also doesn't cache anything
> >     > So if I try to go for a file in S3, it says MISS, and after that,
> MISS
> >     > again, and I see no new objects in cache being created.
> >
> >     > If I try the same thing from the proxy itself, I get the MISS, and
> the
> >     > object gets cached, as it should.
> >     > When I go back to the test server, and try again, it sees the
> >     object in
> >     > cache and returns TCP_MEM_HIT/200 instead.
> >
> >     Without more details, I can only speculate that the client running on
> >     the test server sends different HTTP request headers than the client
> >     running on the proxy itself. You can see the headers in cache.log if
> you
> >     set debug_options to ALL,2. If you are not sure whether they are the
> >     same, please share those logs. They will also contain response
> headers
> >     and other potentially useful details.
> >
> >     If the request headers are the same in both tests, then I would
> >     recommend sharing compressed ALL,7 or ALL,9 debugging logs of both
> >     successful and unsuccessful tests (four transactions, two logs) for
> >     analysis. Do not use sensitive data for these tests.
> >
> >
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
> >
> >     Alex.
> >
> >
> >
> >     > This is the entire config file:
> >     >
> >     >
> >     > visible_hostname squid
> >     > cache_dir ufs /test/cache/squid 10000 16 256
> >     >
> >     > http_access allow localhost
> >     > http_access alow all
> >     >
> >     > http_port 3128
> >     > http_port 3129 intercept
> >     > acl allowed_http_sites dstdomain .amazonaws.com
> >     <http://amazonaws.com> <http://amazonaws.com>
> >     > http_access allow allowed_http_sites
> >     >
> >     > https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> >     > acl SSL_port port 443
> >     > http_access allow SSL_port
> >     > acl allowed_https_sites ssl::server_name .amazonaws.com
> >     <http://amazonaws.com>
> >     > <http://amazonaws.com>
> >     >
> >     > ssl_bump stare all
> >     > ssl_bump bump allowed_https_sites
> >     > ssl_bump terminate all
> >
> >
> >     > On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov wrote:
> >     >
> >     >     On 1/26/21 1:54 PM, Milos Dodic wrote:
> >     >
> >     >     > when the test server goes for a picture I have stored
> >     somewhere in
> >     >     > the cloud, the squid access log shows "TCP_TUNNEL/200". But
> >     when I
> >     >     > try from the proxy itself with squidclient tool, I get
> >     >     > "TCP_MEM_HIT/200"
> >     >
> >     >
> >     >     Given the very limited information you have provided, I am
> >     guessing that
> >     >
> >     >     * the primary tests opens a CONNECT tunnel through Squid
> >     >     * the squidclient test sends a plain text HTTP request to Squid
> >     >
> >     >     The final origin server destination may be the same in both
> >     tests, but
> >     >     the two transactions are completely different from Squid point
> >     of view.
> >     >
> >     >
> >     >     > ssl_bump peek step1 all
> >     >     > ssl_bump peek step2 allowed_https_sites
> >     >     > ssl_bump splice step3 allowed_https_sites
> >     >     > ssl_bump terminate step3 all
> >     >
> >     >
> >     >     AFAICT, this configuration is splicing or terminating all TLS
> >     traffic.
> >     >     No bumping at all. If you want your Squid to bump TLS tunnels,
> >     then you
> >     >     have to have at least one "bump" rule!
> >     >
> >     >     I do not know what your overall SslBump needs are, but perhaps
> >     you meant
> >     >     something like the following?
> >     >
> >     >         acl shouldBeBumped ssl::server_name .amazonaws.com
> >     <http://amazonaws.com>
> >     >     <http://amazonaws.com>
> >     >
> >     >         ssl_bump stare all
> >     >         ssl_bump bump shouldBeBumped
> >     >         ssl_bump terminate all
> >     >
> >     >     Please do not use the configuration above until you understand
> >     what it
> >     >     does. Please see
> >     https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >     >     for details.
> >     >
> >     >     Depending on your environment, the http_access rules may need
> >     to be
> >     >     adjusted to allow CONNECT requests (to TLS-safe ports) to IP
> >     addresses
> >     >     that do not result in .amazonaws.com <http://amazonaws.com>
> >     <http://amazonaws.com> in
> >     >     reverse DNS lookups.
> >     >
> >     >
> >     >     HTH,
> >     >
> >     >     Alex.
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210130/a16ba29e/attachment.htm>

From rousskov at measurement-factory.com  Sat Jan 30 03:22:11 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 29 Jan 2021 22:22:11 -0500
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <CAPGQvVQMsMUci9ycHpS8aUOBYSsbAZ+VVS3_ao7iqKP+Rbf8Tg@mail.gmail.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
 <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
 <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
 <22bd979a-131a-cc08-666b-aa927a0df0cc@measurement-factory.com>
 <CAPGQvVQMsMUci9ycHpS8aUOBYSsbAZ+VVS3_ao7iqKP+Rbf8Tg@mail.gmail.com>
Message-ID: <eb45d98e-1e74-72f2-187b-37f66248cc8b@measurement-factory.com>

On 1/29/21 6:06 PM, Milos Dodic wrote:

> The log is very long :)
> I think I found the important part though, but you might need more than
> this.

No, the snippets you found show an already released cache entry. We
still do not know why it was released earlier.


> If you need more, could you tell me what to parse for exactly, as I
> don't think I can paste the whole log :)

Unfortunately, I cannot guess the exact lines to look for -- the true
answer may be hidden in many places, and the meaning of many messages
depends on their surrounding context. IMO, these low-level debugging
logs are only suitable for analysis by capable Squid developers.

To save time and effort, I always recommend sharing a link to the
compressed log instead. In the past, folks have shared links to DropBox,
GitHub gists, Google Drive, and similar temporary upload areas. As
always, do not use sensitive data in shared tests and test configurations!


Thank you,

Alex.

> On Fri, Jan 29, 2021 at 8:57 PM Alex Rousskov wrote:
> 
>     On 1/29/21 12:56 PM, Milos Dodic wrote:
> 
>     > Here are the logs, but first to mention, from the server that is going
>     > through the Squid, I am using curl -k (-k to ignore SSL insecure
>     > warnings atm). From the Squid iself, I use squidclient, as using curl
>     > from Squid doesn't do much.
> 
>     It is possible that SslBump (your first/"server" test) side effects
>     somehow disable caching, but I cannot tell that from the logs you have
>     shared. IIRC, SslBump does not prevent caching by default.
> 
>     I see nothing special in the HTTP headers. TLS ServerHello parsing
>     errors look suspicious, but they happen in both tests, and I suspect
>     that they are unrelated to the transactions we are talking about -- I
>     could not quickly tell for sure from the logs you shared.
> 
> 
>     >> 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     >> StoreEntry::checkCachable: NO: not cachable
> 
>     If this log entry is about the transaction in question, then it tells us
>     that Squid marked the response for deletion some time ago. We need to
>     figure out why Squid is releasing this response. I do not see the answer
>     in your logs.
> 
>     Sorry, I cannot offer more help without ALL,9 logs [for the first
>     transaction in the first/failing test].
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
>     > So when I curl the newly uploaded test file from the server that has
>     > Squid as default gateway, the access logs shows:
>     >
>     ------------------------------------------------------------------------------------------------------------------
>     > 1611941462.501 ? ? 13 10.10.1.249 NONE/200 0 CONNECT
>     52.217.88.134:443 <http://52.217.88.134:443>
>     > <http://52.217.88.134:443> - ORIGINAL_DST/52.217.88.134
>     <http://52.217.88.134>
>     > <http://52.217.88.134> -
>     > 1611941462.537 ? ? 22 10.10.1.249 TCP_MISS/200 488 GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile -
>     > ORIGINAL_DST/52.217.88.134 <http://52.217.88.134>
>     <http://52.217.88.134> binary/octet-stream
>     >
>     ------------------------------------------------------------------------------------------------------------------
>     >
>     > Cache log is quite long, but won't truncate in order to not omit
>     > something potentially important:
>     >
>     --------------------------------------------------------------------------------------------------------------------------------
>     > 2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New
>     > connection on FD 30
>     > 2021/01/29 17:31:02.488 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
>     > connection on local=[::]:3130 remote=[::] FD 30 flags=41
>     > 2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(2748)
>     > httpsSslBumpAccessCheckDone: sslBump action stareneeded for
>     > local=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443>
>     > remote=10.10.1.249:43538 <http://10.10.1.249:43538>
>     <http://10.10.1.249:43538> FD 13 flags=33
>     > 2021/01/29 17:31:02.488 kid1| 33,2| client_side.cc(3424)
>     > fakeAConnectRequest: fake a CONNECT request to force connState to
>     tunnel
>     > for ssl-bump
>     > 2021/01/29 17:31:02.491 kid1| 85,2| client_side_request.cc(753)
>     > clientAccessCheckDone: The request CONNECT 52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443> is ALLOWED; last ACL checked:
>     allowed_http_sites
>     > 2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(729)
>     > clientAccessCheck2: No adapted_http_access configuration. default:
>     ALLOW
>     > 2021/01/29 17:31:02.492 kid1| 85,2| client_side_request.cc(753)
>     > clientAccessCheckDone: The request CONNECT 52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443> is ALLOWED; last ACL checked:
>     allowed_http_sites
>     > 2021/01/29 17:31:02.494 kid1| 17,2| FwdState.cc(142) FwdState:
>     > Forwarding client request local=52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443> remote=10.10.1.249:43538
>     <http://10.10.1.249:43538>
>     > <http://10.10.1.249:43538> FD 13 flags=33, url=52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443>
>     > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(302)
>     > peerSelectDnsPaths: Found sources for '52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443>'
>     > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(303)
>     > peerSelectDnsPaths: ? always_direct = DENIED
>     > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(304)
>     > peerSelectDnsPaths: ? ?never_direct = DENIED
>     > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(310)
>     > peerSelectDnsPaths: ? ?ORIGINAL_DST = local=0.0.0.0
>     > remote=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443> flags=1
>     > 2021/01/29 17:31:02.494 kid1| 44,2| peer_select.cc(317)
>     > peerSelectDnsPaths: ? ? ? ?timedout = 0
>     > 2021/01/29 17:31:02.496 kid1| 83,2| bio.cc(316) readAndParse: parsing
>     > error on FD 15: check failed: state < atHelloReceived
>     > ? ? exception location: Handshake.cc(324) parseHandshakeMessage
>     >
>     > 2021/01/29 17:31:02.496 kid1| Error parsing SSL Server Hello
>     Message on
>     > FD 15
>     > 2021/01/29 17:31:02.501 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
>     > 52.217.88.134, opcode 3, len 13
>     > 2021/01/29 17:31:02.501| 42,2| IcmpPinger.cc(205) Recv: ?Pass
>     > 52.217.88.134 off to ICMPv4 module.
>     > 2021/01/29 17:31:02.501| 42,2| Icmp.cc(95) Log: pingerLog:
>     > 1611941462.501640 52.217.88.134 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 32
>     > 2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.501 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.502| 42,2| IcmpPinger.cc(218) SendResult: return
>     > result to squid. len=7994
>     > 2021/01/29 17:31:02.502| 42,2| Icmp.cc(95) Log: pingerLog:
>     > 1611941462.502816 52.217.88.134 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0 Echo
>     > Reply ? ? ?1ms 6 hops
>     > 2021/01/29 17:31:02.514 kid1| 83,2| client_side.cc(2683)
>     > clientNegotiateSSL: New session 0x19d4690 on FD 13
>     (10.10.1.249:43538 <http://10.10.1.249:43538>
>     > <http://10.10.1.249:43538>)
>     > 2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1306)
>     > parseHttpRequest: HTTP Client local=52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443> remote=10.10.1.249:43538
>     <http://10.10.1.249:43538>
>     > <http://10.10.1.249:43538> FD 13 flags=33
>     > 2021/01/29 17:31:02.515 kid1| 11,2| client_side.cc(1310)
>     > parseHttpRequest: HTTP Client REQUEST:
>     > ---------
>     > GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile>
>     <http://test.XXXXX.com/testFile> HTTP/1.1
>     > Host: s3.amazonaws.com <http://s3.amazonaws.com>
>     <http://s3.amazonaws.com>
>     > User-Agent: curl/7.61.1
>     > Accept: */*
>     >
>     >
>     > ----------
>     > 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
>     > clientAccessCheckDone: The request GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
>     > checked: allowed_http_sites
>     > 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(729)
>     > clientAccessCheck2: No adapted_http_access configuration. default:
>     ALLOW
>     > 2021/01/29 17:31:02.520 kid1| 85,2| client_side_request.cc(753)
>     > clientAccessCheckDone: The request GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
>     > checked: allowed_http_sites
>     > 2021/01/29 17:31:02.520 kid1| 17,2| FwdState.cc(142) FwdState:
>     > Forwarding client request local=52.217.88.134:443
>     <http://52.217.88.134:443>
>     > <http://52.217.88.134:443> remote=10.10.1.249:43538
>     <http://10.10.1.249:43538>
>     > <http://10.10.1.249:43538> FD 13 flags=33,
>     > url=https://s3.amazonaws.com/test.XXXXX.com/testFile
>     > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(281)
>     > peerSelectDnsPaths: Find IP destination for:
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile' via
>     s3.amazonaws.com <http://s3.amazonaws.com>
>     > <http://s3.amazonaws.com>
>     > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(302)
>     > peerSelectDnsPaths: Found sources for
>     > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
>     > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(303)
>     > peerSelectDnsPaths: ? always_direct = DENIED
>     > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(304)
>     > peerSelectDnsPaths: ? ?never_direct = DENIED
>     > 2021/01/29 17:31:02.520 kid1| 44,2| peer_select.cc(312)
>     > peerSelectDnsPaths: ? ? ? ? ?PINNED = local=0.0.0.0
>     > remote=52.216.80.75:443 <http://52.216.80.75:443>
>     <http://52.216.80.75:443> flags=1
>     > 2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(310)
>     > peerSelectDnsPaths: ? ?ORIGINAL_DST = local=0.0.0.0
>     > remote=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443> flags=1
>     > 2021/01/29 17:31:02.521 kid1| 44,2| peer_select.cc(317)
>     > peerSelectDnsPaths: ? ? ? ?timedout = 0
>     > 2021/01/29 17:31:02.521 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
>     > 52.216.80.75, opcode 3, len 16
>     > 2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(205) Recv: ?Pass
>     > 52.216.80.75 off to ICMPv4 module.
>     > 2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
>     > 1611941462.521215 52.216.80.75 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?32
>     > 2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2260) sendRequest: HTTP
>     > Server local=10.10.0.135:36120 <http://10.10.0.135:36120>
>     <http://10.10.0.135:36120>
>     > remote=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443> FD 15 flags=1
>     > 2021/01/29 17:31:02.521 kid1| 11,2| http.cc(2261) sendRequest: HTTP
>     > Server REQUEST:
>     > ---------
>     > GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile>
>     <http://test.XXXXX.com/testFile> HTTP/1.1
>     > User-Agent: curl/7.61.1
>     > Accept: */*
>     > Host: s3.amazonaws.com <http://s3.amazonaws.com>
>     <http://s3.amazonaws.com>
>     > Via: 1.1 squid (squid/4.9)
>     > X-Forwarded-For: 10.10.1.249
>     > Cache-Control: max-age=259200
>     > Connection: keep-alive
>     >
>     >
>     > ----------
>     > 2021/01/29 17:31:02.521| 42,2| IcmpPinger.cc(218) SendResult: return
>     > result to squid. len=7997
>     > 2021/01/29 17:31:02.521| 42,2| Icmp.cc(95) Log: pingerLog:
>     > 1611941462.521561 52.216.80.75 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0 Echo
>     > Reply ? ? ?0ms 5 hops
>     > 2021/01/29 17:31:02.536 kid1| ctx: enter level ?0:
>     > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
>     > 2021/01/29 17:31:02.536 kid1| 11,2| http.cc(719) processReplyHeader:
>     > HTTP Server local=10.10.0.135:36120 <http://10.10.0.135:36120>
>     <http://10.10.0.135:36120>
>     > remote=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443> FD 15 flags=1
>     > 2021/01/29 17:31:02.536 kid1| 11,2| http.cc(723) processReplyHeader:
>     > HTTP Server RESPONSE:
>     > ---------
>     > HTTP/1.1 200 OK
>     > x-amz-id-2:
>     >
>     hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
>     > x-amz-request-id: CD6D86AAA3FDA43F
>     > Date: Fri, 29 Jan 2021 17:31:03 GMT
>     > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
>     > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
>     > Accept-Ranges: bytes
>     > Content-Type: binary/octet-stream
>     > Content-Length: 8
>     > Server: AmazonS3
>     >
>     > ----------
>     > 2021/01/29 17:31:02.536 kid1| ctx: exit level ?0
>     > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.537 kid1| 88,2| client_side_reply.cc(2061)
>     > processReplyAccessResult: The reply for GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED,
>     because it
>     > matched allowed_http_sites
>     > 2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(266) sendStartOfMessage:
>     > HTTP Client local=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443>
>     > remote=10.10.1.249:43538 <http://10.10.1.249:43538>
>     <http://10.10.1.249:43538> FD 13 flags=33
>     > 2021/01/29 17:31:02.537 kid1| 11,2| Stream.cc(267) sendStartOfMessage:
>     > HTTP Client REPLY:
>     > ---------
>     > HTTP/1.1 200 OK
>     > x-amz-id-2:
>     >
>     hZbtwwRSyeN8TkE+V7V9iUuEEMwyXLVblsFhmazae3kqofWK5EuQf+dH6rU3CF8hDUbj8YcMyw4=
>     > x-amz-request-id: CD6D86AAA3FDA43F
>     > Date: Fri, 29 Jan 2021 17:31:03 GMT
>     > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
>     > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
>     > Accept-Ranges: bytes
>     > Content-Type: binary/octet-stream
>     > Content-Length: 8
>     > Server: AmazonS3
>     > X-Cache: MISS from squid
>     > X-Cache-Lookup: MISS from squid:3128
>     > Via: 1.1 squid (squid/4.9)
>     > Connection: keep-alive
>     >
>     >
>     > ----------
>     > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.537 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2021/01/29 17:31:02.538 kid1| 33,2| client_side.cc(582) swanSong:
>     > local=52.217.88.134:443 <http://52.217.88.134:443>
>     <http://52.217.88.134:443>
>     > remote=10.10.1.249:43538 <http://10.10.1.249:43538>
>     <http://10.10.1.249:43538> flags=33
>     > 2021/01/29 17:31:02.538 kid1| 20,2| store.cc(986) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     >
>     --------------------------------------------------------------------------------------------------------------------------------
>     >
>     >
>     >
>     >
>     > On the other hand, with squidclient from the Squid itself, access log
>     > (the first run, when nothing is cached for the new test file yet):
>     >
>     >
>     ------------------------------------------------------------------------------------------------------------------
>     > 1611942152.986 ? ? 29 127.0.0.1 TCP_MISS/200 483 GET
>     > https://s3.amazonaws.com/test.dvabearqloza.com/testFile -
>     > HIER_DIRECT/52.216.226.131 <http://52.216.226.131>
>     <http://52.216.226.131> binary/octet-stream
>     >
>     ------------------------------------------------------------------------------------------------------------------
>     >
>     > And cache log:
>     >
>     ------------------------------------------------------------------------------------------------------------------
>     > 2021/01/29 17:42:32.956 kid1| 5,2| TcpAcceptor.cc(312) acceptNext:
>     > connection on local=[::]:3128 remote=[::] FD 28 flags=9
>     > 2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1306)
>     > parseHttpRequest: HTTP Client local=127.0.0.1:3128
>     <http://127.0.0.1:3128>
>     > <http://127.0.0.1:3128> remote=127.0.0.1:50584
>     <http://127.0.0.1:50584> <http://127.0.0.1:50584>
>     > FD 13 flags=1
>     > 2021/01/29 17:42:32.957 kid1| 11,2| client_side.cc(1310)
>     > parseHttpRequest: HTTP Client REQUEST:
>     > ---------
>     > GET https://s3.amazonaws.com/test.XXXXX.com/testFile HTTP/1.0
>     > Host: s3.amazonaws.com <http://s3.amazonaws.com>
>     <http://s3.amazonaws.com>
>     > User-Agent: squidclient/4.9
>     > Accept: */*
>     > Connection: close
>     >
>     > ----------
>     > 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
>     > clientAccessCheckDone: The request GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
>     > checked: allowed_http_sites
>     > 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(729)
>     > clientAccessCheck2: No adapted_http_access configuration. default:
>     ALLOW
>     > 2021/01/29 17:42:32.957 kid1| 85,2| client_side_request.cc(753)
>     > clientAccessCheckDone: The request GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED; last ACL
>     > checked: allowed_http_sites
>     > 2021/01/29 17:42:32.957 kid1| 17,2| FwdState.cc(142) FwdState:
>     > Forwarding client request local=127.0.0.1:3128
>     <http://127.0.0.1:3128> <http://127.0.0.1:3128>
>     > remote=127.0.0.1:50584 <http://127.0.0.1:50584>
>     <http://127.0.0.1:50584> FD 13 flags=1,
>     > url=https://s3.amazonaws.com/test.XXXXX.com/testFile
>     > 2021/01/29 17:42:32.957 kid1| 44,2| peer_select.cc(281)
>     > peerSelectDnsPaths: Find IP destination for:
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile' via
>     s3.amazonaws.com <http://s3.amazonaws.com>
>     > <http://s3.amazonaws.com>
>     > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(302)
>     > peerSelectDnsPaths: Found sources for
>     > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
>     > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(303)
>     > peerSelectDnsPaths: ? always_direct = DENIED
>     > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(304)
>     > peerSelectDnsPaths: ? ?never_direct = DENIED
>     > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(308)
>     > peerSelectDnsPaths: ? ? ? ? ?DIRECT = local=0.0.0.0
>     > remote=52.216.226.131:443 <http://52.216.226.131:443>
>     <http://52.216.226.131:443> flags=1
>     > 2021/01/29 17:42:32.959 kid1| 44,2| peer_select.cc(317)
>     > peerSelectDnsPaths: ? ? ? ?timedout = 0
>     > 2021/01/29 17:42:32.961 kid1| 83,2| bio.cc(316) readAndParse: parsing
>     > error on FD 15: check failed: state < atHelloReceived
>     > ? ? exception location: Handshake.cc(324) parseHandshakeMessage
>     >
>     > 2021/01/29 17:42:32.961 kid1| Error parsing SSL Server Hello
>     Message on
>     > FD 15
>     > 2021/01/29 17:42:32.965 kid1| 37,2| IcmpSquid.cc(91) SendEcho: to
>     > 52.216.226.131, opcode 3, len 16
>     > 2021/01/29 17:42:32.965| 42,2| IcmpPinger.cc(205) Recv: ?Pass
>     > 52.216.226.131 off to ICMPv4 module.
>     > 2021/01/29 17:42:32.965| 42,2| Icmp.cc(95) Log: pingerLog:
>     > 1611942152.965403 52.216.226.131 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?32
>     > 2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2260) sendRequest: HTTP
>     > Server local=10.10.0.135:33004 <http://10.10.0.135:33004>
>     <http://10.10.0.135:33004>
>     > remote=52.216.226.131:443 <http://52.216.226.131:443>
>     <http://52.216.226.131:443> FD 15 flags=1
>     > 2021/01/29 17:42:32.965 kid1| 11,2| http.cc(2261) sendRequest: HTTP
>     > Server REQUEST:
>     > ---------
>     > GET /test.XXXXX.com/testFile <http://test.XXXXX.com/testFile>
>     <http://test.XXXXX.com/testFile> HTTP/1.1
>     > User-Agent: squidclient/4.9
>     > Accept: */*
>     > Host: s3.amazonaws.com <http://s3.amazonaws.com>
>     <http://s3.amazonaws.com>
>     > Via: 1.0 squid (squid/4.9)
>     > X-Forwarded-For: 127.0.0.1
>     > Cache-Control: max-age=259200
>     > Connection: keep-alive
>     >
>     >
>     > ----------
>     > 2021/01/29 17:42:32.966| 42,2| IcmpPinger.cc(218) SendResult: return
>     > result to squid. len=7997
>     > 2021/01/29 17:42:32.966| 42,2| Icmp.cc(95) Log: pingerLog:
>     > 1611942152.966514 52.216.226.131 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?0 Echo
>     > Reply ? ? ?1ms 6 hops
>     > 2021/01/29 17:42:32.985 kid1| ctx: enter level ?0:
>     > 'https://s3.amazonaws.com/test.XXXXX.com/testFile'
>     > 2021/01/29 17:42:32.985 kid1| 11,2| http.cc(719) processReplyHeader:
>     > HTTP Server local=10.10.0.135:33004 <http://10.10.0.135:33004>
>     <http://10.10.0.135:33004>
>     > remote=52.216.226.131:443 <http://52.216.226.131:443>
>     <http://52.216.226.131:443> FD 15 flags=1
>     > 2021/01/29 17:42:32.985 kid1| 11,2| http.cc(723) processReplyHeader:
>     > HTTP Server RESPONSE:
>     > ---------
>     > HTTP/1.1 200 OK
>     > x-amz-id-2:
>     >
>     z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
>     > x-amz-request-id: A6E14CC59FE63894
>     > Date: Fri, 29 Jan 2021 17:42:33 GMT
>     > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
>     > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
>     > Accept-Ranges: bytes
>     > Content-Type: binary/octet-stream
>     > Content-Length: 8
>     > Server: AmazonS3
>     >
>     > ----------
>     > 2021/01/29 17:42:32.986 kid1| ctx: exit level ?0
>     > 2021/01/29 17:42:32.986 kid1| 88,2| client_side_reply.cc(2061)
>     > processReplyAccessResult: The reply for GET
>     > https://s3.amazonaws.com/test.XXXXX.com/testFile is ALLOWED,
>     because it
>     > matched allowed_http_sites
>     > 2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(266) sendStartOfMessage:
>     > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
>     <http://127.0.0.1:3128>
>     > remote=127.0.0.1:50584 <http://127.0.0.1:50584>
>     <http://127.0.0.1:50584> FD 13 flags=1
>     > 2021/01/29 17:42:32.986 kid1| 11,2| Stream.cc(267) sendStartOfMessage:
>     > HTTP Client REPLY:
>     > ---------
>     > HTTP/1.1 200 OK
>     > x-amz-id-2:
>     >
>     z//C9o0g1wI5ep44MaSBbU7ptfDlvOjTZLIBYSpaI8+h8oxt607nyA9zumm8eEk+wTJb3jRD7wU=
>     > x-amz-request-id: A6E14CC59FE63894
>     > Date: Fri, 29 Jan 2021 17:42:33 GMT
>     > Last-Modified: Fri, 29 Jan 2021 17:27:47 GMT
>     > ETag: "eb1a3227cdc3fedbaec2fe38bf6c044a"
>     > Accept-Ranges: bytes
>     > Content-Type: binary/octet-stream
>     > Content-Length: 8
>     > Server: AmazonS3
>     > X-Cache: MISS from squid
>     > X-Cache-Lookup: MISS from squid:3128
>     > Via: 1.1 squid (squid/4.9)
>     > Connection: close
>     >
>     >
>     > ----------
>     > 2021/01/29 17:42:32.986 kid1| 20,2| store_io.cc(43) storeCreate:
>     > storeCreate: Selected dir 0 for e:=sp2V/0x1f582b0*4
>     > 2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(891) kick:
>     > local=127.0.0.1:3128 <http://127.0.0.1:3128>
>     <http://127.0.0.1:3128> remote=127.0.0.1:50584 <http://127.0.0.1:50584>
>     > <http://127.0.0.1:50584> flags=1 Connection was closed
>     > 2021/01/29 17:42:32.986 kid1| 33,2| client_side.cc(582) swanSong:
>     > local=127.0.0.1:3128 <http://127.0.0.1:3128>
>     <http://127.0.0.1:3128> remote=127.0.0.1:50584 <http://127.0.0.1:50584>
>     > <http://127.0.0.1:50584> flags=1
>     >
>     ------------------------------------------------------------------------------------------------------------------
>     >
>     > The first thing that caught my attention was the line:
>     > "checkCachable: StoreEntry::checkCachable: NO: not cachable", that
>     > appears in the logs when server tries to go through proxy.
>     >
>     > Any idea what might be the issue overall?
>     >
>     > Thanks again!!!
>     >
>     >
>     >
>     >
>     > On Fri, Jan 29, 2021 at 5:40 PM Alex Rousskov
>     > <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>     > <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>     >
>     >? ? ?On 1/28/21 1:34 PM, Milos Dodic wrote:
>     >
>     >? ? ?> I have noticed that the test server also doesn't cache anything
>     >? ? ?> So if I try to go for a file in S3, it says MISS, and after
>     that, MISS
>     >? ? ?> again, and I see no new objects in cache being created.
>     >
>     >? ? ?> If I try the same thing from the proxy itself, I get the
>     MISS, and the
>     >? ? ?> object gets cached, as it should.
>     >? ? ?> When I go back to the test server, and try again, it sees the
>     >? ? ?object in
>     >? ? ?> cache and returns TCP_MEM_HIT/200 instead.
>     >
>     >? ? ?Without more details, I can only speculate that the client
>     running on
>     >? ? ?the test server sends different HTTP request headers than the
>     client
>     >? ? ?running on the proxy itself. You can see the headers in
>     cache.log if you
>     >? ? ?set debug_options to ALL,2. If you are not sure whether they
>     are the
>     >? ? ?same, please share those logs. They will also contain response
>     headers
>     >? ? ?and other potentially useful details.
>     >
>     >? ? ?If the request headers are the same in both tests, then I would
>     >? ? ?recommend sharing compressed ALL,7 or ALL,9 debugging logs of both
>     >? ? ?successful and unsuccessful tests (four transactions, two
>     logs) for
>     >? ? ?analysis. Do not use sensitive data for these tests.
>     >
>     >? ?
>     ?https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>     >
>     >? ? ?Alex.
>     >
>     >
>     >
>     >? ? ?> This is the entire config file:
>     >? ? ?>
>     >? ? ?>
>     >? ? ?> visible_hostname squid
>     >? ? ?> cache_dir ufs /test/cache/squid 10000 16 256
>     >? ? ?>
>     >? ? ?> http_access allow localhost
>     >? ? ?> http_access alow all
>     >? ? ?>
>     >? ? ?> http_port 3128
>     >? ? ?> http_port 3129 intercept
>     >? ? ?> acl allowed_http_sites dstdomain .amazonaws.com
>     <http://amazonaws.com>
>     >? ? ?<http://amazonaws.com> <http://amazonaws.com>
>     >? ? ?> http_access allow allowed_http_sites
>     >? ? ?>
>     >? ? ?> https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
>     >? ? ?> acl SSL_port port 443
>     >? ? ?> http_access allow SSL_port
>     >? ? ?> acl allowed_https_sites ssl::server_name .amazonaws.com
>     <http://amazonaws.com>
>     >? ? ?<http://amazonaws.com>
>     >? ? ?> <http://amazonaws.com>
>     >? ? ?>
>     >? ? ?> ssl_bump stare all
>     >? ? ?> ssl_bump bump allowed_https_sites
>     >? ? ?> ssl_bump terminate all
>     >
>     >
>     >? ? ?> On Tue, Jan 26, 2021 at 9:14 PM Alex Rousskov wrote:
>     >? ? ?>
>     >? ? ?>? ? ?On 1/26/21 1:54 PM, Milos Dodic wrote:
>     >? ? ?>
>     >? ? ?>? ? ?> when the test server goes for a picture I have stored
>     >? ? ?somewhere in
>     >? ? ?>? ? ?> the cloud, the squid access log shows
>     "TCP_TUNNEL/200". But
>     >? ? ?when I
>     >? ? ?>? ? ?> try from the proxy itself with squidclient tool, I get
>     >? ? ?>? ? ?> "TCP_MEM_HIT/200"
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>? ? ?Given the very limited information you have provided, I am
>     >? ? ?guessing that
>     >? ? ?>
>     >? ? ?>? ? ?* the primary tests opens a CONNECT tunnel through Squid
>     >? ? ?>? ? ?* the squidclient test sends a plain text HTTP request
>     to Squid
>     >? ? ?>
>     >? ? ?>? ? ?The final origin server destination may be the same in both
>     >? ? ?tests, but
>     >? ? ?>? ? ?the two transactions are completely different from Squid
>     point
>     >? ? ?of view.
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>? ? ?> ssl_bump peek step1 all
>     >? ? ?>? ? ?> ssl_bump peek step2 allowed_https_sites
>     >? ? ?>? ? ?> ssl_bump splice step3 allowed_https_sites
>     >? ? ?>? ? ?> ssl_bump terminate step3 all
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>? ? ?AFAICT, this configuration is splicing or terminating
>     all TLS
>     >? ? ?traffic.
>     >? ? ?>? ? ?No bumping at all. If you want your Squid to bump TLS
>     tunnels,
>     >? ? ?then you
>     >? ? ?>? ? ?have to have at least one "bump" rule!
>     >? ? ?>
>     >? ? ?>? ? ?I do not know what your overall SslBump needs are, but
>     perhaps
>     >? ? ?you meant
>     >? ? ?>? ? ?something like the following?
>     >? ? ?>
>     >? ? ?>? ? ?? ? acl shouldBeBumped ssl::server_name .amazonaws.com
>     <http://amazonaws.com>
>     >? ? ?<http://amazonaws.com>
>     >? ? ?>? ? ?<http://amazonaws.com>
>     >? ? ?>
>     >? ? ?>? ? ?? ? ssl_bump stare all
>     >? ? ?>? ? ?? ? ssl_bump bump shouldBeBumped
>     >? ? ?>? ? ?? ? ssl_bump terminate all
>     >? ? ?>
>     >? ? ?>? ? ?Please do not use the configuration above until you
>     understand
>     >? ? ?what it
>     >? ? ?>? ? ?does. Please see
>     >? ? ?https://wiki.squid-cache.org/Features/SslPeekAndSplice
>     >? ? ?>? ? ?for details.
>     >? ? ?>
>     >? ? ?>? ? ?Depending on your environment, the http_access rules may
>     need
>     >? ? ?to be
>     >? ? ?>? ? ?adjusted to allow CONNECT requests (to TLS-safe ports) to IP
>     >? ? ?addresses
>     >? ? ?>? ? ?that do not result in .amazonaws.com
>     <http://amazonaws.com> <http://amazonaws.com>
>     >? ? ?<http://amazonaws.com> in
>     >? ? ?>? ? ?reverse DNS lookups.
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>? ? ?HTH,
>     >? ? ?>
>     >? ? ?>? ? ?Alex.
>     >? ? ?>
>     >
> 



From squid3 at treenet.co.nz  Sat Jan 30 03:30:42 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Jan 2021 16:30:42 +1300
Subject: [squid-users] Fixing Squid configuration for caching proxy?
In-Reply-To: <22bd979a-131a-cc08-666b-aa927a0df0cc@measurement-factory.com>
References: <CAPGQvVSi6hMpbuCJzs=BF6eo9hdMHdRqbhysfDvX+tH_56+jdw@mail.gmail.com>
 <d16f0526-3dcc-1b14-3e42-fdab8c64f26c@measurement-factory.com>
 <CAPGQvVReaS1L7JVOCbSWKMTQvmcEc3MG371iz2gzMLJ9p107jA@mail.gmail.com>
 <0b1894b8-9e01-f93f-a88d-d955d2dc2653@measurement-factory.com>
 <CAPGQvVTZFmf=RgHFBrLsLRTE3rEBadxfityxrG+Lsme-WadAfQ@mail.gmail.com>
 <22bd979a-131a-cc08-666b-aa927a0df0cc@measurement-factory.com>
Message-ID: <e1b79e87-2392-11d8-fef1-54267b7f5060@treenet.co.nz>

On 30/01/21 8:57 am, Alex Rousskov wrote:
> On 1/29/21 12:56 PM, Milos Dodic wrote:
> 
>> Here are the logs, but first to mention, from the server that is going
>> through the Squid, I am using curl -k (-k to ignore SSL insecure
>> warnings atm). From the Squid iself, I use squidclient, as using curl
>> from Squid doesn't do much.
> 
> It is possible that SslBump (your first/"server" test) side effects
> somehow disable caching, but I cannot tell that from the logs you have
> shared. IIRC, SslBump does not prevent caching by default.
> 

I expect the lack of caching is due to the first "request" being 
intercepted port 443 traffic. The second is a forward proxy https:// 
request.


Amos


From wp.rauchholz at gmail.com  Sat Jan 30 07:19:08 2021
From: wp.rauchholz at gmail.com (Wolfgang Paul Rauchholz)
Date: Sat, 30 Jan 2021 08:19:08 +0100
Subject: [squid-users] re-directing through squid using MAC
Message-ID: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>

I got two questions actualy. I want to re-direct all traffic certain users
(parental control...) through squid.

(1)  What i the best possibility to do so independently of whether they are
on the LAN or are outside home?
(2) If I only want to re-direct when they are on the LAN; can I do this by
capturing the MAC address of their devices?

Thank you!


Wolfgang Rauchholz
+34 627 994 977
https://www.linkedin.com/in/wolfgangrauchholz/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210130/8fae0d58/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 30 09:32:10 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Jan 2021 22:32:10 +1300
Subject: [squid-users] re-directing through squid using MAC
In-Reply-To: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>
References: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>
Message-ID: <fdb1f0f8-faa5-3d14-f970-6b6c375ed2f7@treenet.co.nz>

On 30/01/21 8:19 pm, Wolfgang Paul Rauchholz wrote:
> I got two questions actualy. I want to re-direct all traffic certain 
> users (parental control...) through squid.
> 
> (1)? What i the best possibility to do so independently of whether they 
> are on the LAN or are outside home?

There is no single way which can do it for both of those environments.


> (2) If I only want to re-direct when they are on the?LAN; can I do this 
> by capturing the MAC address of their devices?
> 

Moving traffic around is a feature of the network routing system. The 
answer to that depends on which OS you have within yoru network and what 
they can do.

That is also why there is no single solution to (1). You have little or 
no control over router settings beyond your own network - so a very 
different setup is needed for non-LAN traffic.


Amos


From ml at netfence.it  Sat Jan 30 17:08:49 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Sat, 30 Jan 2021 18:08:49 +0100
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <ff0b198a-672a-541b-95d5-c0283aa6ed45@measurement-factory.com>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
 <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
 <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>
 <ff0b198a-672a-541b-95d5-c0283aa6ed45@measurement-factory.com>
Message-ID: <fb6e47bb-a225-6788-5b1e-0e34e745ba8f@netfence.it>

On 1/29/21 8:38 PM, Alex Rousskov wrote:

> IIRC, you did not disclose timeout suspicions before. This explanation
> is news to me, and it eliminates several suspects.

Sorry, I didn't say much in fact.
I gave for granted that it was C-ICAP who stopped answering; I didn't 
suspect a Squid bug and had no other idea.



> If you are talking about Squid timing out when attempting to establish a
> TCP connection with the ICAP server, then this may by as much insight as
> you can get from the Squid side.

What I hoped to find in Squid logs was *what* was being passed to C-ICAP 
when it locked.
I'll try on the C-ICAP side then.



> I do not know much about c-icap, but I would check whether its
> configuration or something like crontab results in hourly restarts and
> associated loss of connectivity.

AFAIK no.



> The network interface or the routing tables might also be reset hourly

They live on the same host.



> The ICAP server/service might be running out of descriptors or memory.

I'd expect it to log that, but I'll investigate better.



> One potentially useful test is to try to connect to the ICAP server
> _while the problem is happening_ using telnet or netcat. When Squid
> cannot establish a connection, can you?

I'll try, but it's going to be hard, since this happens for a few 
minutes once a day at most.



> Packet captures can tell you whether other Squid-ICAP server connections
> were active at the time, whether from-Squid SYN packets were able to
> reach the ICAP server, etc.
> 
> In other words, basic network troubleshooting steps...

As I said, they live on the same host, so it can't be a network problem.



> Higher timeout will delay HTTP client transactions for longer periods of
> time, of course. If you want to go down the road of finding workarounds,
> then check whether raising that timeout actually helps. It is not yet
> clear (to me) whether the connections just need more time to be
> established or are simply doomed.

It's not clear to me either, but I suspect so, given the trouble only 
last a few minutes.




>> Same for disabling icap_service_failure_limit?
> 
> This is an essential ICAP service (icap_service bypass=off). I assume
> there is no backup service -- no adaptation_service_set in play here. If
> so, disabling the limit means that fewer HTTP transactions will be
> inconvenienced in the long run than if the service were to be suspended.
>   Hence, fewer ICAP errors will be delivered to Squid clients.

Agreed.



> You can also enable bypass.

I guess this would open a potential for an attack.
DoS the service (antivirus), then let something nasty pass...



> Fixing the problem would be a much better solution, of course.

Sure, I know these are workarounds and I'd rather avoid them, but I'll 
need to consider them as a last resort.



  bye & Thanks
	av.


From ngtech1ltd at gmail.com  Sat Jan 30 17:12:53 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 30 Jan 2021 19:12:53 +0200
Subject: [squid-users] re-directing through squid using MAC
In-Reply-To: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>
References: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>
Message-ID: <005201d6f72b$260f2730$722d7590$@gmail.com>

Hey,

 

There are many solutions for these however it depends on couple things.

The first thing is the parental and kids/children/others cooperation.

Ie if the kids know and want to use the solution.

 

I believe that parenting starts based on understanding that there is a Threat out there.

Today it?s the  same thing like fire and other hazards awareness.

If the kids/children/others in the house doesn?t believe that there is a threat it is the obligation of the parents and
the community to teach and educate them about the subject(To my believe A demo is always a last resort solution).

( I have seen many adults which doesn?t believe even after they have been hit..

Ie they have a virus on their PC or Mobile and they still believe that there is not issue.

Even after these are being given a demo of what is being leaked from their PC and Mobile they don?t care. )

 

Lately I have seen couple new WIFI solutions(The old doesn?t work anymore..) which offers some parental control 
in the house bundled in the product that has a management and control app for the parents.

I don?t know if these can be compared to squid.

 

I can just say that IDS and AV with squid would require some kind of customization and I believe that it?s worth
to try some ready to use solutions as a part of the kids/children and adults education.

It?s like riding a bicycle, if you will try to create one yourself? it depends on your ?blacksmith? or ?iron man? skills.

 

 

To force the PC or the mobile would be different solution but they both require some application these days.

 

On your LAN it will also depend on the cooperation.

When you want to capture traffic on LAN it would probably be by the combination of MAC and IP.

These two are both tied to one another?

There are many devices these day who tries to dynamically assign mac address to avoid what you are trying to achieve.

To overcome this you are probably better use one of these below (or more..):

*	802x authentication for WIFI
*	Redirect all traffic except the identified devices by their MAC+IP(FROM DHCP)
*	HotSpot authentication

 

I have implemented the above solutions on both a Linux device and Mikrotik.

Currently I am using Mikrotik Router which does all of the above else then the filtering itself which I am using
an external service which does better tls/ssl inspection and categorizing then I can provide with Squid and a subscription.

(?No hard feelings with the Squid project)

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Wolfgang Paul Rauchholz
Sent: Saturday, January 30, 2021 9:19 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] re-directing through squid using MAC

 

I got two questions actualy. I want to re-direct all traffic certain users (parental control...) through squid.

 

(1)  What i the best possibility to do so independently of whether they are on the LAN or are outside home?

(2) If I only want to re-direct when they are on the LAN; can I do this by capturing the MAC address of their devices?

 

Thank you! 

 




Wolfgang Rauchholz

+34 627 994 977

https://www.linkedin.com/in/wolfgangrauchholz/

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210130/48671026/attachment.htm>

From m_zouhairy at ckta.by  Sat Jan 30 17:18:31 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Sat, 30 Jan 2021 20:18:31 +0300
Subject: [squid-users] re-directing through squid using MAC
In-Reply-To: <005201d6f72b$260f2730$722d7590$@gmail.com>
References: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>
 <005201d6f72b$260f2730$722d7590$@gmail.com>
Message-ID: <a2d9897f-0327-31ae-a61c-516d76a0f3af@ckta.by>

wifi ? so you would rather get up at nights just so not to expose to 
some sites?

30.01.21 20:12, Eliezer Croitoru ?????:
>
> Hey,
>
> There are many solutions for these however it depends on couple things.
>
> The first thing is the parental and kids/children/others cooperation.
>
> Ie if the kids know and want to use the solution.
>
> I believe that parenting starts based on understanding that there is a 
> Threat out there.
>
> Today it?s the? same thing like fire and other hazards awareness.
>
> If the kids/children/others in the house doesn?t believe that there is 
> a threat it is the obligation of the parents and
> the community to teach and educate them about the subject(To my 
> believe A demo is always a last resort solution).
>
> ( I have seen many adults which doesn?t believe even after they have 
> been hit..
>
> Ie they have a virus on their PC or Mobile and they still believe that 
> there is not issue.
>
> Even after these are being given a demo of what is being leaked from 
> their PC and Mobile they don?t care. )
>
> Lately I have seen couple new WIFI solutions(The old doesn?t work 
> anymore..) which offers some parental control
> in the house bundled in the product that has a management and control 
> app for the parents.
>
> I don?t know if these can be compared to squid.
>
> I can just say that IDS and AV with squid would require some kind of 
> customization and I believe that it?s worth
> to try some ready to use solutions as a part of the kids/children and 
> adults education.
>
> It?s like riding a bicycle, if you will try to create one yourself? it 
> depends on your ?blacksmith? or ?iron man? skills.
>
> To force the PC or the mobile would be different solution but they 
> both require some application these days.
>
> On your LAN it will also depend on the cooperation.
>
> When you want to capture traffic on LAN it would probably be by the 
> combination of MAC and IP.
>
> These two are both tied to one another?
>
> There are many devices these day who tries to dynamically assign mac 
> address to avoid what you are trying to achieve.
>
> To overcome this you are probably better use one of these below (or 
> more..):
>
>   * 802x authentication for WIFI
>   * Redirect all traffic except the identified devices by their
>     MAC+IP(FROM DHCP)
>   * HotSpot authentication
>
> I have implemented the above solutions on both a Linux device and 
> Mikrotik.
>
> Currently I am using Mikrotik Router which does all of the above else 
> then the filtering itself which I am using
> an external service which does better tls/ssl inspection and 
> categorizing then I can provide with Squid and a subscription.
>
> (?No hard feelings with the Squid project)
>
> All The Bests,
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
> Zoom: Coming soon
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *Wolfgang Paul Rauchholz
> *Sent:* Saturday, January 30, 2021 9:19 AM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] re-directing through squid using MAC
>
> I got two questions actualy. I want to re-direct all traffic certain 
> users (parental control...) through squid.
>
> (1)? What i the best possibility to do so independently of whether 
> they are on the LAN or are outside home?
>
> (2) If I only want to re-direct when they are on the?LAN; can I do 
> this by capturing the MAC address of their devices?
>
> Thank?you!
>
>
> Wolfgang Rauchholz
>
> +34 627 994 977
>
> https://www.linkedin.com/in/wolfgangrauchholz/
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210130/4f1bef46/attachment.htm>

From ngtech1ltd at gmail.com  Sat Jan 30 19:12:46 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 30 Jan 2021 21:12:46 +0200
Subject: [squid-users] re-directing through squid using MAC
In-Reply-To: <a2d9897f-0327-31ae-a61c-516d76a0f3af@ckta.by>
References: <CAETVtpSA_v8szqSY3zU6v2uRzLTom4S2DBr4Fr6Hj-JyH7qmqQ@mail.gmail.com>
 <005201d6f72b$260f2730$722d7590$@gmail.com>
 <a2d9897f-0327-31ae-a61c-516d76a0f3af@ckta.by>
Message-ID: <005c01d6f73b$e57e0220$b07a0660$@gmail.com>

Hey,

 

Sorry I didn?t understood the question?

How exactly what I wrote say what have asked? And Also yes,
I would like to wake up at night for my kids rather then grow up into monsters.

 

WIFI? Whats the question?

Kids do not get wifi? on what ages of kids were you asking?

The same goes about Mobile phones, the difference between WIFI and Mobile 4G is only the .. Power of amplification.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Majed Zouhairy
Sent: Saturday, January 30, 2021 7:19 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] re-directing through squid using MAC

 

wifi ? so you would rather get up at nights just so not to expose to some sites?

30.01.21 20:12, Eliezer Croitoru ?????:

Hey,

 

There are many solutions for these however it depends on couple things.

The first thing is the parental and kids/children/others cooperation.

Ie if the kids know and want to use the solution.

 

I believe that parenting starts based on understanding that there is a Threat out there.

Today it?s the  same thing like fire and other hazards awareness.

If the kids/children/others in the house doesn?t believe that there is a threat it is the obligation of the parents and
the community to teach and educate them about the subject(To my believe A demo is always a last resort solution).

( I have seen many adults which doesn?t believe even after they have been hit..

Ie they have a virus on their PC or Mobile and they still believe that there is not issue.

Even after these are being given a demo of what is being leaked from their PC and Mobile they don?t care. )

 

Lately I have seen couple new WIFI solutions(The old doesn?t work anymore..) which offers some parental control 
in the house bundled in the product that has a management and control app for the parents.

I don?t know if these can be compared to squid.

 

I can just say that IDS and AV with squid would require some kind of customization and I believe that it?s worth
to try some ready to use solutions as a part of the kids/children and adults education.

It?s like riding a bicycle, if you will try to create one yourself? it depends on your ?blacksmith? or ?iron man? skills.

 

 

To force the PC or the mobile would be different solution but they both require some application these days.

 

On your LAN it will also depend on the cooperation.

When you want to capture traffic on LAN it would probably be by the combination of MAC and IP.

These two are both tied to one another?

There are many devices these day who tries to dynamically assign mac address to avoid what you are trying to achieve.

To overcome this you are probably better use one of these below (or more..):

*	802x authentication for WIFI
*	Redirect all traffic except the identified devices by their MAC+IP(FROM DHCP)
*	HotSpot authentication

 

I have implemented the above solutions on both a Linux device and Mikrotik.

Currently I am using Mikrotik Router which does all of the above else then the filtering itself which I am using
an external service which does better tls/ssl inspection and categorizing then I can provide with Squid and a subscription.

(?No hard feelings with the Squid project)

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of Wolfgang Paul Rauchholz
Sent: Saturday, January 30, 2021 9:19 AM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] re-directing through squid using MAC

 

I got two questions actualy. I want to re-direct all traffic certain users (parental control...) through squid.

 

(1)  What i the best possibility to do so independently of whether they are on the LAN or are outside home?

(2) If I only want to re-direct when they are on the LAN; can I do this by capturing the MAC address of their devices?

 

Thank you! 

 




Wolfgang Rauchholz

+34 627 994 977

https://www.linkedin.com/in/wolfgangrauchholz/

 





_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210130/f5a32384/attachment.htm>

From ngtech1ltd at gmail.com  Sat Jan 30 19:20:33 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 30 Jan 2021 21:20:33 +0200
Subject: [squid-users] ngtech repository goes TLS.
Message-ID: <006101d6f73c$fba60bf0$f2f223d0$@gmail.com>

Hey All,

 

I have upgraded my setup from plain HTTP only to HTTPS.

I will leave the HTTP channel open for now but in case you need HTTPS for
any reason you can now use it.

 

The currently maintained repository is at: https://www.ngtech.co.il/repo/

Oracle Enterprise Linux 7 and 8

Amazon Linux 1 and 2

CentOS 7 and 8

 

There are also binary packages for Debian and Ubuntu(not deb packages).

I don't know exactly the future of CentOS and I will try to support it but
there is a chance I will stop supporting it in the future.

For now I am waiting for Squid 5.0.5 release and it will update almost
automatically.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210130/0a0becdd/attachment.htm>

From squid3 at treenet.co.nz  Sun Jan 31 00:11:12 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 31 Jan 2021 13:11:12 +1300
Subject: [squid-users] Squid "suspending ICAP service for too many
 failures"
In-Reply-To: <fb6e47bb-a225-6788-5b1e-0e34e745ba8f@netfence.it>
References: <e271874f-b97d-c7d6-dc64-23e916e7d579@netfence.it>
 <c9c250c0-75ff-00b9-9aee-9233f7522385@measurement-factory.com>
 <878c8baf-4667-1d7b-7cd5-619d955a0bee@netfence.it>
 <ff0b198a-672a-541b-95d5-c0283aa6ed45@measurement-factory.com>
 <fb6e47bb-a225-6788-5b1e-0e34e745ba8f@netfence.it>
Message-ID: <7cd48afa-cecf-99b5-f2bd-5ff7de4282f5@treenet.co.nz>

On 31/01/21 6:08 am, Andrea Venturoli wrote:
> On 1/29/21 8:38 PM, Alex Rousskov wrote:
> 
> 
>> Packet captures can tell you whether other Squid-ICAP server connections
>> were active at the time, whether from-Squid SYN packets were able to
>> reach the ICAP server, etc.
>>
>> In other words, basic network troubleshooting steps...
> 
> As I said, they live on the same host, so it can't be a network problem.
> 

FYI, that conclusion does not follow. Even on the same host there is a 
full TCP/IP networking stack between Squid and ICAP server doing things 
to the packets. All localhost removes is the potential problems due to 
differences in machine networking stacks.

Network config, firewall rules, packet handling, and/or protocol 
negotiation activities between the software are all still happening that 
may affect the outcome.



Amos


From ngtech1ltd at gmail.com  Sun Jan 31 12:52:19 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 31 Jan 2021 14:52:19 +0200
Subject: [squid-users] Many google services IP addresses returns
 invalid2.invalid CN and Error negotiating SSL connection on FD
Message-ID: <000001d6f7cf$ea260720$be721560$@gmail.com>

Hey Alex and Amos,

I have seen the next issue over and over.

2021/01/31 14:26:53 kid1| Error negotiating SSL connection on FD 47: error:00000001:lib(0):func(0):reason(1) (1/-1)
    connection: conn94248 local=216.58.211.194:443 remote=10.200.191.X:33718 flags=33
2021/01/31 14:27:53 kid1| Error negotiating SSL connection on FD 20: error:00000001:lib(0):func(0):reason(1) (1/-1)
    connection: conn94248 local=216.58.211.194:443 remote=10.200.191.X:33718 flags=33
^C?


# Testing for the IP SAN
root at px2-043 ~ [SIGINT]# /opt/tls-check-script/check-dns-san.sh 216.58.211.194 443
Can't use SSL_get_servername
depth=0 OU = "No SNI provided; please fix your client.", CN = invalid2.invalid
verify error:num=18:self signed certificate
verify return:1
depth=0 OU = "No SNI provided; please fix your client.", CN = invalid2.invalid
verify return:1
DONE

# Testing for the IP with google.com SNI
root at px2-043 ~# /opt/tls-check-script/check-dns-san.sh 216.58.211.194 443 google.com
depth=2 OU = GlobalSign Root CA - R2, O = GlobalSign, CN = GlobalSign
verify return:1
depth=1 C = US, O = Google Trust Services, CN = GTS CA 1O1
verify return:1
depth=0 C = US, ST = California, L = Mountain View, O = Google LLC, CN = *.google.com
verify return:1
DONE
X509v3 Subject Alternative Name:
    DNS:*.google.com, DNS:*.android.com, DNS:*.appengine.google.com, DNS:*.bdn.dev, DNS:*.cloud.google.com, DNS:*.crowdsource.google.com, DNS:*.datacompute.google.com, DNS:*.g.co, DNS:*.gcp.gvt2.com
DNS:*.gcpcdn.gvt1.com, DNS:*.ggpht.cn, DNS:*.gkecnapps.cn, DNS:*.google-analytics.com, DNS:*.google.ca, DNS:*.google.cl, DNS:*.google.co.in, DNS:*.google.co.jp, DNS:*.google.co.uk, DNS:*.google.com.
, DNS:*.google.com.au, DNS:*.google.com.br, DNS:*.google.com.co, DNS:*.google.com.mx, DNS:*.google.com.tr, DNS:*.google.com.vn, DNS:*.google.de, DNS:*.google.es, DNS:*.google.fr, DNS:*.google.hu, DN
*.google.it, DNS:*.google.nl, DNS:*.google.pl, DNS:*.google.pt, DNS:*.googleadapis.com, DNS:*.googleapis.cn, DNS:*.googlecnapps.cn, DNS:*.googlecommerce.com, DNS:*.googlevideo.com, DNS:*.gstatic.cn,
NS:*.gstatic.com, DNS:*.gstaticcnapps.cn, DNS:*.gvt1.com, DNS:*.gvt2.com, DNS:*.metric.gstatic.com, DNS:*.urchin.com, DNS:*.url.google.com, DNS:*.wear.gkecnapps.cn, DNS:*.youtube-nocookie.com, DNS:*
outube.com, DNS:*.youtubeeducation.com, DNS:*.youtubekids.com, DNS:*.yt.be, DNS:*.ytimg.com, DNS:android.clients.google.com, DNS:android.com, DNS:developer.android.google.cn, DNS:developers.android.
ogle.cn, DNS:g.co, DNS:ggpht.cn, DNS:gkecnapps.cn, DNS:goo.gl, DNS:google-analytics.com, DNS:google.com, DNS:googlecnapps.cn, DNS:googlecommerce.com, DNS:source.android.google.cn, DNS:urchin.com, DN
www.goo.gl, DNS:youtu.be, DNS:youtube.com, DNS:youtubeeducation.com, DNS:youtubekids.com, DNS:yt.be

And the next test is to verify which ciphers are available on this IP.:
root at px2-043 ~# /opt/tls-check-script/tls-check.rb 216.58.211.194 443 google.com
### Number of Ciphers to be tested: 66
### Timeout per test: 3
### Delay between tests: 1
Testing TLS_AES_256_GCM_SHA384...  NO, SSL_CTX_set_cipher_list
Testing TLS_CHACHA20_POLY1305_SHA256...  NO, SSL_CTX_set_cipher_list
Testing TLS_AES_128_GCM_SHA256...  NO, SSL_CTX_set_cipher_list
Testing TLS_AES_128_CCM_SHA256...  NO, SSL_CTX_set_cipher_list
Testing ECDHE-ECDSA-AES256-GCM-SHA384...  CONNECTED ~: TLS_AES_256_GCM_SHA384, NO, Secure Renegotiation IS NOT supported
Testing ECDHE-RSA-AES256-GCM-SHA384...  CONNECTED ~: TLS_AES_256_GCM_SHA384, NO, Secure Renegotiation IS NOT supported
Testing DHE-RSA-AES256-GCM-SHA384...  CONNECTED ~: TLS_AES_256_GCM_SHA384, NO, Secure Renegotiation IS NOT supported
Testing ECDHE-ECDSA-CHACHA20-POLY1305...  CONNECTED ~: TLS_AES_256_GCM_SHA384, NO, Secure Renegotiation IS NOT supported
^CTraceback (most recent call last):
        3: from /opt/tls-check-script/tls-check.rb:88:in `<main>'
        2: from /opt/tls-check-script/tls-check.rb:88:in `each'
        1: from /opt/tls-check-script/tls-check.rb:136:in `block in <main>'
/opt/tls-check-script/tls-check.rb:136:in `sleep': Interrupt

Which I stopped since most of the output is 
Testing XYZ(CIPHER)...  CONNECTED ~: TLS_AES_256_GCM_SHA384, NO, Secure Renegotiation IS NOT supported

This is probably what is causing the specific issues mentioned above.
I want to try and verify if in this specific session the SNI is known  by google.
Also if there is something that I can do to configure squid for it to work in some way.

I have seen this issue a lot in couple setups which ? google services are being accessed from mobile devices or Google Chrome.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon





From belle at bazuin.nl  Mon Jan  4 09:51:40 2021
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Mon, 04 Jan 2021 09:51:40 -0000
Subject: [squid-users] Setting up a transparent http and https proxy
 server using squid 4.6
In-Reply-To: <6568b326-4ba5-6e07-25a5-009c56416f4a@club-internet.fr>
References: <da84475b-c08e-d377-78ae-c80d116bbc19@club-internet.fr>
Message-ID: <vmime.5ff2e522.412f.75cdf7ee25502a42@ms249-lin-003.rotterdam.bazuin.nl>

Your firewall rules seems off. 



192.168.1.32??????????????????????????????????? is your client, as i seen in the log. 



But your showing 10.3.141.0/24 so.. 

?

Try/look at this. Change interfaces where needed offcourse. 



iptables ? -p tcp \

--dport 80 -j REDIRECT --to-port 3128 -m comment --comment "Squid-Intercept 80->3128"



iptables -p tcp \

--dport 443 -j REDIRECT --to-ports 3129 -m comment --comment "Squid-Intercept 443->3129"



iptables ? -o INTERNET_INTERFACE \

-j MASQUERADE -m comment --comment "IP-Masq allow internet"

?


Greetz, 

?

Louis

?

________________________________________

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens jean francois hasson

Verzonden: zondag 3 januari 2021 19:15

Aan: squid-users at lists.squid-cache.org

Onderwerp: Re: [squid-users] Setting up a transparent http and https proxy server using squid 4.6

?

Hi,

After reading more information on this kind of error I captured a few transactions with Wireshark running on the raspberry pi hosting squid 4.6 and opensll 1.1.1d. I captured some transactions when trying to access ebay.fr which is currently not successful with the setup I have with the error of inappropriate fallback mentioned below.

I am not familiar with TLS transactions so I will try to present a high level view of the transactions between the raspberry pi and the ebay.fr server. I hope you can guide me as to what I should focus on to understand, if possible, the issue I have.

A bird's eye view of the transactions from Wireshark over time is :

???? 23 0.175795327??? 192.168.1.32????????? 192.168.1.1?????????? DNS????? 71???? Standard query 0x057e A www.ebay.fr

???? 24 0.214678299??? 192.168.1.1?????????? 192.168.1.32????????? DNS????? 165??? Standard query response 0x057e A www.ebay.fr CNAME slot11847.ebay.com.edgekey.net CNAME e11847.g.akamaiedge.net A 23.57.6.166

???? 25 0.301067317??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 74???? 53934 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 TSval=365186690 TSecr=0 WS=128

???? 26 0.302488046??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 74???? 53936 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 TSval=365186691 TSecr=0 WS=128

???? 27 0.328959454??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 74???? 443 53934 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 SACK_PERM=1 TSval=3470404062 TSecr=365186690 WS=128

???? 28 0.329115340??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186718 TSecr=3470404062

???? 29 0.329752684??? 192.168.1.32????????? 23.57.6.166?????????? TLSv1.2? 583??? Client Hello

???? 30 0.330530288??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 74???? 443 53936 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 SACK_PERM=1 TSval=3470404064 TSecr=365186691 WS=128

???? 31 0.330644819??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186719 TSecr=3470404064

???? 32 0.331192579??? 192.168.1.32????????? 23.57.6.166?????????? TLSv1.2? 583??? Client Hello

???? 35 0.351054404??? 192.168.1.32????????? 192.168.1.98????????? TCP????? 54???? 5900 49903 [ACK] Seq=14256 Ack=97 Win=501 Len=0

???? 36 0.363323884??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 66???? 443 53934 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404096 TSecr=365186719

???? 37 0.364291801??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 1514?? Server Hello

???? 38 0.364347270??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 TSval=365186753 TSecr=3470404096

???? 39 0.365482999??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1514?? 443 53934 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 TSval=3470404096 TSecr=365186719 [TCP segment of a reassembled PDU]

???? 40 0.365535030??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 TSval=365186754 TSecr=3470404096

???? 41 0.366217999??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1266?? 443 53934 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 TSval=3470404096 TSecr=365186719 [TCP segment of a reassembled PDU]

???? 42 0.366279041??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 TSval=365186755 TSecr=3470404096

???? 43 0.366321697??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 74???? [TCP Retransmission] 443 53936 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 SACK_PERM=1 TSval=3470404096 TSecr=365186691 WS=128

???? 44 0.366410135??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? [TCP Dup ACK 31#1] 53936 443 [ACK] Seq=518 Ack=1 Win=64256 Len=0 TSval=365186755 TSecr=3470404064

???? 45 0.366709770??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 991??? Certificate, Certificate Status, Server Key Exchange, Server Hello Done

???? 46 0.366754978??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186756 TSecr=3470404097

???? 47 0.369138676??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 66???? 443 53936 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404102 TSecr=365186720

???? 48 0.370432739??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 1514?? Server Hello

???? 49 0.370506906??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 TSval=365186759 TSecr=3470404102

???? 50 0.371401125??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1514?? 443 53936 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 TSval=3470404102 TSecr=365186720 [TCP segment of a reassembled PDU]

???? 51 0.371449250??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 TSval=365186760 TSecr=3470404102

???? 52 0.372385968??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1266?? 443 53936 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 TSval=3470404102 TSecr=365186720 [TCP segment of a reassembled PDU]

???? 53 0.372438156??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 TSval=365186761 TSecr=3470404102

???? 54 0.372859562??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 991??? Certificate, Certificate Status, Server Key Exchange, Server Hello Done

???? 55 0.372905395??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186762 TSecr=3470404103

???? 56 0.374064614??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [FIN, ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186763 TSecr=3470404097

???? 57 0.382856646??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [FIN, ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186772 TSecr=3470404103

???? 58 0.387044251??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 74???? 53938 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 TSval=365186776 TSecr=0 WS=128

???? 59 0.401877325??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 74???? 53940 443 [SYN] Seq=0 Win=64240 Len=0 MSS=1460 SACK_PERM=1 TSval=365186791 TSecr=0 WS=128

???? 60 0.402472117??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 66???? 443 53934 [FIN, ACK] Seq=5022 Ack=519 Win=64768 Len=0 TSval=3470404136 TSecr=365186763

???? 61 0.402574981??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53934 443 [ACK] Seq=519 Ack=5023 Win=64128 Len=0 TSval=365186791 TSecr=3470404136

???? 62 0.410122326??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 66???? 443 53936 [FIN, ACK] Seq=5022 Ack=519 Win=64768 Len=0 TSval=3470404143 TSecr=365186772

???? 63 0.410185971??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53936 443 [ACK] Seq=519 Ack=5023 Win=64128 Len=0 TSval=365186799 TSecr=3470404143

???? 64 0.415533941??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 74???? 443 53938 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 SACK_PERM=1 TSval=3470404148 TSecr=365186776 WS=128

???? 65 0.415615607??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53938 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186804 TSecr=3470404148

???? 66 0.416199514??? 192.168.1.32????????? 23.57.6.166?????????? TLSv1.2? 583??? Client Hello

???? 67 0.429629098??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 74???? 443 53940 [SYN, ACK] Seq=0 Ack=1 Win=65160 Len=0 MSS=1460 SACK_PERM=1 TSval=3470404163 TSecr=365186791 WS=128

???? 68 0.429722796??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53940 443 [ACK] Seq=1 Ack=1 Win=64256 Len=0 TSval=365186819 TSecr=3470404163

???? 69 0.430195036??? 192.168.1.32????????? 23.57.6.166?????????? TLSv1.2? 583??? Client Hello

???? 70 0.449937225??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 66???? 443 53938 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404182 TSecr=365186805

???? 71 0.451000037??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 1514?? Server Hello

???? 72 0.451064100??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53938 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 TSval=365186840 TSecr=3470404183

???? 73 0.451980194??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1514?? 443 53938 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 TSval=3470404183 TSecr=365186805 [TCP segment of a reassembled PDU]

???? 74 0.452031756??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53938 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 TSval=365186841 TSecr=3470404183

???? 75 0.452935767??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1266?? 443 53938 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 TSval=3470404183 TSecr=365186805 [TCP segment of a reassembled PDU]

???? 76 0.452991027??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53938 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 TSval=365186842 TSecr=3470404183

???? 77 0.453443475??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 991??? Certificate, Certificate Status, Server Key Exchange, Server Hello Done

???? 78 0.453498215??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53938 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186842 TSecr=3470404184

???? 79 0.461625715??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53938 443 [FIN, ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186850 TSecr=3470404184

???? 80 0.463463320??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 66???? 443 53940 [ACK] Seq=1 Ack=518 Win=64768 Len=0 TSval=3470404196 TSecr=365186819

???? 81 0.464344413??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 1514?? Server Hello

???? 82 0.464433476??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53940 443 [ACK] Seq=518 Ack=1449 Win=64128 Len=0 TSval=365186853 TSecr=3470404197

???? 83 0.465538632??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1514?? 443 53940 [PSH, ACK] Seq=1449 Ack=518 Win=64768 Len=1448 TSval=3470404197 TSecr=365186819 [TCP segment of a reassembled PDU]

???? 84 0.465628789??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53940 443 [ACK] Seq=518 Ack=2897 Win=64128 Len=0 TSval=365186854 TSecr=3470404197

???? 85 0.466298945??? 23.57.6.166?????????? 192.168.1.32????????? TCP????? 1266?? 443 53940 [PSH, ACK] Seq=2897 Ack=518 Win=64768 Len=1200 TSval=3470404197 TSecr=365186819 [TCP segment of a reassembled PDU]

???? 86 0.466437851??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53940 443 [ACK] Seq=518 Ack=4097 Win=64128 Len=0 TSval=365186855 TSecr=3470404197

???? 87 0.467042591??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 991??? Certificate, Certificate Status, Server Key Exchange, Server Hello Done

???? 88 0.467190976??? 192.168.1.32????????? 23.57.6.166?????????? TCP????? 66???? 53940 443 [ACK] Seq=518 Ack=5022 Win=64128 Len=0 TSval=365186856 TSecr=3470404197

I start my description with a Client Hello step from the raspberry pi to the ebay.fr server :

No.???? Time?????????? Source??????????????? Destination?????????? Protocol Length Info

???? 29 0.329752684??? 192.168.1.32????????? 23.57.6.166?????????? TLSv1.2? 583??? Client Hello

...

Transport Layer Security

??? TLSv1.2 Record Layer: Handshake Protocol: Client Hello

??????? Content Type: Handshake (22)

??????? Version: TLS 1.0 (0x0301)

??????? Length: 512

??????? Handshake Protocol: Client Hello

??????????? Handshake Type: Client Hello (1)

??????????? Length: 508

??????????? Version: TLS 1.2 (0x0303)

Then, there is another Client Hello step which seems quite similar to the previous one :

No.???? Time?????????? Source??????????????? Destination?????????? Protocol Length Info

???? 32 0.331192579??? 192.168.1.32????????? 23.57.6.166?????????? TLSv1.2? 583??? Client Hello

...

Transport Layer Security

??? TLSv1.2 Record Layer: Handshake Protocol: Client Hello

??????? Content Type: Handshake (22)

??????? Version: TLS 1.0 (0x0301)

??????? Length: 512

??????? Handshake Protocol: Client Hello

??????????? Handshake Type: Client Hello (1)

??????????? Length: 508

??????????? Version: TLS 1.2 (0x0303)

Then a Server Hello :

No.???? Time?????????? Source??????????????? Destination?????????? Protocol Length Info

???? 37 0.364291801??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 1514?? Server Hello

...

Transport Layer Security

??? TLSv1.2 Record Layer: Handshake Protocol: Server Hello

??????? Content Type: Handshake (22)

??????? Version: TLS 1.2 (0x0303)

??????? Length: 78

??????? Handshake Protocol: Server Hello

??????????? Handshake Type: Server Hello (2)

??????????? Length: 74

??????????? Version: TLS 1.2 (0x0303)

??????????? Random: 08f25b54bfe62d98736a4e5e8cc5a3f4ab97c040c1a892a26110e4d704b2fd9e

??????????????? GMT Unix Time: Oct? 4, 1974 08:40:20.000000000 Paris, Madrid (heure d??t?)

??????????????? Random Bytes: bfe62d98736a4e5e8cc5a3f4ab97c040c1a892a26110e4d704b2fd9e

??????????? Session ID Length: 0

??????????? Cipher Suite: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)

...

So it seems the server found a common cipher with the client. I am not sure then what to look for. Frames 43 and 44 are detected by Wireshark as retransmissions but I am not sure it is a problem.

I noticed frame 45 which is about the Certificate, Certificate Status, Server Key Exchange and Server Hello Done 

No.???? Time?????????? Source??????????????? Destination?????????? Protocol Length Info

???? 45 0.366709770??? 23.57.6.166?????????? 192.168.1.32????????? TLSv1.2? 991??? Certificate, Certificate Status, Server Key Exchange, Server Hello Done

Transport Layer Security

??? TLSv1.2 Record Layer: Handshake Protocol: Certificate

??????? Content Type: Handshake (22)

??????? Version: TLS 1.2 (0x0303)

??????? Length: 4102

??????? Handshake Protocol: Certificate

??????????? Handshake Type: Certificate (11)

???? ...

Transport Layer Security

??? TLSv1.2 Record Layer: Handshake Protocol: Certificate Status

??????? Content Type: Handshake (22)

??????? Version: TLS 1.2 (0x0303)

??????? Length: 479

??????? Handshake Protocol: Certificate Status

??????????? Handshake Type: Certificate Status (22)

??????????? Length: 475

??????????? Certificate Status Type: OCSP (1)

??????????? OCSP Response Length: 471

??????????? OCSP Response

...

??? TLSv1.2 Record Layer: Handshake Protocol: Server Key Exchange

??????? Content Type: Handshake (22)

??????? Version: TLS 1.2 (0x0303)

??????? Length: 333

??????? Handshake Protocol: Server Key Exchange

??????????? Handshake Type: Server Key Exchange (12)

??????????? Length: 329

??????????? EC Diffie-Hellman Server Params

...

??? TLSv1.2 Record Layer: Handshake Protocol: Server Hello Done

??????? Content Type: Handshake (22)

??????? Version: TLS 1.2 (0x0303)

??????? Length: 4

??????? Handshake Protocol: Server Hello Done

??????????? Handshake Type: Server Hello Done (14)

??????????? Length: 0

...

I noticed there is a mention of Diffie-Hellman which may require some attention but I am not sure.

I am sorry for all this information but I really look forward to knowing more and managing to sort this issue out. Is there anything in this information that is relevant to understanding the issue I have ? Where should I focus ?

Best regards,

JF

Le 02/01/2021 ? 11:26, jean francois hasson a ?crit?:

Hi,

Thank you Amos Jeffries and Antony Stone. It seems the configuration I have provides the functionality of filtering I am looking for.

There is a strange behavior I can see when accessing some legitimate sites which I see traces of in cache.log :

2021/01/02 10:55:48 kid1| helperOpenServers: Starting 1/20 'squidGuard' processes

2021/01/02 10:57:31 kid1| ERROR: negotiating TLS on FD 39: error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert inappropriate fallback (1/-1/0)

2021/01/02 10:57:31 kid1| Error negotiating SSL connection on FD 38: error:00000001:lib(0):func(0):reason(1) (1/-1)

2021/01/02 10:57:32 kid1| ERROR: negotiating TLS on FD 38: error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert inappropriate fallback (1/-1/0)

2021/01/02 10:57:32 kid1| Error negotiating SSL connection on FD 35: error:00000001:lib(0):func(0):reason(1) (1/-1)

2021/01/02 10:57:40 kid1| Starting new redirector helpers...

2021/01/02 10:57:40 kid1| helperOpenServers: Starting 1/20 'squidGuard' processes

2021/01/02 10:58:09 kid1| ERROR: negotiating TLS on FD 51: error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert inappropriate fallback (1/-1/0)

2021/01/02 10:58:09 kid1| Error negotiating SSL connection on FD 40: error:00000001:lib(0):func(0):reason(1) (1/-1)

2021/01/02 10:58:10 kid1| ERROR: negotiating TLS on FD 51: error:1407743E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert inappropriate fallback (1/-1/0)

2021/01/02 10:58:10 kid1| Error negotiating SSL connection on FD 40: error:00000001:lib(0):func(0):reason(1) (1/-1)

I noticed other users of squid encountered similar issues but I did not find a clear answer to the issue. Is there a problem with my setup ? I am not sure to be able to solve it on my own ! Any help would be appreciated.

Best regards,

JF Hasson

Le 31/12/2020 ? 10:14, Antony Stone a ?crit?:

On Thursday 31 December 2020 at 10:10:11, jean francois hasson wrote:

?

If I set up on a device connected to the access point a proxy manually

ie 10.3.141.1 on port 8080, I can access the internet. If I put the

following rules for iptables to use in files rules.v4 :

?

*nat

-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j DNAT --to-destination

10.3.141.1:3128

-A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3128

-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j DNAT --to-destination

10.3.141.1:3129

-A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to-ports 3129

-A POSTROUTING -s 10.3.141.0/24 -o eth0 -j MASQUERADE

Try removing the DNAT rules above.? You should be using REDIRECT for intercept 

mode to work correctly.

?

?

Antony.

?

?

?

_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org

http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210104/4a1d66ce/attachment.htm>

