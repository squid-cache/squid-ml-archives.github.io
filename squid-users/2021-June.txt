From koshikmoshik at gmail.com  Fri Jun  4 20:19:36 2021
From: koshikmoshik at gmail.com (koshik moshik)
Date: Fri, 4 Jun 2021 22:19:36 +0200
Subject: [squid-users] Cache Peer Issue with URL
Message-ID: <CAPeL7PFPEydX8Q4PjoOaVZx7-bnBUNkaXbT+-URe-itoASjWZQ@mail.gmail.com>

Hello,

I am running into an issue and would really appreciate your help!

Basically I have created two cashe peers:


never_direct allow all
acl ab1 dstdom_regex "/etc/squid/Bad_Homepages.squid" cache_peer
my.proxy.com parent 31112 0 login=user:pw no-query name=user
cache_peer_access user allow ab1 cache_peer_access user deny all acl ab2
proxy_auth userName1 cache_peer myProxy parent 31112 0 login=user:pw
no-query name=user2 cache_peer_access user2 allow ab2 cache_peer_access
user2 deny all


As you can see, I have a Bad_homepages.squid file which contains regular
expressions for a website, so this cache_peer will only be used, if the
HOMEPAGE is requested. If a sub page is requested, the second
cache_peer(acl ab2) should be used.

Example: www.test.com -> ab1 should be used
www.test.com/hello -> ab2 should be used.

The regex in the Bad_Homepages.squid file is following:
.whatismyip.com($|/$|/?)

So, after running and testing, my squid configuration does not work. When
visiting the homepage and afterwards a sub page, there is no change in
terms of the IP.

Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210604/9afef62f/attachment.htm>

From rousskov at measurement-factory.com  Fri Jun  4 23:10:15 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 4 Jun 2021 19:10:15 -0400
Subject: [squid-users] Cache Peer Issue with URL
In-Reply-To: <CAPeL7PFPEydX8Q4PjoOaVZx7-bnBUNkaXbT+-URe-itoASjWZQ@mail.gmail.com>
References: <CAPeL7PFPEydX8Q4PjoOaVZx7-bnBUNkaXbT+-URe-itoASjWZQ@mail.gmail.com>
Message-ID: <845bf03b-13e2-e5c0-44e5-015a207c040b@measurement-factory.com>

On 6/4/21 4:19 PM, koshik moshik wrote:
> Basically I have created two cashe peers:
> 
> never_direct allow all
> acl ab1 dstdom_regex "/etc/squid/Bad_Homepages.squid" cache_peer
> my.proxy.com <http://my.proxy.com> parent 31112 0 login=user:pw no-query
> name=user cache_peer_access user allow ab1 cache_peer_access user deny
> all acl ab2 proxy_auth userName1 cache_peer myProxyparent 31112 0
> login=user:pwno-query name=user2 cache_peer_access user2 allow ab2
> cache_peer_access user2 deny all
> 
> 
> As you can see, I have a Bad_homepages.squid file which contains regular
> expressions for a website, so this cache_peer will only be used, if the
> HOMEPAGE is requested. If a sub page is requested, the second
> cache_peer(acl ab2) should be used.?
> 
> Example: www.test.com <http://www.test.com> -> ab1 should be used?
> www.test.com/hello <http://www.test.com/hello> -> ab2 should be used.?

Sorry about bad quoting -- Thunderbird is having trouble with your HTML
email...


> The regex in the Bad_Homepages.squid file is following:
> .whatismyip.com($|/$|/?)

It looks like you are using a domain-based dstdom_regex but trying to
match URL paths. If you are trying to detect URLs with non-empty paths,
then you may want to use something like

  acl ab1 urlpath_regex .

or

  acl ab1 urlpath_regex /.

I do not know whether the leading slash is included and the built-in
docs do not say. Others on the list may know the answer or you can find
it using tests.

You can make the regex tighter by excluding repeated "/" characters and
"#", but those are usually unimportant details if you keep in mind that
nothing will work reliably in general because the URL path may contain a
lot of stuff that the origin server may ignore or reinterpret.


Finally, if you want Squid to use either peer A or peer B, then do not
use two different ACLs to direct traffic to them. Use mutually exclusive
tests of one ACL:

  cache_peer_access user allow ab1
  cache_peer_access user deny all

  cache_peer_access user2 deny ab1
  cache_peer_access user2 allow all


Please be mindful of nonhierarchical_direct and encrypted traffic.


HTH,

Alex.




From rousskov at measurement-factory.com  Sat Jun  5 15:08:24 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 5 Jun 2021 11:08:24 -0400
Subject: [squid-users] Cache Peer Issue with URL
In-Reply-To: <CAPeL7PGovmCHh0tB1p78=-OE1NETuaotvG4cPKkU=hC9J8vw4w@mail.gmail.com>
References: <CAPeL7PFPEydX8Q4PjoOaVZx7-bnBUNkaXbT+-URe-itoASjWZQ@mail.gmail.com>
 <845bf03b-13e2-e5c0-44e5-015a207c040b@measurement-factory.com>
 <CAPeL7PGovmCHh0tB1p78=-OE1NETuaotvG4cPKkU=hC9J8vw4w@mail.gmail.com>
Message-ID: <d11ab64a-f931-7a8d-fc7e-d4fb68f6c2fe@measurement-factory.com>

On 6/5/21 11:03 AM, koshik moshik wrote:

> Due to encrypted traffic, is it even possible to get the path for https
> sites?

No, Squid cannot see URL paths of encrypted requests. In an increasing
number of cases, even the exact domain name is not known. Use access.log
to see what URLs Squid sees.

Alex.


> My current squid.conf looks like this and it is not working:?
> 
> http_port 2115
> http_port 2116
> dns_v4_first on
> acl SSL_ports port 1-65535
> acl Safe_ports port 1-65535
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> acl ban_domains dstdomain "/etc/squid/blacklist.txt"
> http_access deny ban_domains
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/.htpasswd
> auth_param basic children 5
> auth_param basic realm Squid Basic Authentication
> auth_param basic credentialsttl 5 hours
> external_acl_type file_userip ipv4 %MYADDR %LOGIN
> /usr/lib/squid/ext_file_userip_acl -f /etc/squid/ip.txt
> external_acl_type file_user %MYPORT %LOGIN
> /usr/lib/squid/ext_file_userip_acl -f /etc/squid/users.txt
> acl password proxy_auth REQUIRED
> acl IP_USER external file_userip
> acl USERS external file_user
> http_access deny !IP_USER
> http_access deny !USERS
> http_access allow password
> http_access deny all
> cache deny all
> never_direct allow all
> 
> 
> 
> 
> acl ab1 urlpath_regex .
> cache_peer my.proxy parent 3128 0 login=user:pw no-query name=user1
> cache_peer_access user1 allow ab1
> cache_peer_access user1 deny all
> 
> 
> 
> 
> acl ab proxy_auth test
> cache_peer my.proxy parent 3128 0 login=user:pw no-query name=user2
> cache_peer_access user2 deny ab1
> cache_peer_access user2 allow all
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> #Rules to anonymize http headers
> forwarded_for off
> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access WWW-Authenticate allow all
> request_header_access Proxy-Authorization allow all
> request_header_access Proxy-Authenticate allow all
> request_header_access Cache-Control allow all
> request_header_access Content-Encoding allow all
> request_header_access Content-Length allow all
> request_header_access Content-Type allow all
> request_header_access Date allow all
> request_header_access Expires allow all
> request_header_access Host allow all
> request_header_access If-Modified-Since allow all
> request_header_access Last-Modified allow all
> request_header_access Location allow all
> request_header_access Pragma allow all
> request_header_access Accept allow all
> request_header_access Accept-Charset allow all
> request_header_access Accept-Encoding allow all
> request_header_access Accept-Language allow all
> request_header_access Content-Language allow all
> request_header_access Mime-Version allow all
> request_header_access Retry-After allow all
> request_header_access Title allow all
> request_header_access Connection allow all
> request_header_access Proxy-Connection allow all
> request_header_access User-Agent allow all
> request_header_access Cookie allow all
> request_header_access All deny all
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern (Release|Packages(.gz)*)$ ? ? ?0 ? ? ? 20% ? ? 2880
> refresh_pattern . 0 20% 4320
> 
> 
> 
> On Sat, Jun 5, 2021 at 1:10 AM Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     On 6/4/21 4:19 PM, koshik moshik wrote:
>     > Basically I have created two cashe peers:
>     >
>     > never_direct allow all
>     > acl ab1 dstdom_regex "/etc/squid/Bad_Homepages.squid" cache_peer
>     > my.proxy.com <http://my.proxy.com> <http://my.proxy.com
>     <http://my.proxy.com>> parent 31112 0 login=user:pw no-query
>     > name=user cache_peer_access user allow ab1 cache_peer_access user deny
>     > all acl ab2 proxy_auth userName1 cache_peer myProxyparent 31112 0
>     > login=user:pwno-query name=user2 cache_peer_access user2 allow ab2
>     > cache_peer_access user2 deny all
>     >
>     >
>     > As you can see, I have a Bad_homepages.squid file which contains
>     regular
>     > expressions for a website, so this cache_peer will only be used,
>     if the
>     > HOMEPAGE is requested. If a sub page is requested, the second
>     > cache_peer(acl ab2) should be used.?
>     >
>     > Example: www.test.com <http://www.test.com> <http://www.test.com
>     <http://www.test.com>> -> ab1 should be used?
>     > www.test.com/hello <http://www.test.com/hello>
>     <http://www.test.com/hello <http://www.test.com/hello>> -> ab2
>     should be used.?
> 
>     Sorry about bad quoting -- Thunderbird is having trouble with your HTML
>     email...
> 
> 
>     > The regex in the Bad_Homepages.squid file is following:
>     > .whatismyip.com <http://whatismyip.com>($|/$|/?)
> 
>     It looks like you are using a domain-based dstdom_regex but trying to
>     match URL paths. If you are trying to detect URLs with non-empty paths,
>     then you may want to use something like
> 
>     ? acl ab1 urlpath_regex .
> 
>     or
> 
>     ? acl ab1 urlpath_regex /.
> 
>     I do not know whether the leading slash is included and the built-in
>     docs do not say. Others on the list may know the answer or you can find
>     it using tests.
> 
>     You can make the regex tighter by excluding repeated "/" characters and
>     "#", but those are usually unimportant details if you keep in mind that
>     nothing will work reliably in general because the URL path may contain a
>     lot of stuff that the origin server may ignore or reinterpret.
> 
> 
>     Finally, if you want Squid to use either peer A or peer B, then do not
>     use two different ACLs to direct traffic to them. Use mutually exclusive
>     tests of one ACL:
> 
>     ? cache_peer_access user allow ab1
>     ? cache_peer_access user deny all
> 
>     ? cache_peer_access user2 deny ab1
>     ? cache_peer_access user2 allow all
> 
> 
>     Please be mindful of nonhierarchical_direct and encrypted traffic.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 



From ben.goz87 at gmail.com  Sun Jun  6 14:09:24 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Sun, 6 Jun 2021 17:09:24 +0300
Subject: [squid-users] Testing eCap module
Message-ID: <CADAqQfw-kxSDutBreDoNsbj4ujWunD2dxPF8GytrS6UTxsvtEg@mail.gmail.com>

By the help of God.
I have an eCap module code that should block traffic on certain cases
and passthru traffic on other cases.
What is the most easy and efficient way to test that module's code is
working as expected?
Thanks,
Ben


From Antony.Stone at squid.open.source.it  Sun Jun  6 14:39:22 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 6 Jun 2021 16:39:22 +0200
Subject: [squid-users] Testing eCap module
In-Reply-To: <CADAqQfw-kxSDutBreDoNsbj4ujWunD2dxPF8GytrS6UTxsvtEg@mail.gmail.com>
References: <CADAqQfw-kxSDutBreDoNsbj4ujWunD2dxPF8GytrS6UTxsvtEg@mail.gmail.com>
Message-ID: <202106061639.22211.Antony.Stone@squid.open.source.it>

On Sunday 06 June 2021 at 16:09:24, Ben Goz wrote:

> I have an eCap module code that should block traffic on certain cases
> and passthru traffic on other cases.
> What is the most easy and efficient way to test that module's code is
> working as expected?

1a. Test some of the cases where traffic should be blocked, and make sure that 
it is.

1b. Then test some of the cases where it should be passed through, and make 
sure that it is.

2. Tell us what "certain cases" and "other cases" are, so that we might be 
able to think of some other way of testing whatever it is you're trying to do.


Summary - the more information you provide, the more readily we can help.


Antony.

-- 
Some mistakes are too much fun to make only once.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Sun Jun  6 14:57:14 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 6 Jun 2021 10:57:14 -0400
Subject: [squid-users] Testing eCap module
In-Reply-To: <CADAqQfw-kxSDutBreDoNsbj4ujWunD2dxPF8GytrS6UTxsvtEg@mail.gmail.com>
References: <CADAqQfw-kxSDutBreDoNsbj4ujWunD2dxPF8GytrS6UTxsvtEg@mail.gmail.com>
Message-ID: <47521950-53d5-ed12-7fa2-0cddbb3d8caa@measurement-factory.com>

On 6/6/21 10:09 AM, Ben Goz wrote:
> By the help of God.
> I have an eCap module code that should block traffic on certain cases
> and passthru traffic on other cases.
> What is the most easy and efficient way to test that module's code is
> working as expected?

Consider writing a script (most likely using a wget or curl client) to
emulate "certain cases" and "other cases". The script can interrogate
Squid and detect that the client got the right response for each test
case. Ideally, use fake origin servers (using "netcat", "busybox httpd",
or similar hacks is often enough).

If your test cases are complex and your testing needs are long-term,
this can evolve in complicated setups using various HTTP(S) and
Squid-control libraries (like the ones used by Squid Project CI) or even
custom eCAP host applications (to remove Squid from the loop), but I
would start simple.


Cheers,

Alex.


From grailsuk at gmail.com  Mon Jun  7 12:04:01 2021
From: grailsuk at gmail.com (Grails UK)
Date: Mon, 7 Jun 2021 13:04:01 +0100
Subject: [squid-users] Limiting Connections & MySQL through SSH Tunnel
Message-ID: <CAMNBN0_VXiD_w96Br=8ghqC8WxieqQWLHrauEB7LonJfnSmckQ@mail.gmail.com>

Hello,
I hope you are well. I have two questions:

1. Is there any easy way to limit concurrent connections by a single squid
user or the local IP the client connected to. I understand there is a
robust framework for limiting max connections via IP address or number of
IP addresses a user can use, however, this is slightly different. I did see
a post that said this is possible through an external ACL, however, what I
struggle with there is how the external ACL would see which concurrent
connections are still active / count the number of current connections to
determine whether a limit is necessary.

2. Our MySQL database is currently only accessible from our local server on
PythonAnywhere and any external access has to be done via an SSH Tunnel, is
there any way to SSH tunnel when using the basic_db_auth or log_db_daemon?

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210607/078d988b/attachment.htm>

From tamurin0525 at gmail.com  Tue Jun  8 04:05:33 2021
From: tamurin0525 at gmail.com (m k)
Date: Tue, 8 Jun 2021 13:05:33 +0900
Subject: [squid-users] about Kerberos Auth and LDAP Auth
Message-ID: <CAL-uOnGr5uGBcOWHLLCXUfbXvKVc-tvbOW28ESmxKV7teknddA@mail.gmail.com>

hi all,

Thank you for always helping me with my difficulties.
With your help I am able to complete the proxy. Please help me again this
time.

I want to configure my squid authentication as follows.

Try single sign-on for squid with Kerberos authentication.
?.
Squid will try authentication with LDAP.

Unfortunately, when Kerberos authentication fails, it retries Kerberos
authentication.
I want squid to work so that if Kerberos authentication fails, it will try
LDAP authentication next.

Can someone please tell me how to do this?

Thanks ,
Kitamura

Translated with www.DeepL.com/Translator (free version)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210608/0f2c19aa/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun  8 09:17:18 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Tue, 08 Jun 2021 21:17:18 +1200
Subject: [squid-users] Limiting Connections & MySQL through SSH Tunnel
In-Reply-To: <CAMNBN0_VXiD_w96Br=8ghqC8WxieqQWLHrauEB7LonJfnSmckQ@mail.gmail.com>
References: <CAMNBN0_VXiD_w96Br=8ghqC8WxieqQWLHrauEB7LonJfnSmckQ@mail.gmail.com>
Message-ID: <d6412def5149dd9259a56addf826423b@treenet.co.nz>

On 2021-06-08 00:04, Grails UK wrote:
> Hello,
> I hope you are well. I have two questions:
> 
> 1. Is there any easy way to limit concurrent connections by a single
> squid user or the local IP the client connected to.

What are you trying to achieve that make you think of doing that?


> 
> 2. Our MySQL database is currently only accessible from our local
> server on PythonAnywhere and any external access has to be done via an
> SSH Tunnel, is there any way to SSH tunnel when using the
> basic_db_auth or log_db_daemon?

Getting TCP/IP data to travel over SSH protocol tunnels is a OS routing 
detail.



Amos


From squid3 at treenet.co.nz  Tue Jun  8 09:44:36 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Tue, 08 Jun 2021 21:44:36 +1200
Subject: [squid-users] about Kerberos Auth and LDAP Auth
In-Reply-To: <CAL-uOnGr5uGBcOWHLLCXUfbXvKVc-tvbOW28ESmxKV7teknddA@mail.gmail.com>
References: <CAL-uOnGr5uGBcOWHLLCXUfbXvKVc-tvbOW28ESmxKV7teknddA@mail.gmail.com>
Message-ID: <77d969af11625c853e284259cfff75f0@treenet.co.nz>

On 2021-06-08 16:05, m k wrote:
> hi all,
> 
> Thank you for always helping me with my difficulties.
> With your help I am able to complete the proxy. Please help me again
> this time.
> 
> I want to configure my squid authentication as follows.
> 
> Try single sign-on for squid with Kerberos authentication.
> 
> Squid will try authentication with LDAP.
> 

Please be aware these are three very different *types* of thing.

  * "Single-Sign On" is just means that the client re-sends the *same 
credentials* to all types of service. Any auth type can be "single-sign 
on" if the client supports it, and this has nothing to do with the 
service(s).

  * Kerberos is an authentication mechanism.

  * LDAP is a database management protocol (like SQL).


> Unfortunately, when Kerberos authentication fails, it retries Kerberos
> authentication.
> I want squid to work so that if Kerberos authentication fails, it will
> try LDAP authentication next.

"LDAP authentication" does not mean what you think.

What squid.conf settings do you have?


Amos


From shadowpilot34 at gmail.com  Tue Jun  8 10:51:20 2021
From: shadowpilot34 at gmail.com (His Shadow)
Date: Tue, 8 Jun 2021 13:51:20 +0300
Subject: [squid-users] Squid modification to only read client SNI without
 bumping.
Message-ID: <CAK7W0xennW+uGE11s7sMN5wZo13J9CcW4hXy+Edd1=XeHwMxXw@mail.gmail.com>

Greetings. I've been trying to make a patch for squid, so that it
could read client hello on connect requests and set the SNI without
using ssl_bump, as that requires generating certificates and is too
complicated for my needs. Here's the patch I've come up with. It seems
to be working, but I'm getting a bunch of connections in CLOSE_WAIT
state after using it under load. I can't seem to reproduce it locally,
but I bet I don't know something, or did something wrong. Can anyone
code check this patch, please? Also, not sure if it's the correct
place to post this. The patch is applicable to the latest release in
4.x series - 4.15.

-- 
HisShadow
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sni.patch
Type: text/x-patch
Size: 14123 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210608/7fb3851d/attachment.bin>

From avinash at indianoillng.in  Tue Jun  8 11:34:49 2021
From: avinash at indianoillng.in (Avinash .)
Date: Tue, 8 Jun 2021 17:04:49 +0530
Subject: [squid-users] Internet is Slow Thru squid proxy server
Message-ID: <CAFGOUwL5CPQMjvN+UD60fja0vK7D-u=rOOejM2pDBobcUzw1JA@mail.gmail.com>

  Dear team, I am using a squid proxy server for 100 + users, but
Internet speed is very slow, I try many method/option but still not able to
resolve the issue

Please find the attached config file & squidclient mgr: info file for
reference.


-- 

Thanks and Regards,


Avinash

Officer IT

IndianOil LNG Pvt Ltd.,

Tel: 044 2796 4593

-- 
Disclaimer
The content of this e-mail is confidential and intended for the 
recipient specified in the message only. It is strictly forbidden to share 
any part of this message with a third party without the sender's written 
consent.?
The information in this electronic message and any attachments to 
this message are intended for the exclusive use of the recipient and may 
contain proprietary, confidential, or privileged information.
If you are 
not the intended recipient, you should not disseminate, distribute or copy 
this e-mail. Please notify the sender immediately and destroy all copies of 
this message and any attachments.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210608/bad9f6dd/attachment.htm>
-------------- next part --------------
squidclient mgr:info
HTTP/1.1 200 OK
Server: squid/4.10
Mime-Version: 1.0
Date: Tue, 08 Jun 2021 10:41:24 GMT
Content-Type: text/plain;charset=utf-8
Expires: Tue, 08 Jun 2021 10:41:24 GMT
Last-Modified: Tue, 08 Jun 2021 10:41:24 GMT
X-Cache: MISS from iolpl-Virtual-Machine
X-Cache-Lookup: MISS from iolpl-Virtual-Machine:3128
Via: 1.1 iolpl-Virtual-Machine (squid/4.10)
Connection: close

Squid Object Cache: Version 4.10
Build Info: Ubuntu linux
Service Name: squid
Start Time: Tue, 08 Jun 2021 10:40:49 GMT
Current Time: Tue, 08 Jun 2021 10:41:24 GMT
Connection information for squid:
Number of clients accessing cache: (client_db off)
Number of HTTP requests received: 149
Number of ICP messages received: 0
Number of ICP messages sent: 0
Number of queued ICP replies: 0
Number of HTCP messages received: 0
Number of HTCP messages sent: 0
Request failure ratio: 0.00
Average HTTP requests per minute since start: 261.7
Average ICP messages per minute since start: 0.0
Select loop called: 18746 times, 1.823 ms avg
Cache information for squid:
Hits as % of all requests: 5min: 0.0%, 60min: 0.0%
Hits as % of bytes sent: 5min: -0.0%, 60min: -0.0%
Memory hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
Storage Swap size: 10544 KB
Storage Swap capacity: 0.3% used, 99.7% free
Storage Mem size: 216 KB
Storage Mem capacity: 0.0% used, 100.0% free
Mean Object Size: 24.93 KB
Requests given to unlinkd: 0
Median Service Times (seconds) 5 min 60 min:
HTTP Requests (All): 0.00000 0.00000
Cache Misses: 0.00000 0.00000
Cache Hits: 0.00000 0.00000
Near Hits: 0.00000 0.00000
Not-Modified Replies: 0.00000 0.00000
DNS Lookups: 0.00000 0.00000
ICP Queries: 0.00000 0.00000
Resource usage for squid:
UP Time: 34.168 seconds
CPU Time: 0.622 seconds
CPU Usage: 1.82%
CPU Usage, 5 minute avg: 0.00%
CPU Usage, 60 minute avg: 0.00%
Maximum Resident Size: 120752 KB
Page faults with physical i/o: 0
Memory accounted for:
Total accounted: 2809 KB
memPoolAlloc calls: 1157
memPoolFree calls: 27502
File descriptor usage for squid:
Maximum number of file descriptors: 4096
Largest file desc currently in use: 273
Number of file desc currently in use: 266
Files queued for open: 0
Available number of file descriptors: 3830
Reserved number of file descriptors: 100
Store Disk files open: 0
Internal Data Structures:
476 StoreEntries
53 StoreEntries with MemObjects
0 Hot Object Cache Items
423 on-disk objects

From squid3 at treenet.co.nz  Tue Jun  8 11:36:14 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Tue, 08 Jun 2021 23:36:14 +1200
Subject: [squid-users] Squid modification to only read client SNI
 without bumping.
In-Reply-To: <CAK7W0xennW+uGE11s7sMN5wZo13J9CcW4hXy+Edd1=XeHwMxXw@mail.gmail.com>
References: <CAK7W0xennW+uGE11s7sMN5wZo13J9CcW4hXy+Edd1=XeHwMxXw@mail.gmail.com>
Message-ID: <7a9b5c791fbbe39041f19d31b8694779@treenet.co.nz>

On 2021-06-08 22:51, His Shadow wrote:
> Greetings. I've been trying to make a patch for squid,

Code changes should be discussed on the squid-dev mailing list.

FWIW, we (Squid devs) have already discussed this functionality change 
and I have a TODO list entry (far down sadly) of supporting your 
use-case. The way I think to approach it though is to start with the 
configuration parser. A simple peek-splice/terminate TLS traffic flow 
should not need certificates setup by admin.

If you want to pickup that TODO item please contact squid-dev to plan 
out the actual best approach with the other dev working on Squid crypto 
code.

Patch submission should be done by submitting a github PR targeted at 
our repository 'master' branch.


> so that it
> could read client hello on connect requests and set the SNI without
> using ssl_bump, as that requires generating certificates and is too
> complicated for my needs.

Should not be too complicated. We have test scripts available that can 
generate fake cert and CA for the *_port config settings. Or snakeoil 
certs can be used.

Apart from the port settings what your patch does is just this:


  acl blocklist dstdomain ...

  ssl_bump peek all
  ssl_bump splice blocklist
  ssl_bump terminate all



Amos


From chogger at gmail.com  Tue Jun  8 11:43:21 2021
From: chogger at gmail.com (Frank Schichterich)
Date: Tue, 8 Jun 2021 13:43:21 +0200
Subject: [squid-users] TCP Connection to parent failed
Message-ID: <CAL0b9hRKftu2Wv=e7XoX+OC3sLquy1qtTtKQbuoYvfXf7GR08A@mail.gmail.com>

Hello Squid users,

i am running multiple squid servers in our company. We have around 5k
current connections per proxy during work hours.
I recently switched two Server from Squid 3.5.28 to Squid 4.15.

The moment i did this my cache.log got filled with "TCP connection to
parent-proxy.de/80 failed" entries. Approximately 50 entries per hour. But
the proxy seems to run normally.
The proxies running with version 3.5.28 dont show any tcp connection errors
(to the exact same parent proxy).

I don't have any information about the parent proxies. They are part of our
"provider".

Is there anything i can do / change?

Greetings Frank
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210608/dcf5a654/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun  8 11:45:48 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Tue, 08 Jun 2021 23:45:48 +1200
Subject: [squid-users] Internet is Slow Thru squid proxy server
In-Reply-To: <CAFGOUwL5CPQMjvN+UD60fja0vK7D-u=rOOejM2pDBobcUzw1JA@mail.gmail.com>
References: <CAFGOUwL5CPQMjvN+UD60fja0vK7D-u=rOOejM2pDBobcUzw1JA@mail.gmail.com>
Message-ID: <b5bee86117b8f70a9b7f397561138fa1@treenet.co.nz>

On 2021-06-08 23:34, Avinash . wrote:
> Dear team, I am using a squid proxy server for 100 + users, but
> Internet speed is very slow, I try many method/option but still not
> able to resolve the issue
> 
> Please find the attached config file & squidclient mgr: info file for
> reference.
> 

The mgr:info log says Squid started less than a minute ago and served 
149 requests total. That is not sufficient time nor traffic to tell how 
fast the proxy is.


Perhapse you should tell us what you have tried, and what results that 
produced (no matter how small a change).

Amos


From shadowpilot34 at gmail.com  Tue Jun  8 13:31:21 2021
From: shadowpilot34 at gmail.com (His Shadow)
Date: Tue, 8 Jun 2021 16:31:21 +0300
Subject: [squid-users] Squid modification to only read client SNI
 without bumping.
Message-ID: <CAK7W0xfDsU8OUR=-Fg3O9ZKxnD9VK7qr-7+dRqEs=kJWtXjoMQ@mail.gmail.com>

Could you direct me to those scripts? Also, am I understanding
correctly that in this mode:
acl blocklist dstdomain ...

ssl_bump peek all
ssl_bump splice blocklist
ssl_bump terminate all

I will only need certs to display an error page from squid via ssl,
but unblocked domains should be just fine?
I think it should be
ssl_bump splice !blocklist
Since blocklist is the list of domains that needs blocking, so we
don't need to splice them. Oh, and one more thing, wouldn't dstdomain
match something that was sent in the CONNECT request itself, instead
of the SNI in the client hello if it is present?

-- 
HisShadow


From sovrajn at gmail.com  Tue Jun  8 13:43:49 2021
From: sovrajn at gmail.com (Jim Freeman)
Date: Tue, 8 Jun 2021 07:43:49 -0600
Subject: [squid-users] tarpit, silent-drop vs. DDoS ?
Message-ID: <CAGYW4nkaJ==PAq4C62EjG1Z5P20LyyyVxYRXNLqETQShcFp5BQ@mail.gmail.com>

I've scoured docs and Google for DDoS/security mechanisms, and hope I
have the lay of the land.

But I've not yet seen anything mentioned like HAProxy's
tarpit/silent-drop mechanisms :
https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#4.2-http-request%20tarpit
 ... blocks the request without responding for a delay specified ...
https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#4.2-http-request%20silent-drop
 ... can resist much higher loads than "tarpit", and slow down
stronger attackers. ...

Does anyone have these kinds of countermeasures in play with squid ?
[ I'm using squid 3.5.20 ]

Thanks,
...jfree


From rousskov at measurement-factory.com  Tue Jun  8 14:22:03 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jun 2021 10:22:03 -0400
Subject: [squid-users] Squid modification to only read client SNI
 without bumping.
In-Reply-To: <7a9b5c791fbbe39041f19d31b8694779@treenet.co.nz>
References: <CAK7W0xennW+uGE11s7sMN5wZo13J9CcW4hXy+Edd1=XeHwMxXw@mail.gmail.com>
 <7a9b5c791fbbe39041f19d31b8694779@treenet.co.nz>
Message-ID: <a5fb3029-1295-a3fd-9402-114351bcc7a0@measurement-factory.com>

On 6/8/21 7:36 AM, squid3 at treenet.co.nz wrote:

> The way I think to approach it though is to start with the
> configuration parser.

That starting point does not compute for me. We do need to agree on how
to configure this feature, but parsing any resulting Squid configuration
ought to be very straightforward. Perhaps you have meant "TLS
ClientHello parser", but Squid already has that.


> A simple peek-splice/terminate TLS traffic flow
> should not need certificates setup by admin.

Squid already does not generate/use certificates for splicing or
terminating connections. In splice-or-terminate use cases, the
certificates come into play only when delivery _errors_. A feature to
prevent bumping for error delivery (and remove any configuration
requirements for CA certificate) should be welcomed IMO.

Please drop squid-users if responding to this email.

Alex.


From rousskov at measurement-factory.com  Tue Jun  8 14:31:20 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jun 2021 10:31:20 -0400
Subject: [squid-users] tarpit, silent-drop vs. DDoS ?
In-Reply-To: <CAGYW4nkaJ==PAq4C62EjG1Z5P20LyyyVxYRXNLqETQShcFp5BQ@mail.gmail.com>
References: <CAGYW4nkaJ==PAq4C62EjG1Z5P20LyyyVxYRXNLqETQShcFp5BQ@mail.gmail.com>
Message-ID: <4171db92-9628-e1b9-3973-b2fdd9ceb9e7@measurement-factory.com>

On 6/8/21 9:43 AM, Jim Freeman wrote:
> I've scoured docs and Google for DDoS/security mechanisms, and hope I
> have the lay of the land.
> 
> But I've not yet seen anything mentioned like HAProxy's
> tarpit/silent-drop mechanisms :
> https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#4.2-http-request%20tarpit
>  ... blocks the request without responding for a delay specified ...
> https://cbonte.github.io/haproxy-dconv/2.2/configuration.html#4.2-http-request%20silent-drop
>  ... can resist much higher loads than "tarpit", and slow down
> stronger attackers. ...
> 
> Does anyone have these kinds of countermeasures in play with squid ?

Squid supports resetting the TCP connection instead of delivering an
error page (look for "TCP_RESET" and "ssl_bump terminate" in
squid.conf.documented). An artificial delay can be created by a simple
external ACL (and, if such delays are popular, we can add a new built-in
ACL type). In your particular use case, the http_access directive can
probably be used to tie TCP_RESET and delay logic together.

Alex.


From alleinerwolf at gmail.com  Tue Jun  8 15:36:46 2021
From: alleinerwolf at gmail.com (Alex Irmel Oviedo Solis)
Date: Tue, 8 Jun 2021 10:36:46 -0500
Subject: [squid-users] Problems with websockets
Message-ID: <CAKZ77-8PSMwTLakvWEDL+3XDwr6Z8GcpDd9wBiRLGuhjfQNxXg@mail.gmail.com>

Hello all, I'm having problems with squid 4.11 on RHEL 8.4. I was trying to
access to whatsapp with no luck, but I'm currently to test with
https://www.websocket.org/echo.html,  the errors in both cases are the same
(400 Bad Request).

My squid.conf is in https://paste.centos.org/view/b98e8510
My cache.log is in https://paste.centos.org/view/a2b6ac81
My access.lorg is in https://paste.centos.org/view/eef2180a

Thanks in advance

-- 
*"Una alegr?a compartida se transforma en doble alegr?a; una pena
compartida, en media pena."*
--> http://www.alexove.me <http://www.alexove.me/>
--> Celular (Movistar): +51-959-625-001
--> Sigueme en Twitter: http://twitter.com/alexove_pe
--> Perfil: http://fedoraproject.org/wiki/user:alexove
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210608/eca0a280/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun  8 15:45:22 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jun 2021 11:45:22 -0400
Subject: [squid-users] Problems with websockets
In-Reply-To: <CAKZ77-8PSMwTLakvWEDL+3XDwr6Z8GcpDd9wBiRLGuhjfQNxXg@mail.gmail.com>
References: <CAKZ77-8PSMwTLakvWEDL+3XDwr6Z8GcpDd9wBiRLGuhjfQNxXg@mail.gmail.com>
Message-ID: <121d5cbe-1f12-9788-78db-e93e9c40d434@measurement-factory.com>

On 6/8/21 11:36 AM, Alex Irmel Oviedo Solis wrote:
> Hello all, I'm having problems with squid 4.11 on RHEL 8.4. I was trying
> to access to whatsapp with no luck, but I'm currently to test with
> https://www.websocket.org/echo.html, ?the errors in both cases are

> http.cc(723) processReplyHeader: HTTP Server RESPONSE:
> HTTP/1.1 400 WebSocket Upgrade Failure

Squid v4 does not fully support HTTP Upgrade (it drops it). You should
splice connections to websocket services or use
http_upgrade_request_protocols available in Squid v5.

HTH,

Alex.
P.S. Thank you for providing detailed triage information!


> My squid.conf is in https://paste.centos.org/view/b98e8510
> My cache.log is in https://paste.centos.org/view/a2b6ac81
> My access.lorg is in https://paste.centos.org/view/eef2180a


From alleinerwolf at gmail.com  Tue Jun  8 15:55:38 2021
From: alleinerwolf at gmail.com (Alex Irmel Oviedo Solis)
Date: Tue, 8 Jun 2021 10:55:38 -0500
Subject: [squid-users] Problems with websockets
In-Reply-To: <121d5cbe-1f12-9788-78db-e93e9c40d434@measurement-factory.com>
References: <CAKZ77-8PSMwTLakvWEDL+3XDwr6Z8GcpDd9wBiRLGuhjfQNxXg@mail.gmail.com>
 <121d5cbe-1f12-9788-78db-e93e9c40d434@measurement-factory.com>
Message-ID: <CAKZ77-9LYSHJWsugPWt-2GDJjMoTjbYoeEZzKA+JH_bDm-oorA@mail.gmail.com>

precisely, I have been trying to do it by placing the rules from line 86 to
line 91 in squid.conf

El mar, 8 de jun. de 2021 a la(s) 10:45, Alex Rousskov (
rousskov at measurement-factory.com) escribi?:

> On 6/8/21 11:36 AM, Alex Irmel Oviedo Solis wrote:
> > Hello all, I'm having problems with squid 4.11 on RHEL 8.4. I was trying
> > to access to whatsapp with no luck, but I'm currently to test with
> > https://www.websocket.org/echo.html,  the errors in both cases are
>
> > http.cc(723) processReplyHeader: HTTP Server RESPONSE:
> > HTTP/1.1 400 WebSocket Upgrade Failure
>
> Squid v4 does not fully support HTTP Upgrade (it drops it). You should
> splice connections to websocket services or use
> http_upgrade_request_protocols available in Squid v5.
>
> HTH,
>
> Alex.
> P.S. Thank you for providing detailed triage information!
>
>
> > My squid.conf is in https://paste.centos.org/view/b98e8510
> > My cache.log is in https://paste.centos.org/view/a2b6ac81
> > My access.lorg is in https://paste.centos.org/view/eef2180a
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
*"Una alegr?a compartida se transforma en doble alegr?a; una pena
compartida, en media pena."*
--> http://www.alexove.me <http://www.alexove.me/>
--> Celular (Movistar): +51-959-625-001
--> Sigueme en Twitter: http://twitter.com/alexove_pe
--> Perfil: http://fedoraproject.org/wiki/user:alexove
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210608/bf99f33a/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun  8 16:02:47 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jun 2021 12:02:47 -0400
Subject: [squid-users] TCP Connection to parent failed
In-Reply-To: <CAL0b9hRKftu2Wv=e7XoX+OC3sLquy1qtTtKQbuoYvfXf7GR08A@mail.gmail.com>
References: <CAL0b9hRKftu2Wv=e7XoX+OC3sLquy1qtTtKQbuoYvfXf7GR08A@mail.gmail.com>
Message-ID: <622a95ad-99d4-1941-1096-5d25923cd6b0@measurement-factory.com>

On 6/8/21 7:43 AM, Frank Schichterich wrote:

> i am running multiple squid servers in our company. We have around 5k
> current connections per proxy during work hours.
> I recently switched two Server from Squid 3.5.28 to Squid 4.15.
> 
> The moment i did this my cache.log got filled with "TCP connection to
> parent-proxy.de/80 failed"
> entries.?Approximately 50 entries per hour. But the proxy seems to run
> normally.?
> The proxies running with version 3.5.28 dont show any tcp connection
> errors (to the exact same parent proxy).

Squid v3 does not log a lot of things that newers Squids log. However,
the message in question is present in v3.5 as well, which increases the
probability (but does not guarantee) that you are dealing with a problem
that should be addressed rather than ignored.


> I don't have any information about the parent proxies. They are part of
> our "provider".


> Is there anything i can do / change?

Probably. I recommend these steps:

  1. Figure out _why_ these connections fail.
  2. Either fix the problem or quell the messages about it.

The first step probably requires analyzing debugging cache.logs and/or
packet traces to determine the reason behind those closures. It could be
idle connection timeouts, capacity limits, access control violations, or
even Squid bugs.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Jun  8 16:08:56 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Jun 2021 12:08:56 -0400
Subject: [squid-users] Problems with websockets
In-Reply-To: <CAKZ77-9LYSHJWsugPWt-2GDJjMoTjbYoeEZzKA+JH_bDm-oorA@mail.gmail.com>
References: <CAKZ77-8PSMwTLakvWEDL+3XDwr6Z8GcpDd9wBiRLGuhjfQNxXg@mail.gmail.com>
 <121d5cbe-1f12-9788-78db-e93e9c40d434@measurement-factory.com>
 <CAKZ77-9LYSHJWsugPWt-2GDJjMoTjbYoeEZzKA+JH_bDm-oorA@mail.gmail.com>
Message-ID: <7f9b44bd-3706-d066-64a1-300657bee828@measurement-factory.com>

On 6/8/21 11:55 AM, Alex Irmel Oviedo Solis wrote:
> I have been trying to do it by placing the rules from line 86
> to line 91 in squid.conf

acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/etc/squid/acl.url.nobump"

ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all

I wonder if your acl.url.nobump regexes do not match step2 CONNECT URIs.
The current cache.log snippet does not show that detail. Consider
posting more detailed logs that show ACL matching attempts (e.g., "ALL,3
28,7"?).

Alex.


> El mar, 8 de jun. de 2021 a la(s) 10:45, Alex Rousskov
> (rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>) escribi?:
> 
>     On 6/8/21 11:36 AM, Alex Irmel Oviedo Solis wrote:
>     > Hello all, I'm having problems with squid 4.11 on RHEL 8.4. I was
>     trying
>     > to access to whatsapp with no luck, but I'm currently to test with
>     > https://www.websocket.org/echo.html
>     <https://www.websocket.org/echo.html>, ?the errors in both cases are
> 
>     > http.cc(723) processReplyHeader: HTTP Server RESPONSE:
>     > HTTP/1.1 400 WebSocket Upgrade Failure
> 
>     Squid v4 does not fully support HTTP Upgrade (it drops it). You should
>     splice connections to websocket services or use
>     http_upgrade_request_protocols available in Squid v5.
> 
>     HTH,
> 
>     Alex.
>     P.S. Thank you for providing detailed triage information!
> 
> 
>     > My squid.conf is in https://paste.centos.org/view/b98e8510
>     <https://paste.centos.org/view/b98e8510>
>     > My cache.log is in https://paste.centos.org/view/a2b6ac81
>     <https://paste.centos.org/view/a2b6ac81>
>     > My access.lorg is in https://paste.centos.org/view/eef2180a
>     <https://paste.centos.org/view/eef2180a>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> -- 
> //"Una alegr?a compartida se transforma en doble alegr?a; una pena
> compartida, en media pena."//
> -->?http://www.alexove.me <http://www.alexove.me/>
> --> Celular (Movistar): +51-959-625-001
> --> Sigueme en Twitter:?http://twitter.com/alexove_pe
> <http://twitter.com/alexove_pe>
> --> Perfil:?http://fedoraproject.org/wiki/user:alexove
> <http://fedoraproject.org/wiki/user:alexove>



From squid.org at bloms.de  Wed Jun  9 12:25:59 2021
From: squid.org at bloms.de (Dieter Bloms)
Date: Wed, 9 Jun 2021 14:25:59 +0200
Subject: [squid-users] Is it possible to force some dstdomain to ipv4
 protocol without define an outgoing ip address ?
Message-ID: <20210609122559.en7srd32v5nic3pz@bloms.de>

Hello,

I use squid 4.15 and want to configure it to connect to some destinations
via IPv4.

I know about the tcp_outgoing_address option, but my outgoing ipv4 and
ipv6 addresses changes every day.

So is there an option like:

acl myipv4onlydest dstdomain .example1.com .example2.com
tcp_outgoing_protocol ipv4 myipv4onlydest


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From matthias at saou.eu  Wed Jun  9 14:04:09 2021
From: matthias at saou.eu (Matthias Saou)
Date: Wed, 9 Jun 2021 16:04:09 +0200
Subject: [squid-users] Issues with SSLBumped high traffic forward caching
Message-ID: <20210609160409.2278fd90@r2d2.marmotte.net>

Hi,

We have a fairly simple (in theory) use case where we have a bunch of
headless Chromium browsers connecting to websites on the Internet
through various geo-specific proxies. To speed things up, we'd like to
add a caching layer, since it's perfectly acceptable for us to honor all
max-age/expires/etc. headers for all of the accessed content.

Nearly all accesses use https, so we've had to implement SSLBump, and
we went with squid 5. That part seems to work well enough.

We initially went with multiple servers configured as cache peers, but
since we've been seeing a lot of different problems, we're now focusing
on a single squid 5.0.6 server.

It has 128GB RAM, a 16 core EPYC CPU, 3TB+ of NVMe storage and 1Gbps
Internet bandwidth, which we'd obviously like to use as much as
possible.

What we have configured is:
* Multiple http_port with a cache_peer each to access remote geo
  specific proxies. We've had to rebuild with -DMAXTCPLISTENPORTS=512
  to increase the 128 default. Example (sorry for the line breaks):

acl port_usa1 localport 21083
http_port 21083 ssl-bump cert=/etc/squid/ssl_cert/myCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=32MB
cache_peer 198.51.100.66 parent 443 3130 no-query no-digest no-delay \
  name=usa1 cache_peer_access usa1 allow port_usa1

* Simple SSLBump setup, where we don't check origins for cached objects
  to avoid the added latency, use a local CA and have Chromium
  configured to ignore all SSL/TLS mismatches:

sslcrtd_program /usr/lib64/squid/security_file_certgen -s \
  /var/cache/squid/ssl_db -M 32MB
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump client-first

* Memory and disk cache to try and use resources as much as possible:

workers 4
cache_mem 81920 MB
memory_cache_shared on
shared_transient_entries_limit 65536
minimum_object_size 0 KB
maximum_object_size 20 MB
maximum_object_size_in_memory 2048 KB
#cache_dir rock /var/spool/squid 3453640 
max_filedescriptors 16384

We've tried a lot of other configuration options, read a lot of
documentation, but we're still getting a lot of errors in the logs.
Here are the most worrying:

assertion failed: Transients.cc:221: "old == e"

When that "assertion failed" happens, the kid dies and a new one gets
forked in its place. We can see that happen multiple times per minute.

ERROR: Collapsed forwarding queue overflow for kid1 at 1024 items

This one seems to be impossible for us to track down. It doesn't show
up immediately, but always ends up coming back, and can be multiple
times per second when we have a high usage peak. We've tried:
 * Enabling/disabling "collapsed_forwarding", nothing changes. It should
   be off by default, but this message is there nevertheless.
 * Recompiling squid with the value raised to 4096. Same message with
   the new value.
 * Disabling the "cache_dir rock". It seems to then take longer to
   appear, but does ultimately appear again.

Could anyone provide pointers on how to track down what could be
causing these two errors? We can provide configuration, logs, traces
and dumps as needed.

Cheers,
Matthias

-- 
            Matthias Saou                  ??          ??
                                             ??      ??
Web: http://matthias.saou.eu/              ??????????????
Mail/XMPP:  matthias at saou.eu             ????  ??????  ????
                                       ??????????????????????
GPG: 4096R/E755CC63                    ??  ??????????????  ??
     8D91 7E2E F048 9C9C 46AF          ??  ??          ??  ??
     21A9 7A51 7B82 E755 CC63                ????  ????


From rousskov at measurement-factory.com  Wed Jun  9 14:45:40 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Jun 2021 10:45:40 -0400
Subject: [squid-users] Issues with SSLBumped high traffic forward caching
In-Reply-To: <20210609160409.2278fd90@r2d2.marmotte.net>
References: <20210609160409.2278fd90@r2d2.marmotte.net>
Message-ID: <60f0cd10-b036-9de4-85ab-98371817854d@measurement-factory.com>

On 6/9/21 10:04 AM, Matthias Saou wrote:
> on a single squid 5.0.6 server.

> assertion failed: Transients.cc:221: "old == e"

This is a Squid bug. Please consider creating a Bugzilla entry and
posting the corresponding backtrace there:
https://wiki.squid-cache.org/SquidFaq/BugReporting#crashes_and_core_dumps


> ERROR: Collapsed forwarding queue overflow for kid1 at 1024 items

This can be an expected side effect of a kid restart or a bug. If these
errors go away shortly after the kid, then they are expected -- other
kids are trying to communicate with a dead/restarting kid and complain
when they cannot. Squid handling of kid restarts is very primitive.
Quality improvements are welcome.


> This one seems to be impossible for us to track down. It doesn't show
> up immediately, but always ends up coming back, and can be multiple
> times per second when we have a high usage peak. We've tried:
>  * Enabling/disabling "collapsed_forwarding", nothing changes. It should
>    be off by default, but this message is there nevertheless.

The message text is misleading because it is outdated. That queue was
originally used exclusively for collapsed forwarding needs, but it is
now used for other essential purposes as well -- cachable traffic should
be using the transients queue now.


>  * Recompiling squid with the value raised to 4096. Same message with
>    the new value.

If the overflow is related to kid crashes, then raising the limit like
that may not help on a busy proxy -- the queue will just overflow a few
seconds later. The solution is to fix those crashes.

If the overflow occurs without crashes, then there may be a bug
somewhere or your Squids are blocking/stalling for some reason.


>  * Disabling the "cache_dir rock". It seems to then take longer to
>    appear, but does ultimately appear again.

Rock caches are not the only users of the transients queue. Memory cache
uses it as well.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jun  9 16:05:40 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Jun 2021 12:05:40 -0400
Subject: [squid-users] Is it possible to force some dstdomain to ipv4
 protocol without define an outgoing ip address ?
In-Reply-To: <20210609122559.en7srd32v5nic3pz@bloms.de>
References: <20210609122559.en7srd32v5nic3pz@bloms.de>
Message-ID: <3e6ea9f4-5fbb-8fc2-7900-4948251d9afc@measurement-factory.com>

On 6/9/21 8:25 AM, Dieter Bloms wrote:
> Hello,
> 
> I use squid 4.15 and want to configure it to connect to some destinations
> via IPv4.
> 
> I know about the tcp_outgoing_address option, but my outgoing ipv4 and
> ipv6 addresses changes every day.
> 
> So is there an option like:
> 
> acl myipv4onlydest dstdomain .example1.com .example2.com
> tcp_outgoing_protocol ipv4 myipv4onlydest

Not that I know of. You can implement this logic inside a custom DNS
resolver script, or you can reconfigure Squid whenever your outgoing
addresses change, but I understand that you are looking for a better
solution.

Alex.


From andreas.weigel at securepoint.de  Wed Jun  9 19:29:22 2021
From: andreas.weigel at securepoint.de (Andreas Weigel)
Date: Wed, 09 Jun 2021 19:29:22 +0000
Subject: [squid-users] Squid spliced TLS handshake failing with chrome/ium
 fallback for certain servers
Message-ID: <20210609192922.Horde.e5blxJcNidWpbfVZ3h05Ryd@webmail.intern.securepoint.de>

Hi everyone,

I stumbled upon a case of someone complaining about a site not being  
reachable via squid. Squid was running as transparent proxy and  
splicing TLS connections.

Squid reported a TLS handshake error to the client  
(SQUID_ERR_SSL_HANDSHAKE; Handshake with SSL server failed:  
error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake  
failure).

After some wireshark-digging, I found that the site in question (for  
reference: www.klebl.net) aborted the handshake, if the Client Hello's  
"Extension: signature_algorithms" did not contain

   Signature Algorithm: rsa_pkcs1_sha1 (0x0201)
       Signature Hash Algorithm Hash: SHA1 (2)
       Signature Hash Algorithm Signature: RSA (1)

though it was perfectly capable of using SHA384 (which was negotiated  
with clients that did include sha1 to their signature algorithms). The  
point is, that chrome seems to have adopted a "fallback" approach  
since version 82 (see  
https://groups.google.com/u/1/a/chromium.org/g/blink-dev/c/yaJcs4p9LNI/m/haZWzX-UBwAJ?pli=1), that first tries to connect without said signature algorithm in its extension and only if that fails does a second handshake with SHA1 included. However, squid see and reports the error on the first attempt back to the client, so that the TLS-connection setup  
fails.

While this seems to be something that probably should be fixed at the  
server side, it does lead users to be believe Squid is at fault.

I tried bypassing TLS-handshake errors using

acl handshake_failure ssl_error SQUID_ERR_SSL_HANDSHAKE
sslproxy_cert_error allow handshake_failure

but this did not change anything (maybe sslproxy_cert_error only works  
for certificate verification, but not for the handshake?)

For the records, other clients, like firefox or openssl s_client  
(Version 1.1.1k) do not have any issues, because their Client Hello  
contains said signature algorithm. Please also note that sites like  
ssllabs do not hint that there is too much wrong with the site (apart  
from still allowing TLS 1.0, and TLS1.2 while disallowing TLS1.1).

The only other client that did not work (squid or no squid) for me was  
openssl s_client Version 1.1.1f (Ubuntu 20.04), I could not even force  
it to include SHA1 into the extensions via --sigalgs  
"RSA+SHA1:RSA+SHA256".

Any suggestions on how to allow the first TLS-connection failure of  
chrome to be passed directly back to client instead of erroeing out,  
or any other ways to fix this problem?

Andreas

PS: Some further reading on why this may cause the server to abort the  
handshake at all:  
https://github.com/openssl/openssl/issues/11438#issuecomment-606927855



From ambrose.li at gmail.com  Wed Jun  9 22:16:52 2021
From: ambrose.li at gmail.com (Ambrose Li)
Date: Wed, 9 Jun 2021 18:16:52 -0400
Subject: [squid-users] custom DNS resolver scripts? (was: Re: Is it possible
 to force some dstdomain to ipv4) protocol without define an outgoing ip
 address ?
In-Reply-To: <3e6ea9f4-5fbb-8fc2-7900-4948251d9afc@measurement-factory.com>
References: <20210609122559.en7srd32v5nic3pz@bloms.de>
 <3e6ea9f4-5fbb-8fc2-7900-4948251d9afc@measurement-factory.com>
Message-ID: <20210609221652.GA12481@pingviini>

On Wed, Jun 09, 2021 at 12:05:40PM -0400, Alex Rousskov wrote:
> 
> Not that I know of. You can implement this logic inside a custom DNS
> resolver script, or you can reconfigure Squid whenever your outgoing
> addresses change, but I understand that you are looking for a better
> solution.

What are the current recommendations for custom DNS resolver scripts?
I've not upgraded squid ever since the custom-dns-program option was
discontinued. I'd be very interested in knowing what replaced it since
I do want to switch to a current, supported version.

Thanks in advance,

-- 
Ambrose Li <ambrose.li at gmail.com> | Time zone: GMT-4 (Eastern)
ambroseli.ca

?We need to put ourselves into positions where we are comfortable for
 being uncomfortable? ? Gaby Brink [AIGA Gain, 2012]



From rousskov at measurement-factory.com  Wed Jun  9 23:42:09 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Jun 2021 19:42:09 -0400
Subject: [squid-users] custom DNS resolver scripts? (was: Re: Is it
 possible to force some dstdomain to ipv4) protocol without define an
 outgoing ip address ?
In-Reply-To: <20210609221652.GA12481@pingviini>
References: <20210609122559.en7srd32v5nic3pz@bloms.de>
 <3e6ea9f4-5fbb-8fc2-7900-4948251d9afc@measurement-factory.com>
 <20210609221652.GA12481@pingviini>
Message-ID: <da2e3de0-b176-a6b7-dea6-45364421d049@measurement-factory.com>

On 6/9/21 6:16 PM, Ambrose Li wrote:
> On Wed, Jun 09, 2021 at 12:05:40PM -0400, Alex Rousskov wrote:
>> Not that I know of. You can implement this logic inside a custom DNS
>> resolver script, or you can reconfigure Squid whenever your outgoing
>> addresses change, but I understand that you are looking for a better
>> solution.

> What are the current recommendations for custom DNS resolver scripts?

I was talking about a custom script that implements a custom DNS
resolver. With modern libraries, it takes a few lines of code to write a
basic one. This kind of resolver does not resolve most of the names, but
forwards queries to another/real resolver, adapting the queries and/or
the answers as needed.

For an oversimplified example, such a script can respond to all AAAA
queries (with answers containing no records) while forwarding all A
queries to a "real" resolver.

Eventually, Squid will support the logic you need internally, but that
requires development.


> I've not upgraded squid ever since the custom-dns-program option was
> discontinued. I'd be very interested in knowing what replaced it since
> I do want to switch to a current, supported version.

I do not even remember what custom-dns-program did exactly, but these
days Squid contains a built-in basic DNS client that talks to DNS
resolver(s) at the address(es) specified by the dns_nameservers directive.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jun  9 23:43:38 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 9 Jun 2021 19:43:38 -0400
Subject: [squid-users] Squid spliced TLS handshake failing with
 chrome/ium fallback for certain servers
In-Reply-To: <20210609192922.Horde.e5blxJcNidWpbfVZ3h05Ryd@webmail.intern.securepoint.de>
References: <20210609192922.Horde.e5blxJcNidWpbfVZ3h05Ryd@webmail.intern.securepoint.de>
Message-ID: <c19c5ef3-aac7-e8e4-832c-e1ba2623fb2c@measurement-factory.com>

On 6/9/21 3:29 PM, Andreas Weigel wrote:
> I stumbled upon a case of someone complaining about a site not being
> reachable via squid. Squid was running as transparent proxy and splicing
> TLS connections.


> Squid reported a TLS handshake error to the client
> (SQUID_ERR_SSL_HANDSHAKE; Handshake with SSL server failed:
> error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure).

If you are _splicing_ connections, then Squid should not be a part of
the TLS handshake at all. If Squid is a part of the TLS handshake in an
ssl_bump peek+splice configuration, then this is a Squid bug or limitation.


> After some wireshark-digging, I found that the site in question (for
> reference: www.klebl.net) aborted the handshake, if the Client Hello's
> "Extension: signature_algorithms" did not contain
> 
> ? Signature Algorithm: rsa_pkcs1_sha1 (0x0201)
> ????? Signature Hash Algorithm Hash: SHA1 (2)
> ????? Signature Hash Algorithm Signature: RSA (1)
> 
> though it was perfectly capable of using SHA384 (which was negotiated
> with clients that did include sha1 to their signature algorithms). The
> point is, that chrome seems to have adopted a "fallback" approach since
> version 82 (see
> https://groups.google.com/u/1/a/chromium.org/g/blink-dev/c/yaJcs4p9LNI/m/haZWzX-UBwAJ?pli=1),
> that first tries to connect without said signature algorithm in its
> extension and only if that fails does a second handshake with SHA1
> included. However, squid see and reports the error on the first attempt
> back to the client, so that the TLS-connection setup fails.
> 
> While this seems to be something that probably should be fixed at the
> server side, it does lead users to be believe Squid is at fault.
> 
> I tried bypassing TLS-handshake errors using
> 
> acl handshake_failure ssl_error SQUID_ERR_SSL_HANDSHAKE
> sslproxy_cert_error allow handshake_failure
> 
> but this did not change anything (maybe sslproxy_cert_error only works
> for certificate verification, but not for the handshake?)

Yes, sslproxy_cert_error is about certificates, not TLS handshakes.


> Any suggestions on how to allow the first TLS-connection failure of
> chrome to be passed directly back to client instead of erroeing out, or
> any other ways to fix this problem?

I can only suggest to either fix the Squid bug/limitation or decide to
splice during step1 (based on client SNI, etc., before Squid talks to
the origin server).


HTH,

Alex.


From squid3 at treenet.co.nz  Thu Jun 10 01:56:57 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Jun 2021 13:56:57 +1200
Subject: [squid-users] custom DNS resolver scripts? (was: Re: Is it
 possible to force some dstdomain to ipv4) protocol without define an
 outgoing ip address ?
In-Reply-To: <da2e3de0-b176-a6b7-dea6-45364421d049@measurement-factory.com>
References: <20210609122559.en7srd32v5nic3pz@bloms.de>
 <3e6ea9f4-5fbb-8fc2-7900-4948251d9afc@measurement-factory.com>
 <20210609221652.GA12481@pingviini>
 <da2e3de0-b176-a6b7-dea6-45364421d049@measurement-factory.com>
Message-ID: <4ada42dd-a13b-101d-9559-5cb442329024@treenet.co.nz>

On 10/06/21 11:42 am, Alex Rousskov wrote:
> On 6/9/21 6:16 PM, Ambrose Li wrote:
>> On Wed, Jun 09, 2021 at 12:05:40PM -0400, Alex Rousskov wrote:
>>> Not that I know of. You can implement this logic inside a custom DNS
>>> resolver script, or you can reconfigure Squid whenever your outgoing
>>> addresses change, but I understand that you are looking for a better
>>> solution.
> 
>> What are the current recommendations for custom DNS resolver scripts?
> 
> I was talking about a custom script that implements a custom DNS
> resolver. With modern libraries, it takes a few lines of code to write a
> basic one. This kind of resolver does not resolve most of the names, but
> forwards queries to another/real resolver, adapting the queries and/or
> the answers as needed.
> 
> For an oversimplified example, such a script can respond to all AAAA
> queries (with answers containing no records) while forwarding all A
> queries to a "real" resolver.
> 

Should be no need for any custom scripts or Squid config at all for this.

The Bind9 filter-aaaa* feature does it already without wasting network 
bandwidth delivering ignored response fields. Other recursive resolvers 
should all have equivalent features too.


Amos


From bruce.rosenberg.au at gmail.com  Thu Jun 10 03:42:25 2021
From: bruce.rosenberg.au at gmail.com (Bruce Rosenberg)
Date: Thu, 10 Jun 2021 13:42:25 +1000
Subject: [squid-users] custom DNS resolver scripts? (was: Re: Is it
 possible to force some dstdomain to ipv4) protocol without define an
 outgoing ip address ?
In-Reply-To: <4ada42dd-a13b-101d-9559-5cb442329024@treenet.co.nz>
References: <20210609122559.en7srd32v5nic3pz@bloms.de>
 <3e6ea9f4-5fbb-8fc2-7900-4948251d9afc@measurement-factory.com>
 <20210609221652.GA12481@pingviini>
 <da2e3de0-b176-a6b7-dea6-45364421d049@measurement-factory.com>
 <4ada42dd-a13b-101d-9559-5cb442329024@treenet.co.nz>
Message-ID: <CAHaxnUK-JwiU8rVpCJBeo54dbrrsXAAGcQpcFj-fpkcs2mNbLg@mail.gmail.com>

You could run unbound on the squid host (or elsewhere) and use this config
to drop all AAAA requests.
It utilises unbound's ability to include custom python scripts.

https://github.com/berstend/unbound-no-aaaa

Configure unbound to forward all other DNS requests to your existing
nameservers and reconfigure squid to use unbound via the dns_nameservers
directive.

On Thu, Jun 10, 2021 at 11:58 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 10/06/21 11:42 am, Alex Rousskov wrote:
> > On 6/9/21 6:16 PM, Ambrose Li wrote:
> >> On Wed, Jun 09, 2021 at 12:05:40PM -0400, Alex Rousskov wrote:
> >>> Not that I know of. You can implement this logic inside a custom DNS
> >>> resolver script, or you can reconfigure Squid whenever your outgoing
> >>> addresses change, but I understand that you are looking for a better
> >>> solution.
> >
> >> What are the current recommendations for custom DNS resolver scripts?
> >
> > I was talking about a custom script that implements a custom DNS
> > resolver. With modern libraries, it takes a few lines of code to write a
> > basic one. This kind of resolver does not resolve most of the names, but
> > forwards queries to another/real resolver, adapting the queries and/or
> > the answers as needed.
> >
> > For an oversimplified example, such a script can respond to all AAAA
> > queries (with answers containing no records) while forwarding all A
> > queries to a "real" resolver.
> >
>
> Should be no need for any custom scripts or Squid config at all for this.
>
> The Bind9 filter-aaaa* feature does it already without wasting network
> bandwidth delivering ignored response fields. Other recursive resolvers
> should all have equivalent features too.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210610/c998b4d9/attachment.htm>

From andreas.weigel at securepoint.de  Thu Jun 10 15:00:01 2021
From: andreas.weigel at securepoint.de (Andreas Weigel)
Date: Thu, 10 Jun 2021 15:00:01 +0000
Subject: [squid-users] Squid spliced TLS handshake failing with
 chrome/ium fallback for certain servers
In-Reply-To: <c19c5ef3-aac7-e8e4-832c-e1ba2623fb2c@measurement-factory.com>
References: <20210609192922.Horde.e5blxJcNidWpbfVZ3h05Ryd@webmail.intern.securepoint.de>
 <c19c5ef3-aac7-e8e4-832c-e1ba2623fb2c@measurement-factory.com>
Message-ID: <20210610150001.Horde.93dCpJtdgw0xqo2_q5dIDr7@webmail.intern.securepoint.de>

Hi Alex,

> I can only suggest to either fix the Squid bug/limitation or decide to
> splice during step1 (based on client SNI, etc., before Squid talks to
> the origin server).

don't know why I haven't yet had the idea, but indeed, if I force  
splicing at step 1 or even 2, the site loads without error. This is  
not exaclty a solution, but at least something to work with. Thanks!

I think I will have a look at the squid sources to see if I can put my  
finger on the reason for this behavior. Any pointers are very welcome,  
it has been some time since I meddled with squid.

Andreas



From matthias at saou.eu  Thu Jun 10 15:11:32 2021
From: matthias at saou.eu (Matthias Saou)
Date: Thu, 10 Jun 2021 17:11:32 +0200
Subject: [squid-users] Issues with SSLBumped high traffic forward caching
In-Reply-To: <60f0cd10-b036-9de4-85ab-98371817854d@measurement-factory.com>
References: <20210609160409.2278fd90@r2d2.marmotte.net>
 <60f0cd10-b036-9de4-85ab-98371817854d@measurement-factory.com>
Message-ID: <20210610171132.58e49f2c@vader.marmotte.ici>

On Wed, 9 Jun 2021 10:45:40 -0400
Alex Rousskov <rousskov at measurement-factory.com> wrote:

> > assertion failed: Transients.cc:221: "old == e"
> 
> This is a Squid bug. Please consider creating a Bugzilla entry and
> posting the corresponding backtrace there:
> https://wiki.squid-cache.org/SquidFaq/BugReporting#crashes_and_core_dumps

Thanks a lot for the super quick and detailed reply! The cores were
dumped, the traces were backed and the bug report was filed! :-)

https://bugs.squid-cache.org/show_bug.cgi?id=5134

Let me know if you need any further details or for us to test any
patches.

Cheers,
Matthias


From andreas.weigel at securepoint.de  Thu Jun 10 18:24:21 2021
From: andreas.weigel at securepoint.de (Andreas Weigel)
Date: Thu, 10 Jun 2021 18:24:21 +0000
Subject: [squid-users] Squid spliced TLS handshake failing with
 chrome/ium fallback for certain servers
In-Reply-To: <c19c5ef3-aac7-e8e4-832c-e1ba2623fb2c@measurement-factory.com>
References: <20210609192922.Horde.e5blxJcNidWpbfVZ3h05Ryd@webmail.intern.securepoint.de>
 <c19c5ef3-aac7-e8e4-832c-e1ba2623fb2c@measurement-factory.com>
Message-ID: <20210610182421.Horde.ZWqYNYE-S-5UO93YX0389tx@webmail.intern.securepoint.de>

Hi again,

> I can only suggest to either fix the Squid bug/limitation

I found Ssl::PeekingPeerConnector::noteNegotiationError in  
src/PeekingPeerConnector.cc. There are some checks for the case that  
srvBio->bumpMode() == Ssl::bumpPeek. If I apply the attached patch,  
matching on the ssl_lib_error for failed handshakes (just for testing,  
not an actual suggestion!) and setting bypassValidator = true in that  
case, the connection from Chrome to the problematic site goes through  
after the initial failure (configured with peeking at step2, splicing  
at step 3).

> If you are _splicing_ connections, then Squid should not be a part of
> the TLS handshake at all. If Squid is a part of the TLS handshake in an
> ssl_bump peek+splice configuration, then this is a Squid bug or limitation.

I agree. Said function produces an error page, if none of the checks  
make it return early. Maybe the logic here should be inverted to  
rather enumerate error cases that actually justify an error page  
(maybe only local errors like failed syscalls, OOM?). I am far from  
understanding all possible paths of execution this may take and  
therefore I may easily miss an important point here, though.

Andreas

Zitat von Alex Rousskov <rousskov at measurement-factory.com>:

> On 6/9/21 3:29 PM, Andreas Weigel wrote:
>> I stumbled upon a case of someone complaining about a site not being
>> reachable via squid. Squid was running as transparent proxy and splicing
>> TLS connections.
>
>
>> Squid reported a TLS handshake error to the client
>> (SQUID_ERR_SSL_HANDSHAKE; Handshake with SSL server failed:
>> error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure).
>
> If you are _splicing_ connections, then Squid should not be a part of
> the TLS handshake at all. If Squid is a part of the TLS handshake in an
> ssl_bump peek+splice configuration, then this is a Squid bug or limitation.
>
>
>> After some wireshark-digging, I found that the site in question (for
>> reference: www.klebl.net) aborted the handshake, if the Client Hello's
>> "Extension: signature_algorithms" did not contain
>>
>> ? Signature Algorithm: rsa_pkcs1_sha1 (0x0201)
>> ????? Signature Hash Algorithm Hash: SHA1 (2)
>> ????? Signature Hash Algorithm Signature: RSA (1)
>>
>> though it was perfectly capable of using SHA384 (which was negotiated
>> with clients that did include sha1 to their signature algorithms). The
>> point is, that chrome seems to have adopted a "fallback" approach since
>> version 82 (see
>> https://groups.google.com/u/1/a/chromium.org/g/blink-dev/c/yaJcs4p9LNI/m/haZWzX-UBwAJ?pli=1),
>> that first tries to connect without said signature algorithm in its
>> extension and only if that fails does a second handshake with SHA1
>> included. However, squid see and reports the error on the first attempt
>> back to the client, so that the TLS-connection setup fails.
>>
>> While this seems to be something that probably should be fixed at the
>> server side, it does lead users to be believe Squid is at fault.
>>
>> I tried bypassing TLS-handshake errors using
>>
>> acl handshake_failure ssl_error SQUID_ERR_SSL_HANDSHAKE
>> sslproxy_cert_error allow handshake_failure
>>
>> but this did not change anything (maybe sslproxy_cert_error only works
>> for certificate verification, but not for the handshake?)
>
> Yes, sslproxy_cert_error is about certificates, not TLS handshakes.
>
>
>> Any suggestions on how to allow the first TLS-connection failure of
>> chrome to be passed directly back to client instead of erroeing out, or
>> any other ways to fix this problem?
>
> I can only suggest to either fix the Squid bug/limitation or decide to
> splice during step1 (based on client SNI, etc., before Squid talks to
> the origin server).
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: test_handshake_failure.patch
Type: text/x-diff
Size: 691 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210610/1a3fb091/attachment.patch>

From rousskov at measurement-factory.com  Thu Jun 10 20:04:01 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 10 Jun 2021 16:04:01 -0400
Subject: [squid-users] Squid spliced TLS handshake failing with
 chrome/ium fallback for certain servers
In-Reply-To: <20210610150001.Horde.93dCpJtdgw0xqo2_q5dIDr7@webmail.intern.securepoint.de>
References: <20210609192922.Horde.e5blxJcNidWpbfVZ3h05Ryd@webmail.intern.securepoint.de>
 <c19c5ef3-aac7-e8e4-832c-e1ba2623fb2c@measurement-factory.com>
 <20210610150001.Horde.93dCpJtdgw0xqo2_q5dIDr7@webmail.intern.securepoint.de>
Message-ID: <0bf164b9-87cb-5a0b-66fe-8cd81fea9c6c@measurement-factory.com>

On 6/10/21 11:00 AM, Andreas Weigel wrote:

>> I can only suggest to either fix the Squid bug/limitation or decide to
>> splice during step1 (based on client SNI, etc., before Squid talks to
>> the origin server).

> don't know why I haven't yet had the idea, but indeed, if I force
> splicing at step 1 or even 2, the site loads without error. This is not
> exaclty a solution, but at least something to work with.


If you tell Squid to proceed to step3, then Squid has a duty to validate
the TLS server. Failure to do so should result in an error. This is what
https://wiki.squid-cache.org/Features/SslPeekAndSplice promises to do
during step3:

i. Get TLS Server Hello info from the server, including the server
certificate.
ii. Validate the TLS server certificate.
iii. Evaluate again all ssl_bump rules and perform the first matching
action (splice, bump, or terminate) for the connection.

We probably should enhance Squid to better control its reaction to 3i
failures, but such adjustments may have to be guarded by some new
explicit configuration because they would allow splicing connections to
non-TLS servers or very insecure TLS servers (that were previously
reported as errors).


HTH,

Alex.


From olivierw.ml at gmail.com  Sat Jun 12 12:37:08 2021
From: olivierw.ml at gmail.com (Olivier W)
Date: Sat, 12 Jun 2021 14:37:08 +0200
Subject: [squid-users] Usage of --enable-gnuregex on FreeBSD?
Message-ID: <CAOAUwZGYDezFd1JVe=cKUn6NNBfecNKBrm9unQrawj8N27nEXw@mail.gmail.com>

Hello,

I use Squid on FreeBSD.
In the past, with Squid 3.5.x and FreeBSD 11.x, I was able to use PCRE
regexp without any problems.

Now, on FreeBSD 13.0 and Squid 4.14, PCRE regexp don't work out of the
box: I have to compile Squid with the option "--enable-gnuregex" to
make it work.
I found this problem when I migrated from FreeBSD 11.x and Squid 3.5.x
to FreeBSD 12.1 and Squid 4.9 but I don't know exactly when it
appeared.

In Squid's configure.ac, it says I shouldn't have to use
"--enable-gnuregex", or only for some specific cases:
https://github.com/squid-cache/squid/blob/master/configure.ac#L3525

Without PCRE regexp, I can't use rules like:
acl example url_regex -i ^http:\/\/www\.example\.com\/.*?\.js
the problem is the lazy quantifier.

Any idea why it is happening?
Is there something which could have changed on Squid or FreeBSD which
could have created this problem?
Should I ask the FreeBSD port maintainer of Squid to compile with this option?

FYI: last year I already opened an issue regarding this on FreeBSD's
bugtracker: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=243291

Best Regards,
Olivier


From squid3 at treenet.co.nz  Sun Jun 13 02:01:03 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 Jun 2021 14:01:03 +1200
Subject: [squid-users] Usage of --enable-gnuregex on FreeBSD?
In-Reply-To: <CAOAUwZGYDezFd1JVe=cKUn6NNBfecNKBrm9unQrawj8N27nEXw@mail.gmail.com>
References: <CAOAUwZGYDezFd1JVe=cKUn6NNBfecNKBrm9unQrawj8N27nEXw@mail.gmail.com>
Message-ID: <08775bcc-8533-9c77-05e0-248a04deb251@treenet.co.nz>

On 13/06/21 12:37 am, Olivier W wrote:
> Hello,
> 
> I use Squid on FreeBSD.
> In the past, with Squid 3.5.x and FreeBSD 11.x, I was able to use PCRE
> regexp without any problems.
> 
> Now, on FreeBSD 13.0 and Squid 4.14, PCRE regexp don't work out of the
> box: I have to compile Squid with the option "--enable-gnuregex" to
> make it work.
> I found this problem when I migrated from FreeBSD 11.x and Squid 3.5.x
> to FreeBSD 12.1 and Squid 4.9 but I don't know exactly when it
> appeared.
> 
> In Squid's configure.ac, it says I shouldn't have to use
> "--enable-gnuregex", or only for some specific cases:
> https://github.com/squid-cache/squid/blob/master/configure.ac#L3525
> 
> Without PCRE regexp, I can't use rules like:
> acl example url_regex -i ^http:\/\/www\.example\.com\/.*?\.js
> the problem is the lazy quantifier.
> 
> Any idea why it is happening?

PCRE is not an officially supported library with Squid. It builds, but 
only "works" when non-backtracking regex features are used.

The type of pattern you have requires backtracking.


> Is there something which could have changed on Squid or FreeBSD which
> could have created this problem?

Insufficient data.


> Should I ask the FreeBSD port maintainer of Squid to compile with this option?
> 

No. That old GNU library is deprecated and will be replaced with 
std::regex C++ feature in coming versions.


Amos


From fsafran at proofpoint.com  Sun Jun 13 11:31:31 2021
From: fsafran at proofpoint.com (Frida Safran)
Date: Sun, 13 Jun 2021 11:31:31 +0000
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>

Hi,


  1.  Is it possible to pass proxy protocol headers to an external acl as part of the format?

  1.  Is it possible to pass all/specific icap headers to an external acl?
  2.  I have been trying to use %icap::>h to pass all the icap headers to an external acl, but it resolves to "-"



Thanks,
Frida
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210613/54051ebd/attachment.htm>

From rousskov at measurement-factory.com  Sun Jun 13 14:46:56 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 13 Jun 2021 10:46:56 -0400
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>

On 6/13/21 7:31 AM, Frida Safran wrote:

>  1. Is it possible to pass proxy protocol headers to an external acl as
>     part of the format?

It should be possible. Use %proxy_protocol::>h logformat %code in your
external_acl_type FORMAT configuration. We added that support to Squid
v5. Not available in the official v4.


>  2. Is it possible to pass all/specific icap headers to an external acl?
>     I have been trying to use %icap::>h to pass all the icap headers to
>     an external acl, but it resolves to "-"

It should be possible if your external ACL is evaluated _after_ the
corresponding ICAP headers are received, but I would not be surprised if
there are bugs in this area -- the ICAP headers may be available but not
 provided to the ACL evaluation code. Which squid.conf directive is
triggering your external ACL evaluation in this use case?


HTH,

Alex.


From fsafran at proofpoint.com  Mon Jun 14 06:29:20 2021
From: fsafran at proofpoint.com (Frida Safran)
Date: Mon, 14 Jun 2021 06:29:20 +0000
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>,
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
Message-ID: <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>

Hi Alex,

Thanks for the quick response.

Regarding proxy_protocol - is there a known patch for v4 I could use by any chance?

Regarding icap, I suppose the acl is getting evaluated before the icap and that is why they aren't available:

external_acl_type TransactionClassificator \
    concurrency=0 \
    children-max=2 \
    ttl=60 \
    %ssl::>sni \
    /usr/local/squid/bin/classify-transaction.sh

acl classifyRequest external TransactionClassificator

acl step1 at_step SslBump1
acl step2 at_step SslBump2

ssl_bump peek step1
ssl_bump splice step2 classifyRequest
ssl_bump stare all
ssl_bump bump all


Thanks,
Frida
________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Sunday, June 13, 2021 17:46
To: squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Cc: Frida Safran <fsafran at proofpoint.com>
Subject: Re: [squid-users] Passing Proxy Protocol Headers to external ACL

On 6/13/21 7:31 AM, Frida Safran wrote:

>  1. Is it possible to pass proxy protocol headers to an external acl as
>     part of the format?

It should be possible. Use %proxy_protocol::>h logformat %code in your
external_acl_type FORMAT configuration. We added that support to Squid
v5. Not available in the official v4.


>  2. Is it possible to pass all/specific icap headers to an external acl?
>     I have been trying to use %icap::>h to pass all the icap headers to
>     an external acl, but it resolves to "-"

It should be possible if your external ACL is evaluated _after_ the
corresponding ICAP headers are received, but I would not be surprised if
there are bugs in this area -- the ICAP headers may be available but not
 provided to the ACL evaluation code. Which squid.conf directive is
triggering your external ACL evaluation in this use case?


HTH,

Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210614/3cabab42/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 14 13:24:09 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Jun 2021 09:24:09 -0400
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>

On 6/14/21 2:29 AM, Frida Safran wrote:

> Regarding proxy_protocol - is there a known patch for v4 I could use by
> any chance?

I am not aware of any such patches. The changes were significant, fixing
many PROXY protocol handling bugs. Virtually anything can be backported,
but it would be a large effort with noticeable stability risks and
long-term maintenance overheads. Preparing for a v5 upgrade may be a
better strategy in this particular case.


> Regarding icap, I suppose the acl is getting evaluated before the icap
> and that is why they aren't available:

> acl classifyRequest external TransactionClassificator
> ssl_bump peek step1
> ssl_bump splice step2 classifyRequest
> ssl_bump stare all
> ssl_bump bump all

According to [1], the above configuration should result in two ICAP
REQMOD requests (if configured) before classifyRequest is consulted
during step2. I am aware of SslBump bugs in that area, but I would
expect at least one ICAP REQMOD request anyway. The requests
existence/timing should be easy to confirm using cache.log with
debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
pausing external ACL script in combination with an icap_log (to compare
logged timestamps).

[1] https://wiki.squid-cache.org/Features/SslPeekAndSplice


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Sunday, June 13, 2021 17:46
> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Cc:* Frida Safran <fsafran at proofpoint.com>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
> ?
> On 6/13/21 7:31 AM, Frida Safran wrote:
> 
>>? 1. Is it possible to pass proxy protocol headers to an external acl as
>>???? part of the format?
> 
> It should be possible. Use %proxy_protocol::>h logformat %code in your
> external_acl_type FORMAT configuration. We added that support to Squid
> v5. Not available in the official v4.
> 
> 
>>? 2. Is it possible to pass all/specific icap headers to an external acl?
>>???? I have been trying to use %icap::>h to pass all the icap headers to
>>???? an external acl, but it resolves to "-"
> 
> It should be possible if your external ACL is evaluated _after_ the
> corresponding ICAP headers are received, but I would not be surprised if
> there are bugs in this area -- the ICAP headers may be available but not
> ?provided to the ACL evaluation code. Which squid.conf directive is
> triggering your external ACL evaluation in this use case?
> 
> 
> HTH,
> 
> Alex.



From fsafran at proofpoint.com  Tue Jun 15 08:18:11 2021
From: fsafran at proofpoint.com (Frida Safran)
Date: Tue, 15 Jun 2021 08:18:11 +0000
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>,
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
Message-ID: <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>

Hi Alex,

In addition to using an external acl to annotate connections and decide whether splice/bump, I would like to try using an ecap service to achieve this.
I would like to create an acl using info from the ecap service, and bump/splice using the following configuration:

acl classifyRequest note splice yes

acl step1 at_step SslBump1
acl step2 at_step SslBump2

ssl_bump peek step1
ssl_bump splice step2 classifyRequest
ssl_bump stare all
ssl_bump bump all


If I set a custom option from within the ecap service, can i use that via the above note acl? Can i set a custom option without setting it in 'adaptation_masterx_shared_names', as I already have another one there already, and it only supports one name?

Or, should I instead use:

acl classifyRequest note %adapt::<last_h{splice} yes

In this case should I set a custom header from within the ecap via: adapted->header().add, or use the visitEachOption() method?

Thanks,
Frida

________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Monday, June 14, 2021 16:24
To: squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Cc: Frida Safran <fsafran at proofpoint.com>
Subject: Re: [squid-users] Passing Proxy Protocol Headers to external ACL

On 6/14/21 2:29 AM, Frida Safran wrote:

> Regarding proxy_protocol - is there a known patch for v4 I could use by
> any chance?

I am not aware of any such patches. The changes were significant, fixing
many PROXY protocol handling bugs. Virtually anything can be backported,
but it would be a large effort with noticeable stability risks and
long-term maintenance overheads. Preparing for a v5 upgrade may be a
better strategy in this particular case.


> Regarding icap, I suppose the acl is getting evaluated before the icap
> and that is why they aren't available:

> acl classifyRequest external TransactionClassificator
> ssl_bump peek step1
> ssl_bump splice step2 classifyRequest
> ssl_bump stare all
> ssl_bump bump all

According to [1], the above configuration should result in two ICAP
REQMOD requests (if configured) before classifyRequest is consulted
during step2. I am aware of SslBump bugs in that area, but I would
expect at least one ICAP REQMOD request anyway. The requests
existence/timing should be easy to confirm using cache.log with
debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
pausing external ACL script in combination with an icap_log (to compare
logged timestamps).

[1] https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Sunday, June 13, 2021 17:46
> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Cc:* Frida Safran <fsafran at proofpoint.com>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>
> On 6/13/21 7:31 AM, Frida Safran wrote:
>
>>  1. Is it possible to pass proxy protocol headers to an external acl as
>>     part of the format?
>
> It should be possible. Use %proxy_protocol::>h logformat %code in your
> external_acl_type FORMAT configuration. We added that support to Squid
> v5. Not available in the official v4.
>
>
>>  2. Is it possible to pass all/specific icap headers to an external acl?
>>     I have been trying to use %icap::>h to pass all the icap headers to
>>     an external acl, but it resolves to "-"
>
> It should be possible if your external ACL is evaluated _after_ the
> corresponding ICAP headers are received, but I would not be surprised if
> there are bugs in this area -- the ICAP headers may be available but not
>  provided to the ACL evaluation code. Which squid.conf directive is
> triggering your external ACL evaluation in this use case?
>
>
> HTH,
>
> Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210615/05d37b67/attachment.htm>

From Joseph.Garbacik at netapp.com  Tue Jun 15 17:57:04 2021
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Tue, 15 Jun 2021 17:57:04 +0000
Subject: [squid-users] Squid ERR_CONNECT_FAIL SYSERR=110
Message-ID: <9C580B63-AE74-47F5-9028-DD14B65D9A04@netapp.com>

I am seeing this occasionally in my squid logs and am little confused because I don't know how to troubleshoot this one. I normally see in the squid logs the destination IP address as dst_ip in my custom logs but in these cases, the field is blank. I'd expect the destination IP to be blank when it encounters a DNS Failure but then the Squid message is ERR_DNS_FAIL not ERR_CONNECT_FAIL. Here is part of an log example:

... ** Flow2** squid_egress_ip=- squid_egress_port=- dst_ip=- dst_host=- dst_port=- request_method=CONNECT request="CONNECT foo.com:443 HTTP/1.1" status_code_from_server=503 total_response_time=59480 ...





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210615/8657e2fe/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 15 18:27:38 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Jun 2021 14:27:38 -0400
Subject: [squid-users] Squid ERR_CONNECT_FAIL SYSERR=110
In-Reply-To: <9C580B63-AE74-47F5-9028-DD14B65D9A04@netapp.com>
References: <9C580B63-AE74-47F5-9028-DD14B65D9A04@netapp.com>
Message-ID: <b8b0426d-aa0c-825e-3749-452e11abfa4a@measurement-factory.com>

On 6/15/21 1:57 PM, Garbacik, Joe wrote:
> I am seeing this occasionally in my squid logs and am little confused
> because I don't know how to troubleshoot this one. I normally see in the
> squid logs the destination IP address as dst_ip in my custom logs but in
> these cases, the field is blank. I'd expect the destination IP to be
> blank when it encounters a DNS Failure but then the Squid message is
> ERR_DNS_FAIL not ERR_CONNECT_FAIL. Here is part of an log example:
> 
> ... ** Flow2**
> squid_egress_ip=-squid_egress_port=-dst_ip=-dst_host=-dst_port=-request_method=CONNECTrequest="CONNECT
> foo.com:443HTTP/1.1" status_code_from_server=503total_response_time=59480...

Too many possibilities for me to offer a good working theory. Squid
error reporting behavior is often Squid version-specific and usually
module-specific. It is possible that your Squid just forgot to relay the
failed connection details (a bug), or that a DNS timeout problem was
somehow overwritten with ERR_CONNECT_FAIL, or that Squid was thinking
that it is not talking to an origin server (only origin server name
resolution problems may result in ERR_DNS_FAIL for some unknown to me
reason), etc., etc.

You can try logging %dt, %<pt, and %<tt to get more details, but
figuring this out may require many iterations if you cannot reproduce
the problem at will.

N.B. If you post more details, please share your logformat. It is often
impossible to guess which exact logformat %code was used for which of
your custom labels.

Alex.


From rousskov at measurement-factory.com  Tue Jun 15 18:35:56 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 Jun 2021 14:35:56 -0400
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>

On 6/15/21 4:18 AM, Frida Safran wrote:

> In addition to using an external acl to annotate connections and decide
> whether splice/bump, I would like to try using an ecap service to
> achieve this.
> I would like to create an acl using info from the ecap service, and
> bump/splice using the following configuration:
> 
> acl classifyRequest note splice yes
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> 
> ssl_bump peek step1
> ssl_bump splice step2 classifyRequest
> ssl_bump stare all
> ssl_bump bump all
> 
> 
> If I set a custom option from within the ecap service, can i use that
> via the above note acl?

I believe that should work in principle, provided the eCAP service is
consulted before Squid starts evaluating "ssl_bump splice step2
classifyRequest". If you do not restrict what requests your eCAP REQMOD
service gets, then it should be consulted during step1 (at least). The
logic is the same for eCAP and ICAP here -- Squid "primary" code does
not know the difference between those two adaptation interfaces.


> Can i set a custom option without setting it in
> 'adaptation_masterx_shared_names'

Yes, you should be able to. The adaptation_masterx_shared_names
directive is unrelated to setting transaction annotations IIRC. It is
about sharing metadata across adaptation services.


> Or, should I instead use:
> 
> acl classifyRequest note %adapt::<last_h{splice} yes

This option cannot work AFAICT because the "note" ACL requires a
constant (i.e. known at configuration time) annotation name. Squid will
not substitute %adapt::<last_h{splice} with anything in this context.


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Monday, June 14, 2021 16:24
> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Cc:* Frida Safran <fsafran at proofpoint.com>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
> ?
> On 6/14/21 2:29 AM, Frida Safran wrote:
> 
>> Regarding proxy_protocol - is there a known patch for v4 I could use by
>> any chance?
> 
> I am not aware of any such patches. The changes were significant, fixing
> many PROXY protocol handling bugs. Virtually anything can be backported,
> but it would be a large effort with noticeable stability risks and
> long-term maintenance overheads. Preparing for a v5 upgrade may be a
> better strategy in this particular case.
> 
> 
>> Regarding icap, I suppose the acl is getting evaluated before the icap
>> and that is why they aren't available:
> 
>> acl classifyRequest external TransactionClassificator
>> ssl_bump peek step1
>> ssl_bump splice step2 classifyRequest
>> ssl_bump stare all
>> ssl_bump bump all
> 
> According to [1], the above configuration should result in two ICAP
> REQMOD requests (if configured) before classifyRequest is consulted
> during step2. I am aware of SslBump bugs in that area, but I would
> expect at least one ICAP REQMOD request anyway. The requests
> existence/timing should be easy to confirm using cache.log with
> debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
> pausing external ACL script in combination with an icap_log (to compare
> logged timestamps).
> 
> [1]
> https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>
> 
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Sunday, June 13, 2021 17:46
>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>> ?
>> On 6/13/21 7:31 AM, Frida Safran wrote:
>> 
>>>? 1. Is it possible to pass proxy protocol headers to an external acl as
>>>???? part of the format?
>> 
>> It should be possible. Use %proxy_protocol::>h logformat %code in your
>> external_acl_type FORMAT configuration. We added that support to Squid
>> v5. Not available in the official v4.
>> 
>> 
>>>? 2. Is it possible to pass all/specific icap headers to an external acl?
>>>???? I have been trying to use %icap::>h to pass all the icap headers to
>>>???? an external acl, but it resolves to "-"
>> 
>> It should be possible if your external ACL is evaluated _after_ the
>> corresponding ICAP headers are received, but I would not be surprised if
>> there are bugs in this area -- the ICAP headers may be available but not
>> ?provided to the ACL evaluation code. Which squid.conf directive is
>> triggering your external ACL evaluation in this use case?
>> 
>> 
>> HTH,
>> 
>> Alex.
> 



From fsafran at proofpoint.com  Wed Jun 16 08:15:28 2021
From: fsafran at proofpoint.com (Frida Safran)
Date: Wed, 16 Jun 2021 08:15:28 +0000
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>,
 <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
Message-ID: <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>

Hi,

I see that the configured acl is not called for all requests.
I do see that the ecap is called, it sets the option X-PMeta-Splice via visitEachOption correctly, but the classifyRequest note acl is not evaluated, and the request is not spliced.
However, I do see that this acl is evaluated for some other requests, but only when the X-PMeta-Bypass is set to: 'no'.
This config worked as expected when using an external acl type for the same request, but i also saw the same issue when using note when i tried to use it to classify the external acl.
Is there perhaps a bug/misconfiguration with the note acl?

The current config is:

acl classifyRequest note -m X-PMeta-Bypass yes

acl step1 at_step SslBump1
acl step2 at_step SslBump2

ssl_bump peek step1
ssl_bump splice step2 classifyRequest
ssl_bump stare all
ssl_bump bump all

adaptation_meta X-PMeta-Bypass "%adapt::<last_h{X-PMeta-Splice}"

Thanks,
Frida
________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Tuesday, June 15, 2021 21:35
To: Frida Safran <fsafran at proofpoint.com>; squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Passing Proxy Protocol Headers to external ACL

On 6/15/21 4:18 AM, Frida Safran wrote:

> In addition to using an external acl to annotate connections and decide
> whether splice/bump, I would like to try using an ecap service to
> achieve this.
> I would like to create an acl using info from the ecap service, and
> bump/splice using the following configuration:
>
> acl classifyRequest note splice yes
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
>
> ssl_bump peek step1
> ssl_bump splice step2 classifyRequest
> ssl_bump stare all
> ssl_bump bump all
>
>
> If I set a custom option from within the ecap service, can i use that
> via the above note acl?

I believe that should work in principle, provided the eCAP service is
consulted before Squid starts evaluating "ssl_bump splice step2
classifyRequest". If you do not restrict what requests your eCAP REQMOD
service gets, then it should be consulted during step1 (at least). The
logic is the same for eCAP and ICAP here -- Squid "primary" code does
not know the difference between those two adaptation interfaces.


> Can i set a custom option without setting it in
> 'adaptation_masterx_shared_names'

Yes, you should be able to. The adaptation_masterx_shared_names
directive is unrelated to setting transaction annotations IIRC. It is
about sharing metadata across adaptation services.


> Or, should I instead use:
>
> acl classifyRequest note %adapt::<last_h{splice} yes

This option cannot work AFAICT because the "note" ACL requires a
constant (i.e. known at configuration time) annotation name. Squid will
not substitute %adapt::<last_h{splice} with anything in this context.


HTH,

Alex.


> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Monday, June 14, 2021 16:24
> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Cc:* Frida Safran <fsafran at proofpoint.com>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>
> On 6/14/21 2:29 AM, Frida Safran wrote:
>
>> Regarding proxy_protocol - is there a known patch for v4 I could use by
>> any chance?
>
> I am not aware of any such patches. The changes were significant, fixing
> many PROXY protocol handling bugs. Virtually anything can be backported,
> but it would be a large effort with noticeable stability risks and
> long-term maintenance overheads. Preparing for a v5 upgrade may be a
> better strategy in this particular case.
>
>
>> Regarding icap, I suppose the acl is getting evaluated before the icap
>> and that is why they aren't available:
>
>> acl classifyRequest external TransactionClassificator
>> ssl_bump peek step1
>> ssl_bump splice step2 classifyRequest
>> ssl_bump stare all
>> ssl_bump bump all
>
> According to [1], the above configuration should result in two ICAP
> REQMOD requests (if configured) before classifyRequest is consulted
> during step2. I am aware of SslBump bugs in that area, but I would
> expect at least one ICAP REQMOD request anyway. The requests
> existence/timing should be easy to confirm using cache.log with
> debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
> pausing external ACL script in combination with an icap_log (to compare
> logged timestamps).
>
> [1]
> https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>
>
>
>
> HTH,
>
> Alex.
>
>
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Sunday, June 13, 2021 17:46
>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>
>> On 6/13/21 7:31 AM, Frida Safran wrote:
>>
>>>  1. Is it possible to pass proxy protocol headers to an external acl as
>>>     part of the format?
>>
>> It should be possible. Use %proxy_protocol::>h logformat %code in your
>> external_acl_type FORMAT configuration. We added that support to Squid
>> v5. Not available in the official v4.
>>
>>
>>>  2. Is it possible to pass all/specific icap headers to an external acl?
>>>     I have been trying to use %icap::>h to pass all the icap headers to
>>>     an external acl, but it resolves to "-"
>>
>> It should be possible if your external ACL is evaluated _after_ the
>> corresponding ICAP headers are received, but I would not be surprised if
>> there are bugs in this area -- the ICAP headers may be available but not
>>  provided to the ACL evaluation code. Which squid.conf directive is
>> triggering your external ACL evaluation in this use case?
>>
>>
>> HTH,
>>
>> Alex.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210616/4d536ece/attachment.htm>

From olivierw.ml at gmail.com  Wed Jun 16 11:27:44 2021
From: olivierw.ml at gmail.com (Olivier W)
Date: Wed, 16 Jun 2021 13:27:44 +0200
Subject: [squid-users] Usage of --enable-gnuregex on FreeBSD?
In-Reply-To: <08775bcc-8533-9c77-05e0-248a04deb251@treenet.co.nz>
References: <CAOAUwZGYDezFd1JVe=cKUn6NNBfecNKBrm9unQrawj8N27nEXw@mail.gmail.com>
 <08775bcc-8533-9c77-05e0-248a04deb251@treenet.co.nz>
Message-ID: <CAOAUwZGHmVUrCOmFBnDqFP+LFbPc02o4Qi2QCOyPp25t1nanOw@mail.gmail.com>

Hello Amos,

Le dim. 13 juin 2021 ? 04:02, Amos Jeffries <squid3 at treenet.co.nz> a ?crit :
>
> On 13/06/21 12:37 am, Olivier W wrote:

> > Should I ask the FreeBSD port maintainer of Squid to compile with this option?
> >
>
> No. That old GNU library is deprecated and will be replaced with
> std::regex C++ feature in coming versions.
>

Thanks for your answers.
As my rules using PCRE are very old, I'll just check them and remove
and/or correct them as needed.
And I'll close my issue on FreeBSD's bugtracker.

Best Regards,
Olivier


From Joseph.Garbacik at netapp.com  Wed Jun 16 14:20:56 2021
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Wed, 16 Jun 2021 14:20:56 +0000
Subject: [squid-users] Squid ERR_CONNECT_FAIL SYSERR=110
Message-ID: <C6091C39-322E-4E87-AA16-4573C19DCA8D@netapp.com>

I'm running Squid 4.14 with the following custom log format:
logformat MyLogFormat  ---> local_time="[%tl]" squid_service=%{service}note squid_status=%Ss squid_hierarchy_status=%Sh ** lb_id=%{X-Request-Id}>h *FLOW0* orig_src_ip=%{X-Client-Egress-Ip}>h orig_src_port=%{X-Client-Egress-Port}>h  lb_ingress_ip=%{X-LB-Ingress-Ip}>h lb_ingress_port=%{X-LB-Ingress-Port}>h *Flow1* lb_egress_ip=%>a lb_egress_port=%>p squid_ingress_ip=%>la squid_ingress_port=%>lp  *FLOW2* squid_egress_ip=%<la squid_egress_port=%<lp dst_ip=%<a dst_host=%<A dst_port=%<p ident_username=%[ui username=%[un request_method=%rm request="%rm %ru HTTP/%rv" status_code_from_server=%>Hs status_code_to_client=%<Hs referer="%{Referer}>h" user_agent="%{User-Agent}>h" protocol_version=%rv ** dns_response_time=%dt response_time=%tr mime_type=%mt *XFER*  total_request_size=%>st total_reply_size=%<st ** %{src_zone}note %{dst_zone}note %{method_category}note %{dst_category}note %{file_upload}note ** REQUEST HEADERS %>h *** RESPONSE HEADERS %<h *** tag_returned=%et tag_string="%ea" previous_hop_mac=%>eui peer_response_time=%<pt total_response_time=%<tt *SSL* src_ssl_negotiated_version=%ssl::>negotiated_version dst_ssl_negotiated_version=%ssl::<negotiated_version src_tls_hello_version=%ssl::>received_hello_version  dst_tls_hello_version=%ssl::<received_hello_version src_tls_max_version=%ssl::>received_supported_version dst_tls_max_version=%ssl::<received_supported_version src_tls_cipher=%ssl::>negotiated_cipher dst_tls_cipher=%ssl::<negotiated_cipher ssl_bump=%<bs ssl_bump_mode=%ssl::bump_mode ssl_sni=%ssl::>sni src_cert_subject="%ssl::>cert_subject" src_cert_issuer="%ssl::>cert_issuer" dst_cert_subject="%ssl::<cert_subject" dst_cert_issuer="%ssl::<cert_issuer" cert_errors="%ssl::<cert_errors" ssl_handshake="%>handshake" *** error_page_presented=%err_code err_detail="%err_detail"  rule_id=%{ruleid}note rule_type=%{ruletype}note  XFF="%{X-Forwarded-For}>h" squid_dst_app=%{dst_app}note

I normally don't see an issue as things are logged with %<a populated unless the destination doesn't resolve then %<a is blank with %err_code value being ERR_DNS_FAIL. Even then I see %>Hs=503, %<a contains an IP but I've noticed that on some very active destinations is just a hyphen. 

From rousskov at measurement-factory.com  Wed Jun 16 14:38:25 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 16 Jun 2021 10:38:25 -0400
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
 <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
 <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <f48985cf-35ca-3de6-d314-f490c5222169@measurement-factory.com>

On 6/16/21 4:15 AM, Frida Safran wrote:

> I see that the configured acl is not called for all requests.

classifyRequest should be evaluated for all CONNECT requests that get to
SslBump step2. That should be all CONNECT requests except for (rare)
cases where Squid cannot peek at the TLS ClientHello during step1 and,
hence, bumps the client connection without reaching step2.

Whether classifyRequest _matches_ when evaluated is a different story,
of course.

> I do see that the ecap is called, it sets the option?X-PMeta-Splice
> via?visitEachOption?correctly, but the classifyRequest note acl is not
> evaluated, and the request is not spliced.
> However, I do see that this acl is evaluated for some other requests,
> but only when the?X-PMeta-Bypass is set to: 'no'.
> This config worked as expected when using an external acl type for the
> same request, but i also saw the same issue when using note when i tried
> to use it to classify the external acl.
> Is there perhaps a bug/misconfiguration with the note acl?

Sorry, I do not have enough information to answer that question. Try
sharing an ALL,9 cache.log that reproduces the problem using a single
transaction:
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction


> The current config is:
> 
> acl classifyRequest note -m X-PMeta-Bypass yes
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> 
> ssl_bump peek step1
> ssl_bump splice step2 classifyRequest
> ssl_bump stare all
> ssl_bump bump all

> adaptation_meta X-PMeta-Bypass "%adapt::<last_h{X-PMeta-Splice}"

This adaptation_meta configuration should pass X-PMeta-Bypass
meta-header field to the adaptation service, with the field value set to
the value of the value of X-PMeta-Splice meta-header field received by
Squid from the previously applied adaption service (within the same
master transaction). Is that what you want?


HTH,

Alex.



> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Tuesday, June 15, 2021 21:35
> *To:* Frida Safran <fsafran at proofpoint.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
> ?
> On 6/15/21 4:18 AM, Frida Safran wrote:
> 
>> In addition to using an external acl to annotate connections and decide
>> whether splice/bump, I would like to try using an ecap service to
>> achieve this.
>> I would like to create an acl using info from the ecap service, and
>> bump/splice using the following configuration:
>> 
>> acl classifyRequest note splice yes
>> 
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> 
>> ssl_bump peek step1
>> ssl_bump splice step2 classifyRequest
>> ssl_bump stare all
>> ssl_bump bump all
>> 
>> 
>> If I set a custom option from within the ecap service, can i use that
>> via the above note acl?
> 
> I believe that should work in principle, provided the eCAP service is
> consulted before Squid starts evaluating "ssl_bump splice step2
> classifyRequest". If you do not restrict what requests your eCAP REQMOD
> service gets, then it should be consulted during step1 (at least). The
> logic is the same for eCAP and ICAP here -- Squid "primary" code does
> not know the difference between those two adaptation interfaces.
> 
> 
>> Can i set a custom option without setting it in
>> 'adaptation_masterx_shared_names'
> 
> Yes, you should be able to. The adaptation_masterx_shared_names
> directive is unrelated to setting transaction annotations IIRC. It is
> about sharing metadata across adaptation services.
> 
> 
>> Or, should I instead use:
>> 
>> acl classifyRequest note %adapt::<last_h{splice} yes
> 
> This option cannot work AFAICT because the "note" ACL requires a
> constant (i.e. known at configuration time) annotation name. Squid will
> not substitute %adapt::<last_h{splice} with anything in this context.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Monday, June 14, 2021 16:24
>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>> ?
>> On 6/14/21 2:29 AM, Frida Safran wrote:
>> 
>>> Regarding proxy_protocol - is there a known patch for v4 I could use by
>>> any chance?
>> 
>> I am not aware of any such patches. The changes were significant, fixing
>> many PROXY protocol handling bugs. Virtually anything can be backported,
>> but it would be a large effort with noticeable stability risks and
>> long-term maintenance overheads. Preparing for a v5 upgrade may be a
>> better strategy in this particular case.
>> 
>> 
>>> Regarding icap, I suppose the acl is getting evaluated before the icap
>>> and that is why they aren't available:
>> 
>>> acl classifyRequest external TransactionClassificator
>>> ssl_bump peek step1
>>> ssl_bump splice step2 classifyRequest
>>> ssl_bump stare all
>>> ssl_bump bump all
>> 
>> According to [1], the above configuration should result in two ICAP
>> REQMOD requests (if configured) before classifyRequest is consulted
>> during step2. I am aware of SslBump bugs in that area, but I would
>> expect at least one ICAP REQMOD request anyway. The requests
>> existence/timing should be easy to confirm using cache.log with
>> debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
>> pausing external ACL script in combination with an icap_log (to compare
>> logged timestamps).
>> 
>> [1]
>> https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>
>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>>
>> 
>> 
>> 
>> HTH,
>> 
>> Alex.
>> 
>> 
>>> ------------------------------------------------------------------------
>>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>>> *Sent:* Sunday, June 13, 2021 17:46
>>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>> ?
>>> On 6/13/21 7:31 AM, Frida Safran wrote:
>>> 
>>>>? 1. Is it possible to pass proxy protocol headers to an external acl as
>>>>???? part of the format?
>>> 
>>> It should be possible. Use %proxy_protocol::>h logformat %code in your
>>> external_acl_type FORMAT configuration. We added that support to Squid
>>> v5. Not available in the official v4.
>>> 
>>> 
>>>>? 2. Is it possible to pass all/specific icap headers to an external acl?
>>>>???? I have been trying to use %icap::>h to pass all the icap headers to
>>>>???? an external acl, but it resolves to "-"
>>> 
>>> It should be possible if your external ACL is evaluated _after_ the
>>> corresponding ICAP headers are received, but I would not be surprised if
>>> there are bugs in this area -- the ICAP headers may be available but not
>>> ?provided to the ACL evaluation code. Which squid.conf directive is
>>> triggering your external ACL evaluation in this use case?
>>> 
>>> 
>>> HTH,
>>> 
>>> Alex.
>> 
> 



From Joseph.Garbacik at netapp.com  Wed Jun 16 17:50:13 2021
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Wed, 16 Jun 2021 17:50:13 +0000
Subject: [squid-users] password
Message-ID: <78E18E29-06F9-475A-B795-0447A06D6379@netapp.com>



From fsafran at proofpoint.com  Thu Jun 17 05:55:32 2021
From: fsafran at proofpoint.com (Frida Safran)
Date: Thu, 17 Jun 2021 05:55:32 +0000
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <f48985cf-35ca-3de6-d314-f490c5222169@measurement-factory.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
 <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
 <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>,
 <f48985cf-35ca-3de6-d314-f490c5222169@measurement-factory.com>
Message-ID: <MN2PR12MB43410854B3851A87FC0F39B6D20E9@MN2PR12MB4341.namprd12.prod.outlook.com>

Hi,

I see that the issue stems from that the %ssl::sni is not getting passed correctly (evaluated as "-") for CONNECT requests when passed via:

adaptation_meta X-PMeta-SNI "%ssl::sni"

Why could that be? I do see that in the access.log the %ssl::sni is logged correctly on CONNECT entries.

I have worked around this by passing the "%>rd" instead - are these 2 values equivalent in all cases?

Thanks,
Frida
________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Wednesday, June 16, 2021 17:38
To: Frida Safran <fsafran at proofpoint.com>; squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Passing Proxy Protocol Headers to external ACL

On 6/16/21 4:15 AM, Frida Safran wrote:

> I see that the configured acl is not called for all requests.

classifyRequest should be evaluated for all CONNECT requests that get to
SslBump step2. That should be all CONNECT requests except for (rare)
cases where Squid cannot peek at the TLS ClientHello during step1 and,
hence, bumps the client connection without reaching step2.

Whether classifyRequest _matches_ when evaluated is a different story,
of course.

> I do see that the ecap is called, it sets the option X-PMeta-Splice
> via visitEachOption correctly, but the classifyRequest note acl is not
> evaluated, and the request is not spliced.
> However, I do see that this acl is evaluated for some other requests,
> but only when the X-PMeta-Bypass is set to: 'no'.
> This config worked as expected when using an external acl type for the
> same request, but i also saw the same issue when using note when i tried
> to use it to classify the external acl.
> Is there perhaps a bug/misconfiguration with the note acl?

Sorry, I do not have enough information to answer that question. Try
sharing an ALL,9 cache.log that reproduces the problem using a single
transaction:
https://urldefense.com/v3/__https://wiki.squid-cache.org/SquidFaq/BugReporting*Debugging_a_single_transaction__;Iw!!ORgEfCBsr282Fw!9mAZmYL4yTa1_p9UoNH3-uc5baXXKzSf1Zi9SkjsnCkMq_JCg50KhEXmwGcowILvtw$


> The current config is:
>
> acl classifyRequest note -m X-PMeta-Bypass yes
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
>
> ssl_bump peek step1
> ssl_bump splice step2 classifyRequest
> ssl_bump stare all
> ssl_bump bump all

> adaptation_meta X-PMeta-Bypass "%adapt::<last_h{X-PMeta-Splice}"

This adaptation_meta configuration should pass X-PMeta-Bypass
meta-header field to the adaptation service, with the field value set to
the value of the value of X-PMeta-Splice meta-header field received by
Squid from the previously applied adaption service (within the same
master transaction). Is that what you want?


HTH,

Alex.



> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Tuesday, June 15, 2021 21:35
> *To:* Frida Safran <fsafran at proofpoint.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>
> On 6/15/21 4:18 AM, Frida Safran wrote:
>
>> In addition to using an external acl to annotate connections and decide
>> whether splice/bump, I would like to try using an ecap service to
>> achieve this.
>> I would like to create an acl using info from the ecap service, and
>> bump/splice using the following configuration:
>>
>> acl classifyRequest note splice yes
>>
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>>
>> ssl_bump peek step1
>> ssl_bump splice step2 classifyRequest
>> ssl_bump stare all
>> ssl_bump bump all
>>
>>
>> If I set a custom option from within the ecap service, can i use that
>> via the above note acl?
>
> I believe that should work in principle, provided the eCAP service is
> consulted before Squid starts evaluating "ssl_bump splice step2
> classifyRequest". If you do not restrict what requests your eCAP REQMOD
> service gets, then it should be consulted during step1 (at least). The
> logic is the same for eCAP and ICAP here -- Squid "primary" code does
> not know the difference between those two adaptation interfaces.
>
>
>> Can i set a custom option without setting it in
>> 'adaptation_masterx_shared_names'
>
> Yes, you should be able to. The adaptation_masterx_shared_names
> directive is unrelated to setting transaction annotations IIRC. It is
> about sharing metadata across adaptation services.
>
>
>> Or, should I instead use:
>>
>> acl classifyRequest note %adapt::<last_h{splice} yes
>
> This option cannot work AFAICT because the "note" ACL requires a
> constant (i.e. known at configuration time) annotation name. Squid will
> not substitute %adapt::<last_h{splice} with anything in this context.
>
>
> HTH,
>
> Alex.
>
>
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Monday, June 14, 2021 16:24
>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>
>> On 6/14/21 2:29 AM, Frida Safran wrote:
>>
>>> Regarding proxy_protocol - is there a known patch for v4 I could use by
>>> any chance?
>>
>> I am not aware of any such patches. The changes were significant, fixing
>> many PROXY protocol handling bugs. Virtually anything can be backported,
>> but it would be a large effort with noticeable stability risks and
>> long-term maintenance overheads. Preparing for a v5 upgrade may be a
>> better strategy in this particular case.
>>
>>
>>> Regarding icap, I suppose the acl is getting evaluated before the icap
>>> and that is why they aren't available:
>>
>>> acl classifyRequest external TransactionClassificator
>>> ssl_bump peek step1
>>> ssl_bump splice step2 classifyRequest
>>> ssl_bump stare all
>>> ssl_bump bump all
>>
>> According to [1], the above configuration should result in two ICAP
>> REQMOD requests (if configured) before classifyRequest is consulted
>> during step2. I am aware of SslBump bugs in that area, but I would
>> expect at least one ICAP REQMOD request anyway. The requests
>> existence/timing should be easy to confirm using cache.log with
>> debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
>> pausing external ACL script in combination with an icap_log (to compare
>> logged timestamps).
>>
>> [1]
>> https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>
>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>>
>>
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> ------------------------------------------------------------------------
>>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>>> *Sent:* Sunday, June 13, 2021 17:46
>>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>>
>>> On 6/13/21 7:31 AM, Frida Safran wrote:
>>>
>>>>  1. Is it possible to pass proxy protocol headers to an external acl as
>>>>     part of the format?
>>>
>>> It should be possible. Use %proxy_protocol::>h logformat %code in your
>>> external_acl_type FORMAT configuration. We added that support to Squid
>>> v5. Not available in the official v4.
>>>
>>>
>>>>  2. Is it possible to pass all/specific icap headers to an external acl?
>>>>     I have been trying to use %icap::>h to pass all the icap headers to
>>>>     an external acl, but it resolves to "-"
>>>
>>> It should be possible if your external ACL is evaluated _after_ the
>>> corresponding ICAP headers are received, but I would not be surprised if
>>> there are bugs in this area -- the ICAP headers may be available but not
>>>  provided to the ACL evaluation code. Which squid.conf directive is
>>> triggering your external ACL evaluation in this use case?
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210617/51c5eb61/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 17 15:41:58 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 17 Jun 2021 11:41:58 -0400
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB43410854B3851A87FC0F39B6D20E9@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
 <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
 <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>
 <f48985cf-35ca-3de6-d314-f490c5222169@measurement-factory.com>
 <MN2PR12MB43410854B3851A87FC0F39B6D20E9@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <2deb3f71-9300-d4af-112b-ba84547493ed@measurement-factory.com>

On 6/17/21 1:55 AM, Frida Safran wrote:

> I see that the issue stems from that the %ssl::sni is not getting passed
> correctly (evaluated as "-") for CONNECT requests when passed via:
> 
> adaptation_meta X-PMeta-SNI "%ssl::sni"
> 
> Why could that be? I do see that in the access.log the %ssl::sni is
> logged correctly on CONNECT entries.


Perhaps it is because SNI is never available during SslBump step1? It is
not clear (to me) whether you are talking about step1 or step2
adaptation_meta evaluation. Also, there could be Squid bugs that result
in missing adaptation_meta re-evaluations during step2 -- I have not
checked.


> I have worked around this by passing the "%>rd" instead - are these 2
> values equivalent in all cases?

No, they are not.

1. SNI should always be a domain name. CONNECT request URI may be an IP.

1.1 A true CONNECT request may use an IP address -- client choice.

1.2 A faked during step1 CONNECT request uses an IP address.
   This faking always happens when you use SslBump on https_port.

2. SNI may differ from the domain name in the CONNECT request.
  For example, the CONNECT request may be for example.com while
  SNI may say something more specific like service.example.com.
  Some SNI domains are obfuscated/encrypted.

3. SNI may be missing in TLS ClientHello.
   CONNECT requests always have a URI.

HTH,

Alex.

> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Wednesday, June 16, 2021 17:38
> *To:* Frida Safran <fsafran at proofpoint.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
> ?
> On 6/16/21 4:15 AM, Frida Safran wrote:
> 
>> I see that the configured acl is not called for all requests.
> 
> classifyRequest should be evaluated for all CONNECT requests that get to
> SslBump step2. That should be all CONNECT requests except for (rare)
> cases where Squid cannot peek at the TLS ClientHello during step1 and,
> hence, bumps the client connection without reaching step2.
> 
> Whether classifyRequest _matches_ when evaluated is a different story,
> of course.
> 
>> I do see that the ecap is called, it sets the option?X-PMeta-Splice
>> via?visitEachOption?correctly, but the classifyRequest note acl is not
>> evaluated, and the request is not spliced.
>> However, I do see that this acl is evaluated for some other requests,
>> but only when the?X-PMeta-Bypass is set to: 'no'.
>> This config worked as expected when using an external acl type for the
>> same request, but i also saw the same issue when using note when i tried
>> to use it to classify the external acl.
>> Is there perhaps a bug/misconfiguration with the note acl?
> 
> Sorry, I do not have enough information to answer that question. Try
> sharing an ALL,9 cache.log that reproduces the problem using a single
> transaction:
> https://urldefense.com/v3/__https://wiki.squid-cache.org/SquidFaq/BugReporting*Debugging_a_single_transaction__;Iw!!ORgEfCBsr282Fw!9mAZmYL4yTa1_p9UoNH3-uc5baXXKzSf1Zi9SkjsnCkMq_JCg50KhEXmwGcowILvtw$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/SquidFaq/BugReporting*Debugging_a_single_transaction__;Iw!!ORgEfCBsr282Fw!9mAZmYL4yTa1_p9UoNH3-uc5baXXKzSf1Zi9SkjsnCkMq_JCg50KhEXmwGcowILvtw$>
> 
> 
> 
>> The current config is:
>> 
>> acl classifyRequest note -m X-PMeta-Bypass yes
>> 
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> 
>> ssl_bump peek step1
>> ssl_bump splice step2 classifyRequest
>> ssl_bump stare all
>> ssl_bump bump all
> 
>> adaptation_meta X-PMeta-Bypass "%adapt::<last_h{X-PMeta-Splice}"
> 
> This adaptation_meta configuration should pass X-PMeta-Bypass
> meta-header field to the adaptation service, with the field value set to
> the value of the value of X-PMeta-Splice meta-header field received by
> Squid from the previously applied adaption service (within the same
> master transaction). Is that what you want?
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Tuesday, June 15, 2021 21:35
>> *To:* Frida Safran <fsafran at proofpoint.com>;
>> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>> ?
>> On 6/15/21 4:18 AM, Frida Safran wrote:
>> 
>>> In addition to using an external acl to annotate connections and decide
>>> whether splice/bump, I would like to try using an ecap service to
>>> achieve this.
>>> I would like to create an acl using info from the ecap service, and
>>> bump/splice using the following configuration:
>>> 
>>> acl classifyRequest note splice yes
>>> 
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> 
>>> ssl_bump peek step1
>>> ssl_bump splice step2 classifyRequest
>>> ssl_bump stare all
>>> ssl_bump bump all
>>> 
>>> 
>>> If I set a custom option from within the ecap service, can i use that
>>> via the above note acl?
>> 
>> I believe that should work in principle, provided the eCAP service is
>> consulted before Squid starts evaluating "ssl_bump splice step2
>> classifyRequest". If you do not restrict what requests your eCAP REQMOD
>> service gets, then it should be consulted during step1 (at least). The
>> logic is the same for eCAP and ICAP here -- Squid "primary" code does
>> not know the difference between those two adaptation interfaces.
>> 
>> 
>>> Can i set a custom option without setting it in
>>> 'adaptation_masterx_shared_names'
>> 
>> Yes, you should be able to. The adaptation_masterx_shared_names
>> directive is unrelated to setting transaction annotations IIRC. It is
>> about sharing metadata across adaptation services.
>> 
>> 
>>> Or, should I instead use:
>>> 
>>> acl classifyRequest note %adapt::<last_h{splice} yes
>> 
>> This option cannot work AFAICT because the "note" ACL requires a
>> constant (i.e. known at configuration time) annotation name. Squid will
>> not substitute %adapt::<last_h{splice} with anything in this context.
>> 
>> 
>> HTH,
>> 
>> Alex.
>> 
>> 
>>> ------------------------------------------------------------------------
>>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>>> *Sent:* Monday, June 14, 2021 16:24
>>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>> ?
>>> On 6/14/21 2:29 AM, Frida Safran wrote:
>>> 
>>>> Regarding proxy_protocol - is there a known patch for v4 I could use by
>>>> any chance?
>>> 
>>> I am not aware of any such patches. The changes were significant, fixing
>>> many PROXY protocol handling bugs. Virtually anything can be backported,
>>> but it would be a large effort with noticeable stability risks and
>>> long-term maintenance overheads. Preparing for a v5 upgrade may be a
>>> better strategy in this particular case.
>>> 
>>> 
>>>> Regarding icap, I suppose the acl is getting evaluated before the icap
>>>> and that is why they aren't available:
>>> 
>>>> acl classifyRequest external TransactionClassificator
>>>> ssl_bump peek step1
>>>> ssl_bump splice step2 classifyRequest
>>>> ssl_bump stare all
>>>> ssl_bump bump all
>>> 
>>> According to [1], the above configuration should result in two ICAP
>>> REQMOD requests (if configured) before classifyRequest is consulted
>>> during step2. I am aware of SslBump bugs in that area, but I would
>>> expect at least one ICAP REQMOD request anyway. The requests
>>> existence/timing should be easy to confirm using cache.log with
>>> debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
>>> pausing external ACL script in combination with an icap_log (to compare
>>> logged timestamps).
>>> 
>>> [1]
>>> https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>
>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>>
>>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>>>
>>> 
>>> 
>>> 
>>> HTH,
>>> 
>>> Alex.
>>> 
>>> 
>>>> ------------------------------------------------------------------------
>>>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>>>> *Sent:* Sunday, June 13, 2021 17:46
>>>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>>>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>>>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>>> ?
>>>> On 6/13/21 7:31 AM, Frida Safran wrote:
>>>> 
>>>>>? 1. Is it possible to pass proxy protocol headers to an external acl as
>>>>>???? part of the format?
>>>> 
>>>> It should be possible. Use %proxy_protocol::>h logformat %code in your
>>>> external_acl_type FORMAT configuration. We added that support to Squid
>>>> v5. Not available in the official v4.
>>>> 
>>>> 
>>>>>? 2. Is it possible to pass all/specific icap headers to an external acl?
>>>>>???? I have been trying to use %icap::>h to pass all the icap headers to
>>>>>???? an external acl, but it resolves to "-"
>>>> 
>>>> It should be possible if your external ACL is evaluated _after_ the
>>>> corresponding ICAP headers are received, but I would not be surprised if
>>>> there are bugs in this area -- the ICAP headers may be available but not
>>>> ?provided to the ACL evaluation code. Which squid.conf directive is
>>>> triggering your external ACL evaluation in this use case?
>>>> 
>>>> 
>>>> HTH,
>>>> 
>>>> Alex.
>>> 
>> 
> 



From fsafran at proofpoint.com  Sun Jun 20 06:55:52 2021
From: fsafran at proofpoint.com (Frida Safran)
Date: Sun, 20 Jun 2021 06:55:52 +0000
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <2deb3f71-9300-d4af-112b-ba84547493ed@measurement-factory.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
 <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
 <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>
 <f48985cf-35ca-3de6-d314-f490c5222169@measurement-factory.com>
 <MN2PR12MB43410854B3851A87FC0F39B6D20E9@MN2PR12MB4341.namprd12.prod.outlook.com>,
 <2deb3f71-9300-d4af-112b-ba84547493ed@measurement-factory.com>
Message-ID: <MN2PR12MB43418DEB16FFF9BF089F62F2D20B9@MN2PR12MB4341.namprd12.prod.outlook.com>

As far as I understand the ecap service should be called for each step:

  *   In step1 there is no SNI, and X-PMeta-Splice should be set in the ecap to 'no'.
  *   In step2 there should be an SNI, and X-PMeta-Splice should be set to 'yes', and the classifyRequest acl should return true, causing the request to be spliced.

In the debug log however, I see prints saying that it was decided to bump the request, suggesting that step2 is not working as expected.
Please advise how to proceed?
________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
Sent: Thursday, June 17, 2021 18:41
To: Frida Safran <fsafran at proofpoint.com>; squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Passing Proxy Protocol Headers to external ACL

On 6/17/21 1:55 AM, Frida Safran wrote:

> I see that the issue stems from that the %ssl::sni is not getting passed
> correctly (evaluated as "-") for CONNECT requests when passed via:
>
> adaptation_meta X-PMeta-SNI "%ssl::sni"
>
> Why could that be? I do see that in the access.log the %ssl::sni is
> logged correctly on CONNECT entries.


Perhaps it is because SNI is never available during SslBump step1? It is
not clear (to me) whether you are talking about step1 or step2
adaptation_meta evaluation. Also, there could be Squid bugs that result
in missing adaptation_meta re-evaluations during step2 -- I have not
checked.


> I have worked around this by passing the "%>rd" instead - are these 2
> values equivalent in all cases?

No, they are not.

1. SNI should always be a domain name. CONNECT request URI may be an IP.

1.1 A true CONNECT request may use an IP address -- client choice.

1.2 A faked during step1 CONNECT request uses an IP address.
   This faking always happens when you use SslBump on https_port.

2. SNI may differ from the domain name in the CONNECT request.
  For example, the CONNECT request may be for example.com while
  SNI may say something more specific like service.example.com.
  Some SNI domains are obfuscated/encrypted.

3. SNI may be missing in TLS ClientHello.
   CONNECT requests always have a URI.

HTH,

Alex.

> ------------------------------------------------------------------------
> *From:* Alex Rousskov <rousskov at measurement-factory.com>
> *Sent:* Wednesday, June 16, 2021 17:38
> *To:* Frida Safran <fsafran at proofpoint.com>;
> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>
> On 6/16/21 4:15 AM, Frida Safran wrote:
>
>> I see that the configured acl is not called for all requests.
>
> classifyRequest should be evaluated for all CONNECT requests that get to
> SslBump step2. That should be all CONNECT requests except for (rare)
> cases where Squid cannot peek at the TLS ClientHello during step1 and,
> hence, bumps the client connection without reaching step2.
>
> Whether classifyRequest _matches_ when evaluated is a different story,
> of course.
>
>> I do see that the ecap is called, it sets the option X-PMeta-Splice
>> via visitEachOption correctly, but the classifyRequest note acl is not
>> evaluated, and the request is not spliced.
>> However, I do see that this acl is evaluated for some other requests,
>> but only when the X-PMeta-Bypass is set to: 'no'.
>> This config worked as expected when using an external acl type for the
>> same request, but i also saw the same issue when using note when i tried
>> to use it to classify the external acl.
>> Is there perhaps a bug/misconfiguration with the note acl?
>
> Sorry, I do not have enough information to answer that question. Try
> sharing an ALL,9 cache.log that reproduces the problem using a single
> transaction:
> https://urldefense.com/v3/__https://wiki.squid-cache.org/SquidFaq/BugReporting*Debugging_a_single_transaction__;Iw!!ORgEfCBsr282Fw!9mAZmYL4yTa1_p9UoNH3-uc5baXXKzSf1Zi9SkjsnCkMq_JCg50KhEXmwGcowILvtw$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/SquidFaq/BugReporting*Debugging_a_single_transaction__;Iw!!ORgEfCBsr282Fw!9mAZmYL4yTa1_p9UoNH3-uc5baXXKzSf1Zi9SkjsnCkMq_JCg50KhEXmwGcowILvtw$>
>
>
>
>> The current config is:
>>
>> acl classifyRequest note -m X-PMeta-Bypass yes
>>
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>>
>> ssl_bump peek step1
>> ssl_bump splice step2 classifyRequest
>> ssl_bump stare all
>> ssl_bump bump all
>
>> adaptation_meta X-PMeta-Bypass "%adapt::<last_h{X-PMeta-Splice}"
>
> This adaptation_meta configuration should pass X-PMeta-Bypass
> meta-header field to the adaptation service, with the field value set to
> the value of the value of X-PMeta-Splice meta-header field received by
> Squid from the previously applied adaption service (within the same
> master transaction). Is that what you want?
>
>
> HTH,
>
> Alex.
>
>
>
>> ------------------------------------------------------------------------
>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>> *Sent:* Tuesday, June 15, 2021 21:35
>> *To:* Frida Safran <fsafran at proofpoint.com>;
>> squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>
>> On 6/15/21 4:18 AM, Frida Safran wrote:
>>
>>> In addition to using an external acl to annotate connections and decide
>>> whether splice/bump, I would like to try using an ecap service to
>>> achieve this.
>>> I would like to create an acl using info from the ecap service, and
>>> bump/splice using the following configuration:
>>>
>>> acl classifyRequest note splice yes
>>>
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>>
>>> ssl_bump peek step1
>>> ssl_bump splice step2 classifyRequest
>>> ssl_bump stare all
>>> ssl_bump bump all
>>>
>>>
>>> If I set a custom option from within the ecap service, can i use that
>>> via the above note acl?
>>
>> I believe that should work in principle, provided the eCAP service is
>> consulted before Squid starts evaluating "ssl_bump splice step2
>> classifyRequest". If you do not restrict what requests your eCAP REQMOD
>> service gets, then it should be consulted during step1 (at least). The
>> logic is the same for eCAP and ICAP here -- Squid "primary" code does
>> not know the difference between those two adaptation interfaces.
>>
>>
>>> Can i set a custom option without setting it in
>>> 'adaptation_masterx_shared_names'
>>
>> Yes, you should be able to. The adaptation_masterx_shared_names
>> directive is unrelated to setting transaction annotations IIRC. It is
>> about sharing metadata across adaptation services.
>>
>>
>>> Or, should I instead use:
>>>
>>> acl classifyRequest note %adapt::<last_h{splice} yes
>>
>> This option cannot work AFAICT because the "note" ACL requires a
>> constant (i.e. known at configuration time) annotation name. Squid will
>> not substitute %adapt::<last_h{splice} with anything in this context.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> ------------------------------------------------------------------------
>>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>>> *Sent:* Monday, June 14, 2021 16:24
>>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>>
>>> On 6/14/21 2:29 AM, Frida Safran wrote:
>>>
>>>> Regarding proxy_protocol - is there a known patch for v4 I could use by
>>>> any chance?
>>>
>>> I am not aware of any such patches. The changes were significant, fixing
>>> many PROXY protocol handling bugs. Virtually anything can be backported,
>>> but it would be a large effort with noticeable stability risks and
>>> long-term maintenance overheads. Preparing for a v5 upgrade may be a
>>> better strategy in this particular case.
>>>
>>>
>>>> Regarding icap, I suppose the acl is getting evaluated before the icap
>>>> and that is why they aren't available:
>>>
>>>> acl classifyRequest external TransactionClassificator
>>>> ssl_bump peek step1
>>>> ssl_bump splice step2 classifyRequest
>>>> ssl_bump stare all
>>>> ssl_bump bump all
>>>
>>> According to [1], the above configuration should result in two ICAP
>>> REQMOD requests (if configured) before classifyRequest is consulted
>>> during step2. I am aware of SslBump bugs in that area, but I would
>>> expect at least one ICAP REQMOD request anyway. The requests
>>> existence/timing should be easy to confirm using cache.log with
>>> debug_options set to at least "ALL,3 82,9 93,9" and/or a logging or
>>> pausing external ACL script in combination with an icap_log (to compare
>>> logged timestamps).
>>>
>>> [1]
>>> https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>
>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>>
>>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
>> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$
> <https://urldefense.com/v3/__https://wiki.squid-cache.org/Features/SslPeekAndSplice__;!!ORgEfCBsr282Fw!6SDlCpq2n2kV1WuiNQ7focWt2YMDj-Xs9aEp29dC32pwZUHSLIkrD7dnojPghFBhQQ$>>>
>>>
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>> ------------------------------------------------------------------------
>>>> *From:* Alex Rousskov <rousskov at measurement-factory.com>
>>>> *Sent:* Sunday, June 13, 2021 17:46
>>>> *To:* squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
>>>> *Cc:* Frida Safran <fsafran at proofpoint.com>
>>>> *Subject:* Re: [squid-users] Passing Proxy Protocol Headers to external ACL
>>>>
>>>> On 6/13/21 7:31 AM, Frida Safran wrote:
>>>>
>>>>>  1. Is it possible to pass proxy protocol headers to an external acl as
>>>>>     part of the format?
>>>>
>>>> It should be possible. Use %proxy_protocol::>h logformat %code in your
>>>> external_acl_type FORMAT configuration. We added that support to Squid
>>>> v5. Not available in the official v4.
>>>>
>>>>
>>>>>  2. Is it possible to pass all/specific icap headers to an external acl?
>>>>>     I have been trying to use %icap::>h to pass all the icap headers to
>>>>>     an external acl, but it resolves to "-"
>>>>
>>>> It should be possible if your external ACL is evaluated _after_ the
>>>> corresponding ICAP headers are received, but I would not be surprised if
>>>> there are bugs in this area -- the ICAP headers may be available but not
>>>>  provided to the ACL evaluation code. Which squid.conf directive is
>>>> triggering your external ACL evaluation in this use case?
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210620/7359613b/attachment.htm>

From doconnor at transsee.ca  Sun Jun 20 18:41:40 2021
From: doconnor at transsee.ca (Darwin O'Connor)
Date: Sun, 20 Jun 2021 14:41:40 -0400
Subject: [squid-users] Data not being cached
Message-ID: <099ec2a0-65eb-bf74-fcb7-7380a9c606c4@transsee.ca>

I run a transit prediction web app <https://www.transsee.ca/>. It 
connects to a variety of web APIs to collect the real time transit data 
it needs. The app's activities are split among many processes. They 
currently uses libcurl to connect to squid for caching (often for as 
little as 10-30 seconds) and benefits of connection sharing.

There is still cases where data isn't being cached no matter what I do. 
It is https data, but I am able to cache other https pages like 
https://cdn.mbta.com/realtime/Alerts.pb

The refresh_pattern:

refresh_pattern .?????????????? 60????? 99999%? 7200 override-expire 
override-lastmod reload-into-ims ignore-reload ignore-no-cache 
ignore-no-store ignore-private ignore-auth store-stale

The http headers from curl of an example where it is not being cached:

*?? Trying 127.0.0.1:3128...
* Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
 > GET https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses HTTP/1.1
Host: 127.0.0.1:3128
User-Agent: curl/7.77.0 (+https://www.transsee.ca/)
Accept: */*
Accept-Encoding: gzip
Authorization: apikey 2eYEqXXxOPEDChnpeF7sZL2aR8moD2DtdNmn
Cache-Control: max-age=60
Content-Encoding: aes128gcm

* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Date: Sun, 20 Jun 2021 17:52:14 GMT
< Content-Type: application/protobuf
< Content-Length: 7455
< Set-Cookie: 
AWSALB=EEAv0EXNaGzBkgAPS1JzrAiQZi7aKDk063qjM4ApeOuIArbwQ+s17/jimuq7AzzuC+a35t5Lx/y+H5YMqTC9bL6LqS4Y75auumXt7LWnvSFE/SMXQd/OFa6TSCW9; 
Expires=Sun, 27 Jun 2021 17:52:14 GMT; Path=/
< Set-Cookie: 
AWSALBCORS=EEAv0EXNaGzBkgAPS1JzrAiQZi7aKDk063qjM4ApeOuIArbwQ+s17/jimuq7AzzuC+a35t5Lx/y+H5YMqTC9bL6LqS4Y75auumXt7LWnvSFE/SMXQd/OFa6TSCW9; 
Expires=Sun, 27 Jun 2021 17:52:14 GMT; Path=/; SameSite=None; Secure
< Server: Apache-Coyote/1.1
< X-Powered-By: Express
< Access-Control-Allow-Credentials: true
< ETag: W/"ab70-8SI2GdBV4SJG4edSc4E5W8LBJWk"
< Vary: Accept-Encoding
< X-Cache: Hit from cloudfront
< X-Amz-Cf-Pop: SYD1-C1
< X-Amz-Cf-Id: hCoQckLsNONQMpgPr2kXJVdTDHu98jxl-rPXqV_PHB2vTCEomAd-Nw==
< Age: 35
< Access-Control-Allow-Origin: *
< Content-Encoding: gzip
< X-Cache: MISS from transsee
< X-Cache-Lookup: MISS from transsee:3128
< Via: 1.1 359a113ca166631b42f31a0f2e6a1aab.cloudfront.net (CloudFront), 
1.1 transsee (squid/4.15)
< Connection: keep-alive

Here is a sample from the Squid access log:

1624212034.891??? 246 127.0.0.1 59216 TCP_MISS/200 8517 GET 
https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses - 
HIER_DIRECT/52.65.222.24 application/protobuf



From moberger at metanetworks.com  Mon Jun 21 12:18:35 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Mon, 21 Jun 2021 15:18:35 +0300
Subject: [squid-users] cacheProtoClientHttpRequests OID
Message-ID: <CAGSk-43a1oZu0F1NDcEyRiLOQ=+CapPCZ=bCSYxXjomaczN67g@mail.gmail.com>

Hi,

If I have the cache disabled:

> cache deny all


Can we be sure the OID cacheProtoClientHttpRequests really counts the HTTP
requests received by Squid (v4.15)?

https://wiki.squid-cache.org/Features/Snmp

Thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210621/d4349790/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 21 12:29:56 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Jun 2021 00:29:56 +1200
Subject: [squid-users] Data not being cached
In-Reply-To: <099ec2a0-65eb-bf74-fcb7-7380a9c606c4@transsee.ca>
References: <099ec2a0-65eb-bf74-fcb7-7380a9c606c4@transsee.ca>
Message-ID: <141c2c53-171a-f30d-6658-ab1044ca24c3@treenet.co.nz>

On 21/06/21 6:41 am, Darwin O'Connor wrote:
> I run a transit prediction web app <https://www.transsee.ca/>. It 
> connects to a variety of web APIs to collect the real time transit data 
> it needs. The app's activities are split among many processes. They 
> currently uses libcurl to connect to squid for caching (often for as 
> little as 10-30 seconds) and benefits of connection sharing.
> 
> There is still cases where data isn't being cached no matter what I do. 
> It is https data, but I am able to cache other https pages like 
> https://cdn.mbta.com/realtime/Alerts.pb
> 
> The refresh_pattern:
> 
> refresh_pattern .?????????????? 60????? 99999%? 7200 override-expire 
> override-lastmod reload-into-ims ignore-reload ignore-no-cache 
> ignore-no-store ignore-private ignore-auth store-stale
> 

Please be aware that several of those options may be causing you more 
problems than they solve:

* ignore-no-cache - no longer exists.

* ignore-reload - contradicts reload-into-ims and that can cause 
inconsistent behaviour between initial MISS and followup HIT responses.

* override-lastmod - replaces all Last-Modification (L-M) headers with 
values indicating the object is almost brand new. That prevents caches 
detecting that objects have stuck around long enough not to need replacing.
  Luckily the example response has no L-M header, so that is not the 
problem here. But it may be affecting other traffic.



> The http headers from curl of an example where it is not being cached:
> 
> *?? Trying 127.0.0.1:3128...
> * Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
>  > GET https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses HTTP/1.1
> Host: 127.0.0.1:3128
> User-Agent: curl/7.77.0 (+https://www.transsee.ca/)
> Accept: */*
> Accept-Encoding: gzip

Note this header value is used for variant selection of the responses.

Is this test value the same as what the client apps send?


> Authorization: apikey 2eYEqXXxOPEDChnpeF7sZL2aR8moD2DtdNmn
> Cache-Control: max-age=60
> Content-Encoding: aes128gcm

Apparently the GET message contains some content. Yet there is no 
Content-Length or Transfer-Encoding to determine the length of that 
content.


> 
> * Mark bundle as not supporting multiuse
> < HTTP/1.1 200 OK
> < Date: Sun, 20 Jun 2021 17:52:14 GMT
> < Content-Type: application/protobuf
> < Content-Length: 7455
...
> < Access-Control-Allow-Credentials: true
> < ETag: W/"ab70-8SI2GdBV4SJG4edSc4E5W8LBJWk"
> < Vary: Accept-Encoding
> < X-Cache: Hit from cloudfront
> < X-Amz-Cf-Pop: SYD1-C1
> < X-Amz-Cf-Id: hCoQckLsNONQMpgPr2kXJVdTDHu98jxl-rPXqV_PHB2vTCEomAd-Nw==
> < Age: 35
> < Access-Control-Allow-Origin: *
> < Content-Encoding: gzip
> < X-Cache: MISS from transsee
> < X-Cache-Lookup: MISS from transsee:3128
> < Via: 1.1 359a113ca166631b42f31a0f2e6a1aab.cloudfront.net (CloudFront), 
> 1.1 transsee (squid/4.15)
> < Connection: keep-alive
> 
> Here is a sample from the Squid access log:
> 
> 1624212034.891??? 246 127.0.0.1 59216 TCP_MISS/200 8517 GET 
> https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses - 
> HIER_DIRECT/52.65.222.24 application/protobuf
> 

FYI, A single request is not sufficient to demonstrate caching issues. 
Caches always require one MISS to fetch the data which then is expected 
to show up as HIT on the second or later requests.


Couple of things going on here with timing make this a MISS:

* The log timestamp says your test happened at Mon, 21 Jun 2021 12:21:11 GMT

* The response Date says it was created Sun, 20 Jun 2021 17:52:14 GMT

* The request Cache-Control forbids receiving anything from cache older 
than 60 seconds.

So, the response being many hours old cannot be delivered from a cache 
to this test request.

NP: CloudFront are a reverse-proxy service, thus allowed to ignore 
Cache-Control from clients. Which is why they respond with a HIT.


Amos


From squid3 at treenet.co.nz  Mon Jun 21 12:35:09 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Jun 2021 00:35:09 +1200
Subject: [squid-users] cacheProtoClientHttpRequests OID
In-Reply-To: <CAGSk-43a1oZu0F1NDcEyRiLOQ=+CapPCZ=bCSYxXjomaczN67g@mail.gmail.com>
References: <CAGSk-43a1oZu0F1NDcEyRiLOQ=+CapPCZ=bCSYxXjomaczN67g@mail.gmail.com>
Message-ID: <80b19474-0b60-8fec-63cd-d302ed1959d1@treenet.co.nz>

On 22/06/21 12:18 am, Moti Berger wrote:
> Hi,
> 
> If I have the cache disabled:
> 
>     cache deny all
> 
> 
> Can we be sure the OID cacheProtoClientHttpRequests really counts the 
> HTTP requests received by Squid (v4.15)?
> 

The caching feature has nothing to do with that OID. So yes you SHOULD 
despite what the 'cache' directive contains.

Be aware though that OID are limited to 32-bit counters. They can wrap 
in modern proxies that run for very long times and/or high traffic loads.

Amos


From squid3 at treenet.co.nz  Mon Jun 21 12:39:10 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 Jun 2021 00:39:10 +1200
Subject: [squid-users] Passing Proxy Protocol Headers to external ACL
In-Reply-To: <MN2PR12MB43418DEB16FFF9BF089F62F2D20B9@MN2PR12MB4341.namprd12.prod.outlook.com>
References: <MN2PR12MB43418F44454A9934801303C8D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <MN2PR12MB4341AC2BD1E9F15B26BA02F1D2329@MN2PR12MB4341.namprd12.prod.outlook.com>
 <35760c1b-db1b-095c-ee33-6989b5c5e613@measurement-factory.com>
 <MN2PR12MB4341DB0F1C1EC0E488D4FC3BD2319@MN2PR12MB4341.namprd12.prod.outlook.com>
 <58d328b4-1d1f-0c1f-cd1a-2bdf0d402478@measurement-factory.com>
 <MN2PR12MB43413C28D8BC0934EAE308B2D2309@MN2PR12MB4341.namprd12.prod.outlook.com>
 <7de490bb-8182-a6b3-6bb6-70867e29af91@measurement-factory.com>
 <MN2PR12MB43411DD42A9CABEA086EF1DBD20F9@MN2PR12MB4341.namprd12.prod.outlook.com>
 <f48985cf-35ca-3de6-d314-f490c5222169@measurement-factory.com>
 <MN2PR12MB43410854B3851A87FC0F39B6D20E9@MN2PR12MB4341.namprd12.prod.outlook.com>
 <2deb3f71-9300-d4af-112b-ba84547493ed@measurement-factory.com>
 <MN2PR12MB43418DEB16FFF9BF089F62F2D20B9@MN2PR12MB4341.namprd12.prod.outlook.com>
Message-ID: <855e2158-5ca8-8b12-5033-a4053c7764bc@treenet.co.nz>

On 20/06/21 6:55 pm, Frida Safran wrote:
> As far as I understand the ecap service should be called for each step:
> 
>   * In step1 there is no SNI, and X-PMeta-Splice should be set in the
>     ecap to 'no'.
>   * In step2 there should be an SNI, and X-PMeta-Splice should be set to
>     'yes', and the classifyRequest acl should return true, causing the
>     request to be spliced.
> 
> In the debug log however, I see prints saying that it was decided to 
> bump the request, suggesting that step2 is not working as expected.
> Please advise how to proceed?

Since the thing deciding between splice and stare at step2 is an ACL
test I would add debug_options "28,5 82,5" to your log output to see
what the ACL is doing to get the bump.

Amos


From rousskov at measurement-factory.com  Mon Jun 21 17:56:12 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Jun 2021 13:56:12 -0400
Subject: [squid-users] cacheProtoClientHttpRequests OID
In-Reply-To: <CAGSk-43a1oZu0F1NDcEyRiLOQ=+CapPCZ=bCSYxXjomaczN67g@mail.gmail.com>
References: <CAGSk-43a1oZu0F1NDcEyRiLOQ=+CapPCZ=bCSYxXjomaczN67g@mail.gmail.com>
Message-ID: <5575c018-52ed-2ca5-b5b4-2edaa5799969@measurement-factory.com>

On 6/21/21 8:18 AM, Moti Berger wrote:

> Can we be sure the OID?cacheProtoClientHttpRequests really counts the
> HTTP requests received by Squid (v4.15)?

In _addition_ to what Amos has said: AFAICT, that counter counts
completed or finished more-or-less well-formed HTTP, HTTPS, and FTP
transactions that are not excluded by the stats_collection directive. If
you use SslBump, keep in mind that some SslBump steps are treated as
transactions.

Very roughly speaking, in most configurations, that counter is
incremented once whenever Squid adds a line to access.log.


HTH,

Alex.
P.S. The OID documentation at https://wiki.squid-cache.org/Features/Snmp
lies about "received". It is more like "serviced", "completed", etc.



From marcelorodrigo at graminsta.com.br  Mon Jun 21 18:20:15 2021
From: marcelorodrigo at graminsta.com.br (marcelorodrigo at graminsta.com.br)
Date: Mon, 21 Jun 2021 15:20:15 -0300
Subject: [squid-users] Make Squid to route out through specific ports
Message-ID: <1bc4332eb6075125e214862c6381d268@graminsta.com.br>

Hi ;)

I would like to use tcp_outgoing_address to route out to a specific IP 
and port.

How does is the correct way to do this?

I could not find any answer about how to do it in any forum.

I am trying to route out to an 4g router and unfortunately this router 
just can route using different ports at the same IP.


Tks in advance

Marcelo


From uhlar at fantomas.sk  Mon Jun 21 18:49:34 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 21 Jun 2021 20:49:34 +0200
Subject: [squid-users] Make Squid to route out through specific ports
In-Reply-To: <1bc4332eb6075125e214862c6381d268@graminsta.com.br>
References: <1bc4332eb6075125e214862c6381d268@graminsta.com.br>
Message-ID: <20210621184934.GB15086@fantomas.sk>

On 21.06.21 15:20, marcelorodrigo at graminsta.com.br wrote:
>I would like to use tcp_outgoing_address to route out to a specific IP 
>and port.
>
>How does is the correct way to do this?
>
>I could not find any answer about how to do it in any forum.
>
>I am trying to route out to an 4g router and unfortunately this router 
>just can route using different ports at the same IP.

tcp_outgoing_address congures which IP address on server running SQUID will
be used for outgoing connections as source address.

routers can later route different address to different links or give them
different priorities.

It apparently does not do what you mean

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Your mouse has moved. Windows NT will now restart for changes to take
to take effect. [OK]


From hoper at free.fr  Tue Jun 22 09:20:59 2021
From: hoper at free.fr (hoper at free.fr)
Date: Tue, 22 Jun 2021 11:20:59 +0200 (CEST)
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <632609882.119122182.1624352443542.JavaMail.root@zimbra84-e15.priv.proxad.net>
Message-ID: <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>


Hi,

I have a big problem here. If someone know well squid helpers and external acl, please take a look.
We are using a database with a list of user, and the proxy they need to use.

So in squid.conf file, we declare an external acl:

-------------------------------------------------
auth_param basic program /mydir/myprogram.sh
auth_param basic children 10 startup=1 idle=3
auth_param basic realm myrealm
auth_param basic credentialsttl 2 minutes
-------------------------------------------------

program.sh will check the login/password given by the user.
again the ones found in the database. And, if the authentication
is sucessfull, it also write on stdout the proxy we need to use for this user.

Example (If this user need to use the proxy number 2):
OK proxychoice=p2

If the squid configuration file, we also include another file,
which look like this :

-----------------------------------------------------------------
cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
acl p1auth note proxychoice p1
cache_peer_access proxy1 allow p1auth
http_access allow authenticated p1auth
cache_peer_access proxy1 deny all

cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
acl p2auth note proxychoice p2
cache_peer_access proxy2 allow p2auth
http_access allow authenticated p2auth
cache_peer_access proxy2 deny all
-----------------------------------------------------------------

This configuration is working. The parent proxy used by squid is the good one.
BUT: If we change the configuration (proxy for a user) in the database,
the change is not take into account until we fully restart squid :(
(Even squid -k reconfigure does not work).

Please, any ideas ? What can we do to make this "dynamic" ? 

Any change in the database should be taken into account immediatly.
First I though it was link to credentialsttl value, but no.
No changes happens until a full restart :(

Thanks.


From rousskov at measurement-factory.com  Tue Jun 22 14:05:52 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jun 2021 10:05:52 -0400
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
References: <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
Message-ID: <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>

On 6/22/21 5:20 AM, hoper at free.fr wrote:

> We are using a database with a list of user, and the proxy they need to use.
> 
> So in squid.conf file, we declare an external acl:

FYI: Your are not declaring an external ACL. You are declaring an
authentication helper.

> -------------------------------------------------
> auth_param basic program /mydir/myprogram.sh
> auth_param basic children 10 startup=1 idle=3
> auth_param basic realm myrealm
> auth_param basic credentialsttl 2 minutes
> -------------------------------------------------
> 
> program.sh will check the login/password given by the user.
> again the ones found in the database. And, if the authentication
> is sucessfull, it also write on stdout the proxy we need to use for this user.
> 
> Example (If this user need to use the proxy number 2):
> OK proxychoice=p2
> 
> If the squid configuration file, we also include another file,
> which look like this :
> 
> -----------------------------------------------------------------
> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
> acl p1auth note proxychoice p1
> cache_peer_access proxy1 allow p1auth
> http_access allow authenticated p1auth
> cache_peer_access proxy1 deny all
> 
> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
> acl p2auth note proxychoice p2
> cache_peer_access proxy2 allow p2auth
> http_access allow authenticated p2auth
> cache_peer_access proxy2 deny all
> -----------------------------------------------------------------

The relative order of your cache_peer_access and http_access directives
may imply that you think that cache_peer_access is applied/decided
first. In reality, http_access is applied/decided first. This
configuration change does not affect Squid decisions, but the following
order would match the reality better:

  acl p1auth note proxychoice p1
  acl p2auth note proxychoice p2

  http_access allow authenticated p1auth
  http_access allow authenticated p2auth

  cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
  cache_peer_access proxy1 allow p1auth
  cache_peer_access proxy1 deny all

  cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
  cache_peer_access proxy2 allow p2auth
  cache_peer_access proxy2 deny all


BTW, if you want to allow all authenticated users, regardless of their
cache_peer choice, then you can remove p1auth and p2auth from
http_access lines.



> This configuration is working. The parent proxy used by squid is the good one.
> BUT: If we change the configuration (proxy for a user) in the database,
> the change is not take into account until we fully restart squid :(
> (Even squid -k reconfigure does not work).
> 
> Please, any ideas ? What can we do to make this "dynamic" ? 

If you want the changes to apply to every received HTTP request, then
you need to make sure that your authentication helper is called for
every received request. That means disabling caching of authenticated
credentials. It also means that this approach will not work well for
bumped HTTP requests because their authentication information is (or
should be) inherited from their CONNECT tunnel. You will have to force
CONNECT tunnels to have at most one request by disabling persistent
connections, I guess.

I would also consider separating user authentication from request
routing. To do that, move the proxychoice=... annotation functionality
from the authentication helper to the external ACL helper (with caching
disabled via external_acl_type options). In this approach,
authentication results can be cached for as long as you want without
interfering with more up-to-date routing decisions. However, you should
not use that [slow] external ACL with cache_peer_access lines as
detailed below).


> Any change in the database should be taken into account immediatly.

cache_peer_access does not support slow ACLs (yet -- we have a
backburner project to add such support; it is a large change). Until
such support is added, there will always be some delay between obtaining
routing information (at authentication or external_acl computation time)
and applying that routing information (at request routing time).


HTH,

Alex.


From arctic5824 at protonmail.com  Tue Jun 22 19:32:10 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 19:32:10 +0000
Subject: [squid-users] Newbie question, How to fully disable/disallow https?
Message-ID: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>

Hello, Recently I setup my first squid proxy,

I want it when users try to acces a website via https, they get redirected to the http version, I tried disabling https by reading the comments in the config, the squid docs, and online forums, but I am unable to figure this out, I also tried blocking port 443 using ufw but it just resulted in users timing out.

Please rest assured I understand the security and other risks this brings, thanks.
To reiterate as this email is a bit long, I'd like to know how to dis-allow https and redirect users to http versions of websites when they try to use https
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210622/939fb626/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 22 19:40:48 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jun 2021 15:40:48 -0400
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
Message-ID: <9e9c4890-681c-fb41-43ba-e7ed7458b2e5@measurement-factory.com>

On 6/22/21 3:32 PM, Arctic5824 wrote:
> Hello, Recently I setup my first squid proxy,
> 
> I want it when users try to acces a website via https, they get
> redirected to the http version, I tried disabling https by reading the
> comments in the config, the squid docs, and online forums, but I am
> unable to figure this out, I also tried blocking port 443 using ufw but
> it just resulted in users timing out.
> 
> Please rest assured I understand the security and other risks this
> brings, thanks.
> To? reiterate as this email is a bit long, I'd like to know how to
> dis-allow https and redirect users to http versions of websites when
> they try to use https

To disable HTTPS access through the proxy, simply deny all CONNECT
requests using http_access rules.

Redirecting HTTPS attempts to HTTP is a lot harder and is unreliable.
You will have to bump TLS connections and then deny all bumped requests
with a redirection response (that many browsers may not even follow --
you should test this). This will not work in many cases because TLS is
not supposed to be bumped -- many clients and origin servers will work
hard to prevent you from bumping their connections. See ssl_bump and
deny_info for starting points.


HTH,

Alex.


From Antony.Stone at squid.open.source.it  Tue Jun 22 19:41:22 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Jun 2021 21:41:22 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
Message-ID: <202106222141.22874.Antony.Stone@squid.open.source.it>

On Tuesday 22 June 2021 at 21:32:10, Arctic5824 wrote:

> Hello, Recently I setup my first squid proxy,
> 
> I want it when users try to acces a website via https, they get redirected
> to the http version

1. What makes you believe that sites *have* an HTTP version?

2. What do you think should happen when sites *do* have an HTTP version, and 
that consists solely of a 301 Permanent Redirect to the HTTPS version, which 
contains the content?

(In other words, the actual web server is never going to provide the content 
you want to see if you only speak HTTP to it.)


Antony.

-- 
The words "e pluribus unum" on the Great Seal of the United States are from a 
poem by Virgil entitled "Moretum", which is about cheese and garlic salad 
dressing.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From arctic5824 at protonmail.com  Tue Jun 22 20:28:56 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 20:28:56 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106222141.22874.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106222141.22874.Antony.Stone@squid.open.source.it>
Message-ID: <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>

sorry, i accidentally sent email to you instead of the email list (Im new to mailing lists) so im re-sending it but to the list

On Tuesday, June 22nd, 2021 at 12:41 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> To disable HTTPS access through the proxy, simply deny all CONNECT
>
> requests using http_access rules.

Hey! thanks for the info, I just tried that but it seems https is still being allowed, and I can see it in the logs as well
"TCP_TUNNEL/200 717 CONNECT s.youtube.com:443 -"
my config is https://pastebin.com/8txzkEnG
and a version of the config without comments: https://pastebin.com/zuJYQpXW

thanks, any help will be appreciated
- Arctic


From rousskov at measurement-factory.com  Tue Jun 22 20:37:16 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jun 2021 16:37:16 -0400
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106222141.22874.Antony.Stone@squid.open.source.it>
 <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>
Message-ID: <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>

On 6/22/21 4:28 PM, Arctic5824 wrote:
>> To disable HTTPS access through the proxy, simply deny all CONNECT
>> requests using http_access rules.

> Hey! thanks for the info, I just tried that but it seems https is still being allowed, and I can see it in the logs as well
> "TCP_TUNNEL/200 717 CONNECT s.youtube.com:443 -"
> my config is https://pastebin.com/8txzkEnG
> and a version of the config without comments: https://pastebin.com/zuJYQpXW

> acl CONNECT method CONNECT
> http_access allow localhost
> http_access deny CONNECT

Squid bugs notwithstanding, either your Squid is not running with the
configuration that you have shared with us OR that logged request comes
from localhost. If you are not sure, I suggest shutting down Squid,
making sure that nobody listens on port 3128 and then restarting Squid.
Due to the first http_access rule, the test request must not come from
the same machine Squid runs on.

HTH,

Alex.
P.S. If you are worried about custom clients or scripts (not regular
browsers) bypassing your controls, then you will also need to ban "GET
https://..." requests, but let's figure out the above basics first.


From Antony.Stone at squid.open.source.it  Tue Jun 22 20:44:33 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Jun 2021 22:44:33 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>
 <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
Message-ID: <202106222244.33744.Antony.Stone@squid.open.source.it>

On Tuesday 22 June 2021 at 22:37:16, Alex Rousskov wrote:

> On 6/22/21 4:28 PM, Arctic5824 wrote:
> > 
> > Hey! thanks for the info, I just tried that but it seems https is still
> > being allowed, and I can see it in the logs as well "TCP_TUNNEL/200 717
> > CONNECT s.youtube.com:443 -"
> > my config is https://pastebin.com/8txzkEnG
> > and a version of the config without comments:
> > https://pastebin.com/zuJYQpXW

> Squid bugs notwithstanding, either your Squid is not running with the
> configuration that you have shared with us OR that logged request comes
> from localhost. If you are not sure, I suggest shutting down Squid,
> making sure that nobody listens on port 3128 and then restarting Squid.
> Due to the first http_access rule, the test request must not come from
> the same machine Squid runs on.

I would also comment on:

#http_access deny !Safe_ports

Has that been consciously and deliberately commented-out?

#http_access allow localnet
http_access allow localhost

Is that a typo?  Did you mean to allow access from your local networks, rather 
than just from localhost?

#http_access deny all

Has that been consciously and deliberately commented-out?


Antony.

-- 
Behind the counter a boy with a shaven head stared vacantly into space,
a dozen spikes of microsoft protruding from the socket behind his ear.

 - William Gibson, Neuromancer (1984)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From arctic5824 at protonmail.com  Tue Jun 22 20:53:08 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 20:53:08 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106222141.22874.Antony.Stone@squid.open.source.it>
 <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>
 <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
Message-ID: <7c5o62XwXF0ZekG6dRCrQ636GNU0ayvnqRZ7Kbk-hU4Y0IEauKgf1eQfi3MAgdHL1TCkesz5MlIvhezJ0HDleV0787nNyvsx2C0QGSp19u8=@protonmail.com>

On Tuesday, June 22nd, 2021 at 1:37 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> Squid bugs notwithstanding, either your Squid is not running with the  configuration that you have shared with us OR...

Hey, yes this is actually the case, for testing instead of
> http_access allow localhost
im running with
> http_access allow all

to make sure this isnt a configuration issue on my side, and that other users also can use https
(I know this probably wont bring good traffic but i have confirmed with my vps provider that they allow this type of stuff/traffic)



From arctic5824 at protonmail.com  Tue Jun 22 20:54:42 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 20:54:42 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106222244.33744.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>
 <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
 <202106222244.33744.Antony.Stone@squid.open.source.it>
Message-ID: <deSr9KfJTe4iwWf1o2I6_xE_xeX7-dJyS370ejBoyWua6pVNkc4yo1ZnrH2_taZCuAS9LY-QMPHfp_svihI8VqfSqVEAUuDUz871HXPJeCs=@protonmail.com>

On Tuesday, June 22nd, 2021 at 1:44 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 22 June 2021 at 22:37:16, Alex Rousskov wrote:
>
> > On 6/22/21 4:28 PM, Arctic5824 wrote:
> >
> > > Hey! thanks for the info, I just tried that but it seems https is still
> > >
> > > being allowed, and I can see it in the logs as well "TCP_TUNNEL/200 717
> > >
> > > CONNECT s.youtube.com:443 -"
> > >
> > > my config is https://pastebin.com/8txzkEnG
> > >
> > > and a version of the config without comments:
> > >
> > > https://pastebin.com/zuJYQpXW
>
> > Squid bugs notwithstanding, either your Squid is not running with the
> >
> > configuration that you have shared with us OR that logged request comes
> >
> > from localhost. If you are not sure, I suggest shutting down Squid,
> >
> > making sure that nobody listens on port 3128 and then restarting Squid.
> >
> > Due to the first http_access rule, the test request must not come from
> >
> > the same machine Squid runs on.
>
> I would also comment on:
>
> #http_access deny !Safe_ports
>
> Has that been consciously and deliberately commented-out?
>
> #http_access allow localnet
>
> http_access allow localhost
>
> Is that a typo? Did you mean to allow access from your local networks, rather
>
> than just from localhost?
>
> #http_access deny all
>
> Has that been consciously and deliberately commented-out?
>
> Antony.

Hey, all of those where deliberately done, although I have only been using this program for a short amount of time, so they might be incorrect/dumb, I am not sure,

-Arctic


From rousskov at measurement-factory.com  Tue Jun 22 20:59:26 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 Jun 2021 16:59:26 -0400
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <7c5o62XwXF0ZekG6dRCrQ636GNU0ayvnqRZ7Kbk-hU4Y0IEauKgf1eQfi3MAgdHL1TCkesz5MlIvhezJ0HDleV0787nNyvsx2C0QGSp19u8=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106222141.22874.Antony.Stone@squid.open.source.it>
 <qj3VDaXj-rp2e-hFmm_SJpLh4xUbzlxqK8EZuk4J4RrACsIrET4FllZTFVJJHAbc75regdmL46jfH_x8vxb4bMRuDu540xfh_59_uZg7vWo=@protonmail.com>
 <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
 <7c5o62XwXF0ZekG6dRCrQ636GNU0ayvnqRZ7Kbk-hU4Y0IEauKgf1eQfi3MAgdHL1TCkesz5MlIvhezJ0HDleV0787nNyvsx2C0QGSp19u8=@protonmail.com>
Message-ID: <3794c0fa-7379-6852-4e73-87c5dbb7e22e@measurement-factory.com>

On 6/22/21 4:53 PM, Arctic5824 wrote:
> On Tuesday, June 22nd, 2021 at 1:37 PM, Alex Rousskov <rousskov at measurement-factory.com> wrote:
>> Squid bugs notwithstanding, either your Squid is not running with the  configuration that you have shared with us OR...
> 
> Hey, yes this is actually the case, for testing instead of
>> http_access allow localhost
> im running with
>> http_access allow all

All the http_access rules below "allow all" do not matter because the
first matching rule wins -- Squid would not even try to evaluate the
rest of the rules. Thus, your "http_access deny CONNECT" rule has no effect.

Alex.


From Antony.Stone at squid.open.source.it  Tue Jun 22 20:59:51 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Jun 2021 22:59:51 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <deSr9KfJTe4iwWf1o2I6_xE_xeX7-dJyS370ejBoyWua6pVNkc4yo1ZnrH2_taZCuAS9LY-QMPHfp_svihI8VqfSqVEAUuDUz871HXPJeCs=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106222244.33744.Antony.Stone@squid.open.source.it>
 <deSr9KfJTe4iwWf1o2I6_xE_xeX7-dJyS370ejBoyWua6pVNkc4yo1ZnrH2_taZCuAS9LY-QMPHfp_svihI8VqfSqVEAUuDUz871HXPJeCs=@protonmail.com>
Message-ID: <202106222259.51783.Antony.Stone@squid.open.source.it>

On Tuesday 22 June 2021 at 22:54:42, Arctic5824 wrote:

> On Tuesday, June 22nd, 2021 at 1:44 PM, Antony Stone wrote:
> > 
> > #http_access deny !Safe_ports
> > 
> > Has that been consciously and deliberately commented-out?
> > 
> > #http_access allow localnet
> > 
> > http_access allow localhost
> > 
> > Is that a typo? Did you mean to allow access from your local networks,
> > rather than just from localhost?
> > 
> > #http_access deny all
> > 
> > Has that been consciously and deliberately commented-out?
> 
> Hey, all of those where deliberately done, although I have only been using
> this program for a short amount of time, so they might be incorrect/dumb,
> I am not sure,

I would strongly advise *against* commenting out:

http_access deny !Safe_ports
http_access deny all

Also, since you do not have (at least in the configuration file you showed us)

http_access allow localnet

I do not see how you expect any machine other than the one Squid is running on 
to be able to connect.

However, as in my last posting, please show us the configuration you are 
actually using to carry out these tests.


Antony.

-- 
People say that nothing is impossible, so I try to do the impossible every 
day.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Tue Jun 22 21:00:16 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Jun 2021 23:00:16 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
Message-ID: <202106222300.16215.Antony.Stone@squid.open.source.it>

On Tuesday 22 June 2021 at 22:53:08, Arctic5824 wrote:

> Hey, yes this is actually the case, for testing instead of
> 
> > http_access allow localhost
> 
> im running with
> 
> > http_access allow all

Please do not test and report problems with one configuration, and then tell us 
you have a different one.

Please post the actual configuration file (without comments) which you are 
using, show us the log entry which occurs when you can successfully do 
something which you expected to be blocked, and please tell us the IP address 
of the client machine you performed the test from.


Antony.

-- 
Late in 1972 President Richard Nixon announced that the rate of increase of 
inflation was decreasing.   This was the first time a sitting president used a 
third derivative to advance his case for re-election.

 - Hugo Rossi, Notices of the American Mathematical Society

                                                   Please reply to the list;
                                                         please *don't* CC me.


From arctic5824 at protonmail.com  Tue Jun 22 21:05:20 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 21:05:20 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106222256.18050.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <a0a584d7-e77a-fd20-5205-533ae5ee5cbd@measurement-factory.com>
 <7c5o62XwXF0ZekG6dRCrQ636GNU0ayvnqRZ7Kbk-hU4Y0IEauKgf1eQfi3MAgdHL1TCkesz5MlIvhezJ0HDleV0787nNyvsx2C0QGSp19u8=@protonmail.com>
 <202106222256.18050.Antony.Stone@squid.open.source.it>
Message-ID: <IWp0hS1z_2N3PsLL7v78H5ZAOaAzWK8DU34tMbRcG6Q0nNMycQ-WJRzV0No78jdCXPI5ORCZSh4StQp0HI8KAAJTHyN-AqBzA9wR4B3JNLI=@protonmail.com>

On Tuesday, June 22nd, 2021 at 1:56 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 22 June 2021 at 22:53:08, Arctic5824 wrote:
>
> > Hey, yes this is actually the case, for testing instead of
> >
> > > http_access allow localhost
> >
> > im running with
> >
> > > http_access allow all
>
> Please do not test and report problems with one configuration, and then tell us
>
> you have a different one.
>
> Please post the actual configuration file (without comments) which you are
>
> using, show us the log entry which occurs when you can successfully do
>
> something which you expected to be blocked, and please tell us the IP address
>
> of the client machine you performed the test from.
>
> Antony.

Sorry, I shouldnt have done that.
my config(but the only change is allowing all instead of localhost): https://paste.gg/p/anonymous/e660bab698224e1aa1fd320b1bf22081

here is a snippet (as the file is very large due,i can send full if you would like) of the acces log when I was doing testing:
https://termbin.com/vj7t
the ip i tested from was 73.189.239.235



From Antony.Stone at squid.open.source.it  Tue Jun 22 21:13:19 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Jun 2021 23:13:19 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <IWp0hS1z_2N3PsLL7v78H5ZAOaAzWK8DU34tMbRcG6Q0nNMycQ-WJRzV0No78jdCXPI5ORCZSh4StQp0HI8KAAJTHyN-AqBzA9wR4B3JNLI=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106222256.18050.Antony.Stone@squid.open.source.it>
 <IWp0hS1z_2N3PsLL7v78H5ZAOaAzWK8DU34tMbRcG6Q0nNMycQ-WJRzV0No78jdCXPI5ORCZSh4StQp0HI8KAAJTHyN-AqBzA9wR4B3JNLI=@protonmail.com>
Message-ID: <202106222313.19426.Antony.Stone@squid.open.source.it>

On Tuesday 22 June 2021 at 23:05:20, Arctic5824 wrote:

> On Tuesday, June 22nd, 2021 at 1:56 PM, Antony Stone wrote:
> > 
> > Please do not test and report problems with one configuration, and then
> > tell us you have a different one.
> 
> Sorry, I shouldnt have done that.
> my config(but the only change is allowing all instead of localhost):
> https://paste.gg/p/anonymous/e660bab698224e1aa1fd320b1bf22081

So, as Alex already said, the lines:

http_access allow all
http_access deny CONNECT

mean that anyone, from anyway, can connect.  That's it.

I recommend you turn this off now and hope your ISP doesn't block you for 
running an open proxy.

> here is a snippet (as the file is very large due,i can send full if you
> would like) of the acces log when I was doing testing:
> https://termbin.com/vj7t

No, please send us *only* the lines relating to a _single_ request which you 
think should have been blocked.

> the ip i tested from was 73.189.239.235

What!?

That is not even one of your listed IP addresses.

Are you *really* running an open proxy on the Internet!?

Please turn it off _now_ until you understand the advice Alex and I are giving 
you, and you understand the default settings in the standard Squid 
configuration file, some of which you have changed.


Antony.

-- 
The Magic Words are Squeamish Ossifrage.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Tue Jun 22 21:15:24 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 22 Jun 2021 23:15:24 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106222313.19426.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <IWp0hS1z_2N3PsLL7v78H5ZAOaAzWK8DU34tMbRcG6Q0nNMycQ-WJRzV0No78jdCXPI5ORCZSh4StQp0HI8KAAJTHyN-AqBzA9wR4B3JNLI=@protonmail.com>
 <202106222313.19426.Antony.Stone@squid.open.source.it>
Message-ID: <202106222315.24440.Antony.Stone@squid.open.source.it>

On Tuesday 22 June 2021 at 23:13:19, Antony Stone wrote:

> On Tuesday 22 June 2021 at 23:05:20, Arctic5824 wrote:
> > On Tuesday, June 22nd, 2021 at 1:56 PM, Antony Stone wrote:
> > > Please do not test and report problems with one configuration, and then
> > > tell us you have a different one.
> > 
> > Sorry, I shouldnt have done that.
> > my config(but the only change is allowing all instead of localhost):
> > https://paste.gg/p/anonymous/e660bab698224e1aa1fd320b1bf22081
> 
> So, as Alex already said, the lines:
> 
> http_access allow all
> http_access deny CONNECT
> 
> mean that anyone, from anyway, can connect.  That's it.

Correction: "anyone, from anywhere".  That means anywhere on the planet.  
Please turn this off now.

> I recommend you turn this off now and hope your ISP doesn't block you for
> running an open proxy.
> 
> > here is a snippet (as the file is very large due,i can send full if you
> > would like) of the acces log when I was doing testing:
> > https://termbin.com/vj7t
> 
> No, please send us *only* the lines relating to a _single_ request which
> you think should have been blocked.
> 
> > the ip i tested from was 73.189.239.235
> 
> What!?
> 
> That is not even one of your listed IP addresses.
> 
> Are you *really* running an open proxy on the Internet!?
> 
> Please turn it off _now_ until you understand the advice Alex and I are
> giving you, and you understand the default settings in the standard Squid
> configuration file, some of which you have changed.
> 
> 
> Antony.

-- 
Perfection in design is achieved not when there is nothing left to add, but 
rather when there is nothing left to take away.

 - Antoine de Saint-Exupery

                                                   Please reply to the list;
                                                         please *don't* CC me.


From arctic5824 at protonmail.com  Tue Jun 22 21:33:25 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 21:33:25 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106222315.24440.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <IWp0hS1z_2N3PsLL7v78H5ZAOaAzWK8DU34tMbRcG6Q0nNMycQ-WJRzV0No78jdCXPI5ORCZSh4StQp0HI8KAAJTHyN-AqBzA9wR4B3JNLI=@protonmail.com>
 <202106222313.19426.Antony.Stone@squid.open.source.it>
 <202106222315.24440.Antony.Stone@squid.open.source.it>
Message-ID: <H4WrV81M78ohqBCmtYCPogenL7owx269wjEYVDu0WaFa52GKWZxqJ5F7FBRJbrmWF916XkvFXWvhKAmw2F31B3AMU8obCDQR9kyBYPYbIo8=@protonmail.com>

On Tuesday, June 22nd, 2021 at 2:15 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> >
> > What!?
> >
> > That is not even one of your listed IP addresses.
> >
> > Are you really running an open proxy on the Internet!?
> >
> > Please turn it off now until you understand the advice Alex and I are


Hey Antony, I appreciate your concern but I have already confirmed with my VPS provider (that I am hosting this proxy on) that they will not termante me for this, they have also clarified this in their TOS, may it be tor relays or any other form of dodgy traffic.

>  No, please send us only the lines relating to a single request which you think should have been blocked.

My bad, here are a few
> 1624395604.354   2430 73.189.239.235 TCP_TUNNEL/200 5162 CONNECT accounts.google.com:443 - HIER_DIRECT/2a00:1450:4001:80e::200d -


>  3070 73.189.239.235 TCP_TUNNEL/200 6778 CONNECT www.reddit.com:443 - HIER_DIRECT/151.101.129.140 -

according to Alex:
"All the http_access rules below "allow all" do not matter because the
first matching rule wins -- Squid would not even try to evaluate the
rest of the rules. Thus, your "http_access deny CONNECT" rule has no effect."
so I am now using: https://paste.gg/p/anonymous/e7d5080091bc400e8a75e8285b3dea77
instead of "http_access allow all" i replaced that line with "http_access allow all !CONNECT"

and it seems to be working, atleast in my browser, yet i still see some users using https,
>    359 5.253.19.75 TCP_MISS/502 4957 GET https://search.yahoo.com/search? - HIER_DIRECT/212.82.100.137 text/html

>  0 5.188.211.10 TCP_DENIED/403 3718 CONNECT www.google.com:443 - HIER_NONE/- text/html

Im not sure how they are doing this, I'd like to prevent this without everyone being forced to install custom (SSL?) cirts into their browser and stuff, thanks

-Arctic


From coenraad at wish.org.za  Tue Jun 22 22:01:31 2021
From: coenraad at wish.org.za (Coenraad Loubser)
Date: Wed, 23 Jun 2021 00:01:31 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
Message-ID: <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>

This seems all good and well if you're just proxying traffic to your own
servers... but if you want to run an actual proxy this doesn't really make
sense any more.

You can block HTTPS through Squid, and even do some redirection with your
firewall too - but when it comes to whether it will work, your problem is
with the browsers - and everyone else on the internet: as a start, you
might want to read up on
https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security - and browser
implementations. The only way to force HTTP, or to redirect to it, is to
compile and ship your own browsers too - and that would be a terrible idea
as anyone (on the planet) who found out that you have people using such
modified browsers, would be able to impersonate the sites they visited and
steal their credentials, in many cases without them knowing. This is the
actual problem that HTTPS and HSTS helps prevent.

You can install your own certificates and follow
https://wiki.squid-cache.org/Features/SslBump and then redirect to a
non-HTTPS page, but even so no up to date browser will obey the redirect if
HSTS is enabled for the site.

If it's caching you want to do, there was a time that you could cache
almost everything and emulate a 1Gbps connection on a 256kbps ADSL line...
but that time ended around 2010... we're now in 2021... it is now cheaper
and easier (esp. if you consider the cost of your time) than ever to just
build fast connections to the internet than ever before. Get yourself a
Starlink modem and share the connection - and costs - with your street, if
you're trying to save on bandwidth. I understand all about wanting to cache
things and run things offline and not having connectivity...

If you want to cache content the proper way today, you will need to make
deals with the content providers you're trying to cache, and then set up
the infrastructure to host their content on your own server, and either get
them to issue you with SSL Certificates or point their DNS to you... or
easier, just connect to people who have already done this and already has
servers in a regional data center near you.

Alternatively, I guess you could mirror or spider some sites, and then just
host them on your non-HTTPS mirror. Likely against the wishes and terms of
those sites... but no proxy needed. But if you started messing with a proxy
and DNS in front of it, it would just break on all browsers today.

A better way to do it would be to write a browser addon that modifies the
URL to a custom url much like https://web.archive.org/http://web.archive.org
does it by just having the whole URL as the actual URL path... but why not
just browse the Web Archive directly then... bonus,* they run a Non-SSL
version of the whole archive*! No need to mess with anything.

If it's just a package repository you want to cache... it almost certainly
still has http support if you dig deeper... but you might want to enable
whatever hash checking mechanisms it has to save yourself some grey hairs.

Perhaps if you shared your actual use case we could help you come up with a
better (and more responsible and sustainable) solution?

On Tue, 22 Jun 2021 at 21:32, Arctic5824 <arctic5824 at protonmail.com> wrote:

> Hello, Recently I setup my first squid proxy,
>
> I want it when users try to acces a website via https, they get redirected
> to the http version, I tried disabling https by reading the comments in the
> config, the squid docs, and online forums, but I am unable to figure this
> out, I also tried blocking port 443 using ufw but it just resulted in users
> timing out.
>
> Please rest assured I understand the security and other risks this brings,
> thanks.
> To  reiterate as this email is a bit long, I'd like to know how to
> dis-allow https and redirect users to http versions of websites when they
> try to use https
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210623/b660a2a3/attachment.htm>

From coenraad at wish.org.za  Tue Jun 22 22:06:21 2021
From: coenraad at wish.org.za (Coenraad Loubser)
Date: Wed, 23 Jun 2021 00:06:21 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>
Message-ID: <CADZv+uZ3JbZA1NJXRKiZu3yYmC6tSt2dVPNSiAvzneSfNL=ruA@mail.gmail.com>

Of course you could always just run your own web-based proxy such as these:
https://www.google.com/search?q=web+based+proxies - that would fetch the
https site if necessary, and render it as http - but it will rarely be a
perfect copy.

I'm sure there are many other ways to do this too... again, what's your
real use case here?

On Wed, 23 Jun 2021 at 00:01, Coenraad Loubser <coenraad at wish.org.za> wrote:

> This seems all good and well if you're just proxying traffic to your own
> servers... but if you want to run an actual proxy this doesn't really make
> sense any more.
>
> You can block HTTPS through Squid, and even do some redirection with your
> firewall too - but when it comes to whether it will work, your problem is
> with the browsers - and everyone else on the internet: as a start, you
> might want to read up on
> https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security - and
> browser implementations. The only way to force HTTP, or to redirect to it,
> is to compile and ship your own browsers too - and that would be a terrible
> idea as anyone (on the planet) who found out that you have people using
> such modified browsers, would be able to impersonate the sites they visited
> and steal their credentials, in many cases without them knowing. This is
> the actual problem that HTTPS and HSTS helps prevent.
>
> You can install your own certificates and follow
> https://wiki.squid-cache.org/Features/SslBump and then redirect to a
> non-HTTPS page, but even so no up to date browser will obey the redirect if
> HSTS is enabled for the site.
>
> If it's caching you want to do, there was a time that you could cache
> almost everything and emulate a 1Gbps connection on a 256kbps ADSL line...
> but that time ended around 2010... we're now in 2021... it is now cheaper
> and easier (esp. if you consider the cost of your time) than ever to just
> build fast connections to the internet than ever before. Get yourself a
> Starlink modem and share the connection - and costs - with your street, if
> you're trying to save on bandwidth. I understand all about wanting to cache
> things and run things offline and not having connectivity...
>
> If you want to cache content the proper way today, you will need to make
> deals with the content providers you're trying to cache, and then set up
> the infrastructure to host their content on your own server, and either get
> them to issue you with SSL Certificates or point their DNS to you... or
> easier, just connect to people who have already done this and already has
> servers in a regional data center near you.
>
> Alternatively, I guess you could mirror or spider some sites, and then
> just host them on your non-HTTPS mirror. Likely against the wishes and
> terms of those sites... but no proxy needed. But if you started messing
> with a proxy and DNS in front of it, it would just break on all browsers
> today.
>
> A better way to do it would be to write a browser addon that modifies the
> URL to a custom url much like
> https://web.archive.org/http://web.archive.org does it by just having the
> whole URL as the actual URL path... but why not just browse the Web Archive
> directly then... bonus,* they run a Non-SSL version of the whole archive*!
> No need to mess with anything.
>
> If it's just a package repository you want to cache... it almost certainly
> still has http support if you dig deeper... but you might want to enable
> whatever hash checking mechanisms it has to save yourself some grey hairs.
>
> Perhaps if you shared your actual use case we could help you come up with
> a better (and more responsible and sustainable) solution?
>
> On Tue, 22 Jun 2021 at 21:32, Arctic5824 <arctic5824 at protonmail.com>
> wrote:
>
>> Hello, Recently I setup my first squid proxy,
>>
>> I want it when users try to acces a website via https, they get
>> redirected to the http version, I tried disabling https by reading the
>> comments in the config, the squid docs, and online forums, but I am unable
>> to figure this out, I also tried blocking port 443 using ufw but it just
>> resulted in users timing out.
>>
>> Please rest assured I understand the security and other risks this
>> brings, thanks.
>> To  reiterate as this email is a bit long, I'd like to know how to
>> dis-allow https and redirect users to http versions of websites when they
>> try to use https
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210623/39222852/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jun 22 22:37:50 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 23 Jun 2021 00:37:50 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <CADZv+uZ3JbZA1NJXRKiZu3yYmC6tSt2dVPNSiAvzneSfNL=ruA@mail.gmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>
 <CADZv+uZ3JbZA1NJXRKiZu3yYmC6tSt2dVPNSiAvzneSfNL=ruA@mail.gmail.com>
Message-ID: <202106230037.50644.Antony.Stone@squid.open.source.it>

On Wednesday 23 June 2021 at 00:06:21, Coenraad Loubser wrote:

> I'm sure there are many other ways to do this too... again, what's your
> real use case here?

My _guess_ now that I know Arctic 5824 is deliberately running an open web 
proxy on the Internet (with co-operation from the hosting provider or not) is 
that the objective is to convert all HTTPS connections into HTTP so that the 
content can be cached / scraped / captured on the way past, and the 
"interesting bits" used later, perhaps by some of Artic5824's "customers" 
without the people who chose to browse the Internet through an open proxy 
realising that this is even possible.

It's possibly even being advertised / promoted / sold as an "anonymising 
service", where people can browse the sort of websites they would prefer not 
to do directly through their own connectivity providers, comfortable in the 
knowledge that the IP address hitting those sites is not theirs, but not 
realising that the HTTP traffic they are then using can be intercepted and 
examined not only by Artic5824 but also by their connectivity provider's 
transparent interception proxy.

I'd be happy to entertain any less dubious explanation of what the real 
purpose in setting up such a system might be.


Antony.

-- 
There's a good theatrical performance about puns on in the West End.  It's a 
play on words.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From arctic5824 at protonmail.com  Tue Jun 22 22:44:44 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 22:44:44 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106230037.50644.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>
 <CADZv+uZ3JbZA1NJXRKiZu3yYmC6tSt2dVPNSiAvzneSfNL=ruA@mail.gmail.com>
 <202106230037.50644.Antony.Stone@squid.open.source.it>
Message-ID: <Ik6lrzLhvj-Q1uNGiv28VOAUKCq2Q2VcGsQw8R4ryWY8_tCyAGifcLwZD8R5x71h-C6ehG2KJXKPEQ6xIz7GT72Mgl1b6Gf2VQzUNY5xKhU=@protonmail.com>



On Tuesday, June 22nd, 2021 at 3:37 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 23 June 2021 at 00:06:21, Coenraad Loubser wrote:
>
> > I'm sure there are many other ways to do this too... again, what's your
> >
> > real use case here?
>
> My guess now that I know Arctic 5824 is deliberately running an open web
>
> proxy on the Internet (with co-operation from the hosting provider or not) is
>
> that the objective is to convert all HTTPS connections into HTTP so that the
>
> content can be cached / scraped / captured on the way past, and the
>
> "interesting bits" used later, perhaps by some of Artic5824's "customers"
>
> without the people who chose to browse the Internet through an open proxy
>
> realising that this is even possible.
>
> It's possibly even being advertised / promoted / sold as an "anonymising
>
> service", where people can browse the sort of websites they would prefer not
>
> to do directly through their own connectivity providers, comfortable in the
>
> knowledge that the IP address hitting those sites is not theirs, but not
>
> realising that the HTTP traffic they are then using can be intercepted and
>
> examined not only by Artic5824 but also by their connectivity provider's
>
> transparent interception proxy.
>
> I'd be happy to entertain any less dubious explanation of what the real
>
> purpose in setting up such a system might be.
>
> Antony.
Antony, bro im not evil, i want to run an open proxy and replace google adverts w/my adverts

i already have a perl script for url_rewrite_program thing


From arctic5824 at protonmail.com  Tue Jun 22 22:46:42 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 22:46:42 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106230037.50644.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>
 <CADZv+uZ3JbZA1NJXRKiZu3yYmC6tSt2dVPNSiAvzneSfNL=ruA@mail.gmail.com>
 <202106230037.50644.Antony.Stone@squid.open.source.it>
Message-ID: <WQVBoOm2KbfFM_1RSw_my-Z-GNBgyINQWgLbKrY_P4Jjp0Qzezi_oEEuUD_CR5WCrTdBReKOXDxrp11pRlXwPLFBDGwQhXV8UzK2tyUnn7Q=@protonmail.com>

On Tuesday, June 22nd, 2021 at 3:37 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 23 June 2021 at 00:06:21, Coenraad Loubser wrote:
>
> > I'm sure there are many other ways to do this too... again, what's your
> >
> > real use case here?
>
> My guess now that I know Arctic 5824 is deliberately running an open web
>
> proxy on the Internet (with co-operation from the hosting provider or not) is
>
> that the objective is to convert all HTTPS connections into HTTP so that the
>
> content can be cached / scraped / captured on the way past, and the
>
> "interesting bits" used later, perhaps by some of Artic5824's "customers"
>
> without the people who chose to browse the Internet through an open proxy
>
> realising that this is even possible.
>
> It's possibly even being advertised / promoted / sold as an "anonymising
>
> service", where people can browse the sort of websites they would prefer not
>
> to do directly through their own connectivity providers, comfortable in the
>
> knowledge that the IP address hitting those sites is not theirs, but not
>
> realising that the HTTP traffic they are then using can be intercepted and
>
> examined not only by Artic5824 but also by their connectivity provider's
>
> transparent interception proxy.
>
> I'd be happy to entertain any less dubious explanation of what the real
>
> purpose in setting up such a system might be.
>
> Antony.
>
>

continuation from my last message: im not advertizing it at all, im relying on port scanners and sites like proxyscrape that post open proxys they scanned


From coenraad at wish.org.za  Tue Jun 22 22:48:30 2021
From: coenraad at wish.org.za (Coenraad Loubser)
Date: Wed, 23 Jun 2021 00:48:30 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106230037.50644.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <CADZv+uYKPvHyNG2EO9xvzCOdY4jDNCFSXhGU1BbQyfafLjbsiA@mail.gmail.com>
 <CADZv+uZ3JbZA1NJXRKiZu3yYmC6tSt2dVPNSiAvzneSfNL=ruA@mail.gmail.com>
 <202106230037.50644.Antony.Stone@squid.open.source.it>
Message-ID: <CADZv+uZfgS__QLd7F7hDbDLQj6RpBzooVd7MJxhthikEX9xw+A@mail.gmail.com>

Just like for every regulation there is an equal and opposite loophole,
there are many legitimate uses for something like this... be it archival,
ease of access, or making things simpler to access or available for offline
use...

And ... when I think of all the potentially nefarious uses... I like to
think of the inevitable heat death of the universe, in the same breath. If
someone really can't think of something better to do with their time or
life, sigh.... who are we to judge. Maybe they're bringing "balance to the
force" :-D

On Wed, 23 Jun 2021 at 00:38, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Wednesday 23 June 2021 at 00:06:21, Coenraad Loubser wrote:
>
> > I'm sure there are many other ways to do this too... again, what's your
> > real use case here?
>
> My _guess_ now that I know Arctic 5824 is deliberately running an open web
> proxy on the Internet (with co-operation from the hosting provider or not)
> is
> that the objective is to convert all HTTPS connections into HTTP so that
> the
> content can be cached / scraped / captured on the way past, and the
> "interesting bits" used later, perhaps by some of Artic5824's "customers"
> without the people who chose to browse the Internet through an open proxy
> realising that this is even possible.
>
> It's possibly even being advertised / promoted / sold as an "anonymising
> service", where people can browse the sort of websites they would prefer
> not
> to do directly through their own connectivity providers, comfortable in
> the
> knowledge that the IP address hitting those sites is not theirs, but not
> realising that the HTTP traffic they are then using can be intercepted and
> examined not only by Artic5824 but also by their connectivity provider's
> transparent interception proxy.
>
> I'd be happy to entertain any less dubious explanation of what the real
> purpose in setting up such a system might be.
>
>
> Antony.
>
> --
> There's a good theatrical performance about puns on in the West End.  It's
> a
> play on words.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210623/f67f571f/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jun 22 22:50:37 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 23 Jun 2021 00:50:37 +0200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <Ik6lrzLhvj-Q1uNGiv28VOAUKCq2Q2VcGsQw8R4ryWY8_tCyAGifcLwZD8R5x71h-C6ehG2KJXKPEQ6xIz7GT72Mgl1b6Gf2VQzUNY5xKhU=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106230037.50644.Antony.Stone@squid.open.source.it>
 <Ik6lrzLhvj-Q1uNGiv28VOAUKCq2Q2VcGsQw8R4ryWY8_tCyAGifcLwZD8R5x71h-C6ehG2KJXKPEQ6xIz7GT72Mgl1b6Gf2VQzUNY5xKhU=@protonmail.com>
Message-ID: <202106230050.37746.Antony.Stone@squid.open.source.it>

On Wednesday 23 June 2021 at 00:44:44, Arctic5824 wrote:

> I want to run an open proxy and replace google adverts w/my adverts.

You might want to be aware that this is illegal in many countries, and a 
number of Internet Service Providers have been sued and/or fined for 
manipulating the content of websites as they pass through their systems.

Anyway, just for the sake of technical discussion, let me repeat my original 
questions:

On Tuesday 22 June 2021 at 21:41:22, Antony Stone wrote:

> On Tuesday 22 June 2021 at 21:32:10, Arctic5824 wrote:
> > Hello, Recently I setup my first squid proxy,
> > 
> > I want it when users try to acces a website via https, they get
> > redirected to the http version
> 
> 1. What makes you believe that sites *have* an HTTP version?
> 
> 2. What do you think should happen when sites *do* have an HTTP version,
> and that consists solely of a 301 Permanent Redirect to the HTTPS version,
> which contains the content?
> 
> (In other words, the actual web server is never going to provide the
> content you want to see if you only speak HTTP to it.)
> 
> 
> Antony.

-- 
This email was created using 100% recycled electrons.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From arctic5824 at protonmail.com  Tue Jun 22 23:20:33 2021
From: arctic5824 at protonmail.com (Arctic5824)
Date: Tue, 22 Jun 2021 23:20:33 +0000
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <202106230050.37746.Antony.Stone@squid.open.source.it>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106230037.50644.Antony.Stone@squid.open.source.it>
 <Ik6lrzLhvj-Q1uNGiv28VOAUKCq2Q2VcGsQw8R4ryWY8_tCyAGifcLwZD8R5x71h-C6ehG2KJXKPEQ6xIz7GT72Mgl1b6Gf2VQzUNY5xKhU=@protonmail.com>
 <202106230050.37746.Antony.Stone@squid.open.source.it>
Message-ID: <vlPrVVg6sYYbHdFira156WgrYrf5BBFCBnsyR-q9-PuYDu9jMV-eyCGCb-IZiiVjwtDe6aE-ataMvdX02aQa_gAuTA3K_3il7xJ-xsScjTc=@protonmail.com>

hey sorry i accidently directly sent it again, instead of the email list:


On Tuesday, June 22nd, 2021 at 3:50 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:

> You might want to be aware that this is illegal in many countries, and a number of Internet Service Providers have been sued and/or fined for manipulating the content of websites as they pass through their systems.

Thanks for the warning, I dont think this will really be a problem for me though.


 1.  What makes you believe that sites have an HTTP version?

I dont see why they wouldnt, like sure they would prefer https but why would http not work if forced


2.  What do you think should happen when sites do have an HTTP version,  and that consists solely of a 301 Permanent Redirect to the HTTPS version

I didnt think of this, this would be a problem i guess, but I dont think it would be too common.

Maybe squid isnt the right software for this?
Thanks,
- Arctic


From squid3 at treenet.co.nz  Wed Jun 23 00:49:52 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Wed, 23 Jun 2021 12:49:52 +1200
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <vlPrVVg6sYYbHdFira156WgrYrf5BBFCBnsyR-q9-PuYDu9jMV-eyCGCb-IZiiVjwtDe6aE-ataMvdX02aQa_gAuTA3K_3il7xJ-xsScjTc=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <202106230037.50644.Antony.Stone@squid.open.source.it>
 <Ik6lrzLhvj-Q1uNGiv28VOAUKCq2Q2VcGsQw8R4ryWY8_tCyAGifcLwZD8R5x71h-C6ehG2KJXKPEQ6xIz7GT72Mgl1b6Gf2VQzUNY5xKhU=@protonmail.com>
 <202106230050.37746.Antony.Stone@squid.open.source.it>
 <vlPrVVg6sYYbHdFira156WgrYrf5BBFCBnsyR-q9-PuYDu9jMV-eyCGCb-IZiiVjwtDe6aE-ataMvdX02aQa_gAuTA3K_3il7xJ-xsScjTc=@protonmail.com>
Message-ID: <eecbd028b0fe7293e71f1668e950c378@treenet.co.nz>

On 2021-06-23 11:20, Arctic5824 wrote:
> hey sorry i accidently directly sent it again, instead of the email 
> list:
> 
> 
> On Tuesday, June 22nd, 2021 at 3:50 PM, Antony Stone wrote:
> 
>> You might want to be aware that this is illegal in many countries, and 
>> a number of Internet Service Providers have been sued and/or fined for 
>> manipulating the content of websites as they pass through their 
>> systems.
> 
> Thanks for the warning, I dont think this will really be a problem for
> me though.
> 
> 
>  1.  What makes you believe that sites have an HTTP version?
> 
> I dont see why they wouldnt, like sure they would prefer https but why
> would http not work if forced
> 

Because this idea you have about changing advert content is not a
new thing.

It has been done and tried so many times in the past by others for
http:// traffic that the major content providers whose income depended
on those ads got together and started a project to get rid of http://
completely. They have had much success with the support of privacy
and security advocate groups.

> 
> 2.  What do you think should happen when sites do have an HTTP
> version,  and that consists solely of a 301 Permanent Redirect to the
> HTTPS version
> 
> I didnt think of this, this would be a problem i guess, but I dont
> think it would be too common.

Reality is that today the vast majority of websites still offering
http:// versions at all, do exactly that.

> 
> Maybe squid isnt the right software for this?

Squid is fine for the content adaptation part of what you are wanting.

What is not going to work is the HTTP->HTTP conversion part. That is
because of protocol and Browser features. No intermediary software can
get around those without the SSL-Bump (or similar) mechanism - as
others already mentioned that too has its limits. TLS is specifically
designed to prevent intermediaries touching the content - the only
reliable action a proxy can do is terminate unwanted TLS connections.

Amos


From doconnor at transsee.ca  Wed Jun 23 02:04:29 2021
From: doconnor at transsee.ca (Darwin O'Connor)
Date: Tue, 22 Jun 2021 22:04:29 -0400
Subject: [squid-users] Data not being cached
In-Reply-To: <mailman.3.1624363201.982548.squid-users@lists.squid-cache.org>
References: <mailman.3.1624363201.982548.squid-users@lists.squid-cache.org>
Message-ID: <ba005970-af36-b46e-e8f6-345b58967f24@transsee.ca>

On 2021-06-22 8:00 a.m., squid-users-request at lists.squid-cache.org wrote:

> On 21/06/21 6:41 am, Darwin O'Connor wrote:
>> I run a transit prediction web app <https://www.transsee.ca/>. It
>> connects to a variety of web APIs to collect the real time transit data
>> it needs. The app's activities are split among many processes. They
>> currently uses libcurl to connect to squid for caching (often for as
>> little as 10-30 seconds) and benefits of connection sharing.
>>
>> There is still cases where data isn't being cached no matter what I do.
>> It is https data, but I am able to cache other https pages like
>> https://cdn.mbta.com/realtime/Alerts.pb
>>
>> The refresh_pattern:
>>
>> refresh_pattern .?????????????? 60????? 99999%? 7200 override-expire
>> override-lastmod reload-into-ims ignore-reload ignore-no-cache
>> ignore-no-store ignore-private ignore-auth store-stale
>>
> Please be aware that several of those options may be causing you more
> problems than they solve:
>
> * ignore-no-cache - no longer exists.
>
> * ignore-reload - contradicts reload-into-ims and that can cause
> inconsistent behaviour between initial MISS and followup HIT responses.
>
> * override-lastmod - replaces all Last-Modification (L-M) headers with
> values indicating the object is almost brand new. That prevents caches
> detecting that objects have stuck around long enough not to need 
> replacing.
> Luckily the example response has no L-M header, so that is not the
> problem here. But it may be affecting other traffic.

I have removed these options. My refresh_pattern is now

refresh_pattern .?????????????? 60????? 99999%? 7200 override-expire 
ignore-no-store ignore-private ignore-auth store-stale

Here are two requests, one after another. The headers match the actual 
client apps.

*?? Trying 127.0.0.1:3128...
* Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
> GET https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses HTTP/1.1
Host: 127.0.0.1:3128
User-Agent: curl/7.77.0 (+https://www.transsee.ca/)
Accept: */*
Accept-Encoding: gzip
Authorization: apikey 4sc5KapPRVzX6OeQm74FPCx9O0bFsIB7lei3
Cache-Control: max-age=60

* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Date: Tue, 22 Jun 2021 23:23:06 GMT
< Content-Type: application/protobuf
< Content-Length: 7317
< Set-Cookie: 
AWSALB=uydcNa4qcva/6YErLw+Ztl55Xvjtkf1N6HQiEfXOrPv46pjWn6uHEnqcRlPMWjcHXu6vAsbYjB/1WdxrVIYu0AAbrtIcz00zyAIiimoWQhWUlQFb6PO9120gf46N; 
Expires=Tue, 29 Jun 2021 23:23:06 GMT; Path=/
< Set-Cookie: 
AWSALBCORS=uydcNa4qcva/6YErLw+Ztl55Xvjtkf1N6HQiEfXOrPv46pjWn6uHEnqcRlPMWjcHXu6vAsbYjB/1WdxrVIYu0AAbrtIcz00zyAIiimoWQhWUlQFb6PO9120gf46N; 
Expires=Tue, 29 Jun 2021 23:23:06 GMT; Path=/; SameSite=None; Secure
< Server: Apache-Coyote/1.1
< X-Powered-By: Express
< Access-Control-Allow-Credentials: true
< ETag: W/"9b9e-32oicwXN7rJop21q+Y++RZhtnac"
< Vary: Accept-Encoding
< X-Cache: Hit from cloudfront
< X-Amz-Cf-Pop: SYD1-C
< X-Amz-Cf-Id: fQeOCQLj5CiuSv2JV06y7O0r5rM0LvNchlXKFSBDKkES0-tUrmaorQ==
< Age: 31
< Access-Control-Allow-Origin: *
< Content-Encoding: gzip
< X-Cache: MISS from transsee
< X-Cache-Lookup: MISS from transsee:3128
< Via: 1.1 e57fe70b9ed429fb51b4b2432cadc67b.cloudfront.net (CloudFront), 
1.1 transsee (squid/4.15)
< Connection: keep-alive

*?? Trying 127.0.0.1:3128...
* Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
> GET https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses HTTP/1.1
Host: 127.0.0.1:3128
User-Agent: curl/7.77.0 (+https://www.transsee.ca/)
Accept: */*
Accept-Encoding: gzip
Authorization: apikey 4sc5KapPRVzX6OeQm74FPCx9O0bFsIB7lei3
Cache-Control: max-age=60

* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Date: Tue, 22 Jun 2021 23:23:13 GMT
< Content-Type: application/protobuf
< Content-Length: 7317
< Set-Cookie: 
AWSALB=zjeR+TBhpLTKE+mr042891Cxkb7caSUqya7wvG82Ca7cK86Z7iQco0ahsj/9H9WtdgJuiX/D9++AOYiQf3+9yryXrZscG5eX+X0+vVjI+t2fI/86DjV27tXLyt3o; 
Expires=Tue, 29 Jun 2021 23:23:13 GMT; Path=/
< Set-Cookie: 
AWSALBCORS=zjeR+TBhpLTKE+mr042891Cxkb7caSUqya7wvG82Ca7cK86Z7iQco0ahsj/9H9WtdgJuiX/D9++AOYiQf3+9yryXrZscG5eX+X0+vVjI+t2fI/86DjV27tXLyt3o; 
Expires=Tue, 29 Jun 2021 23:23:13 GMT; Path=/; SameSite=None; Secure
< Server: Apache-Coyote/1.1
< X-Powered-By: Express
< Access-Control-Allow-Credentials: true
< ETag: W/"9b9e-32oicwXN7rJop21q+Y++RZhtnac"
< Vary: Accept-Encoding
< X-Cache: Hit from cloudfront
< X-Amz-Cf-Pop: SYD1-C1
< X-Amz-Cf-Id: cL-w8TjYWTb6cjEOLsyfwyIvZvQYTI7Hn4R8Uz6cppHEJwVUWejO1w==
< Age: 38
< Access-Control-Allow-Origin: *
< Content-Encoding: gzip
< X-Cache: MISS from transsee
< X-Cache-Lookup: MISS from transsee:3128
< Via: 1.1 e57fe70b9ed429fb51b4b2432cadc67b.cloudfront.net (CloudFront), 
1.1 transsee (squid/4.15)
< Connection: keep-alive

Here is the corresponding records from the access log. Before the 
records where from different fetches.

1624404186.746??? 298 127.0.0.1 59540 TCP_MISS/200 8380 GET 
https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses - 
HIER_DIRECT/3.105.153.49 application/protobuf

1624404193.862??? 269 127.0.0.1 59886 TCP_MISS/200 8380 GET 
https://api.transport.nsw.gov.au/v1/gtfs/alerts/buses - 
HIER_DIRECT/3.105.153.49 application/protobuf




From robertkwild at gmail.com  Wed Jun 23 09:06:44 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 23 Jun 2021 10:06:44 +0100
Subject: [squid-users] UDP support for squid
Message-ID: <CAGU_Ci+LArs4jKGZtVP1eoVmsum3kLh9-KNZXgt1o=RzYNmNiw@mail.gmail.com>

hi all,

after reading this guide, is this for enabling squid for SOCKSv5 ie UDP -

https://wiki.squid-cache.org/Features/Socks

export CFLAGS=" -Dbind=SOCKSbind "export CXXFLAGS=" -Dbind=SOCKSbind
"export LDADD=" -lsocks "


when building squid from source, do i append it at the end of the
"configure options"

cd /tmp/squid-4.15
./configure --with-openssl --enable-ssl-crtd --enable-icap-client
--enable-http-violations && make && make install

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210623/c7fc551e/attachment.htm>

From hoper at free.fr  Wed Jun 23 11:26:52 2021
From: hoper at free.fr (hoper)
Date: Wed, 23 Jun 2021 11:26:52 +0000
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
References: <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
Message-ID: <2a6e43df8d47c722733a2b436e98e880@free.fr>

Thanks you very much Alex.

First, the word "immediatly" was a really bad choice of word from me.
I just wanted to say that we don't want to have to restart squid, or the browser,
or to wait for a really long time. 
If the change occurs in the few next seconds, or a few minutes at worst, it's good.

I thought we can do this with a really small TTL
(this explain the auth_param basic credentialsttl 2 minutes).
But even after 5 minutes, the new proxy is still not taken into account :(

> That means disabling caching of authenticated credentials.

You're talking about this page ?
http://www.squid-cache.org/Doc/config/external_acl_type/
I need to add "cache=0" somewhere ?
In the same page, there is another option:
"ttl=n		TTL in seconds for cached results (defaults 1 hour)"
Is it going to help us to acheive our goal if we change this value ?

> It also means that this approach will not work well for bumped HTTP requests
Are you talking about http redirection ? I need more time to understand the impact
and your answer on this subject. 

> yet we have a backburner project to add such support; it is a large change)
Any idea about when we will have this new feature ? Are we talking about a year ?
3 years ? or probably more ? 

Thanks again for your time.

22 juin 2021 16:05 "Alex Rousskov" <rousskov at measurement-factory.com> a ?crit:

> On 6/22/21 5:20 AM, hoper at free.fr wrote:
> 
>> We are using a database with a list of user, and the proxy they need to use.
>> 
>> So in squid.conf file, we declare an external acl:
> 
> FYI: Your are not declaring an external ACL. You are declaring an
> authentication helper.
> 
>> -------------------------------------------------
>> auth_param basic program /mydir/myprogram.sh
>> auth_param basic children 10 startup=1 idle=3
>> auth_param basic realm myrealm
>> auth_param basic credentialsttl 2 minutes
>> -------------------------------------------------
>> 
>> program.sh will check the login/password given by the user.
>> again the ones found in the database. And, if the authentication
>> is sucessfull, it also write on stdout the proxy we need to use for this user.
>> 
>> Example (If this user need to use the proxy number 2):
>> OK proxychoice=p2
>> 
>> If the squid configuration file, we also include another file,
>> which look like this :
>> 
>> -----------------------------------------------------------------
>> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
>> acl p1auth note proxychoice p1
>> cache_peer_access proxy1 allow p1auth
>> http_access allow authenticated p1auth
>> cache_peer_access proxy1 deny all
>> 
>> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
>> acl p2auth note proxychoice p2
>> cache_peer_access proxy2 allow p2auth
>> http_access allow authenticated p2auth
>> cache_peer_access proxy2 deny all
>> -----------------------------------------------------------------
> 
> The relative order of your cache_peer_access and http_access directives
> may imply that you think that cache_peer_access is applied/decided
> first. In reality, http_access is applied/decided first. This
> configuration change does not affect Squid decisions, but the following
> order would match the reality better:
> 
> acl p1auth note proxychoice p1
> acl p2auth note proxychoice p2
> 
> http_access allow authenticated p1auth
> http_access allow authenticated p2auth
> 
> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
> cache_peer_access proxy1 allow p1auth
> cache_peer_access proxy1 deny all
> 
> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
> cache_peer_access proxy2 allow p2auth
> cache_peer_access proxy2 deny all
> 
> BTW, if you want to allow all authenticated users, regardless of their
> cache_peer choice, then you can remove p1auth and p2auth from
> http_access lines.
> 
>> This configuration is working. The parent proxy used by squid is the good one.
>> BUT: If we change the configuration (proxy for a user) in the database,
>> the change is not take into account until we fully restart squid :(
>> (Even squid -k reconfigure does not work).
>> 
>> Please, any ideas ? What can we do to make this "dynamic" ?
> 
> If you want the changes to apply to every received HTTP request, then
> you need to make sure that your authentication helper is called for
> every received request. That means disabling caching of authenticated
> credentials. It also means that this approach will not work well for
> bumped HTTP requests because their authentication information is (or
> should be) inherited from their CONNECT tunnel. You will have to force
> CONNECT tunnels to have at most one request by disabling persistent
> connections, I guess.
> 
> I would also consider separating user authentication from request
> routing. To do that, move the proxychoice=... annotation functionality
> from the authentication helper to the external ACL helper (with caching
> disabled via external_acl_type options). In this approach,
> authentication results can be cached for as long as you want without
> interfering with more up-to-date routing decisions. However, you should
> not use that [slow] external ACL with cache_peer_access lines as
> detailed below).
> 
>> Any change in the database should be taken into account immediatly.
> 
> cache_peer_access does not support slow ACLs (yet -- we have a
> backburner project to add such support; it is a large change). Until
> such support is added, there will always be some delay between obtaining
> routing information (at authentication or external_acl computation time)
> and applying that routing information (at request routing time).
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From m_zouhairy at ckta.by  Wed Jun 23 11:56:40 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Wed, 23 Jun 2021 14:56:40 +0300
Subject: [squid-users] certificate issuer not known
Message-ID: <2697f8b6-30dd-4261-e4ef-1c6e68181e5e@ckta.by>


Health be upon you,
when visiting
https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

on squid 4.15

it displays:


ERROR
The requested URL could not be retrieved

The following error was encountered while trying to retrieve the URL: 
https://wiki.squid-cache.org/*

     Failed to establish a secure connection to 104.130.201.120

The system returned:

     (71) Protocol error (TLS code: 
X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

     SSL Certficate error: certificate issuer (CA) not known: 
/C=US/O=Let's Encrypt/CN=R3

This proxy and the remote host failed to negotiate a mutually acceptable 
security settings for handling your request. It is possible that the 
remote host does not support secure connections, or the proxy is not 
satisfied with the host security credentials.

Your cache administrator is webmaster.

configuration:

http_port 3128 ssl-bump  cert=/etc/squid/certs/myCA.pem 
generate-host-certificates=on dynamic_cert_mem_cache_size=8MB



acl 	tls_s1_connect		at_step SslBump1
acl 	tls_s2_client_hello 	at_step SslBump2
acl 	tls_s3_server_hello 	at_step SslBump3

# define acls for sites that must not be actively bumped

acl 	tls_allowed_hsts	ssl::server_name 			.akamaihd.net
acl 	tls_allowed_hsts	ssl::server_name 			.proxy.skko.by
#acl 	tls_server_is_bank 	ssl::server_name 	.abnamro.nl
#acl 	tls_server_is_bank 	ssl::server_name 	.abnamro.comacl
tls_server_is_bank 		ssl::server_name 
"/usr/local/ufdbguard/blacklists/finance/domains.squidsplice"
acl 	tls_to_splice 	any-of 	tls_allowed_hsts tls_server_is_bank

# TLS/SSL bumping steps

ssl_bump 	peek	tls_s1_connect 	# peek at TLS/SSL connect data
ssl_bump 	splice 	tls_to_splice	# splice some: no active bump
ssl_bump 	stare 	all		# stare(peek) at server
														# properties of the webserver
ssl_bump	 bump	# bump if we can (if the stare succeeded)



#ssl_bump peek all
#ssl_bump splice all

##ssl_bump server-first all

#sslproxy_cert_error allow all



cache_dir ufs /var/cache/squid 8000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mem 960 MB

netdb_filename none

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:			1440	20%	10080
refresh_pattern ^gopher:		1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 	0	0%	0
refresh_pattern .			0	20%	4320

url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode 
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4 queue-size=64
#debug_options ALL,1 33,2 28,9

what needs to be done to fix?


From Joseph.Garbacik at netapp.com  Wed Jun 23 13:02:25 2021
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Wed, 23 Jun 2021 13:02:25 +0000
Subject: [squid-users] Squid Name Resolution Question
Message-ID: <6177FDDC-A77E-4F6C-A1A0-35C470AEF711@netapp.com>

I noticed that my squid server queries every second for the same destination (because we have a clients requiring access to this destination). My squid server does not have the negative_dns_ttl nor the positive_dns_ttl values set so I would expect that they are using the defaults. The destination has a 5 second TTL in DNS. My question is this, why does it still query so often if it should at least have the answer for five seconds? 

From rousskov at measurement-factory.com  Wed Jun 23 14:32:07 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 Jun 2021 10:32:07 -0400
Subject: [squid-users] certificate issuer not known
In-Reply-To: <2697f8b6-30dd-4261-e4ef-1c6e68181e5e@ckta.by>
References: <2697f8b6-30dd-4261-e4ef-1c6e68181e5e@ckta.by>
Message-ID: <cfd5e3fd-5e4a-ed89-305e-7bc7f98397b5@measurement-factory.com>

On 6/23/21 7:56 AM, Majed Zouhairy wrote:
> when visiting
> https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> 
> on squid 4.15
> 
> it displays:
> 
> 
> ERROR
> The requested URL could not be retrieved
> 
> The following error was encountered while trying to retrieve the URL:
> https://wiki.squid-cache.org/*
> 
> ??? Failed to establish a secure connection to 104.130.201.120
> 
> The system returned:
> 
> ??? (71) Protocol error (TLS code:
> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> 
> ??? SSL Certficate error: certificate issuer (CA) not known:
> /C=US/O=Let's Encrypt/CN=R3


Your Squid was not able to validate the certificate used by the Squid
Project wiki site. I am not sure, but perhaps your OpenSSL installation
on the Squid box is missing fresh Let's Encrypt intermediate
certificates? IIRC, there were similar complaints recently. Try
searching the mailing list archives if you have not already.

Alex.
P.S. Squid tries to fetch missing intermediate certificates, but that is
not always possible, and Squid v4 supports fewer environments where it
is possible to fetch them (than more recent Squid versions).


> configuration:
> 
> http_port 3128 ssl-bump? cert=/etc/squid/certs/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=8MB
> 
> 
> 
> acl???? tls_s1_connect??????? at_step SslBump1
> acl???? tls_s2_client_hello???? at_step SslBump2
> acl???? tls_s3_server_hello???? at_step SslBump3
> 
> # define acls for sites that must not be actively bumped
> 
> acl???? tls_allowed_hsts??? ssl::server_name???????????? .akamaihd.net
> acl???? tls_allowed_hsts??? ssl::server_name???????????? .proxy.skko.by
> #acl???? tls_server_is_bank???? ssl::server_name???? .abnamro.nl
> #acl???? tls_server_is_bank???? ssl::server_name???? .abnamro.comacl
> tls_server_is_bank???????? ssl::server_name
> "/usr/local/ufdbguard/blacklists/finance/domains.squidsplice"
> acl???? tls_to_splice???? any-of???? tls_allowed_hsts tls_server_is_bank
> 
> # TLS/SSL bumping steps
> 
> ssl_bump???? peek??? tls_s1_connect???? # peek at TLS/SSL connect data
> ssl_bump???? splice???? tls_to_splice??? # splice some: no active bump
> ssl_bump???? stare???? all??????? # stare(peek) at server
> ??????????????????????????????????????????????????????? # properties of
> the webserver
> ssl_bump???? bump??? # bump if we can (if the stare succeeded)
> 
> 
> 
> #ssl_bump peek all
> #ssl_bump splice all
> 
> ##ssl_bump server-first all
> 
> #sslproxy_cert_error allow all
> 
> 
> 
> cache_dir ufs /var/cache/squid 8000 16 256
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/cache/squid
> 
> cache_mem 960 MB
> 
> netdb_filename none
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:??????????? 1440??? 20%??? 10080
> refresh_pattern ^gopher:??????? 1440??? 0%??? 1440
> refresh_pattern -i (/cgi-bin/|\?)???? 0??? 0%??? 0
> refresh_pattern .??????????? 0??? 20%??? 4320
> 
> url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
> sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l
> /var/log/squid/
> url_rewrite_children 16 startup=8 idle=2 concurrency=4 queue-size=64
> #debug_options ALL,1 33,2 28,9
> 
> what needs to be done to fix?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Wed Jun 23 14:48:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 Jun 2021 10:48:00 -0400
Subject: [squid-users] Squid Name Resolution Question
In-Reply-To: <6177FDDC-A77E-4F6C-A1A0-35C470AEF711@netapp.com>
References: <6177FDDC-A77E-4F6C-A1A0-35C470AEF711@netapp.com>
Message-ID: <d9c60bf0-b24b-68ff-f4ed-3d2da374a5c2@measurement-factory.com>

On 6/23/21 9:02 AM, Garbacik, Joe wrote:
> I noticed that my squid server queries every second for the same
> destination (because we have a clients requiring access to this
> destination). My squid server does not have the negative_dns_ttl nor
> the positive_dns_ttl values set so I would expect that they are using
> the defaults. The destination has a 5 second TTL in DNS. My question
> is this, why does it still query so often if it should at least have
> the answer for five seconds?

I can think of two general reasons:

* The cached address is purged by other, newer addresses. See
ipcache_size and other ipcache_* directives in squid.conf.documented.

* Squid DNS TTL handling bugs. For example, Squid v4 does not have the
following v5 fix AFAICT:

>     Also fixed two DNS TTL bugs. Squid now uses minimum TTL among all used
>     DNS records[2]. Old ipcacheParse() was trying to do the same but:
>     * could overwrite a zero TTL with a positive value
>     * took into account TTLs from unused record types (e.g., CNAME).
>     [2] Subject to *_dns_ttl limits in squid.conf, as before.


Cache.log analysis with debug options set to "ALL,9" (or possibly just
"ALL,1 14,9") can determine the exact cause.

Alex.


From rousskov at measurement-factory.com  Wed Jun 23 15:16:58 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 Jun 2021 11:16:58 -0400
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <2a6e43df8d47c722733a2b436e98e880@free.fr>
References: <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
 <2a6e43df8d47c722733a2b436e98e880@free.fr>
Message-ID: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>

On 6/23/21 7:26 AM, hoper wrote:

> I thought we can do this with a really small TTL
> (this explain the auth_param basic credentialsttl 2 minutes).
> But even after 5 minutes, the new proxy is still not taken into account :(

If Squid trusts stale user credentials (i.e. allows new requests with
stale cached credentials without revalidating them with your
authentication helper), then this is a Squid bug. Credentials become
stale when their configured (via credentialsttl) TTL expires. The clock
should start ticking when Squid makes the corresponding authentication
helper request.

If you can reproduce this bug with a supported Squid version, then I
recommend filing a bug report. This problem is essentially unrelated to
proxychoice annotations though -- either the authentication helper is
contacted for credentials (re)validation or it is not. It should be
triaged/reported without paying attention to annotations.


>> That means disabling caching of authenticated credentials.

> You're talking about this page ?
> http://www.squid-cache.org/Doc/config/external_acl_type/

In this context, no, I am not. Here, I meant auth_param. As I mentioned
earlier, external ACLs are not related to authentication.


> I need to add "cache=0" somewhere ?
> In the same page, there is another option:
> "ttl=n		TTL in seconds for cached results (defaults 1 hour)"
> Is it going to help us to acheive our goal if we change this value ?

The configuration you previously shared does not use external ACLs so
changing external ACL cache parameters will affect nothing.


>> It also means that this approach will not work well for bumped HTTP requests
> Are you talking about http redirection?

No, I am talking about SslBump features. Search squid.conf.documented
for letters "bump" to find starting points. If you do not use those
features, then you can ignore that part of my comment.



>> yet we have a backburner project to add such support; it is a large change)
> Any idea about when we will have this new feature ? Are we talking about a year ?
> 3 years ? or probably more ? 

Insufficient demand for that feature does not allow me to provide a
reliable ETA at this time.

Alex.


> 22 juin 2021 16:05 Alex Rousskov a ?crit:
> 
>> On 6/22/21 5:20 AM, hoper at free.fr wrote:
>>
>>> We are using a database with a list of user, and the proxy they need to use.
>>>
>>> So in squid.conf file, we declare an external acl:
>>
>> FYI: Your are not declaring an external ACL. You are declaring an
>> authentication helper.
>>
>>> -------------------------------------------------
>>> auth_param basic program /mydir/myprogram.sh
>>> auth_param basic children 10 startup=1 idle=3
>>> auth_param basic realm myrealm
>>> auth_param basic credentialsttl 2 minutes
>>> -------------------------------------------------
>>>
>>> program.sh will check the login/password given by the user.
>>> again the ones found in the database. And, if the authentication
>>> is sucessfull, it also write on stdout the proxy we need to use for this user.
>>>
>>> Example (If this user need to use the proxy number 2):
>>> OK proxychoice=p2
>>>
>>> If the squid configuration file, we also include another file,
>>> which look like this :
>>>
>>> -----------------------------------------------------------------
>>> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
>>> acl p1auth note proxychoice p1
>>> cache_peer_access proxy1 allow p1auth
>>> http_access allow authenticated p1auth
>>> cache_peer_access proxy1 deny all
>>>
>>> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
>>> acl p2auth note proxychoice p2
>>> cache_peer_access proxy2 allow p2auth
>>> http_access allow authenticated p2auth
>>> cache_peer_access proxy2 deny all
>>> -----------------------------------------------------------------
>>
>> The relative order of your cache_peer_access and http_access directives
>> may imply that you think that cache_peer_access is applied/decided
>> first. In reality, http_access is applied/decided first. This
>> configuration change does not affect Squid decisions, but the following
>> order would match the reality better:
>>
>> acl p1auth note proxychoice p1
>> acl p2auth note proxychoice p2
>>
>> http_access allow authenticated p1auth
>> http_access allow authenticated p2auth
>>
>> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
>> cache_peer_access proxy1 allow p1auth
>> cache_peer_access proxy1 deny all
>>
>> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
>> cache_peer_access proxy2 allow p2auth
>> cache_peer_access proxy2 deny all
>>
>> BTW, if you want to allow all authenticated users, regardless of their
>> cache_peer choice, then you can remove p1auth and p2auth from
>> http_access lines.
>>
>>> This configuration is working. The parent proxy used by squid is the good one.
>>> BUT: If we change the configuration (proxy for a user) in the database,
>>> the change is not take into account until we fully restart squid :(
>>> (Even squid -k reconfigure does not work).
>>>
>>> Please, any ideas ? What can we do to make this "dynamic" ?
>>
>> If you want the changes to apply to every received HTTP request, then
>> you need to make sure that your authentication helper is called for
>> every received request. That means disabling caching of authenticated
>> credentials. It also means that this approach will not work well for
>> bumped HTTP requests because their authentication information is (or
>> should be) inherited from their CONNECT tunnel. You will have to force
>> CONNECT tunnels to have at most one request by disabling persistent
>> connections, I guess.
>>
>> I would also consider separating user authentication from request
>> routing. To do that, move the proxychoice=... annotation functionality
>> from the authentication helper to the external ACL helper (with caching
>> disabled via external_acl_type options). In this approach,
>> authentication results can be cached for as long as you want without
>> interfering with more up-to-date routing decisions. However, you should
>> not use that [slow] external ACL with cache_peer_access lines as
>> detailed below).
>>
>>> Any change in the database should be taken into account immediatly.
>>
>> cache_peer_access does not support slow ACLs (yet -- we have a
>> backburner project to add such support; it is a large change). Until
>> such support is added, there will always be some delay between obtaining
>> routing information (at authentication or external_acl computation time)
>> and applying that routing information (at request routing time).
>>
>> HTH,
>>
>> Alex.



From rousskov at measurement-factory.com  Wed Jun 23 15:26:50 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 Jun 2021 11:26:50 -0400
Subject: [squid-users] Newbie question,
 How to fully disable/disallow https?
In-Reply-To: <H4WrV81M78ohqBCmtYCPogenL7owx269wjEYVDu0WaFa52GKWZxqJ5F7FBRJbrmWF916XkvFXWvhKAmw2F31B3AMU8obCDQR9kyBYPYbIo8=@protonmail.com>
References: <rqW70nMiHjCoWJiBdEGi_u4Dez-EP6k7WTFR4lHWHprY1Nj_yWavJ-yGaFT1lEpcBCYouQfwlW78xq8_luE6WvaCHkQAm9m_2NEAFX01kdQ=@protonmail.com>
 <IWp0hS1z_2N3PsLL7v78H5ZAOaAzWK8DU34tMbRcG6Q0nNMycQ-WJRzV0No78jdCXPI5ORCZSh4StQp0HI8KAAJTHyN-AqBzA9wR4B3JNLI=@protonmail.com>
 <202106222313.19426.Antony.Stone@squid.open.source.it>
 <202106222315.24440.Antony.Stone@squid.open.source.it>
 <H4WrV81M78ohqBCmtYCPogenL7owx269wjEYVDu0WaFa52GKWZxqJ5F7FBRJbrmWF916XkvFXWvhKAmw2F31B3AMU8obCDQR9kyBYPYbIo8=@protonmail.com>
Message-ID: <bed6bd5b-ea4f-ffb3-bf05-6443314a00ad@measurement-factory.com>

On 6/22/21 5:33 PM, Arctic5824 wrote:
> I am now using: https://paste.gg/p/anonymous/e7d5080091bc400e8a75e8285b3dea77
> instead of "http_access allow all" i replaced that line with "http_access allow all !CONNECT"
> 
> and it seems to be working, atleast in my browser, yet i still see some users using https,

> 359 5.253.19.75 TCP_MISS/502 4957 GET https://search.yahoo.com/search? - HIER_DIRECT/212.82.100.137 text/html

> Im not sure how they are doing this, I'd like to prevent this

It looks like they are sending plain text "GET https://..." requests to
your Squid. Popular browsers would not do that, but many other clients
can. As I mentioned earlier, you also need to deny such requests. I am
not sure what the best way to do that is, but you can try something like
this:

acl usesHttpsScheme url_regex -i ^https:
...
http_access deny CONNECT
http_access deny usesHttpsScheme
...


Or you can be even more strict and only allow http: scheme:


acl usesHttpScheme url_regex -i ^http:
...
http_access deny CONNECT
http_access deny !usesHttpScheme
...


None of the above configuration snippets were tested by me. Be careful
with the order of your http_access rules.


HTH,

Alex.


From shadowpilot34 at gmail.com  Wed Jun 23 15:45:16 2021
From: shadowpilot34 at gmail.com (His Shadow)
Date: Wed, 23 Jun 2021 18:45:16 +0300
Subject: [squid-users] maxconn acl equivalents
Message-ID: <CAK7W0xff8fbgRjGK81DDyO8PwwbJgE9_j_n_jNFkKO=N_E_WdA@mail.gmail.com>

Hello. I know there's an acl that limits the amount of simultaneous
connections to the proxy server, but it counts these connections for
each incoming remote IP
address. Is there some equivalent to making it use another key for
counting? Like, after authorization every request gets a userid
transaction annotation. Would
it be possible to use that? I can't find any way except by using an
external acl, that would count each request and yield OK or ERR based
on the count, but the
problem with that is I don't know when client disconnects, since I
won't be notified in the external acl.

-- 
HisShadow


From hoper at free.fr  Fri Jun 25 13:16:56 2021
From: hoper at free.fr (hoper)
Date: Fri, 25 Jun 2021 13:16:56 +0000
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>
References: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>
 <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
 <2a6e43df8d47c722733a2b436e98e880@free.fr>
Message-ID: <9802867c55d28c51b6e9beb5f2f96136@free.fr>

Hi again,

> If Squid trusts stale user credentials (i.e. allows new requests with
> stale cached credentials without revalidating them with your
> authentication helper), then this is a Squid bug.

No, I don't think there is a bug here. 
Because each time my helper is used by squid, it write a line in a dedicated log file.
And it seems to work well. In detail :

Let's say I have a account in my DB with: user1,password1,proxy1
As a client, I start my browser and connect myself with user1/password1

In my helper log file, all is good and I can see that squid used the helper,
and it's answer was "OK proxychoice=proxy1".

Now I switch from proxy1 to proxy2 for user1 in the database.

On my browser, I'm still authenticated as user1, and I'm still use proxy1.
(Ok, that's normal). Later, when the TTL is reached (2 minutes in the configuration I sent),
I can see in my helper's log file that squid used it again. This time, the
answer was : "OK proxychoice=proxy2". So, all seems good here too.

But the routing did'nt change. The parent proxy used after 2 minutes is still proxy1, and
it never change until I restart squid.

I hope to have better explain the problem. So you think there is a bug somewhere,
or do we have a configuration problem ? How can we obtain the result we are looking for ?
(Squid should change the parent proxy if needed after the authentication TTL period). 

>Insufficient demand for that feature does not allow me to provide a
>reliable ETA at this time.

Do you have a vague idea of the cost of the developement of this feature ?

Thanks again.


23 juin 2021 17:16 "Alex Rousskov" <rousskov at measurement-factory.com> a ?crit:

> On 6/23/21 7:26 AM, hoper wrote:
> 
>> I thought we can do this with a really small TTL
>> (this explain the auth_param basic credentialsttl 2 minutes).
>> But even after 5 minutes, the new proxy is still not taken into account :(
> 
> If Squid trusts stale user credentials (i.e. allows new requests with
> stale cached credentials without revalidating them with your
> authentication helper), then this is a Squid bug. Credentials become
> stale when their configured (via credentialsttl) TTL expires. The clock
> should start ticking when Squid makes the corresponding authentication
> helper request.
> 
> If you can reproduce this bug with a supported Squid version, then I
> recommend filing a bug report. This problem is essentially unrelated to
> proxychoice annotations though -- either the authentication helper is
> contacted for credentials (re)validation or it is not. It should be
> triaged/reported without paying attention to annotations.
> 
>>> That means disabling caching of authenticated credentials.
>> 
>> You're talking about this page ?
>> http://www.squid-cache.org/Doc/config/external_acl_type
> 
> In this context, no, I am not. Here, I meant auth_param. As I mentioned
> earlier, external ACLs are not related to authentication.
> 
>> I need to add "cache=0" somewhere ?
>> In the same page, there is another option:
>> "ttl=n TTL in seconds for cached results (defaults 1 hour)"
>> Is it going to help us to acheive our goal if we change this value ?
> 
> The configuration you previously shared does not use external ACLs so
> changing external ACL cache parameters will affect nothing.
> 
>>> It also means that this approach will not work well for bumped HTTP requests
>> 
>> Are you talking about http redirection?
> 
> No, I am talking about SslBump features. Search squid.conf.documented
> for letters "bump" to find starting points. If you do not use those
> features, then you can ignore that part of my comment.
> 
>>> yet we have a backburner project to add such support; it is a large change)
>> 
>> Any idea about when we will have this new feature ? Are we talking about a year ?
>> 3 years ? or probably more ?
> 
> Insufficient demand for that feature does not allow me to provide a
> reliable ETA at this time.
> 
> Alex.
> 
>> 22 juin 2021 16:05 Alex Rousskov a ?crit:
>> 
>>> On 6/22/21 5:20 AM, hoper at free.fr wrote:
>> 
>> We are using a database with a list of user, and the proxy they need to use.
>> 
>> So in squid.conf file, we declare an external acl:
>>> FYI: Your are not declaring an external ACL. You are declaring an
>>> authentication helper.
>> 
>> -------------------------------------------------
>> auth_param basic program /mydir/myprogram.sh
>> auth_param basic children 10 startup=1 idle=3
>> auth_param basic realm myrealm
>> auth_param basic credentialsttl 2 minutes
>> -------------------------------------------------
>> 
>> program.sh will check the login/password given by the user.
>> again the ones found in the database. And, if the authentication
>> is sucessfull, it also write on stdout the proxy we need to use for this user.
>> 
>> Example (If this user need to use the proxy number 2):
>> OK proxychoice=p2
>> 
>> If the squid configuration file, we also include another file,
>> which look like this :
>> 
>> -----------------------------------------------------------------
>> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
>> acl p1auth note proxychoice p1
>> cache_peer_access proxy1 allow p1auth
>> http_access allow authenticated p1auth
>> cache_peer_access proxy1 deny all
>> 
>> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
>> acl p2auth note proxychoice p2
>> cache_peer_access proxy2 allow p2auth
>> http_access allow authenticated p2auth
>> cache_peer_access proxy2 deny all
>> -----------------------------------------------------------------
>>> The relative order of your cache_peer_access and http_access directives
>>> may imply that you think that cache_peer_access is applied/decided
>>> first. In reality, http_access is applied/decided first. This
>>> configuration change does not affect Squid decisions, but the following
>>> order would match the reality better:
>>> 
>>> acl p1auth note proxychoice p1
>>> acl p2auth note proxychoice p2
>>> 
>>> http_access allow authenticated p1auth
>>> http_access allow authenticated p2auth
>>> 
>>> cache_peer 10.0.0.1 parent 3128 0 no-query no-digest name=proxy1
>>> cache_peer_access proxy1 allow p1auth
>>> cache_peer_access proxy1 deny all
>>> 
>>> cache_peer 10.0.0.2 parent 3128 0 no-query no-digest name=proxy2
>>> cache_peer_access proxy2 allow p2auth
>>> cache_peer_access proxy2 deny all
>>> 
>>> BTW, if you want to allow all authenticated users, regardless of their
>>> cache_peer choice, then you can remove p1auth and p2auth from
>>> http_access lines.
>> 
>> This configuration is working. The parent proxy used by squid is the good one.
>> BUT: If we change the configuration (proxy for a user) in the database,
>> the change is not take into account until we fully restart squid :(
>> (Even squid -k reconfigure does not work).
>> 
>> Please, any ideas ? What can we do to make this "dynamic" ?
>>> If you want the changes to apply to every received HTTP request, then
>>> you need to make sure that your authentication helper is called for
>>> every received request. That means disabling caching of authenticated
>>> credentials. It also means that this approach will not work well for
>>> bumped HTTP requests because their authentication information is (or
>>> should be) inherited from their CONNECT tunnel. You will have to force
>>> CONNECT tunnels to have at most one request by disabling persistent
>>> connections, I guess.
>>> 
>>> I would also consider separating user authentication from request
>>> routing. To do that, move the proxychoice=... annotation functionality
>>> from the authentication helper to the external ACL helper (with caching
>>> disabled via external_acl_type options). In this approach,
>>> authentication results can be cached for as long as you want without
>>> interfering with more up-to-date routing decisions. However, you should
>>> not use that [slow] external ACL with cache_peer_access lines as
>>> detailed below).
>> 
>> Any change in the database should be taken into account immediatly.
>>> cache_peer_access does not support slow ACLs (yet -- we have a
>>> backburner project to add such support; it is a large change). Until
>>> such support is added, there will always be some delay between obtaining
>>> routing information (at authentication or external_acl computation time)
>>> and applying that routing information (at request routing time).
>>> 
>>> HTH,
>>> 
>>> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ygreenfield at kewsystems.com  Fri Jun 25 19:18:40 2021
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Fri, 25 Jun 2021 15:18:40 -0400
Subject: [squid-users] How to use request headers in external_acl_type
Message-ID: <F8D8224A429A46F1BD90923F85A56C66@OhrSomayach>

Hello all,
 
I'm trying to use request headers in an external acl, and I'm probably doing
it incorrectly, and it's not working.
 
Here's my acl definiton:
 
external_acl_type ext_acl_program  %SRC %>{Connection} %>{Accept}
%>{Custom_header} %>{Host} /etc/squid/ext_acl_program.pl
 
 
The program ext_acl_program.pl simply prints out the input
 
   chomp ($line);
   @fields          = split(' ', $line);
   my $ip           = $fields[0];
   my $connection   = $fields[1];
   my $accept       = $fields[2];
   my $custom       = $fields[3];
   my $host         = $fields[4];
 
   print LOGFILE  "IP: $ip\n Conn: $connection\n Accept: $accept\n Custom:
$custom\n Host: $host";
 
     
The output looks like this:
 
IP: 10.200.10.2
Conn: keep-alive
Accept: -
Custom: -
Host:  <http://www.wsws.com:443> www.wsws.com:443

 
As you see, it has values for %SRC, %>{Connection} and %>{Host}.  It does
not have values for %>{Accept} and %>{Custom_header}
 
 
So the question is, are these %>{} substitutions coming from request_headers
(as I thought)?
 
If yes, why does it only have Connection and Host, and not Accept or my
custom header?
 
If they are not coming from request headers, where are they coming from?
 
And mostly, how can I pass my custom header into the program?
 
Thanks so much for your responses,
Yosi
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210625/af5e0b21/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun 25 23:20:06 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Sat, 26 Jun 2021 11:20:06 +1200
Subject: [squid-users] How to use request headers in external_acl_type
In-Reply-To: <F8D8224A429A46F1BD90923F85A56C66@OhrSomayach>
References: <F8D8224A429A46F1BD90923F85A56C66@OhrSomayach>
Message-ID: <ef3618b8b036c03d7d208de91e2daf78@treenet.co.nz>

On 2021-06-26 07:18, Yosi Greenfield wrote:
> Hello all,
> 
> I'm trying to use request headers in an external acl, and I'm probably
> doing it incorrectly, and it's not working.
> 

Looks like its working fine.


> Here's my acl definiton:
> 
> external_acl_type ext_acl_program  %SRC %>{Connection} %>{Accept}
> %>{Custom_header} %>{Host} /etc/squid/ext_acl_program.pl
> 
> The program ext_acl_program.pl simply prints out the input
> 
>    chomp ($line);
>    @fields          = split(' ', $line);
>    my $ip           = $fields[0];
>    my $connection   = $fields[1];
>    my $accept       = $fields[2];
>    my $custom       = $fields[3];
>    my $host         = $fields[4];
> 
>    print LOGFILE  "IP: $ip\n Conn: $connection\n Accept: $accept\n
> Custom: $custom\n Host: $host";
> 
> The output looks like this:
> 
> IP: 10.200.10.2
> Conn: keep-alive
> Accept: -
> Custom: -
> Host: www.wsws.com:443 [1]
> 
> As you see, it has values for %SRC, %>{Connection} and %>{Host}.  It
> does not have values for %>{Accept} and %>{Custom_header}
> 
> So the question is, are these %>{} substitutions coming from
> request_headers (as I thought)?

The Host header only exists in request messages so I would say they are.
It may not be the request message you are thinking about though. Request 
headers can come from clients, but they could also be generated by Squid 
or ICAP/eCAP services.

> 
> If yes, why does it only have Connection and Host, and not Accept or
> my custom header?
> 

Because those are the headers the message being printed contain.
You do not provide enough details about where the request came from. eg 
how it was created and/or changed between creation and the helper being 
called.


> If they are not coming from request headers, where are they coming
> from?
> 

You can use "debug_options 11,2" to see the HTTP messages Squid is 
processing.


> And mostly, how can I pass my custom header into the program?

Exactly as you configured above. Assuming that the header is actually 
"Custom_header: ..." with that underscore included.


Amos


From squid3 at treenet.co.nz  Sat Jun 26 00:01:40 2021
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Sat, 26 Jun 2021 12:01:40 +1200
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <9802867c55d28c51b6e9beb5f2f96136@free.fr>
References: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>
 <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
 <2a6e43df8d47c722733a2b436e98e880@free.fr>
 <9802867c55d28c51b6e9beb5f2f96136@free.fr>
Message-ID: <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>

On 2021-06-26 01:16, hoper wrote:
> Hi again,
> 
>> If Squid trusts stale user credentials (i.e. allows new requests with
>> stale cached credentials without revalidating them with your
>> authentication helper), then this is a Squid bug.
> 
> No, I don't think there is a bug here.
> Because each time my helper is used by squid, it write a line in a
> dedicated log file.

A Squid bug would likely be later on.

Before we go any further. Which versino of Squid are you using.


> And it seems to work well. In detail :
> 
> Let's say I have a account in my DB with: user1,password1,proxy1
> As a client, I start my browser and connect myself with user1/password1
> 
> In my helper log file, all is good and I can see that squid used the 
> helper,
> and it's answer was "OK proxychoice=proxy1".
> 
> Now I switch from proxy1 to proxy2 for user1 in the database.
> 
> On my browser, I'm still authenticated as user1, and I'm still use 
> proxy1.
> (Ok, that's normal). Later, when the TTL is reached (2 minutes in the
> configuration I sent),
> I can see in my helper's log file that squid used it again. This time, 
> the
> answer was : "OK proxychoice=proxy2". So, all seems good here too.
> 
> But the routing did'nt change. The parent proxy used after 2 minutes
> is still proxy1, and
> it never change until I restart squid.
> 
> I hope to have better explain the problem. So you think there is a bug
> somewhere,
> or do we have a configuration problem ? How can we obtain the result
> we are looking for ?
> (Squid should change the parent proxy if needed after the
> authentication TTL period).
> 

You seem to think that user credentials are thrown away when they reach 
TTL. That is not true.

What actually happens is that shortly *before* TTL is reached they enter 
a grace period during which they will be refreshed using the helper. The 
info the helper provides is then used to *update* the existing 
credentials.

Also, the foo= annotations are additive by default. On more detailed 
inspection you will find the user has become "proxy1" *OR* "proxy2" 
allowed.



>> Insufficient demand for that feature does not allow me to provide a
>> reliable ETA at this time.
> 
> Do you have a vague idea of the cost of the developement of this 
> feature ?
> 

I'm not sure why Alex is offering a feature. A change to helper 
annotations was already implemented in Squid-5 to avoid this exact 
behaviour you are seeing.


> Thanks again.
> 


FYI. The Squid-5 code already has the feature implemented. It is only 
the Squid-4 code which behaves like above.

Amos


From rousskov at measurement-factory.com  Mon Jun 28 18:04:03 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Jun 2021 14:04:03 -0400
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>
References: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>
 <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
 <2a6e43df8d47c722733a2b436e98e880@free.fr>
 <9802867c55d28c51b6e9beb5f2f96136@free.fr>
 <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>
Message-ID: <3432206f-39b9-f491-84b8-d2a0a38cbd4f@measurement-factory.com>

On 6/25/21 8:01 PM, squid3 at treenet.co.nz wrote:

>>> Insufficient demand for that feature does not allow me to provide a
>>> reliable ETA at this time.

>> Do you have a vague idea of the cost of the developement of this
>> feature ?

> I'm not sure why Alex is offering a feature. A change to helper
> annotations was already implemented in Squid-5 to avoid this exact
> behaviour you are seeing.

The feature I was describing (without really offering anything) was
cache_peer_access support for slow ACLs. That missing feature is
orthogonal to helper annotations that you are talking about.


Hope this clarifies,

Alex.


From rousskov at measurement-factory.com  Mon Jun 28 19:15:08 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 28 Jun 2021 15:15:08 -0400
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>
References: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>
 <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
 <2a6e43df8d47c722733a2b436e98e880@free.fr>
 <9802867c55d28c51b6e9beb5f2f96136@free.fr>
 <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>
Message-ID: <41b53139-638f-3c23-45f8-036fb7f38fbf@measurement-factory.com>

On 6/25/21 8:01 PM, squid3 at treenet.co.nz wrote:
> Also, the foo= annotations are additive by default.

I would like to add words of caution to this email thread: Squid helper
annotation handling depends on Squid version, the helper category, and
the annotation name. Most of this is undocumented. Test well before
deploying!

The overall intent of Squid v4+ code processing custom annotations (i.e.
annotations with names that Squid code does not know about in advance),
is for Squid to replace the old annotation value with the new one.
Documentation[1] says that custom annotation names should end with an
underscore ("_")  to avoid clashes with "standard" annotations that
Squid code already knows (or will know) about and, hence, treats (or
will treat) specially.

Based on Amos' summary[2], the values of the following standard
annotations are appended/concatenated/accumulated instead of being replaced:

* Basic: "group", "tag"
* Digest: "group", "tag", "nonce"
* NTLM / Negotiate: "group", "tag"
* external ACL: "group", "tag"
* URL-rewrite: "group", "tag"


Squid v4 (incorrectly) accumulates same-name/different-value
annotations. This problem, reported as bug #4912, was probably fixed in
[3], but

* that fix has not been ported to v4 AFAICT, and
* Squid v5 Auth::User::absorb() code that can be interpreted as
  accumulating same-name/different-value annotations, so there might be
  more similar bugs in Squid that still need to be discovered and fixed.

[1] https://wiki.squid-cache.org/Features/AddonHelpers
[2] https://github.com/squid-cache/squid/pull/393#issuecomment-491481060
[3] https://github.com/squid-cache/squid/commit/d665de3

Alex.


From hoper at free.fr  Mon Jun 28 20:21:05 2021
From: hoper at free.fr (hoper)
Date: Mon, 28 Jun 2021 22:21:05 +0200
Subject: [squid-users] How to execute external helpers for each request ?
In-Reply-To: <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>
References: <98a8831c-02e3-a5bb-1072-0e61f0e16191@measurement-factory.com>
 <b3a2d53a-d3c8-29c0-6f9e-21dece0363e8@measurement-factory.com>
 <84597481.119198178.1624353659230.JavaMail.root@zimbra84-e15.priv.proxad.net>
 <2a6e43df8d47c722733a2b436e98e880@free.fr>
 <9802867c55d28c51b6e9beb5f2f96136@free.fr>
 <115a6f0645634c16fdc4795d7b45bb26@treenet.co.nz>
Message-ID: <2215c712-94c5-425c-f716-0870a9aa4200@free.fr>


 > Before we go any further. Which versino of Squid are you using.

Sorry for the delay, and for not had mention it before.
I'm using squid 4.11 (package found in debian 10 backport repository).
I will try to build a 5.x version for debian 10.
(Not sure if I can do that but... At least I can try :)

 >Also, the foo= annotations are additive by default. On more detailed 
inspection you will find the user has become "proxy1" *OR* "proxy2" 
allowed.

For the helper, yes it is. But not for squid. I'm sure that the client 
is still using the old parent proxy.
I can check this just by visiting web site like whatismyip.com. (Because 
of course, parents proxys does not have the same IP).
If I want to see the IP of the new proxy set in DB, I have to restart 
squid. As soon as the restart is done, I see that the browser is now 
using the new parent proxy for this user.

Thanks everybody.

_______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From jason.spashett at menlosecurity.com  Tue Jun 29 14:00:13 2021
From: jason.spashett at menlosecurity.com (Jason Spashett)
Date: Tue, 29 Jun 2021 15:00:13 +0100
Subject: [squid-users] More detail for access logs on error
Message-ID: <CANj0NToUJkZEA_cbLBnVwq9Hqe7X3XLQ4q7XCPouadbYf3aYFg@mail.gmail.com>

Hello,

I am looking to get more detailed access log errors when failures
occur. Some of the items I am after are present in the squid-cache
logs, however it isn't always easy to correlate them.

For example, DNS failures are an example, in the access log squid
reports a 503 for non-existent domains, but the cache log reports this
explicitly as Sending -3 (Name Error: The domain name does not exist.)

I do note that the custom format option err_detail is available, but I
am not sure what this format option can work with, and in particular
it does not *appear* to work with the response code.

There are many other failures reported in the squid cache log, that I
would potentially like to have attributed to the request as seen in
the access log. I do appreciate also, that some of the cache log
errors are not necessarily attributable to a subsystem failure on a
one to one basis, although they do play a role in the causal chain of
events.

Does anyone have any suggestions on extracting further details in the
case of failed requests?

Regards,

Jason


From moberger at metanetworks.com  Wed Jun 30 08:51:13 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Wed, 30 Jun 2021 11:51:13 +0300
Subject: [squid-users] HTTP requests queue
Message-ID: <CAGSk-42MjS3mBWMD1tpqPnS4dkP_1t41jVja9tdrxYWCcqbQSw@mail.gmail.com>

Hi,

I'm using Squid with ICAPs and ECAPs. Some of them are faster than others.
Does squid hold one queue for each chain (REQMOD/RESPMOD) or per ICAP/ECAP?
Meaning, if I have one ECAP and one ICAP in a chain where the faster ECAP
is first.
Will the ECAP keep getting new HTTP requests to adapt even if the ICAP that
comes after it in the chain hasn't finished adapting the previous requests?
Or is it the other way around where a new HTTP request will be sent to
adaptation only after the previous one was handled by all ICAP/ECAPs in the
chain?

Thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210630/3fb295ac/attachment.htm>

From robertkwild at gmail.com  Wed Jun 30 10:41:15 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 30 Jun 2021 11:41:15 +0100
Subject: [squid-users] Error negotiating SSL connection on FD 366 - cache.log
Message-ID: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>

hi all,

never really noticed this as i rarely "tail -f" the cache log but im
noticing these lines like every second

2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 266:
error:00000001:lib(0):func(0):reason(1) (1/-1)
2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 270:
error:00000001:lib(0):func(0):reason(1) (1/-1)
2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 285:
error:00000001:lib(0):func(0):reason(1) (1/0)

is this something to be worried about

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210630/a554fc41/attachment.htm>

From ben.goz87 at gmail.com  Wed Jun 30 12:16:09 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Wed, 30 Jun 2021 15:16:09 +0300
Subject: [squid-users] TPROXY Error
Message-ID: <CADAqQfwR4w-cFidq+8ea7zns=OUwyan6W1wXuLcrge4jSE4B9Q@mail.gmail.com>

 By the help of God.

Hi All,
I'm trying to configure squid as a transparent proxy using TPROXY.
The machine I'm using has 2 NICs, one for input and the other one for
output traffic.
The TPROXY iptables rules are configured on the input NIC.
It looks like iptables TPROXY redirect works but squid prints out the
following error:

ERROR: NAT/TPROXY lookup failed to locate original IPs on
local=xxx:443 remote=xxx:49471 FD 14 flags=17

I think I loaded all TPROXY required kernel modules.

The ip forwarding works fine without the iptables rules. and I don't
see any squid ERROR on getsockopt

Please let me know what I'm missing?

Thanks,
Ben


From Antony.Stone at squid.open.source.it  Wed Jun 30 12:25:23 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 30 Jun 2021 14:25:23 +0200
Subject: [squid-users] TPROXY Error
In-Reply-To: <CADAqQfwR4w-cFidq+8ea7zns=OUwyan6W1wXuLcrge4jSE4B9Q@mail.gmail.com>
References: <CADAqQfwR4w-cFidq+8ea7zns=OUwyan6W1wXuLcrge4jSE4B9Q@mail.gmail.com>
Message-ID: <202106301425.24180.Antony.Stone@squid.open.source.it>

On Wednesday 30 June 2021 at 14:16:09, Ben Goz wrote:

> I'm trying to configure squid as a transparent proxy using TPROXY.
> The machine I'm using has 2 NICs, one for input and the other one for
> output traffic.
> The TPROXY iptables rules are configured on the input NIC.

1. Which version of Squid are you using?

2. Please show us the TPROXY rules you have.

3. Please show us the relevant lines for intercept proxying from your 
squid.conf


Regards,


Antony.

-- 
"The future is already here.   It's just not evenly distributed yet."

 - William Gibson

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ben.goz87 at gmail.com  Wed Jun 30 12:55:59 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Wed, 30 Jun 2021 15:55:59 +0300
Subject: [squid-users] TPROXY Error
In-Reply-To: <202106301425.24180.Antony.Stone@squid.open.source.it>
References: <CADAqQfwR4w-cFidq+8ea7zns=OUwyan6W1wXuLcrge4jSE4B9Q@mail.gmail.com>
 <202106301425.24180.Antony.Stone@squid.open.source.it>
Message-ID: <b75f828b-c3da-8eef-b882-7b0b16fa6f6d@gmail.com>


On 30/06/2021 15:25, Antony Stone wrote:
> On Wednesday 30 June 2021 at 14:16:09, Ben Goz wrote:
>
>> I'm trying to configure squid as a transparent proxy using TPROXY.
>> The machine I'm using has 2 NICs, one for input and the other one for
>> output traffic.
>> The TPROXY iptables rules are configured on the input NIC.
> 1. Which version of Squid are you using?
# ./squid -v
Squid Cache: Version 4.15
Service Name: squid

This binary uses OpenSSL 1.1.1f? 31 Mar 2020. For legal restrictions on 
distribution see https://www.openssl.org/source/license.html

configure options:? '--with-openssl' '--enable-ssl-crtd' '--enable-ecap' 
'--enable-linux-netfilter' --enable-ltdl-convenience

>
> 2. Please show us the TPROXY rules you have.


iptables -t mangle -N DIVERT
iptables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 1
iptables -t mangle -A DIVERT -j ACCEPT

iptables -t mangle -A PREROUTING -i bond0.213 -p tcp --dport 80 -j 
TPROXY --tproxy-mark 0x1/0x1 --on-port 15644
iptables -t mangle -A PREROUTING -i bond0.213 -p tcp --dport 443 -j 
TPROXY --tproxy-mark 0x1/0x1 --on-port 15645


including:

ip rule add fwmark 1 lookup 100
ip -f inet route add local default dev lo table 100

>
> 3. Please show us the relevant lines for intercept proxying from your
> squid.conf


http_port 15644 tproxy
https_port 15645 ssl-bump tproxy generate-host-certificates=on 
options=ALL dynamic_cert_mem_cache_size=4MB 
cert=/usr/local/squid/etc/ssl_cert/myCA.pem 
dhparams=/usr/local/squid/etc/dhparam.pem
always_direct allow all



>
>
> Regards,
>
>
> Antony.
>


From rousskov at measurement-factory.com  Wed Jun 30 14:54:25 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 30 Jun 2021 10:54:25 -0400
Subject: [squid-users] HTTP requests queue
In-Reply-To: <CAGSk-42MjS3mBWMD1tpqPnS4dkP_1t41jVja9tdrxYWCcqbQSw@mail.gmail.com>
References: <CAGSk-42MjS3mBWMD1tpqPnS4dkP_1t41jVja9tdrxYWCcqbQSw@mail.gmail.com>
Message-ID: <d6dd9b4a-13c8-d8de-ad6c-fb839a813a3d@measurement-factory.com>

On 6/30/21 4:51 AM, Moti Berger wrote:

> I'm using Squid with ICAPs and ECAPs. Some of them are faster than others.
> Does squid hold one queue for each chain (REQMOD/RESPMOD) or per ICAP/ECAP?

Squid maintains an adaptation chain dedicated to each HTTP message. At
that level, Squid does not know the difference between ICAP and eCAP --
they are all treated as generic "adaptation services". REQMOD and
RESPMOD chains associated with the same master transaction may be active
at the same time and can even be viewed as a single chain spanning two
HTTP messages (request and response).


> Meaning, if I have one ECAP and one ICAP in a chain where the faster
> ECAP is first.
> Will the ECAP keep getting new HTTP requests to adapt even if the ICAP
> that comes after it in the chain hasn't finished adapting the previous
> requests?

In general, there is no dependency among adaptation chains: Adaptation
of any two distinct HTTP messages happens independently or "in
parallel". When ICAP service options need to be refreshed, all affected
REQMOD or RESPMOD transactions will wait for that single OPTIONS
transaction, introducing what could be misinterpreted as a dependency.
An adaptation service may stall or prioritize some transactions, but
that is outside Squid's control.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jun 30 15:15:55 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 30 Jun 2021 11:15:55 -0400
Subject: [squid-users] Error negotiating SSL connection on FD 366 -
 cache.log
In-Reply-To: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>
References: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>
Message-ID: <a3dc4695-3600-ac3b-a9dc-34c03a41f2d7@measurement-factory.com>

On 6/30/21 6:41 AM, robert k Wild wrote:

> never really noticed this as i rarely "tail -f" the cache log but im
> noticing these lines like every second

> 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 266:
> error:00000001:lib(0):func(0):reason(1) (1/-1)
> 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 270:
> error:00000001:lib(0):func(0):reason(1) (1/-1)
> 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 285:
> error:00000001:lib(0):func(0):reason(1) (1/0)

> is this something to be worried about

IMHO, you should worry about two things, at least:

1) The fact that you did not know about Squid complaints, especially
frequent ones. I do not think that constantly watching "tail -f" is the
answer here, but something in your Squid administration approach should
change to prevent similar lack of problem awareness in the future.

2) The fact that your Squid is complaining about something every second.
If the actual problem behind these errors does not deserve your
attention, then Squid should not be logging it at level 1 (and you
should complain that it does). Otherwise, the problem itself should be
addressed.

As for the error itself, it looks like your Squid cannot negotiate TLS
with some client(s). I do not know whether it is Squid's fault or the
client's. Enabling "ALL,9" debugging for a few seconds should be
sufficient to identify the client (at least by its IP address), which
may be enough to understand why the negotiation fails (or to give you
enough information to collect more details for triage).


HTH,

Alex.


From robertkwild at gmail.com  Wed Jun 30 15:48:39 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 30 Jun 2021 16:48:39 +0100
Subject: [squid-users] Error negotiating SSL connection on FD 366 -
 cache.log
In-Reply-To: <a3dc4695-3600-ac3b-a9dc-34c03a41f2d7@measurement-factory.com>
References: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>
 <a3dc4695-3600-ac3b-a9dc-34c03a41f2d7@measurement-factory.com>
Message-ID: <CAGU_CiLwbtEeBbT4n82MrMWtmBsVVarn6cMCGWBntM1XoeWWhA@mail.gmail.com>

Thanks Alex,

How do I enable all 9 debugging to find out what client ip it is thats
sending all these tls errors.

There's a lot of mac/pcs that are connected to this squid server and I have
added the myca.der file to there machines as I'm doing ssl bumping.

Thanks,
Rob



On Wed, 30 Jun 2021, 16:16 Alex Rousskov, <rousskov at measurement-factory.com>
wrote:

> On 6/30/21 6:41 AM, robert k Wild wrote:
>
> > never really noticed this as i rarely "tail -f" the cache log but im
> > noticing these lines like every second
>
> > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 266:
> > error:00000001:lib(0):func(0):reason(1) (1/-1)
> > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 270:
> > error:00000001:lib(0):func(0):reason(1) (1/-1)
> > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 285:
> > error:00000001:lib(0):func(0):reason(1) (1/0)
>
> > is this something to be worried about
>
> IMHO, you should worry about two things, at least:
>
> 1) The fact that you did not know about Squid complaints, especially
> frequent ones. I do not think that constantly watching "tail -f" is the
> answer here, but something in your Squid administration approach should
> change to prevent similar lack of problem awareness in the future.
>
> 2) The fact that your Squid is complaining about something every second.
> If the actual problem behind these errors does not deserve your
> attention, then Squid should not be logging it at level 1 (and you
> should complain that it does). Otherwise, the problem itself should be
> addressed.
>
> As for the error itself, it looks like your Squid cannot negotiate TLS
> with some client(s). I do not know whether it is Squid's fault or the
> client's. Enabling "ALL,9" debugging for a few seconds should be
> sufficient to identify the client (at least by its IP address), which
> may be enough to understand why the negotiation fails (or to give you
> enough information to collect more details for triage).
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210630/dffd486b/attachment.htm>

From robertkwild at gmail.com  Wed Jun 30 16:00:02 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 30 Jun 2021 17:00:02 +0100
Subject: [squid-users] Error negotiating SSL connection on FD 366 -
 cache.log
In-Reply-To: <CAGU_CiLwbtEeBbT4n82MrMWtmBsVVarn6cMCGWBntM1XoeWWhA@mail.gmail.com>
References: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>
 <a3dc4695-3600-ac3b-a9dc-34c03a41f2d7@measurement-factory.com>
 <CAGU_CiLwbtEeBbT4n82MrMWtmBsVVarn6cMCGWBntM1XoeWWhA@mail.gmail.com>
Message-ID: <CAGU_CiJ6QaHY8=FnM5DJ8yvJojFhAvJ_X5W7Mu5imA4X6N7TQA@mail.gmail.com>

Cool, so I put this in squid.conf

debug_options 9

And then restart squid and tail the cache.log


On Wed, 30 Jun 2021, 16:48 robert k Wild, <robertkwild at gmail.com> wrote:

> Thanks Alex,
>
> How do I enable all 9 debugging to find out what client ip it is thats
> sending all these tls errors.
>
> There's a lot of mac/pcs that are connected to this squid server and I
> have added the myca.der file to there machines as I'm doing ssl bumping.
>
> Thanks,
> Rob
>
>
>
> On Wed, 30 Jun 2021, 16:16 Alex Rousskov, <
> rousskov at measurement-factory.com> wrote:
>
>> On 6/30/21 6:41 AM, robert k Wild wrote:
>>
>> > never really noticed this as i rarely "tail -f" the cache log but im
>> > noticing these lines like every second
>>
>> > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 266:
>> > error:00000001:lib(0):func(0):reason(1) (1/-1)
>> > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 270:
>> > error:00000001:lib(0):func(0):reason(1) (1/-1)
>> > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 285:
>> > error:00000001:lib(0):func(0):reason(1) (1/0)
>>
>> > is this something to be worried about
>>
>> IMHO, you should worry about two things, at least:
>>
>> 1) The fact that you did not know about Squid complaints, especially
>> frequent ones. I do not think that constantly watching "tail -f" is the
>> answer here, but something in your Squid administration approach should
>> change to prevent similar lack of problem awareness in the future.
>>
>> 2) The fact that your Squid is complaining about something every second.
>> If the actual problem behind these errors does not deserve your
>> attention, then Squid should not be logging it at level 1 (and you
>> should complain that it does). Otherwise, the problem itself should be
>> addressed.
>>
>> As for the error itself, it looks like your Squid cannot negotiate TLS
>> with some client(s). I do not know whether it is Squid's fault or the
>> client's. Enabling "ALL,9" debugging for a few seconds should be
>> sufficient to identify the client (at least by its IP address), which
>> may be enough to understand why the negotiation fails (or to give you
>> enough information to collect more details for triage).
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210630/2778dfdc/attachment.htm>

From rousskov at measurement-factory.com  Wed Jun 30 16:09:10 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 30 Jun 2021 12:09:10 -0400
Subject: [squid-users] Error negotiating SSL connection on FD 366 -
 cache.log
In-Reply-To: <CAGU_CiLwbtEeBbT4n82MrMWtmBsVVarn6cMCGWBntM1XoeWWhA@mail.gmail.com>
References: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>
 <a3dc4695-3600-ac3b-a9dc-34c03a41f2d7@measurement-factory.com>
 <CAGU_CiLwbtEeBbT4n82MrMWtmBsVVarn6cMCGWBntM1XoeWWhA@mail.gmail.com>
Message-ID: <e500e061-6477-8c4a-7891-e1245897c803@measurement-factory.com>

On 6/30/21 11:48 AM, robert k Wild wrote:

> How do I enable all 9 debugging to find out what client ip it is thats
> sending all these tls errors.

0. Start Squid if necessary.

1. Locate your Squid log file or equivalent. In this example, we will
call it cache.log.

2. Run "tail -f cache.log > partial-cache.log" in background or another
terminal. This will start appending new debugging to the
partial-cache.log file.

3. Run "squid -k debug" to enable ALL,9 debugging.

4. Wait a few seconds.

5. Run "squid -k debug" to disable ALL,9 debugging.

6. Kill the "tail" command in step 2.

7. Check that partial-cache.log contains at least one "Error negotiating
SSL connection" entry. If not, go to step 2 and repeat. Perhaps give
Squid a few extra seconds this time.

8. Analyze the resulting partial-cache.log (or share it with those who
are willing to analyze it for you, compressing it if needed). Please
note that this debugging log may contain sensitive information such as
user names and passwords.


HTH,

Alex.


> On Wed, 30 Jun 2021, 16:16 Alex Rousskov wrote:
> 
>     On 6/30/21 6:41 AM, robert k Wild wrote:
> 
>     > never really noticed this as i rarely "tail -f" the cache log but im
>     > noticing these lines like every second
> 
>     > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 266:
>     > error:00000001:lib(0):func(0):reason(1) (1/-1)
>     > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 270:
>     > error:00000001:lib(0):func(0):reason(1) (1/-1)
>     > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD 285:
>     > error:00000001:lib(0):func(0):reason(1) (1/0)
> 
>     > is this something to be worried about
> 
>     IMHO, you should worry about two things, at least:
> 
>     1) The fact that you did not know about Squid complaints, especially
>     frequent ones. I do not think that constantly watching "tail -f" is the
>     answer here, but something in your Squid administration approach should
>     change to prevent similar lack of problem awareness in the future.
> 
>     2) The fact that your Squid is complaining about something every second.
>     If the actual problem behind these errors does not deserve your
>     attention, then Squid should not be logging it at level 1 (and you
>     should complain that it does). Otherwise, the problem itself should be
>     addressed.
> 
>     As for the error itself, it looks like your Squid cannot negotiate TLS
>     with some client(s). I do not know whether it is Squid's fault or the
>     client's. Enabling "ALL,9" debugging for a few seconds should be
>     sufficient to identify the client (at least by its IP address), which
>     may be enough to understand why the negotiation fails (or to give you
>     enough information to collect more details for triage).
> 
> 
>     HTH,
> 
>     Alex.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 



From robertkwild at gmail.com  Wed Jun 30 16:36:21 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Wed, 30 Jun 2021 17:36:21 +0100
Subject: [squid-users] Error negotiating SSL connection on FD 366 -
 cache.log
In-Reply-To: <e500e061-6477-8c4a-7891-e1245897c803@measurement-factory.com>
References: <CAGU_CiKm7QzYzqNBqE69qCFc+-J0GN98k_=s737w4hzO9K6TPg@mail.gmail.com>
 <a3dc4695-3600-ac3b-a9dc-34c03a41f2d7@measurement-factory.com>
 <CAGU_CiLwbtEeBbT4n82MrMWtmBsVVarn6cMCGWBntM1XoeWWhA@mail.gmail.com>
 <e500e061-6477-8c4a-7891-e1245897c803@measurement-factory.com>
Message-ID: <CAGU_CiJAQdF1+6dC0P_zgk-RZpn01CkP1W0jbgkEaMqmnfeqjg@mail.gmail.com>

thanks Alex

seems like one client (it shows the ip) is trying to get to this site but i
havnt added it to my white list, so thats why its getting blocked

events.gfe.nvidia.com

thanks a bunch alex, your awesome







On Wed, 30 Jun 2021 at 17:09, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 6/30/21 11:48 AM, robert k Wild wrote:
>
> > How do I enable all 9 debugging to find out what client ip it is thats
> > sending all these tls errors.
>
> 0. Start Squid if necessary.
>
> 1. Locate your Squid log file or equivalent. In this example, we will
> call it cache.log.
>
> 2. Run "tail -f cache.log > partial-cache.log" in background or another
> terminal. This will start appending new debugging to the
> partial-cache.log file.
>
> 3. Run "squid -k debug" to enable ALL,9 debugging.
>
> 4. Wait a few seconds.
>
> 5. Run "squid -k debug" to disable ALL,9 debugging.
>
> 6. Kill the "tail" command in step 2.
>
> 7. Check that partial-cache.log contains at least one "Error negotiating
> SSL connection" entry. If not, go to step 2 and repeat. Perhaps give
> Squid a few extra seconds this time.
>
> 8. Analyze the resulting partial-cache.log (or share it with those who
> are willing to analyze it for you, compressing it if needed). Please
> note that this debugging log may contain sensitive information such as
> user names and passwords.
>
>
> HTH,
>
> Alex.
>
>
> > On Wed, 30 Jun 2021, 16:16 Alex Rousskov wrote:
> >
> >     On 6/30/21 6:41 AM, robert k Wild wrote:
> >
> >     > never really noticed this as i rarely "tail -f" the cache log but
> im
> >     > noticing these lines like every second
> >
> >     > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD
> 266:
> >     > error:00000001:lib(0):func(0):reason(1) (1/-1)
> >     > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD
> 270:
> >     > error:00000001:lib(0):func(0):reason(1) (1/-1)
> >     > 2021/06/30 11:39:13 kid1| Error negotiating SSL connection on FD
> 285:
> >     > error:00000001:lib(0):func(0):reason(1) (1/0)
> >
> >     > is this something to be worried about
> >
> >     IMHO, you should worry about two things, at least:
> >
> >     1) The fact that you did not know about Squid complaints, especially
> >     frequent ones. I do not think that constantly watching "tail -f" is
> the
> >     answer here, but something in your Squid administration approach
> should
> >     change to prevent similar lack of problem awareness in the future.
> >
> >     2) The fact that your Squid is complaining about something every
> second.
> >     If the actual problem behind these errors does not deserve your
> >     attention, then Squid should not be logging it at level 1 (and you
> >     should complain that it does). Otherwise, the problem itself should
> be
> >     addressed.
> >
> >     As for the error itself, it looks like your Squid cannot negotiate
> TLS
> >     with some client(s). I do not know whether it is Squid's fault or the
> >     client's. Enabling "ALL,9" debugging for a few seconds should be
> >     sufficient to identify the client (at least by its IP address), which
> >     may be enough to understand why the negotiation fails (or to give you
> >     enough information to collect more details for triage).
> >
> >
> >     HTH,
> >
> >     Alex.
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210630/2732f591/attachment.htm>

From ygreenfield at kewsystems.com  Wed Jun 30 17:17:19 2021
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Wed, 30 Jun 2021 13:17:19 -0400
Subject: [squid-users] How to use request headers in external_acl_type
In-Reply-To: <ef3618b8b036c03d7d208de91e2daf78@treenet.co.nz>
References: <F8D8224A429A46F1BD90923F85A56C66@OhrSomayach>
 <ef3618b8b036c03d7d208de91e2daf78@treenet.co.nz>
Message-ID: <7416EB54B03743A4B7B8AFDB52229FDC@OhrSomayach>

Amos,

As always, thank you for your dedication answering all our questions.

Ok, turns out, as you noted, the browser is sending the correct request
headers. However, on https requests the external acl program is not getting
the custom header we're sending. SSL Bump is set, and works for our
redirector program, but not for the external acl program.

Here are the relevant lines from squid.conf:

   http_port 3128 name=non-bumped
   http_port 3130 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=6MB cert=/etc/squid/ssl/newCA.pem name=bumped
options=ALL

   acl non-bumped myportname non-bumped
   acl bumped myportname bumped

   acl step1 at_step SslBump1
   acl broken_sites dstdomain "/etc/squid/nobump/domains"
   acl broken_sites_regex dstdom_regex -i "/etc/squid/nobump/regexes"
   ssl_bump splice broken_sites
   ssl_bump splice broken_sites_regex
   ssl_bump peek step1
   ssl_bump bump all

   external_acl_type portal_gatekeeper  %SRC %>{Connection} %>{Accept}
%>{abc_session} %>{Host} /etc/squid/portal.pl
   acl check-portal external portal_gatekeeper
   deny_info http://www.our_portal_site.com/ check-portal

   acl myIP1 src 10.200.10.2
   http_access deny myIP1 !check-portal

   sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB
   sslcrtd_children 15 startup=5

   sslproxy_cert_error allow all
   request_header_access Surrogate-Capability deny all

   url_rewrite_access allow non-bumped
   url_rewrite_access deny bumped CONNECT
   url_rewrite_children 15  startup=7

   acl our_users src 10.10.0.0/24 10.10.1.0/24 10.200.0.0/16
   http_access allow our_users


Is it possible to get the custom abc_session header on https requests?

Thank you again.



> -----Original Message-----
> From: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf 
> Of squid3 at treenet.co.nz
> Sent: Friday, June 25, 2021 7:20 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] How to use request headers in 
> external_acl_type
> 
> On 2021-06-26 07:18, Yosi Greenfield wrote:
> > Hello all,
> > 
> > I'm trying to use request headers in an external acl, and 
> I'm probably 
> > doing it incorrectly, and it's not working.
> > 
> 
> Looks like its working fine.
> 
> 
> > Here's my acl definiton:
> > 
> > external_acl_type ext_acl_program  %SRC %>{Connection} %>{Accept} 
> > %>{Custom_header} %>{Host} /etc/squid/ext_acl_program.pl
> > 
> > The program ext_acl_program.pl simply prints out the input
> > 
> >    chomp ($line);
> >    @fields          = split(' ', $line);
> >    my $ip           = $fields[0];
> >    my $connection   = $fields[1];
> >    my $accept       = $fields[2];
> >    my $custom       = $fields[3];
> >    my $host         = $fields[4];
> > 
> >    print LOGFILE  "IP: $ip\n Conn: $connection\n Accept: $accept\n
> > Custom: $custom\n Host: $host";
> > 
> > The output looks like this:
> > 
> > IP: 10.200.10.2
> > Conn: keep-alive
> > Accept: -
> > Custom: -
> > Host: www.wsws.com:443 [1]
> > 
> > As you see, it has values for %SRC, %>{Connection} and 
> %>{Host}.  It 
> > does not have values for %>{Accept} and %>{Custom_header}
> > 
> > So the question is, are these %>{} substitutions coming from 
> > request_headers (as I thought)?
> 
> The Host header only exists in request messages so I would 
> say they are.
> It may not be the request message you are thinking about 
> though. Request headers can come from clients, but they could 
> also be generated by Squid or ICAP/eCAP services.
> 
> > 
> > If yes, why does it only have Connection and Host, and not 
> Accept or 
> > my custom header?
> > 
> 
> Because those are the headers the message being printed contain.
> You do not provide enough details about where the request 
> came from. eg how it was created and/or changed between 
> creation and the helper being called.
> 
> 
> > If they are not coming from request headers, where are they coming 
> > from?
> > 
> 
> You can use "debug_options 11,2" to see the HTTP messages 
> Squid is processing.
> 
> 
> > And mostly, how can I pass my custom header into the program?
> 
> Exactly as you configured above. Assuming that the header is actually 
> "Custom_header: ..." with that underscore included.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



