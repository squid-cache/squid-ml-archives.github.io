From m_zouhairy at ckta.by  Mon Mar  1 07:07:00 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 1 Mar 2021 10:07:00 +0300
Subject: [squid-users] squid cache
In-Reply-To: <39cb4439-afd4-64c2-1d8f-f65bf088e717@ckta.by>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
 <39cb4439-afd4-64c2-1d8f-f65bf088e717@ckta.by>
Message-ID: <4da8b799-3f9b-ee24-673a-14e0c85e2775@ckta.by>

i tried this, but neither the https download bandwidth restriction nor 
caching seems to be working as expected

acl slower src 10.46.10.78
acl localnet src 10.46.10.0/24

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 8080	# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost
visible_hostname proxy.lk.sk


delay_pools 1
delay_class 1 3
delay_access 1 allow slower
delay_access 1 deny all
delay_parameters 1 51200/51200 -1/-1 51200/25600

http_access allow localnet
http_access allow localhost



# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 8080

# Uncomment and adjust the following to add a disk cache directory.
# Updates: chrome and acrobat
refresh_pattern -i gvt1.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80% 
129600 reload-into-ims
refresh_pattern -i adobe.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80% 
129600 reload-into-ims



range_offset_limit 200 MB
maximum_object_size 200 MB
quick_abort_min -1

# DONT MODIFY THESE LINES
refresh_pattern \^ftp:           1440    20%     10080
refresh_pattern \^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0      0%      0
refresh_pattern . 		  0      20%     43200

cache_dir ufs /var/cache/squid 3000 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

cache_mem 1024 MB

netdb_filename none

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4
#debug_options ALL,1 33,2 28,9


any help?


On 2/26/21 10:22 AM, Majed Zouhairy wrote:
> 
> Health be Upon you,
> 
> i want to cache certain files, let's say exe, msi... above 20MB and 
> below 300MB, limit the cache directory to 3GB
> i have no ssl bump not configured
> version 4.14
> how to do that?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From kbvz at ymail.com  Mon Mar  1 13:23:48 2021
From: kbvz at ymail.com (Stephane Simon)
Date: Mon, 1 Mar 2021 13:23:48 +0000 (UTC)
Subject: [squid-users] SNMP OID SQUID 4
References: <589643160.1681291.1614605028190.ref@mail.yahoo.com>
Message-ID: <589643160.1681291.1614605028190@mail.yahoo.com>


Hello,

?

I am looking for snmp's oid for squid 4


?

?
Here (https://wiki.squid-cache.org/Features/Snmp#Squid_OIDs)i found for squid 2 and 3



?

Can u help me please ?

?

Thanks

?

St?phane

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210301/21b7474e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 1614604933065blob.jpg
Type: image/png
Size: 5910 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210301/21b7474e/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 1614604960214blob.jpg
Type: image/png
Size: 44505 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210301/21b7474e/attachment-0001.png>

From uhlar at fantomas.sk  Mon Mar  1 15:01:22 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 1 Mar 2021 16:01:22 +0100
Subject: [squid-users] SNMP OID SQUID 4
In-Reply-To: <589643160.1681291.1614605028190@mail.yahoo.com>
References: <589643160.1681291.1614605028190.ref@mail.yahoo.com>
 <589643160.1681291.1614605028190@mail.yahoo.com>
Message-ID: <20210301150122.GB11865@fantomas.sk>

On 01.03.21 13:23, Stephane Simon wrote:
>I am looking for snmp's oid for squid 4
>Here (https://wiki.squid-cache.org/Features/Snmp#Squid_OIDs)i found for squid 2 and 3

I use those for squid4 without problems.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"Where do you want to go to die?" [Microsoft]


From rousskov at measurement-factory.com  Mon Mar  1 15:12:15 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 1 Mar 2021 10:12:15 -0500
Subject: [squid-users] squid cache
In-Reply-To: <4da8b799-3f9b-ee24-673a-14e0c85e2775@ckta.by>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
 <39cb4439-afd4-64c2-1d8f-f65bf088e717@ckta.by>
 <4da8b799-3f9b-ee24-673a-14e0c85e2775@ckta.by>
Message-ID: <9c7254ec-8a1c-3560-4d2c-937b9f9f0c1d@measurement-factory.com>

On 3/1/21 2:07 AM, Majed Zouhairy wrote:
> i tried this, but neither the https download bandwidth restriction nor
> caching seems to be working as expected

Squid cannot cache HTTP responses without bumping HTTPS traffic. This is
a protocol-level limitation, not a bug.

There are known delay pools bugs for not-bumped (i.e. tunneled or
CONNECT) traffic. IIRC, the pools may work for some tunnels, but the
imposed limits may vary significantly from the configured values.


HTH,

Alex.


> acl slower src 10.46.10.78
> acl localnet src 10.46.10.0/24
> 
> acl SSL_ports port 443
> acl Safe_ports port 80??????? # http
> acl Safe_ports port 8080??? # http
> acl Safe_ports port 21??????? # ftp
> acl Safe_ports port 443??????? # https
> acl Safe_ports port 70??????? # gopher
> acl Safe_ports port 210??????? # wais
> acl Safe_ports port 1025-65535??? # unregistered ports
> acl Safe_ports port 280??????? # http-mgmt
> acl Safe_ports port 488??????? # gss-http
> acl Safe_ports port 591??????? # filemaker
> acl Safe_ports port 777??????? # multiling http
> acl CONNECT method CONNECT
> acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> visible_hostname proxy.lk.sk
> 
> 
> delay_pools 1
> delay_class 1 3
> delay_access 1 allow slower
> delay_access 1 deny all
> delay_parameters 1 51200/51200 -1/-1 51200/25600
> 
> http_access allow localnet
> http_access allow localhost
> 
> 
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 8080
> 
> # Uncomment and adjust the following to add a disk cache directory.
> # Updates: chrome and acrobat
> refresh_pattern -i gvt1.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80%
> 129600 reload-into-ims
> refresh_pattern -i adobe.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80%
> 129600 reload-into-ims
> 
> 
> 
> range_offset_limit 200 MB
> maximum_object_size 200 MB
> quick_abort_min -1
> 
> # DONT MODIFY THESE LINES
> refresh_pattern \^ftp:?????????? 1440??? 20%???? 10080
> refresh_pattern \^gopher:??????? 1440??? 0%????? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0????? 0%????? 0
> refresh_pattern .?????????? 0????? 20%???? 43200
> 
> cache_dir ufs /var/cache/squid 3000 16 256
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/cache/squid
> 
> cache_mem 1024 MB
> 
> netdb_filename none
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:??????? 1440??? 20%??? 10080
> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
> refresh_pattern .??????? 0??? 20%??? 4320
> 
> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l
> /var/log/squid/
> url_rewrite_children 16 startup=8 idle=2 concurrency=4
> #debug_options ALL,1 33,2 28,9
> 
> 
> any help?
> 
> 
> On 2/26/21 10:22 AM, Majed Zouhairy wrote:
>>
>> Health be Upon you,
>>
>> i want to cache certain files, let's say exe, msi... above 20MB and
>> below 300MB, limit the cache directory to 3GB
>> i have no ssl bump not configured
>> version 4.14
>> how to do that?
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From service.mv at gmail.com  Tue Mar  2 01:35:46 2021
From: service.mv at gmail.com (Service MV)
Date: Mon, 1 Mar 2021 22:35:46 -0300
Subject: [squid-users] Exclude transaction from being logged
In-Reply-To: <05ca2db3-d2a3-ece6-88dc-8e5946decbba@measurement-factory.com>
References: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>
 <4f7fadbe-1a53-3d06-0d29-c8ec237a4da9@measurement-factory.com>
 <CA+d==oG7k5+LG-LrkOAgV8fPusqMaJPXGsq8n8hX7ZVMeRu-CQ@mail.gmail.com>
 <05ca2db3-d2a3-ece6-88dc-8e5946decbba@measurement-factory.com>
Message-ID: <CA+d==oGPprf4Y3dQdZTKyytrO_evgjXH7snu9YjCbTcrx7Sisw@mail.gmail.com>

Unfortunately, the log that I whant to avoid is being still logged.

El vie., 26 feb. 2021 17:14, Alex Rousskov <rousskov at measurement-factory.com>
escribi?:

> On 2/26/21 2:58 PM, Service MV wrote:
> > Ok, but whit this two lines disabled completely access logs. I've no
> > more access_log directives configured.
>
> Yeah, this outcome is surprising to many admins (and there are also
> related bugs in Squid code).
>
> To see all other records, add access_log lines that define how you want
> the rest of the records to be logged. For example, if you want Squid to
> use the default record format and other defaults, then try this untested
> sketch:
>
>    acl zabbix_proxy src x.x.x.x
>    access_log none zabbix_proxy
>    access_log daemon:/specify/exact/log/file/location/here
>
> You can see the default access_log configuration specific to your Squid
> build in squid.conf.documented.
>
>
> HTH,
>
> Alex.
>
>
>
> > El vie., 26 feb. 2021 16:21, Alex Rousskov escribi?:
> >
> >     On 2/26/21 12:36 PM, Service MV wrote:
> >
> >     > NONE/000 0 NONE error:transaction-end-before-headers - HIER_NONE/-
> -
> >     >
> >     > I know that this is not an error. But I want to exclude this log
> when
> >     > the Zabbix server checks the port status of SQUID. Zabbix server
> does
> >     > not use the SQUID as a proxy.
> >
> >     If you do not want to see what Zabbix is doing to your Squid, you can
> >     exclude its requests from access.log. For example, if you do not
> want to
> >     see any requests from Zabbix IP address:
> >
> >       acl zabbix_proxy src x.x.x.x
> >       access_log none zabbix_proxy
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210301/32109821/attachment.htm>

From rousskov at measurement-factory.com  Tue Mar  2 02:39:49 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 1 Mar 2021 21:39:49 -0500
Subject: [squid-users] Exclude transaction from being logged
In-Reply-To: <CA+d==oGPprf4Y3dQdZTKyytrO_evgjXH7snu9YjCbTcrx7Sisw@mail.gmail.com>
References: <CA+d==oG9o=QK3FpqSOj-wqyboJ=oD9cpm7eS7Xz5pdPPArsWrQ@mail.gmail.com>
 <4f7fadbe-1a53-3d06-0d29-c8ec237a4da9@measurement-factory.com>
 <CA+d==oG7k5+LG-LrkOAgV8fPusqMaJPXGsq8n8hX7ZVMeRu-CQ@mail.gmail.com>
 <05ca2db3-d2a3-ece6-88dc-8e5946decbba@measurement-factory.com>
 <CA+d==oGPprf4Y3dQdZTKyytrO_evgjXH7snu9YjCbTcrx7Sisw@mail.gmail.com>
Message-ID: <d5e75acb-40aa-2c5a-ffcd-7538f21cf063@measurement-factory.com>

On 3/1/21 8:35 PM, Service MV wrote:
> Unfortunately, the log that I whant to avoid is being still logged.

Do the unwanted logged records match zabbix_proxy?

* If yes, I am out of ideas. If you share a debugging log dedicated to
the incorrectly logged transaction[1], I may be able to figure it out.

* If not, then perhaps I misunderstood what your zabbix_proxy ACL means
to you. You may need a different ACL.

[1]
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

Alex.


> El vie., 26 feb. 2021 17:14, Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>> escribi?:
> 
>     On 2/26/21 2:58 PM, Service MV wrote:
>     > Ok, but whit this two lines disabled completely access logs. I've no
>     > more access_log directives configured.
> 
>     Yeah, this outcome is surprising to many admins (and there are also
>     related bugs in Squid code).
> 
>     To see all other records, add access_log lines that define how you want
>     the rest of the records to be logged. For example, if you want Squid to
>     use the default record format and other defaults, then try this untested
>     sketch:
> 
>     ? ?acl zabbix_proxy src x.x.x.x
>     ? ?access_log none zabbix_proxy
>     ? ?access_log daemon:/specify/exact/log/file/location/here
> 
>     You can see the default access_log configuration specific to your Squid
>     build in squid.conf.documented.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
>     > El vie., 26 feb. 2021 16:21, Alex Rousskov escribi?:
>     >
>     >? ? ?On 2/26/21 12:36 PM, Service MV wrote:
>     >
>     >? ? ?> NONE/000 0 NONE error:transaction-end-before-headers -
>     HIER_NONE/- -
>     >? ? ?>
>     >? ? ?> I know that this is not an error. But I want to exclude this
>     log when
>     >? ? ?> the Zabbix server checks the port status of SQUID. Zabbix
>     server does
>     >? ? ?> not use the SQUID as a proxy.
>     >
>     >? ? ?If you do not want to see what Zabbix is doing to your Squid,
>     you can
>     >? ? ?exclude its requests from access.log. For example, if you do
>     not want to
>     >? ? ?see any requests from Zabbix IP address:
>     >
>     >? ? ?? acl zabbix_proxy src x.x.x.x
>     >? ? ?? access_log none zabbix_proxy
>     >
>     >
>     >? ? ?HTH,
>     >
>     >? ? ?Alex.
>     >
> 



From m_zouhairy at ckta.by  Tue Mar  2 06:04:53 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Tue, 2 Mar 2021 09:04:53 +0300
Subject: [squid-users] squid cache
In-Reply-To: <9c7254ec-8a1c-3560-4d2c-937b9f9f0c1d@measurement-factory.com>
References: <CAGEc96==77=NOQZkV3zjw-h98+ye95Y2U=kEdcrbj5UJWXYfCQ@mail.gmail.com>
 <518f37bf-5e04-b77a-176e-e823abe83e80@measurement-factory.com>
 <39cb4439-afd4-64c2-1d8f-f65bf088e717@ckta.by>
 <4da8b799-3f9b-ee24-673a-14e0c85e2775@ckta.by>
 <9c7254ec-8a1c-3560-4d2c-937b9f9f0c1d@measurement-factory.com>
Message-ID: <1e33d98a-5f75-8af2-f896-dafac6e821d7@ckta.by>

Thanks for, at least, the explanation

On 3/1/21 6:12 PM, Alex Rousskov wrote:
> On 3/1/21 2:07 AM, Majed Zouhairy wrote:
>> i tried this, but neither the https download bandwidth restriction nor
>> caching seems to be working as expected
> 
> Squid cannot cache HTTP responses without bumping HTTPS traffic. This is
> a protocol-level limitation, not a bug.
> 
> There are known delay pools bugs for not-bumped (i.e. tunneled or
> CONNECT) traffic. IIRC, the pools may work for some tunnels, but the
> imposed limits may vary significantly from the configured values.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> acl slower src 10.46.10.78
>> acl localnet src 10.46.10.0/24
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80??????? # http
>> acl Safe_ports port 8080??? # http
>> acl Safe_ports port 21??????? # ftp
>> acl Safe_ports port 443??????? # https
>> acl Safe_ports port 70??????? # gopher
>> acl Safe_ports port 210??????? # wais
>> acl Safe_ports port 1025-65535??? # unregistered ports
>> acl Safe_ports port 280??????? # http-mgmt
>> acl Safe_ports port 488??????? # gss-http
>> acl Safe_ports port 591??????? # filemaker
>> acl Safe_ports port 777??????? # multiling http
>> acl CONNECT method CONNECT
>> acl blockfiles urlpath_regex -i "/etc/squid/blocks.files.acl"
>>
>> #
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> http_access deny !Safe_ports
>>
>> # Deny CONNECT to other than secure SSL ports
>> http_access deny CONNECT !SSL_ports
>>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> #http_access deny to_localhost
>> visible_hostname proxy.lk.sk
>>
>>
>> delay_pools 1
>> delay_class 1 3
>> delay_access 1 allow slower
>> delay_access 1 deny all
>> delay_parameters 1 51200/51200 -1/-1 51200/25600
>>
>> http_access allow localnet
>> http_access allow localhost
>>
>>
>>
>> # And finally deny all other access to this proxy
>> http_access deny all
>>
>> # Squid normally listens to port 3128
>> http_port 8080
>>
>> # Uncomment and adjust the following to add a disk cache directory.
>> # Updates: chrome and acrobat
>> refresh_pattern -i gvt1.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80%
>> 129600 reload-into-ims
>> refresh_pattern -i adobe.com/.*\.(exe|ms[i|u|f|p]|dat|zip|psf) 43200 80%
>> 129600 reload-into-ims
>>
>>
>>
>> range_offset_limit 200 MB
>> maximum_object_size 200 MB
>> quick_abort_min -1
>>
>> # DONT MODIFY THESE LINES
>> refresh_pattern \^ftp:?????????? 1440??? 20%???? 10080
>> refresh_pattern \^gopher:??????? 1440??? 0%????? 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0????? 0%????? 0
>> refresh_pattern .?????????? 0????? 20%???? 43200
>>
>> cache_dir ufs /var/cache/squid 3000 16 256
>>
>> # Leave coredumps in the first cache dir
>> coredump_dir /var/cache/squid
>>
>> cache_mem 1024 MB
>>
>> netdb_filename none
>>
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>> refresh_pattern ^ftp:??????? 1440??? 20%??? 10080
>> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
>> refresh_pattern .??????? 0??? 20%??? 4320
>>
>> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l
>> /var/log/squid/
>> url_rewrite_children 16 startup=8 idle=2 concurrency=4
>> #debug_options ALL,1 33,2 28,9
>>
>>
>> any help?
>>
>>
>> On 2/26/21 10:22 AM, Majed Zouhairy wrote:
>>>
>>> Health be Upon you,
>>>
>>> i want to cache certain files, let's say exe, msi... above 20MB and
>>> below 300MB, limit the cache directory to 3GB
>>> i have no ssl bump not configured
>>> version 4.14
>>> how to do that?
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 


From jmpatagonia at gmail.com  Tue Mar  2 12:21:14 2021
From: jmpatagonia at gmail.com (jmpatagonia)
Date: Tue, 2 Mar 2021 12:21:14 +0000
Subject: [squid-users] permit google chrome updates
Message-ID: <CADZCxssqrWdhEuTxyKHNq1sOJjvpfXU6cGnEu8zvLr3Bg_09iw@mail.gmail.com>

I need help to permit google chrome on squid

02/Mar/2021:12:18:01 -0300 || - || 10.114.37.20 || TCP_DENIED/407|| CONNECT
|| clients1.google.com:443 || text/html


I think that clients1.google.com:443 request are from google chrome.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210302/b880a1d6/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar  2 15:35:36 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Mar 2021 04:35:36 +1300
Subject: [squid-users] permit google chrome updates
In-Reply-To: <CADZCxssqrWdhEuTxyKHNq1sOJjvpfXU6cGnEu8zvLr3Bg_09iw@mail.gmail.com>
References: <CADZCxssqrWdhEuTxyKHNq1sOJjvpfXU6cGnEu8zvLr3Bg_09iw@mail.gmail.com>
Message-ID: <818b9955-1d72-fd0c-669f-6a2fd364d201@treenet.co.nz>

On 3/03/21 1:21 am, jmpatagonia wrote:
> I need help to permit google chrome on squid
> 
> 02/Mar/2021:12:18:01 -0300 || - || 10.114.37.20 || TCP_DENIED/407|| 
> CONNECT || clients1.google.com:443 || text/html
> 


The 407 means authentication is required.

see <https://wiki.squid-cache.org/Features/Authentication>


Amos


From squid3 at treenet.co.nz  Tue Mar  2 16:27:33 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Mar 2021 05:27:33 +1300
Subject: [squid-users] permit google chrome updates
In-Reply-To: <CADZCxssYXM9gp1o_exRwXr_ML=ng+sLb8QPEb8bj_Fu5pJoPEg@mail.gmail.com>
References: <CADZCxssqrWdhEuTxyKHNq1sOJjvpfXU6cGnEu8zvLr3Bg_09iw@mail.gmail.com>
 <818b9955-1d72-fd0c-669f-6a2fd364d201@treenet.co.nz>
 <CADZCxssYXM9gp1o_exRwXr_ML=ng+sLb8QPEb8bj_Fu5pJoPEg@mail.gmail.com>
Message-ID: <006b4c58-7aea-fbbf-813c-5eacda231208@treenet.co.nz>

On 3/03/21 1:59 am, jmpatagonia wrote:
> Yes the proxy have external authentication --> auth_param basic program 
> /usr/lib/squid/basic_ldap_auth....
> but I receive a lot of request from users like
> 
> 02/Mar/2021:12:45:02 -0300 || - || xx.xx.xx.xxxx || TCP_DENIED/407|| 
> CONNECT || update.googleapis.com:443 || text/html
> 
> I think the users use the browser for local network services and not 
> validate on proxy, but the browser (chrome) still trying to use the 
> proxy for update.
> 
> How can I fix this ?
> 

No secure client will send login credentials unless they are actually 
needed. 407 are simply a message from the proxy telling the client to 
send credentials, and what type(s) to send (eg Basic).


First check that these requests are not followed almost immediately by a 
second request from the client with credentials. If that is happening 
there is no problem with the client.

Secondly, check if clients are having trouble logging in with correct 
credentials. If they are, figure out why. It may be a problem with the 
auth helper or your access controls sequence.


Amos


From hello at ironpeak.be  Thu Mar  4 10:36:27 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 4 Mar 2021 11:36:27 +0100
Subject: [squid-users] squid ssl-bump with icap returns 503
Message-ID: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>

Hi guys,

I?m asking here but since I?m not too comfortable with a mailing list, it?s also on serverfault.com: https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately <https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately>

I have an odd issue that squid will return a HTTP 503 when I try to do ICAP for an ssl-bumped HTTPS website. HTTP website works fine.
Any ideas?

Config:

visible_hostname proxy
forwarded_for delete
via off
httpd_suppress_version_string on
logfile_rotate 0
cache_log stdio:/dev/stdout
access_log stdio:/dev/stdout
cache_store_log stdio:/dev/stdout
dns_v4_first on
cache_dir ufs /cache 100 16 256
pid_filename /cache/squid.pid
mime_table /usr/share/squid/mime.conf
http_port 0.0.0.0:3128
https_port 0.0.0.0:3129 \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key
ssl_bump peek all
ssl_bump bump all
quick_abort_min 0
quick_abort_max 0
quick_abort_pct 95
pinger_enable off
icap_enable on
icap_service_failure_limit -1
icap_service service_req reqmod_precache bypass=0 icap://10.10.0.119:1344/
icap_preview_enable on
adaptation_access service_req allow all
cache_mem 512 mb
dns_nameservers 1.1.1.1 1.0.0.1
cache_effective_user proxy
sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
sslproxy_cert_error allow all
http_access allow all

Log line HTTPS when it doesn?t work:
1614853306.542     40 172.17.0.1 NONE/503 0 CONNECT //ironpeak.be:443 - HIER_NONE/- -

< HTTP/1.1 503 Service Unavailable
< Server: squid
< Mime-Version: 1.0
< Date: Thu, 04 Mar 2021 10:36:05 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 1849
< X-Squid-Error: ERR_DNS_FAIL 0


Log line HTTP when it does work:
  -1 1614851916 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/
1614853320.743 SWAPOUT 00 00000002 F7A390D89822E9BA831C47E1B4CDD0A8  301 1614853320        -1 1614853320 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/
1614853320.748    302 172.17.0.1 TCP_REFRESH_MODIFIED/301 1647 GET http://ironpeak.be/blog/big-sur-t2rminator/ - HIER_DIRECT/104.21.60.47 text/plain

Example CLI command used:
ALL_PROXY="https://127.0.0.1:3129" curl -vvv --proxy-insecure http://ironpeak.be/ <http://ironpeak.be/>

Command used to start squid:
exec /usr/sbin/squid -f /etc/squid/squid.conf --foreground -YCd 1
Package info:
Package: squid-openssl
Version: 4.13-5

Many thanks!
Regards,
Niels Hofmans

SITE   https://ironpeak.be
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/0b83649d/attachment.htm>

From ngtech1ltd at gmail.com  Thu Mar  4 11:01:39 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Thu, 4 Mar 2021 13:01:39 +0200
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
Message-ID: <CABA8h=QNeHYntM5tweSsEMBtaYRwdvnsJWA2DaKxvdBPY09UgA@mail.gmail.com>

Would it be possible to dump some icap traffic so we would be able to
understand what might cause this issue if at all?

Eliezer

?????? ??? ??, 4 ???? 2021, 12:36, ??? Niels Hofmans ?<hello at ironpeak.be>:

> Hi guys,
>
> I?m asking here but since I?m not too comfortable with a mailing list,
> it?s also on serverfault.com:
> https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately
>
> I have an odd issue that squid will return a HTTP 503 when I try to do
> ICAP for an ssl-bumped HTTPS website. HTTP website works fine.
> Any ideas?
>
> Config:
>
> visible_hostname proxy
> forwarded_for delete
> via off
> httpd_suppress_version_string on
> logfile_rotate 0
> cache_log stdio:/dev/stdout
> access_log stdio:/dev/stdout
> cache_store_log stdio:/dev/stdout
> dns_v4_first on
> cache_dir ufs /cache 100 16 256
> pid_filename /cache/squid.pid
> mime_table /usr/share/squid/mime.conf
> http_port 0.0.0.0:3128
> https_port 0.0.0.0:3129 \
>     generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
>     tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key
> ssl_bump peek all
> ssl_bump bump all
> quick_abort_min 0
> quick_abort_max 0
> quick_abort_pct 95
> pinger_enable off
> icap_enable on
> icap_service_failure_limit -1
> icap_service service_req reqmod_precache bypass=0 icap://10.10.0.119:1344/
> icap_preview_enable on
> adaptation_access service_req allow all
> cache_mem 512 mb
> dns_nameservers 1.1.1.1 1.0.0.1
> cache_effective_user proxy
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl_db -M
> 4MB
> sslcrtd_children 8 startup=1 idle=1
> sslproxy_cert_error allow all
> http_access allow all
>
> Log line HTTPS when it doesn?t work:
> 1614853306.542     40 172.17.0.1 NONE/503 0 CONNECT //ironpeak.be:443 -
> HIER_NONE/- -
>
> < HTTP/1.1 503 Service Unavailable
> < Server: squid
> < Mime-Version: 1.0
> < Date: Thu, 04 Mar 2021 10:36:05 GMT
> < Content-Type: text/html;charset=utf-8
> < Content-Length: 1849
> < X-Squid-Error: ERR_DNS_FAIL 0
>
>
> Log line HTTP when it does work:
>   -1 1614851916 text/plain 60/60 GET
> http://ironpeak.be/blog/big-sur-t2rminator/
> 1614853320.743 SWAPOUT 00 00000002 F7A390D89822E9BA831C47E1B4CDD0A8  301
> 1614853320        -1 1614853320 text/plain 60/60 GET
> http://ironpeak.be/blog/big-sur-t2rminator/
> 1614853320.748    302 172.17.0.1 TCP_REFRESH_MODIFIED/301 1647 GET
> http://ironpeak.be/blog/big-sur-t2rminator/ - HIER_DIRECT/104.21.60.47
> text/plain
>
> Example CLI command used:
> ALL_PROXY="https://127.0.0.1:3129" curl -vvv --proxy-insecure
> http://ironpeak.be/
>
> Command used to start squid:
>
> exec /usr/sbin/squid -f /etc/squid/squid.conf --foreground -YCd 1
>
> Package info:
> Package: squid-openssl
> Version: 4.13-5
>
> Many thanks!
> Regards,
> Niels Hofmans
>
> SITE   https://ironpeak.be
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/695846e4/attachment.htm>

From hello at ironpeak.be  Thu Mar  4 11:21:09 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 4 Mar 2021 12:21:09 +0100
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <CABA8h=QNeHYntM5tweSsEMBtaYRwdvnsJWA2DaKxvdBPY09UgA@mail.gmail.com>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
 <CABA8h=QNeHYntM5tweSsEMBtaYRwdvnsJWA2DaKxvdBPY09UgA@mail.gmail.com>
Message-ID: <D8643B75-5229-4DA2-9D9B-9B8F5038D093@ironpeak.be>

Hi,

I think I may have found an issue: it only seems to ICAP the CONNECT request, whereas it will not pass any subsequent requests in that CONNECT tunnel to ICAP?

So my original implementation did not check for the HTTP method in ICAP, so it returned the wrong CONNECT hostname:

OPTIONS icap://10.10.0.119:1344/ ICAP/1.0
Host: 10.10.0.119:1344
Allow: 206

ICAP/1.0 200 OK
Allow: 200,204
Connection: close
Date: Thu, 04 Mar 2021 11:11:45 GMT
Encapsulated: null-body=0
Methods: REQMOD,REQRESP
Preview: 0
Transfer-Preview: *

CONNECT ironpeak.be:443 HTTP/1.1
User-Agent: curl/7.64.1
Host: ironpeak.be:443

REQMOD icap://10.10.0.119:1344/ ICAP/1.0
Host: 10.10.0.119:1344
Date: Thu, 04 Mar 2021 11:11:23 GMT
Encapsulated: req-hdr=0, null-body=84
Preview: 0
Allow: 204

ICAP/1.0 200 OK
Connection: close
Date: Thu, 04 Mar 2021 11:11:23 GMT
Encapsulated: req-hdr=0, null-body=111

CONNECT //ironpeak.be:443/blog/big-sur-t2rminator/ HTTP/1.1  <<<< here is my bug
Host: ironpeak.be:443
User-Agent: curl/7.64.1

But now, it does not pass any HTTP request in the CONNECT tunnel to ICAP:

CONNECT ironpeak.be:443 HTTP/1.1
User-Agent: curl/7.64.1
Host: ironpeak.be:443

REQMOD icap://10.10.0.119:1344/ ICAP/1.0
Host: 10.10.0.119:1344
Date: Thu, 04 Mar 2021 11:19:00 GMT
Encapsulated: req-hdr=0, null-body=84
Preview: 0
Allow: 204

ICAP/1.0 204 No Modifications
Connection: close
Date: Thu, 04 Mar 2021 11:19:00 GMT
Encapsulated: null-body=0

..TLS ciphertext..    <<<<. No more ICAP requests


Any idea on how I pass -every- sslbumped request to ICAP?
Thank you.

Regards,
Niels Hofmans
SITE   https://ironpeak.be

On 4 Mar 2021, at 12:01, NgTech LTD <ngtech1ltd at gmail.com> wrote:

Would it be possible to dump some icap traffic so we would be able to understand what might cause this issue if at all?

Eliezer

?????? ??? ??, 4 ???? 2021, 12:36, ??? Niels Hofmans ?<hello at ironpeak.be <mailto:hello at ironpeak.be>>:
Hi guys,

I?m asking here but since I?m not too comfortable with a mailing list, it?s also on serverfault.com <http://serverfault.com/>: https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately <https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately>

I have an odd issue that squid will return a HTTP 503 when I try to do ICAP for an ssl-bumped HTTPS website. HTTP website works fine.
Any ideas?

Config:

visible_hostname proxy
forwarded_for delete
via off
httpd_suppress_version_string on
logfile_rotate 0
cache_log stdio:/dev/stdout
access_log stdio:/dev/stdout
cache_store_log stdio:/dev/stdout
dns_v4_first on
cache_dir ufs /cache 100 16 256
pid_filename /cache/squid.pid
mime_table /usr/share/squid/mime.conf
http_port 0.0.0.0:3128 <http://0.0.0.0:3128/>
https_port 0.0.0.0:3129 <http://0.0.0.0:3129/> \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key
ssl_bump peek all
ssl_bump bump all
quick_abort_min 0
quick_abort_max 0
quick_abort_pct 95
pinger_enable off
icap_enable on
icap_service_failure_limit -1
icap_service service_req reqmod_precache bypass=0 icap://10.10.0.119:1344/ <>
icap_preview_enable on
adaptation_access service_req allow all
cache_mem 512 mb
dns_nameservers 1.1.1.1 1.0.0.1
cache_effective_user proxy
sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
sslproxy_cert_error allow all
http_access allow all

Log line HTTPS when it doesn?t work:
1614853306.542     40 172.17.0.1 NONE/503 0 CONNECT //ironpeak.be:443 <http://ironpeak.be:443/> - HIER_NONE/- -

< HTTP/1.1 503 Service Unavailable
< Server: squid
< Mime-Version: 1.0
< Date: Thu, 04 Mar 2021 10:36:05 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 1849
< X-Squid-Error: ERR_DNS_FAIL 0


Log line HTTP when it does work:
  -1 1614851916 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/ <http://ironpeak.be/blog/big-sur-t2rminator/>
1614853320.743 SWAPOUT 00 00000002 F7A390D89822E9BA831C47E1B4CDD0A8  301 1614853320        -1 1614853320 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/ <http://ironpeak.be/blog/big-sur-t2rminator/>
1614853320.748    302 172.17.0.1 TCP_REFRESH_MODIFIED/301 1647 GET http://ironpeak.be/blog/big-sur-t2rminator/ <http://ironpeak.be/blog/big-sur-t2rminator/> - HIER_DIRECT/104.21.60.47 <http://104.21.60.47/> text/plain

Example CLI command used:
ALL_PROXY="https://127.0.0.1:3129 <https://127.0.0.1:3129/>" curl -vvv --proxy-insecure http://ironpeak.be/ <http://ironpeak.be/>

Command used to start squid:
exec /usr/sbin/squid -f /etc/squid/squid.conf --foreground -YCd 1
Package info:
Package: squid-openssl
Version: 4.13-5

Many thanks!
Regards,
Niels Hofmans

SITE   https://ironpeak.be <https://ironpeak.be/>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/c3efc9e7/attachment.htm>

From hello at ironpeak.be  Thu Mar  4 12:13:07 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 4 Mar 2021 13:13:07 +0100
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <D8643B75-5229-4DA2-9D9B-9B8F5038D093@ironpeak.be>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
 <CABA8h=QNeHYntM5tweSsEMBtaYRwdvnsJWA2DaKxvdBPY09UgA@mail.gmail.com>
 <D8643B75-5229-4DA2-9D9B-9B8F5038D093@ironpeak.be>
Message-ID: <EBE79ACA-BABA-4F24-A41C-D4D011856432@ironpeak.be>

Hi,

Interestingly this seems to work on a http_proxy listener:

http_port 0.0.0.0:3129 ssl-bump \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    cert=/etc/squid/ssl/squid.crt key=/etc/squid/ssl/squid.key
    #tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key

always_direct allow all
ssl_bump bump all

But with https_port, I require tproxy/intercept which if I configure it returns:

http_port 0.0.0.0:3128 ssl-bump
https_port 0.0.0.0:3129 ssl-bump intercept \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    cert=/etc/squid/ssl/squid.crt key=/etc/squid/ssl/squid.key \
    tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key
2021/03/04 12:11:27 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=172.17.0.2:3129 remote=172.17.0.1:64488 FD 13 flags=33: (2) No such file or directory
2021/03/04 12:11:27 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=172.17.0.2:3129 remote=172.17.0.1:64488 FD 13 flags=33: (2) No such file or directory
2021/03/04 12:11:27 kid1| ERROR: NAT/TPROXY lookup failed to locate original IPs on local=172.17.0.2:3129 remote=172.17.0.1:64488 FD 13 flags=33
2021/03/04 12:11:27 kid1| ERROR: NAT/TPROXY lookup failed to locate original IPs on local=172.17.0.2:3129 remote=172.17.0.1:64488 FD 13 flags=33
1614859887.972      0 172.17.0.1 NONE/000 0 NONE error:accept-client-connection - HIER_NONE/- -


And:

http_port 0.0.0.0:3128 ssl-bump
https_port 0.0.0.0:3129 ssl-bump tproxy \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    cert=/etc/squid/ssl/squid.crt key=/etc/squid/ssl/squid.key \
    tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key

FATAL: https_port: TPROXY support in the system does not work.


Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 4 Mar 2021, at 12:21, Niels Hofmans <hello at ironpeak.be> wrote:

Hi,

I think I may have found an issue: it only seems to ICAP the CONNECT request, whereas it will not pass any subsequent requests in that CONNECT tunnel to ICAP?

So my original implementation did not check for the HTTP method in ICAP, so it returned the wrong CONNECT hostname:

OPTIONS icap://10.10.0.119:1344/ <icap://10.10.0.119:1344/> ICAP/1.0
Host: 10.10.0.119:1344
Allow: 206

ICAP/1.0 200 OK
Allow: 200,204
Connection: close
Date: Thu, 04 Mar 2021 11:11:45 GMT
Encapsulated: null-body=0
Methods: REQMOD,REQRESP
Preview: 0
Transfer-Preview: *

CONNECT ironpeak.be:443 <http://ironpeak.be:443/> HTTP/1.1
User-Agent: curl/7.64.1
Host: ironpeak.be:443 <http://ironpeak.be:443/>

REQMOD icap://10.10.0.119:1344/ <icap://10.10.0.119:1344/> ICAP/1.0
Host: 10.10.0.119:1344
Date: Thu, 04 Mar 2021 11:11:23 GMT
Encapsulated: req-hdr=0, null-body=84
Preview: 0
Allow: 204

ICAP/1.0 200 OK
Connection: close
Date: Thu, 04 Mar 2021 11:11:23 GMT
Encapsulated: req-hdr=0, null-body=111

CONNECT //ironpeak.be:443 <http://ironpeak.be:443/>/blog/big-sur-t2rminator/ HTTP/1.1  <<<< here is my bug
Host: ironpeak.be:443 <http://ironpeak.be:443/>
User-Agent: curl/7.64.1

But now, it does not pass any HTTP request in the CONNECT tunnel to ICAP:

CONNECT ironpeak.be:443 <http://ironpeak.be:443/> HTTP/1.1
User-Agent: curl/7.64.1
Host: ironpeak.be:443 <http://ironpeak.be:443/>

REQMOD icap://10.10.0.119:1344/ <icap://10.10.0.119:1344/> ICAP/1.0
Host: 10.10.0.119:1344
Date: Thu, 04 Mar 2021 11:19:00 GMT
Encapsulated: req-hdr=0, null-body=84
Preview: 0
Allow: 204

ICAP/1.0 204 No Modifications
Connection: close
Date: Thu, 04 Mar 2021 11:19:00 GMT
Encapsulated: null-body=0

..TLS ciphertext..    <<<<. No more ICAP requests


Any idea on how I pass -every- sslbumped request to ICAP?
Thank you.

Regards,
Niels Hofmans
SITE   https://ironpeak.be <https://ironpeak.be/>

On 4 Mar 2021, at 12:01, NgTech LTD <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>> wrote:

Would it be possible to dump some icap traffic so we would be able to understand what might cause this issue if at all?

Eliezer

?????? ??? ??, 4 ???? 2021, 12:36, ??? Niels Hofmans ?<hello at ironpeak.be <mailto:hello at ironpeak.be>>:
Hi guys,

I?m asking here but since I?m not too comfortable with a mailing list, it?s also on serverfault.com <http://serverfault.com/>: https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately <https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately>

I have an odd issue that squid will return a HTTP 503 when I try to do ICAP for an ssl-bumped HTTPS website. HTTP website works fine.
Any ideas?

Config:

visible_hostname proxy
forwarded_for delete
via off
httpd_suppress_version_string on
logfile_rotate 0
cache_log stdio:/dev/stdout
access_log stdio:/dev/stdout
cache_store_log stdio:/dev/stdout
dns_v4_first on
cache_dir ufs /cache 100 16 256
pid_filename /cache/squid.pid
mime_table /usr/share/squid/mime.conf
http_port 0.0.0.0:3128 <http://0.0.0.0:3128/>
https_port 0.0.0.0:3129 <http://0.0.0.0:3129/> \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key
ssl_bump peek all
ssl_bump bump all
quick_abort_min 0
quick_abort_max 0
quick_abort_pct 95
pinger_enable off
icap_enable on
icap_service_failure_limit -1
icap_service service_req reqmod_precache bypass=0 icap://10.10.0.119:1344/ <>
icap_preview_enable on
adaptation_access service_req allow all
cache_mem 512 mb
dns_nameservers 1.1.1.1 1.0.0.1
cache_effective_user proxy
sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
sslproxy_cert_error allow all
http_access allow all

Log line HTTPS when it doesn?t work:
1614853306.542     40 172.17.0.1 NONE/503 0 CONNECT //ironpeak.be:443 <http://ironpeak.be:443/> - HIER_NONE/- -

< HTTP/1.1 503 Service Unavailable
< Server: squid
< Mime-Version: 1.0
< Date: Thu, 04 Mar 2021 10:36:05 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 1849
< X-Squid-Error: ERR_DNS_FAIL 0


Log line HTTP when it does work:
  -1 1614851916 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/ <http://ironpeak.be/blog/big-sur-t2rminator/>
1614853320.743 SWAPOUT 00 00000002 F7A390D89822E9BA831C47E1B4CDD0A8  301 1614853320        -1 1614853320 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/ <http://ironpeak.be/blog/big-sur-t2rminator/>
1614853320.748    302 172.17.0.1 TCP_REFRESH_MODIFIED/301 1647 GET http://ironpeak.be/blog/big-sur-t2rminator/ <http://ironpeak.be/blog/big-sur-t2rminator/> - HIER_DIRECT/104.21.60.47 <http://104.21.60.47/> text/plain

Example CLI command used:
ALL_PROXY="https://127.0.0.1:3129 <https://127.0.0.1:3129/>" curl -vvv --proxy-insecure http://ironpeak.be/ <http://ironpeak.be/>

Command used to start squid:
exec /usr/sbin/squid -f /etc/squid/squid.conf --foreground -YCd 1
Package info:
Package: squid-openssl
Version: 4.13-5

Many thanks!
Regards,
Niels Hofmans

SITE   https://ironpeak.be <https://ironpeak.be/>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/8ba13e3d/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar  4 12:25:09 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Mar 2021 01:25:09 +1300
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
Message-ID: <1eebb7c9-f59d-21c1-8605-a544eda18bb3@treenet.co.nz>

On 4/03/21 11:36 pm, Niels Hofmans wrote:
> Hi guys,
> 
> I?m asking here but since I?m not too comfortable with a mailing list, 
> it?s also on serverfault.com <http://serverfault.com>: 
> https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately 
> <https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately>
> 
> I have an odd issue that squid will return a HTTP 503 when I try to do 
> ICAP for an ssl-bumped HTTPS website. HTTP website works fine.
> Any ideas?
> 
> Config:
> 
> visible_hostname proxy
> forwarded_for delete
> via off
> httpd_suppress_version_string on
> logfile_rotate 0
> cache_log stdio:/dev/stdout
> access_log stdio:/dev/stdout
> cache_store_log stdio:/dev/stdout
> dns_v4_first on
> cache_dir ufs /cache 100 16 256
> pid_filename /cache/squid.pid
> mime_table /usr/share/squid/mime.conf
> http_port 0.0.0.0:3128
> https_port 0.0.0.0:3129 \
>  ? ? generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
>  ? ? tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key


Neither of these Squid listening ports do SSL-Bump (aka. interception of 
TLS) in any way.

The first receives normal HTTP forward/explicit proxy traffic over TCP.

The second receives normal HTTP forward/explicit proxy traffic over TLS 
(aka "TLS explicit proxy"). Not to be confused with HTTPS (https:// URLs).



> ssl_bump peek all
> ssl_bump bump all
> quick_abort_min 0
> quick_abort_max 0
> quick_abort_pct 95
> pinger_enable off
> icap_enable on
> icap_service_failure_limit -1
> icap_service service_req reqmod_precache bypass=0 
>   icap://10.10.0.119:1344/
> icap_preview_enable on
> adaptation_access service_req allow all
> cache_mem 512 mb
> dns_nameservers 1.1.1.1 1.0.0.1
> cache_effective_user proxy
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl_db -M 4MB
> sslcrtd_children 8 startup=1 idle=1
> sslproxy_cert_error allow all
> http_access allow all
> 
> Log line HTTPS when it doesn?t work:
> 1614853306.542 ? ? 40 172.17.0.1 NONE/503 0 CONNECT //ironpeak.be:443 
>  - HIER_NONE/- -

This is a https:// request which the client is tunneling (CONNECT) 
through a forward/explicit proxy.


> 
> < HTTP/1.1 503 Service Unavailable
> < Server: squid
> < Mime-Version: 1.0
> < Date: Thu, 04 Mar 2021 10:36:05 GMT
> < Content-Type: text/html;charset=utf-8
> < Content-Length: 1849
> < X-Squid-Error: ERR_DNS_FAIL 0
> 
> 
> Log line HTTP when it does work:
>  ? -1 1614851916 text/plain 60/60 GET 
> http://ironpeak.be/blog/big-sur-t2rminator/ 


As you can see this is *not* an HTTPS (https://) request. It is a normal 
HTTP (http://) request sent to a proxy over TLS - which is what your 
port 3129 is expecting.


Amos


From hello at ironpeak.be  Thu Mar  4 12:39:26 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 4 Mar 2021 13:39:26 +0100
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <1eebb7c9-f59d-21c1-8605-a544eda18bb3@treenet.co.nz>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
 <1eebb7c9-f59d-21c1-8605-a544eda18bb3@treenet.co.nz>
Message-ID: <DF7095B6-8626-41EC-AC0D-7839DB6AF5C2@ironpeak.be>

Hi Amos,

Thank you for getting back to me.
So if ssl-bump is required on the http(s)_port directive, I end up at:

http_port 0.0.0.0:3128
https_port 0.0.0.0:3129 ssl-bump intercept \
    generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
    cert=/etc/squid/ssl/squid.crt key=/etc/squid/ssl/squid.key \
    tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key

always_direct allow all
ssl_bump bump all

This however ends up with following logs:

2021/03/04 12:37:43 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=172.17.0.2:3129 remote=172.17.0.1:55508 FD 13 flags=33: (2) No such file or directory
2021/03/04 12:37:43 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=172.17.0.2:3129 remote=172.17.0.1:55508 FD 13 flags=33: (2) No such file or directory
2021/03/04 12:37:43 kid1| ERROR: NAT/TPROXY lookup failed to locate original IPs on local=172.17.0.2:3129 remote=172.17.0.1:55508 FD 13 flags=33
2021/03/04 12:37:43 kid1| ERROR: NAT/TPROXY lookup failed to locate original IPs on local=172.17.0.2:3129 remote=172.17.0.1:55508 FD 13 flags=33
1614861463.880      0 172.17.0.1 NONE/000 0 NONE error:accept-client-connection - HIER_NONE/- -

Command to reproduce:

 % ALL_PROXY="http://127.0.0.1:3129" curl -k -vvv --proxy-insecure -X POST --data 'foo' https://ironpeak.be/


Regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 4 Mar 2021, at 13:25, Amos Jeffries <squid3 at treenet.co.nz> wrote:

On 4/03/21 11:36 pm, Niels Hofmans wrote:
> Hi guys,
> I?m asking here but since I?m not too comfortable with a mailing list, it?s also on serverfault.com <http://serverfault.com>: https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately <https://serverfault.com/questions/1055663/squid-icap-not-working-if-using-tls-interception-but-both-work-separately>
> I have an odd issue that squid will return a HTTP 503 when I try to do ICAP for an ssl-bumped HTTPS website. HTTP website works fine.
> Any ideas?
> Config:
> visible_hostname proxy
> forwarded_for delete
> via off
> httpd_suppress_version_string on
> logfile_rotate 0
> cache_log stdio:/dev/stdout
> access_log stdio:/dev/stdout
> cache_store_log stdio:/dev/stdout
> dns_v4_first on
> cache_dir ufs /cache 100 16 256
> pid_filename /cache/squid.pid
> mime_table /usr/share/squid/mime.conf
> http_port 0.0.0.0:3128
> https_port 0.0.0.0:3129 \
>     generate-host-certificates=on dynamic_cert_mem_cache_size=10MB \
>     tls-cert=/etc/squid/ssl/squid.crt tls-key=/etc/squid/ssl/squid.key


Neither of these Squid listening ports do SSL-Bump (aka. interception of TLS) in any way.

The first receives normal HTTP forward/explicit proxy traffic over TCP.

The second receives normal HTTP forward/explicit proxy traffic over TLS (aka "TLS explicit proxy"). Not to be confused with HTTPS (https:// URLs).



> ssl_bump peek all
> ssl_bump bump all
> quick_abort_min 0
> quick_abort_max 0
> quick_abort_pct 95
> pinger_enable off
> icap_enable on
> icap_service_failure_limit -1
> icap_service service_req reqmod_precache bypass=0   icap://10.10.0.119:1344/
> icap_preview_enable on
> adaptation_access service_req allow all
> cache_mem 512 mb
> dns_nameservers 1.1.1.1 1.0.0.1
> cache_effective_user proxy
> sslcrtd_program /usr/lib/squid/security_file_certgen -s /cache/ssl_db -M 4MB
> sslcrtd_children 8 startup=1 idle=1
> sslproxy_cert_error allow all
> http_access allow all
> Log line HTTPS when it doesn?t work:
> 1614853306.542     40 172.17.0.1 NONE/503 0 CONNECT //ironpeak.be:443  - HIER_NONE/- -

This is a https:// request which the client is tunneling (CONNECT) through a forward/explicit proxy.


> < HTTP/1.1 503 Service Unavailable
> < Server: squid
> < Mime-Version: 1.0
> < Date: Thu, 04 Mar 2021 10:36:05 GMT
> < Content-Type: text/html;charset=utf-8
> < Content-Length: 1849
> < X-Squid-Error: ERR_DNS_FAIL 0
> Log line HTTP when it does work:
>   -1 1614851916 text/plain 60/60 GET http://ironpeak.be/blog/big-sur-t2rminator/ 


As you can see this is *not* an HTTPS (https://) request. It is a normal HTTP (http://) request sent to a proxy over TLS - which is what your port 3129 is expecting.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/a98da0d3/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar  4 13:21:32 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Mar 2021 02:21:32 +1300
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <DF7095B6-8626-41EC-AC0D-7839DB6AF5C2@ironpeak.be>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
 <1eebb7c9-f59d-21c1-8605-a544eda18bb3@treenet.co.nz>
 <DF7095B6-8626-41EC-AC0D-7839DB6AF5C2@ironpeak.be>
Message-ID: <25e245a4-d886-ba4a-59af-3d22483749a8@treenet.co.nz>

On 5/03/21 1:39 am, Niels Hofmans wrote:
> Hi Amos,
> 
> Thank you for getting back to me.
> So if ssl-bump is required on the http(s)_port directive, I end up at:
> 

https_port simply means TLS is the transport protocol. The transport is 
terminated at the proxy. There are many permutations of what is being 
done inside that TLS.

So no http_port does not require "ssl-bump".

Squid does not support TLS-inside-TLS encryption layering. Which is why 
"ssl-bump" only works for "intercept" or "tproxy" modes on that port 
directive.




> http_port 0.0.0.0:3128
> https_port 0.0.0.0:3129 ssl-bump intercept \
>      generate-host-certificates=ondynamic_cert_mem_cache_size=10MB \
>      cert=/etc/squid/ssl/squid.crtkey=/etc/squid/ssl/squid.key \
>      tls-cert=/etc/squid/ssl/squid.crttls-key=/etc/squid/ssl/squid.key
> 
> always_direct allow all
> ssl_bump bump all
> 
> 
> This however ends up with following logs:
> 
> 2021/03/04 12:37:43 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on 
> local=172.17.0.2:3129 remote=172.17.0.1:55508 FD 13 flags=33: (2) No 
> such file or directory

Which means your NAT systems have no records of port 443 being diverted 
to port 3129.


> Command to reproduce:
> 
>  ?% ALL_PROXY="http://127.0.0.1:3129 <http://127.0.0.1:3129>" curl -k 
> -vvv --proxy-insecure -X POST --data 'foo' https://ironpeak.be/ 
> <https://ironpeak.be/>
> 

Correct test command for "https_port 3129 intercept ssl-bump" is:

   curl --cacert /etc/squid/ssl/squid.crt -vvv \
        -X POST --data 'foo' https://ironpeak.be/

This verifies that the system is diverting traffic to the proxy, the 
proxy is the TLS agent for those connections, and that connectivity 
through the proxy is working.
  TLS failure to verify the cert(s) indicate the proxy is not being reached.

Amos


From hello at ironpeak.be  Thu Mar  4 19:52:44 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 4 Mar 2021 20:52:44 +0100
Subject: [squid-users] Squid full request logging
Message-ID: <FB702A6F-6FDB-4D1E-AF04-149F6ED3D42F@ironpeak.be>

Hi,

In addition to my ICAP server now authorising requests for my squid instance, is it possible to do full request/response logging?
I do not see the appropriate log_format directive in the docs.
I was hoping not having to do this in my ICAP service since this slows down approval of the HTTP request. (Empty preview v.s. a request capped at 1MB that needs to be sent over every time)
Any suggestions on what the best way forward is here?
Thank you.

Regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/8a2e9466/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar  4 21:23:17 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Mar 2021 16:23:17 -0500
Subject: [squid-users] Squid full request logging
In-Reply-To: <FB702A6F-6FDB-4D1E-AF04-149F6ED3D42F@ironpeak.be>
References: <FB702A6F-6FDB-4D1E-AF04-149F6ED3D42F@ironpeak.be>
Message-ID: <7447d4bc-e8f5-d51b-f91c-e15d7921b285@measurement-factory.com>

On 3/4/21 2:52 PM, Niels Hofmans wrote:

> is it possible to do full request/response logging?

Squid can log HTTP headers with %>h and %<h logformat codes.

Squid cannot log HTTP message bodies.


> I do not see the appropriate log_format directive in the docs.
> I was hoping not having to do this in my ICAP service since this slows
> down approval of the HTTP request. (Empty preview v.s. a request capped
> at 1MB that needs to be sent over every time)

FWIW, an ICAP or eCAP service can start responding to the request
_before_ the service receives the entire HTTP message body. To get
things going, all the service needs is HTTP headers (and even that is,
technically, optional in some cases).

Using an adaptation service is still an overhead, of course, but, very
few legitimate Squid use cases involve logging message bodies, so there
is no built-in mechanism optimized for that specific rare purpose
(yet?). The fastest option available today is probably a dedicated eCAP
service that refuses to adapt the message bit continues to receive (and
log) the message body.


HTH,

Alex.


From hello at ironpeak.be  Thu Mar  4 21:33:17 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Thu, 4 Mar 2021 22:33:17 +0100
Subject: [squid-users] Squid full request logging
In-Reply-To: <7447d4bc-e8f5-d51b-f91c-e15d7921b285@measurement-factory.com>
References: <FB702A6F-6FDB-4D1E-AF04-149F6ED3D42F@ironpeak.be>
 <7447d4bc-e8f5-d51b-f91c-e15d7921b285@measurement-factory.com>
Message-ID: <384D8BCA-4AD7-4C47-9EB6-470FB82398B7@ironpeak.be>

Hi Alex,

Thanks for the feedback. Although I am not proficient in C for writing an ecap service, is there some binding available online for Go?
This was the reason I originally opted for an ICAP service since I can abstract Go behind the HTTP ICAP layer.
Now I understand this has its limitations, but AFAIK a preview cap at 100kb would be sufficient per request.
But this will slow down my current setup greatly, as I?m currently sending -only- the headers.

Would you think that a) using Go for the ecap adapter or b) using two ICAP services.
One would validate the headers and return OK or NOT (bypass=0), while the other only pushes the 1kb request/response to a queue.
Ideally those two would be contacted simultaneously while only the first one is blocking.
..just thinking aloud tough.

Regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 4 Mar 2021, at 22:23, Alex Rousskov <rousskov at measurement-factory.com> wrote:

On 3/4/21 2:52 PM, Niels Hofmans wrote:

> is it possible to do full request/response logging?

Squid can log HTTP headers with %>h and %<h logformat codes.

Squid cannot log HTTP message bodies.


> I do not see the appropriate log_format directive in the docs.
> I was hoping not having to do this in my ICAP service since this slows
> down approval of the HTTP request. (Empty preview v.s. a request capped
> at 1MB that needs to be sent over every time)

FWIW, an ICAP or eCAP service can start responding to the request
_before_ the service receives the entire HTTP message body. To get
things going, all the service needs is HTTP headers (and even that is,
technically, optional in some cases).

Using an adaptation service is still an overhead, of course, but, very
few legitimate Squid use cases involve logging message bodies, so there
is no built-in mechanism optimized for that specific rare purpose
(yet?). The fastest option available today is probably a dedicated eCAP
service that refuses to adapt the message bit continues to receive (and
log) the message body.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210304/239f4a34/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar  4 21:53:23 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Mar 2021 16:53:23 -0500
Subject: [squid-users] Squid full request logging
In-Reply-To: <384D8BCA-4AD7-4C47-9EB6-470FB82398B7@ironpeak.be>
References: <FB702A6F-6FDB-4D1E-AF04-149F6ED3D42F@ironpeak.be>
 <7447d4bc-e8f5-d51b-f91c-e15d7921b285@measurement-factory.com>
 <384D8BCA-4AD7-4C47-9EB6-470FB82398B7@ironpeak.be>
Message-ID: <cd937d3c-f1fa-38a1-49bc-7923900b209e@measurement-factory.com>

On 3/4/21 4:33 PM, Niels Hofmans wrote:

> Although I am not proficient in C for writing
> an ecap service, is there some binding available online for Go?

eCAP is C++ and, unfortunately, the first version of its API evidently
requires elevated programming skills. I am not aware of Go bindings, and
I would not recommend eCAP for those who are not fluent in C++ (or
cannot hire somebody fluent in C++).


> This was the reason I originally opted for an ICAP service since I can
> abstract Go behind the HTTP ICAP layer.

I do not know what "abstract Go behind the HTTP ICAP layer" means, but I
hope that you are not writing an ICAP service from scratch. If you do,
be prepared to battle many problems, both short- and long-term. ICAP
looks simple but it is difficult to get right.


> Now I understand this has its limitations, but AFAIK a preview cap at
> 100kb would be sufficient per request.

I am not sure Squid supports 100KB previews -- the internal buffers may
not be big enough for that. 64KB might be the limit, but do not quote me
on that.

BTW, please note that, AFAICT, Squid ignores icap_preview_size in
squid.conf (a bug) -- your ICAP service has to request the desired
maximum preview size via an ICAP OPTIONS response.


> Would you think that ... using two ICAP services.
> One would validate the headers and return OK or NOT (bypass=0), while
> the other only pushes the 1kb request/response to a queue.
> Ideally those two would be contacted simultaneously while only the first
> one is blocking.
> ..just thinking aloud tough.

Sorry, I cannot evaluate this design because I do not know what you want
to optimize and what your logging requirements/limitations are.


Good luck,

Alex.


> On 4 Mar 2021, at 22:23, Alex Rousskov wrote:
> 
> On 3/4/21 2:52 PM, Niels Hofmans wrote:
> 
>> is it possible to do full request/response logging?
> 
> Squid can log HTTP headers with %>h and %<h logformat codes.
> 
> Squid cannot log HTTP message bodies.
> 
> 
>> I do not see the appropriate log_format directive in the docs.
>> I was hoping not having to do this in my ICAP service since this slows
>> down approval of the HTTP request. (Empty preview v.s. a request capped
>> at 1MB that needs to be sent over every time)
> 
> FWIW, an ICAP or eCAP service can start responding to the request
> _before_ the service receives the entire HTTP message body. To get
> things going, all the service needs is HTTP headers (and even that is,
> technically, optional in some cases).
> 
> Using an adaptation service is still an overhead, of course, but, very
> few legitimate Squid use cases involve logging message bodies, so there
> is no built-in mechanism optimized for that specific rare purpose
> (yet?). The fastest option available today is probably a dedicated eCAP
> service that refuses to adapt the message bit continues to receive (and
> log) the message body.
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ngtech1ltd at gmail.com  Thu Mar  4 22:05:10 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Fri, 5 Mar 2021 00:05:10 +0200
Subject: [squid-users] Squid full request logging
In-Reply-To: <384D8BCA-4AD7-4C47-9EB6-470FB82398B7@ironpeak.be>
References: <FB702A6F-6FDB-4D1E-AF04-149F6ED3D42F@ironpeak.be>
 <7447d4bc-e8f5-d51b-f91c-e15d7921b285@measurement-factory.com>
 <384D8BCA-4AD7-4C47-9EB6-470FB82398B7@ironpeak.be>
Message-ID: <CABA8h=S9QrmFhfA7Tj7zy9HyPxYUzDyLWGABSHiwRkiLzgs_eg@mail.gmail.com>

Hey Niels,

Take a peek at:
https://github.com/andybalholm/redwood

I am using it in production and it was written because of squid limitations.
squid is great but take a peek and see how it works for you.
I have 2 servers in ha cluster which works great.

An example I wrote to filter youtube traffic is at:
https://github.com/elico/yt-classification-service-example

let me know if it helps you or gives you any direction.

?????? ??? ??, 4 ???? 2021, 23:33, ??? Niels Hofmans ?<hello at ironpeak.be>:

> Hi Alex,
>
> Thanks for the feedback. Although I am not proficient in C for writing an
> ecap service, is there some binding available online for Go?
> This was the reason I originally opted for an ICAP service since I can
> abstract Go behind the HTTP ICAP layer.
> Now I understand this has its limitations, but AFAIK a preview cap at
> 100kb would be sufficient per request.
> But this will slow down my current setup greatly, as I?m currently sending
> -only- the headers.
>
> Would you think that a) using Go for the ecap adapter or b) using two ICAP
> services.
> One would validate the headers and return OK or NOT (bypass=0), while the
> other only pushes the 1kb request/response to a queue.
> Ideally those two would be contacted simultaneously while only the first
> one is blocking.
> ..just thinking aloud tough.
>
> Regards,
> Niels Hofmans
>
> SITE   https://ironpeak.be
> BTW   BE0694785660
> BANK BE76068909740795
>
> On 4 Mar 2021, at 22:23, Alex Rousskov <rousskov at measurement-factory.com>
> wrote:
>
> On 3/4/21 2:52 PM, Niels Hofmans wrote:
>
> is it possible to do full request/response logging?
>
>
> Squid can log HTTP headers with %>h and %<h logformat codes.
>
> Squid cannot log HTTP message bodies.
>
>
> I do not see the appropriate log_format directive in the docs.
> I was hoping not having to do this in my ICAP service since this slows
> down approval of the HTTP request. (Empty preview v.s. a request capped
> at 1MB that needs to be sent over every time)
>
>
> FWIW, an ICAP or eCAP service can start responding to the request
> _before_ the service receives the entire HTTP message body. To get
> things going, all the service needs is HTTP headers (and even that is,
> technically, optional in some cases).
>
> Using an adaptation service is still an overhead, of course, but, very
> few legitimate Squid use cases involve logging message bodies, so there
> is no built-in mechanism optimized for that specific rare purpose
> (yet?). The fastest option available today is probably a dedicated eCAP
> service that refuses to adapt the message bit continues to receive (and
> log) the message body.
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210305/a26f31c4/attachment.htm>

From hello at ironpeak.be  Fri Mar  5 07:55:54 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Fri, 5 Mar 2021 08:55:54 +0100
Subject: [squid-users] Icap preview size
Message-ID: <E3C5CAF0-158A-4292-A2CE-6F39BC467688@ironpeak.be>

Hi guys,

One more: I believe ICAP is not respecting the Preview header for REQMOD nor RESPMOD.

For the REQMOD OPTIONS requests, I respond with:

ICAP/1.0 200 OK
Allow: 200,204
Connection: close
Date: Fri, 05 Mar 2021 07:34:56 GMT
Encapsulated: null-body=0
Methods: REQMOD
Preview: 64000

However in my ICAP service, I often receive a request body over 64000 bytes, e.g. 1000000 request bytes.

I also noticed that a "Preview: 0? will not have any effect and the ICAP service will still receive a complete HTTP REQMOD+REQRESP body.
Any idea if I?m doing something wrong here?

icap_enable on
icap_connect_timeout 1 second
icap_service_failure_limit -1
icap_send_client_ip    on
icap_preview_size 0
icap_persistent_connections on
icap_service service_req reqmod_precache bypass=0 icap://172.17.0.2:1344/request
icap_service service_res respmod_precache bypass=0 icap://172.17.0.2:1344/response
icap_preview_enable on
adaptation_access service_req allow all
adaptation_access service_res allow all

Man thanks.

Regards,
Best regards
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210305/86b5d58c/attachment.htm>

From rousskov at measurement-factory.com  Fri Mar  5 16:21:01 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Mar 2021 11:21:01 -0500
Subject: [squid-users] Icap preview size
In-Reply-To: <E3C5CAF0-158A-4292-A2CE-6F39BC467688@ironpeak.be>
References: <E3C5CAF0-158A-4292-A2CE-6F39BC467688@ironpeak.be>
Message-ID: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>

On 3/5/21 2:55 AM, Niels Hofmans wrote:

> One more: I believe ICAP is not respecting the Preview header for REQMOD
> nor RESPMOD.

> For the REQMOD OPTIONS requests, I respond with:
> 
> ICAP/1.0 200 OK
> Allow: 200,204
> Connection: close
> Date: Fri, 05 Mar 2021 07:34:56 GMT
> Encapsulated: null-body=0
> Methods: REQMOD
> Preview: 64000
> 
> However in my ICAP service, I often receive a request body over 64000
> bytes, e.g.?1000000 request bytes.

Receive when? During preview, instead of Preview, and/or after Preview?


> I also noticed that a "Preview: 0? will not have any effect and the ICAP
> service will still receive a complete HTTP REQMOD+REQRESP body.

Receive when? During preview, instead of Preview, and/or after Preview?

Alex.



> icap_enable on
> icap_connect_timeout 1 second
> icap_service_failure_limit -1
> icap_send_client_ip    on
> icap_preview_size 0
> icap_persistent_connections on
> icap_service service_req reqmod_precache bypass=0 icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>
> icap_service service_res respmod_precache bypass=0 icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>
> icap_preview_enable on
> adaptation_access service_req allow all
> adaptation_access service_res allow all




From hello at ironpeak.be  Fri Mar  5 22:21:16 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Fri, 5 Mar 2021 23:21:16 +0100
Subject: [squid-users] Icap preview size
In-Reply-To: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
References: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
Message-ID: <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>

Hi Alex,

I receive that large payload right after an OPTIONS call to my ICAP endpoint. It is the preview.

Regards,
Niels

Sent from my mobile

On 5 Mar 2021, at 17:21, Alex Rousskov <rousskov at measurement-factory.com> wrote:

?On 3/5/21 2:55 AM, Niels Hofmans wrote:

> One more: I believe ICAP is not respecting the Preview header for REQMOD
> nor RESPMOD.

> For the REQMOD OPTIONS requests, I respond with:
> 
> ICAP/1.0 200 OK
> Allow: 200,204
> Connection: close
> Date: Fri, 05 Mar 2021 07:34:56 GMT
> Encapsulated: null-body=0
> Methods: REQMOD
> Preview: 64000
> 
> However in my ICAP service, I often receive a request body over 64000
> bytes, e.g. 1000000 request bytes.

Receive when? During preview, instead of Preview, and/or after Preview?


> I also noticed that a "Preview: 0? will not have any effect and the ICAP
> service will still receive a complete HTTP REQMOD+REQRESP body.

Receive when? During preview, instead of Preview, and/or after Preview?

Alex.



> icap_enable on
> icap_connect_timeout 1 second
> icap_service_failure_limit -1
> icap_send_client_ip    on
> icap_preview_size 0
> icap_persistent_connections on
> icap_service service_req reqmod_precache bypass=0 icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>
> icap_service service_res respmod_precache bypass=0 icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>
> icap_preview_enable on
> adaptation_access service_req allow all
> adaptation_access service_res allow all




From rousskov at measurement-factory.com  Fri Mar  5 22:32:22 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Mar 2021 17:32:22 -0500
Subject: [squid-users] Icap preview size
In-Reply-To: <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>
References: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
 <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>
Message-ID: <67cb3f02-6766-0569-76ce-73ad41ea4483@measurement-factory.com>

On 3/5/21 5:21 PM, Niels Hofmans wrote:

> I receive that large payload right after an OPTIONS call to my ICAP endpoint. It is the preview.

The timing of an ICAP request does not determine whether that request
has a Preview stage and whether the "unexpected" body bytes were sent
during that Preview stage. I am asking these questions because I want to
determine whether Squid increases Preview size beyond what your server
has requested or does not send Preview at all.

Please share the ICAP headers of the problematic REQMOD request and the
last chunk metadata of that request body. You can get the latter from a
packet capture if your ICAP server does not report it in a convenient
form. In fact, sharing (a pointer to) the packet capture of the whole
problematic ICAP request is probably a good idea!

Alex.



> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
> 
> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
> 
>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>> nor RESPMOD.
> 
>> For the REQMOD OPTIONS requests, I respond with:
>>
>> ICAP/1.0 200 OK
>> Allow: 200,204
>> Connection: close
>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>> Encapsulated: null-body=0
>> Methods: REQMOD
>> Preview: 64000
>>
>> However in my ICAP service, I often receive a request body over 64000
>> bytes, e.g. 1000000 request bytes.
> 
> Receive when? During preview, instead of Preview, and/or after Preview?
> 
> 
>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>> service will still receive a complete HTTP REQMOD+REQRESP body.
> 
> Receive when? During preview, instead of Preview, and/or after Preview?
> 
> Alex.
> 
> 
> 
>> icap_enable on
>> icap_connect_timeout 1 second
>> icap_service_failure_limit -1
>> icap_send_client_ip    on
>> icap_preview_size 0
>> icap_persistent_connections on
>> icap_service service_req reqmod_precache bypass=0 icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>
>> icap_service service_res respmod_precache bypass=0 icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>
>> icap_preview_enable on
>> adaptation_access service_req allow all
>> adaptation_access service_res allow all
> 



From hello at ironpeak.be  Sat Mar  6 10:09:00 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Sat, 6 Mar 2021 11:09:00 +0100
Subject: [squid-users] Icap preview size
In-Reply-To: <67cb3f02-6766-0569-76ce-73ad41ea4483@measurement-factory.com>
References: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
 <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>
 <67cb3f02-6766-0569-76ce-73ad41ea4483@measurement-factory.com>
Message-ID: <B35B53AF-7FC7-471B-AB2D-4E8207887425@ironpeak.be>

Hi Alex,

I?ve uploaded a tcpdump sample of where I?m requesting an image through squid + icap and where you can see the image response being posted fully to the ICAP service.
Headers passed on are Preview: 0 for RESPMOD and Transfer-Preview: *. (I only need the headers for the response)

PCAP: https://www.mediafire.com/file/k7zs51lg9s5ka3h/icap.pcap/file <https://www.mediafire.com/file/k7zs51lg9s5ka3h/icap.pcap/file>

Many thanks for co-debugging. :-)
Enjoy your weekend!

Regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 5 Mar 2021, at 23:32, Alex Rousskov <rousskov at measurement-factory.com> wrote:

On 3/5/21 5:21 PM, Niels Hofmans wrote:

> I receive that large payload right after an OPTIONS call to my ICAP endpoint. It is the preview.

The timing of an ICAP request does not determine whether that request
has a Preview stage and whether the "unexpected" body bytes were sent
during that Preview stage. I am asking these questions because I want to
determine whether Squid increases Preview size beyond what your server
has requested or does not send Preview at all.

Please share the ICAP headers of the problematic REQMOD request and the
last chunk metadata of that request body. You can get the latter from a
packet capture if your ICAP server does not report it in a convenient
form. In fact, sharing (a pointer to) the packet capture of the whole
problematic ICAP request is probably a good idea!

Alex.



> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
> 
> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
> 
>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>> nor RESPMOD.
> 
>> For the REQMOD OPTIONS requests, I respond with:
>> 
>> ICAP/1.0 200 OK
>> Allow: 200,204
>> Connection: close
>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>> Encapsulated: null-body=0
>> Methods: REQMOD
>> Preview: 64000
>> 
>> However in my ICAP service, I often receive a request body over 64000
>> bytes, e.g. 1000000 request bytes.
> 
> Receive when? During preview, instead of Preview, and/or after Preview?
> 
> 
>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>> service will still receive a complete HTTP REQMOD+REQRESP body.
> 
> Receive when? During preview, instead of Preview, and/or after Preview?
> 
> Alex.
> 
> 
> 
>> icap_enable on
>> icap_connect_timeout 1 second
>> icap_service_failure_limit -1
>> icap_send_client_ip    on
>> icap_preview_size 0
>> icap_persistent_connections on
>> icap_service service_req reqmod_precache bypass=0 icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>
>> icap_service service_res respmod_precache bypass=0 icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>
>> icap_preview_enable on
>> adaptation_access service_req allow all
>> adaptation_access service_res allow all
> 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210306/3c461bb1/attachment.htm>

From hello at ironpeak.be  Sat Mar  6 10:10:13 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Sat, 6 Mar 2021 11:10:13 +0100
Subject: [squid-users] squid ssl-bump with icap returns 503
In-Reply-To: <25e245a4-d886-ba4a-59af-3d22483749a8@treenet.co.nz>
References: <4B7C506E-BBCF-47FC-A6C9-497D1CA32FF4@ironpeak.be>
 <1eebb7c9-f59d-21c1-8605-a544eda18bb3@treenet.co.nz>
 <DF7095B6-8626-41EC-AC0D-7839DB6AF5C2@ironpeak.be>
 <25e245a4-d886-ba4a-59af-3d22483749a8@treenet.co.nz>
Message-ID: <CA6F9292-ED74-47A2-A2D2-7F0E9BEE37E9@ironpeak.be>

Hi Amos,

Just to get back to this, the conclusion is that https_port does not support ssl-bump of requests passing through CONNECT.
I?ll terminate the TLS connection in front of squid through a load balancer and use http_port, which works fine.
Thank you!

Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 4 Mar 2021, at 14:21, Amos Jeffries <squid3 at treenet.co.nz> wrote:

On 5/03/21 1:39 am, Niels Hofmans wrote:
> Hi Amos,
> Thank you for getting back to me.
> So if ssl-bump is required on the http(s)_port directive, I end up at:

https_port simply means TLS is the transport protocol. The transport is terminated at the proxy. There are many permutations of what is being done inside that TLS.

So no http_port does not require "ssl-bump".

Squid does not support TLS-inside-TLS encryption layering. Which is why "ssl-bump" only works for "intercept" or "tproxy" modes on that port directive.




> http_port 0.0.0.0:3128
> https_port 0.0.0.0:3129 ssl-bump intercept \
>     generate-host-certificates=ondynamic_cert_mem_cache_size=10MB \
>     cert=/etc/squid/ssl/squid.crtkey=/etc/squid/ssl/squid.key \
>     tls-cert=/etc/squid/ssl/squid.crttls-key=/etc/squid/ssl/squid.key
> always_direct allow all
> ssl_bump bump all
> This however ends up with following logs:
> 2021/03/04 12:37:43 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=172.17.0.2:3129 remote=172.17.0.1:55508 FD 13 flags=33: (2) No such file or directory

Which means your NAT systems have no records of port 443 being diverted to port 3129.


> Command to reproduce:
>  % ALL_PROXY="http://127.0.0.1:3129 <http://127.0.0.1:3129/> <http://127.0.0.1:3129 <http://127.0.0.1:3129/>>" curl -k -vvv --proxy-insecure -X POST --data 'foo' https://ironpeak.be/ <https://ironpeak.be/> <https://ironpeak.be/ <https://ironpeak.be/>>

Correct test command for "https_port 3129 intercept ssl-bump" is:

 curl --cacert /etc/squid/ssl/squid.crt -vvv \
      -X POST --data 'foo' https://ironpeak.be/ <https://ironpeak.be/>

This verifies that the system is diverting traffic to the proxy, the proxy is the TLS agent for those connections, and that connectivity through the proxy is working.
TLS failure to verify the cert(s) indicate the proxy is not being reached.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210306/55606f21/attachment.htm>

From rousskov at measurement-factory.com  Sat Mar  6 18:29:45 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 6 Mar 2021 13:29:45 -0500
Subject: [squid-users] Icap preview size
In-Reply-To: <B35B53AF-7FC7-471B-AB2D-4E8207887425@ironpeak.be>
References: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
 <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>
 <67cb3f02-6766-0569-76ce-73ad41ea4483@measurement-factory.com>
 <B35B53AF-7FC7-471B-AB2D-4E8207887425@ironpeak.be>
Message-ID: <05e89d37-e09f-749b-242c-8b17a1ee0f6a@measurement-factory.com>

On 3/6/21 5:09 AM, Niels Hofmans wrote:

> I?ve uploaded a tcpdump sample of where I?m requesting an image through
> squid + icap and where you can see the image response being posted fully
> to the ICAP service.

AFAICT, all ICAP REQMOD and RESPMOD requests in your packet capture send
zero-size Preview containing just the HTTP headers (e.g., see frame
#44). In one case, Squid also sends the HTTP response body but only
because your ICAP service explicitly requests that HTTP response body by
responding to Squid Preview with ICAP 100 Continue control message
(e.g., see frame #48)!

If your ICAP service does not want to see an HTTP body, then it should
not ask for it. It should respond (usually with ICAP 200 or ICAP 204)
based on the Preview alone, without sending ICAP 100 Continue control
message first.


HTH,

Alex.


> On 5 Mar 2021, at 23:32, Alex Rousskov wrote:
> 
> On 3/5/21 5:21 PM, Niels Hofmans wrote:
> 
>> I receive that large payload right after an OPTIONS call to my ICAP
>> endpoint. It is the preview.
> 
> The timing of an ICAP request does not determine whether that request
> has a Preview stage and whether the "unexpected" body bytes were sent
> during that Preview stage. I am asking these questions because I want to
> determine whether Squid increases Preview size beyond what your server
> has requested or does not send Preview at all.
> 
> Please share the ICAP headers of the problematic REQMOD request and the
> last chunk metadata of that request body. You can get the latter from a
> packet capture if your ICAP server does not report it in a convenient
> form. In fact, sharing (a pointer to) the packet capture of the whole
> problematic ICAP request is probably a good idea!
> 
> Alex.
> 
> 
> 
>> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
>>
>> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
>>
>>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>>> nor RESPMOD.
>>
>>> For the REQMOD OPTIONS requests, I respond with:
>>>
>>> ICAP/1.0 200 OK
>>> Allow: 200,204
>>> Connection: close
>>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>>> Encapsulated: null-body=0
>>> Methods: REQMOD
>>> Preview: 64000
>>>
>>> However in my ICAP service, I often receive a request body over 64000
>>> bytes, e.g. 1000000 request bytes.
>>
>> Receive when? During preview, instead of Preview, and/or after Preview?
>>
>>
>>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>>> service will still receive a complete HTTP REQMOD+REQRESP body.
>>
>> Receive when? During preview, instead of Preview, and/or after Preview?
>>
>> Alex.
>>
>>
>>
>>> icap_enable on
>>> icap_connect_timeout 1 second
>>> icap_service_failure_limit -1
>>> icap_send_client_ip ???on
>>> icap_preview_size 0
>>> icap_persistent_connections on
>>> icap_service service_req reqmod_precache bypass=0
>>> icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>
>>> <icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>>
>>> icap_service service_res respmod_precache bypass=0
>>> icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>
>>> <icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>>
>>> icap_preview_enable on
>>> adaptation_access service_req allow all
>>> adaptation_access service_res allow all
>>
> 
> 



From hello at ironpeak.be  Sat Mar  6 20:35:21 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Sat, 6 Mar 2021 21:35:21 +0100
Subject: [squid-users] Icap preview size
In-Reply-To: <4A211966-BBBF-4DE2-A1E9-BCCB64DFFCAD@ironpeak.be>
References: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
 <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>
 <67cb3f02-6766-0569-76ce-73ad41ea4483@measurement-factory.com>
 <B35B53AF-7FC7-471B-AB2D-4E8207887425@ironpeak.be>
 <05e89d37-e09f-749b-242c-8b17a1ee0f6a@measurement-factory.com>
 <4A211966-BBBF-4DE2-A1E9-BCCB64DFFCAD@ironpeak.be>
Message-ID: <E820AE00-C78F-4F9D-98C5-1AB7C3C5813A@ironpeak.be>



On 6 Mar 2021, at 21:33, Niels Hofmans <hello at ironpeak.be> wrote:

Hi Alex,

I fixed a bug in the go-icap/icap library, seen here:
But this made me wonder about the following: https://i.imgur.com/bA3w8YT.png <https://i.imgur.com/bA3w8YT.png>

Here you can see squid issuing a RESPMOD to my ICAP server with a Preview of ?0?, but it does contain a HTTP response.
And since my ICAP service will not return a HTTP response since the Preview size is 0, my http client receives nothing from squid.
I checked but I don?t think I see a 100 continuation from ICAP in the dump anymore.

Tcpdump: https://www.mediafire.com/file/sr7hvk9ezepoc8d/icap.pcap3/file <https://www.mediafire.com/file/sr7hvk9ezepoc8d/icap.pcap3/file>
(I?ve revoked my session cookies for this dump)

One step at a time closer to the truth! :-)

Regards,
Niels Hofmans

SITE   https://ironpeak.be <https://ironpeak.be/>
BTW   BE0694785660
BANK BE76068909740795

On 6 Mar 2021, at 19:29, Alex Rousskov <rousskov at measurement-factory.com <mailto:rousskov at measurement-factory.com>> wrote:

On 3/6/21 5:09 AM, Niels Hofmans wrote:

> I?ve uploaded a tcpdump sample of where I?m requesting an image through
> squid + icap and where you can see the image response being posted fully
> to the ICAP service.

AFAICT, all ICAP REQMOD and RESPMOD requests in your packet capture send
zero-size Preview containing just the HTTP headers (e.g., see frame
#44). In one case, Squid also sends the HTTP response body but only
because your ICAP service explicitly requests that HTTP response body by
responding to Squid Preview with ICAP 100 Continue control message
(e.g., see frame #48)!

If your ICAP service does not want to see an HTTP body, then it should
not ask for it. It should respond (usually with ICAP 200 or ICAP 204)
based on the Preview alone, without sending ICAP 100 Continue control
message first.


HTH,

Alex.


> On 5 Mar 2021, at 23:32, Alex Rousskov wrote:
> 
> On 3/5/21 5:21 PM, Niels Hofmans wrote:
> 
>> I receive that large payload right after an OPTIONS call to my ICAP
>> endpoint. It is the preview.
> 
> The timing of an ICAP request does not determine whether that request
> has a Preview stage and whether the "unexpected" body bytes were sent
> during that Preview stage. I am asking these questions because I want to
> determine whether Squid increases Preview size beyond what your server
> has requested or does not send Preview at all.
> 
> Please share the ICAP headers of the problematic REQMOD request and the
> last chunk metadata of that request body. You can get the latter from a
> packet capture if your ICAP server does not report it in a convenient
> form. In fact, sharing (a pointer to) the packet capture of the whole
> problematic ICAP request is probably a good idea!
> 
> Alex.
> 
> 
> 
>> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
>> 
>> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
>> 
>>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>>> nor RESPMOD.
>> 
>>> For the REQMOD OPTIONS requests, I respond with:
>>> 
>>> ICAP/1.0 200 OK
>>> Allow: 200,204
>>> Connection: close
>>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>>> Encapsulated: null-body=0
>>> Methods: REQMOD
>>> Preview: 64000
>>> 
>>> However in my ICAP service, I often receive a request body over 64000
>>> bytes, e.g. 1000000 request bytes.
>> 
>> Receive when? During preview, instead of Preview, and/or after Preview?
>> 
>> 
>>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>>> service will still receive a complete HTTP REQMOD+REQRESP body.
>> 
>> Receive when? During preview, instead of Preview, and/or after Preview?
>> 
>> Alex.
>> 
>> 
>> 
>>> icap_enable on
>>> icap_connect_timeout 1 second
>>> icap_service_failure_limit -1
>>> icap_send_client_ip    on
>>> icap_preview_size 0
>>> icap_persistent_connections on
>>> icap_service service_req reqmod_precache bypass=0
>>> icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request> <icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>>
>>> <icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request> <icap://172.17.0.2:1344/request <icap://172.17.0.2:1344/request>>>
>>> icap_service service_res respmod_precache bypass=0
>>> icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response> <icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>>
>>> <icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response> <icap://172.17.0.2:1344/response <icap://172.17.0.2:1344/response>>>
>>> icap_preview_enable on
>>> adaptation_access service_req allow all
>>> adaptation_access service_res allow all

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210306/474387b3/attachment.htm>

From rousskov at measurement-factory.com  Sat Mar  6 22:22:18 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 6 Mar 2021 17:22:18 -0500
Subject: [squid-users] Icap preview size
In-Reply-To: <4A211966-BBBF-4DE2-A1E9-BCCB64DFFCAD@ironpeak.be>
References: <2ce4d639-4e7a-7d30-1bf0-5a1600b0abe7@measurement-factory.com>
 <32DF2BEA-7E00-409D-8643-678A976AC19B@ironpeak.be>
 <67cb3f02-6766-0569-76ce-73ad41ea4483@measurement-factory.com>
 <B35B53AF-7FC7-471B-AB2D-4E8207887425@ironpeak.be>
 <05e89d37-e09f-749b-242c-8b17a1ee0f6a@measurement-factory.com>
 <4A211966-BBBF-4DE2-A1E9-BCCB64DFFCAD@ironpeak.be>
Message-ID: <a2c2db6b-0206-7d63-2f80-b673086eefe6@measurement-factory.com>

On 3/6/21 3:33 PM, Niels Hofmans wrote:

> I fixed a bug in the go-icap/icap library, seen here:

FYI: Whatever you were trying to share in this paragraph, did not come
through the mailing list. However, we should not be discussing some ICAP
library bugs on this mailing list, so perhaps it is not important.


> But this made me wonder about the following: <PastedGraphic-1.tiff>

In the future, please paste message _text_ and/or provide frame/stream
references to pieces of shared captures. Working with images is very
inefficient when it comes to protocol analysis.


> Here you can see squid issuing a RESPMOD to my ICAP server with a
> Preview of ?0?,

Yes.


> but it does contain a HTTP response.

It contains an HTTP request header (shown in the PastedGraphic-1.tiff
image you have attached), probably an HTTP response header (not shown),
and probably no HTTP response body. The HTTP response does have a body,
but the ICAP client does not send it during the Preview:0 stage. This is
how ICAP Preview works. I see nothing wrong here.


> And since my ICAP service will not return a HTTP response since the
> Preview size is 0, my http client receives nothing from squid.

AFAICT, your ICAP service is butchering HTTP responses by responding
with ICAP 200 responses that encapsulate HTTP messages without bodies. A
standard ICAP client will _not_ re-assemble the HTTP message by
combining your header with client-cached body parts. The ICAP standard
just does not work that way. When the ICAP service sends an ICAP 200
response, the service must return the _entire_ HTTP message, headers and
body.

As a consequence, an ICAP service cannot use standard ICAP to adapt HTTP
headers without receiving and returning the HTTP body as well. This
applies to REQMOD and RESPMOD transactions. You are just lucky that your
test REQMOD transactions have requests without bodies.


> I checked but I don?t think I see a 100 continuation from ICAP in the
> dump anymore.

Yes, that is correct. However, the new trace shows another ICAP service
problem as detailed above.


HTH,

Alex.


> On 6 Mar 2021, at 19:29, Alex Rousskov wrote:
> 
> On 3/6/21 5:09 AM, Niels Hofmans wrote:
> 
>> I?ve uploaded a tcpdump sample of where I?m requesting an image through
>> squid + icap and where you can see the image response being posted fully
>> to the ICAP service.
> 
> AFAICT, all ICAP REQMOD and RESPMOD requests in your packet capture send
> zero-size Preview containing just the HTTP headers (e.g., see frame
> #44). In one case, Squid also sends the HTTP response body but only
> because your ICAP service explicitly requests that HTTP response body by
> responding to Squid Preview with ICAP 100 Continue control message
> (e.g., see frame #48)!
> 
> If your ICAP service does not want to see an HTTP body, then it should
> not ask for it. It should respond (usually with ICAP 200 or ICAP 204)
> based on the Preview alone, without sending ICAP 100 Continue control
> message first.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> On 5 Mar 2021, at 23:32, Alex Rousskov wrote:
>>
>> On 3/5/21 5:21 PM, Niels Hofmans wrote:
>>
>>> I receive that large payload right after an OPTIONS call to my ICAP
>>> endpoint. It is the preview.
>>
>> The timing of an ICAP request does not determine whether that request
>> has a Preview stage and whether the "unexpected" body bytes were sent
>> during that Preview stage. I am asking these questions because I want to
>> determine whether Squid increases Preview size beyond what your server
>> has requested or does not send Preview at all.
>>
>> Please share the ICAP headers of the problematic REQMOD request and the
>> last chunk metadata of that request body. You can get the latter from a
>> packet capture if your ICAP server does not report it in a convenient
>> form. In fact, sharing (a pointer to) the packet capture of the whole
>> problematic ICAP request is probably a good idea!
>>
>> Alex.
>>
>>
>>
>>> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
>>>
>>> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
>>>
>>>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>>>> nor RESPMOD.
>>>
>>>> For the REQMOD OPTIONS requests, I respond with:
>>>>
>>>> ICAP/1.0 200 OK
>>>> Allow: 200,204
>>>> Connection: close
>>>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>>>> Encapsulated: null-body=0
>>>> Methods: REQMOD
>>>> Preview: 64000
>>>>
>>>> However in my ICAP service, I often receive a request body over 64000
>>>> bytes, e.g. 1000000 request bytes.
>>>
>>> Receive when? During preview, instead of Preview, and/or after Preview?
>>>
>>>
>>>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>>>> service will still receive a complete HTTP REQMOD+REQRESP body.
>>>
>>> Receive when? During preview, instead of Preview, and/or after Preview?
>>>
>>> Alex.
>>>
>>>
>>>
>>>> icap_enable on
>>>> icap_connect_timeout 1 second
>>>> icap_service_failure_limit -1
>>>> icap_send_client_ip ???on
>>>> icap_preview_size 0
>>>> icap_persistent_connections on
>>>> icap_service service_req reqmod_precache bypass=0
>>>> icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request>?<icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request>>
>>>> <icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request>?<icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request>>>
>>>> icap_service service_res respmod_precache bypass=0
>>>> icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response>?<icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response>>
>>>> <icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response>?<icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response>>>
>>>> icap_preview_enable on
>>>> adaptation_access service_req allow all
>>>> adaptation_access service_res allow all
> 



From hello at ironpeak.be  Sat Mar  6 22:38:08 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Sat, 6 Mar 2021 23:38:08 +0100
Subject: [squid-users] Icap preview size
In-Reply-To: <a2c2db6b-0206-7d63-2f80-b673086eefe6@measurement-factory.com>
References: <a2c2db6b-0206-7d63-2f80-b673086eefe6@measurement-factory.com>
Message-ID: <3231DC09-5F61-4F14-9361-13659648CF34@ironpeak.be>

Hi Alex,

Missed that in the RFC, I was hoping I could only send the headers across and modify those, without having to send over everything, including very large responses for performance reasons.
Thanks for following along.

Regards,
Niels

Sent from my mobile

On 6 Mar 2021, at 23:22, Alex Rousskov <rousskov at measurement-factory.com> wrote:

?On 3/6/21 3:33 PM, Niels Hofmans wrote:

> I fixed a bug in the go-icap/icap library, seen here:

FYI: Whatever you were trying to share in this paragraph, did not come
through the mailing list. However, we should not be discussing some ICAP
library bugs on this mailing list, so perhaps it is not important.


> But this made me wonder about the following: <PastedGraphic-1.tiff>

In the future, please paste message _text_ and/or provide frame/stream
references to pieces of shared captures. Working with images is very
inefficient when it comes to protocol analysis.


> Here you can see squid issuing a RESPMOD to my ICAP server with a
> Preview of ?0?,

Yes.


> but it does contain a HTTP response.

It contains an HTTP request header (shown in the PastedGraphic-1.tiff
image you have attached), probably an HTTP response header (not shown),
and probably no HTTP response body. The HTTP response does have a body,
but the ICAP client does not send it during the Preview:0 stage. This is
how ICAP Preview works. I see nothing wrong here.


> And since my ICAP service will not return a HTTP response since the
> Preview size is 0, my http client receives nothing from squid.

AFAICT, your ICAP service is butchering HTTP responses by responding
with ICAP 200 responses that encapsulate HTTP messages without bodies. A
standard ICAP client will _not_ re-assemble the HTTP message by
combining your header with client-cached body parts. The ICAP standard
just does not work that way. When the ICAP service sends an ICAP 200
response, the service must return the _entire_ HTTP message, headers and
body.

As a consequence, an ICAP service cannot use standard ICAP to adapt HTTP
headers without receiving and returning the HTTP body as well. This
applies to REQMOD and RESPMOD transactions. You are just lucky that your
test REQMOD transactions have requests without bodies.


> I checked but I don?t think I see a 100 continuation from ICAP in the
> dump anymore.

Yes, that is correct. However, the new trace shows another ICAP service
problem as detailed above.


HTH,

Alex.


> On 6 Mar 2021, at 19:29, Alex Rousskov wrote:
> 
> On 3/6/21 5:09 AM, Niels Hofmans wrote:
> 
>> I?ve uploaded a tcpdump sample of where I?m requesting an image through
>> squid + icap and where you can see the image response being posted fully
>> to the ICAP service.
> 
> AFAICT, all ICAP REQMOD and RESPMOD requests in your packet capture send
> zero-size Preview containing just the HTTP headers (e.g., see frame
> #44). In one case, Squid also sends the HTTP response body but only
> because your ICAP service explicitly requests that HTTP response body by
> responding to Squid Preview with ICAP 100 Continue control message
> (e.g., see frame #48)!
> 
> If your ICAP service does not want to see an HTTP body, then it should
> not ask for it. It should respond (usually with ICAP 200 or ICAP 204)
> based on the Preview alone, without sending ICAP 100 Continue control
> message first.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> On 5 Mar 2021, at 23:32, Alex Rousskov wrote:
>> On 3/5/21 5:21 PM, Niels Hofmans wrote:
>>> I receive that large payload right after an OPTIONS call to my ICAP
>>> endpoint. It is the preview.
>> The timing of an ICAP request does not determine whether that request
>> has a Preview stage and whether the "unexpected" body bytes were sent
>> during that Preview stage. I am asking these questions because I want to
>> determine whether Squid increases Preview size beyond what your server
>> has requested or does not send Preview at all.
>> Please share the ICAP headers of the problematic REQMOD request and the
>> last chunk metadata of that request body. You can get the latter from a
>> packet capture if your ICAP server does not report it in a convenient
>> form. In fact, sharing (a pointer to) the packet capture of the whole
>> problematic ICAP request is probably a good idea!
>> Alex.
>>> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
>>> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
>>>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>>>> nor RESPMOD.
>>>> For the REQMOD OPTIONS requests, I respond with:
>>>> ICAP/1.0 200 OK
>>>> Allow: 200,204
>>>> Connection: close
>>>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>>>> Encapsulated: null-body=0
>>>> Methods: REQMOD
>>>> Preview: 64000
>>>> However in my ICAP service, I often receive a request body over 64000
>>>> bytes, e.g. 1000000 request bytes.
>>> Receive when? During preview, instead of Preview, and/or after Preview?
>>>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>>>> service will still receive a complete HTTP REQMOD+REQRESP body.
>>> Receive when? During preview, instead of Preview, and/or after Preview?
>>> Alex.
>>>> icap_enable on
>>>> icap_connect_timeout 1 second
>>>> icap_service_failure_limit -1
>>>> icap_send_client_ip    on
>>>> icap_preview_size 0
>>>> icap_persistent_connections on
>>>> icap_service service_req reqmod_precache bypass=0
>>>> icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request> <icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request>>
>>>> <icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request> <icap://172.17.0.2:1344/request
>>>> <icap://172.17.0.2:1344/request>>>
>>>> icap_service service_res respmod_precache bypass=0
>>>> icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response> <icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response>>
>>>> <icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response> <icap://172.17.0.2:1344/response
>>>> <icap://172.17.0.2:1344/response>>>
>>>> icap_preview_enable on
>>>> adaptation_access service_req allow all
>>>> adaptation_access service_res allow all



From rousskov at measurement-factory.com  Sat Mar  6 23:07:28 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 6 Mar 2021 18:07:28 -0500
Subject: [squid-users] Icap preview size
In-Reply-To: <3231DC09-5F61-4F14-9361-13659648CF34@ironpeak.be>
References: <a2c2db6b-0206-7d63-2f80-b673086eefe6@measurement-factory.com>
 <3231DC09-5F61-4F14-9361-13659648CF34@ironpeak.be>
Message-ID: <19ea5131-c3d1-520b-ad72-96a14768601c@measurement-factory.com>

On 3/6/21 5:38 PM, Niels Hofmans wrote:

> Missed that in the RFC, I was hoping I could only send the headers across and modify those, without having to send over everything, including very large responses for performance reasons.

There are ICAP extensions that allow header-only adaptation, but, IMO,
it is best to get the basics working before adding support for
experimental protocol extensions.

Alex.


> On 6 Mar 2021, at 23:22, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> ?On 3/6/21 3:33 PM, Niels Hofmans wrote:
> 
>> I fixed a bug in the go-icap/icap library, seen here:
> 
> FYI: Whatever you were trying to share in this paragraph, did not come
> through the mailing list. However, we should not be discussing some ICAP
> library bugs on this mailing list, so perhaps it is not important.
> 
> 
>> But this made me wonder about the following: <PastedGraphic-1.tiff>
> 
> In the future, please paste message _text_ and/or provide frame/stream
> references to pieces of shared captures. Working with images is very
> inefficient when it comes to protocol analysis.
> 
> 
>> Here you can see squid issuing a RESPMOD to my ICAP server with a
>> Preview of ?0?,
> 
> Yes.
> 
> 
>> but it does contain a HTTP response.
> 
> It contains an HTTP request header (shown in the PastedGraphic-1.tiff
> image you have attached), probably an HTTP response header (not shown),
> and probably no HTTP response body. The HTTP response does have a body,
> but the ICAP client does not send it during the Preview:0 stage. This is
> how ICAP Preview works. I see nothing wrong here.
> 
> 
>> And since my ICAP service will not return a HTTP response since the
>> Preview size is 0, my http client receives nothing from squid.
> 
> AFAICT, your ICAP service is butchering HTTP responses by responding
> with ICAP 200 responses that encapsulate HTTP messages without bodies. A
> standard ICAP client will _not_ re-assemble the HTTP message by
> combining your header with client-cached body parts. The ICAP standard
> just does not work that way. When the ICAP service sends an ICAP 200
> response, the service must return the _entire_ HTTP message, headers and
> body.
> 
> As a consequence, an ICAP service cannot use standard ICAP to adapt HTTP
> headers without receiving and returning the HTTP body as well. This
> applies to REQMOD and RESPMOD transactions. You are just lucky that your
> test REQMOD transactions have requests without bodies.
> 
> 
>> I checked but I don?t think I see a 100 continuation from ICAP in the
>> dump anymore.
> 
> Yes, that is correct. However, the new trace shows another ICAP service
> problem as detailed above.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> On 6 Mar 2021, at 19:29, Alex Rousskov wrote:
>>
>> On 3/6/21 5:09 AM, Niels Hofmans wrote:
>>
>>> I?ve uploaded a tcpdump sample of where I?m requesting an image through
>>> squid + icap and where you can see the image response being posted fully
>>> to the ICAP service.
>>
>> AFAICT, all ICAP REQMOD and RESPMOD requests in your packet capture send
>> zero-size Preview containing just the HTTP headers (e.g., see frame
>> #44). In one case, Squid also sends the HTTP response body but only
>> because your ICAP service explicitly requests that HTTP response body by
>> responding to Squid Preview with ICAP 100 Continue control message
>> (e.g., see frame #48)!
>>
>> If your ICAP service does not want to see an HTTP body, then it should
>> not ask for it. It should respond (usually with ICAP 200 or ICAP 204)
>> based on the Preview alone, without sending ICAP 100 Continue control
>> message first.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> On 5 Mar 2021, at 23:32, Alex Rousskov wrote:
>>> On 3/5/21 5:21 PM, Niels Hofmans wrote:
>>>> I receive that large payload right after an OPTIONS call to my ICAP
>>>> endpoint. It is the preview.
>>> The timing of an ICAP request does not determine whether that request
>>> has a Preview stage and whether the "unexpected" body bytes were sent
>>> during that Preview stage. I am asking these questions because I want to
>>> determine whether Squid increases Preview size beyond what your server
>>> has requested or does not send Preview at all.
>>> Please share the ICAP headers of the problematic REQMOD request and the
>>> last chunk metadata of that request body. You can get the latter from a
>>> packet capture if your ICAP server does not report it in a convenient
>>> form. In fact, sharing (a pointer to) the packet capture of the whole
>>> problematic ICAP request is probably a good idea!
>>> Alex.
>>>> On 5 Mar 2021, at 17:21, Alex Rousskov wrote:
>>>> ?On 3/5/21 2:55 AM, Niels Hofmans wrote:
>>>>> One more: I believe ICAP is not respecting the Preview header for REQMOD
>>>>> nor RESPMOD.
>>>>> For the REQMOD OPTIONS requests, I respond with:
>>>>> ICAP/1.0 200 OK
>>>>> Allow: 200,204
>>>>> Connection: close
>>>>> Date: Fri, 05 Mar 2021 07:34:56 GMT
>>>>> Encapsulated: null-body=0
>>>>> Methods: REQMOD
>>>>> Preview: 64000
>>>>> However in my ICAP service, I often receive a request body over 64000
>>>>> bytes, e.g. 1000000 request bytes.
>>>> Receive when? During preview, instead of Preview, and/or after Preview?
>>>>> I also noticed that a "Preview: 0? will not have any effect and the ICAP
>>>>> service will still receive a complete HTTP REQMOD+REQRESP body.
>>>> Receive when? During preview, instead of Preview, and/or after Preview?
>>>> Alex.
>>>>> icap_enable on
>>>>> icap_connect_timeout 1 second
>>>>> icap_service_failure_limit -1
>>>>> icap_send_client_ip    on
>>>>> icap_preview_size 0
>>>>> icap_persistent_connections on
>>>>> icap_service service_req reqmod_precache bypass=0
>>>>> icap://172.17.0.2:1344/request
>>>>> <icap://172.17.0.2:1344/request> <icap://172.17.0.2:1344/request
>>>>> <icap://172.17.0.2:1344/request>>
>>>>> <icap://172.17.0.2:1344/request
>>>>> <icap://172.17.0.2:1344/request> <icap://172.17.0.2:1344/request
>>>>> <icap://172.17.0.2:1344/request>>>
>>>>> icap_service service_res respmod_precache bypass=0
>>>>> icap://172.17.0.2:1344/response
>>>>> <icap://172.17.0.2:1344/response> <icap://172.17.0.2:1344/response
>>>>> <icap://172.17.0.2:1344/response>>
>>>>> <icap://172.17.0.2:1344/response
>>>>> <icap://172.17.0.2:1344/response> <icap://172.17.0.2:1344/response
>>>>> <icap://172.17.0.2:1344/response>>>
>>>>> icap_preview_enable on
>>>>> adaptation_access service_req allow all
>>>>> adaptation_access service_res allow all



From email_arjun at yahoo.com  Mon Mar  8 15:07:43 2021
From: email_arjun at yahoo.com (Arjun K)
Date: Mon, 8 Mar 2021 15:07:43 +0000 (UTC)
Subject: [squid-users] Allow specific set of IP to access a specific set
 of URL
In-Reply-To: <770396db-631d-e095-a60b-36957937dcf9@treenet.co.nz>
References: <1647957146.1399922.1614103747474.ref@mail.yahoo.com>
 <1647957146.1399922.1614103747474@mail.yahoo.com>
 <5333674b494217f5157b01430bba5977af970b36.camel@genua.de>
 <770396db-631d-e095-a60b-36957937dcf9@treenet.co.nz>
Message-ID: <1742754913.639659.1615216063152@mail.yahoo.com>

 Thanks Amos and Klauss for your response.


    On Wednesday, 24 February, 2021, 05:03:57 pm IST, Amos Jeffries <squid3 at treenet.co.nz> wrote:  
 
 On 24/02/21 10:14 pm, Klaus Brandl wrote:
> The acl for the url must be of type url_regex, or something else:
> 
> acl allowedurl url_regex "url.txt"
> 

This line tells Squid to load a file full of regex patterns. Nothing more.


The http_access line is the list of rules that determines when those 
loaded values are tested against a URL, and what happens when the URL 
matches (or not) any of the patterns.


> 
> Am Dienstag, den 23.02.2021, 18:09 +0000 schrieb Arjun K:
>> Hi Team
>>
>> Could you please let me know how to define an acl so that a specific
>> set of IPs can access alone a specific set of URLs.


Have you read the docs on how Squid does access control?
 
<https://wiki.squid-cache.org/SquidFaq/SquidAcl#The_Basics:_How_the_parts_fit_together>

The example config you showed indicates a lack of understanding the 
syntax. That understanding is key to these things actually working the 
way you want.


Lets look at your stated requirements:

 >? define an acl

acl okay_urls url_regex "/etc/squid/url.txt"


 > a specific set of IPs can access alone a specific set of URLs.

That sentence is an access policy. It has three parts:

? 1) "set of IPs"

? I have assumed from the "can access" words that you mean client IPs. 
Which in networking are the TCP src-IP value.

So ...

? ? acl set_of_IPs src 192.0.2.1 192.0.2.45 192.0.2.156

? 2) "set of URLs"

So ...

? ? acl set_of_urls url_regex "/etc/squid/url.txt"


? 3) "can access alone"

I assume that means you want these IPs to access the URLs. But no others 
to be able to access those same URLs.

So ...
? # permit set_of_ips
? http_access allow set_of_ips set_of_urls
? http_access deny set_of_urls



 > Also, I have an existing configuration which should not get affected
 > and it should not interfere in the rules which were allowed for the
 > servers previously.

The most important word there is "previously".

This is where *you* understanding how Squid access controls fit together 
matters *a lot*.

The first http_access line that matches entirely will _end_ processing 
of the 'http_access' sequence. The action (allow/deny) specified on that 
matched line will be done.

So where you place the above http_access lines determine which 
transactions will be able to reach and be tested by them.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210308/fe6dc452/attachment.htm>

From hello at ironpeak.be  Mon Mar  8 15:10:47 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Mon, 8 Mar 2021 16:10:47 +0100
Subject: [squid-users] websocket with sslbump
Message-ID: <5D5420CB-75A3-44F5-8045-266FD9073977@ironpeak.be>

Hi guys,

During testing sslbump + icap I noticed that websockets (ws + was) are not supported by squid. (Even if using on_unsupported_protocol)
Are there any plans for supporting this with sslbump?
Thanks.

Regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210308/e43ff482/attachment.htm>

From roeeklinger60 at gmail.com  Tue Mar  9 11:57:46 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 9 Mar 2021 13:57:46 +0200
Subject: [squid-users] How to completely blacklist a domain + subdomains,
 including HTTPS?
Message-ID: <CAGCa14qAXTGCgiHJ6cO2ShiBx5VyGWNvsBDNe0WphEq9oajFog@mail.gmail.com>

Hey,

I have found a lot of outdated or conflicting information about this
online, and since this is a really important matter, I wanted to make sure
I am doing this correctly.

I am attempting to block some websites completely, including all HTTPS
traffic and subdomains.

Squid.conf:

acl domain_blacklist dstdomain "/etc/squid/domain_blacklist.txt"
http_access deny all domain_blacklist
http_reply_access deny domain_blacklist
http_access deny CONNECT domain_blacklist


/etc/squid/domain_blacklist.txt:

.ph
.somepornwebsite.com
.facebook.com


.ph - will block all websites that have the .ph TLD including subdomains.
.somepornwebsite.com - will block all traffic to somepornwebsite.com,
including all subdomains.
.facebook.com - will block all traffic to FB including subdomains.

Am I doing this the right way?
Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210309/549dfbfb/attachment.htm>

From heimarbeit123.99 at web.de  Tue Mar  9 12:08:06 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Tue, 9 Mar 2021 13:08:06 +0100
Subject: [squid-users] 
 =?utf-8?q?How_to_completely_blacklist_a_domain_+_s?=
 =?utf-8?q?ubdomains=2C_including_HTTPS=3F?=
Message-ID: <trinity-7b83c21c-d45c-4337-9c32-7586a2ae3ea4-1615291686696@msvc-mesg-web006>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210309/c48996b6/attachment.htm>

From squid3 at treenet.co.nz  Tue Mar  9 12:43:25 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Mar 2021 01:43:25 +1300
Subject: [squid-users] How to completely blacklist a domain + subdomains,
 including HTTPS?
In-Reply-To: <CAGCa14qAXTGCgiHJ6cO2ShiBx5VyGWNvsBDNe0WphEq9oajFog@mail.gmail.com>
References: <CAGCa14qAXTGCgiHJ6cO2ShiBx5VyGWNvsBDNe0WphEq9oajFog@mail.gmail.com>
Message-ID: <609e9e89-0ea4-8d59-c735-1b26a1739ae7@treenet.co.nz>

On 10/03/21 12:57 am, roee klinger wrote:
> Hey,
> 
> I have found a lot of outdated or conflicting?information about this 
> online, and since this is a really important matter, I wanted to make 
> sure I am doing this correctly.
> 
> I am attempting to block some websites completely, including?all HTTPS 
> traffic and subdomains.
> 

Basically there are two protocols that need to be considered for this. 
HTTP and TLS.

In HTTP the "website" is identified by a domain name in the 
request-target (aka URI, sometimes called URL).
  * The 'dstdomain' ACL type matches URI domain name.
  * The http_access directive is where that domain name becomes 
available for Squid to check.


In TLS the "website" is identified by the TLS SNI sent by the client, or 
a field in the server X.509 certificate.
  * The 'ssl::server_name' ACL type matches those details.
  * The ssl_bump directive


Next thing is to be aware that there are many ways to layer protocols. 
Do expect to see vastly different proxy behaviours for each permutation 
of those.
  * port 443 "HTTPS" is TLS then HTTP
  * port 80 "HTTPS" is HTTP then TLS (quite rare)
  * forward-proxy "HTTPS" is HTTP then TLS then HTTP



> Squid.conf:
> 
>     acl domain_blacklist dstdomain "/etc/squid/domain_blacklist.txt"
>     http_access deny all domain_blacklist

The "all" here is pointless.


>     http_reply_access deny domain_blacklist

Use of reply access directive for blacklisting by request details is not 
useful.

The request already got blocked. So any response reaching here is just 
the error page saying forbidden. Blocking that error page would just 
change it to a slightly different error page saying the *response* was 
forbidden - which is a bit confusing for any user trying to understand 
why their request didn't work.


>     http_access deny CONNECT domain_blacklist
> 

This line is useless here.

squid.conf lines are interpreted top-down. The "deny all 
domain_blacklist" already stopped all requests that could possibly match 
the second condition of this line.


> 
> /etc/squid/domain_blacklist.txt:
> 
>     .ph
>     .somepornwebsite.com
>     .facebook.com
> 
...
> 
> Am I doing this the?right way?


Sort of. Your http_access denial will catch all the HTTP and decrypted 
HTTP(S) traffic. It will not be able to block any HTTP(S) requests that 
are not able to decrypt.

To catch and block these domains without needing the decrypt you should 
also use:

  acl server_blacklist ssl::server_name "/etc/squid/domain_blacklist.txt"
  ssl_bump terminate server_blacklist

Of course there is always the failure case where traffic cannot decrypt 
and the TLS details use different server names.


Amos


From rousskov at measurement-factory.com  Tue Mar  9 15:58:49 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Mar 2021 10:58:49 -0500
Subject: [squid-users] websocket with sslbump
In-Reply-To: <5D5420CB-75A3-44F5-8045-266FD9073977@ironpeak.be>
References: <5D5420CB-75A3-44F5-8045-266FD9073977@ironpeak.be>
Message-ID: <48960e02-e060-c6de-99b9-bf6fc9fab55d@measurement-factory.com>

On 3/8/21 10:10 AM, Niels Hofmans wrote:

> During testing sslbump + icap I noticed that websockets (ws + was) are
> not supported by squid. (Even if using?on_unsupported_protocol)
> Are there any plans for supporting this with sslbump?

Your question can be misinterpreted in many different ways. I will
answer the following related question instead:

Q: Are there any plans for Squid to send tunneled traffic through
adaptation services?

The ICAP and eCAP protocols cannot support opaque/messageless traffic
natively. Squid can be enhanced to wrap tunneled traffic into something
resembling HTTP messages so that it can be analyzed using adaptation
services (e.g., Squid applies similar wrapping to FTP traffic already).

I recall occasional requests for such a feature. I am not aware of
anybody working on that right now.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.
P.S. Latest Squids support forwarding websocket tunnels that use HTTP
Upgrade mechanism (see http_upgrade_request_protocols in v5
squid.conf.documented).


From Walter.H at mathemainzel.info  Wed Mar 10 05:55:18 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Wed, 10 Mar 2021 06:55:18 +0100
Subject: [squid-users] a specific host generates a 503 ...
Message-ID: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>

Hello,

can someone test the following URL

http://db.local.clamav.net/daily-26102.cdiff

e.g.?? wget http://db.local.clamav.net/daily-26102.cdiff

I have an older squid (v3.1) there this works,
but with the newer ones (v3.4 and v3.5) this doesn't;

is there an explanation why?

the log shows this:

client-ip - - [10/Mar/2021:06:43:50 +0100] "GET 
http://db.local.clamav.net/daily-26102.cdiff HTTP/1.0" 503 8645 "-" 
"Wget/1.12 (linux-gnu)" TCP_MISS:HIER_DIRECT

the suspicious thing: when using a browser: this works with any squid, 
but this doesn't help because the clamav signature updates are loaded
by the freshclam which shows the? same failure as e.g. wget

client-ip - - [09/Mar/2021:06:00:03 +0100] "GET 
http://db.local.clamav.net/daily-26102.cdiff HTTP/1.0" 503 8642 "-" 
"ClamAV/0.103.1 (OS: linux-gnu, ARCH: x86_64, CPU: x86_64)" 
TCP_MISS:HIER_DIRECT

I noticed this two days after the nightly freshclam (signature update) 
failure,
and changed the freshclam config to use the squid v3.1;

Thanks,
Walter



From hello at ironpeak.be  Wed Mar 10 07:41:47 2021
From: hello at ironpeak.be (Niels Hofmans)
Date: Wed, 10 Mar 2021 08:41:47 +0100
Subject: [squid-users] websocket with sslbump
In-Reply-To: <48960e02-e060-c6de-99b9-bf6fc9fab55d@measurement-factory.com>
References: <5D5420CB-75A3-44F5-8045-266FD9073977@ironpeak.be>
 <48960e02-e060-c6de-99b9-bf6fc9fab55d@measurement-factory.com>
Message-ID: <BF251233-F8BC-4543-893B-D618E0BEEB6A@ironpeak.be>

Hi Alex,

Thank you for your response. I?ll be opening up a Bugzilla ticket for opaque messages through ICAP if it doesn?t exist already.
Related to the squid 5.x, I?ve reached out to the debian package maintainer last week for a binary install in the repos but no response as of yet.

Best regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

On 9 Mar 2021, at 16:58, Alex Rousskov <rousskov at measurement-factory.com> wrote:

On 3/8/21 10:10 AM, Niels Hofmans wrote:

> During testing sslbump + icap I noticed that websockets (ws + was) are
> not supported by squid. (Even if using on_unsupported_protocol)
> Are there any plans for supporting this with sslbump?

Your question can be misinterpreted in many different ways. I will
answer the following related question instead:

Q: Are there any plans for Squid to send tunneled traffic through
adaptation services?

The ICAP and eCAP protocols cannot support opaque/messageless traffic
natively. Squid can be enhanced to wrap tunneled traffic into something
resembling HTTP messages so that it can be analyzed using adaptation
services (e.g., Squid applies similar wrapping to FTP traffic already).

I recall occasional requests for such a feature. I am not aware of
anybody working on that right now.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.
P.S. Latest Squids support forwarding websocket tunnels that use HTTP
Upgrade mechanism (see http_upgrade_request_protocols in v5
squid.conf.documented).

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210310/607384d2/attachment.htm>

From roeeklinger60 at gmail.com  Wed Mar 10 12:28:57 2021
From: roeeklinger60 at gmail.com (roee klinger)
Date: Wed, 10 Mar 2021 14:28:57 +0200
Subject: [squid-users] How to completely blacklist a domain + subdomains,
 including HTTPS?
In-Reply-To: <609e9e89-0ea4-8d59-c735-1b26a1739ae7@treenet.co.nz>
References: <CAGCa14qAXTGCgiHJ6cO2ShiBx5VyGWNvsBDNe0WphEq9oajFog@mail.gmail.com>
 <609e9e89-0ea4-8d59-c735-1b26a1739ae7@treenet.co.nz>
Message-ID: <CAGCa14o4t4yAurM-gv0Mcwq3dYJ7oZX9VYiOw06tTgJ7n12E-A@mail.gmail.com>

Thanks, Amos.

I tried implementing the configuration you suggested but I am getting an
error message:

FATAL: Invalid ACL type 'ssl::server_name'
FATAL: Bungled /etc/squid/squid.conf line 36: acl server_blacklist
ssl::server_name "/etc/squid/domain_blacklist.txt"


I tried reading the documentation but can't find anything wrong in my
config file, I used the 2 lines exactly like they are in your suggestion,
and I am running Squid 4.10.

On Tue, Mar 9, 2021 at 2:48 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 10/03/21 12:57 am, roee klinger wrote:
> > Hey,
> >
> > I have found a lot of outdated or conflicting information about this
> > online, and since this is a really important matter, I wanted to make
> > sure I am doing this correctly.
> >
> > I am attempting to block some websites completely, including all HTTPS
> > traffic and subdomains.
> >
>
> Basically there are two protocols that need to be considered for this.
> HTTP and TLS.
>
> In HTTP the "website" is identified by a domain name in the
> request-target (aka URI, sometimes called URL).
>   * The 'dstdomain' ACL type matches URI domain name.
>   * The http_access directive is where that domain name becomes
> available for Squid to check.
>
>
> In TLS the "website" is identified by the TLS SNI sent by the client, or
> a field in the server X.509 certificate.
>   * The 'ssl::server_name' ACL type matches those details.
>   * The ssl_bump directive
>
>
> Next thing is to be aware that there are many ways to layer protocols.
> Do expect to see vastly different proxy behaviours for each permutation
> of those.
>   * port 443 "HTTPS" is TLS then HTTP
>   * port 80 "HTTPS" is HTTP then TLS (quite rare)
>   * forward-proxy "HTTPS" is HTTP then TLS then HTTP
>
>
>
> > Squid.conf:
> >
> >     acl domain_blacklist dstdomain "/etc/squid/domain_blacklist.txt"
> >     http_access deny all domain_blacklist
>
> The "all" here is pointless.
>
>
> >     http_reply_access deny domain_blacklist
>
> Use of reply access directive for blacklisting by request details is not
> useful.
>
> The request already got blocked. So any response reaching here is just
> the error page saying forbidden. Blocking that error page would just
> change it to a slightly different error page saying the *response* was
> forbidden - which is a bit confusing for any user trying to understand
> why their request didn't work.
>
>
> >     http_access deny CONNECT domain_blacklist
> >
>
> This line is useless here.
>
> squid.conf lines are interpreted top-down. The "deny all
> domain_blacklist" already stopped all requests that could possibly match
> the second condition of this line.
>
>
> >
> > /etc/squid/domain_blacklist.txt:
> >
> >     .ph
> >     .somepornwebsite.com
> >     .facebook.com
> >
> ...
> >
> > Am I doing this the right way?
>
>
> Sort of. Your http_access denial will catch all the HTTP and decrypted
> HTTP(S) traffic. It will not be able to block any HTTP(S) requests that
> are not able to decrypt.
>
> To catch and block these domains without needing the decrypt you should
> also use:
>
>   acl server_blacklist ssl::server_name "/etc/squid/domain_blacklist.txt"
>   ssl_bump terminate server_blacklist
>
> Of course there is always the failure case where traffic cannot decrypt
> and the TLS details use different server names.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210310/3828e7f1/attachment.htm>

From email_arjun at yahoo.com  Wed Mar 10 13:15:13 2021
From: email_arjun at yahoo.com (Arjun K)
Date: Wed, 10 Mar 2021 13:15:13 +0000 (UTC)
Subject: [squid-users] Squid Logs - TAG_NONE/503 errors
References: <1211174580.1330623.1615382113175.ref@mail.yahoo.com>
Message-ID: <1211174580.1330623.1615382113175@mail.yahoo.com>

Hi Team
Can you please let us know what this error means - TAG_NONE/503 in the access logs.Some times, I get the above error for all the URLs and no one is able to reach the internet and now I am facing the issue only for specific URLs.
The concern is that the issue gets automatically solved without making any changes to any configuration and it comes back again and again and it gets resolved.

The below is the configuration in the proxy server.
###IP Ranges
acl localnet src "/etc/squid/linux_server.txt"acl localnet src "/etc/squid/server_allowed.txt"

### URL - Allow / Deny
acl allowedurl dstdomain "/etc/squid/allowed_url.txt"acl denylist dstdomain "/etc/squid/denylist.txt"
acl Safe_ports port 80 # httpacl Safe_ports port 443 # httpsacl CONNECT method CONNECT
http_access deny denylisthttp_access deny !Safe_portshttp_access allow allowedurl
http_access allow localhost managerhttp_access deny manager
http_access allow localnethttp_access allow localhosthttp_access deny all
http_port 8080
cache_dir ufs /var/spool/squid 10000 16 256coredump_dir /var/spool/squid
refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-imsrefresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-imsrefresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80% 43200 reload-into-imsrefresh_pattern ^ftp:? ? ? ? ? ?1440? ? 20%? ? ?10080refresh_pattern ^gopher:? ? ? ? 1440? ? 0%? ? ? 1440refresh_pattern -i (/cgi-bin/|\?) 0? ? ?0%? ? ? 0refresh_pattern .? ? ? ? ? ? ? ?0? ? ? ?20%? ? ?4320

Could you please let me know what causes these issues and how to give a permanent fix.

RegardsArjun K.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210310/9997f8fc/attachment.htm>

From rousskov at measurement-factory.com  Wed Mar 10 14:19:48 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Mar 2021 09:19:48 -0500
Subject: [squid-users] Squid Logs - TAG_NONE/503 errors
In-Reply-To: <1211174580.1330623.1615382113175@mail.yahoo.com>
References: <1211174580.1330623.1615382113175.ref@mail.yahoo.com>
 <1211174580.1330623.1615382113175@mail.yahoo.com>
Message-ID: <65c93c05-99dc-6fab-aa86-b7f7bdb5bba6@measurement-factory.com>

On 3/10/21 8:15 AM, Arjun K wrote:
> Can you please let us know what this error means - TAG_NONE/503 in the
> access logs.

Most likely, Squid generated an error response and sent that to the
client. The response was probably generated before Squid made the cache
hit/miss decision.

In modern Squids, adding %err_code/%err_detail to your custom logformat
definition may help detail the error further in some cases. In all
Squids, looking at the Squid error response itself may help detail the
error further.


HTH,

Alex.


> Some times, I get the above error for all the URLs and no one is able to
> reach the internet and now I am facing the issue only for specific URLs.
> 
> The concern is that the issue gets automatically solved without making
> any changes to any configuration and it comes back again and again and
> it gets resolved.
> 
> 
> The below is the configuration in the proxy server.
> 
> ###IP Ranges
> 
> acl localnet src "/etc/squid/linux_server.txt"
> acl localnet src "/etc/squid/server_allowed.txt"
> 
> 
> ### URL - Allow / Deny
> 
> acl allowedurl dstdomain "/etc/squid/allowed_url.txt"
> acl denylist dstdomain "/etc/squid/denylist.txt"
> 
> acl Safe_ports port 80 # http
> acl Safe_ports port 443 # https
> acl CONNECT method CONNECT
> 
> http_access deny denylist
> http_access deny !Safe_ports
> http_access allow allowedurl
> 
> http_access allow localhost manager
> http_access deny manager
> 
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> http_port 8080
> 
> cache_dir ufs /var/spool/squid 10000 16 256
> coredump_dir /var/spool/squid
> 
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
> 80% 43200 reload-into-ims
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern ^ftp:? ? ? ? ? ?1440? ? 20%? ? ?10080
> refresh_pattern ^gopher:? ? ? ? 1440? ? 0%? ? ? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0? ? ?0%? ? ? 0
> refresh_pattern .? ? ? ? ? ? ? ?0? ? ? ?20%? ? ?4320
> 
> 
> Could you please let me know what causes these issues and how to give a
> permanent fix.
> 
> 
> Regards
> Arjun K.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From email_arjun at yahoo.com  Thu Mar 11 10:33:43 2021
From: email_arjun at yahoo.com (Arjun K)
Date: Thu, 11 Mar 2021 10:33:43 +0000 (UTC)
Subject: [squid-users] Squid Logs - TAG_NONE/503 errors
In-Reply-To: <65c93c05-99dc-6fab-aa86-b7f7bdb5bba6@measurement-factory.com>
References: <1211174580.1330623.1615382113175.ref@mail.yahoo.com>
 <1211174580.1330623.1615382113175@mail.yahoo.com>
 <65c93c05-99dc-6fab-aa86-b7f7bdb5bba6@measurement-factory.com>
Message-ID: <1249385231.189809.1615458823979@mail.yahoo.com>

 Hi Alex/Team
The end user are receiving an error in the browser stating : "The site can't be reached"and " <URL> took long time to respond ".
So can you assist me to include the custom log format which will provide further details.
[Thu Mar 11 11:02:15 2021].001 119629 10.197.10.140 TAG_NONE/503 0 CONNECT <URL>.amazoncognito.com:443 - HIER_NONE/- -



RegardsArjun K.
    On Wednesday, 10 March, 2021, 07:50:00 pm IST, Alex Rousskov <rousskov at measurement-factory.com> wrote:  
 
 On 3/10/21 8:15 AM, Arjun K wrote:
> Can you please let us know what this error means - TAG_NONE/503 in the
> access logs.

Most likely, Squid generated an error response and sent that to the
client. The response was probably generated before Squid made the cache
hit/miss decision.

In modern Squids, adding %err_code/%err_detail to your custom logformat
definition may help detail the error further in some cases. In all
Squids, looking at the Squid error response itself may help detail the
error further.


HTH,

Alex.


> Some times, I get the above error for all the URLs and no one is able to
> reach the internet and now I am facing the issue only for specific URLs.
> 
> The concern is that the issue gets automatically solved without making
> any changes to any configuration and it comes back again and again and
> it gets resolved.
> 
> 
> The below is the configuration in the proxy server.
> 
> ###IP Ranges
> 
> acl localnet src "/etc/squid/linux_server.txt"
> acl localnet src "/etc/squid/server_allowed.txt"
> 
> 
> ### URL - Allow / Deny
> 
> acl allowedurl dstdomain "/etc/squid/allowed_url.txt"
> acl denylist dstdomain "/etc/squid/denylist.txt"
> 
> acl Safe_ports port 80 # http
> acl Safe_ports port 443 # https
> acl CONNECT method CONNECT
> 
> http_access deny denylist
> http_access deny !Safe_ports
> http_access allow allowedurl
> 
> http_access allow localhost manager
> http_access deny manager
> 
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> http_port 8080
> 
> cache_dir ufs /var/spool/squid 10000 16 256
> coredump_dir /var/spool/squid
> 
> refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320
> 80% 43200 reload-into-ims
> refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip) 4320 80%
> 43200 reload-into-ims
> refresh_pattern ^ftp:? ? ? ? ? ?1440? ? 20%? ? ?10080
> refresh_pattern ^gopher:? ? ? ? 1440? ? 0%? ? ? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0? ? ?0%? ? ? 0
> refresh_pattern .? ? ? ? ? ? ? ?0? ? ? ?20%? ? ?4320
> 
> 
> Could you please let me know what causes these issues and how to give a
> permanent fix.
> 
> 
> Regards
> Arjun K.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210311/f928ba48/attachment.htm>

From ngtech1ltd at gmail.com  Thu Mar 11 12:12:24 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 11 Mar 2021 14:12:24 +0200
Subject: [squid-users] websocket with sslbump
In-Reply-To: <BF251233-F8BC-4543-893B-D618E0BEEB6A@ironpeak.be>
References: <5D5420CB-75A3-44F5-8045-266FD9073977@ironpeak.be>
 <48960e02-e060-c6de-99b9-bf6fc9fab55d@measurement-factory.com>
 <BF251233-F8BC-4543-893B-D618E0BEEB6A@ironpeak.be>
Message-ID: <002301d7166f$cc8ea3c0$65abeb40$@gmail.com>

Hey Niels,

 

I can help you with this if you need.
I have a pre-compiled version and while it?s not a Debian packaged ie .deb file it?s a matter of unpacking the files into the FS.

Also take a peek at the docker build:

https://github.com/elico/squid-docker-build-nodes

 


Let me know if you need this binaries, I can put them at:

https://ngtech.co.il/repo/bin/debian/10.4/amd64/

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Niels Hofmans
Sent: Wednesday, March 10, 2021 9:42 AM
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] websocket with sslbump

 

Hi Alex,

 

Thank you for your response. I?ll be opening up a Bugzilla ticket for opaque messages through ICAP if it doesn?t exist already.

Related to the squid 5.x, I?ve reached out to the debian package maintainer last week for a binary install in the repos but no response as of yet.

 

Best regards,
Niels Hofmans

SITE   https://ironpeak.be
BTW   BE0694785660
BANK BE76068909740795

 

On 9 Mar 2021, at 16:58, Alex Rousskov <rousskov at measurement-factory.com <mailto:rousskov at measurement-factory.com> > wrote:

 

On 3/8/21 10:10 AM, Niels Hofmans wrote:




During testing sslbump + icap I noticed that websockets (ws + was) are
not supported by squid. (Even if using on_unsupported_protocol)
Are there any plans for supporting this with sslbump?


Your question can be misinterpreted in many different ways. I will
answer the following related question instead:

Q: Are there any plans for Squid to send tunneled traffic through
adaptation services?

The ICAP and eCAP protocols cannot support opaque/messageless traffic
natively. Squid can be enhanced to wrap tunneled traffic into something
resembling HTTP messages so that it can be analyzed using adaptation
services (e.g., Squid applies similar wrapping to FTP traffic already).

I recall occasional requests for such a feature. I am not aware of
anybody working on that right now.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.
P.S. Latest Squids support forwarding websocket tunnels that use HTTP
Upgrade mechanism (see http_upgrade_request_protocols in v5
squid.conf.documented).

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210311/c53d6b31/attachment.htm>

From ngtech1ltd at gmail.com  Thu Mar 11 12:14:41 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 11 Mar 2021 14:14:41 +0200
Subject: [squid-users] a specific host generates a 503 ...
In-Reply-To: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>
References: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>
Message-ID: <002801d71670$1e2972a0$5a7c57e0$@gmail.com>

Hey Walter,

It's sitting behind:  DDoS protection by Cloudflare
So it makes sense that you would not be able to download it using wget.
The only option probably is using a web browser.
I would suggest contacting clamav.net web/system admins to verify what are the options.

All The Bests,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Walter H.
Sent: Wednesday, March 10, 2021 7:55 AM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] a specific host generates a 503 ...

Hello,

can someone test the following URL

http://db.local.clamav.net/daily-26102.cdiff

e.g.   wget http://db.local.clamav.net/daily-26102.cdiff

I have an older squid (v3.1) there this works,
but with the newer ones (v3.4 and v3.5) this doesn't;

is there an explanation why?

the log shows this:

client-ip - - [10/Mar/2021:06:43:50 +0100] "GET 
http://db.local.clamav.net/daily-26102.cdiff HTTP/1.0" 503 8645 "-" 
"Wget/1.12 (linux-gnu)" TCP_MISS:HIER_DIRECT

the suspicious thing: when using a browser: this works with any squid, 
but this doesn't help because the clamav signature updates are loaded
by the freshclam which shows the  same failure as e.g. wget

client-ip - - [09/Mar/2021:06:00:03 +0100] "GET 
http://db.local.clamav.net/daily-26102.cdiff HTTP/1.0" 503 8642 "-" 
"ClamAV/0.103.1 (OS: linux-gnu, ARCH: x86_64, CPU: x86_64)" 
TCP_MISS:HIER_DIRECT

I noticed this two days after the nightly freshclam (signature update) 
failure,
and changed the freshclam config to use the squid v3.1;

Thanks,
Walter

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ben.goz87 at gmail.com  Thu Mar 11 13:41:11 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 11 Mar 2021 15:41:11 +0200
Subject: [squid-users] Protecting squid
Message-ID: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>

By the help of God.

Hi,
I tried to open squid with some special port other than the default 3128
port.
But after a while I saw that my squid was being abused by unknown IP
addresses so I decided to password protect my squid so that only authorized
users could use it.
But it's pretty annoying for the users to enter user/password repeatedly.

Is there any other solution than password protection that only authorized
users can have access to my squid server?

Regards,
Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210311/378543eb/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Mar 11 13:50:24 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 11 Mar 2021 14:50:24 +0100
Subject: [squid-users] Protecting squid
In-Reply-To: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
Message-ID: <202103111450.25190.Antony.Stone@squid.open.source.it>

On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:

> I tried to open squid with some special port other than the default 3128
> port.

Obscurity is not equivalent to security.

> But after a while I saw that my squid was being abused by unknown IP
> addresses

I'm assuming this means your Squid proxy is accessible from the Internet.

Why?

> so I decided to password protect my squid so that only authorized
> users could use it.
> But it's pretty annoying for the users to enter user/password repeatedly.

What authentication method are you using?  At the very least, a user should 
not have to authenticate more than once per browser session - are they saying 
that even that is excessive?

> Is there any other solution than password protection that only authorized
> users can have access to my squid server?

Depends what "authorised" means.  Can you define the network range they are 
expected to come from, and restrict access only to those IPs?

Tell about your network setup and what you are trying to achieve - we might be 
able to suggest solutions.


Antony.

-- 
The best time to plant a tree is 20 years ago.
The second best time is now.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Thu Mar 11 14:02:48 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Mar 2021 03:02:48 +1300
Subject: [squid-users] Squid Logs - TAG_NONE/503 errors
In-Reply-To: <1249385231.189809.1615458823979@mail.yahoo.com>
References: <1211174580.1330623.1615382113175.ref@mail.yahoo.com>
 <1211174580.1330623.1615382113175@mail.yahoo.com>
 <65c93c05-99dc-6fab-aa86-b7f7bdb5bba6@measurement-factory.com>
 <1249385231.189809.1615458823979@mail.yahoo.com>
Message-ID: <e364e346-64e8-1ed4-8650-ebe2d65d636d@treenet.co.nz>

On 11/03/21 11:33 pm, Arjun K wrote:
> Hi Alex/Team
> 
> The end user are receiving an error in the browser stating : "The site 
> can't be reached"and " <URL> took long time to respond ".
> So can you assist me to include the custom log format which will provide 
> further details.
> 
> [Thu Mar 11 11:02:15 2021].001 119629 10.197.10.140 TAG_NONE/503 0 
> CONNECT <URL>.amazoncognito.com:443 - HIER_NONE/- -
> 

First issue, "why does the error page get produced?"

1) that log entry says that Squid waited 119.6 seconds for the server 
TCP connection to be setup. Before giving up and sending the client a 
503 message.


Second issue, "why is the server connection so slow?"

A) There should not be a *URL* in the *domain name* part of CONNECT 
requests. If that really is a URL instead of a hostname, that is very 
likely your problem.

B) what happens in the network outside of Squid can have many different 
effects on speed of TCP connection setup. Or packet loss can prevent 
connections being setup at all.

C) problems with DNS server(s) can add to the delays during server 
connection opening. That ranges from problems connecting to the DNS 
server(s), outdated records being returned by them, and again packet 
loss and general network conditions affecting DNS delivery time.



FWIW, your config has some issues. But nothing that would cause or 
related to the problem you are having.


Amos


From squid3 at treenet.co.nz  Thu Mar 11 14:08:33 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Mar 2021 03:08:33 +1300
Subject: [squid-users] How to completely blacklist a domain + subdomains,
 including HTTPS?
In-Reply-To: <CAGCa14o4t4yAurM-gv0Mcwq3dYJ7oZX9VYiOw06tTgJ7n12E-A@mail.gmail.com>
References: <CAGCa14qAXTGCgiHJ6cO2ShiBx5VyGWNvsBDNe0WphEq9oajFog@mail.gmail.com>
 <609e9e89-0ea4-8d59-c735-1b26a1739ae7@treenet.co.nz>
 <CAGCa14o4t4yAurM-gv0Mcwq3dYJ7oZX9VYiOw06tTgJ7n12E-A@mail.gmail.com>
Message-ID: <6aea0a8e-d8f9-1035-d15b-b4c9519f2e03@treenet.co.nz>

On 11/03/21 1:28 am, roee klinger wrote:
> Thanks, Amos.
> 
> I tried implementing the configuration you suggested but I am getting an 
> error message:
> 
>     FATAL: Invalid ACL type 'ssl::server_name'
>     FATAL: Bungled /etc/squid/squid.conf line 36: acl server_blacklist
>     ssl::server_name "/etc/squid/domain_blacklist.txt"
> 

That means your Squid lacks OpenSSL support (--with-openssl build 
option) required to do SSL-Bump related things.

I assumed from previous threads you had that enabled.


Amos


From squid3 at treenet.co.nz  Thu Mar 11 14:31:34 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Mar 2021 03:31:34 +1300
Subject: [squid-users] websocket with sslbump
In-Reply-To: <BF251233-F8BC-4543-893B-D618E0BEEB6A@ironpeak.be>
References: <5D5420CB-75A3-44F5-8045-266FD9073977@ironpeak.be>
 <48960e02-e060-c6de-99b9-bf6fc9fab55d@measurement-factory.com>
 <BF251233-F8BC-4543-893B-D618E0BEEB6A@ironpeak.be>
Message-ID: <f899deee-d539-6bf8-59d8-79d3a1d1af6b@treenet.co.nz>

On 10/03/21 8:41 pm, Niels Hofmans wrote:
> Hi Alex,
> 
> Thank you for your response. I?ll be opening up a Bugzilla ticket for 
> opaque messages through ICAP if it doesn?t exist already.
> Related to the squid 5.x, I?ve reached out to the debian package 
> maintainer last week for a binary install in the repos but no response 
> as of yet.
> 

I did not see it on the team tracker. If you tried to contact Luigi 
directly he is fairly busy with other software's issues these days.

It's usually me who is preparing the packages for Squid beta series and 
adding them to the Debian 'experimental' repository. I just have not had 
time this summer to do so. Not sure when I will be able to provide an 
ETA either, sorry.

Debian itself is going through its usual preparations for the next 
Debian major version. So there will likely only be small changes to the 
Debian squid (and now squid-openssl !!) packages for the next months.


Amos


From ben.goz87 at gmail.com  Thu Mar 11 14:37:26 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 11 Mar 2021 16:37:26 +0200
Subject: [squid-users] Protecting squid
In-Reply-To: <202103111450.25190.Antony.Stone@squid.open.source.it>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
Message-ID: <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>


On 11/03/2021 15:50, Antony Stone wrote:
> On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:
>
>> I tried to open squid with some special port other than the default 3128
>> port.
> Obscurity is not equivalent to security.
>
>> But after a while I saw that my squid was being abused by unknown IP
>> addresses
> I'm assuming this means your Squid proxy is accessible from the Internet.
>
> Why?
My users access squid via the internet.
>
>> so I decided to password protect my squid so that only authorized
>> users could use it.
>> But it's pretty annoying for the users to enter user/password repeatedly.
> What authentication method are you using?  At the very least, a user should
> not have to authenticate more than once per browser session - are they saying
> that even that is excessive?
Yep.
>
>> Is there any other solution than password protection that only authorized
>> users can have access to my squid server?
> Depends what "authorised" means.  Can you define the network range they are
> expected to come from, and restrict access only to those IPs?
This solution is least preferred because IPs range can by dynamically 
change.
>
> Tell about your network setup and what you are trying to achieve - we might be
> able to suggest solutions.

End users machine using some client application while their system proxy 
points to the above squid proxy server.

>
>
> Antony.
>
Regards,

Ben



From squid3 at treenet.co.nz  Thu Mar 11 14:33:43 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Mar 2021 03:33:43 +1300
Subject: [squid-users] a specific host generates a 503 ...
In-Reply-To: <002801d71670$1e2972a0$5a7c57e0$@gmail.com>
References: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>
 <002801d71670$1e2972a0$5a7c57e0$@gmail.com>
Message-ID: <f4592fa2-ee28-a5fd-7d62-39082e636016@treenet.co.nz>

On 12/03/21 1:14 am, Eliezer Croitoru wrote:
> Hey Walter,
> 
> It's sitting behind:  DDoS protection by Cloudflare
> So it makes sense that you would not be able to download it using wget.
> The only option probably is using a web browser.
> I would suggest contacting clamav.net web/system admins to verify what are the options.
> 

FWIW, the tools I use seem to fetch it fine when adding the header 
"User-Agent: ClamAV/0.103.1 (OS: linux-gnu, ARCH: x86_64, CPU: x86_64)".

Amos



From squid3 at treenet.co.nz  Thu Mar 11 14:44:53 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Mar 2021 03:44:53 +1300
Subject: [squid-users] Protecting squid
In-Reply-To: <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
Message-ID: <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>

On 12/03/21 3:37 am, Ben Goz wrote:
> 
> On 11/03/2021 15:50, Antony Stone wrote:
>> On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:
>>
>> Tell about your network setup and what you are trying to achieve - we 
>> might be
>> able to suggest solutions.
> 
> End users machine using some client application while their system proxy 
> points to the above squid proxy server.
> 

Please also provide your squid.conf settings so we can check they 
achieve your described need(s) properly. At least any lines starting 
with the http_access, auth_param, acl, or external_acl_type directives 
would be most useful.

Do not forget to anonymize sensitive details before posting. PLEASE do 
so in a way that we can tell whether a hidden value was correct for its 
usage, and whether any two hidden values are the same or different.


Amos


From ben.goz87 at gmail.com  Thu Mar 11 14:56:28 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 11 Mar 2021 16:56:28 +0200
Subject: [squid-users] Protecting squid
In-Reply-To: <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
Message-ID: <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>


On 11/03/2021 16:44, Amos Jeffries wrote:
> On 12/03/21 3:37 am, Ben Goz wrote:
>>
>> On 11/03/2021 15:50, Antony Stone wrote:
>>> On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:
>>>
>>> Tell about your network setup and what you are trying to achieve - 
>>> we might be
>>> able to suggest solutions.
>>
>> End users machine using some client application while their system 
>> proxy points to the above squid proxy server.
>>
>
> Please also provide your squid.conf settings so we can check they 
> achieve your described need(s) properly. At least any lines starting 
> with the http_access, auth_param, acl, or external_acl_type directives 
> would be most useful.
>
> Do not forget to anonymize sensitive details before posting. PLEASE do 
> so in a way that we can tell whether a hidden value was correct for 
> its usage, and whether any two hidden values are the same or different.


It's fork of default configuration with some changes.

# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
#http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
#http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

http_access allow localnet
http_access allow localhost

auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth 
/usr/local/squid/etc/passwd
auth_param basic realm proxy
acl authenticated proxy_auth REQUIRED
http_access allow authenticated

>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Thu Mar 11 16:41:38 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Mar 2021 11:41:38 -0500
Subject: [squid-users] Squid Logs - TAG_NONE/503 errors
In-Reply-To: <1249385231.189809.1615458823979@mail.yahoo.com>
References: <1211174580.1330623.1615382113175.ref@mail.yahoo.com>
 <1211174580.1330623.1615382113175@mail.yahoo.com>
 <65c93c05-99dc-6fab-aa86-b7f7bdb5bba6@measurement-factory.com>
 <1249385231.189809.1615458823979@mail.yahoo.com>
Message-ID: <8edb5494-0d48-160a-1710-53db6c4e0d72@measurement-factory.com>

On 3/11/21 5:33 AM, Arjun K wrote:

> So can you assist me to include the custom log format which will provide
> further details.

If you still want to add these details after reading Amos response, then
please see logformat and access_log directives in squid.conf.documented:

* logformat description has examples of defining formats. You just need
to add the %codes I mentioned. Disclaimer: I do not know whether your
Squid supports those %codes and whether your Squid will populate those
%codes with useful information in your specific use case.

* access_log description has an example of specifying the format when
configuring access.log. You just need to use your custom format name
instead of "squid".


HTH,

Alex.


> In modern Squids, adding %err_code/%err_detail to your custom logformat
> definition may help detail the error further in some cases. In all
> Squids, looking at the Squid error response itself may help detail the
> error further.



From rousskov at measurement-factory.com  Thu Mar 11 19:22:35 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Mar 2021 14:22:35 -0500
Subject: [squid-users] Protecting squid
In-Reply-To: <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
Message-ID: <90fdc96d-bdb3-b302-9d72-59cb071dd62b@measurement-factory.com>

On 3/11/21 9:37 AM, Ben Goz wrote:
> End users machine using some client application while their system proxy
> points to the above squid proxy server.

Client certificate-based authentication may be the best option if their
system proxy supports it and you do not need to bump user traffic with
SslBump. Otherwise, what authentication options does their system proxy
support (as an HTTP proxy client)?

Alex.


From squid3 at treenet.co.nz  Fri Mar 12 05:13:11 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Mar 2021 18:13:11 +1300
Subject: [squid-users] Protecting squid
In-Reply-To: <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
 <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
Message-ID: <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>

On 12/03/21 3:56 am, Ben Goz wrote:
> 
> On 11/03/2021 16:44, Amos Jeffries wrote:
>> On 12/03/21 3:37 am, Ben Goz wrote:
>>>
>>> On 11/03/2021 15:50, Antony Stone wrote:
>>>> On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:
>>>>
>>>> Tell about your network setup and what you are trying to achieve - 
>>>> we might be
>>>> able to suggest solutions.
>>>
>>> End users machine using some client application while their system 
>>> proxy points to the above squid proxy server.
>>>
>>
>> Please also provide your squid.conf settings so we can check they 
>> achieve your described need(s) properly. At least any lines starting 
>> with the http_access, auth_param, acl, or external_acl_type directives 
>> would be most useful.
>>
>> Do not forget to anonymize sensitive details before posting. PLEASE do 
>> so in a way that we can tell whether a hidden value was correct for 
>> its usage, and whether any two hidden values are the same or different.
> 
> 
> It's fork of default configuration with some changes.
> 
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> #http_access deny !Safe_ports
> 


Please restore this security protection. It prevents malware abusing 
HTTP's similarity to certain other protocols to perform attacks 
*through* your proxy.

The default Safe_ports list allows all ports not known to be dangerous, 
and all ports above 1024. So it should not have any noticeable effect on 
to any legitimate HTTP proxy clients - unless there is something really 
dodgy happening on your network. If you actually want something like 
that happening, then add the appropriate port for that activity to the 
Safe_ports list. Do not drop the protection completely.


> # Deny CONNECT to other than secure SSL ports
> #http_access deny CONNECT !SSL_ports
> 

The same can be said about this. Except this line is arguably even more 
important. CONNECT tunnels can literally contain anything. Let clients 
do things by adding ports to SSL_Ports list as-needed.

Please do some due-diligence checks before that to verify you are okay 
with all the uses of that port. Even ones you think the client 
themselves is unlikely to be doing. Once you open a port here *anyone* 
with access to the proxy can do whatever they like on that port.



> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> http_access allow localnet
> http_access allow localhost
> 
> auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth 
> /usr/local/squid/etc/passwd
> auth_param basic realm proxy

I notice you are missing a line setting the login TTL value.

There is currently a potential problem in the default which means Squid 
encounters situations where the credentials are seen as still going to 
be valid for hours so do not get refreshed. But garbage collection 
decides to throw them away.

This may not be related to the complaints you reported getting. But 
should be fixed to ensure the side effect of having to re-authenticate 
users does not complicate your actual problem.

"auth_param basic credentialsttl ..." sets how often Squid will re-check 
your auth system to confirm the users is still allowed. Default: 2 hr.

"authenticate_ttl ..." sets how often Squid will try to throw away all 
info about old clients being logged in. Default: 1 hr.


> acl authenticated proxy_auth REQUIRED
> http_access allow authenticated
> 

I recommend a slightly different form of check for logins. It prevents 
the situation where a user trying the wrong credentials gets a loop of 
popups.

Like so:
  http_access deny !authenticated

That guarantees they are not asked to login again if their software 
agent (aka browser, or such) provided or can locate the proper credentials.

After that you can add other rules about what the logged in users can 
do. eg allow them to do whatever they want. Like so:
  http_access allow all


Amos


From hsmtkk at gmail.com  Fri Mar 12 07:44:33 2021
From: hsmtkk at gmail.com (=?UTF-8?B?5qmL5pys57SY5biM?=)
Date: Fri, 12 Mar 2021 16:44:33 +0900
Subject: [squid-users] Squid 5 does not send ICAP request
Message-ID: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>

I made squid and ICAP system using docker-compose.

Squid 4 started sending ICAP requests 1 minute after boot.

However, squid 5 sends no ICAP request even 10 minutes after boot.
Squid continued to mark the ICAP service down.

How can I make squid 5 to start ICAP conversation?

* squid version
5.0.5-20210223-r4af19cc24

* squid.conf

```
http_port 3128
http_access allow all
icap_enable on
icap_service icapsvc reqmod_precache icap://icap5:1344 bypass=off
adaptation_access icapsvc allow all
icap_persistent_connections off
icap_service_revival_delay 60
debug_options ALL,9
```

* This is my environment.
https://github.com/hsmtkk/squidicap

* I uploaded access.log and cache.log to the GitHub issue.
https://github.com/hsmtkk/squidicap/issues/1

Best regards,
Kouki Hashimoto


From jrogers at opera.com  Fri Mar 12 17:17:24 2021
From: jrogers at opera.com (Joshua Rogers)
Date: Fri, 12 Mar 2021 18:17:24 +0100
Subject: [squid-users] Forcing authentication on specific websites
 (forwarding)
Message-ID: <CAOCwWhGbwMJgb7r9AcdDka5jyN691v3Lv+G=k1bf39cT2Fi61g@mail.gmail.com>

Hi all,

I am trying to get Squid to work as an open forwarding proxy, but with a
greylist.

I am hoping to allow access to all websites through squid except certain
websites. Sites which are not allowed will require authentication.

I tried this configuration:
http_access allow all
acl my_auth proxy_auth REQUIRED
acl google_users proxy_auth user1 user2 user3
acl google dstdomain .google.com
http_access deny google !google_users
http_access allow my_auth

but it simply allowed access to all websites instead of blocking *only* .
google.com.

Is this sort of configuration possible with Squid, and if so, how can it be
done?

Cheers,
Josh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210312/a9c793f1/attachment.htm>

From rousskov at measurement-factory.com  Fri Mar 12 18:42:41 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Mar 2021 13:42:41 -0500
Subject: [squid-users] Squid 5 does not send ICAP request
In-Reply-To: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
References: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
Message-ID: <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>

I suspect you are suffering from Bug 4528:
https://bugs.squid-cache.org/show_bug.cgi?id=4528

Which has also been discussed earlier as Bug 3621:
https://bugs.squid-cache.org/show_bug.cgi?id=3621

Does adding icap5 to /etc/hosts (or whatever your hosts_file points to)
help?

Unfortunately, I currently do not have enough free time to study your
logs to explain why Squid v5 delay is longer than that of v4, but I hope
that you can work around the problem by adjusting your hosts file.


HTH,

Alex.


On 3/12/21 2:44 AM, ???? wrote:
> I made squid and ICAP system using docker-compose.
> 
> Squid 4 started sending ICAP requests 1 minute after boot.
> 
> However, squid 5 sends no ICAP request even 10 minutes after boot.
> Squid continued to mark the ICAP service down.
> 
> How can I make squid 5 to start ICAP conversation?
> 
> * squid version
> 5.0.5-20210223-r4af19cc24
> 
> * squid.conf
> 
> ```
> http_port 3128
> http_access allow all
> icap_enable on
> icap_service icapsvc reqmod_precache icap://icap5:1344 bypass=off
> adaptation_access icapsvc allow all
> icap_persistent_connections off
> icap_service_revival_delay 60
> debug_options ALL,9
> ```
> 
> * This is my environment.
> https://github.com/hsmtkk/squidicap
> 
> * I uploaded access.log and cache.log to the GitHub issue.
> https://github.com/hsmtkk/squidicap/issues/1
> 
> Best regards,
> Kouki Hashimoto
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Fri Mar 12 18:50:31 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Mar 2021 13:50:31 -0500
Subject: [squid-users] Forcing authentication on specific websites
 (forwarding)
In-Reply-To: <CAOCwWhGbwMJgb7r9AcdDka5jyN691v3Lv+G=k1bf39cT2Fi61g@mail.gmail.com>
References: <CAOCwWhGbwMJgb7r9AcdDka5jyN691v3Lv+G=k1bf39cT2Fi61g@mail.gmail.com>
Message-ID: <8f28f3cb-6f0c-3bbe-5a57-bc164ab687c2@measurement-factory.com>

On 3/12/21 12:17 PM, Joshua Rogers wrote:

> I am hoping to allow access to all websites through squid except certain
> websites. Sites which are not allowed will require authentication.
> 
> I tried this configuration:
> http_access allow all

Game over. The order of http_access rules matters. The first matching
rule wins. An always-matching "all" rule renders any further http_access
rules useless.

I have not tested this, but I would start with something like this:

    http_access allow google google_users
    http_access deny google
    http_access allow all

Alex.


> acl my_auth proxy_auth REQUIRED
> acl google_users proxy_auth user1 user2 user3
> acl google dstdomain .google.com <http://google.com>
> http_access deny google !google_users
> http_access allow my_auth?
> 
> but it simply allowed access to all websites instead of blocking *only*
> .google.com <http://google.com>.
> 
> Is this sort of configuration possible with Squid, and if so, how can it
> be done?
> 
> Cheers,
> Josh
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jrogers at opera.com  Fri Mar 12 19:00:59 2021
From: jrogers at opera.com (Joshua Rogers)
Date: Fri, 12 Mar 2021 20:00:59 +0100
Subject: [squid-users] Forcing authentication on specific websites
 (forwarding)
In-Reply-To: <8f28f3cb-6f0c-3bbe-5a57-bc164ab687c2@measurement-factory.com>
References: <CAOCwWhGbwMJgb7r9AcdDka5jyN691v3Lv+G=k1bf39cT2Fi61g@mail.gmail.com>
 <8f28f3cb-6f0c-3bbe-5a57-bc164ab687c2@measurement-factory.com>
Message-ID: <CAOCwWhGhNLmSK8bGCvudGSufS7k1hw0ZFSpjsX+1Ugk3+JHatw@mail.gmail.com>

Hi Alex,

Thank you, that works perfectly.

Cheers,
Josh

On Fri, Mar 12, 2021 at 7:50 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 3/12/21 12:17 PM, Joshua Rogers wrote:
>
> > I am hoping to allow access to all websites through squid except certain
> > websites. Sites which are not allowed will require authentication.
> >
> > I tried this configuration:
> > http_access allow all
>
> Game over. The order of http_access rules matters. The first matching
> rule wins. An always-matching "all" rule renders any further http_access
> rules useless.
>
> I have not tested this, but I would start with something like this:
>
>     http_access allow google google_users
>     http_access deny google
>     http_access allow all
>
> Alex.
>
>
> > acl my_auth proxy_auth REQUIRED
> > acl google_users proxy_auth user1 user2 user3
> > acl google dstdomain .google.com <http://google.com>
> > http_access deny google !google_users
> > http_access allow my_auth
> >
> > but it simply allowed access to all websites instead of blocking *only*
> > .google.com <http://google.com>.
> >
> > Is this sort of configuration possible with Squid, and if so, how can it
> > be done?
> >
> > Cheers,
> > Josh
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210312/92b55b2d/attachment.htm>

From Walter.H at mathemainzel.info  Sat Mar 13 12:03:03 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 13 Mar 2021 13:03:03 +0100
Subject: [squid-users] a specific host generates a 503 ...
In-Reply-To: <f4592fa2-ee28-a5fd-7d62-39082e636016@treenet.co.nz>
References: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>
 <002801d71670$1e2972a0$5a7c57e0$@gmail.com>
 <f4592fa2-ee28-a5fd-7d62-39082e636016@treenet.co.nz>
Message-ID: <667a9656-c2af-10db-f884-ad055b9c7974@mathemainzel.info>

On 11.03.2021 15:33, Amos Jeffries wrote:
> On 12/03/21 1:14 am, Eliezer Croitoru wrote:
>> Hey Walter,
>>
>> It's sitting behind:? DDoS protection by Cloudflare
>> So it makes sense that you would not be able to download it using wget.
>> The only option probably is using a web browser.
>> I would suggest contacting clamav.net web/system admins to verify 
>> what are the options.
>>
>
> FWIW, the tools I use seem to fetch it fine when adding the header 
> "User-Agent: ClamAV/0.103.1 (OS: linux-gnu, ARCH: x86_64, CPU: x86_64)".

the freshclam updater does adding this User-Agent, and fails on the 
newer squids, only the older ones succeeeds;

and wget succeeds using the older squid, too?
(without adding a User-Agent)

why is that?

Thanks,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3550 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210313/b927065d/attachment.bin>

From ben.goz87 at gmail.com  Sun Mar 14 13:26:28 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Sun, 14 Mar 2021 15:26:28 +0200
Subject: [squid-users] Protecting squid
In-Reply-To: <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
 <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
 <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>
Message-ID: <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>


On 12/03/2021 7:13, Amos Jeffries wrote:
> On 12/03/21 3:56 am, Ben Goz wrote:
>>
>> On 11/03/2021 16:44, Amos Jeffries wrote:
>>> On 12/03/21 3:37 am, Ben Goz wrote:
>>>>
>>>> On 11/03/2021 15:50, Antony Stone wrote:
>>>>> On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:
>>>>>
>>>>> Tell about your network setup and what you are trying to achieve - 
>>>>> we might be
>>>>> able to suggest solutions.
>>>>
>>>> End users machine using some client application while their system 
>>>> proxy points to the above squid proxy server.
>>>>
>>>
>>> Please also provide your squid.conf settings so we can check they 
>>> achieve your described need(s) properly. At least any lines starting 
>>> with the http_access, auth_param, acl, or external_acl_type 
>>> directives would be most useful.
>>>
>>> Do not forget to anonymize sensitive details before posting. PLEASE 
>>> do so in a way that we can tell whether a hidden value was correct 
>>> for its usage, and whether any two hidden values are the same or 
>>> different.
>>
>>
>> It's fork of default configuration with some changes.
>>
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> #http_access deny !Safe_ports
>>
>
>
> Please restore this security protection. It prevents malware abusing 
> HTTP's similarity to certain other protocols to perform attacks 
> *through* your proxy.
>
> The default Safe_ports list allows all ports not known to be 
> dangerous, and all ports above 1024. So it should not have any 
> noticeable effect on to any legitimate HTTP proxy clients - unless 
> there is something really dodgy happening on your network. If you 
> actually want something like that happening, then add the appropriate 
> port for that activity to the Safe_ports list. Do not drop the 
> protection completely.
>
>
>> # Deny CONNECT to other than secure SSL ports
>> #http_access deny CONNECT !SSL_ports
>>
>
> The same can be said about this. Except this line is arguably even 
> more important. CONNECT tunnels can literally contain anything. Let 
> clients do things by adding ports to SSL_Ports list as-needed.
>
> Please do some due-diligence checks before that to verify you are okay 
> with all the uses of that port. Even ones you think the client 
> themselves is unlikely to be doing. Once you open a port here *anyone* 
> with access to the proxy can do whatever they like on that port.
>
>
>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> http_access allow localnet
>> http_access allow localhost
>>
>> auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth 
>> /usr/local/squid/etc/passwd
>> auth_param basic realm proxy
>
> I notice you are missing a line setting the login TTL value.
>
> There is currently a potential problem in the default which means 
> Squid encounters situations where the credentials are seen as still 
> going to be valid for hours so do not get refreshed. But garbage 
> collection decides to throw them away.
>
> This may not be related to the complaints you reported getting. But 
> should be fixed to ensure the side effect of having to re-authenticate 
> users does not complicate your actual problem.
>
> "auth_param basic credentialsttl ..." sets how often Squid will 
> re-check your auth system to confirm the users is still allowed. 
> Default: 2 hr.
>
> "authenticate_ttl ..." sets how often Squid will try to throw away all 
> info about old clients being logged in. Default: 1 hr.
>
>
>> acl authenticated proxy_auth REQUIRED
>> http_access allow authenticated
>>
>
> I recommend a slightly different form of check for logins. It prevents 
> the situation where a user trying the wrong credentials gets a loop of 
> popups.
>
> Like so:
> ?http_access deny !authenticated
>
> That guarantees they are not asked to login again if their software 
> agent (aka browser, or such) provided or can locate the proper 
> credentials.
>
> After that you can add other rules about what the logged in users can 
> do. eg allow them to do whatever they want. Like so:
> ?http_access allow all

Can I configure squid authentication TTL per only source IP and ignores 
other parameters so authentication will be requested only once in TTL 
for all the sessions?

>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From hsmtkk at gmail.com  Mon Mar 15 01:55:33 2021
From: hsmtkk at gmail.com (=?UTF-8?B?5qmL5pys57SY5biM?=)
Date: Mon, 15 Mar 2021 10:55:33 +0900
Subject: [squid-users] Squid 5 does not send ICAP request
In-Reply-To: <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
References: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
 <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
Message-ID: <CAAif--qN0nsdoMVaQuZp0WeYGnSBe8JRNJXedOB_oc+mUVj41Q@mail.gmail.com>

Dear Alex,

> Does adding icap5 to /etc/hosts (or whatever your hosts_file points to) help?

Yes, it dit.

When I added ICAP hostname and IP mapping to the /etc/hosts file
before starting the Squid, the system worked fine.
(I put some shell scripts in the container entry point.)

Thanks for your help.
I hope some fundamental solution might be implemented in the future version.

Best regards,
Kouki Hashimoto

2021?3?13?(?) 3:42 Alex Rousskov <rousskov at measurement-factory.com>:
>
> I suspect you are suffering from Bug 4528:
> https://bugs.squid-cache.org/show_bug.cgi?id=4528
>
> Which has also been discussed earlier as Bug 3621:
> https://bugs.squid-cache.org/show_bug.cgi?id=3621
>
> Does adding icap5 to /etc/hosts (or whatever your hosts_file points to)
> help?
>
> Unfortunately, I currently do not have enough free time to study your
> logs to explain why Squid v5 delay is longer than that of v4, but I hope
> that you can work around the problem by adjusting your hosts file.
>
>
> HTH,
>
> Alex.
>
>
> On 3/12/21 2:44 AM, ???? wrote:
> > I made squid and ICAP system using docker-compose.
> >
> > Squid 4 started sending ICAP requests 1 minute after boot.
> >
> > However, squid 5 sends no ICAP request even 10 minutes after boot.
> > Squid continued to mark the ICAP service down.
> >
> > How can I make squid 5 to start ICAP conversation?
> >
> > * squid version
> > 5.0.5-20210223-r4af19cc24
> >
> > * squid.conf
> >
> > ```
> > http_port 3128
> > http_access allow all
> > icap_enable on
> > icap_service icapsvc reqmod_precache icap://icap5:1344 bypass=off
> > adaptation_access icapsvc allow all
> > icap_persistent_connections off
> > icap_service_revival_delay 60
> > debug_options ALL,9
> > ```
> >
> > * This is my environment.
> > https://github.com/hsmtkk/squidicap
> >
> > * I uploaded access.log and cache.log to the GitHub issue.
> > https://github.com/hsmtkk/squidicap/issues/1
> >
> > Best regards,
> > Kouki Hashimoto
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>


From uhlar at fantomas.sk  Mon Mar 15 09:14:43 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 15 Mar 2021 10:14:43 +0100
Subject: [squid-users] a specific host generates a 503 ...
In-Reply-To: <667a9656-c2af-10db-f884-ad055b9c7974@mathemainzel.info>
References: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>
 <002801d71670$1e2972a0$5a7c57e0$@gmail.com>
 <f4592fa2-ee28-a5fd-7d62-39082e636016@treenet.co.nz>
 <667a9656-c2af-10db-f884-ad055b9c7974@mathemainzel.info>
Message-ID: <20210315091443.GB10069@fantomas.sk>

>>On 12/03/21 1:14 am, Eliezer Croitoru wrote:
>>>It's sitting behind:? DDoS protection by Cloudflare
>>>So it makes sense that you would not be able to download it using wget.
>>>The only option probably is using a web browser.
>>>I would suggest contacting clamav.net web/system admins to verify 
>>>what are the options.

>On 11.03.2021 15:33, Amos Jeffries wrote:
>>FWIW, the tools I use seem to fetch it fine when adding the header 
>>"User-Agent: ClamAV/0.103.1 (OS: linux-gnu, ARCH: x86_64, CPU: 
>>x86_64)".

On 13.03.21 13:03, Walter H. wrote:
>the freshclam updater does adding this User-Agent, and fails on the 
>newer squids, only the older ones succeeeds;
>
>and wget succeeds using the older squid, too?
>(without adding a User-Agent)
>
>why is that?

due to huge abuse from web fetchers like wget, clamav has recently blocked
fetching virus databases by non-freshclam clients and freshclam older than
0.100:
https://lists.clamav.net/pipermail/clamav-users/2021-March/010578.html


I can't really tell why or how it worked with older squid releases (maybe
clearing the user-agent?), but you should definitely download ClamAV updates
using recent freshclam.

Please, upgrade to recent ClamAV first, or set up cvdupdate:
https://lists.clamav.net/pipermail/clamav-users/2021-March/010613.html

If that doesn't work, contact the clamav - changes are, you are blacklisted
from fetching updates.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Micro random number generator: 0, 0, 0, 4.33e+67, 0, 0, 0...


From ngtech1ltd at gmail.com  Mon Mar 15 09:18:32 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 15 Mar 2021 11:18:32 +0200
Subject: [squid-users] Squid 5 does not send ICAP request
In-Reply-To: <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
References: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
 <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
Message-ID: <003801d7197c$2bd530b0$837f9210$@gmail.com>

Hey Alex and Amos.

These bugs are open since forever.
There is a simple way to re-produce it.
There is also a simple bypass to the issue but still.
However, Would it be possible to fix it for 6?
It doesn't to even require a single http request to test and verify.
The ICAP hosts doesn't resolve for at-least 3 minutes.

Thanks,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Friday, March 12, 2021 8:43 PM
To: ???? <hsmtkk at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 5 does not send ICAP request

I suspect you are suffering from Bug 4528:
https://bugs.squid-cache.org/show_bug.cgi?id=4528

Which has also been discussed earlier as Bug 3621:
https://bugs.squid-cache.org/show_bug.cgi?id=3621

Does adding icap5 to /etc/hosts (or whatever your hosts_file points to)
help?

Unfortunately, I currently do not have enough free time to study your
logs to explain why Squid v5 delay is longer than that of v4, but I hope
that you can work around the problem by adjusting your hosts file.


HTH,

Alex.


On 3/12/21 2:44 AM, ???? wrote:
> I made squid and ICAP system using docker-compose.
> 
> Squid 4 started sending ICAP requests 1 minute after boot.
> 
> However, squid 5 sends no ICAP request even 10 minutes after boot.
> Squid continued to mark the ICAP service down.
> 
> How can I make squid 5 to start ICAP conversation?
> 
> * squid version
> 5.0.5-20210223-r4af19cc24
> 
> * squid.conf
> 
> ```
> http_port 3128
> http_access allow all
> icap_enable on
> icap_service icapsvc reqmod_precache icap://icap5:1344 bypass=off
> adaptation_access icapsvc allow all
> icap_persistent_connections off
> icap_service_revival_delay 60
> debug_options ALL,9
> ```
> 
> * This is my environment.
> https://github.com/hsmtkk/squidicap
> 
> * I uploaded access.log and cache.log to the GitHub issue.
> https://github.com/hsmtkk/squidicap/issues/1
> 
> Best regards,
> Kouki Hashimoto
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Mon Mar 15 09:20:31 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 15 Mar 2021 11:20:31 +0200
Subject: [squid-users] Protecting squid
In-Reply-To: <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
 <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
 <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>
 <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>
Message-ID: <003901d7197c$729f7190$57de54b0$@gmail.com>

Hey Ben,

Since you probably doesn?t have 100k users and there for passwords it wouldn't do a thing.
Nobody will feel you dropping the TTL.
The content of the credentials file will be in RAM so you should give it a try first and ask later.

All The Bests,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Ben Goz
Sent: Sunday, March 14, 2021 3:26 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Protecting squid


On 12/03/2021 7:13, Amos Jeffries wrote:
> On 12/03/21 3:56 am, Ben Goz wrote:
>>
>> On 11/03/2021 16:44, Amos Jeffries wrote:
>>> On 12/03/21 3:37 am, Ben Goz wrote:
>>>>
>>>> On 11/03/2021 15:50, Antony Stone wrote:
>>>>> On Thursday 11 March 2021 at 14:41:11, Ben Goz wrote:
>>>>>
>>>>> Tell about your network setup and what you are trying to achieve - 
>>>>> we might be
>>>>> able to suggest solutions.
>>>>
>>>> End users machine using some client application while their system 
>>>> proxy points to the above squid proxy server.
>>>>
>>>
>>> Please also provide your squid.conf settings so we can check they 
>>> achieve your described need(s) properly. At least any lines starting 
>>> with the http_access, auth_param, acl, or external_acl_type 
>>> directives would be most useful.
>>>
>>> Do not forget to anonymize sensitive details before posting. PLEASE 
>>> do so in a way that we can tell whether a hidden value was correct 
>>> for its usage, and whether any two hidden values are the same or 
>>> different.
>>
>>
>> It's fork of default configuration with some changes.
>>
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> #http_access deny !Safe_ports
>>
>
>
> Please restore this security protection. It prevents malware abusing 
> HTTP's similarity to certain other protocols to perform attacks 
> *through* your proxy.
>
> The default Safe_ports list allows all ports not known to be 
> dangerous, and all ports above 1024. So it should not have any 
> noticeable effect on to any legitimate HTTP proxy clients - unless 
> there is something really dodgy happening on your network. If you 
> actually want something like that happening, then add the appropriate 
> port for that activity to the Safe_ports list. Do not drop the 
> protection completely.
>
>
>> # Deny CONNECT to other than secure SSL ports
>> #http_access deny CONNECT !SSL_ports
>>
>
> The same can be said about this. Except this line is arguably even 
> more important. CONNECT tunnels can literally contain anything. Let 
> clients do things by adding ports to SSL_Ports list as-needed.
>
> Please do some due-diligence checks before that to verify you are okay 
> with all the uses of that port. Even ones you think the client 
> themselves is unlikely to be doing. Once you open a port here *anyone* 
> with access to the proxy can do whatever they like on that port.
>
>
>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> http_access allow localnet
>> http_access allow localhost
>>
>> auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth 
>> /usr/local/squid/etc/passwd
>> auth_param basic realm proxy
>
> I notice you are missing a line setting the login TTL value.
>
> There is currently a potential problem in the default which means 
> Squid encounters situations where the credentials are seen as still 
> going to be valid for hours so do not get refreshed. But garbage 
> collection decides to throw them away.
>
> This may not be related to the complaints you reported getting. But 
> should be fixed to ensure the side effect of having to re-authenticate 
> users does not complicate your actual problem.
>
> "auth_param basic credentialsttl ..." sets how often Squid will 
> re-check your auth system to confirm the users is still allowed. 
> Default: 2 hr.
>
> "authenticate_ttl ..." sets how often Squid will try to throw away all 
> info about old clients being logged in. Default: 1 hr.
>
>
>> acl authenticated proxy_auth REQUIRED
>> http_access allow authenticated
>>
>
> I recommend a slightly different form of check for logins. It prevents 
> the situation where a user trying the wrong credentials gets a loop of 
> popups.
>
> Like so:
>  http_access deny !authenticated
>
> That guarantees they are not asked to login again if their software 
> agent (aka browser, or such) provided or can locate the proper 
> credentials.
>
> After that you can add other rules about what the logged in users can 
> do. eg allow them to do whatever they want. Like so:
>  http_access allow all

Can I configure squid authentication TTL per only source IP and ignores 
other parameters so authentication will be requested only once in TTL 
for all the sessions?

>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Mar 15 13:27:46 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Mar 2021 02:27:46 +1300
Subject: [squid-users] Protecting squid
In-Reply-To: <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
 <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
 <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>
 <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>
Message-ID: <4dedac61-60ff-d2cf-3d12-cc7de62b8590@treenet.co.nz>

On 15/03/21 2:26 am, Ben Goz wrote:
> 
> Can I configure squid authentication TTL per only source IP and ignores 
> other parameters so authentication will be requested only once in TTL 
> for all the sessions?
> 

Not with just authentication. You will need to use a slightly more 
complicated system involving an external_acl_type helper as well and 
switch to an SQL database auth system.


The idea for that is that you have a database of authenticated users 
with their last-known IP address.

  Your auth_param helper is changed to one which takes client IP address 
in the auth_param key_extras setting, and adds records to the SQL 
database before telling Squid the login is OK.

  Use an ext_sql_session_acl helper which takes IP address and checks 
the database to find the username who last authenticated from there. 
This needs to be checked and permit existing sessions before the auth 
helper.

The config looks something like this:


   external_acl_type ipuser negative_ttl=0 ttl=7200 %<a \
     /usr/bin/squid/ext_sql_session_acl \
     --dsn "..." --user dbUsername --password dbPassword --persist \
     --usercol username --uidcol ipaddress

   acl user_known external ipuser
   http_access allow user_known


   auth_param basic program /path/to/helper
   auth_param basic key_extras %<a
   auth_param basic credentialsttl 2 hours

   acl authenticated proxy_auth REQUIRED

   http_access allow authenticated



Amos


From rousskov at measurement-factory.com  Mon Mar 15 14:51:46 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Mar 2021 10:51:46 -0400
Subject: [squid-users] Squid 5 does not send ICAP request
In-Reply-To: <003801d7197c$2bd530b0$837f9210$@gmail.com>
References: <CAAif--p9ajbxLL3k+BbrsyJuuaYvXB-zS4OVertRd_gW3wYE+g@mail.gmail.com>
 <48aee7a2-31d6-b6a4-a2e9-47426b41c189@measurement-factory.com>
 <003801d7197c$2bd530b0$837f9210$@gmail.com>
Message-ID: <d79e9d88-6726-3290-f0d8-a91bd4cf9ed4@measurement-factory.com>

On 3/15/21 5:18 AM, Eliezer Croitoru wrote:

> These bugs are open since forever.
> There is a simple way to re-produce it.
> There is also a simple bypass to the issue but still.
> However, Would it be possible to fix it for 6?

Yes, it would be! We are waiting for a volunteer to contribute a quality
fix or a sponsor who can facilitate the same. The problem can be solved,
but there is no trivial solution AFAICT, and nobody volunteered to do or
pay for the necessary work.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
> Sent: Friday, March 12, 2021 8:43 PM
> To: ???? <hsmtkk at gmail.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 5 does not send ICAP request
> 
> I suspect you are suffering from Bug 4528:
> https://bugs.squid-cache.org/show_bug.cgi?id=4528
> 
> Which has also been discussed earlier as Bug 3621:
> https://bugs.squid-cache.org/show_bug.cgi?id=3621
> 
> Does adding icap5 to /etc/hosts (or whatever your hosts_file points to)
> help?
> 
> Unfortunately, I currently do not have enough free time to study your
> logs to explain why Squid v5 delay is longer than that of v4, but I hope
> that you can work around the problem by adjusting your hosts file.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> On 3/12/21 2:44 AM, ???? wrote:
>> I made squid and ICAP system using docker-compose.
>>
>> Squid 4 started sending ICAP requests 1 minute after boot.
>>
>> However, squid 5 sends no ICAP request even 10 minutes after boot.
>> Squid continued to mark the ICAP service down.
>>
>> How can I make squid 5 to start ICAP conversation?
>>
>> * squid version
>> 5.0.5-20210223-r4af19cc24
>>
>> * squid.conf
>>
>> ```
>> http_port 3128
>> http_access allow all
>> icap_enable on
>> icap_service icapsvc reqmod_precache icap://icap5:1344 bypass=off
>> adaptation_access icapsvc allow all
>> icap_persistent_connections off
>> icap_service_revival_delay 60
>> debug_options ALL,9
>> ```
>>
>> * This is my environment.
>> https://github.com/hsmtkk/squidicap
>>
>> * I uploaded access.log and cache.log to the GitHub issue.
>> https://github.com/hsmtkk/squidicap/issues/1
>>
>> Best regards,
>> Kouki Hashimoto
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rsa.sro.csse at rsa.com  Mon Mar 15 16:49:43 2021
From: rsa.sro.csse at rsa.com (rsa.sro.csse at rsa.com)
Date: Mon, 15 Mar 2021 16:49:43 +0000
Subject: [squid-users] Solarwinds Information
Message-ID: <fd884d52202f4b1d9d2a45ccf6f5688e@KULX13MDC132.APAC.DELL.COM>

Hi, is there any available information regarding the Solarwinds vulnerability on the Squid site?
Thank you,
Brian


RSA - Security and Risk Office

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210315/3e4c348b/attachment.htm>

From rousskov at measurement-factory.com  Mon Mar 15 18:02:33 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Mar 2021 14:02:33 -0400
Subject: [squid-users] Solarwinds Information
In-Reply-To: <fd884d52202f4b1d9d2a45ccf6f5688e@KULX13MDC132.APAC.DELL.COM>
References: <fd884d52202f4b1d9d2a45ccf6f5688e@KULX13MDC132.APAC.DELL.COM>
Message-ID: <f3ed93ab-4826-af40-74fc-3e6f1a09301c@measurement-factory.com>

On 3/15/21 12:49 PM, rsa.sro.csse at rsa.com wrote:
> Hi, is there any available information regarding the Solarwinds
> vulnerability on the Squid site?

AFAICT, Squid is unaffected by (and unrelated to) CVE-2020-14005 and
CVE-2020-13169, so I would not expect the Squid site to mention those
CVEs. If you are asking about some other CVEs, please clarify.

Alex.


From Walter.H at mathemainzel.info  Mon Mar 15 20:56:27 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Mon, 15 Mar 2021 21:56:27 +0100
Subject: [squid-users] a specific host generates a 503 ...
In-Reply-To: <20210315091443.GB10069@fantomas.sk>
References: <abab8185-ea2f-cb8f-ffdd-653760c1781e@mathemainzel.info>
 <002801d71670$1e2972a0$5a7c57e0$@gmail.com>
 <f4592fa2-ee28-a5fd-7d62-39082e636016@treenet.co.nz>
 <667a9656-c2af-10db-f884-ad055b9c7974@mathemainzel.info>
 <20210315091443.GB10069@fantomas.sk>
Message-ID: <7d3a5f95-06cf-4511-5b12-ee895a9b6bc6@mathemainzel.info>

On 15.03.2021 10:14, Matus UHLAR - fantomas wrote:
>>> On 12/03/21 1:14 am, Eliezer Croitoru wrote:
>>>> It's sitting behind:? DDoS protection by Cloudflare
>>>> So it makes sense that you would not be able to download it using 
>>>> wget.
>>>> The only option probably is using a web browser.
>>>> I would suggest contacting clamav.net web/system admins to verify 
>>>> what are the options.
>
>> On 11.03.2021 15:33, Amos Jeffries wrote:
>>> FWIW, the tools I use seem to fetch it fine when adding the header 
>>> "User-Agent: ClamAV/0.103.1 (OS: linux-gnu, ARCH: x86_64, CPU: 
>>> x86_64)".
>
>
> due to huge abuse from web fetchers like wget, clamav has recently 
> blocked
> fetching virus databases by non-freshclam clients and freshclam older 
> than
> 0.100:
> https://lists.clamav.net/pipermail/clamav-users/2021-March/010578.html
>
I found out, my older squid was the only squid, not clearing the User-Agent;

thanks for the infos;

now it works again with the originally used squid;

Thanks,
Walter



-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3550 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210315/f7c3b40d/attachment.bin>

From ben.goz87 at gmail.com  Wed Mar 17 13:54:25 2021
From: ben.goz87 at gmail.com (Ben Goz)
Date: Wed, 17 Mar 2021 15:54:25 +0200
Subject: [squid-users] Protecting squid
In-Reply-To: <4dedac61-60ff-d2cf-3d12-cc7de62b8590@treenet.co.nz>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
 <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
 <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>
 <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>
 <4dedac61-60ff-d2cf-3d12-cc7de62b8590@treenet.co.nz>
Message-ID: <cf4376cb-223e-49b0-abd5-7200a4d21a9d@gmail.com>

By the help of God.


Hi Amos,

Sounds interesting.

Maybe I should modify the external_acl_type to talk with internal API 
inside my system.

Can you please point me to some code examples and documentation?

Also if you can please point me to squid code that invokes the external 
ACL program?

Thanks,

Ben

On 15/03/2021 15:27, Amos Jeffries wrote:
> On 15/03/21 2:26 am, Ben Goz wrote:
>>
>> Can I configure squid authentication TTL per only source IP and 
>> ignores other parameters so authentication will be requested only 
>> once in TTL for all the sessions?
>>
>
> Not with just authentication. You will need to use a slightly more 
> complicated system involving an external_acl_type helper as well and 
> switch to an SQL database auth system.
>
>
> The idea for that is that you have a database of authenticated users 
> with their last-known IP address.
>
> ?Your auth_param helper is changed to one which takes client IP 
> address in the auth_param key_extras setting, and adds records to the 
> SQL database before telling Squid the login is OK.
>
> ?Use an ext_sql_session_acl helper which takes IP address and checks 
> the database to find the username who last authenticated from there. 
> This needs to be checked and permit existing sessions before the auth 
> helper.
>
> The config looks something like this:
>
>
> ? external_acl_type ipuser negative_ttl=0 ttl=7200 %<a \
> ??? /usr/bin/squid/ext_sql_session_acl \
> ??? --dsn "..." --user dbUsername --password dbPassword --persist \
> ??? --usercol username --uidcol ipaddress
>
> ? acl user_known external ipuser
> ? http_access allow user_known
>
>
> ? auth_param basic program /path/to/helper
> ? auth_param basic key_extras %<a
> ? auth_param basic credentialsttl 2 hours
>
> ? acl authenticated proxy_auth REQUIRED
>
> ? http_access allow authenticated
>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rafael.akchurin at diladele.com  Sun Mar 21 10:25:53 2021
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sun, 21 Mar 2021 10:25:53 +0000
Subject: [squid-users] Ubuntu 20.04 LTS repository for Squid 4.13-8 (rebuilt
 with sslbump support from sources in Debian unstable)
Message-ID: <AM0PR04MB4753057E3CDF73187600C4BE8F669@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello everyone,

The online repository with latest Squid 4.13-8 (rebuilt from Debian unstable with sslbump support) for Ubuntu 20.04 LTS 64-bit is available at https://squid413-ubuntu20.diladele.com/. Github repo  https://github.com/diladele/squid-ubuntu/tree/master/src/ubuntu20 contains the scripts we used to make this compilation.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add new repo
echo "deb https://squid413-ubuntu20.diladele.com/ubuntu/ focal main" \
    > /etc/apt/sources.list.d/squid413-ubuntu20.diladele.com.list

# and install
apt-get update && apt-get install -y \
    squid-common \
    squid-openssl \
    squidclient \
    libecap3 libecap3-dev

Hope you will find this useful.

Best regards,
Rafael Akchurin
Diladele B.V.

--
The same Squid 4.13 will be part of upcoming Web Safety 7.6 planned for release in June, 2021. This version has the ability to use any header value set by Squid for selecting of web filtering policies in ICAP server as well as other small fixes. Download the latest virtual appliance from https://www.diladele.com/download.html


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210321/a8687909/attachment.htm>

From squid3 at treenet.co.nz  Sun Mar 21 10:51:16 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Mar 2021 23:51:16 +1300
Subject: [squid-users] Protecting squid
In-Reply-To: <cf4376cb-223e-49b0-abd5-7200a4d21a9d@gmail.com>
References: <CADAqQfzMBSFE=8bXOX1LjnCYPqDD3WS9JvmLiUcDmThWto-jGQ@mail.gmail.com>
 <202103111450.25190.Antony.Stone@squid.open.source.it>
 <6ab34342-1b91-8172-34b4-6c2d6bff36de@gmail.com>
 <4ebd18b5-27e2-3f3b-bc8c-1981a79dce08@treenet.co.nz>
 <9fa1e08a-5b75-a139-6cfc-5e7a46b770f2@gmail.com>
 <da3b5157-587b-f284-dfd5-9e2b5c8b4b1a@treenet.co.nz>
 <03dcd53f-7d18-f6cf-516a-f8f25e08713e@gmail.com>
 <4dedac61-60ff-d2cf-3d12-cc7de62b8590@treenet.co.nz>
 <cf4376cb-223e-49b0-abd5-7200a4d21a9d@gmail.com>
Message-ID: <d3c5ad51-f1e8-6981-1c97-977ca92ad49b@treenet.co.nz>

On 18/03/21 2:54 am, Ben Goz wrote:
> Hi Amos,
> 
> Sounds interesting.
> 
> Maybe I should modify the external_acl_type to talk with internal API 
> inside my system.

You do not need to modify any Squid code.

You provide a helper process to translate between Squid APIs and some 
internal system API. see 
<https://wiki.squid-cache.org/Features/AddonHelpers> for details on the 
Squid APIs.

Though as I posted, there are likely already some helpers you can find 
(maybe bundled with Squid) which interface with your internal systems.


Amos


From wangangelo at hotmail.com  Mon Mar 22 14:59:37 2021
From: wangangelo at hotmail.com (Angelo Wang)
Date: Mon, 22 Mar 2021 14:59:37 +0000
Subject: [squid-users] How to automatically Restart Squid on Ubuntu?
Message-ID: <BL0PR04MB475528C934E0E31988714D23DE659@BL0PR04MB4755.namprd04.prod.outlook.com>

Hi,

I have a /22 subnet on a server and sometimes Squid crashes when there are too many connections. Can someone help me create a script/command to automatically restart squid if this happens?

Best,


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210322/bd49879d/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Mar 22 15:19:36 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 22 Mar 2021 16:19:36 +0100
Subject: [squid-users] How to automatically Restart Squid on Ubuntu?
In-Reply-To: <BL0PR04MB475528C934E0E31988714D23DE659@BL0PR04MB4755.namprd04.prod.outlook.com>
References: <BL0PR04MB475528C934E0E31988714D23DE659@BL0PR04MB4755.namprd04.prod.outlook.com>
Message-ID: <202103221619.37165.Antony.Stone@squid.open.source.it>

On Monday 22 March 2021 at 15:59:37, Angelo Wang wrote:

> Hi,
> 
> I have a /22 subnet on a server and sometimes Squid crashes when there are
> too many connections. Can someone help me create a script/command to
> automatically restart squid if this happens?

I would use http://manpages.ubuntu.com/manpages/xenial/man1/monit.1.html

On the other hand, I'd try to identify what's causing the crash and prevent it 
instead.


Antony.

-- 
The best time to plant a tree is 20 years ago.
The second best time is now.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From gkinkie at gmail.com  Mon Mar 22 15:20:21 2021
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 22 Mar 2021 15:20:21 +0000
Subject: [squid-users] How to automatically Restart Squid on Ubuntu?
In-Reply-To: <BL0PR04MB475528C934E0E31988714D23DE659@BL0PR04MB4755.namprd04.prod.outlook.com>
References: <BL0PR04MB475528C934E0E31988714D23DE659@BL0PR04MB4755.namprd04.prod.outlook.com>
Message-ID: <CA+Y8hcOoYCC5XYiz-ep_zFA=WqjEOwgB8HeV+F1U4b8txLuhug@mail.gmail.com>

Hi Angelo,
   Squid shouldn't crash with any number of connections.
Anything in the logs?

On Mon, Mar 22, 2021 at 2:59 PM Angelo Wang <wangangelo at hotmail.com> wrote:

> Hi,
>
> I have a /22 subnet on a server and sometimes Squid crashes when there are
> too many connections. Can someone help me create a script/command to
> automatically restart squid if this happens?
>
> Best,
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210322/b4b031cb/attachment.htm>

From ngtech1ltd at gmail.com  Mon Mar 22 19:05:03 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 22 Mar 2021 21:05:03 +0200
Subject: [squid-users] How to automatically Restart Squid on Ubuntu?
In-Reply-To: <CA+Y8hcOoYCC5XYiz-ep_zFA=WqjEOwgB8HeV+F1U4b8txLuhug@mail.gmail.com>
References: <BL0PR04MB475528C934E0E31988714D23DE659@BL0PR04MB4755.namprd04.prod.outlook.com>
 <CA+Y8hcOoYCC5XYiz-ep_zFA=WqjEOwgB8HeV+F1U4b8txLuhug@mail.gmail.com>
Message-ID: <001801d71f4e$44d63c80$ce82b580$@gmail.com>

It can crash if the memory is low compared to the number of allowed connections in the ulimit.

I don?t know the proxy and the setup but there are couple ways to limit connections per IP if indeed the
proxy is overloaded sometimes by specific users.

 

Angelo, you should really try to verify why is it crashing the proxy.

 

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Francesco Chemolli
Sent: Monday, March 22, 2021 5:20 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] How to automatically Restart Squid on Ubuntu?

 

Hi Angelo,

   Squid shouldn't crash with any number of connections. 

Anything in the logs?

 

On Mon, Mar 22, 2021 at 2:59 PM Angelo Wang <wangangelo at hotmail.com <mailto:wangangelo at hotmail.com> > wrote:

Hi,

 

I have a /22 subnet on a server and sometimes Squid crashes when there are too many connections. Can someone help me create a script/command to automatically restart squid if this happens?

 

Best,

 

 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users




 

-- 

    Francesco

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210322/21aa0088/attachment.htm>

From vigneshisvignesh at gmail.com  Tue Mar 23 06:10:07 2021
From: vigneshisvignesh at gmail.com (Vignesh Ramessh)
Date: Tue, 23 Mar 2021 11:40:07 +0530
Subject: [squid-users] HTTPS caching is not working in squid with ssl-bump
 enabled
Message-ID: <CAGFwki1SF-jj0zGt7C64yYzoTdGkZk2_Q8fRTw95awpiG1uDkg@mail.gmail.com>

Hi Team,

Currently am running squid version 4.14 on RPi3.
Trying to cache https responses with cache-control:max-age headers
available,
using ssl bump - peek n splice feature with examples available in this link
:- https://elatov.github.io/2019/01/using-squid-to-proxy-ssl-sites/
But the https caching doesnt seem to work, https connections are getting
established as TCP_TUNNEL/200 in squid access logs.
I wasnt able to find any proper documentation on https caching using squid.
Can you kindly help me with this request.

Regards,
Vignesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210323/84888be8/attachment.htm>

From rousskov at measurement-factory.com  Tue Mar 23 14:14:17 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Mar 2021 10:14:17 -0400
Subject: [squid-users] HTTPS caching is not working in squid with
 ssl-bump enabled
In-Reply-To: <CAGFwki1SF-jj0zGt7C64yYzoTdGkZk2_Q8fRTw95awpiG1uDkg@mail.gmail.com>
References: <CAGFwki1SF-jj0zGt7C64yYzoTdGkZk2_Q8fRTw95awpiG1uDkg@mail.gmail.com>
Message-ID: <84d0cdd9-1871-898e-a32f-e65637beaa13@measurement-factory.com>

On 3/23/21 2:10 AM, Vignesh Ramessh wrote:

> Currently am running squid version 4.14 on RPi3.
> Trying to cache https responses with cache-control:max-age headers
> available,
> using ssl bump - peek n splice feature with examples available in this
> link :-?https://elatov.github.io/2019/01/using-squid-to-proxy-ssl-sites/


   ssl_bump peek step1
   ssl_bump bump all
   ssl_bump splice all

AFAICT, the above "bump all during step2" configuration (the last line
does not do anything and should be removed) should bump all traffic. Is
that your configuration?


> But the https caching doesnt seem to work, https connections are getting
> established as TCP_TUNNEL/200 in squid access logs.

Do you see non-CONNECT HTTP traffic (e.g. GET, POST, etc.) in access
log? If not, then Squid is not bumping traffic OR the client does not
like what Squid is doing. Please post your http*_port and ssl_bump
configuration with access.log lines corresponding to a single test
transaction that you think should be bumped.

Also, does the client (e.g. curl, wget, or browser) get an error from
Squid? Does the client display any kind of warning or error at all? What
certificate does the client show for the test connection?


> I wasnt able to find any proper documentation on https caching using squid.

What you call "HTTPS caching" consists of two virtually independent
actions: Bumping HTTPS connections and caching. Documentation exists for
each action. Currently, it sounds like the first action (bumping) is not
working in your setup. Until that is addressed, you can ignore the
caching part.


HTH,

Alex.


From vigneshisvignesh at gmail.com  Wed Mar 24 10:11:01 2021
From: vigneshisvignesh at gmail.com (Vignesh Ramessh)
Date: Wed, 24 Mar 2021 15:41:01 +0530
Subject: [squid-users] HTTPS caching is not working in squid with
 ssl-bump enabled
In-Reply-To: <84d0cdd9-1871-898e-a32f-e65637beaa13@measurement-factory.com>
References: <CAGFwki1SF-jj0zGt7C64yYzoTdGkZk2_Q8fRTw95awpiG1uDkg@mail.gmail.com>
 <84d0cdd9-1871-898e-a32f-e65637beaa13@measurement-factory.com>
Message-ID: <CAGFwki1KJpL6P+WKRHKgs7ks6Zpg4wuMKWEL89V9G4jf-NOaUA@mail.gmail.com>

Hi Alex,

We have just started to integrate squid proxy in our project, thanks for
your reply and support.

Previously we were seeing TCP_TUNNEL for https://www.google.com.
Now, we are able to see the TCP_MISS transactions for https://www.google.com
.
As https://www.google.com does not contain a cache-control header in
response, the response cannot be cached which we are aware of.


1616580079.857     73 ::1 NONE/200 0 CONNECT www.google.com:443 -
HIER_DIRECT/172.217.163.68 -
1616580079.945     44 ::1 TCP_MISS/200 967 HEAD https://www.google.com/ -
HIER_DIRECT/172.217.163.68 text/html

I generated key.pem and cert.pem files using openssl and when i tried to
connect a local python https web server which has cache-control headers in
https response, got the below error,

root at raspberrypi-rdk-hybrid:~# curl -I https://192.168.1.41:443
--proxy-cacert cert.pem --proxy http://localhost:3128
HTTP/1.1 200 Connection established

curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.

Have attached the squid.conf file for your reference. Can you please check
and let me know if I am missing something ?

Regards,
Vignesh

On Tue, Mar 23, 2021 at 7:44 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 3/23/21 2:10 AM, Vignesh Ramessh wrote:
>
> > Currently am running squid version 4.14 on RPi3.
> > Trying to cache https responses with cache-control:max-age headers
> > available,
> > using ssl bump - peek n splice feature with examples available in this
> > link :- https://elatov.github.io/2019/01/using-squid-to-proxy-ssl-sites/
>
>
>    ssl_bump peek step1
>    ssl_bump bump all
>    ssl_bump splice all
>
> AFAICT, the above "bump all during step2" configuration (the last line
> does not do anything and should be removed) should bump all traffic. Is
> that your configuration?
>
>
> > But the https caching doesnt seem to work, https connections are getting
> > established as TCP_TUNNEL/200 in squid access logs.
>
> Do you see non-CONNECT HTTP traffic (e.g. GET, POST, etc.) in access
> log? If not, then Squid is not bumping traffic OR the client does not
> like what Squid is doing. Please post your http*_port and ssl_bump
> configuration with access.log lines corresponding to a single test
> transaction that you think should be bumped.
>
> Also, does the client (e.g. curl, wget, or browser) get an error from
> Squid? Does the client display any kind of warning or error at all? What
> certificate does the client show for the test connection?
>
>
> > I wasnt able to find any proper documentation on https caching using
> squid.
>
> What you call "HTTPS caching" consists of two virtually independent
> actions: Bumping HTTPS connections and caching. Documentation exists for
> each action. Currently, it sounds like the first action (bumping) is not
> working in your setup. Until that is addressed, you can ignore the
> caching part.
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210324/9629f4d1/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 2989 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210324/9629f4d1/attachment.obj>

From squid3 at treenet.co.nz  Wed Mar 24 13:26:53 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Mar 2021 02:26:53 +1300
Subject: [squid-users] HTTPS caching is not working in squid with
 ssl-bump enabled
In-Reply-To: <CAGFwki1KJpL6P+WKRHKgs7ks6Zpg4wuMKWEL89V9G4jf-NOaUA@mail.gmail.com>
References: <CAGFwki1SF-jj0zGt7C64yYzoTdGkZk2_Q8fRTw95awpiG1uDkg@mail.gmail.com>
 <84d0cdd9-1871-898e-a32f-e65637beaa13@measurement-factory.com>
 <CAGFwki1KJpL6P+WKRHKgs7ks6Zpg4wuMKWEL89V9G4jf-NOaUA@mail.gmail.com>
Message-ID: <aea9ef76-e029-9c0d-fde0-cbd640f13115@treenet.co.nz>

On 24/03/21 11:11 pm, Vignesh Ramessh wrote:
> Hi Alex,
> 
> We have just started to integrate squid proxy in our project, thanks for 
> your reply and support.
> 
> Previously we were seeing TCP_TUNNEL for https://www.google.com 
> <https://www.google.com>.
> Now, we are able to see the TCP_MISS transactions for 
> https://www.google.com <https://www.google.com>.

> As https://www.google.com <https://www.google.com> does not contain a 
> cache-control header in response, the response cannot be cached which we 
> are aware of.
> 

That is not true. The request method is primarily what determines 
cacheability. Other things like Cache-Control only modify what the 
method implies.


Amos


From rousskov at measurement-factory.com  Wed Mar 24 14:17:55 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 24 Mar 2021 10:17:55 -0400
Subject: [squid-users] HTTPS caching is not working in squid with
 ssl-bump enabled
In-Reply-To: <CAGFwki1KJpL6P+WKRHKgs7ks6Zpg4wuMKWEL89V9G4jf-NOaUA@mail.gmail.com>
References: <CAGFwki1SF-jj0zGt7C64yYzoTdGkZk2_Q8fRTw95awpiG1uDkg@mail.gmail.com>
 <84d0cdd9-1871-898e-a32f-e65637beaa13@measurement-factory.com>
 <CAGFwki1KJpL6P+WKRHKgs7ks6Zpg4wuMKWEL89V9G4jf-NOaUA@mail.gmail.com>
Message-ID: <52eca759-0c91-c77a-3bf3-2ac6aa7347d2@measurement-factory.com>

On 3/24/21 6:11 AM, Vignesh Ramessh wrote:


> Previously we were seeing TCP_TUNNEL for https://www.google.com
> Now, we are able to see the TCP_MISS transactions for
> https://www.google.com <https://www.google.com>.?
> As?https://www.google.com <https://www.google.com> does not contain a
> cache-control header in response, the response cannot be cached which we
> are aware of.

Well, the Cache-Control header is not required for responses to be
cachable (and, depending on the value, may even prevent caching) -- a
lot of factors go into that decision.


> 1616580079.857 ? ? 73 ::1 NONE/200 0 CONNECT www.google.com:443
> <http://www.google.com:443> - HIER_DIRECT/172.217.163.68
> <http://172.217.163.68> -
> 1616580079.945 ? ? 44 ::1 TCP_MISS/200 967 HEAD https://www.google.com/
> <https://www.google.com/> - HIER_DIRECT/172.217.163.68
> <http://172.217.163.68> text/html

It looks like your Squid is bumping TLS connections.


> I generated key.pem and cert.pem files using openssl and when i tried to
> connect a local python https web server which has cache-control headers
> in https response, got the below error,
> 
> root at raspberrypi-rdk-hybrid:~# curl -I https://192.168.1.41:443
> --proxy-cacert cert.pem --proxy http://localhost:3128

Your are telling curl to connect to a plain HTTP proxy (no encryption)
so --proxy-cacert does not apply to this transaction. The insides of the
CONNECT tunnel will be encrypted, of course, but those insides are meant
for the origin server, not the proxy.

For curl to trust a Squid-bumped connection, you want to specify
--cacert (or equivalent), and the file you specify must match the CA
certificate that Squid uses to generate fake certificates --
squid-ca-cert-key.pem in your squid.conf.


HTH,

Alex.


> HTTP/1.1 200 Connection established
> 
> curl: (60) SSL certificate problem: unable to get local issuer certificate
> More details here: https://curl.haxx.se/docs/sslcerts.html
> <https://curl.haxx.se/docs/sslcerts.html>
> 
> curl failed to verify the legitimacy of the server and therefore could not
> establish a secure connection to it. To learn more about this situation and
> how to fix it, please visit the web page mentioned above.
> 
> Have attached the squid.conf file for your reference. Can you please
> check and let me know if I am missing something ?


> On Tue, Mar 23, 2021 at 7:44 PM Alex Rousskov wrote:
> 
>     On 3/23/21 2:10 AM, Vignesh Ramessh wrote:
> 
>     > Currently am running squid version 4.14 on RPi3.
>     > Trying to cache https responses with cache-control:max-age headers
>     > available,
>     > using ssl bump - peek n splice feature with examples available in this
>     > link
>     :-?https://elatov.github.io/2019/01/using-squid-to-proxy-ssl-sites/
>     <https://elatov.github.io/2019/01/using-squid-to-proxy-ssl-sites/>
> 
> 
>     ? ?ssl_bump peek step1
>     ? ?ssl_bump bump all
>     ? ?ssl_bump splice all
> 
>     AFAICT, the above "bump all during step2" configuration (the last line
>     does not do anything and should be removed) should bump all traffic. Is
>     that your configuration?
> 
> 
>     > But the https caching doesnt seem to work, https connections are
>     getting
>     > established as TCP_TUNNEL/200 in squid access logs.
> 
>     Do you see non-CONNECT HTTP traffic (e.g. GET, POST, etc.) in access
>     log? If not, then Squid is not bumping traffic OR the client does not
>     like what Squid is doing. Please post your http*_port and ssl_bump
>     configuration with access.log lines corresponding to a single test
>     transaction that you think should be bumped.
> 
>     Also, does the client (e.g. curl, wget, or browser) get an error from
>     Squid? Does the client display any kind of warning or error at all? What
>     certificate does the client show for the test connection?
> 
> 
>     > I wasnt able to find any proper documentation on https caching
>     using squid.
> 
>     What you call "HTTPS caching" consists of two virtually independent
>     actions: Bumping HTTPS connections and caching. Documentation exists for
>     each action. Currently, it sounds like the first action (bumping) is not
>     working in your setup. Until that is addressed, you can ignore the
>     caching part.
> 
> 
>     HTH,
> 
>     Alex.
> 



From mr.miroslaw.malinowski at gmail.com  Wed Mar 24 16:48:26 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Wed, 24 Mar 2021 16:48:26 +0000
Subject: [squid-users] squid won't return cached even with refresh_pattern
 extra options override-lastmod override-expire ignore-reload
 ignore-no-store ignore-private store-stale
Message-ID: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>

Hi,

Probably, me missing on something silly or it can't be done but I don't
know why but squid won't return the cached version even when I turn all
override options ON in refresh_pattern. It's an API call where we call many
of the same requests and by knowing it we would like to stop those calls to
go out if it's already been sent once.
With debug, I can see the rule is matched and the cache is fresh but still
in access.log is TCP_REFRESH_MODIFIED

squid conf:
refresh_pattern -i <URL> 4320 80% 129600 override-lastmod override-expire
ignore-reload ignore-no-store ignore-private store-stale

curl headers:
curl --insecure --verbose --request GET --url 'URL' >/dev/null
* TCP_NODELAY set
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
 CApath: /etc/ssl/certs
} [5 bytes data]
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
} [512 bytes data]
* TLSv1.3 (IN), TLS handshake, Server hello (2):
{ [122 bytes data]
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
{ [6 bytes data]
* TLSv1.3 (IN), TLS handshake, Certificate (11):
{ [1956 bytes data]
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
{ [78 bytes data]
* TLSv1.3 (IN), TLS handshake, Finished (20):
{ [52 bytes data]
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
} [1 bytes data]
* TLSv1.3 (OUT), TLS handshake, Finished (20):
} [52 bytes data]
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384

> GET URL HTTP/1.1
> Host: URL
> User-Agent: curl/7.68.0
> Accept: */*
>
{ [5 bytes data]
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
{ [217 bytes data]
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
{ [217 bytes data]
* old SSL session ID is stale, removing
{ [5 bytes data]
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Cache-Control: no-cache
< Content-Type: application/json
< X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
< Date: Wed, 24 Mar 2021 15:04:34 GMT
< Server: Google Frontend
< Content-Length: 7950
< X-Cache: MISS from server
< X-Cache-Lookup: HIT from server
< Via: 1.1 server (squid/4.14)
< Connection: keep-alive

access log:
243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL - ORIGINAL_DST/IP
application/json

cache log:
2021-03-24T15:04:34 squid .710 kid1| 11,3| http.cc(982)
haveParsedReplyHeaders: decided: cache positively and share because refresh
check returned cacheable; HTTP status 200 e:=p2V/0x34868914670*3
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(470) refreshCheck:
returning FRESH_MIN_RULE
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(455) refreshCheck:
Object isn't stale..
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(327) refreshCheck:
Staleness = -1
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(199)
refreshStaleness: FRESH: age (60 sec) is less than configured minimum
(259200 sec)
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(166)
refreshStaleness: No explicit expiry given, using heuristics to determine
freshness
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(307) refreshCheck:
entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(305) refreshCheck:
check_time: Wed, 24 Mar 2021 15:05:34 GMT
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(303) refreshCheck:
age: 60
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(301) refreshCheck:
Matched 'URL 259200 80%% 7776000'
2021-03-24T15:04:34 squid .710 kid1| 22,3| refresh.cc(279) refreshCheck:
checking freshness of URI: https://URL
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210324/bbc5f438/attachment.htm>

From rousskov at measurement-factory.com  Wed Mar 24 18:15:19 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 24 Mar 2021 14:15:19 -0400
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
Message-ID: <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>

On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:

> Probably, me missing on something silly or it can't be done but I don't
> know why but squid won't return the cached version even when I turn all
> override options ON in refresh_pattern.

AFAICT, no configuration options that can disable revalidation of
Cache-Control:no-cache responses. refresh_pattern does not have an
(equivalent of) "ignore-no-cache-in-responses" option.

IIRC, older Squids were violating an HTTP MUST by forgetting to
revalidate Cache-Control:no-cache responses, but that was fixed in [1].
Your Squid version has that fix.

[1]
https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa


> With debug, I can see the rule is matched and the cache is fresh but
> still in access.log is TCP_REFRESH_MODIFIED

> 2021-03-24T15:04:34	squid	.710 kid1| 11,3| http.cc(982)
> haveParsedReplyHeaders: decided: cache positively and share because

FYI: You are looking at cache.log lines logged _after_ Squid has already
decided to refresh the cached version. If you want to analyze why Squid
decided to refresh the cached version, you should look _before_ Squid
logged the request to the server (and before any FwdState.cc lines). I
have not checked the details, but I bet that your Squid revalidates
because of Cache-Control:no-cache in the response. Look for "YES: Must
revalidate stale object".


HTH,

Alex.

> squid conf:
> refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
> override-expire ignore-reload ignore-no-store ignore-private store-stale
> 
> curl headers:
> curl --insecure --verbose --request GET --url 'URL' >/dev/null
> * TCP_NODELAY set
> * ALPN, offering h2
> * ALPN, offering http/1.1
> * successfully set certificate verify locations:
> * ??CAfile: /etc/ssl/certs/ca-certificates.crt
> ?CApath: /etc/ssl/certs
> } [5 bytes data]
> * TLSv1.3 (OUT), TLS handshake, Client hello (1):
> } [512 bytes data]
> * TLSv1.3 (IN), TLS handshake, Server hello (2):
> { [122 bytes data]
> * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
> { [6 bytes data]
> * TLSv1.3 (IN), TLS handshake, Certificate (11):
> { [1956 bytes data]
> * TLSv1.3 (IN), TLS handshake, CERT verify (15):
> { [78 bytes data]
> * TLSv1.3 (IN), TLS handshake, Finished (20):
> { [52 bytes data]
> * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
> } [1 bytes data]
> * TLSv1.3 (OUT), TLS handshake, Finished (20):
> } [52 bytes data]
> * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
> 
>> GET URL HTTP/1.1
>> Host: URL
>> User-Agent: curl/7.68.0
>> Accept: */*
>>
> { [5 bytes data]
> * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
> { [217 bytes data]
> * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
> { [217 bytes data]
> * old SSL session ID is stale, removing
> { [5 bytes data]
> * Mark bundle as not supporting multiuse
> < HTTP/1.1 200 OK
> < Cache-Control: no-cache
> < Content-Type: application/json
> < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
> < Date: Wed, 24 Mar 2021 15:04:34 GMT
> < Server: Google Frontend
> < Content-Length: 7950
> < X-Cache: MISS from server
> < X-Cache-Lookup: HIT from server
> < Via: 1.1 server (squid/4.14)
> < Connection: keep-alive
> 
> access log:
> 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
> ORIGINAL_DST/IP application/json
> 
> cache log:
> 2021-03-24T15:04:34	squid	.710 kid1| 11,3| http.cc(982)
> haveParsedReplyHeaders: decided: cache positively and share because
> refresh check returned cacheable; HTTP status 200 e:=p2V/0x34868914670*3	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(470) refreshCheck:
> returning FRESH_MIN_RULE	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(455) refreshCheck:
> Object isn't stale..	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(327) refreshCheck:
> Staleness = -1	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(199)
> refreshStaleness: FRESH: age (60 sec) is less than configured minimum
> (259200 sec)	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(166)
> refreshStaleness: No explicit expiry given, using heuristics to
> determine freshness	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(307) refreshCheck:
> entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(305) refreshCheck:
> check_time: Wed, 24 Mar 2021 15:05:34 GMT	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(303) refreshCheck:
> age: 60	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(301) refreshCheck:
> Matched 'URL 259200 80%% 7776000'	?
> 2021-03-24T15:04:34	squid	.710 kid1| 22,3| refresh.cc(279) refreshCheck:
> checking freshness of URI: https://URL <https://URL>
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From mr.miroslaw.malinowski at gmail.com  Wed Mar 24 18:28:51 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Wed, 24 Mar 2021 18:28:51 +0000
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
 <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
Message-ID: <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>

Hi,

You've right yes it's revalidating as API server I'm requesting data is
setting Cache-Control: no-cache. My question is how I can force squid to
cache and not validate as I know it's safe to do so. As I've explained
earlier we are making the same request and receiving the same response from
100+ server so as to reduce number of requests to the external server we
would like squid to cache the response and issue a cached version.

2021/03/24 18:00:54.867 kid1| 22,3| refresh.cc(351) refreshCheck: YES: Must
revalidate stale object (origin set no-cache or private)

Mirek

On Wed, Mar 24, 2021 at 6:15 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:
>
> > Probably, me missing on something silly or it can't be done but I don't
> > know why but squid won't return the cached version even when I turn all
> > override options ON in refresh_pattern.
>
> AFAICT, no configuration options that can disable revalidation of
> Cache-Control:no-cache responses. refresh_pattern does not have an
> (equivalent of) "ignore-no-cache-in-responses" option.
>
> IIRC, older Squids were violating an HTTP MUST by forgetting to
> revalidate Cache-Control:no-cache responses, but that was fixed in [1].
> Your Squid version has that fix.
>
> [1]
>
> https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>
>
> > With debug, I can see the rule is matched and the cache is fresh but
> > still in access.log is TCP_REFRESH_MODIFIED
>
> > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
> > haveParsedReplyHeaders: decided: cache positively and share because
>
> FYI: You are looking at cache.log lines logged _after_ Squid has already
> decided to refresh the cached version. If you want to analyze why Squid
> decided to refresh the cached version, you should look _before_ Squid
> logged the request to the server (and before any FwdState.cc lines). I
> have not checked the details, but I bet that your Squid revalidates
> because of Cache-Control:no-cache in the response. Look for "YES: Must
> revalidate stale object".
>
>
> HTH,
>
> Alex.
>
> > squid conf:
> > refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
> > override-expire ignore-reload ignore-no-store ignore-private store-stale
> >
> > curl headers:
> > curl --insecure --verbose --request GET --url 'URL' >/dev/null
> > * TCP_NODELAY set
> > * ALPN, offering h2
> > * ALPN, offering http/1.1
> > * successfully set certificate verify locations:
> > *   CAfile: /etc/ssl/certs/ca-certificates.crt
> >  CApath: /etc/ssl/certs
> > } [5 bytes data]
> > * TLSv1.3 (OUT), TLS handshake, Client hello (1):
> > } [512 bytes data]
> > * TLSv1.3 (IN), TLS handshake, Server hello (2):
> > { [122 bytes data]
> > * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
> > { [6 bytes data]
> > * TLSv1.3 (IN), TLS handshake, Certificate (11):
> > { [1956 bytes data]
> > * TLSv1.3 (IN), TLS handshake, CERT verify (15):
> > { [78 bytes data]
> > * TLSv1.3 (IN), TLS handshake, Finished (20):
> > { [52 bytes data]
> > * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
> > } [1 bytes data]
> > * TLSv1.3 (OUT), TLS handshake, Finished (20):
> > } [52 bytes data]
> > * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
> >
> >> GET URL HTTP/1.1
> >> Host: URL
> >> User-Agent: curl/7.68.0
> >> Accept: */*
> >>
> > { [5 bytes data]
> > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
> > { [217 bytes data]
> > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
> > { [217 bytes data]
> > * old SSL session ID is stale, removing
> > { [5 bytes data]
> > * Mark bundle as not supporting multiuse
> > < HTTP/1.1 200 OK
> > < Cache-Control: no-cache
> > < Content-Type: application/json
> > < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
> > < Date: Wed, 24 Mar 2021 15:04:34 GMT
> > < Server: Google Frontend
> > < Content-Length: 7950
> > < X-Cache: MISS from server
> > < X-Cache-Lookup: HIT from server
> > < Via: 1.1 server (squid/4.14)
> > < Connection: keep-alive
> >
> > access log:
> > 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
> > ORIGINAL_DST/IP application/json
> >
> > cache log:
> > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
> > haveParsedReplyHeaders: decided: cache positively and share because
> > refresh check returned cacheable; HTTP status 200
> e:=p2V/0x34868914670*3
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(470)
> refreshCheck:
> > returning FRESH_MIN_RULE
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(455)
> refreshCheck:
> > Object isn't stale..
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(327)
> refreshCheck:
> > Staleness = -1
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(199)
> > refreshStaleness: FRESH: age (60 sec) is less than configured minimum
> > (259200 sec)
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(166)
> > refreshStaleness: No explicit expiry given, using heuristics to
> > determine freshness
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(307)
> refreshCheck:
> > entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(305)
> refreshCheck:
> > check_time: Wed, 24 Mar 2021 15:05:34 GMT
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(303)
> refreshCheck:
> > age: 60
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(301)
> refreshCheck:
> > Matched 'URL 259200 80%% 7776000'
> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(279)
> refreshCheck:
> > checking freshness of URI: https://URL <https://URL>
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210324/3367f87f/attachment.htm>

From mr.miroslaw.malinowski at gmail.com  Wed Mar 24 18:49:35 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Wed, 24 Mar 2021 18:49:35 +0000
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
 <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
 <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>
Message-ID: <CAJtdwuuR3+Qj7KG_Ycyvo=nkQ8H4th0o=OSG-F0uUPqN5CUDaA@mail.gmail.com>

I've probably replied to quickly thinking there is a way to do it. So
looking at the code and reading carefully your response, you're saying
there is no way you can do it with squid.

Mirek

On Wed, Mar 24, 2021 at 6:28 PM Miroslaw Malinowski <
mr.miroslaw.malinowski at gmail.com> wrote:

> Hi,
>
> You've right yes it's revalidating as API server I'm requesting data is
> setting Cache-Control: no-cache. My question is how I can force squid to
> cache and not validate as I know it's safe to do so. As I've explained
> earlier we are making the same request and receiving the same response from
> 100+ server so as to reduce number of requests to the external server we
> would like squid to cache the response and issue a cached version.
>
> 2021/03/24 18:00:54.867 kid1| 22,3| refresh.cc(351) refreshCheck: YES:
> Must revalidate stale object (origin set no-cache or private)
>
> Mirek
>
> On Wed, Mar 24, 2021 at 6:15 PM Alex Rousskov <
> rousskov at measurement-factory.com> wrote:
>
>> On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:
>>
>> > Probably, me missing on something silly or it can't be done but I don't
>> > know why but squid won't return the cached version even when I turn all
>> > override options ON in refresh_pattern.
>>
>> AFAICT, no configuration options that can disable revalidation of
>> Cache-Control:no-cache responses. refresh_pattern does not have an
>> (equivalent of) "ignore-no-cache-in-responses" option.
>>
>> IIRC, older Squids were violating an HTTP MUST by forgetting to
>> revalidate Cache-Control:no-cache responses, but that was fixed in [1].
>> Your Squid version has that fix.
>>
>> [1]
>>
>> https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>>
>>
>> > With debug, I can see the rule is matched and the cache is fresh but
>> > still in access.log is TCP_REFRESH_MODIFIED
>>
>> > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
>> > haveParsedReplyHeaders: decided: cache positively and share because
>>
>> FYI: You are looking at cache.log lines logged _after_ Squid has already
>> decided to refresh the cached version. If you want to analyze why Squid
>> decided to refresh the cached version, you should look _before_ Squid
>> logged the request to the server (and before any FwdState.cc lines). I
>> have not checked the details, but I bet that your Squid revalidates
>> because of Cache-Control:no-cache in the response. Look for "YES: Must
>> revalidate stale object".
>>
>>
>> HTH,
>>
>> Alex.
>>
>> > squid conf:
>> > refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
>> > override-expire ignore-reload ignore-no-store ignore-private store-stale
>> >
>> > curl headers:
>> > curl --insecure --verbose --request GET --url 'URL' >/dev/null
>> > * TCP_NODELAY set
>> > * ALPN, offering h2
>> > * ALPN, offering http/1.1
>> > * successfully set certificate verify locations:
>> > *   CAfile: /etc/ssl/certs/ca-certificates.crt
>> >  CApath: /etc/ssl/certs
>> > } [5 bytes data]
>> > * TLSv1.3 (OUT), TLS handshake, Client hello (1):
>> > } [512 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, Server hello (2):
>> > { [122 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
>> > { [6 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, Certificate (11):
>> > { [1956 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, CERT verify (15):
>> > { [78 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, Finished (20):
>> > { [52 bytes data]
>> > * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
>> > } [1 bytes data]
>> > * TLSv1.3 (OUT), TLS handshake, Finished (20):
>> > } [52 bytes data]
>> > * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
>> >
>> >> GET URL HTTP/1.1
>> >> Host: URL
>> >> User-Agent: curl/7.68.0
>> >> Accept: */*
>> >>
>> > { [5 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>> > { [217 bytes data]
>> > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>> > { [217 bytes data]
>> > * old SSL session ID is stale, removing
>> > { [5 bytes data]
>> > * Mark bundle as not supporting multiuse
>> > < HTTP/1.1 200 OK
>> > < Cache-Control: no-cache
>> > < Content-Type: application/json
>> > < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
>> > < Date: Wed, 24 Mar 2021 15:04:34 GMT
>> > < Server: Google Frontend
>> > < Content-Length: 7950
>> > < X-Cache: MISS from server
>> > < X-Cache-Lookup: HIT from server
>> > < Via: 1.1 server (squid/4.14)
>> > < Connection: keep-alive
>> >
>> > access log:
>> > 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
>> > ORIGINAL_DST/IP application/json
>> >
>> > cache log:
>> > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
>> > haveParsedReplyHeaders: decided: cache positively and share because
>> > refresh check returned cacheable; HTTP status 200
>> e:=p2V/0x34868914670*3
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(470)
>> refreshCheck:
>> > returning FRESH_MIN_RULE
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(455)
>> refreshCheck:
>> > Object isn't stale..
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(327)
>> refreshCheck:
>> > Staleness = -1
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(199)
>> > refreshStaleness: FRESH: age (60 sec) is less than configured minimum
>> > (259200 sec)
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(166)
>> > refreshStaleness: No explicit expiry given, using heuristics to
>> > determine freshness
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(307)
>> refreshCheck:
>> > entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(305)
>> refreshCheck:
>> > check_time: Wed, 24 Mar 2021 15:05:34 GMT
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(303)
>> refreshCheck:
>> > age: 60
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(301)
>> refreshCheck:
>> > Matched 'URL 259200 80%% 7776000'
>> > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(279)
>> refreshCheck:
>> > checking freshness of URI: https://URL <https://URL>
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210324/b78bb915/attachment.htm>

From rousskov at measurement-factory.com  Wed Mar 24 19:11:45 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 24 Mar 2021 15:11:45 -0400
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <CAJtdwuuR3+Qj7KG_Ycyvo=nkQ8H4th0o=OSG-F0uUPqN5CUDaA@mail.gmail.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
 <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
 <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>
 <CAJtdwuuR3+Qj7KG_Ycyvo=nkQ8H4th0o=OSG-F0uUPqN5CUDaA@mail.gmail.com>
Message-ID: <9d9dcec6-6134-964a-5198-7a7623b3049a@measurement-factory.com>

On 3/24/21 2:49 PM, Miroslaw Malinowski wrote:

> looking at the code and reading carefully your response, you're saying
> there is no way you can do it with squid.

With Squid, your options include:

1. Squid source code changes. Should not be too difficult and, IMO, a
high-quality implementation would deserve official acceptance because it
is a generally useful feature in line with existing control knobs.
https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

2. An adaptation service that removes Cache-Control:no-cache from the
response before Squid processes it:
https://wiki.squid-cache.org/SquidFaq/ContentAdaptation


HTH,

Alex.

> On Wed, Mar 24, 2021 at 6:28 PM Miroslaw Malinowski wrote:
> 
>     Hi,
> 
>     You've right yes it's revalidating as API server I'm requesting data
>     is setting Cache-Control: no-cache. My question is how I can force
>     squid to cache and not validate as I know it's safe to do so. As
>     I've explained earlier we are making the same request and receiving
>     the same response from 100+ server so as to reduce number of
>     requests to the external server we would like squid to cache the
>     response and issue a cached version.
> 
>     2021/03/24 18:00:54.867 kid1| 22,3| refresh.cc(351) refreshCheck:
>     YES: Must revalidate stale object (origin set no-cache or private)
> 
>     Mirek
> 
>     On Wed, Mar 24, 2021 at 6:15 PM Alex Rousskov
>     <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>> wrote:
> 
>         On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:
> 
>         > Probably, me missing on something silly or it can't be done
>         but I don't
>         > know why but squid won't return the cached version even when I
>         turn all
>         > override options ON in refresh_pattern.
> 
>         AFAICT, no configuration options that can disable revalidation of
>         Cache-Control:no-cache responses. refresh_pattern does not have an
>         (equivalent of) "ignore-no-cache-in-responses" option.
> 
>         IIRC, older Squids were violating an HTTP MUST by forgetting to
>         revalidate Cache-Control:no-cache responses, but that was fixed
>         in [1].
>         Your Squid version has that fix.
> 
>         [1]
>         https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>         <https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa>
> 
> 
>         > With debug, I can see the rule is matched and the cache is
>         fresh but
>         > still in access.log is TCP_REFRESH_MODIFIED
> 
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 11,3| http.cc(982)
>         > haveParsedReplyHeaders: decided: cache positively and share
>         because
> 
>         FYI: You are looking at cache.log lines logged _after_ Squid has
>         already
>         decided to refresh the cached version. If you want to analyze
>         why Squid
>         decided to refresh the cached version, you should look _before_
>         Squid
>         logged the request to the server (and before any FwdState.cc
>         lines). I
>         have not checked the details, but I bet that your Squid revalidates
>         because of Cache-Control:no-cache in the response. Look for
>         "YES: Must
>         revalidate stale object".
> 
> 
>         HTH,
> 
>         Alex.
> 
>         > squid conf:
>         > refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
>         > override-expire ignore-reload ignore-no-store ignore-private
>         store-stale
>         >
>         > curl headers:
>         > curl --insecure --verbose --request GET --url 'URL' >/dev/null
>         > * TCP_NODELAY set
>         > * ALPN, offering h2
>         > * ALPN, offering http/1.1
>         > * successfully set certificate verify locations:
>         > * ??CAfile: /etc/ssl/certs/ca-certificates.crt
>         > ?CApath: /etc/ssl/certs
>         > } [5 bytes data]
>         > * TLSv1.3 (OUT), TLS handshake, Client hello (1):
>         > } [512 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, Server hello (2):
>         > { [122 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
>         > { [6 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, Certificate (11):
>         > { [1956 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, CERT verify (15):
>         > { [78 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, Finished (20):
>         > { [52 bytes data]
>         > * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
>         > } [1 bytes data]
>         > * TLSv1.3 (OUT), TLS handshake, Finished (20):
>         > } [52 bytes data]
>         > * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
>         >
>         >> GET URL HTTP/1.1
>         >> Host: URL
>         >> User-Agent: curl/7.68.0
>         >> Accept: */*
>         >>
>         > { [5 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>         > { [217 bytes data]
>         > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>         > { [217 bytes data]
>         > * old SSL session ID is stale, removing
>         > { [5 bytes data]
>         > * Mark bundle as not supporting multiuse
>         > < HTTP/1.1 200 OK
>         > < Cache-Control: no-cache
>         > < Content-Type: application/json
>         > < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
>         > < Date: Wed, 24 Mar 2021 15:04:34 GMT
>         > < Server: Google Frontend
>         > < Content-Length: 7950
>         > < X-Cache: MISS from server
>         > < X-Cache-Lookup: HIT from server
>         > < Via: 1.1 server (squid/4.14)
>         > < Connection: keep-alive
>         >
>         > access log:
>         > 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
>         > ORIGINAL_DST/IP application/json
>         >
>         > cache log:
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 11,3| http.cc(982)
>         > haveParsedReplyHeaders: decided: cache positively and share
>         because
>         > refresh check returned cacheable; HTTP status 200
>         e:=p2V/0x34868914670*3? ? ? ?
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(470)
>         refreshCheck:
>         > returning FRESH_MIN_RULE? ? ? ?
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(455)
>         refreshCheck:
>         > Object isn't stale..? ?
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(327)
>         refreshCheck:
>         > Staleness = -1? ? ? ? ?
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(199)
>         > refreshStaleness: FRESH: age (60 sec) is less than configured
>         minimum
>         > (259200 sec)? ?
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(166)
>         > refreshStaleness: No explicit expiry given, using heuristics to
>         > determine freshness? ??
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(307)
>         refreshCheck:
>         > entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT? ? ? ??
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(305)
>         refreshCheck:
>         > check_time: Wed, 24 Mar 2021 15:05:34 GMT? ? ??
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(303)
>         refreshCheck:
>         > age: 60? ? ? ??
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(301)
>         refreshCheck:
>         > Matched 'URL 259200 80%% 7776000'? ? ??
>         > 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3| refresh.cc(279)
>         refreshCheck:
>         > checking freshness of URI: https://URL <https://URL>
>         <https://URL <https://URL>>
>         >
>         >
>         > _______________________________________________
>         > squid-users mailing list
>         > squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         > http://lists.squid-cache.org/listinfo/squid-users
>         <http://lists.squid-cache.org/listinfo/squid-users>
>         >
> 



From mr.miroslaw.malinowski at gmail.com  Wed Mar 24 19:34:30 2021
From: mr.miroslaw.malinowski at gmail.com (Miroslaw Malinowski)
Date: Wed, 24 Mar 2021 19:34:30 +0000
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <9d9dcec6-6134-964a-5198-7a7623b3049a@measurement-factory.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
 <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
 <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>
 <CAJtdwuuR3+Qj7KG_Ycyvo=nkQ8H4th0o=OSG-F0uUPqN5CUDaA@mail.gmail.com>
 <9d9dcec6-6134-964a-5198-7a7623b3049a@measurement-factory.com>
Message-ID: <CAJtdwuvQ-Xa11MHRYxp+yq1UViS4NhUQxda2523BK=cHHJZyMQ@mail.gmail.com>

I thought about upper service but as is not required at the moment,
introducing extra hop just to remove the header looks a bit like a hammer
approach. I'll look into how easily I can amend the code as the other
option is to introduce a proxy like a feature to the application, so either
way, it is a code change. The only problem here is that it's an OPNSense
squid service so I have to compile from source on BSD and then keep adding
in manually each time they do the update.

Mirek

On Wed, Mar 24, 2021 at 7:11 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 3/24/21 2:49 PM, Miroslaw Malinowski wrote:
>
> > looking at the code and reading carefully your response, you're saying
> > there is no way you can do it with squid.
>
> With Squid, your options include:
>
> 1. Squid source code changes. Should not be too difficult and, IMO, a
> high-quality implementation would deserve official acceptance because it
> is a generally useful feature in line with existing control knobs.
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> 2. An adaptation service that removes Cache-Control:no-cache from the
> response before Squid processes it:
> https://wiki.squid-cache.org/SquidFaq/ContentAdaptation
>
>
> HTH,
>
> Alex.
>
> > On Wed, Mar 24, 2021 at 6:28 PM Miroslaw Malinowski wrote:
> >
> >     Hi,
> >
> >     You've right yes it's revalidating as API server I'm requesting data
> >     is setting Cache-Control: no-cache. My question is how I can force
> >     squid to cache and not validate as I know it's safe to do so. As
> >     I've explained earlier we are making the same request and receiving
> >     the same response from 100+ server so as to reduce number of
> >     requests to the external server we would like squid to cache the
> >     response and issue a cached version.
> >
> >     2021/03/24 18:00:54.867 kid1| 22,3| refresh.cc(351) refreshCheck:
> >     YES: Must revalidate stale object (origin set no-cache or private)
> >
> >     Mirek
> >
> >     On Wed, Mar 24, 2021 at 6:15 PM Alex Rousskov
> >     <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>> wrote:
> >
> >         On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:
> >
> >         > Probably, me missing on something silly or it can't be done
> >         but I don't
> >         > know why but squid won't return the cached version even when I
> >         turn all
> >         > override options ON in refresh_pattern.
> >
> >         AFAICT, no configuration options that can disable revalidation of
> >         Cache-Control:no-cache responses. refresh_pattern does not have
> an
> >         (equivalent of) "ignore-no-cache-in-responses" option.
> >
> >         IIRC, older Squids were violating an HTTP MUST by forgetting to
> >         revalidate Cache-Control:no-cache responses, but that was fixed
> >         in [1].
> >         Your Squid version has that fix.
> >
> >         [1]
> >
> https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
> >         <
> https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
> >
> >
> >
> >         > With debug, I can see the rule is matched and the cache is
> >         fresh but
> >         > still in access.log is TCP_REFRESH_MODIFIED
> >
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
> >         > haveParsedReplyHeaders: decided: cache positively and share
> >         because
> >
> >         FYI: You are looking at cache.log lines logged _after_ Squid has
> >         already
> >         decided to refresh the cached version. If you want to analyze
> >         why Squid
> >         decided to refresh the cached version, you should look _before_
> >         Squid
> >         logged the request to the server (and before any FwdState.cc
> >         lines). I
> >         have not checked the details, but I bet that your Squid
> revalidates
> >         because of Cache-Control:no-cache in the response. Look for
> >         "YES: Must
> >         revalidate stale object".
> >
> >
> >         HTH,
> >
> >         Alex.
> >
> >         > squid conf:
> >         > refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
> >         > override-expire ignore-reload ignore-no-store ignore-private
> >         store-stale
> >         >
> >         > curl headers:
> >         > curl --insecure --verbose --request GET --url 'URL' >/dev/null
> >         > * TCP_NODELAY set
> >         > * ALPN, offering h2
> >         > * ALPN, offering http/1.1
> >         > * successfully set certificate verify locations:
> >         > *   CAfile: /etc/ssl/certs/ca-certificates.crt
> >         >  CApath: /etc/ssl/certs
> >         > } [5 bytes data]
> >         > * TLSv1.3 (OUT), TLS handshake, Client hello (1):
> >         > } [512 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, Server hello (2):
> >         > { [122 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
> >         > { [6 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, Certificate (11):
> >         > { [1956 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, CERT verify (15):
> >         > { [78 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, Finished (20):
> >         > { [52 bytes data]
> >         > * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
> >         > } [1 bytes data]
> >         > * TLSv1.3 (OUT), TLS handshake, Finished (20):
> >         > } [52 bytes data]
> >         > * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
> >         >
> >         >> GET URL HTTP/1.1
> >         >> Host: URL
> >         >> User-Agent: curl/7.68.0
> >         >> Accept: */*
> >         >>
> >         > { [5 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
> >         > { [217 bytes data]
> >         > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
> >         > { [217 bytes data]
> >         > * old SSL session ID is stale, removing
> >         > { [5 bytes data]
> >         > * Mark bundle as not supporting multiuse
> >         > < HTTP/1.1 200 OK
> >         > < Cache-Control: no-cache
> >         > < Content-Type: application/json
> >         > < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
> >         > < Date: Wed, 24 Mar 2021 15:04:34 GMT
> >         > < Server: Google Frontend
> >         > < Content-Length: 7950
> >         > < X-Cache: MISS from server
> >         > < X-Cache-Lookup: HIT from server
> >         > < Via: 1.1 server (squid/4.14)
> >         > < Connection: keep-alive
> >         >
> >         > access log:
> >         > 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
> >         > ORIGINAL_DST/IP application/json
> >         >
> >         > cache log:
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
> >         > haveParsedReplyHeaders: decided: cache positively and share
> >         because
> >         > refresh check returned cacheable; HTTP status 200
> >         e:=p2V/0x34868914670*3
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(470)
> >         refreshCheck:
> >         > returning FRESH_MIN_RULE
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(455)
> >         refreshCheck:
> >         > Object isn't stale..
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(327)
> >         refreshCheck:
> >         > Staleness = -1
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(199)
> >         > refreshStaleness: FRESH: age (60 sec) is less than configured
> >         minimum
> >         > (259200 sec)
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(166)
> >         > refreshStaleness: No explicit expiry given, using heuristics to
> >         > determine freshness
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(307)
> >         refreshCheck:
> >         > entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(305)
> >         refreshCheck:
> >         > check_time: Wed, 24 Mar 2021 15:05:34 GMT
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(303)
> >         refreshCheck:
> >         > age: 60
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(301)
> >         refreshCheck:
> >         > Matched 'URL 259200 80%% 7776000'
> >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3| refresh.cc(279)
> >         refreshCheck:
> >         > checking freshness of URI: https://URL <https://URL>
> >         <https://URL <https://URL>>
> >         >
> >         >
> >         > _______________________________________________
> >         > squid-users mailing list
> >         > squid-users at lists.squid-cache.org
> >         <mailto:squid-users at lists.squid-cache.org>
> >         > http://lists.squid-cache.org/listinfo/squid-users
> >         <http://lists.squid-cache.org/listinfo/squid-users>
> >         >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210324/1964ce31/attachment.htm>

From mauamend at tiscali.it  Thu Mar 25 11:53:09 2021
From: mauamend at tiscali.it (maurizio)
Date: Thu, 25 Mar 2021 12:53:09 +0100
Subject: [squid-users] squid ftp list files problem
Message-ID: <111a2d1b2b8237d6b0a9aa40660ab68e@tiscali.it>

  
Hello 
I have a squid 4.14 version installed recently. I have a
problem when we use that like ftp proxy(via port 21): when a client use
that and try to 
use the ftp command ls(list) in a directory with a lot
files (in my test 250 files) the list command freezing(list partial list
files). 
I have tried with a very old proxy ftp(frox) and the list
command doesn't free, it's return the list correctly. 
Please, is it a
bug or misconfiguration? 
Thank you 
Mau  


Con Tiscali Mobile Smart 30 4G hai minuti illimitati, 100 SMS e 30 Giga in 4G a soli 8,99? al mese. http://tisca.li/smart30

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210325/9a80c182/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Mar 25 12:01:19 2021
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 25 Mar 2021 13:01:19 +0100
Subject: [squid-users] squid ftp list files problem
In-Reply-To: <111a2d1b2b8237d6b0a9aa40660ab68e@tiscali.it>
References: <111a2d1b2b8237d6b0a9aa40660ab68e@tiscali.it>
Message-ID: <202103251301.19455.Antony.Stone@squid.open.source.it>

On Thursday 25 March 2021 at 12:53:09, maurizio wrote:

> Hello
> I have a squid 4.14 version installed recently. I have a problem when we use
> that like ftp proxy(via port 21): when a client use that and try to use the
> ftp command ls(list) in a directory with a lot files (in my test 250 files)
> the list command freezing(list partial list files).

Is this passive or active FTP?

> I have tried with a very old proxy ftp(frox) and the list command doesn't
> free, it's return the list correctly.
> Please, is it a bug or misconfiguration?

1. We don't know whether it's a misconfiguration until you show us your 
configuration.

2. What is displayed in Squid's log files at the time this occurs?


Antony.

-- 
What do you call a dinosaur with only one eye?  A Doyouthinkesaurus.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From uhlar at fantomas.sk  Thu Mar 25 12:36:25 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 25 Mar 2021 13:36:25 +0100
Subject: [squid-users] squid ftp list files problem
In-Reply-To: <202103251301.19455.Antony.Stone@squid.open.source.it>
References: <111a2d1b2b8237d6b0a9aa40660ab68e@tiscali.it>
 <202103251301.19455.Antony.Stone@squid.open.source.it>
Message-ID: <20210325123625.GA939@fantomas.sk>

>On Thursday 25 March 2021 at 12:53:09, maurizio wrote:
>> I have a squid 4.14 version installed recently. I have a problem when we use
>> that like ftp proxy(via port 21): when a client use that and try to use the
>> ftp command ls(list) in a directory with a lot files (in my test 250 files)
>> the list command freezing(list partial list files).

On 25.03.21 13:01, Antony Stone wrote:
>Is this passive or active FTP?

yes, this can be passive/port FTP issue.
It also can be an issue of PASV/EPSV and PORT/EPTR

on one proxy I've had this error I configured:

acl epsv_fail dst "/etc/squid/epsv-fail-servers"
ftp_epsv deny epsv_fail
ftp_epsv allow all

>> I have tried with a very old proxy ftp(frox) and the list command doesn't
>> free, it's return the list correctly.
>> Please, is it a bug or misconfiguration?
>
>1. We don't know whether it's a misconfiguration until you show us your
>configuration.
>
>2. What is displayed in Squid's log files at the time this occurs?
>
>
>Antony.
>
>-- 
>What do you call a dinosaur with only one eye?  A Doyouthinkesaurus.
>
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I don't have lysdexia. The Dog wouldn't allow that.


From moberger at metanetworks.com  Thu Mar 25 13:06:41 2021
From: moberger at metanetworks.com (Moti Berger)
Date: Thu, 25 Mar 2021 15:06:41 +0200
Subject: [squid-users] X-Next-Services
Message-ID: <CAGSk-40jH5BUhJYw-RKj000quZ3By2Q23jyUYB4q2uuTdA7dMw@mail.gmail.com>

Hi

I want to be able to skip all subsequent ICAP servers defined in squid
based on some logic I have in one of my ICAP servers.
I used the X-Next-Services and it seems to control only the current ICAP
chain.
I also saw it while configuring two ICAP servers to handle requests and one
ICAP server to handle responses. When the header was sent from the first
ICAP on the request chain, the ICAP request didn't arrive at the second
ICAP server handling requests but did get to the ICAP server that handles
responses. I wish to also skip the ICAP which handles the responses.
Is that possible?

If not, is it possible for an ICAP server to add an ICAP header during
request handling to be read by the other ICAP servers that come after it
when they handle the same request or the same request's response?
I'm aware I can add a header to the encapsulated HTTP request, but this is
something I want to avoid because I don't want to have a performance hit
because I need to copy all HTTP request bodies for this header (I work in
zero-bytes preview mode).

Thanks,
Moti
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210325/bd098c1c/attachment.htm>

From m_zouhairy at ckta.by  Thu Mar 25 13:11:06 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Thu, 25 Mar 2021 16:11:06 +0300
Subject: [squid-users] compile with ssl support
In-Reply-To: <20210325123625.GA939@fantomas.sk>
References: <111a2d1b2b8237d6b0a9aa40660ab68e@tiscali.it>
 <202103251301.19455.Antony.Stone@squid.open.source.it>
 <20210325123625.GA939@fantomas.sk>
Message-ID: <b06514ea-97b7-df56-767c-2ecb8d287f93@ckta.by>

Peace,
as part of self developing, we decided that turning on sslbump + splice 
is a good idea, so how to install squid with ssl support on tumbleweed?


From mauamend at tiscali.it  Thu Mar 25 13:26:18 2021
From: mauamend at tiscali.it (maurizio)
Date: Thu, 25 Mar 2021 14:26:18 +0100
Subject: [squid-users] squid ftp list files problem
Message-ID: <e08230f470f5641d980c507648748627@tiscali.it>

  
Hello 
I have also an installation with squid-4.13 and it's work
correctly. 
These are configurations: 
4.14 
acl ftp_permit dstdomain
.xxx.yy
acl ftp_permit_ip dst xxx.xxx.xxx.xxx/32
acl Safe_ports port 21
# ftp
acl Safe_ports port 22 # sftp
acl FTP proto FTP
http_access allow
FTP ftp_permit vvvv
http_access allow FTP ftp_permit_ip
xxx.xxx.xxx.xxx/zz
http_access deny FTP
ftp_port 21
ftp_user
proxy at xxxx
ftp_passive on
ftp_epsv_all off
refresh_pattern ^ftp: 1440
20% 10080
always_direct allow FTP 
4.13 
acl Safe_ports port 21 #
ftp
acl FTP proto FTP
ftp_port 21
ftp_user proxy at xxx.yy
ftp_passive
on
ftp_epsv off
refresh_pattern ^ftp: 1440 20% 10080
always_direct allow
FTP 
Here the logs on connection with error: 
1616678414.797 0 xxx:43806
NONE/400 NONE error:ftp-must-login-first - HIER_NONE/- - - HTTP/1.1 "-"
0 22 "-"
1616678414.798 0 xxx:43806 NONE/400 NONE
error:ftp-must-login-first - HIER_NONE/- - - HTTP/1.1 "-" 0 22
"-"
1616678415.080 280 xxx:43806 TCP_MISS/204 GET ftp://ftp.zzz.it/ -
HIER_DIRECT/ftp.zzz.it yyy - HTTP/1.1 "-" 29 21 "-"
1616678429.019 48
xxx:43806 TCP_MISS/204 GET ftp://ftp.zzz.it/ - HIER_DIRECT/ftp.zzz.it
yyy - HTTP/1.1 "-" 14 27 "-"
1616678429.072 51 xxx:43806 TCP_MISS/204
GET ftp://ftp.zzz.it/ - HIER_DIRECT/ftp.zzz.it yyy - HTTP/1.1 "-" 6 77
"-"
1616678434.547 50 xxx:43806 TCP_MISS/204 GET ftp://ftp.zzz.it/ -
HIER_DIRECT/ftp.zzz.it yyy - HTTP/1.1 "-" 12 70 "-"
1616678437.269 46
xxx:43806 TCP_MISS/204 GET ftp://ftp.zzz.it/ - HIER_DIRECT/ftp.zzz.it
yyy - HTTP/1.1 "-" 29 42 "-" 
Thank you

  


Con Tiscali Mobile Smart 30 4G hai minuti illimitati, 100 SMS e 30 Giga in 4G a soli 8,99? al mese. http://tisca.li/smart30

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210325/d394b6fa/attachment.htm>

From mauamend at tiscali.it  Thu Mar 25 14:40:14 2021
From: mauamend at tiscali.it (maurizio)
Date: Thu, 25 Mar 2021 15:40:14 +0100
Subject: [squid-users] squid ftp list files problem
Message-ID: <04e496423b6141cfe549d620579a8112@tiscali.it>

  
Resolved, I put ftp_epsv off 
and now it's working. 
Thank for your
tips. 
Mau  


Con Tiscali Mobile Smart 30 4G hai minuti illimitati, 100 SMS e 30 Giga in 4G a soli 8,99? al mese. http://tisca.li/smart30

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210325/e8a60e84/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar 25 17:34:46 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Mar 2021 13:34:46 -0400
Subject: [squid-users] X-Next-Services
In-Reply-To: <CAGSk-40jH5BUhJYw-RKj000quZ3By2Q23jyUYB4q2uuTdA7dMw@mail.gmail.com>
References: <CAGSk-40jH5BUhJYw-RKj000quZ3By2Q23jyUYB4q2uuTdA7dMw@mail.gmail.com>
Message-ID: <c3614051-4519-ee07-2888-88b99d3dc6ae@measurement-factory.com>

On 3/25/21 9:06 AM, Moti Berger wrote:

> I want to be able to skip all subsequent ICAP servers defined in squid
> based on some logic I have in one of my ICAP servers.
> I used the X-Next-Services and it seems to control only the current ICAP
> chain.
> I also saw it while configuring two ICAP servers to handle requests and
> one ICAP server to handle responses. When the header was sent from the
> first ICAP on the request chain, the ICAP request didn't arrive at the
> second ICAP server handling requests but did get to the ICAP server that
> handles responses. I wish to also skip the ICAP which handles the
> responses.?
> Is that possible?

Yes, it is.

IIRC, the current X-Next-Services implementation at REQMOD vectoring
point always proceeds to RESPMOD. Ideally, this algorithm should be
enhanced to allow a service to be explicit about its desire to end all
adaption (or continue with the default service at the explicitly
specified vectoring point). This enhancement should not be very
difficult to implement, but it does require non-trivial source code
changes.
https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Meanwhile, you should be able to block RESPMOD adaptation for selected
requests using icap_service_access ACLs. As you know, this is possible
by adapting the HTTP request.

Ideally, it would also be possible by annotating the master transaction
using an ICAP response header, but only eCAP services can do that today
IIRC. Another missing enhancement for ICAP... However, you should try
this approach in combination with the adaptation_masterx_shared_names
mentioned below. I hope that when an ICAP response header field has the
shared header field name, the "note" ACL will have access to that field.


> If not, is it possible for an ICAP server to add an ICAP header during
> request handling to be read by the other ICAP servers that come after it
> when they handle the same request or the same request's response?

Yes, cross-service sharing should be possible by specifying your custom
ICAP header field name in adaptation_masterx_shared_names. As you can
tell, this will allow you to short-circuit unwanted RESPMOD adaptation
if the "note" ACL trick mentioned above does not work.


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Mar 26 19:35:54 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Mar 2021 15:35:54 -0400
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <CAJtdwuvQ-Xa11MHRYxp+yq1UViS4NhUQxda2523BK=cHHJZyMQ@mail.gmail.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
 <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
 <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>
 <CAJtdwuuR3+Qj7KG_Ycyvo=nkQ8H4th0o=OSG-F0uUPqN5CUDaA@mail.gmail.com>
 <9d9dcec6-6134-964a-5198-7a7623b3049a@measurement-factory.com>
 <CAJtdwuvQ-Xa11MHRYxp+yq1UViS4NhUQxda2523BK=cHHJZyMQ@mail.gmail.com>
Message-ID: <9fd62eef-02d3-a46c-75bf-c717af869089@measurement-factory.com>

On 3/24/21 3:34 PM, Miroslaw Malinowski wrote:
> I thought about upper service but as is not required at the moment,
> introducing extra hop just to remove the header looks a bit like a
> hammer approach. I'll look into how easily I can amend the code as the
> other option is to introduce a proxy like a feature to the application,
> so either way, it is a code change. The only problem here is that it's
> an OPNSense squid service so I have to compile from source on BSD and
> then keep adding in manually each time they do the update.

At the risk of stating the obvious: If your feature is officially
accepted into Squid sources, then you would not have to keep adding it
manually (once the changes reach your Squid packaging source).

Alex.


> On Wed, Mar 24, 2021 at 7:11 PM Alex Rousskov wrote:
> 
>     On 3/24/21 2:49 PM, Miroslaw Malinowski wrote:
> 
>     > looking at the code and reading carefully your response, you're saying
>     > there is no way you can do it with squid.
> 
>     With Squid, your options include:
> 
>     1. Squid source code changes. Should not be too difficult and, IMO, a
>     high-quality implementation would deserve official acceptance because it
>     is a generally useful feature in line with existing control knobs.
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
>     2. An adaptation service that removes Cache-Control:no-cache from the
>     response before Squid processes it:
>     https://wiki.squid-cache.org/SquidFaq/ContentAdaptation
> 
> 
>     HTH,
> 
>     Alex.
> 
>     > On Wed, Mar 24, 2021 at 6:28 PM Miroslaw Malinowski wrote:
>     >
>     >? ? ?Hi,
>     >
>     >? ? ?You've right yes it's revalidating as API server I'm
>     requesting data
>     >? ? ?is setting Cache-Control: no-cache. My question is how I can force
>     >? ? ?squid to cache and not validate as I know it's safe to do so. As
>     >? ? ?I've explained earlier we are making the same request and
>     receiving
>     >? ? ?the same response from 100+ server so as to reduce number of
>     >? ? ?requests to the external server we would like squid to cache the
>     >? ? ?response and issue a cached version.
>     >
>     >? ? ?2021/03/24 18:00:54.867 kid1| 22,3| refresh.cc(351) refreshCheck:
>     >? ? ?YES: Must revalidate stale object (origin set no-cache or private)
>     >
>     >? ? ?Mirek
>     >
>     >? ? ?On Wed, Mar 24, 2021 at 6:15 PM Alex Rousskov
>     >? ? ?<rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>     >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>     >
>     >? ? ? ? ?On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:
>     >
>     >? ? ? ? ?> Probably, me missing on something silly or it can't be done
>     >? ? ? ? ?but I don't
>     >? ? ? ? ?> know why but squid won't return the cached version even
>     when I
>     >? ? ? ? ?turn all
>     >? ? ? ? ?> override options ON in refresh_pattern.
>     >
>     >? ? ? ? ?AFAICT, no configuration options that can disable
>     revalidation of
>     >? ? ? ? ?Cache-Control:no-cache responses. refresh_pattern does not
>     have an
>     >? ? ? ? ?(equivalent of) "ignore-no-cache-in-responses" option.
>     >
>     >? ? ? ? ?IIRC, older Squids were violating an HTTP MUST by
>     forgetting to
>     >? ? ? ? ?revalidate Cache-Control:no-cache responses, but that was
>     fixed
>     >? ? ? ? ?in [1].
>     >? ? ? ? ?Your Squid version has that fix.
>     >
>     >? ? ? ? ?[1]
>     >? ? ? ?
>     ?https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>     <https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa>
>     >? ? ? ?
>     ?<https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>     <https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa>>
>     >
>     >
>     >? ? ? ? ?> With debug, I can see the rule is matched and the cache is
>     >? ? ? ? ?fresh but
>     >? ? ? ? ?> still in access.log is TCP_REFRESH_MODIFIED
>     >
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 11,3| http.cc(982)
>     >? ? ? ? ?> haveParsedReplyHeaders: decided: cache positively and share
>     >? ? ? ? ?because
>     >
>     >? ? ? ? ?FYI: You are looking at cache.log lines logged _after_
>     Squid has
>     >? ? ? ? ?already
>     >? ? ? ? ?decided to refresh the cached version. If you want to analyze
>     >? ? ? ? ?why Squid
>     >? ? ? ? ?decided to refresh the cached version, you should look
>     _before_
>     >? ? ? ? ?Squid
>     >? ? ? ? ?logged the request to the server (and before any FwdState.cc
>     >? ? ? ? ?lines). I
>     >? ? ? ? ?have not checked the details, but I bet that your Squid
>     revalidates
>     >? ? ? ? ?because of Cache-Control:no-cache in the response. Look for
>     >? ? ? ? ?"YES: Must
>     >? ? ? ? ?revalidate stale object".
>     >
>     >
>     >? ? ? ? ?HTH,
>     >
>     >? ? ? ? ?Alex.
>     >
>     >? ? ? ? ?> squid conf:
>     >? ? ? ? ?> refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
>     >? ? ? ? ?> override-expire ignore-reload ignore-no-store ignore-private
>     >? ? ? ? ?store-stale
>     >? ? ? ? ?>
>     >? ? ? ? ?> curl headers:
>     >? ? ? ? ?> curl --insecure --verbose --request GET --url 'URL'
>     >/dev/null
>     >? ? ? ? ?> * TCP_NODELAY set
>     >? ? ? ? ?> * ALPN, offering h2
>     >? ? ? ? ?> * ALPN, offering http/1.1
>     >? ? ? ? ?> * successfully set certificate verify locations:
>     >? ? ? ? ?> * ??CAfile: /etc/ssl/certs/ca-certificates.crt
>     >? ? ? ? ?> ?CApath: /etc/ssl/certs
>     >? ? ? ? ?> } [5 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (OUT), TLS handshake, Client hello (1):
>     >? ? ? ? ?> } [512 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, Server hello (2):
>     >? ? ? ? ?> { [122 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
>     >? ? ? ? ?> { [6 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, Certificate (11):
>     >? ? ? ? ?> { [1956 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, CERT verify (15):
>     >? ? ? ? ?> { [78 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, Finished (20):
>     >? ? ? ? ?> { [52 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
>     >? ? ? ? ?> } [1 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (OUT), TLS handshake, Finished (20):
>     >? ? ? ? ?> } [52 bytes data]
>     >? ? ? ? ?> * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
>     >? ? ? ? ?>
>     >? ? ? ? ?>> GET URL HTTP/1.1
>     >? ? ? ? ?>> Host: URL
>     >? ? ? ? ?>> User-Agent: curl/7.68.0
>     >? ? ? ? ?>> Accept: */*
>     >? ? ? ? ?>>
>     >? ? ? ? ?> { [5 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>     >? ? ? ? ?> { [217 bytes data]
>     >? ? ? ? ?> * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>     >? ? ? ? ?> { [217 bytes data]
>     >? ? ? ? ?> * old SSL session ID is stale, removing
>     >? ? ? ? ?> { [5 bytes data]
>     >? ? ? ? ?> * Mark bundle as not supporting multiuse
>     >? ? ? ? ?> < HTTP/1.1 200 OK
>     >? ? ? ? ?> < Cache-Control: no-cache
>     >? ? ? ? ?> < Content-Type: application/json
>     >? ? ? ? ?> < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
>     >? ? ? ? ?> < Date: Wed, 24 Mar 2021 15:04:34 GMT
>     >? ? ? ? ?> < Server: Google Frontend
>     >? ? ? ? ?> < Content-Length: 7950
>     >? ? ? ? ?> < X-Cache: MISS from server
>     >? ? ? ? ?> < X-Cache-Lookup: HIT from server
>     >? ? ? ? ?> < Via: 1.1 server (squid/4.14)
>     >? ? ? ? ?> < Connection: keep-alive
>     >? ? ? ? ?>
>     >? ? ? ? ?> access log:
>     >? ? ? ? ?> 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
>     >? ? ? ? ?> ORIGINAL_DST/IP application/json
>     >? ? ? ? ?>
>     >? ? ? ? ?> cache log:
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 11,3| http.cc(982)
>     >? ? ? ? ?> haveParsedReplyHeaders: decided: cache positively and share
>     >? ? ? ? ?because
>     >? ? ? ? ?> refresh check returned cacheable; HTTP status 200
>     >? ? ? ? ?e:=p2V/0x34868914670*3? ? ? ?
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(470)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> returning FRESH_MIN_RULE? ? ? ?
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(455)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> Object isn't stale..? ?
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(327)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> Staleness = -1? ? ? ? ?
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(199)
>     >? ? ? ? ?> refreshStaleness: FRESH: age (60 sec) is less than
>     configured
>     >? ? ? ? ?minimum
>     >? ? ? ? ?> (259200 sec)? ?
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(166)
>     >? ? ? ? ?> refreshStaleness: No explicit expiry given, using
>     heuristics to
>     >? ? ? ? ?> determine freshness? ??
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(307)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT? ? ? ??
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(305)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> check_time: Wed, 24 Mar 2021 15:05:34 GMT? ? ??
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(303)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> age: 60? ? ? ??
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(301)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> Matched 'URL 259200 80%% 7776000'? ? ??
>     >? ? ? ? ?> 2021-03-24T15:04:34? ?squid? ?.710 kid1| 22,3|
>     refresh.cc(279)
>     >? ? ? ? ?refreshCheck:
>     >? ? ? ? ?> checking freshness of URI: https://URL <https://URL>
>     <https://URL <https://URL>>
>     >? ? ? ? ?<https://URL <https://URL> <https://URL <https://URL>>>
>     >? ? ? ? ?>
>     >? ? ? ? ?>
>     >? ? ? ? ?> _______________________________________________
>     >? ? ? ? ?> squid-users mailing list
>     >? ? ? ? ?> squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >? ? ? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>     >? ? ? ? ?> http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     >? ? ? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>     >? ? ? ? ?>
>     >
> 



From acidflash_ at linuxmail.org  Sat Mar 27 07:55:19 2021
From: acidflash_ at linuxmail.org (acidflash acidflash)
Date: Sat, 27 Mar 2021 08:55:19 +0100
Subject: [squid-users] Squid stops serving requests after squid -k
 reconfigure
References: <trinity-7ab47fde-5c1e-42a3-9f2a-fcdba3db53f7-1616831387168@3c-app-mailcom-lxa13>
Message-ID: <trinity-8cc473c8-2797-4258-bf96-557538804f63-1616831719086@3c-app-mailcom-lxa13>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210327/a54a0efc/attachment.htm>

From ngtech1ltd at gmail.com  Sat Mar 27 13:25:59 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 27 Mar 2021 16:25:59 +0300
Subject: [squid-users] squid won't return cached even with
 refresh_pattern extra options override-lastmod override-expire
 ignore-reload ignore-no-store ignore-private store-stale
In-Reply-To: <9fd62eef-02d3-a46c-75bf-c717af869089@measurement-factory.com>
References: <CAJtdwuuoy8ZGvqQzwh=ZMEeBoo3+h9-KFz3aDsYr7ip=vO3PBw@mail.gmail.com>
 <064208f8-0a62-e49c-2640-857cdc9fa9b8@measurement-factory.com>
 <CAJtdwuv5oqFd+ZJiGkqt480O41X96_mU8jtc_=feD+Nf0X+fVA@mail.gmail.com>
 <CAJtdwuuR3+Qj7KG_Ycyvo=nkQ8H4th0o=OSG-F0uUPqN5CUDaA@mail.gmail.com>
 <9d9dcec6-6134-964a-5198-7a7623b3049a@measurement-factory.com>
 <CAJtdwuvQ-Xa11MHRYxp+yq1UViS4NhUQxda2523BK=cHHJZyMQ@mail.gmail.com>
 <9fd62eef-02d3-a46c-75bf-c717af869089@measurement-factory.com>
Message-ID: <!&!AAAAAAAAAAAYAAAAAAAAAG15AYh8TcJOh7jZfP/beGvCgAAAEAAAAL32rjCYo6hPkiu9G5KSyPEBAAAAAA==@gmail.com>

Hey Mirek,

This is not the first time this issue rises.
There are risks in implementing any solution for this *issue*.

I have implemented YouTube caching in the past using couple twisted techniques while leaving squid un-touched.
The desire to caching sometimes can overcome couple very big risks to the integrity if the date/content.
It is possible to use an ICAP service with a 206 response instead of 204 or 200 however I believe
that you wouldn?t need to cache any POST requests so a simple ICAP service would be sufficient.
I believe that It is preferred to leave squid sources un touched for such a purpose.
An example for such a twist is at:
* https://github.com/elico/squid-helpers/tree/master/squid_helpers/youtubetwist
* https://wiki.squid-cache.org/ConfigExamples/DynamicContent/Coordinator?highlight=%28cache_peer%29#Implementing_ICAP_solution
* https://ieeexplore.ieee.org/abstract/document/9072556

I wrote a public example of an ICAP server that was used to  prove vulnerabilities in HTTP which is now used in the prove of HTTPS vulnerabilities.
Take a peek at:
* https://github.com/elico/bgu-icap-example

It's written in GoLang and works under pretty heavy loads.

Let me know if you need more help,
Eliezer

----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com
Zoom: Coming soon


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Friday, March 26, 2021 10:36 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid won't return cached even with refresh_pattern extra options override-lastmod override-expire ignore-reload ignore-no-store ignore-private store-stale

On 3/24/21 3:34 PM, Miroslaw Malinowski wrote:
> I thought about upper service but as is not required at the moment,
> introducing extra hop just to remove the header looks a bit like a
> hammer approach. I'll look into how easily I can amend the code as the
> other option is to introduce a proxy like a feature to the application,
> so either way, it is a code change. The only problem here is that it's
> an OPNSense squid service so I have to compile from source on BSD and
> then keep adding in manually each time they do the update.

At the risk of stating the obvious: If your feature is officially
accepted into Squid sources, then you would not have to keep adding it
manually (once the changes reach your Squid packaging source).

Alex.


> On Wed, Mar 24, 2021 at 7:11 PM Alex Rousskov wrote:
> 
>     On 3/24/21 2:49 PM, Miroslaw Malinowski wrote:
> 
>     > looking at the code and reading carefully your response, you're saying
>     > there is no way you can do it with squid.
> 
>     With Squid, your options include:
> 
>     1. Squid source code changes. Should not be too difficult and, IMO, a
>     high-quality implementation would deserve official acceptance because it
>     is a generally useful feature in line with existing control knobs.
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
>     2. An adaptation service that removes Cache-Control:no-cache from the
>     response before Squid processes it:
>     https://wiki.squid-cache.org/SquidFaq/ContentAdaptation
> 
> 
>     HTH,
> 
>     Alex.
> 
>     > On Wed, Mar 24, 2021 at 6:28 PM Miroslaw Malinowski wrote:
>     >
>     >     Hi,
>     >
>     >     You've right yes it's revalidating as API server I'm
>     requesting data
>     >     is setting Cache-Control: no-cache. My question is how I can force
>     >     squid to cache and not validate as I know it's safe to do so. As
>     >     I've explained earlier we are making the same request and
>     receiving
>     >     the same response from 100+ server so as to reduce number of
>     >     requests to the external server we would like squid to cache the
>     >     response and issue a cached version.
>     >
>     >     2021/03/24 18:00:54.867 kid1| 22,3| refresh.cc(351) refreshCheck:
>     >     YES: Must revalidate stale object (origin set no-cache or private)
>     >
>     >     Mirek
>     >
>     >     On Wed, Mar 24, 2021 at 6:15 PM Alex Rousskov
>     >     <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>     >     <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>     >
>     >         On 3/24/21 12:48 PM, Miroslaw Malinowski wrote:
>     >
>     >         > Probably, me missing on something silly or it can't be done
>     >         but I don't
>     >         > know why but squid won't return the cached version even
>     when I
>     >         turn all
>     >         > override options ON in refresh_pattern.
>     >
>     >         AFAICT, no configuration options that can disable
>     revalidation of
>     >         Cache-Control:no-cache responses. refresh_pattern does not
>     have an
>     >         (equivalent of) "ignore-no-cache-in-responses" option.
>     >
>     >         IIRC, older Squids were violating an HTTP MUST by
>     forgetting to
>     >         revalidate Cache-Control:no-cache responses, but that was
>     fixed
>     >         in [1].
>     >         Your Squid version has that fix.
>     >
>     >         [1]
>     >       
>      https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>     <https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa>
>     >       
>      <https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa
>     <https://github.com/squid-cache/squid/commit/fa83b766a208b27abed8da4c9073cf8784cf10fa>>
>     >
>     >
>     >         > With debug, I can see the rule is matched and the cache is
>     >         fresh but
>     >         > still in access.log is TCP_REFRESH_MODIFIED
>     >
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
>     >         > haveParsedReplyHeaders: decided: cache positively and share
>     >         because
>     >
>     >         FYI: You are looking at cache.log lines logged _after_
>     Squid has
>     >         already
>     >         decided to refresh the cached version. If you want to analyze
>     >         why Squid
>     >         decided to refresh the cached version, you should look
>     _before_
>     >         Squid
>     >         logged the request to the server (and before any FwdState.cc
>     >         lines). I
>     >         have not checked the details, but I bet that your Squid
>     revalidates
>     >         because of Cache-Control:no-cache in the response. Look for
>     >         "YES: Must
>     >         revalidate stale object".
>     >
>     >
>     >         HTH,
>     >
>     >         Alex.
>     >
>     >         > squid conf:
>     >         > refresh_pattern -i <URL> 4320 80% 129600 override-lastmod
>     >         > override-expire ignore-reload ignore-no-store ignore-private
>     >         store-stale
>     >         >
>     >         > curl headers:
>     >         > curl --insecure --verbose --request GET --url 'URL'
>     >/dev/null
>     >         > * TCP_NODELAY set
>     >         > * ALPN, offering h2
>     >         > * ALPN, offering http/1.1
>     >         > * successfully set certificate verify locations:
>     >         > *   CAfile: /etc/ssl/certs/ca-certificates.crt
>     >         >  CApath: /etc/ssl/certs
>     >         > } [5 bytes data]
>     >         > * TLSv1.3 (OUT), TLS handshake, Client hello (1):
>     >         > } [512 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, Server hello (2):
>     >         > { [122 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
>     >         > { [6 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, Certificate (11):
>     >         > { [1956 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, CERT verify (15):
>     >         > { [78 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, Finished (20):
>     >         > { [52 bytes data]
>     >         > * TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
>     >         > } [1 bytes data]
>     >         > * TLSv1.3 (OUT), TLS handshake, Finished (20):
>     >         > } [52 bytes data]
>     >         > * SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384
>     >         >
>     >         >> GET URL HTTP/1.1
>     >         >> Host: URL
>     >         >> User-Agent: curl/7.68.0
>     >         >> Accept: */*
>     >         >>
>     >         > { [5 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>     >         > { [217 bytes data]
>     >         > * TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
>     >         > { [217 bytes data]
>     >         > * old SSL session ID is stale, removing
>     >         > { [5 bytes data]
>     >         > * Mark bundle as not supporting multiuse
>     >         > < HTTP/1.1 200 OK
>     >         > < Cache-Control: no-cache
>     >         > < Content-Type: application/json
>     >         > < X-Cloud-Trace-Context: d3c27833b8b4312ce31a2dbae7e12fd0
>     >         > < Date: Wed, 24 Mar 2021 15:04:34 GMT
>     >         > < Server: Google Frontend
>     >         > < Content-Length: 7950
>     >         > < X-Cache: MISS from server
>     >         > < X-Cache-Lookup: HIT from server
>     >         > < Via: 1.1 server (squid/4.14)
>     >         > < Connection: keep-alive
>     >         >
>     >         > access log:
>     >         > 243 172.16.230.249 TCP_REFRESH_MODIFIED/200 8328 GET URL -
>     >         > ORIGINAL_DST/IP application/json
>     >         >
>     >         > cache log:
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 11,3| http.cc(982)
>     >         > haveParsedReplyHeaders: decided: cache positively and share
>     >         because
>     >         > refresh check returned cacheable; HTTP status 200
>     >         e:=p2V/0x34868914670*3       
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(470)
>     >         refreshCheck:
>     >         > returning FRESH_MIN_RULE       
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(455)
>     >         refreshCheck:
>     >         > Object isn't stale..   
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(327)
>     >         refreshCheck:
>     >         > Staleness = -1         
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(199)
>     >         > refreshStaleness: FRESH: age (60 sec) is less than
>     configured
>     >         minimum
>     >         > (259200 sec)   
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(166)
>     >         > refreshStaleness: No explicit expiry given, using
>     heuristics to
>     >         > determine freshness    
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(307)
>     >         refreshCheck:
>     >         > entry->timestamp: Wed, 24 Mar 2021 15:04:34 GMT        
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(305)
>     >         refreshCheck:
>     >         > check_time: Wed, 24 Mar 2021 15:05:34 GMT      
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(303)
>     >         refreshCheck:
>     >         > age: 60        
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(301)
>     >         refreshCheck:
>     >         > Matched 'URL 259200 80%% 7776000'      
>     >         > 2021-03-24T15:04:34   squid   .710 kid1| 22,3|
>     refresh.cc(279)
>     >         refreshCheck:
>     >         > checking freshness of URI: https://URL <https://URL>
>     <https://URL <https://URL>>
>     >         <https://URL <https://URL> <https://URL <https://URL>>>
>     >         >
>     >         >
>     >         > _______________________________________________
>     >         > squid-users mailing list
>     >         > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >         <mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>     >         > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     >         <http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>     >         >
>     >
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Sun Mar 28 00:59:35 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 28 Mar 2021 03:59:35 +0300
Subject: [squid-users] Looking for subscription plan plain text Blacklists
 for a spin
Message-ID: <000601d7236d$9f9e2d30$deda8790$@gmail.com>

Hey List,

 

I wanted to test my SquidBlocker DB newest version with a paid categories
list.

I am looking for a paid list of categorized sites which I can load into the
DB and test performance.

 

For now I have found the next list:

https://github.com/blocklistproject/Lists

 

But since there are many vendors for blacklists I would be happy to get any
recommendations.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210328/c285d73d/attachment.htm>

From ngtech1ltd at gmail.com  Sun Mar 28 01:42:25 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 28 Mar 2021 04:42:25 +0300
Subject: [squid-users] [SPAM] Squid stops serving requests after squid
 -k reconfigure
Message-ID: <000b01d72373$9b4e9a70$d1ebcf50$@gmail.com>

Hey Acid,

 

Haven?t seen you here for a very long time.

The first thing is to upgrade squid if possible?

 

It?s better that you won?t use squid -kreconf for big blacklists.

Instead you should use some external software to match the blacklists.

The most recommended software these days is ufdbguard.

Depends on the size of your blacklist your might need to find the right solution.

The best solution would be to store the list in ram somehow.

Have you tried some kind of rbl server?

 

At the time I wrote some code to and some of it was merged into:

https://github.com/looterz/grimd

 

It has a reload url so you can update the files on disk and send a reload.

 

Another service I am using is:

https://github.com/andybalholm/redwood

 

Which has a ?Classification Service? function.

It?s pretty easy to write a json http client that can run queries against this classification service.

 

Also you?d better use a file In the dstdomain ac ie:

acl Blacklist dstdomain ?/var/blacklists/xyx.list?

http_access deny Blacklist

 

and inside the xyx.list file just add lines of domains like

.blacklisted-domain.com

.example.com

 

Etc..

 

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of acidflash acidflash
Sent: Saturday, March 27, 2021 10:55 AM
To: squid-users at lists.squid-cache.org
Subject: [SPAM] [squid-users] Squid stops serving requests after squid -k reconfigure

 

I have gone through the forums, and I haven't found an answer to the question, although it has been asked more than once.

I am running squid 3.5.X on Centos 7, the compile options are:
"configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group,kerberos_ldap_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'"

 

I have a service which adds domains to a blacklist file, and then calls squid -k reconfigure.
Instead of writing to the file, this service updates the file completely with new rules,
by deleting the old file, and creating a new one in its place, and then calling squid -k reconfigure.
After doing this, on odd occasions, squid will stop serving traffic completely,
until you do a squid stop, and squid start. After shutting down squid,
and starting squid up with the same rules, squid will continue to work normally.
Its probably worth mentioning that during the time that these events are taking place,
the server is under quite a bit of load, and clients don't stop sending requests via the server.
What these directives look like:

acl Porn dstdomain .xnxx.com .sex.com 
acl Drugs dstdomain .drugs.com .silkroad.eu 
http_access deny Porn
http_access deny Drugs

 

This also seems to be amplified when there are several squid workers (child processes) running.
In regards to order, these ACL's are above any other ACL's in the list. We have a very basic squid conf file that looks like this:
## START OF FILE
http_port 3128
cache deny all
#
access_log /var/log/squid/access.log 
cache_store_log none
cache_log /dev/null
logfile_rotate 4
#
auth_param basic program /usr/lib64/squid/basic_db_auth --dsn "DBI:mysql:host=XX.XX.XX.XX;port=XXXX;database=XXXXX" --user XXXXXX --password XXXXXXXX --plaintext --persist
# 
acl db-auth proxy_auth REQUIRED
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive on
#
connect_timeout 55 minutes
#
request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access User-Agent allow all
request_header_access Cookie allow all
request_header_access All deny all
dns_v4_first on
via off
forwarded_for off
follow_x_forwarded_for deny all
dns_nameservers 8.8.8.8 8.8.4.4
## END OF FILE

 

Your help is greatly appreciated, maybe there has been some insight into this issue after 10+ years since the last time it was asked.

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210328/7a6ffaf9/attachment.htm>

From rentorbuy at yahoo.com  Mon Mar 29 15:00:57 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 29 Mar 2021 15:00:57 +0000 (UTC)
Subject: [squid-users] kswapd0 and memory usage
References: <765673474.1603920.1617030057230.ref@mail.yahoo.com>
Message-ID: <765673474.1603920.1617030057230@mail.yahoo.com>

Hi,

I've been running squid & c-icap for years, and only recently have I had a severe system slowdown.

My kswapd0 process was at a constant 100% CPU usage level until I forced restarting of both squid and c-icap.

I've been using several Squid versions over the years, but the only differences I know of between my previous recent setup that worked and the current setup that has "failed" once (for now)? are:

- upgraded from 5.0.4-20201125-r5fadc09ee to Version 5.0.5-20210223-r4af19cc24

- set cgroups for both squid and c-icap services with just one setting: cpu.shares 512

- upgraded to c-icap 0.5.8

Given the stressful situation I only had time to notice that kswapd0 was at 100%, that top reported that all swap space was being used, and that the whole server was very sluggish. The additional problem is that the system is a router and uses TPROXY with squid sslbump so I don't think I can virtualize the web proxying services. Hence the use of cgroups to try to contain squid, c-icap and clamav. I have yet to define a cgroup for memory usage.

Restarting Squid and c-icap alone (not clamd) immediately solved the kswapd0 "gone wild" issue.
Mem usage went back to something like:

# free -h
????????????? total??????? used??????? free????? shared? buff/cache?? available
Mem:?????????? 31Gi?????? 9.2Gi??????? 21Gi??????? 48Mi?????? 1.0Gi??????? 21Gi
Swap:????????? 35Gi?????? 1.7Gi??????? 33Gi

I only have debug_options rotate=1 ALL,1 in my squid config file, and sifting through cache.log doesn't give me any clues.

If this were to happen again (not sure when or if) what should I try to search for?

Regards,

Vieri


From squid3 at treenet.co.nz  Tue Mar 30 05:55:52 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Mar 2021 18:55:52 +1300
Subject: [squid-users] kswapd0 and memory usage
In-Reply-To: <765673474.1603920.1617030057230@mail.yahoo.com>
References: <765673474.1603920.1617030057230.ref@mail.yahoo.com>
 <765673474.1603920.1617030057230@mail.yahoo.com>
Message-ID: <bfb3ccab-3db4-408b-e4d0-08f845b64d02@treenet.co.nz>

On 30/03/21 4:00 am, Vieri wrote:
> 
> If this were to happen again (not sure when or if) what should I try to search for?
> 

Output of the "squidclient mgr:mem", "top" and "ps waux" commands would 
be good.

Those will show how Squid is using the memory it has, what processes are 
using the most memory, and what processes are running. Most memory 
issues can be triaged with that info.

Amos


From acidflash_ at linuxmail.org  Tue Mar 30 17:41:04 2021
From: acidflash_ at linuxmail.org (acidflash acidflash)
Date: Tue, 30 Mar 2021 19:41:04 +0200
Subject: [squid-users] [SPAM] Squid stops serving requests after squid
 -k reconfigure
In-Reply-To: <000b01d72373$9b4e9a70$d1ebcf50$@gmail.com>
References: <000b01d72373$9b4e9a70$d1ebcf50$@gmail.com>
Message-ID: <trinity-0dab7e38-ee9e-466e-9118-48007dfaf3f6-1617126064039@3c-app-mailcom-lxa07>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210330/9f1551b5/attachment.htm>

From ngtech1ltd at gmail.com  Tue Mar 30 20:08:02 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Tue, 30 Mar 2021 23:08:02 +0300
Subject: [squid-users] [SPAM] Squid stops serving requests after squid
 -k reconfigure
In-Reply-To: <trinity-0dab7e38-ee9e-466e-9118-48007dfaf3f6-1617126064039@3c-app-mailcom-lxa07>
References: <000b01d72373$9b4e9a70$d1ebcf50$@gmail.com>
 <trinity-0dab7e38-ee9e-466e-9118-48007dfaf3f6-1617126064039@3c-app-mailcom-lxa07>
Message-ID: <CABA8h=TE1uV_Ojj5XGS0N4MM_8z2QPf5CL+N3Tbwob3OsE2bGQ@mail.gmail.com>

Hey Acid,

First you should try 4.x latest.
a reload of 50 domains every 10 second doesn't make sense.
I don't understand the config and the setup.
for 50 sites you just need a basic script and even one in bash will work
dor you with grep.
I wrote an example in ruby a while ago, I will try to find it in the next
week.
Maybe the server is overloaded.

I will try to give you an example later on.

Eliezer


?????? ??? ??, 30 ???? 2021, 20:41, ??? acidflash acidflash ?<
acidflash_ at linuxmail.org>:

>
> Hi Eliezer,
>
> I hope your doing ok. Thanks for the reply. Yeah currently what I am doing
> is:
>
> include /etc/squid/blockedsites.list
> and adding the ACL's and the denies in the list file. What version do you
> recommend I upgrade to, and is this a known issue? The list is actually
> pretty small, probably no more than 50 sites or so, and thats split across
> 4 or 5 groups (different ACL's). I'll look into ufdbguard and the other
> projects as well, does this sound familiar though? If you think that the
> best path forward is to alleviate the burden off of squid to some external
> tool, I could probably think up a few hacks to for that, but would
> obviously prefer to keep it all within squid. Is this occurance common with
> squid -k reconfigure and dstdomain matching? Thanks for your time. Stay
> safe.
>
> *Sent:* Sunday, March 28, 2021 at 4:42 AM
> *From:* "Eliezer Croitoru" <ngtech1ltd at gmail.com>
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] [SPAM] Squid stops serving requests after
> squid -k reconfigure
>
> Hey Acid,
>
>
>
> Haven?t seen you here for a very long time.
>
> The first thing is to upgrade squid if possible?
>
>
>
> It?s better that you won?t use squid -kreconf for big blacklists.
>
> Instead you should use some external software to match the blacklists.
>
> The most recommended software these days is ufdbguard.
>
> Depends on the size of your blacklist your might need to find the right
> solution.
>
> The best solution would be to store the list in ram somehow.
>
> Have you tried some kind of rbl server?
>
>
>
> At the time I wrote some code to and some of it was merged into:
>
> https://github.com/looterz/grimd
>
>
>
> It has a reload url so you can update the files on disk and send a reload.
>
>
>
> Another service I am using is:
>
> https://github.com/andybalholm/redwood
>
>
>
> Which has a ?Classification Service? function.
>
> It?s pretty easy to write a json http client that can run queries against
> this classification service.
>
>
>
> Also you?d better use a file In the dstdomain ac ie:
>
> acl Blacklist dstdomain ?/var/blacklists/xyx.list?
>
> http_access deny Blacklist
>
>
>
> and inside the xyx.list file just add lines of domains like
>
> .blacklisted-domain.com
>
> .example.com
>
>
>
> Etc..
>
>
>
>
>
> All The Bests,
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> Zoom: Coming soon
>
>
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *acidflash acidflash
> *Sent:* Saturday, March 27, 2021 10:55 AM
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [SPAM] [squid-users] Squid stops serving requests after squid
> -k reconfigure
>
>
>
> I have gone through the forums, and I haven't found an answer to the
> question, although it has been asked more than once.
>
> I am running squid 3.5.X on Centos 7, the compile options are:
> "configure options:  '--build=x86_64-redhat-linux-gnu'
> '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
> '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
> '--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
> '--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--disable-strict-error-checking'
> '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-logdir=$(localstatedir)/log/squid'
> '--with-pidfile=$(localstatedir)/run/squid.pid'
> '--disable-dependency-tracking' '--enable-eui'
> '--enable-follow-x-forwarded-for' '--enable-auth'
> '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam'
> '--enable-auth-ntlm=smb_lm,fake'
> '--enable-auth-digest=file,LDAP,eDirectory'
> '--enable-auth-negotiate=kerberos'
> '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group,kerberos_ldap_group'
> '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
> '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups'
> '--enable-linux-netfilter' '--enable-removal-policies=heap,lru'
> '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs'
> '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio'
> '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads'
> '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu'
> 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
> -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
> --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
> -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
> -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
> -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
> -m64 -mtune=generic -fpie'
> 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'"
>
>
>
> I have a service which adds domains to a blacklist file, and then calls
> squid -k reconfigure.
> Instead of writing to the file, this service updates the file completely
> with new rules,
> by deleting the old file, and creating a new one in its place, and then
> calling squid -k reconfigure.
> After doing this, on odd occasions, squid will stop serving traffic
> completely,
> until you do a squid stop, and squid start. After shutting down squid,
> and starting squid up with the same rules, squid will continue to work
> normally.
> Its probably worth mentioning that during the time that these events are
> taking place,
> the server is under quite a bit of load, and clients don't stop sending
> requests via the server.
> What these directives look like:
>
> acl Porn dstdomain .xnxx.com .sex.com
> acl Drugs dstdomain .drugs.com .silkroad.eu
> http_access deny Porn
> http_access deny Drugs
>
>
>
> This also seems to be amplified when there are several squid workers
> (child processes) running.
> In regards to order, these ACL's are above any other ACL's in the list. We
> have a very basic squid conf file that looks like this:
> ## START OF FILE
> http_port 3128
> cache deny all
> #
> access_log /var/log/squid/access.log
> cache_store_log none
> cache_log /dev/null
> logfile_rotate 4
> #
> auth_param basic program /usr/lib64/squid/basic_db_auth --dsn
> "DBI:mysql:host=XX.XX.XX.XX;port=XXXX;database=XXXXX" --user XXXXXX
> --password XXXXXXXX --plaintext --persist
> #
> acl db-auth proxy_auth REQUIRED
> auth_param basic credentialsttl 2 hours
> auth_param basic casesensitive on
> #
> connect_timeout 55 minutes
> #
> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access WWW-Authenticate allow all
> request_header_access Proxy-Authorization allow all
> request_header_access Proxy-Authenticate allow all
> request_header_access Cache-Control allow all
> request_header_access Content-Encoding allow all
> request_header_access Content-Length allow all
> request_header_access Content-Type allow all
> request_header_access Date allow all
> request_header_access Expires allow all
> request_header_access Host allow all
> request_header_access If-Modified-Since allow all
> request_header_access Last-Modified allow all
> request_header_access Location allow all
> request_header_access Pragma allow all
> request_header_access Accept allow all
> request_header_access Accept-Charset allow all
> request_header_access Accept-Encoding allow all
> request_header_access Accept-Language allow all
> request_header_access Content-Language allow all
> request_header_access Mime-Version allow all
> request_header_access Retry-After allow all
> request_header_access Title allow all
> request_header_access Connection allow all
> request_header_access Proxy-Connection allow all
> request_header_access User-Agent allow all
> request_header_access Cookie allow all
> request_header_access All deny all
> dns_v4_first on
> via off
> forwarded_for off
> follow_x_forwarded_for deny all
> dns_nameservers 8.8.8.8 8.8.4.4
> ## END OF FILE
>
>
>
> Your help is greatly appreciated, maybe there has been some insight into
> this issue after 10+ years since the last time it was asked.
>
>
> _______________________________________________ squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210330/f185a701/attachment.htm>

From klaus_brandl at genua.de  Wed Mar 31 14:02:28 2021
From: klaus_brandl at genua.de (Klaus Brandl)
Date: Wed, 31 Mar 2021 14:02:28 +0000
Subject: [squid-users] icap adaptation chains with adaptation sets
Message-ID: <02db8cf0b66521807b2f4e1db63feb07d89cf9d5.camel@genua.de>

Hi,

is there a way to use more adaptation sets(for redundancy) combined in
an adaptation chain?
What we need is something like this:

icap_service b1 reqmod_precache ...
icap_service b2 reqmod_precache ...
icap_service b3 reqmod_precache ...
icap_service b4 reqmod_precache ...

icap_service m1 reqmod_precache ...
icap_service m2 reqmod_precache ...
icap_service m3 reqmod_precache ...
icap_service m4 reqmod_precache ...

#blacklist
adaptation_service_set pool1 b1 b2 b3 b4
#malware scanner
adaptation_service_set pool2 m1 m2 m3 m4

adaptation_service_chain checks pool1 pool2

adaptation_access checks allow all

Regards

Klaus


From rentorbuy at yahoo.com  Wed Mar 31 16:22:44 2021
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 31 Mar 2021 16:22:44 +0000 (UTC)
Subject: [squid-users] kswapd0 and memory usage
In-Reply-To: <bfb3ccab-3db4-408b-e4d0-08f845b64d02@treenet.co.nz>
References: <765673474.1603920.1617030057230.ref@mail.yahoo.com>
 <765673474.1603920.1617030057230@mail.yahoo.com>
 <bfb3ccab-3db4-408b-e4d0-08f845b64d02@treenet.co.nz>
Message-ID: <1048309536.2557916.1617207764398@mail.yahoo.com>

 On Tuesday, March 30, 2021, 8:01:30 AM GMT+2, Amos Jeffries <squid3 at treenet.co.nz> wrote: 

>> If this were to happen again (not sure when or if) what should I try to search for?
>
> Output of the "squidclient mgr:mem", "top" and "ps waux" commands would 
> be good.
>
> Those will show how Squid is using the memory it has, what processes are 
> using the most memory, and what processes are running. Most memory 
> issues can be triaged with that info.

Will do, thanks. I have a script that tries to "predict" when these problems are about to happen. It runs something like
timeout 30 squidclient mgr:info
and if it actually times out then it restarts both squid and c-icap.
So I'm afraid I might not get anything out of "squidclient mgr:mem", but I will run top -b -n 1 and ps waux.

Thanks,

Vieri





From ngtech1ltd at gmail.com  Wed Mar 31 17:36:14 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 31 Mar 2021 20:36:14 +0300
Subject: [squid-users] [SPAM] Squid stops serving requests after squid
 -k reconfigure
In-Reply-To: <CABA8h=TE1uV_Ojj5XGS0N4MM_8z2QPf5CL+N3Tbwob3OsE2bGQ@mail.gmail.com>
References: <000b01d72373$9b4e9a70$d1ebcf50$@gmail.com>	<trinity-0dab7e38-ee9e-466e-9118-48007dfaf3f6-1617126064039@3c-app-mailcom-lxa07>
 <CABA8h=TE1uV_Ojj5XGS0N4MM_8z2QPf5CL+N3Tbwob3OsE2bGQ@mail.gmail.com>
Message-ID: <008301d72654$5a0eb2a0$0e2c17e0$@gmail.com>

Hey Acid,

 

I created as an example scripts the next:

https://github.com/elico/squid-external-matchers

 

The bash script is the pseudo and the ruby are production grade services.

I have use these services in production for a very long time and it works great.

Auto reloads the files as you append the content/domains/urls/sni.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: NgTech LTD <ngtech1ltd at gmail.com> 
Sent: Tuesday, March 30, 2021 11:08 PM
To: acidflash acidflash <acidflash_ at linuxmail.org>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] [SPAM] Squid stops serving requests after squid -k reconfigure

 

Hey Acid,

 

First you should try 4.x latest.

a reload of 50 domains every 10 second doesn't make sense.

I don't understand the config and the setup.

for 50 sites you just need a basic script and even one in bash will work dor you with grep.

I wrote an example in ruby a while ago, I will try to find it in the next week.

Maybe the server is overloaded.

 

I will try to give you an example later on.

 

Eliezer

 

 

?????? ??? ??, 30 ???? 2021, 20:41, ??? acidflash acidflash ?<acidflash_ at linuxmail.org <mailto:acidflash_ at linuxmail.org> >:

 

Hi Eliezer,

I hope your doing ok. Thanks for the reply. Yeah currently what I am doing is:

include /etc/squid/blockedsites.list
and adding the ACL's and the denies in the list file. What version do you recommend I upgrade to, and is this a known issue? The list is actually pretty small, probably no more than 50 sites or so, and thats split across 4 or 5 groups (different ACL's). I'll look into ufdbguard and the other projects as well, does this sound familiar though? If you think that the best path forward is to alleviate the burden off of squid to some external tool, I could probably think up a few hacks to for that, but would obviously prefer to keep it all within squid. Is this occurance common with squid -k reconfigure and dstdomain matching? Thanks for your time. Stay safe. 

  

Sent: Sunday, March 28, 2021 at 4:42 AM
From: "Eliezer Croitoru" <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> >
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] [SPAM] Squid stops serving requests after squid -k reconfigure

Hey Acid,

 

Haven?t seen you here for a very long time.

The first thing is to upgrade squid if possible?

 

It?s better that you won?t use squid -kreconf for big blacklists.

Instead you should use some external software to match the blacklists.

The most recommended software these days is ufdbguard.

Depends on the size of your blacklist your might need to find the right solution.

The best solution would be to store the list in ram somehow.

Have you tried some kind of rbl server?

 

At the time I wrote some code to and some of it was merged into:

https://github.com/looterz/grimd

 

It has a reload url so you can update the files on disk and send a reload.

 

Another service I am using is:

https://github.com/andybalholm/redwood

 

Which has a ?Classification Service? function.

It?s pretty easy to write a json http client that can run queries against this classification service.

 

Also you?d better use a file In the dstdomain ac ie:

acl Blacklist dstdomain ?/var/blacklists/xyx.list?

http_access deny Blacklist

 

and inside the xyx.list file just add lines of domains like

.blacklisted-domain.com <http://blacklisted-domain.com> 

.example.com <http://example.com> 

 

Etc..

 

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

Zoom: Coming soon

 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of acidflash acidflash
Sent: Saturday, March 27, 2021 10:55 AM
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [SPAM] [squid-users] Squid stops serving requests after squid -k reconfigure

 

I have gone through the forums, and I haven't found an answer to the question, although it has been asked more than once.

I am running squid 3.5.X on Centos 7, the compile options are:
"configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group,kerberos_ldap_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'"

 

I have a service which adds domains to a blacklist file, and then calls squid -k reconfigure.
Instead of writing to the file, this service updates the file completely with new rules,
by deleting the old file, and creating a new one in its place, and then calling squid -k reconfigure.
After doing this, on odd occasions, squid will stop serving traffic completely,
until you do a squid stop, and squid start. After shutting down squid,
and starting squid up with the same rules, squid will continue to work normally.
Its probably worth mentioning that during the time that these events are taking place,
the server is under quite a bit of load, and clients don't stop sending requests via the server.
What these directives look like:

acl Porn dstdomain .xnxx.com <http://xnxx.com>  .sex.com <http://sex.com>  
acl Drugs dstdomain .drugs.com <http://drugs.com>  .silkroad.eu <http://silkroad.eu>  
http_access deny Porn
http_access deny Drugs

 

This also seems to be amplified when there are several squid workers (child processes) running.
In regards to order, these ACL's are above any other ACL's in the list. We have a very basic squid conf file that looks like this:
## START OF FILE
http_port 3128
cache deny all
#
access_log /var/log/squid/access.log 
cache_store_log none
cache_log /dev/null
logfile_rotate 4
#
auth_param basic program /usr/lib64/squid/basic_db_auth --dsn "DBI:mysql:host=XX.XX.XX.XX;port=XXXX;database=XXXXX" --user XXXXXX --password XXXXXXXX --plaintext --persist
# 
acl db-auth proxy_auth REQUIRED
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive on
#
connect_timeout 55 minutes
#
request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access User-Agent allow all
request_header_access Cookie allow all
request_header_access All deny all
dns_v4_first on
via off
forwarded_for off
follow_x_forwarded_for deny all
dns_nameservers 8.8.8.8 8.8.4.4
## END OF FILE

 

Your help is greatly appreciated, maybe there has been some insight into this issue after 10+ years since the last time it was asked.

 

_______________________________________________ squid-users mailing list squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>  http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210331/c872f363/attachment.htm>

From Joseph.Garbacik at netapp.com  Wed Mar 31 17:59:47 2021
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Wed, 31 Mar 2021 17:59:47 +0000
Subject: [squid-users] Linking Squid Logs
Message-ID: <MN2PR06MB5951FD073FE9AE408423FFAD987C9@MN2PR06MB5951.namprd06.prod.outlook.com>

In my squid.conf, I have the following logformat which passes all the data from the client via the load balancer to the squid server as headers:
logformat MyLogFormat  ---> local_time="[%tl]" squid_service=%{service}note squid_status=%Ss squid_hierarchy_status=%Sh ** haproxy_id=%{X-Request-Id}>h orig_src_ip=%{X-Client-Egress-Ip}>h orig_src_port=%{X-Client-Egress-Port}>h  haproxy_ingress_ip=%{X-Haproxy-Ingress-Ip}>h haproxy_ingress_port=%{X-Haproxy-Ingress-Port}>h haproxy_egress_ip=%>a haproxy_egress_port=%>p squid_ingress_ip=%>la squid_ingress_port=%>lp squid_egress_ip=%<la squid_egress_port=%<lp dst_ip=%<a dst_host=%<A dst_port=%<p ident_username=%[ui username=%[un request_method=%rm request="%rm %ru HTTP/%rv" status_code_from_server=%>Hs status_code_to_client=%<Hs referer="%{Referer}>h" user_agent="%{User-Agent}>h" protocol_version=%rv ** dns_response_time=%dt response_time=%tr mime_type=%mt *XFER*  total_request_size=%>st total_reply_size=%<st ** %{src_zone}note %{dst_zone}note %{method_category}note %{dst_category}note %{file_upload}note ** REQUEST HEADERS %>h *** RESPONSE HEADERS %<h *** tag_returned=%et tag_string="%ea" previous_hop_mac=%>eui peer_response_time=%<pt total_response_time=%<tt *SSL* src_ssl_negotiated_version=%ssl::>negotiated_version dst_ssl_negotiated_version=%ssl::<negotiated_version src_tls_hello_version=%ssl::>received_hello_version  dst_tls_hello_version=%ssl::<received_hello_version src_tls_max_version=%ssl::>received_supported_version dst_tls_max_version=%ssl::<received_supported_version src_tls_cipher=%ssl::>negotiated_cipher dst_tls_cipher=%ssl::<negotiated_cipher ssl_bump=%<bs ssl_bump_mode=%ssl::bump_mode ssl_sni=%ssl::>sni src_cert_subject="%ssl::>cert_subject" src_cert_issuer="%ssl::>cert_issuer" dst_cert_subject="%ssl::<cert_subject" dst_cert_issuer="%ssl::<cert_issuer" cert_errors="%ssl::<cert_errors" *** error_page_presented=%err_code err_detail="%err_detail"  rule_id=%{ruleid}note rule_type=%{ruletype}note  XFF=%{X-Forwarded-For}>h dst_app=%{dst_app}note

This creates the two logs at the end of this message, What I am wondering is:

  1.  Why aren't all the request headers (look between  ** REQUEST HEADERS and *** RESPONSE HEADERS in each log) seen in the first log present in the second log
  2.  I'm assuming since squid is then making the request in the second log, it leaves the items in Flow0 (client ? load balancer) empty but does retain the data for flow1 (load-balancer-> squid)and flow2 (squid -> destination). Even the XFF is not passed. It there anyway to included retain this data?
  3.  Is there a way to generate an unique Id for each flow so, besides the data in flow0, once can easily link these logs together?

Thanks


Which generates these two logs when doing SSL intercept
Log 1-----
2021-03-31T12:22:08.402609+00:00 squid1 ---> local_time="[31/Mar/2021:08:22:08 -0400]" squid_service=explicit squid_status=NONE squid_hierarchy_status=HIER_DIRECT ** haproxy_id=73834348 | **Flow0** src_ip=10.11.63.205 src_port=55624 haproxy_ingress_ip=192.16.8.1.33 haproxy_ingress_port=3128 | ** Flow1** haproxy_egress_ip=192.16.8.1.39 haproxy_egress_port=6079 squid_ingress_ip=192.16.8.1.36 squid_ingress_port=3128 | ** Flow2* squid_egress_ip=192.16.8.1.40 squid_egress_port=55984 dst_ip=10.51.129.182 dst_host=myhost.foo.com dst_port=443 ident_username=- username=- request_method=CONNECT request="CONNECT myhost.foo.com:443 HTTP/1.1" status_code_from_server=200 status_code_to_client=- referer="-" user_agent="git/2.7.4" protocol_version=1.1 ** dns_response_time=- response_time=174 mime_type=- *XFER* total_request_size=763 total_reply_size=0 ** src_zone=CoreLab - method_category=Safe - - ** REQUEST HEADERS User-Agent=git/2.7.4 HDR_Proxy-Connection=Keep-Alive HDR_X-Client-Environment=SecLab HDR_X-Client-Environment=Corporate HDR_X-Client-IP=10.11.63.205 HDR_X-Proxy-Channel=3128 HDR_X-Haproxy-Role=Squid HDR_X-Correlation-ID=73834348  HDR_X-Client-Egress-Ip=10.11.63.205 HDR_X-Client-Egress-Port=55624 HDR_X-Haproxy-Ingress-Ip=192.16.8.1.33 HDR_X-Haproxy-Ingress-Port=3128 HDR_X-Haproxy-Egress-Ip="" HDR_X-Haproxy-Egress-Port="" HDR_X-Server-Ingress-Ip="" HDR_X-Server-Ingress-Port="" HDR_X-Server-Queue=0 HDR_X-App-Node=%3CNOSRV%3E HDR_X-SSL-Cipher="" HDR_X-SSL-Version="" HDR_X-Request-Id=73834348 HDR_X-Forwarded-For=10.11.63.205 HDR_Connection=close HDR_Host=myhost.foo.com:443 HDR_ *** RESPONSE HEADERS - *** tag_returned=- tag_string="-" previous_hop_mac=00:50:56:b8:03:73 peer_response_time=- total_response_time=98 *SSL* src_ssl_negotiated_version=- dst_ssl_negotiated_version=TLS/1.2 src_tls_hello_version=TLS/1.0 dst_tls_hello_version=TLS/1.2 src_tls_max_version=TLS/1.2 dst_tls_max_version=TLS/1.2 src_tls_cipher=- dst_tls_cipher=ECDHE-RSA-AES256-GCM-SHA384 ssl_bump=- ssl_bump_mode=bump ssl_sni=myhost.foo.com src_cert_subject="-" src_cert_issuer="-" dst_cert_subject="/C=US/postalCode=12345/ST=California/L=Sunnyvale/street=123 Any Street/O=Demo, Inc./OU=None/CN=foo.com" dst_cert_issuer="/C=GB/ST=Greater Manchester/L=Salford/O=Sectigo Limited/CN=Sectigo RSA Organization Validation Secure Server CA" cert_errors="-" *** error_page_presented=- err_detail="-" rule_id=Explicit-45-Rule1.conf_2 rule_type=ALLOW XFF="10.11.63.205" squid_dst_app=MyApp

Log 2----
2021-03-31T12:22:08.495914+00:00 squid1 ---> local_time="[31/Mar/2021:08:22:08 -0400]" squid_service=explicit squid_status=TCP_MISS squid_hierarchy_status=HIER_DIRECT ** haproxy_id=- | **Flow0** src_ip=- src_port=- haproxy_ingress_ip=- haproxy_ingress_port=- | ** Flow1** haproxy_egress_ip=192.16.8.1.39 haproxy_egress_port=6079 squid_ingress_ip=192.16.8.1.36 squid_ingress_port=3128 | ** Flow2** squid_egress_ip=192.16.8.1.40 squid_egress_port=55984 dst_ip=10.51.129.182 dst_host=myhost.foo.com dst_port=443 ident_username=- username=- request_method=GET request="GET https://myhost.foo.com/test.js HTTP/1.1" status_code_from_server=401 status_code_to_client=401 referer="-" user_agent="git/2.7.4" protocol_version=1.1 ** dns_response_time=- response_time=33 mime_type=- *XFER* total_request_size=231 total_reply_size=434 ** src_zone=CoreLab - method_category=Safe - - ** REQUEST HEADERS User-Agent=git/2.7.4 HDR_Accept=*/* HDR_Accept-Encoding=gzip HDR_Accept-Language=en-US, *;q=0.9 HDR_Pragma=no-cache HDR_Host=myhost.foo.com HDR_ *** RESPONSE HEADERS HTTP/1.1 401 Unauthorized HDR_Date=Wed, 31 Mar 2021 12:22:07 GMT HDR_%0D *** tag_returned=- tag_string="-" previous_hop_mac=00:50:56:b8:03:73 peer_response_time=32 total_response_time=33 *SSL* src_ssl_negotiated_version=TLS/1.2 dst_ssl_negotiated_version=TLS/1.2 src_tls_hello_version=TLS/1.0 dst_tls_hello_version=TLS/1.2 src_tls_max_version=TLS/1.2 dst_tls_max_version=TLS/1.2 src_tls_cipher=ECDHE-RSA-AES256-GCM-SHA384 dst_tls_cipher=ECDHE-RSA-AES256-GCM-SHA384 ssl_bump=0 ssl_bump_mode=bump ssl_sni=myhost.foo.com src_cert_subject="-" src_cert_issuer="-" dst_cert_subject="/C=US/postalCode=12345/ST=California/L=Sunnyvale/street=123 Any Street/O=Demo, Inc./OU=None/CN=foo.com" dst_cert_issuer="/C=GB/ST=Greater Manchester/L=Salford/O=Sectigo Limited/CN=Sectigo RSA Organization Validation Secure Server CA" cert_errors="-" *** error_page_presented=- err_detail="-" rule_id=Explicit-45-Rule1.conf_2 rule_type=ALLOW XFF="-" squid_dst_app=MyApp

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210331/073a1a75/attachment.htm>

From xeron.oskom at gmail.com  Wed Mar 31 23:35:19 2021
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Wed, 31 Mar 2021 16:35:19 -0700
Subject: [squid-users] client_delay_pools doesn't work as expected
Message-ID: <CAHvB88xYg1XrKcP0Em+TENzK=FZ+QOSATr6Ngrf0189duEMOnw@mail.gmail.com>

Hello.

We've recently had an incident where misbehaving cluster of clients started
fetching 4MB file from squid cache with ~1200 RPS (slowed down to 600 RPS
later) which resulted in up to 2Gb/s of traffic sent to clients from each
of our squid hosts and quickly overloaded squid.

I'm trying to use client_delay_pools to limit bandwidth per client and
prevent misbehaving actors from saturating client-side network / CPU on
squid hosts.

However I can't get it to work reliably. It seems to be working as expected
for cache MISS, e.g. getting a speed limit of 10MB/s. But it's completely
broken for cache HIT, speed I'm getting is ~5KB/s!

The following configuration:

client_delay_pools 1
client_delay_access 1 allow localnet
client_delay_access 1 deny all
client_delay_parameters 1 10000000 20000000

Testing with an already cached big object (2GB ISO file).

client_delay_pools disabled MISS: 20MB/s (probably speed limit on origin
side)
client_delay_pools disabled HIT: 110MB/s (probably EBS disk speed)

client_delay_pools enabled MISS: 10MB/s (limit from client_delay_parameters)
client_delay_pools enabled HIT: 5KB/s (what ???)

I retested with a smaller file (337MB) but it made no difference. Still got
5KB download speed on cache HIT.

Any ideas? Am I doing something wrong? Any other ways to limit client-side
bandwidth?

Squid version:

Squid Cache: Version 4.14
Service Name: squid
configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid'
'--includedir=/usr/include' '--datadir=/usr/share/squid'
'--sharedstatedir=/usr/com' '--localstatedir=/var'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--with-large-files' '--with-maxfd=16384' '--enable-htcp'

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210331/ebbe59bf/attachment.htm>

