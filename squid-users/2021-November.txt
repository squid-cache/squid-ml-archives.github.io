From david at articatech.com  Mon Nov  1 11:55:32 2021
From: david at articatech.com (David Touzeau)
Date: Mon, 1 Nov 2021 12:55:32 +0100
Subject: [squid-users] Squid 5.2 Peer parent TCP connection to x.x.x.x/x
 failed
Message-ID: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>

Hello Community,

We use child Squid proxies that connect to boxes that act as parents.
In version 4.x this configuration does not pose any problem.
In version 5.2, since, we have a lot of errors like :

01h 47mn kid1| TCP connection to 10.32.0.18/3150 failed
01h 47mn kid1| TCP connection to 10.32.0.17/3150 failed
01h 47mn kid1| TCP connection to 10.32.0.17/3150 failed
01h 47mn kid1| TCP connection to 10.32.0.17/3150 failed
01h 47mn kid1| TCP connection to 10.32.0.17/3150 failed
01h 47mn kid1| TCP connection to 10.32.0.17/3150 failed
01h 47mn kid1| TCP connection to 10.32.0.17/3150 failed

However we are sure that the parent proxies are available.
To make sure this is the case, we installed a local HaProxy that scales 
with the parent proxies

The Squid uses the loopback as a parent.

The same problem occurs:
06:19:47 kid1| TCP connection to 127.0.0.1/2320 failed
06:15:13 kid1| TCP connection to 127.0.0.1/2320 failed
06:14:41 kid1| TCP connection to 127.0.0.1/2320 failed
06:14:38 kid1| TCP connection to 127.0.0.1/2320 failed
06:13:15 kid1| TCP connection to 127.0.0.1/2320 failed
06:11:23 kid1| TCP connection to 127.0.0.1/2320 failed

But in no case the local HaProxy service was down

This makes us understand that the parent squid process randomly stalls 
when in fact there is no reason for this to happen.
There is a software problem rather than a network problem

It is possible that the configuration is wrong but we have tried many 
possibilities.

Here is our last configuration

cache_peer 127.0.0.1 parent 2320 0 name=Peer11 no-query default 
connect-timeout=3 connect-fail-limit=5 no-tproxy

maybe we forgot something?

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211101/d9561a8b/attachment.htm>

From rousskov at measurement-factory.com  Mon Nov  1 14:53:20 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 1 Nov 2021 10:53:20 -0400
Subject: [squid-users] Squid 5.2 Peer parent TCP connection to x.x.x.x/x
 failed
In-Reply-To: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>
References: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>
Message-ID: <f29e9040-61c3-f91f-b2a5-6849b99964fd@measurement-factory.com>

On 11/1/21 7:55 AM, David Touzeau wrote:

> The Squid uses the loopback as a parent.
> 
> The same problem occurs:
> 06:19:47 kid1| TCP connection to 127.0.0.1/2320 failed
> 06:15:13 kid1| TCP connection to 127.0.0.1/2320 failed
> 06:14:41 kid1| TCP connection to 127.0.0.1/2320 failed
> 06:14:38 kid1| TCP connection to 127.0.0.1/2320 failed
> 06:13:15 kid1| TCP connection to 127.0.0.1/2320 failed
> 06:11:23 kid1| TCP connection to 127.0.0.1/2320 failed

> cache_peer 127.0.0.1 parent 2320 0 name=Peer11 no-query default
> connect-timeout=3 connect-fail-limit=5 no-tproxy

It is impossible to tell for sure what is going on because Squid does
not (unfortunately; yet) report the exact reason behind these connection
establishment failures or even the context in which a failure has
occurred. You may be able to tell more by collecting/analyzing packet
captures. Developers may be able to tell more if you share, say, ALL,5
debugging logs that show what led to the failure report.

Alex.


From david at articatech.com  Tue Nov  2 14:40:00 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 2 Nov 2021 15:40:00 +0100
Subject: [squid-users] Squid 5.2 Peer parent TCP connection to x.x.x.x/x
 failed
In-Reply-To: <f29e9040-61c3-f91f-b2a5-6849b99964fd@measurement-factory.com>
References: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>
 <f29e9040-61c3-f91f-b2a5-6849b99964fd@measurement-factory.com>
Message-ID: <2b3d22cb-3af2-e84f-c454-5e55eaf399e6@articatech.com>

Hi,

Take time to enable the debug log an parsing the 10GB of logs

Here the piece of code:

2021/11/01 16:50:48.786 kid1| 33,5| AsyncCall.cc(30) AsyncCall: The 
AsyncCall Server::clientWriteDone constructed, this=0x55849cb132b0 
[call252226641]
2021/11/01 16:50:48.786 kid1| 5,5| Write.cc(37) Write: conn9813869 
local=10.33.50.22:3128 remote=10.33.50.109:50157 FD 95 flags=1: sz 4529: 
asynCall 0x55849cb132b0*1
2021/11/01 16:50:48.786 kid1| 5,5| ModEpoll.cc(118) SetSelect: FD 95, 
type=2, handler=1, client_data=0x7f1caaa1a2d0, timeout=0
2021/11/01 16:50:48.786 kid1| 20,3| store.cc(467) unlock: 
store_client::copy unlocking key 115EFC00000000009915000001000000 
e:=sXIV/0x55849dfec190*4
2021/11/01 16:50:48.786 kid1| 20,3| store.cc(467) unlock: 
ClientHttpRequest::doCallouts-sslBumpNeeded unlocking key 
115EFC00000000009915000001000000 e:=sXIV/0x55849dfec190*3
2021/11/01 16:50:48.786 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x55849316fc88
2021/11/01 16:50:48.786 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x55849316fc88
2021/11/01 16:50:48.786 kid1| 84,5| helper.cc(1319) 
StatefulGetFirstAvailable: StatefulGetFirstAvailable: Running servers 4
2021/11/01 16:50:48.786 kid1| 84,5| helper.cc(1344) 
StatefulGetFirstAvailable: StatefulGetFirstAvailable: returning srv-Hlpr469
2021/11/01 16:50:48.786 kid1| 5,4| AsyncCall.cc(30) AsyncCall: The 
AsyncCall helperStatefulHandleRead constructed, this=0x55848ad88730 
[call252226642]
2021/11/01 16:50:48.786 kid1| 5,5| Read.cc(58) comm_read_base: 
comm_read, queueing read for conn9811325 local=[::] remote=[::] FD 49 
flags=1; asynCall 0x55848ad88730*1
2021/11/01 16:50:48.786 kid1| 5,5| ModEpoll.cc(118) SetSelect: FD 49, 
type=1, handler=1, client_data=0x7f1caaa18a20, timeout=0
2021/11/01 16:50:48.786 kid1| 5,4| AsyncCallQueue.cc(61) fireNext: 
leaving helperStatefulHandleRead(conn9811325 local=[::] remote=[::] FD 
49 flags=1, data=0x5584982781c8, size=300, buf=0x558498dde700)
2021/11/01 16:50:48.786 kid1| 1,5| CodeContext.cc(60) Entering: 
master25501192
2021/11/01 16:50:48.786 kid1| 5,3| IoCallback.cc(112) finish: called for 
conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT 
FD 85 flags=1 (0, 0)
2021/11/01 16:50:48.786 kid1| 93,3| AsyncCall.cc(97) ScheduleCall: 
IoCallback.cc(131) will call Http::Tunneler::handleReadyRead(conn9812727 
local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT FD 85 
flags=1, data=0x55849b747e18) [call252202273]
2021/11/01 16:50:48.786 kid1| 5,5| Write.cc(69) HandleWrite: conn9813869 
local=10.33.50.22:3128 remote=10.33.50.109:50157 FD 95 flags=1: off 0, 
sz 4529.
2021/11/01 16:50:48.786 kid1| 5,5| Write.cc(89) HandleWrite: write() 
returns 4529
2021/11/01 16:50:48.787 kid1| 5,3| IoCallback.cc(112) finish: called for 
conn9813869 local=10.33.50.22:3128 remote=10.33.50.109:50157 FD 95 
flags=1 (0, 0)
2021/11/01 16:50:48.787 kid1| 33,5| AsyncCall.cc(97) ScheduleCall: 
IoCallback.cc(131) will call Server::clientWriteDone(conn9813869 
local=10.33.50.22:3128 remote=10.33.50.109:50157 FD 95 flags=1, 
data=0x55849e4c8218) [call252226641]
2021/11/01 16:50:48.787 kid1| 1,5| CodeContext.cc(60) Entering: 
master25501192
2021/11/01 16:50:48.787 kid1| 93,3| AsyncCallQueue.cc(59) fireNext: 
entering Http::Tunneler::handleReadyRead(conn9812727 
local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT FD 85 
flags=1, data=0x55849b747e18)
2021/11/01 16:50:48.787 kid1| 93,3| AsyncCall.cc(42) make: make call 
Http::Tunneler::handleReadyRead [call252202273]
2021/11/01 16:50:48.787 kid1| 93,3| AsyncJob.cc(123) callStart: 
Http::Tunneler status in: [state:w FD 85 job26507207]
2021/11/01 16:50:48.787 kid1| 5,3| Read.cc(93) ReadNow: conn9812727 
local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT FD 85 
flags=1, size 65535, retval 7782, errno 0
2021/01 16:50:48.787 kid1| 24,5| Tokenizer.cc(27) consume: consuming 1 bytes
2021/11/01 16:50:48.787 kid1| 24,5| Tokenizer.cc(27) consume: consuming 
3 bytes
2021/11/01 16:50:48.787 kid1| 24,5| Tokenizer.cc(27) consume: consuming 
1 bytes
2021/11/01 16:50:48.787 kid1| 24,5| Tokenizer.cc(27) consume: consuming 
19 bytes
2021/11/01 16:50:48.787 kid1| 24,5| Tokenizer.cc(27) consume: consuming 
2 bytes
2021/11/01 16:50:48.787 kid1| 74,5| ResponseParser.cc(224) parse: 
status-line: retval 1
2021/11/01 16:50:48.787 kid1| 74,5| ResponseParser.cc(225) parse: 
status-line: proto HTTP/1.1
2021/11/01 16:50:48.787 kid1| 74,5| ResponseParser.cc(226) parse: 
status-line: status-code 503
2021/11/01 16:50:48.787 kid1| 74,5| ResponseParser.cc(227) parse: 
status-line: reason-phrase Service Unavailable
2021/11/01 16:50:48.787 kid1| 74,5| ResponseParser.cc(228) parse: 
Parser: bytes processed=34
2021/11/01 16:50:48.787 kid1| 74,5| Parser.cc(192) grabMimeBlock: mime 
header (0-171) {Server: squid^M
Mime-Version: 1.0^M
Date: Mon, 01 Nov 2021 15:50:48 GMT^M
Content-Type: text/html;charset=utf-8^M
Content-Length: 7577^M
X-Squid-Error: ERR_CONNECT_FAIL 110^M
^M
}
2021/11/01 16:50:48.787 kid1| 11,2| HttpTunneler.cc(323) handleResponse: 
Tunnel Server conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 
FIRSTUP_PARENT FD 85 flags=1
2021/11/01 16:50:48.787 kid1| 11,2| HttpTunneler.cc(326) handleResponse: 
Tunnel Server RESPONSE:
---------
<!DOCTYPE HTML>
<html>
<head>
<title>ERROR: The requested URL could not be retrieved</title>
<script type="text/javascript" language="javascript" 
src="/squid-internal-static/icons/silk/jquery-1.8.3.js"></s----------
2021/11/01 16:50:48.787 kid1| 83,3| HttpTunneler.cc(345) 
bailOnResponseError: unsupported CONNECT response status code [state:w 
FD 85 job26507207]
2021/11/01 16:50:48.787 kid1| TCP connection to 127.0.0.1/2320 failed
 ??? current master transaction: master25501192
2021/11/01 16:50:48.787 kid1| 83,5| HttpTunneler.cc(404) callBack: 
conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT 
FD 85 flags=1 [state:w FD 85 job26507207]
2021/11/01 16:50:48.787 kid1| 5,4| AsyncCall.cc(97) ScheduleCall: 
HttpTunneler.cc(409) will call 
TunnelStateData::tunnelEstablishmentDone([SquidErr:46 503]) [call252202269]
2021/11/01 16:50:48.787 kid1| 5,5| comm.cc(1042) 
comm_remove_close_handler: comm_remove_close_handler: FD 85, 
AsyncCall=0x558485cc45e0*2
2021/11/01 16:50:48.787 kid1| 9,5| AsyncCall.cc(60) cancel: will not 
call Http::Tunneler::handleConnectionClosure [call252202270] because 
comm_remove_close_handler
2021/11/01 16:50:48.787 kid1| 5,3| comm.cc(595) commUnsetConnTimeout: 
Remove timeout for conn9812727 local=127.0.0.1:23408 
remote=127.0.0.1:2320 FIRSTUP_PARENT FD 85 flags=1
2021/11/01 16:50:48.787 kid1| 5,3| comm.cc(569) commSetConnTimeout: 
conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT 
FD 85 flags=1 timeout -1
2021/11/01 16:50:48.787 kid1| 5,3| comm.cc(871) _comm_close: start 
closing FD 85 by Connection.cc:89
2021/11/01 16:50:48.787 kid1| 5,3| comm.cc(556) commUnsetFdTimeout: 
Remove timeout for FD 85
2021/11/01 16:50:48.787 kid1| 5,5| comm.cc(737) commCallCloseHandlers: 
commCallCloseHandlers: FD 85
2021/11/01 16:50:48.787 kid1| 5,4| AsyncCall.cc(30) AsyncCall: The 
AsyncCall comm_close_complete constructed, this=0x558484c02d30 
[call252226643]
2021/11/01 16:50:48.787 kid1| 5,4| AsyncCall.cc(97) ScheduleCall: 
comm.cc(942) will call comm_close_complete(FD 85) [call252226643]
2021/11/01 16:50:48.803 kid1| 93,5| AsyncJob.cc(139) callEnd: 
Http::Tunneler::handleReadyRead(conn9812727 local=127.0.0.1:23408 
remote=127.0.0.1:2320 FIRSTUP_PARENT flags=1, data=0x55849b747e18) ends 
job [state:wx job26507207]
2021/11/01 16:50:48.803 kid1| 83,5| HttpTunneler.cc(52) ~Tunneler: 
Http::Tunneler destructed, this=0x55849b747e18
2021/11/01 16:50:48.803 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob 
destructed, this=0x55849b747ec8 type=Http::Tunneler [job26507207]
2021/11/01 16:50:48.803 kid1| 93,3| AsyncCallQueue.cc(61) fireNext: 
leaving Http::Tunneler::handleReadyRead(conn9812727 
local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT flags=1, 
data=0x55849b747e18)


Le 01/11/2021 ? 15:53, Alex Rousskov a ?crit?:
> On 11/1/21 7:55 AM, David Touzeau wrote:
>
>> The Squid uses the loopback as a parent.
>>
>> The same problem occurs:
>> 06:19:47 kid1| TCP connection to 127.0.0.1/2320 failed
>> 06:15:13 kid1| TCP connection to 127.0.0.1/2320 failed
>> 06:14:41 kid1| TCP connection to 127.0.0.1/2320 failed
>> 06:14:38 kid1| TCP connection to 127.0.0.1/2320 failed
>> 06:13:15 kid1| TCP connection to 127.0.0.1/2320 failed
>> 06:11:23 kid1| TCP connection to 127.0.0.1/2320 failed
>> cache_peer 127.0.0.1 parent 2320 0 name=Peer11 no-query default
>> connect-timeout=3 connect-fail-limit=5 no-tproxy
> It is impossible to tell for sure what is going on because Squid does
> not (unfortunately; yet) report the exact reason behind these connection
> establishment failures or even the context in which a failure has
> occurred. You may be able to tell more by collecting/analyzing packet
> captures. Developers may be able to tell more if you share, say, ALL,5
> debugging logs that show what led to the failure report.
>
> Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211102/6cc3266b/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov  2 15:17:59 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 2 Nov 2021 11:17:59 -0400
Subject: [squid-users] Squid 5.2 Peer parent TCP connection to x.x.x.x/x
 failed
In-Reply-To: <2b3d22cb-3af2-e84f-c454-5e55eaf399e6@articatech.com>
References: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>
 <f29e9040-61c3-f91f-b2a5-6849b99964fd@measurement-factory.com>
 <2b3d22cb-3af2-e84f-c454-5e55eaf399e6@articatech.com>
Message-ID: <b14300f3-fe86-ddb8-190f-3a09282f026b@measurement-factory.com>

On 11/2/21 10:40 AM, David Touzeau wrote:
> 2021/11/01 16:50:48.787 kid1| 93,3| Http::Tunneler::handleReadyRead(conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT)

> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: proto HTTP/1.1
> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: status-code 503
> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: reason-phrase Service Unavailable

> Server: squid
> Date: Mon, 01 Nov 2021 15:50:48 GMT
> X-Squid-Error: ERR_CONNECT_FAIL 110

> 2021/11/01 16:50:48.787 kid1| 83,3| bailOnResponseError: unsupported CONNECT response status code
> 2021/11/01 16:50:48.787 kid1| TCP connection to 127.0.0.1/2320 failed


A parent[^1] proxy is a Squid proxy that cannot connect to the server in
question. That Squid proxy responds with an HTTP 503 Error to your Squid
CONNECT request. Your Squid logs the "TCP connection to ... failed"
error that you were wondering about.

This sequence highlights a deficiency in Squid CONNECT error handling
code (and possibly cache_peer configuration abilities). Ideally, Squid
should recognize Squid error responses coming from a parent HTTP proxy
and avoid complaining about remote Squid-origin errors as if they are
local Squid-parent errors. IIRC, some folks still insist on Squid
complaining about the latter "within hierarchy" errors, but the former
"external Squid-origin" errors are definitely not supposed to be
reported to admins via level-0/1 messages in cache.log.


HTH,

Alex.

[^1]: Direct or indirect parent -- I could not tell quickly but you
should be able to tell by looking at addresses, configurations, and/or
access logs. My bet is that it is an indirect parent if you are still
using a load balancer between Squids.



> Le 01/11/2021 ? 15:53, Alex Rousskov a ?crit?:
>> On 11/1/21 7:55 AM, David Touzeau wrote:
>>
>>> The Squid uses the loopback as a parent.
>>>
>>> The same problem occurs:
>>> 06:19:47 kid1| TCP connection to 127.0.0.1/2320 failed
>>> 06:15:13 kid1| TCP connection to 127.0.0.1/2320 failed
>>> 06:14:41 kid1| TCP connection to 127.0.0.1/2320 failed
>>> 06:14:38 kid1| TCP connection to 127.0.0.1/2320 failed
>>> 06:13:15 kid1| TCP connection to 127.0.0.1/2320 failed
>>> 06:11:23 kid1| TCP connection to 127.0.0.1/2320 failed
>>> cache_peer 127.0.0.1 parent 2320 0 name=Peer11 no-query default
>>> connect-timeout=3 connect-fail-limit=5 no-tproxy
>> It is impossible to tell for sure what is going on because Squid does
>> not (unfortunately; yet) report the exact reason behind these connection
>> establishment failures or even the context in which a failure has
>> occurred. You may be able to tell more by collecting/analyzing packet
>> captures. Developers may be able to tell more if you share, say, ALL,5
>> debugging logs that show what led to the failure report.
>>
>> Alex.
> 



From ml at netfence.it  Tue Nov  2 16:17:06 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 2 Nov 2021 17:17:06 +0100
Subject: [squid-users] ftp_port and squidclamav
In-Reply-To: <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>
References: <7e98a5dc-c8fa-e46f-97ae-151f3d30c98b@netfence.it>
 <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
 <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>
 <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>
Message-ID: <9ba12945-cf00-ec42-d727-b62de9848f97@netfence.it>


On 10/12/21 16:51, Alex Rousskov wrote:

> Squid has a configuration option to work around such adaptation service
> deficiencies: force_request_body_continuation. Please see if enabling
> that workaround helps in your environment:
> http://www.squid-cache.org/Doc/config/force_request_body_continuation/

Thanks, but that didn't work: with "force_request_body_continuation 
allow ftptraffic", I'm able to delete remote files, create remote 
directories, but file upload fails.

I'm back to
adaptation_access service_req deny ftptraffic
adaptation_access service_resp deny ftptraffic
which works fine.

  bye & Thanks
	av.


From rousskov at measurement-factory.com  Tue Nov  2 16:18:12 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 2 Nov 2021 12:18:12 -0400
Subject: [squid-users] ftp_port and squidclamav
In-Reply-To: <9ba12945-cf00-ec42-d727-b62de9848f97@netfence.it>
References: <7e98a5dc-c8fa-e46f-97ae-151f3d30c98b@netfence.it>
 <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
 <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>
 <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>
 <9ba12945-cf00-ec42-d727-b62de9848f97@netfence.it>
Message-ID: <07983881-36c3-eb42-3bb8-ec47aa2c091c@measurement-factory.com>

On 11/2/21 12:17 PM, Andrea Venturoli wrote:
> On 10/12/21 16:51, Alex Rousskov wrote:
> 
>> Squid has a configuration option to work around such adaptation service
>> deficiencies: force_request_body_continuation. Please see if enabling
>> that workaround helps in your environment:
>> http://www.squid-cache.org/Doc/config/force_request_body_continuation/
> 
> Thanks, but that didn't work: with "force_request_body_continuation
> allow ftptraffic", I'm able to delete remote files, create remote
> directories, but file upload fails.


Thank you for this update. If you want to solve this mystery, my
original triage recommendation still applies (but now with the
"force_request_body_continuation allow ftptraffic" in squid.conf).

Alex.


From david at articatech.com  Tue Nov  2 16:28:00 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 2 Nov 2021 17:28:00 +0100
Subject: [squid-users] Squid 5.2 Peer parent TCP connection to x.x.x.x/x
 failed
In-Reply-To: <b14300f3-fe86-ddb8-190f-3a09282f026b@measurement-factory.com>
References: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>
 <f29e9040-61c3-f91f-b2a5-6849b99964fd@measurement-factory.com>
 <2b3d22cb-3af2-e84f-c454-5e55eaf399e6@articatech.com>
 <b14300f3-fe86-ddb8-190f-3a09282f026b@measurement-factory.com>
Message-ID: <47bb5639-e80a-6b08-f209-acd563d11cad@articatech.com>

Ok, we will investigate on the Parent Proxy but it seems that when squid 
child claim about TCP failed, it understand that the peer is dead and 
the whole surf is stopped during several times ( a squid -k reconfigure? 
fix the issue quickly? ) because it did not have any other path to 
forward the request.





Le 02/11/2021 ? 16:17, Alex Rousskov a ?crit?:
> On 11/2/21 10:40 AM, David Touzeau wrote:
>> 2021/11/01 16:50:48.787 kid1| 93,3| Http::Tunneler::handleReadyRead(conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT)
>> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: proto HTTP/1.1
>> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: status-code 503
>> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: reason-phrase Service Unavailable
>> Server: squid
>> Date: Mon, 01 Nov 2021 15:50:48 GMT
>> X-Squid-Error: ERR_CONNECT_FAIL 110
>> 2021/11/01 16:50:48.787 kid1| 83,3| bailOnResponseError: unsupported CONNECT response status code
>> 2021/11/01 16:50:48.787 kid1| TCP connection to 127.0.0.1/2320 failed
>
> A parent[^1] proxy is a Squid proxy that cannot connect to the server in
> question. That Squid proxy responds with an HTTP 503 Error to your Squid
> CONNECT request. Your Squid logs the "TCP connection to ... failed"
> error that you were wondering about.
>
> This sequence highlights a deficiency in Squid CONNECT error handling
> code (and possibly cache_peer configuration abilities). Ideally, Squid
> should recognize Squid error responses coming from a parent HTTP proxy
> and avoid complaining about remote Squid-origin errors as if they are
> local Squid-parent errors. IIRC, some folks still insist on Squid
> complaining about the latter "within hierarchy" errors, but the former
> "external Squid-origin" errors are definitely not supposed to be
> reported to admins via level-0/1 messages in cache.log.
>
>
> HTH,
>
> Alex.
>
> [^1]: Direct or indirect parent -- I could not tell quickly but you
> should be able to tell by looking at addresses, configurations, and/or
> access logs. My bet is that it is an indirect parent if you are still
> using a load balancer between Squids.
>
>
>
>> Le 01/11/2021 ? 15:53, Alex Rousskov a ?crit?:
>>> On 11/1/21 7:55 AM, David Touzeau wrote:
>>>
>>>> The Squid uses the loopback as a parent.
>>>>
>>>> The same problem occurs:
>>>> 06:19:47 kid1| TCP connection to 127.0.0.1/2320 failed
>>>> 06:15:13 kid1| TCP connection to 127.0.0.1/2320 failed
>>>> 06:14:41 kid1| TCP connection to 127.0.0.1/2320 failed
>>>> 06:14:38 kid1| TCP connection to 127.0.0.1/2320 failed
>>>> 06:13:15 kid1| TCP connection to 127.0.0.1/2320 failed
>>>> 06:11:23 kid1| TCP connection to 127.0.0.1/2320 failed
>>>> cache_peer 127.0.0.1 parent 2320 0 name=Peer11 no-query default
>>>> connect-timeout=3 connect-fail-limit=5 no-tproxy
>>> It is impossible to tell for sure what is going on because Squid does
>>> not (unfortunately; yet) report the exact reason behind these connection
>>> establishment failures or even the context in which a failure has
>>> occurred. You may be able to tell more by collecting/analyzing packet
>>> captures. Developers may be able to tell more if you share, say, ALL,5
>>> debugging logs that show what led to the failure report.
>>>
>>> Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211102/f606e2a5/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov  2 18:24:26 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 2 Nov 2021 14:24:26 -0400
Subject: [squid-users] Squid 5.2 Peer parent TCP connection to x.x.x.x/x
 failed
In-Reply-To: <47bb5639-e80a-6b08-f209-acd563d11cad@articatech.com>
References: <39f90c0d-1b53-5d28-3d21-608906384d3b@articatech.com>
 <f29e9040-61c3-f91f-b2a5-6849b99964fd@measurement-factory.com>
 <2b3d22cb-3af2-e84f-c454-5e55eaf399e6@articatech.com>
 <b14300f3-fe86-ddb8-190f-3a09282f026b@measurement-factory.com>
 <47bb5639-e80a-6b08-f209-acd563d11cad@articatech.com>
Message-ID: <5ae7df68-711b-3733-77df-a0bb8f76dfee@measurement-factory.com>

On 11/2/21 12:28 PM, David Touzeau wrote:
> Ok, we will investigate on the Parent Proxy but it seems that when squid
> child claim about TCP failed, it understand that the peer is dead

Yes, that is the side effect of the deficiency I was talking about --
Squid inability to recognize that specific error response from the
(indirect) parent Squid as something completely unrelated to the TCP
connection to the next hop (and to the responding parent Squid ability
to serve traffic in general). There are quite a few things that could be
improved here for your and similar use cases.

Alex.


> Le 02/11/2021 ? 16:17, Alex Rousskov a ?crit?:
>> On 11/2/21 10:40 AM, David Touzeau wrote:
>>> 2021/11/01 16:50:48.787 kid1| 93,3| Http::Tunneler::handleReadyRead(conn9812727 local=127.0.0.1:23408 remote=127.0.0.1:2320 FIRSTUP_PARENT)
>>> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: proto HTTP/1.1
>>> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: status-code 503
>>> 2021/11/01 16:50:48.787 kid1| 74,5| parse: status-line: reason-phrase Service Unavailable
>>> Server: squid
>>> Date: Mon, 01 Nov 2021 15:50:48 GMT
>>> X-Squid-Error: ERR_CONNECT_FAIL 110
>>> 2021/11/01 16:50:48.787 kid1| 83,3| bailOnResponseError: unsupported CONNECT response status code
>>> 2021/11/01 16:50:48.787 kid1| TCP connection to 127.0.0.1/2320 failed
>> A parent[^1] proxy is a Squid proxy that cannot connect to the server in
>> question. That Squid proxy responds with an HTTP 503 Error to your Squid
>> CONNECT request. Your Squid logs the "TCP connection to ... failed"
>> error that you were wondering about.
>>
>> This sequence highlights a deficiency in Squid CONNECT error handling
>> code (and possibly cache_peer configuration abilities). Ideally, Squid
>> should recognize Squid error responses coming from a parent HTTP proxy
>> and avoid complaining about remote Squid-origin errors as if they are
>> local Squid-parent errors. IIRC, some folks still insist on Squid
>> complaining about the latter "within hierarchy" errors, but the former
>> "external Squid-origin" errors are definitely not supposed to be
>> reported to admins via level-0/1 messages in cache.log.
>>
>>
>> HTH,
>>
>> Alex.
>>
>> [^1]: Direct or indirect parent -- I could not tell quickly but you
>> should be able to tell by looking at addresses, configurations, and/or
>> access logs. My bet is that it is an indirect parent if you are still
>> using a load balancer between Squids.
>>
>>
>>
>>> Le 01/11/2021 ? 15:53, Alex Rousskov a ?crit?:
>>>> On 11/1/21 7:55 AM, David Touzeau wrote:
>>>>
>>>>> The Squid uses the loopback as a parent.
>>>>>
>>>>> The same problem occurs:
>>>>> 06:19:47 kid1| TCP connection to 127.0.0.1/2320 failed
>>>>> 06:15:13 kid1| TCP connection to 127.0.0.1/2320 failed
>>>>> 06:14:41 kid1| TCP connection to 127.0.0.1/2320 failed
>>>>> 06:14:38 kid1| TCP connection to 127.0.0.1/2320 failed
>>>>> 06:13:15 kid1| TCP connection to 127.0.0.1/2320 failed
>>>>> 06:11:23 kid1| TCP connection to 127.0.0.1/2320 failed
>>>>> cache_peer 127.0.0.1 parent 2320 0 name=Peer11 no-query default
>>>>> connect-timeout=3 connect-fail-limit=5 no-tproxy
>>>> It is impossible to tell for sure what is going on because Squid does
>>>> not (unfortunately; yet) report the exact reason behind these connection
>>>> establishment failures or even the context in which a failure has
>>>> occurred. You may be able to tell more by collecting/analyzing packet
>>>> captures. Developers may be able to tell more if you share, say, ALL,5
>>>> debugging logs that show what led to the failure report.
>>>>
>>>> Alex.
> 



From ngtech1ltd at gmail.com  Tue Nov  2 18:53:43 2021
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 2 Nov 2021 20:53:43 +0200
Subject: [squid-users] Error 503 accessing Instagram/facebook via IPv6
In-Reply-To: <6b435bb29b247eca6639ecd04d59438c@graminsta.com.br>
References: <6b435bb29b247eca6639ecd04d59438c@graminsta.com.br>
Message-ID: <000501d7d01a$f613f870$e23be950$@gmail.com>

Hey,

Is this a tproxy or intercept setup?

Eliezer

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
marcelorodrigo at graminsta.com.br
Sent: Saturday, October 30, 2021 09:10
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Error 503 accessing Instagram/facebook via IPv6

Hi,

I have been using squid for several years and am very grateful for the 
solution.

Since last 3-4 days my customers haven't been able to access 
www.instagram.com and Facebook throug IPv6s that were already working as 
proxies for years.

I only get 503 error after a time out.
The strangest thing is that I can connect 20-30% of the attempts.

I didn't make any changes to the VPSs.

I switched IPv6s provider to a totally different subnet, changed several 
equipments, but it didn't work at all.

If I access through the same VPS using the same IPs as squid is running 
using curl command but not going through the proxy It works.

But through Squid it no longer accesses as before.
This only happens with v6 IPs. On V4 Squid runs normally.

I upgraded to Ubuntu 20.04 and Squid 5.2-10 but everything remains the 
same.

My agency depends on it and I've already lost half of my clients.
Could you please help me, even if I have to pay for support?
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From John.Yuen at moodys.com  Wed Nov  3 16:53:31 2021
From: John.Yuen at moodys.com (Yuen, John)
Date: Wed, 3 Nov 2021 16:53:31 +0000
Subject: [squid-users] Squid upgrade failure support questions
In-Reply-To: <AM8PR04MB7745BDB89B0BAC52D28A671B8F839@AM8PR04MB7745.eurprd04.prod.outlook.com>
References: <CO6PR20MB3732B4CED4271476E9E0ED0E9C829@CO6PR20MB3732.namprd20.prod.outlook.com>
 <CO6PR20MB3732B24A5A499F9CEC8CDB559C829@CO6PR20MB3732.namprd20.prod.outlook.com>
 <f112cec8-8c67-7cbe-c68f-c89a100de3bc@treenet.co.nz>
 <AM8PR04MB7745BDB89B0BAC52D28A671B8F839@AM8PR04MB7745.eurprd04.prod.outlook.com>
Message-ID: <CO6PR20MB3732F173AB16179F76EE9BD09C8C9@CO6PR20MB3732.namprd20.prod.outlook.com>

Hi Rafael,

Thanks for getting back to me on this. The link you provided to me for issues #95 and #101 pretty much sound like what I'm experiencing on my 'Squid-01' server after it got upgraded to v4.14.  As you mentioned, I checked the non-working 'Squid-01' server as well as the working 'Squid-02' server to compare the two. 

- On the non-working 'Squid-01' server, the 'Squid for Windows' service is running, and I can see the associated ' Diladele.Squid.Service.exe' service executable running and listed in the Windows task manager. (under the 'Details' tab). I also see a separate 'Diladele.Squid.Tray.exe' process running as well in the Windows Task Manager. However, I don't see the 'squid.exe' process listed in the Windows Task manager. I wasn't aware there were 2 separate processes that should be running, until you told me. The CPU, memory, etc. seem fine, so it can't be related to those things. Lastly, when I typed in the following command from a command prompt, I don't see any entries as it should if Squid was working and listening properly on port 3128:
=> netstat -a | find "3128"

- On the working 'Squid-02' server, the 'Squid for Windows' service is running, and I can see the associated ' Diladele.Squid.Service.exe' service executable running and listed in the Windows task manager. (under the 'Details' tab). I don't see a separate 'Diladele.Squid.Tray.exe' process running (like on the 'Squid-01' server) in the Windows Task Manager. However, I do see the 'squid.exe' process listed in the Windows Task manager. Lastly, when I type in the following command from a command prompt, I see that it's listening to the default Squid port 3128 and that there are remote computers establishing connections to the Squid port 3128, as it should since it's working and listening properly on port 3128:
=> netstat -a | find "3128"

The question is, why isn't the 'squid.exe' process running and listed in the Windows Task manager on the 'Squid-01' server after it was upgraded, and even with an uninstall and re-install of Squid v4.14? Not sure if the upgrade did anything to make it not work and listen on port 3128 on the 'Squid-01' server. Please let me know if this is a known issue and if others are facing the same issue. If there's a way to fix this, please provide the detailed steps on how to make it work again. As mentioned, we have other Squid servers that we need to upgrade the version to the same v4.14, so we don't want to face the same upgrade issues like this one. Or maybe you can tell me what's the best way to upgrade to v4.14 so I can avoid having this issue in the future.

Both the 'Squid-01' and 'Squid-02' are virtual machines running on Microsoft Hyper-V physical servers. So, will that be an issue as you mentioned v4.14 does not work on older processors? Does it matter about the CPU age on the physical Hyper-V servers, where the 2 VMs reside? Please advise.

Hope all this makes sense. If you have any questions, please let me know. Thanks for any help in advance. 

John

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Rafael Akchurin
Sent: Monday, October 25, 2021 2:07 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid upgrade failure support questions

 

CAUTION: This email originated from outside of Moody's. Do not click links or open attachments unless you recognize the sender and know the content is safe.

 

Be sure to also check the actual squid.exe is running - not just the service wrapper, you might be facing the https://github.com/diladele/squid-windows/issues/101 bug (4.14 does not work on older processors)

Best regards,
Rafael

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Sunday, 24 October 2021 22:45
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid upgrade failure support questions

On 25/10/21 6:33 am, Yuen, John wrote:
> 
> http_port 3128
> 
> The 'Squid for Windows' service is set to 'Automatic' startup type and 
> shows the 'Running' status. So it can't be that. I can telnet to port
> 3128 on the new working Squid v4.14 server. But I can't telnet to the 
> same port 3128 on the upgraded/non-working Squid server. When I do, it 
> doesn't work and errors out like this:
> 
> C:\>telnet Squid-01 3128
> 
> Connecting To Squid-01...Could not open connection to the host, on 
> port
> 3128: Connect failed
> 

At no point in your post do I see any mention of what the IP addresses are. Can you please indicate what "localhost", "Squid-01" resolve to on both the machine used as client and the Squid machine. And what IP(s) those firewall rules by Local subnet "Any" and Remote address "local subnet".


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From jason.spashett at menlosecurity.com  Thu Nov  4 15:14:26 2021
From: jason.spashett at menlosecurity.com (Jason Spashett)
Date: Thu, 4 Nov 2021 15:14:26 +0000
Subject: [squid-users] acl / format code evaluation
Message-ID: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>

Hello,

I am using squid 5, and after reading the following I have attempted
to link the connect requests to the other requests within a TLS
tunnel.
http://lists.squid-cache.org/pipermail/squid-users/2021-April/023526.html

I added an extra log format code to squid 5, called %random, which
always returns a random string, in the hopes of using this to stamp
against log entries to tie them together.

The squid configuration follows, this seems to partially work in that
the requests in the tunnel have the same conn_id, but the connect
request itself has a different one, which leads me to to believe that
the format code is being evaluated twice in that case perhaps. I am
not sure why the in tunnel requests appear to work, but the id on the
connect request is different.

Note that I added %master_xaction to the log too, to see if that
worked, and it does, but it's not particularly practical on it's own
due to the problem of it not being unique enough.

Can anyone tell me why this isn't working, and or when the log format
codes get evaluated.


Squid configuration
-------------------
#
acl connection_id_acl annotate_client conn_id+="%master_xaction/%random"
acl has_conn_id_acl note conn_id
acl set_conn_id_once_acl any-of has_conn_id_acl connection_id_acl
note "" "" set_conn_id_once_acl
#
logformat log time="%tl" conn_id=%{conn_id}note request_type=%>rm url=%>ru

log output
----------
time="04/Nov/2021:14:54:19 +0000" conn_id=2550/Fh0Lje1
request_type=CONNECT url=blog.jason.spashett.com:443
time="04/Nov/2021:14:54:19 +0000" conn_id=2550/e5sVhqi
request_type=GET
url=https://blog.jason.spashett.com/minecraft-4k-ported-to-the-d-programming-language/
time="04/Nov/2021:14:54:20 +0000" conn_id=2550/e5sVhqi
request_type=GET url=https://blog.jason.spashett.com/css/main.css


From squid3 at treenet.co.nz  Fri Nov  5 08:53:54 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Nov 2021 21:53:54 +1300
Subject: [squid-users] acl / format code evaluation
In-Reply-To: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
References: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
Message-ID: <f2321709-eb45-9a58-50c6-55d0bcb7f6e8@treenet.co.nz>

On 5/11/21 04:14, Jason Spashett wrote:
> Hello,
> 
> I am using squid 5, and after reading the following I have attempted
> to link the connect requests to the other requests within a TLS
> tunnel.
> 
> Can anyone tell me why this isn't working, and or when the log format
> codes get evaluated.

The logformat %macros get expanded any time Squid needs to use the 
format string containing them.


For your config snippet below



That means usually;
  - helper queries at several points processing each request/transaction,
  - each time those ACLs of yours are *checked*,
  - log outputs when each request finishes, and
  - deny_info URL generation for redirection.


> 
> Squid configuration
> -------------------
> #
> acl connection_id_acl annotate_client conn_id+="%master_xaction/%random"
> acl has_conn_id_acl note conn_id
> acl set_conn_id_once_acl any-of has_conn_id_acl connection_id_acl
> note "" "" set_conn_id_once_acl
> #
> logformat log time="%tl" conn_id=%{conn_id}note request_type=%>rm url=%>ru
> 
> log output
> ----------
> time="04/Nov/2021:14:54:19 +0000" conn_id=2550/Fh0Lje1
> request_type=CONNECT url=blog.jason.spashett.com:443
> time="04/Nov/2021:14:54:19 +0000" conn_id=2550/e5sVhqi
> request_type=GET
> url=https://blog.jason.spashett.com/minecraft-4k-ported-to-the-d-programming-language/
> time="04/Nov/2021:14:54:20 +0000" conn_id=2550/e5sVhqi
> request_type=GET url=https://blog.jason.spashett.com/css/main.css

This looks like its working to me.

  "2550/" is the TCP connection being handled.

  "2550/Fh0Lje1" is the CONNECT received via TCP.

  "2550/e5sVhqi" are the requests decoded from inside the CONNECT tunnel.


The problem you have is that the CONNECT request ceases to exist at the 
point it is accepted to be decrypted. The TLS handshake takes time - so 
the conn_id %random value you assigned to that CONNECT is long gone by 
the time the decrypted requests are received. We have several bugs open 
about this situation, but my fix has got stuck with QA rejections from 
other team memmbers.

Amos


From jason.spashett at menlosecurity.com  Fri Nov  5 09:57:02 2021
From: jason.spashett at menlosecurity.com (Jason Spashett)
Date: Fri, 5 Nov 2021 09:57:02 +0000
Subject: [squid-users] acl / format code evaluation
In-Reply-To: <f2321709-eb45-9a58-50c6-55d0bcb7f6e8@treenet.co.nz>
References: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
 <f2321709-eb45-9a58-50c6-55d0bcb7f6e8@treenet.co.nz>
Message-ID: <CANj0NTrGk-Tw_gnmDw51549hupW4sQ3K1f2TM-CXMVXHKOH_HA@mail.gmail.com>

On Fri, 5 Nov 2021 at 09:03, Amos Jeffries <squid3 at treenet.co.nz> wrote:
...
> For your config snippet below
>
> That means usually;
> - helper queries at several points processing each request/transaction,
> - each time those ACLs of yours are *checked*,
> - log outputs when each request finishes, and
> - deny_info URL generation for redirection.
...
> > Squid configuration
> > -------------------
> > #
> > acl connection_id_acl annotate_client conn_id+="%master_xaction/%random"
> > acl has_conn_id_acl note conn_id
> > acl set_conn_id_once_acl any-of has_conn_id_acl connection_id_acl
> > note "" "" set_conn_id_once_acl
> > #
> > logformat log time="%tl" conn_id=%{conn_id}note request_type=%>rm url=%>ru
...
> The problem you have is that the CONNECT request ceases to exist at the
> point it is accepted to be decrypted. The TLS handshake takes time - so
> the conn_id %random value you assigned to that CONNECT is long gone by
> the time the decrypted requests are received. We have several bugs open
> about this situation, but my fix has got stuck with QA rejections from
> other team memmbers.
...

Thanks Amos,

Is it not the case then, that connection_id_acl should be evaluated on connect,
once, and the resulting format code evaluated (expanded), and conn_id "stamped"
on the client connection at this time?

I appreciate that the connect request may dematerialise, but by this
time is there
not a note on the client connection with conn_id="some already-evaluated thing"
which persists until the client connection closes?

What perhaps you mean is that the client connection object does change
on account of the connect
being intercepted, is that what is happening?

Regards,
Jason


From rousskov at measurement-factory.com  Fri Nov  5 14:35:21 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Nov 2021 10:35:21 -0400
Subject: [squid-users] acl / format code evaluation
In-Reply-To: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
References: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
Message-ID: <d84a6f32-f740-57c9-0180-8bc26251ede1@measurement-factory.com>

On 11/4/21 11:14 AM, Jason Spashett wrote:

> I am using squid 5, and after reading the following I have attempted
> to link the connect requests to the other requests within a TLS
> tunnel.
> http://lists.squid-cache.org/pipermail/squid-users/2021-April/023526.html

> I added an extra log format code to squid 5, called %random, which
> always returns a random string, in the hopes of using this to stamp
> against log entries to tie them together.

It is difficult for me to imagine the right use case for identifying
clients with random strings, especially if you are adding new %codes:

* If you want uniqueness within a Squid kid, then you should use a
simple ever-increasing counter, not a random string.

* If you want uniqueness across kids, processes, and instances, then you
probably need a UUID, not just a random string.


> The squid configuration follows, this seems to partially work in that
> the requests in the tunnel have the same conn_id, but the connect
> request itself has a different one, which leads me to to believe that
> the format code is being evaluated twice in that case perhaps.

Bugs notwithstanding, Squid applies the note directive once for every
client transaction. That includes one or more CONNECT transactions and
each GET/POST/etc. transaction inside the bumped CONNECT tunnel. Squid
applies the note directive at the end of the client transaction
lifetime. The resulting annotations may be logged (via %note) when/if
the transaction is logged.


> Note that I added %master_xaction to the log too, to see if that
> worked, and it does, but it's not particularly practical on it's own
> due to the problem of it not being unique enough.

Whether it is unique enough depends on the desired uniqueness scope. I
bet that, strictly speaking, it is no less unique than your current
%random algorithm!

The first CONNECT request on a TCP connection should have a unique
master transaction ID compared to all other CONNECT requests on other
TCP connections accepted by the same kid process. That is exactly what
you need if you need per-process uniqueness. If you need a larger
uniqueness scope, then you probably need to use UUIDs.


> Can anyone tell me why this isn't working,

I am not sure at all, but it could be a Squid bug related to how client
annotations are propagated across CONNECT and in-tunnel requests. I am
also worried about the note directive application time, but I cannot
offer a specific explanation on how it can break things in your use case.


I suggest that you revise your configuration to:

1. Avoid the note directive. It might be applied too late, and it is not
meant to do what you are trying to accomplish.

2. Use ssl_bump directive to annotate the client during SslBump step1.
This directive is evaluated early, even before http_access. For plain
requests, you can use http_access.

3. Continue to annotate each client exactly once. Replace "+=" with "="
to avoid creating the impression that you are updating annotations.


> and or when the log format codes get evaluated.

They are evaluated when used. In your specific case, you have not told
us how your configuration uses logformat "log", but if you are using
"log" just for the access_log directive, then "log" %codes are evaluated
when Squid creates the access.log record for each transaction.


HTH,

Alex.



> 
> Squid configuration
> -------------------
> #
> acl connection_id_acl annotate_client conn_id+="%master_xaction/%random"
> acl has_conn_id_acl note conn_id
> acl set_conn_id_once_acl any-of has_conn_id_acl connection_id_acl
> note "" "" set_conn_id_once_acl
> #
> logformat log time="%tl" conn_id=%{conn_id}note request_type=%>rm url=%>ru
> 
> log output
> ----------
> time="04/Nov/2021:14:54:19 +0000" conn_id=2550/Fh0Lje1 request_type=CONNECT url=blog.jason.spashett.com:443
> time="04/Nov/2021:14:54:19 +0000" conn_id=2550/e5sVhqi request_type=GET url=https://blog.jason.spashett.com/minecraft-4k-ported-to-the-d-programming-language/
> time="04/Nov/2021:14:54:20 +0000" conn_id=2550/e5sVhqi request_type=GET url=https://blog.jason.spashett.com/css/main.css
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jason.spashett at menlosecurity.com  Fri Nov  5 15:00:05 2021
From: jason.spashett at menlosecurity.com (Jason Spashett)
Date: Fri, 5 Nov 2021 15:00:05 +0000
Subject: [squid-users] acl / format code evaluation
In-Reply-To: <d84a6f32-f740-57c9-0180-8bc26251ede1@measurement-factory.com>
References: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
 <d84a6f32-f740-57c9-0180-8bc26251ede1@measurement-factory.com>
Message-ID: <CANj0NTqwaYC7JX5U3=6XimmnPn9m0gcQ+-f-j54-Rq49DDuxGA@mail.gmail.com>

On Fri, 5 Nov 2021 at 14:35, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
...
> They are evaluated when used. In your specific case, you have not told
> us how your configuration uses logformat "log", but if you are using
> "log" just for the access_log directive, then "log" %codes are evaluated
> when Squid creates the access.log record for each transaction.
>
>
> HTH,
>
> Alex.

Thanks Alex,

After looking at the source code again I am beginning to understand
how the format codes are used.

On the subject of unique ids, the code %random isn't necessarily of my
choosing. But it has 54^8 combinations, and comes from /dev/urandom by
way of a Mersenne Twister. But I will give due consideration to usage
of such a thing. I think you are suggesting that a "proper" UUID may
be better, that incorporates a time element and is suitably long.

Given that %master_axtion is a counter which resets when you restart
squid, and is not unique among squids (or restarts), is there not a
case to be made for making one available?

Regards,
Jason


From rousskov at measurement-factory.com  Fri Nov  5 16:11:26 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 5 Nov 2021 12:11:26 -0400
Subject: [squid-users] acl / format code evaluation
In-Reply-To: <CANj0NTqwaYC7JX5U3=6XimmnPn9m0gcQ+-f-j54-Rq49DDuxGA@mail.gmail.com>
References: <CANj0NTqFKEbtAzS9KN-9_jcg1aAVDHRtCFB=yLtcskiaSUT-zA@mail.gmail.com>
 <d84a6f32-f740-57c9-0180-8bc26251ede1@measurement-factory.com>
 <CANj0NTqwaYC7JX5U3=6XimmnPn9m0gcQ+-f-j54-Rq49DDuxGA@mail.gmail.com>
Message-ID: <54d26826-e84b-366c-5f8a-8f398f8622f9@measurement-factory.com>

On 11/5/21 11:00 AM, Jason Spashett wrote:

> On the subject of unique ids, the code %random isn't necessarily of my
> choosing. But it has 54^8 combinations,

The number of unique values and the period of the random number
generator are largely irrelevant here. What matters here is how soon the
generator will repeat a _single_ value. Many random number generators
repeat too soon, but most random number generators descriptions do not
mention that fact because it is irrelevant for most things random
numbers are (designed to be) used for.

I do not know how soon your random number generator will repeat a single
value, and I would worry about that if global uniqueness is the goal.


> I think you are suggesting that a "proper" UUID may be better

What is "better" depends on your desired uniqueness scope. If one does
not care about cross-instance uniqueness and kid restarts, then an
ever-increasing 64-bit integer counter is the best for them. This is
similar to what %master_xaction provides.


> Given that %master_axtion is a counter which resets when you restart
> squid, and is not unique among squids (or restarts), is there not a
> case to be made for making one available?

Yes, there is absolutely such a use case! It can already be addressed
via external ACLs, but a quality pull request adding native UUID support
to globally-uniquely identify Squid processes (%process_uuid) should be
welcomed IMO. The combination of (slowly computed once per process)
%process_uuid and (fast) %master_xaction ID would give many admins what
they want.


Cheers,

Alex.


From mavuluruprasad at gmail.com  Sun Nov  7 10:56:09 2021
From: mavuluruprasad at gmail.com (prasad mavuluru)
Date: Sun, 7 Nov 2021 10:56:09 +0000
Subject: [squid-users] Fwd: Need help squid with whitelist
In-Reply-To: <CABiikPoy=3xNN-tTwJ5dr9BWNfZXke88LT4srRtBVRGgy4dh-A@mail.gmail.com>
References: <CABiikPoy=3xNN-tTwJ5dr9BWNfZXke88LT4srRtBVRGgy4dh-A@mail.gmail.com>
Message-ID: <CABiikPpct6EFFWcKOLFge6BoC3j8gY9mFamo4Hatg=nuOhpdNg@mail.gmail.com>

Hi  All,

I have a squid proxy running on port 3128 allowing whitelisted file to
allow specific sites. Is it possible to have another port squid proxy to
allow different whitelist files ?. Is there any other approach to achieve
this?

Thanks,
Prasad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211107/9012283f/attachment.htm>

From m_zouhairy at ckta.by  Mon Nov  8 10:30:12 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 8 Nov 2021 13:30:12 +0300
Subject: [squid-users] problem in squid log
Message-ID: <1882cbd9-4f5e-f570-826f-6742e492ecd5@ckta.by>

when i run sarg

SARG: sarg version: 2.4.0 Jan-16-2020
SARG: Reading access log file: /var/log/squid/access.log
SARG: Log format identified as "squid log format" for 
/var/log/squid/access.log
SARG: The following line read from /var/log/squid/access.log could not 
be parsed and is ignored
1636349341.484     12 10.184.0.2 NONE_NONE/400 20417 GET 
https://zen.yandex.by/lz5XeGt8f/ir4w02684/13f5fd2qrAJ2/p_CMhOoMLrxy4M2QFtQI-HLBvD5tHT6JdGbykwp9eDzBNcrpN2RIqcyiFH9pWekXwFsAEtIMz3_5FVo5y8zXIrAwGER6-e4cM0VckNJR_CjjEd2OObzKrHDSM2ZrfFzJ9CELTSJAeFt45wBcaGm_VqdcIXKVKFp7THc-uX7PdjLGAUpRv63aKSdE2OOnMXyOt0SJK0vNXql0thIirh9cGORGu31DYR9cCKZAW9gYjiGgfTFlxfgLOitwTohOyMZzx3ZNcK_K-rk2Vb_
....
UPVydoTW1636349696.714    629 10.106.0.2 NONE_NONE/200 0 CONNECT 
azscus1-client-s.gateway.messenger.live.com:443 - HIER_DIRECT/40.74.219.49 -
SARG: 4 consecutive errors found in the input log file 
/var/log/squid/access.log

so i think the solution would be to exclude zen.yandex.by from processing ?


From heimarbeit123.99 at web.de  Mon Nov  8 12:19:29 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Mon, 8 Nov 2021 13:19:29 +0100
Subject: [squid-users] Squid very slow with kerberos auth and LDAP Group
 Search(AD)
Message-ID: <trinity-e7fd76b4-da76-4557-8151-461a3254f45b-1636373969338@3c-app-webde-bs60>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211108/0ed93039/attachment.htm>

From uhlar at fantomas.sk  Mon Nov  8 12:26:18 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 8 Nov 2021 13:26:18 +0100
Subject: [squid-users] Fwd: Need help squid with whitelist
In-Reply-To: <CABiikPpct6EFFWcKOLFge6BoC3j8gY9mFamo4Hatg=nuOhpdNg@mail.gmail.com>
References: <CABiikPoy=3xNN-tTwJ5dr9BWNfZXke88LT4srRtBVRGgy4dh-A@mail.gmail.com>
 <CABiikPpct6EFFWcKOLFge6BoC3j8gY9mFamo4Hatg=nuOhpdNg@mail.gmail.com>
Message-ID: <20211108122617.GD30392@fantomas.sk>

On 07.11.21 10:56, prasad mavuluru wrote:
>I have a squid proxy running on port 3128 allowing whitelisted file to
>allow specific sites. Is it possible to have another port squid proxy to
>allow different whitelist files ?. Is there any other approach to achieve
>this?


you can define multiple acls for multiple lists and you can apply different
lists for clients based on multiple conditions.

acl src1 src /etc/squid/src1
acl src2 src /etc/squid/src2
acl dst1 dstdomain /etc/squid/dst1
acl dst2 dstdomain /etc/squid/dst2

http_access allow src1 dst1
http_access allow src2 dst1
http_access deny all

this will

allow ip adresses listed in /etc/squid/src1 to contact detinations listed in /etc/squid/dst1
allow ip adresses listed in /etc/squid/src2 to contact detinations listed in /etc/squid/dst2

deny all the rest.


Note that squid.conf has default rules on what not to allow, e.g. safe
ports, safe SSL ports etc.



-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Depression is merely anger without enthusiasm.


From rousskov at measurement-factory.com  Mon Nov  8 19:42:14 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 8 Nov 2021 14:42:14 -0500
Subject: [squid-users] problem in squid log
In-Reply-To: <1882cbd9-4f5e-f570-826f-6742e492ecd5@ckta.by>
References: <1882cbd9-4f5e-f570-826f-6742e492ecd5@ckta.by>
Message-ID: <68bff468-f887-4254-e6ba-78fdef5e83c1@measurement-factory.com>

On 11/8/21 5:30 AM, Majed Zouhairy wrote:
> when i run sarg
> 
> SARG: sarg version: 2.4.0 Jan-16-2020
> SARG: Reading access log file: /var/log/squid/access.log
> SARG: Log format identified as "squid log format" for
> /var/log/squid/access.log
> SARG: The following line read from /var/log/squid/access.log could not
> be parsed and is ignored
> 1636349341.484???? 12 10.184.0.2 NONE_NONE/400 20417 GET
> https://zen.yandex.by/lz5XeGt8f/ir4w02684/13f5fd2qrAJ2/p_CMhOoMLrxy4M2QFtQI-HLBvD5tHT6JdGbykwp9eDzBNcrpN2RIqcyiFH9pWekXwFsAEtIMz3_5FVo5y8zXIrAwGER6-e4cM0VckNJR_CjjEd2OObzKrHDSM2ZrfFzJ9CELTSJAeFt45wBcaGm_VqdcIXKVKFp7THc-uX7PdjLGAUpRv63aKSdE2OOnMXyOt0SJK0vNXql0thIirh9cGORGu31DYR9cCKZAW9gYjiGgfTFlxfgLOitwTohOyMZzx3ZNcK_K-rk2Vb_
> 
> ....
> UPVydoTW1636349696.714??? 629 10.106.0.2 NONE_NONE/200 0 CONNECT
> azscus1-client-s.gateway.messenger.live.com:443 -
> HIER_DIRECT/40.74.219.49 -
> SARG: 4 consecutive errors found in the input log file
> /var/log/squid/access.log
> 
> so i think the solution would be to exclude zen.yandex.by from processing ?

The correct solution would depend on what you are trying to accomplish
(with sarg), but that solution is unlikely to include disabling logging
of requests to any domains IMHO.

Based on the above output (that could have been changed by multiple mail
agents), it is difficult for me to guess what sarg did not like, but if
you are suffering from Squid SMP workers corrupting each-other
access.log entries, then please see Bug 5173:
https://bugs.squid-cache.org/show_bug.cgi?id=5173


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Nov  9 02:04:48 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Nov 2021 15:04:48 +1300
Subject: [squid-users] Squid very slow with kerberos auth and LDAP Group
 Search(AD)
In-Reply-To: <trinity-e7fd76b4-da76-4557-8151-461a3254f45b-1636373969338@3c-app-webde-bs60>
References: <trinity-e7fd76b4-da76-4557-8151-461a3254f45b-1636373969338@3c-app-webde-bs60>
Message-ID: <136f4720-061c-09a4-7a60-5817b5375f3d@treenet.co.nz>

On 9/11/21 01:19, heimarbeit123.99 at web.de wrote:
> Hello all,
> I finaly got a squid proxy with kerberos authentification and LDAP group 
> check to work! With a small amount of clients(1-10) everything works as 
> it should and the squid is fast(no noticeable waiting time for websites 
> to open). Users get authenticated, different AD groups can access the 
> internet with blacklists/whitelists/full access and so on..

> But as soon as I make the whole company(round about 80 clients) use the 
> new proxy, it begins to be very slow. And by very slow I mean like 1-2 
> minutes waiting time(response time in access.log is like 60000-270000 
> milliseconds for TCP_TUNNEL) until a website is fully loaded.

That could just mean the entire website was loaded through that one 
tunnel. Which is often the case if the clients are using HTTP/2 or HTTPS 
at version 1.1 through it.


> We got a 
> old squid proxy too, but without any authentication (just some dstdomain 
> in general) and it's working great. But the new one is very slow..
> Btw. some of our clients have ipv6, others ipv4(~90%)..There were no 
> errors in cache.log(activated it for some minutes with debug ALL for 
> error checking).

ALL at what level? "ALL,0" log barely anything on a working proxy, but 
will definitely complain about critical problems.



> Can anyone help?
> What I tried so far:
> dns_v4_first on at the very end/very beginning from squid.conf
> enable/disable (memory) caching
> use Google DNS instead of our own

That can be a recipe for slowness. Since the Google DNS service produces 
different responses to every request - even identical repeated ones.


> connect_timeout 3 seconds
> Nothing realy helped..
> Here is my squid.conf:
> ######### allowed port part ########################
> acl Allowed_port port 80????????? # http
> acl Allowed_port port 21????????? # ftp
> acl Allowed_port port 443???????? # https
> acl Allowed_port port 70????????? # gopher
> acl Allowed_port port 210???????? # wais
> acl Allowed_port port 1025-65535? # unregistered ports
> acl Allowed_port port 280???????? # http-mgmt
> acl Allowed_port port 488???????? # gss-http
> acl Allowed_port port 591???????? # filemaker
> acl Allowed_port port 777???????? # multiling http
> acl Allowed_port port 10000?????? # Proofpoint
> acl CONNECT method CONNECT
> http_access deny CONNECT !Allowed_port

Please no. The default security protections were created to cover a 
range of security risks commonly seen in Internet traffic.


# forbids touching protocols that can be confused with HTTP
http_access deny !Safe_ports

# prevent arbitrary exfiltration from malware in the network.
http_access deny CONNECT !SSL_ports


> ##################### cache/logs ########################
> cache_log /dev/null

Do set that to an actual file. You may find the thing causing your 
problems is detectable by Squid.



> logformat myformat %{%d.%m %H:%M:%S}tl %>a %Ss %ru %tr
> access_log /var/log/squid/access.log myformat


> cache deny all
> coredump_dir /dev/null

Core dumps are something you should probably disable at the system level 
instead if you don't want them. Writing all that can be quite time 
consuming, even to /dev/null.


> cache_dir null /dev/null

"null" cache type does not exist anymore. That is one thing your 
cache.log should be warning you about if you could see it.


> cache_store_log none

This is a default in all current Squid.


> ########## Debug ########################
> #debug_options ALL,1 33,2 28,9
> ######################### squid-port #######
> http_port 3128????????????????????????????? #proxy port
> authenticate_ttl 2 hours??????????????????? #auth timeout 
> squid->passwd_server

> acl black_regex url_regex "/etc/squid/regex_black.acl"

> acl white_regex url_regex "/etc/squid/regex_white.acl"
> acl license_regex url_regex "/etc/squid/regex_license_servers_no_auth.acl"
> ############################# allow License Managers ##########
> http_access allow license_regex all

The " all" at the end of this line is pointless. Authentication is not 
being performed by the regex ACL listed.


> ################### Kerberos ##################################
> auth_param negotiate program /lib/squid/negotiate_wrapper_auth -d --ntlm 
> /bin/ntlm_auth --diagnostics --helper-protocol=squid-2.5-ntlmssp 
> --domain=DOMAIN.TLD --kerberos /lib/squid/negotiate_kerberos_auth -d -s 
> HTTP/proxy.domain.tld at DOMAIN.TLD
> auth_param negotiate children 200

You should not need 200 helpers for 80 users with Kerberos operational.


> auth_param negotiate keep_alive on
> ########################## Allow based on group membership ######
> # Authentication required, otherwise Pop-Up
> acl Authenticated_Users proxy_auth REQUIRED
> http_access deny !Authenticated_Users all

FYI: the " all" ACL check at the end of this line forbids Squid sending 
the 40x challenge which triggers popups. Users will be getting full 
rejection 403 instead if they match this line.


> # Define external acl for group check
> external_acl_type ldap_group ipv4 ttl=300 negative_ttl=120 
> children-max=200 %LOGIN /lib/squid/ext_ldap_group_acl -K -S -R \
> -b "ou=Users,DC=domain,DC=tld" \
> -D "ProxyUser at DOMAIN.TLD" \
> -W /etc/squid/authfile \
> -f 
> "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%a,OU=Groups,DC=domain,DC=tl))" 
> \
> -h 192.0.1.1

> acl Users_Internet_Users external ldap_group Users
> http_access allow Users_Internet_Users !black_regex

The above performs the slowest ACL test first. It can be optimized as:
   http_access allow !black_regex Users_Internet_Users all


> http_access deny all
> dns_v4_first on
> connect_timeout 3 seconds


Amos


From m_zouhairy at ckta.by  Tue Nov  9 08:45:53 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Tue, 9 Nov 2021 11:45:53 +0300
Subject: [squid-users] problem in squid log
In-Reply-To: <68bff468-f887-4254-e6ba-78fdef5e83c1@measurement-factory.com>
References: <1882cbd9-4f5e-f570-826f-6742e492ecd5@ckta.by>
 <68bff468-f887-4254-e6ba-78fdef5e83c1@measurement-factory.com>
Message-ID: <590d6623-6b61-dc20-9a67-930721555372@ckta.by>

hmmm, this started happening after the last squid update.. i just 
noticed it is now version 5.2
i have ufdbguard but i don't think i have smp..

the last line of squid conf are

url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode 
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l 
/var/log/squid/
url_rewrite_children 16 startup=8 idle=2 concurrency=4 queue-size=64

i think ufdbguard does not support squid version 5 yet, which might be 
the problem

On 11/8/21 10:42 PM, Alex Rousskov wrote:
> On 11/8/21 5:30 AM, Majed Zouhairy wrote:
>> when i run sarg
>>
>> SARG: sarg version: 2.4.0 Jan-16-2020
>> SARG: Reading access log file: /var/log/squid/access.log
>> SARG: Log format identified as "squid log format" for
>> /var/log/squid/access.log
>> SARG: The following line read from /var/log/squid/access.log could not
>> be parsed and is ignored
>> 1636349341.484???? 12 10.184.0.2 NONE_NONE/400 20417 GET
>> https://zen.yandex.by/lz5XeGt8f/ir4w02684/13f5fd2qrAJ2/p_CMhOoMLrxy4M2QFtQI-HLBvD5tHT6JdGbykwp9eDzBNcrpN2RIqcyiFH9pWekXwFsAEtIMz3_5FVo5y8zXIrAwGER6-e4cM0VckNJR_CjjEd2OObzKrHDSM2ZrfFzJ9CELTSJAeFt45wBcaGm_VqdcIXKVKFp7THc-uX7PdjLGAUpRv63aKSdE2OOnMXyOt0SJK0vNXql0thIirh9cGORGu31DYR9cCKZAW9gYjiGgfTFlxfgLOitwTohOyMZzx3ZNcK_K-rk2Vb_
>>
>> ....
>> UPVydoTW1636349696.714??? 629 10.106.0.2 NONE_NONE/200 0 CONNECT
>> azscus1-client-s.gateway.messenger.live.com:443 -
>> HIER_DIRECT/40.74.219.49 -
>> SARG: 4 consecutive errors found in the input log file
>> /var/log/squid/access.log
>>
>> so i think the solution would be to exclude zen.yandex.by from processing ?
> 
> The correct solution would depend on what you are trying to accomplish
> (with sarg), but that solution is unlikely to include disabling logging
> of requests to any domains IMHO.
> 
> Based on the above output (that could have been changed by multiple mail
> agents), it is difficult for me to guess what sarg did not like, but if
> you are suffering from Squid SMP workers corrupting each-other
> access.log entries, then please see Bug 5173:
> https://bugs.squid-cache.org/show_bug.cgi?id=5173
> 
> 
> HTH,
> 
> Alex.
> 


From heimarbeit123.99 at web.de  Tue Nov  9 12:22:12 2021
From: heimarbeit123.99 at web.de (heimarbeit123.99 at web.de)
Date: Tue, 9 Nov 2021 13:22:12 +0100
Subject: [squid-users] Squid very slow with kerberos auth and LDAP Group
 Search(AD)
In-Reply-To: <trinity-5a8a9252-8208-4503-8063-e370d77a27f9-1636443516324@3c-app-webde-bs50>
References: <trinity-e7fd76b4-da76-4557-8151-461a3254f45b-1636373969338@3c-app-webde-bs60>
 <trinity-5a8a9252-8208-4503-8063-e370d77a27f9-1636443516324@3c-app-webde-bs50>
Message-ID: <trinity-583c9221-d3d4-495c-b219-26cb94ba257f-1636460532537@3c-app-webde-bap34>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211109/802c336e/attachment.htm>

From marcus.kool at urlfilterdb.com  Tue Nov  9 12:50:41 2021
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 9 Nov 2021 12:50:41 +0000
Subject: [squid-users] problem in squid log
In-Reply-To: <590d6623-6b61-dc20-9a67-930721555372@ckta.by>
References: <1882cbd9-4f5e-f570-826f-6742e492ecd5@ckta.by>
 <68bff468-f887-4254-e6ba-78fdef5e83c1@measurement-factory.com>
 <590d6623-6b61-dc20-9a67-930721555372@ckta.by>
Message-ID: <cab5276f-6d52-8c60-055a-5e5b73ef2787@urlfilterdb.com>

Hi, I am the author of ufdbGuard and ufdbGuard supports Squid 5.x

The SARG error in access.log has nothing to do with ufdbGuard.


On 09/11/2021 08:45, Majed Zouhairy wrote:
> hmmm, this started happening after the last squid update.. i just noticed it is now version 5.2
> i have ufdbguard but i don't think i have smp..
>
> the last line of squid conf are
>
> url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l /var/log/squid/
> url_rewrite_children 16 startup=8 idle=2 concurrency=4 queue-size=64
>
> i think ufdbguard does not support squid version 5 yet, which might be the problem
>
> On 11/8/21 10:42 PM, Alex Rousskov wrote:
>> On 11/8/21 5:30 AM, Majed Zouhairy wrote:
>>> when i run sarg
>>>
>>> SARG: sarg version: 2.4.0 Jan-16-2020
>>> SARG: Reading access log file: /var/log/squid/access.log
>>> SARG: Log format identified as "squid log format" for
>>> /var/log/squid/access.log
>>> SARG: The following line read from /var/log/squid/access.log could not
>>> be parsed and is ignored
>>> 1636349341.484???? 12 10.184.0.2 NONE_NONE/400 20417 GET
>>> https://zen.yandex.by/lz5XeGt8f/ir4w02684/13f5fd2qrAJ2/p_CMhOoMLrxy4M2QFtQI-HLBvD5tHT6JdGbykwp9eDzBNcrpN2RIqcyiFH9pWekXwFsAEtIMz3_5FVo5y8zXIrAwGER6-e4cM0VckNJR_CjjEd2OObzKrHDSM2ZrfFzJ9CELTSJAeFt45wBcaGm_VqdcIXKVKFp7THc-uX7PdjLGAUpRv63aKSdE2OOnMXyOt0SJK0vNXql0thIirh9cGORGu31DYR9cCKZAW9gYjiGgfTFlxfgLOitwTohOyMZzx3ZNcK_K-rk2Vb_ 
>>>
>>>
>>> ....
>>> UPVydoTW1636349696.714??? 629 10.106.0.2 NONE_NONE/200 0 CONNECT
>>> azscus1-client-s.gateway.messenger.live.com:443 -
>>> HIER_DIRECT/40.74.219.49 -
>>> SARG: 4 consecutive errors found in the input log file
>>> /var/log/squid/access.log
>>>
>>> so i think the solution would be to exclude zen.yandex.by from processing ?
>>
>> The correct solution would depend on what you are trying to accomplish
>> (with sarg), but that solution is unlikely to include disabling logging
>> of requests to any domains IMHO.
>>
>> Based on the above output (that could have been changed by multiple mail
>> agents), it is difficult for me to guess what sarg did not like, but if
>> you are suffering from Squid SMP workers corrupting each-other
>> access.log entries, then please see Bug 5173:
>> https://bugs.squid-cache.org/show_bug.cgi?id=5173
>>
>>
>> HTH,
>>
>> Alex.
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Tue Nov  9 14:12:13 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Nov 2021 09:12:13 -0500
Subject: [squid-users] problem in squid log
In-Reply-To: <590d6623-6b61-dc20-9a67-930721555372@ckta.by>
References: <1882cbd9-4f5e-f570-826f-6742e492ecd5@ckta.by>
 <68bff468-f887-4254-e6ba-78fdef5e83c1@measurement-factory.com>
 <590d6623-6b61-dc20-9a67-930721555372@ckta.by>
Message-ID: <469616ba-59d2-6afc-723b-221286a16b7c@measurement-factory.com>

On 11/9/21 3:45 AM, Majed Zouhairy wrote:

> i have ufdbguard but i don't think i have smp..

The problem, whatever it is, is unlikely to be caused by ufdbguard.

My current bet is on access.log record truncation fixed in February 2020
in master/v6[1]. I do not know why that fix has not been backported to
v5. I have reminded about it ~5 months ago[2], but that did not seem to
help. We need to improve how backporting requests/decisions are tracked.

[1] https://github.com/squid-cache/squid/commit/a03343c
[2] https://github.com/squid-cache/squid/pull/332/#issuecomment-853275691

You may be able to confirm my suspicion by studying raw rejected
access.log records. Please note that the problematic record might be
merged with the next record/line.


If my suspicions are correct, then you may be able to work around the
problem by limiting the length of logged URLs. Look for ".width_max" in
your logformat documentation.


HTH,

Alex.


> the last line of squid conf are
> 
> url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
> sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
> url_rewrite_program /usr/local/ufdbguard/bin/ufdbgclient -m 4 -l
> /var/log/squid/
> url_rewrite_children 16 startup=8 idle=2 concurrency=4 queue-size=64
> 
> i think ufdbguard does not support squid version 5 yet, which might be
> the problem
> 
> On 11/8/21 10:42 PM, Alex Rousskov wrote:
>> On 11/8/21 5:30 AM, Majed Zouhairy wrote:
>>> when i run sarg
>>>
>>> SARG: sarg version: 2.4.0 Jan-16-2020
>>> SARG: Reading access log file: /var/log/squid/access.log
>>> SARG: Log format identified as "squid log format" for
>>> /var/log/squid/access.log
>>> SARG: The following line read from /var/log/squid/access.log could not
>>> be parsed and is ignored
>>> 1636349341.484???? 12 10.184.0.2 NONE_NONE/400 20417 GET
>>> https://zen.yandex.by/lz5XeGt8f/ir4w02684/13f5fd2qrAJ2/p_CMhOoMLrxy4M2QFtQI-HLBvD5tHT6JdGbykwp9eDzBNcrpN2RIqcyiFH9pWekXwFsAEtIMz3_5FVo5y8zXIrAwGER6-e4cM0VckNJR_CjjEd2OObzKrHDSM2ZrfFzJ9CELTSJAeFt45wBcaGm_VqdcIXKVKFp7THc-uX7PdjLGAUpRv63aKSdE2OOnMXyOt0SJK0vNXql0thIirh9cGORGu31DYR9cCKZAW9gYjiGgfTFlxfgLOitwTohOyMZzx3ZNcK_K-rk2Vb_
>>>
>>>
>>> ....
>>> UPVydoTW1636349696.714??? 629 10.106.0.2 NONE_NONE/200 0 CONNECT
>>> azscus1-client-s.gateway.messenger.live.com:443 -
>>> HIER_DIRECT/40.74.219.49 -
>>> SARG: 4 consecutive errors found in the input log file
>>> /var/log/squid/access.log
>>>
>>> so i think the solution would be to exclude zen.yandex.by from
>>> processing ?
>>
>> The correct solution would depend on what you are trying to accomplish
>> (with sarg), but that solution is unlikely to include disabling logging
>> of requests to any domains IMHO.
>>
>> Based on the above output (that could have been changed by multiple mail
>> agents), it is difficult for me to guess what sarg did not like, but if
>> you are suffering from Squid SMP workers corrupting each-other
>> access.log entries, then please see Bug 5173:
>> https://bugs.squid-cache.org/show_bug.cgi?id=5173
>>
>>
>> HTH,
>>
>> Alex.
>>



From david at articatech.com  Thu Nov 11 01:12:04 2021
From: david at articatech.com (David Touzeau)
Date: Thu, 11 Nov 2021 02:12:04 +0100
Subject: [squid-users] squid 5.2: ntlm_fake_auth refuse to valid credentials
Message-ID: <b7b9ccc8-c394-552c-f99c-833463c52ff9@articatech.com>

Hi,
i would like to use ntlm_fake_auth but it seems Squid refuse to switch 
to authenticated user and return a 407 to the browser and squid never 
accept? credentials.

What i missing ?

Configuration seems simple:
auth_param ntlm program /lib/squid3/ntlm_fake_auth -v
auth_param ntlm children 20 startup=5 idle=1 concurrency=0 queue-size=80 
on-persistent-overload=ERR
acl AUTHENTICATED proxy_auth REQUIRED
http_access deny? !AUTHENTICATED

Here the debug mode;

2021/11/11 01:36:16.862 kid1| 14,3| ipcache.cc(614) 
ipcache_gethostbyname: ipcache_gethostbyname: 'www.squid-cache.org', flags=1
2021/11/11 01:36:16.862 kid1| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'212.199.163.170' NOT found
2021/11/11 01:36:16.862 kid1| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'196.200.160.70' NOT found
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
NetworksBlackLists = 0
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
http_access#29 = 0
2021/11/11 01:36:16.862 kid1| 28,5| Checklist.cc(397) bannedAction: 
Action 'DENIED/0' is not banned
2021/11/11 01:36:16.862 kid1| 28,5| Acl.cc(124) matches: checking 
http_access#30
2021/11/11 01:36:16.862 kid1| 28,5| Acl.cc(124) matches: checking 
NormalPorts
2021/11/11 01:36:16.862 kid1| 24,7| SBuf.cc(212) append: from c-string 
to id SBuf1021843
2021/11/11 01:36:16.862 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 13 
for SBuf1021843
2021/11/11 01:36:16.862 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf1021843 
new store capacity: 40
2021/11/11 01:36:16.862 kid1| 28,3| StringData.cc(33) match: 
aclMatchStringList: checking 'MyPortNameID1'
2021/11/11 01:36:16.862 kid1| 28,3| StringData.cc(36) match: 
aclMatchStringList: 'MyPortNameID1' found
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
NormalPorts = 1
2021/11/11 01:36:16.862 kid1| 28,5| Acl.cc(124) matches: checking 
!AUTHENTICATED
2021/11/11 01:36:16.862 kid1| 28,5| Acl.cc(124) matches: checking 
AUTHENTICATED
2021/11/11 01:36:16.862 kid1| 29,4| UserRequest.cc(354) authenticate: No 
connection authentication type
2021/11/11 01:36:16.862 kid1| 29,5| User.cc(36) User: Initialised 
auth_user '0x5570e8c4d240'.
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(99) UserRequest: 
initialised request 0x5570e8cdacf0
2021/11/11 01:36:16.862 kid1| 24,7| SBuf.cc(212) append: from c-string 
to id SBuf1021846
2021/11/11 01:36:16.862 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 61 
for SBuf1021846
2021/11/11 01:36:16.862 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf1021846 
new store capacity: 128
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 33,2| client_side.cc(507) setAuth: Adding 
connection-auth to local=192.168.90.170:3128 remote=192.168.90.10:50746 
FD 12 flags=1 from new NTLM handshake request
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 28,3| AclProxyAuth.cc(131) checkForAsync: 
checking password via authenticator
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 84,5| helper.cc(1292) 
StatefulGetFirstAvailable: StatefulGetFirstAvailable: Running servers 5
2021/11/11 01:36:16.862 kid1| 84,5| helper.cc(1309) 
StatefulGetFirstAvailable: StatefulGetFirstAvailable: returning srv-Hlpr66
2021/11/11 01:36:16.862 kid1| 5,5| AsyncCall.cc(26) AsyncCall: The 
AsyncCall helperStatefulDispatchWriteDone constructed, 
this=0x5570e8c8f8e0 [call581993]
2021/11/11 01:36:16.862 kid1| 5,5| Write.cc(35) Write: local=[::] 
remote=[::] FD 10 flags=1: sz 60: asynCall 0x5570e8c8f8e0*1
2021/11/11 01:36:16.862 kid1| 5,5| ModEpoll.cc(117) SetSelect: FD 10, 
type=2, handler=1, client_data=0x7f9e5d8a75a8, timeout=0
2021/11/11 01:36:16.862 kid1| 84,5| helper.cc(1430) 
helperStatefulDispatch: helperStatefulDispatch: Request sent to 
ntlmauthenticator #Hlpr66, 60 bytes
2021/11/11 01:36:16.862 kid1| 28,4| Acl.cc(72) AuthenticateAcl: 
returning 2 sending credentials to helper.
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
AUTHENTICATED = -1 async
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
!AUTHENTICATED = -1 async
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
http_access#30 = -1 async
2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
http_access = -1 async
2021/11/11 01:36:16.862 kid1| 33,4| Server.cc(90) readSomeData: 
local=192.168.90.170:3128 remote=192.168.90.10:50746 FD 12 flags=1: 
reading request...
2021/11/11 01:36:16.862 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The 
AsyncCall Server::doClientRead constructed, this=0x5570e87cfd50 [call581994]
2021/11/11 01:36:16.862 kid1| 5,5| Read.cc(57) comm_read_base: 
comm_read, queueing read for local=192.168.90.170:3128 
remote=192.168.90.10:50746 FD 12 flags=1; asynCall 0x5570e87cfd50*1
2021/11/11 01:36:16.862 kid1| 5,5| ModEpoll.cc(117) SetSelect: FD 12, 
type=1, handler=1, client_data=0x7f9e5d8a7678, timeout=0
2021/11/11 01:36:16.862 kid1| 33,5| AsyncJob.cc(154) callEnd: 
Http1::Server status out: [ job17296]
2021/11/11 01:36:16.862 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: 
leaving Server::doClientRead(local=192.168.90.170:3128 
remote=192.168.90.10:50746 FD 12 flags=1, data=0x5570e8c63468)
2021/11/11 01:36:16.862 kid1| 5,5| Write.cc(66) HandleWrite: local=[::] 
remote=[::] FD 10 flags=1: off 0, sz 60.
2021/11/11 01:36:16.862 kid1| 5,5| Write.cc(108) HandleWrite: write() 
returns 60
2021/11/11 01:36:16.862 kid1| 5,3| IoCallback.cc(116) finish: called for 
local=[::] remote=[::] FD 10 flags=1 (0, 0)
2021/11/11 01:36:16.862 kid1| 5,5| AsyncCall.cc(93) ScheduleCall: 
IoCallback.cc(135) will call helperStatefulDispatchWriteDone(local=[::] 
remote=[::] FD 10 flags=1, data=0x5570e8731e08, size=60, 
buf=0x5570e8d2c1a0) [call581993]
ntlm_fake_auth.cc(170): pid=4454 :2021/11/11 01:36:16.862 kid1| 5,5| 
AsyncCallQueue.cc(55) fireNext: entering 
helperStatefulDispatchWriteDone(local=[::] remote=[::] FD 10 flags=1, 
data=0x5570e8731e08, size=60, buf=0x5570e8d2c1a0)
Got 'YR' from Squid with data:
2021/11/11 01:36:16.862 kid1| 5,5| AsyncCall.cc(38) make: make call 
helperStatefulDispatchWriteDone [call581993]
[0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
2021/11/11 01:36:16.862 kid1| 5,5| AsyncCallQueue.cc(57) fireNext: 
leaving helperStatefulDispatchWriteDone(local=[::] remote=[::] FD 10 
flags=1, data=0x5570e8731e08, size=60, buf=0x5570e8d2c1a0)
[0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
2021/11/11 01:36:16.862 kid1| 5,5| ModEpoll.cc(117) SetSelect: FD 10, 
type=2, handler=0, client_data=0, timeout=0
[0020]? 0A 00 63 45 00 00 00 0F ..cE....
*ntlm_fake_auth.cc(197): pid=4454 :sending 'TT' to squid with data:*
*[0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........*
*2021/11/11 01:36:16.862 kid1| 5,3| Read.cc(145) HandleRead: FD 10, size 
32767, retval 72, errno 0*
*[0010]? AE AA AA AA 07 82 08 A2? E8 DB F0 45 D4 04 00 32 ........ ...E...2*
*2021/11/11 01:36:16.862 kid1| 5,3| IoCallback.cc(116) finish: called 
for local=[::] remote=[::] FD 10 flags=1 (0, 0)*
*[0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU*
*[0030]? 50 * ? ? ? P
2021/11/11 01:36:16.862 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: 
IoCallback.cc(135) will call helperStatefulHandleRead(local=[::] 
remote=[::] FD 10 flags=1, data=0x5570e8752b18, size=72, 
buf=0x5570e87635b0) [call581969]
2021/11/11 01:36:16.862 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: 
entering helperStatefulHandleRead(local=[::] remote=[::] FD 10 flags=1, 
data=0x5570e8752b18, size=72, buf=0x5570e87635b0)
2021/11/11 01:36:16.862 kid1| 5,4| AsyncCall.cc(38) make: make call 
helperStatefulHandleRead [call581969]
2021/11/11 01:36:16.862 kid1| 84,5| helper.cc(1081) 
helperStatefulHandleRead: helperStatefulHandleRead: 72 bytes from 
ntlmauthenticator #Hlpr66
2021/11/11 01:36:16.862 kid1| 84,3| helper.cc(1103) 
helperStatefulHandleRead: helperStatefulHandleRead: end of reply found
2021/11/11 01:36:16.862 kid1| 84,3| Reply.cc(41) finalize: Parsing 
helper buffer
2021/11/11 01:36:16.862 kid1| 84,3| Reply.cc(59) finalize: Buff length 
is larger than 2
*2021/11/11 01:36:16.862 kid1| 29,4| UserRequest.cc(306) HandleReply: 
Need to challenge the client with a server token: 
'TlRMTVNTUAACAAAACQAJAK6qqqoHggii6NvwRdQEADIAAAAAAAA6AFdPUktHUk9VUA=='*
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 28,5| InnerNode.cc(94) resumeMatchingAt: 
checking http_access at 29
2021/11/11 01:36:16.862 kid1| 28,5| Checklist.cc(397) bannedAction: 
Action 'DENIED/0' is not banned
2021/11/11 01:36:16.862 kid1| 28,5| InnerNode.cc(94) resumeMatchingAt: 
checking http_access#30 at 1
2021/11/11 01:36:16.862 kid1| 28,5| InnerNode.cc(94) resumeMatchingAt: 
checking !AUTHENTICATED at 0
2021/11/11 01:36:16.862 kid1| 28,5| Acl.cc(124) matches: checking 
AUTHENTICATED
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
*2021/11/11 01:36:16.862 kid1| 29,2| UserRequest.cc(197) authenticate: 
need to challenge client 
'TlRMTVNTUAACAAAACQAJAK6qqqoHggii6NvwRdQEADIAAAAAAAA6AFdPUktHUk9VUA=='!*
2021/11/11 01:36:16.862 kid1| 29,5| UserRequest.cc(77) valid: Validated. 
Auth::UserRequest '0x5570e8cdacf0'.
2021/11/11 01:36:16.862 kid1| 28,4| Acl.cc(78) AuthenticateAcl: 
returning 3 sending authentication challenge.
2021/11/11 01:36:16.862 kid1| 28,3| Checklist.cc(63) markFinished: 
0x5570e876da88 answer AUTH_REQUIRED for AuthenticateAcl exception
*2021/11/11 01:36:16.862 kid1| 28,3| Acl.cc(151) matches: checked: 
AUTHENTICATED = -1*
2021/11/11 01:36:16.863 kid1| 28,3| InnerNode.cc(97) resumeMatchingAt: 
checked: !AUTHENTICATED = -1
2021/11/11 01:36:16.863 kid1| 28,3| InnerNode.cc(97) resumeMatchingAt: 
checked: http_access#30 = -1
2021/11/11 01:36:16.863 kid1| 28,3| InnerNode.cc(97) resumeMatchingAt: 
checked: http_access = -1
2021/11/11 01:36:16.863 kid1| 28,3| Checklist.cc(163) checkCallback: 
ACLChecklist::checkCallback: 0x5570e876da88 answer=AUTH_REQUIRED
2021/11/11 01:36:16.863 kid1| 85,2| client_side_request.cc(759) 
clientAccessCheckDone: The request GET http://www.squid-cache.org/ is 
AUTH_REQUIRED; last ACL checked: AUTHENTICATED
2021/11/11 01:36:16.863 kid1| 85,5| client_side_request.cc(775) 
clientAccessCheckDone: *Access Denied*: http://www.squid-cache.org/
2021/11/11 01:36:16.863 kid1| 85,5| client_side_request.cc(776) 
clientAccessCheckDone: AclMatchedName = AUTHENTICATED
2021/11/11 01:36:16.863 kid1| 33,5| client_side_request.cc(779) 
clientAccessCheckDone: Proxy Auth Message = Authentication in progress
2021/11/11 01:36:16.863 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff22f7a0b0
2021/11/11 01:36:16.863 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7fff22f7a0b0
2021/11/11 01:36:16.863 kid1| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7fff22f7a0b0
2021/11/11 01:36:16.863 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7fff22f7a0b0
2021/11/11 01:36:16.863 kid1| 85,5| client_side_request.cc(1445) 
sslBumpAccessCheck: cannot SslBump this request
2021/11/11 01:36:16.863 kid1| 73,3| HttpRequest.cc(662) storeId: sent 
back effectiveRequestUrl: http://www.squid-cache.org/
2021/11/11 01:36:16.863 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 1 
for SBuf1021849
2021/11/11 01:36:16.863 kid1| 24,7| SBuf.cc(167) rawSpace: SBuf1021849 
not growing
2021/11/11 01:36:16.863 kid1| 20,3| store.cc(785) storeCreatePureEntry: 
storeCreateEntry: 'http://www.squid-cache.org/'
2021/11/11 01:36:16.863 kid1| 20,5| store.cc(347) StoreEntry: StoreEntry 
constructed, this=0x5570e87b5620
2021/11/11 01:36:16.863 kid1| 20,3| MemObject.cc(101) MemObject: 
MemObject constructed, this=0x5570e8ad2990
2021/11/11 01:36:16.863 kid1| 55,7| HttpHeader.cc(152) HttpHeader: 
init-ing hdr: 0x5570e8c55128 owner: 3
2021/11/11 01:36:16.863 kid1| 88,3| MemObject.cc(84) setUris: 
0x5570e8ad2990 storeId: http://www.squid-cache.org/
2021/11/11 01:36:16.863 kid1| 24,7| SBuf.cc(85) assign: assigning 
SBuf1021850 from SBuf1021792
2021/11/11 01:36:16.863 kid1| 20,3| store.cc(444) lock: storeCreateEntry 
locked key [null_store_key] e:=V/0x5570e87b5620*1
2021/11/11 01:36:16.863 kid1| 20,3| store.cc(586) setPrivateKey: 01 
e:=V/0x5570e87b5620*1
2021/11/11 01:36:16.863 kid1| 20,3| store.cc(422) hashInsert: 
StoreEntry::hashInsert: Inserting Entry e:=XIV/0x5570e87b5620*1 key 
'771C000000000000A607000001000000'
2021/11/11 01:36:16.863 kid1| 20,3| store.cc(444) lock: 
clientReplyContext::setReplyToStoreEntry locked key 
771C000000000000A607000001000000 e:=XIV/0x5570e87b5620*2
2021/11/11 01:36:16.863 kid1| 4,4| errorpage.cc(604) errorAppendEntry: 
Creating an error page for entry 0x5570e87b5620 with errorstate 
0x5570e8c527b8 page id 2
2021/11/11 01:36:16.863 kid1| 55,7| HttpHeader.cc(152) HttpHeader: 
init-ing hdr: 0x5570e8ddec88 owner: 3
2021/11/11 01:36:16.863 kid1| 4,2| errorpage.cc(1259) BuildContent: No 
existing error page language negotiated for ERR_CACHE_ACCESS_DENIED. 
Using default error file.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211111/e2e5d121/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 11 07:40:44 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Nov 2021 20:40:44 +1300
Subject: [squid-users] squid 5.2: ntlm_fake_auth refuse to valid
 credentials
In-Reply-To: <b7b9ccc8-c394-552c-f99c-833463c52ff9@articatech.com>
References: <b7b9ccc8-c394-552c-f99c-833463c52ff9@articatech.com>
Message-ID: <e2c4f5fa-b797-b532-ccfe-3c40f52e01fe@treenet.co.nz>

On 11/11/21 14:12, David Touzeau wrote:
> Hi,
> i would like to use ntlm_fake_auth but it seems Squid refuse to switch 
> to authenticated user and return a 407 to the browser and squid never 
> accept? credentials.
> 
> What i missing ?
> 
> Configuration seems simple:
> auth_param ntlm program /lib/squid3/ntlm_fake_auth -v
> auth_param ntlm children 20 startup=5 idle=1 concurrency=0 queue-size=80 
> on-persistent-overload=ERR
> acl AUTHENTICATED proxy_auth REQUIRED
> http_access deny? !AUTHENTICATED
> 
> Here the debug mode;
> 

The log you presented shows the helper delivering a TT response to 
Squid. Which is NTLM step 2 response token for a 407 challenge response.
That is only sent if there were not auth headers received from the 
client - which is correct per your config shown.

The log snippet stops before Squid sends that response to the client, so 
whatever follows is unknown.

Amos


From omar.salem at kubota.com  Thu Nov 11 11:05:37 2021
From: omar.salem at kubota.com (Omar Salem - KSACO IT Manager)
Date: Thu, 11 Nov 2021 14:05:37 +0300
Subject: [squid-users] Rotate squid log files
Message-ID: <CAKaLTeMhS=hHAPgV7rQC-q4bHgEwh+407hBsrOa8T+vidrA7Tg@mail.gmail.com>

hi all,

I have ran the command (c:\squid\sbin\squid -n squid -k rotate)  to rotate
log files (Cache,store and access) but only cache and store logs were
rotated.
I need to rotate access.log too because it reached 4GB.

Also how to see users visited websites in date and time because it is not
included in access.log.









Omar Salem
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211111/541cf274/attachment.htm>

From milan at serveo.nl  Thu Nov 11 11:11:18 2021
From: milan at serveo.nl (Milan)
Date: Thu, 11 Nov 2021 12:11:18 +0100
Subject: [squid-users] User linked to static unique outbound address
Message-ID: <013E9FFB-A330-4281-89C2-0998D92755A5@serveo.nl>

Hi Guys,

Currently I?m trying to setup a SQUID 4.11 proxy for approx 100 users, which should be assigned to static outbound IP. The inbound IP address should not be filtered / checked. Following multiple topics and the documentation I did setup the following configuration. Yet seemingly I keep running into non-functioning of the intended configuration. Basis Idea is as followed:

user1 > 199.199.199.2 (outbound address)
user2 > 199.199.199.3
user3 > 199.199.199.4
user4 > 199.199.199.5
etc...

Hence I filled /etc/squid/userip.conf as followed:

###
199.199.199.2 user1
199.199.199.3 user2
199.199.199.4 user3
199.199.199.5 user4
###

With the following configuration file /etc/squid/squid.conf:

###
acl http proto http
acl port_80 port 80
acl port_443 port 443
acl CONNECT method CONNECT

auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwords
auth_param basic realm Please enter username and password

acl authenticated proxy_auth REQUIRED

external_acl_type userip %MYADDR %LOGIN /usr/lib64/squid/ext_file_userip_acl -f /etc/squid/userip.conf

acl userip external userip

http_access allow userip
http_access deny all

http_port 3128
acl ip1 myip 199.199.199.2
tcp_outgoing_address 199.199.199.2 ip1

acl ip2 myip 199.199.199.3
tcp_outgoing_address 199.199.199.3 ip2

acl ip2 myip 199.199.199.4
tcp_outgoing_address 199.199.199.3 ip3

acl ip2 myip 199.199.199.5
tcp_outgoing_address 199.199.199.3 ip4
EOF
###

So far I tried both ?external_acl_type userip %MYADDR?  &  "external_acl_type userip %SRC? yet I keep running into this error and the proxy does not to functioning:

1636498565.031      0 xx.xxx.xxx.xxx TCP_DENIED/407 4186 CONNECT xxx:443 user1 HIER_NONE/- text/html

Anyone knows how to solve this matter? Your input would be much appreciated!

Please advice, thanks!


Best,

Milan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211111/ef798a1b/attachment.htm>

From david at articatech.com  Thu Nov 11 12:16:01 2021
From: david at articatech.com (David Touzeau)
Date: Thu, 11 Nov 2021 13:16:01 +0100
Subject: [squid-users] squid 5.2: ntlm_fake_auth refuse to valid
 credentials
In-Reply-To: <e2c4f5fa-b797-b532-ccfe-3c40f52e01fe@treenet.co.nz>
References: <b7b9ccc8-c394-552c-f99c-833463c52ff9@articatech.com>
 <e2c4f5fa-b797-b532-ccfe-3c40f52e01fe@treenet.co.nz>
Message-ID: <913abf20-f525-52c5-3933-63a5d19478a2@articatech.com>

Thanks Amos it will help understand something

I think modern browser sending NTLMv2 as the ntlm_fake_auth 
understanding only NTLMv1 ( perhaps )

Using curl with --proxy-ntlm option is OK for squid as using browser 
return allways a 407
DO you know the limitation of ntlm_fake_auth according NTLM version.
Is there a way to fix it ?

************* CURL ************

[0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 06 82 08 00? NTLMSSP. ........
[0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
[0010]? AE AA AA AA 06 82 08 00? 15 3A CC 83 0B 80 7B 45 ........ .......E
[0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
[0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
ntlm_fake_auth.cc(170): pid=31874 :Got 'KK' from Squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 03 00 00 00 18 00 18 00 NTLMSSP. ........
[0010]? 40 00 00 00 30 00 30 00? 58 00 00 00 00 00 00 00 ....0.0. X.......
[0020]? 88 00 00 00 04 00 04 00? 88 00 00 00 09 00 09 00 ........ ........
[0030]? 8C 00 00 00 00 00 00 00? 00 00 00 00 06 82 08 00 ........ ........
[0040]? EB C7 B7 11 26 62 FD 82? B0 45 68 62 E0 6C E6 A3 .....b.. .Ehb.l..
[0050]? 57 A7 E6 76 1C 7B 79 74? 17 71 72 5B 72 38 DA 30 W..v..yt .qr.r8.0
[0060]? 06 4D 15 1F 9B D1 A2 A5? 01 01 00 00 00 00 00 00 .M...... ........
[0070]? 80 38 3C 2A EA D6 D7 01? 57 A7 E6 76 1C 7B 79 74 .8...... W..v..yt
[0080]? 00 00 00 00 00 00 00 00? 74 6F 74 6F 6E 74 6C 6D ........ totontlm
[0090]? 70 72 6F 78 79 proxy
ntlmauth.cc(244): pid=31874 :ntlm_unpack_auth: size of 149
ntlmauth.cc(245): pid=31874 :ntlm_unpack_auth: flg 00088206
ntlmauth.cc(246): pid=31874 :ntlm_unpack_auth: lmr o(64) l(24)
ntlmauth.cc(247): pid=31874 :ntlm_unpack_auth: ntr o(88) l(48)
ntlmauth.cc(248): pid=31874 :ntlm_unpack_auth: dom o(136) l(0)
ntlmauth.cc(249): pid=31874 :ntlm_unpack_auth: usr o(136) l(4)
ntlmauth.cc(250): pid=31874 :ntlm_unpack_auth: wst o(140) l(9)
ntlmauth.cc(251): pid=31874 :ntlm_unpack_auth: key o(0) l(0)
ntlmauth.cc(257): pid=31874 :ntlm_unpack_auth: Domain 't' (len=1).
*ntlmauth.cc(268): pid=31874 :ntlm_unpack_auth: Username 'toton' (len=5).*
ntlm_fake_auth.cc(210): pid=31874 :sending 'AF toton' to squid


********* But when connecting any modern browser to squid ***********

[0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2? NTLMSSP. ........
[0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
[0020]? 0A 00 63 45 00 00 00 0F ..cE....
ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
[0010]? AE AA AA AA 07 82 08 A2? C9 F0 4C 07 E0 79 9F CF ........ ..L..y..
[0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
[0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
ntlm_fake_auth.cc(170): pid=31874 :Got 'YR' from Squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
[0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
[0020]? 0A 00 63 45 00 00 00 0F ..cE....
ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
[0010]? AE AA AA AA 07 82 08 A2? 49 12 A5 8A C8 17 3E 9D ........ I.......
[0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
[0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
ntlm_fake_auth.cc(170): pid=31874 :Got 'YR' from Squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
[0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
[0020]? 0A 00 63 45 00 00 00 0F ..cE....
ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
[0010]? AE AA AA AA 07 82 08 A2? 09 6D 48 E6 12 9C 4B 30 ........ .mH...K0
[0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
[0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
ntlm_fake_auth.cc(170): pid=31874 :Got 'YR' from Squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
[0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
[0020]? 0A 00 63 45 00 00 00 0F ..cE....
ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
[0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
[0010]? AE AA AA AA 07 82 08 A2? F5 F6 8C B4 16 B9 20 CD ........ ........
[0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU



Le 11/11/2021 ? 08:40, Amos Jeffries a ?crit?:
> On 11/11/21 14:12, David Touzeau wrote:
>> Hi,
>> i would like to use ntlm_fake_auth but it seems Squid refuse to 
>> switch to authenticated user and return a 407 to the browser and 
>> squid never accept? credentials.
>>
>> What i missing ?
>>
>> Configuration seems simple:
>> auth_param ntlm program /lib/squid3/ntlm_fake_auth -v
>> auth_param ntlm children 20 startup=5 idle=1 concurrency=0 
>> queue-size=80 on-persistent-overload=ERR
>> acl AUTHENTICATED proxy_auth REQUIRED
>> http_access deny? !AUTHENTICATED
>>
>> Here the debug mode;
>>
>
> The log you presented shows the helper delivering a TT response to 
> Squid. Which is NTLM step 2 response token for a 407 challenge response.
> That is only sent if there were not auth headers received from the 
> client - which is correct per your config shown.
>
> The log snippet stops before Squid sends that response to the client, 
> so whatever follows is unknown.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211111/1cd20571/attachment.htm>

From Loucansky.Lukas at kjj.cz  Thu Nov 11 15:19:42 2021
From: Loucansky.Lukas at kjj.cz (=?windows-1250?B?TG916GFuc2v9IEx1a+Ga?=)
Date: Thu, 11 Nov 2021 16:19:42 +0100
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue overflow
 for kid2 at 1024 items
Message-ID: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>

Hi,
recently I'm facing too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items lines in my Squid 5.2 log files. So far I was able to find it in the CollapsedForwarding.cc func Broadcast as a result of a full queue which limits beeing defined by the QueueCapacity const at 1024 items.
 
Now I have squid in VM with 6 CPU cores, 16GB RAM and 2 workers and 4 diskers with rock (various slot sizes). /var/run/shm squid-cf*.shm files exist. Mounted with
shm        /dev/shm    tmpfs    nodev,nosuid,noexec,size=8G    0    0 (is 8GB enough or too much?)

I have collapsed_forwarding on

Could someone elaborate how the queue is filled - what is clogging it? What can be done to work it out? I don't mind too much if I have to turn collapsed forwarding off - as I don?t see any significant gain. On the other hand I don't see any slowdown (because of non-cacheable object being forwarded individually) either.

Thanks!
LL



From david at articatech.com  Thu Nov 11 19:08:13 2021
From: david at articatech.com (David Touzeau)
Date: Thu, 11 Nov 2021 20:08:13 +0100
Subject: [squid-users] Squid 5.2 unstable in production mode
Message-ID: <1a583ffa-c89d-028b-5712-d934687fb919@articatech.com>

Hi

Just for information and i hope it will help.

We have installed Squid 5.1 and Squid 5.2 in production mode.
It seems that after several days, the Squid become very unstable.
We mention that when switching to 4.x we did not encounter these errors 
with the same configuration, same users, same network ( replace binaries 
and keep same configuration )

All production servers are installed in a virtual environment ( ESXI or 
Nutanix ) on Debian 10.x with about 4 to 8 vCPUs and 8GB of memory.
and from 20 to 5000 users.

After severals tests we see that the number of users did not have impact 
with the stability.
We encounter same errors on a 20 users proxy and the same way of a 5000 
users proxy.


1) Memory leak
---------------------------------
This was encounter on computer that handle more than 10Gb of memory, 
squid eat more than 8Gb of memory.
After eating all memory, squid is unable to load helpers and freeze 
listen ports.
A restart service free the memory and fix the issue.

2) Max filedescriptors issues:
--------------------------------
This is a strange behavior that Squid did not accept defined parameter:
Example we set 65535 filedescriptors but squidclient mgr:info report 
4096 and sometimes return back to 1024.

Several times squid report

 ??? current master transaction: master15881
2021/11/11 17:10:09 kid1| WARNING! Your cache is running out of 
filedescriptors
 ??? listening port: MyPortNameID1
2021/11/11 17:10:29 kid1| WARNING! Your cache is running out of 
filedescriptors
 ??? listening port: MyPortNameID1
2021/11/11 17:10:51 kid1| WARNING! Your cache is running out of 
filedescriptors
 ??? listening port: MyPortNameID1
2021/11/11 17:11:56 kid1| TCP connection to 127.0.0.1/2320 failed
 ??? current master transaction: master15881
2021/11/11 17:13:02 kid1| WARNING! Your cache is running out of 
filedescriptors
 ??? listening port: MyPortNameID1
2021/11/11 17:13:19 kid1| WARNING! Your cache is running out of 
filedescriptors

But a mgr:info report:

 ??????? memPoolFree calls:??? 4295601
File descriptor usage for squid:
 ??????? Maximum number of file descriptors:?? 10048
 ??????? Largest file desc currently in use:??? 262
 ??????? Number of file desc currently in use:? 135
 ??????? Files queued for open:?????????????????? 0
 ??????? Available number of file descriptors: 9913
 ??????? Reserved number of file descriptors:? 9789

After these errors the listen port is freeze and nobody is able to surf.
a just "squid -k reconfigure" fix the issue and the proxy return to 
normal mode for several minutes and back again to filedescriptors issues.

There is no relationship between filedescriptors issues and the number 
of clients.
Sometimes the issue is discovered during the night when there is no user 
that using the proxy ( just some robots like windows update )




Is there something other we can investigate to help more stability of 
the 5.x branch ?
Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211111/da810875/attachment.htm>

From flashdown at data-core.org  Thu Nov 11 19:58:03 2021
From: flashdown at data-core.org (Flashdown)
Date: Thu, 11 Nov 2021 20:58:03 +0100
Subject: [squid-users] Squid 5.2 unstable in production mode
In-Reply-To: <1a583ffa-c89d-028b-5712-d934687fb919@articatech.com>
References: <1a583ffa-c89d-028b-5712-d934687fb919@articatech.com>
Message-ID: <62a7784138282fa3665fc03b1815112c@data-core.org>

Hi David,

well I am curious, where did you set the max filedescriptors? Only in 
the OS configuration? If so, you also need to define it in the 
squid.conf as well -> 
http://www.squid-cache.org/Versions/v5/cfgman/max_filedescriptors.html

Regarding the memory leak, do you use an adaption service such as 
c-icap?
If so, what is the result of: ss -ant | grep CLOSE_WAIT | wc -l

May you should try to build Squid5 against Debian 11 to have the latest 
version of any dependencies needed to see if the memory leak is gone or 
not.

I run multiple Squid 5.2 servers on Debian 11 in production and do not 
have any issues.
---
Best regards,
Enrico Heine

Am 2021-11-11 20:08, schrieb David Touzeau:
> Hi
> 
> Just for information and i hope it will help.
> 
> We have installed Squid 5.1 and Squid 5.2 in production mode.
> It seems that after several days, the Squid become very unstable.
> We mention that when switching to 4.x we did not encounter these
> errors with the same configuration, same users, same network ( replace
> binaries and keep same configuration )
> 
> All production servers are installed in a virtual environment ( ESXI
> or Nutanix ) on Debian 10.x with about 4 to 8 vCPUs and 8GB of memory.
> and from 20 to 5000 users.
> 
> After severals tests we see that the number of users did not have
> impact with the stability.
> We encounter same errors on a 20 users proxy and the same way of a
> 5000 users proxy.
> 
> 1) Memory leak
> ---------------------------------
> This was encounter on computer that handle more than 10Gb of memory,
> squid eat more than 8Gb of memory.
> After eating all memory, squid is unable to load helpers and freeze
> listen ports.
> A restart service free the memory and fix the issue.
> 
> 2) Max filedescriptors issues:
> --------------------------------
> This is a strange behavior that Squid did not accept defined
> parameter:
> Example we set 65535 filedescriptors but squidclient mgr:info report
> 4096 and sometimes return back to 1024.
> 
> Several times squid report
> 
>     current master transaction: master15881
> 2021/11/11 17:10:09 kid1| WARNING! Your cache is running out of
> filedescriptors
>     listening port: MyPortNameID1
> 2021/11/11 17:10:29 kid1| WARNING! Your cache is running out of
> filedescriptors
>     listening port: MyPortNameID1
> 2021/11/11 17:10:51 kid1| WARNING! Your cache is running out of
> filedescriptors
>     listening port: MyPortNameID1
> 2021/11/11 17:11:56 kid1| TCP connection to 127.0.0.1/2320 failed
>     current master transaction: master15881
> 2021/11/11 17:13:02 kid1| WARNING! Your cache is running out of
> filedescriptors
>     listening port: MyPortNameID1
> 2021/11/11 17:13:19 kid1| WARNING! Your cache is running out of
> filedescriptors
> 
> But a mgr:info report:
> 
>         memPoolFree calls:    4295601
> File descriptor usage for squid:
>         Maximum number of file descriptors:   10048
>         Largest file desc currently in use:    262
>         Number of file desc currently in use:  135
>         Files queued for open:                   0
>         Available number of file descriptors: 9913
>         Reserved number of file descriptors:  9789
> 
> After these errors the listen port is freeze and nobody is able to
> surf.
> a just "squid -k reconfigure" fix the issue and the proxy return to
> normal mode for several minutes and back again to filedescriptors
> issues.
> 
> There is no relationship between filedescriptors issues and the number
> of clients.
> Sometimes the issue is discovered during the night when there is no
> user that using the proxy ( just some robots like windows update )
> 
> Is there something other we can investigate to help more stability of
> the 5.x branch ?
> Regards
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From david at articatech.com  Thu Nov 11 23:13:26 2021
From: david at articatech.com (David Touzeau)
Date: Fri, 12 Nov 2021 00:13:26 +0100
Subject: [squid-users] Squid 5.2 unstable in production mode
In-Reply-To: <62a7784138282fa3665fc03b1815112c@data-core.org>
References: <1a583ffa-c89d-028b-5712-d934687fb919@articatech.com>
 <62a7784138282fa3665fc03b1815112c@data-core.org>
Message-ID: <7e1eaaeb-3c7a-9465-e703-aebd747e41f5@articatech.com>

Hi

Max filedescriptors is defined in squid.conf.
Yes, in some cases a c-icap was installed and the proxy became more 
stable for a while.
But Filedescriptors issue still unstable... I really did not know why.


Running Debian 11 is very difficult it is a very new OS and we consider 
debian 10 as currently stable .
Also the Squid 4 working very well on Debian 10


Le 11/11/2021 ? 20:58, Flashdown a ?crit?:
> Hi David,
>
> well I am curious, where did you set the max filedescriptors? Only in 
> the OS configuration? If so, you also need to define it in the 
> squid.conf as well -> 
> http://www.squid-cache.org/Versions/v5/cfgman/max_filedescriptors.html
>
> Regarding the memory leak, do you use an adaption service such as c-icap?
> If so, what is the result of: ss -ant | grep CLOSE_WAIT | wc -l
>
> May you should try to build Squid5 against Debian 11 to have the 
> latest version of any dependencies needed to see if the memory leak is 
> gone or not.
>
> I run multiple Squid 5.2 servers on Debian 11 in production and do not 
> have any issues.
> ---
> Best regards,
> Enrico Heine
>
> Am 2021-11-11 20:08, schrieb David Touzeau:
>> Hi
>>
>> Just for information and i hope it will help.
>>
>> We have installed Squid 5.1 and Squid 5.2 in production mode.
>> It seems that after several days, the Squid become very unstable.
>> We mention that when switching to 4.x we did not encounter these
>> errors with the same configuration, same users, same network ( replace
>> binaries and keep same configuration )
>>
>> All production servers are installed in a virtual environment ( ESXI
>> or Nutanix ) on Debian 10.x with about 4 to 8 vCPUs and 8GB of memory.
>> and from 20 to 5000 users.
>>
>> After severals tests we see that the number of users did not have
>> impact with the stability.
>> We encounter same errors on a 20 users proxy and the same way of a
>> 5000 users proxy.
>>
>> 1) Memory leak
>> ---------------------------------
>> This was encounter on computer that handle more than 10Gb of memory,
>> squid eat more than 8Gb of memory.
>> After eating all memory, squid is unable to load helpers and freeze
>> listen ports.
>> A restart service free the memory and fix the issue.
>>
>> 2) Max filedescriptors issues:
>> --------------------------------
>> This is a strange behavior that Squid did not accept defined
>> parameter:
>> Example we set 65535 filedescriptors but squidclient mgr:info report
>> 4096 and sometimes return back to 1024.
>>
>> Several times squid report
>>
>> ??? current master transaction: master15881
>> 2021/11/11 17:10:09 kid1| WARNING! Your cache is running out of
>> filedescriptors
>> ??? listening port: MyPortNameID1
>> 2021/11/11 17:10:29 kid1| WARNING! Your cache is running out of
>> filedescriptors
>> ??? listening port: MyPortNameID1
>> 2021/11/11 17:10:51 kid1| WARNING! Your cache is running out of
>> filedescriptors
>> ??? listening port: MyPortNameID1
>> 2021/11/11 17:11:56 kid1| TCP connection to 127.0.0.1/2320 failed
>> ??? current master transaction: master15881
>> 2021/11/11 17:13:02 kid1| WARNING! Your cache is running out of
>> filedescriptors
>> ??? listening port: MyPortNameID1
>> 2021/11/11 17:13:19 kid1| WARNING! Your cache is running out of
>> filedescriptors
>>
>> But a mgr:info report:
>>
>> ??????? memPoolFree calls:??? 4295601
>> File descriptor usage for squid:
>> ??????? Maximum number of file descriptors:?? 10048
>> ??????? Largest file desc currently in use:??? 262
>> ??????? Number of file desc currently in use:? 135
>> ??????? Files queued for open:?????????????????? 0
>> ??????? Available number of file descriptors: 9913
>> ??????? Reserved number of file descriptors:? 9789
>>
>> After these errors the listen port is freeze and nobody is able to
>> surf.
>> a just "squid -k reconfigure" fix the issue and the proxy return to
>> normal mode for several minutes and back again to filedescriptors
>> issues.
>>
>> There is no relationship between filedescriptors issues and the number
>> of clients.
>> Sometimes the issue is discovered during the night when there is no
>> user that using the proxy ( just some robots like windows update )
>>
>> Is there something other we can investigate to help more stability of
>> the 5.x branch ?
>> Regards
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211112/c7dd88a6/attachment.htm>

From rousskov at measurement-factory.com  Fri Nov 12 16:24:11 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Nov 2021 11:24:11 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
Message-ID: <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>

On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:

> recently I'm facing too many ERROR: Collapsed forwarding queue
> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.

We see those overflows when kids die. Do you see any FATAL messages,
assertions, or similar deadly errors in cache.log?


> Could someone elaborate how the queue is filled - what is clogging it?

The sender/writer sends messages faster than the recipient/reader is
reading them, eventually exceeding the queue capacity (i.e. 1024
messages). These messages are about Store entries that may need
synchronization across workers. Each message is very small.


> I don't mind too much if I have to turn collapsed forwarding off

Most likely, the problem is not tied to collapsed forwarding. These
queues were used for collapsed forwarding when they were added, but they
are used for regular traffic as well in modern SMP Squids. We need to
change the queue names (and related code/message text) to reflect the
expanded nature of these queues.


HTH,

Alex.


From rousskov at measurement-factory.com  Mon Nov 15 16:16:56 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Nov 2021 11:16:56 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
Message-ID: <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>

On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:

> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"

I recommend ignoring queue overflows until the above assertions are
fixed because worker deaths cause queue overflows. Your Squid is buggy,
and those bugs essentially cause queue overflows.

The assertion itself is known as Bug 5134:
https://bugs.squid-cache.org/show_bug.cgi?id=5134

That bug has a speculative fix (master/v6 commit 5210df4). Please try it
if you can.


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Friday, November 12, 2021 5:24 PM
> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
> 
> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
> 
>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
> 
> We see those overflows when kids die. Do you see any FATAL messages, assertions, or similar deadly errors in cache.log?
> 
> 
>> Could someone elaborate how the queue is filled - what is clogging it?
> 
> The sender/writer sends messages faster than the recipient/reader is reading them, eventually exceeding the queue capacity (i.e. 1024 messages). These messages are about Store entries that may need synchronization across workers. Each message is very small.
> 
> 
>> I don't mind too much if I have to turn collapsed forwarding off
> 
> Most likely, the problem is not tied to collapsed forwarding. These queues were used for collapsed forwarding when they were added, but they are used for regular traffic as well in modern SMP Squids. We need to change the queue names (and related code/message text) to reflect the expanded nature of these queues.
> 
> 
> HTH,
> 
> Alex.
> 



From milan at serveo.nl  Mon Nov 15 17:09:02 2021
From: milan at serveo.nl (Milan)
Date: Mon, 15 Nov 2021 18:09:02 +0100
Subject: [squid-users] User linked to static unique outbound address
In-Reply-To: <013E9FFB-A330-4281-89C2-0998D92755A5@serveo.nl>
References: <013E9FFB-A330-4281-89C2-0998D92755A5@serveo.nl>
Message-ID: <12D9040D-3ADB-41E5-A798-77224E784F96@serveo.nl>

Hi,

Never mind, we found a different solution.

Cheers,

Milan 

> On 11 Nov 2021, at 12:11, Milan <milan at serveo.nl> wrote:
> 
> Hi Guys,
> 
> Currently I?m trying to setup a SQUID 4.11 proxy for approx 100 users, which should be assigned to static outbound IP. The inbound IP address should not be filtered / checked. Following multiple topics and the documentation I did setup the following configuration. Yet seemingly I keep running into non-functioning of the intended configuration. Basis Idea is as followed:
> 
> user1 > 199.199.199.2 (outbound address)
> user2 > 199.199.199.3
> user3 > 199.199.199.4
> user4 > 199.199.199.5
> etc...
> 
> Hence I filled /etc/squid/userip.conf as followed:
> 
> ###
> 199.199.199.2 user1
> 199.199.199.3 user2
> 199.199.199.4 user3
> 199.199.199.5 user4
> ###
> 
> With the following configuration file /etc/squid/squid.conf:
> 
> ###
> acl http proto http
> acl port_80 port 80
> acl port_443 port 443
> acl CONNECT method CONNECT
> 
> auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwords
> auth_param basic realm Please enter username and password
> 
> acl authenticated proxy_auth REQUIRED
> 
> external_acl_type userip %MYADDR %LOGIN /usr/lib64/squid/ext_file_userip_acl -f /etc/squid/userip.conf
> 
> acl userip external userip
> 
> http_access allow userip
> http_access deny all
> 
> http_port 3128
> acl ip1 myip 199.199.199.2
> tcp_outgoing_address 199.199.199.2 ip1
> 
> acl ip2 myip 199.199.199.3
> tcp_outgoing_address 199.199.199.3 ip2
> 
> acl ip2 myip 199.199.199.4
> tcp_outgoing_address 199.199.199.3 ip3
> 
> acl ip2 myip 199.199.199.5
> tcp_outgoing_address 199.199.199.3 ip4
> EOF
> ###
> 
> So far I tried both ?external_acl_type userip %MYADDR?  &  "external_acl_type userip %SRC? yet I keep running into this error and the proxy does not to functioning:
> 
> 1636498565.031      0 xx.xxx.xxx.xxx TCP_DENIED/407 4186 CONNECT xxx:443 user1 HIER_NONE/- text/html
> 
> Anyone knows how to solve this matter? Your input would be much appreciated!
> 
> Please advice, thanks!
> 
> 
> Best,
> 
> Milan
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211115/8e47c38c/attachment.htm>

From Loucansky.Lukas at kjj.cz  Tue Nov 16 09:38:12 2021
From: Loucansky.Lukas at kjj.cz (=?windows-1250?B?TG916GFuc2v9IEx1a+Ga?=)
Date: Tue, 16 Nov 2021 10:38:12 +0100
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>

Ok - but is it going to be patched only in the v6 version? I've just grabbed  squid-5.2-20211028-r74f073ecd and (for example) ReadWriteLock.cc does not look like in the patch (no finalizeExclusive() etc.)

Anyway - in the morning I run debug with 20,9 to see:

2021/11/16 09:02:06.459 kid2| ctx: enter level  0: 'http://i5.c.eset.com/v1/auth/4574614B3E4735238620/updlist/9/eid/1041372/lid/1041535'
2021/11/16 09:02:06.459 kid2| 20,7| store.cc(1431) timestampsSet: e:tw3490=p2VC/0x5620c7a7b000*3 had LV:-1        LU:1637049726 LM:-1        EX:-1
2021/11/16 09:02:06.459 kid2| 20,5| store.cc(1489) timestampsSet: e:tw3490=p2VC/0x5620c7a7b000*3 has LV:1637049726 LU:1637049726 LM:1637049726 EX:1637049736
2021/11/16 09:02:06.459 kid2| 20,3| Controller.cc(441) peek: 4F9D54687B6F14B553B031EA2D48E686
2021/11/16 09:02:06.459 kid2| 20,3| Controller.cc(449) peek: got local in-transit entry: e:tw3490=p2VC/0x5620c7a7b000*3
2021/11/16 09:02:06.459 kid2| 20,3| store.cc(443) lock: HttpStateData::haveParsedReplyHeaders locked key 4F9D54687B6F14B553B031EA2D48E686 e:tw3490=p2VC/0x5620c7a7b000*4
2021/11/16 09:02:06.459 kid2| 20,3| store.cc(467) unlock: HttpStateData::haveParsedReplyHeaders unlocking key 4F9D54687B6F14B553B031EA2D48E686 e:tw3490=p2VC/0x5620c7a7b000*4
2021/11/16 09:02:06.460 kid2| 20,3| store.cc(596) setPublicKey: e:tw3490=p2VC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| ctx: exit level  0
2021/11/16 09:02:06.460 kid2| 20,5| store.cc(815) write: storeWrite: writing 300 bytes for '4F9D54687B6F14B553B031EA2D48E686'
2021/11/16 09:02:06.460 kid2| 20,7| Controller.cc(537) memoryCacheHasSpaceFor: 1: 87+1?1038435
2021/11/16 09:02:06.460 kid2| 20,2| store.cc(991) checkCachable: StoreEntry::checkCachable: NO: too small
2021/11/16 09:02:06.460 kid2| 20,3| store.cc(455) releaseRequest: 0 e:tw3490=p2VC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,3| store.cc(569) setPrivateKey: 01 e:tw3490=p2VC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,7| Controller.cc(504) evictCached: e:tw3490=p2XVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,5| Transients.cc(347) evictCached: e:tw3490=p2XVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,3| store.cc(421) hashInsert: StoreEntry::hashInsert: Inserting Entry e:tw3490=p2XIVC/0x5620c7a7b000*3 key '4F710300000000008338000002000000'
2021/11/16 09:02:06.460 kid2| 20,3| store_swapout.cc(379) mayStartSwapOut: not cachable
2021/11/16 09:02:06.460 kid2| 20,7| store.cc(1833) transientsAbandonmentCheck: cannot be shared: e:tw3490=p2XIVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,5| Transients.cc(347) evictCached: e:tw3490=p2XIVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,7| MemStore.cc(828) write: entry e:tw3490=p2XIVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,2| store.cc(967) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/11/16 09:02:06.460 kid2| 20,7| MemStore.cc(585) shouldCache: Not memory cachable: e:tw3490=p2XIVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,7| store.cc(1833) transientsAbandonmentCheck: cannot be shared: e:tw3490=p2XIVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,5| Transients.cc(347) evictCached: e:tw3490=p2XIVC/0x5620c7a7b000*3
2021/11/16 09:02:06.460 kid2| 20,7| Controller.cc(609) memoryOut: keepInLocalMemory: 0
2021/11/16 09:02:06.460 kid2| 20,3| store.cc(443) lock: ClientHttpRequest::loggingEntry locked key 4F710300000000008338000002000000 e:tw3490=p2XIVC/0x5620c7a7b000*4
2021/11/16 09:02:06.460 kid2| 20,5| store.cc(815) write: storeWrite: writing 62 bytes for '4F710300000000008338000002000000'
2021/11/16 09:02:06.460 kid2| 20,7| Controller.cc(537) memoryCacheHasSpaceFor: 1: 88+1?1038435
2021/11/16 09:02:06.460 kid2| 20,3| store_swapout.cc(347) mayStartSwapOut:  already rejected
2021/11/16 09:02:06.460 kid2| 20,7| MemStore.cc(828) write: entry e:tw3490=p2XIV/0x5620c7a7b000*4
2021/11/16 09:02:06.460 kid2| 20,7| Controller.cc(609) memoryOut: keepInLocalMemory: 0
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(1061) complete: storeComplete: '4F710300000000008338000002000000'
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(1244) validLength: storeEntryValidLength: Checking '4F710300000000008338000002000000'
2021/11/16 09:02:06.461 kid2| 20,5| store.cc(1246) validLength: storeEntryValidLength:     object_len = 362
2021/11/16 09:02:06.461 kid2| 20,5| store.cc(1247) validLength: storeEntryValidLength:         hdr_sz = 300
2021/11/16 09:02:06.461 kid2| 20,5| store.cc(1248) validLength: storeEntryValidLength: content_length = 62
2021/11/16 09:02:06.461 kid2| 20,3| store_swapout.cc(347) mayStartSwapOut:  already rejected
2021/11/16 09:02:06.461 kid2| 20,7| MemStore.cc(828) write: entry e:tw3490=sp2XIV/0x5620c7a7b000*4
2021/11/16 09:02:06.461 kid2| 20,7| Controller.cc(609) memoryOut: keepInLocalMemory: 0
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(467) unlock: Client unlocking key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*4
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(467) unlock: FwdState unlocking key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*3
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(443) lock: store_client::copy locked key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*3
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(467) unlock: store_client::copy unlocking key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*3
2021/11/16 09:02:06.461 kid2| 20,3| store_swapout.cc(347) mayStartSwapOut:  already rejected
2021/11/16 09:02:06.461 kid2| 20,7| MemStore.cc(828) write: entry e:tw3490=sp2XIV/0x5620c7a7b000*2
2021/11/16 09:02:06.461 kid2| 20,7| Controller.cc(609) memoryOut: keepInLocalMemory: 0
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(443) lock: storeUnregister locked key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*3
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(467) unlock: storeUnregister unlocking key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*3
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(467) unlock: clientReplyContext::removeStoreReference unlocking key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*2
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(467) unlock: ClientHttpRequest::loggingEntry unlocking key 4F710300000000008338000002000000 e:tw3490=sp2XIV/0x5620c7a7b000*1
2021/11/16 09:02:06.461 kid2| 20,5| store.cc(483) doAbandon: e:tw3490=sp2XIV/0x5620c7a7b000*0 via ClientHttpRequest::loggingEntry
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(1180) release: 0 e:tw3490=sp2XIV/0x5620c7a7b000*0 4F710300000000008338000002000000
2021/11/16 09:02:06.461 kid2| 20,7| Controller.cc(504) evictCached: e:tw3490=sp2XIV/0x5620c7a7b000*0
2021/11/16 09:02:06.461 kid2| 20,5| Transients.cc(347) evictCached: e:tw3490=sp2XIV/0x5620c7a7b000*0
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(399) destroyStoreEntry: destroyStoreEntry: destroying 0x5620c7a7b008
2021/11/16 09:02:06.461 kid2| 20,3| store.cc(381) destroyMemObject: 0x5620ae385100 in e:tw3490=sp2XIV/0x5620c7a7b000*0
2021/11/16 09:02:06.461 kid2| 20,5| Transients.cc(372) disconnect: e:tw3490=sp2XIV/0x5620c7a7b000*0
2021/11/16 09:02:06.461 kid2| 20,3| MemObject.cc(108) ~MemObject: MemObject destructed, this=0x5620ae385100
2021/11/16 09:02:06.461 kid2| 20,5| store.cc(354) ~StoreEntry: StoreEntry destructed, this=0x5620c7a7b000
2021/11/16 09:02:06.488 kid2| 20,3| store.cc(1061) complete: storeComplete: '4E710300000000008338000002000000'
2021/11/16 09:02:06.488 kid2| 20,3| store.cc(1244) validLength: storeEntryValidLength: Checking '4E710300000000008338000002000000'
2021/11/16 09:02:06.488 kid2| 20,5| store.cc(1246) validLength: storeEntryValidLength:     object_len = 0
2021/11/16 09:02:06.488 kid2| 20,5| store.cc(1247) validLength: storeEntryValidLength:         hdr_sz = 0
2021/11/16 09:02:06.488 kid2| 20,5| store.cc(1248) validLength: storeEntryValidLength: content_length = -1
2021/11/16 09:02:06.488 kid2| 20,5| store.cc(1251) validLength: storeEntryValidLength: Unspecified content length: 4E710300000000008338000002000000
2021/11/16 09:02:06.488 kid2| 20,2| store.cc(991) checkCachable: StoreEntry::checkCachable: NO: too small
2021/11/16 09:02:06.488 kid2| 20,3| store.cc(455) releaseRequest: 0 e:=sp2IV/0x5620ef474d90*2
2021/11/16 09:02:06.488 kid2| 20,3| store.cc(569) setPrivateKey: 01 e:=sp2IV/0x5620ef474d90*2
2021/11/16 09:02:06.488 kid2| 20,3| store_swapout.cc(379) mayStartSwapOut: not cachable
2021/11/16 09:02:06.488 kid2| 20,7| MemStore.cc(828) write: entry e:=sp2XIV/0x5620ef474d90*2
2021/11/16 09:02:06.488 kid2| 20,2| store.cc(967) checkCachable: StoreEntry::checkCachable: NO: not cachable
2021/11/16 09:02:06.488 kid2| 20,7| MemStore.cc(585) shouldCache: Not memory cachable: e:=sp2XIV/0x5620ef474d90*2
2021/11/16 09:02:06.488 kid2| 20,7| Controller.cc(609) memoryOut: keepInLocalMemory: 0
2021/11/16 09:02:06.488 kid2| 20,3| store.cc(467) unlock: FwdState unlocking key 4E710300000000008338000002000000 e:=sp2XIV/0x5620ef474d90*2
2021/11/16 09:02:06.496 kid2| 20,3| Controller.cc(441) peek: 1E955E11E1FC7030D6ABDD39209EB79E
2021/11/16 09:02:06.496 kid2| 20,5| store.cc(349) StoreEntry: StoreEntry constructed, this=0x5620d722e560
2021/11/16 09:02:06.496 kid2| 20,3| MemObject.cc(100) MemObject: MemObject constructed, this=0x5620ef479200
2021/11/16 09:02:06.496 kid2| 20,7| Disks.cc(232) get: cache_dir 0 has: e:d1063200 at 0=sw2V/0x5620d722e560*0
2021/11/16 09:02:06.496 kid2| 20,3| Controller.cc(470) peek: got disk-cached entry: e:d1063200 at 0=sw2V/0x5620d722e560*0
2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old == e"

In fact - I don't even know if it's related to the eset files. I have refresh_pattern only for u.*\.eset\.com.*\/ and nothing for eset in the storeid_rewrite patterns.

LL


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Monday, November 15, 2021 5:17 PM
To: Squid Users
Cc: Lou?ansk? Luk??
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items

On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:

> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"

I recommend ignoring queue overflows until the above assertions are fixed because worker deaths cause queue overflows. Your Squid is buggy, and those bugs essentially cause queue overflows.

The assertion itself is known as Bug 5134:
https://bugs.squid-cache.org/show_bug.cgi?id=5134

That bug has a speculative fix (master/v6 commit 5210df4). Please try it if you can.


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Friday, November 12, 2021 5:24 PM
> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
> overflow for kid2 at 1024 items
> 
> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
> 
>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
> 
> We see those overflows when kids die. Do you see any FATAL messages, assertions, or similar deadly errors in cache.log?
> 
> 
>> Could someone elaborate how the queue is filled - what is clogging it?
> 
> The sender/writer sends messages faster than the recipient/reader is 
> reading them, eventually exceeding the queue capacity (i.e. 1024 
> messages). These messages are about Store entries that may need 
> synchronization across workers. Each message is very sm
all.
> 
> 
>> I don't mind too much if I have to turn collapsed forwarding off
> 
> Most likely, the problem is not tied to collapsed forwarding. These 
> queues were used for collapsed forwarding when they were added, but 
> they are used for regular traffic as well in modern SMP Squids. We 
> need to change the queue names (and related code/m
essage text) to reflect the expanded nature of these queues.
> 
> 
> HTH,
> 
> Alex.
> 



From david at articatech.com  Tue Nov 16 13:25:17 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 16 Nov 2021 14:25:17 +0100
Subject: [squid-users] squid 5.2: ntlm_fake_auth refuse to valid
 credentials
In-Reply-To: <913abf20-f525-52c5-3933-63a5d19478a2@articatech.com>
References: <b7b9ccc8-c394-552c-f99c-833463c52ff9@articatech.com>
 <e2c4f5fa-b797-b532-ccfe-3c40f52e01fe@treenet.co.nz>
 <913abf20-f525-52c5-3933-63a5d19478a2@articatech.com>
Message-ID: <17e61425-12df-a613-4a51-163aa24cfb45@articatech.com>

Any tips,

Is someone using Fake NTLM with modern browsers ?

Le 11/11/2021 ? 13:16, David Touzeau a ?crit?:
> Thanks Amos it will help understand something
>
> I think modern browser sending NTLMv2 as the ntlm_fake_auth 
> understanding only NTLMv1 ( perhaps )
>
> Using curl with --proxy-ntlm option is OK for squid as using browser 
> return allways a 407
> DO you know the limitation of ntlm_fake_auth according NTLM version.
> Is there a way to fix it ?
>
> ************* CURL ************
>
> [0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 06 82 08 00? NTLMSSP. 
> ........
> [0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
> ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
> [0010]? AE AA AA AA 06 82 08 00? 15 3A CC 83 0B 80 7B 45 ........ .......E
> [0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
> [0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
> ntlm_fake_auth.cc(170): pid=31874 :Got 'KK' from Squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 03 00 00 00 18 00 18 00 NTLMSSP. ........
> [0010]? 40 00 00 00 30 00 30 00? 58 00 00 00 00 00 00 00 ....0.0. X.......
> [0020]? 88 00 00 00 04 00 04 00? 88 00 00 00 09 00 09 00 ........ ........
> [0030]? 8C 00 00 00 00 00 00 00? 00 00 00 00 06 82 08 00 ........ ........
> [0040]? EB C7 B7 11 26 62 FD 82? B0 45 68 62 E0 6C E6 A3 .....b.. .Ehb.l..
> [0050]? 57 A7 E6 76 1C 7B 79 74? 17 71 72 5B 72 38 DA 30 W..v..yt .qr.r8.0
> [0060]? 06 4D 15 1F 9B D1 A2 A5? 01 01 00 00 00 00 00 00 .M...... ........
> [0070]? 80 38 3C 2A EA D6 D7 01? 57 A7 E6 76 1C 7B 79 74 .8...... W..v..yt
> [0080]? 00 00 00 00 00 00 00 00? 74 6F 74 6F 6E 74 6C 6D ........ totontlm
> [0090]? 70 72 6F 78 79 proxy
> ntlmauth.cc(244): pid=31874 :ntlm_unpack_auth: size of 149
> ntlmauth.cc(245): pid=31874 :ntlm_unpack_auth: flg 00088206
> ntlmauth.cc(246): pid=31874 :ntlm_unpack_auth: lmr o(64) l(24)
> ntlmauth.cc(247): pid=31874 :ntlm_unpack_auth: ntr o(88) l(48)
> ntlmauth.cc(248): pid=31874 :ntlm_unpack_auth: dom o(136) l(0)
> ntlmauth.cc(249): pid=31874 :ntlm_unpack_auth: usr o(136) l(4)
> ntlmauth.cc(250): pid=31874 :ntlm_unpack_auth: wst o(140) l(9)
> ntlmauth.cc(251): pid=31874 :ntlm_unpack_auth: key o(0) l(0)
> ntlmauth.cc(257): pid=31874 :ntlm_unpack_auth: Domain 't' (len=1).
> *ntlmauth.cc(268): pid=31874 :ntlm_unpack_auth: Username 'toton' (len=5).*
> ntlm_fake_auth.cc(210): pid=31874 :sending 'AF toton' to squid
>
>
> ********* But when connecting any modern browser to squid ***********
>
> [0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2? NTLMSSP. 
> ........
> [0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
> [0020]? 0A 00 63 45 00 00 00 0F ..cE....
> ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
> [0010]? AE AA AA AA 07 82 08 A2? C9 F0 4C 07 E0 79 9F CF ........ ..L..y..
> [0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
> [0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
> ntlm_fake_auth.cc(170): pid=31874 :Got 'YR' from Squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
> [0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
> [0020]? 0A 00 63 45 00 00 00 0F ..cE....
> ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
> [0010]? AE AA AA AA 07 82 08 A2? 49 12 A5 8A C8 17 3E 9D ........ I.......
> [0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
> [0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
> ntlm_fake_auth.cc(170): pid=31874 :Got 'YR' from Squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
> [0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
> [0020]? 0A 00 63 45 00 00 00 0F ..cE....
> ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
> [0010]? AE AA AA AA 07 82 08 A2? 09 6D 48 E6 12 9C 4B 30 ........ .mH...K0
> [0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
> [0030]? 50? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? P
> ntlm_fake_auth.cc(170): pid=31874 :Got 'YR' from Squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 01 00 00 00 07 82 08 A2 NTLMSSP. ........
> [0010]? 00 00 00 00 00 00 00 00? 00 00 00 00 00 00 00 00 ........ ........
> [0020]? 0A 00 63 45 00 00 00 0F ..cE....
> ntlm_fake_auth.cc(197): pid=31874 :sending 'TT' to squid with data:
> [0000]? 4E 54 4C 4D 53 53 50 00? 02 00 00 00 09 00 09 00 NTLMSSP. ........
> [0010]? AE AA AA AA 07 82 08 A2? F5 F6 8C B4 16 B9 20 CD ........ ........
> [0020]? 00 00 00 00 00 00 3A 00? 57 4F 52 4B 47 52 4F 55 ........ WORKGROU
>
>
>
> Le 11/11/2021 ? 08:40, Amos Jeffries a ?crit?:
>> On 11/11/21 14:12, David Touzeau wrote:
>>> Hi,
>>> i would like to use ntlm_fake_auth but it seems Squid refuse to 
>>> switch to authenticated user and return a 407 to the browser and 
>>> squid never accept? credentials.
>>>
>>> What i missing ?
>>>
>>> Configuration seems simple:
>>> auth_param ntlm program /lib/squid3/ntlm_fake_auth -v
>>> auth_param ntlm children 20 startup=5 idle=1 concurrency=0 
>>> queue-size=80 on-persistent-overload=ERR
>>> acl AUTHENTICATED proxy_auth REQUIRED
>>> http_access deny? !AUTHENTICATED
>>>
>>> Here the debug mode;
>>>
>>
>> The log you presented shows the helper delivering a TT response to 
>> Squid. Which is NTLM step 2 response token for a 407 challenge response.
>> That is only sent if there were not auth headers received from the 
>> client - which is correct per your config shown.
>>
>> The log snippet stops before Squid sends that response to the client, 
>> so whatever follows is unknown.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/66730e09/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov 16 14:42:21 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Nov 2021 09:42:21 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
Message-ID: <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>

On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
> is it going to be patched only in the v6 version? 

I hope the existing fix applies to v5 cleanly, and I am ready to help
with backporting if it does not. Beyond that, it is in the maintainer
hands. I cannot predict whether or when the fix will be officially
merged into v5 because I do not understand how those decisions are made.


> Anyway - in the morning I run debug with 20,9 to see:
> ...
> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old == e"

Unfortunately, I cannot see the cause of the assertion in this
short/partial trace -- the problematic actions happened before the trace
or were not logged during the trace.

Patching your Squid with commit 5210df4 is the best next step IMO. If
that patch does not help, then there are probably other bugs that we
need to fix in v5 (at least).


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Monday, November 15, 2021 5:17 PM
> To: Squid Users
> Cc: Lou?ansk? Luk??
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
> 
> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
> 
>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"
> 
> I recommend ignoring queue overflows until the above assertions are fixed because worker deaths cause queue overflows. Your Squid is buggy, and those bugs essentially cause queue overflows.
> 
> The assertion itself is known as Bug 5134:
> https://bugs.squid-cache.org/show_bug.cgi?id=5134
> 
> That bug has a speculative fix (master/v6 commit 5210df4). Please try it if you can.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Friday, November 12, 2021 5:24 PM
>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>
>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>
>> We see those overflows when kids die. Do you see any FATAL messages, assertions, or similar deadly errors in cache.log?
>>
>>
>>> Could someone elaborate how the queue is filled - what is clogging it?
>>
>> The sender/writer sends messages faster than the recipient/reader is 
>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>> messages). These messages are about Store entries that may need 
>> synchronization across workers. Each message is very sm
> all.
>>
>>
>>> I don't mind too much if I have to turn collapsed forwarding off
>>
>> Most likely, the problem is not tied to collapsed forwarding. These 
>> queues were used for collapsed forwarding when they were added, but 
>> they are used for regular traffic as well in modern SMP Squids. We 
>> need to change the queue names (and related code/m
> essage text) to reflect the expanded nature of these queues.
>>
>>
>> HTH,
>>
>> Alex.
>>



From marcelorodrigo at graminsta.com.br  Tue Nov 16 16:40:53 2021
From: marcelorodrigo at graminsta.com.br (Graminsta)
Date: Tue, 16 Nov 2021 13:40:53 -0300
Subject: [squid-users] Stable Squid Version for production on Linux
Message-ID: <005701d7db08$ba9f6ab0$2fde4010$@graminsta.com.br>

Hey folks  ;)

 

What is the most stable squid version for production on Ubuntu 18 or 20?

 

Marcelo

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/70fa76bd/attachment.htm>

From marcelorodrigo at graminsta.com.br  Tue Nov 16 16:53:52 2021
From: marcelorodrigo at graminsta.com.br (Graminsta)
Date: Tue, 16 Nov 2021 13:53:52 -0300
Subject: [squid-users] Multi-clients VPS - Authentication been shared.
Message-ID: <005e01d7db0a$8ab2ab80$a0180280$@graminsta.com.br>

Hello friends,

 

I'm using these user authentication lines in squid.conf based on user's
authentication list:

 

auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/users

auth_param basic children 5

auth_param basic realm Squid proxy-caching web server

auth_param basic credentialsttl 2 hours

auth_param basic casesensitive off

 

http_access allow localhost

acl clientes proxy_auth REQUIRED

http_access allow clientes

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager

http_access deny manager

http_access deny all

 

#List of outgoings (all IPs are fake)

http_port 181.111.11.111:4000 name=3

acl ip3 myportname 3

tcp_outgoing_address 2804:1934:2E1::3D6 ip3

 

http_port 181.111.11.112:4001 name=4

acl ip4 myportname 4

tcp_outgoing_address 2804:1934:3a8::3D7 ip4

 

The problem is that everyone whom is in the users file are allow to use all
tcp_outgoing_address.

If a smarter client scans for open IPs and ports will be able to find these
outgoings.

 

How can I restrict each user to their own tcp_outgoing_address output?

 

Tks.

Marcelo

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/69fb0a22/attachment.htm>

From Loucansky.Lukas at kjj.cz  Tue Nov 16 17:00:56 2021
From: Loucansky.Lukas at kjj.cz (=?windows-1250?B?TG916GFuc2v9IEx1a+Ga?=)
Date: Tue, 16 Nov 2021 18:00:56 +0100
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>

Ok - I will try to backport it from that patch into the v5 tree I've downloaded today. As we were using the mentioned build I came across these new assertions:

2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241: "anchorAt(anchorId).reading()"
2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
2021/11/16 13:02:09 kid2| assertion failed: Transients.cc:221: "old == e"
2021/11/16 13:52:05 kid2| assertion failed: Transients.cc:221: "old == e"
2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status == STORE_PENDING"
2021/11/16 15:26:15 kid1| assertion failed: Transients.cc:221: "old == e"
2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie == ((long)this ^ Cookie)" 


(no config changes)

My 1w cache.log is about 300MB - without elevated debug options (debug options ALL,1) - so it?s not easy to find something relevant with "9" options enabled...

LL

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Tuesday, November 16, 2021 3:42 PM
To: Lou?ansk? Luk??; Squid Users
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items

On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
> is it going to be patched only in the v6 version? 

I hope the existing fix applies to v5 cleanly, and I am ready to help with backporting if it does not. Beyond that, it is in the maintainer hands. I cannot predict whether or when the fix will be officially merged into v5 because I do not understand how those decisions are made.


> Anyway - in the morning I run debug with 20,9 to see:
> ...
> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old == e"

Unfortunately, I cannot see the cause of the assertion in this short/partial trace -- the problematic actions happened before the trace or were not logged during the trace.

Patching your Squid with commit 5210df4 is the best next step IMO. If that patch does not help, then there are probably other bugs that we need to fix in v5 (at least).


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Monday, November 15, 2021 5:17 PM
> To: Squid Users
> Cc: Lou?ansk? Luk??
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
> overflow for kid2 at 1024 items
> 
> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
> 
>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"
> 
> I recommend ignoring queue overflows until the above assertions are fixed because worker deaths cause queue overflows. Your Squid is buggy, and those bugs essentially cause queue overflows.
> 
> The assertion itself is known as Bug 5134:
> https://bugs.squid-cache.org/show_bug.cgi?id=5134
> 
> That bug has a speculative fix (master/v6 commit 5210df4). Please try it if you can.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Friday, November 12, 2021 5:24 PM
>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>
>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>
>> We see those overflows when kids die. Do you see any FATAL messages, assertions, or similar deadly errors in cache.log?
>>
>>
>>> Could someone elaborate how the queue is filled - what is clogging it?
>>
>> The sender/writer sends messages faster than the recipient/reader is 
>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>> messages). These messages are about Store entries that may need 
>> synchronization across workers. Each message is very sm
> all.
>>
>>
>>> I don't mind too much if I have to turn collapsed forwarding off
>>
>> Most likely, the problem is not tied to collapsed forwarding. These 
>> queues were used for collapsed forwarding when they were added, but 
>> they are used for regular traffic as well in modern SMP Squids. We 
>> need to change the queue names (and related code/m
> essage text) to reflect the expanded nature of these queues.
>>
>>
>> HTH,
>>
>> Alex.
>>



From david at articatech.com  Tue Nov 16 17:33:30 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 16 Nov 2021 18:33:30 +0100
Subject: [squid-users] Stable Squid Version for production on Linux
In-Reply-To: <005701d7db08$ba9f6ab0$2fde4010$@graminsta.com.br>
References: <005701d7db08$ba9f6ab0$2fde4010$@graminsta.com.br>
Message-ID: <60b9c5c9-202c-ad3c-c90d-a78f45f6c89c@articatech.com>

Hi,

For us it is Squid v4.17

Le 16/11/2021 ? 17:40, Graminsta a ?crit?:
>
> Hey folks ?;)
>
> What is the most stable squid version for production on Ubuntu 18 or 20?
>
> Marcelo
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/5b31cc34/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov 16 18:09:12 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Nov 2021 13:09:12 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
Message-ID: <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>

On 11/16/21 12:00 PM, Lou?ansk? Luk?? wrote:

> I will try to backport it from that patch into the v5 tree I've
> downloaded today. As we were using the mentioned build I came across
> these new assertions:

> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241: "anchorAt(anchorId).reading()"
> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status == STORE_PENDING"

I hope that at least some of the above assertions are fixed by master/v6
commit 5210df4.


> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie == ((long)this ^ Cookie)" 

This is probably an unrelated bug. I recommend filing a bug report in
Squid bugzilla and posting the corresponding "bt full" backtrace there.


Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Tuesday, November 16, 2021 3:42 PM
> To: Lou?ansk? Luk??; Squid Users
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
> 
> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
>> is it going to be patched only in the v6 version? 
> 
> I hope the existing fix applies to v5 cleanly, and I am ready to help with backporting if it does not. Beyond that, it is in the maintainer hands. I cannot predict whether or when the fix will be officially merged into v5 because I do not understand how those decisions are made.
> 
> 
>> Anyway - in the morning I run debug with 20,9 to see:
>> ...
>> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old == e"
> 
> Unfortunately, I cannot see the cause of the assertion in this short/partial trace -- the problematic actions happened before the trace or were not logged during the trace.
> 
> Patching your Squid with commit 5210df4 is the best next step IMO. If that patch does not help, then there are probably other bugs that we need to fix in v5 (at least).
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Monday, November 15, 2021 5:17 PM
>> To: Squid Users
>> Cc: Lou?ansk? Luk??
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
>>
>>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
>>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
>>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
>>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"
>>
>> I recommend ignoring queue overflows until the above assertions are fixed because worker deaths cause queue overflows. Your Squid is buggy, and those bugs essentially cause queue overflows.
>>
>> The assertion itself is known as Bug 5134:
>> https://bugs.squid-cache.org/show_bug.cgi?id=5134
>>
>> That bug has a speculative fix (master/v6 commit 5210df4). Please try it if you can.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>>> Sent: Friday, November 12, 2021 5:24 PM
>>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>>> overflow for kid2 at 1024 items
>>>
>>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>>
>>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>>
>>> We see those overflows when kids die. Do you see any FATAL messages, assertions, or similar deadly errors in cache.log?
>>>
>>>
>>>> Could someone elaborate how the queue is filled - what is clogging it?
>>>
>>> The sender/writer sends messages faster than the recipient/reader is 
>>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>>> messages). These messages are about Store entries that may need 
>>> synchronization across workers. Each message is very sm
>> all.
>>>
>>>
>>>> I don't mind too much if I have to turn collapsed forwarding off
>>>
>>> Most likely, the problem is not tied to collapsed forwarding. These 
>>> queues were used for collapsed forwarding when they were added, but 
>>> they are used for regular traffic as well in modern SMP Squids. We 
>>> need to change the queue names (and related code/m
>> essage text) to reflect the expanded nature of these queues.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>



From rousskov at measurement-factory.com  Tue Nov 16 18:23:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Nov 2021 13:23:00 -0500
Subject: [squid-users] Multi-clients VPS - Authentication been shared.
In-Reply-To: <005e01d7db0a$8ab2ab80$a0180280$@graminsta.com.br>
References: <005e01d7db0a$8ab2ab80$a0180280$@graminsta.com.br>
Message-ID: <7d3ac49b-d2f7-fffa-cb2d-e2377b8ada5e@measurement-factory.com>

On 11/16/21 11:53 AM, Graminsta wrote:
> Hello friends,
> 
> ?
> 
> I'm using these user authentication lines in squid.conf based on user?s
> authentication list:
> 
> ?
> 
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/users
> 
> auth_param basic children 5
> 
> auth_param basic realm Squid proxy-caching web server
> 
> auth_param basic credentialsttl 2 hours
> 
> auth_param basic casesensitive off
> 
> ?
> 
> http_access allow localhost
> 
> acl clientes proxy_auth REQUIRED
> 
> http_access allow clientes
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access deny all

Please note that you are allowing authenticated clients to send traffic
to unsafe ports. For example, they can CONNECT to non-SSL ports. You may
want to reorder the above rules if that is not what you want.


> #List of outgoings (all IPs are fake)
> 
> http_port 181.111.11.111:4000 name=3
> acl ip3 myportname 3
> tcp_outgoing_address 2804:1934:2E1::3D6 ip3
> 
> ?
> 
> http_port 181.111.11.112:4001 name=4
> acl ip4 myportname 4
> tcp_outgoing_address 2804:1934:3a8::3D7 ip4
> 
> ?
> 
> The problem is that everyone whom is in the users file are allow to use
> all tcp_outgoing_address.
> 
> If a smarter client scans for open IPs and ports will be able to find
> these outgoings.
> 
> ?
> 
> How can I restrict each user to their own tcp_outgoing_address output?

I suspect you are asking the wrong question. A better question is "How
do I restrict each user to their own http_port?". The answer is "Use
http_access to deny authenticated users connected to wrong ports."

However, you should also ask yourself another question: "Why am I using
multiple http_ports if all I care about is who uses which
tcp_outgoing_address?". The listening ports have virtually nothing to do
with tcp_outgoing_address...

I suspect you want something like this instead:

    http_port ...
    tcp_outgoing_address ...:3D01 user1
    tcp_outgoing_address ...:3D02 user2
    tcp_outgoing_address ...:3D03 user3
    ...

...where userN is an ACL that matches an authenticated user N.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Nov 16 22:52:43 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Nov 2021 17:52:43 -0500
Subject: [squid-users] Squid 5.2 unstable in production mode
In-Reply-To: <1a583ffa-c89d-028b-5712-d934687fb919@articatech.com>
References: <1a583ffa-c89d-028b-5712-d934687fb919@articatech.com>
Message-ID: <551c3295-3922-5381-f3d2-60dd112e7cc6@measurement-factory.com>

On 11/11/21 2:08 PM, David Touzeau wrote:

> 1) Memory leak

Currently tracked as https://bugs.squid-cache.org/show_bug.cgi?id=5132



> 2) Max filedescriptors issues:
> --------------------------------
> This is a strange behavior that Squid did not accept defined parameter:
> Example we set 65535 filedescriptors but squidclient mgr:info report
> 4096 and sometimes return back to 1024.

You may want to file a bug report with Squid bugzilla, detailing how you
set the parameter, whether cache.log output at startup matches your
setting, and sharing suspicious mgr:info output.


HTH,

Alex.


From squid3 at treenet.co.nz  Wed Nov 17 01:23:10 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Nov 2021 14:23:10 +1300
Subject: [squid-users] Rotate squid log files
In-Reply-To: <CAKaLTeMhS=hHAPgV7rQC-q4bHgEwh+407hBsrOa8T+vidrA7Tg@mail.gmail.com>
References: <CAKaLTeMhS=hHAPgV7rQC-q4bHgEwh+407hBsrOa8T+vidrA7Tg@mail.gmail.com>
Message-ID: <dacd3a07-b3ee-4274-14ad-1cdcc0c6db49@treenet.co.nz>

On 12/11/21 00:05, Omar Salem - KSACO IT Manager wrote:
> hi all,
> 
> I have ran the command (c:\squid\sbin\squid -n squid -k rotate)? to 
> rotate log files (Cache,store and access) but only cache and store logs 
> were rotated.
> I need to rotate access.log too because it reached 4GB.
> 
> Also how to see users visited websites in date and time because it is 
> not included in access.log.
> 


Things to check on:


A) does your access.log (and the directory path it site in) have the 
right permissions?
  Best way to test this is to shutdown squid, move the existing 
access.log file to a different path/name and restart squid. A new 
access.log file should be created with correct ownership.
  If a new file is *not* created, then there is a problem with the path. 
cache.log should have details about what is broken amongst the startup 
messages.


B) is your Squid configured or built to rely on external systems 
managing the access.log file rotation/renaming?

That is determined by the logfile_rotate directive. We publish Squid 
code have a default of 10, but some vendors provide integration with 
their OS logging and patch the default to be 0. You can set the 
directive in squid.conf to test for yourself Squid's rotation behaviour.

  NP: if your vendor is using third-party software to manage access.log 
you should use whatever rotation feature that software offers instead of 
"-k rotate"


Cheers,
Amos


From m_zouhairy at ckta.by  Thu Nov 18 07:08:28 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Thu, 18 Nov 2021 10:08:28 +0300
Subject: [squid-users] cannot open site
Message-ID: <7620cd14-903b-9559-ea63-df52017ecab3@ckta.by>

using squid 5.2, does it support TLS1.3?



     Failed to establish a secure connection to [unknown]

The system returned:

     [No Error] (TLS code: SQUID_TLS_ERR_CONNECT+TLS_IO_ERR=1)

     Failed to establish a secure connection: [No Error]

This proxy and the remote host failed to negotiate a mutually acceptable 
security settings for handling your request. It is possible that the 
remote host does not support secure connections, or the proxy is not 
satisfied with the host security credentials.

the site actually uses: (TLS_AES_128_GCM_SHA_256 128 bit keys, TLS 1.3)

squid is using ssl bump

and  i don't have tls options

#tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

anything to be configured?


From squid3 at treenet.co.nz  Thu Nov 18 09:44:02 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Nov 2021 22:44:02 +1300
Subject: [squid-users] cannot open site
In-Reply-To: <7620cd14-903b-9559-ea63-df52017ecab3@ckta.by>
References: <7620cd14-903b-9559-ea63-df52017ecab3@ckta.by>
Message-ID: <594a94ab-c0c9-e48d-9038-7dda42d71d57@treenet.co.nz>

On 18/11/21 20:08, Majed Zouhairy wrote:
> using squid 5.2, does it support TLS1.3?
> 

It does.

> 
>  ??? Failed to establish a secure connection to [unknown]
> 
> The system returned:
> 
>  ??? [No Error] (TLS code: SQUID_TLS_ERR_CONNECT+TLS_IO_ERR=1)
> 

That is an I/O error. Unable to read or write some bytes.


> 
> squid is using ssl bump
> 


TLS/1.3 handshakes are encrypted. It often cannot be bumped, only 
spliced. Check that traffic to this server is not attempting to 
bump/decrypt.


Amos


From m_zouhairy at ckta.by  Thu Nov 18 12:20:41 2021
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Thu, 18 Nov 2021 15:20:41 +0300
Subject: [squid-users] cannot open site
In-Reply-To: <594a94ab-c0c9-e48d-9038-7dda42d71d57@treenet.co.nz>
References: <7620cd14-903b-9559-ea63-df52017ecab3@ckta.by>
 <594a94ab-c0c9-e48d-9038-7dda42d71d57@treenet.co.nz>
Message-ID: <08f7eb07-7dd1-5be0-76fd-d93ad665480f@ckta.by>



On 11/18/21 12:44 PM, Amos Jeffries wrote:
> On 18/11/21 20:08, Majed Zouhairy wrote:
>> using squid 5.2, does it support TLS1.3?
>>
> 
> It does.
> 
>>
>> ???? Failed to establish a secure connection to [unknown]
>>
>> The system returned:
>>
>> ???? [No Error] (TLS code: SQUID_TLS_ERR_CONNECT+TLS_IO_ERR=1)
>>
> 
> That is an I/O error. Unable to read or write some bytes.
> 
> 
>>
>> squid is using ssl bump
>>
> 
> 
> TLS/1.3 handshakes are encrypted. It often cannot be bumped, only 
> spliced. Check that traffic to this server is not attempting to 
> bump/decrypt.
> 
> 
> Amos
thanks
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Thu Nov 18 16:26:24 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Nov 2021 11:26:24 -0500
Subject: [squid-users] cannot open site
In-Reply-To: <594a94ab-c0c9-e48d-9038-7dda42d71d57@treenet.co.nz>
References: <7620cd14-903b-9559-ea63-df52017ecab3@ckta.by>
 <594a94ab-c0c9-e48d-9038-7dda42d71d57@treenet.co.nz>
Message-ID: <497f7d06-cbd4-fc5b-859c-5b4413929a05@measurement-factory.com>

On 11/18/21 4:44 AM, Amos Jeffries wrote:
> On 18/11/21 20:08, Majed Zouhairy wrote:
>> squid is using ssl bump

> TLS/1.3 handshakes are encrypted. It often cannot be bumped, only
> spliced. Check that traffic to this server is not attempting to
> bump/decrypt.

Just to clarify: IIRC, bugs notwithstanding, Squid basic ability to bump
connections to TLS server does not depend on the TLS version. For
example, if the decision to bump is made during step2, then Squid should
be able to bump connections to TLS v1.3 servers.

However, when dealing with TLS v1.3 servers, some SslBump configurations
may match ssl_bump rules that admins do not expect to be matched and may
result in generation of deficient fake certificates because the plain
text parts of the handshake do not contain the server certificate.

Due to the lack of server certificates in the plain text part of the
Squid-server handshake, peeking or staring at the TLS v1.3 server is a
lot less useful than peeking or staring at TLS servers that use earlier
TLS versions.


HTH,

Alex.


From marcelorodrigo at graminsta.com.br  Thu Nov 18 20:54:26 2021
From: marcelorodrigo at graminsta.com.br (Graminsta)
Date: Thu, 18 Nov 2021 17:54:26 -0300
Subject: [squid-users] Multi-clients VPS - Authentication been shared.
Message-ID: <00aa01d7dcbe$7a9a2ba0$6fce82e0$@graminsta.com.br>

Tks for the answers.

Considerations:

1- "Please note that you are allowing authenticated clients to send traffic
to unsafe ports. For example, they can CONNECT to non-SSL ports. You may
want to reorder the above rules if that is not what you want."

ANSWER:
Tks for the advice, I already had it changed.


2- "However, you should also ask yourself another question: "Why am I using
multiple http_ports if all I care about is who uses which
tcp_outgoing_address?". The listening ports have virtually nothing to do
with tcp_outgoing_address..."

ANSWER:
Because I have to route each http_port to specific tcp_outgoing_address.
I have several customers per VPS.
Each one uses like 10 different ports to direct connections through
different IPv6s.

3- "Use http_access to deny authenticated users connected to wrong ports."

ANSWER:
So, in this scenario, how can I prevent users in the same users list to
access ports that not belong to them.
How to deny it in http_access rules?

Marcelo

-----Mensagem original-----
De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Em nome
de squid-users-request at lists.squid-cache.org
Enviada em: ter?a-feira, 16 de novembro de 2021 15:23
Para: squid-users at lists.squid-cache.org
Assunto: squid-users Digest, Vol 87, Issue 19

Send squid-users mailing list submissions to
	squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
	http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
	squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
	squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific than
"Re: Contents of squid-users digest..."


Today's Topics:

   1. Multi-clients VPS - Authentication been shared. (Graminsta)
   2. Re: Too many ERROR: Collapsed forwarding queue overflow for
      kid2 at 1024 items (Lou?ansk? Luk??)
   3. Re: Stable Squid Version for production on Linux (David Touzeau)
   4. Re: Too many ERROR: Collapsed forwarding queue overflow for
      kid2 at 1024 items (Alex Rousskov)
   5. Re: Multi-clients VPS - Authentication been shared.
      (Alex Rousskov)


----------------------------------------------------------------------

Message: 1
Date: Tue, 16 Nov 2021 13:53:52 -0300
From: "Graminsta" <marcelorodrigo at graminsta.com.br>
To: <squid-users at lists.squid-cache.org>
Subject: [squid-users] Multi-clients VPS - Authentication been shared.
Message-ID: <005e01d7db0a$8ab2ab80$a0180280$@graminsta.com.br>
Content-Type: text/plain; charset="us-ascii"

Hello friends,

 

I'm using these user authentication lines in squid.conf based on user's
authentication list:

 

auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/users

auth_param basic children 5

auth_param basic realm Squid proxy-caching web server

auth_param basic credentialsttl 2 hours

auth_param basic casesensitive off

 

http_access allow localhost

acl clientes proxy_auth REQUIRED

http_access allow clientes

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager

http_access deny manager

http_access deny all

 

#List of outgoings (all IPs are fake)

http_port 181.111.11.111:4000 name=3

acl ip3 myportname 3

tcp_outgoing_address 2804:1934:2E1::3D6 ip3

 

http_port 181.111.11.112:4001 name=4

acl ip4 myportname 4

tcp_outgoing_address 2804:1934:3a8::3D7 ip4

 

The problem is that everyone whom is in the users file are allow to use all
tcp_outgoing_address.

If a smarter client scans for open IPs and ports will be able to find these
outgoings.

 

How can I restrict each user to their own tcp_outgoing_address output?

 

Tks.

Marcelo

-------------- next part --------------
An HTML attachment was scrubbed...
URL:
<http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/69f
b0a22/attachment-0001.htm>

------------------------------

Message: 2
Date: Tue, 16 Nov 2021 18:00:56 +0100
From: Lou?ansk? Luk?? <Loucansky.Lukas at kjj.cz>
To: "Alex Rousskov" <rousskov at measurement-factory.com>, "Squid Users"
	<squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
	overflow for kid2 at 1024 items
Message-ID:
	<72DD5D5CF661B5459DC08A060BF26B53010897A8 at kjj-server.KJJ.local>
Content-Type: text/plain;	charset="windows-1250"

Ok - I will try to backport it from that patch into the v5 tree I've
downloaded today. As we were using the mentioned build I came across these
new assertions:

2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
"anchorAt(anchorId).reading()"
2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
2021/11/16 13:02:09 kid2| assertion failed: Transients.cc:221: "old == e"
2021/11/16 13:52:05 kid2| assertion failed: Transients.cc:221: "old == e"
2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status ==
STORE_PENDING"
2021/11/16 15:26:15 kid1| assertion failed: Transients.cc:221: "old == e"
2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
((long)this ^ Cookie)" 


(no config changes)

My 1w cache.log is about 300MB - without elevated debug options (debug
options ALL,1) - so it?s not easy to find something relevant with "9"
options enabled...

LL

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Tuesday, November 16, 2021 3:42 PM
To: Lou?ansk? Luk??; Squid Users
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
overflow for kid2 at 1024 items

On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
> is it going to be patched only in the v6 version? 

I hope the existing fix applies to v5 cleanly, and I am ready to help with
backporting if it does not. Beyond that, it is in the maintainer hands. I
cannot predict whether or when the fix will be officially merged into v5
because I do not understand how those decisions are made.


> Anyway - in the morning I run debug with 20,9 to see:
> ...
> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old ==
e"

Unfortunately, I cannot see the cause of the assertion in this short/partial
trace -- the problematic actions happened before the trace or were not
logged during the trace.

Patching your Squid with commit 5210df4 is the best next step IMO. If that
patch does not help, then there are probably other bugs that we need to fix
in v5 (at least).


HTH,

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Monday, November 15, 2021 5:17 PM
> To: Squid Users
> Cc: Lou?ansk? Luk??
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
> overflow for kid2 at 1024 items
> 
> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
> 
>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"
> 
> I recommend ignoring queue overflows until the above assertions are fixed
because worker deaths cause queue overflows. Your Squid is buggy, and those
bugs essentially cause queue overflows.
> 
> The assertion itself is known as Bug 5134:
> https://bugs.squid-cache.org/show_bug.cgi?id=5134
> 
> That bug has a speculative fix (master/v6 commit 5210df4). Please try it
if you can.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Friday, November 12, 2021 5:24 PM
>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>
>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>
>> We see those overflows when kids die. Do you see any FATAL messages,
assertions, or similar deadly errors in cache.log?
>>
>>
>>> Could someone elaborate how the queue is filled - what is clogging it?
>>
>> The sender/writer sends messages faster than the recipient/reader is 
>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>> messages). These messages are about Store entries that may need 
>> synchronization across workers. Each message is very sm
> all.
>>
>>
>>> I don't mind too much if I have to turn collapsed forwarding off
>>
>> Most likely, the problem is not tied to collapsed forwarding. These 
>> queues were used for collapsed forwarding when they were added, but 
>> they are used for regular traffic as well in modern SMP Squids. We 
>> need to change the queue names (and related code/m
> essage text) to reflect the expanded nature of these queues.
>>
>>
>> HTH,
>>
>> Alex.
>>



------------------------------

Message: 3
Date: Tue, 16 Nov 2021 18:33:30 +0100
From: David Touzeau <david at articatech.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Stable Squid Version for production on
	Linux
Message-ID: <60b9c5c9-202c-ad3c-c90d-a78f45f6c89c at articatech.com>
Content-Type: text/plain; charset="utf-8"; Format="flowed"

Hi,

For us it is Squid v4.17

Le 16/11/2021 ? 17:40, Graminsta a ?crit?:
>
> Hey folks ?;)
>
> What is the most stable squid version for production on Ubuntu 18 or 20?
>
> Marcelo
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL:
<http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/5b3
1cc34/attachment-0001.htm>

------------------------------

Message: 4
Date: Tue, 16 Nov 2021 13:09:12 -0500
From: Alex Rousskov <rousskov at measurement-factory.com>
To: Lou?ansk? Luk?? <Loucansky.Lukas at kjj.cz>, Squid Users
	<squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
	overflow for kid2 at 1024 items
Message-ID:
	<369d5766-b6ab-f4cd-8cef-64b298d84638 at measurement-factory.com>
Content-Type: text/plain; charset=windows-1250

On 11/16/21 12:00 PM, Lou?ansk? Luk?? wrote:

> I will try to backport it from that patch into the v5 tree I've 
> downloaded today. As we were using the mentioned build I came across 
> these new assertions:

> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
"anchorAt(anchorId).reading()"
> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status
== STORE_PENDING"

I hope that at least some of the above assertions are fixed by master/v6
commit 5210df4.


> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
((long)this ^ Cookie)" 

This is probably an unrelated bug. I recommend filing a bug report in Squid
bugzilla and posting the corresponding "bt full" backtrace there.


Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, November 16, 2021 3:42 PM
> To: Lou?ansk? Luk??; Squid Users
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
> overflow for kid2 at 1024 items
> 
> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
>> is it going to be patched only in the v6 version? 
> 
> I hope the existing fix applies to v5 cleanly, and I am ready to help with
backporting if it does not. Beyond that, it is in the maintainer hands. I
cannot predict whether or when the fix will be officially merged into v5
because I do not understand how those decisions are made.
> 
> 
>> Anyway - in the morning I run debug with 20,9 to see:
>> ...
>> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old
== e"
> 
> Unfortunately, I cannot see the cause of the assertion in this
short/partial trace -- the problematic actions happened before the trace or
were not logged during the trace.
> 
> Patching your Squid with commit 5210df4 is the best next step IMO. If that
patch does not help, then there are probably other bugs that we need to fix
in v5 (at least).
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Monday, November 15, 2021 5:17 PM
>> To: Squid Users
>> Cc: Lou?ansk? Luk??
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
>>
>>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old ==
e"
>>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old ==
e"
>>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old ==
e"
>>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old ==
e"
>>
>> I recommend ignoring queue overflows until the above assertions are fixed
because worker deaths cause queue overflows. Your Squid is buggy, and those
bugs essentially cause queue overflows.
>>
>> The assertion itself is known as Bug 5134:
>> https://bugs.squid-cache.org/show_bug.cgi?id=5134
>>
>> That bug has a speculative fix (master/v6 commit 5210df4). Please try it
if you can.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>>> Sent: Friday, November 12, 2021 5:24 PM
>>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding 
>>> queue overflow for kid2 at 1024 items
>>>
>>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>>
>>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>>
>>> We see those overflows when kids die. Do you see any FATAL messages,
assertions, or similar deadly errors in cache.log?
>>>
>>>
>>>> Could someone elaborate how the queue is filled - what is clogging it?
>>>
>>> The sender/writer sends messages faster than the recipient/reader is 
>>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>>> messages). These messages are about Store entries that may need 
>>> synchronization across workers. Each message is very sm
>> all.
>>>
>>>
>>>> I don't mind too much if I have to turn collapsed forwarding off
>>>
>>> Most likely, the problem is not tied to collapsed forwarding. These 
>>> queues were used for collapsed forwarding when they were added, but 
>>> they are used for regular traffic as well in modern SMP Squids. We 
>>> need to change the queue names (and related code/m
>> essage text) to reflect the expanded nature of these queues.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>



------------------------------

Message: 5
Date: Tue, 16 Nov 2021 13:23:00 -0500
From: Alex Rousskov <rousskov at measurement-factory.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Multi-clients VPS - Authentication been
	shared.
Message-ID:
	<7d3ac49b-d2f7-fffa-cb2d-e2377b8ada5e at measurement-factory.com>
Content-Type: text/plain; charset=windows-1252

On 11/16/21 11:53 AM, Graminsta wrote:
> Hello friends,
> 
> ?
> 
> I'm using these user authentication lines in squid.conf based on 
> user?s authentication list:
> 
> ?
> 
> auth_param basic program /usr/lib/squid/basic_ncsa_auth 
> /etc/squid/users
> 
> auth_param basic children 5
> 
> auth_param basic realm Squid proxy-caching web server
> 
> auth_param basic credentialsttl 2 hours
> 
> auth_param basic casesensitive off
> 
> ?
> 
> http_access allow localhost
> 
> acl clientes proxy_auth REQUIRED
> 
> http_access allow clientes
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access deny all

Please note that you are allowing authenticated clients to send traffic to
unsafe ports. For example, they can CONNECT to non-SSL ports. You may want
to reorder the above rules if that is not what you want.


> #List of outgoings (all IPs are fake)
> 
> http_port 181.111.11.111:4000 name=3
> acl ip3 myportname 3
> tcp_outgoing_address 2804:1934:2E1::3D6 ip3
> 
> ?
> 
> http_port 181.111.11.112:4001 name=4
> acl ip4 myportname 4
> tcp_outgoing_address 2804:1934:3a8::3D7 ip4
> 
> ?
> 
> The problem is that everyone whom is in the users file are allow to 
> use all tcp_outgoing_address.
> 
> If a smarter client scans for open IPs and ports will be able to find 
> these outgoings.
> 
> ?
> 
> How can I restrict each user to their own tcp_outgoing_address output?

I suspect you are asking the wrong question. A better question is "How do I
restrict each user to their own http_port?". The answer is "Use http_access
to deny authenticated users connected to wrong ports."

However, you should also ask yourself another question: "Why am I using
multiple http_ports if all I care about is who uses which
tcp_outgoing_address?". The listening ports have virtually nothing to do
with tcp_outgoing_address...

I suspect you want something like this instead:

    http_port ...
    tcp_outgoing_address ...:3D01 user1
    tcp_outgoing_address ...:3D02 user2
    tcp_outgoing_address ...:3D03 user3
    ...

...where userN is an ACL that matches an authenticated user N.


HTH,

Alex.


------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 87, Issue 19
*******************************************



From ngtech1ltd at gmail.com  Fri Nov 19 09:06:41 2021
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Fri, 19 Nov 2021 11:06:41 +0200
Subject: [squid-users] Multi-clients VPS - Authentication been shared.
In-Reply-To: <00aa01d7dcbe$7a9a2ba0$6fce82e0$@graminsta.com.br>
References: <00aa01d7dcbe$7a9a2ba0$6fce82e0$@graminsta.com.br>
Message-ID: <CABA8h=RQF0GhCcn73tqT6G5nzDdYYetpZRmprCf58PqORKUx_Q@mail.gmail.com>

I have created an example how to use and match usernames to
tcp_outgoing_ports

https://github.com/elico/vagrant-squid-outgoing-addresses

its better to use a single port with different user names (if possible).

Let me know what do you think about the solution I am offering and if the
example is understandable.

Eliezer


?????? ??? ??, 18 ????? 2021, 22:56, ??? Graminsta ?<
marcelorodrigo at graminsta.com.br>:

> Tks for the answers.
>
> Considerations:
>
> 1- "Please note that you are allowing authenticated clients to send traffic
> to unsafe ports. For example, they can CONNECT to non-SSL ports. You may
> want to reorder the above rules if that is not what you want."
>
> ANSWER:
> Tks for the advice, I already had it changed.
>
>
> 2- "However, you should also ask yourself another question: "Why am I using
> multiple http_ports if all I care about is who uses which
> tcp_outgoing_address?". The listening ports have virtually nothing to do
> with tcp_outgoing_address..."
>
> ANSWER:
> Because I have to route each http_port to specific tcp_outgoing_address.
> I have several customers per VPS.
> Each one uses like 10 different ports to direct connections through
> different IPv6s.
>
> 3- "Use http_access to deny authenticated users connected to wrong ports."
>
> ANSWER:
> So, in this scenario, how can I prevent users in the same users list to
> access ports that not belong to them.
> How to deny it in http_access rules?
>
> Marcelo
>
> -----Mensagem original-----
> De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Em nome
> de squid-users-request at lists.squid-cache.org
> Enviada em: ter?a-feira, 16 de novembro de 2021 15:23
> Para: squid-users at lists.squid-cache.org
> Assunto: squid-users Digest, Vol 87, Issue 19
>
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific than
> "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Multi-clients VPS - Authentication been shared. (Graminsta)
>    2. Re: Too many ERROR: Collapsed forwarding queue overflow for
>       kid2 at 1024 items (Lou?ansk? Luk??)
>    3. Re: Stable Squid Version for production on Linux (David Touzeau)
>    4. Re: Too many ERROR: Collapsed forwarding queue overflow for
>       kid2 at 1024 items (Alex Rousskov)
>    5. Re: Multi-clients VPS - Authentication been shared.
>       (Alex Rousskov)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 16 Nov 2021 13:53:52 -0300
> From: "Graminsta" <marcelorodrigo at graminsta.com.br>
> To: <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Multi-clients VPS - Authentication been shared.
> Message-ID: <005e01d7db0a$8ab2ab80$a0180280$@graminsta.com.br>
> Content-Type: text/plain; charset="us-ascii"
>
> Hello friends,
>
>
>
> I'm using these user authentication lines in squid.conf based on user's
> authentication list:
>
>
>
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/users
>
> auth_param basic children 5
>
> auth_param basic realm Squid proxy-caching web server
>
> auth_param basic credentialsttl 2 hours
>
> auth_param basic casesensitive off
>
>
>
> http_access allow localhost
>
> acl clientes proxy_auth REQUIRED
>
> http_access allow clientes
>
> http_access deny !Safe_ports
>
> http_access deny CONNECT !SSL_ports
>
> http_access allow localhost manager
>
> http_access deny manager
>
> http_access deny all
>
>
>
> #List of outgoings (all IPs are fake)
>
> http_port 181.111.11.111:4000 name=3
>
> acl ip3 myportname 3
>
> tcp_outgoing_address 2804:1934:2E1::3D6 ip3
>
>
>
> http_port 181.111.11.112:4001 name=4
>
> acl ip4 myportname 4
>
> tcp_outgoing_address 2804:1934:3a8::3D7 ip4
>
>
>
> The problem is that everyone whom is in the users file are allow to use all
> tcp_outgoing_address.
>
> If a smarter client scans for open IPs and ports will be able to find these
> outgoings.
>
>
>
> How can I restrict each user to their own tcp_outgoing_address output?
>
>
>
> Tks.
>
> Marcelo
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:
> <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/69f
> b0a22/attachment-0001.htm
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/69fb0a22/attachment-0001.htm>
> >
>
> ------------------------------
>
> Message: 2
> Date: Tue, 16 Nov 2021 18:00:56 +0100
> From: Lou?ansk? Luk?? <Loucansky.Lukas at kjj.cz>
> To: "Alex Rousskov" <rousskov at measurement-factory.com>, "Squid Users"
>         <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
>         overflow for kid2 at 1024 items
> Message-ID:
>         <72DD5D5CF661B5459DC08A060BF26B53010897A8 at kjj-server.KJJ.local>
> Content-Type: text/plain;       charset="windows-1250"
>
> Ok - I will try to backport it from that patch into the v5 tree I've
> downloaded today. As we were using the mentioned build I came across these
> new assertions:
>
> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
> "anchorAt(anchorId).reading()"
> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 13:02:09 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 13:52:05 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status ==
> STORE_PENDING"
> 2021/11/16 15:26:15 kid1| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
> ((long)this ^ Cookie)"
>
>
> (no config changes)
>
> My 1w cache.log is about 300MB - without elevated debug options (debug
> options ALL,1) - so it?s not easy to find something relevant with "9"
> options enabled...
>
> LL
>
> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, November 16, 2021 3:42 PM
> To: Lou?ansk? Luk??; Squid Users
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> overflow for kid2 at 1024 items
>
> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
> > is it going to be patched only in the v6 version?
>
> I hope the existing fix applies to v5 cleanly, and I am ready to help with
> backporting if it does not. Beyond that, it is in the maintainer hands. I
> cannot predict whether or when the fix will be officially merged into v5
> because I do not understand how those decisions are made.
>
>
> > Anyway - in the morning I run debug with 20,9 to see:
> > ...
> > 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old
> ==
> e"
>
> Unfortunately, I cannot see the cause of the assertion in this
> short/partial
> trace -- the problematic actions happened before the trace or were not
> logged during the trace.
>
> Patching your Squid with commit 5210df4 is the best next step IMO. If that
> patch does not help, then there are probably other bugs that we need to fix
> in v5 (at least).
>
>
> HTH,
>
> Alex.
>
>
> > -----Original Message-----
> > From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> > Sent: Monday, November 15, 2021 5:17 PM
> > To: Squid Users
> > Cc: Lou?ansk? Luk??
> > Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> > overflow for kid2 at 1024 items
> >
> > On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
> >
> >> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old ==
> e"
> >> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old ==
> e"
> >> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old ==
> e"
> >> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old ==
> e"
> >
> > I recommend ignoring queue overflows until the above assertions are fixed
> because worker deaths cause queue overflows. Your Squid is buggy, and those
> bugs essentially cause queue overflows.
> >
> > The assertion itself is known as Bug 5134:
> > https://bugs.squid-cache.org/show_bug.cgi?id=5134
> >
> > That bug has a speculative fix (master/v6 commit 5210df4). Please try it
> if you can.
> >
> >
> > HTH,
> >
> > Alex.
> >
> >
> >> -----Original Message-----
> >> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> >> Sent: Friday, November 12, 2021 5:24 PM
> >> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> >> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> >> overflow for kid2 at 1024 items
> >>
> >> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
> >>
> >>> recently I'm facing too many ERROR: Collapsed forwarding queue
> >>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
> >>
> >> We see those overflows when kids die. Do you see any FATAL messages,
> assertions, or similar deadly errors in cache.log?
> >>
> >>
> >>> Could someone elaborate how the queue is filled - what is clogging it?
> >>
> >> The sender/writer sends messages faster than the recipient/reader is
> >> reading them, eventually exceeding the queue capacity (i.e. 1024
> >> messages). These messages are about Store entries that may need
> >> synchronization across workers. Each message is very sm
> > all.
> >>
> >>
> >>> I don't mind too much if I have to turn collapsed forwarding off
> >>
> >> Most likely, the problem is not tied to collapsed forwarding. These
> >> queues were used for collapsed forwarding when they were added, but
> >> they are used for regular traffic as well in modern SMP Squids. We
> >> need to change the queue names (and related code/m
> > essage text) to reflect the expanded nature of these queues.
> >>
> >>
> >> HTH,
> >>
> >> Alex.
> >>
>
>
>
> ------------------------------
>
> Message: 3
> Date: Tue, 16 Nov 2021 18:33:30 +0100
> From: David Touzeau <david at articatech.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Stable Squid Version for production on
>         Linux
> Message-ID: <60b9c5c9-202c-ad3c-c90d-a78f45f6c89c at articatech.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
> Hi,
>
> For us it is Squid v4.17
>
> Le 16/11/2021 ? 17:40, Graminsta a ?crit?:
> >
> > Hey folks ?;)
> >
> > What is the most stable squid version for production on Ubuntu 18 or 20?
> >
> > Marcelo
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:
> <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/5b3
> 1cc34/attachment-0001.htm
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/5b31cc34/attachment-0001.htm>
> >
>
> ------------------------------
>
> Message: 4
> Date: Tue, 16 Nov 2021 13:09:12 -0500
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: Lou?ansk? Luk?? <Loucansky.Lukas at kjj.cz>, Squid Users
>         <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
>         overflow for kid2 at 1024 items
> Message-ID:
>         <369d5766-b6ab-f4cd-8cef-64b298d84638 at measurement-factory.com>
> Content-Type: text/plain; charset=windows-1250
>
> On 11/16/21 12:00 PM, Lou?ansk? Luk?? wrote:
>
> > I will try to backport it from that patch into the v5 tree I've
> > downloaded today. As we were using the mentioned build I came across
> > these new assertions:
>
> > 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
> "anchorAt(anchorId).reading()"
> > 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
> > 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status
> == STORE_PENDING"
>
> I hope that at least some of the above assertions are fixed by master/v6
> commit 5210df4.
>
>
> > 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
> > 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
> ((long)this ^ Cookie)"
>
> This is probably an unrelated bug. I recommend filing a bug report in Squid
> bugzilla and posting the corresponding "bt full" backtrace there.
>
>
> Alex.
>
>
> > -----Original Message-----
> > From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> > Sent: Tuesday, November 16, 2021 3:42 PM
> > To: Lou?ansk? Luk??; Squid Users
> > Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> > overflow for kid2 at 1024 items
> >
> > On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
> >> is it going to be patched only in the v6 version?
> >
> > I hope the existing fix applies to v5 cleanly, and I am ready to help
> with
> backporting if it does not. Beyond that, it is in the maintainer hands. I
> cannot predict whether or when the fix will be officially merged into v5
> because I do not understand how those decisions are made.
> >
> >
> >> Anyway - in the morning I run debug with 20,9 to see:
> >> ...
> >> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old
> == e"
> >
> > Unfortunately, I cannot see the cause of the assertion in this
> short/partial trace -- the problematic actions happened before the trace or
> were not logged during the trace.
> >
> > Patching your Squid with commit 5210df4 is the best next step IMO. If
> that
> patch does not help, then there are probably other bugs that we need to fix
> in v5 (at least).
> >
> >
> > HTH,
> >
> > Alex.
> >
> >
> >> -----Original Message-----
> >> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> >> Sent: Monday, November 15, 2021 5:17 PM
> >> To: Squid Users
> >> Cc: Lou?ansk? Luk??
> >> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> >> overflow for kid2 at 1024 items
> >>
> >> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
> >>
> >>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old ==
> e"
> >>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old ==
> e"
> >>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old ==
> e"
> >>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old ==
> e"
> >>
> >> I recommend ignoring queue overflows until the above assertions are
> fixed
> because worker deaths cause queue overflows. Your Squid is buggy, and those
> bugs essentially cause queue overflows.
> >>
> >> The assertion itself is known as Bug 5134:
> >> https://bugs.squid-cache.org/show_bug.cgi?id=5134
> >>
> >> That bug has a speculative fix (master/v6 commit 5210df4). Please try it
> if you can.
> >>
> >>
> >> HTH,
> >>
> >> Alex.
> >>
> >>
> >>> -----Original Message-----
> >>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> >>> Sent: Friday, November 12, 2021 5:24 PM
> >>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> >>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding
> >>> queue overflow for kid2 at 1024 items
> >>>
> >>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
> >>>
> >>>> recently I'm facing too many ERROR: Collapsed forwarding queue
> >>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
> >>>
> >>> We see those overflows when kids die. Do you see any FATAL messages,
> assertions, or similar deadly errors in cache.log?
> >>>
> >>>
> >>>> Could someone elaborate how the queue is filled - what is clogging it?
> >>>
> >>> The sender/writer sends messages faster than the recipient/reader is
> >>> reading them, eventually exceeding the queue capacity (i.e. 1024
> >>> messages). These messages are about Store entries that may need
> >>> synchronization across workers. Each message is very sm
> >> all.
> >>>
> >>>
> >>>> I don't mind too much if I have to turn collapsed forwarding off
> >>>
> >>> Most likely, the problem is not tied to collapsed forwarding. These
> >>> queues were used for collapsed forwarding when they were added, but
> >>> they are used for regular traffic as well in modern SMP Squids. We
> >>> need to change the queue names (and related code/m
> >> essage text) to reflect the expanded nature of these queues.
> >>>
> >>>
> >>> HTH,
> >>>
> >>> Alex.
> >>>
>
>
>
> ------------------------------
>
> Message: 5
> Date: Tue, 16 Nov 2021 13:23:00 -0500
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Multi-clients VPS - Authentication been
>         shared.
> Message-ID:
>         <7d3ac49b-d2f7-fffa-cb2d-e2377b8ada5e at measurement-factory.com>
> Content-Type: text/plain; charset=windows-1252
>
> On 11/16/21 11:53 AM, Graminsta wrote:
> > Hello friends,
> >
> > ?
> >
> > I'm using these user authentication lines in squid.conf based on
> > user?s authentication list:
> >
> > ?
> >
> > auth_param basic program /usr/lib/squid/basic_ncsa_auth
> > /etc/squid/users
> >
> > auth_param basic children 5
> >
> > auth_param basic realm Squid proxy-caching web server
> >
> > auth_param basic credentialsttl 2 hours
> >
> > auth_param basic casesensitive off
> >
> > ?
> >
> > http_access allow localhost
> >
> > acl clientes proxy_auth REQUIRED
> >
> > http_access allow clientes
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > http_access allow localhost manager
> > http_access deny manager
> > http_access deny all
>
> Please note that you are allowing authenticated clients to send traffic to
> unsafe ports. For example, they can CONNECT to non-SSL ports. You may want
> to reorder the above rules if that is not what you want.
>
>
> > #List of outgoings (all IPs are fake)
> >
> > http_port 181.111.11.111:4000 name=3
> > acl ip3 myportname 3
> > tcp_outgoing_address 2804:1934:2E1::3D6 ip3
> >
> > ?
> >
> > http_port 181.111.11.112:4001 name=4
> > acl ip4 myportname 4
> > tcp_outgoing_address 2804:1934:3a8::3D7 ip4
> >
> > ?
> >
> > The problem is that everyone whom is in the users file are allow to
> > use all tcp_outgoing_address.
> >
> > If a smarter client scans for open IPs and ports will be able to find
> > these outgoings.
> >
> > ?
> >
> > How can I restrict each user to their own tcp_outgoing_address output?
>
> I suspect you are asking the wrong question. A better question is "How do I
> restrict each user to their own http_port?". The answer is "Use http_access
> to deny authenticated users connected to wrong ports."
>
> However, you should also ask yourself another question: "Why am I using
> multiple http_ports if all I care about is who uses which
> tcp_outgoing_address?". The listening ports have virtually nothing to do
> with tcp_outgoing_address...
>
> I suspect you want something like this instead:
>
>     http_port ...
>     tcp_outgoing_address ...:3D01 user1
>     tcp_outgoing_address ...:3D02 user2
>     tcp_outgoing_address ...:3D03 user3
>     ...
>
> ...where userN is an ACL that matches an authenticated user N.
>
>
> HTH,
>
> Alex.
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 87, Issue 19
> *******************************************
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211119/e3693b49/attachment.htm>

From rousskov at measurement-factory.com  Fri Nov 19 16:06:39 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Nov 2021 11:06:39 -0500
Subject: [squid-users] Multi-clients VPS - Authentication been shared.
In-Reply-To: <00aa01d7dcbe$7a9a2ba0$6fce82e0$@graminsta.com.br>
References: <00aa01d7dcbe$7a9a2ba0$6fce82e0$@graminsta.com.br>
Message-ID: <44de04e6-d4af-0fcc-91c8-a3d0b9946b85@measurement-factory.com>

On 11/18/21 3:54 PM, Graminsta wrote:
> 2- "However, you should also ask yourself another question: "Why am I using
> multiple http_ports if all I care about is who uses which
> tcp_outgoing_address?". The listening ports have virtually nothing to do
> with tcp_outgoing_address..."
> 
> ANSWER:
> Because I have to route each http_port to specific tcp_outgoing_address.

This statement does not explain why you need multiple http_ports.


> I have several customers per VPS.
> Each one uses like 10 different ports to direct connections through
> different IPv6s.

These statements may conceal the answer, but I still do not understand
why all your customers cannot use one IPv6 http_port. Please note that I
am not saying that they can. I am only saying that it is not clear (to
me) why you cannot simplify this configuration to one http_port.



> 3- "Use http_access to deny authenticated users connected to wrong ports."
> 
> ANSWER:
> So, in this scenario, how can I prevent users in the same users list to
> access ports that not belong to them.
> How to deny it in http_access rules?

For each port X, you need to define an ACL that matches users that are
allowed to use http_port X. I do not know how to do that exactly because
I do not know what rules you use to make that decision -- these rules
are specific to your business/application logic. Once you have that ACL,
you only allow to-port-X traffic that matches it. Here is a sketch:

    http_port ... name=X
    http_port ... name=Y
    ...

    acl toPortX myportname X
    acl toPortX myportname Y
    ...

    acl allowedToUsePortX ...
    acl allowedToUsePortY ...
    ...

    http_access ... "standard" safety/admin rules here ...
    http_access allow toPortX allowedToUsePortX
    http_access allow toPortY allowedToUsePortY
    ...
    http_access deny all


HTH,

Alex.


> -----Mensagem original-----
> De: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Em nome
> de squid-users-request at lists.squid-cache.org
> Enviada em: ter?a-feira, 16 de novembro de 2021 15:23
> Para: squid-users at lists.squid-cache.org
> Assunto: squid-users Digest, Vol 87, Issue 19
> 
> Send squid-users mailing list submissions to
> 	squid-users at lists.squid-cache.org
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
> 	squid-users-request at lists.squid-cache.org
> 
> You can reach the person managing the list at
> 	squid-users-owner at lists.squid-cache.org
> 
> When replying, please edit your Subject line so it is more specific than
> "Re: Contents of squid-users digest..."
> 
> 
> Today's Topics:
> 
>    1. Multi-clients VPS - Authentication been shared. (Graminsta)
>    2. Re: Too many ERROR: Collapsed forwarding queue overflow for
>       kid2 at 1024 items (Lou?ansk? Luk??)
>    3. Re: Stable Squid Version for production on Linux (David Touzeau)
>    4. Re: Too many ERROR: Collapsed forwarding queue overflow for
>       kid2 at 1024 items (Alex Rousskov)
>    5. Re: Multi-clients VPS - Authentication been shared.
>       (Alex Rousskov)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Tue, 16 Nov 2021 13:53:52 -0300
> From: "Graminsta" <marcelorodrigo at graminsta.com.br>
> To: <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Multi-clients VPS - Authentication been shared.
> Message-ID: <005e01d7db0a$8ab2ab80$a0180280$@graminsta.com.br>
> Content-Type: text/plain; charset="us-ascii"
> 
> Hello friends,
> 
>  
> 
> I'm using these user authentication lines in squid.conf based on user's
> authentication list:
> 
>  
> 
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/users
> 
> auth_param basic children 5
> 
> auth_param basic realm Squid proxy-caching web server
> 
> auth_param basic credentialsttl 2 hours
> 
> auth_param basic casesensitive off
> 
>  
> 
> http_access allow localhost
> 
> acl clientes proxy_auth REQUIRED
> 
> http_access allow clientes
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> 
> http_access deny manager
> 
> http_access deny all
> 
>  
> 
> #List of outgoings (all IPs are fake)
> 
> http_port 181.111.11.111:4000 name=3
> 
> acl ip3 myportname 3
> 
> tcp_outgoing_address 2804:1934:2E1::3D6 ip3
> 
>  
> 
> http_port 181.111.11.112:4001 name=4
> 
> acl ip4 myportname 4
> 
> tcp_outgoing_address 2804:1934:3a8::3D7 ip4
> 
>  
> 
> The problem is that everyone whom is in the users file are allow to use all
> tcp_outgoing_address.
> 
> If a smarter client scans for open IPs and ports will be able to find these
> outgoings.
> 
>  
> 
> How can I restrict each user to their own tcp_outgoing_address output?
> 
>  
> 
> Tks.
> 
> Marcelo
> 
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/69f
> b0a22/attachment-0001.htm>
> 
> ------------------------------
> 
> Message: 2
> Date: Tue, 16 Nov 2021 18:00:56 +0100
> From: Lou?ansk? Luk?? <Loucansky.Lukas at kjj.cz>
> To: "Alex Rousskov" <rousskov at measurement-factory.com>, "Squid Users"
> 	<squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> 	overflow for kid2 at 1024 items
> Message-ID:
> 	<72DD5D5CF661B5459DC08A060BF26B53010897A8 at kjj-server.KJJ.local>
> Content-Type: text/plain;	charset="windows-1250"
> 
> Ok - I will try to backport it from that patch into the v5 tree I've
> downloaded today. As we were using the mentioned build I came across these
> new assertions:
> 
> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
> "anchorAt(anchorId).reading()"
> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 13:02:09 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 13:52:05 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status ==
> STORE_PENDING"
> 2021/11/16 15:26:15 kid1| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
> ((long)this ^ Cookie)" 
> 
> 
> (no config changes)
> 
> My 1w cache.log is about 300MB - without elevated debug options (debug
> options ALL,1) - so it?s not easy to find something relevant with "9"
> options enabled...
> 
> LL
> 
> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, November 16, 2021 3:42 PM
> To: Lou?ansk? Luk??; Squid Users
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> overflow for kid2 at 1024 items
> 
> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
>> is it going to be patched only in the v6 version? 
> 
> I hope the existing fix applies to v5 cleanly, and I am ready to help with
> backporting if it does not. Beyond that, it is in the maintainer hands. I
> cannot predict whether or when the fix will be officially merged into v5
> because I do not understand how those decisions are made.
> 
> 
>> Anyway - in the morning I run debug with 20,9 to see:
>> ...
>> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old ==
> e"
> 
> Unfortunately, I cannot see the cause of the assertion in this short/partial
> trace -- the problematic actions happened before the trace or were not
> logged during the trace.
> 
> Patching your Squid with commit 5210df4 is the best next step IMO. If that
> patch does not help, then there are probably other bugs that we need to fix
> in v5 (at least).
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Monday, November 15, 2021 5:17 PM
>> To: Squid Users
>> Cc: Lou?ansk? Luk??
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
>>
>>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old == e"
>>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old == e"
>>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old == e"
>>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old == e"
>>
>> I recommend ignoring queue overflows until the above assertions are fixed
> because worker deaths cause queue overflows. Your Squid is buggy, and those
> bugs essentially cause queue overflows.
>>
>> The assertion itself is known as Bug 5134:
>> https://bugs.squid-cache.org/show_bug.cgi?id=5134
>>
>> That bug has a speculative fix (master/v6 commit 5210df4). Please try it
> if you can.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>>> Sent: Friday, November 12, 2021 5:24 PM
>>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>>> overflow for kid2 at 1024 items
>>>
>>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>>
>>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>>
>>> We see those overflows when kids die. Do you see any FATAL messages,
> assertions, or similar deadly errors in cache.log?
>>>
>>>
>>>> Could someone elaborate how the queue is filled - what is clogging it?
>>>
>>> The sender/writer sends messages faster than the recipient/reader is 
>>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>>> messages). These messages are about Store entries that may need 
>>> synchronization across workers. Each message is very sm
>> all.
>>>
>>>
>>>> I don't mind too much if I have to turn collapsed forwarding off
>>>
>>> Most likely, the problem is not tied to collapsed forwarding. These 
>>> queues were used for collapsed forwarding when they were added, but 
>>> they are used for regular traffic as well in modern SMP Squids. We 
>>> need to change the queue names (and related code/m
>> essage text) to reflect the expanded nature of these queues.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
> 
> 
> 
> ------------------------------
> 
> Message: 3
> Date: Tue, 16 Nov 2021 18:33:30 +0100
> From: David Touzeau <david at articatech.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Stable Squid Version for production on
> 	Linux
> Message-ID: <60b9c5c9-202c-ad3c-c90d-a78f45f6c89c at articatech.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
> 
> Hi,
> 
> For us it is Squid v4.17
> 
> Le 16/11/2021 ? 17:40, Graminsta a ?crit?:
>>
>> Hey folks ?;)
>>
>> What is the most stable squid version for production on Ubuntu 18 or 20?
>>
>> Marcelo
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:
> <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211116/5b3
> 1cc34/attachment-0001.htm>
> 
> ------------------------------
> 
> Message: 4
> Date: Tue, 16 Nov 2021 13:09:12 -0500
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: Lou?ansk? Luk?? <Loucansky.Lukas at kjj.cz>, Squid Users
> 	<squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> 	overflow for kid2 at 1024 items
> Message-ID:
> 	<369d5766-b6ab-f4cd-8cef-64b298d84638 at measurement-factory.com>
> Content-Type: text/plain; charset=windows-1250
> 
> On 11/16/21 12:00 PM, Lou?ansk? Luk?? wrote:
> 
>> I will try to backport it from that patch into the v5 tree I've 
>> downloaded today. As we were using the mentioned build I came across 
>> these new assertions:
> 
>> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
> "anchorAt(anchorId).reading()"
>> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status
> == STORE_PENDING"
> 
> I hope that at least some of the above assertions are fixed by master/v6
> commit 5210df4.
> 
> 
>> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
>> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
> ((long)this ^ Cookie)" 
> 
> This is probably an unrelated bug. I recommend filing a bug report in Squid
> bugzilla and posting the corresponding "bt full" backtrace there.
> 
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Tuesday, November 16, 2021 3:42 PM
>> To: Lou?ansk? Luk??; Squid Users
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
>>> is it going to be patched only in the v6 version? 
>>
>> I hope the existing fix applies to v5 cleanly, and I am ready to help with
> backporting if it does not. Beyond that, it is in the maintainer hands. I
> cannot predict whether or when the fix will be officially merged into v5
> because I do not understand how those decisions are made.
>>
>>
>>> Anyway - in the morning I run debug with 20,9 to see:
>>> ...
>>> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old
> == e"
>>
>> Unfortunately, I cannot see the cause of the assertion in this
> short/partial trace -- the problematic actions happened before the trace or
> were not logged during the trace.
>>
>> Patching your Squid with commit 5210df4 is the best next step IMO. If that
> patch does not help, then there are probably other bugs that we need to fix
> in v5 (at least).
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> -----Original Message-----
>>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>>> Sent: Monday, November 15, 2021 5:17 PM
>>> To: Squid Users
>>> Cc: Lou?ansk? Luk??
>>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>>> overflow for kid2 at 1024 items
>>>
>>> On 11/15/21 7:43 AM, Lou?ansk? Luk?? wrote:
>>>
>>>> 2021/11/14 10:13:30 kid2| assertion failed: Transients.cc:221: "old ==
> e"
>>>> 2021/11/15 08:37:36 kid2| assertion failed: Transients.cc:221: "old ==
> e"
>>>> 2021/11/15 11:54:14 kid1| assertion failed: Transients.cc:221: "old ==
> e"
>>>> 2021/11/15 12:16:27 kid1| assertion failed: Transients.cc:221: "old ==
> e"
>>>
>>> I recommend ignoring queue overflows until the above assertions are fixed
> because worker deaths cause queue overflows. Your Squid is buggy, and those
> bugs essentially cause queue overflows.
>>>
>>> The assertion itself is known as Bug 5134:
>>> https://bugs.squid-cache.org/show_bug.cgi?id=5134
>>>
>>> That bug has a speculative fix (master/v6 commit 5210df4). Please try it
> if you can.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>> -----Original Message-----
>>>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>>>> Sent: Friday, November 12, 2021 5:24 PM
>>>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding 
>>>> queue overflow for kid2 at 1024 items
>>>>
>>>> On 11/11/21 10:19 AM, Lou?ansk? Luk?? wrote:
>>>>
>>>>> recently I'm facing too many ERROR: Collapsed forwarding queue 
>>>>> overflow for kid2 at 1024 items lines in my Squid 5.2 log files.
>>>>
>>>> We see those overflows when kids die. Do you see any FATAL messages,
> assertions, or similar deadly errors in cache.log?
>>>>
>>>>
>>>>> Could someone elaborate how the queue is filled - what is clogging it?
>>>>
>>>> The sender/writer sends messages faster than the recipient/reader is 
>>>> reading them, eventually exceeding the queue capacity (i.e. 1024 
>>>> messages). These messages are about Store entries that may need 
>>>> synchronization across workers. Each message is very sm
>>> all.
>>>>
>>>>
>>>>> I don't mind too much if I have to turn collapsed forwarding off
>>>>
>>>> Most likely, the problem is not tied to collapsed forwarding. These 
>>>> queues were used for collapsed forwarding when they were added, but 
>>>> they are used for regular traffic as well in modern SMP Squids. We 
>>>> need to change the queue names (and related code/m
>>> essage text) to reflect the expanded nature of these queues.
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>>
> 
> 
> 
> ------------------------------
> 
> Message: 5
> Date: Tue, 16 Nov 2021 13:23:00 -0500
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Multi-clients VPS - Authentication been
> 	shared.
> Message-ID:
> 	<7d3ac49b-d2f7-fffa-cb2d-e2377b8ada5e at measurement-factory.com>
> Content-Type: text/plain; charset=windows-1252
> 
> On 11/16/21 11:53 AM, Graminsta wrote:
>> Hello friends,
>>
>> ?
>>
>> I'm using these user authentication lines in squid.conf based on 
>> user?s authentication list:
>>
>> ?
>>
>> auth_param basic program /usr/lib/squid/basic_ncsa_auth 
>> /etc/squid/users
>>
>> auth_param basic children 5
>>
>> auth_param basic realm Squid proxy-caching web server
>>
>> auth_param basic credentialsttl 2 hours
>>
>> auth_param basic casesensitive off
>>
>> ?
>>
>> http_access allow localhost
>>
>> acl clientes proxy_auth REQUIRED
>>
>> http_access allow clientes
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>> http_access deny all
> 
> Please note that you are allowing authenticated clients to send traffic to
> unsafe ports. For example, they can CONNECT to non-SSL ports. You may want
> to reorder the above rules if that is not what you want.
> 
> 
>> #List of outgoings (all IPs are fake)
>>
>> http_port 181.111.11.111:4000 name=3
>> acl ip3 myportname 3
>> tcp_outgoing_address 2804:1934:2E1::3D6 ip3
>>
>> ?
>>
>> http_port 181.111.11.112:4001 name=4
>> acl ip4 myportname 4
>> tcp_outgoing_address 2804:1934:3a8::3D7 ip4
>>
>> ?
>>
>> The problem is that everyone whom is in the users file are allow to 
>> use all tcp_outgoing_address.
>>
>> If a smarter client scans for open IPs and ports will be able to find 
>> these outgoings.
>>
>> ?
>>
>> How can I restrict each user to their own tcp_outgoing_address output?
> 
> I suspect you are asking the wrong question. A better question is "How do I
> restrict each user to their own http_port?". The answer is "Use http_access
> to deny authenticated users connected to wrong ports."
> 
> However, you should also ask yourself another question: "Why am I using
> multiple http_ports if all I care about is who uses which
> tcp_outgoing_address?". The listening ports have virtually nothing to do
> with tcp_outgoing_address...
> 
> I suspect you want something like this instead:
> 
>     http_port ...
>     tcp_outgoing_address ...:3D01 user1
>     tcp_outgoing_address ...:3D02 user2
>     tcp_outgoing_address ...:3D03 user3
>     ...
> 
> ...where userN is an ACL that matches an authenticated user N.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> ------------------------------
> 
> Subject: Digest Footer
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> ------------------------------
> 
> End of squid-users Digest, Vol 87, Issue 19
> *******************************************
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From marcelorodrigo at graminsta.com.br  Fri Nov 19 19:45:46 2021
From: marcelorodrigo at graminsta.com.br (Graminsta)
Date: Fri, 19 Nov 2021 16:45:46 -0300
Subject: [squid-users] How to prevent Squid to use more and more HD space in
 virtualbox
Message-ID: <00ce01d7dd7e$0dd43db0$297cb910$@graminsta.com.br>

Thanks so much for the answers to my queries about squid.

 

I have a problem with the continuous increase in disk space usage on linux
where squid is hosted. Even having turned off the cache ("#cache_dir ufs
/var/spool/squid/ 1000 16 256").

 

I have to keep recreating virtualbox instances and deleting the old ones
frequently.

 

VMs starts at 7gb and in a few weeks grows to 10...20...30Gb.

 

Why does Squid need so much growing hard drive space and how to prevent it?

 

Marcelo

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211119/daaa7bae/attachment.htm>

From rousskov at measurement-factory.com  Fri Nov 19 22:51:32 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Nov 2021 17:51:32 -0500
Subject: [squid-users] How to prevent Squid to use more and more HD
 space in virtualbox
In-Reply-To: <00ce01d7dd7e$0dd43db0$297cb910$@graminsta.com.br>
References: <00ce01d7dd7e$0dd43db0$297cb910$@graminsta.com.br>
Message-ID: <fa294143-ada8-6425-b11e-0479db8b9fe6@measurement-factory.com>

On 11/19/21 2:45 PM, Graminsta wrote:

> I have a problem with the continuous increase in disk space usage on
> linux where squid is hosted. Even having turned off the cache
> ("#cache_dir ufs /var/spool/squid/ 1000 16 256").

This is unrelated to your question, but please note that removing
cache_dir directives disables disk caching but not memory caching.
Memory caching does not consume disk space, but if you want to disable
memory caching as well, use "cache_mem 0".
 ?

> VMs starts at 7gb and in a few weeks grows to 10...20...30Gb.
> 
> Why does Squid need so much growing hard drive space

There could be several reasons, but the most common one is logging.
There are several kinds of logs (cache_log, access_log, icap_log, etc.).
You should find out which ones are growing beyond your expectations in
your environment. Look for files that are actively growing in size on
the disk partition that grows in size (e.g., some logs are often found
in /use/local/squid/var/logs or similar directories).

If logs are not the reason for excessive disk usage, then I would check
for coredump files (with unique file names).


> and how to prevent it?

0. If you start your Squid with "-X" command line option, then stop
doing that. That option enables extensive debugging that goes into
cache_log. It is meant for triage sessions only.

1. If you set "debug_options" to anything other than "ALL,1", then
either set it to ALL,1 or remove all debug_options lines. The default is
ALL,1.

2. Rotate your logs as needed. There is some good advice in the Squid
FAQ[F], but since some of the wiki pages are quite all/stale, use common
sense and best sysadmin practices.

[F] https://wiki.squid-cache.org/SquidFaq/SquidLogs


3. If your Squids are crashing (and leaving coredumps with unique file
names), then fixing Squid or removing crash triggers is usually the best
way forward.


HTH,

Alex.


From uhlar at fantomas.sk  Sat Nov 20 18:02:53 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 20 Nov 2021 19:02:53 +0100
Subject: [squid-users] How to prevent Squid to use more and more HD
 space in virtualbox
In-Reply-To: <00ce01d7dd7e$0dd43db0$297cb910$@graminsta.com.br>
References: <00ce01d7dd7e$0dd43db0$297cb910$@graminsta.com.br>
Message-ID: <20211120180253.GA22479@fantomas.sk>

On 19.11.21 16:45, Graminsta wrote:
>Thanks so much for the answers to my queries about squid.
>
>I have a problem with the continuous increase in disk space usage on linux
>where squid is hosted. Even having turned off the cache ("#cache_dir ufs
>/var/spool/squid/ 1000 16 256").
>
>I have to keep recreating virtualbox instances and deleting the old ones
>frequently.
>
>VMs starts at 7gb and in a few weeks grows to 10...20...30Gb.
>
>Why does Squid need so much growing hard drive space and how to prevent it?


have you checked which files/directories use most of space?

du -kax / |sort -nr|head

repeat for every on-disk filesystem (e.g. if you have separate /var)

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
   One OS to rule them all, One OS to find them,
One OS to bring them all and into darkness bind them


From Loucansky.Lukas at kjj.cz  Sun Nov 21 19:08:32 2021
From: Loucansky.Lukas at kjj.cz (=?iso-8859-2?B?TG916GFuc2v9IEx1a+G5?=)
Date: Sun, 21 Nov 2021 20:08:32 +0100
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
 <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>

Hello, I've replaced src/ipc/ReadWritelock.cc and ReadWriteLock.h with modified versions (with finalizeExclusive calls), but afeter some time I have got queue overflows agains.
So I've downloaded squid-6.0.0-20211116-r90086c5a8 - run it with my v5 configure and now I'm testing it. So far I get these:
2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b5f3d740*0
2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b8e67480*0
2021/11/21 19:05:07 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b7af7b40*0
2021/11/21 19:05:07 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b8993850*0
2021/11/21 19:05:08 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b5269200*0
2021/11/21 19:05:08 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b5269200*0
2021/11/21 19:05:18 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr6455=V/0x5642b7c91210*0
2021/11/21 19:53:48 kid1| Bug: Missing MemObject::storeId value
2021/11/21 20:02:16 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for e:tr11100=V/0x5642b4898050*0
etc.

Regard the transient queue - could this be any interesting?

by kid1 {
Transients queues:
  kid1 receiving from kid1: { size: 0, capacity: 1024, other: 112, popIndex: 112 }
  kid1 receiving from kid2: { size: 0, capacity: 1024, other: 1506, popIndex: 1506 }

  kid1 sending to kid1: { size: 0, capacity: 1024, pushIndex: 112, other: 112 }
  kid1 sending to kid2: { size: 0, capacity: 1024, pushIndex: 3301, other: 3301 }

SMP disk I/O queues:
  kid1 receiving from kid3: { size: 0, capacity: 1024, other: 339, popIndex: 339 }
  kid1 receiving from kid4: { size: 0, capacity: 1024, other: 40, popIndex: 40 }
  kid1 receiving from kid5: { size: 0, capacity: 1024, other: 589, popIndex: 589 }
  kid1 receiving from kid6: { size: 0, capacity: 1024, other: 58192, popIndex: 58192 }

  kid1 sending to kid3: { size: 0, capacity: 1024, pushIndex: 339, other: 339 }
  kid1 sending to kid4: { size: 0, capacity: 1024, pushIndex: 40, other: 40 }
  kid1 sending to kid5: { size: 0, capacity: 1024, pushIndex: 589, other: 589 }
  kid1 sending to kid6: { size: 0, capacity: 1024, pushIndex: 58192, other: 58192 }
} by kid1

by kid2 {
Transients queues:
  kid2 receiving from kid1: { size: 0, capacity: 1024, other: 3301, popIndex: 3301 }
  kid2 receiving from kid2: { size: 0, capacity: 1024, other: 97, popIndex: 97 }

  kid2 sending to kid1: { size: 0, capacity: 1024, pushIndex: 1506, other: 1506 }
  kid2 sending to kid2: { size: 0, capacity: 1024, pushIndex: 97, other: 97 }

SMP disk I/O queues:
  kid2 receiving from kid3: { size: 0, capacity: 1024, other: 604, popIndex: 604 }
  kid2 receiving from kid4: { size: 0, capacity: 1024, other: 12, popIndex: 12 }
  kid2 receiving from kid5: { size: 0, capacity: 1024, other: 144, popIndex: 144 }
  kid2 receiving from kid6: { size: 0, capacity: 1024, other: 25132, popIndex: 25132 }

  kid2 sending to kid3: { size: 0, capacity: 1024, pushIndex: 604, other: 604 }
  kid2 sending to kid4: { size: 0, capacity: 1024, pushIndex: 12, other: 12 }
  kid2 sending to kid5: { size: 0, capacity: 1024, pushIndex: 144, other: 144 }
  kid2 sending to kid6: { size: 0, capacity: 1024, pushIndex: 25132, other: 25132 }
} by kid2

by kid3 {

SMP disk I/O queues:
  kid3 receiving from kid1: { size: 0, capacity: 1024, other: 339, popIndex: 339 }
  kid3 receiving from kid2: { size: 0, capacity: 1024, other: 604, popIndex: 604 }

  kid3 sending to kid1: { size: 0, capacity: 1024, pushIndex: 339, other: 339 }
  kid3 sending to kid2: { size: 0, capacity: 1024, pushIndex: 604, other: 604 }
} by kid3

by kid4 {

SMP disk I/O queues:
  kid4 receiving from kid1: { size: 0, capacity: 1024, other: 40, popIndex: 40 }
  kid4 receiving from kid2: { size: 0, capacity: 1024, other: 12, popIndex: 12 }

  kid4 sending to kid1: { size: 0, capacity: 1024, pushIndex: 40, other: 40 }
  kid4 sending to kid2: { size: 0, capacity: 1024, pushIndex: 12, other: 12 }
} by kid4

by kid5 {

SMP disk I/O queues:
  kid5 receiving from kid1: { size: 0, capacity: 1024, other: 589, popIndex: 589 }
  kid5 receiving from kid2: { size: 0, capacity: 1024, other: 144, popIndex: 144 }

  kid5 sending to kid1: { size: 0, capacity: 1024, pushIndex: 589, other: 589 }
  kid5 sending to kid2: { size: 0, capacity: 1024, pushIndex: 144, other: 144 }
} by kid5

by kid6 {

SMP disk I/O queues:
  kid6 receiving from kid1: { size: 0, capacity: 1024, other: 58192, popIndex: 58192 }
  kid6 receiving from kid2: { size: 0, capacity: 1024, other: 25132, popIndex: 25132 }

  kid6 sending to kid1: { size: 0, capacity: 1024, pushIndex: 58192, other: 58192 }
  kid6 sending to kid2: { size: 0, capacity: 1024, pushIndex: 25132, other: 25132 }
} by kid6

Seems like a new item in the cachemgr.cgi menu...

LL


-----P?vodn? zpr?va-----
Od: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Odesl?no: ?t 16.11.2021 19:09
Komu: Lou?ansk? Luk??; Squid Users
P?edm?t: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
 
On 11/16/21 12:00 PM, Lou?ansk? Luk?? wrote:

> I will try to backport it from that patch into the v5 tree I've
> downloaded today. As we were using the mentioned build I came across
> these new assertions:

> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241: "anchorAt(anchorId).reading()"
> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108: "store_status == STORE_PENDING"

I hope that at least some of the above assertions are fixed by master/v6
commit 5210df4.


> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie == ((long)this ^ Cookie)" 

This is probably an unrelated bug. I recommend filing a bug report in
Squid bugzilla and posting the corresponding "bt full" backtrace there.


Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Tuesday, November 16, 2021 3:42 PM
> To: Lou?ansk? Luk??; Squid Users
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
> 
> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
>> is it going to be patched only in the v6 version? 
> 
> I hope the existing fix applies to v5 cleanly, and I am ready to help with backporting if it does not. Beyond that, it is in the maintainer hands. I cannot predict whether or when the fix will be officially merged into v5 because I do not understand how
 those decisions are made.
> 
> 
>> Anyway - in the morning I run debug with 20,9 to see:
>> ...
>> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221: "old == e"
> 
> Unfortunately, I cannot see the cause of the assertion in this short/partial trace -- the problematic actions happened before the trace or were not logged during the trace.
> 
> Patching your Squid with commit 5210df4 is the best next step IMO. If that patch does not help, then there are probably other bugs that we need to fix in v5 (at least).
> 
> 
> HTH,
> 
> Alex.
> 
> 



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211121/39aa5111/attachment.htm>

From rousskov at measurement-factory.com  Sun Nov 21 19:27:47 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 21 Nov 2021 14:27:47 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
 <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>
Message-ID: <abba1216-7f63-9dea-a569-dac63493165e@measurement-factory.com>

On 11/21/21 2:08 PM, Lou?ansk? Luk?? wrote:
> Hello, I've replaced src/ipc/ReadWritelock.cc and ReadWriteLock.h with
> modified versions (with finalizeExclusive calls), but afeter some time I
> have got queue overflows agains.

After each bug fix, we have to restart the triage sequence from scratch:
Any assertions, crashes, FATAL messages or similar things leading to kid
deaths? If they are present, then they explain queue overflows.


> So I've downloaded squid-6.0.0-20211116-r90086c5a8 - run it with my v5
> configure and now I'm testing it. So far I get these:
> 2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for

Noted. If you cannot reproduce this bug at will, then I recommend
focusing on other, easier-to-address problems first.

Alex.



> 2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr6455=V/0x5642b8e67480*0
> 2021/11/21 19:05:07 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr6455=V/0x5642b7af7b40*0
> 2021/11/21 19:05:07 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr6455=V/0x5642b8993850*0
> 2021/11/21 19:05:08 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr6455=V/0x5642b5269200*0
> 2021/11/21 19:05:08 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr6455=V/0x5642b5269200*0
> 2021/11/21 19:05:18 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr6455=V/0x5642b7c91210*0
> 2021/11/21 19:53:48 kid1| Bug: Missing MemObject::storeId value
> 2021/11/21 20:02:16 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> e:tr11100=V/0x5642b4898050*0
> etc.
> 
> Regard the transient queue - could this be any interesting?
> 
> by kid1 {
> Transients queues:
> ? kid1 receiving from kid1: { size: 0, capacity: 1024, other: 112,
> popIndex: 112 }
> ? kid1 receiving from kid2: { size: 0, capacity: 1024, other: 1506,
> popIndex: 1506 }
> 
> ? kid1 sending to kid1: { size: 0, capacity: 1024, pushIndex: 112,
> other: 112 }
> ? kid1 sending to kid2: { size: 0, capacity: 1024, pushIndex: 3301,
> other: 3301 }
> 
> SMP disk I/O queues:
> ? kid1 receiving from kid3: { size: 0, capacity: 1024, other: 339,
> popIndex: 339 }
> ? kid1 receiving from kid4: { size: 0, capacity: 1024, other: 40,
> popIndex: 40 }
> ? kid1 receiving from kid5: { size: 0, capacity: 1024, other: 589,
> popIndex: 589 }
> ? kid1 receiving from kid6: { size: 0, capacity: 1024, other: 58192,
> popIndex: 58192 }
> 
> ? kid1 sending to kid3: { size: 0, capacity: 1024, pushIndex: 339,
> other: 339 }
> ? kid1 sending to kid4: { size: 0, capacity: 1024, pushIndex: 40, other:
> 40 }
> ? kid1 sending to kid5: { size: 0, capacity: 1024, pushIndex: 589,
> other: 589 }
> ? kid1 sending to kid6: { size: 0, capacity: 1024, pushIndex: 58192,
> other: 58192 }
> } by kid1
> 
> by kid2 {
> Transients queues:
> ? kid2 receiving from kid1: { size: 0, capacity: 1024, other: 3301,
> popIndex: 3301 }
> ? kid2 receiving from kid2: { size: 0, capacity: 1024, other: 97,
> popIndex: 97 }
> 
> ? kid2 sending to kid1: { size: 0, capacity: 1024, pushIndex: 1506,
> other: 1506 }
> ? kid2 sending to kid2: { size: 0, capacity: 1024, pushIndex: 97, other:
> 97 }
> 
> SMP disk I/O queues:
> ? kid2 receiving from kid3: { size: 0, capacity: 1024, other: 604,
> popIndex: 604 }
> ? kid2 receiving from kid4: { size: 0, capacity: 1024, other: 12,
> popIndex: 12 }
> ? kid2 receiving from kid5: { size: 0, capacity: 1024, other: 144,
> popIndex: 144 }
> ? kid2 receiving from kid6: { size: 0, capacity: 1024, other: 25132,
> popIndex: 25132 }
> 
> ? kid2 sending to kid3: { size: 0, capacity: 1024, pushIndex: 604,
> other: 604 }
> ? kid2 sending to kid4: { size: 0, capacity: 1024, pushIndex: 12, other:
> 12 }
> ? kid2 sending to kid5: { size: 0, capacity: 1024, pushIndex: 144,
> other: 144 }
> ? kid2 sending to kid6: { size: 0, capacity: 1024, pushIndex: 25132,
> other: 25132 }
> } by kid2
> 
> by kid3 {
> 
> SMP disk I/O queues:
> ? kid3 receiving from kid1: { size: 0, capacity: 1024, other: 339,
> popIndex: 339 }
> ? kid3 receiving from kid2: { size: 0, capacity: 1024, other: 604,
> popIndex: 604 }
> 
> ? kid3 sending to kid1: { size: 0, capacity: 1024, pushIndex: 339,
> other: 339 }
> ? kid3 sending to kid2: { size: 0, capacity: 1024, pushIndex: 604,
> other: 604 }
> } by kid3
> 
> by kid4 {
> 
> SMP disk I/O queues:
> ? kid4 receiving from kid1: { size: 0, capacity: 1024, other: 40,
> popIndex: 40 }
> ? kid4 receiving from kid2: { size: 0, capacity: 1024, other: 12,
> popIndex: 12 }
> 
> ? kid4 sending to kid1: { size: 0, capacity: 1024, pushIndex: 40, other:
> 40 }
> ? kid4 sending to kid2: { size: 0, capacity: 1024, pushIndex: 12, other:
> 12 }
> } by kid4
> 
> by kid5 {
> 
> SMP disk I/O queues:
> ? kid5 receiving from kid1: { size: 0, capacity: 1024, other: 589,
> popIndex: 589 }
> ? kid5 receiving from kid2: { size: 0, capacity: 1024, other: 144,
> popIndex: 144 }
> 
> ? kid5 sending to kid1: { size: 0, capacity: 1024, pushIndex: 589,
> other: 589 }
> ? kid5 sending to kid2: { size: 0, capacity: 1024, pushIndex: 144,
> other: 144 }
> } by kid5
> 
> by kid6 {
> 
> SMP disk I/O queues:
> ? kid6 receiving from kid1: { size: 0, capacity: 1024, other: 58192,
> popIndex: 58192 }
> ? kid6 receiving from kid2: { size: 0, capacity: 1024, other: 25132,
> popIndex: 25132 }
> 
> ? kid6 sending to kid1: { size: 0, capacity: 1024, pushIndex: 58192,
> other: 58192 }
> ? kid6 sending to kid2: { size: 0, capacity: 1024, pushIndex: 25132,
> other: 25132 }
> } by kid6
> 
> Seems like a new item in the cachemgr.cgi menu...
> 
> LL
> 
> 
> -----P?vodn? zpr?va-----
> Od: Alex Rousskov [mailto:rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>]
> Odesl?no: ?t 16.11.2021 19:09
> Komu: Lou?ansk? Luk??; Squid Users
> P?edm?t: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> overflow for kid2 at 1024 items
> 
> On 11/16/21 12:00 PM, Lou?ansk? Luk?? wrote:
> 
>> I will try to backport it from that patch into the v5 tree I've
>> downloaded today. As we were using the mentioned build I came across
>> these new assertions:
> 
>> 2021/11/16 10:29:46 kid1| assertion failed: StoreMap.cc:241:
> "anchorAt(anchorId).reading()"
>> 2021/11/16 11:32:51 kid2| assertion failed: Transients.cc:221: "old == e"
>> 2021/11/16 14:29:41 kid2| assertion failed: store.cc:1108:
> "store_status == STORE_PENDING"
> 
> I hope that at least some of the above assertions are fixed by master/v6
> commit 5210df4.
> 
> 
>> 2021/11/16 17:40:21 kid1| assertion failed: cbdata.cc:372: "c->locks > 0"
>> 2021/11/16 17:40:44 kid1| assertion failed: cbdata.cc:115: "cookie ==
> ((long)this ^ Cookie)"
> 
> This is probably an unrelated bug. I recommend filing a bug report in
> Squid bugzilla and posting the corresponding "bt full" backtrace there.
> 
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>]
>> Sent: Tuesday, November 16, 2021 3:42 PM
>> To: Lou?ansk? Luk??; Squid Users
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue
> overflow for kid2 at 1024 items
>>
>> On 11/16/21 4:38 AM, Lou?ansk? Luk?? wrote:
>>> is it going to be patched only in the v6 version?
>>
>> I hope the existing fix applies to v5 cleanly, and I am ready to help
> with backporting if it does not. Beyond that, it is in the maintainer
> hands. I cannot predict whether or when the fix will be officially
> merged into v5 because I do not understand how
> ?those decisions are made.
>>
>>
>>> Anyway - in the morning I run debug with 20,9 to see:
>>> ...
>>> 2021/11/16 09:02:06.496 kid2| assertion failed: Transients.cc:221:
> "old == e"
>>
>> Unfortunately, I cannot see the cause of the assertion in this
> short/partial trace -- the problematic actions happened before the trace
> or were not logged during the trace.
>>
>> Patching your Squid with commit 5210df4 is the best next step IMO. If
> that patch does not help, then there are probably other bugs that we
> need to fix in v5 (at least).
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
> 
> 
> 



From Loucansky.Lukas at kjj.cz  Mon Nov 22 08:59:38 2021
From: Loucansky.Lukas at kjj.cz (=?windows-1250?B?TG916GFuc2v9IEx1a+Ga?=)
Date: Mon, 22 Nov 2021 09:59:38 +0100
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <abba1216-7f63-9dea-a569-dac63493165e@measurement-factory.com>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
 <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>
 <abba1216-7f63-9dea-a569-dac63493165e@measurement-factory.com>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B53010897AA@kjj-server.KJJ.local>

I'm running Squid Object Cache: Version 6.0.0-20211116-r90086c5a8 for about 14hrs now. I've noticed many new  "ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items" after about 11hrs runtime.
And I have
Transients queues:
  kid1 receiving from kid1: { size: 268, capacity: 1024, other: 1198, popIndex: 930, items: [
    { sender: 1, xitIndex: 5801 },
    { sender: 1, xitIndex: 1322 },
    { sender: 1, xitIndex: 6790 },
    # ... 262 items not shown ...
    { sender: 1, xitIndex: 1928 },
    { sender: 1, xitIndex: 7030 },
    { sender: 1, xitIndex: 6297 },
  ]}
  kid1 receiving from kid2: { size: 1024, capacity: 1024, other: 60523, popIndex: 59499, items: [
    { sender: 2, xitIndex: 4481 },
    { sender: 2, xitIndex: 4481 },
    { sender: 2, xitIndex: 3277 },
    # ... 1018 items not shown ...
    { sender: 2, xitIndex: 12520 },
    { sender: 2, xitIndex: 12520 },
    { sender: 2, xitIndex: 12520 },
  ]}

  kid1 sending to kid1: { size: 268, capacity: 1024, pushIndex: 1198, other: 930, items: [
    { sender: 1, xitIndex: 5801 },
    { sender: 1, xitIndex: 1322 },
    { sender: 1, xitIndex: 6790 },
    # ... 262 items not shown ...
    { sender: 1, xitIndex: 1928 },
    { sender: 1, xitIndex: 7030 },
    { sender: 1, xitIndex: 6297 },
  ]}
  kid1 sending to kid2: { size: 1024, capacity: 1024, pushIndex: 39566, other: 38542, items: [
    { sender: 1, xitIndex: 13750 },
    { sender: 1, xitIndex: 608 },
    { sender: 1, xitIndex: 608 },
    # ... 1018 items not shown ...
    { sender: 1, xitIndex: 14740 },
    { sender: 1, xitIndex: 14740 },
    { sender: 1, xitIndex: 14740 },
  ]}

Similar for kid2.

cat cache.log | grep assert
2021/11/22 07:54:24 kid2| assertion failed: store.cc:1094: "store_status == STORE_PENDING"
2021/11/22 08:51:48 kid1| assertion failed: store.cc:1094: "store_status == STORE_PENDING"

(maybe consequence of too generous max-swap-rate)
LL

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Sunday, November 21, 2021 8:28 PM
To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items

On 11/21/21 2:08 PM, Lou?ansk? Luk?? wrote:
> Hello, I've replaced src/ipc/ReadWritelock.cc and ReadWriteLock.h with 
> modified versions (with finalizeExclusive calls), but afeter some time 
> I have got queue overflows agains.

After each bug fix, we have to restart the triage sequence from scratch:
Any assertions, crashes, FATAL messages or similar things leading to kid deaths? If they are present, then they explain queue overflows.


> So I've downloaded squid-6.0.0-20211116-r90086c5a8 - run it with my v5 
> configure and now I'm testing it. So far I get these:
> 2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for

Noted. If you cannot reproduce this bug at will, then I recommend focusing on other, easier-to-address problems first.

Alex.



From rousskov at measurement-factory.com  Mon Nov 22 16:35:17 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 22 Nov 2021 11:35:17 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53010897AA@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
 <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>
 <abba1216-7f63-9dea-a569-dac63493165e@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897AA@kjj-server.KJJ.local>
Message-ID: <26a9b746-58bd-e6e0-f77c-cf42e1a72fbf@measurement-factory.com>

On 11/22/21 3:59 AM, Lou?ansk? Luk?? wrote:

> I'm running Squid Object Cache: Version 6.0.0-20211116-r90086c5a8 for
> about 14hrs now. I've noticed many new  "ERROR: Collapsed forwarding
> queue overflow for kid2 at 1024 items" after about 11hrs runtime.

Ignore queue overflows (for now) because they are a known side effect of
assertions.


> 2021/11/22 07:54:24 kid2| assertion failed: store.cc:1094: "store_status == STORE_PENDING"
> 2021/11/22 08:51:48 kid1| assertion failed: store.cc:1094: "store_status == STORE_PENDING"

I recommend filing a bug report with Squid Bugzilla and providing a
stack trace ("bt full") from this assertion. If possible, please keep
the core dump for further analysis.


Thank you,

Alex.



> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Sunday, November 21, 2021 8:28 PM
> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
> 
> On 11/21/21 2:08 PM, Lou?ansk? Luk?? wrote:
>> Hello, I've replaced src/ipc/ReadWritelock.cc and ReadWriteLock.h with 
>> modified versions (with finalizeExclusive calls), but afeter some time 
>> I have got queue overflows agains.
> 
> After each bug fix, we have to restart the triage sequence from scratch:
> Any assertions, crashes, FATAL messages or similar things leading to kid deaths? If they are present, then they explain queue overflows.
> 
> 
>> So I've downloaded squid-6.0.0-20211116-r90086c5a8 - run it with my v5 
>> configure and now I'm testing it. So far I get these:
>> 2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> 
> Noted. If you cannot reproduce this bug at will, then I recommend focusing on other, easier-to-address problems first.
> 
> Alex.
> 



From david at articatech.com  Mon Nov 22 16:55:59 2021
From: david at articatech.com (David Touzeau)
Date: Mon, 22 Nov 2021 17:55:59 +0100
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
Message-ID: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>

Hi, community

What does mean this error :

2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930: 
"!transients || e.hasTransients()"
 ??? current master transaction: master69


We are unable to start the service it always crashes.
How can we can fix it ( purge cache , reboot )... ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211122/373be0c0/attachment.htm>

From rousskov at measurement-factory.com  Mon Nov 22 17:18:52 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 22 Nov 2021 12:18:52 -0500
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
In-Reply-To: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
References: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
Message-ID: <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>

On 11/22/21 11:55 AM, David Touzeau wrote:

> What does mean this error :
> 
> 2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930:
> "!transients || e.hasTransients()"

> We are unable to start the service it always crashes.
> How can we can fix it ( purge cache , reboot )... ?

This is a Squid bug or misconfiguration. If you are using a UFS-based
cache_dir with multiple workers, then it is a misconfiguration. If you
want to use SMP disk caching, please use rock store instead.

HTH,

Alex.
P.S. This assertion has been reported several times, including for Squid
v4, but it was probably always due to a Squid misconfiguration. We need
to find a good way to explicitly reject such configurations instead of
asserting (while not rejecting similar unsupported configurations that
still "work" from their admins point of view).


From david at articatech.com  Mon Nov 22 17:48:24 2021
From: david at articatech.com (David Touzeau)
Date: Mon, 22 Nov 2021 18:48:24 +0100
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
In-Reply-To: <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>
References: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
 <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>
Message-ID: <c709d5c6-8fa8-91b7-f605-053a7d474f14@articatech.com>

Here our SMP configuration:

workers 2

cache_dir rock /home/squid/cache/rock 0 min-size=0 max-size=131072 slot-size=32000

if ${process_number} = 1
memory_cache_mode always
cpu_affinity_map process_numbers=${process_number} cores=1
cache_dir??? aufs??? /home/squid/Caches/disk??? 50024??? 16??? 256 min-size=131072 max-size=3221225472
endif

if ${process_number} = 2
memory_cache_mode always
cpu_affinity_map process_numbers=${process_number} cores=2
endif


where is the false settings ?
Missing cache_dir ?


Le 22/11/2021 ? 18:18, Alex Rousskov a ?crit?:
> On 11/22/21 11:55 AM, David Touzeau wrote:
>
>> What does mean this error :
>>
>> 2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930:
>> "!transients || e.hasTransients()"
>> We are unable to start the service it always crashes.
>> How can we can fix it ( purge cache , reboot )... ?
> This is a Squid bug or misconfiguration. If you are using a UFS-based
> cache_dir with multiple workers, then it is a misconfiguration. If you
> want to use SMP disk caching, please use rock store instead.
>
> HTH,
>
> Alex.
> P.S. This assertion has been reported several times, including for Squid
> v4, but it was probably always due to a Squid misconfiguration. We need
> to find a good way to explicitly reject such configurations instead of
> asserting (while not rejecting similar unsupported configurations that
> still "work" from their admins point of view).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211122/f54e1c94/attachment.htm>

From rousskov at measurement-factory.com  Mon Nov 22 18:33:59 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 22 Nov 2021 13:33:59 -0500
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
In-Reply-To: <c709d5c6-8fa8-91b7-f605-053a7d474f14@articatech.com>
References: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
 <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>
 <c709d5c6-8fa8-91b7-f605-053a7d474f14@articatech.com>
Message-ID: <37a8b3e3-2bfb-868e-4e37-b5583968c255@measurement-factory.com>

On 11/22/21 12:48 PM, David Touzeau wrote:
> Here our SMP configuration:
> 
> workers 2
> 
> cache_dir rock /home/squid/cache/rock 0 min-size=0 max-size=131072 slot-size=32000
> 
> if ${process_number} = 1
> memory_cache_mode always
> cpu_affinity_map process_numbers=${process_number} cores=1
> cache_dir??? aufs??? /home/squid/Caches/disk??? 50024??? 16??? 256 min-size=131072 max-size=3221225472
> endif
> 
> if ${process_number} = 2
> memory_cache_mode always
> cpu_affinity_map process_numbers=${process_number} cores=2
> endif
> 
> 
> where is the false settings ?

I am limiting my answer to the problems in this email thread scope: aufs
cache_dirs are UFS-based cache_dirs. UFS-based cache_dirs are not
SMP-aware and are not supported in SMP configurations. Your choices include:

* drop SMP (i.e. remove "workers" and ARA)
* drop aufs (i.e. remove "cache_dir aufs" and ARA)

... where ARA is "adjust the rest of the configuration accordingly".


HTH,

Alex.


> Le 22/11/2021 ? 18:18, Alex Rousskov a ?crit?:
>> On 11/22/21 11:55 AM, David Touzeau wrote:
>>
>>> What does mean this error :
>>>
>>> 2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930:
>>> "!transients || e.hasTransients()"
>>> We are unable to start the service it always crashes.
>>> How can we can fix it ( purge cache , reboot )... ?
>> This is a Squid bug or misconfiguration. If you are using a UFS-based
>> cache_dir with multiple workers, then it is a misconfiguration. If you
>> want to use SMP disk caching, please use rock store instead.
>>
>> HTH,
>>
>> Alex.
>> P.S. This assertion has been reported several times, including for Squid
>> v4, but it was probably always due to a Squid misconfiguration. We need
>> to find a good way to explicitly reject such configurations instead of
>> asserting (while not rejecting similar unsupported configurations that
>> still "work" from their admins point of view).
> 



From sandorsz at birosag.hu  Tue Nov 23 08:02:23 2021
From: sandorsz at birosag.hu (=?iso-8859-2?Q?S=E1ndor_Szabolcs_=5BBudapest_K=F6rny=E9ki_T=F6rv=E9nysz?= =?iso-8859-2?B?6Wtd?=)
Date: Tue, 23 Nov 2021 08:02:23 +0000
Subject: [squid-users] ipcacheParse No Address records in response to 'DNS
 address'
Message-ID: <a6c19a3b8ced4a77938979a8630ed165@birosag.hu>

Hi!

I need some help, because I have got stucked. I recieved ther next two errors in cache log:

ipcacheParse No Address records in response to 'DNS address'
WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.

I have already search for this error, but I haven't found any solution.

Here is a part of my cache.log:
2021/11/23 08:54:03 kid4| ipcacheParse No Address records in response to 'dt.adsafeprotected.com'
2021/11/23 08:54:03 kid4| ipcacheParse No Address records in response to 'dt.adsafeprotected.com'
2021/11/23 08:54:03 kid4| ipcacheParse No Address records in response to 'dt.adsafeprotected.com'
2021/11/23 08:54:03 kid4| ipcacheParse No Address records in response to 'dt.adsafeprotected.com'
2021/11/23 08:54:03 kid4| ipcacheParse No Address records in response to 'dt.adsafeprotected.com'
2021/11/23 08:54:03 kid4| ipcacheParse No Address records in response to 'dt.adsafeprotected.com'
2021/11/23 08:54:08 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:08 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:08 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:08 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:16 kid4| ipcacheParse No Address records in response to 'eus.rubiconproject.com'
2021/11/23 08:54:16 kid4| ipcacheParse No Address records in response to 'eus.rubiconproject.com'
2021/11/23 08:54:24 kid3| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:24 kid3| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:41 kid3| ipcacheParse No Address records in response to 's.update.rubiconproject.com'
2021/11/23 08:54:41 kid4| ipcacheParse No Address records in response to 'contents.mediadecathlon.com'
2021/11/23 08:54:41 kid4| ipcacheParse No Address records in response to 'contents.mediadecathlon.com'
2021/11/23 08:54:48 kid4| ipcacheParse No Address records in response to 'pre-usermatch.targeting.unrulymedia.com'
2021/11/23 08:54:49 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: local-servers ACL is used in context without an HTTP request. Assuming mismatch.
2021/11/23 08:54:49 kid4| WARNING: no-access-log ACL is used in context without an HTTP request. Assuming mismatch.

Thank You four help.

Szabolcs Sandor

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211123/1b010d81/attachment.htm>

From Loucansky.Lukas at kjj.cz  Tue Nov 23 08:32:35 2021
From: Loucansky.Lukas at kjj.cz (=?windows-1250?B?TG916GFuc2v9IEx1a+Ga?=)
Date: Tue, 23 Nov 2021 09:32:35 +0100
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <26a9b746-58bd-e6e0-f77c-cf42e1a72fbf@measurement-factory.com>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
 <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>
 <abba1216-7f63-9dea-a569-dac63493165e@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897AA@kjj-server.KJJ.local>
 <26a9b746-58bd-e6e0-f77c-cf42e1a72fbf@measurement-factory.com>
Message-ID: <72DD5D5CF661B5459DC08A060BF26B53010897AC@kjj-server.KJJ.local>

I agree - it seems I'm catching red herrings here. Restarted squid yesterday at about 11:30. Everything seemed ok until 8am today - when this assert was again raised: (note - it's another from these in version 5)

2021/11/23 08:00:15 kid2| assertion failed: store.cc:1094: "store_status == STORE_PENDING"

then kid2 restarted itself and after another 9mins the "collapsed forwarding queue overflow" message began filling my log file.

Besides this there was one "unable to allocate 64kB mem" error

Nov 23 06:27:01 proxy-gw (squid-1): FATAL: xmalloc: Unable to allocate 65535 bytes!#012#012    current master transaction: master54
Nov 23 06:27:01 proxy-gw squid[20213]: Squid Parent: squid-1 process 20221 exited due to signal 6 with status 0
Nov 23 06:27:01 proxy-gw squid[20213]: Squid Parent: (squid-1) process 24763 started

But it does not trigger the 1024 items overflow error.

Nov 23 08:00:16 proxy-gw squid[20213]: Squid Parent: squid-2 process 20220 exited due to signal 6 with status 0
Nov 23 08:00:16 proxy-gw squid[20213]: Squid Parent: (squid-2) process 25372 started

Is it possible that after kid worker restarts it stops synchronizing or sending messages via this queues? ie. like the new one is unable to process or read the messages intended for the old one? In on of the old mails I've posted  list from the cachemgr showing transient queues being filled - which I don't see if the queue overflow messageis not logged. But it showed more items even for kid1 receiving from kid1 and kid1 sending to kid1 - so why would it stop sending it to itself? As I'm a laymen in this field (can not see the whole picture or how it works) I'm not currently able to tell how to induce such behaviour or speculate why it's filling. So far I've recompiled squid with disabled optimizations and enabled backtraces - let's see what it does while it crashes again....

LL 

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Monday, November 22, 2021 5:35 PM
To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items

On 11/22/21 3:59 AM, Lou?ansk? Luk?? wrote:

> I'm running Squid Object Cache: Version 6.0.0-20211116-r90086c5a8 for 
> about 14hrs now. I've noticed many new  "ERROR: Collapsed forwarding 
> queue overflow for kid2 at 1024 items" after about 11hrs runtime.

Ignore queue overflows (for now) because they are a known side effect of assertions.


> 2021/11/22 07:54:24 kid2| assertion failed: store.cc:1094: "store_status == STORE_PENDING"
> 2021/11/22 08:51:48 kid1| assertion failed: store.cc:1094: "store_status == STORE_PENDING"

I recommend filing a bug report with Squid Bugzilla and providing a stack trace ("bt full") from this assertion. If possible, please keep the core dump for further analysis.


Thank you,

Alex.



> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Sunday, November 21, 2021 8:28 PM
> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
> overflow for kid2 at 1024 items
> 
> On 11/21/21 2:08 PM, Lou?ansk? Luk?? wrote:
>> Hello, I've replaced src/ipc/ReadWritelock.cc and ReadWriteLock.h 
>> with modified versions (with finalizeExclusive calls), but afeter 
>> some time I have got queue overflows agains.
> 
> After each bug fix, we have to restart the triage sequence from scratch:
> Any assertions, crashes, FATAL messages or similar things leading to kid deaths? If they are present, then they explain queue overflows.
> 
> 
>> So I've downloaded squid-6.0.0-20211116-r90086c5a8 - run it with my 
>> v5 configure and now I'm testing it. So far I get these:
>> 2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
> 
> Noted. If you cannot reproduce this bug at will, then I recommend focusing on other, easier-to-address problems first.
> 
> Alex.
> 



From squid3 at treenet.co.nz  Tue Nov 23 08:28:44 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Nov 2021 21:28:44 +1300
Subject: [squid-users] ipcacheParse No Address records in response to
 'DNS address'
In-Reply-To: <a6c19a3b8ced4a77938979a8630ed165@birosag.hu>
References: <a6c19a3b8ced4a77938979a8630ed165@birosag.hu>
Message-ID: <504f6fc1-980d-bb82-c5a1-5abbcff56b75@treenet.co.nz>

On 23/11/21 21:02, S?ndor Szabolcs [Budapest K?rny?ki T?rv?nysz?k] wrote:
> Hi!
> 
> I need some help, because I have got stucked. I recieved ther next two 
> errors in cache log:
> 
> ipcacheParse No Address records in response to 'DNS address'
> 

The domain(s) mentioned has misconfigured CNAME response to IPv6 lookups.

What version of Squid are you using? that log message has not existed 
for some time.


> WARNING: local-servers ACL is used in context without an HTTP request. 
> Assuming mismatch.
> 

How are you using the named ACL(s) in squid.conf ?


Amos


From david at articatech.com  Tue Nov 23 10:01:50 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 23 Nov 2021 11:01:50 +0100
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
In-Reply-To: <37a8b3e3-2bfb-868e-4e37-b5583968c255@measurement-factory.com>
References: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
 <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>
 <c709d5c6-8fa8-91b7-f605-053a7d474f14@articatech.com>
 <37a8b3e3-2bfb-868e-4e37-b5583968c255@measurement-factory.com>
Message-ID: <ea43083d-5450-b2d4-9f98-8549ddc410e7@articatech.com>

Ok thanks, we will investigate in this way

Le 22/11/2021 ? 19:33, Alex Rousskov a ?crit?:
> On 11/22/21 12:48 PM, David Touzeau wrote:
>> Here our SMP configuration:
>>
>> workers 2
>>
>> cache_dir rock /home/squid/cache/rock 0 min-size=0 max-size=131072 slot-size=32000
>>
>> if ${process_number} = 1
>> memory_cache_mode always
>> cpu_affinity_map process_numbers=${process_number} cores=1
>> cache_dir??? aufs??? /home/squid/Caches/disk??? 50024??? 16??? 256 min-size=131072 max-size=3221225472
>> endif
>>
>> if ${process_number} = 2
>> memory_cache_mode always
>> cpu_affinity_map process_numbers=${process_number} cores=2
>> endif
>>
>>
>> where is the false settings ?
> I am limiting my answer to the problems in this email thread scope: aufs
> cache_dirs are UFS-based cache_dirs. UFS-based cache_dirs are not
> SMP-aware and are not supported in SMP configurations. Your choices include:
>
> * drop SMP (i.e. remove "workers" and ARA)
> * drop aufs (i.e. remove "cache_dir aufs" and ARA)
>
> ... where ARA is "adjust the rest of the configuration accordingly".
>
>
> HTH,
>
> Alex.
>
>
>> Le 22/11/2021 ? 18:18, Alex Rousskov a ?crit?:
>>> On 11/22/21 11:55 AM, David Touzeau wrote:
>>>
>>>> What does mean this error :
>>>>
>>>> 2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930:
>>>> "!transients || e.hasTransients()"
>>>> We are unable to start the service it always crashes.
>>>> How can we can fix it ( purge cache , reboot )... ?
>>> This is a Squid bug or misconfiguration. If you are using a UFS-based
>>> cache_dir with multiple workers, then it is a misconfiguration. If you
>>> want to use SMP disk caching, please use rock store instead.
>>>
>>> HTH,
>>>
>>> Alex.
>>> P.S. This assertion has been reported several times, including for Squid
>>> v4, but it was probably always due to a Squid misconfiguration. We need
>>> to find a good way to explicitly reject such configurations instead of
>>> asserting (while not rejecting similar unsupported configurations that
>>> still "work" from their admins point of view).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211123/f241a2f7/attachment.htm>

From david at articatech.com  Tue Nov 23 10:09:30 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 23 Nov 2021 11:09:30 +0100
Subject: [squid-users] tlu.dl.delivery.mp.microsoft.com and HTTP range header
Message-ID: <b2b37910-6ee7-aa2d-7549-0f73ed215a7c@articatech.com>

Hi community,

tlu.dl.delivery.mp.microsoft.com is from the app store and it encounters 
an issue with high bandwidth usage.
We think that it was caused because Squid filtering the HTTP Range 
header from the HTTP requests.
This caused the app store download everything in an endless loop

We know that Squid is not currently compatible with http range :
https://wiki.squid-cache.org/Features/HTTP11#Range_Requests

Is there any workaround in order to avoid high bandwidth usage of 
Microsoft clients without needing caching objects ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211123/ef1f5d7b/attachment.htm>

From Loucansky.Lukas at kjj.cz  Tue Nov 23 10:10:24 2021
From: Loucansky.Lukas at kjj.cz (=?windows-1250?B?TG916GFuc2v9IEx1a+Ga?=)
Date: Tue, 23 Nov 2021 11:10:24 +0100
Subject: [squid-users] Is this related? store.cc:1094: "store_status ==
 STORE_PENDING" + TCP_MISS_ABORTED
Message-ID: <72DD5D5CF661B5459DC08A060BF26B53010897AE@kjj-server.KJJ.local>

Hello,
as I was going through some upgrades during my investigation of the "transient" queues overflows - I've noticed that my squid asserted on store.cc line 1094 which I read as a test to see if the store is in progress while the transfer is being aborted. Sure enough I've found aborted requests for windows updates. I have dstdomain ACL for WU sites and set quick_abort (min 4MB, max 12MB, pct 100) rules and range_offset_limit (for this ACL to 16MB). I've seen squid downloading ot its own so I eyeballed these values on my sightsigns. I have 1Gbps download so I don't mind downloading 16MB multiple times.But it seems my rock storage is not fast enough as I see I/O timeouts and warnings (delays I/O requests for 72.84 seconds to obey 200/sec rate limit
 etc.).  Is it possible that the store is too slow and is still pending operation while there is request abort? This rock storage is 64GB in size, slot size 32kB, max-swap-rate 200, swap-timeout 250.
fio with randomread shows 587 IOPS (bs 32k, iodepth 1), with rwmixread 75 read IOPS 490 write IOPS 167.
Is this related? Can I do anyzhing to avoid such asserts?
Thanks!
LL


From david at articatech.com  Tue Nov 23 11:39:15 2021
From: david at articatech.com (David Touzeau)
Date: Tue, 23 Nov 2021 12:39:15 +0100
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
In-Reply-To: <ea43083d-5450-b2d4-9f98-8549ddc410e7@articatech.com>
References: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
 <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>
 <c709d5c6-8fa8-91b7-f605-053a7d474f14@articatech.com>
 <37a8b3e3-2bfb-868e-4e37-b5583968c255@measurement-factory.com>
 <ea43083d-5450-b2d4-9f98-8549ddc410e7@articatech.com>
Message-ID: <bd7e6b13-bf93-7740-f07f-da8bc68b346c@articatech.com>

Hi

According to your documentation,
cache dir rock : objects larger than 32,000 bytes cannot be cached
if aufs cannot be implemented in SMP configuration how can we handle 
larger files in cache ?

Le 23/11/2021 ? 11:01, David Touzeau a ?crit?:
> Ok thanks, we will investigate in this way
>
> Le 22/11/2021 ? 19:33, Alex Rousskov a ?crit?:
>> On 11/22/21 12:48 PM, David Touzeau wrote:
>>> Here our SMP configuration:
>>>
>>> workers 2
>>>
>>> cache_dir rock /home/squid/cache/rock 0 min-size=0 max-size=131072 slot-size=32000
>>>
>>> if ${process_number} = 1
>>> memory_cache_mode always
>>> cpu_affinity_map process_numbers=${process_number} cores=1
>>> cache_dir??? aufs??? /home/squid/Caches/disk??? 50024??? 16??? 256 min-size=131072 max-size=3221225472
>>> endif
>>>
>>> if ${process_number} = 2
>>> memory_cache_mode always
>>> cpu_affinity_map process_numbers=${process_number} cores=2
>>> endif
>>>
>>>
>>> where is the false settings ?
>> I am limiting my answer to the problems in this email thread scope: aufs
>> cache_dirs are UFS-based cache_dirs. UFS-based cache_dirs are not
>> SMP-aware and are not supported in SMP configurations. Your choices include:
>>
>> * drop SMP (i.e. remove "workers" and ARA)
>> * drop aufs (i.e. remove "cache_dir aufs" and ARA)
>>
>> ... where ARA is "adjust the rest of the configuration accordingly".
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> Le 22/11/2021 ? 18:18, Alex Rousskov a ?crit?:
>>>> On 11/22/21 11:55 AM, David Touzeau wrote:
>>>>
>>>>> What does mean this error :
>>>>>
>>>>> 2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930:
>>>>> "!transients || e.hasTransients()"
>>>>> We are unable to start the service it always crashes.
>>>>> How can we can fix it ( purge cache , reboot )... ?
>>>> This is a Squid bug or misconfiguration. If you are using a UFS-based
>>>> cache_dir with multiple workers, then it is a misconfiguration. If you
>>>> want to use SMP disk caching, please use rock store instead.
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>> P.S. This assertion has been reported several times, including for Squid
>>>> v4, but it was probably always due to a Squid misconfiguration. We need
>>>> to find a good way to explicitly reject such configurations instead of
>>>> asserting (while not rejecting similar unsupported configurations that
>>>> still "work" from their admins point of view).
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211123/b4c27979/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov 23 14:28:14 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Nov 2021 09:28:14 -0500
Subject: [squid-users] Squid 5.2: assertion failed: Controller.cc:930:
 "!transients || e.hasTransients()"
In-Reply-To: <bd7e6b13-bf93-7740-f07f-da8bc68b346c@articatech.com>
References: <b411b7aa-2afd-54c7-3dab-bfec4ae4c78f@articatech.com>
 <f6556e34-7dbd-8fa9-19ed-e9878eae6042@measurement-factory.com>
 <c709d5c6-8fa8-91b7-f605-053a7d474f14@articatech.com>
 <37a8b3e3-2bfb-868e-4e37-b5583968c255@measurement-factory.com>
 <ea43083d-5450-b2d4-9f98-8549ddc410e7@articatech.com>
 <bd7e6b13-bf93-7740-f07f-da8bc68b346c@articatech.com>
Message-ID: <4326149a-a1d4-3647-002a-e67ac237f56f@measurement-factory.com>

On 11/23/21 6:39 AM, David Touzeau wrote:

> cache dir rock : objects larger than 32,000 bytes cannot be cached

Modern rock versions do not have that limitation. Relying on wiki pages
for documentation carries elevated risks, especially when reading old
pages, but introduction of large object support was documented at
https://wiki.squid-cache.org/Features/LargeRockStore

A better starting point may be cache_dir documentation in your
squid.conf.documented generated for your Squid version.


HTH,

Alex.



> Le 23/11/2021 ? 11:01, David Touzeau a ?crit?:
>> Ok thanks, we will investigate in this way
>>
>> Le 22/11/2021 ? 19:33, Alex Rousskov a ?crit?:
>>> On 11/22/21 12:48 PM, David Touzeau wrote:
>>>> Here our SMP configuration:
>>>>
>>>> workers 2
>>>>
>>>> cache_dir rock /home/squid/cache/rock 0 min-size=0 max-size=131072 slot-size=32000
>>>>
>>>> if ${process_number} = 1
>>>> memory_cache_mode always
>>>> cpu_affinity_map process_numbers=${process_number} cores=1
>>>> cache_dir??? aufs??? /home/squid/Caches/disk??? 50024??? 16??? 256 min-size=131072 max-size=3221225472
>>>> endif
>>>>
>>>> if ${process_number} = 2
>>>> memory_cache_mode always
>>>> cpu_affinity_map process_numbers=${process_number} cores=2
>>>> endif
>>>>
>>>>
>>>> where is the false settings ?
>>> I am limiting my answer to the problems in this email thread scope: aufs
>>> cache_dirs are UFS-based cache_dirs. UFS-based cache_dirs are not
>>> SMP-aware and are not supported in SMP configurations. Your choices include:
>>>
>>> * drop SMP (i.e. remove "workers" and ARA)
>>> * drop aufs (i.e. remove "cache_dir aufs" and ARA)
>>>
>>> ... where ARA is "adjust the rest of the configuration accordingly".
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>> Le 22/11/2021 ? 18:18, Alex Rousskov a ?crit?:
>>>>> On 11/22/21 11:55 AM, David Touzeau wrote:
>>>>>
>>>>>> What does mean this error :
>>>>>>
>>>>>> 2021/11/21 17:23:06 kid1| assertion failed: Controller.cc:930:
>>>>>> "!transients || e.hasTransients()"
>>>>>> We are unable to start the service it always crashes.
>>>>>> How can we can fix it ( purge cache , reboot )... ?
>>>>> This is a Squid bug or misconfiguration. If you are using a UFS-based
>>>>> cache_dir with multiple workers, then it is a misconfiguration. If you
>>>>> want to use SMP disk caching, please use rock store instead.
>>>>>
>>>>> HTH,
>>>>>
>>>>> Alex.
>>>>> P.S. This assertion has been reported several times, including for Squid
>>>>> v4, but it was probably always due to a Squid misconfiguration. We need
>>>>> to find a good way to explicitly reject such configurations instead of
>>>>> asserting (while not rejecting similar unsupported configurations that
>>>>> still "work" from their admins point of view).
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Nov 23 14:58:49 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Nov 2021 09:58:49 -0500
Subject: [squid-users] Is this related? store.cc:1094: "store_status ==
 STORE_PENDING" + TCP_MISS_ABORTED
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53010897AE@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B53010897AE@kjj-server.KJJ.local>
Message-ID: <16fec301-cf16-633f-88b3-91ffb6ddaec7@measurement-factory.com>

On 11/23/21 5:10 AM, Lou?ansk? Luk?? wrote:

> I've noticed that my squid asserted on store.cc line 1094 which I
> read as a test to see if the store is in progress while the transfer
> is being aborted.

I would not describe STORE_PENDING status as "the store is in progress"
because that status covers other StoreEntry states as well. I cannot be
sure without research, but I suspect that the code author was trying to
assert() that the StoreEntry has not been fully written/stored at the
time of the abort() method call. I do not like that assertion, but I
hesitate suggesting specific fixes without understanding the cause of
the problem in your use case.


> Is it possible that the store is too slow 

Yes, of course: Any storage can be too slow, for some definition of "too
slow".


> and is still pending operation while there is request abort?

There is a significant difference between StoreEntry::abort() and a
premature closure of a client-to-Squid connection (i.e. what you
probably mean by "request abort"). That difference aside and roughly
speaking, yes, a client or server may abort the transaction at any time.
And there might be other reasons beyond Squid control for transaction
and StoreEntry aborts.


> This rock storage is 64GB in size, slot size 32kB, max-swap-rate 200,
> swap-timeout 250. fio with randomread shows 587 IOPS (bs 32k, iodepth
> 1), with rwmixread 75 read IOPS 490 write IOPS 167.

If the measured limit is 167 raw writes/s, then I would be concerned
that you are overloading your disks with 200 IOs/s max-swap-rate,
especially if your disk byte hit ratio is low (it usually is). You may
be able to tell whether the disks are overloaded by monitoring their
utilization percentage in "iostats -dx" or similar, especially if you
are using hard drives rather than SSDs.


> Is this related? Can I do anyzhing to avoid such asserts?

I can only repeat my earlier suggestion: File a bug report with "bt
full" backtrace from the assertion and keep the core file around for
analysis. If you can reproduce the assertion at will (e.g., by aborting
long downloads), then please post the reproducing instructions as well.
We can often fix the bugs we can reproduce much faster than other bugs.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Nov 23 15:17:31 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Nov 2021 10:17:31 -0500
Subject: [squid-users] tlu.dl.delivery.mp.microsoft.com and HTTP range
 header
In-Reply-To: <b2b37910-6ee7-aa2d-7549-0f73ed215a7c@articatech.com>
References: <b2b37910-6ee7-aa2d-7549-0f73ed215a7c@articatech.com>
Message-ID: <b6dd05a0-de9a-1419-5c09-f4952c91ec70@measurement-factory.com>

On 11/23/21 5:09 AM, David Touzeau wrote:

> tlu.dl.delivery.mp.microsoft.com is from the app store and it encounters
> an issue with high bandwidth usage.
> We think that it was caused because Squid filtering the HTTP Range
> header from the HTTP requests.

Does Squid actually filter the HTTP Range header when forwarding HTTP
requests sent by that app?

If the resource is served over a TLS connection, does your Squid bump
the corresponding transaction using the SslBump feature?

Do you disable all caching? Caching of these specific responses?


> This caused the app store download everything in an endless loop

Does Squid return a partial response to the application? It should do
that in most cases, even if it filters the HTTP Range header out when
forwarding the request to the origin server, but there are exceptions to
that general principle.


> We know that Squid is not currently compatible with http range :
> https://wiki.squid-cache.org/Features/HTTP11#Range_Requests

That wiki page does not say that Squid is incompatible with HTTP Range
feature. It says that Squid does not support caching of partial responses.


> Is there any workaround in order to avoid high bandwidth usage of
> Microsoft clients without needing caching objects ?

More information may help troubleshoot this problem. Please use the
questions above as starting points.


Thank you,

Alex.


From rousskov at measurement-factory.com  Tue Nov 23 15:34:28 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Nov 2021 10:34:28 -0500
Subject: [squid-users] Too many ERROR: Collapsed forwarding queue
 overflow for kid2 at 1024 items
In-Reply-To: <72DD5D5CF661B5459DC08A060BF26B53010897AC@kjj-server.KJJ.local>
References: <72DD5D5CF661B5459DC08A060BF26B530108979E@kjj-server.KJJ.local>
 <db48287e-021f-744b-6624-7c9c20624f48@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A5@kjj-server.KJJ.local>
 <3f74400e-666b-a3a1-8ab2-5b8075e52bdf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A6@kjj-server.KJJ.local>
 <a753bbcb-e6dd-e447-bb6e-cb8c534e5607@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897A8@kjj-server.KJJ.local>
 <369d5766-b6ab-f4cd-8cef-64b298d84638@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53C45BE4@kjj-server.KJJ.local>
 <abba1216-7f63-9dea-a569-dac63493165e@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897AA@kjj-server.KJJ.local>
 <26a9b746-58bd-e6e0-f77c-cf42e1a72fbf@measurement-factory.com>
 <72DD5D5CF661B5459DC08A060BF26B53010897AC@kjj-server.KJJ.local>
Message-ID: <09f65187-7d37-d1f1-67ed-f406de7421db@measurement-factory.com>

On 11/23/21 3:32 AM, Lou?ansk? Luk?? wrote:

> Is it possible that after kid worker restarts it stops synchronizing
> or sending messages via this queues?

Queue overflows after a kid restart is a known problem that I mentioned
before. That is why I keep suggesting to ignore queue overflows in your
environment until kid restarts are gone. There is no good reason to dig
any deeper here (unless you are a Squid developer working to improve SMP
queuing code). Focus on the assertions/crashes/restarts instead.

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Monday, November 22, 2021 5:35 PM
> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue overflow for kid2 at 1024 items
> 
> On 11/22/21 3:59 AM, Lou?ansk? Luk?? wrote:
> 
>> I'm running Squid Object Cache: Version 6.0.0-20211116-r90086c5a8 for 
>> about 14hrs now. I've noticed many new  "ERROR: Collapsed forwarding 
>> queue overflow for kid2 at 1024 items" after about 11hrs runtime.
> 
> Ignore queue overflows (for now) because they are a known side effect of assertions.
> 
> 
>> 2021/11/22 07:54:24 kid2| assertion failed: store.cc:1094: "store_status == STORE_PENDING"
>> 2021/11/22 08:51:48 kid1| assertion failed: store.cc:1094: "store_status == STORE_PENDING"
> 
> I recommend filing a bug report with Squid Bugzilla and providing a stack trace ("bt full") from this assertion. If possible, please keep the core dump for further analysis.
> 
> 
> Thank you,
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Sunday, November 21, 2021 8:28 PM
>> To: Lou?ansk? Luk??; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Too many ERROR: Collapsed forwarding queue 
>> overflow for kid2 at 1024 items
>>
>> On 11/21/21 2:08 PM, Lou?ansk? Luk?? wrote:
>>> Hello, I've replaced src/ipc/ReadWritelock.cc and ReadWriteLock.h 
>>> with modified versions (with finalizeExclusive calls), but afeter 
>>> some time I have got queue overflows agains.
>>
>> After each bug fix, we have to restart the triage sequence from scratch:
>> Any assertions, crashes, FATAL messages or similar things leading to kid deaths? If they are present, then they explain queue overflows.
>>
>>
>>> So I've downloaded squid-6.0.0-20211116-r90086c5a8 - run it with my 
>>> v5 configure and now I'm testing it. So far I get these:
>>> 2021/11/21 19:05:06 kid1| BUG: missing ENTRY_REQUIRES_COLLAPSING for
>>
>> Noted. If you cannot reproduce this bug at will, then I recommend focusing on other, easier-to-address problems first.
>>
>> Alex.
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From david at articatech.com  Thu Nov 25 21:46:42 2021
From: david at articatech.com (David Touzeau)
Date: Thu, 25 Nov 2021 22:46:42 +0100
Subject: [squid-users] %notes in error pages
Message-ID: <9b8ed8c0-4238-e894-d32e-061c6d5d2103@articatech.com>


Hi,

We need to add %note added from external helper using a deny_info and 
specific squid error page.

tried with %o or %m without success

Is there a token to build an error page with an external acl helper output ?

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211125/1742d7c1/attachment.htm>

From rousskov at measurement-factory.com  Fri Nov 26 16:43:34 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Nov 2021 11:43:34 -0500
Subject: [squid-users] %notes in error pages
In-Reply-To: <9b8ed8c0-4238-e894-d32e-061c6d5d2103@articatech.com>
References: <9b8ed8c0-4238-e894-d32e-061c6d5d2103@articatech.com>
Message-ID: <44c337d2-5a8d-18f4-f820-4a48d256668f@measurement-factory.com>

On 11/25/21 4:46 PM, David Touzeau wrote:

> We need to add %note added from external helper using a deny_info and
> specific squid error page.
> 
> tried with %o or %m without success
> 
> Is there a token to build an error page with an external acl helper output ?

Use @Squid{%code} syntax to add logformat %code (including %note) to
your error page. The feature is available in v5 and beyond. More details
may be available at https://github.com/squid-cache/squid/commit/7e6eabb


HTH,

Alex.


From marcelorodrigo at graminsta.com.br  Sat Nov 27 02:48:56 2021
From: marcelorodrigo at graminsta.com.br (Graminsta)
Date: Fri, 26 Nov 2021 23:48:56 -0300
Subject: [squid-users] How to make tcp_outgoing_address forward to IP:port
Message-ID: <000501d7e339$54321b90$fc9652b0$@graminsta.com.br>

Hi,

 

I have multiple customers that must to be forwarded to different routes.
More than 100 routes per VM.

It is already working for years that way.

 

But now I need that Squid forwards each tcp_outgoing_address to a different
IP and port, because each of those IP:port forwards to a router that make it
goes out through a pool of modens.

 

I have been studding but I can't find a way to tcp_outgoing_address uses
IP:port, it appears to uses only IPs to send traffic out.

 

How can I use Squid to forward traffic to different IP:port?

 

Marcelo Rodrigo

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211126/94a5ead5/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov 27 05:47:08 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Nov 2021 18:47:08 +1300
Subject: [squid-users] How to make tcp_outgoing_address forward to
 IP:port
In-Reply-To: <000501d7e339$54321b90$fc9652b0$@graminsta.com.br>
References: <000501d7e339$54321b90$fc9652b0$@graminsta.com.br>
Message-ID: <7b79228f-54b7-6e19-02f1-05067b74b132@treenet.co.nz>

On 27/11/21 15:48, Graminsta wrote:
> Hi,
> 
> I have multiple customers that must to be forwarded to different routes. 
> More than 100 routes per VM.
> 
> It is already working for years that way.
> 
> But now I need that Squid forwards each tcp_outgoing_address to a 
> different IP and port, because each of those IP:port forwards to a 
> router that make it goes out through a pool of modens.
> 

tcp_outgoing_address cannot do that. It is only a *suggestion* to the 
networking stack of the local machine about what IPA to use for a 
connection.


You are best off having Squid set outgoing TOS and/or NFMARK. Your OS 
and/or router can use those to decide routing.

Amos


From david at articatech.com  Sat Nov 27 16:16:21 2021
From: david at articatech.com (David Touzeau)
Date: Sat, 27 Nov 2021 17:16:21 +0100
Subject: [squid-users] %notes in error pages
In-Reply-To: <44c337d2-5a8d-18f4-f820-4a48d256668f@measurement-factory.com>
References: <9b8ed8c0-4238-e894-d32e-061c6d5d2103@articatech.com>
 <44c337d2-5a8d-18f4-f820-4a48d256668f@measurement-factory.com>
Message-ID: <d74b8b55-0218-fac4-aac3-6fd1e994b51d@articatech.com>

Hi

Working like a charm !!!

Many thanks!!

Le 26/11/2021 ? 17:43, Alex Rousskov a ?crit?:
> On 11/25/21 4:46 PM, David Touzeau wrote:
>
>> We need to add %note added from external helper using a deny_info and
>> specific squid error page.
>>
>> tried with %o or %m without success
>>
>> Is there a token to build an error page with an external acl helper output ?
> Use @Squid{%code} syntax to add logformat %code (including %note) to
> your error page. The feature is available in v5 and beyond. More details
> may be available athttps://github.com/squid-cache/squid/commit/7e6eabb
>
>
> HTH,
>
> Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211127/a06e2e9b/attachment.htm>

From rousskov at measurement-factory.com  Sun Nov 28 16:11:36 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 28 Nov 2021 11:11:36 -0500
Subject: [squid-users] How to make tcp_outgoing_address forward to
 IP:port
In-Reply-To: <000501d7e339$54321b90$fc9652b0$@graminsta.com.br>
References: <000501d7e339$54321b90$fc9652b0$@graminsta.com.br>
Message-ID: <c759d58b-d37f-9be2-d83d-260271fc9a13@measurement-factory.com>

On 11/26/21 9:48 PM, Graminsta wrote:

> How can I use Squid to forward traffic to different IP:port?

You may be able to use a cache_peer directive with the originserver
option to direct traffic to a different TCP address, but I do not know
whether there are enough configuration options to avoid rewriting Host
and/or SNI values when forwarding to such a peer.

Alex.


From jason.spashett at menlosecurity.com  Tue Nov 30 17:19:53 2021
From: jason.spashett at menlosecurity.com (Jason Spashett)
Date: Tue, 30 Nov 2021 17:19:53 +0000
Subject: [squid-users] squid5: assert with IPv6 address
Message-ID: <CANj0NToFrSz+LcaOdD2pzYdEvgCtCQO-ghF9ZBcBpEkMzz=C3A@mail.gmail.com>

Hello Folks,

We are running squid5, and have noticed that squid asserts when an
ipv6 address is used. In fact the initial problem was noticed when
squid's resolver returned, and used an ip v6 address. Browsing to
[2001:4860:4864:5::6d] can cause the assertion

The situation seems to be similar to this older bug, but I have not
worked through it yet:
https://bugs.squid-cache.org/show_bug.cgi?id=3732. So this would
appear to be a bug, but perhaps there is something on the OS that
isn't correct.

Note that ipv6 is disabled in configure, and we also have the sysctl setting:
net.ipv6.conf.all.disable_ipv6 = 1

$ ./squid --version
Squid Cache: Version 5.2-VCS
Service Name: squid

This binary uses OpenSSL 1.1.1  11 Sep 2018. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--disable-arch-native'
'--disable-dependency-tracking' '--disable-eui' '--enable-auth'
'--enable-basic-auth-helpers=getpwnam,LDAP,PAM'
'--enable-digest-auth-helpers=password'
'--enable-external-acl-helpers=file_userip,LDAP_group,unix_group'
'--enable-follow-x-forwarded-for' '--enable-ssl-crtd'
'--enable-translation' '--localstatedir=/var'
'--with-logdir=/var/log/squid' '--with-openssl' '--disable-ipv6'
'--enable-linux-netfilter' '--prefix=/usr/local/squid5


The stack trace follows:
-------------------------------

Program terminated with signal SIGABRT, Aborted.
#0  0x00007fbea1047fb7 in raise () from /lib/x86_64-linux-gnu/libc.so.6
(gdb) bt
#0  0x00007fbea1047fb7 in raise () from /lib/x86_64-linux-gnu/libc.so.6
#1  0x00007fbea1049921 in abort () from /lib/x86_64-linux-gnu/libc.so.6
#2  0x00007fbea103948a in ?? () from /lib/x86_64-linux-gnu/libc.so.6
#3  0x00007fbea1039502 in __assert_fail () from /lib/x86_64-linux-gnu/libc.so.6
#4  0x000055a0706a3edc in Ip::Address::getAddrInfo
(this=this at entry=0x55a07d191c28, dst=@0x7ffc00c10480: 0x55a073de3c80,
force=force at entry=0) at Address.cc:663
#5  0x000055a07069f330 in comm_openex (sock_type=sock_type at entry=1,
proto=proto at entry=6, addr=..., flags=1, note=0x55a0838a0b60
"[2001:4860:4864:5::6d]") at comm.cc:345
#6  0x000055a0706efe6e in Comm::ConnOpener::createFd
(this=this at entry=0x55a079e1d258) at ConnOpener.cc:266
#7  0x000055a0706f0973 in Comm::ConnOpener::start
(this=0x55a079e1d258) at ConnOpener.cc:240
#8  0x000055a070690992 in JobDialer<AsyncJob>::dial
(this=0x55a0881f3848, call=...) at ../../src/base/AsyncJobCalls.h:175
#9  0x000055a07068bd59 in AsyncCall::make
(this=this at entry=0x55a0881f3810) at AsyncCall.cc:44
#10 0x000055a07068d528 in AsyncCallQueue::fireNext
(this=this at entry=0x55a071be1500) at AsyncCallQueue.cc:60
#11 0x000055a07068d8dc in AsyncCallQueue::fire (this=0x55a071be1500)
at AsyncCallQueue.cc:43
#12 0x000055a07045e179 in EventLoop::dispatchCalls
(this=0x7ffc00c10d50) at EventLoop.cc:144
#13 EventLoop::runOnce (this=this at entry=0x7ffc00c10d50) at EventLoop.cc:121
#14 0x000055a07045e268 in EventLoop::run (this=0x7ffc00c10d50) at
EventLoop.cc:83
#15 0x000055a07056701e in SquidMain (argc=<optimized out>,
argv=<optimized out>) at main.cc:1716
#16 0x000055a07040fac1 in SquidMainSafe (argv=0x7ffc00c111b8, argc=6)
at main.cc:1403
#17 main (argc=6, argv=0x7ffc00c111b8) at main.cc:1391

Regards,

Jason


From rousskov at measurement-factory.com  Tue Nov 30 17:27:21 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Nov 2021 12:27:21 -0500
Subject: [squid-users] squid5: assert with IPv6 address
In-Reply-To: <CANj0NToFrSz+LcaOdD2pzYdEvgCtCQO-ghF9ZBcBpEkMzz=C3A@mail.gmail.com>
References: <CANj0NToFrSz+LcaOdD2pzYdEvgCtCQO-ghF9ZBcBpEkMzz=C3A@mail.gmail.com>
Message-ID: <5688e7f3-e0b3-6651-7f2b-555f990f8a75@measurement-factory.com>

On 11/30/21 12:19 PM, Jason Spashett wrote:
> Hello Folks,
> 
> We are running squid5, and have noticed that squid asserts when an
> ipv6 address is used. In fact the initial problem was noticed when
> squid's resolver returned, and used an ip v6 address. Browsing to
> [2001:4860:4864:5::6d] can cause the assertion

AFAICT, this assertion is tracked as Bug 5154:
https://bugs.squid-cache.org/show_bug.cgi?id=5154

HTH,

Alex.



> The situation seems to be similar to this older bug, but I have not
> worked through it yet:
> https://bugs.squid-cache.org/show_bug.cgi?id=3732. So this would
> appear to be a bug, but perhaps there is something on the OS that
> isn't correct.
> 
> Note that ipv6 is disabled in configure, and we also have the sysctl setting:
> net.ipv6.conf.all.disable_ipv6 = 1
> 
> $ ./squid --version
> Squid Cache: Version 5.2-VCS
> Service Name: squid
> 
> This binary uses OpenSSL 1.1.1  11 Sep 2018. For legal restrictions on
> distribution see https://www.openssl.org/source/license.html
> 
> configure options:  '--disable-arch-native'
> '--disable-dependency-tracking' '--disable-eui' '--enable-auth'
> '--enable-basic-auth-helpers=getpwnam,LDAP,PAM'
> '--enable-digest-auth-helpers=password'
> '--enable-external-acl-helpers=file_userip,LDAP_group,unix_group'
> '--enable-follow-x-forwarded-for' '--enable-ssl-crtd'
> '--enable-translation' '--localstatedir=/var'
> '--with-logdir=/var/log/squid' '--with-openssl' '--disable-ipv6'
> '--enable-linux-netfilter' '--prefix=/usr/local/squid5
> 
> 
> The stack trace follows:
> -------------------------------
> 
> Program terminated with signal SIGABRT, Aborted.
> #0  0x00007fbea1047fb7 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> (gdb) bt
> #0  0x00007fbea1047fb7 in raise () from /lib/x86_64-linux-gnu/libc.so.6
> #1  0x00007fbea1049921 in abort () from /lib/x86_64-linux-gnu/libc.so.6
> #2  0x00007fbea103948a in ?? () from /lib/x86_64-linux-gnu/libc.so.6
> #3  0x00007fbea1039502 in __assert_fail () from /lib/x86_64-linux-gnu/libc.so.6
> #4  0x000055a0706a3edc in Ip::Address::getAddrInfo
> (this=this at entry=0x55a07d191c28, dst=@0x7ffc00c10480: 0x55a073de3c80,
> force=force at entry=0) at Address.cc:663
> #5  0x000055a07069f330 in comm_openex (sock_type=sock_type at entry=1,
> proto=proto at entry=6, addr=..., flags=1, note=0x55a0838a0b60
> "[2001:4860:4864:5::6d]") at comm.cc:345
> #6  0x000055a0706efe6e in Comm::ConnOpener::createFd
> (this=this at entry=0x55a079e1d258) at ConnOpener.cc:266
> #7  0x000055a0706f0973 in Comm::ConnOpener::start
> (this=0x55a079e1d258) at ConnOpener.cc:240
> #8  0x000055a070690992 in JobDialer<AsyncJob>::dial
> (this=0x55a0881f3848, call=...) at ../../src/base/AsyncJobCalls.h:175
> #9  0x000055a07068bd59 in AsyncCall::make
> (this=this at entry=0x55a0881f3810) at AsyncCall.cc:44
> #10 0x000055a07068d528 in AsyncCallQueue::fireNext
> (this=this at entry=0x55a071be1500) at AsyncCallQueue.cc:60
> #11 0x000055a07068d8dc in AsyncCallQueue::fire (this=0x55a071be1500)
> at AsyncCallQueue.cc:43
> #12 0x000055a07045e179 in EventLoop::dispatchCalls
> (this=0x7ffc00c10d50) at EventLoop.cc:144
> #13 EventLoop::runOnce (this=this at entry=0x7ffc00c10d50) at EventLoop.cc:121
> #14 0x000055a07045e268 in EventLoop::run (this=0x7ffc00c10d50) at
> EventLoop.cc:83
> #15 0x000055a07056701e in SquidMain (argc=<optimized out>,
> argv=<optimized out>) at main.cc:1716
> #16 0x000055a07040fac1 in SquidMainSafe (argv=0x7ffc00c111b8, argc=6)
> at main.cc:1403
> #17 main (argc=6, argv=0x7ffc00c111b8) at main.cc:1391
> 
> Regards,
> 
> Jason
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From jason.spashett at menlosecurity.com  Tue Nov 30 18:00:06 2021
From: jason.spashett at menlosecurity.com (Jason Spashett)
Date: Tue, 30 Nov 2021 18:00:06 +0000
Subject: [squid-users] squid5: assert with IPv6 address
In-Reply-To: <5688e7f3-e0b3-6651-7f2b-555f990f8a75@measurement-factory.com>
References: <CANj0NToFrSz+LcaOdD2pzYdEvgCtCQO-ghF9ZBcBpEkMzz=C3A@mail.gmail.com>
 <5688e7f3-e0b3-6651-7f2b-555f990f8a75@measurement-factory.com>
Message-ID: <CANj0NTrSUBk=tRDc_S2qns+rhPXmE6b2SG7PgZ9vT9H7s1UnZA@mail.gmail.com>

Hello Alex,

Thanks I did not see that one.
...
> AFAICT, this assertion is tracked as Bug 5154:
> https://bugs.squid-cache.org/show_bug.cgi?id=5154
...


