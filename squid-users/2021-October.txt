From pponakanti at roblox.com  Fri Oct  1 06:37:10 2021
From: pponakanti at roblox.com (Praveen Ponakanti)
Date: Thu, 30 Sep 2021 23:37:10 -0700
Subject: [squid-users] Running SMP workers on multi-core systems
In-Reply-To: <15d7139c-154f-f0eb-c486-b346b63a68fd@measurement-factory.com>
References: <CACabJxMpcV+ixKRm9Z7EGfE9NEWN50N2g16_5vkje8oj7A6tGw@mail.gmail.com>
 <15d7139c-154f-f0eb-c486-b346b63a68fd@measurement-factory.com>
Message-ID: <CACabJxOXfFu-PkEyAeFi2OQtBMCppszmoydSEc5oVubCi_a51A@mail.gmail.com>

Hi Alex,

Thanks a lot for the clarification on the config for running multiple
workers with version 5.1, its a lot simpler now with just the "workers"
config! I have been able to resolve the "commBind Cannot bind socket FD" error
by ensuring the var/run/squid folder is created with write permissions. I
have a docker container (ubuntu) with squid configured with
the --localstatedir option prior to build (as the installation is at
/squid), I have managed to fix it with some changes to the docker
entrypoint script and am able to run 16 kid processes on my test server.
However, I am still trying to get the container working in a nomad job
(which was fine with version 4.13 with almost the same config but w/o
additional workers). I havent been able to extract the cache log when run
inside nomad so far, will try again and post here if I need
further assistance on that.

BTW, when I attempt to use the following config with 5.1 the cache log says
that I need to run 'squid -z' to create the swap directories. Is that
something I would need to add to my Docker build steps so that it is set
up before the squid server is run?

Thanks again for the config examples and pointers.

Praveen

"cache_dir ufs /squid/var/cache/squid 400000 16 256"

2021/09/30 22:27:39 kid3| Swap maxSize 409600000 + 262144 KB, estimated
31527857 objects

2021/09/30 22:27:39 kid4| storeDirWriteCleanLogs: Operation aborted.

2021/09/30 22:27:39 kid3| Target number of buckets: 1576392

2021/09/30 22:27:39 kid3| Using 2097152 Store buckets

2021/09/30 22:27:39 kid4| FATAL: Failed to verify one of the swap
directories, Check cache.log

for details.  Run 'squid -z' to create swap directories

if needed, or if running Squid for the first time.

2021/09/30 22:27:39 kid3| Max Mem  size: 262144 KB [shared]

2021/09/30 22:27:39 kid3| Max Swap size: 409600000 KB

2021/09/30 22:27:39 kid3| ERROR: /squid/var/cache/squid/00: (2) No such
file or directory

2021/09/30 22:27:39 kid3| Not currently OK to rewrite swap log.

2021/09/30 22:27:39 kid4| Squid Cache (Version 5.1): Terminated abnormally.

2021/09/30 22:27:39 kid3| storeDirWriteCleanLogs: Operation aborted.

2021/09/30 22:27:39 kid3| FATAL: Failed to verify one of the swap
directories, Check cache.log

for details.  Run 'squid -z' to create swap directories

if needed, or if running Squid for the first time.

2021/09/30 22:27:39 kid2| Store logging disabled

2021/09/30 22:27:39 kid3| Squid Cache (Version 5.1): Terminated abnormally.




root at testhost:/squid# ls -l /squid/var/cache/

total 4

drwxr-xr-x 2 root root 4096 Sep 30 17:52 squid

On Thu, Sep 30, 2021 at 6:30 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 9/30/21 2:49 AM, Praveen Ponakanti wrote:
>
> > I am trying to use the squid cache as an outbound HTTP/S proxy service
> > for our production network and would like to scale up the request per
> > sec by running it on multi-core systems.
>
> > I have searched up the archives and config docs, but couldn't find
> > anything more recent than the below on setting up SMP workers.
> > https://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster
> > https://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem
>
> The two old exotic configurations above are not what you should be
> starting with when thinking about SMP support in modern Squids. The best
> starting points I know about are
>
> * https://wiki.squid-cache.org/Features/SmpScale
> * http://www.squid-cache.org/Doc/config/workers/
> * http://www.squid-cache.org/Doc/config/cpu_affinity_map/
> * http(s)_port worker-queues option
>
> That Feature wiki page does not get many updates these days; if there is
> a conflict between information sources, the directive documentation in
> your squid.conf.documented may have more recent information.
>
>
> > * Can someone please let me know if the config examples in those docs
> > still apply to the latest version 5.1.
>
> I bet those use cases themselves do not apply well to your situation.
>
>
> > * Do we still need the combination of squid, frontend, backend.conf
> > files, or is it enabled simply with the "workers" line in the main
> > config file?
>
> The workers directive does not require multiple Squid instances (with
> multiple configuration files, etc.).
>
>
> > * When I attempt to run the squid server with "workers 4" in the
> > squid.conf, I get the following error (one for each kid) and the main
> > process does not bind to the "http_port 3128". I see several UDP ports
> > opened up for each kid process.
>
> > 2021/09/29 00:44:10 kid5| commBind Cannot bind socket FD 11 to [::]: (2)
> No such file or directory
>
> Unfortunately, I cannot tell exactly what went wrong based on that
> low-level message alone, but it could be a variant of [1]. If you cannot
> figure it out after checking [1] suggestions, consider sharing "squid
> -X" startup cache.log for analysis. There is
>
> [1]
>
> https://wiki.squid-cache.org/Features/SmpScale#Cannot_bind_socket_FD_NN_to_.5B::.5D:_.2813.29_Permission_denied
>
> The UDP ports you are seeing are probably for the internal DNS resolver
> which is not SMP-aware.
>
>
> > * Does anyone have recommendations on the maximum number of workers to
> > use on a 64 core host (assuming no other CPU intensive apps are running
> > on the same host). Caching is not a must for our initial deployment, so
> > we are fine disabling caching.
>
> 28-30. See the following wiki section for the corresponding rules of
> thumb:
>
> https://wiki.squid-cache.org/Features/SmpScale#How_to_configure_SMP_Squid_for_top_performance.3F
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20210930/f61df658/attachment.htm>

From squid3 at treenet.co.nz  Fri Oct  1 11:38:36 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Oct 2021 00:38:36 +1300
Subject: [squid-users] Upgradation of squid version 3.5.27 on ubuntu
 18.04
In-Reply-To: <CAAUHBy6=HgeOb_7dgvhC-=csCK58f_5rZLwtb2sWn1Rt7n7enA@mail.gmail.com>
References: <CAAUHBy6=HgeOb_7dgvhC-=csCK58f_5rZLwtb2sWn1Rt7n7enA@mail.gmail.com>
Message-ID: <f78790c4-6aa5-f2e7-f78d-b03bf97edf45@treenet.co.nz>

On 30/09/21 1:26 am, sheik abdul wrote:
> Hellow TEam,
> 
> Hope you're doing well!
> 
> I have installed Ubuntu 18.04 (Bionic) with the squid version of 3.5.27 
> (maybe?that's that latest version) .

That is the squid version shipped by Ubuntu 18.04 LTS.

> I'm always getting in the 
> Vulnerability list and stats that need to upgrade to the latest version 
> of squid so that I can avoid those vuln.

An upgrade of Squid is not going to help avoid the "issue" you list below.

For two reasons:
  1) It is a side effect of the build environment used to build Squid, 
not the Squid code itself.

  2) The Squid security team rejected the CVE you reference. On grounds 
that behaviour is intentional - the Squid main/'master' process never 
actually finishes with root privileges. It needs to be able to start and 
assign some child processes high privileges from time to time.



> Please find the below vuln. details?for you ref.
> 
> https://gitlab.com/jeriko.one/security/-/blob/master/squid/CVEs/CVE-2019-12522.txt 
> <https://gitlab.com/jeriko.one/security/-/blob/master/squid/CVEs/CVE-2019-12522.txt>
> 

Before worrying, please notice that document states the behaviour does 
not occur in the HAVE_SETRESUID or HAVE_SETEUID code.

The OS function setresuid() has been provided by Linux since 2.1 and 
glibc since 2.3.2. Both predate Ubuntu 18.* by many years. So I am 
doubtful your Squid is actually relevant to that documented behaviour. 
If that HAVE_SETRESUID code is not being built, that is an issue the 
vendor building your Squid package (Ubuntu) needs to fix.


> And I just want to know whether?is possible to update the squid version 
> in ubuntu 18 because it's already been in?the?latest version but I'm not 
> sure why it's asking us to update the latest?version(from 4.1 to 5 
> onwards?).

What is asking you to upgrade? Squid does not ask such things. AFAIK, 
Ubuntu package managers will only mention upgrades if the repositories 
you are using actually have a newer version available - at which point a 
regular "apt upgrade" command should do it for you.


Cheers
Amos


From rousskov at measurement-factory.com  Fri Oct  1 16:20:33 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 1 Oct 2021 12:20:33 -0400
Subject: [squid-users] Running SMP workers on multi-core systems
In-Reply-To: <CACabJxOXfFu-PkEyAeFi2OQtBMCppszmoydSEc5oVubCi_a51A@mail.gmail.com>
References: <CACabJxMpcV+ixKRm9Z7EGfE9NEWN50N2g16_5vkje8oj7A6tGw@mail.gmail.com>
 <15d7139c-154f-f0eb-c486-b346b63a68fd@measurement-factory.com>
 <CACabJxOXfFu-PkEyAeFi2OQtBMCppszmoydSEc5oVubCi_a51A@mail.gmail.com>
Message-ID: <7c693683-334c-97e5-ba36-c673c8f3d711@measurement-factory.com>

On 10/1/21 2:37 AM, Praveen Ponakanti wrote:

> when I attempt to use the following config with 5.1 the cache log
> says that I need to run?'squid -z' to create the swap directories. Is
> that something I would need to add to my Docker build steps so that it
> is set up?before?the squid server is run??

The exact solution is up to you, but Squid cannot be configured to
automatically initialize cache_dirs for you (yet) -- something must run
"squid -z" before a new cache_dir directory can be used by a regular
Squid instance.

> "cache_dir ufs /squid/var/cache/squid 400000 16 256"

Please note that "ufs" cache_dirs are not compatible with SMP Squids.
SMP Squid behavior with UFS-based cache_dirs is undefined. If you need
disk caching, use "rock" cache_dirs instead.

Alex.


> 2021/09/30 22:27:39 kid4| FATAL: Failed to verify one of the swap
> directories, Check cache.log
> for details.? Run 'squid -z' to create swap directories
> if needed, or if running Squid for the first time.


> On Thu, Sep 30, 2021 at 6:30 AM Alex Rousskov wrote:
> 
>     On 9/30/21 2:49 AM, Praveen Ponakanti wrote:
> 
>     > I am trying?to use the squid cache as an outbound HTTP/S proxy service
>     > for our production network and would like to scale up the request per
>     > sec by running it on multi-core systems.?
> 
>     > I have searched up the archives and config docs, but couldn't?find
>     > anything more recent than the below on setting?up SMP workers.?
>     > https://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster
>     <https://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster>
>     > https://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem
>     <https://wiki.squid-cache.org/ConfigExamples/MultiCpuSystem>
> 
>     The two old exotic configurations above are not what you should be
>     starting with when thinking about SMP support in modern Squids. The best
>     starting points I know about are
> 
>     * https://wiki.squid-cache.org/Features/SmpScale
>     <https://wiki.squid-cache.org/Features/SmpScale>
>     * http://www.squid-cache.org/Doc/config/workers/
>     <http://www.squid-cache.org/Doc/config/workers/>
>     * http://www.squid-cache.org/Doc/config/cpu_affinity_map/
>     <http://www.squid-cache.org/Doc/config/cpu_affinity_map/>
>     * http(s)_port worker-queues option
> 
>     That Feature wiki page does not get many updates these days; if there is
>     a conflict between information sources, the directive documentation in
>     your squid.conf.documented may have more recent information.
> 
> 
>     > * Can someone please let me know if the config examples in those docs
>     > still apply to the latest version 5.1.
> 
>     I bet those use cases themselves do not apply well to your situation.
> 
> 
>     > * Do we still need the combination of squid, frontend, backend.conf
>     > files, or is it enabled simply with the "workers" line in the main
>     > config file??
> 
>     The workers directive does not require multiple Squid instances (with
>     multiple configuration files, etc.).
> 
> 
>     > * When I attempt to run the squid server with "workers 4" in the
>     > squid.conf, I get the following error (one for each kid) and the main
>     > process does not bind to the "http_port 3128". I see several UDP ports
>     > opened up for each kid process.
> 
>     > 2021/09/29 00:44:10 kid5| commBind Cannot bind socket FD 11 to
>     [::]: (2) No such file or directory
> 
>     Unfortunately, I cannot tell exactly what went wrong based on that
>     low-level message alone, but it could be a variant of [1]. If you cannot
>     figure it out after checking [1] suggestions, consider sharing "squid
>     -X" startup cache.log for analysis. There is
> 
>     [1]
>     https://wiki.squid-cache.org/Features/SmpScale#Cannot_bind_socket_FD_NN_to_.5B::.5D:_.2813.29_Permission_denied
>     <https://wiki.squid-cache.org/Features/SmpScale#Cannot_bind_socket_FD_NN_to_.5B::.5D:_.2813.29_Permission_denied>
> 
>     The UDP ports you are seeing are probably for the internal DNS resolver
>     which is not SMP-aware.
> 
> 
>     > * Does anyone have recommendations on the maximum number of workers to
>     > use on a 64 core host (assuming no other CPU intensive apps are
>     running
>     > on the same host). Caching is not a must for our initial
>     deployment, so
>     > we are fine disabling caching.
> 
>     28-30. See the following wiki section for the corresponding rules of
>     thumb:
>     https://wiki.squid-cache.org/Features/SmpScale#How_to_configure_SMP_Squid_for_top_performance.3F
>     <https://wiki.squid-cache.org/Features/SmpScale#How_to_configure_SMP_Squid_for_top_performance.3F>
> 
> 
>     HTH,
> 
>     Alex.
> 



From rousskov at measurement-factory.com  Sat Oct  2 02:00:08 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 1 Oct 2021 22:00:08 -0400
Subject: [squid-users] Sorry if this has been asked but I can't find an
 answer anywhere ...
In-Reply-To: <CAPqnvd==HMiJFfvuBWcp2G303SJ4RHhYRybLFz2Yit9pCkKqYQ@mail.gmail.com>
References: <009101d7b188$84ee3030$8eca9090$@gmail.com>
 <ef4b516c-4412-8355-16ca-c303c83ab368@measurement-factory.com>
 <CAPqnvd=4QfFrGOK_uUidwgMbOfvhSCVnqCR2Yn0y4SepZguhHw@mail.gmail.com>
 <c86d051d-f556-50e1-863f-a151b90133f2@measurement-factory.com>
 <CAPqnvdmp=M=BvgRT8bvD9J+QmFHEyDzuq0N+_BHued-OZ216FQ@mail.gmail.com>
 <fdcbb748-9b5e-43b7-dd68-2e747f96098c@measurement-factory.com>
 <CAPqnvdk0aXr98aEpbZxmOnhND=DMfHOQ47zP2rJ8H_sA2nvNQA@mail.gmail.com>
 <0b2dfedd-d9b7-ae8b-b77b-ed06445cd48b@measurement-factory.com>
 <CAPqnvd==HMiJFfvuBWcp2G303SJ4RHhYRybLFz2Yit9pCkKqYQ@mail.gmail.com>
Message-ID: <207ab639-9baa-dc7a-8f73-a21474a44bd8@measurement-factory.com>

On 9/27/21 9:32 AM, Mike Yates wrote:

> I just want squid to redirect any requests (http for instance) to a
> specific external url so for instance http://mysuidserver:80
> to http://externalserver:80

I am ignoring the "any" part -- the answer would heavily depend on the
protocol and other factors -- and focusing on your specific HTTP example.

The information above is not enough to guess how requests will arrive at
Squid, but if they will be sent to Squid as if Squid was an origin
server (representing mysuidserver domain), then you should use a
cache_peer directive to tell Squid to send all (relevant) requests to
the externalserver. Use the "originserver" option to tell Squid that the
peer is not a proxy but an origin server.

Here is an untested sketch to use as a starting point:

    http_port 80 accel
    cache_peer externalserver parent 80 0 \
        originserver \
        forceddomain=externalserver \
        no-digest \
        no-netdb-exchange

You can start with squid.conf.default and adjust its http_access rules
as needed while merging the above snippet into that simple configuration
file. To learn more about various directives, search for their names in
squid.conf.documented.

The above snippet will probably not "work" as is, but it may be the best
available next step for us to learn enough about your environment to
(eventually) arrive at a working configuration.


HTH,

Alex.


> On Mon, Sep 27, 2021 at 9:23 AM Alex Rousskov wrote:
> 
>     On 9/27/21 8:44 AM, Mike Yates wrote:
> 
>     > Sorry Alex but if using postman I just post to the internal URL
>     with no
>     > certificates and everything works fine. All I'm trying to do is
>     post to
>     > the squid server that will then redirect the post to the external
>     url.?
>     > Its that simple ..
> 
>     Unfortunately, since I am not familiar with postman, I cannot convert
>     the pending questions about protocols the server software uses into
>     questions about your postman configuration. Hopefully, somebody else on
>     the list can do that for you. We also need to make sure that what you do
>     in postman matches what your servers are actually doing (as far as
>     communication protocols are concerned) -- the API server may support
>     several protocols, and it is possible, at least in theory, that postman
>     tests use a different set of communication protocols compared to the
>     actual servers you need to handle.
> 
>     Without answers to those pending (or similar) questions and without
>     traffic samples, it is very difficult to guess what is actually going on
>     in your environment. And without that knowledge, it is not clear how to
>     configure Squid to do what you want.
> 
>     I know your environment sounds simple to you, but since you want more
>     than an "It is simple, just use Squid" answer, we need protocol-level
>     details to give specific advice.
> 
>     Alex.
> 
>     > On Sat, Sep 25, 2021 at 11:55 AM Alex Rousskov wrote:
>     >
>     >? ? ?On 9/25/21 5:23 AM, Mike Yates wrote:
>     >? ? ?> There are no certificates to worry about, the api is
>     expecting a token
>     >? ? ?> to be included in the payload of the call.? ?So all squid
>     needs to
>     >? ? ?do is
>     >? ? ?> accept the post from the internal server and pass that post
>     to the
>     >? ? ?> external servers url including the payload.??
>     >
>     >? ? ?> I hope that helps.??
>     >
>     >? ? ?Not yet. Forget about payload for a second. Our problems are
>     still at a
>     >? ? ?much higher level: In your examples, you used https://... URLs. Do
>     >? ? ?internal servers make HTTPS requests (i.e. HTTP requests over
>     SSL/TLS
>     >? ? ?connections)? If yes, then why are you saying that there are no
>     >? ? ?certificates to worry about? TLS connections normally involve
>     >? ? ?certificate validation!..
>     >
>     >? ? ?Perhaps those internal servers make plain HTTP requests, and
>     you used
>     >? ? ?"https://..." URLs in your examples by accident?
>     >
>     >? ? ?BTW, if you do not know the answers to some of the questions,
>     please
>     >? ? ?just say so -- there is nothing wrong with that. If you can
>     share a
>     >? ? ?small packet capture of a single request/response (in libpcap
>     format),
>     >? ? ?that may reduce the number of questions we have to ask.
>     >
>     >? ? ?Alex.
>     >
>     >
>     >? ? ?> On Fri, Sep 24, 2021, 18:01 Alex Rousskov wrote:
>     >? ? ?>
>     >? ? ?>? ? ?On 9/24/21 5:26 PM, Mike Yates wrote:
>     >? ? ?>? ? ?> Ok so let's say the new server outside the dmz has a
>     >? ? ?different name. I
>     >? ? ?>? ? ?> need a squid server configuration that will just
>     forward the
>     >? ? ?api calls
>     >? ? ?>? ? ?> to an external address.? So my internal servers will
>     still point
>     >? ? ?>? ? ?to Fred
>     >? ? ?>? ? ?> ( which is now a squid server and has access to the
>     outside
>     >? ? ?world) and
>     >? ? ?>? ? ?> will then forward the requests to the new server I have in
>     >? ? ?the cloud.?
>     >? ? ?>? ? ?> Long story short I just need a pass through squid
>     server.? ?
>     >? ? ?>
>     >? ? ?>? ? ?Will those internal servers trust the certificate you
>     >? ? ?configure Squid
>     >? ? ?>? ? ?with? In your example, you used "https://...". That usually
>     >? ? ?means the
>     >? ? ?>? ? ?internal servers are going to validate the server
>     certificate.
>     >? ? ?Can you
>     >? ? ?>? ? ?make them trust the Squid certificate? Or does the API
>     >? ? ?communication
>     >? ? ?>? ? ?have to be signed by a fred.mydomain.com
>     <http://fred.mydomain.com>
>     >? ? ?<http://fred.mydomain.com <http://fred.mydomain.com>>
>     <http://fred.mydomain.com <http://fred.mydomain.com>
>     >? ? ?<http://fred.mydomain.com <http://fred.mydomain.com>>>
>     >? ? ?>? ? ?certificate that you do not
>     >? ? ?>? ? ?control?
>     >? ? ?>
>     >? ? ?>? ? ?The other pending question is whether those internal
>     servers are
>     >? ? ?>? ? ?configured to use a proxy (see the previous email on this
>     >? ? ?thread) or
>     >? ? ?>? ? ?always talk directly to (what they think is) the API
>     service?
>     >? ? ?>
>     >? ? ?>? ? ?Alex.
>     >? ? ?>
>     >? ? ?>
>     >? ? ?>? ? ?> On Fri, Sep 24, 2021, 17:18 Alex Rousskov wrote:
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?On 9/24/21 5:09 PM, Mike Yates wrote:
>     >? ? ?>? ? ?>? ? ?> I have a bunch of internal machines that do not have
>     >? ? ?internet
>     >? ? ?>? ? ?>? ? ?access and
>     >? ? ?>? ? ?>? ? ?> any one of them is sending api post requests to
>     another
>     >? ? ?>? ? ?system on prem
>     >? ? ?>? ? ?>? ? ?> and having no issues ?.
>     >? ? ?>? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?> ?
>     >? ? ?>? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?> Example would be
>     https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>
>     >? ? ?>? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>>
>     >? ? ?>? ? ?>? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>
>     >? ? ?>? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>>>
>     >? ? ?>? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?> ?
>     >? ? ?>? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?> Now the problem becomes the fred server is being
>     moved to
>     >? ? ?>? ? ?the cloud so
>     >? ? ?>? ? ?>? ? ?> the same https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>
>     >? ? ?>? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>>
>     >? ? ?>? ? ?>? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>
>     >? ? ?>? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>
>     >? ? ?<https://fred.mydomain.com/api/event
>     <https://fred.mydomain.com/api/event>>>> is still valid but none of my
>     >? ? ?>? ? ?>? ? ?> internal server can see fred and I don?t have access
>     >? ? ?to the
>     >? ? ?>? ? ?backend
>     >? ? ?>? ? ?>? ? ?> servers to change their api calls.
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?AFAICT from your summary, "moved to cloud" here means
>     >? ? ?that the API
>     >? ? ?>? ? ?>? ? ?protocol stays the same, the API server domain name
>     >? ? ?stays the
>     >? ? ?>? ? ?same, the
>     >? ? ?>? ? ?>? ? ?API URL path stays the same, but the IP address of
>     that
>     >? ? ?domain
>     >? ? ?>? ? ?name will
>     >? ? ?>? ? ?>? ? ?change. Please clarify if that conclusion is wrong.
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?If it is correct, then it is not clear how the
>     change of
>     >? ? ?an IP
>     >? ? ?>? ? ?address
>     >? ? ?>? ? ?>? ? ?would affect those making API requests using the
>     domain
>     >? ? ?name,
>     >? ? ?>? ? ?and what
>     >? ? ?>? ? ?>? ? ?role Squid is playing here.
>     >? ? ?>? ? ?>
>     >? ? ?>? ? ?>? ? ?Alex.
>     >? ? ?>? ? ?>
>     >? ? ?>
>     >
> 



From squid3 at treenet.co.nz  Mon Oct  4 02:05:15 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 04 Oct 2021 02:05:15 -0000
Subject: [squid-users] [squid-announce] Squid 5.1 is available
Message-ID: <9b22ccb1-4aea-c0ef-4932-736f866e41a4@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-5.1 release!


This release is we believe, stable enough for general production use.


Support for Squid-4 bug fixes has now officially ceased. Bugs in 4.x
will continue to be fixed, however the fixes will be added to the 5.x
series. All users of Squid-4.x are encouraged to plan for upgrades.


A short list of the major new features is:

  * ICAP Trailers
  * Happy Eyeballs Update
  * Kerberos Group Helper
  * TrivialDB Support
  * Loop Detection in Content Delivery Networks
  * Peering support for SSL-Bump


Several features have been removed in 5.1:

  * dns_v4_first directive replaced with "Happy Eyeballs" algorithm
  * --disable-inline and USE_CHUNKEDMEMPOOLS built options

Further details can be found in the release notes or the wiki.
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
https://wiki.squid-cache.org/Squid-5


Please remember to run "squid -k parse" when testing upgrade to a new
version of Squid. It will audit your configuration files and report
any identifiable issues the new release will have in your installation
before you "press go".

Please be particularly aware that databases for ext_session_acl and
ext_time_quota_acl helpers when built with TrivialDB may differ from
helpers built with BerkleyDB. Manual removal and rebuild of the
database is advised when upgrading.


All feature additions are considered *experimental* until they have
survived at least one series of releases in general production use.
Please be aware of that when rolling out features which are new in
this series. Not all use-cases have been well tested yet and some may
not even have been implemented. Assistance is still needed despite the
releases general stability level.


Plans for the next series of releases is already well underway. Our
future release plan and upcoming features can be found at:
   https://wiki.squid-cache.org/ReleaseSchedule
   https://wiki.squid-cache.org/RoadMap


  See the ChangeLog for the full list of changes in this and earlier
  releases.

   All users of Squid-5.0 beta releases are urged to upgrade to this
   release as soon as possible.

   All users of Squid-4 are encouraged to upgrades where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

   http://www.squid-cache.org/Versions/v5/
   ftp://ftp.squid-cache.org/pub/squid/
   ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

   http://www.squid-cache.org/Download/http-mirrors.html
   http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
   https://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Mon Oct  4 01:34:07 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Oct 2021 14:34:07 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2020:12
 Out-Of-Bounds memory access in WCCPv2
Message-ID: <ac306466-7332-e402-7792-766b68b7641c@treenet.co.nz>

__________________________________________________________________

     Squid Proxy Cache Security Update Advisory SQUID-2020:12
__________________________________________________________________

Advisory ID:       | SQUID-2020:12
Date:              | Oct 03, 2021
Summary:           | Out-Of-Bounds memory access in WCCPv2
Affected versions: | Squid 2.6 -> 2.7.STABLE9
                    | Squid 3.x -> 3.5.28
                    | Squid 4.x -> 4.16
                    | Squid 5.x -> 5.1
Fixed in version:  | Squid 4.17 and 5.2
__________________________________________________________________

   <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-28116>
   <https://www.zerodayinitiative.com/advisories/ZDI-CAN-11610>
__________________________________________________________________

Problem Description:

  Due to an out of bounds memory access Squid is vulnerable to an
  information leak vulnerability when processing WCCPv2 messages.

__________________________________________________________________

Severity:

  This problem allows a WCCPv2 sender to corrupt Squids list of
  known WCCP routers and divert client traffic to attacker
   controlled routers.

  This attack is limited to Squid proxy with WCCPv2 enabled and
  IP spoofing of a router IP address configured as trusted in
  squid.conf.

CVSS Score of 7.7
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:H/PR:N/UI:N/S:C/C:H/I:H/A:N/E:X/RL:O/RC:C/CR:H/IR:H/AR:X/MAV:N/MAC:H/MPR:N/MUI:X/MS:U/MC:H/MI:H/MA:X&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid versions 4.17 and 5.2.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 4:
  <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2020_12.patch>

Squid 5:
 
<http://www.squid-cache.org/Versions/v5/changesets/squid-5-7a73a54cefff6bb83c03de219a73276e42d183d0.patch>

  If you are using a prepackaged version of Squid then please
  refer to the package vendor for availability information on
  updated packages.

__________________________________________________________________

Determining if your version is vulnerable:

  All Squid built with --disable-wccpv2 are not vulnerable.

  All Squid-3.x up to and including 3.5.28 built with
  --enable-wccpv2 and configured with wccp2_router in squid.conf
  are vulnerable.

  All Squid-3.x up to and including 3.5.28 built without
  --disable-wccpv2 and configured with wccp2_router in squid.conf
  are vulnerable.

  All Squid-4.x up to and including 4.16 built with
  --enable-wccpv2 and configured with wccp2_router in squid.conf
  are vulnerable.

  All Squid-4.x up to and including 4.16 built without
  --disable-wccpv2 and configured with wccp2_router in squid.conf
  are vulnerable.

  All Squid-5.x up to and including 5.1 built with
  --enable-wccpv2 and configured with wccp2_router in squid.conf
  are vulnerable.

  All Squid-5.x up to and including 5.1 built without
  --disable-wccpv2 and configured with wccp2_router in squid.conf
  are vulnerable.

__________________________________________________________________

Workaround:

Either,

The following network security Best Practices will greatly
restrict the ability of any attacker utilizing this
vulnerability. They can be considered workarounds for this
issue:

  * Use Private IP address for control communications (eg WCCPv2)
    with routers.

  * Firewall restriction of UDP traffic on port 2048 and any
    other UDP ports used for WCCP(v2) control messages to only
    permit known devices to communicate with WCCP(v2).

    Note that ports used by clients and diverted by WCCP (eg 80
    or 443) are not relevant.

  * Ensure the network implements BCP 38 spoofing protection.
    Include protection against LAN traffic spoofing as much as
    possible.
    See also <http://www.bcp38.info> and 
<https://tools.ietf.org/html/bcp38>.

Or,

  Build Squid with --disable-wccpv2

Or,

  Remove all lines for wccp2_* directives from squid.conf.
  The default configuration is not to enable WCCPv2.

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If you install and build Squid from the original Squid sources
  then the <squid-users at lists.squid-cache.org> mailing list is
  your primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <http://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  This vulnerability was discovered by Lyu working with Trend
  Micro Zero Day Initiative.

  Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

  2020-08-17 10:43:36 UTC Initial Report
  2021-02-09 00:00:00 UTC Advisory Release by ZDI
  2021-10-03 00:00:00 UTC Packages Released

__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Mon Oct  4 01:36:37 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Oct 2021 14:36:37 +1300
Subject: [squid-users] [squid-announce] Squid 4.17 is available
Message-ID: <69a6dc7b-4314-dcd6-8fca-77cb528be981@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.17 release!


This release is a security release resolving a vulnerability
found in the prior Squid releases.


The major changes to be aware of:

  * SQUID-2020:12 Out-Of-Bounds memory access in WCCPv2
    (CVE-2021-28116 aka ZDI-CAN-11610)

  Due to an out of bounds memory access Squid is vulnerable to an
  information leak vulnerability when processing WCCPv2 messages.

  This problem allows a WCCPv2 sender to corrupt Squids list of
  known WCCP routers and divert client traffic to attacker
  controlled routers.

  This attack is limited to Squid proxy with WCCPv2 enabled and
  IP spoofing of a router IP address configured as trusted in
  squid.conf.


   All users of Squid are encouraged to upgrade as soon as possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

   http://www.squid-cache.org/Versions/v4/
   ftp://ftp.squid-cache.org/pub/squid/
   ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

   http://www.squid-cache.org/Download/http-mirrors.html
   http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
   https://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Mon Oct  4 01:56:23 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Oct 2021 14:56:23 +1300
Subject: [squid-users] [squid-announce] Squid 5.2 is available
Message-ID: <40cc8163-88fd-76c1-cbf3-f32db863a4db@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the
availability of the Squid-5.2 release!


This release is a security release resolving several
vulnerabilities and bugs found in the prior Squid releases.


The major changes to be aware of:

  * SQUID-2020:12 Out-Of-Bounds memory access in WCCPv2
    (CVE-2021-28116 aka ZDI-CAN-11610)

  Due to an out of bounds memory access Squid is vulnerable to an
  information leak vulnerability when processing WCCPv2 messages.

  This problem allows a WCCPv2 sender to corrupt Squids list of
  known WCCP routers and divert client traffic to attacker
  controlled routers.

  This attack is limited to Squid proxy with WCCPv2 enabled and
  IP spoofing of a router IP address configured as trusted in
  squid.conf.


  * SQUID-2021:6 Improper Certificate Validation of TLS server
    certificates
    (CVE-2021-41611)

  When validating an origin server or peer certificate, Squid may
  incorrectly classify certain certificates as trusted.

  This problem allows a remote server to obtain security trust
  when the trust is not valid. This indication of trust may be
  passed along to clients allowing access to unsafe or hijacked
  services.

  This problem is guaranteed to occur when multiple CA have
  signed the TLS server certificate. It may also occur in cases
  of broken server certificate chains.


  * Bug 4922: Improve ftp://... filename extraction

  Since 3.5 Squid has incorrectly truncated FTP downloads when
  the transfer is made in ASCII mode (with ';type=' argument).
  This release can be expected to work when downloading from all
  FTP servers.


  * Bug 5164: a copy-paste typo in HttpHdrCc::hasMinFresh()

  This bug shows up as incorrect HIT and MISS results when
  caching responses from a server using Cache-Control:min-fresh.


   All users of Squid are encouraged to upgrade as soon as
   possible.


See the ChangeLog for the full list of changes in this and
earlier releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

   http://www.squid-cache.org/Versions/v5/
   ftp://ftp.squid-cache.org/pub/squid/
   ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

   http://www.squid-cache.org/Download/http-mirrors.html
   http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug
report.
   https://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Mon Oct  4 01:59:26 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Oct 2021 14:59:26 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2021:6 Improper
 Certificate Validation in TLS
Message-ID: <d8adde24-ec6e-af45-c7cc-4ca2f2807b64@treenet.co.nz>


__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2021:1
__________________________________________________________________

Advisory ID:       | SQUID-2021:6
Date:              | October 3, 2021
Summary:           | Improper Certificate Validation in TLS
Affected versions: | Squid 5.0.6 -> 5.1
Fixed in version:  | Squid 5.2
__________________________________________________________________

   <http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41611>
__________________________________________________________________

Problem Description:

  When validating an origin server or peer certificate, Squid may
  incorrectly classify certain certificates as trusted.

__________________________________________________________________

Severity:

  This problem allows a remote server to obtain security trust
  when the trust is not valid. This indication of trust may be
  passed along to clients allowing access to unsafe or hijacked
  services.

  This problem is guaranteed to occur when multiple CA have
  signed the TLS server certificate. It may also occur in cases
  of broken server certificate chains.

CVSS Score of 8.4
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:N/E:H/RL:O/RC:C/CR:H/IR:H/AR:X/MAV:X/MAC:X/MPR:X/MUI:R/MS:X/MC:H/MI:H/MA:X&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid version 5.2.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 5:

<http://www.squid-cache.org/Versions/v5/changesets/squid-5-533b4359f16cf9ed15a6d709a57a4b06e4222cfe.patch>

  If you are using a prepackaged version of Squid then please
  refer to the package vendor for availability information on
  updated packages.

__________________________________________________________________

Determining if your version is vulnerable:

  All Squid-4 and older are not vulnerable.

  All Squid-5.0.1 up to and including 5.0.5 are not vulnerable.

  All Squid-5.0.6 up to and including 5.1 are vulnerable.

__________________________________________________________________

Workaround:

  The only workaround is complete denial to TLS and HTTPS servers
  publishing affected certificate chains. The set of affected
  servers varies over time and is left out of this document.

   acl vulnerableDomains dstdomain .example.net
   http_access deny vulnerableDomains

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If you install and build Squid from the original Squid sources
  then the <squid-users at lists.squid-cache.org> mailing list is
  your primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <https://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  This vulnerability was discovered by Jean-Paul Larocque of
  RodeoTV, LLC.

  Fixed by The Measurement Factory.

__________________________________________________________________

Revision history:

  2021-09-07 02:54:48 UTC Initial Report
  2021-09-24 20:10:37 UTC Patches Released
  2021-10-03 00:00:00 UTC Packages Released

__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From tibor.matyas at dsi-as.de  Mon Oct  4 08:53:45 2021
From: tibor.matyas at dsi-as.de (MATYAS, Tibor)
Date: Mon, 4 Oct 2021 10:53:45 +0200
Subject: [squid-users] Portal Splash Page - exceptions
Message-ID: <2184f568-3526-0466-61ef-caa326d15ba7@dsi-as.de>

Hello List,

I have the following situation: Squid 4 is working in non-transparent
mode, with Portal Splash Page
https://wiki.squid-cache.org/ConfigExamples/Portal/Splash
configured. This is a nice feature, the users must regularly accept the
internet usage policy.

Since we have our new VoIP/PBX solution, the connection breakdowns
caused by the session timeout / reaccept policy are very annoying.
I would like to create exceptions for specific sites/domains. Is this
possible?

Thanks a lot and br
Tibor





--------------------------------------------------
DSI Aerospace Technologie GmbH

Sitz der Gesellschaft: Otto-Lilienthal-Str. 1, D-28199 Bremen, Germany
Web: http://www.dsi-as.de

Geschaeftsfuehrer: Dr.-Ing. Christian Dierker
                   M. Sc. Elias Hashem

HRB 17726, Amtsgericht Bremen
USt-IdNr.: DE 192 681 774
--------------------------------------------------





From hsv at energy.dk  Mon Oct  4 16:32:55 2021
From: hsv at energy.dk (Henning Svane)
Date: Mon, 4 Oct 2021 16:32:55 +0000
Subject: [squid-users] Squid do not reply
Message-ID: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>

Hi
I have tried to install my first Squid proxy as a proxy for installation for ubuntu APT
But the apt-get cannot access cache or get through the proxy.
What have I missed or should have done otherwise ?

Most of the time when I run
sudo apt-get update
or other updates or install commands
I get the reply:
odin at APTProxy:~$ sudo apt-get update
[sudo] password for odin:
0% [Waiting for headers]

And it stay on 0% until it time out and sends this output:
[sudo] password for odin:
Err:1 http://dk.archive.ubuntu.com/ubuntu focal InRelease
  Connection failed [IP: 10.40.63.11 20080]
Hit:2 http://dk.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://dk.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://dk.archive.ubuntu.com/ubuntu focal-security InRelease
Reading package lists... Done
W: Failed to fetch http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease  Connection failed [IP: 10.40.63.11 3128]
W: Some index files failed to download. They have been ignored, or old ones used instead.

Squid.conf :
acl localhost src 127.0.0.1/32 ::1
acl localhost src 10.0.0.0/8
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
#acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src FC00:18F:11AB::/48

acl SSL_ports port 443
acl Safe_ports port 80      # http
acl Safe_ports port 443     # https
acl CONNECT method CONNECT
#acl allowed_websites dstdomain "/etc/squid/web_allowed.acl"
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128
maximum_object_size 4096  MB
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .       0   20% 4320
refresh_all_ims on
#debug_options ALL,5

"/etc/squid/web_allowed.acl"
.ubuntu.com
.launchpad.net

If I run
squidclient -h 10.40.63.11 -p 3128 -v  mgr:info
The reply are the following:
Request:
GET cache_object://10.40.63.11/info HTTP/1.0
Host: 10.40.63.11
User-Agent: squidclient/4.10
Accept: */*
Connection: close


.
HTTP/1.1 200 OK
Server: squid/4.10
Mime-Version: 1.0
Date: Mon, 04 Oct 2021 15:21:37 GMT
Content-Type: text/plain;charset=utf-8
Expires: Mon, 04 Oct 2021 15:21:37 GMT
Last-Modified: Mon, 04 Oct 2021 15:21:37 GMT
X-Cache: MISS from APTProxy
X-Cache-Lookup: MISS from APTProxy:20080
Via: 1.1 APTProxy (squid/4.10)
Connection: close

Squid Object Cache: Version 4.10
Build Info: Ubuntu linux
Service Name: squid
Start Time:     Mon, 04 Oct 2021 07:33:47 GMT
Current Time:   Mon, 04 Oct 2021 15:21:37 GMT
Connection information for squid:
        Number of clients accessing cache:      2
        Number of HTTP requests received:       20
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   0.0
        Average ICP messages per minute since start:    0.0
        Select loop called: 65913 times, 425.864 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 0.0%, 60min: 0.0%
        Hits as % of bytes sent:        5min: -0.0%, 60min: -0.0%
        Memory hits as % of hit requests:       5min: 0.0%, 60min: 0.0%
        Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
        Storage Swap size:      0 KB
        Storage Swap capacity:   0.0% used,  0.0% free
        Storage Mem size:       212 KB
        Storage Mem capacity:    0.1% used, 99.9% free
        Mean Object Size:       0.00 KB
        Requests given to unlinkd:      0
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.00000  0.00000
        Cache Misses:          0.00000  0.00000
        Cache Hits:            0.00000  0.00000
        Near Hits:             0.00000  0.00000
        Not-Modified Replies:  0.00000  0.00000
        DNS Lookups:           0.00000  0.00000
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:        28069.974 seconds
        CPU Time:       2.997 seconds
        CPU Usage:      0.01%
        CPU Usage, 5 minute avg:        0.01%
        CPU Usage, 60 minute avg:       0.01%
        Maximum Resident Size: 94016 KB
        Page faults with physical i/o: 0
Memory accounted for:
        Total accounted:          655 KB
        memPoolAlloc calls:     68992
        memPoolFree calls:      69118
File descriptor usage for squid:
        Maximum number of file descriptors:   1024
        Largest file desc currently in use:     16
        Number of file desc currently in use:    9
        Files queued for open:                   0
        Available number of file descriptors: 1015
        Reserved number of file descriptors:   100
        Store Disk files open:                   0
Internal Data Structures:
            53 StoreEntries
            53 StoreEntries with MemObjects
             0 Hot Object Cache Items
             0 on-disk objects

Regards
Henning

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211004/14657f6c/attachment.htm>

From rousskov at measurement-factory.com  Mon Oct  4 16:56:30 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Oct 2021 12:56:30 -0400
Subject: [squid-users] Squid do not reply
In-Reply-To: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
References: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
Message-ID: <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>

On 10/4/21 12:32 PM, Henning Svane wrote:

> I have tried to install my first Squid proxy as a proxy for installation
> for ubuntu APT
> 
> But the apt-get cannot access cache or get through the proxy.
> 
> What have I missed or should have done otherwise ?

What apt-related lines do you see in Squid's access.log?

Alex.



> Most of the time when I run
> 
> sudo apt-get update
> 
> or other updates or install commands
> 
> I get the reply:
> 
> odin at APTProxy:~$ sudo apt-get update
> 
> [sudo] password for odin:
> 
> 0% [Waiting for headers]
> 
> ?
> 
> And it stay on 0% until it time out and sends this output:
> 
> [sudo] password for odin:
> 
> Err:1 http://dk.archive.ubuntu.com/ubuntu
> <http://dk.archive.ubuntu.com/ubuntu> focal InRelease
> 
> ? Connection failed [IP: 10.40.63.11 20080]
> 
> Hit:2 http://dk.archive.ubuntu.com/ubuntu focal-updates InRelease
> 
> Hit:3 http://dk.archive.ubuntu.com/ubuntu focal-backports InRelease
> 
> Hit:4 http://dk.archive.ubuntu.com/ubuntu focal-security InRelease
> 
> Reading package lists... Done
> 
> W: Failed to fetch
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease Connection
> failed [IP: 10.40.63.11 3128]
> 
> W: Some index files failed to download. They have been ignored, or old
> ones used instead.
> 
> ?
> 
> Squid.conf :
> 
> acl localhost src 127.0.0.1/32 ::1
> 
> acl localhost src 10.0.0.0/8
> 
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> 
> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
> 
> #acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
> 
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> 
> acl localnet src FC00:18F:11AB::/48
> 
> ?
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80????? # http
> 
> acl Safe_ports port 443???? # https
> 
> acl CONNECT method CONNECT
> 
> #acl allowed_websites dstdomain ?/etc/squid/web_allowed.acl"
> 
> http_access allow manager localhost
> 
> http_access deny manager
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localnet
> 
> http_access allow localhost
> 
> http_access deny all
> 
> http_port 3128
> 
> maximum_object_size 4096? MB
> 
> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
> 
> refresh_pattern (Release|Packages(.gz)*)$? ????0?????? 20%???? 2880
> 
> refresh_pattern .?????? 0?? 20% 4320
> 
> refresh_all_ims on
> 
> #debug_options ALL,5
> 
> ?
> 
> ?/etc/squid/web_allowed.acl"
> 
> .ubuntu.com
> 
> .launchpad.net
> 
> ?
> 
> If I run
> 
> squidclient -h 10.40.63.11 -p 3128 -v? mgr:info
> 
> The reply are the following:
> 
> Request:
> 
> GET cache_object://10.40.63.11/info HTTP/1.0
> 
> Host: 10.40.63.11
> 
> User-Agent: squidclient/4.10
> 
> Accept: */*
> 
> Connection: close
> 
> ?
> 
> ?
> 
> .
> 
> HTTP/1.1 200 OK
> 
> Server: squid/4.10
> 
> Mime-Version: 1.0
> 
> Date: Mon, 04 Oct 2021 15:21:37 GMT
> 
> Content-Type: text/plain;charset=utf-8
> 
> Expires: Mon, 04 Oct 2021 15:21:37 GMT
> 
> Last-Modified: Mon, 04 Oct 2021 15:21:37 GMT
> 
> X-Cache: MISS from APTProxy
> 
> X-Cache-Lookup: MISS from APTProxy:20080
> 
> Via: 1.1 APTProxy (squid/4.10)
> 
> Connection: close
> 
> ?
> 
> Squid Object Cache: Version 4.10
> 
> Build Info: Ubuntu linux
> 
> Service Name: squid
> 
> Start Time:???? Mon, 04 Oct 2021 07:33:47 GMT
> 
> Current Time:?? Mon, 04 Oct 2021 15:21:37 GMT
> 
> Connection information for squid:
> 
> ??????? Number of clients accessing cache:????? 2
> 
> ??????? Number of HTTP requests received:?????? 20
> 
> ??????? Number of ICP messages received:??????? 0
> 
> ??????? Number of ICP messages sent:??? 0
> 
> ??????? Number of queued ICP replies:?? 0
> 
> ??????? Number of HTCP messages received:?????? 0
> 
> ??????? Number of HTCP messages sent:?? 0
> 
> ??????? Request failure ratio:?? 0.00
> 
> ??????? Average HTTP requests per minute since start:?? 0.0
> 
> ??????? Average ICP messages per minute since start:??? 0.0
> 
> ??????? Select loop called: 65913 times, 425.864 ms avg
> 
> Cache information for squid:
> 
> ??????? Hits as % of all requests:????? 5min: 0.0%, 60min: 0.0%
> 
> ??????? Hits as % of bytes sent:??????? 5min: -0.0%, 60min: -0.0%
> 
> ???? ???Memory hits as % of hit requests:?????? 5min: 0.0%, 60min: 0.0%
> 
> ??????? Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
> 
> ??????? Storage Swap size:????? 0 KB
> 
> ??????? Storage Swap capacity:?? 0.0% used,? 0.0% free
> 
> ??????? Storage Mem size:?????? 212 KB
> 
> ??????? Storage Mem capacity:??? 0.1% used, 99.9% free
> 
> ??????? Mean Object Size:?????? 0.00 KB
> 
> ??????? Requests given to unlinkd:????? 0
> 
> Median Service Times (seconds)? 5 min??? 60 min:
> 
> ??????? HTTP Requests (All):?? 0.00000? 0.00000
> 
> ??????? Cache Misses:????????? 0.00000? 0.00000
> 
> ??????? Cache Hits:??????????? 0.00000? 0.00000
> 
> ??????? Near Hits:???????????? 0.00000? 0.00000
> 
> ??????? Not-Modified Replies:? 0.00000? 0.00000
> 
> ??????? DNS Lookups:?????????? 0.00000? 0.00000
> 
> ??????? ICP Queries:?????????? 0.00000? 0.00000
> 
> Resource usage for squid:
> 
> ??????? UP Time:??????? 28069.974 seconds
> 
> ??????? CPU Time:?????? 2.997 seconds
> 
> ??????? CPU Usage:????? 0.01%
> 
> ??????? CPU Usage, 5 minute avg:??????? 0.01%
> 
> ??????? CPU Usage, 60 minute avg:?????? 0.01%
> 
> ??????? Maximum Resident Size: 94016 KB
> 
> ??????? Page faults with physical i/o: 0
> 
> Memory accounted for:
> 
> ??????? Total accounted:????????? 655 KB
> 
> ??????? memPoolAlloc calls:???? 68992
> 
> ??????? memPoolFree calls:? ????69118
> 
> File descriptor usage for squid:
> 
> ??????? Maximum number of file descriptors:?? 1024
> 
> ??????? Largest file desc currently in use:???? 16
> 
> ??????? Number of file desc currently in use:??? 9
> 
> ??????? Files queued for open:?????????????????? 0
> 
> ??????? Available number of file descriptors: 1015
> 
> ??????? Reserved number of file descriptors:?? 100
> 
> ??????? Store Disk files open:?????????????????? 0
> 
> Internal Data Structures:
> 
> ??????????? 53 StoreEntries
> 
> ??????????? 53 StoreEntries with MemObjects
> 
> ???????????? 0 Hot Object Cache Items
> 
> ???????????? 0 on-disk objects
> 
> ?
> 
> Regards
> 
> Henning
> 
> ?
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From hsv at energy.dk  Mon Oct  4 17:18:41 2021
From: hsv at energy.dk (Henning Svane)
Date: Mon, 4 Oct 2021 17:18:41 +0000
Subject: [squid-users] Squid do not reply
In-Reply-To: <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>
References: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
 <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>
Message-ID: <5ABC601D16A6044981773D154D5C9F7102F4640948@XMail-DAG1.energy.local>

Hi Alex

This from the end of the access.log
I can see some request to IPv6, but I do not have IPv6 as my ISP do not offer it yet.
As you can see I only request from IPv4, but I use dual stack to be future ready, as I hope my ISP sone will enter the modern age.

1633311799.437     57 10.40.63.11 TCP_REFRESH_MODIFIED/200 114201 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
1633311799.470     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 100971 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
1633311799.503     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 114203 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
1633311799.768    224 10.40.63.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
1633311799.914    137 10.40.63.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
1633319549.416  30020 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633319549.425  30027 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
1633319579.447  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633319579.455  30028 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
1633319579.487     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114208 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
1633319579.528     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 100978 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
1633319579.566     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114210 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
1633319579.875    230 10.40.61.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
1633319580.046    156 10.40.61.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633339173.636     21 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
1633339173.646     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
1633339173.657     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
1633339214.784     11 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/130.225.254.116 -
1633339214.795     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
1633339214.806     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
1633339214.814     41 10.40.61.11 TCP_MISS/304 351 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/91.189.95.85 -
1633339214.816      9 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
1633339314.276 239404 10.40.61.11 NONE/503 0 CONNECT changelogs.ubuntu.com:443 - HIER_NONE/- -
1633351198.272 240065 10.40.63.11 NONE/503 0 CONNECT changelogs.ubuntu.com:443 - HIER_NONE/- -
1633353563.799  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633353593.823  30023 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633353593.845     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
1633353593.858     13 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
1633353593.869     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
1633360882.874  30025 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633360897.217      0 10.40.63.11 TCP_MISS/200 2581 GET cache_object://10.40.63.11/info - HIER_NONE/- text/plain
1633360912.905  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633360942.936  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/2001:878:346::116 -
1633360942.959     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
1633360942.970     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
1633360942.981     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
1633363175.859  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633363205.889  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633363205.913     23 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
1633363205.926     12 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
1633363205.937     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
1633364224.510  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633364254.540  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
1633364254.563     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
1633364254.574     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
1633364254.585     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -

Regards
Henning
-----Oprindelig meddelelse-----
Fra: Alex Rousskov <rousskov at measurement-factory.com> 
Sendt: 4. oktober 2021 18:57
Til: Henning Svane <hsv at energy.dk>; squid-users at lists.squid-cache.org
Emne: Re: [squid-users] Squid do not reply

On 10/4/21 12:32 PM, Henning Svane wrote:

> I have tried to install my first Squid proxy as a proxy for 
> installation for ubuntu APT
> 
> But the apt-get cannot access cache or get through the proxy.
> 
> What have I missed or should have done otherwise ?

What apt-related lines do you see in Squid's access.log?

Alex.



> Most of the time when I run
> 
> sudo apt-get update
> 
> or other updates or install commands
> 
> I get the reply:
> 
> odin at APTProxy:~$ sudo apt-get update
> 
> [sudo] password for odin:
> 
> 0% [Waiting for headers]
> 
> ?
> 
> And it stay on 0% until it time out and sends this output:
> 
> [sudo] password for odin:
> 
> Err:1 http://dk.archive.ubuntu.com/ubuntu
> <http://dk.archive.ubuntu.com/ubuntu> focal InRelease
> 
> ? Connection failed [IP: 10.40.63.11 20080]
> 
> Hit:2 http://dk.archive.ubuntu.com/ubuntu focal-updates InRelease
> 
> Hit:3 http://dk.archive.ubuntu.com/ubuntu focal-backports InRelease
> 
> Hit:4 http://dk.archive.ubuntu.com/ubuntu focal-security InRelease
> 
> Reading package lists... Done
> 
> W: Failed to fetch
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease Connection 
> failed [IP: 10.40.63.11 3128]
> 
> W: Some index files failed to download. They have been ignored, or old 
> ones used instead.
> 
> ?
> 
> Squid.conf :
> 
> acl localhost src 127.0.0.1/32 ::1
> 
> acl localhost src 10.0.0.0/8
> 
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> 
> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
> 
> #acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
> 
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> 
> acl localnet src FC00:18F:11AB::/48
> 
> ?
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80????? # http
> 
> acl Safe_ports port 443???? # https
> 
> acl CONNECT method CONNECT
> 
> #acl allowed_websites dstdomain "/etc/squid/web_allowed.acl"
> 
> http_access allow manager localhost
> 
> http_access deny manager
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localnet
> 
> http_access allow localhost
> 
> http_access deny all
> 
> http_port 3128
> 
> maximum_object_size 4096? MB
> 
> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
> 
> refresh_pattern (Release|Packages(.gz)*)$? ????0?????? 20%???? 2880
> 
> refresh_pattern .?????? 0?? 20% 4320
> 
> refresh_all_ims on
> 
> #debug_options ALL,5
> 
> ?
> 
> "/etc/squid/web_allowed.acl"
> 
> .ubuntu.com
> 
> .launchpad.net
> 
> ?
> 
> If I run
> 
> squidclient -h 10.40.63.11 -p 3128 -v? mgr:info
> 
> The reply are the following:
> 
> Request:
> 
> GET cache_object://10.40.63.11/info HTTP/1.0
> 
> Host: 10.40.63.11
> 
> User-Agent: squidclient/4.10
> 
> Accept: */*
> 
> Connection: close
> 
> ?
> 
> ?
> 
> .
> 
> HTTP/1.1 200 OK
> 
> Server: squid/4.10
> 
> Mime-Version: 1.0
> 
> Date: Mon, 04 Oct 2021 15:21:37 GMT
> 
> Content-Type: text/plain;charset=utf-8
> 
> Expires: Mon, 04 Oct 2021 15:21:37 GMT
> 
> Last-Modified: Mon, 04 Oct 2021 15:21:37 GMT
> 
> X-Cache: MISS from APTProxy
> 
> X-Cache-Lookup: MISS from APTProxy:20080
> 
> Via: 1.1 APTProxy (squid/4.10)
> 
> Connection: close
> 
> ?
> 
> Squid Object Cache: Version 4.10
> 
> Build Info: Ubuntu linux
> 
> Service Name: squid
> 
> Start Time:???? Mon, 04 Oct 2021 07:33:47 GMT
> 
> Current Time:?? Mon, 04 Oct 2021 15:21:37 GMT
> 
> Connection information for squid:
> 
> ??????? Number of clients accessing cache:????? 2
> 
> ??????? Number of HTTP requests received:?????? 20
> 
> ??????? Number of ICP messages received:??????? 0
> 
> ??????? Number of ICP messages sent:??? 0
> 
> ??????? Number of queued ICP replies:?? 0
> 
> ??????? Number of HTCP messages received:?????? 0
> 
> ??????? Number of HTCP messages sent:?? 0
> 
> ??????? Request failure ratio:?? 0.00
> 
> ??????? Average HTTP requests per minute since start:?? 0.0
> 
> ??????? Average ICP messages per minute since start:??? 0.0
> 
> ??????? Select loop called: 65913 times, 425.864 ms avg
> 
> Cache information for squid:
> 
> ??????? Hits as % of all requests:????? 5min: 0.0%, 60min: 0.0%
> 
> ??????? Hits as % of bytes sent:??????? 5min: -0.0%, 60min: -0.0%
> 
> ???? ???Memory hits as % of hit requests:?????? 5min: 0.0%, 60min: 
> 0.0%
> 
> ??????? Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
> 
> ??????? Storage Swap size:????? 0 KB
> 
> ??????? Storage Swap capacity:?? 0.0% used,? 0.0% free
> 
> ??????? Storage Mem size:?????? 212 KB
> 
> ??????? Storage Mem capacity:??? 0.1% used, 99.9% free
> 
> ??????? Mean Object Size:?????? 0.00 KB
> 
> ??????? Requests given to unlinkd:????? 0
> 
> Median Service Times (seconds)? 5 min??? 60 min:
> 
> ??????? HTTP Requests (All):?? 0.00000? 0.00000
> 
> ??????? Cache Misses:????????? 0.00000? 0.00000
> 
> ??????? Cache Hits:??????????? 0.00000? 0.00000
> 
> ??????? Near Hits:???????????? 0.00000? 0.00000
> 
> ??????? Not-Modified Replies:? 0.00000? 0.00000
> 
> ??????? DNS Lookups:?????????? 0.00000? 0.00000
> 
> ??????? ICP Queries:?????????? 0.00000? 0.00000
> 
> Resource usage for squid:
> 
> ??????? UP Time:??????? 28069.974 seconds
> 
> ??????? CPU Time:?????? 2.997 seconds
> 
> ??????? CPU Usage:????? 0.01%
> 
> ??????? CPU Usage, 5 minute avg:??????? 0.01%
> 
> ??????? CPU Usage, 60 minute avg:?????? 0.01%
> 
> ??????? Maximum Resident Size: 94016 KB
> 
> ??????? Page faults with physical i/o: 0
> 
> Memory accounted for:
> 
> ??????? Total accounted:????????? 655 KB
> 
> ??????? memPoolAlloc calls:???? 68992
> 
> ??????? memPoolFree calls:? ????69118
> 
> File descriptor usage for squid:
> 
> ??????? Maximum number of file descriptors:?? 1024
> 
> ??????? Largest file desc currently in use:???? 16
> 
> ??????? Number of file desc currently in use:??? 9
> 
> ??????? Files queued for open:?????????????????? 0
> 
> ??????? Available number of file descriptors: 1015
> 
> ??????? Reserved number of file descriptors:?? 100
> 
> ??????? Store Disk files open:?????????????????? 0
> 
> Internal Data Structures:
> 
> ??????????? 53 StoreEntries
> 
> ??????????? 53 StoreEntries with MemObjects
> 
> ???????????? 0 Hot Object Cache Items
> 
> ???????????? 0 on-disk objects
> 
> ?
> 
> Regards
> 
> Henning
> 
> ?
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Mon Oct  4 17:55:00 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Oct 2021 13:55:00 -0400
Subject: [squid-users] Squid do not reply
In-Reply-To: <5ABC601D16A6044981773D154D5C9F7102F4640948@XMail-DAG1.energy.local>
References: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
 <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>
 <5ABC601D16A6044981773D154D5C9F7102F4640948@XMail-DAG1.energy.local>
Message-ID: <78914dfc-745e-8ae1-669b-2c9ae24eaf49@measurement-factory.com>

On 10/4/21 1:18 PM, Henning Svane wrote:

> I can see some request to IPv6, but I do not have IPv6 as my ISP do not offer it yet.

It looks like your Squid does not know that your environment does not
support IPv6. This is either a bug in Squid OR a misconfigured Squid
build or deployment environment.


> I use dual stack to be future ready, as I hope my ISP sone will enter
> the modern age.

This is not my area of expertise, but if your environment is lying to
Squid that IPv6 is supported, then Squid may use IPv6 (and Squid will
fail when it does use IPv6). Consider disabling IPv6 in your environment
because IPv6 does not work yet, even though it is your ISP fault that it
does not.


HTH,

Alex.


> 1633311799.437     57 10.40.63.11 TCP_REFRESH_MODIFIED/200 114201 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633311799.470     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 100971 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633311799.503     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 114203 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633311799.768    224 10.40.63.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
> 1633311799.914    137 10.40.63.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
> 1633319549.416  30020 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633319549.425  30027 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633319579.447  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633319579.455  30028 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633319579.487     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114208 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633319579.528     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 100978 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633319579.566     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114210 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633319579.875    230 10.40.61.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
> 1633319580.046    156 10.40.61.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
> 1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633339173.636     21 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339173.646     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339173.657     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.784     11 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.795     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.806     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.814     41 10.40.61.11 TCP_MISS/304 351 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/91.189.95.85 -
> 1633339214.816      9 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339314.276 239404 10.40.61.11 NONE/503 0 CONNECT changelogs.ubuntu.com:443 - HIER_NONE/- -
> 1633351198.272 240065 10.40.63.11 NONE/503 0 CONNECT changelogs.ubuntu.com:443 - HIER_NONE/- -
> 1633353563.799  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633353593.823  30023 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633353593.845     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633353593.858     13 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633353593.869     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633360882.874  30025 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633360897.217      0 10.40.63.11 TCP_MISS/200 2581 GET cache_object://10.40.63.11/info - HIER_NONE/- text/plain
> 1633360912.905  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633360942.936  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633360942.959     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633360942.970     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633360942.981     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633363175.859  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633363205.889  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633363205.913     23 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633363205.926     12 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633363205.937     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633364224.510  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633364254.540  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633364254.563     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633364254.574     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633364254.585     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 
> Regards
> Henning
> -----Oprindelig meddelelse-----
> Fra: Alex Rousskov <rousskov at measurement-factory.com> 
> Sendt: 4. oktober 2021 18:57
> Til: Henning Svane <hsv at energy.dk>; squid-users at lists.squid-cache.org
> Emne: Re: [squid-users] Squid do not reply
> 
> On 10/4/21 12:32 PM, Henning Svane wrote:
> 
>> I have tried to install my first Squid proxy as a proxy for 
>> installation for ubuntu APT
>>
>> But the apt-get cannot access cache or get through the proxy.
>>
>> What have I missed or should have done otherwise ?
> 
> What apt-related lines do you see in Squid's access.log?
> 
> Alex.
> 
> 
> 
>> Most of the time when I run
>>
>> sudo apt-get update
>>
>> or other updates or install commands
>>
>> I get the reply:
>>
>> odin at APTProxy:~$ sudo apt-get update
>>
>> [sudo] password for odin:
>>
>> 0% [Waiting for headers]
>>
>> ?
>>
>> And it stay on 0% until it time out and sends this output:
>>
>> [sudo] password for odin:
>>
>> Err:1 http://dk.archive.ubuntu.com/ubuntu
>> <http://dk.archive.ubuntu.com/ubuntu> focal InRelease
>>
>> ? Connection failed [IP: 10.40.63.11 20080]
>>
>> Hit:2 http://dk.archive.ubuntu.com/ubuntu focal-updates InRelease
>>
>> Hit:3 http://dk.archive.ubuntu.com/ubuntu focal-backports InRelease
>>
>> Hit:4 http://dk.archive.ubuntu.com/ubuntu focal-security InRelease
>>
>> Reading package lists... Done
>>
>> W: Failed to fetch
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease Connection 
>> failed [IP: 10.40.63.11 3128]
>>
>> W: Some index files failed to download. They have been ignored, or old 
>> ones used instead.
>>
>> ?
>>
>> Squid.conf :
>>
>> acl localhost src 127.0.0.1/32 ::1
>>
>> acl localhost src 10.0.0.0/8
>>
>> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
>>
>> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
>>
>> #acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
>>
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>
>> acl localnet src FC00:18F:11AB::/48
>>
>> ?
>>
>> acl SSL_ports port 443
>>
>> acl Safe_ports port 80????? # http
>>
>> acl Safe_ports port 443???? # https
>>
>> acl CONNECT method CONNECT
>>
>> #acl allowed_websites dstdomain "/etc/squid/web_allowed.acl"
>>
>> http_access allow manager localhost
>>
>> http_access deny manager
>>
>> http_access deny !Safe_ports
>>
>> http_access deny CONNECT !SSL_ports
>>
>> http_access allow localnet
>>
>> http_access allow localhost
>>
>> http_access deny all
>>
>> http_port 3128
>>
>> maximum_object_size 4096? MB
>>
>> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
>>
>> refresh_pattern (Release|Packages(.gz)*)$? ????0?????? 20%???? 2880
>>
>> refresh_pattern .?????? 0?? 20% 4320
>>
>> refresh_all_ims on
>>
>> #debug_options ALL,5
>>
>> ?
>>
>> "/etc/squid/web_allowed.acl"
>>
>> .ubuntu.com
>>
>> .launchpad.net
>>
>> ?
>>
>> If I run
>>
>> squidclient -h 10.40.63.11 -p 3128 -v? mgr:info
>>
>> The reply are the following:
>>
>> Request:
>>
>> GET cache_object://10.40.63.11/info HTTP/1.0
>>
>> Host: 10.40.63.11
>>
>> User-Agent: squidclient/4.10
>>
>> Accept: */*
>>
>> Connection: close
>>
>> ?
>>
>> ?
>>
>> .
>>
>> HTTP/1.1 200 OK
>>
>> Server: squid/4.10
>>
>> Mime-Version: 1.0
>>
>> Date: Mon, 04 Oct 2021 15:21:37 GMT
>>
>> Content-Type: text/plain;charset=utf-8
>>
>> Expires: Mon, 04 Oct 2021 15:21:37 GMT
>>
>> Last-Modified: Mon, 04 Oct 2021 15:21:37 GMT
>>
>> X-Cache: MISS from APTProxy
>>
>> X-Cache-Lookup: MISS from APTProxy:20080
>>
>> Via: 1.1 APTProxy (squid/4.10)
>>
>> Connection: close
>>
>> ?
>>
>> Squid Object Cache: Version 4.10
>>
>> Build Info: Ubuntu linux
>>
>> Service Name: squid
>>
>> Start Time:???? Mon, 04 Oct 2021 07:33:47 GMT
>>
>> Current Time:?? Mon, 04 Oct 2021 15:21:37 GMT
>>
>> Connection information for squid:
>>
>> ??????? Number of clients accessing cache:????? 2
>>
>> ??????? Number of HTTP requests received:?????? 20
>>
>> ??????? Number of ICP messages received:??????? 0
>>
>> ??????? Number of ICP messages sent:??? 0
>>
>> ??????? Number of queued ICP replies:?? 0
>>
>> ??????? Number of HTCP messages received:?????? 0
>>
>> ??????? Number of HTCP messages sent:?? 0
>>
>> ??????? Request failure ratio:?? 0.00
>>
>> ??????? Average HTTP requests per minute since start:?? 0.0
>>
>> ??????? Average ICP messages per minute since start:??? 0.0
>>
>> ??????? Select loop called: 65913 times, 425.864 ms avg
>>
>> Cache information for squid:
>>
>> ??????? Hits as % of all requests:????? 5min: 0.0%, 60min: 0.0%
>>
>> ??????? Hits as % of bytes sent:??????? 5min: -0.0%, 60min: -0.0%
>>
>> ???? ???Memory hits as % of hit requests:?????? 5min: 0.0%, 60min: 
>> 0.0%
>>
>> ??????? Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
>>
>> ??????? Storage Swap size:????? 0 KB
>>
>> ??????? Storage Swap capacity:?? 0.0% used,? 0.0% free
>>
>> ??????? Storage Mem size:?????? 212 KB
>>
>> ??????? Storage Mem capacity:??? 0.1% used, 99.9% free
>>
>> ??????? Mean Object Size:?????? 0.00 KB
>>
>> ??????? Requests given to unlinkd:????? 0
>>
>> Median Service Times (seconds)? 5 min??? 60 min:
>>
>> ??????? HTTP Requests (All):?? 0.00000? 0.00000
>>
>> ??????? Cache Misses:????????? 0.00000? 0.00000
>>
>> ??????? Cache Hits:??????????? 0.00000? 0.00000
>>
>> ??????? Near Hits:???????????? 0.00000? 0.00000
>>
>> ??????? Not-Modified Replies:? 0.00000? 0.00000
>>
>> ??????? DNS Lookups:?????????? 0.00000? 0.00000
>>
>> ??????? ICP Queries:?????????? 0.00000? 0.00000
>>
>> Resource usage for squid:
>>
>> ??????? UP Time:??????? 28069.974 seconds
>>
>> ??????? CPU Time:?????? 2.997 seconds
>>
>> ??????? CPU Usage:????? 0.01%
>>
>> ??????? CPU Usage, 5 minute avg:??????? 0.01%
>>
>> ??????? CPU Usage, 60 minute avg:?????? 0.01%
>>
>> ??????? Maximum Resident Size: 94016 KB
>>
>> ??????? Page faults with physical i/o: 0
>>
>> Memory accounted for:
>>
>> ??????? Total accounted:????????? 655 KB
>>
>> ??????? memPoolAlloc calls:???? 68992
>>
>> ??????? memPoolFree calls:? ????69118
>>
>> File descriptor usage for squid:
>>
>> ??????? Maximum number of file descriptors:?? 1024
>>
>> ??????? Largest file desc currently in use:???? 16
>>
>> ??????? Number of file desc currently in use:??? 9
>>
>> ??????? Files queued for open:?????????????????? 0
>>
>> ??????? Available number of file descriptors: 1015
>>
>> ??????? Reserved number of file descriptors:?? 100
>>
>> ??????? Store Disk files open:?????????????????? 0
>>
>> Internal Data Structures:
>>
>> ??????????? 53 StoreEntries
>>
>> ??????????? 53 StoreEntries with MemObjects
>>
>> ???????????? 0 Hot Object Cache Items
>>
>> ???????????? 0 on-disk objects
>>
>> ?
>>
>> Regards
>>
>> Henning
>>
>> ?
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>



From hsv at energy.dk  Mon Oct  4 21:18:05 2021
From: hsv at energy.dk (Henning Svane)
Date: Mon, 4 Oct 2021 21:18:05 +0000
Subject: [squid-users] Squid do not reply
In-Reply-To: <78914dfc-745e-8ae1-669b-2c9ae24eaf49@measurement-factory.com>
References: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
 <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>
 <5ABC601D16A6044981773D154D5C9F7102F4640948@XMail-DAG1.energy.local>
 <78914dfc-745e-8ae1-669b-2c9ae24eaf49@measurement-factory.com>
Message-ID: <5ABC601D16A6044981773D154D5C9F7102F46412E5@XMail-DAG1.energy.local>

Hi Alex

You are right it is IPv6 there is the problem.
I search more on the problem and it shows that Squid as default only use IPv6
See here from status " sudo systemctl status squid"
Oct 04 20:39:01 APTProxy squid[51517]: Max Swap size: 0 KB
Oct 04 20:39:01 APTProxy squid[51517]: Using Least Load store dir selection
Oct 04 20:39:01 APTProxy squid[51517]: Current Directory is /
Oct 04 20:39:01 APTProxy squid[51517]: Finished loading MIME types and icons.
Oct 04 20:39:01 APTProxy squid[51517]: HTCP Disabled.
Oct 04 20:39:01 APTProxy squid[51517]: Pinger socket opened on FD 14
Oct 04 20:39:01 APTProxy squid[51517]: Squid plugin modules loaded: 0
Oct 04 20:39:01 APTProxy squid[51517]: Adaptation support is off.
Oct 04 20:39:01 APTProxy squid[51517]: Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 12 flags=9
Oct 04 20:39:02 APTProxy squid[51517]: storeLateRelease: released 0 objects

sudo netstat -ntulp | grep LISTEN
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
tcp6       0      0 :::8000                 :::*                    LISTEN
tcp6       0      0 :::3128                :::*                    LISTEN
tcp6       0      0 :::22                   :::*                    LISTEN

According to documentation this should get it to listen on both IPv4 and IPv6, but it do not work
X:X:X:X:Port
X:X:X:X:X:X:X:X:Port

I have now remove dual stack as it is not possible to get IPv4 to work with dual stack.
It looks like works as design, but think that somebody need to look at the design as this is not smart.

If anybody knows of a way to get this to work with dual stack. Please let me know.

I have set it up to use IPv6 instead but that work not, get an error 403  Forbidden.
I cannot see why it is forbidden, I have allowed the IPv6 ip range.

But force it to use pure IPv4 works.

Regards
Henning

-----Oprindelig meddelelse-----
Fra: Alex Rousskov <rousskov at measurement-factory.com> 
Sendt: 4. oktober 2021 19:55
Til: Henning Svane <hsv at energy.dk>; squid-users at lists.squid-cache.org
Emne: Re: SV: [squid-users] Squid do not reply

On 10/4/21 1:18 PM, Henning Svane wrote:

> I can see some request to IPv6, but I do not have IPv6 as my ISP do not offer it yet.

It looks like your Squid does not know that your environment does not support IPv6. This is either a bug in Squid OR a misconfigured Squid build or deployment environment.


> I use dual stack to be future ready, as I hope my ISP sone will enter 
> the modern age.

This is not my area of expertise, but if your environment is lying to Squid that IPv6 is supported, then Squid may use IPv6 (and Squid will fail when it does use IPv6). Consider disabling IPv6 in your environment because IPv6 does not work yet, even though it is your ISP fault that it does not.


HTH,

Alex.


> 1633311799.437     57 10.40.63.11 TCP_REFRESH_MODIFIED/200 114201 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633311799.470     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 100971 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633311799.503     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 114203 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633311799.768    224 10.40.63.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
> 1633311799.914    137 10.40.63.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
> 1633319549.416  30020 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633319549.425  30027 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
> http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRele
> ase - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633319579.447  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633319579.455  30028 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633319579.487     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114208 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633319579.528     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 100978 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633319579.566     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114210 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
> 1633319579.875    230 10.40.61.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
> 1633319580.046    156 10.40.61.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
> 1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
> http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRele
> ase - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
> http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRele
> ase - HIER_DIRECT/2001:67c:1560:8008::19 -
> 1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633339173.636     21 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339173.646     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339173.657     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.784     11 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.795     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.806     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339214.814     41 10.40.61.11 TCP_MISS/304 351 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/91.189.95.85 -
> 1633339214.816      9 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633339314.276 239404 10.40.61.11 NONE/503 0 CONNECT 
> changelogs.ubuntu.com:443 - HIER_NONE/- -
> 1633351198.272 240065 10.40.63.11 NONE/503 0 CONNECT 
> changelogs.ubuntu.com:443 - HIER_NONE/- -
> 1633353563.799  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633353593.823  30023 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633353593.845     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633353593.858     13 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633353593.869     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633360882.874  30025 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633360897.217      0 10.40.63.11 TCP_MISS/200 2581 GET cache_object://10.40.63.11/info - HIER_NONE/- text/plain
> 1633360912.905  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633360942.936  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633360942.959     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633360942.970     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633360942.981     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633363175.859  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633363205.889  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633363205.913     23 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633363205.926     12 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633363205.937     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633364224.510  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
> HIER_DIRECT/2001:878:346::116 -
> 1633364254.540  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
> 1633364254.563     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633364254.574     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
> 1633364254.585     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
> 
> Regards
> Henning
> -----Oprindelig meddelelse-----
> Fra: Alex Rousskov <rousskov at measurement-factory.com>
> Sendt: 4. oktober 2021 18:57
> Til: Henning Svane <hsv at energy.dk>; squid-users at lists.squid-cache.org
> Emne: Re: [squid-users] Squid do not reply
> 
> On 10/4/21 12:32 PM, Henning Svane wrote:
> 
>> I have tried to install my first Squid proxy as a proxy for 
>> installation for ubuntu APT
>>
>> But the apt-get cannot access cache or get through the proxy.
>>
>> What have I missed or should have done otherwise ?
> 
> What apt-related lines do you see in Squid's access.log?
> 
> Alex.
> 
> 
> 
>> Most of the time when I run
>>
>> sudo apt-get update
>>
>> or other updates or install commands
>>
>> I get the reply:
>>
>> odin at APTProxy:~$ sudo apt-get update
>>
>> [sudo] password for odin:
>>
>> 0% [Waiting for headers]
>>
>> ?
>>
>> And it stay on 0% until it time out and sends this output:
>>
>> [sudo] password for odin:
>>
>> Err:1 http://dk.archive.ubuntu.com/ubuntu
>> <http://dk.archive.ubuntu.com/ubuntu> focal InRelease
>>
>> ? Connection failed [IP: 10.40.63.11 20080]
>>
>> Hit:2 http://dk.archive.ubuntu.com/ubuntu focal-updates InRelease
>>
>> Hit:3 http://dk.archive.ubuntu.com/ubuntu focal-backports InRelease
>>
>> Hit:4 http://dk.archive.ubuntu.com/ubuntu focal-security InRelease
>>
>> Reading package lists... Done
>>
>> W: Failed to fetch
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease Connection 
>> failed [IP: 10.40.63.11 3128]
>>
>> W: Some index files failed to download. They have been ignored, or 
>> old ones used instead.
>>
>> ?
>>
>> Squid.conf :
>>
>> acl localhost src 127.0.0.1/32 ::1
>>
>> acl localhost src 10.0.0.0/8
>>
>> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
>>
>> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
>>
>> #acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
>>
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>
>> acl localnet src FC00:18F:11AB::/48
>>
>> ?
>>
>> acl SSL_ports port 443
>>
>> acl Safe_ports port 80????? # http
>>
>> acl Safe_ports port 443???? # https
>>
>> acl CONNECT method CONNECT
>>
>> #acl allowed_websites dstdomain "/etc/squid/web_allowed.acl"
>>
>> http_access allow manager localhost
>>
>> http_access deny manager
>>
>> http_access deny !Safe_ports
>>
>> http_access deny CONNECT !SSL_ports
>>
>> http_access allow localnet
>>
>> http_access allow localhost
>>
>> http_access deny all
>>
>> http_port 3128
>>
>> maximum_object_size 4096? MB
>>
>> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
>>
>> refresh_pattern (Release|Packages(.gz)*)$? ????0?????? 20%???? 2880
>>
>> refresh_pattern .?????? 0?? 20% 4320
>>
>> refresh_all_ims on
>>
>> #debug_options ALL,5
>>
>> ?
>>
>> "/etc/squid/web_allowed.acl"
>>
>> .ubuntu.com
>>
>> .launchpad.net
>>
>> ?
>>
>> If I run
>>
>> squidclient -h 10.40.63.11 -p 3128 -v? mgr:info
>>
>> The reply are the following:
>>
>> Request:
>>
>> GET cache_object://10.40.63.11/info HTTP/1.0
>>
>> Host: 10.40.63.11
>>
>> User-Agent: squidclient/4.10
>>
>> Accept: */*
>>
>> Connection: close
>>
>> ?
>>
>> ?
>>
>> .
>>
>> HTTP/1.1 200 OK
>>
>> Server: squid/4.10
>>
>> Mime-Version: 1.0
>>
>> Date: Mon, 04 Oct 2021 15:21:37 GMT
>>
>> Content-Type: text/plain;charset=utf-8
>>
>> Expires: Mon, 04 Oct 2021 15:21:37 GMT
>>
>> Last-Modified: Mon, 04 Oct 2021 15:21:37 GMT
>>
>> X-Cache: MISS from APTProxy
>>
>> X-Cache-Lookup: MISS from APTProxy:20080
>>
>> Via: 1.1 APTProxy (squid/4.10)
>>
>> Connection: close
>>
>> ?
>>
>> Squid Object Cache: Version 4.10
>>
>> Build Info: Ubuntu linux
>>
>> Service Name: squid
>>
>> Start Time:???? Mon, 04 Oct 2021 07:33:47 GMT
>>
>> Current Time:?? Mon, 04 Oct 2021 15:21:37 GMT
>>
>> Connection information for squid:
>>
>> ??????? Number of clients accessing cache:????? 2
>>
>> ??????? Number of HTTP requests received:?????? 20
>>
>> ??????? Number of ICP messages received:??????? 0
>>
>> ??????? Number of ICP messages sent:??? 0
>>
>> ??????? Number of queued ICP replies:?? 0
>>
>> ??????? Number of HTCP messages received:?????? 0
>>
>> ??????? Number of HTCP messages sent:?? 0
>>
>> ??????? Request failure ratio:?? 0.00
>>
>> ??????? Average HTTP requests per minute since start:?? 0.0
>>
>> ??????? Average ICP messages per minute since start:??? 0.0
>>
>> ??????? Select loop called: 65913 times, 425.864 ms avg
>>
>> Cache information for squid:
>>
>> ??????? Hits as % of all requests:????? 5min: 0.0%, 60min: 0.0%
>>
>> ??????? Hits as % of bytes sent:??????? 5min: -0.0%, 60min: -0.0%
>>
>> ???? ???Memory hits as % of hit requests:?????? 5min: 0.0%, 60min: 
>> 0.0%
>>
>> ??????? Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
>>
>> ??????? Storage Swap size:????? 0 KB
>>
>> ??????? Storage Swap capacity:?? 0.0% used,? 0.0% free
>>
>> ??????? Storage Mem size:?????? 212 KB
>>
>> ??????? Storage Mem capacity:??? 0.1% used, 99.9% free
>>
>> ??????? Mean Object Size:?????? 0.00 KB
>>
>> ??????? Requests given to unlinkd:????? 0
>>
>> Median Service Times (seconds)? 5 min??? 60 min:
>>
>> ??????? HTTP Requests (All):?? 0.00000? 0.00000
>>
>> ??????? Cache Misses:????????? 0.00000? 0.00000
>>
>> ??????? Cache Hits:??????????? 0.00000? 0.00000
>>
>> ??????? Near Hits:???????????? 0.00000? 0.00000
>>
>> ??????? Not-Modified Replies:? 0.00000? 0.00000
>>
>> ??????? DNS Lookups:?????????? 0.00000? 0.00000
>>
>> ??????? ICP Queries:?????????? 0.00000? 0.00000
>>
>> Resource usage for squid:
>>
>> ??????? UP Time:??????? 28069.974 seconds
>>
>> ??????? CPU Time:?????? 2.997 seconds
>>
>> ??????? CPU Usage:????? 0.01%
>>
>> ??????? CPU Usage, 5 minute avg:??????? 0.01%
>>
>> ??????? CPU Usage, 60 minute avg:?????? 0.01%
>>
>> ??????? Maximum Resident Size: 94016 KB
>>
>> ??????? Page faults with physical i/o: 0
>>
>> Memory accounted for:
>>
>> ??????? Total accounted:????????? 655 KB
>>
>> ??????? memPoolAlloc calls:???? 68992
>>
>> ??????? memPoolFree calls:? ????69118
>>
>> File descriptor usage for squid:
>>
>> ??????? Maximum number of file descriptors:?? 1024
>>
>> ??????? Largest file desc currently in use:???? 16
>>
>> ??????? Number of file desc currently in use:??? 9
>>
>> ??????? Files queued for open:?????????????????? 0
>>
>> ??????? Available number of file descriptors: 1015
>>
>> ??????? Reserved number of file descriptors:?? 100
>>
>> ??????? Store Disk files open:?????????????????? 0
>>
>> Internal Data Structures:
>>
>> ??????????? 53 StoreEntries
>>
>> ??????????? 53 StoreEntries with MemObjects
>>
>> ???????????? 0 Hot Object Cache Items
>>
>> ???????????? 0 on-disk objects
>>
>> ?
>>
>> Regards
>>
>> Henning
>>
>> ?
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>


From rousskov at measurement-factory.com  Tue Oct  5 01:37:09 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 4 Oct 2021 21:37:09 -0400
Subject: [squid-users] Squid do not reply
In-Reply-To: <5ABC601D16A6044981773D154D5C9F7102F46412E5@XMail-DAG1.energy.local>
References: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
 <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>
 <5ABC601D16A6044981773D154D5C9F7102F4640948@XMail-DAG1.energy.local>
 <78914dfc-745e-8ae1-669b-2c9ae24eaf49@measurement-factory.com>
 <5ABC601D16A6044981773D154D5C9F7102F46412E5@XMail-DAG1.energy.local>
Message-ID: <31cac38c-0822-bce0-5a07-f2d6d8bf2ca8@measurement-factory.com>

On 10/4/21 5:18 PM, Henning Svane wrote:


> I search more on the problem and it shows that Squid as default only use IPv6

I really doubt that. Your access.log records seem to confirm my belief
that your Squid was listening on an IPv4 address (at least) -- Squid
would not see (and log) requests from IPv4 clients like 10.40.61.11 if
it were listening on IPv6 only.

Unfortunately, the exact meaning of "::" and "[::]" addresses (and even
"tcp6 ... ::" lines in netstat!) depends on the environment. Lots of
folks (mis)interpret those symbols as "any IPv6 address". That
interpretation is correct in some environments and wrong in many others.
Here is one detailed answer that enumerates a few cases, including what
looks like yours (except for an sshd instead of Squid):
https://serverfault.com/a/39561

You can, of course, verify whether Squid listens on an IPv4 address by
connecting to port 3128 on that IPv4 address using telnet or another
program.


Please note that Squid listening address has nothing to do with Squid
outgoing address. That is why I suggested that you disable
(misconfigured?) IPv6 in your environment so that Squid does not try to
forward a request it received via IPv4 using a Squid-server connection
with IPv6 addresses.


HTH,

Alex.


> See here from status " sudo systemctl status squid"
> Oct 04 20:39:01 APTProxy squid[51517]: Max Swap size: 0 KB
> Oct 04 20:39:01 APTProxy squid[51517]: Using Least Load store dir selection
> Oct 04 20:39:01 APTProxy squid[51517]: Current Directory is /
> Oct 04 20:39:01 APTProxy squid[51517]: Finished loading MIME types and icons.
> Oct 04 20:39:01 APTProxy squid[51517]: HTCP Disabled.
> Oct 04 20:39:01 APTProxy squid[51517]: Pinger socket opened on FD 14
> Oct 04 20:39:01 APTProxy squid[51517]: Squid plugin modules loaded: 0
> Oct 04 20:39:01 APTProxy squid[51517]: Adaptation support is off.
> Oct 04 20:39:01 APTProxy squid[51517]: Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 12 flags=9
> Oct 04 20:39:02 APTProxy squid[51517]: storeLateRelease: released 0 objects
> 
> sudo netstat -ntulp | grep LISTEN
> tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN
> tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN
> tcp6       0      0 :::8000                 :::*                    LISTEN
> tcp6       0      0 :::3128                :::*                    LISTEN
> tcp6       0      0 :::22                   :::*                    LISTEN
> 
> According to documentation this should get it to listen on both IPv4 and IPv6, but it do not work
> X:X:X:X:Port
> X:X:X:X:X:X:X:X:Port
> 
> I have now remove dual stack as it is not possible to get IPv4 to work with dual stack.
> It looks like works as design, but think that somebody need to look at the design as this is not smart.
> 
> If anybody knows of a way to get this to work with dual stack. Please let me know.
> 
> I have set it up to use IPv6 instead but that work not, get an error 403  Forbidden.
> I cannot see why it is forbidden, I have allowed the IPv6 ip range.
> 
> But force it to use pure IPv4 works.
> 
> Regards
> Henning
> 
> -----Oprindelig meddelelse-----
> Fra: Alex Rousskov <rousskov at measurement-factory.com> 
> Sendt: 4. oktober 2021 19:55
> Til: Henning Svane <hsv at energy.dk>; squid-users at lists.squid-cache.org
> Emne: Re: SV: [squid-users] Squid do not reply
> 
> On 10/4/21 1:18 PM, Henning Svane wrote:
> 
>> I can see some request to IPv6, but I do not have IPv6 as my ISP do not offer it yet.
> 
> It looks like your Squid does not know that your environment does not support IPv6. This is either a bug in Squid OR a misconfigured Squid build or deployment environment.
> 
> 
>> I use dual stack to be future ready, as I hope my ISP sone will enter 
>> the modern age.
> 
> This is not my area of expertise, but if your environment is lying to Squid that IPv6 is supported, then Squid may use IPv6 (and Squid will fail when it does use IPv6). Consider disabling IPv6 in your environment because IPv6 does not work yet, even though it is your ISP fault that it does not.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>> 1633311799.437     57 10.40.63.11 TCP_REFRESH_MODIFIED/200 114201 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
>> 1633311799.470     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 100971 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
>> 1633311799.503     31 10.40.63.11 TCP_REFRESH_MODIFIED/200 114203 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
>> 1633311799.768    224 10.40.63.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
>> 1633311799.914    137 10.40.63.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
>> 1633319549.416  30020 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633319549.425  30027 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
>> http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRele
>> ase - HIER_DIRECT/2001:67c:1560:8008::19 -
>> 1633319579.447  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633319579.455  30028 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:67c:1560:8008::19 -
>> 1633319579.487     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114208 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 text/plain
>> 1633319579.528     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 100978 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 text/plain
>> 1633319579.566     37 10.40.61.11 TCP_REFRESH_UNMODIFIED/200 114210 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 text/plain
>> 1633319579.875    230 10.40.61.11 TCP_MISS/200 1256699 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/main/binary-amd64/by-hash/SHA256/7cc00b82849880eab961e81146ad6e23770386b5>
>> 1633319580.046    156 10.40.61.11 TCP_MISS/200 863985 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/universe/binary-amd64/by-hash/SHA256/1711a00ad226337b7ee774ec39dc4abd7841d>
>> 1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
>> http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRele
>> ase - HIER_DIRECT/2001:67c:1560:8008::19 -
>> 1633339143.582  30030 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET 
>> http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRele
>> ase - HIER_DIRECT/2001:67c:1560:8008::19 -
>> 1633339173.613  30029 10.40.61.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
>> 1633339173.636     21 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339173.646     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339173.657     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339214.784     11 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339214.795     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339214.806     10 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339214.814     41 10.40.61.11 TCP_MISS/304 351 GET http://ppa.launchpad.net/vbernat/haproxy-2.4/ubuntu/dists/focal/InRelease - HIER_DIRECT/91.189.95.85 -
>> 1633339214.816      9 10.40.61.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633339314.276 239404 10.40.61.11 NONE/503 0 CONNECT 
>> changelogs.ubuntu.com:443 - HIER_NONE/- -
>> 1633351198.272 240065 10.40.63.11 NONE/503 0 CONNECT 
>> changelogs.ubuntu.com:443 - HIER_NONE/- -
>> 1633353563.799  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633353593.823  30023 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
>> 1633353593.845     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633353593.858     13 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633353593.869     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633360882.874  30025 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
>> 1633360897.217      0 10.40.63.11 TCP_MISS/200 2581 GET cache_object://10.40.63.11/info - HIER_NONE/- text/plain
>> 1633360912.905  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633360942.936  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/2001:878:346::116 -
>> 1633360942.959     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633360942.970     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633360942.981     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633363175.859  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633363205.889  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
>> 1633363205.913     23 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633363205.926     12 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633363205.937     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633364224.510  30029 10.40.63.11 TCP_MISS_ABORTED/000 0 GET 
>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - 
>> HIER_DIRECT/2001:878:346::116 -
>> 1633364254.540  30030 10.40.63.11 TCP_MISS_ABORTED/000 0 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease - HIER_DIRECT/2001:878:346::116 -
>> 1633364254.563     22 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-updates/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633364254.574     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-backports/InRelease - HIER_DIRECT/130.225.254.116 -
>> 1633364254.585     10 10.40.63.11 TCP_MISS/304 355 GET http://dk.archive.ubuntu.com/ubuntu/dists/focal-security/InRelease - HIER_DIRECT/130.225.254.116 -
>>
>> Regards
>> Henning
>> -----Oprindelig meddelelse-----
>> Fra: Alex Rousskov <rousskov at measurement-factory.com>
>> Sendt: 4. oktober 2021 18:57
>> Til: Henning Svane <hsv at energy.dk>; squid-users at lists.squid-cache.org
>> Emne: Re: [squid-users] Squid do not reply
>>
>> On 10/4/21 12:32 PM, Henning Svane wrote:
>>
>>> I have tried to install my first Squid proxy as a proxy for 
>>> installation for ubuntu APT
>>>
>>> But the apt-get cannot access cache or get through the proxy.
>>>
>>> What have I missed or should have done otherwise ?
>>
>> What apt-related lines do you see in Squid's access.log?
>>
>> Alex.
>>
>>
>>
>>> Most of the time when I run
>>>
>>> sudo apt-get update
>>>
>>> or other updates or install commands
>>>
>>> I get the reply:
>>>
>>> odin at APTProxy:~$ sudo apt-get update
>>>
>>> [sudo] password for odin:
>>>
>>> 0% [Waiting for headers]
>>>
>>> ?
>>>
>>> And it stay on 0% until it time out and sends this output:
>>>
>>> [sudo] password for odin:
>>>
>>> Err:1 http://dk.archive.ubuntu.com/ubuntu
>>> <http://dk.archive.ubuntu.com/ubuntu> focal InRelease
>>>
>>> ? Connection failed [IP: 10.40.63.11 20080]
>>>
>>> Hit:2 http://dk.archive.ubuntu.com/ubuntu focal-updates InRelease
>>>
>>> Hit:3 http://dk.archive.ubuntu.com/ubuntu focal-backports InRelease
>>>
>>> Hit:4 http://dk.archive.ubuntu.com/ubuntu focal-security InRelease
>>>
>>> Reading package lists... Done
>>>
>>> W: Failed to fetch
>>> http://dk.archive.ubuntu.com/ubuntu/dists/focal/InRelease Connection 
>>> failed [IP: 10.40.63.11 3128]
>>>
>>> W: Some index files failed to download. They have been ignored, or 
>>> old ones used instead.
>>>
>>> ?
>>>
>>> Squid.conf :
>>>
>>> acl localhost src 127.0.0.1/32 ::1
>>>
>>> acl localhost src 10.0.0.0/8
>>>
>>> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
>>>
>>> acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
>>>
>>> #acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
>>>
>>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>>
>>> acl localnet src FC00:18F:11AB::/48
>>>
>>> ?
>>>
>>> acl SSL_ports port 443
>>>
>>> acl Safe_ports port 80????? # http
>>>
>>> acl Safe_ports port 443???? # https
>>>
>>> acl CONNECT method CONNECT
>>>
>>> #acl allowed_websites dstdomain "/etc/squid/web_allowed.acl"
>>>
>>> http_access allow manager localhost
>>>
>>> http_access deny manager
>>>
>>> http_access deny !Safe_ports
>>>
>>> http_access deny CONNECT !SSL_ports
>>>
>>> http_access allow localnet
>>>
>>> http_access allow localhost
>>>
>>> http_access deny all
>>>
>>> http_port 3128
>>>
>>> maximum_object_size 4096? MB
>>>
>>> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
>>>
>>> refresh_pattern (Release|Packages(.gz)*)$? ????0?????? 20%???? 2880
>>>
>>> refresh_pattern .?????? 0?? 20% 4320
>>>
>>> refresh_all_ims on
>>>
>>> #debug_options ALL,5
>>>
>>> ?
>>>
>>> "/etc/squid/web_allowed.acl"
>>>
>>> .ubuntu.com
>>>
>>> .launchpad.net
>>>
>>> ?
>>>
>>> If I run
>>>
>>> squidclient -h 10.40.63.11 -p 3128 -v? mgr:info
>>>
>>> The reply are the following:
>>>
>>> Request:
>>>
>>> GET cache_object://10.40.63.11/info HTTP/1.0
>>>
>>> Host: 10.40.63.11
>>>
>>> User-Agent: squidclient/4.10
>>>
>>> Accept: */*
>>>
>>> Connection: close
>>>
>>> ?
>>>
>>> ?
>>>
>>> .
>>>
>>> HTTP/1.1 200 OK
>>>
>>> Server: squid/4.10
>>>
>>> Mime-Version: 1.0
>>>
>>> Date: Mon, 04 Oct 2021 15:21:37 GMT
>>>
>>> Content-Type: text/plain;charset=utf-8
>>>
>>> Expires: Mon, 04 Oct 2021 15:21:37 GMT
>>>
>>> Last-Modified: Mon, 04 Oct 2021 15:21:37 GMT
>>>
>>> X-Cache: MISS from APTProxy
>>>
>>> X-Cache-Lookup: MISS from APTProxy:20080
>>>
>>> Via: 1.1 APTProxy (squid/4.10)
>>>
>>> Connection: close
>>>
>>> ?
>>>
>>> Squid Object Cache: Version 4.10
>>>
>>> Build Info: Ubuntu linux
>>>
>>> Service Name: squid
>>>
>>> Start Time:???? Mon, 04 Oct 2021 07:33:47 GMT
>>>
>>> Current Time:?? Mon, 04 Oct 2021 15:21:37 GMT
>>>
>>> Connection information for squid:
>>>
>>> ??????? Number of clients accessing cache:????? 2
>>>
>>> ??????? Number of HTTP requests received:?????? 20
>>>
>>> ??????? Number of ICP messages received:??????? 0
>>>
>>> ??????? Number of ICP messages sent:??? 0
>>>
>>> ??????? Number of queued ICP replies:?? 0
>>>
>>> ??????? Number of HTCP messages received:?????? 0
>>>
>>> ??????? Number of HTCP messages sent:?? 0
>>>
>>> ??????? Request failure ratio:?? 0.00
>>>
>>> ??????? Average HTTP requests per minute since start:?? 0.0
>>>
>>> ??????? Average ICP messages per minute since start:??? 0.0
>>>
>>> ??????? Select loop called: 65913 times, 425.864 ms avg
>>>
>>> Cache information for squid:
>>>
>>> ??????? Hits as % of all requests:????? 5min: 0.0%, 60min: 0.0%
>>>
>>> ??????? Hits as % of bytes sent:??????? 5min: -0.0%, 60min: -0.0%
>>>
>>> ???? ???Memory hits as % of hit requests:?????? 5min: 0.0%, 60min: 
>>> 0.0%
>>>
>>> ??????? Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
>>>
>>> ??????? Storage Swap size:????? 0 KB
>>>
>>> ??????? Storage Swap capacity:?? 0.0% used,? 0.0% free
>>>
>>> ??????? Storage Mem size:?????? 212 KB
>>>
>>> ??????? Storage Mem capacity:??? 0.1% used, 99.9% free
>>>
>>> ??????? Mean Object Size:?????? 0.00 KB
>>>
>>> ??????? Requests given to unlinkd:????? 0
>>>
>>> Median Service Times (seconds)? 5 min??? 60 min:
>>>
>>> ??????? HTTP Requests (All):?? 0.00000? 0.00000
>>>
>>> ??????? Cache Misses:????????? 0.00000? 0.00000
>>>
>>> ??????? Cache Hits:??????????? 0.00000? 0.00000
>>>
>>> ??????? Near Hits:???????????? 0.00000? 0.00000
>>>
>>> ??????? Not-Modified Replies:? 0.00000? 0.00000
>>>
>>> ??????? DNS Lookups:?????????? 0.00000? 0.00000
>>>
>>> ??????? ICP Queries:?????????? 0.00000? 0.00000
>>>
>>> Resource usage for squid:
>>>
>>> ??????? UP Time:??????? 28069.974 seconds
>>>
>>> ??????? CPU Time:?????? 2.997 seconds
>>>
>>> ??????? CPU Usage:????? 0.01%
>>>
>>> ??????? CPU Usage, 5 minute avg:??????? 0.01%
>>>
>>> ??????? CPU Usage, 60 minute avg:?????? 0.01%
>>>
>>> ??????? Maximum Resident Size: 94016 KB
>>>
>>> ??????? Page faults with physical i/o: 0
>>>
>>> Memory accounted for:
>>>
>>> ??????? Total accounted:????????? 655 KB
>>>
>>> ??????? memPoolAlloc calls:???? 68992
>>>
>>> ??????? memPoolFree calls:? ????69118
>>>
>>> File descriptor usage for squid:
>>>
>>> ??????? Maximum number of file descriptors:?? 1024
>>>
>>> ??????? Largest file desc currently in use:???? 16
>>>
>>> ??????? Number of file desc currently in use:??? 9
>>>
>>> ??????? Files queued for open:?????????????????? 0
>>>
>>> ??????? Available number of file descriptors: 1015
>>>
>>> ??????? Reserved number of file descriptors:?? 100
>>>
>>> ??????? Store Disk files open:?????????????????? 0
>>>
>>> Internal Data Structures:
>>>
>>> ??????????? 53 StoreEntries
>>>
>>> ??????????? 53 StoreEntries with MemObjects
>>>
>>> ???????????? 0 Hot Object Cache Items
>>>
>>> ???????????? 0 on-disk objects
>>>
>>> ?
>>>
>>> Regards
>>>
>>> Henning
>>>
>>> ?
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
> 



From squid3 at treenet.co.nz  Tue Oct  5 08:42:28 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Oct 2021 21:42:28 +1300
Subject: [squid-users] Squid do not reply
In-Reply-To: <31cac38c-0822-bce0-5a07-f2d6d8bf2ca8@measurement-factory.com>
References: <5ABC601D16A6044981773D154D5C9F7102F4640639@XMail-DAG1.energy.local>
 <437a3b5e-1f01-38e2-ba60-6c841817273a@measurement-factory.com>
 <5ABC601D16A6044981773D154D5C9F7102F4640948@XMail-DAG1.energy.local>
 <78914dfc-745e-8ae1-669b-2c9ae24eaf49@measurement-factory.com>
 <5ABC601D16A6044981773D154D5C9F7102F46412E5@XMail-DAG1.energy.local>
 <31cac38c-0822-bce0-5a07-f2d6d8bf2ca8@measurement-factory.com>
Message-ID: <aa7b8904-56eb-6528-2751-c2e55208fcaa@treenet.co.nz>

On 5/10/21 2:37 pm, Alex Rousskov wrote:
> On 10/4/21 5:18 PM, Henning Svane wrote:
> 
> 
>> I search more on the problem and it shows that Squid as default only use IPv6
> 
> I really doubt that. Your access.log records seem to confirm my belief
> that your Squid was listening on an IPv4 address (at least) -- Squid
> would not see (and log) requests from IPv4 clients like 10.40.61.11 if
> it were listening on IPv6 only.
> 
> Unfortunately, the exact meaning of "::" and "[::]" addresses (and even
> "tcp6 ... ::" lines in netstat!) depends on the environment. Lots of
> folks (mis)interpret those symbols as "any IPv6 address". That
> interpretation is correct in some environments and wrong in many others.

No. "any IPv6 address" is the literal definition of "::".

The situation is that some (dual-stack, hybrid-stack) environments treat 
IPv4 as a part of IPv6 and others (split-stack) do not. Squid has been 
well tested in this area. It correctly opens the appropriate ports for 
the stack type.
  Ubuntu is dual-stack so the IPv6 port ("[::]:3128" is the correct one 
to open and use for both IPv4 and IPv6 clients.


> Here is one detailed answer that enumerates a few cases, including what
> looks like yours (except for an sshd instead of Squid):
> https://serverfault.com/a/39561
> 

That documentation is about Windows Vista which have a split-stack where 
IPv4 needs a dedicated IPv4-only port opened. As I mentioned above, 
Squid is well tested for these and will properly open both IPv6 and IPv4 
ports on split-stack OS.


> You can, of course, verify whether Squid listens on an IPv4 address by
> connecting to port 3128 on that IPv4 address using telnet or another
> program.
> 
> 
> Please note that Squid listening address has nothing to do with Squid
> outgoing address. That is why I suggested that you disable
> (misconfigured?) IPv6 in your environment so that Squid does not try to
> forward a request it received via IPv4 using a Squid-server connection
> with IPv6 addresses.
> 

To clarify what Alex is suggesting is one of these;

Either,

   Add firewall rules that block public IPv6 traffic trying to be sent 
to your ISP. That means ip6tables rules to deny all traffic (not 
iptables which is IPv4-only).
  - this way Squid will attempt to connect to those IPv6 servers, but 
when rejected should failover to IPv4 ones. If you get ISP IPv6 
connectivity you just need to change firewall rules and Squid starts 
working with those.


Or,

  Configure the OS network interface not to allocate IPv6 addresses even 
for localhost.
  - this way Squid will detect the lack of IPv6 functionality on the 
machine and disable all use of IPv6 - incoming, internal uses, and 
outgoing traffic. If you get ISP IPv6 connectivity you need to change 
both the OS interface config and then restart Squid to pickup the change 
of capability.


However. Before you try those it would be helpful to test whether IPv4 
is actually going to work to those servers. The issue may not be IP 
related at all.

I suggest adding "dns_v4_first on" to squid.conf. Squid-4 and older will 
first try connecting to IPv4 servers, only trying IPv6 ones if those 
fail. You will then only see IPv6 addresses for servers which are not 
available over IPv4. (Squid-5+ work differently and do not provide this 
option).


One possible bug you may be encountering is a known APT issue. When APT 
hands the proxy a URL with raw-IPv6 it uses the 5xx/4xx status response 
as final. It does not try to reach a different IP address of the server 
it was connecting to. There is nothing we can do in Squid to fix that. 
Efforts to get it fixed in APT have so far not been successful.


Finally, please make sure that ICMP and ICMPv6 are functional in your 
environment and do anything you can to ensure your ISP has it functional 
as well. Yes there are *some* packet types that need to be blocked for 
security, but the rest MUST NOT.

Squid and many other software rely on ICMP control signals indicating 
that connections have failed so they can re-try using an different IP 
address. This is important even for retry with two IPv4 addresses. Your 
router or ISPs router should be delivering those signals to Squid, 
otherwise the connection retry may not even be attempted.


Amos


From squid3 at treenet.co.nz  Tue Oct  5 08:49:47 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Oct 2021 21:49:47 +1300
Subject: [squid-users] Portal Splash Page - exceptions
In-Reply-To: <2184f568-3526-0466-61ef-caa326d15ba7@dsi-as.de>
References: <2184f568-3526-0466-61ef-caa326d15ba7@dsi-as.de>
Message-ID: <fa3a1d3e-0cdf-4da5-b365-05d9286d88ea@treenet.co.nz>

On 4/10/21 9:53 pm, MATYAS, Tibor wrote:
> Hello List,
> 
> I have the following situation: Squid 4 is working in non-transparent
> mode, with Portal Splash Page
> https://wiki.squid-cache.org/ConfigExamples/Portal/Splash
> configured. This is a nice feature, the users must regularly accept the
> internet usage policy.
> 
> Since we have our new VoIP/PBX solution, the connection breakdowns
> caused by the session timeout / reaccept policy are very annoying.

What makes you think the VoIP / PBX has anything to do with HTTP traffic 
through Squid?

If (as I suspect), you have the VoIP software configured to use CONNECT 
tunnels through the proxy. You may find it better to move the voice 
channels to direct connections.


> I would like to create exceptions for specific sites/domains. Is this
> possible?

Of course. Squid access controls are a boolean language. You just have 
to define clearly what it is checking for and what is to happen to 
traffic matching the criteria.

For the basics, Please see our FAQ at 
<https://wiki.squid-cache.org/SquidFaq/SquidAcl>.

If you have any specific issues getting it to work we can probably 
assist, but will need some more details about the access policy you want 
obeyed and what config you have so far.


Amos


From steve at opendium.com  Fri Oct  8 08:56:45 2021
From: steve at opendium.com (Steve Hill)
Date: Fri, 8 Oct 2021 09:56:45 +0100
Subject: [squid-users] Squid 5.1 memory usage
Message-ID: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>


I'm seeing high memory usage on Squid 5.1.  Caching is disabled, so I'd 
expect memory usage to be fairly low (and it was under Squid 3.5), but 
some workers are growing pretty large.  I'm using ICAP and SSL bump.

I've got a worker using 5 GB which I've collected memory stats from - 
the things which stand out are:
  - Long Strings: 220 MB
  - Short Strings: 2.1 GB
  - Comm::Connection: 217 MB
  - HttpHeaderEntry: 777 MB
  - MemBlob: 773 MB
  - Entry: 226 MB

What's the best way of debugging this?  It there a way to list all of 
the Comm::Connection objects?

Thanks.

-- 
- Steve Hill
    Technical Director | Cyfarwyddwr Technegol
    Opendium    Online Safety & Web Filtering     http://www.opendium.com
                Diogelwch Ar-Lein a Hidlo Gwefan

    Enquiries | Ymholiadau:   sales at opendium.com     +44-1792-824568
    Support   | Cefnogi:      support at opendium.com   +44-1792-825748

------------------------------------------------------------------------
Opendium Limited is a company registered in England and Wales.
Mae Opendium Limited yn gwmni sydd wedi'i gofrestru yn Lloegr a Chymru.

Company No. | Rhif Cwmni:   5465437
Highfield House, 1 Brue Close, Bruton, Somerset, BA10 0HY, England.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 259 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211008/10c838b3/attachment.vcf>

From david at articatech.com  Fri Oct  8 09:09:38 2021
From: david at articatech.com (David Touzeau)
Date: Fri, 8 Oct 2021 11:09:38 +0200
Subject: [squid-users] Squid 5.1 memory usage
In-Reply-To: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
Message-ID: <9a15002c-0c37-d59a-e1be-db381a4dc2b8@articatech.com>

Hi
Just to mention, we discover high memory usage too without ICAP and SSL bump
after several days, need to restart the service.

Le 08/10/2021 ? 10:56, Steve Hill a ?crit?:
>
> I'm seeing high memory usage on Squid 5.1.? Caching is disabled, so 
> I'd expect memory usage to be fairly low (and it was under Squid 3.5), 
> but some workers are growing pretty large.? I'm using ICAP and SSL bump.
>
> I've got a worker using 5 GB which I've collected memory stats from - 
> the things which stand out are:
> ?- Long Strings: 220 MB
> ?- Short Strings: 2.1 GB
> ?- Comm::Connection: 217 MB
> ?- HttpHeaderEntry: 777 MB
> ?- MemBlob: 773 MB
> ?- Entry: 226 MB
>
> What's the best way of debugging this?? It there a way to list all of 
> the Comm::Connection objects?
>
> Thanks.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211008/ba549d96/attachment.htm>

From Ralf.Hildebrandt at charite.de  Fri Oct  8 09:24:42 2021
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Fri, 8 Oct 2021 11:24:42 +0200
Subject: [squid-users] [SPAM] [ext]  Squid 5.1 memory usage
In-Reply-To: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
Message-ID: <YWAOWq89xKj7kJVp@charite.de>

* Steve Hill <steve at opendium.com>:
> 
> I'm seeing high memory usage on Squid 5.1.  Caching is disabled, so I'd
> expect memory usage to be fairly low (and it was under Squid 3.5), but some
> workers are growing pretty large.  I'm using ICAP and SSL bump.

https://bugs.squid-cache.org/show_bug.cgi?id=5132
is somewhat related

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From odhiambo at gmail.com  Fri Oct  8 09:53:11 2021
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Fri, 8 Oct 2021 12:53:11 +0300
Subject: [squid-users] [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <YWAOWq89xKj7kJVp@charite.de>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
Message-ID: <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>

On Fri, Oct 8, 2021 at 12:24 PM Ralf Hildebrandt <
Ralf.Hildebrandt at charite.de> wrote:

> * Steve Hill <steve at opendium.com>:
> >
> > I'm seeing high memory usage on Squid 5.1.  Caching is disabled, so I'd
> > expect memory usage to be fairly low (and it was under Squid 3.5), but
> some
> > workers are growing pretty large.  I'm using ICAP and SSL bump.
>
> https://bugs.squid-cache.org/show_bug.cgi?id=5132
> is somewhat related
>

There's squid-5.2. Does it also have this problem?



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft.", egrep -v '^$|^.*#' :-)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211008/2eee16a8/attachment.htm>

From tibor.matyas at dsi-as.de  Fri Oct  8 10:58:54 2021
From: tibor.matyas at dsi-as.de (MATYAS, Tibor)
Date: Fri, 8 Oct 2021 12:58:54 +0200
Subject: [squid-users] Portal Splash Page - exceptions
In-Reply-To: <fa3a1d3e-0cdf-4da5-b365-05d9286d88ea@treenet.co.nz>
References: <2184f568-3526-0466-61ef-caa326d15ba7@dsi-as.de>
 <fa3a1d3e-0cdf-4da5-b365-05d9286d88ea@treenet.co.nz>
Message-ID: <bc10764c-9304-3cbf-b688-79b3ab0477b2@dsi-as.de>

Hi Amos,

thank you for your support. It is working now!

...
### VoIP/PBX always allow
acl voip_domains dstdomain "/etc/squid/voip_domains.exclude"
cache deny voip_domains
always_direct allow voip_domains
http_access allow intlan voip_domains
--> *last line here was missing*

### Splash/Policy START
external_acl_type session
...

Am 05.10.2021 um 10:49 schrieb Amos Jeffries:
> On 4/10/21 9:53 pm, MATYAS, Tibor wrote:
>> Hello List,
>>
>> I have the following situation: Squid 4 is working in non-transparent
>> mode, with Portal Splash Page
>> https://wiki.squid-cache.org/ConfigExamples/Portal/Splash
>> configured. This is a nice feature, the users must regularly accept the
>> internet usage policy.
>>
>> Since we have our new VoIP/PBX solution, the connection breakdowns
>> caused by the session timeout / reaccept policy are very annoying.
>
> What makes you think the VoIP / PBX has anything to do with HTTP
> traffic through Squid?
>
> If (as I suspect), you have the VoIP software configured to use
> CONNECT tunnels through the proxy. You may find it better to move the
> voice channels to direct connections.
>
>
>> I would like to create exceptions for specific sites/domains. Is this
>> possible?
>
> Of course. Squid access controls are a boolean language. You just have
> to define clearly what it is checking for and what is to happen to
> traffic matching the criteria.
>
> For the basics, Please see our FAQ at
> <https://wiki.squid-cache.org/SquidFaq/SquidAcl>.
>
> If you have any specific issues getting it to work we can probably
> assist, but will need some more details about the access policy you
> want obeyed and what config you have so far.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



--------------------------------------------------
DSI Aerospace Technologie GmbH

Sitz der Gesellschaft: Otto-Lilienthal-Str. 1, D-28199 Bremen, Germany
Web: http://www.dsi-as.de

Geschaeftsfuehrer: Dr.-Ing. Christian Dierker
                   M. Sc. Elias Hashem

HRB 17726, Amtsgericht Bremen
USt-IdNr.: DE 192 681 774
--------------------------------------------------





From rousskov at measurement-factory.com  Fri Oct  8 14:50:41 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 8 Oct 2021 10:50:41 -0400
Subject: [squid-users] Squid 5.1 memory usage
In-Reply-To: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
Message-ID: <befcb522-3c6f-a1fc-cec6-5cf04e328a42@measurement-factory.com>

On 10/8/21 4:56 AM, Steve Hill wrote:
> 
> I'm seeing high memory usage on Squid 5.1.? Caching is disabled, so I'd
> expect memory usage to be fairly low (and it was under Squid 3.5), but
> some workers are growing pretty large.? I'm using ICAP and SSL bump.

As Ralf Hildebrandt mentioned, this is a known problem that we are
tracking as Bug 5132: https://bugs.squid-cache.org/show_bug.cgi?id=5132

We have fixed a few related bugs and will submit polished fixes for the
official inclusion soon, but those fixes do not seem to solve the
primary problem. We do not know what causes the primary leak, but still
have some suspects to go through, and continue to work on this issue.


> I've got a worker using 5 GB which I've collected memory stats from -
> the things which stand out are:
> ?- Long Strings: 220 MB
> ?- Short Strings: 2.1 GB
> ?- Comm::Connection: 217 MB
> ?- HttpHeaderEntry: 777 MB
> ?- MemBlob: 773 MB
> ?- Entry: 226 MB

> What's the best way of debugging this?

I cannot recommend anything specific at sysadmin level right now because
the tricks I knew about (that would often work at that level) did not
help in this particular use case.


> It there a way to list all of the Comm::Connection objects?

The exact answer is "no", but you can use mgr:filedescriptors as an
approximation.


Alex.


From Ralf.Hildebrandt at charite.de  Fri Oct  8 15:04:42 2021
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Fri, 8 Oct 2021 17:04:42 +0200
Subject: [squid-users] [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
 <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
Message-ID: <YWBeClyaKZ2rhpUx@charite.de>

* Odhiambo Washington <odhiambo at gmail.com>:

> > > I'm seeing high memory usage on Squid 5.1.  Caching is disabled, so I'd
> > > expect memory usage to be fairly low (and it was under Squid 3.5), but
> > some
> > > workers are growing pretty large.  I'm using ICAP and SSL bump.
> >
> > https://bugs.squid-cache.org/show_bug.cgi?id=5132
> > is somewhat related
> >
> 
> There's squid-5.2. Does it also have this problem?

Quite sure, since I've been testing Squid-5-HEAD before it became 5.2
But to be sure, I'm deplyoing it right now.

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From steve at opendium.com  Fri Oct  8 15:23:53 2021
From: steve at opendium.com (Steve Hill)
Date: Fri, 8 Oct 2021 16:23:53 +0100
Subject: [squid-users] Squid 5.1 memory usage
In-Reply-To: <befcb522-3c6f-a1fc-cec6-5cf04e328a42@measurement-factory.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <befcb522-3c6f-a1fc-cec6-5cf04e328a42@measurement-factory.com>
Message-ID: <b730fc17-dbe1-1550-07a3-18ef69ef4001@opendium.com>

On 08/10/2021 15:50, Alex Rousskov wrote:

>> It there a way to list all of the Comm::Connection objects?
> 
> The exact answer is "no", but you can use mgr:filedescriptors as an
> approximation.

I've had to restart this process now (but I'm sure the problem will be 
back next week).  I did use netstat on it though, and the number of 
established TCP connections was 1090 - that is obviously made up of 
client->proxy, proxy->origin and proxy->icap connections - my gut 
feeling was that it wasn't enough connections to account for 200-odd MB 
of Comm::Connection objects.


-- 
- Steve Hill
    Technical Director | Cyfarwyddwr Technegol
    Opendium    Online Safety & Web Filtering     http://www.opendium.com
                Diogelwch Ar-Lein a Hidlo Gwefan

    Enquiries | Ymholiadau:   sales at opendium.com     +44-1792-824568
    Support   | Cefnogi:      support at opendium.com   +44-1792-825748

------------------------------------------------------------------------
Opendium Limited is a company registered in England and Wales.
Mae Opendium Limited yn gwmni sydd wedi'i gofrestru yn Lloegr a Chymru.

Company No. | Rhif Cwmni:   5465437
Highfield House, 1 Brue Close, Bruton, Somerset, BA10 0HY, England.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 259 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211008/87211b94/attachment.vcf>

From steve at opendium.com  Fri Oct  8 16:36:19 2021
From: steve at opendium.com (Steve Hill)
Date: Fri, 8 Oct 2021 17:36:19 +0100
Subject: [squid-users] [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <YWAOWq89xKj7kJVp@charite.de>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
Message-ID: <582a3d13-3d8d-7b6b-bd63-60a8ab6b2021@opendium.com>

On 08/10/2021 10:24, Ralf Hildebrandt wrote:

>> I'm seeing high memory usage on Squid 5.1.  Caching is disabled, so I'd
>> expect memory usage to be fairly low (and it was under Squid 3.5), but some
>> workers are growing pretty large.  I'm using ICAP and SSL bump.
> 
> https://bugs.squid-cache.org/show_bug.cgi?id=5132
> is somewhat related

I'm not sure if its the same thing.  In that bug, Alex said it looked 
like Squid wasn't maintaining counters for the leaked memory, whereas in 
my case the "Total" row in mgr:mem reasonably closely tracks the memory 
usage reported by top, so it looks like it should be accounted for.

There are similarities though - lots of memory going to HttpHeaderEntry 
and Short Strings in both cases.


-- 
- Steve Hill
    Technical Director | Cyfarwyddwr Technegol
    Opendium    Online Safety & Web Filtering     http://www.opendium.com
                Diogelwch Ar-Lein a Hidlo Gwefan

    Enquiries | Ymholiadau:   sales at opendium.com     +44-1792-824568
    Support   | Cefnogi:      support at opendium.com   +44-1792-825748

------------------------------------------------------------------------
Opendium Limited is a company registered in England and Wales.
Mae Opendium Limited yn gwmni sydd wedi'i gofrestru yn Lloegr a Chymru.

Company No. | Rhif Cwmni:   5465437
Highfield House, 1 Brue Close, Bruton, Somerset, BA10 0HY, England.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 259 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211008/b3914cee/attachment.vcf>

From huaraz at moeller.plus.com  Sat Oct  9 00:02:33 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 9 Oct 2021 01:02:33 +0100
Subject: [squid-users] squid 5 and parent peers
Message-ID: <sjqm6q$elh$1@ciao.gmane.io>

Hi,

  I try to setup a proxy chain, but don't get the setup right. I have one 
squid with 2 parents. One with auth for domainA.com and one w/o auth for the 
non local IPs (i.e. Internet).

  With the below config I see domainA.com still going to the unauthenticated 
parent proxy. Any hint why ?

Thank you
Markus


#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localnet src 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localnet src 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localnet src 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localnet src fc00::/7               # RFC 4193 local private network 
range
acl localnet src fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

acl localdst dst 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localdst dst 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localdst dst 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localdst dst 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localdst dst 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localdst dst fc00::/7               # RFC 4193 local private network 
range
acl localdst dst fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

acl listA dstdomain -n  domainA.com

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http

cache_peer auth-proxy parent   3128 0  no-query default login=NEGOTIATE
cache_peer parent-proxy parent   3128 0  no-query default
cache_peer_access auth-proxy allow listA
cache_peer_access parent-proxy allow !localdst
never_direct deny localdst
never_direct allow all

debug_options 44,10 11,20




From rousskov at measurement-factory.com  Sat Oct  9 02:12:22 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 8 Oct 2021 22:12:22 -0400
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <sjqm6q$elh$1@ciao.gmane.io>
References: <sjqm6q$elh$1@ciao.gmane.io>
Message-ID: <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>

On 10/8/21 8:02 PM, Markus Moeller wrote:

> I try to setup a proxy chain, but don't get the setup right. I have one
> squid with 2 parents. One with auth for domainA.com and one w/o auth for
> the non local IPs (i.e. Internet).

> With the below config I see domainA.com still going to the
> unauthenticated parent proxy. Any hint why ?

Several factors can explain that, but I would start by rephrasing your
request routing requirements (and the corresponding configuration rules)
as mutually exclusive (if they are). Currently, you have formulated and
configured the equivalent of

* send green traffic to auth-proxy
* send blue traffic to parent-proxy

This approach leaves important questions like "What about yellow
traffic?" and "What about traffic with green and blue dots?" unanswered.

If you want every request to go to either auth-proxy or parent-proxy,
then say so explicitly:

# green (and only green!) traffic to auth-proxy
cache_peer_access auth-proxy allow green
cache_peer_access auth-proxy deny all

# not green (and only not green!) traffic to parent-proxy
cache_peer_access auth-proxy deny green
cache_peer_access auth-proxy allow all

What "green" means exactly in your case, I do not know (due to the
questions like those listed above).


If you want every request to go to either auth-proxy, parent-proxy, or
direct, then your rules will become a bit more complex, but all three
routes should still be mutually exclusive:

# green (and only green) traffic to auth-proxy
# but exclude traffic that should go direct
cache_peer_access auth-proxy deny meantToGoDirect
cache_peer_access auth-proxy allow green
cache_peer_access auth-proxy deny all

# not green (and only not green) traffic to parent-proxy
# but exclude traffic that should go direct
cache_peer_access auth-proxy deny meantToGoDirect
cache_peer_access auth-proxy deny green
cache_peer_access auth-proxy allow all

# traffic that should go direct (and only that traffic)
# should always go direct
always_direct allow meantToGoDirect
always_direct deny all

# traffic that should not go direct (and only that traffic)
# should never go direct
never_direct deny meantToGoDirect
never_direct allow all

Disclaimer: The above configuration snippets are not complete, are not
tested, and can probably be reduced (some might say "simplified") if you
prefer to rely on certain defaults. See also: nonhierarchical_direct.

Once you get the above working for plain HTTP requests that have
resolvable domain names as targets, please note that your listA ACL will
not work for requests that have IP addresses, including some CONNECT
requests that ask your Squid to tunnel HTTPS traffic. Your Squid may not
get any such requests, but if it does, then your "green" and
"meantToGoDirect" ACLs may need to be more complex than "dstdomain -n"
and "dst".


HTH,

Alex.
P.S. I would not call the second proxy "parent-proxy" because both of
your proxies are configured as parent proxies.



> # Recommended minimum configuration:
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 0.0.0.1-0.255.255.255? # RFC 1122 "this" network (LAN)
> acl localnet src 10.0.0.0/8???????????? # RFC 1918 local private network
> (LAN)
> acl localnet src 100.64.0.0/10????????? # RFC 6598 shared address space
> (CGN)
> acl localnet src 169.254.0.0/16???????? # RFC 3927 link-local (directly
> plugged) machines
> acl localnet src 172.16.0.0/12????????? # RFC 1918 local private network
> (LAN)
> acl localnet src 192.168.0.0/16???????? # RFC 1918 local private network
> (LAN)
> acl localnet src fc00::/7?????????????? # RFC 4193 local private network
> range
> acl localnet src fe80::/10????????????? # RFC 4291 link-local (directly
> plugged) machines
> 
> acl localdst dst 10.0.0.0/8???????????? # RFC 1918 local private network
> (LAN)
> acl localdst dst 100.64.0.0/10????????? # RFC 6598 shared address space
> (CGN)
> acl localdst dst 169.254.0.0/16???????? # RFC 3927 link-local (directly
> plugged) machines
> acl localdst dst 172.16.0.0/12????????? # RFC 1918 local private network
> (LAN)
> acl localdst dst 192.168.0.0/16???????? # RFC 1918 local private network
> (LAN)
> acl localdst dst fc00::/7?????????????? # RFC 4193 local private network
> range
> acl localdst dst fe80::/10????????????? # RFC 4291 link-local (directly
> plugged) machines
> 
> acl listA dstdomain -n? domainA.com
> 
> acl SSL_ports port 443
> acl Safe_ports port 80????????? # http
> acl Safe_ports port 21????????? # ftp
> acl Safe_ports port 443???????? # https
> acl Safe_ports port 70????????? # gopher
> acl Safe_ports port 210???????? # wais
> acl Safe_ports port 1025-65535? # unregistered ports
> acl Safe_ports port 280???????? # http-mgmt
> acl Safe_ports port 488???????? # gss-http
> acl Safe_ports port 591???????? # filemaker
> acl Safe_ports port 777???????? # multiling http
> 
> cache_peer auth-proxy parent?? 3128 0? no-query default login=NEGOTIATE
> cache_peer parent-proxy parent?? 3128 0? no-query default
> cache_peer_access auth-proxy allow listA
> cache_peer_access parent-proxy allow !localdst
> never_direct deny localdst
> never_direct allow all
> 
> debug_options 44,10 11,20
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From huaraz at moeller.plus.com  Sat Oct  9 10:39:36 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 9 Oct 2021 11:39:36 +0100
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
References: <sjqm6q$elh$1@ciao.gmane.io>
 <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
Message-ID: <sjrrhc$lat$1@ciao.gmane.io>

I understand now better the concept.

Thank you
Markus


"Alex Rousskov"  wrote in message 
news:3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d at measurement-factory.com...

On 10/8/21 8:02 PM, Markus Moeller wrote:

> I try to setup a proxy chain, but don't get the setup right. I have one
> squid with 2 parents. One with auth for domainA.com and one w/o auth for
> the non local IPs (i.e. Internet).

> With the below config I see domainA.com still going to the
> unauthenticated parent proxy. Any hint why ?

Several factors can explain that, but I would start by rephrasing your
request routing requirements (and the corresponding configuration rules)
as mutually exclusive (if they are). Currently, you have formulated and
configured the equivalent of

* send green traffic to auth-proxy
* send blue traffic to parent-proxy

This approach leaves important questions like "What about yellow
traffic?" and "What about traffic with green and blue dots?" unanswered.

If you want every request to go to either auth-proxy or parent-proxy,
then say so explicitly:

# green (and only green!) traffic to auth-proxy
cache_peer_access auth-proxy allow green
cache_peer_access auth-proxy deny all

# not green (and only not green!) traffic to parent-proxy
cache_peer_access auth-proxy deny green
cache_peer_access auth-proxy allow all

What "green" means exactly in your case, I do not know (due to the
questions like those listed above).


If you want every request to go to either auth-proxy, parent-proxy, or
direct, then your rules will become a bit more complex, but all three
routes should still be mutually exclusive:

# green (and only green) traffic to auth-proxy
# but exclude traffic that should go direct
cache_peer_access auth-proxy deny meantToGoDirect
cache_peer_access auth-proxy allow green
cache_peer_access auth-proxy deny all

# not green (and only not green) traffic to parent-proxy
# but exclude traffic that should go direct
cache_peer_access auth-proxy deny meantToGoDirect
cache_peer_access auth-proxy deny green
cache_peer_access auth-proxy allow all

# traffic that should go direct (and only that traffic)
# should always go direct
always_direct allow meantToGoDirect
always_direct deny all

# traffic that should not go direct (and only that traffic)
# should never go direct
never_direct deny meantToGoDirect
never_direct allow all

Disclaimer: The above configuration snippets are not complete, are not
tested, and can probably be reduced (some might say "simplified") if you
prefer to rely on certain defaults. See also: nonhierarchical_direct.

Once you get the above working for plain HTTP requests that have
resolvable domain names as targets, please note that your listA ACL will
not work for requests that have IP addresses, including some CONNECT
requests that ask your Squid to tunnel HTTPS traffic. Your Squid may not
get any such requests, but if it does, then your "green" and
"meantToGoDirect" ACLs may need to be more complex than "dstdomain -n"
and "dst".


HTH,

Alex.
P.S. I would not call the second proxy "parent-proxy" because both of
your proxies are configured as parent proxies.



> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
> acl localnet src 10.0.0.0/8             # RFC 1918 local private network
> (LAN)
> acl localnet src 100.64.0.0/10          # RFC 6598 shared address space
> (CGN)
> acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly
> plugged) machines
> acl localnet src 172.16.0.0/12          # RFC 1918 local private network
> (LAN)
> acl localnet src 192.168.0.0/16         # RFC 1918 local private network
> (LAN)
> acl localnet src fc00::/7               # RFC 4193 local private network
> range
> acl localnet src fe80::/10              # RFC 4291 link-local (directly
> plugged) machines
>
> acl localdst dst 10.0.0.0/8             # RFC 1918 local private network
> (LAN)
> acl localdst dst 100.64.0.0/10          # RFC 6598 shared address space
> (CGN)
> acl localdst dst 169.254.0.0/16         # RFC 3927 link-local (directly
> plugged) machines
> acl localdst dst 172.16.0.0/12          # RFC 1918 local private network
> (LAN)
> acl localdst dst 192.168.0.0/16         # RFC 1918 local private network
> (LAN)
> acl localdst dst fc00::/7               # RFC 4193 local private network
> range
> acl localdst dst fe80::/10              # RFC 4291 link-local (directly
> plugged) machines
>
> acl listA dstdomain -n  domainA.com
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
>
> cache_peer auth-proxy parent   3128 0  no-query default login=NEGOTIATE
> cache_peer parent-proxy parent   3128 0  no-query default
> cache_peer_access auth-proxy allow listA
> cache_peer_access parent-proxy allow !localdst
> never_direct deny localdst
> never_direct allow all
>
> debug_options 44,10 11,20
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From huaraz at moeller.plus.com  Sat Oct  9 13:06:24 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 9 Oct 2021 14:06:24 +0100
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <sjrrhc$lat$1@ciao.gmane.io>
References: <sjqm6q$elh$1@ciao.gmane.io>
 <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
 <sjrrhc$lat$1@ciao.gmane.io>
Message-ID: <sjs44m$une$1@ciao.gmane.io>

Hi,

I have now tested with the below config and I see my first request works, 
but the second fails. So I am not sure if it is still a configuration issue 
or something else.


....
# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
#acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localnet src 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localnet src 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localnet src 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localnet src 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localnet src fc00::/7               # RFC 4193 local private network 
range
acl localnet src fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

#acl localdst dst 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
acl localdst dst 10.0.0.0/8             # RFC 1918 local private network 
(LAN)
acl localdst dst 100.64.0.0/10          # RFC 6598 shared address space 
(CGN)
acl localdst dst 169.254.0.0/16         # RFC 3927 link-local (directly 
plugged) machines
acl localdst dst 172.16.0.0/12          # RFC 1918 local private network 
(LAN)
acl localdst dst 192.168.0.0/16         # RFC 1918 local private network 
(LAN)
acl localdst dst fc00::/7               # RFC 4193 local private network 
range
acl localdst dst fe80::/10              # RFC 4291 link-local (directly 
plugged) machines

acl google dstdomain -n .google.com

cache_peer internetproxy.example.com parent 8080 0 no-query no-digest 
no-netdb-exchange default
cache_peer authproxy.example.com parent 8080 0 no-query no-digest 
no-netdb-exchange default login=NEGOTIATE auth-no-keytab
# Only google to auth proxy
cache_peer_access authproxy.example.com deny localdst
cache_peer_access authproxy.example.com allow google
cache_peer_access authproxy.example.com deny all
# All other external domains
cache_peer_access internetproxy.example.com deny localdst
cache_peer_access internetproxy.example.com deny google
cache_peer_access internetproxy.example.com allow all
# Local goes direct
always_direct allow localdst
always_direct deny all
never_direct deny !localdst
never_direct allow all

debug_options 44,10 11,20

....

The first test looked fine:

#curl -vvv -x http://localhost:3128 http://www.google.com
* Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
*   Trying 127.0.0.1:3128...
* Connected to localhost (127.0.0.1) port 3128 (#0)
> GET http://www.google.com/ HTTP/1.1
> Host: www.google.com
> User-Agent: curl/7.75.0
> Accept: */*
> Proxy-Connection: Keep-Alive
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 301 Moved Permanently
< Location: https://www.google.com/
< Content-Length: 0
< Date: Sat, 09 Oct 2021 12:29:23 GMT
< X-Cache: MISS from clientproxy
< X-Cache-Lookup: MISS from clientproxy:3128
< Connection: keep-alive
<
* Connection #0 to host localhost left intact


Second request failed with a cache error:


#curl -vvv -x http://localhost:3128 http://www.google.com
* Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
*   Trying 127.0.0.1:3128...
* Connected to localhost (127.0.0.1) port 3128 (#0)
> GET http://www.google.com/ HTTP/1.1
> Host: www.google.com
> User-Agent: curl/7.75.0
> Accept: */*
> Proxy-Connection: Keep-Alive
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 503 Service Unavailable
< Server: squid/5.1-VCS
< Mime-Version: 1.0
< Date: Sat, 09 Oct 2021 12:30:27 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 3573
< X-Squid-Error: ERR_CONNECT_FAIL 110
< Vary: Accept-Language
< Content-Language: en
< X-Cache: MISS from clientproxy
< X-Cache-Lookup: MISS from clientproxy:3128
< Connection: keep-alive
<
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" 
http://www.w3.org/TR/html4/strict.dtd>
<html><head>
<meta type="copyright" content="Copyright (C) 1996-2021 The Squid Software 
Foundation and contributors">
<meta http-equiv="Content-Type" CONTENT="text/html; charset=utf-8">
<title>ERROR: The requested URL could not be retrieved</title>
.....


The cache log says:

2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1353) parseHttpRequest: 
HTTP Client conn10 local=127.0.0.1:3128 remote=127.0.0.1:45192 FD 12 flags=1
2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1354) parseHttpRequest: 
HTTP Client REQUEST:
---------
GET http://www.google.com/ HTTP/1.1
Host: www.google.com
User-Agent: curl/7.75.0
Accept: */*
Proxy-Connection: Keep-Alive


----------
2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(309) peerSelect: 
e:=IV/0x12e63f0*2 http://www.google.com/
2021/10/09 13:29:23.520 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(612) selectMore: GET 
www.google.com
2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(617) selectMore: direct = 
DIRECT_UNKNOWN (always_direct to be checked)
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(373) 
checkAlwaysDirectDone: DENIED
2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET 
www.google.com
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(626) selectMore: direct = 
DIRECT_UNKNOWN (never_direct to be checked)
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(345) 
checkNeverDirectDone: DENIED
2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET 
www.google.com
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(577) checkNetdbDirect: MY 
RTT = 0 msec
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(578) checkNetdbDirect: 
minimum_direct_rtt = 400 msec
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(585) checkNetdbDirect: MY 
hops = 0
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(586) checkNetdbDirect: 
minimum_direct_hops = 4
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(647) selectMore: direct = 
DIRECT_MAYBE (default)
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(650) selectMore: direct = 
DIRECT_MAYBE
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(286) peerSelectIcpPing: 
http://www.google.com/
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(297) peerSelectIcpPing: 
counted 0 neighbors
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(833) selectSomeParent: 
GET www.google.com
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection: 
adding FIRSTUP_PARENT/authproxy.example.com
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection: 
skipping ANY_OLD_PARENT/authproxy.example.com; have 
FIRSTUP_PARENT/authproxy.example.com
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection: 
skipping DEFAULT_PARENT/authproxy.example.com; have 
FIRSTUP_PARENT/authproxy.example.com
2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection: 
adding HIER_DIRECT#www.google.com
2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: http://www.google.com/' via authproxy.example.com
2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1171) handlePath: 
PeerSelector1 found conn11 local=0.0.0.0 remote=10.20.1.1:8080 
FIRSTUP_PARENT flags=1, destination #1 for http://www.google.com/
2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1177) handlePath: 
always_direct = DENIED
2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1178) handlePath: 
never_direct = DENIED
2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1179) handlePath: 
timedout = 0
2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.523 kid1| 11,7| HttpRequest.cc(468) clearError: old: 
ERR_NONE
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: http://www.google.com/' via www.google.com
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1171) handlePath: 
PeerSelector1 found conn12 local=0.0.0.0 remote=172.217.23.100:80 
HIER_DIRECT flags=1, destination #2 for http://www.google.com/
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1177) handlePath: 
always_direct = DENIED
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1178) handlePath: 
never_direct = DENIED
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1179) handlePath: 
timedout = 0
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector1 found all 2 destinations for http://www.google.com/
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(480) resolveSelected: 
always_direct = DENIED
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(481) resolveSelected: 
never_direct = DENIED
2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(482) resolveSelected: 
timedout = 0
2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector1
2021/10/09 13:29:23.524 kid1| 44,3| peer_select.cc(241) ~PeerSelector: 
http://www.google.com/
2021/10/09 13:29:23.526 kid1| 11,4| HttpRequest.cc(453) prepForPeering: 
0x1154cf0 to authproxy.example.com proxy
2021/10/09 13:29:23.526 kid1| 11,3| http.cc(2486) httpStart: GET 
http://www.google.com/
2021/10/09 13:29:23.527 kid1| 11,5| http.cc(87) HttpStateData: HttpStateData 
0x12e9988 created
2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2367) sendRequest: conn13 
local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, 
request 0x1154cf0*6, this 0x12e9988.
2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The 
AsyncCall HttpStateData::httpTimeout constructed, this=0x12e8920 [call65]
2021/10/09 13:29:23.527 kid1| 11,8| http.cc(1656) maybeMakeSpaceAvailable: 
may read up to 65536 bytes info buf(0/65536) from conn13 
local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The 
AsyncCall HttpStateData::readReply constructed, this=0x12f9c10 [call66]
2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The 
AsyncCall HttpStateData::wroteLast constructed, this=0x12f9cc0 [call67]
2021/10/09 13:29:23.527 kid1| 11,8| http.cc(2309) decideIfWeDoRanges: 
decideIfWeDoRanges: range specs: 0, cachable: 1; we_do_ranges: 0
2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113) 
copyOneHeaderFromClientsideRequestToUpstreamRequest: httpBuildRequestHeader: 
User-Agent: curl/7.75.0
2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113) 
copyOneHeaderFromClientsideRequestToUpstreamRequest: httpBuildRequestHeader: 
Accept: */*
2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113) 
copyOneHeaderFromClientsideRequestToUpstreamRequest: httpBuildRequestHeader: 
Proxy-Connection: Keep-Alive
2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113) 
copyOneHeaderFromClientsideRequestToUpstreamRequest: httpBuildRequestHeader: 
Host: www.google.com
2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(539) 
peer_proxy_negotiate_auth: Import gss name
2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(546) 
peer_proxy_negotiate_auth: Initialize gss security context
2021/10/09 13:29:23.531 kid1| 11,5| peer_proxy_negotiate_auth.cc(560) 
peer_proxy_negotiate_auth: Got token with length 2568
2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2442) sendRequest: HTTP Server 
conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 
flags=1
2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2443) sendRequest: HTTP Server 
REQUEST:
---------
GET http://www.google.com/ HTTP/1.1
User-Agent: curl/7.75.0
Accept: */*
Host: www.google.com
Proxy-Authorization: Negotiate YIIK....
Cache-Control: max-age=259200
Connection: keep-alive


----------
2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(96) ScheduleCall: 
IoCallback.cc(131) will call HttpStateData::wroteLast(conn13 
local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, 
data=0x12e9988) [call67]
2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(59) fireNext: entering 
HttpStateData::wroteLast(conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 
FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(41) make: make call 
HttpStateData::wroteLast [call67]
2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(122) callStart: 
HttpStateData status in: [ job8]
2021/10/09 13:29:23.531 kid1| 11,5| http.cc(1667) wroteLast: conn13 
local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1: 
size 3611: errflag 0.
2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The 
AsyncCall HttpStateData::httpTimeout constructed, this=0xe34fa0 [call69]
2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(153) callEnd: HttpStateData 
status out: [ job8]
2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(61) fireNext: leaving 
HttpStateData::wroteLast(conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 
FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(96) ScheduleCall: 
IoCallback.cc(131) will call HttpStateData::readReply(conn13 
local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, 
data=0x12e9988) [call66]
2021/10/09 13:29:23.615 kid1| 11,5| AsyncCallQueue.cc(59) fireNext: entering 
HttpStateData::readReply(conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 
FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(41) make: make call 
HttpStateData::readReply [call66]
2021/10/09 13:29:23.615 kid1| 11,5| AsyncJob.cc(122) callStart: 
HttpStateData status in: [ job8]
2021/10/09 13:29:23.615 kid1| 11,5| http.cc(1215) readReply: conn13 
local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
2021/10/09 13:29:23.615 kid1| ctx: enter level  0: 'http://www.google.com/'
2021/10/09 13:29:23.615 kid1| 11,3| http.cc(666) processReplyHeader: 
processReplyHeader: key '0200000000000000843D000001000000'
2021/10/09 13:29:23.615 kid1| 11,2| http.cc(720) processReplyHeader: HTTP 
Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 
13 flags=1
2021/10/09 13:29:23.615 kid1| 11,2| http.cc(721) processReplyHeader: HTTP 
Server RESPONSE:
---------
HTTP/1.1 301 Moved Permanently
Location: https://www.google.com/
Content-Length: 0
Proxy-Connection: Keep-Alive

----------
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(119) setVirginReply: 0x12e9988 
setting virgin reply to 0x12fa850
2021/10/09 13:29:23.616 kid1| ctx: exit level  0
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(973) adaptOrFinalizeReply: 
adaptationAccessCheckPending=0
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(139) setFinalReply: 0x12e9988 
setting final reply to 0x12fa850
2021/10/09 13:29:23.616 kid1| ctx: enter level  0: 'http://www.google.com/'
2021/10/09 13:29:23.616 kid1| 11,3| http.cc(979) haveParsedReplyHeaders: 
HTTP CODE: 301
2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1054) haveParsedReplyHeaders: 
decided: do not cache but share because refresh check returned 
non-cacheable; HTTP status 301 e:=p2XIV/0x12e63f0*3
2021/10/09 13:29:23.616 kid1| ctx: exit level  0
2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(279) sendStartOfMessage: HTTP 
Client conn10 local=127.0.0.1:3128 remote=127.0.0.1:45192 FD 12 flags=1
2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(280) sendStartOfMessage: HTTP 
Client REPLY:
---------
HTTP/1.1 301 Moved Permanently
Location: https://www.google.com/
Content-Length: 0
Date: Sat, 09 Oct 2021 12:29:23 GMT
X-Cache: MISS from clientproxy
X-Cache-Lookup: MISS from clientproxy:3128
Connection: keep-alive


----------
2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1491) processReplyBody: 
adaptationAccessCheckPending=0
2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1154) persistentConnStatus: 
conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 
flags=1 eof=0
2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1174) persistentConnStatus: 
persistentConnStatus: content_length=0
2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1178) persistentConnStatus: 
persistentConnStatus: clen=0
2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1537) processReplyBody: 
processReplyBody: COMPLETE_PERSISTENT_MSG from conn13 local=10.10.1.1:36928 
remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(162) serverComplete: 
serverComplete 0x12e9988
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(184) serverComplete2: 
serverComplete2 0x12e9988
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(212) completeForwarding: 
completing forwarding for 0x12e6e28*2
2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(586) cleanAdaptation: cleaning 
ICAP; ACL: 0
2021/10/09 13:29:23.616 kid1| 11,5| http.cc(134) ~HttpStateData: 
HttpStateData 0x12e9988 destroyed;
2021/10/09 13:29:23.616 kid1| 11,5| AsyncCallQueue.cc(61) fireNext: leaving 
HttpStateData::readReply(conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 
FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1353) parseHttpRequest: 
HTTP Client conn15 local=127.0.0.1:3128 remote=127.0.0.1:45219 FD 12 flags=1
2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1354) parseHttpRequest: 
HTTP Client REQUEST:
---------
GET http://www.google.com/ HTTP/1.1
Host: www.google.com
User-Agent: curl/7.75.0
Accept: */*
Proxy-Connection: Keep-Alive


----------
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(309) peerSelect: 
e:=IV/0x12e63f0*2 http://www.google.com/
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET 
www.google.com
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(617) selectMore: direct = 
DIRECT_UNKNOWN (always_direct to be checked)
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(373) 
checkAlwaysDirectDone: DENIED
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET 
www.google.com
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(626) selectMore: direct = 
DIRECT_UNKNOWN (never_direct to be checked)
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(345) 
checkNeverDirectDone: DENIED
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET 
www.google.com
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(577) checkNetdbDirect: MY 
RTT = 1 msec
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(578) checkNetdbDirect: 
minimum_direct_rtt = 400 msec
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(644) selectMore: direct = 
DIRECT_YES (checkNetdbDirect)
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(650) selectMore: direct = 
DIRECT_YES
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(1098) addSelection: 
adding HIER_DIRECT#www.google.com
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: http://www.google.com/' via www.google.com
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1171) handlePath: 
PeerSelector2 found conn16 local=0.0.0.0 remote=172.217.23.100:80 
HIER_DIRECT flags=1, destination #1 for http://www.google.com/
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1177) handlePath: 
always_direct = DENIED
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1178) handlePath: 
never_direct = DENIED
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1179) handlePath: 
timedout = 0
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 11,7| HttpRequest.cc(468) clearError: old: 
ERR_NONE
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector2 found all 1 destinations for http://www.google.com/
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(480) resolveSelected: 
always_direct = DENIED
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(481) resolveSelected: 
never_direct = DENIED
2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(482) resolveSelected: 
timedout = 0
2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149) 
interestedInitiator: PeerSelector2
2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(241) ~PeerSelector: 
http://www.google.com/
2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(279) sendStartOfMessage: HTTP 
Client conn15 local=127.0.0.1:3128 remote=127.0.0.1:45219 FD 12 flags=1
2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(280) sendStartOfMessage: HTTP 
Client REPLY:
---------
HTTP/1.1 503 Service Unavailable
Server: squid/5.1-VCS
Mime-Version: 1.0
Date: Sat, 09 Oct 2021 12:30:27 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3573
X-Squid-Error: ERR_CONNECT_FAIL 110
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from clientproxy
X-Cache-Lookup: MISS from clientproxy:3128
Connection: keep-alive


----------






Thank you
Markus





"Markus Moeller"  wrote in message news:sjrrhc$lat$1 at ciao.gmane.io...

I understand now better the concept.

Thank you
Markus


"Alex Rousskov"  wrote in message
news:3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d at measurement-factory.com...

On 10/8/21 8:02 PM, Markus Moeller wrote:

> I try to setup a proxy chain, but don't get the setup right. I have one
> squid with 2 parents. One with auth for domainA.com and one w/o auth for
> the non local IPs (i.e. Internet).

> With the below config I see domainA.com still going to the
> unauthenticated parent proxy. Any hint why ?

Several factors can explain that, but I would start by rephrasing your
request routing requirements (and the corresponding configuration rules)
as mutually exclusive (if they are). Currently, you have formulated and
configured the equivalent of

* send green traffic to auth-proxy
* send blue traffic to parent-proxy

This approach leaves important questions like "What about yellow
traffic?" and "What about traffic with green and blue dots?" unanswered.

If you want every request to go to either auth-proxy or parent-proxy,
then say so explicitly:

# green (and only green!) traffic to auth-proxy
cache_peer_access auth-proxy allow green
cache_peer_access auth-proxy deny all

# not green (and only not green!) traffic to parent-proxy
cache_peer_access auth-proxy deny green
cache_peer_access auth-proxy allow all

What "green" means exactly in your case, I do not know (due to the
questions like those listed above).


If you want every request to go to either auth-proxy, parent-proxy, or
direct, then your rules will become a bit more complex, but all three
routes should still be mutually exclusive:

# green (and only green) traffic to auth-proxy
# but exclude traffic that should go direct
cache_peer_access auth-proxy deny meantToGoDirect
cache_peer_access auth-proxy allow green
cache_peer_access auth-proxy deny all

# not green (and only not green) traffic to parent-proxy
# but exclude traffic that should go direct
cache_peer_access auth-proxy deny meantToGoDirect
cache_peer_access auth-proxy deny green
cache_peer_access auth-proxy allow all

# traffic that should go direct (and only that traffic)
# should always go direct
always_direct allow meantToGoDirect
always_direct deny all

# traffic that should not go direct (and only that traffic)
# should never go direct
never_direct deny meantToGoDirect
never_direct allow all

Disclaimer: The above configuration snippets are not complete, are not
tested, and can probably be reduced (some might say "simplified") if you
prefer to rely on certain defaults. See also: nonhierarchical_direct.

Once you get the above working for plain HTTP requests that have
resolvable domain names as targets, please note that your listA ACL will
not work for requests that have IP addresses, including some CONNECT
requests that ask your Squid to tunnel HTTPS traffic. Your Squid may not
get any such requests, but if it does, then your "green" and
"meantToGoDirect" ACLs may need to be more complex than "dstdomain -n"
and "dst".


HTH,

Alex.
P.S. I would not call the second proxy "parent-proxy" because both of
your proxies are configured as parent proxies.



> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
> acl localnet src 10.0.0.0/8             # RFC 1918 local private network
> (LAN)
> acl localnet src 100.64.0.0/10          # RFC 6598 shared address space
> (CGN)
> acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly
> plugged) machines
> acl localnet src 172.16.0.0/12          # RFC 1918 local private network
> (LAN)
> acl localnet src 192.168.0.0/16         # RFC 1918 local private network
> (LAN)
> acl localnet src fc00::/7               # RFC 4193 local private network
> range
> acl localnet src fe80::/10              # RFC 4291 link-local (directly
> plugged) machines
>
> acl localdst dst 10.0.0.0/8             # RFC 1918 local private network
> (LAN)
> acl localdst dst 100.64.0.0/10          # RFC 6598 shared address space
> (CGN)
> acl localdst dst 169.254.0.0/16         # RFC 3927 link-local (directly
> plugged) machines
> acl localdst dst 172.16.0.0/12          # RFC 1918 local private network
> (LAN)
> acl localdst dst 192.168.0.0/16         # RFC 1918 local private network
> (LAN)
> acl localdst dst fc00::/7               # RFC 4193 local private network
> range
> acl localdst dst fe80::/10              # RFC 4291 link-local (directly
> plugged) machines
>
> acl listA dstdomain -n  domainA.com
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
>
> cache_peer auth-proxy parent   3128 0  no-query default login=NEGOTIATE
> cache_peer parent-proxy parent   3128 0  no-query default
> cache_peer_access auth-proxy allow listA
> cache_peer_access parent-proxy allow !localdst
> never_direct deny localdst
> never_direct allow all
>
> debug_options 44,10 11,20
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From rousskov at measurement-factory.com  Sat Oct  9 15:41:04 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 9 Oct 2021 11:41:04 -0400
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <sjs44m$une$1@ciao.gmane.io>
References: <sjqm6q$elh$1@ciao.gmane.io>
 <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
 <sjrrhc$lat$1@ciao.gmane.io> <sjs44m$une$1@ciao.gmane.io>
Message-ID: <cbe23671-7b3c-e270-f3f4-593d4f030f36@measurement-factory.com>

On 10/9/21 9:06 AM, Markus Moeller wrote:
> Hi,
> 
> I have now tested with the below config and I see my first request
> works, but the second fails. So I am not sure if it is still a
> configuration issue or something else.


> always_direct allow localdst
> never_direct deny !localdst

I (still) do not know what you want to achive exactly (see my previous
response for more specific questions), but the above combination looks
suspicious to me. I would expect traffic that should always go direct to
be denied in the never_direct rule instead. Did you mean for that "!" to
be there?

I did not check the debugging trace carefully, but it may be the reason
why Squid cannot forward some requests -- it is getting an
impossible-to-satisfy or self-contradictory directions.


BTW, thank you for posting the debugging trace! Please keep doing that
if you need further help.

Alex.


> ....
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> #acl localnet src 0.0.0.1-0.255.255.255? # RFC 1122 "this" network (LAN)
> acl localnet src 10.0.0.0/8???????????? # RFC 1918 local private network
> (LAN)
> acl localnet src 100.64.0.0/10????????? # RFC 6598 shared address space
> (CGN)
> acl localnet src 169.254.0.0/16???????? # RFC 3927 link-local (directly
> plugged) machines
> acl localnet src 172.16.0.0/12????????? # RFC 1918 local private network
> (LAN)
> acl localnet src 192.168.0.0/16???????? # RFC 1918 local private network
> (LAN)
> acl localnet src fc00::/7?????????????? # RFC 4193 local private network
> range
> acl localnet src fe80::/10????????????? # RFC 4291 link-local (directly
> plugged) machines
> 
> #acl localdst dst 0.0.0.1-0.255.255.255? # RFC 1122 "this" network (LAN)
> acl localdst dst 10.0.0.0/8???????????? # RFC 1918 local private network
> (LAN)
> acl localdst dst 100.64.0.0/10????????? # RFC 6598 shared address space
> (CGN)
> acl localdst dst 169.254.0.0/16???????? # RFC 3927 link-local (directly
> plugged) machines
> acl localdst dst 172.16.0.0/12????????? # RFC 1918 local private network
> (LAN)
> acl localdst dst 192.168.0.0/16???????? # RFC 1918 local private network
> (LAN)
> acl localdst dst fc00::/7?????????????? # RFC 4193 local private network
> range
> acl localdst dst fe80::/10????????????? # RFC 4291 link-local (directly
> plugged) machines
> 
> acl google dstdomain -n .google.com
> 
> cache_peer internetproxy.example.com parent 8080 0 no-query no-digest
> no-netdb-exchange default
> cache_peer authproxy.example.com parent 8080 0 no-query no-digest
> no-netdb-exchange default login=NEGOTIATE auth-no-keytab
> # Only google to auth proxy
> cache_peer_access authproxy.example.com deny localdst
> cache_peer_access authproxy.example.com allow google
> cache_peer_access authproxy.example.com deny all
> # All other external domains
> cache_peer_access internetproxy.example.com deny localdst
> cache_peer_access internetproxy.example.com deny google
> cache_peer_access internetproxy.example.com allow all
> # Local goes direct
> always_direct allow localdst
> always_direct deny all
> never_direct deny !localdst
> never_direct allow all
> 
> debug_options 44,10 11,20
> 
> ....
> 
> The first test looked fine:
> 
> #curl -vvv -x http://localhost:3128 http://www.google.com
> * Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
> *?? Trying 127.0.0.1:3128...
> * Connected to localhost (127.0.0.1) port 3128 (#0)
>> GET http://www.google.com/ HTTP/1.1
>> Host: www.google.com
>> User-Agent: curl/7.75.0
>> Accept: */*
>> Proxy-Connection: Keep-Alive
>>
> * Mark bundle as not supporting multiuse
> < HTTP/1.1 301 Moved Permanently
> < Location: https://www.google.com/
> < Content-Length: 0
> < Date: Sat, 09 Oct 2021 12:29:23 GMT
> < X-Cache: MISS from clientproxy
> < X-Cache-Lookup: MISS from clientproxy:3128
> < Connection: keep-alive
> <
> * Connection #0 to host localhost left intact
> 
> 
> Second request failed with a cache error:
> 
> 
> #curl -vvv -x http://localhost:3128 http://www.google.com
> * Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
> *?? Trying 127.0.0.1:3128...
> * Connected to localhost (127.0.0.1) port 3128 (#0)
>> GET http://www.google.com/ HTTP/1.1
>> Host: www.google.com
>> User-Agent: curl/7.75.0
>> Accept: */*
>> Proxy-Connection: Keep-Alive
>>
> * Mark bundle as not supporting multiuse
> < HTTP/1.1 503 Service Unavailable
> < Server: squid/5.1-VCS
> < Mime-Version: 1.0
> < Date: Sat, 09 Oct 2021 12:30:27 GMT
> < Content-Type: text/html;charset=utf-8
> < Content-Length: 3573
> < X-Squid-Error: ERR_CONNECT_FAIL 110
> < Vary: Accept-Language
> < Content-Language: en
> < X-Cache: MISS from clientproxy
> < X-Cache-Lookup: MISS from clientproxy:3128
> < Connection: keep-alive
> <
> <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
> http://www.w3.org/TR/html4/strict.dtd>
> <html><head>
> <meta type="copyright" content="Copyright (C) 1996-2021 The Squid
> Software Foundation and contributors">
> <meta http-equiv="Content-Type" CONTENT="text/html; charset=utf-8">
> <title>ERROR: The requested URL could not be retrieved</title>
> .....
> 
> 
> The cache log says:
> 
> 2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1353)
> parseHttpRequest: HTTP Client conn10 local=127.0.0.1:3128
> remote=127.0.0.1:45192 FD 12 flags=1
> 2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1354)
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET http://www.google.com/ HTTP/1.1
> Host: www.google.com
> User-Agent: curl/7.75.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
> 
> ----------
> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(309) peerSelect:
> e:=IV/0x12e63f0*2 http://www.google.com/
> 2021/10/09 13:29:23.520 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(612) selectMore: GET
> www.google.com
> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(617) selectMore:
> direct = DIRECT_UNKNOWN (always_direct to be checked)
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(373)
> checkAlwaysDirectDone: DENIED
> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET
> www.google.com
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(626) selectMore:
> direct = DIRECT_UNKNOWN (never_direct to be checked)
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(345)
> checkNeverDirectDone: DENIED
> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET
> www.google.com
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(577)
> checkNetdbDirect: MY RTT = 0 msec
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(578)
> checkNetdbDirect: minimum_direct_rtt = 400 msec
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(585)
> checkNetdbDirect: MY hops = 0
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(586)
> checkNetdbDirect: minimum_direct_hops = 4
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(647) selectMore:
> direct = DIRECT_MAYBE (default)
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(650) selectMore:
> direct = DIRECT_MAYBE
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(286)
> peerSelectIcpPing: http://www.google.com/
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(297)
> peerSelectIcpPing: counted 0 neighbors
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(833)
> selectSomeParent: GET www.google.com
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection:
> adding FIRSTUP_PARENT/authproxy.example.com
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection:
> skipping ANY_OLD_PARENT/authproxy.example.com; have
> FIRSTUP_PARENT/authproxy.example.com
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection:
> skipping DEFAULT_PARENT/authproxy.example.com; have
> FIRSTUP_PARENT/authproxy.example.com
> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection:
> adding HIER_DIRECT#www.google.com
> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(460) resolveSelected:
> Find IP destination for: http://www.google.com/' via authproxy.example.com
> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1171) handlePath:
> PeerSelector1 found conn11 local=0.0.0.0 remote=10.20.1.1:8080
> FIRSTUP_PARENT flags=1, destination #1 for http://www.google.com/
> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1177) handlePath:
> always_direct = DENIED
> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1178) handlePath:
> never_direct = DENIED
> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1179) handlePath:
> timedout = 0
> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.523 kid1| 11,7| HttpRequest.cc(468) clearError: old:
> ERR_NONE
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(460) resolveSelected:
> Find IP destination for: http://www.google.com/' via www.google.com
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1171) handlePath:
> PeerSelector1 found conn12 local=0.0.0.0 remote=172.217.23.100:80
> HIER_DIRECT flags=1, destination #2 for http://www.google.com/
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1177) handlePath:
> always_direct = DENIED
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1178) handlePath:
> never_direct = DENIED
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1179) handlePath:
> timedout = 0
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(479) resolveSelected:
> PeerSelector1 found all 2 destinations for http://www.google.com/
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(480) resolveSelected:
> always_direct = DENIED
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(481) resolveSelected:
> never_direct = DENIED
> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(482) resolveSelected:
> timedout = 0
> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector1
> 2021/10/09 13:29:23.524 kid1| 44,3| peer_select.cc(241) ~PeerSelector:
> http://www.google.com/
> 2021/10/09 13:29:23.526 kid1| 11,4| HttpRequest.cc(453) prepForPeering:
> 0x1154cf0 to authproxy.example.com proxy
> 2021/10/09 13:29:23.526 kid1| 11,3| http.cc(2486) httpStart: GET
> http://www.google.com/
> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(87) HttpStateData:
> HttpStateData 0x12e9988 created
> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2367) sendRequest: conn13
> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
> flags=1, request 0x1154cf0*6, this 0x12e9988.
> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
> AsyncCall HttpStateData::httpTimeout constructed, this=0x12e8920 [call65]
> 2021/10/09 13:29:23.527 kid1| 11,8| http.cc(1656)
> maybeMakeSpaceAvailable: may read up to 65536 bytes info buf(0/65536)
> from conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT
> FD 13 flags=1
> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
> AsyncCall HttpStateData::readReply constructed, this=0x12f9c10 [call66]
> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
> AsyncCall HttpStateData::wroteLast constructed, this=0x12f9cc0 [call67]
> 2021/10/09 13:29:23.527 kid1| 11,8| http.cc(2309) decideIfWeDoRanges:
> decideIfWeDoRanges: range specs: 0, cachable: 1; we_do_ranges: 0
> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
> copyOneHeaderFromClientsideRequestToUpstreamRequest:
> httpBuildRequestHeader: User-Agent: curl/7.75.0
> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
> copyOneHeaderFromClientsideRequestToUpstreamRequest:
> httpBuildRequestHeader: Accept: */*
> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
> copyOneHeaderFromClientsideRequestToUpstreamRequest:
> httpBuildRequestHeader: Proxy-Connection: Keep-Alive
> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
> copyOneHeaderFromClientsideRequestToUpstreamRequest:
> httpBuildRequestHeader: Host: www.google.com
> 2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(539)
> peer_proxy_negotiate_auth: Import gss name
> 2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(546)
> peer_proxy_negotiate_auth: Initialize gss security context
> 2021/10/09 13:29:23.531 kid1| 11,5| peer_proxy_negotiate_auth.cc(560)
> peer_proxy_negotiate_auth: Got token with length 2568
> 2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2442) sendRequest: HTTP
> Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT
> FD 13 flags=1
> 2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2443) sendRequest: HTTP
> Server REQUEST:
> ---------
> GET http://www.google.com/ HTTP/1.1
> User-Agent: curl/7.75.0
> Accept: */*
> Host: www.google.com
> Proxy-Authorization: Negotiate YIIK....
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(96) ScheduleCall:
> IoCallback.cc(131) will call HttpStateData::wroteLast(conn13
> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
> flags=1, data=0x12e9988) [call67]
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(59) fireNext:
> entering HttpStateData::wroteLast(conn13 local=10.10.1.1:36928
> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(41) make: make call
> HttpStateData::wroteLast [call67]
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(122) callStart:
> HttpStateData status in: [ job8]
> 2021/10/09 13:29:23.531 kid1| 11,5| http.cc(1667) wroteLast: conn13
> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
> flags=1: size 3611: errflag 0.
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
> AsyncCall HttpStateData::httpTimeout constructed, this=0xe34fa0 [call69]
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(153) callEnd:
> HttpStateData status out: [ job8]
> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(61) fireNext:
> leaving HttpStateData::wroteLast(conn13 local=10.10.1.1:36928
> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(96) ScheduleCall:
> IoCallback.cc(131) will call HttpStateData::readReply(conn13
> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
> flags=1, data=0x12e9988) [call66]
> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCallQueue.cc(59) fireNext:
> entering HttpStateData::readReply(conn13 local=10.10.1.1:36928
> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(41) make: make call
> HttpStateData::readReply [call66]
> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncJob.cc(122) callStart:
> HttpStateData status in: [ job8]
> 2021/10/09 13:29:23.615 kid1| 11,5| http.cc(1215) readReply: conn13
> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
> 2021/10/09 13:29:23.615 kid1| ctx: enter level? 0: 'http://www.google.com/'
> 2021/10/09 13:29:23.615 kid1| 11,3| http.cc(666) processReplyHeader:
> processReplyHeader: key '0200000000000000843D000001000000'
> 2021/10/09 13:29:23.615 kid1| 11,2| http.cc(720) processReplyHeader:
> HTTP Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080
> FIRSTUP_PARENT FD 13 flags=1
> 2021/10/09 13:29:23.615 kid1| 11,2| http.cc(721) processReplyHeader:
> HTTP Server RESPONSE:
> ---------
> HTTP/1.1 301 Moved Permanently
> Location: https://www.google.com/
> Content-Length: 0
> Proxy-Connection: Keep-Alive
> 
> ----------
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(119) setVirginReply:
> 0x12e9988 setting virgin reply to 0x12fa850
> 2021/10/09 13:29:23.616 kid1| ctx: exit level? 0
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(973) adaptOrFinalizeReply:
> adaptationAccessCheckPending=0
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(139) setFinalReply:
> 0x12e9988 setting final reply to 0x12fa850
> 2021/10/09 13:29:23.616 kid1| ctx: enter level? 0: 'http://www.google.com/'
> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(979) haveParsedReplyHeaders:
> HTTP CODE: 301
> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1054)
> haveParsedReplyHeaders: decided: do not cache but share because refresh
> check returned non-cacheable; HTTP status 301 e:=p2XIV/0x12e63f0*3
> 2021/10/09 13:29:23.616 kid1| ctx: exit level? 0
> 2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(279) sendStartOfMessage:
> HTTP Client conn10 local=127.0.0.1:3128 remote=127.0.0.1:45192 FD 12
> flags=1
> 2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 301 Moved Permanently
> Location: https://www.google.com/
> Content-Length: 0
> Date: Sat, 09 Oct 2021 12:29:23 GMT
> X-Cache: MISS from clientproxy
> X-Cache-Lookup: MISS from clientproxy:3128
> Connection: keep-alive
> 
> 
> ----------
> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1491) processReplyBody:
> adaptationAccessCheckPending=0
> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1154) persistentConnStatus:
> conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
> flags=1 eof=0
> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1174) persistentConnStatus:
> persistentConnStatus: content_length=0
> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1178) persistentConnStatus:
> persistentConnStatus: clen=0
> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1537) processReplyBody:
> processReplyBody: COMPLETE_PERSISTENT_MSG from conn13
> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(162) serverComplete:
> serverComplete 0x12e9988
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(184) serverComplete2:
> serverComplete2 0x12e9988
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(212) completeForwarding:
> completing forwarding for 0x12e6e28*2
> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(586) cleanAdaptation:
> cleaning ICAP; ACL: 0
> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(134) ~HttpStateData:
> HttpStateData 0x12e9988 destroyed;
> 2021/10/09 13:29:23.616 kid1| 11,5| AsyncCallQueue.cc(61) fireNext:
> leaving HttpStateData::readReply(conn13 local=10.10.1.1:36928
> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
> 2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1353)
> parseHttpRequest: HTTP Client conn15 local=127.0.0.1:3128
> remote=127.0.0.1:45219 FD 12 flags=1
> 2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1354)
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET http://www.google.com/ HTTP/1.1
> Host: www.google.com
> User-Agent: curl/7.75.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
> 
> ----------
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(309) peerSelect:
> e:=IV/0x12e63f0*2 http://www.google.com/
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
> www.google.com
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(617) selectMore:
> direct = DIRECT_UNKNOWN (always_direct to be checked)
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(373)
> checkAlwaysDirectDone: DENIED
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
> www.google.com
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(626) selectMore:
> direct = DIRECT_UNKNOWN (never_direct to be checked)
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(345)
> checkNeverDirectDone: DENIED
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
> www.google.com
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(577)
> checkNetdbDirect: MY RTT = 1 msec
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(578)
> checkNetdbDirect: minimum_direct_rtt = 400 msec
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(644) selectMore:
> direct = DIRECT_YES (checkNetdbDirect)
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(650) selectMore:
> direct = DIRECT_YES
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(1098) addSelection:
> adding HIER_DIRECT#www.google.com
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(460) resolveSelected:
> Find IP destination for: http://www.google.com/' via www.google.com
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1171) handlePath:
> PeerSelector2 found conn16 local=0.0.0.0 remote=172.217.23.100:80
> HIER_DIRECT flags=1, destination #1 for http://www.google.com/
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1177) handlePath:
> always_direct = DENIED
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1178) handlePath:
> never_direct = DENIED
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1179) handlePath:
> timedout = 0
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 11,7| HttpRequest.cc(468) clearError: old:
> ERR_NONE
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(479) resolveSelected:
> PeerSelector2 found all 1 destinations for http://www.google.com/
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(480) resolveSelected:
> always_direct = DENIED
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(481) resolveSelected:
> never_direct = DENIED
> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(482) resolveSelected:
> timedout = 0
> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
> interestedInitiator: PeerSelector2
> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(241) ~PeerSelector:
> http://www.google.com/
> 2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(279) sendStartOfMessage:
> HTTP Client conn15 local=127.0.0.1:3128 remote=127.0.0.1:45219 FD 12
> flags=1
> 2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 503 Service Unavailable
> Server: squid/5.1-VCS
> Mime-Version: 1.0
> Date: Sat, 09 Oct 2021 12:30:27 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3573
> X-Squid-Error: ERR_CONNECT_FAIL 110
> Vary: Accept-Language
> Content-Language: en
> X-Cache: MISS from clientproxy
> X-Cache-Lookup: MISS from clientproxy:3128
> Connection: keep-alive
> 
> 
> ----------
> 
> 
> 
> 
> 
> 
> Thank you
> Markus
> 
> 
> 
> 
> 
> "Markus Moeller"? wrote in message news:sjrrhc$lat$1 at ciao.gmane.io...
> 
> I understand now better the concept.
> 
> Thank you
> Markus
> 
> 
> "Alex Rousskov"? wrote in message
> news:3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d at measurement-factory.com...
> 
> On 10/8/21 8:02 PM, Markus Moeller wrote:
> 
>> I try to setup a proxy chain, but don't get the setup right. I have one
>> squid with 2 parents. One with auth for domainA.com and one w/o auth for
>> the non local IPs (i.e. Internet).
> 
>> With the below config I see domainA.com still going to the
>> unauthenticated parent proxy. Any hint why ?
> 
> Several factors can explain that, but I would start by rephrasing your
> request routing requirements (and the corresponding configuration rules)
> as mutually exclusive (if they are). Currently, you have formulated and
> configured the equivalent of
> 
> * send green traffic to auth-proxy
> * send blue traffic to parent-proxy
> 
> This approach leaves important questions like "What about yellow
> traffic?" and "What about traffic with green and blue dots?" unanswered.
> 
> If you want every request to go to either auth-proxy or parent-proxy,
> then say so explicitly:
> 
> # green (and only green!) traffic to auth-proxy
> cache_peer_access auth-proxy allow green
> cache_peer_access auth-proxy deny all
> 
> # not green (and only not green!) traffic to parent-proxy
> cache_peer_access auth-proxy deny green
> cache_peer_access auth-proxy allow all
> 
> What "green" means exactly in your case, I do not know (due to the
> questions like those listed above).
> 
> 
> If you want every request to go to either auth-proxy, parent-proxy, or
> direct, then your rules will become a bit more complex, but all three
> routes should still be mutually exclusive:
> 
> # green (and only green) traffic to auth-proxy
> # but exclude traffic that should go direct
> cache_peer_access auth-proxy deny meantToGoDirect
> cache_peer_access auth-proxy allow green
> cache_peer_access auth-proxy deny all
> 
> # not green (and only not green) traffic to parent-proxy
> # but exclude traffic that should go direct
> cache_peer_access auth-proxy deny meantToGoDirect
> cache_peer_access auth-proxy deny green
> cache_peer_access auth-proxy allow all
> 
> # traffic that should go direct (and only that traffic)
> # should always go direct
> always_direct allow meantToGoDirect
> always_direct deny all
> 
> # traffic that should not go direct (and only that traffic)
> # should never go direct
> never_direct deny meantToGoDirect
> never_direct allow all
> 
> Disclaimer: The above configuration snippets are not complete, are not
> tested, and can probably be reduced (some might say "simplified") if you
> prefer to rely on certain defaults. See also: nonhierarchical_direct.
> 
> Once you get the above working for plain HTTP requests that have
> resolvable domain names as targets, please note that your listA ACL will
> not work for requests that have IP addresses, including some CONNECT
> requests that ask your Squid to tunnel HTTPS traffic. Your Squid may not
> get any such requests, but if it does, then your "green" and
> "meantToGoDirect" ACLs may need to be more complex than "dstdomain -n"
> and "dst".
> 
> 
> HTH,
> 
> Alex.
> P.S. I would not call the second proxy "parent-proxy" because both of
> your proxies are configured as parent proxies.
> 
> 
> 
>> # Recommended minimum configuration:
>> #
>>
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>> acl localnet src 0.0.0.1-0.255.255.255? # RFC 1122 "this" network (LAN)
>> acl localnet src 10.0.0.0/8???????????? # RFC 1918 local private network
>> (LAN)
>> acl localnet src 100.64.0.0/10????????? # RFC 6598 shared address space
>> (CGN)
>> acl localnet src 169.254.0.0/16???????? # RFC 3927 link-local (directly
>> plugged) machines
>> acl localnet src 172.16.0.0/12????????? # RFC 1918 local private network
>> (LAN)
>> acl localnet src 192.168.0.0/16???????? # RFC 1918 local private network
>> (LAN)
>> acl localnet src fc00::/7?????????????? # RFC 4193 local private network
>> range
>> acl localnet src fe80::/10????????????? # RFC 4291 link-local (directly
>> plugged) machines
>>
>> acl localdst dst 10.0.0.0/8???????????? # RFC 1918 local private network
>> (LAN)
>> acl localdst dst 100.64.0.0/10????????? # RFC 6598 shared address space
>> (CGN)
>> acl localdst dst 169.254.0.0/16???????? # RFC 3927 link-local (directly
>> plugged) machines
>> acl localdst dst 172.16.0.0/12????????? # RFC 1918 local private network
>> (LAN)
>> acl localdst dst 192.168.0.0/16???????? # RFC 1918 local private network
>> (LAN)
>> acl localdst dst fc00::/7?????????????? # RFC 4193 local private network
>> range
>> acl localdst dst fe80::/10????????????? # RFC 4291 link-local (directly
>> plugged) machines
>>
>> acl listA dstdomain -n? domainA.com
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80????????? # http
>> acl Safe_ports port 21????????? # ftp
>> acl Safe_ports port 443???????? # https
>> acl Safe_ports port 70????????? # gopher
>> acl Safe_ports port 210???????? # wais
>> acl Safe_ports port 1025-65535? # unregistered ports
>> acl Safe_ports port 280???????? # http-mgmt
>> acl Safe_ports port 488???????? # gss-http
>> acl Safe_ports port 591???????? # filemaker
>> acl Safe_ports port 777???????? # multiling http
>>
>> cache_peer auth-proxy parent?? 3128 0? no-query default login=NEGOTIATE
>> cache_peer parent-proxy parent?? 3128 0? no-query default
>> cache_peer_access auth-proxy allow listA
>> cache_peer_access parent-proxy allow !localdst
>> never_direct deny localdst
>> never_direct allow all
>>
>> debug_options 44,10 11,20
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From huaraz at moeller.plus.com  Sat Oct  9 17:46:23 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 9 Oct 2021 18:46:23 +0100
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <cbe23671-7b3c-e270-f3f4-593d4f030f36@measurement-factory.com>
References: <sjqm6q$elh$1@ciao.gmane.io>
 <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
 <sjrrhc$lat$1@ciao.gmane.io> <sjs44m$une$1@ciao.gmane.io>
 <cbe23671-7b3c-e270-f3f4-593d4f030f36@measurement-factory.com>
Message-ID: <sjskhi$574$1@ciao.gmane.io>

>"Alex Rousskov"  wrote in message 
>news:cbe23671-7b3c-e270-f3f4-593d4f030f36 at measurement-factory.com...
>
>On 10/9/21 9:06 AM, Markus Moeller wrote:
>> Hi,
>>
>> I have now tested with the below config and I see my first request
>> works, but the second fails. So I am not sure if it is still a
>> configuration issue or something else.
>
>
>> always_direct allow localdst
>> never_direct deny !localdst
>
>I (still) do not know what you want to achive exactly (see my previous
>response for more specific questions), but the above combination looks
>suspicious to me. I would expect traffic that should always go direct to
>be denied in the never_direct rule instead. Did you mean for that "!" to
>be there?

Apologies if it still not clear. I want to chain 2 proxies to the Internet, 
but a subset of Internet domains have to go to a "special" set of proxies. 
So i try to find a way how squid can "route" all Internet domains to a 
default proxy and a subset of well defined domains to the "special" proxy 
(and having  "internal" traffic based on IP ranges go direct)

Thank you for spotting the !. I got confused with the combinations of the 
never/always direct statement.


>
>I did not check the debugging trace carefully, but it may be the reason
>why Squid cannot forward some requests -- it is getting an
>impossible-to-satisfy or self-contradictory directions.
>
>
>BTW, thank you for posting the debugging trace! Please keep doing that
>if you need further help.
>
>Alex.
>
>
>> ....
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>> #acl localnet src 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
>> acl localnet src 10.0.0.0/8             # RFC 1918 local private network
>> (LAN)
>> acl localnet src 100.64.0.0/10          # RFC 6598 shared address space
>> (CGN)
>> acl localnet src 169.254.0.0/16         # RFC 3927 link-local (directly
>> plugged) machines
>> acl localnet src 172.16.0.0/12          # RFC 1918 local private network
>> (LAN)
>> acl localnet src 192.168.0.0/16         # RFC 1918 local private network
>> (LAN)
>> acl localnet src fc00::/7               # RFC 4193 local private network
>> range
>> acl localnet src fe80::/10              # RFC 4291 link-local (directly
>> plugged) machines
>>
>> #acl localdst dst 0.0.0.1-0.255.255.255  # RFC 1122 "this" network (LAN)
>> acl localdst dst 10.0.0.0/8             # RFC 1918 local private network
>> (LAN)
>> acl localdst dst 100.64.0.0/10          # RFC 6598 shared address space
>> (CGN)
>> acl localdst dst 169.254.0.0/16         # RFC 3927 link-local (directly
>> plugged) machines
>> acl localdst dst 172.16.0.0/12          # RFC 1918 local private network
>> (LAN)
>> acl localdst dst 192.168.0.0/16         # RFC 1918 local private network
>> (LAN)
>> acl localdst dst fc00::/7               # RFC 4193 local private network
>> range
>> acl localdst dst fe80::/10              # RFC 4291 link-local (directly
>> plugged) machines
>>
>> acl google dstdomain -n .google.com
>>
>> cache_peer internetproxy.example.com parent 8080 0 no-query no-digest
>> no-netdb-exchange default
>> cache_peer authproxy.example.com parent 8080 0 no-query no-digest
>> no-netdb-exchange default login=NEGOTIATE auth-no-keytab
>> # Only google to auth proxy
>> cache_peer_access authproxy.example.com deny localdst
>> cache_peer_access authproxy.example.com allow google
>> cache_peer_access authproxy.example.com deny all
>> # All other external domains
>> cache_peer_access internetproxy.example.com deny localdst
>> cache_peer_access internetproxy.example.com deny google
>> cache_peer_access internetproxy.example.com allow all
>> # Local goes direct
>> always_direct allow localdst
>> always_direct deny all
>> never_direct deny !localdst
>> never_direct allow all
>>
>> debug_options 44,10 11,20
>>
>> ....
>>
>> The first test looked fine:
>>
>> #curl -vvv -x http://localhost:3128 http://www.google.com
>> * Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
>> *   Trying 127.0.0.1:3128...
>> * Connected to localhost (127.0.0.1) port 3128 (#0)
>>> GET http://www.google.com/ HTTP/1.1
>>> Host: www.google.com
>>> User-Agent: curl/7.75.0
>>> Accept: */*
>>> Proxy-Connection: Keep-Alive
>>>
>> * Mark bundle as not supporting multiuse
>> < HTTP/1.1 301 Moved Permanently
>> < Location: https://www.google.com/
>> < Content-Length: 0
>> < Date: Sat, 09 Oct 2021 12:29:23 GMT
>> < X-Cache: MISS from clientproxy
>> < X-Cache-Lookup: MISS from clientproxy:3128
>> < Connection: keep-alive
>> <
>> * Connection #0 to host localhost left intact
>>
>>
>> Second request failed with a cache error:
>>
>>
>> #curl -vvv -x http://localhost:3128 http://www.google.com
>> * Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
>> *   Trying 127.0.0.1:3128...
>> * Connected to localhost (127.0.0.1) port 3128 (#0)
>>> GET http://www.google.com/ HTTP/1.1
>>> Host: www.google.com
>>> User-Agent: curl/7.75.0
>>> Accept: */*
>>> Proxy-Connection: Keep-Alive
>>>
>> * Mark bundle as not supporting multiuse
>> < HTTP/1.1 503 Service Unavailable
>> < Server: squid/5.1-VCS
>> < Mime-Version: 1.0
>> < Date: Sat, 09 Oct 2021 12:30:27 GMT
>> < Content-Type: text/html;charset=utf-8
>> < Content-Length: 3573
>> < X-Squid-Error: ERR_CONNECT_FAIL 110
>> < Vary: Accept-Language
>> < Content-Language: en
>> < X-Cache: MISS from clientproxy
>> < X-Cache-Lookup: MISS from clientproxy:3128
>> < Connection: keep-alive
>> <
>> <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
>> http://www.w3.org/TR/html4/strict.dtd>
>> <html><head>
>> <meta type="copyright" content="Copyright (C) 1996-2021 The Squid
>> Software Foundation and contributors">
>> <meta http-equiv="Content-Type" CONTENT="text/html; charset=utf-8">
>> <title>ERROR: The requested URL could not be retrieved</title>
>> .....
>>
>>
>> The cache log says:
>>
>> 2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1353)
>> parseHttpRequest: HTTP Client conn10 local=127.0.0.1:3128
>> remote=127.0.0.1:45192 FD 12 flags=1
>> 2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1354)
>> parseHttpRequest: HTTP Client REQUEST:
>> ---------
>> GET http://www.google.com/ HTTP/1.1
>> Host: www.google.com
>> User-Agent: curl/7.75.0
>> Accept: */*
>> Proxy-Connection: Keep-Alive
>>
>>
>> ----------
>> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(309) peerSelect:
>> e:=IV/0x12e63f0*2 http://www.google.com/
>> 2021/10/09 13:29:23.520 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(612) selectMore: GET
>> www.google.com
>> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(617) selectMore:
>> direct = DIRECT_UNKNOWN (always_direct to be checked)
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(373)
>> checkAlwaysDirectDone: DENIED
>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET
>> www.google.com
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(626) selectMore:
>> direct = DIRECT_UNKNOWN (never_direct to be checked)
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(345)
>> checkNeverDirectDone: DENIED
>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET
>> www.google.com
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(577)
>> checkNetdbDirect: MY RTT = 0 msec
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(578)
>> checkNetdbDirect: minimum_direct_rtt = 400 msec
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(585)
>> checkNetdbDirect: MY hops = 0
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(586)
>> checkNetdbDirect: minimum_direct_hops = 4
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(647) selectMore:
>> direct = DIRECT_MAYBE (default)
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(650) selectMore:
>> direct = DIRECT_MAYBE
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(286)
>> peerSelectIcpPing: http://www.google.com/
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(297)
>> peerSelectIcpPing: counted 0 neighbors
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(833)
>> selectSomeParent: GET www.google.com
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection:
>> adding FIRSTUP_PARENT/authproxy.example.com
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection:
>> skipping ANY_OLD_PARENT/authproxy.example.com; have
>> FIRSTUP_PARENT/authproxy.example.com
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection:
>> skipping DEFAULT_PARENT/authproxy.example.com; have
>> FIRSTUP_PARENT/authproxy.example.com
>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection:
>> adding HIER_DIRECT#www.google.com
>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(460) resolveSelected:
>> Find IP destination for: http://www.google.com/' via 
>> authproxy.example.com
>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1171) handlePath:
>> PeerSelector1 found conn11 local=0.0.0.0 remote=10.20.1.1:8080
>> FIRSTUP_PARENT flags=1, destination #1 for http://www.google.com/
>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1177) handlePath:
>> always_direct = DENIED
>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1178) handlePath:
>> never_direct = DENIED
>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1179) handlePath:
>> timedout = 0
>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.523 kid1| 11,7| HttpRequest.cc(468) clearError: old:
>> ERR_NONE
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(460) resolveSelected:
>> Find IP destination for: http://www.google.com/' via www.google.com
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1171) handlePath:
>> PeerSelector1 found conn12 local=0.0.0.0 remote=172.217.23.100:80
>> HIER_DIRECT flags=1, destination #2 for http://www.google.com/
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1177) handlePath:
>> always_direct = DENIED
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1178) handlePath:
>> never_direct = DENIED
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1179) handlePath:
>> timedout = 0
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(479) resolveSelected:
>> PeerSelector1 found all 2 destinations for http://www.google.com/
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(480) resolveSelected:
>> always_direct = DENIED
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(481) resolveSelected:
>> never_direct = DENIED
>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(482) resolveSelected:
>> timedout = 0
>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector1
>> 2021/10/09 13:29:23.524 kid1| 44,3| peer_select.cc(241) ~PeerSelector:
>> http://www.google.com/
>> 2021/10/09 13:29:23.526 kid1| 11,4| HttpRequest.cc(453) prepForPeering:
>> 0x1154cf0 to authproxy.example.com proxy
>> 2021/10/09 13:29:23.526 kid1| 11,3| http.cc(2486) httpStart: GET
>> http://www.google.com/
>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(87) HttpStateData:
>> HttpStateData 0x12e9988 created
>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2367) sendRequest: conn13
>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>> flags=1, request 0x1154cf0*6, this 0x12e9988.
>> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>> AsyncCall HttpStateData::httpTimeout constructed, this=0x12e8920 [call65]
>> 2021/10/09 13:29:23.527 kid1| 11,8| http.cc(1656)
>> maybeMakeSpaceAvailable: may read up to 65536 bytes info buf(0/65536)
>> from conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT
>> FD 13 flags=1
>> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>> AsyncCall HttpStateData::readReply constructed, this=0x12f9c10 [call66]
>> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>> AsyncCall HttpStateData::wroteLast constructed, this=0x12f9cc0 [call67]
>> 2021/10/09 13:29:23.527 kid1| 11,8| http.cc(2309) decideIfWeDoRanges:
>> decideIfWeDoRanges: range specs: 0, cachable: 1; we_do_ranges: 0
>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>> httpBuildRequestHeader: User-Agent: curl/7.75.0
>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>> httpBuildRequestHeader: Accept: */*
>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>> httpBuildRequestHeader: Proxy-Connection: Keep-Alive
>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>> httpBuildRequestHeader: Host: www.google.com
>> 2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(539)
>> peer_proxy_negotiate_auth: Import gss name
>> 2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(546)
>> peer_proxy_negotiate_auth: Initialize gss security context
>> 2021/10/09 13:29:23.531 kid1| 11,5| peer_proxy_negotiate_auth.cc(560)
>> peer_proxy_negotiate_auth: Got token with length 2568
>> 2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2442) sendRequest: HTTP
>> Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT
>> FD 13 flags=1
>> 2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2443) sendRequest: HTTP
>> Server REQUEST:
>> ---------
>> GET http://www.google.com/ HTTP/1.1
>> User-Agent: curl/7.75.0
>> Accept: */*
>> Host: www.google.com
>> Proxy-Authorization: Negotiate YIIK....
>> Cache-Control: max-age=259200
>> Connection: keep-alive
>>
>>
>> ----------
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(96) ScheduleCall:
>> IoCallback.cc(131) will call HttpStateData::wroteLast(conn13
>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>> flags=1, data=0x12e9988) [call67]
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(59) fireNext:
>> entering HttpStateData::wroteLast(conn13 local=10.10.1.1:36928
>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(41) make: make call
>> HttpStateData::wroteLast [call67]
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(122) callStart:
>> HttpStateData status in: [ job8]
>> 2021/10/09 13:29:23.531 kid1| 11,5| http.cc(1667) wroteLast: conn13
>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>> flags=1: size 3611: errflag 0.
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>> AsyncCall HttpStateData::httpTimeout constructed, this=0xe34fa0 [call69]
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(153) callEnd:
>> HttpStateData status out: [ job8]
>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(61) fireNext:
>> leaving HttpStateData::wroteLast(conn13 local=10.10.1.1:36928
>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(96) ScheduleCall:
>> IoCallback.cc(131) will call HttpStateData::readReply(conn13
>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>> flags=1, data=0x12e9988) [call66]
>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCallQueue.cc(59) fireNext:
>> entering HttpStateData::readReply(conn13 local=10.10.1.1:36928
>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(41) make: make call
>> HttpStateData::readReply [call66]
>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncJob.cc(122) callStart:
>> HttpStateData status in: [ job8]
>> 2021/10/09 13:29:23.615 kid1| 11,5| http.cc(1215) readReply: conn13
>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
>> 2021/10/09 13:29:23.615 kid1| ctx: enter level  0: 
>> 'http://www.google.com/'
>> 2021/10/09 13:29:23.615 kid1| 11,3| http.cc(666) processReplyHeader:
>> processReplyHeader: key '0200000000000000843D000001000000'
>> 2021/10/09 13:29:23.615 kid1| 11,2| http.cc(720) processReplyHeader:
>> HTTP Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080
>> FIRSTUP_PARENT FD 13 flags=1
>> 2021/10/09 13:29:23.615 kid1| 11,2| http.cc(721) processReplyHeader:
>> HTTP Server RESPONSE:
>> ---------
>> HTTP/1.1 301 Moved Permanently
>> Location: https://www.google.com/
>> Content-Length: 0
>> Proxy-Connection: Keep-Alive
>>
>> ----------
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(119) setVirginReply:
>> 0x12e9988 setting virgin reply to 0x12fa850
>> 2021/10/09 13:29:23.616 kid1| ctx: exit level  0
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(973) adaptOrFinalizeReply:
>> adaptationAccessCheckPending=0
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(139) setFinalReply:
>> 0x12e9988 setting final reply to 0x12fa850
>> 2021/10/09 13:29:23.616 kid1| ctx: enter level  0: 
>> 'http://www.google.com/'
>> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(979) haveParsedReplyHeaders:
>> HTTP CODE: 301
>> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1054)
>> haveParsedReplyHeaders: decided: do not cache but share because refresh
>> check returned non-cacheable; HTTP status 301 e:=p2XIV/0x12e63f0*3
>> 2021/10/09 13:29:23.616 kid1| ctx: exit level  0
>> 2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(279) sendStartOfMessage:
>> HTTP Client conn10 local=127.0.0.1:3128 remote=127.0.0.1:45192 FD 12
>> flags=1
>> 2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
>> HTTP Client REPLY:
>> ---------
>> HTTP/1.1 301 Moved Permanently
>> Location: https://www.google.com/
>> Content-Length: 0
>> Date: Sat, 09 Oct 2021 12:29:23 GMT
>> X-Cache: MISS from clientproxy
>> X-Cache-Lookup: MISS from clientproxy:3128
>> Connection: keep-alive
>>
>>
>> ----------
>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1491) processReplyBody:
>> adaptationAccessCheckPending=0
>> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1154) persistentConnStatus:
>> conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>> flags=1 eof=0
>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1174) persistentConnStatus:
>> persistentConnStatus: content_length=0
>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1178) persistentConnStatus:
>> persistentConnStatus: clen=0
>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1537) processReplyBody:
>> processReplyBody: COMPLETE_PERSISTENT_MSG from conn13
>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(162) serverComplete:
>> serverComplete 0x12e9988
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(184) serverComplete2:
>> serverComplete2 0x12e9988
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(212) completeForwarding:
>> completing forwarding for 0x12e6e28*2
>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(586) cleanAdaptation:
>> cleaning ICAP; ACL: 0
>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(134) ~HttpStateData:
>> HttpStateData 0x12e9988 destroyed;
>> 2021/10/09 13:29:23.616 kid1| 11,5| AsyncCallQueue.cc(61) fireNext:
>> leaving HttpStateData::readReply(conn13 local=10.10.1.1:36928
>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>> 2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1353)
>> parseHttpRequest: HTTP Client conn15 local=127.0.0.1:3128
>> remote=127.0.0.1:45219 FD 12 flags=1
>> 2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1354)
>> parseHttpRequest: HTTP Client REQUEST:
>> ---------
>> GET http://www.google.com/ HTTP/1.1
>> Host: www.google.com
>> User-Agent: curl/7.75.0
>> Accept: */*
>> Proxy-Connection: Keep-Alive
>>
>>
>> ----------
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(309) peerSelect:
>> e:=IV/0x12e63f0*2 http://www.google.com/
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
>> www.google.com
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(617) selectMore:
>> direct = DIRECT_UNKNOWN (always_direct to be checked)
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(373)
>> checkAlwaysDirectDone: DENIED
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
>> www.google.com
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(626) selectMore:
>> direct = DIRECT_UNKNOWN (never_direct to be checked)
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(345)
>> checkNeverDirectDone: DENIED
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
>> www.google.com
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(577)
>> checkNetdbDirect: MY RTT = 1 msec
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(578)
>> checkNetdbDirect: minimum_direct_rtt = 400 msec
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(644) selectMore:
>> direct = DIRECT_YES (checkNetdbDirect)
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(650) selectMore:
>> direct = DIRECT_YES
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(1098) addSelection:
>> adding HIER_DIRECT#www.google.com
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(460) resolveSelected:
>> Find IP destination for: http://www.google.com/' via www.google.com
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1171) handlePath:
>> PeerSelector2 found conn16 local=0.0.0.0 remote=172.217.23.100:80
>> HIER_DIRECT flags=1, destination #1 for http://www.google.com/
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1177) handlePath:
>> always_direct = DENIED
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1178) handlePath:
>> never_direct = DENIED
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1179) handlePath:
>> timedout = 0
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 11,7| HttpRequest.cc(468) clearError: old:
>> ERR_NONE
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(479) resolveSelected:
>> PeerSelector2 found all 1 destinations for http://www.google.com/
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(480) resolveSelected:
>> always_direct = DENIED
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(481) resolveSelected:
>> never_direct = DENIED
>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(482) resolveSelected:
>> timedout = 0
>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>> interestedInitiator: PeerSelector2
>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(241) ~PeerSelector:
>> http://www.google.com/
>> 2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(279) sendStartOfMessage:
>> HTTP Client conn15 local=127.0.0.1:3128 remote=127.0.0.1:45219 FD 12
>> flags=1
>> 2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
>> HTTP Client REPLY:
>> ---------
>> HTTP/1.1 503 Service Unavailable
>> Server: squid/5.1-VCS
>> Mime-Version: 1.0
>> Date: Sat, 09 Oct 2021 12:30:27 GMT
>> Content-Type: text/html;charset=utf-8
>> Content-Length: 3573
>> X-Squid-Error: ERR_CONNECT_FAIL 110
>> Vary: Accept-Language
>> Content-Language: en
>> X-Cache: MISS from clientproxy
>> X-Cache-Lookup: MISS from clientproxy:3128
>> Connection: keep-alive
>>
>>
>> ----------
>>
>>
>>
>>
>>
>>
>> Thank you
>> Markus
>>
>>
>>
>>
>>
>> "Markus Moeller"  wrote in message news:sjrrhc$lat$1 at ciao.gmane.io...
>>
>> I understand now better the concept.
>>
>> Thank you
>> Markus
>>
>>
>

Markus 




From rousskov at measurement-factory.com  Sat Oct  9 21:59:10 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 9 Oct 2021 17:59:10 -0400
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <sjskhi$574$1@ciao.gmane.io>
References: <sjqm6q$elh$1@ciao.gmane.io>
 <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
 <sjrrhc$lat$1@ciao.gmane.io> <sjs44m$une$1@ciao.gmane.io>
 <cbe23671-7b3c-e270-f3f4-593d4f030f36@measurement-factory.com>
 <sjskhi$574$1@ciao.gmane.io>
Message-ID: <7e75c2bf-51db-f8c3-73f0-ba7fca55efcb@measurement-factory.com>

On 10/9/21 1:46 PM, Markus Moeller wrote:
> i try to find a way how squid can "route" all Internet
> domains to a default proxy and a subset of well defined domains to the
> "special" proxy (and having? "internal" traffic based on IP ranges go
> direct)

Assuming the latter conditions overwrite the former ones, the part that
remains unclear is what you want Squid to do when the request does not
match any of the three conditions above. For example, consider a request
that uses an IP address as a destination, and that IP address is not in
the "go direct" range, and its reverse DNS lookup is unsuccessful so
there is no "domain" that the proxy selection rules are based on.

Another similar question is what should Squid do with domain names that
do not resolve to an IP address. Since Squid is configured to use parent
proxies, Squid could let those proxies try to resolve the domain name,
blindly assuming that the resolution at a parent proxy will not match
one of the "go direct" IPs (a matches would possibly indicate that the
decision to go to a parent proxy was wrong in the first place!).

The final set of questions deals with HTTPS traffic. For example, if
clients sent HTTPS requests, are you OK with Squid making routing
decisions based on the target of the initial CONNECT request?


> Thank you for spotting the !. I got confused with the combinations of
> the never/always direct statement.

Does your test case work after removing that "!"? If not, please share
the updated debugging snippets.


Thank you,

Alex.


>>> ....
>>> # Example rule allowing access from your local networks.
>>> # Adapt to list your (internal) IP networks from where browsing
>>> # should be allowed
>>> #acl localnet src 0.0.0.1-0.255.255.255? # RFC 1122 "this" network (LAN)
>>> acl localnet src 10.0.0.0/8???????????? # RFC 1918 local private network
>>> (LAN)
>>> acl localnet src 100.64.0.0/10????????? # RFC 6598 shared address space
>>> (CGN)
>>> acl localnet src 169.254.0.0/16???????? # RFC 3927 link-local (directly
>>> plugged) machines
>>> acl localnet src 172.16.0.0/12????????? # RFC 1918 local private network
>>> (LAN)
>>> acl localnet src 192.168.0.0/16???????? # RFC 1918 local private network
>>> (LAN)
>>> acl localnet src fc00::/7?????????????? # RFC 4193 local private network
>>> range
>>> acl localnet src fe80::/10????????????? # RFC 4291 link-local (directly
>>> plugged) machines
>>>
>>> #acl localdst dst 0.0.0.1-0.255.255.255? # RFC 1122 "this" network (LAN)
>>> acl localdst dst 10.0.0.0/8???????????? # RFC 1918 local private network
>>> (LAN)
>>> acl localdst dst 100.64.0.0/10????????? # RFC 6598 shared address space
>>> (CGN)
>>> acl localdst dst 169.254.0.0/16???????? # RFC 3927 link-local (directly
>>> plugged) machines
>>> acl localdst dst 172.16.0.0/12????????? # RFC 1918 local private network
>>> (LAN)
>>> acl localdst dst 192.168.0.0/16???????? # RFC 1918 local private network
>>> (LAN)
>>> acl localdst dst fc00::/7?????????????? # RFC 4193 local private network
>>> range
>>> acl localdst dst fe80::/10????????????? # RFC 4291 link-local (directly
>>> plugged) machines
>>>
>>> acl google dstdomain -n .google.com
>>>
>>> cache_peer internetproxy.example.com parent 8080 0 no-query no-digest
>>> no-netdb-exchange default
>>> cache_peer authproxy.example.com parent 8080 0 no-query no-digest
>>> no-netdb-exchange default login=NEGOTIATE auth-no-keytab
>>> # Only google to auth proxy
>>> cache_peer_access authproxy.example.com deny localdst
>>> cache_peer_access authproxy.example.com allow google
>>> cache_peer_access authproxy.example.com deny all
>>> # All other external domains
>>> cache_peer_access internetproxy.example.com deny localdst
>>> cache_peer_access internetproxy.example.com deny google
>>> cache_peer_access internetproxy.example.com allow all
>>> # Local goes direct
>>> always_direct allow localdst
>>> always_direct deny all
>>> never_direct deny !localdst
>>> never_direct allow all
>>>
>>> debug_options 44,10 11,20
>>>
>>> ....
>>>
>>> The first test looked fine:
>>>
>>> #curl -vvv -x http://localhost:3128 http://www.google.com
>>> * Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
>>> *?? Trying 127.0.0.1:3128...
>>> * Connected to localhost (127.0.0.1) port 3128 (#0)
>>>> GET http://www.google.com/ HTTP/1.1
>>>> Host: www.google.com
>>>> User-Agent: curl/7.75.0
>>>> Accept: */*
>>>> Proxy-Connection: Keep-Alive
>>>>
>>> * Mark bundle as not supporting multiuse
>>> < HTTP/1.1 301 Moved Permanently
>>> < Location: https://www.google.com/
>>> < Content-Length: 0
>>> < Date: Sat, 09 Oct 2021 12:29:23 GMT
>>> < X-Cache: MISS from clientproxy
>>> < X-Cache-Lookup: MISS from clientproxy:3128
>>> < Connection: keep-alive
>>> <
>>> * Connection #0 to host localhost left intact
>>>
>>>
>>> Second request failed with a cache error:
>>>
>>>
>>> #curl -vvv -x http://localhost:3128 http://www.google.com
>>> * Uses proxy env variable no_proxy == 'localhost, 127.0.0.1'
>>> *?? Trying 127.0.0.1:3128...
>>> * Connected to localhost (127.0.0.1) port 3128 (#0)
>>>> GET http://www.google.com/ HTTP/1.1
>>>> Host: www.google.com
>>>> User-Agent: curl/7.75.0
>>>> Accept: */*
>>>> Proxy-Connection: Keep-Alive
>>>>
>>> * Mark bundle as not supporting multiuse
>>> < HTTP/1.1 503 Service Unavailable
>>> < Server: squid/5.1-VCS
>>> < Mime-Version: 1.0
>>> < Date: Sat, 09 Oct 2021 12:30:27 GMT
>>> < Content-Type: text/html;charset=utf-8
>>> < Content-Length: 3573
>>> < X-Squid-Error: ERR_CONNECT_FAIL 110
>>> < Vary: Accept-Language
>>> < Content-Language: en
>>> < X-Cache: MISS from clientproxy
>>> < X-Cache-Lookup: MISS from clientproxy:3128
>>> < Connection: keep-alive
>>> <
>>> <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
>>> http://www.w3.org/TR/html4/strict.dtd>
>>> <html><head>
>>> <meta type="copyright" content="Copyright (C) 1996-2021 The Squid
>>> Software Foundation and contributors">
>>> <meta http-equiv="Content-Type" CONTENT="text/html; charset=utf-8">
>>> <title>ERROR: The requested URL could not be retrieved</title>
>>> .....
>>>
>>>
>>> The cache log says:
>>>
>>> 2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1353)
>>> parseHttpRequest: HTTP Client conn10 local=127.0.0.1:3128
>>> remote=127.0.0.1:45192 FD 12 flags=1
>>> 2021/10/09 13:29:23.520 kid1| 11,2| client_side.cc(1354)
>>> parseHttpRequest: HTTP Client REQUEST:
>>> ---------
>>> GET http://www.google.com/ HTTP/1.1
>>> Host: www.google.com
>>> User-Agent: curl/7.75.0
>>> Accept: */*
>>> Proxy-Connection: Keep-Alive
>>>
>>>
>>> ----------
>>> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(309) peerSelect:
>>> e:=IV/0x12e63f0*2 http://www.google.com/
>>> 2021/10/09 13:29:23.520 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(612) selectMore: GET
>>> www.google.com
>>> 2021/10/09 13:29:23.520 kid1| 44,3| peer_select.cc(617) selectMore:
>>> direct = DIRECT_UNKNOWN (always_direct to be checked)
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(373)
>>> checkAlwaysDirectDone: DENIED
>>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET
>>> www.google.com
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(626) selectMore:
>>> direct = DIRECT_UNKNOWN (never_direct to be checked)
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(345)
>>> checkNeverDirectDone: DENIED
>>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(612) selectMore: GET
>>> www.google.com
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(577)
>>> checkNetdbDirect: MY RTT = 0 msec
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(578)
>>> checkNetdbDirect: minimum_direct_rtt = 400 msec
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(585)
>>> checkNetdbDirect: MY hops = 0
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(586)
>>> checkNetdbDirect: minimum_direct_hops = 4
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(647) selectMore:
>>> direct = DIRECT_MAYBE (default)
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(650) selectMore:
>>> direct = DIRECT_MAYBE
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(286)
>>> peerSelectIcpPing: http://www.google.com/
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(297)
>>> peerSelectIcpPing: counted 0 neighbors
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(833)
>>> selectSomeParent: GET www.google.com
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection:
>>> adding FIRSTUP_PARENT/authproxy.example.com
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection:
>>> skipping ANY_OLD_PARENT/authproxy.example.com; have
>>> FIRSTUP_PARENT/authproxy.example.com
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1091) addSelection:
>>> skipping DEFAULT_PARENT/authproxy.example.com; have
>>> FIRSTUP_PARENT/authproxy.example.com
>>> 2021/10/09 13:29:23.523 kid1| 44,3| peer_select.cc(1098) addSelection:
>>> adding HIER_DIRECT#www.google.com
>>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(460) resolveSelected:
>>> Find IP destination for: http://www.google.com/' via
>>> authproxy.example.com
>>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1171) handlePath:
>>> PeerSelector1 found conn11 local=0.0.0.0 remote=10.20.1.1:8080
>>> FIRSTUP_PARENT flags=1, destination #1 for http://www.google.com/
>>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1177) handlePath:
>>> always_direct = DENIED
>>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1178) handlePath:
>>> never_direct = DENIED
>>> 2021/10/09 13:29:23.523 kid1| 44,2| peer_select.cc(1179) handlePath:
>>> timedout = 0
>>> 2021/10/09 13:29:23.523 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.523 kid1| 11,7| HttpRequest.cc(468) clearError: old:
>>> ERR_NONE
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(460) resolveSelected:
>>> Find IP destination for: http://www.google.com/' via www.google.com
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1171) handlePath:
>>> PeerSelector1 found conn12 local=0.0.0.0 remote=172.217.23.100:80
>>> HIER_DIRECT flags=1, destination #2 for http://www.google.com/
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1177) handlePath:
>>> always_direct = DENIED
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1178) handlePath:
>>> never_direct = DENIED
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(1179) handlePath:
>>> timedout = 0
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(479) resolveSelected:
>>> PeerSelector1 found all 2 destinations for http://www.google.com/
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(480) resolveSelected:
>>> always_direct = DENIED
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(481) resolveSelected:
>>> never_direct = DENIED
>>> 2021/10/09 13:29:23.524 kid1| 44,2| peer_select.cc(482) resolveSelected:
>>> timedout = 0
>>> 2021/10/09 13:29:23.524 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector1
>>> 2021/10/09 13:29:23.524 kid1| 44,3| peer_select.cc(241) ~PeerSelector:
>>> http://www.google.com/
>>> 2021/10/09 13:29:23.526 kid1| 11,4| HttpRequest.cc(453) prepForPeering:
>>> 0x1154cf0 to authproxy.example.com proxy
>>> 2021/10/09 13:29:23.526 kid1| 11,3| http.cc(2486) httpStart: GET
>>> http://www.google.com/
>>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(87) HttpStateData:
>>> HttpStateData 0x12e9988 created
>>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2367) sendRequest: conn13
>>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>>> flags=1, request 0x1154cf0*6, this 0x12e9988.
>>> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>>> AsyncCall HttpStateData::httpTimeout constructed, this=0x12e8920
>>> [call65]
>>> 2021/10/09 13:29:23.527 kid1| 11,8| http.cc(1656)
>>> maybeMakeSpaceAvailable: may read up to 65536 bytes info buf(0/65536)
>>> from conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT
>>> FD 13 flags=1
>>> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>>> AsyncCall HttpStateData::readReply constructed, this=0x12f9c10 [call66]
>>> 2021/10/09 13:29:23.527 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>>> AsyncCall HttpStateData::wroteLast constructed, this=0x12f9cc0 [call67]
>>> 2021/10/09 13:29:23.527 kid1| 11,8| http.cc(2309) decideIfWeDoRanges:
>>> decideIfWeDoRanges: range specs: 0, cachable: 1; we_do_ranges: 0
>>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>>> httpBuildRequestHeader: User-Agent: curl/7.75.0
>>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>>> httpBuildRequestHeader: Accept: */*
>>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>>> httpBuildRequestHeader: Proxy-Connection: Keep-Alive
>>> 2021/10/09 13:29:23.527 kid1| 11,5| http.cc(2113)
>>> copyOneHeaderFromClientsideRequestToUpstreamRequest:
>>> httpBuildRequestHeader: Host: www.google.com
>>> 2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(539)
>>> peer_proxy_negotiate_auth: Import gss name
>>> 2021/10/09 13:29:23.527 kid1| 11,5| peer_proxy_negotiate_auth.cc(546)
>>> peer_proxy_negotiate_auth: Initialize gss security context
>>> 2021/10/09 13:29:23.531 kid1| 11,5| peer_proxy_negotiate_auth.cc(560)
>>> peer_proxy_negotiate_auth: Got token with length 2568
>>> 2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2442) sendRequest: HTTP
>>> Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT
>>> FD 13 flags=1
>>> 2021/10/09 13:29:23.531 kid1| 11,2| http.cc(2443) sendRequest: HTTP
>>> Server REQUEST:
>>> ---------
>>> GET http://www.google.com/ HTTP/1.1
>>> User-Agent: curl/7.75.0
>>> Accept: */*
>>> Host: www.google.com
>>> Proxy-Authorization: Negotiate YIIK....
>>> Cache-Control: max-age=259200
>>> Connection: keep-alive
>>>
>>>
>>> ----------
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(96) ScheduleCall:
>>> IoCallback.cc(131) will call HttpStateData::wroteLast(conn13
>>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>>> flags=1, data=0x12e9988) [call67]
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(59) fireNext:
>>> entering HttpStateData::wroteLast(conn13 local=10.10.1.1:36928
>>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(41) make: make call
>>> HttpStateData::wroteLast [call67]
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(122) callStart:
>>> HttpStateData status in: [ job8]
>>> 2021/10/09 13:29:23.531 kid1| 11,5| http.cc(1667) wroteLast: conn13
>>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>>> flags=1: size 3611: errflag 0.
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCall.cc(29) AsyncCall: The
>>> AsyncCall HttpStateData::httpTimeout constructed, this=0xe34fa0 [call69]
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncJob.cc(153) callEnd:
>>> HttpStateData status out: [ job8]
>>> 2021/10/09 13:29:23.531 kid1| 11,5| AsyncCallQueue.cc(61) fireNext:
>>> leaving HttpStateData::wroteLast(conn13 local=10.10.1.1:36928
>>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(96) ScheduleCall:
>>> IoCallback.cc(131) will call HttpStateData::readReply(conn13
>>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>>> flags=1, data=0x12e9988) [call66]
>>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCallQueue.cc(59) fireNext:
>>> entering HttpStateData::readReply(conn13 local=10.10.1.1:36928
>>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncCall.cc(41) make: make call
>>> HttpStateData::readReply [call66]
>>> 2021/10/09 13:29:23.615 kid1| 11,5| AsyncJob.cc(122) callStart:
>>> HttpStateData status in: [ job8]
>>> 2021/10/09 13:29:23.615 kid1| 11,5| http.cc(1215) readReply: conn13
>>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
>>> 2021/10/09 13:29:23.615 kid1| ctx: enter level? 0:
>>> 'http://www.google.com/'
>>> 2021/10/09 13:29:23.615 kid1| 11,3| http.cc(666) processReplyHeader:
>>> processReplyHeader: key '0200000000000000843D000001000000'
>>> 2021/10/09 13:29:23.615 kid1| 11,2| http.cc(720) processReplyHeader:
>>> HTTP Server conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080
>>> FIRSTUP_PARENT FD 13 flags=1
>>> 2021/10/09 13:29:23.615 kid1| 11,2| http.cc(721) processReplyHeader:
>>> HTTP Server RESPONSE:
>>> ---------
>>> HTTP/1.1 301 Moved Permanently
>>> Location: https://www.google.com/
>>> Content-Length: 0
>>> Proxy-Connection: Keep-Alive
>>>
>>> ----------
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(119) setVirginReply:
>>> 0x12e9988 setting virgin reply to 0x12fa850
>>> 2021/10/09 13:29:23.616 kid1| ctx: exit level? 0
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(973) adaptOrFinalizeReply:
>>> adaptationAccessCheckPending=0
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(139) setFinalReply:
>>> 0x12e9988 setting final reply to 0x12fa850
>>> 2021/10/09 13:29:23.616 kid1| ctx: enter level? 0:
>>> 'http://www.google.com/'
>>> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(979) haveParsedReplyHeaders:
>>> HTTP CODE: 301
>>> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1054)
>>> haveParsedReplyHeaders: decided: do not cache but share because refresh
>>> check returned non-cacheable; HTTP status 301 e:=p2XIV/0x12e63f0*3
>>> 2021/10/09 13:29:23.616 kid1| ctx: exit level? 0
>>> 2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(279) sendStartOfMessage:
>>> HTTP Client conn10 local=127.0.0.1:3128 remote=127.0.0.1:45192 FD 12
>>> flags=1
>>> 2021/10/09 13:29:23.616 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
>>> HTTP Client REPLY:
>>> ---------
>>> HTTP/1.1 301 Moved Permanently
>>> Location: https://www.google.com/
>>> Content-Length: 0
>>> Date: Sat, 09 Oct 2021 12:29:23 GMT
>>> X-Cache: MISS from clientproxy
>>> X-Cache-Lookup: MISS from clientproxy:3128
>>> Connection: keep-alive
>>>
>>>
>>> ----------
>>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1491) processReplyBody:
>>> adaptationAccessCheckPending=0
>>> 2021/10/09 13:29:23.616 kid1| 11,3| http.cc(1154) persistentConnStatus:
>>> conn13 local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13
>>> flags=1 eof=0
>>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1174) persistentConnStatus:
>>> persistentConnStatus: content_length=0
>>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1178) persistentConnStatus:
>>> persistentConnStatus: clen=0
>>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(1537) processReplyBody:
>>> processReplyBody: COMPLETE_PERSISTENT_MSG from conn13
>>> local=10.10.1.1:36928 remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(162) serverComplete:
>>> serverComplete 0x12e9988
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(184) serverComplete2:
>>> serverComplete2 0x12e9988
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(212) completeForwarding:
>>> completing forwarding for 0x12e6e28*2
>>> 2021/10/09 13:29:23.616 kid1| 11,5| Client.cc(586) cleanAdaptation:
>>> cleaning ICAP; ACL: 0
>>> 2021/10/09 13:29:23.616 kid1| 11,5| http.cc(134) ~HttpStateData:
>>> HttpStateData 0x12e9988 destroyed;
>>> 2021/10/09 13:29:23.616 kid1| 11,5| AsyncCallQueue.cc(61) fireNext:
>>> leaving HttpStateData::readReply(conn13 local=10.10.1.1:36928
>>> remote=10.20.1.1:8080 FIRSTUP_PARENT FD 13 flags=1, data=0x12e9988)
>>> 2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1353)
>>> parseHttpRequest: HTTP Client conn15 local=127.0.0.1:3128
>>> remote=127.0.0.1:45219 FD 12 flags=1
>>> 2021/10/09 13:29:27.287 kid1| 11,2| client_side.cc(1354)
>>> parseHttpRequest: HTTP Client REQUEST:
>>> ---------
>>> GET http://www.google.com/ HTTP/1.1
>>> Host: www.google.com
>>> User-Agent: curl/7.75.0
>>> Accept: */*
>>> Proxy-Connection: Keep-Alive
>>>
>>>
>>> ----------
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(309) peerSelect:
>>> e:=IV/0x12e63f0*2 http://www.google.com/
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
>>> www.google.com
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(617) selectMore:
>>> direct = DIRECT_UNKNOWN (always_direct to be checked)
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(373)
>>> checkAlwaysDirectDone: DENIED
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
>>> www.google.com
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(626) selectMore:
>>> direct = DIRECT_UNKNOWN (never_direct to be checked)
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(345)
>>> checkNeverDirectDone: DENIED
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(612) selectMore: GET
>>> www.google.com
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(577)
>>> checkNetdbDirect: MY RTT = 1 msec
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(578)
>>> checkNetdbDirect: minimum_direct_rtt = 400 msec
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(644) selectMore:
>>> direct = DIRECT_YES (checkNetdbDirect)
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(650) selectMore:
>>> direct = DIRECT_YES
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(1098) addSelection:
>>> adding HIER_DIRECT#www.google.com
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(460) resolveSelected:
>>> Find IP destination for: http://www.google.com/' via www.google.com
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1171) handlePath:
>>> PeerSelector2 found conn16 local=0.0.0.0 remote=172.217.23.100:80
>>> HIER_DIRECT flags=1, destination #1 for http://www.google.com/
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1177) handlePath:
>>> always_direct = DENIED
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1178) handlePath:
>>> never_direct = DENIED
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(1179) handlePath:
>>> timedout = 0
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 11,7| HttpRequest.cc(468) clearError: old:
>>> ERR_NONE
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(479) resolveSelected:
>>> PeerSelector2 found all 1 destinations for http://www.google.com/
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(480) resolveSelected:
>>> always_direct = DENIED
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(481) resolveSelected:
>>> never_direct = DENIED
>>> 2021/10/09 13:29:27.287 kid1| 44,2| peer_select.cc(482) resolveSelected:
>>> timedout = 0
>>> 2021/10/09 13:29:27.287 kid1| 44,7| peer_select.cc(1149)
>>> interestedInitiator: PeerSelector2
>>> 2021/10/09 13:29:27.287 kid1| 44,3| peer_select.cc(241) ~PeerSelector:
>>> http://www.google.com/
>>> 2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(279) sendStartOfMessage:
>>> HTTP Client conn15 local=127.0.0.1:3128 remote=127.0.0.1:45219 FD 12
>>> flags=1
>>> 2021/10/09 13:30:27.421 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
>>> HTTP Client REPLY:
>>> ---------
>>> HTTP/1.1 503 Service Unavailable
>>> Server: squid/5.1-VCS
>>> Mime-Version: 1.0
>>> Date: Sat, 09 Oct 2021 12:30:27 GMT
>>> Content-Type: text/html;charset=utf-8
>>> Content-Length: 3573
>>> X-Squid-Error: ERR_CONNECT_FAIL 110
>>> Vary: Accept-Language
>>> Content-Language: en
>>> X-Cache: MISS from clientproxy
>>> X-Cache-Lookup: MISS from clientproxy:3128
>>> Connection: keep-alive
>>>
>>>
>>> ----------
>>>
>>>
>>>
>>>
>>>
>>>
>>> Thank you
>>> Markus
>>>
>>>
>>>
>>>
>>>
>>> "Markus Moeller"? wrote in message news:sjrrhc$lat$1 at ciao.gmane.io...
>>>
>>> I understand now better the concept.
>>>
>>> Thank you
>>> Markus
>>>
>>>
>>
> 
> Markus
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From huaraz at moeller.plus.com  Sat Oct  9 23:53:53 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 10 Oct 2021 00:53:53 +0100
Subject: [squid-users] squid 5 and parent peers
In-Reply-To: <7e75c2bf-51db-f8c3-73f0-ba7fca55efcb@measurement-factory.com>
References: <sjqm6q$elh$1@ciao.gmane.io>
 <3dec529a-b62e-1e95-6cb7-0b68f6bf3c8d@measurement-factory.com>
 <sjrrhc$lat$1@ciao.gmane.io> <sjs44m$une$1@ciao.gmane.io>
 <cbe23671-7b3c-e270-f3f4-593d4f030f36@measurement-factory.com>
 <sjskhi$574$1@ciao.gmane.io>
 <7e75c2bf-51db-f8c3-73f0-ba7fca55efcb@measurement-factory.com>
Message-ID: <sjtabp$7tp$1@ciao.gmane.io>

>
>
>"Alex Rousskov"  wrote in message 
>news:7e75c2bf-51db-f8c3-73f0-ba7fca55efcb at measurement-factory.com...
>
>On 10/9/21 1:46 PM, Markus Moeller wrote:
>> i try to find a way how squid can "route" all Internet
>> domains to a default proxy and a subset of well defined domains to the
>> "special" proxy (and having  "internal" traffic based on IP ranges go
>> direct)
>
>Assuming the latter conditions overwrite the former ones, the part that
>remains unclear is what you want Squid to do when the request does not
>match any of the three conditions above. For example, consider a request
>that uses an IP address as a destination, and that IP address is not in
>the "go direct" range, and its reverse DNS lookup is unsuccessful so
>there is no "domain" that the proxy selection rules are based on.
>

Thank you I am aware of these "edge" cases. Do I assume correctly if an IP 
use used and no reverse DNS is performed it would forward to the Internet 
proxy (in my example)


>Another similar question is what should Squid do with domain names that
>do not resolve to an IP address. Since Squid is configured to use parent
>proxies, Squid could let those proxies try to resolve the domain name,
>blindly assuming that the resolution at a parent proxy will not match
>one of the "go direct" IPs (a matches would possibly indicate that the
>decision to go to a parent proxy was wrong in the first place!).
>

Did I see correctly acls can be build with regex to handle this ? For now I 
ignore it.

>The final set of questions deals with HTTPS traffic. For example, if
>clients sent HTTPS requests, are you OK with Squid making routing
>decisions based on the target of the initial CONNECT request?
>

Sorry I don't get this. What is different when using CONNECT to a GET in 
regards  to routing ?

>
>> Thank you for spotting the !. I got confused with the combinations of
>> the never/always direct statement.
>
>Does your test case work after removing that "!"? If not, please share
>the updated debugging snippets.
>

Yes it looks good now. Thank you.

>
>Thank you,
>
>Alex.
>
>
Thank you for pointing out the "edge" cases
Markus 




From Walter.H at mathemainzel.info  Sun Oct 10 07:28:01 2021
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sun, 10 Oct 2021 09:28:01 +0200
Subject: [squid-users] Squid 4.11,
 Almalinux 8.4 (RHEL 8.4 based) - user defined directory for
 certificate cache?
Message-ID: <30cafe5d-93e9-d1c4-d763-f6930d42fd4b@mathemainzel.info>

Hello,

this
sudo -u squid /usr/lib64/squid/security_file_certgen -c -s 
/var/local/squid/ssl_db -M 4MB

gives the error

/usr/lib64/squid/security_file_certgen: Cannot create 
/var/local/squid/ssl_db

but this
sudo -u squid /usr/lib64/squid/security_file_certgen -c -s 
/var/spool/squid/ssl_db -M 4MB

works

both directories /var/spool/squid and /var/local/squid exist
and have the have squid_cache_t SELinux context and owned by squid:squid

Thanks


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3550 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211010/3d108885/attachment.bin>

From rafael.akchurin at diladele.com  Mon Oct 11 08:57:11 2021
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 11 Oct 2021 08:57:11 +0000
Subject: [squid-users] Ubuntu 20.04 LTS repository for Squid 5.2 (rebuilt
 from sources in Debian unstable)
Message-ID: <AM8PR04MB77453CEF1DB27403E00FF0488FB59@AM8PR04MB7745.eurprd04.prod.outlook.com>

Hello everyone,

Online repository with latest Squid 5.2 (rebuilt from sources in Debian unstable) for Ubuntu 20.04 LTS 64-bit is available at https://squid52.diladele.com/.
Github repo  https://github.com/diladele/squid-ubuntu/tree/squid-52/src/ubuntu20 contains the scripts we used to make this compilation.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add new repo
echo "deb https://squid52.diladele.com/ubuntu/ focal main" \
    > /etc/apt/sources.list.d/squid52.diladele.com.list

# and install
apt-get update && apt-get install -y \
    squid-common \
    squid-openssl \
    squidclient \
    libecap3 libecap3-dev

Hope you will find this useful.

Best regards,
Rafael Akchurin
Diladele B.V.

--
The same Squid 5.2 will be part of upcoming Web Safety 7.7 planned for release in December, 2021.
Download the latest virtual appliance from https://www.diladele.com/download.html


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211011/3ea8d6e5/attachment.htm>

From uhlar at fantomas.sk  Mon Oct 11 09:54:47 2021
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 11 Oct 2021 11:54:47 +0200
Subject: [squid-users] Ubuntu 20.04 LTS repository for Squid 5.2
 (rebuilt from sources in Debian unstable)
In-Reply-To: <AM8PR04MB77453CEF1DB27403E00FF0488FB59@AM8PR04MB7745.eurprd04.prod.outlook.com>
References: <AM8PR04MB77453CEF1DB27403E00FF0488FB59@AM8PR04MB7745.eurprd04.prod.outlook.com>
Message-ID: <20211011095447.GA10234@fantomas.sk>

On 11.10.21 08:57, Rafael Akchurin wrote:
>Online repository with latest Squid 5.2 (rebuilt from sources in Debian unstable) for Ubuntu 20.04 LTS 64-bit is available at https://squid52.diladele.com/.
>Github repo  https://github.com/diladele/squid-ubuntu/tree/squid-52/src/ubuntu20 contains the scripts we used to make this compilation.
>
>Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .
>
># add diladele apt key
>wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key add -
>
># add new repo
>echo "deb https://squid52.diladele.com/ubuntu/ focal main" \
>    > /etc/apt/sources.list.d/squid52.diladele.com.list
>
># and install
>apt-get update && apt-get install -y \
>    squid-common \
>    squid-openssl \
>    squidclient \
>    libecap3 libecap3-dev

I guess squid-openssl and possibly squidclient should be enough. 
squid-common and libecap3 should be pulled by squid-openssl and you only
need libecap3-dev if you are going to build it yourself.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Remember half the people you know are below average.


From t-mori at acty-sys.co.jp  Tue Oct 12 08:33:40 2021
From: t-mori at acty-sys.co.jp (=?UTF-8?B?5qOuIOmahuiBoQ==?=)
Date: Tue, 12 Oct 2021 17:33:40 +0900
Subject: [squid-users] Kerberos authentication with multiple squids
Message-ID: <20211012173340.36E2.64A803C2@acty-sys.co.jp>

I made Single Sign On environment with AD+Squid and it worked fine.

[It works]
Client(Windows) -> Squid(CentOS) -> Internet

* Client is joined the domain and Squid configured Kerberos Authentication with AD.

But after add another squid, it didn't work.

[Not works]
Client -> Squid(No Auth.) ->  Squid(Kerberos Auth) ->Internet

I added two line below in squid.conf of 1st Squid(No Auth.) 
to relay authentication information.

---
cache_peer [host_of_second_squid] parent [port_no] 0 no-query login=PASSTHRU
never_direct allow all
---

I confirmed access.log of 1st Squid.
It seemed squid got 407 and challenged with credentials but failed.

---
"time=2021/10/12 15:46:23","un=-","credentials=-","host=[ip_of_first_squid]",
"src_ip=[ip_of_client_pc]","src_port=49776","dest_ip=-","dest_port=-",
"url=www.yahoo.co.jp:443","status=407","http_method=CONNECT"...

"time=2021/10/12 15:46:24","un=[user at domain]","credentials=KK (null)\n",
"host=[ip_of_first_squid]","src_ip=[ip_of_client_pc]","src_port=49776",
"dest_ip=[ip_of_second_squid]","dest_port=3128","url=www.yahoo.co.jp:443",
"status=407","http_method=CONNECT"....
---

I also tested with login=PASS and connection-auth=on but got same result.
I don't understand why authentication fails with relayed 
authentication information by "login=PASSTHRU"

Do I misundastand something or squid originally don't support 
multiple proxy those relay Kerberos authentication information?

For this question I referenced this mail of mailing list.
https://www.spinics.net/lists/squid/msg85519.html

I submitted a question with images to stackoverflow.
https://stackoverflow.com/questions/69536317/is-it-possible-to-pass-kerberos-credentials-between-multiple-squids

I would appreciate it if you could point out any points you noticed.

Regards,




From Ralf.Hildebrandt at charite.de  Tue Oct 12 08:34:20 2021
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 12 Oct 2021 10:34:20 +0200
Subject: [squid-users] [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <YWBeClyaKZ2rhpUx@charite.de>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
 <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
 <YWBeClyaKZ2rhpUx@charite.de>
Message-ID: <YWVIjKZb5Fxi6tJP@charite.de>

* Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:

> > There's squid-5.2. Does it also have this problem?
> 
> Quite sure, since I've been testing Squid-5-HEAD before it became 5.2
> But to be sure, I'm deplyoing it right now.

Yep, squid-5.2 is also leaking.

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From squid3 at treenet.co.nz  Tue Oct 12 12:25:57 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Oct 2021 01:25:57 +1300
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
Message-ID: <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>

On 12/10/21 9:33 pm, ? ?? wrote:
> I made Single Sign On environment with AD+Squid and it worked fine.
> 
> [It works]
> Client(Windows) -> Squid(CentOS) -> Internet
> 
> * Client is joined the domain and Squid configured Kerberos Authentication with AD.
> 
> But after add another squid, it didn't work.
> 
...
> 
> Do I misundastand something or squid originally don't support
> multiple proxy those relay Kerberos authentication information?
> 

login=PASSTHRU means your Squid plays no part in the authentication. It 
literally passes the peer the same Proxy-Auth* headers it receives from 
the client, and the resulting response ones go back to the client.
  Which means auth issues are a problem with either the client or server 
software. Squid cannot do anything about those.


Amos


From ml at netfence.it  Tue Oct 12 13:58:55 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 12 Oct 2021 15:58:55 +0200
Subject: [squid-users] ftp_port and squidclamav
In-Reply-To: <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
References: <7e98a5dc-c8fa-e46f-97ae-151f3d30c98b@netfence.it>
 <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
Message-ID: <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>


On 8/28/21 17:10, Alex Rousskov wrote:

Sorry for taking so long.
Meanwhile I upgraded to Squid 5.0.6, but the problem was not solved.



> Reproduce the problem using a single transaction on an otherwise idle
> Squid with full debugging enabled and share the corresponding cache.log:
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction

It's here:
https://www.netfence.it/download/cache.log.bz2



>> Or, is there any way I can tell Squid to avoid passing FTP traffic
>> (coming on port 2121) to ICAP (while of course doing that for the rest)?
> 
> Yes, the adaptation_access directive controls what traffic goes to your
> ICAP services. To match ftp_port traffic, I would give the ftp_port a
> name and then try using that name in a myportname ACL. Other ACLs may
> also work, but I would start with myportname. If myportname does not
> work for ftp_port traffic, it is a Squid bug.

This works.
Thanks!

  bye
	av.


From rousskov at measurement-factory.com  Tue Oct 12 14:51:01 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Oct 2021 10:51:01 -0400
Subject: [squid-users] ftp_port and squidclamav
In-Reply-To: <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>
References: <7e98a5dc-c8fa-e46f-97ae-151f3d30c98b@netfence.it>
 <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
 <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>
Message-ID: <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>

On 10/12/21 9:58 AM, Andrea Venturoli wrote:

> On 8/28/21 17:10, Alex Rousskov wrote:
>> Reproduce the problem using a single transaction on an otherwise idle
>> Squid with full debugging enabled and share the corresponding cache.log:
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>>
> 
> It's here:
> https://www.netfence.it/download/cache.log.bz2

I am not sure, but I suspect that you are suffering from your ICAP
service inability to handle REQMOD transactions with HTTP 100-Conntinue
semantics, including (but not limited to) FTP STOR requests (translated
into HTTP by Squid).

Squid has a configuration option to work around such adaptation service
deficiencies: force_request_body_continuation. Please see if enabling
that workaround helps in your environment:
http://www.squid-cache.org/Doc/config/force_request_body_continuation/

HTH,

Alex.
P.S. When you sanitized your cache.log, you have stripped lines dumping
message headers. Those lines do not start with the usual
"2021/10/12...|" prefix. Stripping them complicates triage.




>>> Or, is there any way I can tell Squid to avoid passing FTP traffic
>>> (coming on port 2121) to ICAP (while of course doing that for the rest)?
>>
>> Yes, the adaptation_access directive controls what traffic goes to your
>> ICAP services. To match ftp_port traffic, I would give the ftp_port a
>> name and then try using that name in a myportname ACL. Other ACLs may
>> also work, but I would start with myportname. If myportname does not
>> work for ftp_port traffic, it is a Squid bug.
> 
> This works.
> Thanks!
> 
> ?bye
> ????av.



From ml at netfence.it  Tue Oct 12 16:11:02 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 12 Oct 2021 18:11:02 +0200
Subject: [squid-users] ftp_port and squidclamav
In-Reply-To: <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>
References: <7e98a5dc-c8fa-e46f-97ae-151f3d30c98b@netfence.it>
 <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
 <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>
 <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>
Message-ID: <8743262f-0fa4-112b-75ae-4bf57785a03a@netfence.it>

On 10/12/21 16:51, Alex Rousskov wrote:

> I am not sure, but I suspect that you are suffering from your ICAP
> service inability to handle REQMOD transactions with HTTP 100-Conntinue
> semantics, including (but not limited to) FTP STOR requests (translated
> into HTTP by Squid).
> 
> Squid has a configuration option to work around such adaptation service
> deficiencies: force_request_body_continuation. Please see if enabling
> that workaround helps in your environment:
> http://www.squid-cache.org/Doc/config/force_request_body_continuation/

I'll try and let you know.



> P.S. When you sanitized your cache.log, you have stripped lines dumping
> message headers. Those lines do not start with the usual
> "2021/10/12...|" prefix. Stripping them complicates triage.

Oh, sorry!
I didn't notice them and "grep"ped the time period I was interested in.
Do you want me to extract the logs again?



  bye & Thanks
	av.


From rousskov at measurement-factory.com  Tue Oct 12 16:25:51 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 12 Oct 2021 12:25:51 -0400
Subject: [squid-users] ftp_port and squidclamav
In-Reply-To: <8743262f-0fa4-112b-75ae-4bf57785a03a@netfence.it>
References: <7e98a5dc-c8fa-e46f-97ae-151f3d30c98b@netfence.it>
 <5eef8506-331d-a5cd-0204-2553b671c97b@measurement-factory.com>
 <19187188-5fd1-5636-5961-c35221d8955c@netfence.it>
 <13c0d2d9-5c19-8fc8-ca20-977c81d02602@measurement-factory.com>
 <8743262f-0fa4-112b-75ae-4bf57785a03a@netfence.it>
Message-ID: <de336a69-6be6-f211-bfbb-7cfbd7c61534@measurement-factory.com>

On 10/12/21 12:11 PM, Andrea Venturoli wrote:
> On 10/12/21 16:51, Alex Rousskov wrote:
>> P.S. When you sanitized your cache.log, you have stripped lines dumping
>> message headers. Those lines do not start with the usual
>> "2021/10/12...|" prefix. Stripping them complicates triage.

> Oh, sorry!
> I didn't notice them and "grep"ped the time period I was interested in.
> Do you want me to extract the logs again?

No, thank you. This is just something to keep in mind for the future...

Alex.


From huaraz at moeller.plus.com  Wed Oct 13 19:48:25 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Wed, 13 Oct 2021 20:48:25 +0100
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
Message-ID: <sk7d6c$10fh$1@ciao.gmane.io>

The problem lies more in the way how Kerberos proxy authentication works. 
The client uses the proxy name to create a ticket and in this case it would 
be the name of the first proxy e.g. proxy1.internal.  The first proxy will 
pass it through to the authenticating proxy for authentication 
proxy2.internal. Now the client receiving a 407 thinks that proxy1 asked for 
authentication (not knowing it is only a passthrough) and will ask for a 
ticket for proxy1, which it can't get as proxy1 is not in AD.  Even if 
proxy1 would be in AD, the client would send a proxy1 ticket to proxy2 which 
will be rejected.

Markus





"Amos Jeffries"  wrote in message 
news:ac36f75f-97c7-211e-a5bd-b12b7035ad0d at treenet.co.nz...

On 12/10/21 9:33 pm, ? ?? wrote:
> I made Single Sign On environment with AD+Squid and it worked fine.
>
> [It works]
> Client(Windows) -> Squid(CentOS) -> Internet
>
> * Client is joined the domain and Squid configured Kerberos Authentication 
> with AD.
>
> But after add another squid, it didn't work.
>
...
>
> Do I misundastand something or squid originally don't support
> multiple proxy those relay Kerberos authentication information?
>

login=PASSTHRU means your Squid plays no part in the authentication. It
literally passes the peer the same Proxy-Auth* headers it receives from
the client, and the resulting response ones go back to the client.
  Which means auth issues are a problem with either the client or server
software. Squid cannot do anything about those.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From squid3 at treenet.co.nz  Thu Oct 14 09:39:44 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Oct 2021 22:39:44 +1300
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <sk7d6c$10fh$1@ciao.gmane.io>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
Message-ID: <95c70ccd-5c15-3395-2103-3025ef043ebd@treenet.co.nz>


On 14/10/21 8:48 am, Markus Moeller wrote:
> The problem lies more in the way how Kerberos proxy authentication 
> works. The client uses the proxy name to create a ticket and in this 
> case it would be the name of the first proxy e.g. proxy1.internal.? The 
> first proxy will pass it through to the authenticating proxy for 
> authentication proxy2.internal. Now the client receiving a 407 thinks 
> that proxy1 asked for authentication (not knowing it is only a 
> passthrough) and will ask for a ticket for proxy1, which it can't get as 
> proxy1 is not in AD.? Even if proxy1 would be in AD, the client would 
> send a proxy1 ticket to proxy2 which will be rejected.
> 
> Markus
> \

Aha. That make ssense.

Can we get the Kerberos auth wiki page updated with that info? this is 
something that has come up a few times.


Cheers
Amos


From gtaylor at tnetconsulting.net  Thu Oct 14 17:49:04 2021
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Thu, 14 Oct 2021 11:49:04 -0600
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <sk7d6c$10fh$1@ciao.gmane.io>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
Message-ID: <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>

On 10/13/21 1:48 PM, Markus Moeller wrote:
> The problem lies more in the way how Kerberos proxy authentication 
> works. The client uses the proxy name to create a ticket and in this 
> case it would be the name of the first proxy e.g. proxy1.internal.? The 
> first proxy will pass it through to the authenticating proxy for 
> authentication proxy2.internal.

My understanding is that there is a way that a Kerberized service 
(proxy1 in this case) could act as a Kerberos protocol proxy agent (of 
sorts) and ask for a special type of Kerberos ticket on behalf of the 
client (client0) asking it (proxy1) for service which it (proxy1) would 
use when forwarding connections on to another host (proxy2 in this 
case).  Is my general understanding of Kerberos wrong?

Does Squid support such Kerberos protocol proxy agent (term?) support?



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4013 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211014/d81449a3/attachment.bin>

From ryan.absolom at gmail.com  Fri Oct 15 11:33:26 2021
From: ryan.absolom at gmail.com (Ryan Absolom)
Date: Fri, 15 Oct 2021 12:33:26 +0100
Subject: [squid-users] Squid Proxy - One subnet recognised
Message-ID: <CALeeoF4dJk849waPLr1r6jD_kJs2popJKA-3-4X9qJqwL+Zk-g@mail.gmail.com>

Hi All

Wondering if anyone can help - we've recently added a new subnet to
squid.conf (exactly the same layout / variables used as previous subnets)
however this doesn't get recognised.

What makes this more confusing is we have to disable Selinux in order to be
able to pick up a connection to the server (telnet/nc) for this specific
subnet, though other subnets using the same port (8080) aren't having this
issue.

Any ideas?

Thanks
Ryan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211015/01da8cc9/attachment.htm>

From JOfficer at istreamfs.com  Fri Oct 15 15:10:35 2021
From: JOfficer at istreamfs.com (Joey Officer)
Date: Fri, 15 Oct 2021 15:10:35 +0000
Subject: [squid-users] [EXTERNAL] Squid Proxy - One subnet recognised
In-Reply-To: <CALeeoF4dJk849waPLr1r6jD_kJs2popJKA-3-4X9qJqwL+Zk-g@mail.gmail.com>
References: <CALeeoF4dJk849waPLr1r6jD_kJs2popJKA-3-4X9qJqwL+Zk-g@mail.gmail.com>
Message-ID: <CH2PR19MB3496AE95DF87643F4F8E7988CDB99@CH2PR19MB3496.namprd19.prod.outlook.com>

Did you check the iptables rules ? make sure that the linux firewall isn?t blocking the traffic from the new subnet?

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Ryan Absolom
Sent: Friday, October 15, 2021 6:33 AM
To: squid-users at lists.squid-cache.org
Subject: [EXTERNAL][squid-users] Squid Proxy - One subnet recognised

CAUTION: This email originated from outside of the organization. Do not click links or open attachments unless you recognize the sender and know the content is safe.
Hi All

Wondering if anyone can help - we've recently added a new subnet to squid.conf (exactly the same layout / variables used as previous subnets) however this doesn't get recognised.

What makes this more confusing is we have to disable Selinux in order to be able to pick up a connection to the server (telnet/nc) for this specific subnet, though other subnets using the same port (8080) aren't having this issue.

Any ideas?

Thanks
Ryan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211015/ca420418/attachment.htm>

From steve at opendium.com  Fri Oct 15 16:13:45 2021
From: steve at opendium.com (Steve Hill)
Date: Fri, 15 Oct 2021 17:13:45 +0100
Subject: [squid-users] [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <YWVIjKZb5Fxi6tJP@charite.de>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
 <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
 <YWBeClyaKZ2rhpUx@charite.de> <YWVIjKZb5Fxi6tJP@charite.de>
Message-ID: <bebb5d1d-6a15-6b03-73be-e470acd7fead@opendium.com>

On 12/10/2021 09:34, Ralf Hildebrandt wrote:

>> Quite sure, since I've been testing Squid-5-HEAD before it became 5.2
>> But to be sure, I'm deplyoing it right now.
> 
> Yep, squid-5.2 is also leaking.

:(

I'm now reasonably sure that mine is a recurrence of:
     https://bugs.squid-cache.org/show_bug.cgi?id=4526
...which I had thought to have gone away in Squid 5.1.  I will apply the 
patch next week and see if the problem goes away again.


-- 
- Steve Hill
    Technical Director | Cyfarwyddwr Technegol
    Opendium    Online Safety & Web Filtering     http://www.opendium.com
                Diogelwch Ar-Lein a Hidlo Gwefan

    Enquiries | Ymholiadau:   sales at opendium.com     +44-1792-824568
    Support   | Cefnogi:      support at opendium.com   +44-1792-825748

------------------------------------------------------------------------
Opendium Limited is a company registered in England and Wales.
Mae Opendium Limited yn gwmni sydd wedi'i gofrestru yn Lloegr a Chymru.

Company No. | Rhif Cwmni:   5465437
Highfield House, 1 Brue Close, Bruton, Somerset, BA10 0HY, England.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: steve.vcf
Type: text/x-vcard
Size: 259 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211015/326d49d9/attachment.vcf>

From squid3 at treenet.co.nz  Sat Oct 16 12:06:20 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Oct 2021 01:06:20 +1300
Subject: [squid-users] [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <bebb5d1d-6a15-6b03-73be-e470acd7fead@opendium.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
 <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
 <YWBeClyaKZ2rhpUx@charite.de> <YWVIjKZb5Fxi6tJP@charite.de>
 <bebb5d1d-6a15-6b03-73be-e470acd7fead@opendium.com>
Message-ID: <45dc5798-4a2c-b7f2-2a26-454f99a10bbc@treenet.co.nz>


On 16/10/21 5:13 am, Steve Hill wrote:
> On 12/10/2021 09:34, Ralf Hildebrandt wrote:
> 
>>> Quite sure, since I've been testing Squid-5-HEAD before it became 5.2
>>> But to be sure, I'm deplyoing it right now.
>>
>> Yep, squid-5.2 is also leaking.
> 
> :(
> 
> I'm now reasonably sure that mine is a recurrence of:
>  ??? https://bugs.squid-cache.org/show_bug.cgi?id=4526

It was 4528 that got resolved. That one is still unidentified cause.

> ...which I had thought to have gone away in Squid 5.1.? I will apply the 
> patch next week and see if the problem goes away again.
> 

That would be useful info for the 4526 bug. Thanks.


Amos


From squid3 at treenet.co.nz  Sat Oct 16 12:18:11 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Oct 2021 01:18:11 +1300
Subject: [squid-users] Squid Proxy - One subnet recognised
In-Reply-To: <CALeeoF4dJk849waPLr1r6jD_kJs2popJKA-3-4X9qJqwL+Zk-g@mail.gmail.com>
References: <CALeeoF4dJk849waPLr1r6jD_kJs2popJKA-3-4X9qJqwL+Zk-g@mail.gmail.com>
Message-ID: <f848fe75-bef9-7ea5-60e5-f0ca3afef552@treenet.co.nz>

On 16/10/21 12:33 am, Ryan Absolom wrote:
> Hi All
> 
> Wondering if anyone can help - we've recently added a new subnet to 
> squid.conf (exactly the same layout / variables used as previous 
> subnets) however this doesn't get recognised.
> 

What do you mean by "added" ?

With a default squid.conf it would just be a new value on the "acl 
localnet" line(s). Then it works.


What do you mean by "recognised" ?

No meaning of the word "recognise" aligns with any action a proxy like 
Squid performs.


> What makes this more confusing is we have to disable Selinux in order to 
> be able to pick up a connection to the server (telnet/nc) for this 

That is indeed very confusing. SELinux protects OS API calls against 
unwanted accesses. It can interfere with some Squid features, but 
network connections is usually not one of them.

DO you have ARP / EUI based ACLs other subnets somehow avoid?
or doing NAT interception of this new traffic source differently from 
others?
or receiving this traffic over an unusual (eg non-Ethernet) type of 
interface?

As Joey mentioned already, the network firewall (eg iptables/ip6tables) 
is another thing to check.

Amos


From huaraz at moeller.plus.com  Sat Oct 16 19:31:48 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 16 Oct 2021 20:31:48 +0100
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
 <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>
Message-ID: <skf9b7$ekr$1@ciao.gmane.io>

I think you talk about a kdc proxy, which is for another case.

Regards
Markus

"Grant Taylor"  wrote in message 
news:b815528d-34ff-0fed-3194-dc6f34199a1c at spamtrap.tnetconsulting.net...

On 10/13/21 1:48 PM, Markus Moeller wrote:
> The problem lies more in the way how Kerberos proxy authentication
> works. The client uses the proxy name to create a ticket and in this
> case it would be the name of the first proxy e.g. proxy1.internal.  The
> first proxy will pass it through to the authenticating proxy for
> authentication proxy2.internal.

My understanding is that there is a way that a Kerberized service
(proxy1 in this case) could act as a Kerberos protocol proxy agent (of
sorts) and ask for a special type of Kerberos ticket on behalf of the
client (client0) asking it (proxy1) for service which it (proxy1) would
use when forwarding connections on to another host (proxy2 in this
case).  Is my general understanding of Kerberos wrong?

Does Squid support such Kerberos protocol proxy agent (term?) support?



-- 
Grant. . . .
unix || die

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From huaraz at moeller.plus.com  Sat Oct 16 19:39:31 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sat, 16 Oct 2021 20:39:31 +0100
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <95c70ccd-5c15-3395-2103-3025ef043ebd@treenet.co.nz>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
 <95c70ccd-5c15-3395-2103-3025ef043ebd@treenet.co.nz>
Message-ID: <skf9pr$bam$1@ciao.gmane.io>

Hi Amos,

   If you let me know where exactly I can add a few lines.

   One way to make this setup work would be to add proxy1 also to AD like 
proxy2 and then merge the keytab for proxy1 into the keytab of proxy2 using 
ktutil. The negotiate_kerberos_auth handle would require the -s 
GSS_C_NO_NAME option to select either key.

  A second option is to add a second service principal name to the proxy2 AD 
account and use -s GSS_C_NO_NAME.

Regards
Markus


"Amos Jeffries"  wrote in message 
news:95c70ccd-5c15-3395-2103-3025ef043ebd at treenet.co.nz...


On 14/10/21 8:48 am, Markus Moeller wrote:
> The problem lies more in the way how Kerberos proxy authentication works. 
> The client uses the proxy name to create a ticket and in this case it 
> would be the name of the first proxy e.g. proxy1.internal.  The first 
> proxy will pass it through to the authenticating proxy for authentication 
> proxy2.internal. Now the client receiving a 407 thinks that proxy1 asked 
> for authentication (not knowing it is only a passthrough) and will ask for 
> a ticket for proxy1, which it can't get as proxy1 is not in AD.  Even if 
> proxy1 would be in AD, the client would send a proxy1 ticket to proxy2 
> which will be rejected.
>
> Markus
> \

Aha. That make ssense.

Can we get the Kerberos auth wiki page updated with that info? this is
something that has come up a few times.


Cheers
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From gtaylor at tnetconsulting.net  Sun Oct 17 05:27:44 2021
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Sat, 16 Oct 2021 23:27:44 -0600
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <skf9b7$ekr$1@ciao.gmane.io>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
 <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>
 <skf9b7$ekr$1@ciao.gmane.io>
Message-ID: <a2070fca-07fd-9a67-3f23-551c1fe77288@spamtrap.tnetconsulting.net>

On 10/16/21 1:31 PM, Markus Moeller wrote:
> I think you talk about a kdc proxy, which is for another case.

I don't think so.  I'm not talking about using a proxy to access the KDC.

I'm talking about using a component of the following scenario:

1)  Client uses traditional username and password to authenticate to an 
IMAP server.
2)  IMAP server uses the provided credentials to request some sort of 
ticket (I don't remember what type) on the user's behalf.
3)  IMAP server uses the ticket on the user's behalf to access the 
user's messages stored on an NFS server.

I'm suggesting that the proxy1 (from the other message) do something on 
the user's behalf to request a ticket for the user that proxy1 can then 
use to authenticate as the user to proxy2.

It's been quite a while since I've read about this so I may be 
completely wrong.  But I distinctly remember there was a way to have an 
intermediate (e.g. IMAP) server accept username and password from 
clients and access a backend file server on the client's behalf in such 
a way that the backend server saw normal kerberized connections.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4013 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211016/bad33b41/attachment.bin>

From huaraz at moeller.plus.com  Sun Oct 17 16:46:25 2021
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 17 Oct 2021 17:46:25 +0100
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <a2070fca-07fd-9a67-3f23-551c1fe77288@spamtrap.tnetconsulting.net>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
 <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>
 <skf9b7$ekr$1@ciao.gmane.io>
 <a2070fca-07fd-9a67-3f23-551c1fe77288@spamtrap.tnetconsulting.net>
Message-ID: <skhk14$10c5$1@ciao.gmane.io>

I see,  I think this would mean using Basic Auth to proxy1 which then gets a 
Kerberos ticket for the user to authenticate to proxy2.  This is possible, 
but I would not think it is a good secure option.

Regards
Markus

"Grant Taylor"  wrote in message 
news:a2070fca-07fd-9a67-3f23-551c1fe77288 at spamtrap.tnetconsulting.net...
On 10/16/21 1:31 PM, Markus Moeller wrote:
> I think you talk about a kdc proxy, which is for another case.

I don't think so.  I'm not talking about using a proxy to access the KDC.

I'm talking about using a component of the following scenario:

1)  Client uses traditional username and password to authenticate to an
IMAP server.
2)  IMAP server uses the provided credentials to request some sort of
ticket (I don't remember what type) on the user's behalf.
3)  IMAP server uses the ticket on the user's behalf to access the
user's messages stored on an NFS server.

I'm suggesting that the proxy1 (from the other message) do something on
the user's behalf to request a ticket for the user that proxy1 can then
use to authenticate as the user to proxy2.

It's been quite a while since I've read about this so I may be
completely wrong.  But I distinctly remember there was a way to have an
intermediate (e.g. IMAP) server accept username and password from
clients and access a backend file server on the client's behalf in such
a way that the backend server saw normal kerberized connections.



-- 
Grant. . . .
unix || die
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From gtaylor at tnetconsulting.net  Sun Oct 17 16:57:10 2021
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Sun, 17 Oct 2021 10:57:10 -0600
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <skhk14$10c5$1@ciao.gmane.io>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
 <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>
 <skf9b7$ekr$1@ciao.gmane.io>
 <a2070fca-07fd-9a67-3f23-551c1fe77288@spamtrap.tnetconsulting.net>
 <skhk14$10c5$1@ciao.gmane.io>
Message-ID: <fedec760-ae30-1242-5b7c-da02adabab51@spamtrap.tnetconsulting.net>

On 10/17/21 10:46 AM, Markus Moeller wrote:
> I see,? I think this would mean using Basic Auth to proxy1 which then 
> gets a Kerberos ticket for the user to authenticate to proxy2.? This is 
> possible, but I would not think it is a good secure option.

I think that we're now talking about the same function.

I don't think that HTTP's Basic (realm) Authentication is required.

My understanding is that you can use Kerberos from clinet0 to proxy1 and 
that proxy1 can use the same mechanism to get a special ticket to 
communicate from proxy1 to proxy2 as the original user.

The scenario I described in the last email was to stet the stage to 
describe where the Kerberos protocol proxying was happening, not the 
method in the client to server part.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4013 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211017/fd1c3249/attachment.bin>

From gtaylor at tnetconsulting.net  Mon Oct 18 05:06:55 2021
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Sun, 17 Oct 2021 23:06:55 -0600
Subject: [squid-users] Kerberos authentication with multiple squids
In-Reply-To: <fedec760-ae30-1242-5b7c-da02adabab51@spamtrap.tnetconsulting.net>
References: <20211012173340.36E2.64A803C2@acty-sys.co.jp>
 <ac36f75f-97c7-211e-a5bd-b12b7035ad0d@treenet.co.nz>
 <sk7d6c$10fh$1@ciao.gmane.io>
 <b815528d-34ff-0fed-3194-dc6f34199a1c@spamtrap.tnetconsulting.net>
 <skf9b7$ekr$1@ciao.gmane.io>
 <a2070fca-07fd-9a67-3f23-551c1fe77288@spamtrap.tnetconsulting.net>
 <skhk14$10c5$1@ciao.gmane.io>
 <fedec760-ae30-1242-5b7c-da02adabab51@spamtrap.tnetconsulting.net>
Message-ID: <87d0591b-3edf-fd6c-0268-3880dc95015c@spamtrap.tnetconsulting.net>

On 10/17/21 10:57 AM, Grant Taylor wrote:
> My understanding is that you can use Kerberos from clinet0 to proxy1 and 
> that proxy1 can use the same mechanism to get a special ticket to 
> communicate from proxy1 to proxy2 as the original user.

I looked at my copy of Kerberos - The Definitive Guide by Jason Garman 
from O'Reilly and found the following terms that seem to be in play here.

The concept that I'm alluding to seems to be broadly known as 
"credential forwarding".  More specifically there are a couple of 
options / constraints that can be added to a TGT that seem to come into 
play here; forwardable tickets and proxiable tickets.  The latter seems 
to be a subset of the former.

The following quote comes form the Ticket Options section of chapter 3 - 
Protocols.  (Sorry, I don't have a page number when looking at 
O'Reilly's learning portal.)

--8<--
Proxiable tickets -- You can also set the proxiable flag on a ticket. 
Proxiable tickets are similar to forwardable tickets in that they can be 
transferred to another host.  However, a proxiable TGT can only be used 
to acquire further service tickets; it cannot be used to acquire a new 
TGT on the target host.
-->8--

This sounds to me like clinet0 could use a forwardable or proxiable 
ticket when talking to squid1 and squid running on squid1 can get and 
use a service ticket for the user on squid2.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4013 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211017/b82735f2/attachment.bin>

From Ralf.Hildebrandt at charite.de  Mon Oct 18 06:01:58 2021
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 18 Oct 2021 08:01:58 +0200
Subject: [squid-users] [SPAM] Re:  [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <bebb5d1d-6a15-6b03-73be-e470acd7fead@opendium.com>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
 <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
 <YWBeClyaKZ2rhpUx@charite.de> <YWVIjKZb5Fxi6tJP@charite.de>
 <bebb5d1d-6a15-6b03-73be-e470acd7fead@opendium.com>
Message-ID: <YW0N1glQN10poRYH@charite.de>

* Steve Hill <steve at opendium.com>:
> On 12/10/2021 09:34, Ralf Hildebrandt wrote:
> 
> > > Quite sure, since I've been testing Squid-5-HEAD before it became 5.2
> > > But to be sure, I'm deplyoing it right now.
> > 
> > Yep, squid-5.2 is also leaking.
> 
> :(
> 
> I'm now reasonably sure that mine is a recurrence of:
>     https://bugs.squid-cache.org/show_bug.cgi?id=4526

I'm using ICAP, yes.

> ...which I had thought to have gone away in Squid 5.1.  I will apply the
> patch next week and see if the problem goes away again.

Can you rebase the patch for 5.1/5.2 and append it to the bug report;
I'd be happy to test it here.

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From sandorsz at birosag.hu  Mon Oct 18 09:04:37 2021
From: sandorsz at birosag.hu (=?iso-8859-2?Q?S=E1ndor_Szabolcs_=5BBudapest_K=F6rny=E9ki_T=F6rv=E9nysz?= =?iso-8859-2?B?6Wtd?=)
Date: Mon, 18 Oct 2021 09:04:37 +0000
Subject: [squid-users] tunnel.cc error: local=my_proxy-ip_address
 remote=client_ip_address FD flags=1: read/write failure: (104) Connection
 reset ny peer
Message-ID: <7d9eb0acbf744125bba5815e165e80b8@birosag.hu>

Hi!

My squid cache log has a lot of error:
tunnel.cc error: local=my_proxy-ip_address remote=client_ip_address FD flags=1: read/write failure: (104) Connection reset ny peer

workers 4
cpu_affinity_map process_numbers=1,2,3,4 cores=3,4,5,6
acl localnet src 10.0.0.0/8             # RFC 1918 local private network (LAN)
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
include /etc/squid/conf.d/*
http_access allow localhost
http_access deny all
http_port 3128
cache_peer 10.0.249.120 parent 3128 0 no-query default
cache_mem 1024 MB
maximum_object_size_in_memory 1024 KB
memory_cache_shared on
memory_cache_mode always
memory_replacement_policy lru
debug_options ALL,2
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
dns_v4_first on

In include /etc/squid/conf.d/*
acl business_hours time MTWHF 8:00-16:00
acl VIP src "/etc/squid/conf.d/bkt/vip.acl"
acl Restricted_Workstations src "/etc/squid/conf.d/bkt/restricted_workstations.acl"
acl RestrictedURL dstdom_regex  "/etc/squid/conf.d/bkt/regex_restricted_url.acl"
acl RestrictedSites dstdomain "/etc/squid/conf.d/bkt/restricted_sites.acl"
acl AllowedSites dstdomain "/etc/squid/conf.d/bkt/allowed_sites.acl"
acl StreamingRequest req_mime_type -i "/etc/squid/conf.d/bkt/StreamingRequest.acl"
acl local-servers dstdomain "/etc/squid/conf.d/bkt/local-servers.acl"
acl no-access-log dstdom_regex "/etc/squid/conf.d/bkt/no-access-log.acl"
acl no-log any-of local-servers no-access-log
acl hasRequest has request
acl logMe note important_transaction
always_direct allow local-servers
http_access allow AllowedSites Restricted_Workstations
http_access deny Restricted_Workstations
http_access allow VIP RestrictedSites
http_access allow VIP RestrictedURL
http_access allow VIP StreamingRequest
http_access deny localnet StreamingRequest business_hours
http_access deny localnet RestrictedSites business_hours
http_access deny localnet RestrictedURL business_hours
http_access allow localnet AllowedSites
http_access allow localnet
http_access allow localhost
http_access deny all
access_log daemon:/var/log/squid/access.log logformat=squid hasRequest logMe no-log
never_direct allow all


Although my squid working perfectly.

Should I do anything against these errors messages.


Szabolcs Sandor

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211018/11398a63/attachment.htm>

From flashdown at data-core.org  Mon Oct 18 09:27:08 2021
From: flashdown at data-core.org (Flashdown)
Date: Mon, 18 Oct 2021 11:27:08 +0200
Subject: [squid-users] [SPAM] Re:  [SPAM] [ext] Squid 5.1 memory usage
In-Reply-To: <YW0N1glQN10poRYH@charite.de>
References: <d4f67cbf-f8fd-6081-5eb7-8c1575c5a6dc@opendium.com>
 <YWAOWq89xKj7kJVp@charite.de>
 <CAAdA2WNvH0+RUheTE97vQ+tBanahdcpv3H_CkrWFZQv-evEXUA@mail.gmail.com>
 <YWBeClyaKZ2rhpUx@charite.de> <YWVIjKZb5Fxi6tJP@charite.de>
 <bebb5d1d-6a15-6b03-73be-e470acd7fead@opendium.com>
 <YW0N1glQN10poRYH@charite.de>
Message-ID: <81495AB1-842F-436B-90F3-E8C770C7A9F8@data-core.org>

Hi, you can use the rebased patch for Squid4 that I have attached to the bug 4526. It works also against 5.1 and 5.2 and I am currently running 5.2 with it and I have not seen any issues yet.

Best Regards
Enrico Heine

Am 18. Oktober 2021 08:01:58 MESZ schrieb Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>:
>* Steve Hill <steve at opendium.com>:
>> On 12/10/2021 09:34, Ralf Hildebrandt wrote:
>> 
>> > > Quite sure, since I've been testing Squid-5-HEAD before it became 5.2
>> > > But to be sure, I'm deplyoing it right now.
>> > 
>> > Yep, squid-5.2 is also leaking.
>> 
>> :(
>> 
>> I'm now reasonably sure that mine is a recurrence of:
>>     https://bugs.squid-cache.org/show_bug.cgi?id=4526
>
>I'm using ICAP, yes.
>
>> ...which I had thought to have gone away in Squid 5.1.  I will apply the
>> patch next week and see if the problem goes away again.
>
>Can you rebase the patch for 5.1/5.2 and append it to the bug report;
>I'd be happy to test it here.
>
>Ralf Hildebrandt
>Charit? - Universit?tsmedizin Berlin
>Gesch?ftsbereich IT | Abteilung Netzwerk
>
>Campus Benjamin Franklin (CBF)
>Haus I | 1. OG | Raum 105
>Hindenburgdamm 30 | D-12203 Berlin
>
>Tel. +49 30 450 570 155
>ralf.hildebrandt at charite.de
>https://www.charite.de
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211018/79b3386d/attachment.htm>

From ty at vergesense.com  Mon Oct 18 16:11:43 2021
From: ty at vergesense.com (Ty Martin)
Date: Mon, 18 Oct 2021 11:11:43 -0500
Subject: [squid-users] AWS NLB Proxy Protocol V2
Message-ID: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>

Good morning,

I am looking to run Squid as a forward proxy with basic auth in Docker on
AWS ECS behind a network load balancer. I seem to have things up and
running for the most part; however, I am having difficulty in getting proxy
protocol to work so that I get access to client IP addresses beyond that of
the private IPs of my NLB. As soon as I enable proxy protocol v2 on the AWS
NLB, requests to Squid start failing with errors similar to the following:

Squid log: `1634330668.200      5 <nlb-private-ip> NONE_NONE/400 2032 -
error:invalid-request - HIER_NONE/- text/html`
Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`

--- Environment and Configuration details ---

Operating System: Alpine Linux 3.14.2

```
$ squid -v
Squid Cache: Version 5.0.6
Service Name: squid

This binary uses OpenSSL 1.1.1l  24 Aug 2021. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-alpine-linux-musl'
'--host=x86_64-alpine-linux-musl' '--prefix=/usr'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--localstatedir=/var'
'--with-logdir=/var/log/squid' '--disable-strict-error-checking'
'--disable-arch-native' '--enable-removal-policies=lru,heap'
'--enable-auth-digest'
'--enable-auth-basic=getpwnam,NCSA,SMB,SMB_LM,RADIUS' '--enable-epoll'
'--enable-external-acl-helpers=file_userip,unix_group,wbinfo_group'
'--enable-auth-ntlm=fake,SMB_LM' '--enable-auth-negotiate=kerberos,wrapper'
'--disable-mit' '--enable-heimdal' '--enable-delay-pools'
'--enable-openssl' '--enable-ssl-crtd' '--enable-linux-netfilter'
'--enable-ident-lookups' '--enable-useragent-log' '--enable-cache-digests'
'--enable-referer-log' '--enable-async-io' '--enable-truncate'
'--enable-arp-acl' '--enable-htcp' '--enable-carp' '--enable-poll'
'--enable-follow-x-forwarded-for' '--with-large-files'
'--with-default-user=squid' '--with-openssl'
'build_alias=x86_64-alpine-linux-musl'
'host_alias=x86_64-alpine-linux-musl' 'CC=gcc' 'CFLAGS=-Os
-fomit-frame-pointer' 'CPPFLAGS=-Os -fomit-frame-pointer' 'CXX=g++'
'CXXFLAGS=-Os -fomit-frame-pointer'
```

```
$ cat /etc/squid/squid.conf
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic realm proxy

acl authenticated proxy_auth REQUIRED
acl SSL_ports port 443
acl Safe_ports port 80

http_access allow authenticated
http_port 3128

cache deny all

pid_file
name /var/run/squid/squid.pid

visible_hostname <dns-for-nlb>

debug_options ALL,1

acl hasRequest has request
access_log stdio:/proc/self/fd/1 hasRequest
```

I've looked into `proxy_protocol_access` and
`http_port require-proxy-header`, but those both appear to be options to
provide access control to Squid around proxy information. I'm simply
looking to keep the basic auth in place as the access control mechanism
while getting at the forwarded client information for logging purposes.

Is there something silly that I'm missing to get proxy protocol working
with Squid and AWS NLBs?

Thanks,

Ty
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211018/aea7f3a7/attachment.htm>

From rousskov at measurement-factory.com  Mon Oct 18 17:34:44 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Oct 2021 13:34:44 -0400
Subject: [squid-users] tunnel.cc error: local=my_proxy-ip_address
 remote=client_ip_address FD flags=1: read/write failure: (104) Connection
 reset ny peer
In-Reply-To: <7d9eb0acbf744125bba5815e165e80b8@birosag.hu>
References: <7d9eb0acbf744125bba5815e165e80b8@birosag.hu>
Message-ID: <10914d03-c313-326b-958d-03b3edf1229b@measurement-factory.com>

On 10/18/21 5:04 AM, S?ndor Szabolcs [Budapest K?rny?ki T?rv?nysz?k] wrote:

> My squid cache log has a lot of error:
> 
> tunnel.cc error: local=my_proxy-ip_address remote=client_ip_address FD
> flags=1: read/write failure: (104) Connection reset ny peer

> ... my squid working perfectly.

Assuming the above message is logged with debugging level 2 or higher:

You should not be worried about it. That debugging information is meant
for developers. To be more precise, one should either not look at
developer-only debugging information or, if they do look at it, they
should know whether they should worry about ECONNRESET errors :-).

If this is a production Squid, consider limiting debugging sessions to
active problem investigations -- one should not be running Squid with
debugging level 2 or higher without a good reason because it obscures
important messages, slows Squid down, increases attack surfaces, etc.


HTH,

Alex.




 ?

> workers 4
> 
> cpu_affinity_map process_numbers=1,2,3,4 cores=3,4,5,6
> 
> acl localnet src 10.0.0.0/8???????????? # RFC 1918 local private network
> (LAN)
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80????????? # http
> 
> acl Safe_ports port 21????????? # ftp
> 
> acl Safe_ports port 443???????? # https
> 
> acl Safe_ports port 70????????? # gopher
> 
> acl Safe_ports port 210???????? # wais
> 
> acl Safe_ports port 1025-65535? # unregistered ports
> 
> acl Safe_ports port 280???????? # http-mgmt
> 
> acl Safe_ports port 488???????? # gss-http
> 
> acl Safe_ports port 591???????? # filemaker
> 
> acl Safe_ports port 777???????? # multiling http
> 
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> 
> http_access deny manager
> 
> include /etc/squid/conf.d/*
> 
> http_access allow localhost
> 
> http_access deny all
> 
> http_port 3128
> 
> cache_peer 10.0.249.120 parent 3128 0 no-query default
> 
> cache_mem 1024 MB
> 
> maximum_object_size_in_memory 1024 KB
> 
> memory_cache_shared on
> 
> memory_cache_mode always
> 
> memory_replacement_policy lru
> 
> debug_options ALL,2
> 
> coredump_dir /var/spool/squid
> 
> refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
> 
> refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
> 
> refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
> 
> refresh_pattern .?????????????? 0?????? 20%???? 4320
> 
> dns_v4_first on
> 
> ?
> 
> In include /etc/squid/conf.d/*
> 
> acl business_hours time MTWHF 8:00-16:00
> 
> acl VIP src "/etc/squid/conf.d/bkt/vip.acl"
> 
> acl Restricted_Workstations src
> "/etc/squid/conf.d/bkt/restricted_workstations.acl"
> 
> acl RestrictedURL dstdom_regex?
> "/etc/squid/conf.d/bkt/regex_restricted_url.acl"
> 
> acl RestrictedSites dstdomain "/etc/squid/conf.d/bkt/restricted_sites.acl"
> 
> acl AllowedSites dstdomain "/etc/squid/conf.d/bkt/allowed_sites.acl"
> 
> acl StreamingRequest req_mime_type -i
> "/etc/squid/conf.d/bkt/StreamingRequest.acl"
> 
> acl local-servers dstdomain "/etc/squid/conf.d/bkt/local-servers.acl"
> 
> acl no-access-log dstdom_regex "/etc/squid/conf.d/bkt/no-access-log.acl"
> 
> acl no-log any-of local-servers no-access-log
> 
> acl hasRequest has request
> 
> acl logMe note important_transaction
> 
> always_direct allow local-servers
> 
> http_access allow AllowedSites Restricted_Workstations
> 
> http_access deny Restricted_Workstations
> 
> http_access allow VIP RestrictedSites
> 
> http_access allow VIP RestrictedURL
> 
> http_access allow VIP StreamingRequest
> 
> http_access deny localnet StreamingRequest business_hours
> 
> http_access deny localnet RestrictedSites business_hours
> 
> http_access deny localnet RestrictedURL business_hours
> 
> http_access allow localnet AllowedSites
> 
> http_access allow localnet
> 
> http_access allow localhost
> 
> http_access deny all
> 
> access_log daemon:/var/log/squid/access.log logformat=squid hasRequest
> logMe no-log
> 
> never_direct allow all
> 
> ?
> 
> ?
> 
> Although my squid working perfectly.
> 
> ?
> 
> Should I do anything against these errors messages.
> 
> ?
> 
> ?
> 
> Szabolcs Sandor
> 
> ?
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Mon Oct 18 17:48:57 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Oct 2021 13:48:57 -0400
Subject: [squid-users] AWS NLB Proxy Protocol V2
In-Reply-To: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
References: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
Message-ID: <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>

On 10/18/21 12:11 PM, Ty Martin wrote:

> I am looking to run Squid as a forward proxy with basic auth in Docker
> on AWS ECS behind a network load balancer. I seem to have things up and
> running for the most part; however, I am having difficulty in getting
> proxy protocol to work so that I get access to client IP addresses
> beyond that of the private IPs of my NLB. As soon as I enable proxy
> protocol v2 on the AWS NLB, requests to Squid start failing with errors
> similar to the following:
> 
> Squid log: `1634330668.200 ? ? ?5 <nlb-private-ip> NONE_NONE/400 2032 -
> error:invalid-request - HIER_NONE/- text/html`
> Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`

> http_port 3128

You must use require-proxy-header http_port option to tell Squid to
always expect/require PROXY protocol messages on connections to that
listening  port. Otherwise, Squid will expect naked HTTP traffic and
fail to parse incoming (PROXY protocol) connection bytes.

According to proxy_protocol_access documentation, after adding
require-proxy-header to http_port, you must also use
proxy_protocol_access to tell Squid which TCP connections to allow on
that port (and, hence, which PROXY protocol messages to trust). Denied
connections will be closed.


HTH,

Alex.


From ty at vergesense.com  Mon Oct 18 21:16:02 2021
From: ty at vergesense.com (Ty Martin)
Date: Mon, 18 Oct 2021 16:16:02 -0500
Subject: [squid-users] AWS NLB Proxy Protocol V2
In-Reply-To: <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>
References: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
 <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>
Message-ID: <CAKC5kzNmOroB7+UG1gBa-cGVU=LYr4vbhRChvv_x8Mvm_gdK4A@mail.gmail.com>

Ah, yep. Adding the following to my config got things working in AWS:

```
acl private src 172.0.0.0/8
proxy_protocol_access allow private
http_port 3128 require-proxy-header
```

I was trying to test it locally without success by running the Docker
container and hitting it with a curl along the lines of:
`curl --proxy http://<un>:<pw>@localhost:3128 -v --header "X-Forwarded-For:
192.168.0.2" https://www.google.com`

--- Resulting Squid logs ---
```
squid-proxy_1  | 2021/10/18 19:55:33| PROXY protocol error: invalid magic
squid-proxy_1  |     exception location: Parser.cc(260) Parse from conn6
local=172.24.0.2:3128 remote=172.24.0.1:65426 FD 12 flags=1
squid-proxy_1  |     connection: conn6 local=172.24.0.2:3128 remote=
172.24.0.1:65426 FD 12 flags=1
```

--- Resulting client logs ---
```
* Proxy CONNECT aborted
* CONNECT phase completed!
* Closing connection 0
curl: (56) Proxy CONNECT aborted
```

Any idea offhand what I'm missing from the local testing scenario? I
thought adding a "X-Forwarded-For" header via curl would be treated as
proxy protocol v1 by Squid, but the "invalid magic" protocol error gives me
the impression I'm not going about it the right way.

On Mon, Oct 18, 2021 at 12:48 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 10/18/21 12:11 PM, Ty Martin wrote:
>
> > I am looking to run Squid as a forward proxy with basic auth in Docker
> > on AWS ECS behind a network load balancer. I seem to have things up and
> > running for the most part; however, I am having difficulty in getting
> > proxy protocol to work so that I get access to client IP addresses
> > beyond that of the private IPs of my NLB. As soon as I enable proxy
> > protocol v2 on the AWS NLB, requests to Squid start failing with errors
> > similar to the following:
> >
> > Squid log: `1634330668.200      5 <nlb-private-ip> NONE_NONE/400 2032 -
> > error:invalid-request - HIER_NONE/- text/html`
> > Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`
>
> > http_port 3128
>
> You must use require-proxy-header http_port option to tell Squid to
> always expect/require PROXY protocol messages on connections to that
> listening  port. Otherwise, Squid will expect naked HTTP traffic and
> fail to parse incoming (PROXY protocol) connection bytes.
>
> According to proxy_protocol_access documentation, after adding
> require-proxy-header to http_port, you must also use
> proxy_protocol_access to tell Squid which TCP connections to allow on
> that port (and, hence, which PROXY protocol messages to trust). Denied
> connections will be closed.
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211018/8b254788/attachment.htm>

From rousskov at measurement-factory.com  Mon Oct 18 22:32:51 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Oct 2021 18:32:51 -0400
Subject: [squid-users] AWS NLB Proxy Protocol V2
In-Reply-To: <CAKC5kzNmOroB7+UG1gBa-cGVU=LYr4vbhRChvv_x8Mvm_gdK4A@mail.gmail.com>
References: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
 <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>
 <CAKC5kzNmOroB7+UG1gBa-cGVU=LYr4vbhRChvv_x8Mvm_gdK4A@mail.gmail.com>
Message-ID: <b80b66a6-472d-aa52-1344-1f39d84c7f7b@measurement-factory.com>

On 10/18/21 5:16 PM, Ty Martin wrote:
> Ah, yep. Adding the following to my config got things working in AWS:

> acl private src 172.0.0.0/8
> proxy_protocol_access allow private
> http_port 3128 require-proxy-header


> I was trying to test it locally without success by running the Docker
> container and hitting it with a curl along the lines of:
> `curl --proxy http://<un>:<pw>@localhost:3128 -v --header
> "X-Forwarded-For: 192.168.0.2" https://www.google.com

To test using curl, try curl --haproxy-protocol ...

PROXY protocol (all versions) is not HTTP.

Alex.


> --- Resulting Squid logs ---
> ```
> squid-proxy_1 ?| 2021/10/18 19:55:33| PROXY protocol error: invalid magic
> squid-proxy_1 ?| ? ? exception location: Parser.cc(260) Parse from conn6
> local=172.24.0.2:3128 <http://172.24.0.2:3128> remote=172.24.0.1:65426
> <http://172.24.0.1:65426> FD 12 flags=1
> squid-proxy_1 ?| ? ? connection: conn6 local=172.24.0.2:3128
> <http://172.24.0.2:3128> remote=172.24.0.1:65426
> <http://172.24.0.1:65426> FD 12 flags=1
> ```
> 
> --- Resulting client logs ---
> ```
> * Proxy CONNECT aborted
> * CONNECT phase completed!
> * Closing connection 0
> curl: (56) Proxy CONNECT aborted
> ```
> 
> Any idea offhand what I'm missing from the local testing scenario? I
> thought adding a "X-Forwarded-For" header via curl would be treated as
> proxy protocol v1 by Squid, but the "invalid magic" protocol error gives
> me the impression I'm not going about it the right way.
> 
> On Mon, Oct 18, 2021 at 12:48 PM Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     On 10/18/21 12:11 PM, Ty Martin wrote:
> 
>     > I am looking to run Squid as a forward proxy with basic auth in Docker
>     > on AWS ECS behind a network load balancer. I seem to have things
>     up and
>     > running for the most part; however, I am having difficulty in getting
>     > proxy protocol to work so that I get access to client IP addresses
>     > beyond that of the private IPs of my NLB. As soon as I enable proxy
>     > protocol v2 on the AWS NLB, requests to Squid start failing with
>     errors
>     > similar to the following:
>     >
>     > Squid log: `1634330668.200 ? ? ?5 <nlb-private-ip> NONE_NONE/400
>     2032 -
>     > error:invalid-request - HIER_NONE/- text/html`
>     > Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`
> 
>     > http_port 3128
> 
>     You must use require-proxy-header http_port option to tell Squid to
>     always expect/require PROXY protocol messages on connections to that
>     listening? port. Otherwise, Squid will expect naked HTTP traffic and
>     fail to parse incoming (PROXY protocol) connection bytes.
> 
>     According to proxy_protocol_access documentation, after adding
>     require-proxy-header to http_port, you must also use
>     proxy_protocol_access to tell Squid which TCP connections to allow on
>     that port (and, hence, which PROXY protocol messages to trust). Denied
>     connections will be closed.
> 
> 
>     HTH,
> 
>     Alex.
> 



From ty at vergesense.com  Tue Oct 19 20:56:25 2021
From: ty at vergesense.com (Ty Martin)
Date: Tue, 19 Oct 2021 15:56:25 -0500
Subject: [squid-users] AWS NLB Proxy Protocol V2
In-Reply-To: <b80b66a6-472d-aa52-1344-1f39d84c7f7b@measurement-factory.com>
References: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
 <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>
 <CAKC5kzNmOroB7+UG1gBa-cGVU=LYr4vbhRChvv_x8Mvm_gdK4A@mail.gmail.com>
 <b80b66a6-472d-aa52-1344-1f39d84c7f7b@measurement-factory.com>
Message-ID: <CAKC5kzOP1fLFbcbT_jzDyKhQv0uvc9ukTnE9xNj3ARsxj_my+g@mail.gmail.com>

Shucks. No dice. I tried the following three variations, which all resulted
in the same "invalid magic" proxy protocol errors in the squid logs and
connection aborted errors in the curl logs as before:

```
curl --haproxy-protocol --proxy http://<un>:<pw>@localhost:3128 -v
https://www.google.com
curl --haproxy-protocol --proxy http://<un>:<pw>@localhost:3128 -v --header
"X-Forwarded-For: 192.168.0.2" https://www.google.com
curl --haproxy-protocol --proxy http://<un>:<pw>@localhost:3128 -v
--proxy-header "X-Forwarded-For: 192.168.0.2" https://www.google.com
```

That `--haproxy-protocol` option seems like it should have done the trick.
Am I just shooting myself in the foot with bad curl commands?

On Mon, Oct 18, 2021 at 5:32 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 10/18/21 5:16 PM, Ty Martin wrote:
> > Ah, yep. Adding the following to my config got things working in AWS:
>
> > acl private src 172.0.0.0/8
> > proxy_protocol_access allow private
> > http_port 3128 require-proxy-header
>
>
> > I was trying to test it locally without success by running the Docker
> > container and hitting it with a curl along the lines of:
> > `curl --proxy http://<un>:<pw>@localhost:3128 -v --header
> > "X-Forwarded-For: 192.168.0.2" https://www.google.com
>
> To test using curl, try curl --haproxy-protocol ...
>
> PROXY protocol (all versions) is not HTTP.
>
> Alex.
>
>
> > --- Resulting Squid logs ---
> > ```
> > squid-proxy_1  | 2021/10/18 19:55:33| PROXY protocol error: invalid magic
> > squid-proxy_1  |     exception location: Parser.cc(260) Parse from conn6
> > local=172.24.0.2:3128 <http://172.24.0.2:3128> remote=172.24.0.1:65426
> > <http://172.24.0.1:65426> FD 12 flags=1
> > squid-proxy_1  |     connection: conn6 local=172.24.0.2:3128
> > <http://172.24.0.2:3128> remote=172.24.0.1:65426
> > <http://172.24.0.1:65426> FD 12 flags=1
> > ```
> >
> > --- Resulting client logs ---
> > ```
> > * Proxy CONNECT aborted
> > * CONNECT phase completed!
> > * Closing connection 0
> > curl: (56) Proxy CONNECT aborted
> > ```
> >
> > Any idea offhand what I'm missing from the local testing scenario? I
> > thought adding a "X-Forwarded-For" header via curl would be treated as
> > proxy protocol v1 by Squid, but the "invalid magic" protocol error gives
> > me the impression I'm not going about it the right way.
> >
> > On Mon, Oct 18, 2021 at 12:48 PM Alex Rousskov
> > <rousskov at measurement-factory.com
> > <mailto:rousskov at measurement-factory.com>> wrote:
> >
> >     On 10/18/21 12:11 PM, Ty Martin wrote:
> >
> >     > I am looking to run Squid as a forward proxy with basic auth in
> Docker
> >     > on AWS ECS behind a network load balancer. I seem to have things
> >     up and
> >     > running for the most part; however, I am having difficulty in
> getting
> >     > proxy protocol to work so that I get access to client IP addresses
> >     > beyond that of the private IPs of my NLB. As soon as I enable proxy
> >     > protocol v2 on the AWS NLB, requests to Squid start failing with
> >     errors
> >     > similar to the following:
> >     >
> >     > Squid log: `1634330668.200      5 <nlb-private-ip> NONE_NONE/400
> >     2032 -
> >     > error:invalid-request - HIER_NONE/- text/html`
> >     > Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`
> >
> >     > http_port 3128
> >
> >     You must use require-proxy-header http_port option to tell Squid to
> >     always expect/require PROXY protocol messages on connections to that
> >     listening  port. Otherwise, Squid will expect naked HTTP traffic and
> >     fail to parse incoming (PROXY protocol) connection bytes.
> >
> >     According to proxy_protocol_access documentation, after adding
> >     require-proxy-header to http_port, you must also use
> >     proxy_protocol_access to tell Squid which TCP connections to allow on
> >     that port (and, hence, which PROXY protocol messages to trust).
> Denied
> >     connections will be closed.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211019/e2b66dc6/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 20 02:27:12 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Oct 2021 22:27:12 -0400
Subject: [squid-users] AWS NLB Proxy Protocol V2
In-Reply-To: <CAKC5kzOP1fLFbcbT_jzDyKhQv0uvc9ukTnE9xNj3ARsxj_my+g@mail.gmail.com>
References: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
 <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>
 <CAKC5kzNmOroB7+UG1gBa-cGVU=LYr4vbhRChvv_x8Mvm_gdK4A@mail.gmail.com>
 <b80b66a6-472d-aa52-1344-1f39d84c7f7b@measurement-factory.com>
 <CAKC5kzOP1fLFbcbT_jzDyKhQv0uvc9ukTnE9xNj3ARsxj_my+g@mail.gmail.com>
Message-ID: <60a3a839-d99f-f470-e570-714dc064f95e@measurement-factory.com>

On 10/19/21 4:56 PM, Ty Martin wrote:

> That `--haproxy-protocol` option seems like it should have done the
> trick. Am I just shooting myself in the foot with bad curl commands?

It look like curl --haproxy-protocol does not work the way you want for
HTTPS requests sent through HTTP proxies:

> curl --haproxy-protocol -x http://127.0.0.1:3128/ https://...
>> CONNECT localhost:443 HTTP/1.1
...
> < HTTP/1.1 200 Connection established
>
> * Proxy replied 200 to CONNECT request
> * CONNECT phase completed!

>> PROXY TCP4 127.0.0.1 127.0.0.1 35628 3128

The above PROXY protocol message is sent to the HTTP origin server
inside the CONNECT tunnel instead of being sent to Squid _before_ the
tunnel.


A similar curl command "works" for plain HTTP requests (because, without
CONNECT, it is impossible to distinguish the target of the PROXY
protocol message):

> curl --haproxy-protocol -x http://127.0.0.1:3128/ http://...
> * Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
>> PROXY TCP4 127.0.0.1 127.0.0.1 35634 3128
>> GET http://... HTTP/1.1

FWIW, my v5.2-based Squid groks the above correctly, forwarding the
request (if proxy_protocol_access allows it). This indicates that the
basic PROXY protocol support in my Squid works.


Unfortunately, there is no curl --proxy-haproxy-protocol (yet?) so you
will need to find another way to test. Alternatively, you can share
Squid debugging logs and/or packet captures when using your network load
balancer.


Alex.


> On Mon, Oct 18, 2021 at 5:32 PM Alex Rousskov wrote:
> 
>     On 10/18/21 5:16 PM, Ty Martin wrote:
>     > Ah, yep. Adding the following to my config got things working in AWS:
> 
>     > acl private src 172.0.0.0/8 <http://172.0.0.0/8>
>     > proxy_protocol_access allow private
>     > http_port 3128 require-proxy-header
> 
> 
>     > I was trying to test it locally without success by running the Docker
>     > container and hitting it with a curl along the lines of:
>     > `curl --proxy http://<un>:<pw>@localhost:3128 -v --header
>     > "X-Forwarded-For: 192.168.0.2" https://www.google.com
>     <https://www.google.com>
> 
>     To test using curl, try curl --haproxy-protocol ...
> 
>     PROXY protocol (all versions) is not HTTP.
> 
>     Alex.
> 
> 
>     > --- Resulting Squid logs ---
>     > ```
>     > squid-proxy_1 ?| 2021/10/18 19:55:33| PROXY protocol error:
>     invalid magic
>     > squid-proxy_1 ?| ? ? exception location: Parser.cc(260) Parse from
>     conn6
>     > local=172.24.0.2:3128 <http://172.24.0.2:3128>
>     <http://172.24.0.2:3128 <http://172.24.0.2:3128>>
>     remote=172.24.0.1:65426 <http://172.24.0.1:65426>
>     > <http://172.24.0.1:65426 <http://172.24.0.1:65426>> FD 12 flags=1
>     > squid-proxy_1 ?| ? ? connection: conn6 local=172.24.0.2:3128
>     <http://172.24.0.2:3128>
>     > <http://172.24.0.2:3128 <http://172.24.0.2:3128>>
>     remote=172.24.0.1:65426 <http://172.24.0.1:65426>
>     > <http://172.24.0.1:65426 <http://172.24.0.1:65426>> FD 12 flags=1
>     > ```
>     >
>     > --- Resulting client logs ---
>     > ```
>     > * Proxy CONNECT aborted
>     > * CONNECT phase completed!
>     > * Closing connection 0
>     > curl: (56) Proxy CONNECT aborted
>     > ```
>     >
>     > Any idea offhand what I'm missing from the local testing scenario? I
>     > thought adding a "X-Forwarded-For" header via curl would be treated as
>     > proxy protocol v1 by Squid, but the "invalid magic" protocol error
>     gives
>     > me the impression I'm not going about it the right way.
>     >
>     > On Mon, Oct 18, 2021 at 12:48 PM Alex Rousskov
>     > <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>     > <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>     >
>     >? ? ?On 10/18/21 12:11 PM, Ty Martin wrote:
>     >
>     >? ? ?> I am looking to run Squid as a forward proxy with basic auth
>     in Docker
>     >? ? ?> on AWS ECS behind a network load balancer. I seem to have things
>     >? ? ?up and
>     >? ? ?> running for the most part; however, I am having difficulty
>     in getting
>     >? ? ?> proxy protocol to work so that I get access to client IP
>     addresses
>     >? ? ?> beyond that of the private IPs of my NLB. As soon as I
>     enable proxy
>     >? ? ?> protocol v2 on the AWS NLB, requests to Squid start failing with
>     >? ? ?errors
>     >? ? ?> similar to the following:
>     >? ? ?>
>     >? ? ?> Squid log: `1634330668.200 ? ? ?5 <nlb-private-ip> NONE_NONE/400
>     >? ? ?2032 -
>     >? ? ?> error:invalid-request - HIER_NONE/- text/html`
>     >? ? ?> Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`
>     >
>     >? ? ?> http_port 3128
>     >
>     >? ? ?You must use require-proxy-header http_port option to tell
>     Squid to
>     >? ? ?always expect/require PROXY protocol messages on connections
>     to that
>     >? ? ?listening? port. Otherwise, Squid will expect naked HTTP
>     traffic and
>     >? ? ?fail to parse incoming (PROXY protocol) connection bytes.
>     >
>     >? ? ?According to proxy_protocol_access documentation, after adding
>     >? ? ?require-proxy-header to http_port, you must also use
>     >? ? ?proxy_protocol_access to tell Squid which TCP connections to
>     allow on
>     >? ? ?that port (and, hence, which PROXY protocol messages to
>     trust). Denied
>     >? ? ?connections will be closed.
>     >
>     >
>     >? ? ?HTH,
>     >
>     >? ? ?Alex.
>     >
> 



From ty at vergesense.com  Wed Oct 20 17:09:37 2021
From: ty at vergesense.com (Ty Martin)
Date: Wed, 20 Oct 2021 12:09:37 -0500
Subject: [squid-users] AWS NLB Proxy Protocol V2
In-Reply-To: <60a3a839-d99f-f470-e570-714dc064f95e@measurement-factory.com>
References: <CAKC5kzMMEhJZz1XkFd6to0Pn4pR7LOPDpuv-qZcFWHF3wbiO=A@mail.gmail.com>
 <9ef6c3d4-c8aa-fe32-adae-3ca7aad8a464@measurement-factory.com>
 <CAKC5kzNmOroB7+UG1gBa-cGVU=LYr4vbhRChvv_x8Mvm_gdK4A@mail.gmail.com>
 <b80b66a6-472d-aa52-1344-1f39d84c7f7b@measurement-factory.com>
 <CAKC5kzOP1fLFbcbT_jzDyKhQv0uvc9ukTnE9xNj3ARsxj_my+g@mail.gmail.com>
 <60a3a839-d99f-f470-e570-714dc064f95e@measurement-factory.com>
Message-ID: <CAKC5kzOHh0xoS+6t+i7YO6bg9AWLHTebaH5ZTu3RuP44AUmYmg@mail.gmail.com>

Got it. Thanks for your help, Alex!

On Tue, Oct 19, 2021 at 9:27 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 10/19/21 4:56 PM, Ty Martin wrote:
>
> > That `--haproxy-protocol` option seems like it should have done the
> > trick. Am I just shooting myself in the foot with bad curl commands?
>
> It look like curl --haproxy-protocol does not work the way you want for
> HTTPS requests sent through HTTP proxies:
>
> > curl --haproxy-protocol -x http://127.0.0.1:3128/ https://...
> >> CONNECT localhost:443 HTTP/1.1
> ...
> > < HTTP/1.1 200 Connection established
> >
> > * Proxy replied 200 to CONNECT request
> > * CONNECT phase completed!
>
> >> PROXY TCP4 127.0.0.1 127.0.0.1 35628 3128
>
> The above PROXY protocol message is sent to the HTTP origin server
> inside the CONNECT tunnel instead of being sent to Squid _before_ the
> tunnel.
>
>
> A similar curl command "works" for plain HTTP requests (because, without
> CONNECT, it is impossible to distinguish the target of the PROXY
> protocol message):
>
> > curl --haproxy-protocol -x http://127.0.0.1:3128/ http://...
> > * Connected to 127.0.0.1 (127.0.0.1) port 3128 (#0)
> >> PROXY TCP4 127.0.0.1 127.0.0.1 35634 3128
> >> GET http://... HTTP/1.1
>
> FWIW, my v5.2-based Squid groks the above correctly, forwarding the
> request (if proxy_protocol_access allows it). This indicates that the
> basic PROXY protocol support in my Squid works.
>
>
> Unfortunately, there is no curl --proxy-haproxy-protocol (yet?) so you
> will need to find another way to test. Alternatively, you can share
> Squid debugging logs and/or packet captures when using your network load
> balancer.
>
>
> Alex.
>
>
> > On Mon, Oct 18, 2021 at 5:32 PM Alex Rousskov wrote:
> >
> >     On 10/18/21 5:16 PM, Ty Martin wrote:
> >     > Ah, yep. Adding the following to my config got things working in
> AWS:
> >
> >     > acl private src 172.0.0.0/8 <http://172.0.0.0/8>
> >     > proxy_protocol_access allow private
> >     > http_port 3128 require-proxy-header
> >
> >
> >     > I was trying to test it locally without success by running the
> Docker
> >     > container and hitting it with a curl along the lines of:
> >     > `curl --proxy http://<un>:<pw>@localhost:3128 -v --header
> >     > "X-Forwarded-For: 192.168.0.2" https://www.google.com
> >     <https://www.google.com>
> >
> >     To test using curl, try curl --haproxy-protocol ...
> >
> >     PROXY protocol (all versions) is not HTTP.
> >
> >     Alex.
> >
> >
> >     > --- Resulting Squid logs ---
> >     > ```
> >     > squid-proxy_1  | 2021/10/18 19:55:33| PROXY protocol error:
> >     invalid magic
> >     > squid-proxy_1  |     exception location: Parser.cc(260) Parse from
> >     conn6
> >     > local=172.24.0.2:3128 <http://172.24.0.2:3128>
> >     <http://172.24.0.2:3128 <http://172.24.0.2:3128>>
> >     remote=172.24.0.1:65426 <http://172.24.0.1:65426>
> >     > <http://172.24.0.1:65426 <http://172.24.0.1:65426>> FD 12 flags=1
> >     > squid-proxy_1  |     connection: conn6 local=172.24.0.2:3128
> >     <http://172.24.0.2:3128>
> >     > <http://172.24.0.2:3128 <http://172.24.0.2:3128>>
> >     remote=172.24.0.1:65426 <http://172.24.0.1:65426>
> >     > <http://172.24.0.1:65426 <http://172.24.0.1:65426>> FD 12 flags=1
> >     > ```
> >     >
> >     > --- Resulting client logs ---
> >     > ```
> >     > * Proxy CONNECT aborted
> >     > * CONNECT phase completed!
> >     > * Closing connection 0
> >     > curl: (56) Proxy CONNECT aborted
> >     > ```
> >     >
> >     > Any idea offhand what I'm missing from the local testing scenario?
> I
> >     > thought adding a "X-Forwarded-For" header via curl would be
> treated as
> >     > proxy protocol v1 by Squid, but the "invalid magic" protocol error
> >     gives
> >     > me the impression I'm not going about it the right way.
> >     >
> >     > On Mon, Oct 18, 2021 at 12:48 PM Alex Rousskov
> >     > <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>
> >     > <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>>> wrote:
> >     >
> >     >     On 10/18/21 12:11 PM, Ty Martin wrote:
> >     >
> >     >     > I am looking to run Squid as a forward proxy with basic auth
> >     in Docker
> >     >     > on AWS ECS behind a network load balancer. I seem to have
> things
> >     >     up and
> >     >     > running for the most part; however, I am having difficulty
> >     in getting
> >     >     > proxy protocol to work so that I get access to client IP
> >     addresses
> >     >     > beyond that of the private IPs of my NLB. As soon as I
> >     enable proxy
> >     >     > protocol v2 on the AWS NLB, requests to Squid start failing
> with
> >     >     errors
> >     >     > similar to the following:
> >     >     >
> >     >     > Squid log: `1634330668.200      5 <nlb-private-ip>
> NONE_NONE/400
> >     >     2032 -
> >     >     > error:invalid-request - HIER_NONE/- text/html`
> >     >     > Client log: `X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0`
> >     >
> >     >     > http_port 3128
> >     >
> >     >     You must use require-proxy-header http_port option to tell
> >     Squid to
> >     >     always expect/require PROXY protocol messages on connections
> >     to that
> >     >     listening  port. Otherwise, Squid will expect naked HTTP
> >     traffic and
> >     >     fail to parse incoming (PROXY protocol) connection bytes.
> >     >
> >     >     According to proxy_protocol_access documentation, after adding
> >     >     require-proxy-header to http_port, you must also use
> >     >     proxy_protocol_access to tell Squid which TCP connections to
> >     allow on
> >     >     that port (and, hence, which PROXY protocol messages to
> >     trust). Denied
> >     >     connections will be closed.
> >     >
> >     >
> >     >     HTH,
> >     >
> >     >     Alex.
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211020/a3fdb96d/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 20 18:49:03 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Oct 2021 14:49:03 -0400
Subject: [squid-users] logformat odd values
In-Reply-To: <d85a66e2-6b00-43e2-0f61-d1f75853df29@measurement-factory.com>
References: <CAGSk-41or=cMZg+VFr82D15UTaiE_SRSt=n2NOVMitz0LMEMmA@mail.gmail.com>
 <1db857b3-123e-a38a-347f-d4369b300eed@measurement-factory.com>
 <CAGSk-43XSk6gU3dGTExuxOvd63VKSU=fzjCpJxX4_rMZeJERxA@mail.gmail.com>
 <d85a66e2-6b00-43e2-0f61-d1f75853df29@measurement-factory.com>
Message-ID: <2b61143a-1196-8b0d-778d-e17bf1708250@measurement-factory.com>

FTR: Factory investigation triggered by this email thread found an
explanation for the observed odd measurements. Armed with that
knowledge, we concluded that %icap::tt documentation is wrong and
proposed a fix at https://github.com/squid-cache/squid/pull/914/


HTH,

Alex.

On 9/17/21 10:56 AM, Alex Rousskov wrote:
> On 9/17/21 10:35 AM, Moti Berger wrote:
> 
>> It's possible that in some cases they will run concurrently, however not
>> in my setup. The ICAPs are chained. If the chain doesn't guarantee it
>> please let me know since I rely on it.
> 
> Chaining adaptation services guarantees that they will _start_
> sequentially, but does not guarantee that they will _run_ sequentially.
> That guarantee can only come from the services themselves.
> 
> For example, imagine two chained REQMOD services handling a 1GB HTTP PUT
> request. If the first service emits the adapted request headers (at
> least), then Squid will start the second service and will send those
> adapted request headers (and any adapted request body trickled after the
> headers) to that second service. The two services, in that case, will
> run concurrently.
> 
> 
>> Regarding what you said about things that can happen in parallel, I'm OK
>> with it since what I care the most is the extra time my ICAPs add to the
>> user latency and this is what I would like to measure.
> 
> The desire to measure ICAP overheads is common and natural.
> Unfortunately, bugs notwithstanding, %icap::tt measures the latency
> added exclusively by adaptation if and only if the corresponding
> adaptation transaction is tiny (e.g., one I/O to receive the entire
> adapted HTTP message from the adaptation service). In most real world
> scenarios involving larger HTTP messages, adaptation services will run
> in parallel with Squid reading/writing from the HTTP client and/or
> server because Squid uses adapted bytes as soon as it can.
> 
> 
> HTH,
> 
> Alex.
> 
>> On Wed, Sep 15, 2021, 20:47 Alex Rousskov wrote:
>>
>>     On 9/14/21 3:04 PM, Moti Berger wrote:
>>
>>     > I have the followings in squid.conf:
>>     >
>>     >? ? ?logformat metrics %icap::tt %adapt::all_trs %adapt::sum_trs
>>     >? ? ?%{service_req_a}adapt::sum_trs %{service_resp_a}adapt::sum_trs
>>     >? ? ?%{service_req_b}adapt::sum_trs %{service_resp_b}adapt::sum_trs
>>     >? ? ?access_log daemon:/var/log/squid/metrics.log metrics
>>     >
>>     > ?
>>     >
>>     >? ? ?icap_service service_req_a reqmod_precache bypass=1
>>     on-overload=wait
>>     >? ? ?routing=1 icap://a.y:12345/request
>>     >? ? ?icap_service service_req_b reqmod_precache bypass=1
>>     on-overload=wait
>>     >? ? ?icap://b.y:10101/request
>>     >? ? ?adaptation_service_chain svcRequest service_req_a?service_req_b
>>     >? ? ?adaptation_access svcRequest deny manager
>>     >? ? ?adaptation_access svcRequest allow all
>>     >? ? ?icap_service service_resp_a respmod_precache bypass=1
>>     >? ? ?on-overload=wait routing=1 icap://a.y:12345/response
>>     >? ? ?icap_service service_resp_b respmod_precache bypass=1
>>     >? ? ?on-overload=wait icap://b.y:10101/response
>>     >? ? ?adaptation_service_chain svcResponse service_resp_a?service_resp_b
>>     >? ? ?adaptation_access svcResponse deny manager
>>     >? ? ?adaptation_access svcResponse allow all
>>     >
>>     >
>>     > ?I see in metrics.log lines like this:
>>     >
>>     >? ? ?4 4,180 4,180 4 180 - -
>>     >
>>     >
>>     > Now I wonder how come the value of %icap:tt isn't at least as the
>>     sum of
>>     > all the numbers appear on %adapt::all_trs or %adapt::sum_trs (assuming
>>     > no failed transactions)?
>>
>>     There is probably a bug somewhere, but please note that %icap:tt may not
>>     be the sum of individual transaction response times (in _some_ cases)
>>     even after that bug is fixed because those individual transactions may
>>     run _concurrently_ (i.e. partially overlap in time).
>>
>>
>>     > If %icap:tt isn't at least the sum of all ICAPs processing time,
>>     what is?
>>
>>     Bugs notwithstanding, it is approximate time a master transaction spent
>>     doing adaptation (including checking whether adaptation is necessary).
>>     This stopwatch ticks when adaptation_access ACLs are checked and also
>>     when at least one adaptation transaction associated with that master
>>     transaction is in progress.
>>
>>     Please note that a master transaction can do a lot of different things
>>     at once or in parallel. For example, it can communicate with an HTTP
>>     client while communicating with an FTP server while communicating with
>>     an eCAP REQMOD adaptation service while communicating with a DNS server
>>     to decide whether to start communicating with an ICAP RESPMOD service.
>>
>>
>>     HTH,
>>
>>     Alex.
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org
>>     <mailto:squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>     <http://lists.squid-cache.org/listinfo/squid-users>
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From userforum790 at gmail.com  Fri Oct 22 09:38:07 2021
From: userforum790 at gmail.com (Forum User)
Date: Fri, 22 Oct 2021 11:38:07 +0200
Subject: [squid-users] How to pass TeamViewer traffic
Message-ID: <CANHwtbRdxCu92GgrJM4NKWTa40Zop415yX3ufh9-Z+Pbiut8wA@mail.gmail.com>

Hello,

I've just read
https://wiki.squid-cache.org/KnowledgeBase/Pass%20Teamviewer%20via%20SSL%20Bump-aware%20Squid
and I am confused how whole configuration should look like.

I need that only (securely) authenticated instances of TeamViewer can pass
my network.
There're four TeamViewer Proxy settings: IP/host, port, login, password.

If Squid configuration starts like https_port 4430 tls-cert ...
then I get error:transaction-end-before-headers in Squid logs.

Thanks.

UF
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211022/4732ffab/attachment.htm>

From rousskov at measurement-factory.com  Fri Oct 22 15:24:55 2021
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 22 Oct 2021 11:24:55 -0400
Subject: [squid-users] How to pass TeamViewer traffic
In-Reply-To: <CANHwtbRdxCu92GgrJM4NKWTa40Zop415yX3ufh9-Z+Pbiut8wA@mail.gmail.com>
References: <CANHwtbRdxCu92GgrJM4NKWTa40Zop415yX3ufh9-Z+Pbiut8wA@mail.gmail.com>
Message-ID: <bee899fa-6878-e2b3-37fc-21504680393a@measurement-factory.com>

On 10/22/21 5:38 AM, Forum User wrote:

> I need that only (securely) authenticated instances of TeamViewer can
> pass my network.

> There're four TeamViewer Proxy settings: IP/host, port, login, password.

I do not know much about TeamViewer, but I bet that it does not support
HTTPS proxies. If I am right, then the above settings are for an HTTP
proxy. Use http_port and http_access rules to accept and authenticate
this kind of traffic. TeamViewer will send a plain text HTTP CONNECT
request to Squid that Squid will authenticate. Upon successful
authentication, Squid will blindly forward anything TeamViewer (and its
servers) are saying via a TCP tunnel to TeamViewer-requested servers.

You do not need SslBump and https_port for this.


HTH,

Alex.


From ml at netfence.it  Sat Oct 23 16:41:02 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Sat, 23 Oct 2021 18:41:02 +0200
Subject: [squid-users] How to pass TeamViewer traffic
In-Reply-To: <bee899fa-6878-e2b3-37fc-21504680393a@measurement-factory.com>
References: <CANHwtbRdxCu92GgrJM4NKWTa40Zop415yX3ufh9-Z+Pbiut8wA@mail.gmail.com>
 <bee899fa-6878-e2b3-37fc-21504680393a@measurement-factory.com>
Message-ID: <5f2be6ce-bda5-0068-3160-9280ee5dcbb2@netfence.it>

On 10/22/21 17:24, Alex Rousskov wrote:

> I do not know much about TeamViewer, 
> ...
> You do not need SslBump and https_port for this.

AFAIK you *cannot* use SslBump, as TeamViewer pinpoints certificates.
If someone can prove me wrong, I'd be curious to know how they manage this.

  bye
	av.


From marcus.kool at urlfilterdb.com  Sat Oct 23 16:56:47 2021
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 23 Oct 2021 17:56:47 +0100
Subject: [squid-users] How to pass TeamViewer traffic
In-Reply-To: <5f2be6ce-bda5-0068-3160-9280ee5dcbb2@netfence.it>
References: <CANHwtbRdxCu92GgrJM4NKWTa40Zop415yX3ufh9-Z+Pbiut8wA@mail.gmail.com>
 <bee899fa-6878-e2b3-37fc-21504680393a@measurement-factory.com>
 <5f2be6ce-bda5-0068-3160-9280ee5dcbb2@netfence.it>
Message-ID: <cd146eb0-f2cb-c0e4-c22c-4b8caca888f6@urlfilterdb.com>

sslbump can be used in peek+splice and peek+bump modes.

Depending on what Squid finds in the peek (e.g. a teamviewer FQDN) Squid can decide to splice (not interfere) the connection.

Below is an example.

Marcus



# TLS/SSL bumping definitions

acl tls_s1_connect at_step SslBump1


# define acls for sites that must not be bumped

acl tls_server_is_bank ssl::server_name .abnamro.nl

acl tls_server_is_bank ssl::server_name .abnamro.com

acl tls_server_is_teamviewer ssl::server_name .teamviewer.com

acl tls_to_splice any-of tls_server_is_teamviewer tls_server_is_bank


# TLS/SSL bumping steps

ssl_bump peek tls_s1_connect ?? # /peek/at TLS/SSL connect data

ssl_bump splice tls_to_splice ? # /splice //some/: no active bump

ssl_bump stare all?? # /stare/(peek) at server

ssl_bump bump ? ? # /bump/if we can (if the /stare///succeeded)




On 23/10/2021 17:41, Andrea Venturoli wrote:
> On 10/22/21 17:24, Alex Rousskov wrote:
>
>> I do not know much about TeamViewer, ...
>> You do not need SslBump and https_port for this.
>
> AFAIK you *cannot* use SslBump, as TeamViewer pinpoints certificates.
> If someone can prove me wrong, I'd be curious to know how they manage this.
>
> ?bye
> ????av.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211023/ce2b5a95/attachment.htm>

From ml at netfence.it  Sat Oct 23 17:49:52 2021
From: ml at netfence.it (Andrea Venturoli)
Date: Sat, 23 Oct 2021 19:49:52 +0200
Subject: [squid-users] How to pass TeamViewer traffic
In-Reply-To: <cd146eb0-f2cb-c0e4-c22c-4b8caca888f6@urlfilterdb.com>
References: <CANHwtbRdxCu92GgrJM4NKWTa40Zop415yX3ufh9-Z+Pbiut8wA@mail.gmail.com>
 <bee899fa-6878-e2b3-37fc-21504680393a@measurement-factory.com>
 <5f2be6ce-bda5-0068-3160-9280ee5dcbb2@netfence.it>
 <cd146eb0-f2cb-c0e4-c22c-4b8caca888f6@urlfilterdb.com>
Message-ID: <081c7871-406a-e562-899a-55c5687f7c9c@netfence.it>


On 10/23/21 18:56, Marcus Kool wrote:
> sslbump can be used in peek+splice and peek+bump modes.

Sure.



> Depending on what Squid finds in the peek (e.g. a teamviewer FQDN) Squid 
> can decide to splice (not interfere) the connection.

I know.



Perhaps I wasn't clear.
What I was saying is that teamviewer traffic must be spliced, not bumped.

  bye
	av.


From John.Yuen at moodys.com  Sun Oct 24 17:33:02 2021
From: John.Yuen at moodys.com (Yuen, John)
Date: Sun, 24 Oct 2021 17:33:02 +0000
Subject: [squid-users] Squid upgrade failure support questions
In-Reply-To: <CO6PR20MB3732B4CED4271476E9E0ED0E9C829@CO6PR20MB3732.namprd20.prod.outlook.com>
References: <CO6PR20MB3732B4CED4271476E9E0ED0E9C829@CO6PR20MB3732.namprd20.prod.outlook.com>
Message-ID: <CO6PR20MB3732B24A5A499F9CEC8CDB559C829@CO6PR20MB3732.namprd20.prod.outlook.com>

Attention: Any Squid for Windows support
Re: Squid upgrade failure support questions

Hi there,

We were using Squid proxy for Windows v3.5.26 on a Windows Server 2016 Standard server,  and we finally upgraded to v4.14. However, we noticed it wasn't listening on the default Squid http port of 3128, even though the 'Squid Cache Server' Windows firewall inbound rule is there and enabled.
[cid:image001.png at 01D7C8D7.F5ABCD80]
When I try to open a web browser and go to the 'http://localhost:3128' URL on the local Squid server, it shows the following message "This page can't be displayed. Make sure the web address http://localhost:3128 is correct.". It's as though the Squid server isn't listening on that URL and port 3128 anymore. It was working before the upgrade to v4.14.

As a test to see if it was Squid v4.14 causing the problem, we installed the same fresh new copy of Squid v4.14 on another server (with no Squid installed, running on Windows Server 2019 Standard), it installs fine, it's listening on the TCP port 3128 and I can get to the 'http://localhost:3128' page fine as expected. So, something with the Squid upgrade from v3.5.26 to v4.14 on the non-working server caused the Squid to not work/listen on port 3128 anymore. The 'C:\Squid\etc\squid\squid.conf' on the non-working/upgraded Squid server looks fine and shows the following lines:

# Squid normally listens to port 3128
http_port 3128

The 'Squid for Windows' service is set to 'Automatic' startup type and shows the 'Running' status. So it can't be that. I can telnet to port 3128 on the new working Squid v4.14 server. But I can't telnet to the same port 3128 on the upgraded/non-working Squid server. When I do, it doesn't work and errors out like this:

C:\>telnet Squid-01 3128
Connecting To Squid-01...Could not open connection to the host, on port 3128: Connect failed

Lastly, as a last resort, we uninstalled the existing non-working Squid v4.14 installation, deleted the old 'C:\Squid' folder after, removed as many mentions of the old Squid installation from the Windows registry, then installed a fresh new Squid v4.14 and it still didn't work. It had the same issue.

Are there any known issues like what I'm describing concerning the upgrade from Squid v3.5.26 to v4.14? I'm hoping you can help me and point me to the right direction to resolve this. I couldn't find anything from a search on Google and I also tried unsuccessfully on your Squid-cache wiki page at 'https://wiki.squid-cache.org'. This can be a problem in the near future, as we have other Squid servers to upgrade and we don't want to face the same problem as this one. Please forward to the right team, group, user(s) as needed. If you have any questions, please let me know. Thanks for any help in advance.

John
-----------------------------------------

Moody's monitors email communications through its networks for regulatory compliance purposes and to protect its customers, employees and business and where allowed to do so by applicable law. The information contained in this e-mail message, and any attachment thereto, is confidential and may not be disclosed without our express permission. If you are not the intended recipient or an employee or agent responsible for delivering this message to the intended recipient, you are hereby notified that you have received this message in error and that any review, dissemination, distribution or copying of this message, or any attachment thereto, in whole or in part, is strictly prohibited. If you have received this message in error, please immediately notify us by telephone, fax or e-mail and delete the message and all of its attachments. Every effort is made to keep our network free from viruses. You should, however, review this e-mail message, as well as any attachment thereto, for viruses. We take no responsibility and have no liability for any computer virus which may be transferred via this e-mail message.

-----------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211024/02504c23/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 7077 bytes
Desc: image001.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211024/02504c23/attachment.png>

From squid3 at treenet.co.nz  Sun Oct 24 20:45:21 2021
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Oct 2021 09:45:21 +1300
Subject: [squid-users] Squid upgrade failure support questions
In-Reply-To: <CO6PR20MB3732B24A5A499F9CEC8CDB559C829@CO6PR20MB3732.namprd20.prod.outlook.com>
References: <CO6PR20MB3732B4CED4271476E9E0ED0E9C829@CO6PR20MB3732.namprd20.prod.outlook.com>
 <CO6PR20MB3732B24A5A499F9CEC8CDB559C829@CO6PR20MB3732.namprd20.prod.outlook.com>
Message-ID: <f112cec8-8c67-7cbe-c68f-c89a100de3bc@treenet.co.nz>

On 25/10/21 6:33 am, Yuen, John wrote:
> 
> http_port 3128
> 
> The ?Squid for Windows? service is set to ?Automatic? startup type and 
> shows the ?Running? status. So it can?t be that. I can telnet to port 
> 3128 on the new working Squid v4.14 server. But I can?t telnet to the 
> same port 3128 on the upgraded/non-working Squid server. When I do, it 
> doesn?t work and errors out like this:
> 
> C:\>telnet Squid-01 3128
> 
> Connecting To Squid-01...Could not open connection to the host, on port 
> 3128: Connect failed
> 

At no point in your post do I see any mention of what the IP addresses 
are. Can you please indicate what "localhost", "Squid-01" resolve to on 
both the machine used as client and the Squid machine. And what IP(s) 
those firewall rules by Local subnet "Any" and Remote address "local 
subnet".


Amos


From rafael.akchurin at diladele.com  Mon Oct 25 06:07:02 2021
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 25 Oct 2021 06:07:02 +0000
Subject: [squid-users] Squid upgrade failure support questions
In-Reply-To: <f112cec8-8c67-7cbe-c68f-c89a100de3bc@treenet.co.nz>
References: <CO6PR20MB3732B4CED4271476E9E0ED0E9C829@CO6PR20MB3732.namprd20.prod.outlook.com>
 <CO6PR20MB3732B24A5A499F9CEC8CDB559C829@CO6PR20MB3732.namprd20.prod.outlook.com>
 <f112cec8-8c67-7cbe-c68f-c89a100de3bc@treenet.co.nz>
Message-ID: <AM8PR04MB7745BDB89B0BAC52D28A671B8F839@AM8PR04MB7745.eurprd04.prod.outlook.com>

Be sure to also check the actual squid.exe is running - not just the service wrapper, you might be facing the https://github.com/diladele/squid-windows/issues/101 bug (4.14 does not work on older processors)

Best regards,
Rafael

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Sunday, 24 October 2021 22:45
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid upgrade failure support questions

On 25/10/21 6:33 am, Yuen, John wrote:
> 
> http_port 3128
> 
> The 'Squid for Windows' service is set to 'Automatic' startup type and 
> shows the 'Running' status. So it can't be that. I can telnet to port
> 3128 on the new working Squid v4.14 server. But I can't telnet to the 
> same port 3128 on the upgraded/non-working Squid server. When I do, it 
> doesn't work and errors out like this:
> 
> C:\>telnet Squid-01 3128
> 
> Connecting To Squid-01...Could not open connection to the host, on 
> port
> 3128: Connect failed
> 

At no point in your post do I see any mention of what the IP addresses are. Can you please indicate what "localhost", "Squid-01" resolve to on both the machine used as client and the Squid machine. And what IP(s) those firewall rules by Local subnet "Any" and Remote address "local subnet".


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From robertkwild at gmail.com  Thu Oct 28 16:05:01 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 28 Oct 2021 17:05:01 +0100
Subject: [squid-users] deny uploads acl not working
Message-ID: <CAGU_CiJX=-M+G3riBtFk0ZBAeAXqbHyXOx-wVT=MHfMZkxbhEw@mail.gmail.com>

hi all,

trying to deny uploads but its not working, heres my config below

#deny up MIME types
acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"

#allow special URL paths
acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"

#deny down MIME types
acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"

http_access deny upmime
http_reply_access allow special_url
http_reply_access deny downmime

in my mimedeny.txt

application/octet-stream
application/x-msi
application/zip
application/x-7z-compressed
application/vnd.ms-cab-compressed
application/x-msdownload
application/x-iso9660-image
application/x-tar

but the deny upmime isnt working as on when i try to upload an exe on teams
website it allows me to upload it

any ideas,

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211028/ebc767eb/attachment.htm>

From robertkwild at gmail.com  Thu Oct 28 16:50:40 2021
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 28 Oct 2021 17:50:40 +0100
Subject: [squid-users] deny uploads acl not working
In-Reply-To: <CAGU_CiJX=-M+G3riBtFk0ZBAeAXqbHyXOx-wVT=MHfMZkxbhEw@mail.gmail.com>
References: <CAGU_CiJX=-M+G3riBtFk0ZBAeAXqbHyXOx-wVT=MHfMZkxbhEw@mail.gmail.com>
Message-ID: <CAGU_CiJ407Z6JzGNkaxrcRwbDPhyF8oi2JFU5wNN9BP6v=Dk=w@mail.gmail.com>

found out i need to block

application/json

but if i do that it blocks teams aswell as thats a json, maybe i will try
this

request body max size

but theres another one i can use called

request header max size

whats the best one to use?

On Thu, 28 Oct 2021 at 17:05, robert k Wild <robertkwild at gmail.com> wrote:

> hi all,
>
> trying to deny uploads but its not working, heres my config below
>
> #deny up MIME types
> acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
>
> #allow special URL paths
> acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
>
> #deny down MIME types
> acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
>
> http_access deny upmime
> http_reply_access allow special_url
> http_reply_access deny downmime
>
> in my mimedeny.txt
>
> application/octet-stream
> application/x-msi
> application/zip
> application/x-7z-compressed
> application/vnd.ms-cab-compressed
> application/x-msdownload
> application/x-iso9660-image
> application/x-tar
>
> but the deny upmime isnt working as on when i try to upload an exe on
> teams website it allows me to upload it
>
> any ideas,
>
> thanks,
> rob
>
> --
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20211028/4821976e/attachment.htm>

From marcelorodrigo at graminsta.com.br  Sat Oct 30 06:10:06 2021
From: marcelorodrigo at graminsta.com.br (marcelorodrigo at graminsta.com.br)
Date: Sat, 30 Oct 2021 03:10:06 -0300
Subject: [squid-users] Error 503 accessing Instagram/facebook via IPv6
Message-ID: <6b435bb29b247eca6639ecd04d59438c@graminsta.com.br>

Hi,

I have been using squid for several years and am very grateful for the 
solution.

Since last 3-4 days my customers haven't been able to access 
www.instagram.com and Facebook throug IPv6s that were already working as 
proxies for years.

I only get 503 error after a time out.
The strangest thing is that I can connect 20-30% of the attempts.

I didn't make any changes to the VPSs.

I switched IPv6s provider to a totally different subnet, changed several 
equipments, but it didn't work at all.

If I access through the same VPS using the same IPs as squid is running 
using curl command but not going through the proxy It works.

But through Squid it no longer accesses as before.
This only happens with v6 IPs. On V4 Squid runs normally.

I upgraded to Ubuntu 20.04 and Squid 5.2-10 but everything remains the 
same.

My agency depends on it and I've already lost half of my clients.
Could you please help me, even if I have to pay for support?


From codemarauder at gmail.com  Sat Oct 30 06:26:10 2021
From: codemarauder at gmail.com (Nishant Sharma)
Date: Sat, 30 Oct 2021 11:56:10 +0530
Subject: [squid-users] Error 503 accessing Instagram/facebook via IPv6
In-Reply-To: <6b435bb29b247eca6639ecd04d59438c@graminsta.com.br>
References: <6b435bb29b247eca6639ecd04d59438c@graminsta.com.br>
Message-ID: <2f60006d-300b-0b2f-e488-55a545153250@gmail.com>

Hello,

On 30/10/21 11:40 am, marcelorodrigo at graminsta.com.br wrote:
> Since last 3-4 days my customers haven't been able to access
> www.instagram.com and Facebook throug IPv6s that were already working as
> proxies for years.
> 
> I only get 503 error after a time out.
> The strangest thing is that I can connect 20-30% of the attempts.

This seems to be the issue with DNS returning the IP addresses of
certain servers in the CDN which might have routing issues with your
ISPs ASN. Or that particular CDN server might be at fault.


Try this:

1. nslookup/dig for destinations giving 503 errors in squid logs
2. step (1) should return multiple IP addresses. One or two of the IPs
would be the one throwing 503 errors in squid logs.
3. `curl https://<IP>` multiple times from the squid machine and see if
you get reply from each of the IPs reliably. You will identify the
problematic IP addresses and they should be the ones in squid logs
throwing 503 errors.
4. override DNS resolution for your destination domain with one of the
IPs responding every time when you did curl.

If things start working, then you might want to change your DNS from
what it is now to some other DNS.

I have faced similar problem with pinterest in the past and I had to use
DNS server of a different geographical region to receive DNS response
with CDN IPs that worked from my network's ASN.

Regards,
Nishant



