From andre.bolinhas at articatech.com  Tue Feb  1 21:47:18 2022
From: andre.bolinhas at articatech.com (=?iso-8859-1?Q?Andr=E9_Bolinhas?=)
Date: Tue, 1 Feb 2022 21:47:18 -0000
Subject: [squid-users] external helper development
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>

Hi

I?m building an external helper to get the categorization of an website, I
know how to build it, but I need you option about the best language for the
job in terms of performance, bottlenecks, I/O blocking..

The helper will work like this.

1?  will check the hot memory for faster response (memcache or redis)

2? If the result not exist in hot memory then will check an external api to
fetch the categorie and saved it in hot memory.

 

In what language do you recommend develop such helper? PHP, Python, Go..

 

Best regards

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220201/f93d683c/attachment.htm>

From roeeklinger60 at gmail.com  Tue Feb  1 21:51:44 2022
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 1 Feb 2022 23:51:44 +0200
Subject: [squid-users] external helper development
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
Message-ID: <61c1d8fd-aa9b-4992-a297-e5edf290220b@Spark>

I don?t consider myself knowledgeable enough to answer your question in regards to Squid, but I have a similar solution for my own needs.

If you don?t need immediate results, you can simply send the log data to something like Elasticsearch, and transform it there later.

This will not effect Squid performance in any way, and you can distribute it to micro services, which is preferable IMO.

Hope that helps.
On 1 Feb 2022, 23:47 +0200, Andr? Bolinhas <andre.bolinhas at articatech.com>, wrote:
> Hi
> I?m building an external helper to get the categorization of an website, I know how to build it, but I need you option about the best language for the job in terms of performance, bottlenecks, I/O blocking..
> The helper will work like this.
> 1?? will check the hot memory for faster response (memcache or redis)
> 2? If the result not exist in hot memory then will check an external api to fetch the categorie and saved it in hot memory.
>
> In what language do you recommend develop such helper? PHP, Python, Go..
>
> Best regards
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220201/cbe0a267/attachment.htm>

From andre.bolinhas at articatech.com  Tue Feb  1 21:54:35 2022
From: andre.bolinhas at articatech.com (=?UTF-8?Q?Andr=C3=A9_Bolinhas?=)
Date: Tue, 1 Feb 2022 21:54:35 -0000
Subject: [squid-users] external helper development
In-Reply-To: <61c1d8fd-aa9b-4992-a297-e5edf290220b@Spark>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <61c1d8fd-aa9b-4992-a297-e5edf290220b@Spark>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABdR9w2cpA6Q5t6Apbw4+dhAQAAAAA=@articatech.com>

Hi

Thanks for your response

I will need the faster response possible because the goal is to then create ACL based on categories, so if the response to get the categories is slow this is also slow the user internet navigation

 

De: roee klinger <roeeklinger60 at gmail.com> 
Enviada: 1 de fevereiro de 2022 21:52
Para: Squid Users <squid-users at lists.squid-cache.org>; Andr? Bolinhas <andre.bolinhas at articatech.com>
Assunto: Re: [squid-users] external helper development

 

I don?t consider myself knowledgeable enough to answer your question in regards to Squid, but I have a similar solution for my own needs.

If you don?t need immediate results, you can simply send the log data to something like Elasticsearch, and transform it there later.

This will not effect Squid performance in any way, and you can distribute it to micro services, which is preferable IMO.

Hope that helps.

On 1 Feb 2022, 23:47 +0200, Andr? Bolinhas <andre.bolinhas at articatech.com <mailto:andre.bolinhas at articatech.com> >, wrote:



Hi

I?m building an external helper to get the categorization of an website, I know how to build it, but I need you option about the best language for the job in terms of performance, bottlenecks, I/O blocking..

The helper will work like this.

1?  will check the hot memory for faster response (memcache or redis)

2? If the result not exist in hot memory then will check an external api to fetch the categorie and saved it in hot memory.

 

In what language do you recommend develop such helper? PHP, Python, Go..

 

Best regards

 

 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220201/ac397d82/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb  1 22:01:02 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Feb 2022 17:01:02 -0500
Subject: [squid-users] external helper development
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
Message-ID: <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>

On 2/1/22 16:47, Andr? Bolinhas wrote:
> Hi
> 
> I?m building an external helper to get the categorization of an website, 
> I know how to build it, but I need you option about the best language 
> for the job in terms of performance, bottlenecks, I/O blocking..
> 
> The helper will work like this.
> 
> 1?? will check the hot memory for faster response (memcache or redis)
> 
> 2? If the result not exist in hot memory then will check an external api 
> to fetch the categorie and saved it in hot memory.
> 
> In what language do you recommend develop such helper? PHP, Python, Go..

If this helper is for long-term production use, and you are willing to 
learn new things, then use Rust[1]. Otherwise, use whatever language you 
are the most comfortable with already (except PHP), especially if that 
language has good libraries/wrappers for the external APIs you will need 
to use.

Alex.
[1] https://www.rust-lang.org/


From andre.bolinhas at articatech.com  Tue Feb  1 22:09:27 2022
From: andre.bolinhas at articatech.com (=?UTF-8?Q?Andr=C3=A9_Bolinhas?=)
Date: Tue, 1 Feb 2022 22:09:27 -0000
Subject: [squid-users] external helper development
In-Reply-To: <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>

Hi
Thanks for the reply.
I will take a look on Rust as you recommend.
Also, between Python and Go and is the best for multithreading and concurrency?
Rust supports multithreading and concurrency?
Best regards

-----Mensagem original-----
De: squid-users <squid-users-bounces at lists.squid-cache.org> Em Nome De Alex Rousskov
Enviada: 1 de fevereiro de 2022 22:01
Para: squid-users at lists.squid-cache.org
Assunto: Re: [squid-users] external helper development

On 2/1/22 16:47, Andr? Bolinhas wrote:
> Hi
> 
> I?m building an external helper to get the categorization of an 
> website, I know how to build it, but I need you option about the best 
> language for the job in terms of performance, bottlenecks, I/O blocking..
> 
> The helper will work like this.
> 
> 1?  will check the hot memory for faster response (memcache or redis)
> 
> 2? If the result not exist in hot memory then will check an external 
> api to fetch the categorie and saved it in hot memory.
> 
> In what language do you recommend develop such helper? PHP, Python, Go..

If this helper is for long-term production use, and you are willing to learn new things, then use Rust[1]. Otherwise, use whatever language you are the most comfortable with already (except PHP), especially if that language has good libraries/wrappers for the external APIs you will need to use.

Alex.
[1] https://www.rust-lang.org/
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Wed Feb  2 00:51:36 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 1 Feb 2022 19:51:36 -0500
Subject: [squid-users] external helper development
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
Message-ID: <b9b392c5-e2ce-3e5c-16d9-fbf8ef590762@measurement-factory.com>

On 2/1/22 17:09, Andr? Bolinhas wrote:

> between Python and Go and is the best for multithreading and concurrency?

Sorry, I do not know the answer to that question (or even if there is a 
meaningful answer).


> Rust supports multithreading and concurrency?

Yes, the fearless kind:
https://doc.rust-lang.org/book/ch16-00-concurrency.html


Cheers,

Alex.



> -----Mensagem original-----
> De: squid-users <squid-users-bounces at lists.squid-cache.org> Em Nome De Alex Rousskov
> Enviada: 1 de fevereiro de 2022 22:01
> Para: squid-users at lists.squid-cache.org
> Assunto: Re: [squid-users] external helper development
> 
> On 2/1/22 16:47, Andr? Bolinhas wrote:
>> Hi
>>
>> I?m building an external helper to get the categorization of an
>> website, I know how to build it, but I need you option about the best
>> language for the job in terms of performance, bottlenecks, I/O blocking..
>>
>> The helper will work like this.
>>
>> 1?  will check the hot memory for faster response (memcache or redis)
>>
>> 2? If the result not exist in hot memory then will check an external
>> api to fetch the categorie and saved it in hot memory.
>>
>> In what language do you recommend develop such helper? PHP, Python, Go..
> 
> If this helper is for long-term production use, and you are willing to learn new things, then use Rust[1]. Otherwise, use whatever language you are the most comfortable with already (except PHP), especially if that language has good libraries/wrappers for the external APIs you will need to use.
> 
> Alex.
> [1] https://www.rust-lang.org/
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Wed Feb  2 14:10:50 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 Feb 2022 09:10:50 -0500
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
Message-ID: <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>

On 1/25/22 15:17, Amos Jeffries wrote:
> On 26/01/22 03:55, Ralf Hildebrandt wrote:
>> * Francesco Chemolli:
>>> See configure --max-filedescriptors
>>
>> ...
>> configure: forcing default of 131072 filedescriptors (user-forced)
>> checking Default FD_SETSIZE value... 1024
>> checking Maximum number of filedescriptors we can open... 32768
>> configure: Default number of filedescriptors: 131072
>> ...
>>
>> Yes, I set "ulimit -n 131072" before running configure
>>
> 
> The ./configure has a 2^15 limit for the _default_ FD number. Runtime 
> should allow configuring larger values later (sans bugs).

In case it was not clear due to the presence of three(!) "defaults" in 
./configure output, the fourth "default" in Amos response is about the 
"Maximum number of filedescriptors we can open" line (that does not 
mention the word "default"): Squid just does not check whether it can 
open more than 32768 descriptors, regardless of --max-filedescriptors.

I hope somebody will change/fix the related ./configure functionality 
and/or message wording. Most humans will be confused by the 
self-contradictory output shared by Ralf. File descriptor limits is a 
complicated subject, but we can do better!


Cheers,

Alex.

> It also depends on the I/O module selected for runtime. Make sure you 
> avoid select(2) and poll(2) for large FD numbers.
> 
>  ?select(2) is limited to 1024.
>  ?poll(2) allows numbers large enough to hit RAM and CPU limits on speed.
>  ?epoll(2) is limited to ~3.5 million.
>  ?kqueue(2) is technically "unlimited" but YMMV regarding bugs etc.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From Ralf.Hildebrandt at charite.de  Wed Feb  2 14:17:26 2022
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 2 Feb 2022 15:17:26 +0100
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
 <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
Message-ID: <YfqSdkk5fToAdFU9@charite.de>

> I hope somebody will change/fix the related ./configure functionality and/or
> message wording. Most humans will be confused by the self-contradictory
> output shared by Ralf. File descriptor limits is a complicated subject, but
> we can do better!

And apparently, my squid is running just fine with
--with-filedescriptors=262144 -- that is up to now :)

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From ngtech1ltd at gmail.com  Thu Feb  3 04:29:39 2022
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Thu, 3 Feb 2022 06:29:39 +0200
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <YfqSdkk5fToAdFU9@charite.de>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
 <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
 <YfqSdkk5fToAdFU9@charite.de>
Message-ID: <CABA8h=RQQWExqGUWgRUFXug4j-JtdvhcvWAoO4pOZenVCu6VMQ@mail.gmail.com>

Hey Ralph,

Did you tried to configure the squid proxy systemd service and squid conf
with the mentioned max fd?

Thanks,
Eliezer

?????? ??? ??, 2 ????? 2022, 16:17, ??? Ralf Hildebrandt ?<
Ralf.Hildebrandt at charite.de>:

> > I hope somebody will change/fix the related ./configure functionality
> and/or
> > message wording. Most humans will be confused by the self-contradictory
> > output shared by Ralf. File descriptor limits is a complicated subject,
> but
> > we can do better!
>
> And apparently, my squid is running just fine with
> --with-filedescriptors=262144 -- that is up to now :)
>
> Ralf Hildebrandt
> Charit? - Universit?tsmedizin Berlin
> Gesch?ftsbereich IT | Abteilung Netzwerk
>
> Campus Benjamin Franklin (CBF)
> Haus I | 1. OG | Raum 105
> Hindenburgdamm 30 | D-12203 Berlin
>
> Tel. +49 30 450 570 155
> ralf.hildebrandt at charite.de
> https://www.charite.de
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220203/877d34bd/attachment.htm>

From ngtech1ltd at gmail.com  Thu Feb  3 06:09:42 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 3 Feb 2022 08:09:42 +0200
Subject: [squid-users] external helper development
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
Message-ID: <000001d818c4$a3ddd330$eb997990$@gmail.com>

Hey Andre,

Every language has a "cost" for it's qualities.
For example, Golang is a very nice language that offers a relatively simple way for concurrency support and cross hardware compilation/compatibility.
One cost in Golang is that the binary is in the size of an OS/Kernel.
In python you must write everything in a specific position and indentation and threading is not simple to implement for a novice.
However when you see what was written in Python you can see that most of OpenStack api's and systems are written in.. python and it means something.
I like very much ruby but it doesn't support threading by nature but supports "concurrency".
Squid doesn't implement threading but implements "concurrency".

Don't touch PHP as a helper!!! (+1 to Alex)

Also take into account that Redis or Memcached is less preferred in many cases if the library doesn't re-use the existing connection for multiple queries.
Squid also implements caching for helpers answers so it's possible to implement the helper and ACL's in such a way that squid caching will
help you to lower the access to the external API and or redis/memcahced/DB.
I also have good experience with some libraries which implements cache that I have used inside a helper with a limited size for "level 1" cache.
It's possible that if you will implement both the helper and server side of the solution like ufdbguard you would be able to optimize the system
to take very high load.

I hope the above will help you.
Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Andr? Bolinhas
Sent: Wednesday, February 2, 2022 00:09
To: 'Alex Rousskov' <rousskov at measurement-factory.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] external helper development

Hi
Thanks for the reply.
I will take a look on Rust as you recommend.
Also, between Python and Go and is the best for multithreading and concurrency?
Rust supports multithreading and concurrency?
Best regards

-----Mensagem original-----
De: squid-users <squid-users-bounces at lists.squid-cache.org> Em Nome De Alex Rousskov
Enviada: 1 de fevereiro de 2022 22:01
Para: squid-users at lists.squid-cache.org
Assunto: Re: [squid-users] external helper development

On 2/1/22 16:47, Andr? Bolinhas wrote:
> Hi
> 
> I?m building an external helper to get the categorization of an 
> website, I know how to build it, but I need you option about the best 
> language for the job in terms of performance, bottlenecks, I/O blocking..
> 
> The helper will work like this.
> 
> 1?  will check the hot memory for faster response (memcache or redis)
> 
> 2? If the result not exist in hot memory then will check an external 
> api to fetch the categorie and saved it in hot memory.
> 
> In what language do you recommend develop such helper? PHP, Python, Go..

If this helper is for long-term production use, and you are willing to learn new things, then use Rust[1]. Otherwise, use whatever language you are the most comfortable with already (except PHP), especially if that language has good libraries/wrappers for the external APIs you will need to use.

Alex.
[1] https://www.rust-lang.org/
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Thu Feb  3 06:33:08 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 3 Feb 2022 08:33:08 +0200
Subject: [squid-users] How to fix the error
 error:transaction-end-before-headers in access log
In-Reply-To: <c85a0789-2066-4674-2b80-1f1d2e454e1d@measurement-factory.com>
References: <ME3P282MB1617D89A730F2A04AB38A4F7D75A9@ME3P282MB1617.AUSP282.PROD.OUTLOOK.COM>
 <20220120074243.GA10312@fantomas.sk>
 <c85a0789-2066-4674-2b80-1f1d2e454e1d@measurement-factory.com>
Message-ID: <000101d818c7$e97ea7e0$bc7bf7a0$@gmail.com>

Hey,

In the case of health checks it's possible to write these in a way that will not result with this error line.
In case we would have the L4 balancer vendor we might be able to suggest on the right way to health check squid.

A nice example in haproxy can be seen at:
https://www.serverlab.ca/tutorials/linux/network-services/how-to-configure-haproxy-health-checks/

The next line should be good enough to understand:
	option httpchk HEAD /squid-internal-static/icons/SN.png HTTP/1.1\r\nHost:\ proxy-host-or-ip\r\n

Another option is:
	option httpchk HEAD /squid-internal-mgr/menu HTTP/1.1\r\nHost:\ proxy-host-or-ip\r\n

Which will require couple manager ACLs to make sure only the LB will have access to the internal mgr pages.

Hope This Helps,
Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Thursday, January 20, 2022 19:40
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] How to fix the error error:transaction-end-before-headers in access log

On 1/20/22 2:42 AM, Matus UHLAR - fantomas wrote:
> On 20.01.22 07:04, Hg Mi wrote:
>> We currently using squid 4.13 on ubuntu 18.04,  the following error
>> generates really frequently in the access.log.
>>
>> error:transaction-end-before-headers
> 
> it means that either client or server closed connection before

In most cases, this means the client opened a TCP connection to a Squid
listening port and then closed it without sending the HTTP headers. To
figure out who is at fault, you need to figure out who is making these
connections to Squid and why they are closing them without sending HTTP
headers (if that is what they are actually doing).

Bugs notwithstanding, server closures should not lead to
transaction-end-before-headers records.

>> Is this a bug in squid4?  or it was misconfigured in my environment?
> 
> most likely your environment. don't you have any content filter in front of
> your proxy?

... or anything that would "probe" or "health check" Squid http_port or
https_port at TCP level.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From Ralf.Hildebrandt at charite.de  Thu Feb  3 07:41:58 2022
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 3 Feb 2022 08:41:58 +0100
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <CABA8h=RQQWExqGUWgRUFXug4j-JtdvhcvWAoO4pOZenVCu6VMQ@mail.gmail.com>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
 <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
 <YfqSdkk5fToAdFU9@charite.de>
 <CABA8h=RQQWExqGUWgRUFXug4j-JtdvhcvWAoO4pOZenVCu6VMQ@mail.gmail.com>
Message-ID: <YfuHRofBKiE2j9xU@charite.de>

* NgTech LTD <ngtech1ltd at gmail.com>:
> Hey Ralph,
> 
> Did you tried to configure the squid proxy systemd service and squid conf
> with the mentioned max fd?

I'm not using systemd to start squid (using runit here)

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From david at articatech.com  Thu Feb  3 12:23:33 2022
From: david at articatech.com (David Touzeau)
Date: Thu, 3 Feb 2022 13:23:33 +0100
Subject: [squid-users] external helper development
In-Reply-To: <000001d818c4$a3ddd330$eb997990$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
Message-ID: <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>

Hi Elizer

You are right in a way but when squid loads multiple helpers, each 
helper will use its own cache.
Using a shared "base" allows helpers to avoid having to compute a query 
already found by another helper who already has the answer.

Concerning PHP what we find strange is that with our tests, a simple 
loop and an "echo OK", php goes faster: 1.5x than python.

Le 03/02/2022 ? 07:09, Eliezer Croitoru a ?crit?:
> Hey Andre,
>
> Every language has a "cost" for it's qualities.
> For example, Golang is a very nice language that offers a relatively simple way for concurrency support and cross hardware compilation/compatibility.
> One cost in Golang is that the binary is in the size of an OS/Kernel.
> In python you must write everything in a specific position and indentation and threading is not simple to implement for a novice.
> However when you see what was written in Python you can see that most of OpenStack api's and systems are written in.. python and it means something.
> I like very much ruby but it doesn't support threading by nature but supports "concurrency".
> Squid doesn't implement threading but implements "concurrency".
>
> Don't touch PHP as a helper!!! (+1 to Alex)
>
> Also take into account that Redis or Memcached is less preferred in many cases if the library doesn't re-use the existing connection for multiple queries.
> Squid also implements caching for helpers answers so it's possible to implement the helper and ACL's in such a way that squid caching will
> help you to lower the access to the external API and or redis/memcahced/DB.
> I also have good experience with some libraries which implements cache that I have used inside a helper with a limited size for "level 1" cache.
> It's possible that if you will implement both the helper and server side of the solution like ufdbguard you would be able to optimize the system
> to take very high load.
>
> I hope the above will help you.
> Eliezer
>
> ----
> Eliezer Croitoru
> NgTech, Tech Support
> Mobile: +972-5-28704261
> Email:ngtech1ltd at gmail.com
>
> -----Original Message-----
> From: squid-users<squid-users-bounces at lists.squid-cache.org>  On Behalf Of Andr? Bolinhas
> Sent: Wednesday, February 2, 2022 00:09
> To: 'Alex Rousskov'<rousskov at measurement-factory.com>;squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] external helper development
>
> Hi
> Thanks for the reply.
> I will take a look on Rust as you recommend.
> Also, between Python and Go and is the best for multithreading and concurrency?
> Rust supports multithreading and concurrency?
> Best regards
>
> -----Mensagem original-----
> De: squid-users<squid-users-bounces at lists.squid-cache.org>  Em Nome De Alex Rousskov
> Enviada: 1 de fevereiro de 2022 22:01
> Para:squid-users at lists.squid-cache.org
> Assunto: Re: [squid-users] external helper development
>
> On 2/1/22 16:47, Andr? Bolinhas wrote:
>> Hi
>>
>> I?m building an external helper to get the categorization of an
>> website, I know how to build it, but I need you option about the best
>> language for the job in terms of performance, bottlenecks, I/O blocking..
>>
>> The helper will work like this.
>>
>> 1?  will check the hot memory for faster response (memcache or redis)
>>
>> 2? If the result not exist in hot memory then will check an external
>> api to fetch the categorie and saved it in hot memory.
>>
>> In what language do you recommend develop such helper? PHP, Python, Go..
> If this helper is for long-term production use, and you are willing to learn new things, then use Rust[1]. Otherwise, use whatever language you are the most comfortable with already (except PHP), especially if that language has good libraries/wrappers for the external APIs you will need to use.
>
> Alex.
> [1]https://www.rust-lang.org/
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220203/5d7e34be/attachment.htm>

From ngtech1ltd at gmail.com  Fri Feb  4 06:06:31 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 4 Feb 2022 08:06:31 +0200
Subject: [squid-users] external helper development
In-Reply-To: <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
Message-ID: <000401d8198d$5c30e120$1492a360$@gmail.com>

Hey David,

 

First, PHP is a good language however it doesn?t handle well STDIN/STDOUT helpers and crashes more then once without any warnings.

It?s documented in the PHP docs (Don?t remember exactly where).

Regarding PHP compared to PHP being faster, it?s pretty simple to test and validate if the cost of the crash is better than the speed.

What python and PHP code have used in your tests? (I would be happy to try and test it..).

You can see this session helper written in python:

https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Python

 

And about the cache of each helpers, the cost of a cache on a single helper is not much in terms of memory comparing to some network access.

Again it?s possible to test and verify this on a loaded system to get results. The delay itself can be seen from squid side in the cache manager statistics.

 

You can also try to compare the next ruby helper:

https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper

 

About a shared ?base? which allows helpers to avoid computation of the query?. It?s a good argument, however it depends what is the cost of
pulling from the cache compared to calculating the answer.

A very simple string comparison or regex matching would probably be faster than reaching a shared storage in many cases.

 

Also take into account the ?concurrency? support from the helper side.

A helper that supports parallel processing of requests/lines can do better then many single helpers in more than once use case.

In any case I would suggest to enable requests concurrency from squid side since the STDIN buffer will emulate some level of concurrency
by itself and will allow squid to keep going forward faster.

 

Just to mention that SquidGuard have used a single helper cache for a very long time, ie every single SquidGuard helper has it?s own copy of the whole

configuration and database files in memory.

 

And again, if you do have any option to implement a server service model and that the helpers will contact this main service you will be able to implement
much faster internal in-memory cache compared to a redis/memcahe/other external daemon(need to be tested).

 

A good example for this is ufdbguard which has helpers that are clients of the main service which does the whole heavy lifting and also holds 
one copy of the DB.

 

I have implemented SquidBlocker this way and have seen that it out-performs any other service I have tried until now.

 

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Thursday, February 3, 2022 14:24
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] external helper development

 

Hi Elizer

You are right in a way but when squid loads multiple helpers, each helper will use its own cache.
Using a shared "base" allows helpers to avoid having to compute a query already found by another helper who already has the answer.

Concerning PHP what we find strange is that with our tests, a simple loop and an "echo OK", php goes faster: 1.5x than python.

Le 03/02/2022 ? 07:09, Eliezer Croitoru a ?crit :

Hey Andre,
 
Every language has a "cost" for it's qualities.
For example, Golang is a very nice language that offers a relatively simple way for concurrency support and cross hardware compilation/compatibility.
One cost in Golang is that the binary is in the size of an OS/Kernel.
In python you must write everything in a specific position and indentation and threading is not simple to implement for a novice.
However when you see what was written in Python you can see that most of OpenStack api's and systems are written in.. python and it means something.
I like very much ruby but it doesn't support threading by nature but supports "concurrency".
Squid doesn't implement threading but implements "concurrency".
 
Don't touch PHP as a helper!!! (+1 to Alex)
 
Also take into account that Redis or Memcached is less preferred in many cases if the library doesn't re-use the existing connection for multiple queries.
Squid also implements caching for helpers answers so it's possible to implement the helper and ACL's in such a way that squid caching will
help you to lower the access to the external API and or redis/memcahced/DB.
I also have good experience with some libraries which implements cache that I have used inside a helper with a limited size for "level 1" cache.
It's possible that if you will implement both the helper and server side of the solution like ufdbguard you would be able to optimize the system
to take very high load.
 
I hope the above will help you.
Eliezer
 
----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 
 
-----Original Message-----
From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of Andr? Bolinhas
Sent: Wednesday, February 2, 2022 00:09
To: 'Alex Rousskov'  <mailto:rousskov at measurement-factory.com> <rousskov at measurement-factory.com>; squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] external helper development
 
Hi
Thanks for the reply.
I will take a look on Rust as you recommend.
Also, between Python and Go and is the best for multithreading and concurrency?
Rust supports multithreading and concurrency?
Best regards
 
-----Mensagem original-----
De: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> Em Nome De Alex Rousskov
Enviada: 1 de fevereiro de 2022 22:01
Para: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Assunto: Re: [squid-users] external helper development
 
On 2/1/22 16:47, Andr? Bolinhas wrote:

Hi
 
I?m building an external helper to get the categorization of an 
website, I know how to build it, but I need you option about the best 
language for the job in terms of performance, bottlenecks, I/O blocking..
 
The helper will work like this.
 
1?  will check the hot memory for faster response (memcache or redis)
 
2? If the result not exist in hot memory then will check an external 
api to fetch the categorie and saved it in hot memory.
 
In what language do you recommend develop such helper? PHP, Python, Go..

 
If this helper is for long-term production use, and you are willing to learn new things, then use Rust[1]. Otherwise, use whatever language you are the most comfortable with already (except PHP), especially if that language has good libraries/wrappers for the external APIs you will need to use.
 
Alex.
[1] https://www.rust-lang.org/
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users
 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users
 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220204/f16bd717/attachment.htm>

From ngtech1ltd at gmail.com  Fri Feb  4 06:07:02 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 4 Feb 2022 08:07:02 +0200
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <YfuHRofBKiE2j9xU@charite.de>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
 <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
 <YfqSdkk5fToAdFU9@charite.de>
 <CABA8h=RQQWExqGUWgRUFXug4j-JtdvhcvWAoO4pOZenVCu6VMQ@mail.gmail.com>
 <YfuHRofBKiE2j9xU@charite.de>
Message-ID: <000901d8198d$6dff9860$49fec920$@gmail.com>

What OS are you using exactly?

Thanks,
Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: Ralf Hildebrandt <Ralf.Hildebrandt at charite.de> 
Sent: Thursday, February 3, 2022 09:42
To: NgTech LTD <ngtech1ltd at gmail.com>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] [ext] Re: Absolute upper limit for filedescriptors in squid-6?

* NgTech LTD <ngtech1ltd at gmail.com>:
> Hey Ralph,
> 
> Did you tried to configure the squid proxy systemd service and squid conf
> with the mentioned max fd?

I'm not using systemd to start squid (using runit here)

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de



From Ralf.Hildebrandt at charite.de  Fri Feb  4 08:12:28 2022
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Fri, 4 Feb 2022 09:12:28 +0100
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <000901d8198d$6dff9860$49fec920$@gmail.com>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
 <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
 <YfqSdkk5fToAdFU9@charite.de>
 <CABA8h=RQQWExqGUWgRUFXug4j-JtdvhcvWAoO4pOZenVCu6VMQ@mail.gmail.com>
 <YfuHRofBKiE2j9xU@charite.de>
 <000901d8198d$6dff9860$49fec920$@gmail.com>
Message-ID: <Yfzf7Hkx02WzUiXm@charite.de>

* Eliezer Croitoru <ngtech1ltd at gmail.com>:

> What OS are you using exactly?

Ubuntu 20.04 on amd64

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From squid at remotenode.org  Fri Feb  4 08:43:14 2022
From: squid at remotenode.org (Nolan)
Date: Fri, 4 Feb 2022 08:43:14 +0000
Subject: [squid-users] Base64 encoding of X-Authenticated-User Request Header
Message-ID: <CAB-hUba6yo=bxpbouuUi2iaMGCAH0ipSUPCc113m_L-M1UgQAw@mail.gmail.com>

Hi All!

I'm using a Squid proxy instance to authenticate users on the local LAN and
then forward requests to an upstream proxy.
I'm trying to figure out a way to do a base64 encoding of the
X-Authenticated-User request header.

Right now I have the following line:
request_header_add X-Authenticated-User "%un" all

I'm just missing the component to convert that %un value into base64 format.

Any ideas on how to do this?

Any guidance would be greatly appreciated as I'm struggling to find good
documentation that explains how to do this.

TIA
Nolan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220204/8c8ffcd8/attachment.htm>

From squid3 at treenet.co.nz  Fri Feb  4 12:45:30 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Feb 2022 01:45:30 +1300
Subject: [squid-users] Base64 encoding of X-Authenticated-User Request
 Header
In-Reply-To: <CAB-hUba6yo=bxpbouuUi2iaMGCAH0ipSUPCc113m_L-M1UgQAw@mail.gmail.com>
References: <CAB-hUba6yo=bxpbouuUi2iaMGCAH0ipSUPCc113m_L-M1UgQAw@mail.gmail.com>
Message-ID: <78ae3a0a-5057-c4af-a532-74adfe5f49ee@treenet.co.nz>

On 4/02/22 21:43, Nolan wrote:
> Hi All!
> 
> I'm using a Squid proxy instance to authenticate users on the local LAN 
> and then forward requests to an upstream proxy.
> I'm trying to figure out a way to do a base64 encoding of the 
> X-Authenticated-User request header.
> 
> Right now I have the following line:
> request_header_add X-Authenticated-User "%un" all
> 
> I'm just missing the component to convert that %un value into base64 format.
> 
> Any ideas on how to do this?
> 


Firstly, why are you using that X- header?

Squid configured with a cache_peer should be able to pass on the 
username in Basic auth headers.


Secondly, base64 encoding is not currently supported for the format codes.

For now, you will have to either have the authenticator or an 
external_acl_type helper provide Squid with the base64 encoded form and 
use the %note code to generate the header.


PS. I am happy to provide a patch adding base64 encoding to logformat's. 
If you (or anyone) wants to sponsor it please contact me.


Amos


From david at articatech.com  Fri Feb  4 14:28:38 2022
From: david at articatech.com (David Touzeau)
Date: Fri, 4 Feb 2022 15:28:38 +0100
Subject: [squid-users] external helper development
In-Reply-To: <000401d8198d$5c30e120$1492a360$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
Message-ID: <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>

Elizer,

Thanks for all this advice and indeed your arguments are valid between 
opening a socket, sending data, receiving data and closing the socket 
unlike direct access to a regex or a memory entry even if the 
calculation has already been done.

But what surprises me the most is that we have produced a python plugin 
in thread which I provide you a code below.
The php code is like your mentioned example ( No thread, just a loop and 
output OK )

Results are after 6k requests, squid freeze and no surf can be made as 
with PHP code we can up to 10K requests and squid is happy
really, we did not understand why python is so low.

Here a python code using threads

#!/usr/bin/env python
import os
import sys
import time
import signal
import locale
import traceback
import threading
import select
import traceback as tb

class ClienThread():

 ??? def __init__(self):
 ??????? self._exiting = False
 ??????? self._cache = {}

 ??? def exit(self):
 ??????? self._exiting = True

 ??? def stdout(self, lineToSend):
 ??????? try:
 ??????????? sys.stdout.write(lineToSend)
 ??????????? sys.stdout.flush()

 ??????? except IOError as e:
 ??????????? if e.errno==32:
 ??????????????? # Error Broken PIPE!"
 ??????????????? pass
 ??????? except:
 ??????????? # other execpt
 ??????????? pass

 ??? def run(self):
 ??????? while not self._exiting:
 ??????????? if sys.stdin in select.select([sys.stdin], [], [], 0.5)[0]:
 ??????????????? line = sys.stdin.readline()
 ??????????????? LenOfline=len(line)

 ??????????????? if LenOfline==0:
 ??????????????????? self._exiting=True
 ??????????????????? break

 ??????????????? if line[-1] == '\n':line = line[:-1]
 ??????????????? channel = None
 ??????????????? options = line.split()

 ??????????????? try:
 ??????????????????? if options[0].isdigit(): channel = options.pop(0)
 ??????????????? except IndexError:
 ??????????????????? self.stdout("0 OK first=ERROR\n")
 ??????????????????? continue

 ??????????????? # Processing here

 ??????????????? try:
 ??????????????????? self.stdout("%s OK\n" % channel)
 ??????????????? except:
 ??????????????????? self.stdout("%s ERROR first=ERROR\n" % channel)




class Main(object):
 ??? def __init__(self):
 ??????? self._threads = []
 ??????? self._exiting = False
 ??????? self._reload = False
 ??????? self._config = ""

 ??????? for sig, action in (
 ??????????? (signal.SIGINT, self.shutdown),
 ??????????? (signal.SIGQUIT, self.shutdown),
 ??????????? (signal.SIGTERM, self.shutdown),
 ??????????? (signal.SIGHUP, lambda s, f: setattr(self, '_reload', True)),
 ??????????? (signal.SIGPIPE, signal.SIG_IGN),
 ??????? ):
 ??????????? try:
 ??????????????? signal.signal(sig, action)
 ??????????? except AttributeError:
 ??????????????? pass



 ??? def shutdown(self, sig = None, frame = None):
 ??????? self._exiting = True
 ??????? self.stop_threads()

 ??? def start_threads(self):

 ??????? sThread = ClienThread()
 ??????? t = threading.Thread(target = sThread.run)
 ??????? t.start()
 ??????? self._threads.append((sThread, t))



 ??? def stop_threads(self):
 ??????? for p, t in self._threads:
 ??????????? p.exit()
 ??????? for p, t in self._threads:
 ??????????? t.join(timeout = 1.0)
 ??????? self._threads = []

 ??? def run(self):
 ??????? """ main loop """
 ??????? ret = 0
 ??????? self.start_threads()
 ??????? return ret


if __name__ == '__main__':
 ??? # set C locale
 ??? locale.setlocale(locale.LC_ALL, 'C')
 ??? os.environ['LANG'] = 'C'
 ??? ret = 0
 ??? try:
 ??????? main = Main()
 ??????? ret = main.run()
 ??? except SystemExit:
 ??????? pass
 ??? except KeyboardInterrupt:
 ??????? ret = 4
 ??? except:
 ??? sys.exit(ret)

Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit?:
>
> And about the cache of each helpers, the cost of a cache on a single 
> helper is not much in terms of memory comparing to some network access.
>
> Again it?s possible to test and verify this on a loaded system to get 
> results. The delay itself can be seen from squid side in the cache 
> manager statistics.
>
> You can also try to compare the next ruby helper:
>
> https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper
>
> About a shared ?base? which allows helpers to avoid computation of the 
> query?. It?s a good argument, however it depends what is the cost of
> pulling from the cache compared to calculating the answer.
>
> A very simple string comparison or regex matching would probably be 
> faster than reaching a shared storage in many cases.
>
> Also take into account the ?concurrency? support from the helper side.
>
> A helper that supports parallel processing of requests/lines can do 
> better then many single helpers in more than once use case.
>
> In any case I would suggest to enable requests concurrency from squid 
> side since the STDIN buffer will emulate some level of concurrency
> by itself and will allow squid to keep going forward faster.
>
> Just to mention that SquidGuard have used a single helper cache for a 
> very long time, ie every single SquidGuard helper has it?s own copy of 
> the whole
>
> configuration and database files in memory.
>
> And again, if you do have any option to implement a server service 
> model and that the helpers will contact this main service you will be 
> able to implement
> much faster internal in-memory cache compared to a redis/memcahe/other 
> external daemon(need to be tested).
>
> A good example for this is ufdbguard which has helpers that are 
> clients of the main service which does the whole heavy lifting and 
> also holds
> one copy of the DB.
>
> I have implemented SquidBlocker this way and have seen that it 
> out-performs any other service I have tried until now.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220204/faa83ea0/attachment.htm>

From squid3 at treenet.co.nz  Fri Feb  4 23:58:28 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 Feb 2022 12:58:28 +1300
Subject: [squid-users] The status of AIA ie: TLS code:
 X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY ?
In-Reply-To: <000001d8120e$b6dabbe0$249033a0$@gmail.com>
References: <000001d8120e$b6dabbe0$249033a0$@gmail.com>
Message-ID: <0405e887-8c33-3cf8-4630-c43f695fe473@treenet.co.nz>

On 26/01/22 06:12, Eliezer Croitoru wrote:
> Hey,
> 
> I have recently seen more then one site that doesn't provide the full CA
> bundle chain.
> An example:
> https://www.ssllabs.com/ssltest/analyze.html?d=www.cloudschool.org
> https://www.ssllabs.com/ssltest/analyze.html?d= certificatechain.io
> 
> I wanted to somehow get this issue logged properly.
> Currently squid sends the client a customized 503 page and the next line in
> cache.log:
> 2022/01/25 19:01:25 kid1| ERROR: negotiating TLS on FD 26:
> error:1416F086:SSL routines:tls_process_server_certificate:certificate
> verify failed (1/-1/0)
> 
> Were there any improvement in this area in 5.x or 6.x brances?


"in this area" yes. Both versions have significant bug fixes around the 
chain handling. As usual the later the Squid version the better SSL-Bump 
and TLS "cutting edge" features work.

YMMV whether those changes help in your particular instances of the 
error. Some are caused by TLS certs just being invalid.


> And also the logging is very uninformative regarding the culprit of the
> issue.

That has improved a little in later versions. It is part of the ongoing 
work to figure out what is going on and what needs to be logged to 
understand the actions without facing a flood of crypto information.


> I would have expected that the remote host ip:port and sni would be logged
> as well in the above mentioned line.
> 

SNI is one of the details TLS/1.3 encrypts now  :(


> Currently I do not know about a way to identify from the logs these specific
> sites.

The "ERROR:" message gives you the FD number of the relevant client 
connection. With that "FD nn" you can scan the preceding cache.log in 
sections:

   5,9 50,9 51,3 (generic I/O)
   83,7 (security I/O)
   11,2 (HTTP messaging for CONNECT tunnel and cert fetches, if any)



Amos


From marcus.kool at urlfilterdb.com  Sat Feb  5 18:25:43 2022
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 5 Feb 2022 18:25:43 +0000
Subject: [squid-users] The status of AIA ie: TLS code:
 X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY ?
In-Reply-To: <0405e887-8c33-3cf8-4630-c43f695fe473@treenet.co.nz>
References: <000001d8120e$b6dabbe0$249033a0$@gmail.com>
 <0405e887-8c33-3cf8-4630-c43f695fe473@treenet.co.nz>
Message-ID: <292723c1-4ff0-fdbb-1682-d8aecb1095c8@urlfilterdb.com>


>> I would have expected that the remote host ip:port and sni would be logged
>> as well in the above mentioned line.
>>
>
> SNI is one of the details TLS/1.3 encrypts now? :(

To prevent misunderstandings, TLS 1.3 does not encrypt the SNI.

See https://datatracker.ietf.org/doc/html/draft-ietf-tls-esni :
Although TLS 1.3 [RFC8446  <https://datatracker.ietf.org/doc/html/rfc8446>] encrypts most of the handshake, including
the server certificate, there are several ways in which an on-path
attacker can learn private information about the connection.  The
plaintext Server Name Indication (SNI) extension in ClientHello
messages, which leaks the target domain for a given connection, is
perhaps the most sensitive, unencrypted information in TLS 1.3.

However, there is an optional TLS 1.3 extension that may encrypt the SNI and refers to it as ESNI.

Marcus


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220205/f818199f/attachment.htm>

From ngtech1ltd at gmail.com  Sun Feb  6 09:14:08 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 6 Feb 2022 11:14:08 +0200
Subject: [squid-users] external helper development
In-Reply-To: <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
 <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
Message-ID: <000001d81b39$e64dbdd0$b2e93970$@gmail.com>

Hey David,

It will take me more then couple seconds to write an example threaded python helper however it?s pretty simple why this is helper is slow.
It uses a select statement and threading in a very wrong way.
Before anything else try to compare the right helpers between php and python.
The next example helper can be used in comparison to a PHP helper and is much faster:
https://gist.githubusercontent.com/elico/03938e3a796c53f7c925872bade78195/raw/85f46ce58db12f30ed99a46c5f300dd8be401674/helper-1.py

#!/usr/bin/env python

import sys
import time

#set debug mode for True or False
debug = False
#debug = True

while True:
     line = sys.stdin.readline().strip()
     arr = line.split()
     msg = ""

     client = ""

     if debug:
       sys.stderr.write("__debug info__" + str(time.time()) +": \"" + line + "\"\n")
     if client and time.time() - time.mktime(time.gmtime(client)) < int(time.time() - logintime*60):
        sys.stdout.write(arr[0] + " OK \n")
        if debug:
          sys.stderr.write("__debug info__ : \"" + line + "\" and time in db is: "+ str(client) +"\n")
     else:
        sys.stdout.write(arr[0] + " ERR \n")
        if debug:
          sys.stderr.write("__debug info__ : " + line + '\n')
     sys.stdout.flush()
## END

If you have a specific API you want to try and test the requests against let me know and I will try to give an example via this:
* HTTP
* DNS
* Others

With the above example you would just need more helpers and add concurrency support to the squid external_acl helper configuration.
With enough helpers the stdin buffers will be enough to compensate the missing threads implementation.

Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: mailto:ngtech1ltd at gmail.com

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Friday, February 4, 2022 16:29
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] external helper development

Elizer,

Thanks for all this advice and indeed your arguments are valid between opening a socket, sending data, receiving data and closing the socket unlike direct access to a regex or a memory entry even if the calculation has already been done.

But what surprises me the most is that we have produced a python plugin in thread which I provide you a code below. 
The php code is like your mentioned example ( No thread, just a loop and output OK ) 

Results are after 6k requests, squid freeze and no surf can be made as with PHP code we can up to 10K requests and squid is happy
really, we did not understand why python is so low.

Here a python code using threads

#!/usr/bin/env python
import os
import sys
import time
import signal
import locale
import traceback
import threading
import select
import traceback as tb

class ClienThread():

    def __init__(self):
        self._exiting = False
        self._cache = {}

    def exit(self):
        self._exiting = True

    def stdout(self, lineToSend):
        try:
            sys.stdout.write(lineToSend)
            sys.stdout.flush()

        except IOError as e:
            if e.errno==32:
                # Error Broken PIPE!"
                pass
        except:
            # other execpt
            pass

    def run(self):
        while not self._exiting:
            if sys.stdin in select.select([sys.stdin], [], [], 0.5)[0]:
                line = sys.stdin.readline()
                LenOfline=len(line)

                if LenOfline==0:
                    self._exiting=True
                    break

                if line[-1] == '\n':line = line[:-1]
                channel = None
                options = line.split()

                try:
                    if options[0].isdigit(): channel = options.pop(0)
                except IndexError:
                    self.stdout("0 OK first=ERROR\n")
                    continue

                # Processing here

                try:
                    self.stdout("%s OK\n" % channel)
                except:
                    self.stdout("%s ERROR first=ERROR\n" % channel)




class Main(object):
    def __init__(self):
        self._threads = []
        self._exiting = False
        self._reload = False
        self._config = ""

        for sig, action in (
            (signal.SIGINT, self.shutdown),
            (signal.SIGQUIT, self.shutdown),
            (signal.SIGTERM, self.shutdown),
            (signal.SIGHUP, lambda s, f: setattr(self, '_reload', True)),
            (signal.SIGPIPE, signal.SIG_IGN),
        ):
            try:
                signal.signal(sig, action)
            except AttributeError:
                pass



    def shutdown(self, sig = None, frame = None):
        self._exiting = True
        self.stop_threads()

    def start_threads(self):

        sThread = ClienThread()
        t = threading.Thread(target = sThread.run)
        t.start()
        self._threads.append((sThread, t))



    def stop_threads(self):
        for p, t in self._threads:
            p.exit()
        for p, t in self._threads:
            t.join(timeout =  1.0)
        self._threads = []

    def run(self):
        """ main loop """
        ret = 0
        self.start_threads()
        return ret


if __name__ == '__main__':
    # set C locale
    locale.setlocale(locale.LC_ALL, 'C')
    os.environ['LANG'] = 'C'
    ret = 0
    try:
        main = Main()
        ret = main.run()
    except SystemExit:
        pass
    except KeyboardInterrupt:
        ret = 4
    except:
    sys.exit(ret)
Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit :
And about the cache of each helpers, the cost of a cache on a single helper is not much in terms of memory comparing to some network access.
Again it?s possible to test and verify this on a loaded system to get results. The delay itself can be seen from squid side in the cache manager statistics.
 
You can also try to compare the next ruby helper:
https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper
 
About a shared ?base? which allows helpers to avoid computation of the query?. It?s a good argument, however it depends what is the cost of
pulling from the cache compared to calculating the answer.
A very simple string comparison or regex matching would probably be faster than reaching a shared storage in many cases.
 
Also take into account the ?concurrency? support from the helper side.
A helper that supports parallel processing of requests/lines can do better then many single helpers in more than once use case.
In any case I would suggest to enable requests concurrency from squid side since the STDIN buffer will emulate some level of concurrency
by itself and will allow squid to keep going forward faster.
 
Just to mention that SquidGuard have used a single helper cache for a very long time, ie every single SquidGuard helper has it?s own copy of the whole
configuration and database files in memory.
 
And again, if you do have any option to implement a server service model and that the helpers will contact this main service you will be able to implement
much faster internal in-memory cache compared to a redis/memcahe/other external daemon(need to be tested).
 
A good example for this is ufdbguard which has helpers that are clients of the main service which does the whole heavy lifting and also holds 
one copy of the DB.
 
I have implemented SquidBlocker this way and have seen that it out-performs any other service I have tried until now.




From ngtech1ltd at gmail.com  Sun Feb  6 10:46:42 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 6 Feb 2022 12:46:42 +0200
Subject: [squid-users] external helper development
In-Reply-To: <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
 <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
Message-ID: <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>

Hey David,

 

Not a fully completed helper but it seems to works pretty nice and might be better then what exist already:

https://gist.githubusercontent.com/elico/03938e3a796c53f7c925872bade78195/raw/21ff1bbc0cf3d91719db27d9d027652e8bd3de4e/threaded-helper-example.py

 

#!/usr/bin/env python

 

import sys

import time

import urllib.request

import signal

import threading

 

#set debug mode for True or False

debug = False

#debug = True

queue = []

threads = []

 

RUNNING = True

 

quit = 0

rand_api_url = "https://cloud1.ngtech.co.il/api/test.php"

 

def sig_handler(signum, frame):

    sys.stderr.write("Signal is received:" + str(signum) + "\n")

    global quit

    quit = 1

    global RUNNING

    RUNNING=False

 

 

def handle_line(line):

     if not RUNNING:

         return

     if not line:

         return

     if quit > 0:

         return

 

     arr = line.split()

     response = urllib.request.urlopen( rand_api_url )

     response_text = response.read()

 

     queue.append(arr[0] + " " + response_text.decode("utf-8"))

 

def handle_stdout(n):

     while RUNNING:

         if quit > 0:

           return

         while len(queue) > 0:

             item = queue.pop(0)

             sys.stdout.write(item)

             sys.stdout.flush()

         time.sleep(0.5)

 

def handle_stdin(n):

    while RUNNING:

         line = sys.stdin.readline()

         if not line:

             break

         if quit > 0:

             break

         line = line.strip()

         thread = threading.Thread(target=handle_line, args=(line,))

         thread.start()

         threads.append(thread)

 

signal.signal(signal.SIGUSR1, sig_handler)

signal.signal(signal.SIGUSR2, sig_handler)

signal.signal(signal.SIGALRM, sig_handler)

signal.signal(signal.SIGINT, sig_handler)

signal.signal(signal.SIGQUIT, sig_handler)

signal.signal(signal.SIGTERM, sig_handler)

 

stdout_thread = threading.Thread(target=handle_stdout, args=(1,))

stdout_thread.start()

 

threads.append(stdout_thread)

 

stdin_thread = threading.Thread(target=handle_stdin, args=(2,))

stdin_thread.start()

 

threads.append(stdin_thread)

 

while(RUNNING):

    time.sleep(3)

 

print("Not RUNNING")

for thread in threads:

    thread.join()

print("All threads stopped.")

## END

 

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Friday, February 4, 2022 16:29
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] external helper development

 

Elizer,

Thanks for all this advice and indeed your arguments are valid between opening a socket, sending data, receiving data and closing the socket unlike direct access to a regex or a memory entry even if the calculation has already been done.

But what surprises me the most is that we have produced a python plugin in thread which I provide you a code below. 
The php code is like your mentioned example ( No thread, just a loop and output OK ) 

Results are after 6k requests, squid freeze and no surf can be made as with PHP code we can up to 10K requests and squid is happy
really, we did not understand why python is so low.

Here a python code using threads

#!/usr/bin/env python
import os
import sys
import time
import signal
import locale
import traceback
import threading
import select
import traceback as tb

class ClienThread():

    def __init__(self):
        self._exiting = False
        self._cache = {}

    def exit(self):
        self._exiting = True

    def stdout(self, lineToSend):
        try:
            sys.stdout.write(lineToSend)
            sys.stdout.flush()

        except IOError as e:
            if e.errno==32:
                # Error Broken PIPE!"
                pass
        except:
            # other execpt
            pass

    def run(self):
        while not self._exiting:
            if sys.stdin in select.select([sys.stdin], [], [], 0.5)[0]:
                line = sys.stdin.readline()
                LenOfline=len(line)

                if LenOfline==0:
                    self._exiting=True
                    break

                if line[-1] == '\n':line = line[:-1]
                channel = None
                options = line.split()

                try:
                    if options[0].isdigit(): channel = options.pop(0)
                except IndexError:
                    self.stdout("0 OK first=ERROR\n")
                    continue

                # Processing here

                try:
                    self.stdout("%s OK\n" % channel)
                except:
                    self.stdout("%s ERROR first=ERROR\n" % channel)




class Main(object):
    def __init__(self):
        self._threads = []
        self._exiting = False
        self._reload = False
        self._config = ""

        for sig, action in (
            (signal.SIGINT, self.shutdown),
            (signal.SIGQUIT, self.shutdown),
            (signal.SIGTERM, self.shutdown),
            (signal.SIGHUP, lambda s, f: setattr(self, '_reload', True)),
            (signal.SIGPIPE, signal.SIG_IGN),
        ):
            try:
                signal.signal(sig, action)
            except AttributeError:
                pass



    def shutdown(self, sig = None, frame = None):
        self._exiting = True
        self.stop_threads()

    def start_threads(self):

        sThread = ClienThread()
        t = threading.Thread(target = sThread.run)
        t.start()
        self._threads.append((sThread, t))



    def stop_threads(self):
        for p, t in self._threads:
            p.exit()
        for p, t in self._threads:
            t.join(timeout =  1.0)
        self._threads = []

    def run(self):
        """ main loop """
        ret = 0
        self.start_threads()
        return ret


if __name__ == '__main__':
    # set C locale
    locale.setlocale(locale.LC_ALL, 'C')
    os.environ['LANG'] = 'C'
    ret = 0
    try:
        main = Main()
        ret = main.run()
    except SystemExit:
        pass
    except KeyboardInterrupt:
        ret = 4
    except:
    sys.exit(ret)

Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit :

And about the cache of each helpers, the cost of a cache on a single helper is not much in terms of memory comparing to some network access.

Again it?s possible to test and verify this on a loaded system to get results. The delay itself can be seen from squid side in the cache manager statistics.

 

You can also try to compare the next ruby helper:

https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper

 

About a shared ?base? which allows helpers to avoid computation of the query?. It?s a good argument, however it depends what is the cost of
pulling from the cache compared to calculating the answer.

A very simple string comparison or regex matching would probably be faster than reaching a shared storage in many cases.

 

Also take into account the ?concurrency? support from the helper side.

A helper that supports parallel processing of requests/lines can do better then many single helpers in more than once use case.

In any case I would suggest to enable requests concurrency from squid side since the STDIN buffer will emulate some level of concurrency
by itself and will allow squid to keep going forward faster.

 

Just to mention that SquidGuard have used a single helper cache for a very long time, ie every single SquidGuard helper has it?s own copy of the whole

configuration and database files in memory.

 

And again, if you do have any option to implement a server service model and that the helpers will contact this main service you will be able to implement
much faster internal in-memory cache compared to a redis/memcahe/other external daemon(need to be tested).

 

A good example for this is ufdbguard which has helpers that are clients of the main service which does the whole heavy lifting and also holds 
one copy of the DB.

 

I have implemented SquidBlocker this way and have seen that it out-performs any other service I have tried until now.

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220206/196e540e/attachment.htm>

From ngtech1ltd at gmail.com  Sun Feb  6 14:39:43 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 6 Feb 2022 16:39:43 +0200
Subject: [squid-users] [ext] Re: Absolute upper limit for
 filedescriptors in squid-6?
In-Reply-To: <Yfzf7Hkx02WzUiXm@charite.de>
References: <Ye/tmgpKefCkWXE1@charite.de>
 <CA+Y8hcPb-5dvucAGCJJLm1hp=_+bLV-_qLjz70kHgc7TJOAfcA@mail.gmail.com>
 <YfAPaR4N1gP9Glya@charite.de>
 <8f6da3b8-736d-5d5b-5d05-7d492a6c252f@treenet.co.nz>
 <8acd9b69-9fc8-b657-e5bf-76f5bf3680f7@measurement-factory.com>
 <YfqSdkk5fToAdFU9@charite.de>
 <CABA8h=RQQWExqGUWgRUFXug4j-JtdvhcvWAoO4pOZenVCu6VMQ@mail.gmail.com>
 <YfuHRofBKiE2j9xU@charite.de> <000901d8198d$6dff9860$49fec920$@gmail.com>
 <Yfzf7Hkx02WzUiXm@charite.de>
Message-ID: <001301d81b67$62301790$269046b0$@gmail.com>

It has Systemd, Use it.....

Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: Ralf Hildebrandt <Ralf.Hildebrandt at charite.de> 
Sent: Friday, February 4, 2022 10:12
To: Eliezer Croitoru <ngtech1ltd at gmail.com>
Cc: 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] [ext] Re: Absolute upper limit for filedescriptors in squid-6?

* Eliezer Croitoru <ngtech1ltd at gmail.com>:

> What OS are you using exactly?

Ubuntu 20.04 on amd64

Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de



From david at articatech.com  Mon Feb  7 00:02:28 2022
From: david at articatech.com (David Touzeau)
Date: Mon, 7 Feb 2022 01:02:28 +0100
Subject: [squid-users] external helper development
In-Reply-To: <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
 <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
 <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>
Message-ID: <9b30ea58-1454-ac41-90b9-88337453b540@articatech.com>

Thanks Elizer !!

I have tested your code as is in /lib/squid3/external_acl_first process 
but it take 100% CPU and squid freeze requests.
Seems a crazy loop somewhere...

root???? 105852? 0.0? 0.1? 73712? 9256 ???????? SNs? 00:27?? 0:00 squid
squid??? 105854? 0.0? 0.3? 89540 27536 ???????? SN?? 00:27?? 0:00 
(squid-1) --kid squid-1
squid??? 105855 91.6? 0.5 219764 47636 ???????? SNl? 00:27?? 2:52 python 
/lib/squid3/external_acl_first
squid??? 105856 91.8? 0.5 219768 47672 ???????? SNl? 00:27?? 2:52 python 
/lib/squid3/external_acl_first
squid??? 105857 92.9? 0.5 293488 47696 ???????? SNl? 00:27?? 2:54 python 
/lib/squid3/external_acl_first
squid??? 105858 91.8? 0.6 367228 49728 ???????? SNl? 00:27?? 2:52 python 
/lib/squid3/external_acl_first

I did not find where it should be...


Le 06/02/2022 ? 11:46, Eliezer Croitoru a ?crit?:
>
> Hey David,
>
> Not a fully completed helper but it seems to works pretty nice and 
> might be better then what exist already:
>
> https://gist.githubusercontent.com/elico/03938e3a796c53f7c925872bade78195/raw/21ff1bbc0cf3d91719db27d9d027652e8bd3de4e/threaded-helper-example.py
>
> #!/usr/bin/env python
>
> import sys
>
> import time
>
> import urllib.request
>
> import signal
>
> import threading
>
> #set debug mode for True or False
>
> debug = False
>
> #debug = True
>
> queue = []
>
> threads = []
>
> RUNNING = True
>
> quit = 0
>
> rand_api_url = "https://cloud1.ngtech.co.il/api/test.php"
>
> def sig_handler(signum, frame):
>
> ??? sys.stderr.write("Signal is received:" + str(signum) + "\n")
>
> ??? global quit
>
> ??? quit = 1
>
> ??? global RUNNING
>
> ??? RUNNING=False
>
> def handle_line(line):
>
> ???? if not RUNNING:
>
> ???????? return
>
> ???? if not line:
>
> ???????? return
>
> ???? if quit > 0:
>
> ???????? return
>
> ???? arr = line.split()
>
> ???? response = urllib.request.urlopen( rand_api_url )
>
> ???? response_text = response.read()
>
> ???? queue.append(arr[0] + " " + response_text.decode("utf-8"))
>
> def handle_stdout(n):
>
> ???? while RUNNING:
>
> ???????? if quit > 0:
>
> ?????????? return
>
> ???????? while len(queue) > 0:
>
> ???????????? item = queue.pop(0)
>
> sys.stdout.write(item)
>
> ???????????? sys.stdout.flush()
>
> ???????? time.sleep(0.5)
>
> def handle_stdin(n):
>
> ??? while RUNNING:
>
> ???????? line = sys.stdin.readline()
>
> ???????? if not line:
>
> ???????????? break
>
> ???????? if quit > 0:
>
> ???????????? break
>
> ???????? line = line.strip()
>
> ???????? thread = threading.Thread(target=handle_line, args=(line,))
>
> ???????? thread.start()
>
> ???????? threads.append(thread)
>
> signal.signal(signal.SIGUSR1, sig_handler)
>
> signal.signal(signal.SIGUSR2, sig_handler)
>
> signal.signal(signal.SIGALRM, sig_handler)
>
> signal.signal(signal.SIGINT, sig_handler)
>
> signal.signal(signal.SIGQUIT, sig_handler)
>
> signal.signal(signal.SIGTERM, sig_handler)
>
> stdout_thread = threading.Thread(target=handle_stdout, args=(1,))
>
> stdout_thread.start()
>
> threads.append(stdout_thread)
>
> stdin_thread = threading.Thread(target=handle_stdin, args=(2,))
>
> stdin_thread.start()
>
> threads.append(stdin_thread)
>
> while(RUNNING):
>
> ??? time.sleep(3)
>
> print("Not RUNNING")
>
> for thread in threads:
>
> ??? thread.join()
>
> print("All threads stopped.")
>
> ## END
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> *From:*squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *David Touzeau
> *Sent:* Friday, February 4, 2022 16:29
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] external helper development
>
> Elizer,
>
> Thanks for all this advice and indeed your arguments are valid between 
> opening a socket, sending data, receiving data and closing the socket 
> unlike direct access to a regex or a memory entry even if the 
> calculation has already been done.
>
> But what surprises me the most is that we have produced a python 
> plugin in thread which I provide you a code below.
> The php code is like your mentioned example ( No thread, just a loop 
> and output OK )
>
> Results are after 6k requests, squid freeze and no surf can be made as 
> with PHP code we can up to 10K requests and squid is happy
> really, we did not understand why python is so low.
>
> Here a python code using threads
>
> #!/usr/bin/env python
> import os
> import sys
> import time
> import signal
> import locale
> import traceback
> import threading
> import select
> import traceback as tb
>
> class ClienThread():
>
> ??? def __init__(self):
> ??????? self._exiting = False
> ??????? self._cache = {}
>
> ??? def exit(self):
> ??????? self._exiting = True
>
> ??? def stdout(self, lineToSend):
> ??????? try:
> ??????????? sys.stdout.write(lineToSend)
> ??????????? sys.stdout.flush()
>
> ??????? except IOError as e:
> ??????????? if e.errno==32:
> ??????????????? # Error Broken PIPE!"
> ??????????????? pass
> ??????? except:
> ??????????? # other execpt
> ??????????? pass
>
> ??? def run(self):
> ??????? while not self._exiting:
> ??????????? if sys.stdin in select.select([sys.stdin], [], [], 0.5)[0]:
> ??????????????? line = sys.stdin.readline()
> ??????????????? LenOfline=len(line)
>
> ??????????????? if LenOfline==0:
> ??????????????????? self._exiting=True
> ??????????????????? break
>
> ??????????????? if line[-1] == '\n':line = line[:-1]
> ??????????????? channel = None
> ??????????????? options = line.split()
>
> ??????????????? try:
> ??????????????????? if options[0].isdigit(): channel = options.pop(0)
> ??????????????? except IndexError:
> ??????????????????? self.stdout("0 OK first=ERROR\n")
> ??????????????????? continue
>
> ??????????????? # Processing here
>
> ??????????????? try:
> ??????????????????? self.stdout("%s OK\n" % channel)
> ??????????????? except:
> ??????????????????? self.stdout("%s ERROR first=ERROR\n" % channel)
>
>
>
>
> class Main(object):
> ??? def __init__(self):
> ??????? self._threads = []
> ??????? self._exiting = False
> ??????? self._reload = False
> ??????? self._config = ""
>
> ??????? for sig, action in (
> ??????????? (signal.SIGINT, self.shutdown),
> ??????????? (signal.SIGQUIT, self.shutdown),
> ??????????? (signal.SIGTERM, self.shutdown),
> ??????????? (signal.SIGHUP, lambda s, f: setattr(self, '_reload', True)),
> ??????????? (signal.SIGPIPE, signal.SIG_IGN),
> ??????? ):
> ??????????? try:
> ??????????????? signal.signal(sig, action)
> ??????????? except AttributeError:
> ??????????????? pass
>
>
>
> ??? def shutdown(self, sig = None, frame = None):
> ??????? self._exiting = True
> ??????? self.stop_threads()
>
> ??? def start_threads(self):
>
> ??????? sThread = ClienThread()
> ??????? t = threading.Thread(target = sThread.run)
> ??????? t.start()
> ??????? self._threads.append((sThread, t))
>
>
>
> ??? def stop_threads(self):
> ??????? for p, t in self._threads:
> ??????????? p.exit()
> ??????? for p, t in self._threads:
> ??????????? t.join(timeout =? 1.0)
> ??????? self._threads = []
>
> ??? def run(self):
> ??????? """ main loop """
> ??????? ret = 0
> ??????? self.start_threads()
> ??????? return ret
>
>
> if __name__ == '__main__':
> ??? # set C locale
> ??? locale.setlocale(locale.LC_ALL, 'C')
> ??? os.environ['LANG'] = 'C'
> ??? ret = 0
> ??? try:
> ??????? main = Main()
> ??????? ret = main.run()
> ??? except SystemExit:
> ??????? pass
> ??? except KeyboardInterrupt:
> ??????? ret = 4
> ??? except:
> ??? sys.exit(ret)
>
> Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit?:
>
>     And about the cache of each helpers, the cost of a cache on a
>     single helper is not much in terms of memory comparing to some
>     network access.
>
>     Again it?s possible to test and verify this on a loaded system to
>     get results. The delay itself can be seen from squid side in the
>     cache manager statistics.
>
>     You can also try to compare the next ruby helper:
>
>     https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper
>
>     About a shared ?base? which allows helpers to avoid computation of
>     the query?. It?s a good argument, however it depends what is the
>     cost of
>     pulling from the cache compared to calculating the answer.
>
>     A very simple string comparison or regex matching would probably
>     be faster than reaching a shared storage in many cases.
>
>     Also take into account the ?concurrency? support from the helper side.
>
>     A helper that supports parallel processing of requests/lines can
>     do better then many single helpers in more than once use case.
>
>     In any case I would suggest to enable requests concurrency from
>     squid side since the STDIN buffer will emulate some level of
>     concurrency
>     by itself and will allow squid to keep going forward faster.
>
>     Just to mention that SquidGuard have used a single helper cache
>     for a very long time, ie every single SquidGuard helper has it?s
>     own copy of the whole
>
>     configuration and database files in memory.
>
>     And again, if you do have any option to implement a server service
>     model and that the helpers will contact this main service you will
>     be able to implement
>     much faster internal in-memory cache compared to a
>     redis/memcahe/other external daemon(need to be tested).
>
>     A good example for this is ufdbguard which has helpers that are
>     clients of the main service which does the whole heavy lifting and
>     also holds
>     one copy of the DB.
>
>     I have implemented SquidBlocker this way and have seen that it
>     out-performs any other service I have tried until now.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220207/7e993ab4/attachment.htm>

From david at articatech.com  Mon Feb  7 00:41:58 2022
From: david at articatech.com (David Touzeau)
Date: Mon, 7 Feb 2022 01:41:58 +0100
Subject: [squid-users] external helper development
In-Reply-To: <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
 <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
 <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>
Message-ID: <f44c5bab-cd76-dda2-96eb-88f3c7462f4e@articatech.com>

Sorry? Elizer

It was a mistake... No, your code is clean..
Impressive for the first shot
Many thanks for your example, we will run our stress tool to see the 
difference...

Just a question

Why did you send 500 milliseconds of sleep in the handle_stdoud ? Is it 
for let squid closing the pipe ?



Le 06/02/2022 ? 11:46, Eliezer Croitoru a ?crit?:
>
> Hey David,
>
> Not a fully completed helper but it seems to works pretty nice and 
> might be better then what exist already:
>
> https://gist.githubusercontent.com/elico/03938e3a796c53f7c925872bade78195/raw/21ff1bbc0cf3d91719db27d9d027652e8bd3de4e/threaded-helper-example.py
>
> #!/usr/bin/env python
>
> import sys
>
> import time
>
> import urllib.request
>
> import signal
>
> import threading
>
> #set debug mode for True or False
>
> debug = False
>
> #debug = True
>
> queue = []
>
> threads = []
>
> RUNNING = True
>
> quit = 0
>
> rand_api_url = "https://cloud1.ngtech.co.il/api/test.php"
>
> def sig_handler(signum, frame):
>
> ??? sys.stderr.write("Signal is received:" + str(signum) + "\n")
>
> ??? global quit
>
> ??? quit = 1
>
> ??? global RUNNING
>
> ??? RUNNING=False
>
> def handle_line(line):
>
> ???? if not RUNNING:
>
> ???????? return
>
> ???? if not line:
>
> ???????? return
>
> ???? if quit > 0:
>
> ???????? return
>
> ???? arr = line.split()
>
> ???? response = urllib.request.urlopen( rand_api_url )
>
> ???? response_text = response.read()
>
> ???? queue.append(arr[0] + " " + response_text.decode("utf-8"))
>
> def handle_stdout(n):
>
> ???? while RUNNING:
>
> ???????? if quit > 0:
>
> ?????????? return
>
> ???????? while len(queue) > 0:
>
> ???????????? item = queue.pop(0)
>
> sys.stdout.write(item)
>
> ???????????? sys.stdout.flush()
>
> ???????? time.sleep(0.5)
>
> def handle_stdin(n):
>
> ??? while RUNNING:
>
> ???????? line = sys.stdin.readline()
>
> ???????? if not line:
>
> ???????????? break
>
> ???????? if quit > 0:
>
> ???????????? break
>
> ???????? line = line.strip()
>
> ???????? thread = threading.Thread(target=handle_line, args=(line,))
>
> ???????? thread.start()
>
> ???????? threads.append(thread)
>
> signal.signal(signal.SIGUSR1, sig_handler)
>
> signal.signal(signal.SIGUSR2, sig_handler)
>
> signal.signal(signal.SIGALRM, sig_handler)
>
> signal.signal(signal.SIGINT, sig_handler)
>
> signal.signal(signal.SIGQUIT, sig_handler)
>
> signal.signal(signal.SIGTERM, sig_handler)
>
> stdout_thread = threading.Thread(target=handle_stdout, args=(1,))
>
> stdout_thread.start()
>
> threads.append(stdout_thread)
>
> stdin_thread = threading.Thread(target=handle_stdin, args=(2,))
>
> stdin_thread.start()
>
> threads.append(stdin_thread)
>
> while(RUNNING):
>
> ??? time.sleep(3)
>
> print("Not RUNNING")
>
> for thread in threads:
>
> ??? thread.join()
>
> print("All threads stopped.")
>
> ## END
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> *From:*squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *David Touzeau
> *Sent:* Friday, February 4, 2022 16:29
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] external helper development
>
> Elizer,
>
> Thanks for all this advice and indeed your arguments are valid between 
> opening a socket, sending data, receiving data and closing the socket 
> unlike direct access to a regex or a memory entry even if the 
> calculation has already been done.
>
> But what surprises me the most is that we have produced a python 
> plugin in thread which I provide you a code below.
> The php code is like your mentioned example ( No thread, just a loop 
> and output OK )
>
> Results are after 6k requests, squid freeze and no surf can be made as 
> with PHP code we can up to 10K requests and squid is happy
> really, we did not understand why python is so low.
>
> Here a python code using threads
>
> #!/usr/bin/env python
> import os
> import sys
> import time
> import signal
> import locale
> import traceback
> import threading
> import select
> import traceback as tb
>
> class ClienThread():
>
> ??? def __init__(self):
> ??????? self._exiting = False
> ??????? self._cache = {}
>
> ??? def exit(self):
> ??????? self._exiting = True
>
> ??? def stdout(self, lineToSend):
> ??????? try:
> ??????????? sys.stdout.write(lineToSend)
> ??????????? sys.stdout.flush()
>
> ??????? except IOError as e:
> ??????????? if e.errno==32:
> ??????????????? # Error Broken PIPE!"
> ??????????????? pass
> ??????? except:
> ??????????? # other execpt
> ??????????? pass
>
> ??? def run(self):
> ??????? while not self._exiting:
> ??????????? if sys.stdin in select.select([sys.stdin], [], [], 0.5)[0]:
> ??????????????? line = sys.stdin.readline()
> ??????????????? LenOfline=len(line)
>
> ??????????????? if LenOfline==0:
> ??????????????????? self._exiting=True
> ??????????????????? break
>
> ??????????????? if line[-1] == '\n':line = line[:-1]
> ??????????????? channel = None
> ??????????????? options = line.split()
>
> ??????????????? try:
> ??????????????????? if options[0].isdigit(): channel = options.pop(0)
> ??????????????? except IndexError:
> ??????????????????? self.stdout("0 OK first=ERROR\n")
> ??????????????????? continue
>
> ??????????????? # Processing here
>
> ??????????????? try:
> ??????????????????? self.stdout("%s OK\n" % channel)
> ??????????????? except:
> ??????????????????? self.stdout("%s ERROR first=ERROR\n" % channel)
>
>
>
>
> class Main(object):
> ??? def __init__(self):
> ??????? self._threads = []
> ??????? self._exiting = False
> ??????? self._reload = False
> ??????? self._config = ""
>
> ??????? for sig, action in (
> ??????????? (signal.SIGINT, self.shutdown),
> ??????????? (signal.SIGQUIT, self.shutdown),
> ??????????? (signal.SIGTERM, self.shutdown),
> ??????????? (signal.SIGHUP, lambda s, f: setattr(self, '_reload', True)),
> ??????????? (signal.SIGPIPE, signal.SIG_IGN),
> ??????? ):
> ??????????? try:
> ??????????????? signal.signal(sig, action)
> ??????????? except AttributeError:
> ??????????????? pass
>
>
>
> ??? def shutdown(self, sig = None, frame = None):
> ??????? self._exiting = True
> ??????? self.stop_threads()
>
> ??? def start_threads(self):
>
> ??????? sThread = ClienThread()
> ??????? t = threading.Thread(target = sThread.run)
> ??????? t.start()
> ??????? self._threads.append((sThread, t))
>
>
>
> ??? def stop_threads(self):
> ??????? for p, t in self._threads:
> ??????????? p.exit()
> ??????? for p, t in self._threads:
> ??????????? t.join(timeout =? 1.0)
> ??????? self._threads = []
>
> ??? def run(self):
> ??????? """ main loop """
> ??????? ret = 0
> ??????? self.start_threads()
> ??????? return ret
>
>
> if __name__ == '__main__':
> ??? # set C locale
> ??? locale.setlocale(locale.LC_ALL, 'C')
> ??? os.environ['LANG'] = 'C'
> ??? ret = 0
> ??? try:
> ??????? main = Main()
> ??????? ret = main.run()
> ??? except SystemExit:
> ??????? pass
> ??? except KeyboardInterrupt:
> ??????? ret = 4
> ??? except:
> ??? sys.exit(ret)
>
> Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit?:
>
>     And about the cache of each helpers, the cost of a cache on a
>     single helper is not much in terms of memory comparing to some
>     network access.
>
>     Again it?s possible to test and verify this on a loaded system to
>     get results. The delay itself can be seen from squid side in the
>     cache manager statistics.
>
>     You can also try to compare the next ruby helper:
>
>     https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper
>
>     About a shared ?base? which allows helpers to avoid computation of
>     the query?. It?s a good argument, however it depends what is the
>     cost of
>     pulling from the cache compared to calculating the answer.
>
>     A very simple string comparison or regex matching would probably
>     be faster than reaching a shared storage in many cases.
>
>     Also take into account the ?concurrency? support from the helper side.
>
>     A helper that supports parallel processing of requests/lines can
>     do better then many single helpers in more than once use case.
>
>     In any case I would suggest to enable requests concurrency from
>     squid side since the STDIN buffer will emulate some level of
>     concurrency
>     by itself and will allow squid to keep going forward faster.
>
>     Just to mention that SquidGuard have used a single helper cache
>     for a very long time, ie every single SquidGuard helper has it?s
>     own copy of the whole
>
>     configuration and database files in memory.
>
>     And again, if you do have any option to implement a server service
>     model and that the helpers will contact this main service you will
>     be able to implement
>     much faster internal in-memory cache compared to a
>     redis/memcahe/other external daemon(need to be tested).
>
>     A good example for this is ufdbguard which has helpers that are
>     clients of the main service which does the whole heavy lifting and
>     also holds
>     one copy of the DB.
>
>     I have implemented SquidBlocker this way and have seen that it
>     out-performs any other service I have tried until now.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220207/9646d2ab/attachment.htm>

From ngtech1ltd at gmail.com  Mon Feb  7 15:14:55 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 7 Feb 2022 17:14:55 +0200
Subject: [squid-users] external helper development
In-Reply-To: <f44c5bab-cd76-dda2-96eb-88f3c7462f4e@articatech.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
 <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
 <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>
 <f44c5bab-cd76-dda2-96eb-88f3c7462f4e@articatech.com>
Message-ID: <001701d81c35$77894980$669bdc80$@gmail.com>

Hey David,

 

Since the handle_stdout runs in it?s own thread it?s sole purpose is to send results to stdout.

If I will run the next code in a simple software without the 0.5 sleep time:

     while RUNNING:

         if quit > 0:

           return

         while len(queue) > 0:

             item = queue.pop(0)

             sys.stdout.write(item)

             sys.stdout.flush()

         time.sleep(0.5)

 

 

what will happen is that the software will run with 100% CPU looping over and over on the size of the queue
while sometimes it will spit some data to stdout.

Adding a small delay with 0.5 secs will allow some ?idle? time for the cpu in the loop preventing it from consuming
all the CPU time.

It?s a very old technique and there are others which are more efficient but it?s enough to demonstrate that a simple
threaded helper is much better then any PHP code that was not meant to be running as a STDIN/OUT daemon/helper software.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: David Touzeau <david at articatech.com> 
Sent: Monday, February 7, 2022 02:42
To: Eliezer Croitoru <ngtech1ltd at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] external helper development

 

Sorry  Elizer

It was a mistake... No, your code is clean..
Impressive for the first shot
Many thanks for your example, we will run our stress tool to see the difference...

Just a question

Why did you send 500 milliseconds of sleep in the handle_stdoud ? Is it for let squid closing the pipe ?




Le 06/02/2022 ? 11:46, Eliezer Croitoru a ?crit :

Hey David,

 

Not a fully completed helper but it seems to works pretty nice and might be better then what exist already:

https://gist.githubusercontent.com/elico/03938e3a796c53f7c925872bade78195/raw/21ff1bbc0cf3d91719db27d9d027652e8bd3de4e/threaded-helper-example.py

 

#!/usr/bin/env python

 

import sys

import time

import urllib.request

import signal

import threading

 

#set debug mode for True or False

debug = False

#debug = True

queue = []

threads = []

 

RUNNING = True

 

quit = 0

rand_api_url =  <https://cloud1.ngtech.co.il/api/test.php> "https://cloud1.ngtech.co.il/api/test.php"

 

def sig_handler(signum, frame):

    sys.stderr.write("Signal is received:" + str(signum) + "\n")

    global quit

    quit = 1

    global RUNNING

    RUNNING=False

 

 

def handle_line(line):

     if not RUNNING:

         return

     if not line:

         return

     if quit > 0:

         return

 

     arr = line.split()

     response = urllib.request.urlopen( rand_api_url )

     response_text = response.read()

 

     queue.append(arr[0] + " " + response_text.decode("utf-8"))

 

def handle_stdout(n):

     while RUNNING:

         if quit > 0:

           return

         while len(queue) > 0:

             item = queue.pop(0)

             sys.stdout.write(item)

             sys.stdout.flush()

         time.sleep(0.5)

 

def handle_stdin(n):

    while RUNNING:

         line = sys.stdin.readline()

         if not line:

             break

         if quit > 0:

             break

         line = line.strip()

         thread = threading.Thread(target=handle_line, args=(line,))

         thread.start()

         threads.append(thread)

 

signal.signal(signal.SIGUSR1, sig_handler)

signal.signal(signal.SIGUSR2, sig_handler)

signal.signal(signal.SIGALRM, sig_handler)

signal.signal(signal.SIGINT, sig_handler)

signal.signal(signal.SIGQUIT, sig_handler)

signal.signal(signal.SIGTERM, sig_handler)

 

stdout_thread = threading.Thread(target=handle_stdout, args=(1,))

stdout_thread.start()

 

threads.append(stdout_thread)

 

stdin_thread = threading.Thread(target=handle_stdin, args=(2,))

stdin_thread.start()

 

threads.append(stdin_thread)

 

while(RUNNING):

    time.sleep(3)

 

print("Not RUNNING")

for thread in threads:

    thread.join()

print("All threads stopped.")

## END

 

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Friday, February 4, 2022 16:29
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] external helper development

 

Elizer,

Thanks for all this advice and indeed your arguments are valid between opening a socket, sending data, receiving data and closing the socket unlike direct access to a regex or a memory entry even if the calculation has already been done.

But what surprises me the most is that we have produced a python plugin in thread which I provide you a code below. 
The php code is like your mentioned example ( No thread, just a loop and output OK ) 

Results are after 6k requests, squid freeze and no surf can be made as with PHP code we can up to 10K requests and squid is happy
really, we did not understand why python is so low.

Here a python code using threads

#!/usr/bin/env python
import os
import sys
import time
import signal
import locale
import traceback
import threading
import select
import traceback as tb

class ClienThread():

    def __init__(self):
        self._exiting = False
        self._cache = {}

    def exit(self):
        self._exiting = True

    def stdout(self, lineToSend):
        try:
            sys.stdout.write(lineToSend)
            sys.stdout.flush()

        except IOError as e:
            if e.errno==32:
                # Error Broken PIPE!"
                pass
        except:
            # other execpt
            pass

    def run(self):
        while not self._exiting:
            if sys.stdin in select.select([sys.stdin], [], [], 0.5)[0]:
                line = sys.stdin.readline()
                LenOfline=len(line)

                if LenOfline==0:
                    self._exiting=True
                    break

                if line[-1] == '\n':line = line[:-1]
                channel = None
                options = line.split()

                try:
                    if options[0].isdigit(): channel = options.pop(0)
                except IndexError:
                    self.stdout("0 OK first=ERROR\n")
                    continue

                # Processing here

                try:
                    self.stdout("%s OK\n" % channel)
                except:
                    self.stdout("%s ERROR first=ERROR\n" % channel)




class Main(object):
    def __init__(self):
        self._threads = []
        self._exiting = False
        self._reload = False
        self._config = ""

        for sig, action in (
            (signal.SIGINT, self.shutdown),
            (signal.SIGQUIT, self.shutdown),
            (signal.SIGTERM, self.shutdown),
            (signal.SIGHUP, lambda s, f: setattr(self, '_reload', True)),
            (signal.SIGPIPE, signal.SIG_IGN),
        ):
            try:
                signal.signal(sig, action)
            except AttributeError:
                pass



    def shutdown(self, sig = None, frame = None):
        self._exiting = True
        self.stop_threads()

    def start_threads(self):

        sThread = ClienThread()
        t = threading.Thread(target = sThread.run)
        t.start()
        self._threads.append((sThread, t))



    def stop_threads(self):
        for p, t in self._threads:
            p.exit()
        for p, t in self._threads:
            t.join(timeout =  1.0)
        self._threads = []

    def run(self):
        """ main loop """
        ret = 0
        self.start_threads()
        return ret


if __name__ == '__main__':
    # set C locale
    locale.setlocale(locale.LC_ALL, 'C')
    os.environ['LANG'] = 'C'
    ret = 0
    try:
        main = Main()
        ret = main.run()
    except SystemExit:
        pass
    except KeyboardInterrupt:
        ret = 4
    except:
    sys.exit(ret)

Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit :

And about the cache of each helpers, the cost of a cache on a single helper is not much in terms of memory comparing to some network access.

Again it?s possible to test and verify this on a loaded system to get results. The delay itself can be seen from squid side in the cache manager statistics.

 

You can also try to compare the next ruby helper:

https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper

 

About a shared ?base? which allows helpers to avoid computation of the query?. It?s a good argument, however it depends what is the cost of
pulling from the cache compared to calculating the answer.

A very simple string comparison or regex matching would probably be faster than reaching a shared storage in many cases.

 

Also take into account the ?concurrency? support from the helper side.

A helper that supports parallel processing of requests/lines can do better then many single helpers in more than once use case.

In any case I would suggest to enable requests concurrency from squid side since the STDIN buffer will emulate some level of concurrency
by itself and will allow squid to keep going forward faster.

 

Just to mention that SquidGuard have used a single helper cache for a very long time, ie every single SquidGuard helper has it?s own copy of the whole

configuration and database files in memory.

 

And again, if you do have any option to implement a server service model and that the helpers will contact this main service you will be able to implement
much faster internal in-memory cache compared to a redis/memcahe/other external daemon(need to be tested).

 

A good example for this is ufdbguard which has helpers that are clients of the main service which does the whole heavy lifting and also holds 
one copy of the DB.

 

I have implemented SquidBlocker this way and have seen that it out-performs any other service I have tried until now.

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220207/68558743/attachment.htm>

From david at articatech.com  Tue Feb  8 00:36:44 2022
From: david at articatech.com (David Touzeau)
Date: Tue, 8 Feb 2022 01:36:44 +0100
Subject: [squid-users] external helper development
In-Reply-To: <001701d81c35$77894980$669bdc80$@gmail.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAABRVD74/vYJTIYjr6LWdCPVAQAAAAA=@articatech.com>
 <7df0efa5-04c0-b6e4-a408-81215f2eb8a5@measurement-factory.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAJJD97gEMN5Ch2HBNn/5W3IBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAJhQ5u2Dn5RJNJ6QPPuRMaAQAAAAA=@articatech.com>
 <000001d818c4$a3ddd330$eb997990$@gmail.com>
 <77ab15ea-65d8-df71-7b1f-0c5f411207db@articatech.com>
 <000401d8198d$5c30e120$1492a360$@gmail.com>
 <c3f005e5-0365-aaac-1532-fda5a2cc913e@articatech.com>
 <000101d81b46$d4ca2d70$7e5e8850$@gmail.com>
 <f44c5bab-cd76-dda2-96eb-88f3c7462f4e@articatech.com>
 <001701d81c35$77894980$669bdc80$@gmail.com>
Message-ID: <d9fa09d4-8a3a-10fd-601c-d7b0ce22ea2c@articatech.com>

You are the best,
We will launch a benchmark to see the diff

Le 07/02/2022 ? 16:14, Eliezer Croitoru a ?crit?:
>
> Hey David,
>
> Since the handle_stdout runs in it?s own thread it?s sole purpose is 
> to send results to stdout.
>
> If I will run the next code in a simple software without the 0.5 sleep 
> time:
>
> ???? while RUNNING:
>
> ???????? if quit > 0:
>
> ?????????? return
>
> ???????? while len(queue) > 0:
>
> ???????????? item = queue.pop(0)
>
> sys.stdout.write(item)
>
> sys.stdout.flush()
>
> ???????? time.sleep(0.5)
>
> what will happen is that the software will run with 100% CPU looping 
> over and over on the size of the queue
> while sometimes it will spit some data to stdout.
>
> Adding a small delay with 0.5 secs will allow some ?idle? time for the 
> cpu in the loop preventing it from consuming
> all the CPU time.
>
> It?s a very old technique and there are others which are more 
> efficient but it?s enough to demonstrate that a simple
> threaded helper is much better then any PHP code that was not meant to 
> be running as a STDIN/OUT daemon/helper software.
>
> All The Bests,
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> *From:*David Touzeau <david at articatech.com>
> *Sent:* Monday, February 7, 2022 02:42
> *To:* Eliezer Croitoru <ngtech1ltd at gmail.com>; 
> squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] external helper development
>
> Sorry Elizer
>
> It was a mistake... No, your code is clean..
> Impressive for the first shot
> Many thanks for your example, we will run our stress tool to see the 
> difference...
>
> Just a question
>
> Why did you send 500 milliseconds of sleep in the handle_stdoud ? Is 
> it for let squid closing the pipe ?
>
>
> Le 06/02/2022 ? 11:46, Eliezer Croitoru a ?crit?:
>
>     Hey David,
>
>     Not a fully completed helper but it seems to works pretty nice and
>     might be better then what exist already:
>
>     https://gist.githubusercontent.com/elico/03938e3a796c53f7c925872bade78195/raw/21ff1bbc0cf3d91719db27d9d027652e8bd3de4e/threaded-helper-example.py
>
>     #!/usr/bin/env python
>
>     import sys
>
>     import time
>
>     import urllib.request
>
>     import signal
>
>     import threading
>
>     #set debug mode for True or False
>
>     debug = False
>
>     #debug = True
>
>     queue = []
>
>     threads = []
>
>     RUNNING = True
>
>     quit = 0
>
>     rand_api_url = "https://cloud1.ngtech.co.il/api/test.php"
>     <https://cloud1.ngtech.co.il/api/test.php>
>
>     def sig_handler(signum, frame):
>
>     sys.stderr.write("Signal is received:" + str(signum) + "\n")
>
>     ??? global quit
>
>     ??? quit = 1
>
>     ??? global RUNNING
>
>     ??? RUNNING=False
>
>     def handle_line(line):
>
>     ???? if not RUNNING:
>
>     ???????? return
>
>     ???? if not line:
>
>     ???????? return
>
>     ???? if quit > 0:
>
>     ???????? return
>
>     ???? arr = line.split()
>
>     ???? response = urllib.request.urlopen( rand_api_url )
>
>     ???? response_text = response.read()
>
>     ???? queue.append(arr[0] + " " + response_text.decode("utf-8"))
>
>     def handle_stdout(n):
>
>     ???? while RUNNING:
>
>     ???????? if quit > 0:
>
>     ?????????? return
>
>     ???????? while len(queue) > 0:
>
>     ???????????? item = queue.pop(0)
>
>     sys.stdout.write(item)
>
>     sys.stdout.flush()
>
>     ???????? time.sleep(0.5)
>
>     def handle_stdin(n):
>
>     ??? while RUNNING:
>
>     ???????? line = sys.stdin.readline()
>
>     ???????? if not line:
>
>     ???????????? break
>
>     ???????? if quit > 0:
>
>     ???????????? break
>
>     ???????? line = line.strip()
>
>     ???????? thread = threading.Thread(target=handle_line, args=(line,))
>
>     ???????? thread.start()
>
>     threads.append(thread)
>
>     signal.signal(signal.SIGUSR1, sig_handler)
>
>     signal.signal(signal.SIGUSR2, sig_handler)
>
>     signal.signal(signal.SIGALRM, sig_handler)
>
>     signal.signal(signal.SIGINT, sig_handler)
>
>     signal.signal(signal.SIGQUIT, sig_handler)
>
>     signal.signal(signal.SIGTERM, sig_handler)
>
>     stdout_thread = threading.Thread(target=handle_stdout, args=(1,))
>
>     stdout_thread.start()
>
>     threads.append(stdout_thread)
>
>     stdin_thread = threading.Thread(target=handle_stdin, args=(2,))
>
>     stdin_thread.start()
>
>     threads.append(stdin_thread)
>
>     while(RUNNING):
>
>     ??? time.sleep(3)
>
>     print("Not RUNNING")
>
>     for thread in threads:
>
>     ??? thread.join()
>
>     print("All threads stopped.")
>
>     ## END
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     NgTech, Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com
>
>     *From:*squid-users <squid-users-bounces at lists.squid-cache.org>
>     <mailto:squid-users-bounces at lists.squid-cache.org> *On Behalf Of
>     *David Touzeau
>     *Sent:* Friday, February 4, 2022 16:29
>     *To:* squid-users at lists.squid-cache.org
>     *Subject:* Re: [squid-users] external helper development
>
>     Elizer,
>
>     Thanks for all this advice and indeed your arguments are valid
>     between opening a socket, sending data, receiving data and closing
>     the socket unlike direct access to a regex or a memory entry even
>     if the calculation has already been done.
>
>     But what surprises me the most is that we have produced a python
>     plugin in thread which I provide you a code below.
>     The php code is like your mentioned example ( No thread, just a
>     loop and output OK )
>
>     Results are after 6k requests, squid freeze and no surf can be
>     made as with PHP code we can up to 10K requests and squid is happy
>     really, we did not understand why python is so low.
>
>     Here a python code using threads
>
>     #!/usr/bin/env python
>     import os
>     import sys
>     import time
>     import signal
>     import locale
>     import traceback
>     import threading
>     import select
>     import traceback as tb
>
>     class ClienThread():
>
>     ??? def __init__(self):
>     ??????? self._exiting = False
>     ??????? self._cache = {}
>
>     ??? def exit(self):
>     ??????? self._exiting = True
>
>     ??? def stdout(self, lineToSend):
>     ??????? try:
>     ??????????? sys.stdout.write(lineToSend)
>     ??????????? sys.stdout.flush()
>
>     ??????? except IOError as e:
>     ??????????? if e.errno==32:
>     ??????????????? # Error Broken PIPE!"
>     ??????????????? pass
>     ??????? except:
>     ??????????? # other execpt
>     ??????????? pass
>
>     ??? def run(self):
>     ??????? while not self._exiting:
>     ??????????? if sys.stdin in select.select([sys.stdin], [], [],
>     0.5)[0]:
>     ??????????????? line = sys.stdin.readline()
>     ??????????????? LenOfline=len(line)
>
>     ??????????????? if LenOfline==0:
>     ??????????????????? self._exiting=True
>     ??????????????????? break
>
>     ??????????????? if line[-1] == '\n':line = line[:-1]
>     ??????????????? channel = None
>     ??????????????? options = line.split()
>
>     ??????????????? try:
>     ??????????????????? if options[0].isdigit(): channel = options.pop(0)
>     ??????????????? except IndexError:
>     ??????????????????? self.stdout("0 OK first=ERROR\n")
>     ??????????????????? continue
>
>     ??????????????? # Processing here
>
>     ??????????????? try:
>     ??????????????????? self.stdout("%s OK\n" % channel)
>     ??????????????? except:
>     ??????????????????? self.stdout("%s ERROR first=ERROR\n" % channel)
>
>
>
>
>     class Main(object):
>     ??? def __init__(self):
>     ??????? self._threads = []
>     ??????? self._exiting = False
>     ??????? self._reload = False
>     ??????? self._config = ""
>
>     ??????? for sig, action in (
>     ??????????? (signal.SIGINT, self.shutdown),
>     ??????????? (signal.SIGQUIT, self.shutdown),
>     ??????????? (signal.SIGTERM, self.shutdown),
>     ??????????? (signal.SIGHUP, lambda s, f: setattr(self, '_reload',
>     True)),
>     ??????????? (signal.SIGPIPE, signal.SIG_IGN),
>     ??????? ):
>     ??????????? try:
>     ??????????????? signal.signal(sig, action)
>     ??????????? except AttributeError:
>     ??????????????? pass
>
>
>
>     ??? def shutdown(self, sig = None, frame = None):
>     ??????? self._exiting = True
>     ??????? self.stop_threads()
>
>     ??? def start_threads(self):
>
>     ??????? sThread = ClienThread()
>     ??????? t = threading.Thread(target = sThread.run)
>     ??????? t.start()
>     ??????? self._threads.append((sThread, t))
>
>
>
>     ??? def stop_threads(self):
>     ??????? for p, t in self._threads:
>     ??????????? p.exit()
>     ??????? for p, t in self._threads:
>     ??????????? t.join(timeout =? 1.0)
>     ??????? self._threads = []
>
>     ??? def run(self):
>     ??????? """ main loop """
>     ??????? ret = 0
>     ??????? self.start_threads()
>     ??????? return ret
>
>
>     if __name__ == '__main__':
>     ??? # set C locale
>     ??? locale.setlocale(locale.LC_ALL, 'C')
>     ??? os.environ['LANG'] = 'C'
>     ??? ret = 0
>     ??? try:
>     ??????? main = Main()
>     ??????? ret = main.run()
>     ??? except SystemExit:
>     ??????? pass
>     ??? except KeyboardInterrupt:
>     ??????? ret = 4
>     ??? except:
>     ??? sys.exit(ret)
>
>     Le 04/02/2022 ? 07:06, Eliezer Croitoru a ?crit?:
>
>         And about the cache of each helpers, the cost of a cache on a
>         single helper is not much in terms of memory comparing to some
>         network access.
>
>         Again it?s possible to test and verify this on a loaded system
>         to get results. The delay itself can be seen from squid side
>         in the cache manager statistics.
>
>         You can also try to compare the next ruby helper:
>
>         https://wiki.squid-cache.org/EliezerCroitoru/SessionHelper
>
>         About a shared ?base? which allows helpers to avoid
>         computation of the query?. It?s a good argument, however it
>         depends what is the cost of
>         pulling from the cache compared to calculating the answer.
>
>         A very simple string comparison or regex matching would
>         probably be faster than reaching a shared storage in many cases.
>
>         Also take into account the ?concurrency? support from the
>         helper side.
>
>         A helper that supports parallel processing of requests/lines
>         can do better then many single helpers in more than once use case.
>
>         In any case I would suggest to enable requests concurrency
>         from squid side since the STDIN buffer will emulate some level
>         of concurrency
>         by itself and will allow squid to keep going forward faster.
>
>         Just to mention that SquidGuard have used a single helper
>         cache for a very long time, ie every single SquidGuard helper
>         has it?s own copy of the whole
>
>         configuration and database files in memory.
>
>         And again, if you do have any option to implement a server
>         service model and that the helpers will contact this main
>         service you will be able to implement
>         much faster internal in-memory cache compared to a
>         redis/memcahe/other external daemon(need to be tested).
>
>         A good example for this is ufdbguard which has helpers that
>         are clients of the main service which does the whole heavy
>         lifting and also holds
>         one copy of the DB.
>
>         I have implemented SquidBlocker this way and have seen that it
>         out-performs any other service I have tried until now.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220208/5541f8d5/attachment.htm>

From roeeklinger60 at gmail.com  Tue Feb  8 14:13:46 2022
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 8 Feb 2022 16:13:46 +0200
Subject: [squid-users] External helper consumes too many DB connections
Message-ID: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>

Hello,

I am running multiple instances of Squid in a K8S environment, each Squid
instance has a helper that authenticates users based on their username and
password, the scripts are written in Python.

I have been facing an issue, that when under load, the helpers (even with
3600 sec TTL) swamp the MariaDB instance, causing it to reach 100% CPU,
basically I believe because each helper opens up its own connection to
MariaDB, which ends up as a lot of connections.

My initial idea was to create a Redis DB next to each Squid instance and
connect each Squid to its own dedicated Redis. I will sync Redis with
MariaDB every minute, thus decreasing the connections count from a few 100s
to just 1 every minute. This will also improve speeds since Redis is much
faster than MariaDB.

The problem is, however, that there will still be many connections from
Squid to Redis, and I probably that will consume a lot of DB resources as
well, which I don't actually know how to optimize, since it seems that
Squid opens many processes, and there is no way to get them to talk to each
other (expect TTL values, which seems not to help in my case, which I also
don't understand why that is).

What is the best practice to handle this? considering I have the following
requirements:

1. Fast
2. Refresh data every minute
3. Consume as least amount of DB resources as possible

Thanks,
Roee
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220208/ab7b4b57/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb  8 14:41:57 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Feb 2022 09:41:57 -0500
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
Message-ID: <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>

On 2/8/22 09:13, roee klinger wrote:

> I am running multiple instances of Squid in a K8S environment, each 
> Squid instance has a helper?that authenticates users based on their 
> username and password, the scripts are written in Python.
> 
> I have been facing an issue, that when under load, the helpers (even 
> with 3600 sec TTL) swamp the MariaDB instance, causing?it to reach 100% 
> CPU, basically?I believe because each helper opens up its own connection 
> to MariaDB, which ends up as a lot of connections.
> 
> My initial idea was to create a Redis DB next to each Squid instance and 
> connect each Squid to its own dedicated Redis. I will sync Redis with 
> MariaDB every minute, thus decreasing the connections count from a few 
> 100s to just 1 every minute. This will also improve speeds since Redis 
> is much faster than MariaDB.
> 
> The problem is, however, that there will still be many connections from 
> Squid to Redis, and I probably that will consume a lot of DB resources 
> as well, which I don't actually know how to optimize, since it seems 
> that Squid opens many processes, and there is no way to get them to talk 
> to each other (expect TTL values, which seems not to help in my case, 
> which I also don't understand why that is).
> 
> What is the best practice to handle this? considering I have the 
> following requirements:
> 
>     1. Fast
>     2. Refresh data every minute
>     3. Consume as least amount of DB resources as possible

I would start from the beginning: Does the aggregate number of database 
requests match your expectations? In other words, do you see lots of 
database requests that should not be there given your user access 
patterns and authentication TTLs? In yet other words, are there many 
repeated authentication accesses that should have been authentication 
cache hits?

If there are a lot more requests than your users/TTLs should generate, 
then you may be able to decrease db load by figuring out where the extra 
requests are coming from. For example, it is possible that your 
authentication cache key includes some noise that renders caching 
ineffective (e.g., see comments about key_extras in 
squid.conf.documented). Or maybe you need a bigger authentication cache.

If the total stream of authentication requests during peak hours is 
reasonable, with few unwarranted cache misses, then you can start 
working on aggregating helper-db connections (helpers can be written to 
talk through a central connection aggregator) and/or adding database 
power (e.g., by introducing additional databases running on previously 
unused hardware -- just like your MariaDB idea).


Cheers,

Alex.


From roeeklinger60 at gmail.com  Tue Feb  8 14:50:11 2022
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 8 Feb 2022 16:50:11 +0200
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
 <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
Message-ID: <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>

Hey Alex,

If there are a lot more requests than your users/TTLs should generate,
> then you may be able to decrease db load by figuring out where the extra
> requests are coming from.


actually, I don't think it matters much now that I think about it again,
since as per my requirements,
I need to reload the cache every 60 seconds, which means that even if it is
perfect, MariaDB will still
get a high load. I think the second approach will be better suited.

and/or adding database
> power (e.g., by introducing additional databases running on previously
> unused hardware -- just like your MariaDB idea).


That is an excellent point,  I think I will work on a central connection
aggregator as you suggested,
and also put in more DB power via Redis on K8S. This way it will best fast
and scale automatically.

aggregating helper-db connections (helpers can be written to
> talk through a central connection aggregator)


 That sounds like exactly what I am looking for, how would one go about
doing this?

On Tue, Feb 8, 2022 at 4:41 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2/8/22 09:13, roee klinger wrote:
>
> > I am running multiple instances of Squid in a K8S environment, each
> > Squid instance has a helper that authenticates users based on their
> > username and password, the scripts are written in Python.
> >
> > I have been facing an issue, that when under load, the helpers (even
> > with 3600 sec TTL) swamp the MariaDB instance, causing it to reach 100%
> > CPU, basically I believe because each helper opens up its own connection
> > to MariaDB, which ends up as a lot of connections.
> >
> > My initial idea was to create a Redis DB next to each Squid instance and
> > connect each Squid to its own dedicated Redis. I will sync Redis with
> > MariaDB every minute, thus decreasing the connections count from a few
> > 100s to just 1 every minute. This will also improve speeds since Redis
> > is much faster than MariaDB.
> >
> > The problem is, however, that there will still be many connections from
> > Squid to Redis, and I probably that will consume a lot of DB resources
> > as well, which I don't actually know how to optimize, since it seems
> > that Squid opens many processes, and there is no way to get them to talk
> > to each other (expect TTL values, which seems not to help in my case,
> > which I also don't understand why that is).
> >
> > What is the best practice to handle this? considering I have the
> > following requirements:
> >
> >     1. Fast
> >     2. Refresh data every minute
> >     3. Consume as least amount of DB resources as possible
>
> I would start from the beginning: Does the aggregate number of database
> requests match your expectations? In other words, do you see lots of
> database requests that should not be there given your user access
> patterns and authentication TTLs? In yet other words, are there many
> repeated authentication accesses that should have been authentication
> cache hits?
>
> If there are a lot more requests than your users/TTLs should generate,
> then you may be able to decrease db load by figuring out where the extra
> requests are coming from. For example, it is possible that your
> authentication cache key includes some noise that renders caching
> ineffective (e.g., see comments about key_extras in
> squid.conf.documented). Or maybe you need a bigger authentication cache.
>
> If the total stream of authentication requests during peak hours is
> reasonable, with few unwarranted cache misses, then you can start
> working on aggregating helper-db connections (helpers can be written to
> talk through a central connection aggregator) and/or adding database
> power (e.g., by introducing additional databases running on previously
> unused hardware -- just like your MariaDB idea).
>
>
> Cheers,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220208/752d265d/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb  8 15:12:23 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Feb 2022 10:12:23 -0500
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
 <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
 <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>
Message-ID: <6a78a89e-2897-065c-43a8-e22ac2f341e5@measurement-factory.com>

On 2/8/22 09:50, roee klinger wrote:

> Alex: If there are a lot more requests than your users/TTLs should
>       generate, then you may be able to decrease db load by figuring out
>       where the extra requests are coming from.

> actually, I don't think it matters much now that I think about it
> again, since as per my requirements, I need to reload the cache every
> 60 seconds, which means that even if it is perfect, MariaDB will
> still get a high load. I think the second approach will be better
> suited.

Your call. Wiping out the entire authentication cache every 60 seconds 
feels odd, but I do not know enough about your environment to judge.


> Alex: aggregating helper-db connections (helpers can be written to
>       talk through a central connection aggregator)
> 

> That sounds like exactly what I am looking for, how would one go about 
> doing this?

You have at least two basic options:

A. Enhance Squid to let SMP workers share helpers. I assume that you 
have C SMP workers and N helpers per worker, with C and N significantly 
greater than 1. Instead of having N helpers per worker and C*N helpers 
total, you will have just one concurrent helper per worker and C helpers 
total. This will be a significant, generally useful improvement that 
should be officially accepted if implemented well. This enhancement 
requires serious Squid code modifications in a neglected error-prone 
area, but it is certainly doable -- Squid already shares rock diskers 
across workers, for example.

B. Convert your helper from a database client program to an Aggregator 
client program (and write the Aggregator). Depending on your needs and 
skill, you can use TCP or Unix Domain Sockets (UDS) for 
helper-Aggregator communication. The Aggregator may look very similar to 
the current helper, except it will not use stdin/stdout for 
receiving/sending helper queries/responses. This option also requires 
development, but it is much simpler than option A.


HTH,

Alex.


> On Tue, Feb 8, 2022 at 4:41 PM Alex Rousskov wrote:
> 
>     On 2/8/22 09:13, roee klinger wrote:
> 
>      > I am running multiple instances of Squid in a K8S environment, each
>      > Squid instance has a helper?that authenticates users based on their
>      > username and password, the scripts are written in Python.
>      >
>      > I have been facing an issue, that when under load, the helpers (even
>      > with 3600 sec TTL) swamp the MariaDB instance, causing?it to
>     reach 100%
>      > CPU, basically?I believe because each helper opens up its own
>     connection
>      > to MariaDB, which ends up as a lot of connections.
>      >
>      > My initial idea was to create a Redis DB next to each Squid
>     instance and
>      > connect each Squid to its own dedicated Redis. I will sync Redis
>     with
>      > MariaDB every minute, thus decreasing the connections count from
>     a few
>      > 100s to just 1 every minute. This will also improve speeds since
>     Redis
>      > is much faster than MariaDB.
>      >
>      > The problem is, however, that there will still be many
>     connections from
>      > Squid to Redis, and I probably that will consume a lot of DB
>     resources
>      > as well, which I don't actually know how to optimize, since it seems
>      > that Squid opens many processes, and there is no way to get them
>     to talk
>      > to each other (expect TTL values, which seems not to help in my
>     case,
>      > which I also don't understand why that is).
>      >
>      > What is the best practice to handle this? considering I have the
>      > following requirements:
>      >
>      >? ? ?1. Fast
>      >? ? ?2. Refresh data every minute
>      >? ? ?3. Consume as least amount of DB resources as possible
> 
>     I would start from the beginning: Does the aggregate number of database
>     requests match your expectations? In other words, do you see lots of
>     database requests that should not be there given your user access
>     patterns and authentication TTLs? In yet other words, are there many
>     repeated authentication accesses that should have been authentication
>     cache hits?
> 
>     If there are a lot more requests than your users/TTLs should generate,
>     then you may be able to decrease db load by figuring out where the
>     extra
>     requests are coming from. For example, it is possible that your
>     authentication cache key includes some noise that renders caching
>     ineffective (e.g., see comments about key_extras in
>     squid.conf.documented). Or maybe you need a bigger authentication cache.
> 
>     If the total stream of authentication requests during peak hours is
>     reasonable, with few unwarranted cache misses, then you can start
>     working on aggregating helper-db connections (helpers can be written to
>     talk through a central connection aggregator) and/or adding database
>     power (e.g., by introducing additional databases running on previously
>     unused hardware -- just like your MariaDB idea).
> 
> 
>     Cheers,
> 
>     Alex.
> 



From roeeklinger60 at gmail.com  Tue Feb  8 16:08:33 2022
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 8 Feb 2022 18:08:33 +0200
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <6a78a89e-2897-065c-43a8-e22ac2f341e5@measurement-factory.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
 <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
 <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>
 <6a78a89e-2897-065c-43a8-e22ac2f341e5@measurement-factory.com>
Message-ID: <CAGCa14oeahSp4sboS=BSwFYij-1UseToz6HeXJEm1tszVs-Rtw@mail.gmail.com>

>
> You have at least two basic options:


A. Enhance Squid to let SMP workers share helpers. I assume that you
> have C SMP workers and N helpers per worker, with C and N significantly
> greater than 1. Instead of having N helpers per worker and C*N helpers
> total, you will have just one concurrent helper per worker and C helpers
> total. This will be a significant, generally useful improvement that
> should be officially accepted if implemented well. This enhancement
> requires serious Squid code modifications in a neglected error-prone
> area, but it is certainly doable -- Squid already shares rock diskers
> across workers, for example.


B. Convert your helper from a database client program to an Aggregator
> client program (and write the Aggregator). Depending on your needs and
> skill, you can use TCP or Unix Domain Sockets (UDS) for
> helper-Aggregator communication. The Aggregator may look very similar to
> the current helper, except it will not use stdin/stdout for
> receiving/sending helper queries/responses. This option also requires
> development, but it is much simpler than option A.


Thank you, Alex, I will keep these in mind.

I thought about the following approach:

1. Have only one python helper, this helper fetches the data every minute
from the main DB.
2. This helper has concurrency set for it.
3. The helper then spawns child processes using multithreading, each
process responds to std/stdout and reads the data from the main process
which spawned it.

What do you think about taking this route?

It will require no extra DBs and no tweaks to Squid, but maybe I am missing
something,

Best regards,
Roee

On Tue, Feb 8, 2022 at 5:12 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2/8/22 09:50, roee klinger wrote:
>
> > Alex: If there are a lot more requests than your users/TTLs should
> >       generate, then you may be able to decrease db load by figuring out
> >       where the extra requests are coming from.
>
> > actually, I don't think it matters much now that I think about it
> > again, since as per my requirements, I need to reload the cache every
> > 60 seconds, which means that even if it is perfect, MariaDB will
> > still get a high load. I think the second approach will be better
> > suited.
>
> Your call. Wiping out the entire authentication cache every 60 seconds
> feels odd, but I do not know enough about your environment to judge.
>
>
> > Alex: aggregating helper-db connections (helpers can be written to
> >       talk through a central connection aggregator)
> >
>
> > That sounds like exactly what I am looking for, how would one go about
> > doing this?
>
> You have at least two basic options:
>
> A. Enhance Squid to let SMP workers share helpers. I assume that you
> have C SMP workers and N helpers per worker, with C and N significantly
> greater than 1. Instead of having N helpers per worker and C*N helpers
> total, you will have just one concurrent helper per worker and C helpers
> total. This will be a significant, generally useful improvement that
> should be officially accepted if implemented well. This enhancement
> requires serious Squid code modifications in a neglected error-prone
> area, but it is certainly doable -- Squid already shares rock diskers
> across workers, for example.
>
> B. Convert your helper from a database client program to an Aggregator
> client program (and write the Aggregator). Depending on your needs and
> skill, you can use TCP or Unix Domain Sockets (UDS) for
> helper-Aggregator communication. The Aggregator may look very similar to
> the current helper, except it will not use stdin/stdout for
> receiving/sending helper queries/responses. This option also requires
> development, but it is much simpler than option A.
>
>
> HTH,
>
> Alex.
>
>
> > On Tue, Feb 8, 2022 at 4:41 PM Alex Rousskov wrote:
> >
> >     On 2/8/22 09:13, roee klinger wrote:
> >
> >      > I am running multiple instances of Squid in a K8S environment,
> each
> >      > Squid instance has a helper that authenticates users based on
> their
> >      > username and password, the scripts are written in Python.
> >      >
> >      > I have been facing an issue, that when under load, the helpers
> (even
> >      > with 3600 sec TTL) swamp the MariaDB instance, causing it to
> >     reach 100%
> >      > CPU, basically I believe because each helper opens up its own
> >     connection
> >      > to MariaDB, which ends up as a lot of connections.
> >      >
> >      > My initial idea was to create a Redis DB next to each Squid
> >     instance and
> >      > connect each Squid to its own dedicated Redis. I will sync Redis
> >     with
> >      > MariaDB every minute, thus decreasing the connections count from
> >     a few
> >      > 100s to just 1 every minute. This will also improve speeds since
> >     Redis
> >      > is much faster than MariaDB.
> >      >
> >      > The problem is, however, that there will still be many
> >     connections from
> >      > Squid to Redis, and I probably that will consume a lot of DB
> >     resources
> >      > as well, which I don't actually know how to optimize, since it
> seems
> >      > that Squid opens many processes, and there is no way to get them
> >     to talk
> >      > to each other (expect TTL values, which seems not to help in my
> >     case,
> >      > which I also don't understand why that is).
> >      >
> >      > What is the best practice to handle this? considering I have the
> >      > following requirements:
> >      >
> >      >     1. Fast
> >      >     2. Refresh data every minute
> >      >     3. Consume as least amount of DB resources as possible
> >
> >     I would start from the beginning: Does the aggregate number of
> database
> >     requests match your expectations? In other words, do you see lots of
> >     database requests that should not be there given your user access
> >     patterns and authentication TTLs? In yet other words, are there many
> >     repeated authentication accesses that should have been authentication
> >     cache hits?
> >
> >     If there are a lot more requests than your users/TTLs should
> generate,
> >     then you may be able to decrease db load by figuring out where the
> >     extra
> >     requests are coming from. For example, it is possible that your
> >     authentication cache key includes some noise that renders caching
> >     ineffective (e.g., see comments about key_extras in
> >     squid.conf.documented). Or maybe you need a bigger authentication
> cache.
> >
> >     If the total stream of authentication requests during peak hours is
> >     reasonable, with few unwarranted cache misses, then you can start
> >     working on aggregating helper-db connections (helpers can be written
> to
> >     talk through a central connection aggregator) and/or adding database
> >     power (e.g., by introducing additional databases running on
> previously
> >     unused hardware -- just like your MariaDB idea).
> >
> >
> >     Cheers,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220208/ff590a25/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb  8 16:38:25 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Feb 2022 11:38:25 -0500
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <CAGCa14oeahSp4sboS=BSwFYij-1UseToz6HeXJEm1tszVs-Rtw@mail.gmail.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
 <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
 <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>
 <6a78a89e-2897-065c-43a8-e22ac2f341e5@measurement-factory.com>
 <CAGCa14oeahSp4sboS=BSwFYij-1UseToz6HeXJEm1tszVs-Rtw@mail.gmail.com>
Message-ID: <f042a505-e273-4cce-a6f9-538413518651@measurement-factory.com>

On 2/8/22 11:08, roee klinger wrote:

> I thought about the following approach:
> 
> 1. Have only one python helper, this helper fetches the data every 
> minute from the main DB.
> 2. This helper has concurrency?set for it.
> 3. The helper then spawns child processes?using multithreading, each 
> process?responds to std/stdout and reads the data from the main process 
> which spawned it.
> 
> What do you think about taking this route?
> 
> It will require no extra DBs and no tweaks to Squid, but maybe I am 
> missing something

With this approach (let's call it C), you will have as many database 
clients as there are workers in your Squid instance, just like in option 
A. Option C is probably a lot easier to implement for a given helper 
than the generic option A. Option B gives you one database client per 
Squid instance.

It is not clear to me why C parallelizes reading/writing from/to 
stdin/stdout -- I doubt that task is the bottleneck in your environment. 
I would expect a single stdin reader thread and a single stdout writer 
thread instead.

This is not my area of expertise, but if you do go option C route, you 
may need to protect helper's stdin/stdout descriptors with a mutex so 
that threads can read/write from/to stdin/stdout without getting 
mangled/partial reads and mangled/overlapping writes.

Alex.


> On Tue, Feb 8, 2022 at 5:12 PM Alex Rousskov  wrote:
> 
>     On 2/8/22 09:50, roee klinger wrote:
> 
>      > Alex: If there are a lot more requests than your users/TTLs should
>      >? ? ? ?generate, then you may be able to decrease db load by
>     figuring out
>      >? ? ? ?where the extra requests are coming from.
> 
>      > actually, I don't think it matters much now that I think about it
>      > again, since as per my requirements, I need to reload the cache every
>      > 60 seconds, which means that even if it is perfect, MariaDB will
>      > still get a high load. I think the second approach will be better
>      > suited.
> 
>     Your call. Wiping out the entire authentication cache every 60 seconds
>     feels odd, but I do not know enough about your environment to judge.
> 
> 
>      > Alex: aggregating helper-db connections (helpers can be written to
>      >? ? ? ?talk through a central connection aggregator)
>      >
> 
>      > That sounds like exactly what I am looking for, how would one go
>     about
>      > doing this?
> 
>     You have at least two basic options:
> 
>     A. Enhance Squid to let SMP workers share helpers. I assume that you
>     have C SMP workers and N helpers per worker, with C and N significantly
>     greater than 1. Instead of having N helpers per worker and C*N helpers
>     total, you will have just one concurrent helper per worker and C
>     helpers
>     total. This will be a significant, generally useful improvement that
>     should be officially accepted if implemented well. This enhancement
>     requires serious Squid code modifications in a neglected error-prone
>     area, but it is certainly doable -- Squid already shares rock diskers
>     across workers, for example.
> 
>     B. Convert your helper from a database client program to an Aggregator
>     client program (and write the Aggregator). Depending on your needs and
>     skill, you can use TCP or Unix Domain Sockets (UDS) for
>     helper-Aggregator communication. The Aggregator may look very
>     similar to
>     the current helper, except it will not use stdin/stdout for
>     receiving/sending helper queries/responses. This option also requires
>     development, but it is much simpler than option A.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>      > On Tue, Feb 8, 2022 at 4:41 PM Alex Rousskov wrote:
>      >
>      >? ? ?On 2/8/22 09:13, roee klinger wrote:
>      >
>      >? ? ? > I am running multiple instances of Squid in a K8S
>     environment, each
>      >? ? ? > Squid instance has a helper?that authenticates users based
>     on their
>      >? ? ? > username and password, the scripts are written in Python.
>      >? ? ? >
>      >? ? ? > I have been facing an issue, that when under load, the
>     helpers (even
>      >? ? ? > with 3600 sec TTL) swamp the MariaDB instance, causing?it to
>      >? ? ?reach 100%
>      >? ? ? > CPU, basically?I believe because each helper opens up its own
>      >? ? ?connection
>      >? ? ? > to MariaDB, which ends up as a lot of connections.
>      >? ? ? >
>      >? ? ? > My initial idea was to create a Redis DB next to each Squid
>      >? ? ?instance and
>      >? ? ? > connect each Squid to its own dedicated Redis. I will sync
>     Redis
>      >? ? ?with
>      >? ? ? > MariaDB every minute, thus decreasing the connections
>     count from
>      >? ? ?a few
>      >? ? ? > 100s to just 1 every minute. This will also improve speeds
>     since
>      >? ? ?Redis
>      >? ? ? > is much faster than MariaDB.
>      >? ? ? >
>      >? ? ? > The problem is, however, that there will still be many
>      >? ? ?connections from
>      >? ? ? > Squid to Redis, and I probably that will consume a lot of DB
>      >? ? ?resources
>      >? ? ? > as well, which I don't actually know how to optimize,
>     since it seems
>      >? ? ? > that Squid opens many processes, and there is no way to
>     get them
>      >? ? ?to talk
>      >? ? ? > to each other (expect TTL values, which seems not to help
>     in my
>      >? ? ?case,
>      >? ? ? > which I also don't understand why that is).
>      >? ? ? >
>      >? ? ? > What is the best practice to handle this? considering I
>     have the
>      >? ? ? > following requirements:
>      >? ? ? >
>      >? ? ? >? ? ?1. Fast
>      >? ? ? >? ? ?2. Refresh data every minute
>      >? ? ? >? ? ?3. Consume as least amount of DB resources as possible
>      >
>      >? ? ?I would start from the beginning: Does the aggregate number
>     of database
>      >? ? ?requests match your expectations? In other words, do you see
>     lots of
>      >? ? ?database requests that should not be there given your user access
>      >? ? ?patterns and authentication TTLs? In yet other words, are
>     there many
>      >? ? ?repeated authentication accesses that should have been
>     authentication
>      >? ? ?cache hits?
>      >
>      >? ? ?If there are a lot more requests than your users/TTLs should
>     generate,
>      >? ? ?then you may be able to decrease db load by figuring out
>     where the
>      >? ? ?extra
>      >? ? ?requests are coming from. For example, it is possible that your
>      >? ? ?authentication cache key includes some noise that renders caching
>      >? ? ?ineffective (e.g., see comments about key_extras in
>      >? ? ?squid.conf.documented). Or maybe you need a bigger
>     authentication cache.
>      >
>      >? ? ?If the total stream of authentication requests during peak
>     hours is
>      >? ? ?reasonable, with few unwarranted cache misses, then you can start
>      >? ? ?working on aggregating helper-db connections (helpers can be
>     written to
>      >? ? ?talk through a central connection aggregator) and/or adding
>     database
>      >? ? ?power (e.g., by introducing additional databases running on
>     previously
>      >? ? ?unused hardware -- just like your MariaDB idea).
>      >
>      >
>      >? ? ?Cheers,
>      >
>      >? ? ?Alex.
>      >
> 



From roeeklinger60 at gmail.com  Tue Feb  8 17:02:57 2022
From: roeeklinger60 at gmail.com (roee klinger)
Date: Tue, 8 Feb 2022 19:02:57 +0200
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <f042a505-e273-4cce-a6f9-538413518651@measurement-factory.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
 <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
 <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>
 <6a78a89e-2897-065c-43a8-e22ac2f341e5@measurement-factory.com>
 <CAGCa14oeahSp4sboS=BSwFYij-1UseToz6HeXJEm1tszVs-Rtw@mail.gmail.com>
 <f042a505-e273-4cce-a6f9-538413518651@measurement-factory.com>
Message-ID: <CAGCa14oH6EYESvZiw5mnM62SVF=SEHMiUr-=G5b5OGAPUQYauw@mail.gmail.com>

Hey Alex,

Thank you for your reply, I am sorry, I think I explained myself wrong.

What I meant by option C, is to have basically 3 functions, 2 functions for
std/stdout, and one function
that will fetch the data from the DB every 60 seconds, and save it into a
global variable for the other functions
to use.

Then, when a new std request comes in, the std handler will simply read
from that variable, instead of from the DB.

I see the following benefits in this approach:

1. We will have only one DB connection every 60 seconds, per Squid worker
instance.
2. It will be very fast since the std handler will simply read from a local
variable.


you will have as many database
> clients as there are workers in your Squid instance


You are definitely right, but as this will be much faster I think I will be
able to decrease my number of workers significantly.
Also, we might be able to use concurrency=n here to decrease it further?

Would love to hear your thoughts on this,
Roee

On Tue, Feb 8, 2022 at 6:38 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2/8/22 11:08, roee klinger wrote:
>
> > I thought about the following approach:
> >
> > 1. Have only one python helper, this helper fetches the data every
> > minute from the main DB.
> > 2. This helper has concurrency set for it.
> > 3. The helper then spawns child processes using multithreading, each
> > process responds to std/stdout and reads the data from the main process
> > which spawned it.
> >
> > What do you think about taking this route?
> >
> > It will require no extra DBs and no tweaks to Squid, but maybe I am
> > missing something
>
> With this approach (let's call it C), you will have as many database
> clients as there are workers in your Squid instance, just like in option
> A. Option C is probably a lot easier to implement for a given helper
> than the generic option A. Option B gives you one database client per
> Squid instance.
>
> It is not clear to me why C parallelizes reading/writing from/to
> stdin/stdout -- I doubt that task is the bottleneck in your environment.
> I would expect a single stdin reader thread and a single stdout writer
> thread instead.
>
> This is not my area of expertise, but if you do go option C route, you
> may need to protect helper's stdin/stdout descriptors with a mutex so
> that threads can read/write from/to stdin/stdout without getting
> mangled/partial reads and mangled/overlapping writes.
>
> Alex.
>
>
> > On Tue, Feb 8, 2022 at 5:12 PM Alex Rousskov  wrote:
> >
> >     On 2/8/22 09:50, roee klinger wrote:
> >
> >      > Alex: If there are a lot more requests than your users/TTLs should
> >      >       generate, then you may be able to decrease db load by
> >     figuring out
> >      >       where the extra requests are coming from.
> >
> >      > actually, I don't think it matters much now that I think about it
> >      > again, since as per my requirements, I need to reload the cache
> every
> >      > 60 seconds, which means that even if it is perfect, MariaDB will
> >      > still get a high load. I think the second approach will be better
> >      > suited.
> >
> >     Your call. Wiping out the entire authentication cache every 60
> seconds
> >     feels odd, but I do not know enough about your environment to judge.
> >
> >
> >      > Alex: aggregating helper-db connections (helpers can be written to
> >      >       talk through a central connection aggregator)
> >      >
> >
> >      > That sounds like exactly what I am looking for, how would one go
> >     about
> >      > doing this?
> >
> >     You have at least two basic options:
> >
> >     A. Enhance Squid to let SMP workers share helpers. I assume that you
> >     have C SMP workers and N helpers per worker, with C and N
> significantly
> >     greater than 1. Instead of having N helpers per worker and C*N
> helpers
> >     total, you will have just one concurrent helper per worker and C
> >     helpers
> >     total. This will be a significant, generally useful improvement that
> >     should be officially accepted if implemented well. This enhancement
> >     requires serious Squid code modifications in a neglected error-prone
> >     area, but it is certainly doable -- Squid already shares rock diskers
> >     across workers, for example.
> >
> >     B. Convert your helper from a database client program to an
> Aggregator
> >     client program (and write the Aggregator). Depending on your needs
> and
> >     skill, you can use TCP or Unix Domain Sockets (UDS) for
> >     helper-Aggregator communication. The Aggregator may look very
> >     similar to
> >     the current helper, except it will not use stdin/stdout for
> >     receiving/sending helper queries/responses. This option also requires
> >     development, but it is much simpler than option A.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >      > On Tue, Feb 8, 2022 at 4:41 PM Alex Rousskov wrote:
> >      >
> >      >     On 2/8/22 09:13, roee klinger wrote:
> >      >
> >      >      > I am running multiple instances of Squid in a K8S
> >     environment, each
> >      >      > Squid instance has a helper that authenticates users based
> >     on their
> >      >      > username and password, the scripts are written in Python.
> >      >      >
> >      >      > I have been facing an issue, that when under load, the
> >     helpers (even
> >      >      > with 3600 sec TTL) swamp the MariaDB instance, causing it
> to
> >      >     reach 100%
> >      >      > CPU, basically I believe because each helper opens up its
> own
> >      >     connection
> >      >      > to MariaDB, which ends up as a lot of connections.
> >      >      >
> >      >      > My initial idea was to create a Redis DB next to each Squid
> >      >     instance and
> >      >      > connect each Squid to its own dedicated Redis. I will sync
> >     Redis
> >      >     with
> >      >      > MariaDB every minute, thus decreasing the connections
> >     count from
> >      >     a few
> >      >      > 100s to just 1 every minute. This will also improve speeds
> >     since
> >      >     Redis
> >      >      > is much faster than MariaDB.
> >      >      >
> >      >      > The problem is, however, that there will still be many
> >      >     connections from
> >      >      > Squid to Redis, and I probably that will consume a lot of
> DB
> >      >     resources
> >      >      > as well, which I don't actually know how to optimize,
> >     since it seems
> >      >      > that Squid opens many processes, and there is no way to
> >     get them
> >      >     to talk
> >      >      > to each other (expect TTL values, which seems not to help
> >     in my
> >      >     case,
> >      >      > which I also don't understand why that is).
> >      >      >
> >      >      > What is the best practice to handle this? considering I
> >     have the
> >      >      > following requirements:
> >      >      >
> >      >      >     1. Fast
> >      >      >     2. Refresh data every minute
> >      >      >     3. Consume as least amount of DB resources as possible
> >      >
> >      >     I would start from the beginning: Does the aggregate number
> >     of database
> >      >     requests match your expectations? In other words, do you see
> >     lots of
> >      >     database requests that should not be there given your user
> access
> >      >     patterns and authentication TTLs? In yet other words, are
> >     there many
> >      >     repeated authentication accesses that should have been
> >     authentication
> >      >     cache hits?
> >      >
> >      >     If there are a lot more requests than your users/TTLs should
> >     generate,
> >      >     then you may be able to decrease db load by figuring out
> >     where the
> >      >     extra
> >      >     requests are coming from. For example, it is possible that
> your
> >      >     authentication cache key includes some noise that renders
> caching
> >      >     ineffective (e.g., see comments about key_extras in
> >      >     squid.conf.documented). Or maybe you need a bigger
> >     authentication cache.
> >      >
> >      >     If the total stream of authentication requests during peak
> >     hours is
> >      >     reasonable, with few unwarranted cache misses, then you can
> start
> >      >     working on aggregating helper-db connections (helpers can be
> >     written to
> >      >     talk through a central connection aggregator) and/or adding
> >     database
> >      >     power (e.g., by introducing additional databases running on
> >     previously
> >      >     unused hardware -- just like your MariaDB idea).
> >      >
> >      >
> >      >     Cheers,
> >      >
> >      >     Alex.
> >      >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220208/1bdfd8a1/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb  8 17:42:41 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 8 Feb 2022 12:42:41 -0500
Subject: [squid-users] External helper consumes too many DB connections
In-Reply-To: <CAGCa14oH6EYESvZiw5mnM62SVF=SEHMiUr-=G5b5OGAPUQYauw@mail.gmail.com>
References: <CAGCa14oKUT2V4f9OiVuJyYHZuxdau1TL+kU3frnvr+Xd3REB2w@mail.gmail.com>
 <586d74b2-6a84-85ca-7ac9-482ed6722d00@measurement-factory.com>
 <CAGCa14rfrpSwDM-Ny8aANNg2EUr0DLu6cEy72PMyU22Pe53dqg@mail.gmail.com>
 <6a78a89e-2897-065c-43a8-e22ac2f341e5@measurement-factory.com>
 <CAGCa14oeahSp4sboS=BSwFYij-1UseToz6HeXJEm1tszVs-Rtw@mail.gmail.com>
 <f042a505-e273-4cce-a6f9-538413518651@measurement-factory.com>
 <CAGCa14oH6EYESvZiw5mnM62SVF=SEHMiUr-=G5b5OGAPUQYauw@mail.gmail.com>
Message-ID: <9fc43243-0eda-c106-0054-021b9b436dcf@measurement-factory.com>

On 2/8/22 12:02, roee klinger wrote:

> What I meant by option C, is to have basically 3 functions, 2 functions 
> for std/stdout, and one function
> that will fetch the data from the DB every 60 seconds, and save it into 
> a global variable for the other functions to use.

Ah, thank you for that clarification. It addresses most of my earlier 
concerns. Ideally, you want four functions AFAICT:

1. stdin reader
2. stdout writer
3. answer generator
4. db updater

I agree that you can join 1+3 (or 2+3) together, but you are sacrificing 
some parallelism if you join. Whether that sacrifice is important or 
not, depends on various local factors.


> Then, when a new std request comes in, the std handler?will simply read 
> from that variable, instead of from the DB.

Yes, and pass the answer to the stdout writing thread/function.


> I see the following benefits?in this approach:
> 
>     1. We will have only?one DB connection every 60 seconds, per Squid
>     worker instance.

Yes, one per Squid worker. Hopefully, 60 seconds (divided by the number 
of workers if the database cannot parallelize these "give me everything" 
queries) will be enough to receive (the relevant portion of) the database.


>     2. It will be very fast since the std handler will simply read from
>     a local variable.

Yes, assuming the query is simple and/or the database is small.


>     you will have as many database
>     clients as there are workers in your Squid instance

> You are definitely?right, but as this will be much faster I think I will 
> be able to decrease my number of workers significantly.

Whether you can decrease the number of Squid workers depends on where 
the bottlenecks are. If your Squid workers are mostly idle now, then 
yes, you will be able to decrease their number (but you can do that even 
without helper rewrites then AFAICT).


> Also, we might be able to use concurrency=n here to decrease?it further?

Yes, probably. Even with just one helper thread/function answering 
helper queries, giving Squid the ability to submit the next query 
without waiting for the answer to the previous one will parallelize I/O 
across the helper/Squid boundary, which is a good thing.


Cheers,

Alex.


> On Tue, Feb 8, 2022 at 6:38 PM Alex Rousskov wrote:
> 
>     On 2/8/22 11:08, roee klinger wrote:
> 
>      > I thought about the following approach:
>      >
>      > 1. Have only one python helper, this helper fetches the data every
>      > minute from the main DB.
>      > 2. This helper has concurrency?set for it.
>      > 3. The helper then spawns child processes?using multithreading, each
>      > process?responds to std/stdout and reads the data from the main
>     process
>      > which spawned it.
>      >
>      > What do you think about taking this route?
>      >
>      > It will require no extra DBs and no tweaks to Squid, but maybe I am
>      > missing something
> 
>     With this approach (let's call it C), you will have as many database
>     clients as there are workers in your Squid instance, just like in
>     option
>     A. Option C is probably a lot easier to implement for a given helper
>     than the generic option A. Option B gives you one database client per
>     Squid instance.
> 
>     It is not clear to me why C parallelizes reading/writing from/to
>     stdin/stdout -- I doubt that task is the bottleneck in your
>     environment.
>     I would expect a single stdin reader thread and a single stdout writer
>     thread instead.
> 
>     This is not my area of expertise, but if you do go option C route, you
>     may need to protect helper's stdin/stdout descriptors with a mutex so
>     that threads can read/write from/to stdin/stdout without getting
>     mangled/partial reads and mangled/overlapping writes.
> 
>     Alex.
> 
> 
>      > On Tue, Feb 8, 2022 at 5:12 PM Alex Rousskov? wrote:
>      >
>      >? ? ?On 2/8/22 09:50, roee klinger wrote:
>      >
>      >? ? ? > Alex: If there are a lot more requests than your
>     users/TTLs should
>      >? ? ? >? ? ? ?generate, then you may be able to decrease db load by
>      >? ? ?figuring out
>      >? ? ? >? ? ? ?where the extra requests are coming from.
>      >
>      >? ? ? > actually, I don't think it matters much now that I think
>     about it
>      >? ? ? > again, since as per my requirements, I need to reload the
>     cache every
>      >? ? ? > 60 seconds, which means that even if it is perfect,
>     MariaDB will
>      >? ? ? > still get a high load. I think the second approach will be
>     better
>      >? ? ? > suited.
>      >
>      >? ? ?Your call. Wiping out the entire authentication cache every
>     60 seconds
>      >? ? ?feels odd, but I do not know enough about your environment to
>     judge.
>      >
>      >
>      >? ? ? > Alex: aggregating helper-db connections (helpers can be
>     written to
>      >? ? ? >? ? ? ?talk through a central connection aggregator)
>      >? ? ? >
>      >
>      >? ? ? > That sounds like exactly what I am looking for, how would
>     one go
>      >? ? ?about
>      >? ? ? > doing this?
>      >
>      >? ? ?You have at least two basic options:
>      >
>      >? ? ?A. Enhance Squid to let SMP workers share helpers. I assume
>     that you
>      >? ? ?have C SMP workers and N helpers per worker, with C and N
>     significantly
>      >? ? ?greater than 1. Instead of having N helpers per worker and
>     C*N helpers
>      >? ? ?total, you will have just one concurrent helper per worker and C
>      >? ? ?helpers
>      >? ? ?total. This will be a significant, generally useful
>     improvement that
>      >? ? ?should be officially accepted if implemented well. This
>     enhancement
>      >? ? ?requires serious Squid code modifications in a neglected
>     error-prone
>      >? ? ?area, but it is certainly doable -- Squid already shares rock
>     diskers
>      >? ? ?across workers, for example.
>      >
>      >? ? ?B. Convert your helper from a database client program to an
>     Aggregator
>      >? ? ?client program (and write the Aggregator). Depending on your
>     needs and
>      >? ? ?skill, you can use TCP or Unix Domain Sockets (UDS) for
>      >? ? ?helper-Aggregator communication. The Aggregator may look very
>      >? ? ?similar to
>      >? ? ?the current helper, except it will not use stdin/stdout for
>      >? ? ?receiving/sending helper queries/responses. This option also
>     requires
>      >? ? ?development, but it is much simpler than option A.
>      >
>      >
>      >? ? ?HTH,
>      >
>      >? ? ?Alex.
>      >
>      >
>      >? ? ? > On Tue, Feb 8, 2022 at 4:41 PM Alex Rousskov wrote:
>      >? ? ? >
>      >? ? ? >? ? ?On 2/8/22 09:13, roee klinger wrote:
>      >? ? ? >
>      >? ? ? >? ? ? > I am running multiple instances of Squid in a K8S
>      >? ? ?environment, each
>      >? ? ? >? ? ? > Squid instance has a helper?that authenticates
>     users based
>      >? ? ?on their
>      >? ? ? >? ? ? > username and password, the scripts are written in
>     Python.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > I have been facing an issue, that when under load, the
>      >? ? ?helpers (even
>      >? ? ? >? ? ? > with 3600 sec TTL) swamp the MariaDB instance,
>     causing?it to
>      >? ? ? >? ? ?reach 100%
>      >? ? ? >? ? ? > CPU, basically?I believe because each helper opens
>     up its own
>      >? ? ? >? ? ?connection
>      >? ? ? >? ? ? > to MariaDB, which ends up as a lot of connections.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > My initial idea was to create a Redis DB next to
>     each Squid
>      >? ? ? >? ? ?instance and
>      >? ? ? >? ? ? > connect each Squid to its own dedicated Redis. I
>     will sync
>      >? ? ?Redis
>      >? ? ? >? ? ?with
>      >? ? ? >? ? ? > MariaDB every minute, thus decreasing the connections
>      >? ? ?count from
>      >? ? ? >? ? ?a few
>      >? ? ? >? ? ? > 100s to just 1 every minute. This will also improve
>     speeds
>      >? ? ?since
>      >? ? ? >? ? ?Redis
>      >? ? ? >? ? ? > is much faster than MariaDB.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > The problem is, however, that there will still be many
>      >? ? ? >? ? ?connections from
>      >? ? ? >? ? ? > Squid to Redis, and I probably that will consume a
>     lot of DB
>      >? ? ? >? ? ?resources
>      >? ? ? >? ? ? > as well, which I don't actually know how to optimize,
>      >? ? ?since it seems
>      >? ? ? >? ? ? > that Squid opens many processes, and there is no way to
>      >? ? ?get them
>      >? ? ? >? ? ?to talk
>      >? ? ? >? ? ? > to each other (expect TTL values, which seems not
>     to help
>      >? ? ?in my
>      >? ? ? >? ? ?case,
>      >? ? ? >? ? ? > which I also don't understand why that is).
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > What is the best practice to handle this? considering I
>      >? ? ?have the
>      >? ? ? >? ? ? > following requirements:
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ?1. Fast
>      >? ? ? >? ? ? >? ? ?2. Refresh data every minute
>      >? ? ? >? ? ? >? ? ?3. Consume as least amount of DB resources as
>     possible
>      >? ? ? >
>      >? ? ? >? ? ?I would start from the beginning: Does the aggregate
>     number
>      >? ? ?of database
>      >? ? ? >? ? ?requests match your expectations? In other words, do
>     you see
>      >? ? ?lots of
>      >? ? ? >? ? ?database requests that should not be there given your
>     user access
>      >? ? ? >? ? ?patterns and authentication TTLs? In yet other words, are
>      >? ? ?there many
>      >? ? ? >? ? ?repeated authentication accesses that should have been
>      >? ? ?authentication
>      >? ? ? >? ? ?cache hits?
>      >? ? ? >
>      >? ? ? >? ? ?If there are a lot more requests than your users/TTLs
>     should
>      >? ? ?generate,
>      >? ? ? >? ? ?then you may be able to decrease db load by figuring out
>      >? ? ?where the
>      >? ? ? >? ? ?extra
>      >? ? ? >? ? ?requests are coming from. For example, it is possible
>     that your
>      >? ? ? >? ? ?authentication cache key includes some noise that
>     renders caching
>      >? ? ? >? ? ?ineffective (e.g., see comments about key_extras in
>      >? ? ? >? ? ?squid.conf.documented). Or maybe you need a bigger
>      >? ? ?authentication cache.
>      >? ? ? >
>      >? ? ? >? ? ?If the total stream of authentication requests during peak
>      >? ? ?hours is
>      >? ? ? >? ? ?reasonable, with few unwarranted cache misses, then
>     you can start
>      >? ? ? >? ? ?working on aggregating helper-db connections (helpers
>     can be
>      >? ? ?written to
>      >? ? ? >? ? ?talk through a central connection aggregator) and/or
>     adding
>      >? ? ?database
>      >? ? ? >? ? ?power (e.g., by introducing additional databases
>     running on
>      >? ? ?previously
>      >? ? ? >? ? ?unused hardware -- just like your MariaDB idea).
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ?Cheers,
>      >? ? ? >
>      >? ? ? >? ? ?Alex.
>      >? ? ? >
>      >
> 



From squid3 at treenet.co.nz  Wed Feb  9 08:53:29 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 Feb 2022 21:53:29 +1300
Subject: [squid-users] [squid-announce] Squid 5.4 is available
Message-ID: <1b07af21-8283-c8db-0005-bc067b939e9a@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the
availability of the Squid-5.4 release!


This release is a bug fix release resolving several issues
found in the prior Squid-5 releases.


The major changes to be aware of:

  * Bug 5190: Preserve configured order of intermediate CA
    certificate chain

  Previous Squid-5 releases inverted the CA certificate chain order
  when delivering the server handshake. Breaking clients which are
  unable to reorder the chain. This release once again conforms with
  TLS specification requirements.


  * Bug 5187: Properly track (and mark) truncated store entries

  Squid used an error-prone approach to identifying truncated responses:
  The response is treated as whole unless somebody remembers to mark
  it as truncated. This dangerous default naturally resulted in bugs
  where truncated responses are treated as complete under various
  conditions.

  This change reverses that approach: Responses not explicitly marked as
  whole are treated as truncated. This change affects all Squid-server
  FwdState-dispatched communications: HTTP, FTP, Gopher, and WHOIS. It
  also affects responses received from the adaptation services.

  Transactions that failed due to origin server or peer timeout (a common
  source of truncation) are now logged with a _TIMEOUT %Ss suffix and
  ERR_READ_TIMEOUT/WITH_SRV %err_code/%err_detail.

  Transactions prematurely canceled by Squid during client-Squid
  communication (usually due to various timeouts) now have WITH_CLT
  default %err_detail. This detail helps distinguish otherwise
  similarly-logged problems that may happen when talking to the client or
  to the origin server/peer.


  * Bug 5134: assertion failed: Transients.cc:221: "old == e"

  This bug appears when caching is enabled and a worker dies and
  is automatically restarted. The SMP cache management was missing
  some necessary cross-checks on hash collision before updating
  stored objects. The worker recovery logic detected the hash collision
  better and would abort with the given error.


  * Bug 5132: Close the tunnel if to-server conn closes after client

  This bug has been present since 5.0.4 and shows up as a growing number
  of open (aka "hung") TCP connections used by Squid regardless of client
  traffic levels.

  It can be expected to affect on all HTTPS traffic, and proxy using
  SSL-Bump features. With the problem being worse the more CONNECT
  tunnels are handled.


  * Bug 5188: Fix reconfiguration leaking tls-cert=... memory

  This bug was found investigating other issues. Installations which
  are reconfiguring often may have been seeing sub-optimal memory
  usage. It has otherwise a minimal impact.



   All users of Squid-5 are encouraged to upgrade as soon as
   possible.


See the ChangeLog for the full list of changes in this and
earlier releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

   http://www.squid-cache.org/Versions/v5/
   ftp://ftp.squid-cache.org/pub/squid/
   ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

   http://www.squid-cache.org/Download/http-mirrors.html
   http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug
report.
   https://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From david at articatech.com  Wed Feb  9 12:43:30 2022
From: david at articatech.com (David Touzeau)
Date: Wed, 9 Feb 2022 13:43:30 +0100
Subject: [squid-users] Squid plugin sponsor
Message-ID: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>

Hi

I would like to sponsor the improvement of ntlm_fake_auth to support new 
protocols or go further produce a new negotiate_kerberos_auth_fake

Who should start the challenge?

regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220209/c3bff2f6/attachment.htm>

From ngtech1ltd at gmail.com  Wed Feb  9 15:09:55 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Wed, 9 Feb 2022 17:09:55 +0200
Subject: [squid-users] [squid-announce] Squid 5.4 is available
In-Reply-To: <1b07af21-8283-c8db-0005-bc067b939e9a@treenet.co.nz>
References: <1b07af21-8283-c8db-0005-bc067b939e9a@treenet.co.nz>
Message-ID: <000f01d81dc7$19823250$4c8696f0$@gmail.com>

Hey All,

I have just published the latest 5.4 RPMS for:
* Oracle Linux 7+8
* CentOS Linux 7+8
* Amazon Linux 2

All the above includes my latest patch that allows intercepted connections
to be passed towards the destination host
in cases which the DNS resolution comes from another DNS which is not shared
between the clients and the proxy. (8.8.8.8,1.1.1.1 etc)

The next patch series has been used on 5.4-1:
https://gist.github.com/elico/eb0f4e99331af5c23a8f5999f405d37b

And the next patch was used on 4.17-8
https://gist.github.com/elico/630fa57d161b0c0b59ef68786d801589

All The Bests,
Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-announce <squid-announce-bounces at lists.squid-cache.org> On
Behalf Of Amos Jeffries
Sent: Wednesday, February 9, 2022 10:53
To: squid-announce at lists.squid-cache.org
Subject: [squid-announce] Squid 5.4 is available

The Squid HTTP Proxy team is very pleased to announce the
availability of the Squid-5.4 release!


This release is a bug fix release resolving several issues
found in the prior Squid-5 releases.


The major changes to be aware of:

  * Bug 5190: Preserve configured order of intermediate CA
    certificate chain

  Previous Squid-5 releases inverted the CA certificate chain order
  when delivering the server handshake. Breaking clients which are
  unable to reorder the chain. This release once again conforms with
  TLS specification requirements.


  * Bug 5187: Properly track (and mark) truncated store entries

  Squid used an error-prone approach to identifying truncated responses:
  The response is treated as whole unless somebody remembers to mark
  it as truncated. This dangerous default naturally resulted in bugs
  where truncated responses are treated as complete under various
  conditions.

  This change reverses that approach: Responses not explicitly marked as
  whole are treated as truncated. This change affects all Squid-server
  FwdState-dispatched communications: HTTP, FTP, Gopher, and WHOIS. It
  also affects responses received from the adaptation services.

  Transactions that failed due to origin server or peer timeout (a common
  source of truncation) are now logged with a _TIMEOUT %Ss suffix and
  ERR_READ_TIMEOUT/WITH_SRV %err_code/%err_detail.

  Transactions prematurely canceled by Squid during client-Squid
  communication (usually due to various timeouts) now have WITH_CLT
  default %err_detail. This detail helps distinguish otherwise
  similarly-logged problems that may happen when talking to the client or
  to the origin server/peer.


  * Bug 5134: assertion failed: Transients.cc:221: "old == e"

  This bug appears when caching is enabled and a worker dies and
  is automatically restarted. The SMP cache management was missing
  some necessary cross-checks on hash collision before updating
  stored objects. The worker recovery logic detected the hash collision
  better and would abort with the given error.


  * Bug 5132: Close the tunnel if to-server conn closes after client

  This bug has been present since 5.0.4 and shows up as a growing number
  of open (aka "hung") TCP connections used by Squid regardless of client
  traffic levels.

  It can be expected to affect on all HTTPS traffic, and proxy using
  SSL-Bump features. With the problem being worse the more CONNECT
  tunnels are handled.


  * Bug 5188: Fix reconfiguration leaking tls-cert=... memory

  This bug was found investigating other issues. Installations which
  are reconfiguring often may have been seeing sub-optimal memory
  usage. It has otherwise a minimal impact.



   All users of Squid-5 are encouraged to upgrade as soon as
   possible.


See the ChangeLog for the full list of changes in this and
earlier releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v5/RELEASENOTES.html
when you are ready to make the switch to Squid-5

This new release can be downloaded from our HTTP or FTP servers

   http://www.squid-cache.org/Versions/v5/
   ftp://ftp.squid-cache.org/pub/squid/
   ftp://ftp.squid-cache.org/pub/archive/5/

or the mirrors. For a list of mirror sites see

   http://www.squid-cache.org/Download/http-mirrors.html
   http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug
report.
   https://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce



From numsys at free.fr  Wed Feb  9 16:40:41 2022
From: numsys at free.fr (FredB)
Date: Wed, 09 Feb 2022 17:40:41 +0100
Subject: [squid-users] [squid-announce] Squid 5.4 is available
In-Reply-To: <000f01d81dc7$19823250$4c8696f0$@gmail.com>
References: <1b07af21-8283-c8db-0005-bc067b939e9a@treenet.co.nz>
 <000f01d81dc7$19823250$4c8696f0$@gmail.com>
Message-ID: <0192ED0B-3CCF-4309-81B7-C645E85D5333@free.fr>

Hello All

Here docker image builds, automatic at each official release

Amd64 and Arm (64 bits os only, tested on raspberry v3,v4)

https://hub.docker.com/r/fredbcode/squid

Fred
-- 
Envoy? de mon appareil Android avec Courriel K-9 Mail. Veuillez excuser ma bri?vet?.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220209/31d65b04/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb 10 04:03:53 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 Feb 2022 17:03:53 +1300
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
Message-ID: <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>

On 10/02/22 01:43, David Touzeau wrote:
> Hi
> 
> I would like to sponsor the improvement of ntlm_fake_auth to support new 
> protocols

ntlm_* helpers are specific to NTLM authentication. All LanManager (LM) 
protocols should already be supported as well as currently possible. 
NTLM is formally discontinued by MS and *very* inefficient.

NP: NTLMv2 with encryption does not *work* because that encryption step 
requires secret keys the proxy is not able to know.

> or go further produce a new negotiate_kerberos_auth_fake
> 

With current Squid this helper only needs to produce an "OK" response 
regardless of the input. The basic_auth_fake does that.

Amos


From david at articatech.com  Thu Feb 10 08:32:04 2022
From: david at articatech.com (David Touzeau)
Date: Thu, 10 Feb 2022 09:32:04 +0100
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
Message-ID: <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>

Hi

What we are looking for is to retrieve a "user" token without having to 
ask anything from the user.
That's why we're looking at Active Directory credentials.
Once the user account is retrieved, a helper would be in charge of 
checking if the user exists in the LDAP database.
This is to avoid any connection to an Active Directory
Maybe this is impossible


Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit?:
> On 10/02/22 01:43, David Touzeau wrote:
>> Hi
>>
>> I would like to sponsor the improvement of ntlm_fake_auth to support 
>> new protocols
>
> ntlm_* helpers are specific to NTLM authentication. All LanManager 
> (LM) protocols should already be supported as well as currently 
> possible. NTLM is formally discontinued by MS and *very* inefficient.
>
> NP: NTLMv2 with encryption does not *work* because that encryption 
> step requires secret keys the proxy is not able to know.
>
>> or go further produce a new negotiate_kerberos_auth_fake
>>
>
> With current Squid this helper only needs to produce an "OK" response 
> regardless of the input. The basic_auth_fake does that.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220210/c1ae796f/attachment.htm>

From ngtech1ltd at gmail.com  Thu Feb 10 18:23:17 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 10 Feb 2022 20:23:17 +0200
Subject: [squid-users] [squid-announce] Squid 5.4 is available
In-Reply-To: <0192ED0B-3CCF-4309-81B7-C645E85D5333@free.fr>
References: <1b07af21-8283-c8db-0005-bc067b939e9a@treenet.co.nz>
 <000f01d81dc7$19823250$4c8696f0$@gmail.com>
 <0192ED0B-3CCF-4309-81B7-C645E85D5333@free.fr>
Message-ID: <000001d81eab$47584e00$d608ea00$@gmail.com>

Thanks Fred,

 

What is this image general purpose?

In what environment can it be used?

I have seen that the docker-compose contains three containers:

*	Squid
*	e2guardian
*	other

 

If you do have couple minutes to elaborate more about this specific use case it would be helpful.

I am working on a setup that will run Squid In linux ip namespaces.

I have seen it can work wonders to separate the interception part of the system from the actual OS.

In Palo Alto and couple other products they have the MANAGMNT Interface and ?plane? usually named ?Control Plane?

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: FredB <numsys at free.fr> 
Sent: Wednesday, February 9, 2022 18:41
To: squid-users at lists.squid-cache.org; Eliezer Croitoru <ngtech1ltd at gmail.com>
Subject: Re: [squid-users] [squid-announce] Squid 5.4 is available

 

Hello All

Here docker image builds, automatic at each official release

Amd64 and Arm (64 bits os only, tested on raspberry v3,v4)

https://hub.docker.com/r/fredbcode/squid

Fred

-- 
Envoy? de mon appareil Android avec Courriel K-9 Mail. Veuillez excuser ma bri?vet?.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220210/d62b126d/attachment.htm>

From robertkwild at gmail.com  Thu Feb 10 18:55:59 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 10 Feb 2022 18:55:59 +0000
Subject: [squid-users] Vulnerabilities with squid 4.15
Message-ID: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>

Hi all,

Is there any security vulnerabilities with squid 4.15, should I update to
4.17 or is it OK to still use as my squid proxy server

Sorry for silly question

Thanks,
Rob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220210/e870a4e4/attachment.htm>

From ngtech1ltd at gmail.com  Thu Feb 10 19:19:51 2022
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Thu, 10 Feb 2022 21:19:51 +0200
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
Message-ID: <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>

Hey Robert,

First: your question is not silly.
The answer will defer based on the complexity of the upgrade process.
What Os are you using and also, did you compiled squid from sources or
installed from a specific package?
Also, what is your squid setup purpose?

Eliezer

?????? ??? ??, 10 ????? 2022, 20:56, ??? robert k Wild ?<
robertkwild at gmail.com>:

> Hi all,
>
> Is there any security vulnerabilities with squid 4.15, should I update to
> 4.17 or is it OK to still use as my squid proxy server
>
> Sorry for silly question
>
> Thanks,
> Rob
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220210/2d0391d5/attachment.htm>

From robertkwild at gmail.com  Thu Feb 10 19:28:14 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 10 Feb 2022 19:28:14 +0000
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>
Message-ID: <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>

I have squid running on centos 7.9, I will move to ubuntu 20 04 03 as
centos is officially dead to me

I have compiled from source ie make make install as I'm running squid with
squidclamav cicap cicap modules

All instances I have compiled from source ie make make install

I did a yum install clamav

On Thu, 10 Feb 2022, 19:20 NgTech LTD, <ngtech1ltd at gmail.com> wrote:

> Hey Robert,
>
> First: your question is not silly.
> The answer will defer based on the complexity of the upgrade process.
> What Os are you using and also, did you compiled squid from sources or
> installed from a specific package?
> Also, what is your squid setup purpose?
>
> Eliezer
>
> ?????? ??? ??, 10 ????? 2022, 20:56, ??? robert k Wild ?<
> robertkwild at gmail.com>:
>
>> Hi all,
>>
>> Is there any security vulnerabilities with squid 4.15, should I update to
>> 4.17 or is it OK to still use as my squid proxy server
>>
>> Sorry for silly question
>>
>> Thanks,
>> Rob
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220210/fb2f0c8f/attachment.htm>

From squid3 at treenet.co.nz  Fri Feb 11 02:24:34 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 Feb 2022 15:24:34 +1300
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
Message-ID: <8b7984a6-d997-3fa0-3010-23638cac9550@treenet.co.nz>

On 11/02/22 07:55, robert k Wild wrote:
> Hi all,
> 
> Is there any security vulnerabilities with squid 4.15, should I update 
> to 4.17 or is it OK to still use as my squid proxy server
> 
> Sorry for silly question
> 

Not silly.

There is this one for WCCP:
<https://github.com/squid-cache/squid/security/advisories/GHSA-rgf3-9v3p-qp82>

However, be aware that the patch has been found to prevent all traffic 
from some routers. We are working on the fix for that.


Amos


From squid.org at bloms.de  Fri Feb 11 04:55:39 2022
From: squid.org at bloms.de (Dieter Bloms)
Date: Fri, 11 Feb 2022 05:55:39 +0100
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
Message-ID: <20220211045539.3q4xzr6bqbunhaxc@bloms.de>

Hello David,

for me it looks like you want to use kerberos authentication.
With kerberos authentication the user don't have to authenticate against
the proxy. The authentication is done in the background.

Mayb this link will help:

https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos

On Thu, Feb 10, David Touzeau wrote:

> Hi
> 
> What we are looking for is to retrieve a "user" token without having to ask
> anything from the user.
> That's why we're looking at Active Directory credentials.
> Once the user account is retrieved, a helper would be in charge of checking
> if the user exists in the LDAP database.
> This is to avoid any connection to an Active Directory
> Maybe this is impossible
> 
> 
> Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit?:
> > On 10/02/22 01:43, David Touzeau wrote:
> > > Hi
> > > 
> > > I would like to sponsor the improvement of ntlm_fake_auth to support
> > > new protocols
> > 
> > ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
> > protocols should already be supported as well as currently possible.
> > NTLM is formally discontinued by MS and *very* inefficient.
> > 
> > NP: NTLMv2 with encryption does not *work* because that encryption step
> > requires secret keys the proxy is not able to know.
> > 
> > > or go further produce a new negotiate_kerberos_auth_fake
> > > 
> > 
> > With current Squid this helper only needs to produce an "OK" response
> > regardless of the input. The basic_auth_fake does that.
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users

> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
From field.


From ngtech1ltd at gmail.com  Fri Feb 11 05:23:50 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 11 Feb 2022 07:23:50 +0200
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>
 <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>
Message-ID: <000001d81f07$8e24fb10$aa6ef130$@gmail.com>

Hey Robert,

 

Don?t rush with the move from CentOS 7 to Ubuntu yet, CentOS 7 has good support for at-least a year from now.

I can try to help you by providing RPMs that has support for ecap which I understand you need.

Alternatively I can try to build an upgrade process for your self compiled version.

 

I can recommend on both:

*	Amazon Linux 2
*	Oracle Enterprise Linux 8\7
*	Open Suse

 

As a general alternative which I can support the RPM builds for.

I have also built binaries for Ubuntu and Debian but in a non deb package file but will be signed by me.

 

As Amos mentioned the current issue is with WCCP based setups.

Can you please elaborate more if you are using WCCP in your setup?

Also, Are you using SSL-BUMP by any chance? (I really don?t know about a setup that doesn?t require this these days)

 

If you would be able to share more information on your setup so I might be able to clone such a setup it will help a lot.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: robert k Wild <robertkwild at gmail.com> 
Sent: Thursday, February 10, 2022 21:28
To: NgTech LTD <ngtech1ltd at gmail.com>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Vulnerabilities with squid 4.15

 

I have squid running on centos 7.9, I will move to ubuntu 20 04 03 as centos is officially dead to me

 

I have compiled from source ie make make install as I'm running squid with squidclamav cicap cicap modules 

 

All instances I have compiled from source ie make make install

 

I did a yum install clamav 

 

On Thu, 10 Feb 2022, 19:20 NgTech LTD, <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > wrote:

Hey Robert,

 

First: your question is not silly.

The answer will defer based on the complexity of the upgrade process.

What Os are you using and also, did you compiled squid from sources or installed from a specific package?

Also, what is your squid setup purpose?

 

Eliezer 

 

?????? ??? ??, 10 ????? 2022, 20:56, ??? robert k Wild ?<robertkwild at gmail.com <mailto:robertkwild at gmail.com> >:

Hi all,

 

Is there any security vulnerabilities with squid 4.15, should I update to 4.17 or is it OK to still use as my squid proxy server 

 

Sorry for silly question

 

Thanks, 

Rob

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220211/8ca08c5d/attachment.htm>

From ngtech1ltd at gmail.com  Fri Feb 11 05:35:02 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 11 Feb 2022 07:35:02 +0200
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
 <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
Message-ID: <000501d81f09$1ec9ad90$5c5d08b0$@gmail.com>

Hey Dieter,

I have tried to use the mentioned wiki document to try and re-create a LAB
with AD 2012-2019.
I got stuck with a setup that is not usable in the terms of transparent
authentication.
I have tried on the next OS:
* Debian 10/11
* Ubuntu 18.04/20.04
* CentOS 7/8
* Oracle Enterprise Linux 7/8

I would be happy to try and re-create the lab here and to make sure that
there will be a well documented configuration guide.
If there is a good tutorial or guide I would be happy to try and verify if
it works in my lab.

Thanks,
Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Dieter Bloms
Sent: Friday, February 11, 2022 06:56
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid plugin sponsor

Hello David,

for me it looks like you want to use kerberos authentication.
With kerberos authentication the user don't have to authenticate against
the proxy. The authentication is done in the background.

Mayb this link will help:

https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos

On Thu, Feb 10, David Touzeau wrote:

> Hi
> 
> What we are looking for is to retrieve a "user" token without having to
ask
> anything from the user.
> That's why we're looking at Active Directory credentials.
> Once the user account is retrieved, a helper would be in charge of
checking
> if the user exists in the LDAP database.
> This is to avoid any connection to an Active Directory
> Maybe this is impossible
> 
> 
> Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit?:
> > On 10/02/22 01:43, David Touzeau wrote:
> > > Hi
> > > 
> > > I would like to sponsor the improvement of ntlm_fake_auth to support
> > > new protocols
> > 
> > ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
> > protocols should already be supported as well as currently possible.
> > NTLM is formally discontinued by MS and *very* inefficient.
> > 
> > NP: NTLMv2 with encryption does not *work* because that encryption step
> > requires secret keys the proxy is not able to know.
> > 
> > > or go further produce a new negotiate_kerberos_auth_fake
> > > 
> > 
> > With current Squid this helper only needs to produce an "OK" response
> > regardless of the input. The basic_auth_fake does that.
> > 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users

> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-- 
Gru?

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From numsys at free.fr  Fri Feb 11 08:30:54 2022
From: numsys at free.fr (FredB)
Date: Fri, 11 Feb 2022 09:30:54 +0100
Subject: [squid-users] [squid-announce] Squid 5.4 is available
In-Reply-To: <000001d81eab$47584e00$d608ea00$@gmail.com>
References: <1b07af21-8283-c8db-0005-bc067b939e9a@treenet.co.nz>
 <000f01d81dc7$19823250$4c8696f0$@gmail.com>
 <0192ED0B-3CCF-4309-81B7-C645E85D5333@free.fr>
 <000001d81eab$47584e00$d608ea00$@gmail.com>
Message-ID: <b5503e87-1adb-fdd8-dd28-e4d095076e55@free.fr>

Hi,

> What is this image general purpose?
>
Have a containerized Squid, easy to install and upgrade, and In my case 
use multi proxies on same machine

Enabled options, here: 
https://gitlab.com/fredbcode-images/squid/-/blob/master/Dockerfile#L8

Squid is automatically compiled, tested (I will add more tests soon) and 
finally released as image every weeks

When a test fail, there is no new release.

I'm already using this process for e2guardian, a pipeline runs every 
time a commit is merged:

You can click on each state to see the process:
https://gitlab.com/fredbcode/e2guardian/-/pipelines/463682244
Example Debian compilation: 
https://gitlab.com/fredbcode/e2guardian/-/jobs/2055075483

Packages, docker images, are generated when nothing is wrong -> In this 
situation I'm testing the web filtering with e2guardian and SSL MITM 
mode enabled

> In what environment can it be used?
>
Any 64 bits with docker (I think it could works also on windows, not 
sure), but only for x86 and ARM v8 architectures

> I have seen that the docker-compose contains three containers:
>
>   * Squid
>   * e2guardian
>   * other
>
It's just a basic example for a simple web filtering machine in icap 
mode, works in progress ...
When I have more time, hum, I will add a load balancer (traefik, ha 
proxy, ?) for an out of box little platform with squid multi instances
I also added some options to my image like supgethosts: - squid stop 
when it can't reach Internet, useful for multi machines and load 
balancer (or proxy pac) - autoreload: - If a file is 
changed/deleted/created squid reloads automatically -/
/

Personally I'm using many squid on each machine for better performance, 
especially with ssl bump

But of course scalability, dead and live of process are using a more 
complex mechanism that my simple example

In _my case_ with same hardware the performance has increased 
significantly, I used a single squid by machine before.
Also better than some proprietary products that I had tried.

Fred
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220211/118d9945/attachment.htm>

From robertkwild at gmail.com  Fri Feb 11 10:04:37 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 11 Feb 2022 10:04:37 +0000
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <000001d81f07$8e24fb10$aa6ef130$@gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>
 <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>
 <000001d81f07$8e24fb10$aa6ef130$@gmail.com>
Message-ID: <CAGU_CiJzUdiVKeLmY3f821DD9+ff-K-O5bfO61uBePr690hL=Q@mail.gmail.com>

thanks Amos and Eliezer!

tbh i dont know if im using WCCP with my squid version, sorry, how do i
find that out?

i am using SSL Bump ie SSL interception and a few websites im doing No ssl
intercept with splice/peek/bump

hope that helps

On Fri, 11 Feb 2022 at 05:24, Eliezer Croitoru <ngtech1ltd at gmail.com> wrote:

> Hey Robert,
>
>
>
> Don?t rush with the move from CentOS 7 to Ubuntu yet, CentOS 7 has good
> support for at-least a year from now.
>
> I can try to help you by providing RPMs that has support for ecap which I
> understand you need.
>
> Alternatively I can try to build an upgrade process for your self compiled
> version.
>
>
>
> I can recommend on both:
>
>    - Amazon Linux 2
>    - Oracle Enterprise Linux 8\7
>    - Open Suse
>
>
>
> As a general alternative which I can support the RPM builds for.
>
> I have also built binaries for Ubuntu and Debian but in a non deb package
> file but will be signed by me.
>
>
>
> As Amos mentioned the current issue is with WCCP based setups.
>
> Can you please elaborate more if you are using WCCP in your setup?
>
> Also, Are you using SSL-BUMP by any chance? (I really don?t know about a
> setup that doesn?t require this these days)
>
>
>
> If you would be able to share more information on your setup so I might be
> able to clone such a setup it will help a lot.
>
>
>
> Thanks,
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
>
> *From:* robert k Wild <robertkwild at gmail.com>
> *Sent:* Thursday, February 10, 2022 21:28
> *To:* NgTech LTD <ngtech1ltd at gmail.com>
> *Cc:* Squid Users <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Vulnerabilities with squid 4.15
>
>
>
> I have squid running on centos 7.9, I will move to ubuntu 20 04 03 as
> centos is officially dead to me
>
>
>
> I have compiled from source ie make make install as I'm running squid with
> squidclamav cicap cicap modules
>
>
>
> All instances I have compiled from source ie make make install
>
>
>
> I did a yum install clamav
>
>
>
> On Thu, 10 Feb 2022, 19:20 NgTech LTD, <ngtech1ltd at gmail.com> wrote:
>
> Hey Robert,
>
>
>
> First: your question is not silly.
>
> The answer will defer based on the complexity of the upgrade process.
>
> What Os are you using and also, did you compiled squid from sources or
> installed from a specific package?
>
> Also, what is your squid setup purpose?
>
>
>
> Eliezer
>
>
>
> ?????? ??? ??, 10 ????? 2022, 20:56, ??? robert k Wild ?<
> robertkwild at gmail.com>:
>
> Hi all,
>
>
>
> Is there any security vulnerabilities with squid 4.15, should I update to
> 4.17 or is it OK to still use as my squid proxy server
>
>
>
> Sorry for silly question
>
>
>
> Thanks,
>
> Rob
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220211/93dcc2af/attachment.htm>

From robertkwild at gmail.com  Fri Feb 11 10:09:05 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Fri, 11 Feb 2022 10:09:05 +0000
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <8b7984a6-d997-3fa0-3010-23638cac9550@treenet.co.nz>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <8b7984a6-d997-3fa0-3010-23638cac9550@treenet.co.nz>
Message-ID: <CAGU_Ci+XUwycucTtY2j_XCSKq5Hi1Bd5zbKpx45AfiOquNLWxw@mail.gmail.com>

ok so build my squid 4.17 with this option

--disable-wccpv2

as i have no lines in my squid.conf referencing wccp

is that what i should do, tbh i dont even know if i do or dont need wccp

On Fri, 11 Feb 2022 at 02:27, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 11/02/22 07:55, robert k Wild wrote:
> > Hi all,
> >
> > Is there any security vulnerabilities with squid 4.15, should I update
> > to 4.17 or is it OK to still use as my squid proxy server
> >
> > Sorry for silly question
> >
>
> Not silly.
>
> There is this one for WCCP:
> <
> https://github.com/squid-cache/squid/security/advisories/GHSA-rgf3-9v3p-qp82
> >
>
> However, be aware that the patch has been found to prevent all traffic
> from some routers. We are working on the fix for that.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220211/f0b18a76/attachment.htm>

From david at articatech.com  Fri Feb 11 15:02:49 2022
From: david at articatech.com (David Touzeau)
Date: Fri, 11 Feb 2022 16:02:49 +0100
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
 <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
Message-ID: <634f7bae-804b-2b2a-f0c3-676f738aad69@articatech.com>

Hello

Thank you but this is not the objective and this is the reason for 
needing the "fake".
Access to Kerberos or NTLM ports of the AD, is not possible. An LDAP 
server would be present with accounts replication.
The idea is to do a silent authentication without joining the AD
We did not need the double user/password credential, only the user sent 
by the browser is required

If the user has an Active Directory session then his account is 
automatically sent without him having to take any action.
If the user is in a workgroup then the account sent will not be in the 
LDAP database and will be rejected.
I don't need to argue about the security value of this method. It saves 
us from setting up a gas factory to make a kind of HotSpot

Le 11/02/2022 ? 05:55, Dieter Bloms a ?crit?:
> Hello David,
>
> for me it looks like you want to use kerberos authentication.
> With kerberos authentication the user don't have to authenticate against
> the proxy. The authentication is done in the background.
>
> Mayb this link will help:
>
> https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos
>
> On Thu, Feb 10, David Touzeau wrote:
>
>> Hi
>>
>> What we are looking for is to retrieve a "user" token without having to ask
>> anything from the user.
>> That's why we're looking at Active Directory credentials.
>> Once the user account is retrieved, a helper would be in charge of checking
>> if the user exists in the LDAP database.
>> This is to avoid any connection to an Active Directory
>> Maybe this is impossible
>>
>>
>> Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit?:
>>> On 10/02/22 01:43, David Touzeau wrote:
>>>> Hi
>>>>
>>>> I would like to sponsor the improvement of ntlm_fake_auth to support
>>>> new protocols
>>> ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
>>> protocols should already be supported as well as currently possible.
>>> NTLM is formally discontinued by MS and *very* inefficient.
>>>
>>> NP: NTLMv2 with encryption does not *work* because that encryption step
>>> requires secret keys the proxy is not able to know.
>>>
>>>> or go further produce a new negotiate_kerberos_auth_fake
>>>>
>>> With current Squid this helper only needs to produce an "OK" response
>>> regardless of the input. The basic_auth_fake does that.
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220211/0c67bca0/attachment.htm>

From ngtech1ltd at gmail.com  Sat Feb 12 05:27:28 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 12 Feb 2022 07:27:28 +0200
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <634f7bae-804b-2b2a-f0c3-676f738aad69@articatech.com>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
 <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
 <634f7bae-804b-2b2a-f0c3-676f738aad69@articatech.com>
Message-ID: <001a01d81fd1$3b14bb70$b13e3250$@gmail.com>

Hey David,

 

The general name of this concept is SSO service.

It can have single or multiple backends.

The main question is how to implement the solution in the optimal way possible.
(taking into account money, coding complexity and other humane parts)

 

You will need to authenticate the client against the main AUTH service.

There is a definitive way or statistical way to implement this solution.

With AD or Kerberos it?s possible to implement the solution in such a way that windows will
?transparently? authenticate to the proxy service.

However you must understand that all of this requires an infrastructure that will provide every piece of the setup.

If your setup doesn?t contains RDP like servers then it?s possible that you can authenticate a user with an IP compared
to pinning every connection to a specific user.

Also, the ?cost? of non-transparent authentication is that the user will be required to enter (manually or automatically) 
the username and the password.

An HotSpot like setup is called ?Captive Portal? and it?s a very simple setup to implement with active directory.

It?s also possible to implement a transparent authentication for such a setup based on session tokens.

 

You actually don?t need to create a ?fake? helper for such a setup but you can create one that is based on Linux.

It?s an ?Advanced? topic but if you do ask me it?s possible that you can take this in steps.

The first step would be to use a session helper that will authenticate the user and will identify the user
based on it?s IP address.

If it?s a wireless setup you can use a radius based authentication ( can also be implemented on a wired setup).

Once you will authenticate the client transparently or in another way you can limit the usage of the username to
a specific client and with that comes a guaranteed situation that a username will not be used from two sources.

I don?t know about your experience but the usage of a captive portal is very common In such situations.

The other option is to create an agent in the client side that will identify the user against the proxy/auth service
and it will create a situation which an authorization will be acquired based on some degree of authentication.

 

In most SSO environments it?s possible that per request/domain/other there is a transparent validation.

 

In all the above scenarios which requires authentication the right way to do it would be to use the proxy as
a configured proxy compared to transparent.

I believe that one thing to consider is that once you authenticate against a RADIUS service you would just
minimize the user interaction.

The main point from what I understand is to actually minimize the authentication steps of the client.

 

My suggestion for you is to first try and asses the complexity of a session helper, raidus and captive portal.

These are steps that you will need to do in order to asses the necessity of transparent SSO.

 

Also take your time to compare how a captive portal is configured in the next general products:

*	Palo Alto
*	FortiGate
*	Untangle
*	Others

 

>From the documentation you would see the different ways and ?grades? that they implement the solutions.

Once you know what the market offers and their equivalent costs you will probably understand what
you want and what you can afford to invest in the development process of each part of setup.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Friday, February 11, 2022 17:03
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid plugin sponsor

 

Hello

Thank you but this is not the objective and this is the reason for needing the "fake".
Access to Kerberos or NTLM ports of the AD, is not possible. An LDAP server would be present with accounts replication.
The idea is to do a silent authentication without joining the AD 
We did not need the double user/password credential, only the user sent by the browser is required

If the user has an Active Directory session then his account is automatically sent without him having to take any action.
If the user is in a workgroup then the account sent will not be in the LDAP database and will be rejected.
I don't need to argue about the security value of this method. It saves us from setting up a gas factory to make a kind of HotSpot

Le 11/02/2022 ? 05:55, Dieter Bloms a ?crit :

Hello David,
 
for me it looks like you want to use kerberos authentication.
With kerberos authentication the user don't have to authenticate against
the proxy. The authentication is done in the background.
 
Mayb this link will help:
 
https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos
 
On Thu, Feb 10, David Touzeau wrote:
 

Hi
 
What we are looking for is to retrieve a "user" token without having to ask
anything from the user.
That's why we're looking at Active Directory credentials.
Once the user account is retrieved, a helper would be in charge of checking
if the user exists in the LDAP database.
This is to avoid any connection to an Active Directory
Maybe this is impossible
 
 
Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit :

On 10/02/22 01:43, David Touzeau wrote:

Hi
 
I would like to sponsor the improvement of ntlm_fake_auth to support
new protocols

 
ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
protocols should already be supported as well as currently possible.
NTLM is formally discontinued by MS and *very* inefficient.
 
NP: NTLMv2 with encryption does not *work* because that encryption step
requires secret keys the proxy is not able to know.
 

or go further produce a new negotiate_kerberos_auth_fake
 

 
With current Squid this helper only needs to produce an "OK" response
regardless of the input. The basic_auth_fake does that.
 
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 
 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220212/11294157/attachment.htm>

From robertkwild at gmail.com  Sat Feb 12 12:35:59 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Sat, 12 Feb 2022 12:35:59 +0000
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CAGU_Ci+XUwycucTtY2j_XCSKq5Hi1Bd5zbKpx45AfiOquNLWxw@mail.gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <8b7984a6-d997-3fa0-3010-23638cac9550@treenet.co.nz>
 <CAGU_Ci+XUwycucTtY2j_XCSKq5Hi1Bd5zbKpx45AfiOquNLWxw@mail.gmail.com>
Message-ID: <CAGU_Ci+7mPK1jnAcAi+8EgDkPOPrb95opbUyr=5mqm=3NMi1nA@mail.gmail.com>

OK I'm fine

All Squid-4.x up to and including 4.16 built without
--disable-wccpv2 and configured with wccp2_router in squid.conf
are vulnerable.

Thanks Amos for this link




On Fri, 11 Feb 2022, 10:09 robert k Wild, <robertkwild at gmail.com> wrote:

> ok so build my squid 4.17 with this option
>
> --disable-wccpv2
>
> as i have no lines in my squid.conf referencing wccp
>
> is that what i should do, tbh i dont even know if i do or dont need wccp
>
> On Fri, 11 Feb 2022 at 02:27, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
>> On 11/02/22 07:55, robert k Wild wrote:
>> > Hi all,
>> >
>> > Is there any security vulnerabilities with squid 4.15, should I update
>> > to 4.17 or is it OK to still use as my squid proxy server
>> >
>> > Sorry for silly question
>> >
>>
>> Not silly.
>>
>> There is this one for WCCP:
>> <
>> https://github.com/squid-cache/squid/security/advisories/GHSA-rgf3-9v3p-qp82
>> >
>>
>> However, be aware that the patch has been found to prevent all traffic
>> from some routers. We are working on the fix for that.
>>
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> --
> Regards,
>
> Robert K Wild.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220212/5a579e44/attachment.htm>

From squid3 at treenet.co.nz  Sat Feb 12 13:41:07 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 Feb 2022 02:41:07 +1300
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CAGU_CiJzUdiVKeLmY3f821DD9+ff-K-O5bfO61uBePr690hL=Q@mail.gmail.com>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>
 <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>
 <000001d81f07$8e24fb10$aa6ef130$@gmail.com>
 <CAGU_CiJzUdiVKeLmY3f821DD9+ff-K-O5bfO61uBePr690hL=Q@mail.gmail.com>
Message-ID: <1e902660-72ef-3b95-0abf-cdf5f2e2c702@treenet.co.nz>

On 11/02/22 23:04, robert k Wild wrote:
> thanks Amos and Eliezer!
> 
> tbh i dont know if im using WCCP with my squid version, sorry, how do i 
> find that out?
> 

If this produces any config lines:

   squid -k parse 2>&1 | grep wccp


Cheers
Amos


From robertkwild at gmail.com  Sat Feb 12 13:56:07 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Sat, 12 Feb 2022 13:56:07 +0000
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <1e902660-72ef-3b95-0abf-cdf5f2e2c702@treenet.co.nz>
References: <CAGU_CiKm2DwK494hZ2NbQBM7mxG7T7eUM2aVde29YYYBq=b5pw@mail.gmail.com>
 <CABA8h=TfWxp_E71XxNB1ad3j6kxKvtO8BD+AQH+_ufoUrc9ZNQ@mail.gmail.com>
 <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>
 <000001d81f07$8e24fb10$aa6ef130$@gmail.com>
 <CAGU_CiJzUdiVKeLmY3f821DD9+ff-K-O5bfO61uBePr690hL=Q@mail.gmail.com>
 <1e902660-72ef-3b95-0abf-cdf5f2e2c702@treenet.co.nz>
Message-ID: <CAGU_CiKLk5KYqno7LPn3uqpbz_CK-h4Q2D3BqMci_OinMZ6i-A@mail.gmail.com>

nice, i dont have any, thanks Amos

i normally dont use parse, i normally use reconfigure and rotate

On Sat, 12 Feb 2022 at 13:43, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 11/02/22 23:04, robert k Wild wrote:
> > thanks Amos and Eliezer!
> >
> > tbh i dont know if im using WCCP with my squid version, sorry, how do i
> > find that out?
> >
>
> If this produces any config lines:
>
>    squid -k parse 2>&1 | grep wccp
>
>
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220212/a1787bc1/attachment.htm>

From uhlar at fantomas.sk  Sat Feb 12 19:36:35 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 12 Feb 2022 20:36:35 +0100
Subject: [squid-users] Vulnerabilities with squid 4.15
In-Reply-To: <CAGU_Ci+XUwycucTtY2j_XCSKq5Hi1Bd5zbKpx45AfiOquNLWxw@mail.gmail.com>
 <CAGU_Ci+MuswamEb3impuRhDYcePoVV_gT9o=4BdGVJcGneBo6A@mail.gmail.com>
Message-ID: <YggMQycSGob162sX@fantomas.sk>

On 10.02.22 19:28, robert k Wild wrote:
>I have squid running on centos 7.9, I will move to ubuntu 20 04 03 as
>centos is officially dead to me

both centos and ubuntu provide security updates for packages in system, 
while it's supported.

>I have compiled from source ie make make install as I'm running squid with
>squidclamav cicap cicap modules

... of course not for compiled packages.

On 11.02.22 10:09, robert k Wild wrote:
>ok so build my squid 4.17 with this option
>
>--disable-wccpv2
>
>as i have no lines in my squid.conf referencing wccp
>
>is that what i should do, tbh i dont even know if i do or dont need wccp

since you don't use wccp, this should not be needed

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux is like a teepee: no Windows, no Gates and an apache inside...


From david at articatech.com  Mon Feb 14 01:21:26 2022
From: david at articatech.com (David Touzeau)
Date: Mon, 14 Feb 2022 02:21:26 +0100
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <001a01d81fd1$3b14bb70$b13e3250$@gmail.com>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
 <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
 <634f7bae-804b-2b2a-f0c3-676f738aad69@articatech.com>
 <001a01d81fd1$3b14bb70$b13e3250$@gmail.com>
Message-ID: <06fc1b54-1bd0-75e9-9341-0a8b5b90b12c@articatech.com>


Thank you for your answer Elizer for all these details, but I've done 
some research to avoid soliciting the community for simple questions.

The objective is to not ask anything to the user and not to break his 
navigation with a session request.
To summarize, An SSO identification like kerberos with the following 
constraints:

 1. unknown Mac addresses
 2. DHCP IP with a short lease
 3. No Active Directory connection.




The network is in VLAN (Mac addr masked) and in DHCP with a short lease.
Even the notion of hotspot is complicated when you can't focus on a 
network attribute.
I try to find a way directly in the HTTP protocol.
This is the reason why a fake could be a solution.

But I think I'm trying to catch a chimera and we'll have to redesign the 
network architecture.

regards

Le 12/02/2022 ? 06:27, Eliezer Croitoru a ?crit?:
>
> Hey David,
>
> The general name of this concept is SSO service.
>
> It can have single or multiple backends.
>
> The main question is how to implement the solution in the optimal way 
> possible.
> (taking into account money, coding complexity and other humane parts)
>
> You will need to authenticate the client against the main AUTH service.
>
> There is a definitive way or statistical way to implement this solution.
>
> With AD or Kerberos it?s possible to implement the solution in such a 
> way that windows will
> ?transparently? authenticate to the proxy service.
>
> However you must understand that all of this requires an 
> infrastructure that will provide every piece of the setup.
>
> If your setup doesn?t contains RDP like servers then it?s possible 
> that you can authenticate a user with an IP compared
> to pinning every connection to a specific user.
>
> Also, the ?cost? of non-transparent authentication is that the user 
> will be required to enter (manually or automatically)
> the username and the password.
>
> An HotSpot like setup is called ?Captive Portal? and it?s a very 
> simple setup to implement with active directory.
>
> It?s also possible to implement a transparent authentication for such 
> a setup based on session tokens.
>
> You actually don?t need to create a ?fake? helper for such a setup but 
> you can create one that is based on Linux.
>
> It?s an ?Advanced? topic but if you do ask me it?s possible that you 
> can take this in steps.
>
> The first step would be to use a session helper that will authenticate 
> the user and will identify the user
> based on it?s IP address.
>
> If it?s a wireless setup you can use a radius based authentication ( 
> can also be implemented on a wired setup).
>
> Once you will authenticate the client transparently or in another way 
> you can limit the usage of the username to
> a specific client and with that comes a guaranteed situation that a 
> username will not be used from two sources.
>
> I don?t know about your experience but the usage of a captive portal 
> is very common In such situations.
>
> The other option is to create an agent in the client side that will 
> identify the user against the proxy/auth service
> and it will create a situation which an authorization will be acquired 
> based on some degree of authentication.
>
> In most SSO environments it?s possible that per request/domain/other 
> there is a transparent validation.
>
> In all the above scenarios which requires authentication the right way 
> to do it would be to use the proxy as
> a configured proxy compared to transparent.
>
> I believe that one thing to consider is that once you authenticate 
> against a RADIUS service you would just
> minimize the user interaction.
>
> The main point from what I understand is to actually minimize the 
> authentication steps of the client.
>
> My suggestion for you is to first try and asses the complexity of a 
> session helper, raidus and captive portal.
>
> These are steps that you will need to do in order to asses the 
> necessity of transparent SSO.
>
> Also take your time to compare how a captive portal is configured in 
> the next general products:
>
>   * Palo Alto
>   * FortiGate
>   * Untangle
>   * Others
>
> From the documentation you would see the different ways and ?grades? 
> that they implement the solutions.
>
> Once you know what the market offers and their equivalent costs you 
> will probably understand what
> you want and what you can afford to invest in the development process 
> of each part of setup.
>
> All The Bests,
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> *From:*squid-users <squid-users-bounces at lists.squid-cache.org> *On 
> Behalf Of *David Touzeau
> *Sent:* Friday, February 11, 2022 17:03
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] Squid plugin sponsor
>
> Hello
>
> Thank you but this is not the objective and this is the reason for 
> needing the "fake".
> Access to Kerberos or NTLM ports of the AD, is not possible. An LDAP 
> server would be present with accounts replication.
> The idea is to do a silent authentication without joining the AD
> We did not need the double user/password credential, only the user 
> sent by the browser is required
>
> If the user has an Active Directory session then his account is 
> automatically sent without him having to take any action.
> If the user is in a workgroup then the account sent will not be in the 
> LDAP database and will be rejected.
> I don't need to argue about the security value of this method. It 
> saves us from setting up a gas factory to make a kind of HotSpot
>
> Le 11/02/2022 ? 05:55, Dieter Bloms a ?crit?:
>
>     Hello David,
>
>     for me it looks like you want to use kerberos authentication.
>
>     With kerberos authentication the user don't have to authenticate against
>
>     the proxy. The authentication is done in the background.
>
>     Mayb this link will help:
>
>     https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos
>
>     On Thu, Feb 10, David Touzeau wrote:
>
>         Hi
>
>         What we are looking for is to retrieve a "user" token without having to ask
>
>         anything from the user.
>
>         That's why we're looking at Active Directory credentials.
>
>         Once the user account is retrieved, a helper would be in charge of checking
>
>         if the user exists in the LDAP database.
>
>         This is to avoid any connection to an Active Directory
>
>         Maybe this is impossible
>
>         Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit?:
>
>             On 10/02/22 01:43, David Touzeau wrote:
>
>                 Hi
>
>                 I would like to sponsor the improvement of ntlm_fake_auth to support
>
>                 new protocols
>
>             ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
>
>             protocols should already be supported as well as currently possible.
>
>             NTLM is formally discontinued by MS and *very* inefficient.
>
>             NP: NTLMv2 with encryption does not *work* because that encryption step
>
>             requires secret keys the proxy is not able to know.
>
>                 or go further produce a new negotiate_kerberos_auth_fake
>
>             With current Squid this helper only needs to produce an "OK" response
>
>             regardless of the input. The basic_auth_fake does that.
>
>             Amos
>
>             _______________________________________________
>
>             squid-users mailing list
>
>             squid-users at lists.squid-cache.org
>
>             http://lists.squid-cache.org/listinfo/squid-users
>
>         _______________________________________________
>
>         squid-users mailing list
>
>         squid-users at lists.squid-cache.org
>
>         http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220214/200bc232/attachment.htm>

From ns at fabbricapolitica.com  Mon Feb 14 09:16:25 2022
From: ns at fabbricapolitica.com (ns at fabbricapolitica.com)
Date: Mon, 14 Feb 2022 09:16:25 +0000
Subject: [squid-users] https interception problem with Squid 5
Message-ID: <a357d87979c56c1ea3fb3415b5427dde@fabbricapolitica.com>

Good morning,

I have been using Squid as an http caching proxy for a long time.

It's the second time I configured Squid for https caching and 
interception/inspection.

The first time everything was fine

The second...not so much.

I use the ssl_bump feature.

With Squid 4.13 and Openssl v 1.1.1k-1 all works well without errors or 
warnings.

With Squid v. 5.2.1 and Openssl v. 3.0.1, I got one error and one 
warning.

I tried to use the same squid.conf for Squid 4 and Squid 5.

Here are the problems with Squid 5.

1) ERROR

I checked the configuration with the command "squid -k parse" and I got 
this error: ERROR: Unable to configure Ephemeral ECDH: 
error:0480006C:PEM routines::no start line

If I remove the curve name from tls-dh in the config file, the error 
disappears.

First question: Which is the problem? How can I do to keep the curve 
name (prime256v1)

2) WARNING

I checked the configuration with the command "squid -k parse" and I got 
this warning: WARNING: Failed to decode DH parameters 
'/var/lib/squid/ssl_cert/squid-self-signed_dhparam.pem'

I generated the file for the Diffie-Hellman algorithm with this command 
(it worked with Squid4): openssl dhparam -outform PEM -out 
squid-self-signed_dhparam.pem 2048

Second question: Have you an idea on how to fix this?

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220214/0063edd6/attachment.htm>

From ngtech1ltd at gmail.com  Mon Feb 14 10:00:33 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 14 Feb 2022 12:00:33 +0200
Subject: [squid-users] https interception problem with Squid 5
In-Reply-To: <a357d87979c56c1ea3fb3415b5427dde@fabbricapolitica.com>
References: <a357d87979c56c1ea3fb3415b5427dde@fabbricapolitica.com>
Message-ID: <001001d82189$b5525e60$1ff71b20$@gmail.com>

Can you share the squid.conf so I can try to reproduce the issue here locally and verify how it could  be resolved?

What OS and other relevant details such as ?squid -v?  output might help.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of ns at fabbricapolitica.com
Sent: Monday, February 14, 2022 11:16
To: squid-users at lists.squid-cache.org
Subject: [squid-users] https interception problem with Squid 5

 

Good morning,

I have been using Squid as an http caching proxy for a long time.

It's the second time I configured Squid for https caching and interception/inspection.

The first time everything was fine

The second...not so much.

I use the ssl_bump feature.

With Squid 4.13 and Openssl v 1.1.1k-1 all works well without errors or warnings.

With Squid v. 5.2.1 and Openssl v. 3.0.1, I got one error and one warning.

I tried to use the same squid.conf for Squid 4 and Squid 5.

Here are the problems with Squid 5.

1) ERROR

I checked the configuration with the command "squid -k parse" and I got this error: ERROR: Unable to configure Ephemeral ECDH: error:0480006C:PEM routines::no start line

If I remove the curve name from tls-dh in the config file, the error disappears.

First question: Which is the problem? How can I do to keep the curve name (prime256v1)

2) WARNING

I checked the configuration with the command "squid -k parse" and I got this warning: WARNING: Failed to decode DH parameters '/var/lib/squid/ssl_cert/squid-self-signed_dhparam.pem'

I generated the file for the Diffie-Hellman algorithm with this command (it worked with Squid4): openssl dhparam -outform PEM -out squid-self-signed_dhparam.pem 2048

Second question: Have you an idea on how to fix this?

Thank you.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220214/6ae3a142/attachment.htm>

From ns at fabbricapolitica.com  Mon Feb 14 10:26:58 2022
From: ns at fabbricapolitica.com (ns at fabbricapolitica.com)
Date: Mon, 14 Feb 2022 10:26:58 +0000
Subject: [squid-users] https interception problem with Squid 5
In-Reply-To: <001001d82189$b5525e60$1ff71b20$@gmail.com>
References: <a357d87979c56c1ea3fb3415b5427dde@fabbricapolitica.com>
 <001001d82189$b5525e60$1ff71b20$@gmail.com>
Message-ID: <eaa225ef8b08c485a574380c15a72c84@fabbricapolitica.com>

-  OS version

Description: Ubuntu Jammy Jellyfish (development branch)
Release: 22.04
Codename: jammy

I couldn't use Ubuntu server 20.04 LTS focal fossa because the 
squid-OpenSSL package was unavailable for that release.

===

- Squid -v output

Squid Cache: Version 5.2

This binary uses OpenSSL 3.0.1 14 Dec 2021. configure options:  
'--build=x86_64-linux-gnu' '--prefix=/usr' 
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man' 
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' 
'--localstatedir=/var' '--disable-option-checking' 
'--disable-silent-rules' '--libdir=${prefix}/lib/x86_64-linux-gnu' 
'--runstatedir=/run' '--disable-maintainer-mode' 
'--disable-dependency-tracking' 'BUILDCXXFLAGS=-g -O2 
-ffile-prefix-map=/build/squid-V7aRc2/squid-5.2=. -flto=auto 
-ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong 
-Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 
-Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto 
-Wl,-z,relro -Wl,-z,now ' 'BUILDCXX=g++' 
'--with-build-environment=default' '--enable-build-info=Ubuntu linux' 
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' 
'--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' 
'--enable-inline' '--disable-arch-native' '--enable-async-io=8' 
'--enable-storeio=ufs,aufs,diskd,rock' 
'--enable-removal-policies=lru,heap' '--enable-delay-pools' 
'--enable-cache-digests' '--enable-icap-client' 
'--enable-follow-x-forwarded-for' 
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB' 
'--enable-auth-digest=file,LDAP' 
'--enable-auth-negotiate=kerberos,wrapper' 
'--enable-auth-ntlm=fake,SMB_LM' 
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' 
'--enable-security-cert-validators=fake' 
'--enable-storeid-rewrite-helpers=file' 
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' 
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' 
'--disable-translation' '--with-swapdir=/var/spool/squid' 
'--with-logdir=/var/log/squid' '--with-pidfile=/run/squid.pid' 
'--with-filedescriptors=65536' '--with-large-files' 
'--with-default-user=proxy' '--enable-linux-netfilter' '--with-systemd' 
'--with-openssl' '--enable-ssl-crtd' 'build_alias=x86_64-linux-gnu' 
'CFLAGS=-g -O2 -ffile-prefix-map=/build/squid-V7aRc2/squid-5.2=. 
-flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects 
-fstack-protector-strong -Wformat -Werror=format-security -Wall' 
'LDFLAGS=-Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects 
-flto=auto -Wl,-z,relro -Wl,-z,now ' 'CPPFLAGS=-Wdate-time 
-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 
-ffile-prefix-map=/build/squid-V7aRc2/squid-5.2=. -flto=auto 
-ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong 
-Wformat -Werror=format-security'

===

- squid.conf

# ACL
acl localnet src 0.0.0.1-0.255.255.255    # RFC 1122 "this" network 
(LAN)
acl localnet src 10.0.0.0/8        # RFC 1918 local private network 
(LAN)
acl localnet src 100.64.0.0/10        # RFC 6598 shared address space 
(CGN)
acl localnet src 169.254.0.0/16            # RFC 3927 link-local 
(directly plugged) machines
acl localnet src 172.16.0.0/12        # RFC 1918 local private network 
(LAN)
acl localnet src 192.168.0.0/16        # RFC 1918 local private network 
(LAN)
acl localnet src fc00::/7            # RFC 4193 local private network 
range
acl localnet src fe80::/10            # RFC 4291 link-local (directly 
plugged) machines
acl SSL_ports port 443        # httpssl
acl Safe_ports port 80        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1025-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl CONNECT method CONNECT
acl intermediate_fetching transaction_initiator certificate-fetching

#Other configuration files
include /etc/squid/conf.d/*

#Access rules
http_access allow intermediate_fetching
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

# HTTPS interception direct proxy
http_port 3128 tcpkeepalive=60,30,3 ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=20MB 
tls-cert=/var/lib/squid/ssl_cert/squid-self-signed.crt 
tls-key=/var/lib/squid/ssl_cert/squid-self-signed.key 
cipher=HIGH:MEDIUM:!LOW:!RC4:!SEED:!IDEA:!3DES:!MD5:!EXP:!PSK:!DSS:!TLS13-AES-256-GCM-SHA384 
options=NO_TLSv1,NO_SSLv3 
tls-dh=prime256v1:/var/lib/squid/ssl_cert/squid-self-signed_dhparam.pem
sslcrtd_program /lib/squid/security_file_certgen -s 
/var/lib/squid/ssl_db -M 20MB
sslcrtd_children 5
acl step1 at_step SslBump1
ssl_bump bump all
sslproxy_cert_error deny all

# Cache
cache_mem 512 MB
maximum_object_size 240 MB
cache_dir aufs /var/spool/squid/ 4096 16 256
coredump_dir /var/spool/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern .        0    20%    4320

# More privacy
via off
forwarded_for off

Il 2022-02-14 10:00 Eliezer Croitoru ha scritto:

> Can you share the squid.conf so I can try to reproduce the issue here 
> locally and verify how it could  be resolved?
> 
> What OS and other relevant details such as "squid -v"  output might 
> help.
> 
> Thanks,
> 
> Eliezer
> 
> ----
> 
> Eliezer Croitoru
> 
> NgTech, Tech Support
> 
> Mobile: +972-5-28704261
> 
> Email: ngtech1ltd at gmail.com
> 
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf 
> Of ns at fabbricapolitica.com
> Sent: Monday, February 14, 2022 11:16
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] https interception problem with Squid 5
> 
> Good morning,
> 
> I have been using Squid as an http caching proxy for a long time.
> 
> It's the second time I configured Squid for https caching and 
> interception/inspection.
> 
> The first time everything was fine
> 
> The second...not so much.
> 
> I use the ssl_bump feature.
> 
> With Squid 4.13 and Openssl v 1.1.1k-1 all works well without errors or 
> warnings.
> 
> With Squid v. 5.2.1 and Openssl v. 3.0.1, I got one error and one 
> warning.
> 
> I tried to use the same squid.conf for Squid 4 and Squid 5.
> 
> Here are the problems with Squid 5.
> 
> 1) ERROR
> 
> I checked the configuration with the command "squid -k parse" and I got 
> this error: ERROR: Unable to configure Ephemeral ECDH: 
> error:0480006C:PEM routines::no start line
> 
> If I remove the curve name from tls-dh in the config file, the error 
> disappears.
> 
> First question: Which is the problem? How can I do to keep the curve 
> name (prime256v1)
> 
> 2) WARNING
> 
> I checked the configuration with the command "squid -k parse" and I got 
> this warning: WARNING: Failed to decode DH parameters 
> '/var/lib/squid/ssl_cert/squid-self-signed_dhparam.pem'
> 
> I generated the file for the Diffie-Hellman algorithm with this command 
> (it worked with Squid4): openssl dhparam -outform PEM -out 
> squid-self-signed_dhparam.pem 2048
> 
> Second question: Have you an idea on how to fix this?
> 
> Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220214/c6154605/attachment.htm>

From m_zouhairy at ckta.by  Mon Feb 14 12:25:34 2022
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 14 Feb 2022 15:25:34 +0300
Subject: [squid-users] squid 5.3 frequent crash
In-Reply-To: <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
References: <6742bd06-14b6-c43a-71ff-3bd3ebb5fe44@ckta.by>
 <09f185c0-533a-cafe-4e80-57a644449888@measurement-factory.com>
 <08060bfb-9502-b112-7dfc-666417a75037@treenet.co.nz>
 <0b2da47a-a862-6f25-5a0b-9b2bb881303b@measurement-factory.com>
 <07c5174b-ea6f-e5e3-80e3-1f3f4508d16d@treenet.co.nz>
 <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
Message-ID: <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>

now on 5.4 i get:

2022/02/14 15:14:32 kid1| FATAL: Unable to open HTTP Socket
2022/02/14 15:14:32 kid1| Squid Cache (Version 5.4): Terminated abnormally.
CPU Usage: 0.401 seconds = 0.357 user + 0.044 sys
Maximum Resident Size: 101920 KB
Page faults with physical i/o: 0
2022/02/14 15:14:32 kid1| Closing Pinger socket on FD 46
2022/02/14 15:14:32| pinger: Initialising ICMP pinger ...
2022/02/14 15:14:32| pinger: ICMP socket opened.
2022/02/14 15:14:32| pinger: ICMPv6 socket opened
2022/02/14 15:14:32| Pinger exiting.
2022/02/14 15:14:32 kid1| ERROR: negotiating TLS on FD 116: 
error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed 
(1/-1/0)
2022/02/14 15:14:32 kid1| Set Current Directory to /var/cache/squid
2022/02/14 15:14:32 kid1| Starting Squid Cache version 5.4 for 
x86_64-suse-linux-gnu...
2022/02/14 15:14:32 kid1| Service Name: squid
2022/02/14 15:14:32 kid1| Process ID 19350
2022/02/14 15:14:32 kid1| Process Roles: worker
2022/02/14 15:14:32 kid1| With 4096 file descriptors available
2022/02/14 15:14:32 kid1| Initializing IP Cache...
2022/02/14 15:14:32 kid1| DNS Socket created at [::], FD 8
2022/02/14 15:14:32 kid1| DNS Socket created at 0.0.0.0, FD 9
2022/02/14 15:14:32 kid1| Adding nameserver 10.0.10.15 from /etc/resolv.conf
2022/02/14 15:14:32 kid1| Adding nameserver 10.0.10.14 from /etc/resolv.conf
2022/02/14 15:14:32 kid1| helperOpenServers: Starting 5/32 
'security_file_certgen' processes
2022/02/14 15:14:32 kid1| helperOpenServers: Starting 8/16 'ufdbgclient' 
processes
2022/02/14 15:14:33 kid1| Logfile: opening log 
daemon:/var/log/squid/access.log
2022/02/14 15:14:33 kid1| Logfile Daemon: opening log 
/var/log/squid/access.log
2022/02/14 15:14:33 kid1| ERROR: negotiating TLS on FD 161: 
error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed 
(1/-1/0)
2022/02/14 15:14:33 kid1| Unlinkd pipe opened on FD 41
2022/02/14 15:14:33 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2022/02/14 15:14:33 kid1| Store logging disabled
2022/02/14 15:14:33 kid1| Swap maxSize 3072000 + 983040 KB, estimated 
311926 objects
2022/02/14 15:14:33 kid1| Target number of buckets: 15596
2022/02/14 15:14:33 kid1| Using 16384 Store buckets
2022/02/14 15:14:33 kid1| Max Mem  size: 983040 KB
2022/02/14 15:14:33 kid1| Max Swap size: 3072000 KB
2022/02/14 15:14:33 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2022/02/14 15:14:33 kid1| Using Least Load store dir selection
2022/02/14 15:14:33 kid1| Set Current Directory to /var/cache/squid
2022/02/14 15:14:33 kid1| Finished loading MIME types and icons.
2022/02/14 15:14:33 kid1| commBind Cannot bind socket FD 44 to 
[::]:8080: (98) Address already in use
2022/02/14 15:14:33 kid1| HTCP Disabled.
2022/02/14 15:14:33 kid1| Pinger socket opened on FD 46
2022/02/14 15:14:33 kid1| Squid plugin modules loaded: 0
2022/02/14 15:14:33 kid1| Adaptation support is off.
2022/02/14 15:14:33 kid1| Closing HTTP(S) port [::]:8080
2022/02/14 15:14:33 kid1| Not currently OK to rewrite swap log.
2022/02/14 15:14:33 kid1| storeDirWriteCleanLogs: Operation aborted.
2022/02/14 15:14:33 kid1| FATAL: Unable to open HTTP Socket
2022/02/14 15:14:33 kid1| Squid Cache (Version 5.4): Terminated abnormally.

On 1/18/22 23:56, Alex Rousskov wrote:
> On 1/18/22 2:51 AM, Amos Jeffries wrote:
>> On 8/01/22 05:02, Alex Rousskov wrote:
>>> On 1/7/22 9:34 AM, Amos Jeffries wrote:
>>>> Others include altering the fundamental AsyncJob API behaviour -
>>>> affecting every feature in Squid at their most fundamental levels.
> 
>>> I disagree with the above summary.
> 
>> This is not an opinion.
> 
> It is impossible to tell for sure whether this is an opinion or a fact
> because the summary is using undefined terms like "every feature" and
> "most fundamental levels". To you, it may sound like a fact. To me, it
> sounds like gross exaggeration at best: Clearly, there are Squid
> features (for some reasonable definition of a "feature") unaffected by
> this change at "most fundamental levels" (for some reasonable definition
> of "most fundamental levels").
> 
> 
>> The patch "part 2" makes logic changes to
>> AsyncJob - specifically destructors and swanSong. That touches
>> *everything* Squid does. The other parts touch I/O in similarly deep
>> ways and we have a history of unexpected weird side effects with I/O
>> refactorings.
> 
> I do not think squid-users is the right place to debate complex
> development issues. I will just note that the commit in question does
> not, IMO, change AsyncJob methods in fundamental ways. It only shrinks
> the long-known gray area of what those functions should (not) do. Before
> this change, we did not know where certain actions should take place.
> Now, we (think we) do, and we have adjusted a few places to follow those
> newly discovered rules.
> 
> Will this complex change have unexpected side effects? Yes, of course! I
> have disclosed that risk when posting the changes. No need to grossly
> exaggerate to agree on that point -- nobody is arguing against it.
> 
> 
>> I am seriously considering using our exceptional beta release process
>> for these changes once v5.4 bug fixes are out.
> 
> FWIW, I see no need for a special process here. We have no reasons to
> believe that the change is making Squid v5 worse overall. All those who
> tested the change in v5 and master reported significant improvement in
> Squid stability. Moreover, since the last numbered v5 release was
> unstable (for many reasons), the bar for the next numbered v5 release is
> pretty low: We are not going from very stable to possibly unstable; we
> are going from very unstable to possibly less unstable.
> 
> Said that, I am not trying to block the "exceptional beta release
> process" you want to use. I am just providing feedback. Most v5 actions
> are your call as a v5 maintainer, including special v5.x.y snapshots
> that have three numbers instead of the usual two.
> 
> 
>> IMO the best code to base your backport PR on would be the v5 HEAD after
>> 1st Feb when I post the "prep for 5.4" or similar QA for review.
> 
> To avoid misunderstanding, when you have a commit SHA that the backport
> should be based on, please let me know that SHA, and I will start
> backporting from that point. That commit/SHA does not have to be in the
> official branch, of course.
> 
> 
>> That gives us 6 weeks for validation before v5.5 release decisions are
>> made. I seriously *hope* that is enough testing not to be hit later with
>> another one of these.
> 
> FWIW, all other factors being equal, I doubt you would see more "beta"
> v5 testers than you would see without any special "beta" releases. If
> anything, the opposite is probably true. That is one of the several
> reasons I do not recommend using special procedures for releasing this
> important bug fix in v5. Again, this is your call.
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Mon Feb 14 13:41:04 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Feb 2022 08:41:04 -0500
Subject: [squid-users] squid 5.3 frequent crash
In-Reply-To: <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>
References: <6742bd06-14b6-c43a-71ff-3bd3ebb5fe44@ckta.by>
 <09f185c0-533a-cafe-4e80-57a644449888@measurement-factory.com>
 <08060bfb-9502-b112-7dfc-666417a75037@treenet.co.nz>
 <0b2da47a-a862-6f25-5a0b-9b2bb881303b@measurement-factory.com>
 <07c5174b-ea6f-e5e3-80e3-1f3f4508d16d@treenet.co.nz>
 <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
 <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>
Message-ID: <541ae9b1-4aba-12f6-11a7-cb6c68fc781d@measurement-factory.com>

On 2/14/22 07:25, Majed Zouhairy wrote:
> now on 5.4 i get:
...
> 2022/02/14 15:14:33 kid1| commBind Cannot bind socket FD 44 to [::]:8080: (98) Address already in use
> 2022/02/14 15:14:33 kid1| Closing HTTP(S) port [::]:8080
> 2022/02/14 15:14:33 kid1| FATAL: Unable to open HTTP Socket
> 2022/02/14 15:14:33 kid1| Squid Cache (Version 5.4): Terminated abnormally.

The above is usually the result of a misconfiguration or mismanagement: 
There are two processes trying to listen on the same port 8080. That 
could be two Squid worker processes or a Squid worker process competing 
with a non-Squid process.

* If your Squid startup scripts include preliminary steps like "squid 
-z", then make sure those scripts wait for that first Squid instance to 
exit before starting the primary Squid instance.

* If you are using SMP macros or conditionals in squid.conf, please 
share your Squid configuration.


HTH,

Alex.


From ben.goz87 at gmail.com  Mon Feb 14 13:49:21 2022
From: ben.goz87 at gmail.com (Ben Goz)
Date: Mon, 14 Feb 2022 15:49:21 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
Message-ID: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>

By the help of God.

Hi,
Ny squid version is 4.15, using it on tproxy configuration.

I'm using ssl bump to intercept https connection, but I want to splice
several domains.
I have a problem that when I'm splicing some google domains eg. youtube.com
then
gmail.com domain also spliced.

I know that it is very common for google servers to host multiple domains
on single server.
And I suspect that when I'm splicing for example youtube.com it'll also
splices google.com.

 Here are my squid configurations for the ssl bump:

https_port xxxx ssl-bump tproxy generate-host-certificates=on options=ALL
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/ssl_cert/myCA.pem
dhparams=/usr/local/squid/etc/dhparam.pem sslflags=NO_DEFAULT_CA

acl DiscoverSNIHost at_step SslBump1

acl NoSSLIntercept ssl::server_name  "/usr/local/squid/etc/url-no-bump"
acl NoSSLInterceptRegexp ssl::server_name_regex -i
"/usr/local/squid/etc/url-no-bump-regexp"
ssl_bump splice NoSSLInterceptRegexp_always
ssl_bump splice NoSSLIntercept
ssl_bump splice NoSSLInterceptRegexp
ssl_bump peek DiscoverSNIHost
ssl_bump bump all
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220214/a49ea564/attachment.htm>

From m_zouhairy at ckta.by  Mon Feb 14 13:49:50 2022
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 14 Feb 2022 16:49:50 +0300
Subject: [squid-users] squid 5.3 frequent crash
In-Reply-To: <541ae9b1-4aba-12f6-11a7-cb6c68fc781d@measurement-factory.com>
References: <6742bd06-14b6-c43a-71ff-3bd3ebb5fe44@ckta.by>
 <09f185c0-533a-cafe-4e80-57a644449888@measurement-factory.com>
 <08060bfb-9502-b112-7dfc-666417a75037@treenet.co.nz>
 <0b2da47a-a862-6f25-5a0b-9b2bb881303b@measurement-factory.com>
 <07c5174b-ea6f-e5e3-80e3-1f3f4508d16d@treenet.co.nz>
 <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
 <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>
 <541ae9b1-4aba-12f6-11a7-cb6c68fc781d@measurement-factory.com>
Message-ID: <61be4d99-76d2-233a-ef96-f03a5e28fef4@ckta.by>

i have squid 4.17 on the machine assembled from source but i did an 
uninstall
sudo make uninstall
before installing 5.4 from the package manager..should i have stopped 
the squid before uninstalling?
or is there something else?

On 2/14/22 16:41, Alex Rousskov wrote:
> On 2/14/22 07:25, Majed Zouhairy wrote:
>> now on 5.4 i get:
> ...
>> 2022/02/14 15:14:33 kid1| commBind Cannot bind socket FD 44 to 
>> [::]:8080: (98) Address already in use
>> 2022/02/14 15:14:33 kid1| Closing HTTP(S) port [::]:8080
>> 2022/02/14 15:14:33 kid1| FATAL: Unable to open HTTP Socket
>> 2022/02/14 15:14:33 kid1| Squid Cache (Version 5.4): Terminated 
>> abnormally.
> 
> The above is usually the result of a misconfiguration or mismanagement: 
> There are two processes trying to listen on the same port 8080. That 
> could be two Squid worker processes or a Squid worker process competing 
> with a non-Squid process.
> 
> * If your Squid startup scripts include preliminary steps like "squid 
> -z", then make sure those scripts wait for that first Squid instance to 
> exit before starting the primary Squid instance.
> 
> * If you are using SMP macros or conditionals in squid.conf, please 
> share your Squid configuration.
> 
> 
> HTH,
> 
> Alex.


From rousskov at measurement-factory.com  Mon Feb 14 13:53:55 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Feb 2022 08:53:55 -0500
Subject: [squid-users] squid 5.3 frequent crash
In-Reply-To: <61be4d99-76d2-233a-ef96-f03a5e28fef4@ckta.by>
References: <6742bd06-14b6-c43a-71ff-3bd3ebb5fe44@ckta.by>
 <09f185c0-533a-cafe-4e80-57a644449888@measurement-factory.com>
 <08060bfb-9502-b112-7dfc-666417a75037@treenet.co.nz>
 <0b2da47a-a862-6f25-5a0b-9b2bb881303b@measurement-factory.com>
 <07c5174b-ea6f-e5e3-80e3-1f3f4508d16d@treenet.co.nz>
 <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
 <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>
 <541ae9b1-4aba-12f6-11a7-cb6c68fc781d@measurement-factory.com>
 <61be4d99-76d2-233a-ef96-f03a5e28fef4@ckta.by>
Message-ID: <c6224b2a-5a11-c9a3-ee8b-6e35689deac4@measurement-factory.com>

On 2/14/22 08:49, Majed Zouhairy wrote:
> i have squid 4.17 on the machine assembled from source but i did an 
> uninstall
> sudo make uninstall
> before installing 5.4 from the package manager..should i have stopped 
> the squid before uninstalling?
> or is there something else?

Sorry, I cannot give you the exact steps to prevent two Squids from 
running on your server -- there are too many unknowns for me to do that.

If you have a Squid instance running, you should stop it before starting 
another Squid instance. You can check whether you have a Squid instance 
running using "ps aux | fgrep squid" or a similar basic command.

Alex.


> On 2/14/22 16:41, Alex Rousskov wrote:
>> On 2/14/22 07:25, Majed Zouhairy wrote:
>>> now on 5.4 i get:
>> ...
>>> 2022/02/14 15:14:33 kid1| commBind Cannot bind socket FD 44 to 
>>> [::]:8080: (98) Address already in use
>>> 2022/02/14 15:14:33 kid1| Closing HTTP(S) port [::]:8080
>>> 2022/02/14 15:14:33 kid1| FATAL: Unable to open HTTP Socket
>>> 2022/02/14 15:14:33 kid1| Squid Cache (Version 5.4): Terminated 
>>> abnormally.
>>
>> The above is usually the result of a misconfiguration or 
>> mismanagement: There are two processes trying to listen on the same 
>> port 8080. That could be two Squid worker processes or a Squid worker 
>> process competing with a non-Squid process.
>>
>> * If your Squid startup scripts include preliminary steps like "squid 
>> -z", then make sure those scripts wait for that first Squid instance 
>> to exit before starting the primary Squid instance.
>>
>> * If you are using SMP macros or conditionals in squid.conf, please 
>> share your Squid configuration.
>>
>>
>> HTH,
>>
>> Alex.



From m_zouhairy at ckta.by  Mon Feb 14 14:34:13 2022
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Mon, 14 Feb 2022 17:34:13 +0300
Subject: [squid-users] squid 5.3 frequent crash
In-Reply-To: <c6224b2a-5a11-c9a3-ee8b-6e35689deac4@measurement-factory.com>
References: <6742bd06-14b6-c43a-71ff-3bd3ebb5fe44@ckta.by>
 <09f185c0-533a-cafe-4e80-57a644449888@measurement-factory.com>
 <08060bfb-9502-b112-7dfc-666417a75037@treenet.co.nz>
 <0b2da47a-a862-6f25-5a0b-9b2bb881303b@measurement-factory.com>
 <07c5174b-ea6f-e5e3-80e3-1f3f4508d16d@treenet.co.nz>
 <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
 <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>
 <541ae9b1-4aba-12f6-11a7-cb6c68fc781d@measurement-factory.com>
 <61be4d99-76d2-233a-ef96-f03a5e28fef4@ckta.by>
 <c6224b2a-5a11-c9a3-ee8b-6e35689deac4@measurement-factory.com>
Message-ID: <d50574bb-021d-3dc8-576d-9406f9b3b7c5@ckta.by>

i restarted the system and now it shows:

2022/02/14 17:29:50 kid1| Starting Squid Cache version 5.4 for 
x86_64-suse-linux-gnu...
2022/02/14 17:29:50 kid1| Service Name: squid
2022/02/14 17:29:50 kid1| Process ID 1731
2022/02/14 17:29:50 kid1| Process Roles: worker
2022/02/14 17:29:50 kid1| With 4096 file descriptors available
2022/02/14 17:29:50 kid1| Initializing IP Cache...
2022/02/14 17:29:50 kid1| DNS Socket created at [::], FD 8
2022/02/14 17:29:50 kid1| DNS Socket created at 0.0.0.0, FD 9
2022/02/14 17:29:50 kid1| Adding nameserver 10.0.10.15 from /etc/resolv.conf
2022/02/14 17:29:50 kid1| Adding nameserver 10.0.10.14 from /etc/resolv.conf
2022/02/14 17:29:50 kid1| helperOpenServers: Starting 5/32 
'security_file_certgen' processes
2022/02/14 17:29:50 kid1| helperOpenServers: Starting 8/16 'ufdbgclient' 
processes
2022/02/14 17:29:50 kid1| Logfile: opening log 
daemon:/var/log/squid/access.log
2022/02/14 17:29:50 kid1| Logfile Daemon: opening log 
/var/log/squid/access.log
2022/02/14 17:29:50 kid1| Unlinkd pipe opened on FD 41
2022/02/14 17:29:50 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2022/02/14 17:29:50 kid1| Store logging disabled
2022/02/14 17:29:50 kid1| Swap maxSize 3072000 + 983040 KB, estimated 
311926 objects
2022/02/14 17:29:50 kid1| Target number of buckets: 15596
2022/02/14 17:29:50 kid1| Using 16384 Store buckets
2022/02/14 17:29:50 kid1| Max Mem  size: 983040 KB
2022/02/14 17:29:50 kid1| Max Swap size: 3072000 KB
2022/02/14 17:29:50 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2022/02/14 17:29:50 kid1| Using Least Load store dir selection
2022/02/14 17:29:50 kid1| Set Current Directory to /var/cache/squid
2022/02/14 17:29:50 kid1| Finished loading MIME types and icons.
2022/02/14 17:29:50 kid1| HTCP Disabled.
2022/02/14 17:29:50 kid1| Pinger socket opened on FD 46
2022/02/14 17:29:50 kid1| Squid plugin modules loaded: 0
2022/02/14 17:29:50 kid1| Adaptation support is off.
2022/02/14 17:29:50 kid1| Accepting SSL bumped HTTP Socket connections 
at conn29 local=[::]:8080 remote=[::] FD 44 flags=9
2022/02/14 17:29:50| pinger: Initialising ICMP pinger ...
2022/02/14 17:29:50| pinger: ICMP socket opened.
2022/02/14 17:29:50| pinger: ICMPv6 socket opened
2022/02/14 17:29:51 kid1| Store rebuilding is 4.63% complete
2022/02/14 17:29:52 kid1| Done reading /var/cache/squid swaplog (86461 
entries)
2022/02/14 17:29:52 kid1| Finished rebuilding storage from disk.
2022/02/14 17:29:52 kid1|     86460 Entries scanned
2022/02/14 17:29:52 kid1|         0 Invalid entries.
2022/02/14 17:29:52 kid1|         0 With invalid flags.
2022/02/14 17:29:52 kid1|     86460 Objects loaded.
2022/02/14 17:29:52 kid1|         0 Objects expired.
2022/02/14 17:29:52 kid1|         1 Objects cancelled.
2022/02/14 17:29:52 kid1|         0 Duplicate URLs purged.
2022/02/14 17:29:52 kid1|         0 Swapfile clashes avoided.
2022/02/14 17:29:52 kid1|   Took 1.18 seconds (73141.02 objects/sec).
2022/02/14 17:29:52 kid1| Beginning Validation Procedure
2022/02/14 17:29:52 kid1|   Completed Validation Procedure
2022/02/14 17:29:52 kid1|   Validated 86451 Entries
2022/02/14 17:29:52 kid1|   store_swap_size = 2764772.00 KB
2022/02/14 17:29:52 kid1| storeLateRelease: released 1 objects
2022/02/14 17:29:52| Pinger exiting.
2022/02/14 17:29:52 kid1| ERROR: failure while accepting a TLS 
connection on conn116 local=10.0.0.18:8080 remote=10.64.0.55:60273 FD 66 
flags=1: 0x560ccf160380*1
     current master transaction: master55
2022/02/14 17:29:55| Pinger exiting.
2022/02/14 17:29:56 kid1| FATAL: check failed: opening()
     exception location: FwdState.cc(663) noteDestinationsEnd
     current master transaction: master57
2022/02/14 17:29:56 kid1| Closing Pinger socket on FD 46
     current master transaction: master57
2022/02/14 17:29:57 kid1| Set Current Directory to /var/cache/squid
2022/02/14 17:29:57 kid1| Starting Squid Cache version 5.4 for 
x86_64-suse-linux-gnu...
2022/02/14 17:29:57 kid1| Service Name: squid
2022/02/14 17:29:57 kid1| Process ID 1782
2022/02/14 17:29:57 kid1| Process Roles: worker
2022/02/14 17:29:57 kid1| With 4096 file descriptors available
2022/02/14 17:29:57 kid1| Initializing IP Cache...
2022/02/14 17:29:57 kid1| DNS Socket created at [::], FD 8
2022/02/14 17:29:57 kid1| DNS Socket created at 0.0.0.0, FD 9
2022/02/14 17:29:57 kid1| Adding nameserver 10.0.10.15 from /etc/resolv.conf
2022/02/14 17:29:57 kid1| Adding nameserver 10.0.10.14 from /etc/resolv.conf
2022/02/14 17:29:57 kid1| helperOpenServers: Starting 5/32 
'security_file_certgen' processes
2022/02/14 17:29:57 kid1| helperOpenServers: Starting 8/16 'ufdbgclient' 
processes
2022/02/14 17:29:57 kid1| Logfile: opening log 
daemon:/var/log/squid/access.log
2022/02/14 17:29:57 kid1| Logfile Daemon: opening log 
/var/log/squid/access.log
2022/02/14 17:29:57 kid1| Unlinkd pipe opened on FD 41
2022/02/14 17:29:57 kid1| Local cache digest enabled; rebuild/rewrite 
every 3600/3600 sec
2022/02/14 17:29:57 kid1| Store logging disabled
2022/02/14 17:29:57 kid1| Swap maxSize 3072000 + 983040 KB, estimated 
311926 objects
2022/02/14 17:29:57 kid1| Target number of buckets: 15596
2022/02/14 17:29:57 kid1| Using 16384 Store buckets
2022/02/14 17:29:57 kid1| Max Mem  size: 983040 KB
2022/02/14 17:29:57 kid1| Max Swap size: 3072000 KB
2022/02/14 17:29:57 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2022/02/14 17:29:57 kid1| Using Least Load store dir selection
2022/02/14 17:29:57 kid1| Set Current Directory to /var/cache/squid
2022/02/14 17:29:57 kid1| Finished loading MIME types and icons.
2022/02/14 17:29:57 kid1| HTCP Disabled.
2022/02/14 17:29:57 kid1| Pinger socket opened on FD 46
2022/02/14 17:29:57 kid1| Squid plugin modules loaded: 0
2022/02/14 17:29:57 kid1| Adaptation support is off.
2022/02/14 17:29:57 kid1| Accepting SSL bumped HTTP Socket connections 
at conn29 local=[::]:8080 remote=[::] FD 44 flags=9
2022/02/14 17:29:57| pinger: Initialising ICMP pinger ...
2022/02/14 17:29:57| pinger: ICMP socket opened.
2022/02/14 17:29:57| pinger: ICMPv6 socket opened
2022/02/14 17:29:57 kid1| Store rebuilding is 4.63% complete
2022/02/14 17:29:58 kid1| Done reading /var/cache/squid swaplog (86480 
entries)
2022/02/14 17:29:58 kid1| Finished rebuilding storage from disk.
2022/02/14 17:29:58 kid1|     86470 Entries scanned
2022/02/14 17:29:58 kid1|         0 Invalid entries.
2022/02/14 17:29:58 kid1|         0 With invalid flags.
2022/02/14 17:29:58 kid1|     86470 Objects loaded.
2022/02/14 17:29:58 kid1|         0 Objects expired.
2022/02/14 17:29:58 kid1|        10 Objects cancelled.
2022/02/14 17:29:58 kid1|         0 Duplicate URLs purged.
2022/02/14 17:29:58 kid1|         0 Swapfile clashes avoided.
2022/02/14 17:29:58 kid1|   Took 1.20 seconds (72075.51 objects/sec).
2022/02/14 17:29:58 kid1| Beginning Validation Procedure
2022/02/14 17:29:58 kid1|   Completed Validation Procedure
2022/02/14 17:29:58 kid1|   Validated 86461 Entries
2022/02/14 17:29:58 kid1|   store_swap_size = 2765044.00 KB
2022/02/14 17:29:58 kid1| storeLateRelease: released 10 objects
2022/02/14 17:29:59| Pinger exiting.
2022/02/14 17:29:59| SendEcho ERROR: sending to ICMPv6 packet to 
[2603:1020:600::213]: (101) Network is unreachable
2022/02/14 17:29:59 kid1| ERROR: failure while accepting a TLS 
connection on conn278 local=10.0.0.18:8080 remote=10.134.10.155:60276 FD 
104 flags=1: 0x55dbf94bda30*1
     current master transaction: master57
2022/02/14 17:30:00 kid1| Starting new sslcrtd_program helpers...
     current master transaction: master59
2022/02/14 17:30:00 kid1| helperOpenServers: Starting 1/32 
'security_file_certgen' processes
     current master transaction: master59
2022/02/14 17:30:00 kid1| Starting new sslcrtd_program helpers...
     current master transaction: master153
2022/02/14 17:30:00 kid1| helperOpenServers: Starting 1/32 
'security_file_certgen' processes
     current master transaction: master153
2022/02/14 17:30:00| SendEcho ERROR: sending to ICMPv6 packet to 
[2001:67c:4e8:1033:5:100:0:a]: (101) Network is unreachable
2022/02/14 17:30:01| Pinger exiting.
2022/02/14 17:30:03 kid1| FATAL: check failed: opening()
     exception location: FwdState.cc(663) noteDestinationsEnd
     current master transaction: master55
2022/02/14 17:30:03 kid1| Closing Pinger socket on FD 46
     current master transaction: master55
2022/02/14 17:30:03| Removing PID file (/run/squid.pid)


sudo squid -z
zouhairy at proxy:~> 2022/02/14 17:32:14 kid1| Set Current Directory to 
/var/cache/squid
2022/02/14 17:32:14 kid1| Creating missing swap directories
2022/02/14 17:32:14 kid1| /var/cache/squid exists
2022/02/14 17:32:14 kid1| /var/cache/squid/00 exists
2022/02/14 17:32:14 kid1| Making directories in /var/cache/squid/00
2022/02/14 17:32:15 kid1| /var/cache/squid/01 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/01
2022/02/14 17:32:15 kid1| /var/cache/squid/02 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/02
2022/02/14 17:32:15 kid1| /var/cache/squid/03 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/03
2022/02/14 17:32:15 kid1| /var/cache/squid/04 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/04
2022/02/14 17:32:15 kid1| /var/cache/squid/05 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/05
2022/02/14 17:32:15 kid1| /var/cache/squid/06 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/06
2022/02/14 17:32:15 kid1| /var/cache/squid/07 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/07
2022/02/14 17:32:15 kid1| /var/cache/squid/08 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/08
2022/02/14 17:32:15 kid1| /var/cache/squid/09 exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/09
2022/02/14 17:32:15 kid1| /var/cache/squid/0A exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/0A
2022/02/14 17:32:15 kid1| /var/cache/squid/0B exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/0B
2022/02/14 17:32:15 kid1| /var/cache/squid/0C exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/0C
2022/02/14 17:32:15 kid1| /var/cache/squid/0D exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/0D
2022/02/14 17:32:15 kid1| /var/cache/squid/0E exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/0E
2022/02/14 17:32:15 kid1| /var/cache/squid/0F exists
2022/02/14 17:32:15 kid1| Making directories in /var/cache/squid/0F
2022/02/14 17:32:15| Removing PID file (/run/squid.pid)


On 2/14/22 16:53, Alex Rousskov wrote:
> On 2/14/22 08:49, Majed Zouhairy wrote:
>> i have squid 4.17 on the machine assembled from source but i did an 
>> uninstall
>> sudo make uninstall
>> before installing 5.4 from the package manager..should i have stopped 
>> the squid before uninstalling?
>> or is there something else?
> 
> Sorry, I cannot give you the exact steps to prevent two Squids from 
> running on your server -- there are too many unknowns for me to do that.
> 
> If you have a Squid instance running, you should stop it before starting 
> another Squid instance. You can check whether you have a Squid instance 
> running using "ps aux | fgrep squid" or a similar basic command.
> 
> Alex.
> 
> 
>> On 2/14/22 16:41, Alex Rousskov wrote:
>>> On 2/14/22 07:25, Majed Zouhairy wrote:
>>>> now on 5.4 i get:
>>> ...
>>>> 2022/02/14 15:14:33 kid1| commBind Cannot bind socket FD 44 to 
>>>> [::]:8080: (98) Address already in use
>>>> 2022/02/14 15:14:33 kid1| Closing HTTP(S) port [::]:8080
>>>> 2022/02/14 15:14:33 kid1| FATAL: Unable to open HTTP Socket
>>>> 2022/02/14 15:14:33 kid1| Squid Cache (Version 5.4): Terminated 
>>>> abnormally.
>>>
>>> The above is usually the result of a misconfiguration or 
>>> mismanagement: There are two processes trying to listen on the same 
>>> port 8080. That could be two Squid worker processes or a Squid worker 
>>> process competing with a non-Squid process.
>>>
>>> * If your Squid startup scripts include preliminary steps like "squid 
>>> -z", then make sure those scripts wait for that first Squid instance 
>>> to exit before starting the primary Squid instance.
>>>
>>> * If you are using SMP macros or conditionals in squid.conf, please 
>>> share your Squid configuration.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
> 


From rousskov at measurement-factory.com  Mon Feb 14 15:55:38 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 Feb 2022 10:55:38 -0500
Subject: [squid-users] squid 5.3 frequent crash
In-Reply-To: <d50574bb-021d-3dc8-576d-9406f9b3b7c5@ckta.by>
References: <6742bd06-14b6-c43a-71ff-3bd3ebb5fe44@ckta.by>
 <09f185c0-533a-cafe-4e80-57a644449888@measurement-factory.com>
 <08060bfb-9502-b112-7dfc-666417a75037@treenet.co.nz>
 <0b2da47a-a862-6f25-5a0b-9b2bb881303b@measurement-factory.com>
 <07c5174b-ea6f-e5e3-80e3-1f3f4508d16d@treenet.co.nz>
 <d13fc0ef-76ef-5ca7-0693-b0a0bd2161f5@measurement-factory.com>
 <11cf8414-1cc6-8e2f-9995-2da8833fe5b6@ckta.by>
 <541ae9b1-4aba-12f6-11a7-cb6c68fc781d@measurement-factory.com>
 <61be4d99-76d2-233a-ef96-f03a5e28fef4@ckta.by>
 <c6224b2a-5a11-c9a3-ee8b-6e35689deac4@measurement-factory.com>
 <d50574bb-021d-3dc8-576d-9406f9b3b7c5@ckta.by>
Message-ID: <e8ef4cae-e7e1-1159-16ea-b354d1fa8f37@measurement-factory.com>

On 2/14/22 09:34, Majed Zouhairy wrote:

> 2022/02/14 17:29:50 kid1| Starting Squid Cache version 5.4
> 2022/02/14 17:29:56 kid1| FATAL: check failed: opening()
>  ??? exception location: FwdState.cc(663) noteDestinationsEnd
...
> 2022/02/14 17:30:03 kid1| FATAL: check failed: opening()
>  ??? exception location: FwdState.cc(663) noteDestinationsEnd
>  ??? current master transaction: master55

This is known as Bug #5055:
https://bugs.squid-cache.org/show_bug.cgi?id=5055

Squid v5.4 does not have a fix for that bug. You can wait for the next 
v5 release or build Squid from the latest v5 sources (commit 1332f8d or 
later).

HTH,

Alex.


> On 2/14/22 16:53, Alex Rousskov wrote:
>> On 2/14/22 08:49, Majed Zouhairy wrote:
>>> i have squid 4.17 on the machine assembled from source but i did an 
>>> uninstall
>>> sudo make uninstall
>>> before installing 5.4 from the package manager..should i have stopped 
>>> the squid before uninstalling?
>>> or is there something else?
>>
>> Sorry, I cannot give you the exact steps to prevent two Squids from 
>> running on your server -- there are too many unknowns for me to do that.
>>
>> If you have a Squid instance running, you should stop it before 
>> starting another Squid instance. You can check whether you have a 
>> Squid instance running using "ps aux | fgrep squid" or a similar basic 
>> command.
>>
>> Alex.
>>
>>
>>> On 2/14/22 16:41, Alex Rousskov wrote:
>>>> On 2/14/22 07:25, Majed Zouhairy wrote:
>>>>> now on 5.4 i get:
>>>> ...
>>>>> 2022/02/14 15:14:33 kid1| commBind Cannot bind socket FD 44 to 
>>>>> [::]:8080: (98) Address already in use
>>>>> 2022/02/14 15:14:33 kid1| Closing HTTP(S) port [::]:8080
>>>>> 2022/02/14 15:14:33 kid1| FATAL: Unable to open HTTP Socket
>>>>> 2022/02/14 15:14:33 kid1| Squid Cache (Version 5.4): Terminated 
>>>>> abnormally.
>>>>
>>>> The above is usually the result of a misconfiguration or 
>>>> mismanagement: There are two processes trying to listen on the same 
>>>> port 8080. That could be two Squid worker processes or a Squid 
>>>> worker process competing with a non-Squid process.
>>>>
>>>> * If your Squid startup scripts include preliminary steps like 
>>>> "squid -z", then make sure those scripts wait for that first Squid 
>>>> instance to exit before starting the primary Squid instance.
>>>>
>>>> * If you are using SMP macros or conditionals in squid.conf, please 
>>>> share your Squid configuration.
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>



From ngtech1ltd at gmail.com  Mon Feb 14 18:50:51 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 14 Feb 2022 20:50:51 +0200
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <06fc1b54-1bd0-75e9-9341-0a8b5b90b12c@articatech.com>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
 <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
 <634f7bae-804b-2b2a-f0c3-676f738aad69@articatech.com>
 <001a01d81fd1$3b14bb70$b13e3250$@gmail.com>
 <06fc1b54-1bd0-75e9-9341-0a8b5b90b12c@articatech.com>
Message-ID: <001501d821d3$caea7a10$60bf6e30$@gmail.com>

Hey David,

 

Transparent authentication using Kerberos can only be used with a directory service.

There are couple ways to authenticate?

You can use an ?automatic? hotspot website that will use cookies to authenticate the client once in a very long time.

If the client request is not recognized or the client is not recognized for any reason it?s reasonable to redirect him into a captive portal.

I can try to work on a demo but I need to know more details about the network structure and to verify what is possible and not.

Every device ie Switch and router or AP etc should be mentioned to understand the scenario.

While you assume it?s a chimera I still believe it?s just a three heads Kerberos which? was proved to exists? in the movies and in the virtual world.

 

Eliezer 

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: David Touzeau <david at articatech.com> 
Sent: Monday, February 14, 2022 03:21
To: Eliezer Croitoru <ngtech1ltd at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid plugin sponsor

 


Thank you for your answer Elizer for all these details, but I've done some research to avoid soliciting the community for simple questions.

The objective is to not ask anything to the user and not to break his navigation with a session request.
To summarize, An SSO identification like kerberos with the following constraints:

1.	unknown Mac addresses 
2.	DHCP IP with a short lease
3.	No Active Directory connection.




The network is in VLAN (Mac addr masked) and in DHCP with a short lease.
Even the notion of hotspot is complicated when you can't focus on a network attribute.
I try to find a way directly in the HTTP protocol. 
This is the reason why a fake could be a solution.

But I think I'm trying to catch a chimera and we'll have to redesign the network architecture.

regards

Le 12/02/2022 ? 06:27, Eliezer Croitoru a ?crit :

Hey David,

 

The general name of this concept is SSO service.

It can have single or multiple backends.

The main question is how to implement the solution in the optimal way possible.
(taking into account money, coding complexity and other humane parts)

 

You will need to authenticate the client against the main AUTH service.

There is a definitive way or statistical way to implement this solution.

With AD or Kerberos it?s possible to implement the solution in such a way that windows will
?transparently? authenticate to the proxy service.

However you must understand that all of this requires an infrastructure that will provide every piece of the setup.

If your setup doesn?t contains RDP like servers then it?s possible that you can authenticate a user with an IP compared
to pinning every connection to a specific user.

Also, the ?cost? of non-transparent authentication is that the user will be required to enter (manually or automatically) 
the username and the password.

An HotSpot like setup is called ?Captive Portal? and it?s a very simple setup to implement with active directory.

It?s also possible to implement a transparent authentication for such a setup based on session tokens.

 

You actually don?t need to create a ?fake? helper for such a setup but you can create one that is based on Linux.

It?s an ?Advanced? topic but if you do ask me it?s possible that you can take this in steps.

The first step would be to use a session helper that will authenticate the user and will identify the user
based on it?s IP address.

If it?s a wireless setup you can use a radius based authentication ( can also be implemented on a wired setup).

Once you will authenticate the client transparently or in another way you can limit the usage of the username to
a specific client and with that comes a guaranteed situation that a username will not be used from two sources.

I don?t know about your experience but the usage of a captive portal is very common In such situations.

The other option is to create an agent in the client side that will identify the user against the proxy/auth service
and it will create a situation which an authorization will be acquired based on some degree of authentication.

 

In most SSO environments it?s possible that per request/domain/other there is a transparent validation.

 

In all the above scenarios which requires authentication the right way to do it would be to use the proxy as
a configured proxy compared to transparent.

I believe that one thing to consider is that once you authenticate against a RADIUS service you would just
minimize the user interaction.

The main point from what I understand is to actually minimize the authentication steps of the client.

 

My suggestion for you is to first try and asses the complexity of a session helper, raidus and captive portal.

These are steps that you will need to do in order to asses the necessity of transparent SSO.

 

Also take your time to compare how a captive portal is configured in the next general products:

1.	Palo Alto
2.	FortiGate
3.	Untangle
4.	Others

 

>From the documentation you would see the different ways and ?grades? that they implement the solutions.

Once you know what the market offers and their equivalent costs you will probably understand what
you want and what you can afford to invest in the development process of each part of setup.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of David Touzeau
Sent: Friday, February 11, 2022 17:03
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] Squid plugin sponsor

 

Hello

Thank you but this is not the objective and this is the reason for needing the "fake".
Access to Kerberos or NTLM ports of the AD, is not possible. An LDAP server would be present with accounts replication.
The idea is to do a silent authentication without joining the AD 
We did not need the double user/password credential, only the user sent by the browser is required

If the user has an Active Directory session then his account is automatically sent without him having to take any action.
If the user is in a workgroup then the account sent will not be in the LDAP database and will be rejected.
I don't need to argue about the security value of this method. It saves us from setting up a gas factory to make a kind of HotSpot

Le 11/02/2022 ? 05:55, Dieter Bloms a ?crit :

Hello David,
 
for me it looks like you want to use kerberos authentication.
With kerberos authentication the user don't have to authenticate against
the proxy. The authentication is done in the background.
 
Mayb this link will help:
 
https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos
 
On Thu, Feb 10, David Touzeau wrote:
 

Hi
 
What we are looking for is to retrieve a "user" token without having to ask
anything from the user.
That's why we're looking at Active Directory credentials.
Once the user account is retrieved, a helper would be in charge of checking
if the user exists in the LDAP database.
This is to avoid any connection to an Active Directory
Maybe this is impossible
 
 
Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit :

On 10/02/22 01:43, David Touzeau wrote:

Hi
 
I would like to sponsor the improvement of ntlm_fake_auth to support
new protocols

 
ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
protocols should already be supported as well as currently possible.
NTLM is formally discontinued by MS and *very* inefficient.
 
NP: NTLMv2 with encryption does not *work* because that encryption step
requires secret keys the proxy is not able to know.
 

or go further produce a new negotiate_kerberos_auth_fake
 

 
With current Squid this helper only needs to produce an "OK" response
regardless of the input. The basic_auth_fake does that.
 
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 
 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220214/cdb957c6/attachment.htm>

From david at articatech.com  Tue Feb 15 00:48:37 2022
From: david at articatech.com (David Touzeau)
Date: Tue, 15 Feb 2022 01:48:37 +0100
Subject: [squid-users] Squid plugin sponsor
In-Reply-To: <001501d821d3$caea7a10$60bf6e30$@gmail.com>
References: <d557e409-081e-010f-e58c-5fae5304e854@articatech.com>
 <b7a0ecf7-09d5-120a-fa96-c30cdf3592e4@treenet.co.nz>
 <172dca1a-462c-3ce7-3de4-9ca741c15579@articatech.com>
 <20220211045539.3q4xzr6bqbunhaxc@bloms.de>
 <634f7bae-804b-2b2a-f0c3-676f738aad69@articatech.com>
 <001a01d81fd1$3b14bb70$b13e3250$@gmail.com>
 <06fc1b54-1bd0-75e9-9341-0a8b5b90b12c@articatech.com>
 <001501d821d3$caea7a10$60bf6e30$@gmail.com>
Message-ID: <1689437a-5611-1c64-9828-4e0d052cd1da@articatech.com>

Eliezer,

First of all, thank you for twisting your brain at our request.
I know your skills and your time is very valuable.

HotSpot+Cookies can be interesting but it has a constraint that 
kerberos/NTLM SSO fixes:

1)? Redirecting connections to a HotSpot requires Squid to be able to 
forward the redirection.
When using SSL sites without MAN-IN-THE-MIDDLE, we fall into structural 
issues.

2)? Even if this problem can be circumvented, it is necessary for the 
user to identify himself on the Splash Screen to understand who he is.
While this user is already identified with his Windows session.


Forget about NTLMv2 which does not provide the "Fake" anymore
The advantage of fake_ntlm is that when Squid performs its 407, 
naturally the browser sends its windows session username whether it is 
connected to an Active Directory or not.

This is what we want to catch in the end.

The HotSpot way is a half-solution. It circumvents the limit of 
identification but adds new network constraints you mention.

The dream is a plugin that forces Squid to generate a 407, asks to 
browsers "Give me your user account whatever it is" and allows access in 
any case to place the user=xxx switch for the next processing.

It almost looks like the "ident" method
http://www.squid-cache.org/Misc/ident.html
Without having to install a piece of software and a listening port on 
all the computers in the network

Le 14/02/2022 ? 19:50, Eliezer Croitoru a ?crit?:
>
> Hey David,
>
> Transparent authentication using Kerberos can only be used with a 
> directory service.
>
> There are couple ways to authenticate?
>
> You can use an ?automatic? hotspot website that will use cookies to 
> authenticate the client once in a very long time.
>
> If the client request is not recognized or the client is not 
> recognized for any reason it?s reasonable to redirect him into a 
> captive portal.
>
> I can try to work on a demo but I need to know more details about the 
> network structure and to verify what is possible and not.
>
> Every device ie Switch and router or AP etc should be mentioned to 
> understand the scenario.
>
> While you assume it?s a chimera I still believe it?s just a three 
> heads Kerberos which? was proved to exists? in the movies and in the 
> virtual world.
>
> Eliezer
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
> *From:*David Touzeau <david at articatech.com>
> *Sent:* Monday, February 14, 2022 03:21
> *To:* Eliezer Croitoru <ngtech1ltd at gmail.com>
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] Squid plugin sponsor
>
>
> Thank you for your answer Elizer for all these details, but I've done 
> some research to avoid soliciting the community for simple questions.
>
> The objective is to not ask anything to the user and not to break his 
> navigation with a session request.
> To summarize, An SSO identification like kerberos with the following 
> constraints:
>
>  1. unknown Mac addresses
>  2. DHCP IP with a short lease
>  3. No Active Directory connection.
>
>
>
>
> The network is in VLAN (Mac addr masked) and in DHCP with a short lease.
> Even the notion of hotspot is complicated when you can't focus on a 
> network attribute.
> I try to find a way directly in the HTTP protocol.
> This is the reason why a fake could be a solution.
>
> But I think I'm trying to catch a chimera and we'll have to redesign 
> the network architecture.
>
> regards
>
> Le 12/02/2022 ? 06:27, Eliezer Croitoru a ?crit?:
>
>     Hey David,
>
>     The general name of this concept is SSO service.
>
>     It can have single or multiple backends.
>
>     The main question is how to implement the solution in the optimal
>     way possible.
>     (taking into account money, coding complexity and other humane parts)
>
>     You will need to authenticate the client against the main AUTH
>     service.
>
>     There is a definitive way or statistical way to implement this
>     solution.
>
>     With AD or Kerberos it?s possible to implement the solution in
>     such a way that windows will
>     ?transparently? authenticate to the proxy service.
>
>     However you must understand that all of this requires an
>     infrastructure that will provide every piece of the setup.
>
>     If your setup doesn?t contains RDP like servers then it?s possible
>     that you can authenticate a user with an IP compared
>     to pinning every connection to a specific user.
>
>     Also, the ?cost? of non-transparent authentication is that the
>     user will be required to enter (manually or automatically)
>     the username and the password.
>
>     An HotSpot like setup is called ?Captive Portal? and it?s a very
>     simple setup to implement with active directory.
>
>     It?s also possible to implement a transparent authentication for
>     such a setup based on session tokens.
>
>     You actually don?t need to create a ?fake? helper for such a setup
>     but you can create one that is based on Linux.
>
>     It?s an ?Advanced? topic but if you do ask me it?s possible that
>     you can take this in steps.
>
>     The first step would be to use a session helper that will
>     authenticate the user and will identify the user
>     based on it?s IP address.
>
>     If it?s a wireless setup you can use a radius based authentication
>     ( can also be implemented on a wired setup).
>
>     Once you will authenticate the client transparently or in another
>     way you can limit the usage of the username to
>     a specific client and with that comes a guaranteed situation that
>     a username will not be used from two sources.
>
>     I don?t know about your experience but the usage of a captive
>     portal is very common In such situations.
>
>     The other option is to create an agent in the client side that
>     will identify the user against the proxy/auth service
>     and it will create a situation which an authorization will be
>     acquired based on some degree of authentication.
>
>     In most SSO environments it?s possible that per
>     request/domain/other there is a transparent validation.
>
>     In all the above scenarios which requires authentication the right
>     way to do it would be to use the proxy as
>     a configured proxy compared to transparent.
>
>     I believe that one thing to consider is that once you authenticate
>     against a RADIUS service you would just
>     minimize the user interaction.
>
>     The main point from what I understand is to actually minimize the
>     authentication steps of the client.
>
>     My suggestion for you is to first try and asses the complexity of
>     a session helper, raidus and captive portal.
>
>     These are steps that you will need to do in order to asses the
>     necessity of transparent SSO.
>
>     Also take your time to compare how a captive portal is configured
>     in the next general products:
>
>      1. Palo Alto
>      2. FortiGate
>      3. Untangle
>      4. Others
>
>     From the documentation you would see the different ways and
>     ?grades? that they implement the solutions.
>
>     Once you know what the market offers and their equivalent costs
>     you will probably understand what
>     you want and what you can afford to invest in the development
>     process of each part of setup.
>
>     All The Bests,
>
>     Eliezer
>
>     ----
>
>     Eliezer Croitoru
>
>     NgTech, Tech Support
>
>     Mobile: +972-5-28704261
>
>     Email: ngtech1ltd at gmail.com
>
>     *From:*squid-users <squid-users-bounces at lists.squid-cache.org>
>     <mailto:squid-users-bounces at lists.squid-cache.org> *On Behalf Of
>     *David Touzeau
>     *Sent:* Friday, February 11, 2022 17:03
>     *To:* squid-users at lists.squid-cache.org
>     *Subject:* Re: [squid-users] Squid plugin sponsor
>
>     Hello
>
>     Thank you but this is not the objective and this is the reason for
>     needing the "fake".
>     Access to Kerberos or NTLM ports of the AD, is not possible. An
>     LDAP server would be present with accounts replication.
>     The idea is to do a silent authentication without joining the AD
>     We did not need the double user/password credential, only the user
>     sent by the browser is required
>
>     If the user has an Active Directory session then his account is
>     automatically sent without him having to take any action.
>     If the user is in a workgroup then the account sent will not be in
>     the LDAP database and will be rejected.
>     I don't need to argue about the security value of this method. It
>     saves us from setting up a gas factory to make a kind of HotSpot
>
>     Le 11/02/2022 ? 05:55, Dieter Bloms a ?crit?:
>
>         Hello David,
>
>           
>
>         for me it looks like you want to use kerberos authentication.
>
>         With kerberos authentication the user don't have to authenticate against
>
>         the proxy. The authentication is done in the background.
>
>           
>
>         Mayb this link will help:
>
>           
>
>         https://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos
>
>           
>
>         On Thu, Feb 10, David Touzeau wrote:
>
>           
>
>             Hi
>
>               
>
>             What we are looking for is to retrieve a "user" token without having to ask
>
>             anything from the user.
>
>             That's why we're looking at Active Directory credentials.
>
>             Once the user account is retrieved, a helper would be in charge of checking
>
>             if the user exists in the LDAP database.
>
>             This is to avoid any connection to an Active Directory
>
>             Maybe this is impossible
>
>               
>
>               
>
>             Le 10/02/2022 ? 05:03, Amos Jeffries a ?crit?:
>
>                 On 10/02/22 01:43, David Touzeau wrote:
>
>                     Hi
>
>                       
>
>                     I would like to sponsor the improvement of ntlm_fake_auth to support
>
>                     new protocols
>
>                   
>
>                 ntlm_* helpers are specific to NTLM authentication. All LanManager (LM)
>
>                 protocols should already be supported as well as currently possible.
>
>                 NTLM is formally discontinued by MS and *very* inefficient.
>
>                   
>
>                 NP: NTLMv2 with encryption does not *work* because that encryption step
>
>                 requires secret keys the proxy is not able to know.
>
>                   
>
>                     or go further produce a new negotiate_kerberos_auth_fake
>
>                       
>
>                   
>
>                 With current Squid this helper only needs to produce an "OK" response
>
>                 regardless of the input. The basic_auth_fake does that.
>
>                   
>
>                 Amos
>
>                 _______________________________________________
>
>                 squid-users mailing list
>
>                 squid-users at lists.squid-cache.org
>
>                 http://lists.squid-cache.org/listinfo/squid-users
>
>           
>
>             _______________________________________________
>
>             squid-users mailing list
>
>             squid-users at lists.squid-cache.org
>
>             http://lists.squid-cache.org/listinfo/squid-users
>
>           
>
>           
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220215/e5a97f60/attachment.htm>

From ben.goz87 at gmail.com  Thu Feb 17 12:46:38 2022
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 17 Feb 2022 14:46:38 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
Message-ID: <CADAqQfyNxanWMV_jHO_tX7BfAcnP+kZBUSsq0t-AOra1xwsy8A@mail.gmail.com>

By the help of God.
Any insights?

Thanks,
Ben

??????? ??? ??, 14 ????? 2022 ?-15:49 ??? ?Ben Goz?? <?ben.goz87 at gmail.com
??>:?

> By the help of God.
>
> Hi,
> Ny squid version is 4.15, using it on tproxy configuration.
>
> I'm using ssl bump to intercept https connection, but I want to splice
> several domains.
> I have a problem that when I'm splicing some google domains eg.
> youtube.com then
> gmail.com domain also spliced.
>
> I know that it is very common for google servers to host multiple domains
> on single server.
> And I suspect that when I'm splicing for example youtube.com it'll also
> splices google.com.
>
>  Here are my squid configurations for the ssl bump:
>
> https_port xxxx ssl-bump tproxy generate-host-certificates=on options=ALL
> dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/ssl_cert/myCA.pem
> dhparams=/usr/local/squid/etc/dhparam.pem sslflags=NO_DEFAULT_CA
>
> acl DiscoverSNIHost at_step SslBump1
>
> acl NoSSLIntercept ssl::server_name  "/usr/local/squid/etc/url-no-bump"
> acl NoSSLInterceptRegexp ssl::server_name_regex -i
> "/usr/local/squid/etc/url-no-bump-regexp"
> ssl_bump splice NoSSLInterceptRegexp_always
> ssl_bump splice NoSSLIntercept
> ssl_bump splice NoSSLInterceptRegexp
> ssl_bump peek DiscoverSNIHost
> ssl_bump bump all
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220217/41541d67/attachment.htm>

From ngtech1ltd at gmail.com  Sun Feb 20 11:32:38 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sun, 20 Feb 2022 13:32:38 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <CADAqQfyNxanWMV_jHO_tX7BfAcnP+kZBUSsq0t-AOra1xwsy8A@mail.gmail.com>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
 <CADAqQfyNxanWMV_jHO_tX7BfAcnP+kZBUSsq0t-AOra1xwsy8A@mail.gmail.com>
Message-ID: <000001d8264d$90fc70f0$b2f552d0$@gmail.com>

Hey Ben,

 

I have seen your email however didn?t had enough time to respond.

I and others need some free time?

I am more then willing to test this issue in my local test environment.

I can test it on Oracle Enterprise Linux 8 with the latest 4.x version.

We can simplify things by creating a very specific environment without any unknowns.

You will need to provide the full details of the testing setup and the content of:

acl NoSSLIntercept ssl::server_name  "/usr/local/squid/etc/url-no-bump"
acl NoSSLInterceptRegexp ssl::server_name_regex -i "/usr/local/squid/etc/url-no-bump-regexp"



In my environment it works as expected without any issues while I am not user ssl::server_name_regex

The docs clearly state:

        acl aclname ssl::server_name_regex [-i] \.foo\.com ...

          # regex matches server name obtained from various sources [fast]

 

 

So you should try to use:

        acl aclname ssl::server_name [option] .foo.com ...
          # matches server name obtained from various sources [fast]

 

Instead as a starter point.

 

I understand you need some help but I and others have other obligations in life so it would happen from time to time

that someone is not free to try and help you.

 

All The Bests,

Eliezer

 

*	If someone would have provided me with enough food and other living expenses I might have been free enough to help you.

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Ben Goz
Sent: Thursday, February 17, 2022 14:47
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Splice certain SNIs which served by the same IP

 

By the help of God.

Any insights?

 

Thanks,

Ben

 

??????? ??? ??, 14 ????? 2022 ?-15:49 ??? ?Ben Goz? <? <mailto:ben.goz87 at gmail.com> ben.goz87 at gmail.com?>:

By the help of God.

 

Hi,

Ny squid version is 4.15, using it on tproxy configuration.

 

I'm using ssl bump to intercept https connection, but I want to splice several domains.

I have a problem that when I'm splicing some google domains eg. youtube.com <http://youtube.com>  then

gmail.com <http://gmail.com>  domain also spliced.

 

I know that it is very common for google servers to host multiple domains on single server.

And I suspect that when I'm splicing for example youtube.com <http://youtube.com>  it'll also splices google.com <http://google.com> .

 

 Here are my squid configurations for the ssl bump:

 

https_port xxxx ssl-bump tproxy generate-host-certificates=on options=ALL dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/ssl_cert/myCA.pem dhparams=/usr/local/squid/etc/dhparam.pem sslflags=NO_DEFAULT_CA

acl DiscoverSNIHost at_step SslBump1

acl NoSSLIntercept ssl::server_name  "/usr/local/squid/etc/url-no-bump"
acl NoSSLInterceptRegexp ssl::server_name_regex -i "/usr/local/squid/etc/url-no-bump-regexp"
ssl_bump splice NoSSLInterceptRegexp_always
ssl_bump splice NoSSLIntercept
ssl_bump splice NoSSLInterceptRegexp
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220220/0c039b04/attachment.htm>

From jason_haar at trimble.com  Mon Feb 21 01:43:52 2022
From: jason_haar at trimble.com (Jason Haar)
Date: Mon, 21 Feb 2022 14:43:52 +1300
Subject: [squid-users] squid-5.4 blocking on ipv6 outage
Message-ID: <CAFChrg+bHtamN1gtxaDnNLnWvtgeOFLhYQ9yXvixxNqRc9YihA@mail.gmail.com>

Hi there

I've noticed that the Internet ipv6 is not quite as reliable as ipv4, in
that squid reports it cannot connect to web servers with an ipv6 error when
the web server is still available over ipv4.

eg right now one of our Internet-based web apps (which has 2 ipv6 and 2
ipv4 IP addresses mapped to it's DNS name) is not responding over ipv6 for
some reason (I dunno - not involved myself) - but is working fine over
ipv4. Squid-5.4 is erroring out - saying that it cannot connect to the
first ipv6 address with a "no route to host" error. But if I use good-ol'
telnet to the DNS name, telnet shows it trying-and-failing against both
ipv6 addresses and then succeeds against the ipv4. ie it works and squid
doesn't. BTW the same squid server is currently fine with ipv6 clients
talking to it and it talking over ipv6 to Internet hosts like google.com -
ie this is an ipv6 outage on one Internet host where it's ipv4 is still
working.

This doesn't seem like a negative_dns_ttl setting issue, it seems like
squid just tries one address on a multiple-IP DNS record and stops trying?
I even got tcpdump up and can see that when I do a "shift-reload" on the
webpage, squid only sends a few SYN packets to the same non-working IPv6
address - it doesn't even try the other 3 IPs?

I also checked squidcachemgr.cgi and the DNS record isn't even cached in
"FQDN Cache Stats and Contents", which I guess is consistent with it's
opinion that it's not working.

Any ideas what's going on there? thanks!

-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220221/594bbf09/attachment.htm>

From ngtech1ltd at gmail.com  Mon Feb 21 04:32:40 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Mon, 21 Feb 2022 06:32:40 +0200
Subject: [squid-users] squid-5.4 blocking on ipv6 outage
In-Reply-To: <CAFChrg+bHtamN1gtxaDnNLnWvtgeOFLhYQ9yXvixxNqRc9YihA@mail.gmail.com>
References: <CAFChrg+bHtamN1gtxaDnNLnWvtgeOFLhYQ9yXvixxNqRc9YihA@mail.gmail.com>
Message-ID: <004101d826dc$107df360$3179da20$@gmail.com>

Hey,

 

Bugs to the rescue

+1

 

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Jason Haar
Sent: Monday, February 21, 2022 03:44
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] squid-5.4 blocking on ipv6 outage

 

Hi there

 

I've noticed that the Internet ipv6 is not quite as reliable as ipv4, in that squid reports it cannot connect to web servers with an ipv6 error when the web server is still available over ipv4.

 

eg right now one of our Internet-based web apps (which has 2 ipv6 and 2 ipv4 IP addresses mapped to it's DNS name) is not responding over ipv6 for some reason (I dunno - not involved myself) - but is working fine over ipv4. Squid-5.4 is erroring out - saying that it cannot connect to the first ipv6 address with a "no route to host" error. But if I use good-ol' telnet to the DNS name, telnet shows it trying-and-failing against both ipv6 addresses and then succeeds against the ipv4. ie it works and squid doesn't. BTW the same squid server is currently fine with ipv6 clients talking to it and it talking over ipv6 to Internet hosts like google.com <http://google.com>  - ie this is an ipv6 outage on one Internet host where it's ipv4 is still working.

 

This doesn't seem like a negative_dns_ttl setting issue, it seems like squid just tries one address on a multiple-IP DNS record and stops trying? I even got tcpdump up and can see that when I do a "shift-reload" on the webpage, squid only sends a few SYN packets to the same non-working IPv6 address - it doesn't even try the other 3 IPs?

 

I also checked squidcachemgr.cgi and the DNS record isn't even cached in "FQDN Cache Stats and Contents", which I guess is consistent with it's opinion that it's not working.


 

Any ideas what's going on there? thanks!

 

-- 

Cheers

 

Jason Haar

Information Security Manager, Trimble Navigation Ltd.

Phone: +1 408 481 8171

PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220221/c5c44926/attachment.htm>

From christos at chtsanti.net  Mon Feb 21 09:40:40 2022
From: christos at chtsanti.net (Christos Tsantilas)
Date: Mon, 21 Feb 2022 11:40:40 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
Message-ID: <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>

Hi Ben,

When HTTP/2 is used, requests for two different domains may served using 
the same TLS connection if both domains are served from the same remote 
server and use the same TLS certificate.
There is a description here:
    https://daniel.haxx.se/blog/2016/08/18/http2-connection-coalescing/

And a similar problem report here:
    https://bugs.chromium.org/p/chromium/issues/detail?id=1176673

Regards,
    Christos


On 14/2/22 3:49 ?.?., Ben Goz wrote:
> By the help of God.
> 
> Hi,
> Ny squid version is 4.15, using it on tproxy configuration.
> 
> I'm using ssl bump to intercept https connection, but I want to splice 
> several domains.
> I have a problem that when I'm splicing some google domains eg. 
> youtube.com <http://youtube.com> then
> gmail.com <http://gmail.com> domain also spliced.
> 
> I know that it is very common for google servers to host multiple 
> domains on single?server.
> And I suspect that when I'm splicing for example youtube.com 
> <http://youtube.com> it'll also splices google.com <http://google.com>.
> 
>  ?Here are my squid configurations for the ssl bump:
> 
> https_port xxxx ssl-bump tproxy generate-host-certificates=on 
> options=ALL dynamic_cert_mem_cache_size=4MB 
> cert=/usr/local/squid/etc/ssl_cert/myCA.pem 
> dhparams=/usr/local/squid/etc/dhparam.pem sslflags=NO_DEFAULT_CA
> 
> acl DiscoverSNIHost at_step SslBump1
> 
> acl NoSSLIntercept ssl::server_name ?"/usr/local/squid/etc/url-no-bump"
> acl NoSSLInterceptRegexp ssl::server_name_regex -i 
> "/usr/local/squid/etc/url-no-bump-regexp"
> ssl_bump splice NoSSLInterceptRegexp_always
> ssl_bump splice NoSSLIntercept
> ssl_bump splice NoSSLInterceptRegexp
> ssl_bump peek DiscoverSNIHost
> ssl_bump bump all
> 
> 


From rousskov at measurement-factory.com  Mon Feb 21 14:16:50 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 Feb 2022 09:16:50 -0500
Subject: [squid-users] squid-5.4 blocking on ipv6 outage
In-Reply-To: <CAFChrg+bHtamN1gtxaDnNLnWvtgeOFLhYQ9yXvixxNqRc9YihA@mail.gmail.com>
References: <CAFChrg+bHtamN1gtxaDnNLnWvtgeOFLhYQ9yXvixxNqRc9YihA@mail.gmail.com>
Message-ID: <4f905581-9dee-fbd1-a1b1-84f1df4e5792@measurement-factory.com>

On 2/20/22 20:43, Jason Haar wrote:

> I've noticed that the Internet ipv6 is not quite as reliable as ipv4, in 
> that squid?reports it cannot connect to web servers with an ipv6 error 
> when the web server is still available over ipv4.
> 
> eg right now one of our Internet-based web apps (which has 2 ipv6 and 2 
> ipv4 IP addresses mapped to it's DNS name) is not responding over ipv6 
> for some reason (I dunno - not involved myself) - but is working fine 
> over ipv4. Squid-5.4 is erroring out - saying that it cannot connect to 
> the first ipv6 address with a "no route to host" error. But if I use 
> good-ol' telnet to the DNS name, telnet shows it trying-and-failing 
> against both ipv6 addresses and then succeeds against the ipv4. ie it 
> works and squid doesn't. BTW the same squid server is currently fine 
> with ipv6 clients talking to it and it talking over ipv6 to Internet 
> hosts like google.com <http://google.com> - ie this is an ipv6 outage on 
> one Internet host where it's ipv4 is still working.
> 
> This doesn't seem like a negative_dns_ttl setting issue, it seems like 
> squid?just tries one address on a multiple-IP DNS record and stops 
> trying? I even got tcpdump up and can see that when I do a 
> "shift-reload" on the webpage, squid only sends a few SYN packets to the 
> same non-working IPv6 address - it doesn't even try the other 3 IPs?
> 
> I also checked squidcachemgr.cgi and the DNS record isn't even cached in 
> "FQDN Cache Stats and Contents", which I guess is consistent with it's 
> opinion that it's not working.
> 
> Any ideas what's going on there? thanks!

Squid is supposed to send both A and AAAA DNS queries for the uncached 
domain and then try the first IP it can DNS-resolve and TCP-connect to. 
If that winning destination does not work at HTTP level, then Squid may, 
in some cases, try other destinations. There are lots of variables and 
nuances related to the associated Happy Eyeballs and reforwarding 
algorithms. It is impossible to say for sure what is going on in your 
specific case without more information.

Your best bet may be to share an ALL,9 cache.log that reproduces the 
problem using a single isolated test transaction:

https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction


HTH,

Alex.


From robertkwild at gmail.com  Mon Feb 21 16:41:53 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Mon, 21 Feb 2022 16:41:53 +0000
Subject: [squid-users] squid proxy really slow for web requests
Message-ID: <CAGU_CiJNG6aFE7TwuvcfSS_NH6ziX4uPfuX_quh8-Yg=Q_e6zQ@mail.gmail.com>

hi all,

today my squid responding to web requests from different clients is really
slow

for example when i go on firefox/chrome and open multiple tabs to different
websites, it normally shows the "error url page" as ive denied all websites
apart from some

and some of the websites takes way to long i get "the connection has timed
out"

on my squid server im running htop and pinging google and both seem fine

anything else what it could be

thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220221/efa4b0fa/attachment.htm>

From jason_haar at trimble.com  Mon Feb 21 19:21:42 2022
From: jason_haar at trimble.com (Jason Haar)
Date: Tue, 22 Feb 2022 08:21:42 +1300
Subject: [squid-users] squid-5.4 blocking on ipv6 outage
In-Reply-To: <4f905581-9dee-fbd1-a1b1-84f1df4e5792@measurement-factory.com>
References: <CAFChrg+bHtamN1gtxaDnNLnWvtgeOFLhYQ9yXvixxNqRc9YihA@mail.gmail.com>
 <4f905581-9dee-fbd1-a1b1-84f1df4e5792@measurement-factory.com>
Message-ID: <CAFChrgJBRFt3TcTx0OK3QjoMV=cq1nBiCQEe9L5oeGbvOrmzhA@mail.gmail.com>

Well this was a wild ride, I actually tracked the problem back to....
dns64/nat64!!!!!!!!!

What I discovered is that the affected webserver didn't actually have ipv6
- it only had 2 ipv4 addresses. But something in my DNS-tree (I'm
suspecting the local systemd-resolve, but can't actually find any direct
evidence) had whacked fake  DNS64/NAT64 records for each of them. I've
never seen them before so didn't realise "64:ff9b:XXXXXXXX" was a "special"
IPv6 range. I directly queried our upstream DNS recursive name server and
it didn't have those IPv6 records - but the local systemd-resolve would not
give them up. So I down/up-ed the interface (resetting systemd-resolve) and
the problem disappeared.

This new information really doesn't change the nature of the question, but
I'm afraid the problem is now resolved (for the moment) so debugging won't
catch it. If it happens again (I have never seen this before) I'll be sure
to do the debugging thang.

On Tue, Feb 22, 2022 at 3:16 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2/20/22 20:43, Jason Haar wrote:
>
> > I've noticed that the Internet ipv6 is not quite as reliable as ipv4, in
> > that squid reports it cannot connect to web servers with an ipv6 error
> > when the web server is still available over ipv4.
> >
> > eg right now one of our Internet-based web apps (which has 2 ipv6 and 2
> > ipv4 IP addresses mapped to it's DNS name) is not responding over ipv6
> > for some reason (I dunno - not involved myself) - but is working fine
> > over ipv4. Squid-5.4 is erroring out - saying that it cannot connect to
> > the first ipv6 address with a "no route to host" error. But if I use
> > good-ol' telnet to the DNS name, telnet shows it trying-and-failing
> > against both ipv6 addresses and then succeeds against the ipv4. ie it
> > works and squid doesn't. BTW the same squid server is currently fine
> > with ipv6 clients talking to it and it talking over ipv6 to Internet
> > hosts like google.com <http://google.com> - ie this is an ipv6 outage
> on
> > one Internet host where it's ipv4 is still working.
> >
> > This doesn't seem like a negative_dns_ttl setting issue, it seems like
> > squid just tries one address on a multiple-IP DNS record and stops
> > trying? I even got tcpdump up and can see that when I do a
> > "shift-reload" on the webpage, squid only sends a few SYN packets to the
> > same non-working IPv6 address - it doesn't even try the other 3 IPs?
> >
> > I also checked squidcachemgr.cgi and the DNS record isn't even cached in
> > "FQDN Cache Stats and Contents", which I guess is consistent with it's
> > opinion that it's not working.
> >
> > Any ideas what's going on there? thanks!
>
> Squid is supposed to send both A and AAAA DNS queries for the uncached
> domain and then try the first IP it can DNS-resolve and TCP-connect to.
> If that winning destination does not work at HTTP level, then Squid may,
> in some cases, try other destinations. There are lots of variables and
> nuances related to the associated Happy Eyeballs and reforwarding
> algorithms. It is impossible to say for sure what is going on in your
> specific case without more information.
>
> Your best bet may be to share an ALL,9 cache.log that reproduces the
> problem using a single isolated test transaction:
>
>
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>
>
> HTH,
>
> Alex.
>


-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/7d9964f1/attachment.htm>

From ngtech1ltd at gmail.com  Tue Feb 22 05:35:27 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 22 Feb 2022 07:35:27 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
 <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>
Message-ID: <002701d827ae$0026b880$00742980$@gmail.com>

Thanks Christos,

I was aware of such things but haven't seen such a case.
Is there any way to "reproduce" this?
I believe it should be documented in the wiki.

Thanks,

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Christos Tsantilas
Sent: Monday, February 21, 2022 11:41
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Splice certain SNIs which served by the same IP

Hi Ben,

When HTTP/2 is used, requests for two different domains may served using 
the same TLS connection if both domains are served from the same remote 
server and use the same TLS certificate.
There is a description here:
    https://daniel.haxx.se/blog/2016/08/18/http2-connection-coalescing/

And a similar problem report here:
    https://bugs.chromium.org/p/chromium/issues/detail?id=1176673

Regards,
    Christos


On 14/2/22 3:49 ?.?., Ben Goz wrote:
> By the help of God.
> 
> Hi,
> Ny squid version is 4.15, using it on tproxy configuration.
> 
> I'm using ssl bump to intercept https connection, but I want to splice 
> several domains.
> I have a problem that when I'm splicing some google domains eg. 
> youtube.com <http://youtube.com> then
> gmail.com <http://gmail.com> domain also spliced.
> 
> I know that it is very common for google servers to host multiple 
> domains on single server.
> And I suspect that when I'm splicing for example youtube.com 
> <http://youtube.com> it'll also splices google.com <http://google.com>.
> 
>   Here are my squid configurations for the ssl bump:
> 
> https_port xxxx ssl-bump tproxy generate-host-certificates=on 
> options=ALL dynamic_cert_mem_cache_size=4MB 
> cert=/usr/local/squid/etc/ssl_cert/myCA.pem 
> dhparams=/usr/local/squid/etc/dhparam.pem sslflags=NO_DEFAULT_CA
> 
> acl DiscoverSNIHost at_step SslBump1
> 
> acl NoSSLIntercept ssl::server_name  "/usr/local/squid/etc/url-no-bump"
> acl NoSSLInterceptRegexp ssl::server_name_regex -i 
> "/usr/local/squid/etc/url-no-bump-regexp"
> ssl_bump splice NoSSLInterceptRegexp_always
> ssl_bump splice NoSSLIntercept
> ssl_bump splice NoSSLInterceptRegexp
> ssl_bump peek DiscoverSNIHost
> ssl_bump bump all
> 
> 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Tue Feb 22 05:37:23 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 22 Feb 2022 07:37:23 +0200
Subject: [squid-users] squid proxy really slow for web requests
In-Reply-To: <CAGU_CiJNG6aFE7TwuvcfSS_NH6ziX4uPfuX_quh8-Yg=Q_e6zQ@mail.gmail.com>
References: <CAGU_CiJNG6aFE7TwuvcfSS_NH6ziX4uPfuX_quh8-Yg=Q_e6zQ@mail.gmail.com>
Message-ID: <002801d827ae$4543e5a0$cfcbb0e0$@gmail.com>

Hey Rob,

 

I really didn?t understood the situation?

Since we are in 2022 I believe a screen capture(video/gif) for the scenario would be useful.

You can use the next tool to capture the scenario:

https://getsharex.com/

 

(if you are using windows)

 

Thanks,

Eliezer 

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of robert k Wild
Sent: Monday, February 21, 2022 18:42
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] squid proxy really slow for web requests

 

hi all,

 

today my squid responding to web requests from different clients is really slow

 

for example when i go on firefox/chrome and open multiple tabs to different websites, it normally shows the "error url page" as ive denied all websites apart from some

 

and some of the websites takes way to long i get "the connection has timed out"

 

on my squid server im running htop and pinging google and both seem fine

 

anything else what it could be

 

thanks,

rob


-- 

Regards, 

Robert K Wild.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/416e41c7/attachment.htm>

From robertkwild at gmail.com  Tue Feb 22 08:38:15 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 22 Feb 2022 08:38:15 +0000
Subject: [squid-users] squid proxy really slow for web requests
In-Reply-To: <002801d827ae$4543e5a0$cfcbb0e0$@gmail.com>
References: <CAGU_CiJNG6aFE7TwuvcfSS_NH6ziX4uPfuX_quh8-Yg=Q_e6zQ@mail.gmail.com>
 <002801d827ae$4543e5a0$cfcbb0e0$@gmail.com>
Message-ID: <CAGU_Ci+sQw9C07yzHPKvp1uzYLuU7w=TB0u7rtwBXBZbFiNs3g@mail.gmail.com>

Hi Eliezer,

Thanks for the reply, in the end I had to restart our firewall, as our
squid server is on the dmz and squid users/clients accessing the squid
server are on the lan, so they have to go through the firewall

Once restarted I could access the webpages and I didn't get the timeout
error any more

Thanks,
Rob

On Tue, 22 Feb 2022, 05:37 Eliezer Croitoru, <ngtech1ltd at gmail.com> wrote:

> Hey Rob,
>
>
>
> I really didn?t understood the situation?
>
> Since we are in 2022 I believe a screen capture(video/gif) for the
> scenario would be useful.
>
> You can use the next tool to capture the scenario:
>
> https://getsharex.com/
>
>
>
> (if you are using windows)
>
>
>
> Thanks,
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *robert k Wild
> *Sent:* Monday, February 21, 2022 18:42
> *To:* Squid Users <squid-users at lists.squid-cache.org>
> *Subject:* [squid-users] squid proxy really slow for web requests
>
>
>
> hi all,
>
>
>
> today my squid responding to web requests from different clients is really
> slow
>
>
>
> for example when i go on firefox/chrome and open multiple tabs to
> different websites, it normally shows the "error url page" as ive denied
> all websites apart from some
>
>
>
> and some of the websites takes way to long i get "the connection has timed
> out"
>
>
>
> on my squid server im running htop and pinging google and both seem fine
>
>
>
> anything else what it could be
>
>
>
> thanks,
>
> rob
>
>
> --
>
> Regards,
>
> Robert K Wild.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/e97cb5ca/attachment.htm>

From ngtech1ltd at gmail.com  Tue Feb 22 09:48:52 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 22 Feb 2022 11:48:52 +0200
Subject: [squid-users] squid proxy really slow for web requests
In-Reply-To: <CAGU_Ci+sQw9C07yzHPKvp1uzYLuU7w=TB0u7rtwBXBZbFiNs3g@mail.gmail.com>
References: <CAGU_CiJNG6aFE7TwuvcfSS_NH6ziX4uPfuX_quh8-Yg=Q_e6zQ@mail.gmail.com>
 <002801d827ae$4543e5a0$cfcbb0e0$@gmail.com>
 <CAGU_Ci+sQw9C07yzHPKvp1uzYLuU7w=TB0u7rtwBXBZbFiNs3g@mail.gmail.com>
Message-ID: <000001d827d1$672c8a50$35859ef0$@gmail.com>

Thanks Rob,

 

A good catch.

It?s a very hard one to find.

 

All The Bests,

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: robert k Wild <robertkwild at gmail.com> 
Sent: Tuesday, February 22, 2022 10:38
To: Eliezer Croitoru <ngtech1ltd at gmail.com>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] squid proxy really slow for web requests

 

Hi Eliezer,

 

Thanks for the reply, in the end I had to restart our firewall, as our squid server is on the dmz and squid users/clients accessing the squid server are on the lan, so they have to go through the firewall 

 

Once restarted I could access the webpages and I didn't get the timeout error any more

 

Thanks, 

Rob

 

On Tue, 22 Feb 2022, 05:37 Eliezer Croitoru, <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > wrote:

Hey Rob,

 

I really didn?t understood the situation?

Since we are in 2022 I believe a screen capture(video/gif) for the scenario would be useful.

You can use the next tool to capture the scenario:

https://getsharex.com/

 

(if you are using windows)

 

Thanks,

Eliezer 

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of robert k Wild
Sent: Monday, February 21, 2022 18:42
To: Squid Users <squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> >
Subject: [squid-users] squid proxy really slow for web requests

 

hi all,

 

today my squid responding to web requests from different clients is really slow

 

for example when i go on firefox/chrome and open multiple tabs to different websites, it normally shows the "error url page" as ive denied all websites apart from some

 

and some of the websites takes way to long i get "the connection has timed out"

 

on my squid server im running htop and pinging google and both seem fine

 

anything else what it could be

 

thanks,

rob


-- 

Regards, 

Robert K Wild.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/77f730c7/attachment.htm>

From robertkwild at gmail.com  Tue Feb 22 09:51:16 2022
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 22 Feb 2022 09:51:16 +0000
Subject: [squid-users] squid proxy really slow for web requests
In-Reply-To: <000001d827d1$672c8a50$35859ef0$@gmail.com>
References: <CAGU_CiJNG6aFE7TwuvcfSS_NH6ziX4uPfuX_quh8-Yg=Q_e6zQ@mail.gmail.com>
 <002801d827ae$4543e5a0$cfcbb0e0$@gmail.com>
 <CAGU_Ci+sQw9C07yzHPKvp1uzYLuU7w=TB0u7rtwBXBZbFiNs3g@mail.gmail.com>
 <000001d827d1$672c8a50$35859ef0$@gmail.com>
Message-ID: <CAGU_CiKbtpD2gqatM8ZTiNiFjWJaAfhEgr1xTDyzUstpYU=dtQ@mail.gmail.com>

No problem, glad I can be of small help to this awesome project!!!!

On Tue, 22 Feb 2022, 09:48 Eliezer Croitoru, <ngtech1ltd at gmail.com> wrote:

> Thanks Rob,
>
>
>
> A good catch.
>
> It?s a very hard one to find.
>
>
>
> All The Bests,
>
>
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
>
> *From:* robert k Wild <robertkwild at gmail.com>
> *Sent:* Tuesday, February 22, 2022 10:38
> *To:* Eliezer Croitoru <ngtech1ltd at gmail.com>
> *Cc:* Squid Users <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] squid proxy really slow for web requests
>
>
>
> Hi Eliezer,
>
>
>
> Thanks for the reply, in the end I had to restart our firewall, as our
> squid server is on the dmz and squid users/clients accessing the squid
> server are on the lan, so they have to go through the firewall
>
>
>
> Once restarted I could access the webpages and I didn't get the timeout
> error any more
>
>
>
> Thanks,
>
> Rob
>
>
>
> On Tue, 22 Feb 2022, 05:37 Eliezer Croitoru, <ngtech1ltd at gmail.com> wrote:
>
> Hey Rob,
>
>
>
> I really didn?t understood the situation?
>
> Since we are in 2022 I believe a screen capture(video/gif) for the
> scenario would be useful.
>
> You can use the next tool to capture the scenario:
>
> https://getsharex.com/
>
>
>
> (if you are using windows)
>
>
>
> Thanks,
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *robert k Wild
> *Sent:* Monday, February 21, 2022 18:42
> *To:* Squid Users <squid-users at lists.squid-cache.org>
> *Subject:* [squid-users] squid proxy really slow for web requests
>
>
>
> hi all,
>
>
>
> today my squid responding to web requests from different clients is really
> slow
>
>
>
> for example when i go on firefox/chrome and open multiple tabs to
> different websites, it normally shows the "error url page" as ive denied
> all websites apart from some
>
>
>
> and some of the websites takes way to long i get "the connection has timed
> out"
>
>
>
> on my squid server im running htop and pinging google and both seem fine
>
>
>
> anything else what it could be
>
>
>
> thanks,
>
> rob
>
>
> --
>
> Regards,
>
> Robert K Wild.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/bf5ea820/attachment.htm>

From ben.goz87 at gmail.com  Tue Feb 22 12:05:22 2022
From: ben.goz87 at gmail.com (Ben Goz)
Date: Tue, 22 Feb 2022 14:05:22 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <002701d827ae$0026b880$00742980$@gmail.com>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
 <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>
 <002701d827ae$0026b880$00742980$@gmail.com>
Message-ID: <CADAqQfzaBHY=QF7vzrdE=MNDz2s9UmrBWoGQSxUGzSMnV3fhFw@mail.gmail.com>

By the help of God.

If I'm using the self signed certificate that I created for the ssl bump,
then the browser considers it as the same certificate for any domain I'm
connecting to?

??????? ??? ??, 22 ????? 2022 ?-7:35 ??? ?Eliezer Croitoru?? <?
ngtech1ltd at gmail.com??>:?

> Thanks Christos,
>
> I was aware of such things but haven't seen such a case.
> Is there any way to "reproduce" this?
> I believe it should be documented in the wiki.
>
> Thanks,
>
> ----
> Eliezer Croitoru
> NgTech, Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
>
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Christos Tsantilas
> Sent: Monday, February 21, 2022 11:41
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Splice certain SNIs which served by the same IP
>
> Hi Ben,
>
> When HTTP/2 is used, requests for two different domains may served using
> the same TLS connection if both domains are served from the same remote
> server and use the same TLS certificate.
> There is a description here:
>     https://daniel.haxx.se/blog/2016/08/18/http2-connection-coalescing/
>
> And a similar problem report here:
>     https://bugs.chromium.org/p/chromium/issues/detail?id=1176673
>
> Regards,
>     Christos
>
>
> On 14/2/22 3:49 ?.?., Ben Goz wrote:
> > By the help of God.
> >
> > Hi,
> > Ny squid version is 4.15, using it on tproxy configuration.
> >
> > I'm using ssl bump to intercept https connection, but I want to splice
> > several domains.
> > I have a problem that when I'm splicing some google domains eg.
> > youtube.com <http://youtube.com> then
> > gmail.com <http://gmail.com> domain also spliced.
> >
> > I know that it is very common for google servers to host multiple
> > domains on single server.
> > And I suspect that when I'm splicing for example youtube.com
> > <http://youtube.com> it'll also splices google.com <http://google.com>.
> >
> >   Here are my squid configurations for the ssl bump:
> >
> > https_port xxxx ssl-bump tproxy generate-host-certificates=on
> > options=ALL dynamic_cert_mem_cache_size=4MB
> > cert=/usr/local/squid/etc/ssl_cert/myCA.pem
> > dhparams=/usr/local/squid/etc/dhparam.pem sslflags=NO_DEFAULT_CA
> >
> > acl DiscoverSNIHost at_step SslBump1
> >
> > acl NoSSLIntercept ssl::server_name  "/usr/local/squid/etc/url-no-bump"
> > acl NoSSLInterceptRegexp ssl::server_name_regex -i
> > "/usr/local/squid/etc/url-no-bump-regexp"
> > ssl_bump splice NoSSLInterceptRegexp_always
> > ssl_bump splice NoSSLIntercept
> > ssl_bump splice NoSSLInterceptRegexp
> > ssl_bump peek DiscoverSNIHost
> > ssl_bump bump all
> >
> >
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/7a882a2e/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb 22 14:31:44 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Feb 2022 03:31:44 +1300
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <CADAqQfzaBHY=QF7vzrdE=MNDz2s9UmrBWoGQSxUGzSMnV3fhFw@mail.gmail.com>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
 <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>
 <002701d827ae$0026b880$00742980$@gmail.com>
 <CADAqQfzaBHY=QF7vzrdE=MNDz2s9UmrBWoGQSxUGzSMnV3fhFw@mail.gmail.com>
Message-ID: <6e4a8af9-5091-5c8f-5c77-15e8ee4d15d0@treenet.co.nz>

On 23/02/22 01:05, Ben Goz wrote:
> By the help of God.
> 
> If I'm using the self signed certificate that I created for the ssl 
> bump, then the browser considers it as the same certificate for any 
> domain I'm connecting to?
> 

Key thing to remember is that TLS server certificate validates the 
*server*, not the URL domain name.

HTTP/2 brings the feature of alternate server names. So once connected 
and talking, a server can tell the client a bunch of other domains that 
can be fetched from it.

Since you are using SSL-Bump "splice" to setup the connection Squid has 
no control or interaction over what the server and client tell each 
other within that connection.


HTH
Amos


From Joseph.Garbacik at netapp.com  Tue Feb 22 18:11:45 2022
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Tue, 22 Feb 2022 18:11:45 +0000
Subject: [squid-users] Squid and Certificates
Message-ID: <MN2PR06MB59514E952600593211107C73983B9@MN2PR06MB5951.namprd06.prod.outlook.com>

When the squid proxy validates a certificate of a destination, does it cache that certificate's status for a period of time or does it validate the certificate each time? Would it log when it makes calls to a CRL or OCSP server to validate the certificate or is it just part of the process? Also, does it support putting a CRL/IOCSP data in the certificate provided to the client if doing SSL intercept?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220222/7ab420ce/attachment.htm>

From ngtech1ltd at gmail.com  Tue Feb 22 19:45:28 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Tue, 22 Feb 2022 21:45:28 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <6e4a8af9-5091-5c8f-5c77-15e8ee4d15d0@treenet.co.nz>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
 <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>
 <002701d827ae$0026b880$00742980$@gmail.com>
 <CADAqQfzaBHY=QF7vzrdE=MNDz2s9UmrBWoGQSxUGzSMnV3fhFw@mail.gmail.com>
 <6e4a8af9-5091-5c8f-5c77-15e8ee4d15d0@treenet.co.nz>
Message-ID: <001b01d82824$bf5b8750$3e1295f0$@gmail.com>

Just To mention that once Squid is not splicing the connection it would have
full control in the URL level.
I do not know the scenario but I have yet to have seen a similar case and
it's probably because I am bumping
almost all connections.

Eliezer

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Amos Jeffries
Sent: Tuesday, February 22, 2022 16:32
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Splice certain SNIs which served by the same IP

On 23/02/22 01:05, Ben Goz wrote:
> By the help of God.
> 
> If I'm using the self signed certificate that I created for the ssl 
> bump, then the browser considers it as the same certificate for any 
> domain I'm connecting to?
> 

Key thing to remember is that TLS server certificate validates the 
*server*, not the URL domain name.

HTTP/2 brings the feature of alternate server names. So once connected 
and talking, a server can tell the client a bunch of other domains that 
can be fetched from it.

Since you are using SSL-Bump "splice" to setup the connection Squid has 
no control or interaction over what the server and client tell each 
other within that connection.


HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From christos at chtsanti.net  Tue Feb 22 21:43:36 2022
From: christos at chtsanti.net (Christos Tsantilas)
Date: Tue, 22 Feb 2022 23:43:36 +0200
Subject: [squid-users] Splice certain SNIs which served by the same IP
In-Reply-To: <001b01d82824$bf5b8750$3e1295f0$@gmail.com>
References: <CADAqQfx-PKEE5Z8dMXB-DJekbYMMZzCyJoVUyaaFjN3VXjprDg@mail.gmail.com>
 <2d0b6d47-e10c-13f2-737a-4abd64b59164@chtsanti.net>
 <002701d827ae$0026b880$00742980$@gmail.com>
 <CADAqQfzaBHY=QF7vzrdE=MNDz2s9UmrBWoGQSxUGzSMnV3fhFw@mail.gmail.com>
 <6e4a8af9-5091-5c8f-5c77-15e8ee4d15d0@treenet.co.nz>
 <001b01d82824$bf5b8750$3e1295f0$@gmail.com>
Message-ID: <ebe14db0-ced5-9871-3158-fbb8abf07095@chtsanti.net>

On 22/2/22 9:45 ?.?., Eliezer Croitoru wrote:
> Just To mention that once Squid is not splicing the connection it would have
> full control in the URL level.
Exactly.

For many HTTP2 sites the SNI does not provide enough info for 
splicing/bumping decision.

The google sites is one of them. You can not safely bump google.com or 
youtube.com and splice gmail.com. You have to weighing  the risks and 
probably splice all google sites including the gmail.com.


> I do not know the scenario but I have yet to have seen a similar case and
> it's probably because I am bumping
> almost all connections.

... and because squid while proxying uses HTTP/1.1 protocol not HTTP/2.

Regards,
    Christos

> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> NgTech, Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> 


From squid3 at treenet.co.nz  Wed Feb 23 08:57:14 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 Feb 2022 21:57:14 +1300
Subject: [squid-users] Squid and Certificates
In-Reply-To: <MN2PR06MB59514E952600593211107C73983B9@MN2PR06MB5951.namprd06.prod.outlook.com>
References: <MN2PR06MB59514E952600593211107C73983B9@MN2PR06MB5951.namprd06.prod.outlook.com>
Message-ID: <2dbb4e07-63dc-c45a-5698-adb6d504d172@treenet.co.nz>

On 23/02/22 07:11, Garbacik, Joe wrote:
> When the squid proxy validates a certificate of a destination, does it 
> cache that certificate's status for a period of time or does it validate 
> the certificate each time? Would it log when it makes calls to a CRL or 
> OCSP server to validate the certificate or is it just part of the 
> process?

All of your questions answers depend on the library doing that validation.

AFAIK, Squid only performs AIA lookups to find missing chain 
certificates. CRL/OSCP are part of the libraries internal validation 
process and may not involve server lookups at all.



> Also, does it support putting a CRL/IOCSP data in the 
> certificate provided to the client if doing SSL intercept?
> 

The certificate sent to the client mimics the real server certificate 
fields when available. It is an intentional security design *not* to 
inject details, not even to fix brokenness.


The SSL-Bump signing cert is used as-is for the chain. You can place any 
valid certificate fields you want when it is created.

Amos


From andreas.weigel at securepoint.de  Wed Feb 23 23:05:58 2022
From: andreas.weigel at securepoint.de (Andreas Weigel)
Date: Wed, 23 Feb 2022 23:05:58 +0000
Subject: [squid-users] getsockopt failures,
 although direct access to intercept ports is blocked
Message-ID: <20220223230558.Horde.4oLy24HP_6s1SU7RO0yA1L_@webmail.intern.securepoint.de>

Hi everyone,

I had the following issue with Squid in Transparent Mode (and SSL  
Interception in mode splice). It is working as expected, however after  
multiple long-running (talking about several seconds) anti-virus  
ecap-Processes have finished, I *sometimes* get the following in the  
log:

2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(224)  
doAccept: New connection on FD 21
2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(312)  
acceptNext: connection on local=[::]:2412 remote=[::] FD 21 flags=41
2022/02/23 14:56:40.668 kid1| 89,5| src/ip/Intercept.cc(405) Lookup:  
address BEGIN: me/client= 192.168.180.1:2412, destination/me=  
192.168.180.10:48582
2022/02/23 14:56:40.668 kid1| ERROR: NF getsockopt(ORIGINAL_DST)  
failed on local=192.168.180.1:2412 remote=192.168.180.10:48582 FD 37  
flags=33: (2) No such file or directory
2022/02/23 14:56:40.669 kid1| 89,9| src/ip/Intercept.cc(151)  
NetfilterInterception: address: local=192.168.180.1:2412  
remote=192.168.180.10:48582 FD 37 flags=33
2022/02/23 14:56:40.669 kid1| ERROR: NAT/TPROXY lookup failed to  
locate original IPs on local=192.168.180.1:2412  
remote=192.168.180.10:48582 FD 37 flags=33
2022/02/23 14:56:40.669 kid1| 5,5| src/comm/TcpAcceptor.cc(287)  
acceptOne: non-recoverable error: FD 21, [::] [ job2] handler  
Subscription: 0x55edac3d08d0*1

Sometimes, this only appears on on of the two interception ports,  
sometimes on both. After that, the squid worker does not poll the  
intercept listen port any longer, i.e. stops working.  The firewall is  
configured to drop incoming packets to port 2411/2412 according to  
https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect.

iptables-save
*raw
:PREROUTING ACCEPT [68778:58967535]
:OUTPUT ACCEPT [109753:87606960]
-A PREROUTING -p tcp -m tcp --dport 2411 -j NFLOG --nflog-prefix   
"DROP: proxy intercept port" --nflog-group 13
-A PREROUTING -p tcp -m tcp --dport 2411 -j DROP
-A PREROUTING -p tcp -m tcp --dport 2412 -j NFLOG --nflog-prefix   
"DROP: proxy intercept port" --nflog-group 13
-A PREROUTING -p tcp -m tcp --dport 2412 -j DROP
...
*nat
*nat
:PREROUTING ACCEPT [247:23967]
:INPUT ACCEPT [102:8035]
:OUTPUT ACCEPT [781:59761]
:POSTROUTING ACCEPT [781:59761]
-A PREROUTING -s 192.168.180.10/32 -i eth2 -p tcp -m tcp --dport 80 -j  
REDIRECT --to-ports 2411
-A PREROUTING -s 192.168.180.10/32 -i eth2 -p tcp -m tcp --dport 443  
-j REDIRECT --to-ports 2412
...
-A POSTROUTING -s 192.168.180.10/32 -o eth0 -j MASQUERADE

ip6tables-save
*raw
:PREROUTING ACCEPT [48:3865]
:OUTPUT ACCEPT [9:697]
-A PREROUTING -p tcp -m tcp --dport 2411 -j NFLOG --nflog-prefix   
"DROP: proxy intercept port" --nflog-group 13
-A PREROUTING -p tcp -m tcp --dport 2411 -j DROP
-A PREROUTING -p tcp -m tcp --dport 2412 -j NFLOG --nflog-prefix   
"DROP: proxy intercept port" --nflog-group 13
-A PREROUTING -p tcp -m tcp --dport 2412 -j DROP

I confirmed that the rules are working by sending some requests to the  
ports manually. So, I really do not understand from where these  
packets arrive at the squid. Any pointers or ideas on what I am  
missing or what to check (e.g., debugging sections) would be highly  
appreciated. Looking at the code I only see a rather straightforward  
path from accept to getsockopt.

Squid intercept config:

http_port 2411 connection-auth=off intercept
https_port 2412 connection-auth=off ssl-bump capath=/etc/ssl/certs  
generate-host-certificates=on tls-default-ca=off  
cert=/etc/ssl/proxy_interception.pem intercept

Test setup looks like this:
client 192.168.180.10 --- 192.168.180.1 at eth2 <router running squid>  
192.168.175.1 at eth1 --- 192.168.175.10 destination

curl  
http://192.168.175.10/linux-headers-5.4.0-100_5.4.0-100.113_all.deb -o  
test.deb


Kind regards,
Andreas




From dave at killthe.net  Thu Feb 24 02:26:11 2022
From: dave at killthe.net (Dave Blanchard)
Date: Wed, 23 Feb 2022 20:26:11 -0600
Subject: [squid-users] Trying to set up SSL cache
Message-ID: <20220223202611.5852047e8ec14fc337435e83@killthe.net>

Hello, I'm trying to configure Squid as a HTTPS cache on my local computer, using ssl-bump. I've got it working as a basic proxy, but the traffic seems to just be tunneling through and not being cached. My web browser shows the site's actual certificate, rather than the locally generated self-signed certificate, which I want it to see. I have followed every tutorial I can find and none of them are helpful in figuring out what the hell is going on here. Here is what my config file looks like:

[...]

http_port 3128 ssl-bump \
               generate-host-certificates=on \
               dynamic_cert_mem_cache_size=32MB \
               cert=/path/to/self-signed.pem \
               key=/path/to/self-signed.pem

sslcrtd_program /usr/libexec/security_file_certgen -s /path/to/ssl-database -M 32MB

ssl_bump peek all
ssl_bump bump all
ssl_bump splice localhost

[...]

Otherwise, it's pretty much just the default config. The only thing that seems to halfway work is removing the line:

http_access deny CONNECT !SSL_ports

and changing to:

http_access deny CONNECT

With that change, an older Chromium just hangs trying to load the page, saying "Processing request." On a WebKit-based browser, I get a Squid 'Access Denied' error page. Another WebKit browser complains about the certificate, but when I tell it to continue anyway, it gives the same 'Access Denied' page. A newer Chromium stops right away with an untrusted SSL certificate error, and the details look like it's getting the self-signed certificate, as expected.

I've been battling this for hours and I'm at my wit's end. What am I doing wrong here? Thanks for any help.

-- 
Dave Blanchard <dave at killthe.net>


From dave at killthe.net  Thu Feb 24 03:09:03 2022
From: dave at killthe.net (Dave Blanchard)
Date: Wed, 23 Feb 2022 21:09:03 -0600
Subject: [squid-users] Trying to set up SSL cache - solved!
Message-ID: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>

OK--I solved the problem by removing the "ssl_bump bump all" line. Works fine now.

Damn, this proxy is a TOTAL PAIN IN THE ASS!! to configure. It seems like 90% of the tutorials out there are junk, largely because things keep changing from version to version, obsoleting them. That having been said, it does have a lot of features and when it's eventually configured right it does work, so there's that. It's a lot like CUPS, in that way, or sendmail.

Please add more concrete examples to the Wiki reference pages! Thank you.

-- 
Dave Blanchard <dave at killthe.net>


From ngtech1ltd at gmail.com  Thu Feb 24 07:38:43 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Thu, 24 Feb 2022 09:38:43 +0200
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
Message-ID: <003a01d82951$8d91ebf0$a8b5c3d0$@gmail.com>

Hey Dave,

Lots of tutorials and documentation are out there but ... or out of sync
or..
not good from 0.

What OS are you running squid ontop?

Eliezer

* We are trying to give good examples.

----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Dave Blanchard
Sent: Thursday, February 24, 2022 05:09
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Trying to set up SSL cache - solved!

OK--I solved the problem by removing the "ssl_bump bump all" line. Works
fine now.

Damn, this proxy is a TOTAL PAIN IN THE ASS!! to configure. It seems like
90% of the tutorials out there are junk, largely because things keep
changing from version to version, obsoleting them. That having been said, it
does have a lot of features and when it's eventually configured right it
does work, so there's that. It's a lot like CUPS, in that way, or sendmail.

Please add more concrete examples to the Wiki reference pages! Thank you.

-- 
Dave Blanchard <dave at killthe.net>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From uhlar at fantomas.sk  Thu Feb 24 09:40:24 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 24 Feb 2022 10:40:24 +0100
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
Message-ID: <YhdSiA0MuvmB/Zjn@fantomas.sk>

On 23.02.22 21:09, Dave Blanchard wrote:
>OK--I solved the problem by removing the "ssl_bump bump all" line. Works fine now.
>
>Damn, this proxy is a TOTAL PAIN IN THE ASS!!  to configure.

configuring proxy is very easy, bumping SSL is not.

Since SSL is designed to encrypt traffic between ende - client (browser) and 
server, you need to effectively do man-in-the-middle attack on proxied 
connection.

You need to create certificate authority, install it in your browser (OS), 
insert your certificate on squid and hope that your browser won't reject 
your authority because of DANE DNS records telling browser that remote 
server's certificate should be only signed by their certificate 
authority, not by yours.

Especially when browser uses DNS-over-HTTP to avoid your DNS server that is 
able to provide incorrect data to it.

> It seems like 90% of the tutorials out there are junk, largely because 
> things keep changing from version to version, obsoleting them.

unfortunately, this exactly happens.

>  That having been said, 
> it does have a lot of features and when it's eventually configured right 
> it does work, so there's that.  It's a lot like CUPS, in that way, or 
> sendmail.
>
>Please add more concrete examples to the Wiki reference pages! Thank you.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I intend to live forever - so far so good.


From dave at killthe.net  Thu Feb 24 09:44:13 2022
From: dave at killthe.net (Dave Blanchard)
Date: Thu, 24 Feb 2022 03:44:13 -0600
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <003a01d82951$8d91ebf0$a8b5c3d0$@gmail.com>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
 <003a01d82951$8d91ebf0$a8b5c3d0$@gmail.com>
Message-ID: <20220224034413.52e120cd7d6a210b4aa88b14@killthe.net>

Hi Eliezer, this is on a custom Linux distro. I was using Traffic Server after the failed initial foray into Squid-land, but it also wasn't caching SSL, and it's even more poorly documented. Also, annoyingly, TS was updating its on-disk stat file every five seconds, slowly but steadily wearing out my SSD drive. I tried to patch the source code to fix the problem, and found the code is too cruddy to deal with. It's the sort of code base where the obvious fix doesn't work because the code is doing the same thing in like 5 different random places, with half the relevant code being dead and unused. Found some really dumb shit, like a command line option to enable debugging, that doesn't work--the code doesn't even use the variable it sets! So in disgust I came back to Squid, and am glad to finally have this thing working right. My network connection is only 128kbit, but now quite usable with the caching proxy and other optimizations. 

Dave


On Thu, 24 Feb 2022 09:38:43 +0200
"Eliezer Croitoru" <ngtech1ltd at gmail.com> wrote:

> Hey Dave,
> 
> Lots of tutorials and documentation are out there but ... or out of sync
> or..
> not good from 0.
> 
> What OS are you running squid ontop?
> 
> Eliezer
> 
> * We are trying to give good examples.
> 
> ----
> Eliezer Croitoru
> NgTech, Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
> Dave Blanchard
> Sent: Thursday, February 24, 2022 05:09
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Trying to set up SSL cache - solved!
> 
> OK--I solved the problem by removing the "ssl_bump bump all" line. Works
> fine now.
> 
> Damn, this proxy is a TOTAL PAIN IN THE ASS!! to configure. It seems like
> 90% of the tutorials out there are junk, largely because things keep
> changing from version to version, obsoleting them. That having been said, it
> does have a lot of features and when it's eventually configured right it
> does work, so there's that. It's a lot like CUPS, in that way, or sendmail.
> 
> Please add more concrete examples to the Wiki reference pages! Thank you.
> 
> -- 
> Dave Blanchard <dave at killthe.net>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


-- 
Dave Blanchard <dave at killthe.net>


From felipeapolanco at gmail.com  Thu Feb 24 15:03:05 2022
From: felipeapolanco at gmail.com (Felipe Polanco)
Date: Thu, 24 Feb 2022 11:03:05 -0400
Subject: [squid-users] Squid Question regarding tcp handshake
Message-ID: <CADcj3=77oZ_mSPAkL2KYujLZPOo-HgTJZWaoVZT8AH4WQExgjw@mail.gmail.com>

Hi,

A question Regarding TCP handshake.

Does squid first complete the tcp handshake on its users and then a second
handshake on the destination IP or as soon as it receives the TCP SYN flag
it does the same with the destination.

This is for transparent mode.

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220224/5597634f/attachment.htm>

From rousskov at measurement-factory.com  Thu Feb 24 16:08:48 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Feb 2022 11:08:48 -0500
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
Message-ID: <ff2a9d45-f6d8-e644-08e6-4d2c5367aec2@measurement-factory.com>

On 2/23/22 22:09, Dave Blanchard wrote:
> OK--I solved the problem by removing the "ssl_bump bump all" line.
> Works fine now.

> Damn, this proxy is a TOTAL PAIN IN THE ASS!! to configure. It seems
> like 90% of the tutorials out there are junk, largely because things
> keep changing from version to version, obsoleting them.

This email thread is a good example. The original ssl_bump config shared 
in the beginning of the thread did not make sense at all. Squid bugs 
notwithstanding, the implied second config (the one with "ssl_bump bump 
all" line removed) should not cache any HTTPS transactions either. 
However, folks will read this thread, copy the original config, maybe 
remove the "bump" line, and expect things to "work" because the 
"problem" was "solved" for somebody else.


> Please add more concrete examples to the Wiki reference pages!

IMHO, SslBump is too nuanced/complex to be able to reuse simple 
configurations without understanding their meaning. We should improve 
documentation a lot, but it takes a village to do that, and "more 
examples" is hardly the answer.

Alex.


From gtaylor at tnetconsulting.net  Thu Feb 24 16:41:13 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Thu, 24 Feb 2022 09:41:13 -0700
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <ff2a9d45-f6d8-e644-08e6-4d2c5367aec2@measurement-factory.com>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
 <ff2a9d45-f6d8-e644-08e6-4d2c5367aec2@measurement-factory.com>
Message-ID: <cb3fcb2e-27f7-98a7-42b0-26c3f544cff6@spamtrap.tnetconsulting.net>

On 2/24/22 9:08 AM, Alex Rousskov wrote:
> "more examples" is hardly the answer.

I believe that "more examples" can be additional data that someone can 
derive information ~> knowledge from.

Or said another way, it's a step in the proper direction.

I think one of the most important things to have with examples is meta 
information about the configuration (both server and client side) that 
it is being used and the version.

E.g. Clients are configured to talk directly to Squid (vX.Y) proxy 
server on port 80 / 443 wherein SSL "bumping" a la. monkey in the middle 
for caching purposes on a low bandwidth / metered connection.

/Just/ having configuration examples doesn't do much in and of itself 
without knowing the context the examples are from.  If anything /just/ 
examples is worse in that people have no context and are left with 
trying to identify the version, what was being and intended to be done, 
and then translating that to the version & configuration they are 
working with.  This is something that's non-trivial for seasoned (Squid) 
administrators and can be near impossible to new (Squid) administrators.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220224/4826442a/attachment.bin>

From dave at killthe.net  Thu Feb 24 18:24:35 2022
From: dave at killthe.net (Dave Blanchard)
Date: Thu, 24 Feb 2022 12:24:35 -0600
Subject: [squid-users] Trying to set up SSL cache - solved!
Message-ID: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>

On Thu, 24 Feb 2022 11:08:48 -0500
Alex Rousskov <rousskov at measurement-factory.com> wrote:

> On 2/23/22 22:09, Dave Blanchard wrote:
> > OK--I solved the problem by removing the "ssl_bump bump all" line.
> > Works fine now.
> 
> > Damn, this proxy is a TOTAL PAIN IN THE ASS!! to configure. It seems
> > like 90% of the tutorials out there are junk, largely because things
> > keep changing from version to version, obsoleting them.
> 
> This email thread is a good example. The original ssl_bump config shared 
> in the beginning of the thread did not make sense at all. Squid bugs 
> notwithstanding, the implied second config (the one with "ssl_bump bump 
> all" line removed) should not cache any HTTPS transactions either. 
> However, folks will read this thread, copy the original config, maybe 
> remove the "bump" line, and expect things to "work" because the 
> "problem" was "solved" for somebody else.
> 

Sorry, it was irresponsible of me to forget to mention that I changed the 'peek' line to 'stare', and added in another line. The final config, not counting the other default config items which were left unchanged, is as follows:

http_port 3128 ssl-bump \
               generate-host-certificates=on \
               dynamic_cert_mem_cache_size=32MB \
               cert=/path/to/cert.pem \
               key=/path/to/cert.pem

sslcrtd_program /usr/libexec/security_file_certgen -s /path/to/ssl_database -M 32MB

ssl_bump client-first all
ssl_bump stare all
ssl_bump splice localhost

(Note for any other confused noobs reading this: this configuration apparently requires Squid to be compiled with --with openssl and --with-ssl-crtd options on the 'configure' command line; or at least it did in older versions, and presumably still does.)

This final config works perfectly to cache SSL items, and has greatly increased the utility of my slow connection.

> 
> > Please add more concrete examples to the Wiki reference pages!
> 
> IMHO, SslBump is too nuanced/complex to be able to reuse simple 
> configurations without understanding their meaning. We should improve 
> documentation a lot, but it takes a village to do that, and "more 
> examples" is hardly the answer.
> 
> Alex.

Although I am sure the reference material is extremely valuable, as a non-expert I found it frustrating, as there almost NO concrete examples on each reference page, which SHOW the given config option being used in real world configurations. This is a common problem to a lot of 'man' pages in the Linux world for example which have page after page of information that is essentially useless unless one is already an expert, or extremely tedious to parse through, because it does not give concrete examples. 

On other sections of the wiki there are more explanatory texts showing various how-to scenarios, but again, I couldn't find a single one that showed this exact configuration here and briefly explained why/how it works, step by step according to what Squid is doing at each step. I ended up finding the key parts of the above config on a third party tutorial page ("How I saved countless gigabytes of data with Squid caching" or something like that), while deleting several lines from that config which were apparently unneeded/outdated. Actually I thought I had read somewhere that the 'client-first' line is itself outdated, but Squid doesn't complain about it, so maybe not. Anyhow, it works. 

I don't understand exactly *how* it works, because I don't have time to study all the internal workings of Squid at this time; just needed to quickly get a proxy up and running to solve this problem and move on to other work. As it was, I had like two dozen browser tabs open reading different things, only to slowly and painfully piece together what turns out to be a very simple config.

-- 
Dave Blanchard <dave at killthe.net>


From dave at killthe.net  Thu Feb 24 18:34:55 2022
From: dave at killthe.net (Dave Blanchard)
Date: Thu, 24 Feb 2022 12:34:55 -0600
Subject: [squid-users] Trying to set up SSL cache - solved! - correction
In-Reply-To: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
Message-ID: <20220224123455.27b6dc09f7728dabd6697069@killthe.net>

On Thu, 24 Feb 2022 12:24:35 -0600
Dave Blanchard <dave at killthe.net> wrote:

> (Note for any other confused noobs reading this: this configuration apparently requires Squid to be compiled with --with openssl and --with-ssl-crtd options on the 'configure' command line; or at least it did in older versions, and presumably still does.)

CORRECTION: that's --enable-ssl-crtd , not --with. 

Replace /usr/libexec in the given config with the actual path to your security_file_certgen binary, and run this command once to generate the initial SSL database:

security_file_certgen -c -s /path/to/ssl_database -M 32MB 

'/path/to' must already exist, and 'ssl_database' itself must not, for this command to succeed. Hope this helps somebody.

-- 
Dave Blanchard <dave at killthe.net>



From rousskov at measurement-factory.com  Thu Feb 24 19:22:25 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Feb 2022 14:22:25 -0500
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
Message-ID: <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>

On 2/24/22 13:24, Dave Blanchard wrote:
> ssl_bump client-first all
> ssl_bump stare all
> ssl_bump splice localhost

Bugs notwithstanding, the above configuration is equivalent to the 
configuration below:

     ssl_bump client-first all

The following wiki page can be used to find out what each ssl_bump 
action is supposed to be doing. Legacy actions mentioned there, 
including client-first, should be treated as unsupported, having unknown 
side effects, and meant to be removed from Squid (yesterday). YMMV.

https://wiki.squid-cache.org/Features/SslPeekAndSplice

squid.conf.documented documents how multiple ssl_bump rules are handled, 
but that documentation is difficult to interpret correctly without the 
step documentation at the above wiki page.

Alex.


From dave at killthe.net  Thu Feb 24 19:38:56 2022
From: dave at killthe.net (Dave Blanchard)
Date: Thu, 24 Feb 2022 13:38:56 -0600
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
Message-ID: <20220224133856.b26137522496068ee70ea708@killthe.net>

On Thu, 24 Feb 2022 14:22:25 -0500
Alex Rousskov <rousskov at measurement-factory.com> wrote:

> [...]
>
> action is supposed to be doing. Legacy actions mentioned there, 
> including client-first, should be treated as unsupported, having unknown 
> side effects, and meant to be removed from Squid (yesterday). YMMV.

What is the replacement for client-first? That line is the only way I could get this thing working right. I think the first working config was with server-first instead, but IIRC it was still passing through the server certificate somehow rather than using the locally generated certificate. Only with client-first did it have the desired effect. Maybe I need to set generate-host-certificates to 'off.'

I prefer to handle the certificate validation externally via a different means, i.e. not using the browser or Squid, because neither the browser nor the certificate authority is trustworthy. The 'chain of trust' argument for SSL is total bunk; none of these people can actually be trusted. If the certificate is automatically checked for validity every time one visits a site, then the shadowy people running the certificate authority can effectively track one's every move on the internet, and of course that data will be sold or given away to other shadowy people. Needless to say, censoring some inconvenient web site would be as simple as them revoking or invalidating the site's certificate, especially with the browser so "helpfully" refusing to allow one to bypass the revoked/invalid certificate.

-- 
Dave Blanchard <dave at killthe.net>


From rousskov at measurement-factory.com  Thu Feb 24 20:07:53 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 24 Feb 2022 15:07:53 -0500
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220224133856.b26137522496068ee70ea708@killthe.net>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
Message-ID: <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>

On 2/24/22 14:38, Dave Blanchard wrote:

 > ssl_bump client-first all

> What is the replacement for client-first?

A "good" answer depends on what exactly you are trying to achieve; 
details matter. A "dumb" answer (i.e. a direct replacement without 
considering your true needs and Squid bugs) is:

   ssl_bump bump all

Please do not misinterpret my statement as if that dumb answer is never 
good or correct. It all depends on your needs.


> I prefer to handle the certificate validation externally

It is a common need. Squid supports external certificate validator 
programs (a.k.a. helpers). Look for sslcrtvalidator_program in 
squid.conf.documented. For communication details, see the following 
wikip age and src/security/cert_validators/fake/

https://wiki.squid-cache.org/Features/AddonHelpers


HTH,

Alex.


From musamamehboob at gmail.com  Thu Feb 24 21:45:21 2022
From: musamamehboob at gmail.com (Usama Mehboob)
Date: Thu, 24 Feb 2022 16:45:21 -0500
Subject: [squid-users] Getting SSL Connection Errors
Message-ID: <CAPjjjyiC5R_pBsq7G1z-YqQRgnEi+LBW4=CC8TrQ77iEi34R5A@mail.gmail.com>

Hi I have a squid running on a linux box ( about 16GB ram and 4 cpu ) -- it
runs fine for the most part but when I am launching multiple jobs that are
connecting with salesforce BulkAPI, sometimes connections are dropped. its
not predictable and happens only when there is so much load on squid. Can
anyone shed some light on this? what can I do? is it a file descriptor
issue?

I see only these error messages from the cache logs
```
PeerConnector.cc(639) handleNegotiateError: Error (error:04091068:rsa
routines:INT_RSA_VERIFY:bad signature) but, hold write on SSL connection on
FD 109
```
----------------Config file ----------------
visible_hostname squid

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
###acl Safe_ports port 21 # ftp testing after blocking itp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
#http_access allow CONNECT SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

# And finally deny all other access to this proxy

# Squid normally listens to port 3128
#http_port 3128
http_port 3129 intercept
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
http_access allow SSL_ports #-- this allows every https website
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all

# Deny requests to proxy instance metadata
acl instance_metadata dst 169.254.169.254
http_access deny instance_metadata

# Filter HTTP Only requests based on the whitelist
#acl allowed_http_only dstdomain .veevasourcedev.com .google.com .pypi.org .
youtube.com
#acl allowed_http_only dstdomain .amazonaws.com
#acl allowed_http_only dstdomain .veevanetwork.com .veevacrm.com .
veevacrmdi.com .veeva.com .veevavault.com .vaultdev.com .veevacrmqa.com
#acl allowed_http_only dstdomain .documentforce.com  .sforce.com .force.com
.forceusercontent.com .force-user-content.com .lightning.com .salesforce.com
.salesforceliveagent.com .salesforce-communities.com .
salesforce-experience.com .salesforce-hub.com .salesforce-scrt.com .
salesforce-sites.com .site.com .sfdcopens.com .sfdc.sh .trailblazer.me .
trailhead.com .visualforce.com


# Filter HTTPS requests based on the whitelist
acl allowed_https_sites ssl::server_name .pypi.org .pythonhosted.org .
tfhub.dev .gstatic.com .googleapis.com
acl allowed_https_sites ssl::server_name .amazonaws.com
acl allowed_https_sites ssl::server_name .documentforce.com  .sforce.com .
force.com .forceusercontent.com .force-user-content.com .lightning.com .
salesforce.com .salesforceliveagent.com .salesforce-communities.com .
salesforce-experience.com .salesforce-hub.com .salesforce-scrt.com .
salesforce-sites.com .site.com .sfdcopens.com .sfdc.sh .trailblazer.me .
trailhead.com .visualforce.com
ssl_bump peek step2 allowed_https_sites
ssl_bump splice step3 allowed_https_sites
ssl_bump terminate step2 all


connect_timeout 60 minute
read_timeout 60 minute
write_timeout 60 minute
request_timeout 60 minute

## http filtering ###
#http_access allow localnet allowed_http_only
#http_access allow localhost allowed_http_only
http_access allow localnet allowed_https_sites
http_access allow localhost allowed_https_sites
# And finally deny all other access to this proxy
http_access deny all

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320


thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220224/2da24209/attachment.htm>

From dave at killthe.net  Thu Feb 24 21:49:53 2022
From: dave at killthe.net (Dave Blanchard)
Date: Thu, 24 Feb 2022 15:49:53 -0600
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
 <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
Message-ID: <20220224154953.0623171e887350d6d5db2c08@killthe.net>

On Thu, 24 Feb 2022 15:07:53 -0500
Alex Rousskov <rousskov at measurement-factory.com> wrote:

> > What is the replacement for client-first?
> 
> A "good" answer depends on what exactly you are trying to achieve; 
> details matter. A "dumb" answer (i.e. a direct replacement without 
> considering your true needs and Squid bugs) is:
> 
>    ssl_bump bump all

That's what I had tried first, and was banging my head on the wall for hours trying to get it to work right--though the "ssl_bump peek" was in there also, on the suggestion of various tutorials. Now I just tried it again, with only that line...and it works perfectly! No problem. SMH... 

This tutorial situation is really out of control. Sadly, this is what can be expected to happen when the syntax is changed with every version. Now we're in a real mess. I hope the Squid developers will make up their minds on how they want the syntax to be structured, build it that way, then LEAVE IT ALONE!

> > I prefer to handle the certificate validation externally
> 
> It is a common need. Squid supports external certificate validator 
> programs (a.k.a. helpers). Look for sslcrtvalidator_program in 
> squid.conf.documented. For communication details, see the following 
> wikip age and src/security/cert_validators/fake/
> 
> https://wiki.squid-cache.org/Features/AddonHelpers

Awesome! That's very useful. 

Thanks a lot for your help!

-- 
Dave Blanchard <dave at killthe.net>


From 0xff1f at gmail.com  Fri Feb 25 00:14:23 2022
From: 0xff1f at gmail.com (Ahmad Alzaeem)
Date: Fri, 25 Feb 2022 02:14:23 +0200
Subject: [squid-users] is there any squid 4.x version has delay_pools
 working?
In-Reply-To: <20220224154953.0623171e887350d6d5db2c08@killthe.net>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
 <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
 <20220224154953.0623171e887350d6d5db2c08@killthe.net>
Message-ID: <695E12D0-F862-4A17-9A41-D1D00BFB904C@gmail.com>

I tried many squid 4.x versions and none of them has delay_pools to work .
I have it to work on 3.x versions .

is there any specific 4.x version that ws tested with delay pools to work ?


i would like to report it as bug at least in squid-4.17 <http://www.squid-cache.org/Versions/v4/squid-4.17-RELEASENOTES.html> which i tested today .

Regards 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/4c2c4f64/attachment.htm>

From ngtech1ltd at gmail.com  Fri Feb 25 04:58:05 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 25 Feb 2022 06:58:05 +0200
Subject: [squid-users] is there any squid 4.x version has delay_pools
 working?
In-Reply-To: <695E12D0-F862-4A17-9A41-D1D00BFB904C@gmail.com>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
 <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
 <20220224154953.0623171e887350d6d5db2c08@killthe.net>
 <695E12D0-F862-4A17-9A41-D1D00BFB904C@gmail.com>
Message-ID: <006a01d82a04$4765cf80$d6316e80$@gmail.com>

Hey Ahmad,

 

Can you please give more details on the specific issue or issues you have
verified in 4.17?

What exactly doesn't work in delay_pools? Plain HTTP download or upload
speed?

Is it only on HTTP or also on CONNECT or HTTPS or SSL-BUMP connections?

 

Eliezer

 

*	I was thinking about creating a webinar about Squid ssl(TLS) bump

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Ahmad Alzaeem
Sent: Friday, February 25, 2022 02:14
To: squid-users at lists.squid-cache.org
Subject: [squid-users] is there any squid 4.x version has delay_pools
working?

 

I tried many squid 4.x versions and none of them has delay_pools to work .

I have it to work on 3.x versions .

 

is there any specific 4.x version that ws tested with delay pools to work ?

 

 

i would like to report it as bug at least in
<http://www.squid-cache.org/Versions/v4/squid-4.17-RELEASENOTES.html>
squid-4.17 which i tested today .

 

Regards 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/4defcba1/attachment.htm>

From ngtech1ltd at gmail.com  Fri Feb 25 05:01:12 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Fri, 25 Feb 2022 07:01:12 +0200
Subject: [squid-users] Getting SSL Connection Errors
In-Reply-To: <CAPjjjyiC5R_pBsq7G1z-YqQRgnEi+LBW4=CC8TrQ77iEi34R5A@mail.gmail.com>
References: <CAPjjjyiC5R_pBsq7G1z-YqQRgnEi+LBW4=CC8TrQ77iEi34R5A@mail.gmail.com>
Message-ID: <006f01d82a04$b678b770$236a2650$@gmail.com>

Hey Usama,

 

There are more missing details on the system.

If you provide the OS and squid details I might be able to provide a script that will pull most of the relevant details on the system.

I don?t know about this specific issue yet and it seems like there is a SSL related issue and it might not be even related to Squid.

(@Alex Or @Chrisots might know better then me)

 

All The Bests,

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Usama Mehboob
Sent: Thursday, February 24, 2022 23:45
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Getting SSL Connection Errors

 

Hi I have a squid running on a linux box ( about 16GB ram and 4 cpu ) -- it runs fine for the most part but when I am launching multiple jobs that are connecting with salesforce BulkAPI, sometimes connections are dropped. its not predictable and happens only when there is so much load on squid. Can anyone shed some light on this? what can I do? is it a file descriptor issue?

I see only these error messages from the cache logs
```
PeerConnector.cc(639) handleNegotiateError: Error (error:04091068:rsa routines:INT_RSA_VERIFY:bad signature) but, hold write on SSL connection on FD 109
```

----------------Config file ----------------
visible_hostname squid 

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>  # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>  # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
###acl Safe_ports port 21 # ftp testing after blocking itp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
#http_access allow CONNECT SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

# And finally deny all other access to this proxy

# Squid normally listens to port 3128
#http_port 3128
http_port 3129 intercept
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept 
http_access allow SSL_ports #-- this allows every https website
acl step1 at_step SslBump1 
acl step2 at_step SslBump2 
acl step3 at_step SslBump3 
ssl_bump peek step1 all 

# Deny requests to proxy instance metadata 
acl instance_metadata dst 169.254.169.254 
http_access deny instance_metadata 

# Filter HTTP Only requests based on the whitelist 
#acl allowed_http_only dstdomain .veevasourcedev.com <http://veevasourcedev.com>  .google.com <http://google.com>  .pypi.org <http://pypi.org>  .youtube.com <http://youtube.com> 
#acl allowed_http_only dstdomain .amazonaws.com <http://amazonaws.com> 
#acl allowed_http_only dstdomain .veevanetwork.com <http://veevanetwork.com>  .veevacrm.com <http://veevacrm.com>  .veevacrmdi.com <http://veevacrmdi.com>  .veeva.com <http://veeva.com>  .veevavault.com <http://veevavault.com>  .vaultdev.com <http://vaultdev.com>  .veevacrmqa.com <http://veevacrmqa.com> 
#acl allowed_http_only dstdomain .documentforce.com <http://documentforce.com>   .sforce.com <http://sforce.com>  .force.com <http://force.com>  .forceusercontent.com <http://forceusercontent.com>  .force-user-content.com <http://force-user-content.com>  .lightning.com <http://lightning.com>  .salesforce.com <http://salesforce.com>  .salesforceliveagent.com <http://salesforceliveagent.com>  .salesforce-communities.com <http://salesforce-communities.com>  .salesforce-experience.com <http://salesforce-experience.com>  .salesforce-hub.com <http://salesforce-hub.com>  .salesforce-scrt.com <http://salesforce-scrt.com>  .salesforce-sites.com <http://salesforce-sites.com>  .site.com <http://site.com>  .sfdcopens.com <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>  .trailhead.com <http://trailhead.com>  .visualforce.com <http://visualforce.com> 


# Filter HTTPS requests based on the whitelist 
acl allowed_https_sites ssl::server_name .pypi.org <http://pypi.org>  .pythonhosted.org <http://pythonhosted.org>  .tfhub.dev <http://tfhub.dev>  .gstatic.com <http://gstatic.com>  .googleapis.com <http://googleapis.com> 
acl allowed_https_sites ssl::server_name .amazonaws.com <http://amazonaws.com> 
acl allowed_https_sites ssl::server_name .documentforce.com <http://documentforce.com>   .sforce.com <http://sforce.com>  .force.com <http://force.com>  .forceusercontent.com <http://forceusercontent.com>  .force-user-content.com <http://force-user-content.com>  .lightning.com <http://lightning.com>  .salesforce.com <http://salesforce.com>  .salesforceliveagent.com <http://salesforceliveagent.com>  .salesforce-communities.com <http://salesforce-communities.com>  .salesforce-experience.com <http://salesforce-experience.com>  .salesforce-hub.com <http://salesforce-hub.com>  .salesforce-scrt.com <http://salesforce-scrt.com>  .salesforce-sites.com <http://salesforce-sites.com>  .site.com <http://site.com>  .sfdcopens.com <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>  .trailhead.com <http://trailhead.com>  .visualforce.com <http://visualforce.com> 
ssl_bump peek step2 allowed_https_sites 
ssl_bump splice step3 allowed_https_sites 
ssl_bump terminate step2 all


connect_timeout 60 minute
read_timeout 60 minute
write_timeout 60 minute
request_timeout 60 minute

## http filtering ###
#http_access allow localnet allowed_http_only
#http_access allow localhost allowed_http_only
http_access allow localnet allowed_https_sites
http_access allow localhost allowed_https_sites
# And finally deny all other access to this proxy
http_access deny all

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320



thanks

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/9f94f144/attachment.htm>

From dave at killthe.net  Fri Feb 25 05:14:30 2022
From: dave at killthe.net (Dave Blanchard)
Date: Thu, 24 Feb 2022 23:14:30 -0600
Subject: [squid-users] Random trouble with image downloads
Message-ID: <20220224231430.f046b84f7c376ed796c5f203@killthe.net>

OK, I've got Squid mostly working fine, but have noticed a problem with certain image downloads, which in at least one case are coming from storage.googleapis.com. (Profile images for a forum.) It's as if Squid sometimes randomly fails to download and correctly cache a given image, and instead caches a broken or zero'd file. If I try to open that image in a new browser tab, sometimes it will just be blank, and other times the browser reports ERR_EMPTY_RESPONSE "The server didn't send any data." In the former case the image access shows up in the Squid access log as TCP_REFRESH_UNMODIFIED, and in the latter case it doesn't show up at all. If I download it manually using wget with no proxy, it downloads fine. What could possibly be happening here?

-- 
Dave Blanchard <dave at killthe.net>


From squid.org at bloms.de  Fri Feb 25 07:47:40 2022
From: squid.org at bloms.de (Dieter Bloms)
Date: Fri, 25 Feb 2022 08:47:40 +0100
Subject: [squid-users] slow down response to broken clients ?
Message-ID: <20220225074740.pclldvx4rtrpiifc@bloms.de>

Hello,

Sometimes a client tries to reach a destination that is blocked at the
proxy. The proxy responds with a 403 and the client then immediately
tries again and again, making hundreds of requests per second. Is it
possible to add an artificial delay here so that the proxy answers
the client later?
Best combined with a rate limit, so that the delays only become active
when a certain limit of 403 answers is exceeded?


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From squid3 at treenet.co.nz  Fri Feb 25 10:30:47 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 25 Feb 2022 23:30:47 +1300
Subject: [squid-users] getsockopt failures,
 although direct access to intercept ports is blocked
In-Reply-To: <20220223230558.Horde.4oLy24HP_6s1SU7RO0yA1L_@webmail.intern.securepoint.de>
References: <20220223230558.Horde.4oLy24HP_6s1SU7RO0yA1L_@webmail.intern.securepoint.de>
Message-ID: <b75703c5-3be1-a645-7e05-3b557b5b1336@treenet.co.nz>

On 24/02/22 12:05, Andreas Weigel wrote:
> Hi everyone,
> 
> I had the following issue with Squid in Transparent Mode (and SSL 
> Interception in mode splice). It is working as expected, however after 
> multiple long-running (talking about several seconds) anti-virus 
> ecap-Processes have finished, I *sometimes* get the following in the log:
> 
> 2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(224) 
> doAccept: New connection on FD 21
> 2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(312) 
> acceptNext: connection on local=[::]:2412 remote=[::] FD 21 flags=41
> 2022/02/23 14:56:40.668 kid1| 89,5| src/ip/Intercept.cc(405) Lookup: 
> address BEGIN: me/client= 192.168.180.1:2412, destination/me= 
> 192.168.180.10:48582
> 2022/02/23 14:56:40.668 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed 
> on local=192.168.180.1:2412 remote=192.168.180.10:48582 FD 37 flags=33: 
> (2) No such file or directory
> 2022/02/23 14:56:40.669 kid1| 89,9| src/ip/Intercept.cc(151) 
> NetfilterInterception: address: local=192.168.180.1:2412 
> remote=192.168.180.10:48582 FD 37 flags=33
> 2022/02/23 14:56:40.669 kid1| ERROR: NAT/TPROXY lookup failed to locate 
> original IPs on local=192.168.180.1:2412 remote=192.168.180.10:48582 FD 
> 37 flags=33


These can happen if the NAT table entries expire or otherwise get 
dropped by conntrack between the client initiating TCP SYN and Squid 
accept(2) receiving the connection.

Your config looks good to me and the lack of regularity indicates the 
issue is likely this type of transient state situation.
  Is this happening at times of unusually high client connections 
through the NAT?
  Is eCAP processing blocking the Squid worker for all those seconds?


> 2022/02/23 14:56:40.669 kid1| 5,5| src/comm/TcpAcceptor.cc(287) 
> acceptOne: non-recoverable error: FD 21, [::] [ job2] handler 
> Subscription: 0x55edac3d08d0*1
> 
> Sometimes, this only appears on on of the two interception ports, 
> sometimes on both. After that, the squid worker does not poll the 
> intercept listen port any longer, i.e. stops working.

That part is likely to be the issue recently worked around by 
<http://www.squid-cache.org/Versions/v6/changesets/squid-6-9fd3e68c3d0dfd6035db98ce142cf425be6c5fc1.patch>


Amos


From squid3 at treenet.co.nz  Fri Feb 25 11:16:30 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Feb 2022 00:16:30 +1300
Subject: [squid-users] Trying to set up SSL cache
In-Reply-To: <20220223202611.5852047e8ec14fc337435e83@killthe.net>
References: <20220223202611.5852047e8ec14fc337435e83@killthe.net>
Message-ID: <776152f1-8bf2-286b-6308-0eeead344674@treenet.co.nz>

On 24/02/22 15:26, Dave Blanchard wrote:
> Hello, I'm trying to configure Squid as a HTTPS cache on my local computer, using ssl-bump. I've got it working as a basic proxy, but the traffic seems to just be tunneling through and not being cached.

Do you actually get at least *2* (maybe 3) Squid access.log entries per 
client connection demonstrating that?


> My web browser shows the site's actual certificate, rather than the locally generated self-signed certificate, which I want it to see. I have followed every tutorial I can find and none of them are helpful in figuring out what the hell is going on here. Here is what my config file looks like:
> 
> [...]
> 
> http_port 3128 ssl-bump \
>                 generate-host-certificates=on \
>                 dynamic_cert_mem_cache_size=32MB \
>                 cert=/path/to/self-signed.pem \
>                 key=/path/to/self-signed.pem
> 
> sslcrtd_program /usr/libexec/security_file_certgen -s /path/to/ssl-database -M 32MB
> 
> ssl_bump peek all

Okay TLS handshake clientHello gets observed by Squid.

At this point you should see 2 CONNECT requests in access.log, first 
(step1) with raw-IP and second (step2) with the TLS SNI from this 
clientHello (if any, otherwise raw-IP again).


> ssl_bump bump all

... now (step3) everything gets decrypted.

At this point you should start seeing access.log entries with https:// 
URLs and the actual server name per the serverHello certificate.


> ssl_bump splice localhost
> 

The connection is already bump'ed. This rule can never be reached.


> [...]
> 
> Otherwise, it's pretty much just the default config. The only thing that seems to halfway work is removing the line:
> 
> http_access deny CONNECT !SSL_ports
> 
> and changing to:
> 
> http_access deny CONNECT
> 

This actively rejects *all* CONNECT messages. Including the one Squid 
uses internally for SSL-Bump step1.


> With that change, an older Chromium just hangs trying to load the page, saying "Processing request." On a WebKit-based browser, I get a Squid 'Access Denied' error page.

As expected with a "deny CONNECT" rule on a CONNECT request.

This is actually the best outcome right now. It means the SSL-Bump 
crypto part is working fine for at least this second client.

What you need to do next is figure out which of your other rules is 
causing things to go wrong when the HTTP message inside the TLS is received.

For example; whether it is actually *HTTP/1* messages arriving. Other 
things will revert back to tunneling (see 
<http://www.squid-cache.org/Doc/config/on_unsupported_protocol/>)


> Another WebKit browser complains about the certificate, but when I tell it to continue anyway, it gives the same 'Access Denied' page.
> A newer Chromium stops right away with an untrusted SSL certificate error, and the details look like it's getting the self-signed certificate, as expected.
> 

These clients do not trust the CA Squid is using as signing cert. That 
will need to be fixed to avoid the cert complaint, but does not 
otherwise make any difference to the main issue.


You could use these browsers to test if you don't mind the popup. But I 
recommend the one which already trusts Squids signing CA until you have 
the rest of the basics working.



There are a few things to be aware of while troubleshooting:

* not all TLS connections can be bump'ed. TLS is designed to prevent 
exactly the type of decrypt that bump does. If the client and server are 
using TLS properly bump *will* fail.


* Google are known to be rather pedantic about security. So having their 
software at either end of the TLS when testing is more likely to hit 
such non-decryptable TLS connections.


* Checking the test web service for TLS certificate pinning or DANE. 
Both of these lock the/some client into using the original server 
certificate and they will unavoidably reject the Squid signing CA.

* Check traffic from the web server for HTTPS-Transport-Security or 
Alt-Svc HTTP headers. Both of these can break SSL-Bump if they reach a 
client. What is worse they can force arbitrarily long cache times for 
the info they contain, causing breakage to extend across the whole 
period. Only a full client purge of state and never receiving the info 
again can via any protocol fix these.


Amos


From squid3 at treenet.co.nz  Fri Feb 25 12:32:48 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Feb 2022 01:32:48 +1300
Subject: [squid-users] Squid Question regarding tcp handshake
In-Reply-To: <CADcj3=77oZ_mSPAkL2KYujLZPOo-HgTJZWaoVZT8AH4WQExgjw@mail.gmail.com>
References: <CADcj3=77oZ_mSPAkL2KYujLZPOo-HgTJZWaoVZT8AH4WQExgjw@mail.gmail.com>
Message-ID: <fc695317-7682-a474-5153-6a4061821f9c@treenet.co.nz>

On 25/02/22 04:03, Felipe Polanco wrote:
> Hi,
> 
> A question Regarding TCP handshake.
> 
> Does squid first complete the tcp handshake on its users and then a 
> second handshake on the destination IP or as soon as it receives the TCP 
> SYN flag it does the same with the destination.
> 

The TCP connections handshakes are (almost) completely independent. One 
client connection may result in  multiple (or 0) server connections - 
depending on what needs to be done.


Amos


From squid3 at treenet.co.nz  Fri Feb 25 12:53:11 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Feb 2022 01:53:11 +1300
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <cb3fcb2e-27f7-98a7-42b0-26c3f544cff6@spamtrap.tnetconsulting.net>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
 <ff2a9d45-f6d8-e644-08e6-4d2c5367aec2@measurement-factory.com>
 <cb3fcb2e-27f7-98a7-42b0-26c3f544cff6@spamtrap.tnetconsulting.net>
Message-ID: <1e079475-ddb9-9e0d-37c9-12c821279db1@treenet.co.nz>

On 25/02/22 05:41, Grant Taylor wrote:
> On 2/24/22 9:08 AM, Alex Rousskov wrote:
>> "more examples" is hardly the answer.
> 
> I believe that "more examples" can be additional data that someone can 
> derive information ~> knowledge from.
> 
> Or said another way, it's a step in the proper direction.
> 

At this point I should remind that it is a *wiki* and largely written by 
all of us in the community.

The ConfigExamples section a collection of configs that people here in 
the community are (or were) actively using and found useful to share.
We (the main devs) have made a point of trying to update the contents to 
prune mistakes and mention when there are Squid version-specific 
differences. That is all.

That said, Francesco is working on converting the wiki content to 
another format so right now is not good for edits.


> I think one of the most important things to have with examples is meta 
> information about the configuration (both server and client side) that 
> it is being used and the version.
> 
> E.g. Clients are configured to talk directly to Squid (vX.Y) proxy 
> server on port 80 / 443 wherein SSL "bumping" a la. monkey in the middle 
> for caching purposes on a low bandwidth / metered connection.
> 

Sounds like you are talking about 
<https://wiki.squid-cache.org/Features/HTTPS>. From that page it is a 
matter of selecting which situation is the use-case and diving into the 
rabbit hole of links.


> /Just/ having configuration examples doesn't do much in and of itself 
> without knowing the context the examples are from.? If anything /just/ 
> examples is worse in that people have no context and are left with 
> trying to identify the version, what was being and intended to be done, 
> and then translating that to the version & configuration they are 
> working with.? This is something that's non-trivial for seasoned (Squid) 
> administrators and can be near impossible to new (Squid) administrators.
> 

Nod.

HTH
Amos


From squid3 at treenet.co.nz  Fri Feb 25 13:06:23 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Feb 2022 02:06:23 +1300
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220224154953.0623171e887350d6d5db2c08@killthe.net>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
 <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
 <20220224154953.0623171e887350d6d5db2c08@killthe.net>
Message-ID: <0c59deb4-2b17-30cb-eabe-74ddc82e643b@treenet.co.nz>

On 25/02/22 10:49, Dave Blanchard wrote:
> On Thu, 24 Feb 2022 15:07:53 -0500
> Alex Rousskov wrote:
> 
>>> What is the replacement for client-first?
>>
>> A "good" answer depends on what exactly you are trying to achieve;
>> details matter. A "dumb" answer (i.e. a direct replacement without
>> considering your true needs and Squid bugs) is:
>>
>>     ssl_bump bump all
> 
> That's what I had tried first, and was banging my head on the wall for hours trying to get it to work right--though the "ssl_bump peek" was in there also, on the suggestion of various tutorials. Now I just tried it again, with only that line...and it works perfectly! No problem. SMH...
> 
> This tutorial situation is really out of control. Sadly, this is what can be expected to happen when the syntax is changed with every version. Now we're in a real mess. I hope the Squid developers will make up their minds on how they want the syntax to be structured, build it that way, then LEAVE IT ALONE!
> 

Agreed. Luckily we hear you (Alex and I are pretty much "them" these days).

If it helps. The config for this stabilized in Squid-3.5.
<https://wiki.squid-cache.org/Features/SslPeekAndSplice>



>>> I prefer to handle the certificate validation externally
>>
>> It is a common need. Squid supports external certificate validator
>> programs (a.k.a. helpers). Look for sslcrtvalidator_program in
>> squid.conf.documented.

Or at <http://www.squid-cache.org/Doc/config/sslcrtvalidator_program/>


> For communication details, see the following
>> wikip age and src/security/cert_validators/fake/
>>
>> https://wiki.squid-cache.org/Features/AddonHelpers
> 
> Awesome! That's very useful.
> 
> Thanks a lot for your help!
> 

HTH
Amos


From squid3 at treenet.co.nz  Fri Feb 25 13:09:14 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Feb 2022 02:09:14 +1300
Subject: [squid-users] Random trouble with image downloads
In-Reply-To: <20220224231430.f046b84f7c376ed796c5f203@killthe.net>
References: <20220224231430.f046b84f7c376ed796c5f203@killthe.net>
Message-ID: <1041b94e-d8c8-b4c2-d07c-a37759c39964@treenet.co.nz>

On 25/02/22 18:14, Dave Blanchard wrote:
> OK, I've got Squid mostly working fine, but have noticed a problem with certain image downloads, which in at least one case are coming from storage.googleapis.com. (Profile images for a forum.) It's as if Squid sometimes randomly fails to download and correctly cache a given image, and instead caches a broken or zero'd file. If I try to open that image in a new browser tab, sometimes it will just be blank, and other times the browser reports ERR_EMPTY_RESPONSE "The server didn't send any data." In the former case the image access shows up in the Squid access log as TCP_REFRESH_UNMODIFIED, and in the latter case it doesn't show up at all. If I download it manually using wget with no proxy, it downloads fine. What could possibly be happening here?
> 


I suspect you are seeing <https://bugs.squid-cache.org/show_bug.cgi?id=4882>

Unfortunately we do not have a good fix for it yet.

Amos


From squid3 at treenet.co.nz  Fri Feb 25 13:17:32 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 26 Feb 2022 02:17:32 +1300
Subject: [squid-users] slow down response to broken clients ?
In-Reply-To: <20220225074740.pclldvx4rtrpiifc@bloms.de>
References: <20220225074740.pclldvx4rtrpiifc@bloms.de>
Message-ID: <d4c3fdaa-15de-770a-34c4-35577418a6db@treenet.co.nz>

On 25/02/22 20:47, Dieter Bloms wrote:
> Hello,
> 
> Sometimes a client tries to reach a destination that is blocked at the
> proxy. The proxy responds with a 403 and the client then immediately
> tries again and again, making hundreds of requests per second. Is it
> possible to add an artificial delay here so that the proxy answers
> the client later?
> Best combined with a rate limit, so that the delays only become active
> when a certain limit of 403 answers is exceeded?
> 
> 

Squid ships with a helper called ext_delayer_acl for handling this.
<http://www.squid-cache.org/Versions/v4/manuals/ext_delayer_acl.html>

Amos


From musamamehboob at gmail.com  Fri Feb 25 13:31:55 2022
From: musamamehboob at gmail.com (Usama Mehboob)
Date: Fri, 25 Feb 2022 08:31:55 -0500
Subject: [squid-users] Getting SSL Connection Errors (Eliezer Croitoru)
In-Reply-To: <mailman.8353.1645785237.971461.squid-users@lists.squid-cache.org>
References: <mailman.8353.1645785237.971461.squid-users@lists.squid-cache.org>
Message-ID: <CAPjjjyjZAr4jmFWtUQMfaDDNHgiup4z8-9XVu2VXmB=qKAq-cw@mail.gmail.com>

Hi Eliezer, I am running on amazon linux 2 ami which I suppose is based on
centos.
I ran the uname -a command and this is what I get;;
Linux ip-172-24-9-143.us-east-2.compute.internal
4.14.256-197.484.amzn2.x86_64 #1 SMP Tue Nov 30 00:17:50 UTC 2021 x86_64
x86_64 x86_64 GNU/Linux

[ec2-user at ip-172-24-9-143 ~]$ openssl version
OpenSSL 1.0.2k-fips  26 Jan 2017

thanks so much and let me know the script and I can run on this machine.
Usama

On Fri, Feb 25, 2022 at 5:34 AM <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Getting SSL Connection Errors (Eliezer Croitoru)
>    2. Random trouble with image downloads (Dave Blanchard)
>    3. slow down response to broken clients ? (Dieter Bloms)
>    4. Re: getsockopt failures, although direct access to intercept
>       ports is blocked (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 25 Feb 2022 07:01:12 +0200
> From: "Eliezer Croitoru" <ngtech1ltd at gmail.com>
> To: "'Usama Mehboob'" <musamamehboob at gmail.com>,
>         <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Getting SSL Connection Errors
> Message-ID: <006f01d82a04$b678b770$236a2650$@gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hey Usama,
>
>
>
> There are more missing details on the system.
>
> If you provide the OS and squid details I might be able to provide a
> script that will pull most of the relevant details on the system.
>
> I don?t know about this specific issue yet and it seems like there is a
> SSL related issue and it might not be even related to Squid.
>
> (@Alex Or @Chrisots might know better then me)
>
>
>
> All The Bests,
>
>
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>
>
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Usama Mehboob
> Sent: Thursday, February 24, 2022 23:45
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Getting SSL Connection Errors
>
>
>
> Hi I have a squid running on a linux box ( about 16GB ram and 4 cpu ) --
> it runs fine for the most part but when I am launching multiple jobs that
> are connecting with salesforce BulkAPI, sometimes connections are dropped.
> its not predictable and happens only when there is so much load on squid.
> Can anyone shed some light on this? what can I do? is it a file descriptor
> issue?
>
> I see only these error messages from the cache logs
> ```
> PeerConnector.cc(639) handleNegotiateError: Error (error:04091068:rsa
> routines:INT_RSA_VERIFY:bad signature) but, hold write on SSL connection on
> FD 109
> ```
>
> ----------------Config file ----------------
> visible_hostname squid
>
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>  # RFC1918 possible
> internal network
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  # RFC1918 possible
> internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>  # RFC1918
> possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> ###acl Safe_ports port 21 # ftp testing after blocking itp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> #http_access allow CONNECT SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
>
> # And finally deny all other access to this proxy
>
> # Squid normally listens to port 3128
> #http_port 3128
> http_port 3129 intercept
> https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> http_access allow SSL_ports #-- this allows every https website
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1 all
>
> # Deny requests to proxy instance metadata
> acl instance_metadata dst 169.254.169.254
> http_access deny instance_metadata
>
> # Filter HTTP Only requests based on the whitelist
> #acl allowed_http_only dstdomain .veevasourcedev.com <
> http://veevasourcedev.com>  .google.com <http://google.com>  .pypi.org <
> http://pypi.org>  .youtube.com <http://youtube.com>
> #acl allowed_http_only dstdomain .amazonaws.com <http://amazonaws.com>
> #acl allowed_http_only dstdomain .veevanetwork.com <
> http://veevanetwork.com>  .veevacrm.com <http://veevacrm.com>  .
> veevacrmdi.com <http://veevacrmdi.com>  .veeva.com <http://veeva.com>  .
> veevavault.com <http://veevavault.com>  .vaultdev.com <http://vaultdev.com>
> .veevacrmqa.com <http://veevacrmqa.com>
> #acl allowed_http_only dstdomain .documentforce.com <
> http://documentforce.com>   .sforce.com <http://sforce.com>  .force.com <
> http://force.com>  .forceusercontent.com <http://forceusercontent.com>  .
> force-user-content.com <http://force-user-content.com>  .lightning.com <
> http://lightning.com>  .salesforce.com <http://salesforce.com>  .
> salesforceliveagent.com <http://salesforceliveagent.com>  .
> salesforce-communities.com <http://salesforce-communities.com>  .
> salesforce-experience.com <http://salesforce-experience.com>  .
> salesforce-hub.com <http://salesforce-hub.com>  .salesforce-scrt.com <
> http://salesforce-scrt.com>  .salesforce-sites.com <
> http://salesforce-sites.com>  .site.com <http://site.com>  .sfdcopens.com
> <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>
> .trailhead.com <http://trailhead.com>  .visualforce.com <
> http://visualforce.com>
>
>
> # Filter HTTPS requests based on the whitelist
> acl allowed_https_sites ssl::server_name .pypi.org <http://pypi.org>  .
> pythonhosted.org <http://pythonhosted.org>  .tfhub.dev <http://tfhub.dev>
> .gstatic.com <http://gstatic.com>  .googleapis.com <http://googleapis.com>
>
> acl allowed_https_sites ssl::server_name .amazonaws.com <
> http://amazonaws.com>
> acl allowed_https_sites ssl::server_name .documentforce.com <
> http://documentforce.com>   .sforce.com <http://sforce.com>  .force.com <
> http://force.com>  .forceusercontent.com <http://forceusercontent.com>  .
> force-user-content.com <http://force-user-content.com>  .lightning.com <
> http://lightning.com>  .salesforce.com <http://salesforce.com>  .
> salesforceliveagent.com <http://salesforceliveagent.com>  .
> salesforce-communities.com <http://salesforce-communities.com>  .
> salesforce-experience.com <http://salesforce-experience.com>  .
> salesforce-hub.com <http://salesforce-hub.com>  .salesforce-scrt.com <
> http://salesforce-scrt.com>  .salesforce-sites.com <
> http://salesforce-sites.com>  .site.com <http://site.com>  .sfdcopens.com
> <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>
> .trailhead.com <http://trailhead.com>  .visualforce.com <
> http://visualforce.com>
> ssl_bump peek step2 allowed_https_sites
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate step2 all
>
>
> connect_timeout 60 minute
> read_timeout 60 minute
> write_timeout 60 minute
> request_timeout 60 minute
>
> ## http filtering ###
> #http_access allow localnet allowed_http_only
> #http_access allow localhost allowed_http_only
> http_access allow localnet allowed_https_sites
> http_access allow localhost allowed_https_sites
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
>
>
>
> thanks
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/9f94f144/attachment-0001.htm
> >
>
> ------------------------------
>
> Message: 2
> Date: Thu, 24 Feb 2022 23:14:30 -0600
> From: Dave Blanchard <dave at killthe.net>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Random trouble with image downloads
> Message-ID: <20220224231430.f046b84f7c376ed796c5f203 at killthe.net>
> Content-Type: text/plain; charset=US-ASCII
>
> OK, I've got Squid mostly working fine, but have noticed a problem with
> certain image downloads, which in at least one case are coming from
> storage.googleapis.com. (Profile images for a forum.) It's as if Squid
> sometimes randomly fails to download and correctly cache a given image, and
> instead caches a broken or zero'd file. If I try to open that image in a
> new browser tab, sometimes it will just be blank, and other times the
> browser reports ERR_EMPTY_RESPONSE "The server didn't send any data." In
> the former case the image access shows up in the Squid access log as
> TCP_REFRESH_UNMODIFIED, and in the latter case it doesn't show up at all.
> If I download it manually using wget with no proxy, it downloads fine. What
> could possibly be happening here?
>
> --
> Dave Blanchard <dave at killthe.net>
>
>
> ------------------------------
>
> Message: 3
> Date: Fri, 25 Feb 2022 08:47:40 +0100
> From: Dieter Bloms <squid.org at bloms.de>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] slow down response to broken clients ?
> Message-ID: <20220225074740.pclldvx4rtrpiifc at bloms.de>
> Content-Type: text/plain; charset=us-ascii
>
> Hello,
>
> Sometimes a client tries to reach a destination that is blocked at the
> proxy. The proxy responds with a 403 and the client then immediately
> tries again and again, making hundreds of requests per second. Is it
> possible to add an artificial delay here so that the proxy answers
> the client later?
> Best combined with a rate limit, so that the delays only become active
> when a certain limit of 403 answers is exceeded?
>
>
> --
> Regards
>
>   Dieter
>
> --
> I do not get viruses because I do not use MS software.
> If you use Outlook then please do not put my email address in your
> address-book so that WHEN you get a virus it won't use my address in the
> >From field.
>
>
> ------------------------------
>
> Message: 4
> Date: Fri, 25 Feb 2022 23:30:47 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] getsockopt failures, although direct access
>         to intercept ports is blocked
> Message-ID: <b75703c5-3be1-a645-7e05-3b557b5b1336 at treenet.co.nz>
> Content-Type: text/plain; charset=UTF-8; format=flowed
>
> On 24/02/22 12:05, Andreas Weigel wrote:
> > Hi everyone,
> >
> > I had the following issue with Squid in Transparent Mode (and SSL
> > Interception in mode splice). It is working as expected, however after
> > multiple long-running (talking about several seconds) anti-virus
> > ecap-Processes have finished, I *sometimes* get the following in the log:
> >
> > 2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(224)
> > doAccept: New connection on FD 21
> > 2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(312)
> > acceptNext: connection on local=[::]:2412 remote=[::] FD 21 flags=41
> > 2022/02/23 14:56:40.668 kid1| 89,5| src/ip/Intercept.cc(405) Lookup:
> > address BEGIN: me/client= 192.168.180.1:2412, destination/me=
> > 192.168.180.10:48582
> > 2022/02/23 14:56:40.668 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed
> > on local=192.168.180.1:2412 remote=192.168.180.10:48582 FD 37 flags=33:
> > (2) No such file or directory
> > 2022/02/23 14:56:40.669 kid1| 89,9| src/ip/Intercept.cc(151)
> > NetfilterInterception: address: local=192.168.180.1:2412
> > remote=192.168.180.10:48582 FD 37 flags=33
> > 2022/02/23 14:56:40.669 kid1| ERROR: NAT/TPROXY lookup failed to locate
> > original IPs on local=192.168.180.1:2412 remote=192.168.180.10:48582 FD
> > 37 flags=33
>
>
> These can happen if the NAT table entries expire or otherwise get
> dropped by conntrack between the client initiating TCP SYN and Squid
> accept(2) receiving the connection.
>
> Your config looks good to me and the lack of regularity indicates the
> issue is likely this type of transient state situation.
>   Is this happening at times of unusually high client connections
> through the NAT?
>   Is eCAP processing blocking the Squid worker for all those seconds?
>
>
> > 2022/02/23 14:56:40.669 kid1| 5,5| src/comm/TcpAcceptor.cc(287)
> > acceptOne: non-recoverable error: FD 21, [::] [ job2] handler
> > Subscription: 0x55edac3d08d0*1
> >
> > Sometimes, this only appears on on of the two interception ports,
> > sometimes on both. After that, the squid worker does not poll the
> > intercept listen port any longer, i.e. stops working.
>
> That part is likely to be the issue recently worked around by
> <
> http://www.squid-cache.org/Versions/v6/changesets/squid-6-9fd3e68c3d0dfd6035db98ce142cf425be6c5fc1.patch
> >
>
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 90, Issue 38
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/d3258692/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 25 16:00:17 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Feb 2022 11:00:17 -0500
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <20220224154953.0623171e887350d6d5db2c08@killthe.net>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
 <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
 <20220224154953.0623171e887350d6d5db2c08@killthe.net>
Message-ID: <1cb3203c-2cde-f010-2edc-4ce2ff2a9695@measurement-factory.com>

On 2/24/22 16:49, Dave Blanchard wrote:

> This tutorial situation is really out of control. Sadly, this is what
> can be expected to happen when the syntax is changed with every
> version. Now we're in a real mess. I hope the Squid developers will
> make up their minds on how they want the syntax to be structured,
> build it that way, then LEAVE IT ALONE!

I do not know what tutorial(s) you are talking about, but ssl_bump 
syntax and the underlying principles have not changed for many years. 
The changing syntax is definitely not the problem. Squid bugs, lack of 
official tutorials, and TLS complexities/changes are.

SslBump is not compatible with a natural human desire to cut-and-paste 
examples from the first place they can find and see whether those 
examples "work" (multiplied by another human desire to publish their 
thoughts/results, regardless of the author level of expertise).

Can we publish an authoritative high-quality tutorial on SslBump? Sure! 
Unfortunately, the current lack of Project resources multiplied by 
chronic Project deficiencies do not leave us enough time to do that.

Alex.


From andreas.weigel at securepoint.de  Fri Feb 25 16:16:19 2022
From: andreas.weigel at securepoint.de (Andreas Weigel)
Date: Fri, 25 Feb 2022 16:16:19 +0000
Subject: [squid-users] [EXTERN] Re:  getsockopt failures,
 although direct access to intercept ports is blocked
In-Reply-To: <b75703c5-3be1-a645-7e05-3b557b5b1336@treenet.co.nz>
References: <20220223230558.Horde.4oLy24HP_6s1SU7RO0yA1L_@webmail.intern.securepoint.de>
 <b75703c5-3be1-a645-7e05-3b557b5b1336@treenet.co.nz>
Message-ID: <20220225161619.Horde.7gf_rdQ4chkiUNDGk0wWqQK@webmail.intern.securepoint.de>

Hi Amos,

thank you so much for your reply. I was starting to suspect that the  
cause was located somewhere outside Squid or its config.

>  Is this happening at times of unusually high client connections  
> through the NAT?
>  Is eCAP processing blocking the Squid worker for all those seconds?

I was only able to reproduce the issue by provoking multiple  
long-running ecap transactions by curl-ing a certain .deb file. In my  
little test setup, there was not much of other client traffic (only  
one client with an open browser). The eCAP Adapter uses poll to wait  
for the response of a virus scanner on a unix domain socket, so yes, I  
assume this blocks the worker. I will probably have to look into Async  
operation with regard to this issue.

> That part is likely to be the issue recently worked around by

Thanks for the patch, I will definitely try it, this will already go a  
long way to help mitigate the problem.

Kind regards,
Andreas

Zitat von Amos Jeffries <squid3 at treenet.co.nz>:

> On 24/02/22 12:05, Andreas Weigel wrote:
>> Hi everyone,
>>
>> I had the following issue with Squid in Transparent Mode (and SSL  
>> Interception in mode splice). It is working as expected, however  
>> after multiple long-running (talking about several seconds)  
>> anti-virus ecap-Processes have finished, I *sometimes* get the  
>> following in the log:
>>
>> 2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(224)  
>> doAccept: New connection on FD 21
>> 2022/02/23 14:56:40.668 kid1| 5,2| src/comm/TcpAcceptor.cc(312)  
>> acceptNext: connection on local=[::]:2412 remote=[::] FD 21 flags=41
>> 2022/02/23 14:56:40.668 kid1| 89,5| src/ip/Intercept.cc(405)  
>> Lookup: address BEGIN: me/client= 192.168.180.1:2412,  
>> destination/me= 192.168.180.10:48582
>> 2022/02/23 14:56:40.668 kid1| ERROR: NF getsockopt(ORIGINAL_DST)  
>> failed on local=192.168.180.1:2412 remote=192.168.180.10:48582 FD  
>> 37 flags=33: (2) No such file or directory
>> 2022/02/23 14:56:40.669 kid1| 89,9| src/ip/Intercept.cc(151)  
>> NetfilterInterception: address: local=192.168.180.1:2412  
>> remote=192.168.180.10:48582 FD 37 flags=33
>> 2022/02/23 14:56:40.669 kid1| ERROR: NAT/TPROXY lookup failed to  
>> locate original IPs on local=192.168.180.1:2412  
>> remote=192.168.180.10:48582 FD 37 flags=33
>
>
> These can happen if the NAT table entries expire or otherwise get  
> dropped by conntrack between the client initiating TCP SYN and Squid  
> accept(2) receiving the connection.
>
> Your config looks good to me and the lack of regularity indicates  
> the issue is likely this type of transient state situation.
>  Is this happening at times of unusually high client connections  
> through the NAT?
>  Is eCAP processing blocking the Squid worker for all those seconds?
>
>
>> 2022/02/23 14:56:40.669 kid1| 5,5| src/comm/TcpAcceptor.cc(287)  
>> acceptOne: non-recoverable error: FD 21, [::] [ job2] handler  
>> Subscription: 0x55edac3d08d0*1
>>
>> Sometimes, this only appears on on of the two interception ports,  
>> sometimes on both. After that, the squid worker does not poll the  
>> intercept listen port any longer, i.e. stops working.
>
> That part is likely to be the issue recently worked around by  
> <http://www.squid-cache.org/Versions/v6/changesets/squid-6-9fd3e68c3d0dfd6035db98ce142cf425be6c5fc1.patch>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users





From rousskov at measurement-factory.com  Fri Feb 25 16:18:45 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Feb 2022 11:18:45 -0500
Subject: [squid-users] Squid Question regarding tcp handshake
In-Reply-To: <CADcj3=77oZ_mSPAkL2KYujLZPOo-HgTJZWaoVZT8AH4WQExgjw@mail.gmail.com>
References: <CADcj3=77oZ_mSPAkL2KYujLZPOo-HgTJZWaoVZT8AH4WQExgjw@mail.gmail.com>
Message-ID: <d172f6c4-4c42-b827-61da-d01904b4a76c@measurement-factory.com>

On 2/24/22 10:03, Felipe Polanco wrote:

> Does squid first complete the tcp handshake on its users and then a 
> second handshake on the destination IP 

Yes, kind of. Keep in mind that Squid pretty much does not know anything 
about TCP handshakes, SYN packets, etc. Nearly all TCP-level work is 
done by the kernel and system libraries. Squid uses TCP socket I/O.

Also, there may be no corresponding Squid-server connection at all 
(e.g., errors and cache hits) or, as Amos has already noted, multiple 
Squid-server connections (e.g., trying different IP addresses associated 
with the same DNS name in HTTP request target).


> or as soon as it receives the TCP 
> SYN flag it does the same with the destination.

No, Squid does not known anything about the SYN flag. Squid does not act 
on the incoming TCP connection until a successful accept(2) system call 
announces/gives that connection to Squid. That system call extracts an 
already established (i.e. post-handshake) TCP connection from the 
queue(s) of TCP connections maintained by the kernel. The TCP handshake 
completes before Squid gets control back from accept() and, in the vast 
majority of cases, before Squid even calls accept().


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Feb 25 16:28:58 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Feb 2022 11:28:58 -0500
Subject: [squid-users] Trying to set up SSL cache
In-Reply-To: <776152f1-8bf2-286b-6308-0eeead344674@treenet.co.nz>
References: <20220223202611.5852047e8ec14fc337435e83@killthe.net>
 <776152f1-8bf2-286b-6308-0eeead344674@treenet.co.nz>
Message-ID: <121bd62e-2a91-271c-0cc8-722c94d51137@measurement-factory.com>

On 2/25/22 06:16, Amos Jeffries wrote:
> On 24/02/22 15:26, Dave Blanchard wrote:

>> ssl_bump peek all
> 
> Okay TLS handshake clientHello gets observed by Squid.

... and TLS ServerHello. The "all" ACL will match during SslBump step1 
_and_ step2


>> ssl_bump bump all
> 
> ... now (step3) everything gets decrypted.

No, the above ssl_bump configuration line has no effect. In modern 
environments, one cannot bump after peeking at the server.


Squid bugs notwithstanding, the configuration in question is equivalent to:

   ssl_bump peek step1
   ssl_bump peek step2
   ssl_bump splice step3

No HTTP caching is possible for HTTPS transactions with this (or any 
other splicing) configuration, of course.

Alex.


From dave at killthe.net  Fri Feb 25 19:15:44 2022
From: dave at killthe.net (Dave Blanchard)
Date: Fri, 25 Feb 2022 13:15:44 -0600
Subject: [squid-users] Trying to set up SSL cache
In-Reply-To: <776152f1-8bf2-286b-6308-0eeead344674@treenet.co.nz>
References: <20220223202611.5852047e8ec14fc337435e83@killthe.net>
 <776152f1-8bf2-286b-6308-0eeead344674@treenet.co.nz>
Message-ID: <20220225131544.c3d430ba8eeca73133f498d2@killthe.net>

On Sat, 26 Feb 2022 00:16:30 +1300
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> [...]
>
> There are a few things to be aware of while troubleshooting:
> 
> * not all TLS connections can be bump'ed. TLS is designed to prevent 
> exactly the type of decrypt that bump does. If the client and server are 
> using TLS properly bump *will* fail.
> 
> 
> * Google are known to be rather pedantic about security. So having their 
> software at either end of the TLS when testing is more likely to hit 
> such non-decryptable TLS connections.
> 
> 
> * Checking the test web service for TLS certificate pinning or DANE. 
> Both of these lock the/some client into using the original server 
> certificate and they will unavoidably reject the Squid signing CA.
> 
> * Check traffic from the web server for HTTPS-Transport-Security or 
> Alt-Svc HTTP headers. Both of these can break SSL-Bump if they reach a 
> client. What is worse they can force arbitrarily long cache times for 
> the info they contain, causing breakage to extend across the whole 
> period. Only a full client purge of state and never receiving the info 
> again can via any protocol fix these.
> 
> 
> Amos

Hi Amos, this is very useful info; thank you. I've just added in some rules to drop the HSTS and Alt-Svc headers to avoid this trouble. Thanks for explaining step by step how these bump rules affect the proxy behavior.

-- 
Dave Blanchard <dave at killthe.net>


From uhlar at fantomas.sk  Fri Feb 25 19:36:11 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 25 Feb 2022 20:36:11 +0100
Subject: [squid-users] peek & splice only to log ssl info
Message-ID: <Yhkvq5Y82fpZpKMh@fantomas.sk>

Hello,

I'll upgrade some debian servers to debian 10 where squid-ssl package (4.13) 
is available and I'm searching for way to enhance current logging of CONNECT 
requests to include SSL data - SNI servername and possibly basic certificate 
info.

I only intend to splice connections but after repeated reading 
https://wiki.squid-cache.org/Features/SslPeekAndSplice I still don't 
understand parts of the logic.

- is the combination described at:
https://wiki.squid-cache.org/Features/SslPeekAndSplice#Basic_Splicing_and_Bumping
enough for logging SNI and cert info?

- are peek and they completely equal at step 1?

- what's the difference between peek and splice that makes it impossible 
   (most of the time) to splice (stare) or bump (peek) the connection?
   I guess I don't really need to know this one but I'm curious.


thanks for comments.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Your mouse has moved. Windows NT will now restart for changes to take
to take effect. [OK]


From gtaylor at tnetconsulting.net  Fri Feb 25 19:37:24 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Fri, 25 Feb 2022 12:37:24 -0700
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <1e079475-ddb9-9e0d-37c9-12c821279db1@treenet.co.nz>
References: <20220223210903.6b9730ea30217d9790a58e97@killthe.net>
 <ff2a9d45-f6d8-e644-08e6-4d2c5367aec2@measurement-factory.com>
 <cb3fcb2e-27f7-98a7-42b0-26c3f544cff6@spamtrap.tnetconsulting.net>
 <1e079475-ddb9-9e0d-37c9-12c821279db1@treenet.co.nz>
Message-ID: <aa97b580-f78d-54ef-ad4b-8098d432145a@spamtrap.tnetconsulting.net>

On 2/25/22 5:53 AM, Amos Jeffries wrote:
> At this point I should remind that it is a *wiki* and largely written by 
> all of us in the community.

ACK

> The ConfigExamples section a collection of configs that people here in 
> the community are (or were) actively using and found useful to share.
> We (the main devs) have made a point of trying to update the contents to 
> prune mistakes and mention when there are Squid version-specific 
> differences. That is all.

ACK

> That said, Francesco is working on converting the wiki content to 
> another format so right now is not good for edits.

ACK

> Sounds like you are talking about 
> <https://wiki.squid-cache.org/Features/HTTPS>. From that page it is a 
> matter of selecting which situation is the use-case and diving into the 
> rabbit hole of links.

Please don't take my comments as a complaint, and definitely not related 
to any specific piece of documentation.

I was speaking in generalities and not referring to anything specific.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/6d86ac34/attachment.bin>

From dave at killthe.net  Fri Feb 25 21:08:03 2022
From: dave at killthe.net (Dave Blanchard)
Date: Fri, 25 Feb 2022 15:08:03 -0600
Subject: [squid-users] Random trouble with image downloads
In-Reply-To: <1041b94e-d8c8-b4c2-d07c-a37759c39964@treenet.co.nz>
References: <20220224231430.f046b84f7c376ed796c5f203@killthe.net>
 <1041b94e-d8c8-b4c2-d07c-a37759c39964@treenet.co.nz>
Message-ID: <20220225150803.db324f15b60993eeac9dc3e2@killthe.net>

On Sat, 26 Feb 2022 02:09:14 +1300
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> I suspect you are seeing <https://bugs.squid-cache.org/show_bug.cgi?id=4882>
> 
> Unfortunately we do not have a good fix for it yet.

OK, it looks like the work is still ongoing, so I will implement some kind of workaround for now and check back on this later. Thanks for the info.

-- 
Dave Blanchard <dave at killthe.net>


From dave at killthe.net  Fri Feb 25 21:17:43 2022
From: dave at killthe.net (Dave Blanchard)
Date: Fri, 25 Feb 2022 15:17:43 -0600
Subject: [squid-users] Trying to set up SSL cache - solved!
In-Reply-To: <0c59deb4-2b17-30cb-eabe-74ddc82e643b@treenet.co.nz>
References: <20220224122435.257ddc228d11dff7cd21285c@killthe.net>
 <1ac60e46-7f77-9bcc-72b2-92873d34eb41@measurement-factory.com>
 <20220224133856.b26137522496068ee70ea708@killthe.net>
 <e3380183-ef20-ca1f-54d6-4059cf7c3305@measurement-factory.com>
 <20220224154953.0623171e887350d6d5db2c08@killthe.net>
 <0c59deb4-2b17-30cb-eabe-74ddc82e643b@treenet.co.nz>
Message-ID: <20220225151743.2a2508927844d05fe83e501a@killthe.net>

On Sat, 26 Feb 2022 02:06:23 +1300
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> Agreed. Luckily we hear you (Alex and I are pretty much "them" these days).
> 

Hopefully I don't sound unappreciative; I'm thankful for the work you've put into Squid, and the quick and useful responses you've given here on the list, which has resolved my problem. It's not always easy to properly document something that you are close to and see every day; it's often difficult to really see things how a stranger would, and as you've pointed out, proxy'ing in general isn't a simple subject. Squid is a powerful tool and I'm happy to have it available. Keep up the good work!

-- 
Dave Blanchard <dave at killthe.net>


From rousskov at measurement-factory.com  Fri Feb 25 23:07:11 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 Feb 2022 18:07:11 -0500
Subject: [squid-users] peek & splice only to log ssl info
In-Reply-To: <Yhkvq5Y82fpZpKMh@fantomas.sk>
References: <Yhkvq5Y82fpZpKMh@fantomas.sk>
Message-ID: <6aa96517-cdf8-e6ef-ea71-3279897f327b@measurement-factory.com>

On 2/25/22 14:36, Matus UHLAR - fantomas wrote:

> I only intend to splice connections but after repeated reading 
> https://wiki.squid-cache.org/Features/SslPeekAndSplice I still don't 
> understand parts of the logic.
> 
> - is the combination described at:
> https://wiki.squid-cache.org/Features/SslPeekAndSplice#Basic_Splicing_and_Bumping 
> 
> enough for logging SNI and cert info?

There are three combinations described in that section. The first peeks 
at SNI and certificate info (so the answer is "yes"). The other two are 
more complex and may not have access to some of that info in some cases.


> - are peek and they completely equal at step 1?

Bugs notwithstanding, Squid does the same thing right after discovering 
that a peek or stare rule matched during step1 -- Squid tries to look at 
the TLS client Hello message (where SNI is stored).

The difference, if any, only comes after Squid looks at that 
ClientHello. Bugs notwithstanding(*), if no ssl_bump rule matches during 
step2, then the next Squid action will be either splice or bump, 
depending on which rule (peek or stare) matched at the first step.

By using "peek", you tell Squid that you intend to splice if everything 
goes alright; and by using stare, you tell Squid that you intend to 
bump. After step1, you can still change your mind (because the immediate 
Squid operations are the same -- look at ClientHello). After step2, you 
cannot (because Squid operations differ and, in modern environments, 
peeking precludes future bumping and staring precludes future splicing 
as detailed further below).

(*) There are recently discovered bugs in this area (that we are 
fixing), so you should not rely on this, but that is what Squid will be 
doing when those bugs are fixed. I do not recommend relying on such 
"defaults" anyway -- make sure the step after a peek or stare rule match 
has a matching rule.


> - what's the difference between peek and splice that makes it impossible 
>  ? (most of the time) to splice (stare) or bump (peek) the connection?

* When Squid peeks, it forwards the user agent TLS client Hello message 
to the TLS server intact. After forwarding that virgin Hello, Squid 
cannot become a part of the TLS conversation. Squid has to splice or 
terminate the connections, which are both TCP- not TLS-level operations.

* When Squid stares, Squid modifies the TLS client Hello received from 
the user agent to use Squid-specific TLS secrets and then sends the 
adapted ClientHello to the TLS server. After that, it is impossible for 
Squid to get out of the loop -- the conversation is now based on 
Squid-provided secrets. Squid has to bump or terminate the connections.


HTH,

Alex.


From musamamehboob at gmail.com  Sat Feb 26 05:58:08 2022
From: musamamehboob at gmail.com (Usama Mehboob)
Date: Sat, 26 Feb 2022 00:58:08 -0500
Subject: [squid-users] Getting SSL Connection Errors (Eliezer Croitoru)
In-Reply-To: <mailman.8353.1645785237.971461.squid-users@lists.squid-cache.org>
References: <mailman.8353.1645785237.971461.squid-users@lists.squid-cache.org>
Message-ID: <CAPjjjygJoQOiHV8KNuADTGhX9HiZXHohPd89BrZm+B9OLZx5gA@mail.gmail.com>

I think on previous mailing list I pasted the whole content. So I am again
sending my reply in sort of confined way. :)

Eliezer, I am running on amazon linux 2 ami which I suppose is based on
centos.
I ran the uname -a command and this is what I get;;
Linux ip-172-24-9-143.us-east-2.compute.internal
4.14.256-197.484.amzn2.x86_64 #1 SMP Tue Nov 30 00:17:50 UTC 2021 x86_64
x86_64 x86_64 GNU/Linux

[ec2-user at ip-172-24-9-143 ~]$ openssl version
OpenSSL 1.0.2k-fips  26 Jan 2017

thanks so much and let me know the script and I can run on this machine.
Usama

>
> Message: 1
> Date: Fri, 25 Feb 2022 07:01:12 +0200
> From: "Eliezer Croitoru" <ngtech1ltd at gmail.com>
> To: "'Usama Mehboob'" <musamamehboob at gmail.com>,
>         <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Getting SSL Connection Errors
> Message-ID: <006f01d82a04$b678b770$236a2650$@gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hey Usama,
>
>
>
> There are more missing details on the system.
>
> If you provide the OS and squid details I might be able to provide a
> script that will pull most of the relevant details on the system.
>
> I don?t know about this specific issue yet and it seems like there is a
> SSL related issue and it might not be even related to Squid.
>
> (@Alex Or @Chrisots might know better then me)
>
>
>
> All The Bests,
>
>
>
> ----
>
> Eliezer Croitoru
>
> NgTech, Tech Support
>
> Mobile: +972-5-28704261
>
> Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>
>
>
>
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Usama Mehboob
> Sent: Thursday, February 24, 2022 23:45
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Getting SSL Connection Errors
>
>
>
> Hi I have a squid running on a linux box ( about 16GB ram and 4 cpu ) --
> it runs fine for the most part but when I am launching multiple jobs that
> are connecting with salesforce BulkAPI, sometimes connections are dropped.
> its not predictable and happens only when there is so much load on squid.
> Can anyone shed some light on this? what can I do? is it a file descriptor
> issue?
>
> I see only these error messages from the cache logs
> ```
> PeerConnector.cc(639) handleNegotiateError: Error (error:04091068:rsa
> routines:INT_RSA_VERIFY:bad signature) but, hold write on SSL connection on
> FD 109
> ```
>
> ----------------Config file ----------------
> visible_hostname squid
>
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>  # RFC1918 possible
> internal network
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  # RFC1918 possible
> internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>  # RFC1918
> possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> ###acl Safe_ports port 21 # ftp testing after blocking itp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> #http_access allow CONNECT SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
>
> # And finally deny all other access to this proxy
>
> # Squid normally listens to port 3128
> #http_port 3128
> http_port 3129 intercept
> https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> http_access allow SSL_ports #-- this allows every https website
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1 all
>
> # Deny requests to proxy instance metadata
> acl instance_metadata dst 169.254.169.254
> http_access deny instance_metadata
>
> # Filter HTTP Only requests based on the whitelist
> #acl allowed_http_only dstdomain .veevasourcedev.com <
> http://veevasourcedev.com>  .google.com <http://google.com>  .pypi.org <
> http://pypi.org>  .youtube.com <http://youtube.com>
> #acl allowed_http_only dstdomain .amazonaws.com <http://amazonaws.com>
> #acl allowed_http_only dstdomain .veevanetwork.com <
> http://veevanetwork.com>  .veevacrm.com <http://veevacrm.com>  .
> veevacrmdi.com <http://veevacrmdi.com>  .veeva.com <http://veeva.com>  .
> veevavault.com <http://veevavault.com>  .vaultdev.com <http://vaultdev.com>
> .veevacrmqa.com <http://veevacrmqa.com>
> #acl allowed_http_only dstdomain .documentforce.com <
> http://documentforce.com>   .sforce.com <http://sforce.com>  .force.com <
> http://force.com>  .forceusercontent.com <http://forceusercontent.com>  .
> force-user-content.com <http://force-user-content.com>  .lightning.com <
> http://lightning.com>  .salesforce.com <http://salesforce.com>  .
> salesforceliveagent.com <http://salesforceliveagent.com>  .
> salesforce-communities.com <http://salesforce-communities.com>  .
> salesforce-experience.com <http://salesforce-experience.com>  .
> salesforce-hub.com <http://salesforce-hub.com>  .salesforce-scrt.com <
> http://salesforce-scrt.com>  .salesforce-sites.com <
> http://salesforce-sites.com>  .site.com <http://site.com>  .sfdcopens.com
> <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>
> .trailhead.com <http://trailhead.com>  .visualforce.com <
> http://visualforce.com>
>
>
> # Filter HTTPS requests based on the whitelist
> acl allowed_https_sites ssl::server_name .pypi.org <http://pypi.org>  .
> pythonhosted.org <http://pythonhosted.org>  .tfhub.dev <http://tfhub.dev>
> .gstatic.com <http://gstatic.com>  .googleapis.com <http://googleapis.com>
>
> acl allowed_https_sites ssl::server_name .amazonaws.com <
> http://amazonaws.com>
> acl allowed_https_sites ssl::server_name .documentforce.com <
> http://documentforce.com>   .sforce.com <http://sforce.com>  .force.com <
> http://force.com>  .forceusercontent.com <http://forceusercontent.com>  .
> force-user-content.com <http://force-user-content.com>  .lightning.com <
> http://lightning.com>  .salesforce.com <http://salesforce.com>  .
> salesforceliveagent.com <http://salesforceliveagent.com>  .
> salesforce-communities.com <http://salesforce-communities.com>  .
> salesforce-experience.com <http://salesforce-experience.com>  .
> salesforce-hub.com <http://salesforce-hub.com>  .salesforce-scrt.com <
> http://salesforce-scrt.com>  .salesforce-sites.com <
> http://salesforce-sites.com>  .site.com <http://site.com>  .sfdcopens.com
> <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>
> .trailhead.com <http://trailhead.com>  .visualforce.com <
> http://visualforce.com>
> ssl_bump peek step2 allowed_https_sites
> ssl_bump splice step3 allowed_https_sites
> ssl_bump terminate step2 all
>
>
> connect_timeout 60 minute
> read_timeout 60 minute
> write_timeout 60 minute
> request_timeout 60 minute
>
> ## http filtering ###
> #http_access allow localnet allowed_http_only
> #http_access allow localhost allowed_http_only
> http_access allow localnet allowed_https_sites
> http_access allow localhost allowed_https_sites
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
>
>
>
> thanks
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/9f94f144/attachment-0001.htm
> >
>
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 90, Issue 38
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220226/0bee2457/attachment.htm>

From ngtech1ltd at gmail.com  Sat Feb 26 19:47:25 2022
From: ngtech1ltd at gmail.com (Eliezer Croitoru)
Date: Sat, 26 Feb 2022 21:47:25 +0200
Subject: [squid-users] Getting SSL Connection Errors (Eliezer Croitoru)
In-Reply-To: <CAPjjjygJoQOiHV8KNuADTGhX9HiZXHohPd89BrZm+B9OLZx5gA@mail.gmail.com>
References: <mailman.8353.1645785237.971461.squid-users@lists.squid-cache.org>
 <CAPjjjygJoQOiHV8KNuADTGhX9HiZXHohPd89BrZm+B9OLZx5gA@mail.gmail.com>
Message-ID: <009f01d82b49$ae8a0f00$0b9e2d00$@gmail.com>

Hey Usama,

 

I took the time to make sure that the script will work on amzn linux 2:

https://github.com/elico/squid-suppsave

 

it?s a Makefile and a tiny hardware data collection tool.

You can clone the git and then in the directory of the git repo you can enter the next command:

make amzn2-install-suppsave-deps support-save

 

And also, how did you installed squid on the amazon linux machine? Using:

amazon-linux-extras install squid4

 

??

And also just do you would know that I am compiling squid for amzn linux 2 and the files/repo is at:

https://www.ngtech.co.il/repo/amzn/2/x86_64/

 

It is not compiled with ecap support and it works for most use cases I have seen until now.

 

The support script will create a file at /etc/support?.tar.gz

Please make sure that if you are using ssl bump you will need to remove the ssl bump root CA details.

If you still wish to send the full file to me as is, just make sure you will send it to me and only not and not to the public list.

(Unless it?s a testing machine..)

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Usama Mehboob
Sent: Saturday, February 26, 2022 07:58
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Getting SSL Connection Errors (Eliezer Croitoru)

 

I think on previous mailing list I pasted the whole content. So I am again sending my reply in sort of confined way. :) 

Eliezer, I am running on amazon linux 2 ami which I suppose is based on
centos.
I ran the uname -a command and this is what I get;;
Linux ip-172-24-9-143.us-east-2.compute.internal
4.14.256-197.484.amzn2.x86_64 #1 SMP Tue Nov 30 00:17:50 UTC 2021 x86_64
x86_64 x86_64 GNU/Linux

[ec2-user at ip-172-24-9-143 ~]$ openssl version
OpenSSL 1.0.2k-fips  26 Jan 2017

thanks so much and let me know the script and I can run on this machine.
Usama


Message: 1
Date: Fri, 25 Feb 2022 07:01:12 +0200
From: "Eliezer Croitoru" <ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> >
To: "'Usama Mehboob'" <musamamehboob at gmail.com <mailto:musamamehboob at gmail.com> >,
        <squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> >
Subject: Re: [squid-users] Getting SSL Connection Errors
Message-ID: <006f01d82a04$b678b770$236a2650$@gmail.com <http://gmail.com> >
Content-Type: text/plain; charset="utf-8"

Hey Usama,



There are more missing details on the system.

If you provide the OS and squid details I might be able to provide a script that will pull most of the relevant details on the system.

I don?t know about this specific issue yet and it seems like there is a SSL related issue and it might not be even related to Squid.

(@Alex Or @Chrisots might know better then me)



All The Bests,



----

Eliezer Croitoru

NgTech, Tech Support

Mobile: +972-5-28704261

Email: ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>  <mailto:ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com> > 



From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of Usama Mehboob
Sent: Thursday, February 24, 2022 23:45
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Getting SSL Connection Errors



Hi I have a squid running on a linux box ( about 16GB ram and 4 cpu ) -- it runs fine for the most part but when I am launching multiple jobs that are connecting with salesforce BulkAPI, sometimes connections are dropped. its not predictable and happens only when there is so much load on squid. Can anyone shed some light on this? what can I do? is it a file descriptor issue?

I see only these error messages from the cache logs
```
PeerConnector.cc(639) handleNegotiateError: Error (error:04091068:rsa routines:INT_RSA_VERIFY:bad signature) but, hold write on SSL connection on FD 109
```

----------------Config file ----------------
visible_hostname squid 

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>  <http://10.0.0.0/8>  # RFC1918 possible internal network
acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  <http://172.16.0.0/12>  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>  <http://192.168.0.0/16>  # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80 # http
###acl Safe_ports port 21 # ftp testing after blocking itp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
#http_access allow CONNECT SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed

# And finally deny all other access to this proxy

# Squid normally listens to port 3128
#http_port 3128
http_port 3129 intercept
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept 
http_access allow SSL_ports #-- this allows every https website
acl step1 at_step SslBump1 
acl step2 at_step SslBump2 
acl step3 at_step SslBump3 
ssl_bump peek step1 all 

# Deny requests to proxy instance metadata 
acl instance_metadata dst 169.254.169.254 
http_access deny instance_metadata 

# Filter HTTP Only requests based on the whitelist 
#acl allowed_http_only dstdomain .veevasourcedev.com <http://veevasourcedev.com>  <http://veevasourcedev.com>  .google.com <http://google.com>  <http://google.com>  .pypi.org <http://pypi.org>  <http://pypi.org>  .youtube.com <http://youtube.com>  <http://youtube.com> 
#acl allowed_http_only dstdomain .amazonaws.com <http://amazonaws.com>  <http://amazonaws.com> 
#acl allowed_http_only dstdomain .veevanetwork.com <http://veevanetwork.com>  <http://veevanetwork.com>  .veevacrm.com <http://veevacrm.com>  <http://veevacrm.com>  .veevacrmdi.com <http://veevacrmdi.com>  <http://veevacrmdi.com>  .veeva.com <http://veeva.com>  <http://veeva.com>  .veevavault.com <http://veevavault.com>  <http://veevavault.com>  .vaultdev.com <http://vaultdev.com>  <http://vaultdev.com>  .veevacrmqa.com <http://veevacrmqa.com>  <http://veevacrmqa.com> 
#acl allowed_http_only dstdomain .documentforce.com <http://documentforce.com>  <http://documentforce.com>   .sforce.com <http://sforce.com>  <http://sforce.com>  .force.com <http://force.com>  <http://force.com>  .forceusercontent.com <http://forceusercontent.com>  <http://forceusercontent.com>  .force-user-content.com <http://force-user-content.com>  <http://force-user-content.com>  .lightning.com <http://lightning.com>  <http://lightning.com>  .salesforce.com <http://salesforce.com>  <http://salesforce.com>  .salesforceliveagent.com <http://salesforceliveagent.com>  <http://salesforceliveagent.com>  .salesforce-communities.com <http://salesforce-communities.com>  <http://salesforce-communities.com>  .salesforce-experience.com <http://salesforce-experience.com>  <http://salesforce-experience.com>  .salesforce-hub.com <http://salesforce-hub.com>  <http://salesforce-hub.com>  .salesforce-scrt.com <http://salesforce-scrt.com>  <http://salesforce-scrt.com>  .salesforce-sites.com <http://salesforce-sites.com>  <http://salesforce-sites.com>  .site.com <http://site.com>  <http://site.com>  .sfdcopens.com <http://sfdcopens.com>  <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>  <http://trailblazer.me>  .trailhead.com <http://trailhead.com>  <http://trailhead.com>  .visualforce.com <http://visualforce.com>  <http://visualforce.com> 


# Filter HTTPS requests based on the whitelist 
acl allowed_https_sites ssl::server_name .pypi.org <http://pypi.org>  <http://pypi.org>  .pythonhosted.org <http://pythonhosted.org>  <http://pythonhosted.org>  .tfhub.dev <http://tfhub.dev>  <http://tfhub.dev>  .gstatic.com <http://gstatic.com>  <http://gstatic.com>  .googleapis.com <http://googleapis.com>  <http://googleapis.com> 
acl allowed_https_sites ssl::server_name .amazonaws.com <http://amazonaws.com>  <http://amazonaws.com> 
acl allowed_https_sites ssl::server_name .documentforce.com <http://documentforce.com>  <http://documentforce.com>   .sforce.com <http://sforce.com>  <http://sforce.com>  .force.com <http://force.com>  <http://force.com>  .forceusercontent.com <http://forceusercontent.com>  <http://forceusercontent.com>  .force-user-content.com <http://force-user-content.com>  <http://force-user-content.com>  .lightning.com <http://lightning.com>  <http://lightning.com>  .salesforce.com <http://salesforce.com>  <http://salesforce.com>  .salesforceliveagent.com <http://salesforceliveagent.com>  <http://salesforceliveagent.com>  .salesforce-communities.com <http://salesforce-communities.com>  <http://salesforce-communities.com>  .salesforce-experience.com <http://salesforce-experience.com>  <http://salesforce-experience.com>  .salesforce-hub.com <http://salesforce-hub.com>  <http://salesforce-hub.com>  .salesforce-scrt.com <http://salesforce-scrt.com>  <http://salesforce-scrt.com>  .salesforce-sites.com <http://salesforce-sites.com>  <http://salesforce-sites.com>  .site.com <http://site.com>  <http://site.com>  .sfdcopens.com <http://sfdcopens.com>  <http://sfdcopens.com>  .sfdc.sh .trailblazer.me <http://trailblazer.me>  <http://trailblazer.me>  .trailhead.com <http://trailhead.com>  <http://trailhead.com>  .visualforce.com <http://visualforce.com>  <http://visualforce.com> 
ssl_bump peek step2 allowed_https_sites 
ssl_bump splice step3 allowed_https_sites 
ssl_bump terminate step2 all


connect_timeout 60 minute
read_timeout 60 minute
write_timeout 60 minute
request_timeout 60 minute

## http filtering ###
#http_access allow localnet allowed_http_only
#http_access allow localhost allowed_http_only
http_access allow localnet allowed_https_sites
http_access allow localhost allowed_https_sites
# And finally deny all other access to this proxy
http_access deny all

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320



thanks

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220225/9f94f144/attachment-0001.htm>



------------------------------

End of squid-users Digest, Vol 90, Issue 38
*******************************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220226/7673a622/attachment.htm>

From frank.ruiz at gmail.com  Mon Feb 28 16:42:08 2022
From: frank.ruiz at gmail.com (Frank Ruiz)
Date: Mon, 28 Feb 2022 08:42:08 -0800
Subject: [squid-users] Squid 5 OOM
Message-ID: <CAARSQBcouqJWVmZgjiQ+2=nWGALPURZETCY2t5pjTsi+ybMrhQ@mail.gmail.com>

Greetings,

I am using squid 5, and I am trying to use it strictly as a proxy only with
no caching.

 ip-10-4-0-200 kernel: [7520199.557517] Out of memory: Killed process 17574
(squid5) total-vm:15926268kB, anon-rss:15021268kB, file-rss:0kB,
shmem-rss:72kB, UID:13 pgtables:31104kB oom_score_adj:0

Is there a way to disable caching completely for my use case?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20220228/c786bafd/attachment.htm>

