From gtaylor at tnetconsulting.net  Mon Oct  3 17:52:23 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Mon, 3 Oct 2022 11:52:23 -0600
Subject: [squid-users] What is in a name? Squid vs SQuID
Message-ID: <4f6c9a79-03a9-3a6f-5b36-fccb14bf7172@spamtrap.tnetconsulting.net>

Hi,

I ran across the following statement referring to Squid in an ancient 
Sys Admin article talking about Linux Transparent Proxy.

> Source Quench Introduced Delay (SQuID) is a popular freeware proxy 
> server for UNIX machines (see "Software Resources" sidebar for more 
> information).

Where the "Software Resources" sidebar has:

>  Squid can be obtained from any of following sites:
> 
> http://squid.nalanr.net
> ftp://squid.nlanr.net
> 
> The Squid Web site also contains a wealth of installation and 
> configuration information. Squid manuals are also available from 
> this site.

Does anyone know any history on Squid's name?  Was SQuID the proper name 
for the Squid caching proxy at one point in time?  Or is this perhaps a 
bad expansion in the article?

Link - Linux Transparent Proxy (Sys Admin, May 1999, Volume 8, Issue 5, 
Article 3)
  - https://www.muppetwhore.net/sysadmin/html/v08/i05/a3.htm



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221003/644f582c/attachment.bin>

From rousskov at measurement-factory.com  Mon Oct  3 19:35:40 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 3 Oct 2022 15:35:40 -0400
Subject: [squid-users] What is in a name? Squid vs SQuID
In-Reply-To: <4f6c9a79-03a9-3a6f-5b36-fccb14bf7172@spamtrap.tnetconsulting.net>
References: <4f6c9a79-03a9-3a6f-5b36-fccb14bf7172@spamtrap.tnetconsulting.net>
Message-ID: <682f1a86-9207-f420-70d6-cba44effd1e5@measurement-factory.com>

On 10/3/22 13:52, Grant Taylor wrote:

> I ran across the following statement referring to Squid in an ancient 
> Sys Admin article talking about Linux Transparent Proxy.
> 
>> Source Quench Introduced Delay (SQuID) is a popular freeware proxy 
>> server for UNIX machines (see "Software Resources" sidebar for more 
>> information).
> 
> Where the "Software Resources" sidebar has:
> 
>> ?Squid can be obtained from any of following sites:
>>
>> http://squid.nalanr.net
>> ftp://squid.nlanr.net
>>
>> The Squid Web site also contains a wealth of installation and 
>> configuration information. Squid manuals are also available from this 
>> site.
> 
> Does anyone know any history on Squid's name?? Was SQuID the proper name 
> for the Squid caching proxy at one point in time?? Or is this perhaps a 
> bad expansion in the article?

There is no relationship between the SQuID concept (RFC 1016) and our 
Squid Cache. I double-checked with Duane, the Squid creator. I bet the 
author of that article thought that Squid is an acronym and found a 
matching acronym in RFC 1016 :-).


HTH,

Alex.



> Link - Linux Transparent Proxy (Sys Admin, May 1999, Volume 8, Issue 5, 
> Article 3)
>  ?- https://www.muppetwhore.net/sysadmin/html/v08/i05/a3.htm
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From gtaylor at tnetconsulting.net  Mon Oct  3 19:55:56 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Mon, 3 Oct 2022 13:55:56 -0600
Subject: [squid-users] What is in a name? Squid vs SQuID
In-Reply-To: <682f1a86-9207-f420-70d6-cba44effd1e5@measurement-factory.com>
References: <4f6c9a79-03a9-3a6f-5b36-fccb14bf7172@spamtrap.tnetconsulting.net>
 <682f1a86-9207-f420-70d6-cba44effd1e5@measurement-factory.com>
Message-ID: <bc33b1cd-f7a6-5838-1aae-8dc38adc646c@spamtrap.tnetconsulting.net>

On 10/3/22 1:35 PM, Alex Rousskov wrote:
> There is no relationship between the SQuID concept (RFC 1016) and our 
> Squid Cache. I double-checked with Duane, the Squid creator. I bet the 
> author of that article thought that Squid is an acronym and found a 
> matching acronym in RFC 1016 :-).

Thank you for confirmation of what I suspected was the case.

It was one of those surprising things that cause you to do a double take 
and check things you think you know.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221003/8ef5b1b3/attachment.bin>

From dwd at fnal.gov  Tue Oct  4 16:56:36 2022
From: dwd at fnal.gov (Dave Dykstra)
Date: Tue, 4 Oct 2022 16:56:36 +0000
Subject: [squid-users] Missing security announcements
Message-ID: <YzxlxKnamo7GM4zC@fnal.gov>

How did Red Hat and Debian know about 
    https://github.com/squid-cache/squid/security/advisories/GHSA-394c-rr7q-6g78
before the squid-announce mailing list?  It's not even listed at 
    https://github.com/apptainer/apptainer/security
even though there is another one from the same day.

Dave

On Wed, Sep 21, 2022 at 11:43:41PM +1200, Amos Jeffries wrote:
> Subject: Re: [squid-users] Missing squid 5.6 & 5.7 announcements
> On 21/09/22 10:33, Dave Dykstra wrote:
> > I tried sending this directly to Amos twice over the last week or so but
> > it bounced each time.
> > 
> > I noticed that 5.7 is on the website since 5 September, but I have not
> > see a release announcement for that or for 5.6 from June.
> 
> 
> Mea culpa sorry. I am a bit behind on security paperwork needed for those.
> 
> 
> >  I would like
> > to know if it is considered to be in a stable enough state that all
> > squid 4 users are encouraged to upgrade, or not.  The release notes
> > don't tell me that.
> 
> Basically yes we are back at "encourage to upgrade".
> 
> To be specific:
>  * The initial big troubles were resolved in 5.5.
>  * We have two reports of Delay Pools having weird behaviours, but that is
> shared with v4.
> 
>  * WCCP regression (YMMV) in latest security patches has not fully been
> resolved in the official code. Experimental patches are available if
> necessary.
> 
> 
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From Ralf.Hildebrandt at charite.de  Wed Oct  5 11:48:43 2022
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 5 Oct 2022 13:48:43 +0200
Subject: [squid-users] dns_nameservers directive
Message-ID: <Yz1vG1ALsUUwbguJ@charite.de>

Using squid-6.0.0-20220905-r9358e99f9:
======================================

On sunday, one of our DNS server froze and was not answering any
queries. Shit happens, that's why we have another DNS server.

Our Squid config says:

dns_nameservers 141.42.5.157 141.42.5.156

So, 141.42.5.157 was not answering any queries, and 141.42.5.156 took
over. 

But... monitoring reported dns_query_time rose to about 8000ms,
meaning all http requests took ages, since (I guess) the first dns
server was queried, and (after 8s?) the second server was used.

But according to the docs, "dns_timeout" defaults to 30s.

Is there any way of making squid mark the first server as "dead" (for
e.g. 5 minutes) and use the next server instead?

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From m_zouhairy at ckta.by  Wed Oct  5 11:53:48 2022
From: m_zouhairy at ckta.by (Majed Zouhairy)
Date: Wed, 5 Oct 2022 14:53:48 +0300
Subject: [squid-users] sarg error in squid 5.7
In-Reply-To: <7b8cfcc9-bb7e-c0c0-722f-e6e7d05b98c8@treenet.co.nz>
References: <9a1ca752-16bc-72e3-ffcd-5827040aa43e@ckta.by>
 <7b8cfcc9-bb7e-c0c0-722f-e6e7d05b98c8@treenet.co.nz>
Message-ID: <6dca1219-9378-177c-6647-6a8f5bd46db4@ckta.by>

thanks for replying but,

On 9/29/22 19:04, Amos Jeffries wrote:
 > On 30/09/22 03:45, Majed Zouhairy wrote:
 >> Peace, does squid still not support long urls?
 >>
 >
 > Squid supports long URLs. If it did not those log entries would be 
shorter and indicate clients being rejected with "414 URI Too Long".
 >
 > Your problem is the length of the log line. The logging modules have 
limits which may be shorter than what you have configured the acceptible 
URL / request size to be.
 >
 > I suggest trying TCP logging instead of the default stdio or daemon.
 >
how to configure tcp logging on the local machine and not to a remote 
machine?

 > Sarg itself may also have problems with such long URLs.
 >
 >
 > HTH
 > Amos
 > _______________________________________________
 > squid-users mailing list
 > squid-users at lists.squid-cache.org
 > http://lists.squid-cache.org/listinfo/squid-users



From djerkg at gmail.com  Wed Oct  5 13:29:44 2022
From: djerkg at gmail.com (Djerk Geurts)
Date: Wed, 5 Oct 2022 14:29:44 +0100
Subject: [squid-users] LDAP search filter for FreeIPA
Message-ID: <E1C9A0F1-7F2B-48AF-8DE3-7A7EBAC8B12F@gmail.com>

Hi,

I?ve got DLAP auth working against FreeIPA, but now I?m trying to get LDAP group all controls working. Initially I used the local unix group filter, which works great as the machine running Squid is able to query group membership through pam. But then I found that nested group membership didn?t work. So now I?m trying to query group membership via LDAP and failing miserably.

My config:

auth_param basic program /usr/lib/squid/basic_ldap_auth -v 3 -b "cn=users,cn=accounts,dc=DOMAIN,dc=COM" -D "uid=squid-ldap,cn=sysaccounts,cn=etc,dc=DOMAIN,dc=COM" -W "/etc/squid/squid-ldap.cred" -u uid -H LDAPS://ipa.domain.com:636 <ldaps://ipa.domain.com:636>
[?]

external_acl_type ldap_group %LOGIN /usr/lib/squid/ext_ldap_group_acl -v 3 -b "cn=groups,cn=accounts,dc=DOMAIN,dc=COM" -D "uid=squid-ldap,cn=sysaccounts,cn=etc,dc=DOMAIN,dc=COM" -W "/etc/squid/squid-ldap.cred" -f "(&(cn=%g)(member=uid=%u))" -H LDAPS://ipa.domain.com:636 <ldaps://ipa.domain.com:636>


This ldap search works fine:

user at ipa:~$ ldapsearch -x -D 'cn=Directory Manager' -W -b "cn=groups,cn=accounts,dc=DOMAIN,dc=COM" '(&(cn=proxy)(member=uid=user,*))'
Enter LDAP Password:
# extended LDIF
#
# LDAPv3
# base <cn=groups,cn=accounts,dc=DOMAIN,dc=COM> with scope subtree
# filter: (&(cn=proxy)(member=uid=user,*))
# requesting: ALL
#

# proxy, groups, accounts, ipnexia.com
dn: cn=proxy,cn=groups,cn=accounts,dc=ipnexia,dc=com
member: uid=user,cn=users,cn=accounts,dc=ipnexia,dc=com
memberOf: cn=proxyuser,cn=groups,cn=accounts,dc=ipnexia,dc=com
cn: proxy
objectClass: top
objectClass: groupofnames
objectClass: nestedgroup
objectClass: ipausergroup
objectClass: ipaobject
objectClass: GroupOfUniqueNames
objectClass: posixgroup
ipaUniqueID: ******
gidNumber: ******

# search result
search: 2
result: 0 Success

# numResponses: 2
# numEntries: 1


So how am I meant to set the filter of ext_ldap_group_acl? Most FreeIPA and Squid information centers around using Kerberos (and SSO) but the clients I?m dealing with here are not tied to FreeIPA thus Kerberos is not an option.

https://docs.oracle.com/cd/E88353_01/html/E72487/ext-ldap-group-acl-8.html <https://docs.oracle.com/cd/E88353_01/html/E72487/ext-ldap-group-acl-8.html> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221005/499bdf57/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct  5 13:56:52 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 5 Oct 2022 09:56:52 -0400
Subject: [squid-users] dns_nameservers directive
In-Reply-To: <Yz1vG1ALsUUwbguJ@charite.de>
References: <Yz1vG1ALsUUwbguJ@charite.de>
Message-ID: <fe916758-69d6-8973-bf01-d485038782b6@measurement-factory.com>

On 10/5/22 07:48, Ralf Hildebrandt wrote:
> Using squid-6.0.0-20220905-r9358e99f9:
> ======================================
> 
> On sunday, one of our DNS server froze and was not answering any
> queries. Shit happens, that's why we have another DNS server.
> 
> Our Squid config says:
> 
> dns_nameservers 141.42.5.157 141.42.5.156
> 
> So, 141.42.5.157 was not answering any queries, and 141.42.5.156 took
> over.
> 
> But... monitoring reported dns_query_time rose to about 8000ms,
> meaning all http requests took ages, since (I guess) the first dns
> server was queried, and (after 8s?) the second server was used.
> 
> But according to the docs, "dns_timeout" defaults to 30s.

Disclaimer: My response below is based on quick code analysis without 
any tests. It ignores many complications, including two DNS query types 
for each name (A and AAAA) and chasing dns_defnames after NXDOMAIN.

Squid dns_timeout does not control when Squid sends a query to the 
second DNS nameserver. It controls when Squid completely gives up on 
trying to resolve a name. Such resolution failures often lead to 
transaction forwarding errors.

The time[out] gap between two repeated DNS queries within one resolution 
attempt is controlled by dns_retransmit_interval (including its 
exponential back-off algorithm). See below for more details.


> Is there any way of making squid mark the first server as "dead" (for
> e.g. 5 minutes) and use the next server instead?

Not yet AFAICT: Today, Squid starts with the first nameserver and uses 
the second nameserver only when the first query fails (including 
dns_retransmit_interval timeouts). If there is enough time (see 
dns_timeout) and there are only two DNS nameservers configured, then 
Squid will use the first nameserver again (for the same resolution 
attempt) if the second query/nameserver fails, and so on (i.e. a round 
robin scan across all configured nameservers that always starts with the 
first nameserver).

Thus, if I am reading the code correctly, an unresponsive first 
nameserver will cripple your Squid even if the second nameserver is 
perfectly healthy :-(.


HTH,

Alex.


From Ralf.Hildebrandt at charite.de  Wed Oct  5 13:59:52 2022
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 5 Oct 2022 15:59:52 +0200
Subject: [squid-users] [ext] Re:  dns_nameservers directive
In-Reply-To: <fe916758-69d6-8973-bf01-d485038782b6@measurement-factory.com>
References: <Yz1vG1ALsUUwbguJ@charite.de>
 <fe916758-69d6-8973-bf01-d485038782b6@measurement-factory.com>
Message-ID: <Yz2N2AGo8QSpW5LA@charite.de>

* Alex Rousskov <rousskov at measurement-factory.com>:

> > But... monitoring reported dns_query_time rose to about 8000ms,

Sorry, 18000ms :)

> Disclaimer: My response below is based on quick code analysis without any
> tests. It ignores many complications, including two DNS query types for each
> name (A and AAAA) and chasing dns_defnames after NXDOMAIN.
> 
> Squid dns_timeout does not control when Squid sends a query to the second
> DNS nameserver. It controls when Squid completely gives up on trying to
> resolve a name. Such resolution failures often lead to transaction
> forwarding errors.

OK!

> The time[out] gap between two repeated DNS queries within one resolution
> attempt is controlled by dns_retransmit_interval (including its exponential
> back-off algorithm). See below for more details.

Ah, I see.

> Not yet AFAICT: Today, Squid starts with the first nameserver and uses the
> second nameserver only when the first query fails (including
> dns_retransmit_interval timeouts). If there is enough time (see dns_timeout)
> and there are only two DNS nameservers configured, then Squid will use the
> first nameserver again (for the same resolution attempt) if the second
> query/nameserver fails, and so on (i.e. a round robin scan across all
> configured nameservers that always starts with the first nameserver).
> 
> Thus, if I am reading the code correctly, an unresponsive first nameserver
> will cripple your Squid even if the second nameserver is perfectly healthy
> :-(.

Yes, that's what I observed here :)

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From squid3 at treenet.co.nz  Thu Oct  6 02:40:59 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Oct 2022 15:40:59 +1300
Subject: [squid-users] LDAP search filter for FreeIPA
In-Reply-To: <E1C9A0F1-7F2B-48AF-8DE3-7A7EBAC8B12F@gmail.com>
References: <E1C9A0F1-7F2B-48AF-8DE3-7A7EBAC8B12F@gmail.com>
Message-ID: <7981c308-16e9-6b46-32f1-6a4d78f2537e@treenet.co.nz>

On 6/10/22 02:29, Djerk Geurts wrote:
> Hi,
> 
> I?ve got DLAP auth working against FreeIPA, but now I?m trying to get 
> LDAP group all controls working. Initially I used the local unix group 
> filter, which works great as the machine running Squid is?able to query 
> group membership through pam. But then I found that nested group 
> membership didn?t work. So now I?m trying to query group membership via 
> LDAP and failing miserably.
> 
> My config:
> 
> auth_param basic program /usr/lib/squid/basic_ldap_auth -v 3 -b 
> "cn=users,cn=accounts,dc=DOMAIN,dc=COM" -D 
> "uid=squid-ldap,cn=sysaccounts,cn=etc,dc=DOMAIN,dc=COM" 
> -W?"/etc/squid/squid-ldap.cred" -u uid -H LDAPS://ipa.domain.com:636 
> <ldaps://ipa.domain.com:636>
> [?]
> 

To clarify, does the above description mean login with this helper works 
fine?


 > external_acl_type ldap_group %LOGIN .../ext_ldap_group_acl \
 >   -v 3 \

FYI: LDAP v3 is the default. You should not need to set this.


 >   -b "cn=groups,cn=accounts,dc=DOMAIN,dc=COM" \
 >   -f "(&(cn=%g)(member=uid=%u))" \
 > ...


You can add '-d' (lower case) to get a debug trace in cache.log about 
what is happening inside the helper.

You can use that to confirm the user/group details are arriving properly 
and the filter string is correct before it goes sent to LDAP.

Also, see whether LDAP is having connectivity issues, or search issues, 
or something else is going on.


FWIW, the above reads to me like you are looking up the existence of the 
group rather than the existence of a specific user within a group. My 
LDAP knowledge is weak, so I may be wrong about that.


> 
> This ldap search works fine:
> 
> user at ipa:~$ ldapsearch -x -D 'cn=Directory Manager' -W -b 
> "cn=groups,cn=accounts,dc=DOMAIN,dc=COM" '(&(cn=proxy)(member=uid=user,*))'

I notice that there is an extra ',*' after the username in this filter 
string which is missing on the helper one.



> Enter LDAP Password:
> # extended LDIF
> #
> # LDAPv3
> # base <cn=groups,cn=accounts,dc=DOMAIN,dc=COM> with scope subtree
> # filter: (&(cn=proxy)(member=uid=user,*))
> # requesting: ALL
> #

...

> 
> So how am I meant to set the filter of ext_ldap_group_acl?


FYI, what the Squid helpers do is replace the %g and %u values and pass 
the resulting string as the 'filter' to LDAP.

Meaning that the filter used by Squid should be the same as the 
ldapsearch filter would be if you were searching for username "%u" in 
group "%g".


Also, be aware that the filter string/pattern should be constructed in a 
way that correctly handles non-ASCII characters or whitespace if those 
are possible in your credentials and/or group names.


HTH
Amos


From djerkg at gmail.com  Thu Oct  6 09:28:22 2022
From: djerkg at gmail.com (Djerk Geurts)
Date: Thu, 6 Oct 2022 10:28:22 +0100
Subject: [squid-users] LDAP search filter for FreeIPA
In-Reply-To: <7981c308-16e9-6b46-32f1-6a4d78f2537e@treenet.co.nz>
References: <E1C9A0F1-7F2B-48AF-8DE3-7A7EBAC8B12F@gmail.com>
 <7981c308-16e9-6b46-32f1-6a4d78f2537e@treenet.co.nz>
Message-ID: <65FA78AF-FC50-4950-9970-6EAE854CC2DF@gmail.com>


> On 6 Oct 2022, at 03:40, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 6/10/22 02:29, Djerk Geurts wrote:
>> Hi,
>> I?ve got DLAP auth working against FreeIPA, but now I?m trying to get LDAP group all controls working. Initially I used the local unix group filter, which works great as the machine running Squid is able to query group membership through pam. But then I found that nested group membership didn?t work. So now I?m trying to query group membership via LDAP and failing miserably.
>> My config:
>> auth_param basic program /usr/lib/squid/basic_ldap_auth -v 3 -b "cn=users,cn=accounts,dc=DOMAIN,dc=COM" -D "uid=squid-ldap,cn=sysaccounts,cn=etc,dc=DOMAIN,dc=COM" -W "/etc/squid/squid-ldap.cred" -u uid -H LDAPS://ipa.domain.com:636 <ldaps://ipa.domain.com:636>
>> [?]
> 
> To clarify, does the above description mean login with this helper works fine?

Yes, normal logins work fine if I don?t use the group filtering in http_access

> >   -b "cn=groups,cn=accounts,dc=DOMAIN,dc=COM" \
> >   -f "(&(cn=%g)(member=uid=%u))" \
> > ...
> 
> You can add '-d' (lower case) to get a debug trace in cache.log about what is happening inside the helper.

Thank you, I?d seen the flag just not where the logs would end up and of course checked all but cache.log?

> You can use that to confirm the user/group details are arriving properly and the filter string is correct before it goes sent to LDAP.
> 
> Also, see whether LDAP is having connectivity issues, or search issues, or something else is going on.
> 
> FWIW, the above reads to me like you are looking up the existence of the group rather than the existence of a specific user within a group. My LDAP knowledge is weak, so I may be wrong about that.

Yeah, I?ve been wondering this too and my LDAP knowledge is quite poor. And it turns out that you're absolutely right.

> 
>> This ldap search works fine:
>> user at ipa:~$ ldapsearch -x -D 'cn=Directory Manager' -W -b "cn=groups,cn=accounts,dc=DOMAIN,dc=COM" '(&(cn=proxy)(member=uid=user,*))'
> 
> I notice that there is an extra ',*' after the username in this filter string which is missing on the helper one.

The ldapsearch works fine with and without it. The ,* is meant to match the rest of the DN but I think isn?t needed. I could be very wrong though. In hind sight, comparing the search results I now see that my first query returns a list of group members, but the correct query returns all user details if the user is a member of the given group.

> 
>> So how am I meant to set the filter of ext_ldap_group_acl?
> 
> FYI, what the Squid helpers do is replace the %g and %u values and pass the resulting string as the 'filter' to LDAP.
> 
> Meaning that the filter used by Squid should be the same as the ldapsearch filter would be if you were searching for username "%u" in group "%g".

Thank you! This plus a little more Googling has yielded the following search string and ldap_group config which works (even for nested groups):

## IPA groups via LDAP
external_acl_type ldap_group %LOGIN /usr/lib/squid/ext_ldap_group_acl -d \
  -b "cn=users,cn=accounts,dc=DOMAIN,dc=COM" \
  -D "uid=squid-ldap,cn=sysaccounts,cn=etc,dc=DOMAIN,dc=COM" \
  -W "/etc/squid/squid-ldap.cred" \
  -f "(&(objectclass=person)(uid=%u)(memberOf=cn=%g,cn=groups,cn=accounts,dc=DOMAIN,dc=COM))" \
  -H LDAPS://ipa.domain.com:636

I also found that the credentials file I?m using had the wrong permissions. It hadn?t shown up earlier as anonymous bind was enabled previously. Interesting that logging when using `-d` on the auth plugins only gets put into cache.log and not the Journal.

-- 
Thank you,
Djerk
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221006/a80833a2/attachment.htm>

From squid.org at bloms.de  Mon Oct 10 08:05:25 2022
From: squid.org at bloms.de (Dieter Bloms)
Date: Mon, 10 Oct 2022 10:05:25 +0200
Subject: [squid-users] got error page type ERR_READ_ERROR,
 when a dnslabel can not be resolved
Message-ID: <20221010080525.f56367e4ppv2uh3f@bloms.de>

Hello,

since squid 5.7 I get the error page of type ERR_READ_ERROR, when a dns
label can not be resolved (for example https://dnslabeldoesnotexist.com/).
I expect the error page of type ERR_DNS_FAIL instead of ERR_READ_ERROR.

Can somebody confirm this behavior ?

-- 
Regards

  Dieter Bloms

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From rousskov at measurement-factory.com  Mon Oct 10 13:45:06 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 10 Oct 2022 09:45:06 -0400
Subject: [squid-users] got error page type ERR_READ_ERROR,
 when a dnslabel can not be resolved
In-Reply-To: <20221010080525.f56367e4ppv2uh3f@bloms.de>
References: <20221010080525.f56367e4ppv2uh3f@bloms.de>
Message-ID: <8074cdaf-32dc-5688-7347-e6c7fd31234f@measurement-factory.com>

On 10/10/22 04:05, Dieter Bloms wrote:

> since squid 5.7 I get the error page of type ERR_READ_ERROR, when a dns
> label can not be resolved (for example https://dnslabeldoesnotexist.com/).
> I expect the error page of type ERR_DNS_FAIL instead of ERR_READ_ERROR.
> 
> Can somebody confirm this behavior ?

I cannot quickly confirm or deny that specific behavior in v5, but I 
recently spotted[1] bugs/deficiencies in error relaying master/v6-based 
code that result in ERR_READ_ERROR instead of ERR_DNS_FAIL or, at the 
very least, ERR_CANNOT_FORWARD. Sounds like v5 needs similar fixes.

Do you use SslBump to handle that HTTPS site?

Alex.

[1]: 
https://github.com/measurement-factory/squid/blob/0d3687c69cf7c69c59c101fc125615d5e10a1dc7/src/FwdState.cc#L279-283




From squid.org at bloms.de  Mon Oct 10 16:50:45 2022
From: squid.org at bloms.de (Dieter Bloms)
Date: Mon, 10 Oct 2022 18:50:45 +0200
Subject: [squid-users] got error page type ERR_READ_ERROR,
 when a dnslabel can not be resolved
In-Reply-To: <8074cdaf-32dc-5688-7347-e6c7fd31234f@measurement-factory.com>
References: <20221010080525.f56367e4ppv2uh3f@bloms.de>
 <8074cdaf-32dc-5688-7347-e6c7fd31234f@measurement-factory.com>
Message-ID: <20221010165045.3z2zbm2ad5mphhwj@bloms.de>

Hello Alex,

thank you for the quick answer!

On Mon, Oct 10, Alex Rousskov wrote:

> On 10/10/22 04:05, Dieter Bloms wrote:
> 
> > since squid 5.7 I get the error page of type ERR_READ_ERROR, when a dns
> > label can not be resolved (for example https://dnslabeldoesnotexist.com/).
> > I expect the error page of type ERR_DNS_FAIL instead of ERR_READ_ERROR.
> > 
> > Can somebody confirm this behavior ?
> 
> I cannot quickly confirm or deny that specific behavior in v5, but I
> recently spotted[1] bugs/deficiencies in error relaying master/v6-based code
> that result in ERR_READ_ERROR instead of ERR_DNS_FAIL or, at the very least,
> ERR_CANNOT_FORWARD. Sounds like v5 needs similar fixes.
> 
> Do you use SslBump to handle that HTTPS site?

yes, sslbump is enabled on our proxy server.

-- 
regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From olc at macmillan-craig.net  Tue Oct 11 05:31:02 2022
From: olc at macmillan-craig.net (Ole Craig)
Date: Mon, 10 Oct 2022 23:31:02 -0600
Subject: [squid-users] rejecting CONNECT if Proxy-Authentication header is
 sent but not required
Message-ID: <65f6e9f5-0fa1-e12a-2d52-3a9873218233@macmillan-craig.net>

Background: we are using Squid internally to replicate customer 
environments which require proxy transit for most if not all HTTP/REST 
comms, in order to facilitate bug replication and dev/test of software 
which must operate in those environments.

I would like to configure Squid with a set of allow-listed domains such 
that unauthenticated CONNECTs to sites within those domains succeed, 
_unless_ the following conditions are met:

  * if a client preemptively sends a Proxy-Authenticate header anyway,
    without first receiving a 407
  * _and_ that header is invalid (bad username/password, unsupported
    authN method, &c),

...in which case I want the CONNECT to get a standard 407 response.

Is this conditional possible with Squid's ACL structure? I can't see a 
way to make it happen in Squid 3.5 running on Amazon linux, although 
I've discovered a couple new ways of generating authentication loops. :/

 ??? Thanks for any help/pointers,

 ??? ??? Ole

-- 

Ole Craig | olc at macmillan-craig.net

McQuary was far too generous.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221010/f3b99ca6/attachment.htm>

From ludovit.koren at gmail.com  Wed Oct 12 08:12:09 2022
From: ludovit.koren at gmail.com (Ludovit Koren)
Date: Wed, 12 Oct 2022 10:12:09 +0200
Subject: [squid-users] squid exiting on signal 6
Message-ID: <86mta1pijq.fsf@gmail.com>


Hi,

I am running squid-5.7 on FreeBSD 12.3-STABLE r371879. Occasionally I
get the following error:

Oct 12 09:40:59 xxxxx kernel: pid 88291 (squid), jid 0, uid 100: exited on signal 6 (core dumped)
Oct 12 09:40:59 xxxxx squid[76820]: Squid Parent: squid-1 process 88291 exited due to signal 6 with status 0
Oct 12 09:40:59 xxxxx squid[76820]: Squid Parent: (squid-1) process 89357 started

>From backtrace I get:

gdb /usr/local/sbin/squid squid.core 
GNU gdb (GDB) 12.1 [GDB v12.1 for FreeBSD]
Copyright (C) 2022 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type "show copying" and "show warranty" for details.
This GDB was configured as "x86_64-portbld-freebsd12.3".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
<https://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from /usr/local/sbin/squid...
[New LWP 102541]
Core was generated by `(squid-1) --kid squid-1 -f /usr/local/etc/squid/squid.conf'.
Program terminated with signal SIGABRT, Aborted.
Sent by thr_kill() from pid 88291 and user 100.
#0  thr_kill () at thr_kill.S:3
3       RSYSCALL(thr_kill)
(gdb) bt
#0  thr_kill () at thr_kill.S:3
#1  0x000000080112fd44 in __raise (s=s at entry=6) at /usr/src/lib/libc/gen/raise.c:52
#2  0x00000008010a8409 in abort () at /usr/src/lib/libc/stdlib/abort.c:67
#3  0x000000080111fcb1 in __assert (func=<optimized out>, file=<optimized out>, line=<optimized out>, failedexpr=<optimized out>) at /usr/src/lib/libc/gen/assert.c:51
#4  0x0000000000698fcd in Ip::Address::getAddrInfo (this=0x861c04588, dst=<optimized out>, force=0) at Address.cc:663
#5  0x000000000068b732 in comm_openex (sock_type=sock_type at entry=1, proto=proto at entry=6, addr=..., flags=1, note=0x85ce42dc0 "[fe80::21f:29ff:fe28:7017]") at comm.cc:347
#6  0x00000000006dcdb9 in Comm::ConnOpener::createFd (this=this at entry=0x85e66b8b8) at ConnOpener.cc:288
#7  0x00000000006dcb72 in Comm::ConnOpener::start (this=0x85e66b8b8) at ConnOpener.cc:261
#8  0x0000000000682294 in JobDialer<AsyncJob>::dial (this=0x85390c798, call=...) at ../../src/base/AsyncJobCalls.h:175
#9  0x000000000067e9bb in AsyncCall::make (this=0x85390c760) at AsyncCall.cc:44
#10 0x000000000067fa91 in AsyncCallQueue::fireNext (this=<optimized out>, this at entry=0x801c00740) at AsyncCallQueue.cc:60
#11 0x000000000067f6e8 in AsyncCallQueue::fire (this=0x801c00740) at AsyncCallQueue.cc:43
#12 0x00000000004914f2 in EventLoop::dispatchCalls (this=<optimized out>) at EventLoop.cc:144
#13 EventLoop::runOnce (this=this at entry=0x7fffffffe9d0) at EventLoop.cc:121
#14 0x0000000000491418 in EventLoop::run (this=0x7fffffffe9d0) at EventLoop.cc:83
#15 0x00000000005792a8 in SquidMain (argc=<optimized out>, argc at entry=5, argv=<optimized out>, argv at entry=0x7fffffffeab8) at main.cc:1719
#16 0x00000000005786a0 in SquidMainSafe (argc=102541, argv=0x6) at main.cc:1406
#17 main (argc=102541, argv=0x6) at main.cc:1394
(gdb) 


The squid is compiled without IPv6 option, so I do not understand why it
tries to reach IPv6 address.

Please, can anybody help, why I am getting signal 6 and core dump?

Thank you very much.

Regards,

lk


From squid3 at treenet.co.nz  Wed Oct 12 11:22:39 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Oct 2022 00:22:39 +1300
Subject: [squid-users] squid exiting on signal 6
In-Reply-To: <86mta1pijq.fsf@gmail.com>
References: <86mta1pijq.fsf@gmail.com>
Message-ID: <8b36ba40-e531-b6dd-927b-e70b021efe09@treenet.co.nz>

On 12/10/22 21:12, Ludovit Koren wrote:
> 
> Hi,
> 
> I am running squid-5.7 on FreeBSD 12.3-STABLE r371879. Occasionally I
> get the following error:
> 

> #3  0x000000080111fcb1 in __assert (func=<optimized out>, file=<optimized out>, line=<optimized out>, failedexpr=<optimized out>) at /usr/src/lib/libc/gen/assert.c:51
> #4  0x0000000000698fcd in Ip::Address::getAddrInfo (this=0x861c04588, dst=<optimized out>, force=0) at Address.cc:663
> #5  0x000000000068b732 in comm_openex (sock_type=sock_type at entry=1, proto=proto at entry=6, addr=..., flags=1, note=0x85ce42dc0 "[fe80::21f:29ff:fe28:7017]") at comm.cc:347
...

> 
> The squid is compiled without IPv6 option, so I do not understand why it
> tries to reach IPv6 address.
> 

A client is requiring connection to an IPv6 server. But Squid cannot 
convert that IPv6 address or use on an IPv4-only network. Lack of IPv6 
support also forbids IPv6 failover handling being used.

Amos


From squid3 at treenet.co.nz  Wed Oct 12 11:42:40 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Oct 2022 00:42:40 +1300
Subject: [squid-users] rejecting CONNECT if Proxy-Authentication header
 is sent but not required
In-Reply-To: <65f6e9f5-0fa1-e12a-2d52-3a9873218233@macmillan-craig.net>
References: <65f6e9f5-0fa1-e12a-2d52-3a9873218233@macmillan-craig.net>
Message-ID: <a9425cd3-810b-90e2-cf47-d06859266805@treenet.co.nz>

On 11/10/22 18:31, Ole Craig wrote:
> I would like to configure Squid with a set of allow-listed domains such 
> that unauthenticated CONNECTs to sites within those domains succeed, 
> _unless_ the following conditions are met:
> 
>   * if a client preemptively sends a Proxy-Authenticate header anyway,
>     without first receiving a 407

FYI this requirement (taken by itself) would break HTTP authentication. 
There are many ways for a client to learn that authentication is 
required, some of them are out-of-band and cannot be known by the proxy.


>   * _and_ that header is invalid (bad username/password, unsupported
>     authN method, &c),
> 

... this requirement makes the first requirement irrelevant. Invalid 
credentials are *always* supposed to be rejected with 4xx regardless of 
whether the client has been seen before or not.


Just use the normal recommended authentication access check(s):

  # the usual security protections...
  http_access deny CONNECT !SSL_Ports

  # require valid credentials
  acl auth proxy_auth REQUIRED
  http_access deny !auth

  acl whitelist dstdomain ...
  http_access allow CONNECT whitelist


HTH
Amos


From ludovit.koren at gmail.com  Wed Oct 12 12:30:03 2022
From: ludovit.koren at gmail.com (Ludovit Koren)
Date: Wed, 12 Oct 2022 14:30:03 +0200
Subject: [squid-users] squid exiting on signal 6
In-Reply-To: <8b36ba40-e531-b6dd-927b-e70b021efe09@treenet.co.nz> (Amos
 Jeffries's message of "Thu, 13 Oct 2022 00:22:39 +1300")
References: <86mta1pijq.fsf@gmail.com>
 <8b36ba40-e531-b6dd-927b-e70b021efe09@treenet.co.nz>
Message-ID: <86ilkpp6lw.fsf@gmail.com>

>>>>> Amos Jeffries <squid3 at treenet.co.nz> writes:

    > On 12/10/22 21:12, Ludovit Koren wrote:
    >> Hi,
    >> I am running squid-5.7 on FreeBSD 12.3-STABLE r371879. Occasionally
    >> I
    >> get the following error:
    >> 

    >> #3  0x000000080111fcb1 in __assert (func=<optimized out>, file=<optimized out>, line=<optimized out>, failedexpr=<optimized out>) at /usr/src/lib/libc/gen/assert.c:51
    >> #4  0x0000000000698fcd in Ip::Address::getAddrInfo (this=0x861c04588, dst=<optimized out>, force=0) at Address.cc:663
    >> #5  0x000000000068b732 in comm_openex (sock_type=sock_type at entry=1, proto=proto at entry=6, addr=..., flags=1, note=0x85ce42dc0 "[fe80::21f:29ff:fe28:7017]") at comm.cc:347
    > ...

    >> The squid is compiled without IPv6 option, so I do not understand
    >> why it
    >> tries to reach IPv6 address.
    >> 

    > A client is requiring connection to an IPv6 server. But Squid cannot
    > convert that IPv6 address or use on an IPv4-only network. Lack of IPv6 
    > support also forbids IPv6 failover handling being used.

So the solution is to reenable IPv6 in the squid, as well as in the OS
network stack? Am I right?

Thank you.

Regards,

lk


From david at articatech.com  Wed Oct 12 16:45:57 2022
From: david at articatech.com (David Touzeau)
Date: Wed, 12 Oct 2022 18:45:57 +0200
Subject: [squid-users] Squid 5.7 + bump ERR_READ_ERROR|WITH_SERVER
Message-ID: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>

Hi

We using squid 5.7 after adding ssl-bump we have sometimes several 502 
error? with extended error ERR_READ_ERROR|WITH_SERVER

1665589818.831???? 11 192.168.1.13 NONE_NONE/502 192616 OPTIONS 
https://www2.deepl.com/jsonrpc?method=LMT_split_text - HIER_NONE/-:- 
text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
1665589839.288???? 11 192.168.1.13 NONE_NONE/502 506759 POST 
https://pollserver.lastpass.com/poll_server.php - HIER_NONE/-:- 
text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
1665589719.879???? 44 192.168.1.13 NONE_NONE/502 506954 GET 
https://contile.services.mozilla.com/v1/tiles - HIER_NONE/-:- text/html 
mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"

What does it means.

how can we fix it ?

regards


-- 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221012/5df1e88d/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 12 18:00:03 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 12 Oct 2022 14:00:03 -0400
Subject: [squid-users] Squid 5.7 + bump ERR_READ_ERROR|WITH_SERVER
In-Reply-To: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>
References: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>
Message-ID: <46b27dfc-c4bf-fada-4e1f-4ee90f877da6@measurement-factory.com>

On 10/12/22 12:45, David Touzeau wrote:
> Hi
> 
> We using squid 5.7 after adding ssl-bump we have sometimes several 502 
> error? with extended error ERR_READ_ERROR|WITH_SERVER
> 
> 1665589818.831???? 11 192.168.1.13 NONE_NONE/502 192616 OPTIONS 
> https://www2.deepl.com/jsonrpc?method=LMT_split_text - HIER_NONE/-:- 
> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
> 1665589839.288???? 11 192.168.1.13 NONE_NONE/502 506759 POST 
> https://pollserver.lastpass.com/poll_server.php - HIER_NONE/-:- 
> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
> 1665589719.879???? 44 192.168.1.13 NONE_NONE/502 506954 GET 
> https://contile.services.mozilla.com/v1/tiles - HIER_NONE/-:- text/html 
> mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"

> What does it means.

502 with ERR_READ_ERROR|WITH_SERVER may mean several things 
(unfortunately). Given HIER_NONE, I would suspect that Squid could not 
find a valid destination for the request. There is a similar recent 
squid-users thread at 
http://lists.squid-cache.org/pipermail/squid-users/2022-October/025289.html


> how can we fix it ?

The first step is to identify what causes these errors.

Can you reproduce this problem at will? Perhaps by trying going to 
https://dnslabeldoesnotexist.com mentioned at the above thread? If you 
can, consider sharing (a pointer to) a compressed debugging cache.log 
from a test box that does not expose any internal secrets, as detailed 
at 
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction


HTH,

Alex.


From david at articatech.com  Wed Oct 12 21:58:21 2022
From: david at articatech.com (David Touzeau)
Date: Wed, 12 Oct 2022 23:58:21 +0200
Subject: [squid-users] Squid 5.7 + bump ERR_READ_ERROR|WITH_SERVER
In-Reply-To: <46b27dfc-c4bf-fada-4e1f-4ee90f877da6@measurement-factory.com>
References: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>
 <46b27dfc-c4bf-fada-4e1f-4ee90f877da6@measurement-factory.com>
Message-ID: <5724b3b3-c76f-df95-e07c-117732bc3861@articatech.com>

Thanks Alex

We have put it in debug mode :

you can see :


"2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
ERR_READ_ERROR/WITH_SERVER"

As without ssl-bump, there is no issue.

the full log can be downloaded here 
http://articatech.net/tmpf/cache.log.txt*
*

}
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905084 from SBuf15905303
2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1357) 
parseHttpRequest: HTTP Client conn214046 local=192.168.1.190:3128 
remote=192.168.1.13:62858 FD 21 flags=1
2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1361) 
parseHttpRequest: HTTP Client REQUEST:
---------
GET /fw.ping.php?_=1665576594826 HTTP/1.1
Host: categories.articatech.net
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
Gecko/20100101 Firefox/105.0
Accept: */*
Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate, br
Referer: https://categories.articatech.net/index
X-Requested-With: XMLHttpRequest
Connection: keep-alive
Cookie: _ga=GA1.2.1172828951.1642984448; 
PHPSESSID=28871210b5b031a7b034981e9704a4ac
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: same-origin


----------
2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1393) 
parseHttpRequest: complete request received. prefix_sz = 542, 
request-line-size=43, mime-header-size=499, mime header block:
Host: categories.articatech.net
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
Gecko/20100101 Firefox/105.0
Accept: */*
Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate, br
Referer: https://categories.articatech.net/index
X-Requested-With: XMLHttpRequest
Connection: keep-alive
Cookie: _ga=GA1.2.1172828951.1642984448; 
PHPSESSID=28871210b5b031a7b034981e9704a4ac
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: same-origin


----------
2022/10/12 22:29:49.475 kid3| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob 
constructed, this=0x56260e3814b8 type=ClientHttpRequest [job199106]
2022/10/12 22:29:49.475 kid3| 1,5| CodeContext.cc(60) Entering: ALE w/o 
master
2022/10/12 22:29:49.475 kid3| 87,3| clientStream.cc(140) 
clientStreamInsertHead: clientStreamInsertHead: Inserted node 
0x56260e6d26f8 with data 0x56260e0fa280 after head
2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1413) 
parseHttpRequest: Prepare absolute URL from
2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(228) getHostHeaderField: 
looking for Host
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
not of characterset non-LF in id SBuf15905340
2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
32 bytes
2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
1 bytes
2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(249) getHostHeaderField: 
checking Host: categories.articatech.net
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
not of characterset WSP in id SBuf15905345
2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
1 bytes
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905339 from SBuf15905345
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
not of characterset host in id SBuf15905339
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(771) findFirstNotOf: not found
2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(267) getHostHeaderField: 
returning categories.articatech.net
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905347 from SBuf183
2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1294) 
prepareTlsSwitchingURL: TLS switching host rewrite: 
https://categories.articatech.net/fw.ping.php?_=1665576594826
2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1981) 
clientParseRequests: conn214046 local=192.168.1.190:3128 
remote=192.168.1.13:62858 FD 21 flags=1: done parsing a request
2022/10/12 22:29:49.475 kid3| 5,4| AsyncCall.cc(30) AsyncCall: The 
AsyncCall ConnStateData::lifetimeTimeout constructed, 
this=0x56260e5afd50 [call1972958]
2022/10/12 22:29:49.475 kid3| 5,3| comm.cc(571) commSetConnTimeout: 
conn214046 local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 
flags=1 timeout 86400
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: cleaning 
hdr: 0x56260e392c18 owner: 3
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: cleaning 
hdr: 0x56260e392c18 owner: 3
2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1806) add: 
0x56260e0f9210*3 to 0/1
2022/10/12 22:29:49.475 kid3| 33,3| Pipeline.cc(24) add: Pipeline 
0x56260e6cd470 add request 2 0x56260e0f9210*4
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905349
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 61 
for SBuf15905349
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905349 
new store capacity: 128
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
init-ing hdr: 0x56260e4038f8 owner: 2
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
not of characterset special in id SBuf15905363
2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
5 bytes
2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
1 bytes
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905367 from SBuf183
2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
2 bytes
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
for SBuf15905371
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905371 
not growing
2022/10/12 22:29:49.475 kid3| 23,3| Uri.cc(441) parse: Split URL 
'https://categories.articatech.net/fw.ping.php?_=1665576594826' into 
proto='https', host='categories.articatech.net', port='443', 
path='/fw.ping.php?_=1665576594826'
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905351 from SBuf15905361
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905353 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905353
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 28 
for SBuf15905353
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905353 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'categories.articatech.net': Name or service not known
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905373
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 0 
for SBuf15905373
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905373 
not growing
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905352 from SBuf15905373
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905350 from SBuf15905305
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
for SBuf15905374
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905374 
not growing
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(390) parse: parsing 
hdr: (0x56260e4038f8)
Host: categories.articatech.net
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
Gecko/20100101 Firefox/105.0
Accept: */*
Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate, br
Referer: https://categories.articatech.net/index
X-Requested-With: XMLHttpRequest
Connection: keep-alive
Cookie: _ga=GA1.2.1172828951.1642984448; 
PHPSESSID=28871210b5b031a7b034981e9704a4ac
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: same-origin

2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905375 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905375
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
for SBuf15905375
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905375 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905376 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905376
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
for SBuf15905376
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905376 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Host[30] at 0
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905377 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905377
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
for SBuf15905377
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905377 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905378 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905378
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
for SBuf15905378
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905378 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: User-Agent[67] at 1
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905379 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905379
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
for SBuf15905379
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905379 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905380 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905380
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
for SBuf15905380
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905380 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Accept[0] at 2
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905381 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905381
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
for SBuf15905381
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905381 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905382 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905382
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
for SBuf15905382
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905382 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Accept-Language[3] at 3
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905383 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905383
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
for SBuf15905383
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905383 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905384 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905384
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
for SBuf15905384
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905384 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Accept-Encoding[2] at 4
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905385 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905385
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 7 
for SBuf15905385
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905385 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905386 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905386
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 7 
for SBuf15905386
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905386 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Referer[54] at 5
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905387
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 16 
for SBuf15905387
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905387 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905388 from SBuf15905387
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Other:[87] at 6
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905389 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905389
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
for SBuf15905389
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905389 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905390 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905390
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
for SBuf15905390
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905390 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Connection[12] at 7
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905391 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905391
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
for SBuf15905391
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905391 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905392 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905392
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
for SBuf15905392
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905392 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Cookie[22] at 8
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905393
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
for SBuf15905393
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905393 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905394 from SBuf15905393
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Other:[87] at 9
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905395
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
for SBuf15905395
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905395 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905396 from SBuf15905395
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Other:[87] at 10
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905397
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
for SBuf15905397
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905397 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905398 from SBuf15905397
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Other:[87] at 11
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905355
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 25 
for SBuf15905355
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905355 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905354 from SBuf15905355
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
for SBuf15905355
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905355 
not growing
2022/10/12 22:29:49.475 kid3| 33,5| Http1Server.cc(193) 
buildHttpRequest: normalize 1 Host header using categories.articatech.net
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
for SBuf15905399
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905399 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905401 
from c-string, n=4294967295)
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905401
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
for SBuf15905401
2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905401 
new store capacity: 40
2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e4038f8 adding entry: Host[30] at 12
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905356 
new store capacity: 16384
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 5 
for SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
not growing
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
for SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
not growing
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 2 
for SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
not growing
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 25 
for SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
not growing
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 28 
for SBuf15905356
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
not growing
2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(699) 
clientSetKeepaliveFlag: http_ver = HTTP/1.1
2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(700) 
clientSetKeepaliveFlag: method = GET
2022/10/12 22:29:49.476 kid3| 55,6| HttpHeader.cc(850) getList: 
0x56260e4038f8: joined for id Connection[12]: 0x7ffc7248c400
2022/10/12 22:29:49.476 kid3| 33,4| client_side.cc(1510) quitAfterError: 
Will close after error: conn214046 local=192.168.1.190:3128 
remote=192.168.1.13:62858 FD 21 flags=1
2022/10/12 22:29:49.476 kid3| 33,5| client_side.cc(1531) 
serveDelayedError: Responding with delated error for 
https://categories.articatech.net/fw.ping.php?_=1665576594826
2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
clientReplyContext::setReplyToStoreEntry locked key 
AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*2
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
0x7ffc7248c000 checking fast rules
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
aclCheckFast: list: 0x56260c88d7a8
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'ALLOWED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#1
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
aclMatchDomainList: checking 'categories.articatech.net'
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
aclMatchDomainList: 'categories.articatech.net' NOT found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 = 0
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#1 = 0
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'DENIED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#2
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'192.168.1.13:62858' found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#2 = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access = 1
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
0x7ffc7248c000 answer DENIED for match
2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
0x7ffc7248c000 checking fast rules
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
aclCheckFast: list: 0x56260c8c46a8
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'ALLOWED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#1
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
aclMatchDomainList: checking 'categories.articatech.net'
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
aclMatchDomainList: 'categories.articatech.net' NOT found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 = 0
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#1 = 0
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'DENIED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#2
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'192.168.1.13:62858' found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#2 = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access = 1
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
0x7ffc7248c000 answer DENIED for match
2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
*2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
ERR_READ_ERROR/WITH_SERVER*
2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(117) pullData: 0 written 0 
into conn214046 local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 
flags=1
2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(141) getNextRangeOffset: 
range: 0; http offset 0; reply 0
2022/10/12 22:29:49.476 kid3| 87,3| clientStream.cc(180) 
clientStreamRead: clientStreamRead: Calling 1 with cbdata 0x56260e0d2ba8 
from node 0x56260e6d26f8
2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(246) copy: 
store_client::copy: AFC90000000000002632000003000000, from 0, for length 
4096, cb 1, cbdata 0x56260e0d1ad8
2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
store_client::copy locked key AFC90000000000002632000003000000 
e:=sp2XIV/0x56260e1b8290*3
2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(338) 
storeClientCopy2: storeClientCopy2: AFC90000000000002632000003000000
2022/10/12 22:29:49.476 kid3| 33,5| store_client.cc(368) doCopy: 
store_client::doCopy: co: 0, hi: 506665
2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(472) 
scheduleMemRead: store_client::doCopy: Copying normal from memory
2022/10/12 22:29:49.476 kid3| 19,6| stmem.cc(230) copy: memCopy: 
0x56260db339f8 [0,4096)
2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2213) 
sendMoreData: conn214046 local=192.168.1.190:3128 
remote=192.168.1.13:62858 FD 21 flags=1 'categories.articatech.net:443' 
out.offset=0
2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2241) 
sendMoreData: clientReplyContext::sendMoreData: 
https://categories.articatech.net/fw.ping.php?_=1665576594826, 4096 
bytes (4096 new bytes)
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
init-ing hdr: 0x56260e392c18 owner: 3
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(243) append: appending 
hdr: 0x56260e392c18 += 0x56260db7a728
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905409 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905409
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
for SBuf15905409
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905409 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Server[57] at 0
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905410 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905410
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 12 
for SBuf15905410
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905410 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Mime-Version[43] at 1
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905411 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905411
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
for SBuf15905411
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905411 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Date[24] at 2
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905412 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905412
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 12 
for SBuf15905412
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905412 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Content-Type[21] at 3
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905413 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905413
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
for SBuf15905413
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905413 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Content-Length[17] at 4
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905414 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905414
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 13 
for SBuf15905414
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905414 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: X-Squid-Error[76] at 5
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
httpHeaderParseOffset: offset 506470 parsed as 506470
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
httpHeaderParseOffset: offset 506470 parsed as 506470
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905416 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905416
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 7 
for SBuf15905416
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905416 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: X-Cache[72] at 6
2022/10/12 22:29:49.476 kid3| 33,2| client_side_reply.cc(1585) 
buildReplyHeader: clientBuildReplyHeader: Connection Keep-Alive not 
requested by admin or client
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 16 
for SBuf15905417
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905417 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 96 
for SBuf15905417
2022/10/12 22:29:49.476 kid3| 24,7| MemBlob.cc(130) syncSize: 0 was: 0
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905417 
new store capacity: 128
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(85) assign: assigning 
SBuf15905418 from SBuf15905417
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
for SBuf15905418
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905418 
not growing
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905420 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905420
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 3 
for SBuf15905420
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905420 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Via[69] at 7
2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905422 
from c-string, n=4294967295)
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
to id SBuf15905422
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
for SBuf15905422
2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905422 
new store capacity: 40
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
0x56260e392c18 adding entry: Connection[12] at 8
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
0x7ffc7248bb70 checking fast rules
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
aclCheckFast: list: 0x56260c8adf68
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
http_header_access Server
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'DENIED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
http_header_access Server#1
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'192.168.1.13:62858' found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
http_header_access Server#1 = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
http_header_access Server = 1
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
0x7ffc7248bb70 answer DENIED for match
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(308) 
httpHdrMangle: checklist denied, we have no replacement. Pass
2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bb70
2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bb70
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
httpHdrMangle: couldn't find mangler or access list. Allowing
2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(769) refreshMask: 
refreshing the mask in hdr 0x56260e392c18
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
0x7ffc7248bcb0 checking fast rules
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
aclCheckFast: list: 0x56260c88d7a8
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'ALLOWED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#1
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
aclMatchDomainList: checking 'categories.articatech.net'
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
aclMatchDomainList: 'categories.articatech.net' NOT found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 = 0
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#1 = 0
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'DENIED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#2
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'192.168.1.13:62858' found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#2 = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access = 1
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
0x7ffc7248bcb0 answer DENIED for match
2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
0x7ffc7248bcb0 checking fast rules
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
aclCheckFast: list: 0x56260c8c46a8
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'ALLOWED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#1
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
aclMatchDomainList: checking 'categories.articatech.net'
2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
aclMatchDomainList: 'categories.articatech.net' NOT found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 = 0
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#1 = 0
2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
Action 'DENIED/0' is not banned
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
delay_access#2
2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
'192.168.1.13:62858' found
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access#2 = 1
2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
delay_access = 1
2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
0x7ffc7248bcb0 answer DENIED for match
2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(70) preCheck: 
0x7ffc7248bc10 checking fast ACLs
2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
reply_body_max_size -1
2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
(reply_body_max_size -1 line)
2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
(reply_body_max_size -1 line) = 1
2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
reply_body_max_size -1 = 1
2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(63) markFinished: 
0x7ffc7248bc10 answer ALLOWED for match
2022/10/12 22:29:49.477 kid3| 58,4| HttpReply.cc(564) calcMaxBodySize: 
bodySizeMax=-1
2022/10/12 22:29:49.477 kid3| 28,4| FilledChecklist.cc(67) 
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bc10
2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bc10
2022/10/12 22:29:49.477 kid3| 58,7| HttpReply.cc(528) 
expectedBodyTooLarge: bodySizeMax=-1
2022/10/12 22:29:49.477 kid3| 88,2| client_side_reply.cc(2090) 
processReplyAccessResult: The reply for GET 
https://categories.articatech.net/fw.ping.php?_=1665576594826 is 
ALLOWED, because it matched (reply_body_max_size -1 line)
2022/10/12 22:29:49.477 kid3| 20,3| store.cc(443) lock: 
ClientHttpRequest::loggingEntry locked key 
AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*4
2022/10/12 22:29:49.477 kid3| 88,3| client_side_reply.cc(2128) 
processReplyAccessResult: clientReplyContext::sendMoreData: Appending 
3901 bytes after 195 bytes of headers
2022/10/12 22:29:49.477 kid3| 87,3| clientStream.cc(158) 
clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 
0x56260e0fa280 from node 0x56260e5c0ab8
2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
0x56260e6cd470 front 0x56260e0f9210*4
2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
0x56260e6cd470 front 0x56260e0f9210*4
2022/10/12 22:29:49.477 kid3| 55,7| HttpHeader.cc(589) packInto: 
0x56260e392c18 into 0x56260e26ebe8
2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(279) sendStartOfMessage: 
HTTP Client conn214046 local=192.168.1.190:3128 
remote=192.168.1.13:62858 FD 21 flags=1
2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(280) sendStartOfMessage: 
HTTP Client REPLY:
---------
HTTP/1.1 502 Bad Gateway
Mime-Version: 1.0
Date: Wed, 12 Oct 2022 20:29:49 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 506470
X-Squid-Error: ERR_READ_ERROR 0
X-Cache: MISS from proxy-190.articatech.int
Via: 1.1 789aaa51-a1eb-eb48-639b-000070877aed (squid)
Connection: close


Le 12/10/2022 ? 20:00, Alex Rousskov a ?crit?:
> On 10/12/22 12:45, David Touzeau wrote:
>> Hi
>>
>> We using squid 5.7 after adding ssl-bump we have sometimes several 
>> 502 error? with extended error ERR_READ_ERROR|WITH_SERVER
>>
>> 1665589818.831???? 11 192.168.1.13 NONE_NONE/502 192616 OPTIONS 
>> https://www2.deepl.com/jsonrpc?method=LMT_split_text - HIER_NONE/-:- 
>> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
>> 1665589839.288???? 11 192.168.1.13 NONE_NONE/502 506759 POST 
>> https://pollserver.lastpass.com/poll_server.php - HIER_NONE/-:- 
>> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
>> 1665589719.879???? 44 192.168.1.13 NONE_NONE/502 506954 GET 
>> https://contile.services.mozilla.com/v1/tiles - HIER_NONE/-:- 
>> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
>
>> What does it means.
>
> 502 with ERR_READ_ERROR|WITH_SERVER may mean several things 
> (unfortunately). Given HIER_NONE, I would suspect that Squid could not 
> find a valid destination for the request. There is a similar recent 
> squid-users thread at 
> http://lists.squid-cache.org/pipermail/squid-users/2022-October/025289.html
>
>
>> how can we fix it ?
>
> The first step is to identify what causes these errors.
>
> Can you reproduce this problem at will? Perhaps by trying going to 
> https://dnslabeldoesnotexist.com mentioned at the above thread? If you 
> can, consider sharing (a pointer to) a compressed debugging cache.log 
> from a test box that does not expose any internal secrets, as detailed 
> at 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-- 
Technical Support
	
	
*David Touzeau*
Orgerus, Yvelines, France
*Artica Tech*

P:?+33 6 58 44 69 46
www: wiki.articatech.com <https://wiki.articatech.com>
www: articatech.net <http://articatech.net>



From rousskov at measurement-factory.com  Wed Oct 12 22:16:52 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 12 Oct 2022 18:16:52 -0400
Subject: [squid-users] Squid 5.7 + bump ERR_READ_ERROR|WITH_SERVER
In-Reply-To: <5724b3b3-c76f-df95-e07c-117732bc3861@articatech.com>
References: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>
 <46b27dfc-c4bf-fada-4e1f-4ee90f877da6@measurement-factory.com>
 <5724b3b3-c76f-df95-e07c-117732bc3861@articatech.com>
Message-ID: <03467c7f-d1d0-fa6c-ad93-0ab68f8f9dd3@measurement-factory.com>

On 10/12/22 17:58, David Touzeau wrote:

> We have put it in debug mode :

Unfortunately, this log starts at GET request processing _after_ CONNECT 
request processing has already encountered and saved the error. Thus, I 
cannot tell you why that error was encountered. I need to see the lines 
before the lines you have shared. If you still have the log file, 
searching for AFC90000000000002632000003000000 (above the already shared 
lines) might help you find where the error message was created.

HTH,

Alex.


> you can see :
> 
> 
> "2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
> ERR_READ_ERROR/WITH_SERVER"
> 
> As without ssl-bump, there is no issue.
> 
> the full log can be downloaded here 
> http://articatech.net/tmpf/cache.log.txt*
> *
> 
> }
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905084 from SBuf15905303
> 2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1357) 
> parseHttpRequest: HTTP Client conn214046 local=192.168.1.190:3128 
> remote=192.168.1.13:62858 FD 21 flags=1
> 2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1361) 
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET /fw.ping.php?_=1665576594826 HTTP/1.1
> Host: categories.articatech.net
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
> Gecko/20100101 Firefox/105.0
> Accept: */*
> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
> Accept-Encoding: gzip, deflate, br
> Referer: https://categories.articatech.net/index
> X-Requested-With: XMLHttpRequest
> Connection: keep-alive
> Cookie: _ga=GA1.2.1172828951.1642984448; 
> PHPSESSID=28871210b5b031a7b034981e9704a4ac
> Sec-Fetch-Dest: empty
> Sec-Fetch-Mode: cors
> Sec-Fetch-Site: same-origin
> 
> 
> ----------
> 2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1393) 
> parseHttpRequest: complete request received. prefix_sz = 542, 
> request-line-size=43, mime-header-size=499, mime header block:
> Host: categories.articatech.net
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
> Gecko/20100101 Firefox/105.0
> Accept: */*
> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
> Accept-Encoding: gzip, deflate, br
> Referer: https://categories.articatech.net/index
> X-Requested-With: XMLHttpRequest
> Connection: keep-alive
> Cookie: _ga=GA1.2.1172828951.1642984448; 
> PHPSESSID=28871210b5b031a7b034981e9704a4ac
> Sec-Fetch-Dest: empty
> Sec-Fetch-Mode: cors
> Sec-Fetch-Site: same-origin
> 
> 
> ----------
> 2022/10/12 22:29:49.475 kid3| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob 
> constructed, this=0x56260e3814b8 type=ClientHttpRequest [job199106]
> 2022/10/12 22:29:49.475 kid3| 1,5| CodeContext.cc(60) Entering: ALE w/o 
> master
> 2022/10/12 22:29:49.475 kid3| 87,3| clientStream.cc(140) 
> clientStreamInsertHead: clientStreamInsertHead: Inserted node 
> 0x56260e6d26f8 with data 0x56260e0fa280 after head
> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1413) 
> parseHttpRequest: Prepare absolute URL from
> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(228) getHostHeaderField: 
> looking for Host
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
> not of characterset non-LF in id SBuf15905340
> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
> 32 bytes
> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
> 1 bytes
> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(249) getHostHeaderField: 
> checking Host: categories.articatech.net
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
> not of characterset WSP in id SBuf15905345
> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
> 1 bytes
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905339 from SBuf15905345
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
> not of characterset host in id SBuf15905339
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(771) findFirstNotOf: not found
> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(267) getHostHeaderField: 
> returning categories.articatech.net
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905347 from SBuf183
> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1294) 
> prepareTlsSwitchingURL: TLS switching host rewrite: 
> https://categories.articatech.net/fw.ping.php?_=1665576594826
> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1981) 
> clientParseRequests: conn214046 local=192.168.1.190:3128 
> remote=192.168.1.13:62858 FD 21 flags=1: done parsing a request
> 2022/10/12 22:29:49.475 kid3| 5,4| AsyncCall.cc(30) AsyncCall: The 
> AsyncCall ConnStateData::lifetimeTimeout constructed, 
> this=0x56260e5afd50 [call1972958]
> 2022/10/12 22:29:49.475 kid3| 5,3| comm.cc(571) commSetConnTimeout: 
> conn214046 local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 
> flags=1 timeout 86400
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: cleaning 
> hdr: 0x56260e392c18 owner: 3
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: cleaning 
> hdr: 0x56260e392c18 owner: 3
> 2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1806) add: 
> 0x56260e0f9210*3 to 0/1
> 2022/10/12 22:29:49.475 kid3| 33,3| Pipeline.cc(24) add: Pipeline 
> 0x56260e6cd470 add request 2 0x56260e0f9210*4
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905349
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 61 
> for SBuf15905349
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905349 
> new store capacity: 128
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
> init-ing hdr: 0x56260e4038f8 owner: 2
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: first 
> not of characterset special in id SBuf15905363
> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
> 5 bytes
> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
> 1 bytes
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905367 from SBuf183
> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: consuming 
> 2 bytes
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
> for SBuf15905371
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905371 
> not growing
> 2022/10/12 22:29:49.475 kid3| 23,3| Uri.cc(441) parse: Split URL 
> 'https://categories.articatech.net/fw.ping.php?_=1665576594826' into 
> proto='https', host='categories.articatech.net', port='443', 
> path='/fw.ping.php?_=1665576594826'
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905351 from SBuf15905361
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905353 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905353
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 28 
> for SBuf15905353
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905353 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 14,3| Address.cc(389) lookupHostIP: Given 
> Non-IP 'categories.articatech.net': Name or service not known
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905373
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 0 
> for SBuf15905373
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905373 
> not growing
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905352 from SBuf15905373
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905350 from SBuf15905305
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
> for SBuf15905374
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905374 
> not growing
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(390) parse: parsing 
> hdr: (0x56260e4038f8)
> Host: categories.articatech.net
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
> Gecko/20100101 Firefox/105.0
> Accept: */*
> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
> Accept-Encoding: gzip, deflate, br
> Referer: https://categories.articatech.net/index
> X-Requested-With: XMLHttpRequest
> Connection: keep-alive
> Cookie: _ga=GA1.2.1172828951.1642984448; 
> PHPSESSID=28871210b5b031a7b034981e9704a4ac
> Sec-Fetch-Dest: empty
> Sec-Fetch-Mode: cors
> Sec-Fetch-Site: same-origin
> 
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905375 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905375
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
> for SBuf15905375
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905375 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905376 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905376
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
> for SBuf15905376
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905376 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Host[30] at 0
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905377 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905377
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
> for SBuf15905377
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905377 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905378 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905378
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
> for SBuf15905378
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905378 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: User-Agent[67] at 1
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905379 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905379
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
> for SBuf15905379
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905379 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905380 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905380
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
> for SBuf15905380
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905380 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Accept[0] at 2
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905381 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905381
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
> for SBuf15905381
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905381 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905382 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905382
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
> for SBuf15905382
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905382 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Accept-Language[3] at 3
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905383 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905383
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
> for SBuf15905383
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905383 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905384 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905384
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 15 
> for SBuf15905384
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905384 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Accept-Encoding[2] at 4
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905385 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905385
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 7 
> for SBuf15905385
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905385 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905386 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905386
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 7 
> for SBuf15905386
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905386 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Referer[54] at 5
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905387
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 16 
> for SBuf15905387
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905387 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905388 from SBuf15905387
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Other:[87] at 6
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905389 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905389
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
> for SBuf15905389
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905389 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905390 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905390
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
> for SBuf15905390
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905390 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Connection[12] at 7
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905391 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905391
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
> for SBuf15905391
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905391 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905392 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905392
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
> for SBuf15905392
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905392 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Cookie[22] at 8
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905393
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
> for SBuf15905393
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905393 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905394 from SBuf15905393
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Other:[87] at 9
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905395
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
> for SBuf15905395
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905395 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905396 from SBuf15905395
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Other:[87] at 10
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905397
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
> for SBuf15905397
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905397 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905398 from SBuf15905397
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Other:[87] at 11
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905355
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 25 
> for SBuf15905355
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905355 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905354 from SBuf15905355
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
> for SBuf15905355
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905355 
> not growing
> 2022/10/12 22:29:49.475 kid3| 33,5| Http1Server.cc(193) 
> buildHttpRequest: normalize 1 Host header using categories.articatech.net
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
> for SBuf15905399
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905399 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905401 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905401
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
> for SBuf15905401
> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905401 
> new store capacity: 40
> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e4038f8 adding entry: Host[30] at 12
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905356 
> new store capacity: 16384
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 5 
> for SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
> not growing
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
> for SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
> not growing
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 2 
> for SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
> not growing
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 25 
> for SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
> not growing
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 28 
> for SBuf15905356
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905356 
> not growing
> 2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(699) 
> clientSetKeepaliveFlag: http_ver = HTTP/1.1
> 2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(700) 
> clientSetKeepaliveFlag: method = GET
> 2022/10/12 22:29:49.476 kid3| 55,6| HttpHeader.cc(850) getList: 
> 0x56260e4038f8: joined for id Connection[12]: 0x7ffc7248c400
> 2022/10/12 22:29:49.476 kid3| 33,4| client_side.cc(1510) quitAfterError: 
> Will close after error: conn214046 local=192.168.1.190:3128 
> remote=192.168.1.13:62858 FD 21 flags=1
> 2022/10/12 22:29:49.476 kid3| 33,5| client_side.cc(1531) 
> serveDelayedError: Responding with delated error for 
> https://categories.articatech.net/fw.ping.php?_=1665576594826
> 2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
> clientReplyContext::setReplyToStoreEntry locked key 
> AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*2
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
> 0x7ffc7248c000 checking fast rules
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
> aclCheckFast: list: 0x56260c88d7a8
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'ALLOWED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#1
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
> aclMatchDomainList: checking 'categories.articatech.net'
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
> aclMatchDomainList: 'categories.articatech.net' NOT found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 
> = 0
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#1 = 0
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'DENIED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#2
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
> '192.168.1.13:62858' found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#2 = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
> 0x7ffc7248c000 answer DENIED for match
> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
> 0x7ffc7248c000 checking fast rules
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
> aclCheckFast: list: 0x56260c8c46a8
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'ALLOWED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#1
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
> aclMatchDomainList: checking 'categories.articatech.net'
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
> aclMatchDomainList: 'categories.articatech.net' NOT found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 
> = 0
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#1 = 0
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'DENIED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#2
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
> '192.168.1.13:62858' found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#2 = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
> 0x7ffc7248c000 answer DENIED for match
> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
> *2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
> ERR_READ_ERROR/WITH_SERVER*
> 2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(117) pullData: 0 written 0 
> into conn214046 local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 
> flags=1
> 2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(141) getNextRangeOffset: 
> range: 0; http offset 0; reply 0
> 2022/10/12 22:29:49.476 kid3| 87,3| clientStream.cc(180) 
> clientStreamRead: clientStreamRead: Calling 1 with cbdata 0x56260e0d2ba8 
> from node 0x56260e6d26f8
> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(246) copy: 
> store_client::copy: AFC90000000000002632000003000000, from 0, for length 
> 4096, cb 1, cbdata 0x56260e0d1ad8
> 2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
> store_client::copy locked key AFC90000000000002632000003000000 
> e:=sp2XIV/0x56260e1b8290*3
> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(338) 
> storeClientCopy2: storeClientCopy2: AFC90000000000002632000003000000
> 2022/10/12 22:29:49.476 kid3| 33,5| store_client.cc(368) doCopy: 
> store_client::doCopy: co: 0, hi: 506665
> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(472) 
> scheduleMemRead: store_client::doCopy: Copying normal from memory
> 2022/10/12 22:29:49.476 kid3| 19,6| stmem.cc(230) copy: memCopy: 
> 0x56260db339f8 [0,4096)
> 2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2213) 
> sendMoreData: conn214046 local=192.168.1.190:3128 
> remote=192.168.1.13:62858 FD 21 flags=1 'categories.articatech.net:443' 
> out.offset=0
> 2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2241) 
> sendMoreData: clientReplyContext::sendMoreData: 
> https://categories.articatech.net/fw.ping.php?_=1665576594826, 4096 
> bytes (4096 new bytes)
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
> init-ing hdr: 0x56260e392c18 owner: 3
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(243) append: appending 
> hdr: 0x56260e392c18 += 0x56260db7a728
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905409 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905409
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 6 
> for SBuf15905409
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905409 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Server[57] at 0
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905410 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905410
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 12 
> for SBuf15905410
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905410 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Mime-Version[43] at 1
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905411 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905411
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 4 
> for SBuf15905411
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905411 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Date[24] at 2
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905412 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905412
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 12 
> for SBuf15905412
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905412 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Content-Type[21] at 3
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905413 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905413
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 14 
> for SBuf15905413
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905413 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Content-Length[17] at 4
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905414 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905414
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 13 
> for SBuf15905414
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905414 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: X-Squid-Error[76] at 5
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
> httpHeaderParseOffset: offset 506470 parsed as 506470
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
> httpHeaderParseOffset: offset 506470 parsed as 506470
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905416 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905416
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 7 
> for SBuf15905416
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905416 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: X-Cache[72] at 6
> 2022/10/12 22:29:49.476 kid3| 33,2| client_side_reply.cc(1585) 
> buildReplyHeader: clientBuildReplyHeader: Connection Keep-Alive not 
> requested by admin or client
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 16 
> for SBuf15905417
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905417 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 96 
> for SBuf15905417
> 2022/10/12 22:29:49.476 kid3| 24,7| MemBlob.cc(130) syncSize: 0 was: 0
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905417 
> new store capacity: 128
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(85) assign: assigning 
> SBuf15905418 from SBuf15905417
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 1 
> for SBuf15905418
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: SBuf15905418 
> not growing
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905420 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905420
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 3 
> for SBuf15905420
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905420 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Via[69] at 7
> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905422 
> from c-string, n=4294967295)
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from c-string 
> to id SBuf15905422
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 10 
> for SBuf15905422
> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: SBuf15905422 
> new store capacity: 40
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
> 0x56260e392c18 adding entry: Connection[12] at 8
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
> 0x7ffc7248bb70 checking fast rules
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
> aclCheckFast: list: 0x56260c8adf68
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> http_header_access Server
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'DENIED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> http_header_access Server#1
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
> '192.168.1.13:62858' found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> http_header_access Server#1 = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> http_header_access Server = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
> 0x7ffc7248bb70 answer DENIED for match
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(308) 
> httpHdrMangle: checklist denied, we have no replacement. Pass
> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bb70
> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bb70
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
> httpHdrMangle: couldn't find mangler or access list. Allowing
> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(769) refreshMask: 
> refreshing the mask in hdr 0x56260e392c18
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
> 0x7ffc7248bcb0 checking fast rules
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
> aclCheckFast: list: 0x56260c88d7a8
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'ALLOWED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#1
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
> aclMatchDomainList: checking 'categories.articatech.net'
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
> aclMatchDomainList: 'categories.articatech.net' NOT found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 
> = 0
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#1 = 0
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'DENIED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#2
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
> '192.168.1.13:62858' found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#2 = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
> 0x7ffc7248bcb0 answer DENIED for match
> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
> 0x7ffc7248bcb0 checking fast rules
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
> aclCheckFast: list: 0x56260c8c46a8
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'ALLOWED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#1
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
> aclMatchDomainList: checking 'categories.articatech.net'
> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
> aclMatchDomainList: 'categories.articatech.net' NOT found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: Group2 
> = 0
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#1 = 0
> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
> Action 'DENIED/0' is not banned
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
> delay_access#2
> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
> '192.168.1.13:62858' found
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access#2 = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
> delay_access = 1
> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
> 0x7ffc7248bcb0 answer DENIED for match
> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
> 2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
> 2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(70) preCheck: 
> 0x7ffc7248bc10 checking fast ACLs
> 2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
> reply_body_max_size -1
> 2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
> (reply_body_max_size -1 line)
> 2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
> (reply_body_max_size -1 line) = 1
> 2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
> reply_body_max_size -1 = 1
> 2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(63) markFinished: 
> 0x7ffc7248bc10 answer ALLOWED for match
> 2022/10/12 22:29:49.477 kid3| 58,4| HttpReply.cc(564) calcMaxBodySize: 
> bodySizeMax=-1
> 2022/10/12 22:29:49.477 kid3| 28,4| FilledChecklist.cc(67) 
> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bc10
> 2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bc10
> 2022/10/12 22:29:49.477 kid3| 58,7| HttpReply.cc(528) 
> expectedBodyTooLarge: bodySizeMax=-1
> 2022/10/12 22:29:49.477 kid3| 88,2| client_side_reply.cc(2090) 
> processReplyAccessResult: The reply for GET 
> https://categories.articatech.net/fw.ping.php?_=1665576594826 is 
> ALLOWED, because it matched (reply_body_max_size -1 line)
> 2022/10/12 22:29:49.477 kid3| 20,3| store.cc(443) lock: 
> ClientHttpRequest::loggingEntry locked key 
> AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*4
> 2022/10/12 22:29:49.477 kid3| 88,3| client_side_reply.cc(2128) 
> processReplyAccessResult: clientReplyContext::sendMoreData: Appending 
> 3901 bytes after 195 bytes of headers
> 2022/10/12 22:29:49.477 kid3| 87,3| clientStream.cc(158) 
> clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 
> 0x56260e0fa280 from node 0x56260e5c0ab8
> 2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
> 0x56260e6cd470 front 0x56260e0f9210*4
> 2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
> 0x56260e6cd470 front 0x56260e0f9210*4
> 2022/10/12 22:29:49.477 kid3| 55,7| HttpHeader.cc(589) packInto: 
> 0x56260e392c18 into 0x56260e26ebe8
> 2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(279) sendStartOfMessage: 
> HTTP Client conn214046 local=192.168.1.190:3128 
> remote=192.168.1.13:62858 FD 21 flags=1
> 2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(280) sendStartOfMessage: 
> HTTP Client REPLY:
> ---------
> HTTP/1.1 502 Bad Gateway
> Mime-Version: 1.0
> Date: Wed, 12 Oct 2022 20:29:49 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 506470
> X-Squid-Error: ERR_READ_ERROR 0
> X-Cache: MISS from proxy-190.articatech.int
> Via: 1.1 789aaa51-a1eb-eb48-639b-000070877aed (squid)
> Connection: close
> 
> 
> Le 12/10/2022 ? 20:00, Alex Rousskov a ?crit?:
>> On 10/12/22 12:45, David Touzeau wrote:
>>> Hi
>>>
>>> We using squid 5.7 after adding ssl-bump we have sometimes several 
>>> 502 error? with extended error ERR_READ_ERROR|WITH_SERVER
>>>
>>> 1665589818.831???? 11 192.168.1.13 NONE_NONE/502 192616 OPTIONS 
>>> https://www2.deepl.com/jsonrpc?method=LMT_split_text - HIER_NONE/-:- 
>>> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
>>> 1665589839.288???? 11 192.168.1.13 NONE_NONE/502 506759 POST 
>>> https://pollserver.lastpass.com/poll_server.php - HIER_NONE/-:- 
>>> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
>>> 1665589719.879???? 44 192.168.1.13 NONE_NONE/502 506954 GET 
>>> https://contile.services.mozilla.com/v1/tiles - HIER_NONE/-:- 
>>> text/html mac="68:54:5a:94:e7:56" - exterr="ERR_READ_ERROR|WITH_SERVER"
>>
>>> What does it means.
>>
>> 502 with ERR_READ_ERROR|WITH_SERVER may mean several things 
>> (unfortunately). Given HIER_NONE, I would suspect that Squid could not 
>> find a valid destination for the request. There is a similar recent 
>> squid-users thread at 
>> http://lists.squid-cache.org/pipermail/squid-users/2022-October/025289.html 
>>
>>
>>
>>> how can we fix it ?
>>
>> The first step is to identify what causes these errors.
>>
>> Can you reproduce this problem at will? Perhaps by trying going to 
>> https://dnslabeldoesnotexist.com mentioned at the above thread? If you 
>> can, consider sharing (a pointer to) a compressed debugging cache.log 
>> from a test box that does not expose any internal secrets, as detailed 
>> at 
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction 
>>
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From rafael.akchurin at diladele.com  Thu Oct 13 06:42:37 2022
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 13 Oct 2022 06:42:37 +0000
Subject: [squid-users] Ubuntu 20.04 LTS repository for Squid 5.7 (rebuilt
 from sources in Debian unstable)
Message-ID: <AM8PR04MB77457FBA950CFBF3C7790A868F259@AM8PR04MB7745.eurprd04.prod.outlook.com>

Hello everyone,

Online repository with latest Squid 5.7 (rebuilt from sources in Debian unstable) for Ubuntu 20.04 LTS 64-bit is available at https://squid57.diladele.com/.
Github repo https://github.com/diladele/squid-ubuntu/tree/master/src/ubuntu20 contains the scripts we used to make this compilation.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add new repo
echo "deb https://squid57.diladele.com/ubuntu/ focal main" \
    > /etc/apt/sources.list.d/squid57.diladele.com.list

# and install
apt-get update && apt-get install -y \
    squid-common \
    squid-openssl \
    squidclient \
    libecap3 libecap3-dev

Hope you will find this useful.

Best regards,
Rafael Akchurin
Diladele B.V.

--
The same Squid 5.7 will be part of upcoming Web Safety 8.3 planned for release in winter of 2023.  This version will contain the Microsoft Azure Tenant Restrictions and Google App Limitation functionality as well as GEO-IP filtering. Download the latest virtual appliance of Web Safety from https://www.diladele.com/download.html




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221013/4cb13c44/attachment.htm>

From rousskov at measurement-factory.com  Thu Oct 13 17:41:49 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Oct 2022 13:41:49 -0400
Subject: [squid-users] Squid 5.7 + bump ERR_READ_ERROR|WITH_SERVER
In-Reply-To: <3c031fcb-3488-c2eb-599a-5655e0dc93ad@articatech.com>
References: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>
 <46b27dfc-c4bf-fada-4e1f-4ee90f877da6@measurement-factory.com>
 <5724b3b3-c76f-df95-e07c-117732bc3861@articatech.com>
 <03467c7f-d1d0-fa6c-ad93-0ab68f8f9dd3@measurement-factory.com>
 <3c031fcb-3488-c2eb-599a-5655e0dc93ad@articatech.com>
Message-ID: <f948d84e-e8f2-bdab-14de-4623a41451e7@measurement-factory.com>

On 10/13/22 03:05, David Touzeau wrote:
> I have pushed more previous data here:
> 
> http://articatech.net/tmpf/cache.log.txt

As I suspected, a Squid worker (kid3) found no destinations that can be 
used to forward this request:

> peer_select.cc(479) resolveSelected: PeerSelector43309 found all 0 destinations for categories.articatech.net:443

> FwdState.cc(209) stopAndDestroy: for path selection found no paths

> FwdState.cc(492) fail: ERR_READ_ERROR "Bad Gateway"


There were no destinations because Squid cache had a no-IPs DNS entry 
for the above domain:

> ipcache.cc(647) ipcache_nbgethostbyname_: ... HIT for 'categories.articatech.net'

> ipcache.cc(231) finalCallback: ... lookup_err=No DNS records



AFAICT, the shared logs start _after_ the corresponding DNS cache entry 
was created (or lost its IPs), so I cannot tell what actually happened 
back then.

Later, the expired(?) no-IPs DNS cache entry is purged, and Squid is 
able to resolve the same host name:

> ipcache.cc(532) addGood: categories.articatech.net #1 217.182.193.199


And similar transactions start to succeed from this point.


If you can share more/earlier logs, I may be able to tell what caused 
the 'No DNS records' problem.


HTH,

Alex.
P.S. Please compress debugging logs when sharing.


> Le 13/10/2022 ? 00:16, Alex Rousskov a ?crit?:
>> On 10/12/22 17:58, David Touzeau wrote:
>>
>>> We have put it in debug mode :
>>
>> Unfortunately, this log starts at GET request processing _after_ 
>> CONNECT request processing has already encountered and saved the 
>> error. Thus, I cannot tell you why that error was encountered. I need 
>> to see the lines before the lines you have shared. If you still have 
>> the log file, searching for AFC90000000000002632000003000000 (above 
>> the already shared lines) might help you find where the error message 
>> was created.
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> you can see :
>>>
>>>
>>> "2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
>>> ERR_READ_ERROR/WITH_SERVER"
>>>
>>> As without ssl-bump, there is no issue.
>>>
>>> the full log can be downloaded here 
>>> http://articatech.net/tmpf/cache.log.txt*
>>> *
>>>
>>> }
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905084 from SBuf15905303
>>> 2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1357) 
>>> parseHttpRequest: HTTP Client conn214046 local=192.168.1.190:3128 
>>> remote=192.168.1.13:62858 FD 21 flags=1
>>> 2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1361) 
>>> parseHttpRequest: HTTP Client REQUEST:
>>> ---------
>>> GET /fw.ping.php?_=1665576594826 HTTP/1.1
>>> Host: categories.articatech.net
>>> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
>>> Gecko/20100101 Firefox/105.0
>>> Accept: */*
>>> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
>>> Accept-Encoding: gzip, deflate, br
>>> Referer: https://categories.articatech.net/index
>>> X-Requested-With: XMLHttpRequest
>>> Connection: keep-alive
>>> Cookie: _ga=GA1.2.1172828951.1642984448; 
>>> PHPSESSID=28871210b5b031a7b034981e9704a4ac
>>> Sec-Fetch-Dest: empty
>>> Sec-Fetch-Mode: cors
>>> Sec-Fetch-Site: same-origin
>>>
>>>
>>> ----------
>>> 2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1393) 
>>> parseHttpRequest: complete request received. prefix_sz = 542, 
>>> request-line-size=43, mime-header-size=499, mime header block:
>>> Host: categories.articatech.net
>>> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
>>> Gecko/20100101 Firefox/105.0
>>> Accept: */*
>>> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
>>> Accept-Encoding: gzip, deflate, br
>>> Referer: https://categories.articatech.net/index
>>> X-Requested-With: XMLHttpRequest
>>> Connection: keep-alive
>>> Cookie: _ga=GA1.2.1172828951.1642984448; 
>>> PHPSESSID=28871210b5b031a7b034981e9704a4ac
>>> Sec-Fetch-Dest: empty
>>> Sec-Fetch-Mode: cors
>>> Sec-Fetch-Site: same-origin
>>>
>>>
>>> ----------
>>> 2022/10/12 22:29:49.475 kid3| 93,5| AsyncJob.cc(34) AsyncJob: 
>>> AsyncJob constructed, this=0x56260e3814b8 type=ClientHttpRequest 
>>> [job199106]
>>> 2022/10/12 22:29:49.475 kid3| 1,5| CodeContext.cc(60) Entering: ALE 
>>> w/o master
>>> 2022/10/12 22:29:49.475 kid3| 87,3| clientStream.cc(140) 
>>> clientStreamInsertHead: clientStreamInsertHead: Inserted node 
>>> 0x56260e6d26f8 with data 0x56260e0fa280 after head
>>> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1413) 
>>> parseHttpRequest: Prepare absolute URL from
>>> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(228) 
>>> getHostHeaderField: looking for Host
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>> first not of characterset non-LF in id SBuf15905340
>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>> consuming 32 bytes
>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>> consuming 1 bytes
>>> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(249) 
>>> getHostHeaderField: checking Host: categories.articatech.net
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>> first not of characterset WSP in id SBuf15905345
>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>> consuming 1 bytes
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905339 from SBuf15905345
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>> first not of characterset host in id SBuf15905339
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(771) findFirstNotOf: not 
>>> found
>>> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(267) 
>>> getHostHeaderField: returning categories.articatech.net
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905347 from SBuf183
>>> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1294) 
>>> prepareTlsSwitchingURL: TLS switching host rewrite: 
>>> https://categories.articatech.net/fw.ping.php?_=1665576594826
>>> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1981) 
>>> clientParseRequests: conn214046 local=192.168.1.190:3128 
>>> remote=192.168.1.13:62858 FD 21 flags=1: done parsing a request
>>> 2022/10/12 22:29:49.475 kid3| 5,4| AsyncCall.cc(30) AsyncCall: The 
>>> AsyncCall ConnStateData::lifetimeTimeout constructed, 
>>> this=0x56260e5afd50 [call1972958]
>>> 2022/10/12 22:29:49.475 kid3| 5,3| comm.cc(571) commSetConnTimeout: 
>>> conn214046 local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 
>>> flags=1 timeout 86400
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: 
>>> cleaning hdr: 0x56260e392c18 owner: 3
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: 
>>> cleaning hdr: 0x56260e392c18 owner: 3
>>> 2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1806) add: 
>>> 0x56260e0f9210*3 to 0/1
>>> 2022/10/12 22:29:49.475 kid3| 33,3| Pipeline.cc(24) add: Pipeline 
>>> 0x56260e6cd470 add request 2 0x56260e0f9210*4
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905349
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 61 for SBuf15905349
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905349 new store capacity: 128
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
>>> init-ing hdr: 0x56260e4038f8 owner: 2
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>> first not of characterset special in id SBuf15905363
>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>> consuming 5 bytes
>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>> consuming 1 bytes
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905367 from SBuf183
>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>> consuming 2 bytes
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 1 for SBuf15905371
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905371 not growing
>>> 2022/10/12 22:29:49.475 kid3| 23,3| Uri.cc(441) parse: Split URL 
>>> 'https://categories.articatech.net/fw.ping.php?_=1665576594826' into 
>>> proto='https', host='categories.articatech.net', port='443', 
>>> path='/fw.ping.php?_=1665576594826'
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905351 from SBuf15905361
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905353 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905353
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 28 for SBuf15905353
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905353 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'categories.articatech.net': Name or service not known
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905373
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 0 for SBuf15905373
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905373 not growing
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905352 from SBuf15905373
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905350 from SBuf15905305
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 1 for SBuf15905374
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905374 not growing
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(390) parse: parsing 
>>> hdr: (0x56260e4038f8)
>>> Host: categories.articatech.net
>>> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
>>> Gecko/20100101 Firefox/105.0
>>> Accept: */*
>>> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
>>> Accept-Encoding: gzip, deflate, br
>>> Referer: https://categories.articatech.net/index
>>> X-Requested-With: XMLHttpRequest
>>> Connection: keep-alive
>>> Cookie: _ga=GA1.2.1172828951.1642984448; 
>>> PHPSESSID=28871210b5b031a7b034981e9704a4ac
>>> Sec-Fetch-Dest: empty
>>> Sec-Fetch-Mode: cors
>>> Sec-Fetch-Site: same-origin
>>>
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905375 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905375
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 4 for SBuf15905375
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905375 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905376 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905376
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 4 for SBuf15905376
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905376 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Host[30] at 0
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905377 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905377
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 10 for SBuf15905377
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905377 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905378 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905378
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 10 for SBuf15905378
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905378 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: User-Agent[67] at 1
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905379 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905379
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 6 for SBuf15905379
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905379 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905380 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905380
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 6 for SBuf15905380
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905380 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Accept[0] at 2
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905381 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905381
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 15 for SBuf15905381
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905381 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905382 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905382
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 15 for SBuf15905382
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905382 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Accept-Language[3] at 3
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905383 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905383
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 15 for SBuf15905383
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905383 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905384 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905384
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 15 for SBuf15905384
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905384 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Accept-Encoding[2] at 4
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905385 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905385
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 7 for SBuf15905385
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905385 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905386 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905386
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 7 for SBuf15905386
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905386 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Referer[54] at 5
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905387
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 16 for SBuf15905387
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905387 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905388 from SBuf15905387
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Other:[87] at 6
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905389 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905389
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 10 for SBuf15905389
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905389 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905390 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905390
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 10 for SBuf15905390
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905390 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Connection[12] at 7
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905391 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905391
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 6 for SBuf15905391
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905391 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905392 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905392
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 6 for SBuf15905392
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905392 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Cookie[22] at 8
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905393
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 14 for SBuf15905393
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905393 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905394 from SBuf15905393
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Other:[87] at 9
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905395
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 14 for SBuf15905395
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905395 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905396 from SBuf15905395
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Other:[87] at 10
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905397
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 14 for SBuf15905397
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905397 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905398 from SBuf15905397
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Other:[87] at 11
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905355
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 25 for SBuf15905355
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905355 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905354 from SBuf15905355
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 6 for SBuf15905355
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905355 not growing
>>> 2022/10/12 22:29:49.475 kid3| 33,5| Http1Server.cc(193) 
>>> buildHttpRequest: normalize 1 Host header using 
>>> categories.articatech.net
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 1 for SBuf15905399
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905399 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: SBuf15905401 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905401
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 4 for SBuf15905401
>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905401 new store capacity: 40
>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e4038f8 adding entry: Host[30] at 12
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905356 new store capacity: 16384
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 5 for SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905356 not growing
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 1 for SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905356 not growing
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 2 for SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905356 not growing
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 25 for SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905356 not growing
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 28 for SBuf15905356
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905356 not growing
>>> 2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(699) 
>>> clientSetKeepaliveFlag: http_ver = HTTP/1.1
>>> 2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(700) 
>>> clientSetKeepaliveFlag: method = GET
>>> 2022/10/12 22:29:49.476 kid3| 55,6| HttpHeader.cc(850) getList: 
>>> 0x56260e4038f8: joined for id Connection[12]: 0x7ffc7248c400
>>> 2022/10/12 22:29:49.476 kid3| 33,4| client_side.cc(1510) 
>>> quitAfterError: Will close after error: conn214046 
>>> local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 flags=1
>>> 2022/10/12 22:29:49.476 kid3| 33,5| client_side.cc(1531) 
>>> serveDelayedError: Responding with delated error for 
>>> https://categories.articatech.net/fw.ping.php?_=1665576594826
>>> 2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
>>> clientReplyContext::setReplyToStoreEntry locked key 
>>> AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*2
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>> 0x7ffc7248c000 checking fast rules
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>> aclCheckFast: list: 0x56260c88d7a8
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'ALLOWED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#1
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>> aclMatchDomainList: checking 'categories.articatech.net'
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> Group2 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#1 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'DENIED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#2
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>> '192.168.1.13:62858' found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all 
>>> = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#2 = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>> 0x7ffc7248c000 answer DENIED for match
>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
>>> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>> 0x7ffc7248c000 checking fast rules
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>> aclCheckFast: list: 0x56260c8c46a8
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'ALLOWED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#1
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>> aclMatchDomainList: checking 'categories.articatech.net'
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> Group2 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#1 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'DENIED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#2
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>> '192.168.1.13:62858' found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all 
>>> = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#2 = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>> 0x7ffc7248c000 answer DENIED for match
>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
>>> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
>>> *2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
>>> ERR_READ_ERROR/WITH_SERVER*
>>> 2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(117) pullData: 0 
>>> written 0 into conn214046 local=192.168.1.190:3128 
>>> remote=192.168.1.13:62858 FD 21 flags=1
>>> 2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(141) 
>>> getNextRangeOffset: range: 0; http offset 0; reply 0
>>> 2022/10/12 22:29:49.476 kid3| 87,3| clientStream.cc(180) 
>>> clientStreamRead: clientStreamRead: Calling 1 with cbdata 
>>> 0x56260e0d2ba8 from node 0x56260e6d26f8
>>> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(246) copy: 
>>> store_client::copy: AFC90000000000002632000003000000, from 0, for 
>>> length 4096, cb 1, cbdata 0x56260e0d1ad8
>>> 2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
>>> store_client::copy locked key AFC90000000000002632000003000000 
>>> e:=sp2XIV/0x56260e1b8290*3
>>> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(338) 
>>> storeClientCopy2: storeClientCopy2: AFC90000000000002632000003000000
>>> 2022/10/12 22:29:49.476 kid3| 33,5| store_client.cc(368) doCopy: 
>>> store_client::doCopy: co: 0, hi: 506665
>>> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(472) 
>>> scheduleMemRead: store_client::doCopy: Copying normal from memory
>>> 2022/10/12 22:29:49.476 kid3| 19,6| stmem.cc(230) copy: memCopy: 
>>> 0x56260db339f8 [0,4096)
>>> 2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2213) 
>>> sendMoreData: conn214046 local=192.168.1.190:3128 
>>> remote=192.168.1.13:62858 FD 21 flags=1 
>>> 'categories.articatech.net:443' out.offset=0
>>> 2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2241) 
>>> sendMoreData: clientReplyContext::sendMoreData: 
>>> https://categories.articatech.net/fw.ping.php?_=1665576594826, 4096 
>>> bytes (4096 new bytes)
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
>>> init-ing hdr: 0x56260e392c18 owner: 3
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(243) append: 
>>> appending hdr: 0x56260e392c18 += 0x56260db7a728
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905409 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905409
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 6 for SBuf15905409
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905409 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Server[57] at 0
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905410 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905410
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 12 for SBuf15905410
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905410 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Mime-Version[43] at 1
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905411 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905411
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 4 for SBuf15905411
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905411 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Date[24] at 2
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905412 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905412
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 12 for SBuf15905412
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905412 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Content-Type[21] at 3
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905413 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905413
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 14 for SBuf15905413
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905413 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Content-Length[17] at 4
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905414 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905414
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 13 for SBuf15905414
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905414 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: X-Squid-Error[76] at 5
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
>>> httpHeaderParseOffset: offset 506470 parsed as 506470
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
>>> httpHeaderParseOffset: offset 506470 parsed as 506470
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905416 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905416
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 7 for SBuf15905416
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905416 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: X-Cache[72] at 6
>>> 2022/10/12 22:29:49.476 kid3| 33,2| client_side_reply.cc(1585) 
>>> buildReplyHeader: clientBuildReplyHeader: Connection Keep-Alive not 
>>> requested by admin or client
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 16 for SBuf15905417
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905417 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 96 for SBuf15905417
>>> 2022/10/12 22:29:49.476 kid3| 24,7| MemBlob.cc(130) syncSize: 0 was: 0
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905417 new store capacity: 128
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>> SBuf15905418 from SBuf15905417
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 1 for SBuf15905418
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>> SBuf15905418 not growing
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905420 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905420
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 3 for SBuf15905420
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905420 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Via[69] at 7
>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: SBuf15905422 
>>> from c-string, n=4294967295)
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>> c-string to id SBuf15905422
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: reserving 
>>> 10 for SBuf15905422
>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>> SBuf15905422 new store capacity: 40
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>> 0x56260e392c18 adding entry: Connection[12] at 8
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>> 0x7ffc7248bb70 checking fast rules
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>> aclCheckFast: list: 0x56260c8adf68
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> http_header_access Server
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'DENIED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> http_header_access Server#1
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>> '192.168.1.13:62858' found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all 
>>> = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> http_header_access Server#1 = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> http_header_access Server = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>> 0x7ffc7248bb70 answer DENIED for match
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(308) 
>>> httpHdrMangle: checklist denied, we have no replacement. Pass
>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bb70
>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
>>> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bb70
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(769) refreshMask: 
>>> refreshing the mask in hdr 0x56260e392c18
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>> 0x7ffc7248bcb0 checking fast rules
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>> aclCheckFast: list: 0x56260c88d7a8
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'ALLOWED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#1
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>> aclMatchDomainList: checking 'categories.articatech.net'
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> Group2 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#1 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'DENIED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#2
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>> '192.168.1.13:62858' found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all 
>>> = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#2 = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>> 0x7ffc7248bcb0 answer DENIED for match
>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
>>> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>> 0x7ffc7248bcb0 checking fast rules
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>> aclCheckFast: list: 0x56260c8c46a8
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'ALLOWED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#1
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking Group2
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>> aclMatchDomainList: checking 'categories.articatech.net'
>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> Group2 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#1 = 0
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>> Action 'DENIED/0' is not banned
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>> delay_access#2
>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>> '192.168.1.13:62858' found
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: all 
>>> = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access#2 = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> delay_access = 1
>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>> 0x7ffc7248bcb0 answer DENIED for match
>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
>>> 2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
>>> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
>>> 2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(70) preCheck: 
>>> 0x7ffc7248bc10 checking fast ACLs
>>> 2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
>>> reply_body_max_size -1
>>> 2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
>>> (reply_body_max_size -1 line)
>>> 2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> (reply_body_max_size -1 line) = 1
>>> 2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
>>> reply_body_max_size -1 = 1
>>> 2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(63) markFinished: 
>>> 0x7ffc7248bc10 answer ALLOWED for match
>>> 2022/10/12 22:29:49.477 kid3| 58,4| HttpReply.cc(564) 
>>> calcMaxBodySize: bodySizeMax=-1
>>> 2022/10/12 22:29:49.477 kid3| 28,4| FilledChecklist.cc(67) 
>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bc10
>>> 2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) ~ACLChecklist: 
>>> ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bc10
>>> 2022/10/12 22:29:49.477 kid3| 58,7| HttpReply.cc(528) 
>>> expectedBodyTooLarge: bodySizeMax=-1
>>> 2022/10/12 22:29:49.477 kid3| 88,2| client_side_reply.cc(2090) 
>>> processReplyAccessResult: The reply for GET 
>>> https://categories.articatech.net/fw.ping.php?_=1665576594826 is 
>>> ALLOWED, because it matched (reply_body_max_size -1 line)
>>> 2022/10/12 22:29:49.477 kid3| 20,3| store.cc(443) lock: 
>>> ClientHttpRequest::loggingEntry locked key 
>>> AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*4
>>> 2022/10/12 22:29:49.477 kid3| 88,3| client_side_reply.cc(2128) 
>>> processReplyAccessResult: clientReplyContext::sendMoreData: Appending 
>>> 3901 bytes after 195 bytes of headers
>>> 2022/10/12 22:29:49.477 kid3| 87,3| clientStream.cc(158) 
>>> clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 
>>> 0x56260e0fa280 from node 0x56260e5c0ab8
>>> 2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
>>> 0x56260e6cd470 front 0x56260e0f9210*4
>>> 2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
>>> 0x56260e6cd470 front 0x56260e0f9210*4
>>> 2022/10/12 22:29:49.477 kid3| 55,7| HttpHeader.cc(589) packInto: 
>>> 0x56260e392c18 into 0x56260e26ebe8
>>> 2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(279) 
>>> sendStartOfMessage: HTTP Client conn214046 local=192.168.1.190:3128 
>>> remote=192.168.1.13:62858 FD 21 flags=1
>>> 2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(280) 
>>> sendStartOfMessage: HTTP Client REPLY:
>>> ---------
>>> HTTP/1.1 502 Bad Gateway
>>> Mime-Version: 1.0
>>> Date: Wed, 12 Oct 2022 20:29:49 GMT
>>> Content-Type: text/html;charset=utf-8
>>> Content-Length: 506470
>>> X-Squid-Error: ERR_READ_ERROR 0
>>> X-Cache: MISS from proxy-190.articatech.int
>>> Via: 1.1 789aaa51-a1eb-eb48-639b-000070877aed (squid)
>>> Connection: close
>>>
>>>
>>> Le 12/10/2022 ? 20:00, Alex Rousskov a ?crit?:
>>>> On 10/12/22 12:45, David Touzeau wrote:
>>>>> Hi
>>>>>
>>>>> We using squid 5.7 after adding ssl-bump we have sometimes several 
>>>>> 502 error? with extended error ERR_READ_ERROR|WITH_SERVER
>>>>>
>>>>> 1665589818.831???? 11 192.168.1.13 NONE_NONE/502 192616 OPTIONS 
>>>>> https://www2.deepl.com/jsonrpc?method=LMT_split_text - 
>>>>> HIER_NONE/-:- text/html mac="68:54:5a:94:e7:56" - 
>>>>> exterr="ERR_READ_ERROR|WITH_SERVER"
>>>>> 1665589839.288???? 11 192.168.1.13 NONE_NONE/502 506759 POST 
>>>>> https://pollserver.lastpass.com/poll_server.php - HIER_NONE/-:- 
>>>>> text/html mac="68:54:5a:94:e7:56" - 
>>>>> exterr="ERR_READ_ERROR|WITH_SERVER"
>>>>> 1665589719.879???? 44 192.168.1.13 NONE_NONE/502 506954 GET 
>>>>> https://contile.services.mozilla.com/v1/tiles - HIER_NONE/-:- 
>>>>> text/html mac="68:54:5a:94:e7:56" - 
>>>>> exterr="ERR_READ_ERROR|WITH_SERVER"
>>>>
>>>>> What does it means.
>>>>
>>>> 502 with ERR_READ_ERROR|WITH_SERVER may mean several things 
>>>> (unfortunately). Given HIER_NONE, I would suspect that Squid could 
>>>> not find a valid destination for the request. There is a similar 
>>>> recent squid-users thread at 
>>>> http://lists.squid-cache.org/pipermail/squid-users/2022-October/025289.html 
>>>>
>>>>
>>>>
>>>>> how can we fix it ?
>>>>
>>>> The first step is to identify what causes these errors.
>>>>
>>>> Can you reproduce this problem at will? Perhaps by trying going to 
>>>> https://dnslabeldoesnotexist.com mentioned at the above thread? If 
>>>> you can, consider sharing (a pointer to) a compressed debugging 
>>>> cache.log from a test box that does not expose any internal secrets, 
>>>> as detailed at 
>>>> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction 
>>>>
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>
> -- 
> Technical Support
> 	
> 	
> *David Touzeau*
> Orgerus, Yvelines, France
> *Artica Tech*
> 
> P:?+33 6 58 44 69 46
> www: wiki.articatech.com <https://wiki.articatech.com>
> www: articatech.net <http://articatech.net>
> 



From wadie.lemrazzeq at capgemini.com  Fri Oct 14 14:32:11 2022
From: wadie.lemrazzeq at capgemini.com (LEMRAZZEQ, Wadie)
Date: Fri, 14 Oct 2022 14:32:11 +0000
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>

Hello,

I'm trying to set up an encrypted communication between the browser and squid

theoretically, I followed this section to implement it : https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection

I tried to implement this on a dockerized Alpine, and a squid 5.5 with openssl module
in squid.conf, I have:
...
http_port 3128
https_port 3129 cert=/etc/squid/crt.pem key=/etc/squid/key.pem
...

but when I request squid https port, I got this error every time, in cache.log:
...
ERROR: failure while accepting a TLS connection on conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1: 0x7fbd208f33e0*1
    connection: conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1
Pipeline.cc(31) front: Pipeline 0x7fbd208f13a0 empty
Error.cc(22) update: recent: ERR_SECURE_ACCEPT_FAIL/SQUID_TLS_ERR_ACCEPT+TLS_LIB_ERR=1408F09B+TLS_IO_ERR=1
...

I also tried this with squid 4.10 with gnutls module, in an Ubuntu 20.40 environment, with the same squid.conf, and I got again a TLS error
...
client_side.cc(2597) tlsAttemptHandshake: Error negotiating TLS on local=x.x.x.x:3129 remote=x.x.x.x:50874 FD 11 flags=1: Aborted by client: An unexpected TLS packet was received.
...

I used for certificates, a self signed one, and a generated certificate signed by our CA, for both scenarios

Also, I tried multiple https_port options (disable some SSL implementation, manipulation of client certificates...) but without success

Am I missing something in the squid configuration?
This message contains information that may be privileged or confidential and is the property of the Capgemini Group. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain, copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221014/22d927fb/attachment.htm>

From rousskov at measurement-factory.com  Fri Oct 14 17:34:06 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Oct 2022 13:34:06 -0400
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>

On 10/14/22 10:32, LEMRAZZEQ, Wadie wrote:
> I tried to implement this on a dockerized Alpine, and a squid 5.5 with openssl module

FWIW, Squid v5.5 is unusable in many environments -- too many bugs. Use 
v5.7 or later. I do not know whether one of those bugs are responsible 
for the specific problem you are discussing though.


> in squid.conf, I have:
> 
> ...
> 
> http_port 3128
> 
> https_port 3129 cert=/etc/squid/crt.pem key=/etc/squid/key.pem

OK.


> but when I request squid https port, I got this error every time, in 
> cache.log:

_How_ do you "request squid https port"?


> ERROR: failure while accepting a TLS connection on conn77 
> local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1: 
> 
> connection: conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1
> 
> Error.cc(22) update: recent: 
> ERR_SECURE_ACCEPT_FAIL/SQUID_TLS_ERR_ACCEPT+TLS_LIB_ERR=1408F09B+TLS_IO_ERR=1

According to "openssl errstr", that OpenSSL error is:

     error:1408F09B:SSL routines:ssl3_get_record:https proxy request


Most likely, the client is sending a plain text CONNECT request before 
encrypting the TLS connection to the HTTPS proxy. In other words, the 
client thinks it is talking to an HTTP proxy while you want it to think 
that it is talking to an HTTPS proxy. For example,

* HTTP proxy:  curl -x http://172.17.0.2:3128/ ... https://example.com
* HTTPS proxy: curl -x https://172.17.0.2:3129/ ... https://example.com


HTH,

Alex.




> ...
> 
> I also tried this with squid 4.10 with gnutls module, in an Ubuntu 20.40 
> environment, with the same squid.conf, and I got again a TLS error
> 
> ...
> 
> client_side.cc(2597) tlsAttemptHandshake: Error negotiating TLS on 
> local=x.x.x.x:3129 remote=x.x.x.x:50874 FD 11 flags=1: Aborted by 
> client: An unexpected TLS packet was received.
> 
> ...
> 
> I used for certificates, a self signed one, and a generated certificate 
> signed by our CA, for both scenarios
> 
> Also, I tried multiple https_port options (disable some SSL 
> implementation, manipulation of client certificates...) but without success



From rousskov at measurement-factory.com  Mon Oct 17 13:41:42 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 17 Oct 2022 09:41:42 -0400
Subject: [squid-users] Squid 5.7 + bump ERR_READ_ERROR|WITH_SERVER
In-Reply-To: <eed79f9b-2da7-5faf-c078-8c4ba5183221@articatech.com>
References: <c71c8c45-20af-c43a-2468-f35927d77067@articatech.com>
 <46b27dfc-c4bf-fada-4e1f-4ee90f877da6@measurement-factory.com>
 <5724b3b3-c76f-df95-e07c-117732bc3861@articatech.com>
 <03467c7f-d1d0-fa6c-ad93-0ab68f8f9dd3@measurement-factory.com>
 <3c031fcb-3488-c2eb-599a-5655e0dc93ad@articatech.com>
 <f948d84e-e8f2-bdab-14de-4623a41451e7@measurement-factory.com>
 <eed79f9b-2da7-5faf-c078-8c4ba5183221@articatech.com>
Message-ID: <ea98a479-8cbc-4112-0ec0-14fc40433714@measurement-factory.com>

On 10/17/22 05:55, David Touzeau wrote:

> After few days production, it seems you are right, there is some DNS issues.
> 
> That is strange because in v4 on the same server, we have no issue.
> 
> Maybe a coincidence and the fact that v4 is less sensitive to DNS errors

I would rather not speculate regarding such vague terms as DNS error 
sensitivity of a Squid version. If you find a way to reproduce the 
problem and/or collect more logs, we should be able to get to the bottom 
of this. It is possible that we need to fix some Squid v5 bugs here...

Alex.


> Le 13/10/2022 ? 19:41, Alex Rousskov a ?crit?:
>> On 10/13/22 03:05, David Touzeau wrote:
>>> I have pushed more previous data here:
>>>
>>> http://articatech.net/tmpf/cache.log.txt
>>
>> As I suspected, a Squid worker (kid3) found no destinations that can 
>> be used to forward this request:
>>
>>> peer_select.cc(479) resolveSelected: PeerSelector43309 found all 0 
>>> destinations for categories.articatech.net:443
>>
>>> FwdState.cc(209) stopAndDestroy: for path selection found no paths
>>
>>> FwdState.cc(492) fail: ERR_READ_ERROR "Bad Gateway"
>>
>>
>> There were no destinations because Squid cache had a no-IPs DNS entry 
>> for the above domain:
>>
>>> ipcache.cc(647) ipcache_nbgethostbyname_: ... HIT for 
>>> 'categories.articatech.net'
>>
>>> ipcache.cc(231) finalCallback: ... lookup_err=No DNS records
>>
>>
>>
>> AFAICT, the shared logs start _after_ the corresponding DNS cache 
>> entry was created (or lost its IPs), so I cannot tell what actually 
>> happened back then.
>>
>> Later, the expired(?) no-IPs DNS cache entry is purged, and Squid is 
>> able to resolve the same host name:
>>
>>> ipcache.cc(532) addGood: categories.articatech.net #1 217.182.193.199
>>
>>
>> And similar transactions start to succeed from this point.
>>
>>
>> If you can share more/earlier logs, I may be able to tell what caused 
>> the 'No DNS records' problem.
>>
>>
>> HTH,
>>
>> Alex.
>> P.S. Please compress debugging logs when sharing.
>>
>>
>>> Le 13/10/2022 ? 00:16, Alex Rousskov a ?crit?:
>>>> On 10/12/22 17:58, David Touzeau wrote:
>>>>
>>>>> We have put it in debug mode :
>>>>
>>>> Unfortunately, this log starts at GET request processing _after_ 
>>>> CONNECT request processing has already encountered and saved the 
>>>> error. Thus, I cannot tell you why that error was encountered. I 
>>>> need to see the lines before the lines you have shared. If you still 
>>>> have the log file, searching for AFC90000000000002632000003000000 
>>>> (above the already shared lines) might help you find where the error 
>>>> message was created.
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>>
>>>>
>>>>> you can see :
>>>>>
>>>>>
>>>>> "2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
>>>>> ERR_READ_ERROR/WITH_SERVER"
>>>>>
>>>>> As without ssl-bump, there is no issue.
>>>>>
>>>>> the full log can be downloaded here 
>>>>> http://articatech.net/tmpf/cache.log.txt*
>>>>> *
>>>>>
>>>>> }
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905084 from SBuf15905303
>>>>> 2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1357) 
>>>>> parseHttpRequest: HTTP Client conn214046 local=192.168.1.190:3128 
>>>>> remote=192.168.1.13:62858 FD 21 flags=1
>>>>> 2022/10/12 22:29:49.475 kid3| 11,2| client_side.cc(1361) 
>>>>> parseHttpRequest: HTTP Client REQUEST:
>>>>> ---------
>>>>> GET /fw.ping.php?_=1665576594826 HTTP/1.1
>>>>> Host: categories.articatech.net
>>>>> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
>>>>> Gecko/20100101 Firefox/105.0
>>>>> Accept: */*
>>>>> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
>>>>> Accept-Encoding: gzip, deflate, br
>>>>> Referer: https://categories.articatech.net/index
>>>>> X-Requested-With: XMLHttpRequest
>>>>> Connection: keep-alive
>>>>> Cookie: _ga=GA1.2.1172828951.1642984448; 
>>>>> PHPSESSID=28871210b5b031a7b034981e9704a4ac
>>>>> Sec-Fetch-Dest: empty
>>>>> Sec-Fetch-Mode: cors
>>>>> Sec-Fetch-Site: same-origin
>>>>>
>>>>>
>>>>> ----------
>>>>> 2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1393) 
>>>>> parseHttpRequest: complete request received. prefix_sz = 542, 
>>>>> request-line-size=43, mime-header-size=499, mime header block:
>>>>> Host: categories.articatech.net
>>>>> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
>>>>> Gecko/20100101 Firefox/105.0
>>>>> Accept: */*
>>>>> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
>>>>> Accept-Encoding: gzip, deflate, br
>>>>> Referer: https://categories.articatech.net/index
>>>>> X-Requested-With: XMLHttpRequest
>>>>> Connection: keep-alive
>>>>> Cookie: _ga=GA1.2.1172828951.1642984448; 
>>>>> PHPSESSID=28871210b5b031a7b034981e9704a4ac
>>>>> Sec-Fetch-Dest: empty
>>>>> Sec-Fetch-Mode: cors
>>>>> Sec-Fetch-Site: same-origin
>>>>>
>>>>>
>>>>> ----------
>>>>> 2022/10/12 22:29:49.475 kid3| 93,5| AsyncJob.cc(34) AsyncJob: 
>>>>> AsyncJob constructed, this=0x56260e3814b8 type=ClientHttpRequest 
>>>>> [job199106]
>>>>> 2022/10/12 22:29:49.475 kid3| 1,5| CodeContext.cc(60) Entering: ALE 
>>>>> w/o master
>>>>> 2022/10/12 22:29:49.475 kid3| 87,3| clientStream.cc(140) 
>>>>> clientStreamInsertHead: clientStreamInsertHead: Inserted node 
>>>>> 0x56260e6d26f8 with data 0x56260e0fa280 after head
>>>>> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1413) 
>>>>> parseHttpRequest: Prepare absolute URL from
>>>>> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(228) 
>>>>> getHostHeaderField: looking for Host
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>>>> first not of characterset non-LF in id SBuf15905340
>>>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>>>> consuming 32 bytes
>>>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>>>> consuming 1 bytes
>>>>> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(249) 
>>>>> getHostHeaderField: checking Host: categories.articatech.net
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>>>> first not of characterset WSP in id SBuf15905345
>>>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>>>> consuming 1 bytes
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905339 from SBuf15905345
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>>>> first not of characterset host in id SBuf15905339
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(771) findFirstNotOf: 
>>>>> not found
>>>>> 2022/10/12 22:29:49.475 kid3| 25,5| Parser.cc(267) 
>>>>> getHostHeaderField: returning categories.articatech.net
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905347 from SBuf183
>>>>> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1294) 
>>>>> prepareTlsSwitchingURL: TLS switching host rewrite: 
>>>>> https://categories.articatech.net/fw.ping.php?_=1665576594826
>>>>> 2022/10/12 22:29:49.475 kid3| 33,5| client_side.cc(1981) 
>>>>> clientParseRequests: conn214046 local=192.168.1.190:3128 
>>>>> remote=192.168.1.13:62858 FD 21 flags=1: done parsing a request
>>>>> 2022/10/12 22:29:49.475 kid3| 5,4| AsyncCall.cc(30) AsyncCall: The 
>>>>> AsyncCall ConnStateData::lifetimeTimeout constructed, 
>>>>> this=0x56260e5afd50 [call1972958]
>>>>> 2022/10/12 22:29:49.475 kid3| 5,3| comm.cc(571) commSetConnTimeout: 
>>>>> conn214046 local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 
>>>>> flags=1 timeout 86400
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: 
>>>>> cleaning hdr: 0x56260e392c18 owner: 3
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(194) clean: 
>>>>> cleaning hdr: 0x56260e392c18 owner: 3
>>>>> 2022/10/12 22:29:49.475 kid3| 33,3| client_side.cc(1806) add: 
>>>>> 0x56260e0f9210*3 to 0/1
>>>>> 2022/10/12 22:29:49.475 kid3| 33,3| Pipeline.cc(24) add: Pipeline 
>>>>> 0x56260e6cd470 add request 2 0x56260e0f9210*4
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905349
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 61 for SBuf15905349
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905349 new store capacity: 128
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
>>>>> init-ing hdr: 0x56260e4038f8 owner: 2
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(763) findFirstNotOf: 
>>>>> first not of characterset special in id SBuf15905363
>>>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>>>> consuming 5 bytes
>>>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>>>> consuming 1 bytes
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905367 from SBuf183
>>>>> 2022/10/12 22:29:49.475 kid3| 24,5| Tokenizer.cc(27) consume: 
>>>>> consuming 2 bytes
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 1 for SBuf15905371
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905371 not growing
>>>>> 2022/10/12 22:29:49.475 kid3| 23,3| Uri.cc(441) parse: Split URL 
>>>>> 'https://categories.articatech.net/fw.ping.php?_=1665576594826' 
>>>>> into proto='https', host='categories.articatech.net', port='443', 
>>>>> path='/fw.ping.php?_=1665576594826'
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905351 from SBuf15905361
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905353 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905353
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 28 for SBuf15905353
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905353 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 14,3| Address.cc(389) lookupHostIP: 
>>>>> Given Non-IP 'categories.articatech.net': Name or service not known
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905373
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 0 for SBuf15905373
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905373 not growing
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905352 from SBuf15905373
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905350 from SBuf15905305
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 1 for SBuf15905374
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905374 not growing
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(390) parse: 
>>>>> parsing hdr: (0x56260e4038f8)
>>>>> Host: categories.articatech.net
>>>>> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) 
>>>>> Gecko/20100101 Firefox/105.0
>>>>> Accept: */*
>>>>> Accept-Language: fr,fr-FR;q=0.8,en-US;q=0.5,en;q=0.3
>>>>> Accept-Encoding: gzip, deflate, br
>>>>> Referer: https://categories.articatech.net/index
>>>>> X-Requested-With: XMLHttpRequest
>>>>> Connection: keep-alive
>>>>> Cookie: _ga=GA1.2.1172828951.1642984448; 
>>>>> PHPSESSID=28871210b5b031a7b034981e9704a4ac
>>>>> Sec-Fetch-Dest: empty
>>>>> Sec-Fetch-Mode: cors
>>>>> Sec-Fetch-Site: same-origin
>>>>>
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905375 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905375
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 4 for SBuf15905375
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905375 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905376 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905376
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 4 for SBuf15905376
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905376 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Host[30] at 0
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905377 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905377
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 10 for SBuf15905377
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905377 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905378 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905378
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 10 for SBuf15905378
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905378 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: User-Agent[67] at 1
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905379 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905379
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 6 for SBuf15905379
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905379 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905380 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905380
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 6 for SBuf15905380
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905380 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Accept[0] at 2
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905381 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905381
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 15 for SBuf15905381
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905381 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905382 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905382
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 15 for SBuf15905382
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905382 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Accept-Language[3] at 3
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905383 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905383
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 15 for SBuf15905383
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905383 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905384 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905384
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 15 for SBuf15905384
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905384 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Accept-Encoding[2] at 4
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905385 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905385
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 7 for SBuf15905385
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905385 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905386 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905386
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 7 for SBuf15905386
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905386 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Referer[54] at 5
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905387
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 16 for SBuf15905387
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905387 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905388 from SBuf15905387
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Other:[87] at 6
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905389 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905389
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 10 for SBuf15905389
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905389 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905390 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905390
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 10 for SBuf15905390
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905390 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Connection[12] at 7
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905391 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905391
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 6 for SBuf15905391
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905391 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905392 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905392
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 6 for SBuf15905392
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905392 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Cookie[22] at 8
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905393
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 14 for SBuf15905393
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905393 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905394 from SBuf15905393
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Other:[87] at 9
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905395
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 14 for SBuf15905395
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905395 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905396 from SBuf15905395
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Other:[87] at 10
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905397
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 14 for SBuf15905397
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905397 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905398 from SBuf15905397
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Other:[87] at 11
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905355
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 25 for SBuf15905355
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905355 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905354 from SBuf15905355
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 6 for SBuf15905355
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905355 not growing
>>>>> 2022/10/12 22:29:49.475 kid3| 33,5| Http1Server.cc(193) 
>>>>> buildHttpRequest: normalize 1 Host header using 
>>>>> categories.articatech.net
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 1 for SBuf15905399
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905399 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905401 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905401
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 4 for SBuf15905401
>>>>> 2022/10/12 22:29:49.475 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905401 new store capacity: 40
>>>>> 2022/10/12 22:29:49.475 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e4038f8 adding entry: Host[30] at 12
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905356 new store capacity: 16384
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 5 for SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905356 not growing
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 1 for SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905356 not growing
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 2 for SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905356 not growing
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 25 for SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905356 not growing
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 28 for SBuf15905356
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905356 not growing
>>>>> 2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(699) 
>>>>> clientSetKeepaliveFlag: http_ver = HTTP/1.1
>>>>> 2022/10/12 22:29:49.476 kid3| 33,3| client_side.cc(700) 
>>>>> clientSetKeepaliveFlag: method = GET
>>>>> 2022/10/12 22:29:49.476 kid3| 55,6| HttpHeader.cc(850) getList: 
>>>>> 0x56260e4038f8: joined for id Connection[12]: 0x7ffc7248c400
>>>>> 2022/10/12 22:29:49.476 kid3| 33,4| client_side.cc(1510) 
>>>>> quitAfterError: Will close after error: conn214046 
>>>>> local=192.168.1.190:3128 remote=192.168.1.13:62858 FD 21 flags=1
>>>>> 2022/10/12 22:29:49.476 kid3| 33,5| client_side.cc(1531) 
>>>>> serveDelayedError: Responding with delated error for 
>>>>> https://categories.articatech.net/fw.ping.php?_=1665576594826
>>>>> 2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
>>>>> clientReplyContext::setReplyToStoreEntry locked key 
>>>>> AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>>>> 0x7ffc7248c000 checking fast rules
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>>>> aclCheckFast: list: 0x56260c88d7a8
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'ALLOWED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> Group2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>>>> aclMatchDomainList: checking 'categories.articatech.net'
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> Group2 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#1 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'DENIED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>>>> '192.168.1.13:62858' found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> all = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#2 = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>>>> 0x7ffc7248c000 answer DENIED for match
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) 
>>>>> ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>>>> 0x7ffc7248c000 checking fast rules
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>>>> aclCheckFast: list: 0x56260c8c46a8
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'ALLOWED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> Group2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>>>> aclMatchDomainList: checking 'categories.articatech.net'
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> Group2 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#1 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'DENIED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>>>> '192.168.1.13:62858' found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> all = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#2 = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>>>> 0x7ffc7248c000 answer DENIED for match
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248c000
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) 
>>>>> ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248c000
>>>>> *2022/10/12 22:29:49.476 kid3| 4,3| Error.cc(22) update: recent: 
>>>>> ERR_READ_ERROR/WITH_SERVER*
>>>>> 2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(117) pullData: 0 
>>>>> written 0 into conn214046 local=192.168.1.190:3128 
>>>>> remote=192.168.1.13:62858 FD 21 flags=1
>>>>> 2022/10/12 22:29:49.476 kid3| 33,5| Stream.cc(141) 
>>>>> getNextRangeOffset: range: 0; http offset 0; reply 0
>>>>> 2022/10/12 22:29:49.476 kid3| 87,3| clientStream.cc(180) 
>>>>> clientStreamRead: clientStreamRead: Calling 1 with cbdata 
>>>>> 0x56260e0d2ba8 from node 0x56260e6d26f8
>>>>> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(246) copy: 
>>>>> store_client::copy: AFC90000000000002632000003000000, from 0, for 
>>>>> length 4096, cb 1, cbdata 0x56260e0d1ad8
>>>>> 2022/10/12 22:29:49.476 kid3| 20,3| store.cc(443) lock: 
>>>>> store_client::copy locked key AFC90000000000002632000003000000 
>>>>> e:=sp2XIV/0x56260e1b8290*3
>>>>> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(338) 
>>>>> storeClientCopy2: storeClientCopy2: AFC90000000000002632000003000000
>>>>> 2022/10/12 22:29:49.476 kid3| 33,5| store_client.cc(368) doCopy: 
>>>>> store_client::doCopy: co: 0, hi: 506665
>>>>> 2022/10/12 22:29:49.476 kid3| 90,3| store_client.cc(472) 
>>>>> scheduleMemRead: store_client::doCopy: Copying normal from memory
>>>>> 2022/10/12 22:29:49.476 kid3| 19,6| stmem.cc(230) copy: memCopy: 
>>>>> 0x56260db339f8 [0,4096)
>>>>> 2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2213) 
>>>>> sendMoreData: conn214046 local=192.168.1.190:3128 
>>>>> remote=192.168.1.13:62858 FD 21 flags=1 
>>>>> 'categories.articatech.net:443' out.offset=0
>>>>> 2022/10/12 22:29:49.476 kid3| 88,5| client_side_reply.cc(2241) 
>>>>> sendMoreData: clientReplyContext::sendMoreData: 
>>>>> https://categories.articatech.net/fw.ping.php?_=1665576594826, 4096 
>>>>> bytes (4096 new bytes)
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(155) HttpHeader: 
>>>>> init-ing hdr: 0x56260e392c18 owner: 3
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(243) append: 
>>>>> appending hdr: 0x56260e392c18 += 0x56260db7a728
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905409 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905409
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 6 for SBuf15905409
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905409 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Server[57] at 0
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905410 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905410
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 12 for SBuf15905410
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905410 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Mime-Version[43] at 1
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905411 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905411
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 4 for SBuf15905411
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905411 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Date[24] at 2
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905412 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905412
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 12 for SBuf15905412
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905412 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Content-Type[21] at 3
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905413 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905413
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 14 for SBuf15905413
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905413 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Content-Length[17] at 4
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905414 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905414
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 13 for SBuf15905414
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905414 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: X-Squid-Error[76] at 5
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
>>>>> httpHeaderParseOffset: offset 506470 parsed as 506470
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(156) 
>>>>> httpHeaderParseOffset: offset 506470 parsed as 506470
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905416 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905416
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 7 for SBuf15905416
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905416 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: X-Cache[72] at 6
>>>>> 2022/10/12 22:29:49.476 kid3| 33,2| client_side_reply.cc(1585) 
>>>>> buildReplyHeader: clientBuildReplyHeader: Connection Keep-Alive not 
>>>>> requested by admin or client
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 16 for SBuf15905417
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905417 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 96 for SBuf15905417
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| MemBlob.cc(130) syncSize: 0 was: 0
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905417 new store capacity: 128
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(85) assign: assigning 
>>>>> SBuf15905418 from SBuf15905417
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 1 for SBuf15905418
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(167) rawSpace: 
>>>>> SBuf15905418 not growing
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905420 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905420
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 3 for SBuf15905420
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905420 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Via[69] at 7
>>>>> 2022/10/12 22:29:49.476 kid3| 24,6| SBuf.cc(99) assign: 
>>>>> SBuf15905422 from c-string, n=4294967295)
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(209) append: from 
>>>>> c-string to id SBuf15905422
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(160) rawSpace: 
>>>>> reserving 10 for SBuf15905422
>>>>> 2022/10/12 22:29:49.476 kid3| 24,7| SBuf.cc(866) reAlloc: 
>>>>> SBuf15905422 new store capacity: 40
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(786) addEntry: 
>>>>> 0x56260e392c18 adding entry: Connection[12] at 8
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>>>> 0x7ffc7248bb70 checking fast rules
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>>>> aclCheckFast: list: 0x56260c8adf68
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> http_header_access Server
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'DENIED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> http_header_access Server#1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>>>> '192.168.1.13:62858' found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> all = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> http_header_access Server#1 = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> http_header_access Server = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>>>> 0x7ffc7248bb70 answer DENIED for match
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(308) 
>>>>> httpHdrMangle: checklist denied, we have no replacement. Pass
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bb70
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) 
>>>>> ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bb70
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 66,7| HttpHeaderTools.cc(286) 
>>>>> httpHdrMangle: couldn't find mangler or access list. Allowing
>>>>> 2022/10/12 22:29:49.476 kid3| 55,7| HttpHeader.cc(769) refreshMask: 
>>>>> refreshing the mask in hdr 0x56260e392c18
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>>>> 0x7ffc7248bcb0 checking fast rules
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>>>> aclCheckFast: list: 0x56260c88d7a8
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'ALLOWED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> Group2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>>>> aclMatchDomainList: checking 'categories.articatech.net'
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> Group2 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#1 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'DENIED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>>>> '192.168.1.13:62858' found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> all = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#2 = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>>>> 0x7ffc7248bcb0 answer DENIED for match
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| Checklist.cc(197) 
>>>>> ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(70) preCheck: 
>>>>> 0x7ffc7248bcb0 checking fast rules
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(343) fastCheck: 
>>>>> aclCheckFast: list: 0x56260c8c46a8
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'ALLOWED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> Group2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(110) match: 
>>>>> aclMatchDomainList: checking 'categories.articatech.net'
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| DomainData.cc(115) match: 
>>>>> aclMatchDomainList: 'categories.articatech.net' NOT found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> Group2 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#1 = 0
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Checklist.cc(398) bannedAction: 
>>>>> Action 'DENIED/0' is not banned
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> delay_access#2
>>>>> 2022/10/12 22:29:49.476 kid3| 28,5| Acl.cc(124) matches: checking all
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Ip.cc(538) match: aclIpMatchIp: 
>>>>> '192.168.1.13:62858' found
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> all = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access#2 = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> delay_access = 1
>>>>> 2022/10/12 22:29:49.476 kid3| 28,3| Checklist.cc(63) markFinished: 
>>>>> 0x7ffc7248bcb0 answer DENIED for match
>>>>> 2022/10/12 22:29:49.476 kid3| 28,4| FilledChecklist.cc(67) 
>>>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bcb0
>>>>> 2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) 
>>>>> ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bcb0
>>>>> 2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(70) preCheck: 
>>>>> 0x7ffc7248bc10 checking fast ACLs
>>>>> 2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> reply_body_max_size -1
>>>>> 2022/10/12 22:29:49.477 kid3| 28,5| Acl.cc(124) matches: checking 
>>>>> (reply_body_max_size -1 line)
>>>>> 2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> (reply_body_max_size -1 line) = 1
>>>>> 2022/10/12 22:29:49.477 kid3| 28,3| Acl.cc(151) matches: checked: 
>>>>> reply_body_max_size -1 = 1
>>>>> 2022/10/12 22:29:49.477 kid3| 28,3| Checklist.cc(63) markFinished: 
>>>>> 0x7ffc7248bc10 answer ALLOWED for match
>>>>> 2022/10/12 22:29:49.477 kid3| 58,4| HttpReply.cc(564) 
>>>>> calcMaxBodySize: bodySizeMax=-1
>>>>> 2022/10/12 22:29:49.477 kid3| 28,4| FilledChecklist.cc(67) 
>>>>> ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffc7248bc10
>>>>> 2022/10/12 22:29:49.477 kid3| 28,4| Checklist.cc(197) 
>>>>> ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x7ffc7248bc10
>>>>> 2022/10/12 22:29:49.477 kid3| 58,7| HttpReply.cc(528) 
>>>>> expectedBodyTooLarge: bodySizeMax=-1
>>>>> 2022/10/12 22:29:49.477 kid3| 88,2| client_side_reply.cc(2090) 
>>>>> processReplyAccessResult: The reply for GET 
>>>>> https://categories.articatech.net/fw.ping.php?_=1665576594826 is 
>>>>> ALLOWED, because it matched (reply_body_max_size -1 line)
>>>>> 2022/10/12 22:29:49.477 kid3| 20,3| store.cc(443) lock: 
>>>>> ClientHttpRequest::loggingEntry locked key 
>>>>> AFC90000000000002632000003000000 e:=sp2XIV/0x56260e1b8290*4
>>>>> 2022/10/12 22:29:49.477 kid3| 88,3| client_side_reply.cc(2128) 
>>>>> processReplyAccessResult: clientReplyContext::sendMoreData: 
>>>>> Appending 3901 bytes after 195 bytes of headers
>>>>> 2022/10/12 22:29:49.477 kid3| 87,3| clientStream.cc(158) 
>>>>> clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 
>>>>> 0x56260e0fa280 from node 0x56260e5c0ab8
>>>>> 2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
>>>>> 0x56260e6cd470 front 0x56260e0f9210*4
>>>>> 2022/10/12 22:29:49.477 kid3| 33,3| Pipeline.cc(35) front: Pipeline 
>>>>> 0x56260e6cd470 front 0x56260e0f9210*4
>>>>> 2022/10/12 22:29:49.477 kid3| 55,7| HttpHeader.cc(589) packInto: 
>>>>> 0x56260e392c18 into 0x56260e26ebe8
>>>>> 2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(279) 
>>>>> sendStartOfMessage: HTTP Client conn214046 local=192.168.1.190:3128 
>>>>> remote=192.168.1.13:62858 FD 21 flags=1
>>>>> 2022/10/12 22:29:49.477 kid3| 11,2| Stream.cc(280) 
>>>>> sendStartOfMessage: HTTP Client REPLY:
>>>>> ---------
>>>>> HTTP/1.1 502 Bad Gateway
>>>>> Mime-Version: 1.0
>>>>> Date: Wed, 12 Oct 2022 20:29:49 GMT
>>>>> Content-Type: text/html;charset=utf-8
>>>>> Content-Length: 506470
>>>>> X-Squid-Error: ERR_READ_ERROR 0
>>>>> X-Cache: MISS from proxy-190.articatech.int
>>>>> Via: 1.1 789aaa51-a1eb-eb48-639b-000070877aed (squid)
>>>>> Connection: close
>>>>>
>>>>>
>>>>> Le 12/10/2022 ? 20:00, Alex Rousskov a ?crit?:
>>>>>> On 10/12/22 12:45, David Touzeau wrote:
>>>>>>> Hi
>>>>>>>
>>>>>>> We using squid 5.7 after adding ssl-bump we have sometimes 
>>>>>>> several 502 error? with extended error ERR_READ_ERROR|WITH_SERVER
>>>>>>>
>>>>>>> 1665589818.831???? 11 192.168.1.13 NONE_NONE/502 192616 OPTIONS 
>>>>>>> https://www2.deepl.com/jsonrpc?method=LMT_split_text - 
>>>>>>> HIER_NONE/-:- text/html mac="68:54:5a:94:e7:56" - 
>>>>>>> exterr="ERR_READ_ERROR|WITH_SERVER"
>>>>>>> 1665589839.288???? 11 192.168.1.13 NONE_NONE/502 506759 POST 
>>>>>>> https://pollserver.lastpass.com/poll_server.php - HIER_NONE/-:- 
>>>>>>> text/html mac="68:54:5a:94:e7:56" - 
>>>>>>> exterr="ERR_READ_ERROR|WITH_SERVER"
>>>>>>> 1665589719.879???? 44 192.168.1.13 NONE_NONE/502 506954 GET 
>>>>>>> https://contile.services.mozilla.com/v1/tiles - HIER_NONE/-:- 
>>>>>>> text/html mac="68:54:5a:94:e7:56" - 
>>>>>>> exterr="ERR_READ_ERROR|WITH_SERVER"
>>>>>>
>>>>>>> What does it means.
>>>>>>
>>>>>> 502 with ERR_READ_ERROR|WITH_SERVER may mean several things 
>>>>>> (unfortunately). Given HIER_NONE, I would suspect that Squid could 
>>>>>> not find a valid destination for the request. There is a similar 
>>>>>> recent squid-users thread at 
>>>>>> http://lists.squid-cache.org/pipermail/squid-users/2022-October/025289.html 
>>>>>>
>>>>>>
>>>>>>
>>>>>>> how can we fix it ?
>>>>>>
>>>>>> The first step is to identify what causes these errors.
>>>>>>
>>>>>> Can you reproduce this problem at will? Perhaps by trying going to 
>>>>>> https://dnslabeldoesnotexist.com mentioned at the above thread? If 
>>>>>> you can, consider sharing (a pointer to) a compressed debugging 
>>>>>> cache.log from a test box that does not expose any internal 
>>>>>> secrets, as detailed at 
>>>>>> https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction 




From wadie.lemrazzeq at capgemini.com  Tue Oct 18 08:55:23 2022
From: wadie.lemrazzeq at capgemini.com (LEMRAZZEQ, Wadie)
Date: Tue, 18 Oct 2022 08:55:23 +0000
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
Message-ID: <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>

> On 10/14/22 10:32, LEMRAZZEQ, Wadie wrote:
>> I tried to implement this on a dockerized Alpine, and a squid 5.5 with 
>> openssl module

> FWIW, Squid v5.5 is unusable in many environments -- too many bugs. Use
> v5.7 or later. I do not know whether one of those bugs are responsible for the specific problem you are discussing though.

I tried with squid 5.7, but still have the same issue

>> but when I request squid https port, I got this error every time, in
>> cache.log:

> _How_ do you "request squid https port"?

Ah sorry didn't mentioned that I have problem only web browsers (Firefox, chromium), and I do specify to use https proxy in the browser proxy config
But if I use curl, it works

>> ERROR: failure while accepting a TLS connection on conn77
>> local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1: 
>> 
>> connection: conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 
>> flags=1
>> 
>> Error.cc(22) update: recent: 
>> ERR_SECURE_ACCEPT_FAIL/SQUID_TLS_ERR_ACCEPT+TLS_LIB_ERR=1408F09B+TLS_I
>> O_ERR=1

> According to "openssl errstr", that OpenSSL error is:

>      error:1408F09B:SSL routines:ssl3_get_record:https proxy request


> Most likely, the client is sending a plain text CONNECT request before encrypting the TLS connection to the HTTPS proxy. In other words, the client thinks it is talking to an HTTP proxy while > you want it to think that it is talking to an HTTPS proxy. For example,

> * HTTP proxy:  curl -x http://172.17.0.2:3128/ ... https://example.com
> * HTTPS proxy: curl -x https://172.17.0.2:3129/ ... https://example.com

Yes indeed, requesting with curl works unless the web browsers




> ...
> 
> I also tried this with squid 4.10 with gnutls module, in an Ubuntu 
> 20.40 environment, with the same squid.conf, and I got again a TLS 
> error
> 
> ...
> 
> client_side.cc(2597) tlsAttemptHandshake: Error negotiating TLS on
> local=x.x.x.x:3129 remote=x.x.x.x:50874 FD 11 flags=1: Aborted by
> client: An unexpected TLS packet was received.
> 
> ...
> 
> I used for certificates, a self signed one, and a generated 
> certificate signed by our CA, for both scenarios
> 
> Also, I tried multiple https_port options (disable some SSL 
> implementation, manipulation of client certificates...) but without 
> success

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

This message contains information that may be privileged or confidential and is the property of the Capgemini Group. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain, copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message.



From rousskov at measurement-factory.com  Tue Oct 18 13:52:48 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 18 Oct 2022 09:52:48 -0400
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>

On 10/18/22 04:55, LEMRAZZEQ, Wadie wrote:

> I have problem only web browsers (Firefox, chromium), and I do
> specify to use https proxy in the browser proxy config But if I use
> curl, it works


>>> ERROR: failure while accepting a TLS connection on conn77
>>> local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1:
>>>
>>> connection: conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12
>>> flags=1
>>>
>>> Error.cc(22) update: recent:
>>> ERR_SECURE_ACCEPT_FAIL/SQUID_TLS_ERR_ACCEPT+TLS_LIB_ERR=1408F09B+TLS_I
>>> O_ERR=1

>> According to "openssl errstr", that OpenSSL error is:
>>       error:1408F09B:SSL routines:ssl3_get_record:https proxy request


>> Most likely, the client is sending a plain text CONNECT request
>> before encrypting the TLS connection to the HTTPS proxy. In other
>> words, the client thinks it is talking to an HTTP proxy while > you
>> want it to think that it is talking to an HTTPS proxy. For
>> example,
>> 
>> * HTTP proxy:  curl -x http://172.17.0.2:3128/ ... https://example.com
>> * HTTPS proxy: curl -x https://172.17.0.2:3129/ ... https://example.com


> Yes indeed, requesting with curl works unless the web browsers

As far as I can tell based on the information you have provided, your 
browser is not doing what you want it to do. I can only speculate that 
the browser is misconfigured.

You can confirm what the browser is doing by looking at browser-Squid 
packets using wireshark or a similar tool. If you see an HTTP CONNECT 
requests sent to Squid over a plain text TCP connection, then your 
browser is _not_ configured to use an HTTPS proxy (or is buggy). The 
browser should be opening a TCP connection and then initiating a TLS 
handshake.


HTH,

Alex.


From felipeapolanco at gmail.com  Wed Oct 19 13:32:40 2022
From: felipeapolanco at gmail.com (Felipe Polanco)
Date: Wed, 19 Oct 2022 09:32:40 -0400
Subject: [squid-users] Squid bandwidth accountability
Message-ID: <CADcj3=6Sy6NEq+joKzPWGM3CR9OtPgrsf22Hw5E-udLJ0xJMhg@mail.gmail.com>

Hi,

Is there any way to see how much traffic a connected user has used while
connected to Squid proxy?

Like
john at 100.64.54.112 Since 10-19-2022 12:11:34 AM 806.55 MB
bob at 100.64.54.117 Since 10-19-2022 17:11:33 AM 407.55 MB

Where john and bob are authenticated usernames on Squid.

Thanks,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221019/ec67f259/attachment.htm>

From wadie.lemrazzeq at capgemini.com  Wed Oct 19 13:53:51 2022
From: wadie.lemrazzeq at capgemini.com (LEMRAZZEQ, Wadie)
Date: Wed, 19 Oct 2022 13:53:51 +0000
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
Message-ID: <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>

On 10/18/22 04:55, LEMRAZZEQ, Wadie wrote:

>>> I have problem only web browsers (Firefox, chromium), and I do specify 
>>> to use https proxy in the browser proxy config But if I use curl, it 
>>> works


>>>> ERROR: failure while accepting a TLS connection on conn77
>>>> local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1:
>>>>
>>>> connection: conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 
>>>> 12
>>>> flags=1
>>>>
>>>> Error.cc(22) update: recent:
>>>> ERR_SECURE_ACCEPT_FAIL/SQUID_TLS_ERR_ACCEPT+TLS_LIB_ERR=1408F09B+TLS
>>>> _I
>>>> O_ERR=1

>>> According to "openssl errstr", that OpenSSL error is:
>>>       error:1408F09B:SSL routines:ssl3_get_record:https proxy request


>>> Most likely, the client is sending a plain text CONNECT request 
>>> before encrypting the TLS connection to the HTTPS proxy. In other 
>>> words, the client thinks it is talking to an HTTP proxy while > you 
>>> want it to think that it is talking to an HTTPS proxy. For example,
>>> 
>>> * HTTP proxy:  curl -x http://172.17.0.2:3128/ ... 
>>> https://example.com
>>> * HTTPS proxy: curl -x https://172.17.0.2:3129/ ... 
>>> https://example.com


>> Yes indeed, requesting with curl works unless the web browsers

> As far as I can tell based on the information you have provided, your browser is not doing what you want it to do. I can only speculate that the browser is misconfigured.

> You can confirm what the browser is doing by looking at browser-Squid packets using wireshark or a similar tool. If you see an HTTP CONNECT requests sent to Squid over a plain text TCP 
> connection, then your browser is _not_ configured to use an HTTPS proxy (or is buggy). The browser should be opening a TCP connection and then initiating a TLS handshake.

Yes, that's what I did
Here is the capture of firefox: https://i.stack.imgur.com/NNnGx.png
And here the capture of curl: https://i.stack.imgur.com/OxJJ3.png
As you can see firefox sends a plain text CONNECT request, and I did parameter https proxy in firefox settings
If it is a browser bug, firefox team resolved this compatibility issue a while ago: https://bugzilla.mozilla.org/show_bug.cgi?id=378637#c68
But still the issue persists or I did miss something

Thank you
Regards,


This message contains information that may be privileged or confidential and is the property of the Capgemini Group. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain, copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message.

From gkinkie at gmail.com  Wed Oct 19 14:02:00 2022
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Wed, 19 Oct 2022 15:02:00 +0100
Subject: [squid-users] Squid bandwidth accountability
In-Reply-To: <CADcj3=6Sy6NEq+joKzPWGM3CR9OtPgrsf22Hw5E-udLJ0xJMhg@mail.gmail.com>
References: <CADcj3=6Sy6NEq+joKzPWGM3CR9OtPgrsf22Hw5E-udLJ0xJMhg@mail.gmail.com>
Message-ID: <CA+Y8hcOEHRRg5Oj-k75FyNBiVe8EUtK-s+fZ7SBJ2CQZFF8Heg@mail.gmail.com>

Hi Felipe,
  the best way to do that is through some kind of log analyzer software

On Wed, Oct 19, 2022 at 2:33 PM Felipe Polanco <felipeapolanco at gmail.com>
wrote:

> Hi,
>
> Is there any way to see how much traffic a connected user has used while
> connected to Squid proxy?
>
> Like
> john at 100.64.54.112 Since 10-19-2022 12:11:34 AM 806.55 MB
> bob at 100.64.54.117 Since 10-19-2022 17:11:33 AM 407.55 MB
>
> Where john and bob are authenticated usernames on Squid.
>
> Thanks,
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221019/331045cb/attachment.htm>

From felipeapolanco at gmail.com  Wed Oct 19 14:03:03 2022
From: felipeapolanco at gmail.com (Felipe Polanco)
Date: Wed, 19 Oct 2022 10:03:03 -0400
Subject: [squid-users] Squid bandwidth accountability
In-Reply-To: <CA+Y8hcOEHRRg5Oj-k75FyNBiVe8EUtK-s+fZ7SBJ2CQZFF8Heg@mail.gmail.com>
References: <CADcj3=6Sy6NEq+joKzPWGM3CR9OtPgrsf22Hw5E-udLJ0xJMhg@mail.gmail.com>
 <CA+Y8hcOEHRRg5Oj-k75FyNBiVe8EUtK-s+fZ7SBJ2CQZFF8Heg@mail.gmail.com>
Message-ID: <CADcj3=4c8awg2O0=xqJEBjwkL_h9_Pp4om-Q2SVKeCyqt3DXQQ@mail.gmail.com>

thanks Francesco,

Do you know which log file to monitor?

On Wed, Oct 19, 2022 at 10:02 AM Francesco Chemolli <gkinkie at gmail.com>
wrote:

> Hi Felipe,
>   the best way to do that is through some kind of log analyzer software
>
> On Wed, Oct 19, 2022 at 2:33 PM Felipe Polanco <felipeapolanco at gmail.com>
> wrote:
>
>> Hi,
>>
>> Is there any way to see how much traffic a connected user has used while
>> connected to Squid proxy?
>>
>> Like
>> john at 100.64.54.112 Since 10-19-2022 12:11:34 AM 806.55 MB
>> bob at 100.64.54.117 Since 10-19-2022 17:11:33 AM 407.55 MB
>>
>> Where john and bob are authenticated usernames on Squid.
>>
>> Thanks,
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> --
>     Francesco
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221019/b27acacf/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 19 14:33:18 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 19 Oct 2022 10:33:18 -0400
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>

On 10/19/22 09:53, LEMRAZZEQ, Wadie wrote:

> As you can see firefox sends a plain text CONNECT request, and I did
> parameter https proxy in firefox settings

I do not know exactly what you mean by "https proxy" in this context, 
but I suspect that you are using the wrong FireFox setting. The easily 
accessible "HTTPS proxy" setting in the "Configure Proxy Access to the 
Internet" dialog is _not_ what you need! That setting configures a plain 
text HTTP proxy for handling HTTPS traffic. Very misleading, I know.

You need a PAC file that tells FireFox to use an HTTPS proxy.

See (again) 
https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection 
which refers to https://bugzilla.mozilla.org/show_bug.cgi?id=378637#c68


HTH,

Alex.

> On 10/19/22 09:53, LEMRAZZEQ, Wadie wrote:
>> On 10/18/22 04:55, LEMRAZZEQ, Wadie wrote:
>> 
>>>>> I have problem only web browsers (Firefox, chromium), and I do specify
>>>>> to use https proxy in the browser proxy config But if I use curl, it
>>>>> works
>> 
>> 
>>>>>> ERROR: failure while accepting a TLS connection on conn77
>>>>>> local=172.17.0.2:3129 remote=172.17.0.1:56608 FD 12 flags=1:
>>>>>>
>>>>>> connection: conn77 local=172.17.0.2:3129 remote=172.17.0.1:56608 FD
>>>>>> 12
>>>>>> flags=1
>>>>>>
>>>>>> Error.cc(22) update: recent:
>>>>>> ERR_SECURE_ACCEPT_FAIL/SQUID_TLS_ERR_ACCEPT+TLS_LIB_ERR=1408F09B+TLS
>>>>>> _I
>>>>>> O_ERR=1
>> 
>>>>> According to "openssl errstr", that OpenSSL error is:
>>>>>        error:1408F09B:SSL routines:ssl3_get_record:https proxy request
>> 
>> 
>>>>> Most likely, the client is sending a plain text CONNECT request
>>>>> before encrypting the TLS connection to the HTTPS proxy. In other
>>>>> words, the client thinks it is talking to an HTTP proxy while > you
>>>>> want it to think that it is talking to an HTTPS proxy. For example,
>>>>>
>>>>> * HTTP proxy:  curl -x http://172.17.0.2:3128/ ...
>>>>> https://example.com
>>>>> * HTTPS proxy: curl -x https://172.17.0.2:3129/ ...
>>>>> https://example.com
>> 
>> 
>>>> Yes indeed, requesting with curl works unless the web browsers
>> 
>>> As far as I can tell based on the information you have provided, your browser is not doing what you want it to do. I can only speculate that the browser is misconfigured.
>> 
>>> You can confirm what the browser is doing by looking at browser-Squid packets using wireshark or a similar tool. If you see an HTTP CONNECT requests sent to Squid over a plain text TCP
>>> connection, then your browser is _not_ configured to use an HTTPS proxy (or is buggy). The browser should be opening a TCP connection and then initiating a TLS handshake.
>> 
>> Yes, that's what I did
>> Here is the capture of firefox: https://i.stack.imgur.com/NNnGx.png
>> And here the capture of curl: https://i.stack.imgur.com/OxJJ3.png
>> As you can see firefox sends a plain text CONNECT request, and I did parameter https proxy in firefox settings
>> If it is a browser bug, firefox team resolved this compatibility issue a while ago: https://bugzilla.mozilla.org/show_bug.cgi?id=378637#c68
>> But still the issue persists or I did miss something
>> 
>> Thank you
>> Regards,
>> 
>> 
>> This message contains information that may be privileged or confidential and is the property of the Capgemini Group. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain, copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message.


From gkinkie at gmail.com  Wed Oct 19 15:23:00 2022
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Wed, 19 Oct 2022 16:23:00 +0100
Subject: [squid-users] Squid bandwidth accountability
In-Reply-To: <CADcj3=4c8awg2O0=xqJEBjwkL_h9_Pp4om-Q2SVKeCyqt3DXQQ@mail.gmail.com>
References: <CADcj3=6Sy6NEq+joKzPWGM3CR9OtPgrsf22Hw5E-udLJ0xJMhg@mail.gmail.com>
 <CA+Y8hcOEHRRg5Oj-k75FyNBiVe8EUtK-s+fZ7SBJ2CQZFF8Heg@mail.gmail.com>
 <CADcj3=4c8awg2O0=xqJEBjwkL_h9_Pp4om-Q2SVKeCyqt3DXQQ@mail.gmail.com>
Message-ID: <CA+Y8hcM0OPL3Ph2QepMOyCx3dzSJh42S=HTokZY_=8P8VNeN3w@mail.gmail.com>

Hi Felipe,
  in the defauilt configuration, access.log contains all the information
about who accessed Squid, from what IP, and how much data they downloaded.

On Wed, Oct 19, 2022 at 3:03 PM Felipe Polanco <felipeapolanco at gmail.com>
wrote:

> thanks Francesco,
>
> Do you know which log file to monitor?
>
> On Wed, Oct 19, 2022 at 10:02 AM Francesco Chemolli <gkinkie at gmail.com>
> wrote:
>
>> Hi Felipe,
>>   the best way to do that is through some kind of log analyzer software
>>
>> On Wed, Oct 19, 2022 at 2:33 PM Felipe Polanco <felipeapolanco at gmail.com>
>> wrote:
>>
>>> Hi,
>>>
>>> Is there any way to see how much traffic a connected user has used while
>>> connected to Squid proxy?
>>>
>>> Like
>>> john at 100.64.54.112 Since 10-19-2022 12:11:34 AM 806.55 MB
>>> bob at 100.64.54.117 Since 10-19-2022 17:11:33 AM 407.55 MB
>>>
>>> Where john and bob are authenticated usernames on Squid.
>>>
>>> Thanks,
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>
>> --
>>     Francesco
>>
>

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221019/c67f7baa/attachment.htm>

From gtaylor at tnetconsulting.net  Thu Oct 20 00:38:35 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Wed, 19 Oct 2022 18:38:35 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
Message-ID: <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>

On 10/19/22 8:33 AM, Alex Rousskov wrote:
> I do not know exactly what you mean by "https proxy" in this context, 
> but I suspect that you are using the wrong FireFox setting. The easily 
> accessible "HTTPS proxy" setting in the "Configure Proxy Access to the 
> Internet" dialog is _not_ what you need! That setting configures a plain 
> text HTTP proxy for handling HTTPS traffic. Very misleading, I know.

+10 to the antiquated UI ~> worse UX.

> You need a PAC file that tells FireFox to use an HTTPS proxy.

I believe you can use the FoxyProxy add-on to manage this too.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221019/cc223d29/attachment.bin>

From rafael.akchurin at diladele.com  Thu Oct 20 05:33:47 2022
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 20 Oct 2022 05:33:47 +0000
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
Message-ID: <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>

The following line set in the Script Address box of the browser proxy configuration will help - no need for a PAC file for quick tests. Be sure to adjust the proxy name and port.

data:,function FindProxyForURL(u, h){return "HTTPS proxy.example.lan:8443";}

More info at https://webproxy.diladele.com/docs/network/secure_proxy/browsers/

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Grant Taylor
Sent: Thursday, October 20, 2022 2:39 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] FW: Encrypted browser-Squid connection errors

On 10/19/22 8:33 AM, Alex Rousskov wrote:
> I do not know exactly what you mean by "https proxy" in this context, 
> but I suspect that you are using the wrong FireFox setting. The easily 
> accessible "HTTPS proxy" setting in the "Configure Proxy Access to the 
> Internet" dialog is _not_ what you need! That setting configures a 
> plain text HTTP proxy for handling HTTPS traffic. Very misleading, I know.

+10 to the antiquated UI ~> worse UX.

> You need a PAC file that tells FireFox to use an HTTPS proxy.

I believe you can use the FoxyProxy add-on to manage this too.



--
Grant. . . .
unix || die


From gtaylor at tnetconsulting.net  Thu Oct 20 15:14:21 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Thu, 20 Oct 2022 09:14:21 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
Message-ID: <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>

On 10/19/22 11:33 PM, Rafael Akchurin wrote:
> The following line set in the Script Address box of the browser proxy 
> configuration will help - no need for a PAC file for quick tests. Be 
> sure to adjust the proxy name and port.
> 
> data:,function FindProxyForURL(u, h){return "HTTPS proxy.example.lan:8443";}

Is it just me, or is it slightly disturbing that JavaScript in a 
configurations property box is being executed?

I guess I had naively assumed that something else, ideally hardened 
against malicious content, somewhere else is executing the JavaScript 
retrieved from the PAC file.  --  I feel like there should be a 
separation of responsibilities.

> More info at https://webproxy.diladele.com/docs/network/secure_proxy/browsers/

Aside:  Why the propensity of running the HTTP, HTTPS, FTP, and SOCKS 
proxies on non-standard ports?  Why not run them on their standard 
ports; 80, 443, 21, and 1080 respectively?

I switched to using standard ports years ago to simplify configuring 
HTTP proxy support in Ubuntu installers; "http://proxy.example.net/", no 
need to fiddle with the port.  Or if you have DNS search domains 
configured, "http://proxy/" is sufficient.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221020/ef39c353/attachment.bin>

From uhlar at fantomas.sk  Thu Oct 20 15:49:37 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 20 Oct 2022 17:49:37 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
Message-ID: <Y1FuER/x8qizD34c@fantomas.sk>

>On 10/19/22 11:33 PM, Rafael Akchurin wrote:
>>The following line set in the Script Address box of the browser 
>>proxy configuration will help - no need for a PAC file for quick 
>>tests. Be sure to adjust the proxy name and port.
>>
>>data:,function FindProxyForURL(u, h){return "HTTPS proxy.example.lan:8443";}

On 20.10.22 09:14, Grant Taylor wrote:
>Is it just me, or is it slightly disturbing that JavaScript in a 
>configurations property box is being executed?

proxy autoconfig is javascript-based but uses very limited javascript.

while I agree javascript is not ideal, it's very hard to configure proper  
proxy configuration without using scripting language.

and, when we need scripting language, it's much easier to use something that 
has been implemented and is used in browsers.

>>More info at https://webproxy.diladele.com/docs/network/secure_proxy/browsers/
>
>Aside:  Why the propensity of running the HTTP, HTTPS, FTP, and SOCKS 
>proxies on non-standard ports?  Why not run them on their standard 
>ports; 80, 443, 21, and 1080 respectively?

because standard servers and not proxies usually run on standard ports.
Also, FTP protocol (port 21) does not support proxying, and using FTP proxy 
usually involves hacks.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I'm not interested in your website anymore.
If you need cookies, bake them yourself.


From gtaylor at tnetconsulting.net  Thu Oct 20 16:14:48 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Thu, 20 Oct 2022 10:14:48 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1FuER/x8qizD34c@fantomas.sk>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
Message-ID: <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>

On 10/20/22 9:49 AM, Matus UHLAR - fantomas wrote:
> proxy autoconfig is javascript-based but uses very limited javascript.

My comment was more directed at why is $LANGUAGE_DOESNT_MATTER used /in/ 
/the/ /location/ /field/?

> while I agree javascript is not ideal, it's very hard to configure 
> proper proxy configuration without using scripting language.
> 
> and, when we need scripting language, it's much easier to use something 
> that has been implemented and is used in browsers.

I understand and agree with (re)using JavaScript as the chosen language. 
  That's not my concern.  (See above.)

> because standard servers and not proxies usually run on standard ports.

I trust that you don't intend it to be, but that feels like a non-answer 
to me.

That's sort of tantamount to saying "I drive on the shoulder because 
there are cards on the road."

HTTP(S) connections /are/ the HTTP protocol and the standard port for 
HTTP protocol is port 80 for unencrypted connections and port 443 for 
encrypted connections.

I rarely see a web server and a proxy server (as in different service 
daemons) run /on/ /the/ /same/ /system/.  As such there is no conflict 
between ports on different systems / IPs.

The rare case where I do see a web server and a proxy server (still 
different service daemons) frequently are using different IPs.  The 
proxy is usually listening on a globally routed IP while the web server 
is listening on the loopback IP.

Then there is the entire different class where the same daemon functions 
as the web server and the proxy server.  Apache's HTTPD and Nginx 
immediately come to mind as fulfilling both functions.

So ... I feel like "de-conflicting ports" is as true as "having to have 
different IPs for different TLS certificates".

> Also, FTP protocol (port 21) does not support proxying, and using FTP 
> proxy usually involves hacks.

I completely disagree.

I've been using FTP through proxies for years.  Firefox (and 
Thunderbird) has an option /specifically/ for using FTP through proxies. 
  As depicted in the the picture of Firefox on the page that Rafael A. 
linked to.

All mainstream web browsers have had support for proxying FTP traffic 
for (at least) 15 of the last 25 years.  Up to the point that they 
started removing FTP protocol support from the browser.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221020/87bc747b/attachment.bin>

From amajer at suse.de  Fri Oct 21 05:58:15 2022
From: amajer at suse.de (Adam Majer)
Date: Fri, 21 Oct 2022 07:58:15 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
Message-ID: <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>

On 10/20/22 18:14, Grant Taylor wrote:
> On 10/20/22 9:49 AM, Matus UHLAR - fantomas wrote:
>> because standard servers and not proxies usually run on standard ports.
> 
> I trust that you don't intend it to be, but that feels like a non-answer 
> to me.

It's basically by convention now. Port 3128 has been set as default port 
by Squid for more than 2 decades. Don't expect a change.

Secondly, like it was said already, servers and proxies are different 
things. And you need to understand the difference between forward and 
reverse proxies. Reverse proxies can sit on the regular ports because 
that's their job -- to ask as origins. Forward proxies don't sit on 
regular server ports because they require explicit config on the client. 
And don't forget we used to have transparent proxies which kind of died 
  (I think?) thanks to TLS.

Port 3128 is for *forward* proxy setup.

Cheers,
- Adam


From uhlar at fantomas.sk  Fri Oct 21 08:25:46 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 21 Oct 2022 10:25:46 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
References: <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
Message-ID: <Y1JXiv1bzO6n2s0h@fantomas.sk>

>On 10/20/22 9:49 AM, Matus UHLAR - fantomas wrote:
>>proxy autoconfig is javascript-based but uses very limited javascript.

On 20.10.22 10:14, Grant Taylor wrote:
>My comment was more directed at why is $LANGUAGE_DOESNT_MATTER used 
>/in/ /the/ /location/ /field/?

apparently this is a hack to be able to define proxy autoconfig in the 
location field.

Since it has very restricted capabilities, it's apparently non-issue.

I guess that you can only define FindProxyForURL() this way.

>>because standard servers and not proxies usually run on standard ports.
>
>I trust that you don't intend it to be, but that feels like a 
>non-answer to me.
>
>That's sort of tantamount to saying "I drive on the shoulder because 
>there are cards on the road."
>
>HTTP(S) connections /are/ the HTTP protocol and the standard port for 
>HTTP protocol is port 80 for unencrypted connections and port 443 for 
>encrypted connections.
>
>I rarely see a web server and a proxy server (as in different service 
>daemons) run /on/ /the/ /same/ /system/.  As such there is no conflict 

I know of such servers.
And, HTTP proxy does not even have defined own port so people use random 
ports or ports commonly used for this service.

>Then there is the entire different class where the same daemon 
>functions as the web server and the proxy server.  Apache's HTTPD and 
>Nginx immediately come to mind as fulfilling both functions.

>So ... I feel like "de-conflicting ports" is as true as "having to 
>have different IPs for different TLS certificates".

the beautiful nature of HTTP allows us to define port within URL, and 
therefore people tend so use separate ports instead of allocating extra IP 
addresses for proxy usage.

I think Adam Meyer also explained it nicely.


>>Also, FTP protocol (port 21) does not support proxying, and using 
>>FTP proxy usually involves hacks.
>
>I completely disagree.
>
>I've been using FTP through proxies for years.  Firefox (and 
>Thunderbird) has an option /specifically/ for using FTP through 
>proxies.  As depicted in the the picture of Firefox on the page that 
>Rafael A. linked to.

That is FTP through HTTP proxy. Not FTP through FTP proxy.
I repeat, FTP protocol does not support proxies and port 21 would be of low 
usage here.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I drive way too fast to worry about cholesterol.


From uhlar at fantomas.sk  Fri Oct 21 08:51:13 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 21 Oct 2022 10:51:13 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1JXiv1bzO6n2s0h@fantomas.sk>
References: <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <Y1JXiv1bzO6n2s0h@fantomas.sk>
Message-ID: <Y1JdgXAHrt0ZMpiL@fantomas.sk>

>>On 10/20/22 9:49 AM, Matus UHLAR - fantomas wrote:
>>>Also, FTP protocol (port 21) does not support proxying, and using 
>>>FTP proxy usually involves hacks.

>On 20.10.22 10:14, Grant Taylor wrote:
>>I completely disagree.
>>
>>I've been using FTP through proxies for years.  Firefox (and 
>>Thunderbird) has an option /specifically/ for using FTP through 
>>proxies.  As depicted in the the picture of Firefox on the page that 
>>Rafael A. linked to.

On 21.10.22 10:25, Matus UHLAR - fantomas wrote:
>That is FTP through HTTP proxy. Not FTP through FTP proxy.
>I repeat, FTP protocol does not support proxies and port 21 would be 
>of low usage here.

I should have added, that squid does support FTP proxying using one of hacks 
I mentioned (I haven't tested it yet).

And, since this requires other (FTP) protocol than the default (HTTP) at the 
proxy side, people free to configure it on random port they choose.

FTP proxying is so rarely used that it doesn't even have common port besides 
21 used for FTP.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
On the other hand, you have different fingers.


From andy.armstrong at uk.ibm.com  Fri Oct 21 12:50:08 2022
From: andy.armstrong at uk.ibm.com (Andy Armstrong)
Date: Fri, 21 Oct 2022 12:50:08 +0000
Subject: [squid-users] TCP_MISS only
In-Reply-To: <SA0PR15MB37577549B07B7CF9F5295413B2579@SA0PR15MB3757.namprd15.prod.outlook.com>
References: <SA0PR15MB375725E2BCE8978659B9BE0CB2529@SA0PR15MB3757.namprd15.prod.outlook.com>
 <da33fd71-420c-cc4a-47e9-e057626d4e4f@measurement-factory.com>
 <SA0PR15MB3757DD79FA92D3C545F26938B2559@SA0PR15MB3757.namprd15.prod.outlook.com>
 <4395563c-d41b-c3ca-97a6-a5588a51bfc9@treenet.co.nz>
 <SA0PR15MB3757FF0BD73163E637A119F8B2559@SA0PR15MB3757.namprd15.prod.outlook.com>
 <5d36de4a-715d-4910-c235-2c7d777867b9@treenet.co.nz>
 <SA0PR15MB37577549B07B7CF9F5295413B2579@SA0PR15MB3757.namprd15.prod.outlook.com>
Message-ID: <SA0PR15MB37572B259E38021ACF9BFAA0B22D9@SA0PR15MB3757.namprd15.prod.outlook.com>

Hi Amos, All,

I continue to struggle with only receiving TCP_MISS when using squid-cache. I have now implemented a server which is responding to HTTP GETs. I have checked the endpoint with redbot.org and it states that :

1/The response allows all caches to store it
2/The response is fresh until 7 days from now
The reason may still be served by a cache once it becomes stale.

Therefore I believe the server side of this is now valid as a test for squid cache.

My client, is routing calls to squidcache, and when it arrives there, I see the following log entry:


1666355825.841    106 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json

Infact every single client call I make results in this TCP_MISS.

I therefore think this may be to do with my squid.conf.

Please see my squid conf below ? paying particular attention to the refresh patterns I added specifically for this call (because it wasn?t working without these so I thought I would add these to see if it would work).

Squid.conf:
http_port 3128

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443

acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 1025-65535  # unregistered ports

acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

http_access allow localnet
http_access allow localhost
http_access deny all

coredump_dir /squid/var/cache/squid

cache_dir ufs /var/spool/squid 1024 16 256

refresh_pattern -i http:\/\/192.168.20.1:3001\/.* 10080 100% 43200  override-lastmod
refresh_pattern -i http:\/\/192.168.20.1:3004\/.* 10080 100% 43200  override-lastmod
refresh_pattern ^ftp:                      1440      20%        10080
refresh_pattern ^gopher:             1440      0%          1440
refresh_pattern -i (/cgi-bin/|\?) 0             0%          0
refresh_pattern .                              0              20%        4320

The complete output from the logs is :
admin at zcxsys01:~$ docker logs squid-container
2022/10/21 12:16:58| WARNING: BCP 177 violation. Detected non-functional IPv6 loopback.
2022/10/21 12:16:58| Set Current Directory to /etc/squid
2022/10/21 12:16:58| Creating missing swap directories
2022/10/21 12:16:58| /var/spool/squid exists
2022/10/21 12:16:58| /var/spool/squid/00 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/00
2022/10/21 12:16:58| /var/spool/squid/01 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/01
2022/10/21 12:16:58| /var/spool/squid/02 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/02
2022/10/21 12:16:58| /var/spool/squid/03 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/03
2022/10/21 12:16:58| /var/spool/squid/04 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/04
2022/10/21 12:16:59| /var/spool/squid/05 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/05
2022/10/21 12:16:59| /var/spool/squid/06 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/06
2022/10/21 12:16:59| /var/spool/squid/07 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/07
2022/10/21 12:16:59| /var/spool/squid/08 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/08
2022/10/21 12:16:59| /var/spool/squid/09 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/09
2022/10/21 12:16:59| /var/spool/squid/0A exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0A
2022/10/21 12:16:59| /var/spool/squid/0B exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0B
2022/10/21 12:16:59| /var/spool/squid/0C exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0C
2022/10/21 12:16:59| /var/spool/squid/0D exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0D
2022/10/21 12:16:59| /var/spool/squid/0E exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0E
2022/10/21 12:16:59| /var/spool/squid/0F exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0F
2022/10/21 12:16:59| Removing PID file (/run/squid.pid)
2022/10/21 12:16:59| WARNING: BCP 177 violation. Detected non-functional IPv6 loopback.
2022/10/21 12:16:58| Set Current Directory to /etc/squid
2022/10/21 12:16:58| Creating missing swap directories
2022/10/21 12:16:58| /var/spool/squid exists
2022/10/21 12:16:58| /var/spool/squid/00 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/00
2022/10/21 12:16:58| /var/spool/squid/01 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/01
2022/10/21 12:16:58| /var/spool/squid/02 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/02
2022/10/21 12:16:58| /var/spool/squid/03 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/03
2022/10/21 12:16:58| /var/spool/squid/04 exists
2022/10/21 12:16:58| Making directories in /var/spool/squid/04
2022/10/21 12:16:59| /var/spool/squid/05 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/05
2022/10/21 12:16:59| /var/spool/squid/06 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/06
2022/10/21 12:16:59| /var/spool/squid/07 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/07
2022/10/21 12:16:59| /var/spool/squid/08 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/08
2022/10/21 12:16:59| /var/spool/squid/09 exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/09
2022/10/21 12:16:59| /var/spool/squid/0A exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0A
2022/10/21 12:16:59| /var/spool/squid/0B exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0B
2022/10/21 12:16:59| /var/spool/squid/0C exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0C
2022/10/21 12:16:59| /var/spool/squid/0D exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0D
2022/10/21 12:16:59| /var/spool/squid/0E exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0E
2022/10/21 12:16:59| /var/spool/squid/0F exists
2022/10/21 12:16:59| Making directories in /var/spool/squid/0F
2022/10/21 12:16:59| Removing PID file (/run/squid.pid)
2022/10/21 12:16:59| Set Current Directory to /etc/squid
2022/10/21 12:16:59| Starting Squid Cache version 5.2 for s390x-ibm-linux-gnu...
2022/10/21 12:16:59| Service Name: squid
2022/10/21 12:16:59| Process ID 11
2022/10/21 12:16:59| Process Roles: master worker
2022/10/21 12:16:59| With 1048576 file descriptors available
2022/10/21 12:16:59| Initializing IP Cache...
2022/10/21 12:16:59| DNS Socket created at 0.0.0.0, FD 8
2022/10/21 12:16:59| Adding nameserver 10.11.5.1 from /etc/resolv.conf
2022/10/21 12:16:59| Adding nameserver 10.11.5.2 from /etc/resolv.conf
2022/10/21 12:16:59| Adding domain test.sys.one from /etc/resolv.conf
2022/10/21 12:16:59| Logfile: opening log daemon:/var/log/squid/access.log
2022/10/21 12:16:59| Logfile Daemon: opening log /var/log/squid/access.log
2022/10/21 12:17:00| Unlinkd pipe opened on FD 14
2022/10/21 12:17:00| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2022/10/21 12:17:00| Store logging disabled
2022/10/21 12:17:00| Swap maxSize 1048576 + 262144 KB, estimated 100824 objects
2022/10/21 12:17:00| Target number of buckets: 5041
2022/10/21 12:17:00| Using 8192 Store buckets
2022/10/21 12:17:00| Max Mem  size: 262144 KB
2022/10/21 12:17:00| Max Swap size: 1048576 KB
2022/10/21 12:17:00| Rebuilding storage in /var/spool/squid (dirty log)
2022/10/21 12:17:00| Using Least Load store dir selection
2022/10/21 12:17:00| Set Current Directory to /etc/squid
2022/10/21 12:17:00| Finished loading MIME types and icons.
2022/10/21 12:17:00| HTCP Disabled.
2022/10/21 12:17:00| Pinger socket opened on FD 19
2022/10/21 12:17:00| Squid plugin modules loaded: 0
2022/10/21 12:17:00| Adaptation support is off.
2022/10/21 12:17:00| Accepting HTTP Socket connections at conn2 local=0.0.0.0:3128 remote=[::] FD 17 flags=9
2022/10/21 12:17:00| Done reading /var/spool/squid swaplog (0 entries)
2022/10/21 12:17:00| Store rebuilding is 0.00% complete
2022/10/21 12:17:00| Finished rebuilding storage from disk.
2022/10/21 12:17:00|         0 Entries scanned
2022/10/21 12:17:00|         0 Invalid entries.
2022/10/21 12:17:00|         0 With invalid flags.
2022/10/21 12:17:00|         0 Objects loaded.
2022/10/21 12:17:00|         0 Objects expired.
2022/10/21 12:17:00|         0 Objects cancelled.
2022/10/21 12:17:00|         0 Duplicate URLs purged.
2022/10/21 12:17:00|         0 Swapfile clashes avoided.
2022/10/21 12:17:00|   Took 0.12 seconds (  0.00 objects/sec).
2022/10/21 12:17:00| Beginning Validation Procedure
2022/10/21 12:17:00|   Completed Validation Procedure
2022/10/21 12:17:00|   Validated 0 Entries
2022/10/21 12:17:00|   store_swap_size = 0.00 KB
2022/10/21 12:17:00| WARNING: BCP 177 violation. Detected non-functional IPv6 loopback.
2022/10/21 12:17:00| pinger: Initialising ICMP pinger ...
2022/10/21 12:17:00| pinger: ICMP socket opened.
2022/10/21 12:17:00| pinger: ICMPv6 socket opened
2022/10/21 12:17:01| storeLateRelease: released 0 objects
1666355216.966    213 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355220.808     93 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355221.369     66 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355222.770    254 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355224.514     82 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355225.518     80 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355225.963     65 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355226.366     64 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355226.715     69 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355227.371    315 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355227.700     56 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355239.913    207 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355244.729     96 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355245.014     63 10.1.1.70 TCP_MISS/200 60615 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355408.390    208 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355426.040    197 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355426.650     63 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355427.196     65 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355427.565     60 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355427.915     64 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355824.295    209 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355825.153    120 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json
1666355825.841    106 10.1.1.70 TCP_MISS/200 60646 GET http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - HIER_DIRECT/192.168.20.1 application/json

I suspect something is wrong with my config ? perhaps the rules are not working correctly. Please can you offer some advice as to why I can?t get this response to cache in squid-cache.

Kind regards,

Andy Armstrong
?? ?????
Principal Specialist for Z Technologies
EMEA Squad Leader for Hybrid Cloud
Worldwide Community Leader for Hybrid Cloud
Member of the CTO Office Server & Storage EMEA
Distinguished Technical Specialist ? The Open Group
IBM Master Inventor

Mobile: +447500103874



From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Andy Armstrong <andy.armstrong at uk.ibm.com>
Date: Thursday, 29 September 2022 at 15:00
To: Amos Jeffries <squid3 at treenet.co.nz>, squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: [EXTERNAL] Re: [squid-users] TCP_MISS only
Hi, Excellent I understand and agree with what you are saying. Is this behaviour documented within the Squid documentation anywhere, or is this more ?how does the HTTP specification handle caching?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
ZjQcmQRYFpfptBannerStart
This Message Is From an External Sender
This message came from outside your organization.
ZjQcmQRYFpfptBannerEnd
Hi,

Excellent I understand and agree with what you are saying. Is this behaviour documented within the Squid documentation anywhere, or is this more ?how does the HTTP specification handle caching??

I am moving forward with a HTTP GET to see if that works per my use case. I assume therefore that any other verb is simply not going to work out the box?

Kind regards,

Andy Armstrong
?? ?????
Principal Specialist for Z Technologies
EMEA Squad Leader for Hybrid Cloud
Worldwide Community Leader for Hybrid Cloud
Member of the CTO Office Server & Storage EMEA
Distinguished Technical Specialist ? The Open Group
IBM Master Inventor

Mobile: +447500103874



From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Date: Thursday, 29 September 2022 at 13:06
To: squid-users at lists.squid-cache.org <squid-users at lists.squid-cache.org>
Subject: [EXTERNAL] Re: [squid-users] TCP_MISS only
On 28/09/22 07:56, Andy Armstrong wrote:
> Okay ? but what happens if you are communicating with a non REST
> endpoint.

You are still communicating over HTTP. To interact with and benefit from
HTTP agents like caches you need to comply to the HTTP semantics they use.

IMO, REST is just a useful tool to define (in abstract) an API's
operation when considering what/how it needs to be implemented.


> Consider a Web services endpoint for example where a request
> is only interacted with via POST but the operation for example may
> frequently be a read based function akin to a HTTP GET?

That is by definition a broken implementation of HTTP. The agent is
using a *delivery* API (POST) for retrieval (GET).

If you can separate the delivery and fetch operations HTTP becomes much
easier to use.


> Is Squid just
> simply not going to help cache those requests?

Not *by default*, no.

POST implies changing some arbitrary resource *other* than the URL
presented. Based on data and logic which may not be provided in the
request message URL+headers.

To use POST with caching both the client *and* the server have to
explicitly tell the HTTP cache agent(s) what to do on every single HTTP
message.

  - The client has to tell the cache whether a stored response is able
to be produced as reply, what object-ID it is trying to retrieve, what
object-ID's it already knows about (if any), and how old the stored
object is allowed to be.

  - The server has to tell the cache whether the response can be stored,
what to use for a unique-ID of the reply object, how old it already is,
how long it can be stored for, how and when to update it when it becomes
stale.

The Squid refresh_pattern can provide defaults for the storage times
when they are omitted. But all the ID related things and whether to use
cache at all can only come from the client/server.


As you can see by limiting yourself to POST-only you have imposed a huge
amount of complexity. Using GET instead for fetches makes all the above
*optional* where now it is mandatory.


> It is only helpful for
> more strict alignment to REST principles?
>

You lost me here. Squid implements HTTP.

REST is a very abstract simplification of basic HTTP/1.0 semantics. So
the closer ones code aligns to REST the *easier* it is to implement HTTP
properly. But HTTP/1.1+ are vastly more than REST.

HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users<http://lists.squid-cache.org/listinfo/squid-users>
Unless otherwise stated above:

IBM United Kingdom Limited
Registered in England and Wales with number 741598
Registered office: PO Box 41, North Harbour, Portsmouth, Hants. PO6 3AU

Unless otherwise stated above:

IBM United Kingdom Limited
Registered in England and Wales with number 741598
Registered office: PO Box 41, North Harbour, Portsmouth, Hants. PO6 3AU
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221021/bf01ba01/attachment.htm>

From gtaylor at tnetconsulting.net  Fri Oct 21 17:04:46 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Fri, 21 Oct 2022 11:04:46 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>
Message-ID: <0dc84627-5af0-78e1-9fe8-9a8329db92e1@spamtrap.tnetconsulting.net>

On 10/20/22 11:58 PM, Adam Majer wrote:
> It's basically by convention now.

Sure.

Conventions change over time.

Long enough ago 3128 wasn't the conventional port for Squid.

It used to be a convention to allow smoking in public / government 
offices.  Now the convention is the exact opposite.

> Port 3128 has been set as default port by Squid for more than 2 
> decades.

Agreed.

> Don't expect a change.

I'm not expecting a change.

At most I was hoping for a discussion about it.

Maybe, hopefully, said discussion will spark an idea in at least one 
person's head and that might turn into something in 10 or 20 years.

> Secondly, like it was said already, servers and proxies are different 
> things.

Semantics are VERY important here.  HTTP daemons and proxy daemons are 
both servers.  They just serve slightly different things.

> And you need to understand the difference between forward and reverse 
> proxies.

Agreed.  I've been using / leveraging / exploiting (in a good way) a 
combination of forward and reverse proxies for multiple decades.  They 
are distinctly different, but yet still remarkably similar.

Squid, Apache's HTTPD, Nginx, and even contemporary IIS can act as both 
an HTTP(S) server (a.k.a. reverse proxy) and / or a forward proxy.

> Reverse proxies can sit on the regular ports because that's their 
> job -- to ask as origins.



> Forward proxies don't sit on regular server ports because they require 
> explicit config on the client.

If we're explicitly configuring the client, then what does the port 
that's chosen have any influence on the explicit configuration?

Curl's man page is rather convenient and somewhat supportive ~> telling:

```
        Using an environment variable to set the proxy has the same 
effect as using the -x, --proxy option.

        http_proxy [protocol://]<host>[:port]
               Sets the proxy server to use for HTTP.

        HTTPS_PROXY [protocol://]<host>[:port]
               Sets the proxy server to use for HTTPS.
```

Notice how the `[:port]` is /optional/?

Curl (and other things) will default to using the IANA defined port for 
`[protocol://]` if `[:port]` is unspecified.

So ... why do we /need/ to use a different port than what IANA has 
defined for `[protocol://]`?

I'm genuinely asking why we /need/ to use a different port.

What, other than convention or even port contention, is prompting us to 
use a port other than what IANA has defined for the protocol?

> And don't forget we used to have transparent proxies which kind of died 
> (I think?) thanks to TLS.

I question the veracity of /used/ /to/.

Yes, TLS made things more difficult.  But in a corporate (like) 
environment doing TLS monkey in the middle is quite possible with Squid.

I am and have been doing exactly that on my personal devices for the 
last two years.

> Port 3128 is for *forward* proxy setup.

That's by convention / Squid default.

I've run forward HTTP proxies on port 80 and forward HTTPS proxies on 
port 443 for years without any problems.  What's more is that it 
simplifies the client configuration by removing the need to specify the 
port.  The following works perfectly fine for curl, et al.

    export http_proxy="proxy.home.example"

So -- again -- why do we /need/ to use a different port?

I fully acknowledge /contention/ and /contention/.  If that's the answer 
to the question, then so be it.  But I'm not yet convinced of such.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221021/bf38f1a1/attachment.bin>

From gtaylor at tnetconsulting.net  Fri Oct 21 17:25:10 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Fri, 21 Oct 2022 11:25:10 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1JXiv1bzO6n2s0h@fantomas.sk>
References: <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <Y1JXiv1bzO6n2s0h@fantomas.sk>
Message-ID: <5441e900-91ea-5f22-9045-939dda9adc53@spamtrap.tnetconsulting.net>

On 10/21/22 2:25 AM, Matus UHLAR - fantomas wrote:
> apparently this is a hack to be able to define proxy autoconfig in the 
> location field.
> 
> Since it has very restricted capabilities, it's apparently non-issue.
> 
> I guess that you can only define FindProxyForURL() this way.

ACK

Thank you for the additional details Matus.

> I know of such servers.

I did say /rarely/.  ;-)  I too have seen them.  They are just a 
disproportionately small number of web and proxy servers.

> And, HTTP proxy does not even have defined own port so people use random 
> ports or ports commonly used for this service.

Sure it does.  An HTTP proxy server is an HTTP server.  HTTP has port 80 
defined.

 From memory, the only effective difference between explicit proxy mode 
and transparent proxy mode (from Squid's point of view) is the use of 
the `CONNECT` vs `GET` et al, command and how the hostname is specified.

> the beautiful nature of HTTP allows us to define port within URL,

That is a very nice convenience.  But a /convenience/ does not equate to 
a /need/.

> therefore people tend so use separate ports instead of allocating 
> extra IP addresses for proxy usage.

That is a convention.  But a /convention/ does not equate to a /need/.

> I think Adam Meyer also explained it nicely.

Yes, Adam said that 3128 is a /convention/.

convention != need

> That is FTP through HTTP proxy. Not FTP through FTP proxy.

Hum.  I want to disagree, but I don't have anything to counter that at 
the moment.

> I repeat, FTP protocol does not support proxies and port 21 would be of 
> low usage here.

I remember reading things years ago where people would use a bog 
standard FTP client to connect to an /FTP/ server acting as an /FTP/ 
proxy.  I believe they then issues `OPEN` commands on the /FTP/ proxy 
just like they did on their /FTP/ client.  --  My understanding was that 
this had absolutely /nothing/ to do with /HTTP/, neither protocol nor 
proxy daemon.  Nor was it telnet / rlogin / etc. to run a standard ftp 
client on a bastion host.  Though that was also a solution at the time.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221021/c9368adf/attachment.bin>

From gtaylor at tnetconsulting.net  Fri Oct 21 17:51:47 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Fri, 21 Oct 2022 11:51:47 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <5441e900-91ea-5f22-9045-939dda9adc53@spamtrap.tnetconsulting.net>
References: <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <Y1JXiv1bzO6n2s0h@fantomas.sk>
 <5441e900-91ea-5f22-9045-939dda9adc53@spamtrap.tnetconsulting.net>
Message-ID: <cd85f0b4-86bd-7a5c-a1ce-80a04ae08d20@spamtrap.tnetconsulting.net>

On 10/21/22 11:25 AM, Grant Taylor wrote:
> I remember reading things years ago where people would use a bog 
> standard FTP client to connect to an /FTP/ server acting as an /FTP/ 
> proxy.

I knew that I had seen something about using an FTP proxy that wasn't 
HTTP related.

I encourage you to read ~/.ncftp/firewall for more details. 
Conveniently copied below.

I'd like to point out two things:

1)  The syntax and ports used only reference FTP.
2)  The 'NcFTP does NOT support HTTP proxies that do FTP, such as 
"squid" or Netscape Proxy Server.  Why?  Because you have to communicate 
with them using HTTP, and this is a FTP only program.'

So ... yes, I am quite certain that there are FTP /proxies/ that are NOT 
using HTTP.

--8<--
# NcFTP firewall preferences
# ==========================
#
# If you need to use a proxy for FTP, you can configure it below.
# If you do not need one, leave the ``firewall-type'' variable set
# to 0.  Any line that does not begin with the ``#'' character is
# considered a configuration command line.
#
# NOTE:  NcFTP does NOT support HTTP proxies that do FTP, such as "squid"
#        or Netscape Proxy Server.  Why?  Because you have to 
communicate with
#        them using HTTP, and this is a FTP only program.
#
# Types of firewalls:
# ------------------
#
#    type 1:  Connect to firewall host, but send "USER user at real.host.name"
#
#    type 2:  Connect to firewall, login with "USER fwuser" and
#             "PASS fwpassword", and then "USER user at real.host.name"
#
#    type 3:  Connect to and login to firewall, and then use
#             "SITE real.host.name", followed by the regular USER and PASS.
#
#    type 4:  Connect to and login to firewall, and then use
#             "OPEN real.host.name", followed by the regular USER and PASS.
#
#    type 5:  Connect to firewall host, but send
#             "USER user at fwuser@real.host.name" and
#             "PASS pass at fwpass" to login.
#
#    type 6:  Connect to firewall host, but send
#             "USER fwuser at real.host.name" and
#             "PASS fwpass" followed by a regular
#             "USER user" and
#             "PASS pass" to complete the login.
#
#    type 7:  Connect to firewall host, but send
#             "USER user at real.host.name fwuser" and
#             "PASS pass" followed by
#             "ACCT fwpass" to complete the login.
#
#    type 8:  Connect to firewall host, but send "USER 
user at real.host.name:port"
#
#    type 9:  Connect to firewall host, but send "USER 
user at real.host.name port"
#
#    type 0:  Do NOT use a firewall (most users will choose this).
#
firewall-type=0
#
#
#
# The ``firewall-host'' variable should be the IP address or hostname of
# your firewall server machine.
#
firewall-host=firewall.home.example.net
#
#
#
# The ``firewall-user'' variable tells NcFTP what to use as the user ID
# when it logs in to the firewall before connecting to the outside world.
#
firewall-user=fwuser
#
#
#
# The ``firewall-password'' variable is the password associated with
# the firewall-user ID.  If you set this here, be sure to change the
# permissions on this file so that no one (except the superuser) can
# see your password.  You may also leave this commented out, and then
# NcFTP will prompt you each time for the password.
#
firewall-password=fwpass
#
#
#
# Your firewall may require you to connect to a non-standard port for
# outside FTP services, instead of the internet standard port number (21).
#
firewall-port=21
#
#
#
# You probably do not want to FTP to the firewall for hosts on your own
# domain.  You can set ``firewall-exception-list'' to a list of domains
# or hosts where the firewall should not be used.  For example, if your
# domain was ``probe.net'' you could set this to ``.probe.net''.
#
# If you leave this commented out, the default behavior is to attempt to
# lookup the current domain, and exclude hosts for it.  Otherwise, set it
# to a list of comma-delimited domains or hostnames.  The special token
# ``localdomain'' is used for unqualified hostnames, so if you want hosts
# without explicit domain names to avoid the firewall, be sure to include
# that in your list.
#
firewall-exception-list=.home.example.net,localhost,localdomain
#
#
#
# You may also specify passive mode here.  Normally this is set in the
# regular $HOME/.ncftp/prefs file.  This must be set to one of
# "on", "off", or "optional", which mean always use PASV,
# always use PORT, and try PASV then PORT, respectively.
#
#passive=on
#
#
#
# NOTE:  This file was created for you on Sat Jan 21 23:09:26 2017
#        by NcFTP 3.2.5.  Removing this file will cause the next run of 
NcFTP
#        to generate a new one, possibly with more configurable options.
#
# ALSO:  A /etc/ncftp.firewall file, if present, is processed before 
this file,
#        and a /etc/ncftp.firewall.fixed file, if present, is processed 
after.
-->8--



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221021/5d2ff343/attachment.bin>

From gtaylor at tnetconsulting.net  Fri Oct 21 17:54:31 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Fri, 21 Oct 2022 11:54:31 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1JdgXAHrt0ZMpiL@fantomas.sk>
References: <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <Y1JXiv1bzO6n2s0h@fantomas.sk> <Y1JdgXAHrt0ZMpiL@fantomas.sk>
Message-ID: <3ce201f1-ada7-a563-4444-7892ae774c78@spamtrap.tnetconsulting.net>

On 10/21/22 2:51 AM, Matus UHLAR - fantomas wrote:
> I should have added, that squid does support FTP proxying using one of 
> hacks I mentioned (I haven't tested it yet).

I think I used Squid's FTP protocol support years ago.

> And, since this requires other (FTP) protocol than the default (HTTP) at 
> the proxy side, people free to configure it on random port they choose.
> 
> FTP proxying is so rarely used that it doesn't even have common port 
> besides 21 used for FTP.
The fundamental core component of my (sub)thread is that alternate ports 
aren't /needed/.  The default IANA reserved port is perfectly fine.  -- 
Presuming that there isn't any contention or (site local) convention to 
use a different port.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221021/ee7affee/attachment.bin>

From squid3 at treenet.co.nz  Sat Oct 22 05:30:09 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Oct 2022 18:30:09 +1300
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <0dc84627-5af0-78e1-9fe8-9a8329db92e1@spamtrap.tnetconsulting.net>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>
 <0dc84627-5af0-78e1-9fe8-9a8329db92e1@spamtrap.tnetconsulting.net>
Message-ID: <7b177298-2613-9215-2a99-b322e8b7c8e0@treenet.co.nz>

On 22/10/22 06:04, Grant Taylor wrote:
> On 10/20/22 11:58 PM, Adam Majer wrote:
>> It's basically by convention now.
> 
> Sure.
> 
> Conventions change over time.
> 
> Long enough ago 3128 wasn't the conventional port for Squid.
> 

Not just convention. AFAICT was formally registered with W3C, before 
everyone went to using IETF for registrations.




> Maybe, hopefully, said discussion will spark an idea in at least one 
> person's head and that might turn into something in 10 or 20 years.
> 

FYI, discussion started ~30 years ago.


The problem:

For bandwidth savings HTTP/1.0 defined different URL syntax for origin 
and relay/proxy requests. The form sent to an origin server lacks any 
information about the authority. That was expected to be known 
out-of-band by the origin itself.


HTTP/1.1 has attempted several different mechanisms to fix this over the 
years. None of them has been universally accepted, so the problem 
remains. The best we have is mandatory Host header which most (but sadly 
not all) clients and servers use.


HTTP/2 cements that design with mandatory ":authority" pseudo-header 
field. So the problem is "fixed"for native HTTP/2+ traffic. But until 
HTTP/1.0 and broken HTTP/1.1 clients are all gone the issue will still 
crop up.
  And ... Squid still only supports HTTP/1.1 and older.


> 
>> Forward proxies don't sit on regular server ports because they require 
>> explicit config on the client.
> 
> If we're explicitly configuring the client, then what does the port 
> that's chosen have any influence on the explicit configuration?


More importantly the proxy hostname:port the client is opening TCP 
connections to may be different from the authority-info specified in the 
HTTP request message (or lack thereof). This crosses security boundaries 
and involves out-of-band information sources at all three endpoints 
involved in the transaction for the message semantics and protocol 
negotiations to work properly.


> 
> Curl's man page is rather convenient and somewhat supportive ~> telling:
> 
> ```
>  ?????? Using an environment variable to set the proxy has the same 
> effect as using the -x, --proxy option.
> 
>  ?????? http_proxy [protocol://]<host>[:port]
>  ????????????? Sets the proxy server to use for HTTP.
> 
>  ?????? HTTPS_PROXY [protocol://]<host>[:port]
>  ????????????? Sets the proxy server to use for HTTPS.
> ```
> 
> Notice how the `[:port]` is /optional/?
> 

What that text does not say is that when they are omitted by the 
**user** they are taken from configuration settings in the OS:

  * the environment variable name provides:
     - the protocol name ("http" or "HTTPS", aka plain-text or encrypted)
     - the expected protocol syntax/semantics ("proxy" aka forward-proxy)

  * the machine /etc/services configuration provides the default port 
for the named protocol.


Attempting to use a reverse-proxy or origin server such a configuration 
may work for some messages, but **will** fail due to syntax or semantic 
errors on others.
  Likewise NAT'ing inbound port 443 or port 80 traffic to a 
forward-proxy will encounter the same types of issues - while it is 
perfectly fine to do so towards a reverse-proxy or origin server.


HTH
Amos


From squid3 at treenet.co.nz  Sat Oct 22 06:09:19 2022
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Oct 2022 19:09:19 +1300
Subject: [squid-users] TCP_MISS only
In-Reply-To: <SA0PR15MB37572B259E38021ACF9BFAA0B22D9@SA0PR15MB3757.namprd15.prod.outlook.com>
References: <SA0PR15MB375725E2BCE8978659B9BE0CB2529@SA0PR15MB3757.namprd15.prod.outlook.com>
 <da33fd71-420c-cc4a-47e9-e057626d4e4f@measurement-factory.com>
 <SA0PR15MB3757DD79FA92D3C545F26938B2559@SA0PR15MB3757.namprd15.prod.outlook.com>
 <4395563c-d41b-c3ca-97a6-a5588a51bfc9@treenet.co.nz>
 <SA0PR15MB3757FF0BD73163E637A119F8B2559@SA0PR15MB3757.namprd15.prod.outlook.com>
 <5d36de4a-715d-4910-c235-2c7d777867b9@treenet.co.nz>
 <SA0PR15MB37577549B07B7CF9F5295413B2579@SA0PR15MB3757.namprd15.prod.outlook.com>
 <SA0PR15MB37572B259E38021ACF9BFAA0B22D9@SA0PR15MB3757.namprd15.prod.outlook.com>
Message-ID: <c580c069-bdae-684c-4cda-6e4792bb0540@treenet.co.nz>

On 22/10/22 01:50, Andy Armstrong wrote:
> Hi Amos, All,
> 
> I continue to struggle with only receiving TCP_MISS when using 
> squid-cache. I have now implemented a server which is responding to HTTP 
> GETs. I have checked the endpoint with redbot.org and it states that :
> 
> 1/The response allows all caches to store it
> 
> 2/The response is fresh until 7 days from now
> 
> The reason may still be served by a cache once it becomes stale.
> 
> Therefore I believe the server side of this is now valid as a test for 
> squid cache.
> 

Excellent.


> My client, is routing calls to squidcache, and when it arrives there, I 
> see the following log entry:
> 
> 1666355825.841106 10.1.1.70 TCP_MISS/200 60646 GET 
> http://192.168.20.1:3004/largeapp/largeapp/007005/bongo? - 
> HIER_DIRECT/192.168.20.1 application/json
> 

The URL contains a query-string section. That is elided by default for 
security (in case credentials or other sensitive info is in there).

To more accurately test this you should add this to your squid.conf:

   strip_query_terms off

That will log the full query-string for accurate URL checking in 
access.log. You can remove it for the production system later if you want.


If the resulting access.log shows any difference (one single byte is 
enough) between the URLs they are different objects as far as the cache 
can tell.

Are you using the Vary feature of HTTP at all? If so the exact bytes in 
the named client headers matter (both case-sensitive and order-sensitive).



> Infact every single client call I make results in this TCP_MISS.
> 
> I therefore think this may be to do with my squid.conf.
> 

There does not appear to be anything wrong with your squid.conf.
I suspect some misunderstanding of the HTTP semantics.

...
> 
> refresh_pattern -i http:\/\/192.168.20.1:3001\/.* 10080 100% 43200  
> override-lastmod
> 
> refresh_pattern -i http:\/\/192.168.20.1:3004\/.* 10080 100% 43200  
> override-lastmod
> 

The redbot.org results indicate that the object is cacheable according 
to the server headers so you should not need these. I do not think they 
are being harmful, but please remove for now to simplify the 
troubleshooting.


Please do the following:

  * add this to squid.conf to see the actual traffic flow in cache.log:

    debug_options ALL,1 11,2

  * make two (2) client requests for the same object. Waiting until the 
server responses have both arrived.

  * check access.log to verify the full URL is indeed identical.

We will need to see the HTTP traffic sequence you should be able to find 
in cache.log. The log entries are multi-line and start with the 
following tags, appearing in this order:

   "HTTP Client REQUEST"
   "HTTP Server REQUEST"
   "HTTP Server RESPONSE"
   "HTTP Client RESPONSE"

Please post the line(s) with that tag and the block of headers that 
follow it.

If any other log lines appear interesting or suspicious, it would be 
helpful to see those too and where exactly in the sequence they appear.


HTH
Amos


From gtaylor at tnetconsulting.net  Sat Oct 22 17:10:58 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Sat, 22 Oct 2022 11:10:58 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <7b177298-2613-9215-2a99-b322e8b7c8e0@treenet.co.nz>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>
 <0dc84627-5af0-78e1-9fe8-9a8329db92e1@spamtrap.tnetconsulting.net>
 <7b177298-2613-9215-2a99-b322e8b7c8e0@treenet.co.nz>
Message-ID: <2f52e708-b1d8-ad6d-c9ea-62c3c5cf2965@spamtrap.tnetconsulting.net>

On 10/21/22 11:30 PM, Amos Jeffries wrote:
> Not just convention. AFAICT was formally registered with W3C, before 
> everyone went to using IETF for registrations.

Please elaborate on what was formally registered.  I've only seen 3128 / 
3129 be the default for Squid (and a few things emulating squid).  Other 
proxies of the time, namely Netscape's and Microsoft's counterparts, 
tended to use 8080.

I'd genuinely like to learn more about and understand the history / 
etymology / genesis of the 3128 / 3129.

> FYI, discussion started ~30 years ago.

ACK

> The problem:
> 
> For bandwidth savings HTTP/1.0 defined different URL syntax for origin 
> and relay/proxy requests. The form sent to an origin server lacks any 
> information about the authority. That was expected to be known 
> out-of-band by the origin itself.
> 
> HTTP/1.1 has attempted several different mechanisms to fix this over the 
> years. None of them has been universally accepted, so the problem 
> remains. The best we have is mandatory Host header which most (but sadly 
> not all) clients and servers use.
> 
> HTTP/2 cements that design with mandatory ":authority" pseudo-header 
> field. So the problem is "fixed"for native HTTP/2+ traffic. But until 
> HTTP/1.0 and broken HTTP/1.1 clients are all gone the issue will still 
> crop up.

I'm not entirely sure what you mean by "the authority".  I'm taking it 
to mean the identity of the service that you are wanting content from. 
The Host: header comment with HTTP/1.1 is what makes me think this.

My understanding is that neither HTTP/0.9 nor HTTP/1.0 had a Host: 
header and that it was assumed that the IP address you were connecting 
to conveyed the server that you were wanting to connect to.

I have very little technical understanding of HTTP/2 as I've not needed 
to delve into it and it has largely just worked for me.

> And ... Squid still only supports HTTP/1.1 and older.

Okay.  That sort of surprises me.  But I have zero knowledge to disagree.

> More importantly the proxy hostname:port the client is opening TCP 
> connections to may be different from the authority-info specified in the 
> HTTP request message (or lack thereof).

My working understanding of what the authority is seems to still work 
with this.

> This crosses security boundaries and involves out-of-band information 
> sources at all three endpoints involved in the transaction for the 
> message semantics and protocol negotiations to work properly.

I feel like the nature of web traffic tends to frequently, but not 
always, cross security / administrative boundaries.  As such, I don't 
think that existence of proxies in the communications path alters things 
much.

Please elaborate on what out-of-band information you are describing. 
The most predominant thing that comes to mind, particularly with 
HTTP/1.1 and HTTP/2 is name resolution -- ostensibly DNS -- to identify 
the IP address to connect to.

> What that text does not say is that when they are omitted by the 
> **user** they are taken from configuration settings in the OS:
> 
>  ?* the environment variable name provides:
>  ??? - the protocol name ("http" or "HTTPS", aka plain-text or encrypted)
>  ??? - the expected protocol syntax/semantics ("proxy" aka forward-proxy)
> 
>  ?* the machine /etc/services configuration provides the default port 
> for the named protocol.

Ergo the use of /default/ values when values are not specified.

I feel like this in a round about way supports my stance that the 
default ports are perfectly fine to use.

> Attempting to use a reverse-proxy or origin server such a configuration 
> may work for some messages, but **will** fail due to syntax or semantic 
> errors on others.

I question the veracity of that statement.

Sure, trying to speak contemporary protocols (HTTP/1.1 or HTTP/2) to an 
ancient HTTP server is not going to work.

But I believe that Squid and Apache HTTPD can be configured to perform 
all three roles; origin server, reverse proxy, and forward proxy.

Aside:  Squid might not be a typical origin server in that you can't 
have it /directly/ serve /typical/ origin content.  However I believe it 
does function as an origin server for things like Squid error pages.

> Likewise NAT'ing inbound port 443 or port 80 traffic to a 
> forward-proxy will encounter the same types of issues - while it is 
> perfectly fine to do so towards a reverse-proxy or origin server.

I believe that is entirely dependent on the capability and configuration 
of the forward proxy.  --  I've done exactly this with Apache HTTPD. 
Though I've not had the (dis)pleasure of doing so with Squid.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221022/8e0e1170/attachment.bin>

From squid at marrold.co.uk  Mon Oct 24 00:36:15 2022
From: squid at marrold.co.uk (Matthew H)
Date: Mon, 24 Oct 2022 01:36:15 +0100
Subject: [squid-users] Empty transfer-encoding header causes 502 response
Message-ID: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>

Hi,

I'm using Squid to proxy HTTP requests to another proxy. I can see squid
sending the request to the parent and getting a response, but it sends the
client that initiated the request a 502 Bad Gateway response.

On closer inspection it appears the parent proxy is sending an
empty transfer-encoding header, and this is causing Squid to send a 502. Is
there any way to ignore this? It looks like its enforced in
HttpHeader.cc#L510
<https://github.com/squid-cache/squid/blob/5139f0f64a4d7bba0d91d631b031c4700e7576b4/src/HttpHeader.cc#L510>

I'm using Squid 5.6, or more specifically the ubuntu/squid:5.6-22.10_edge
docker image.

I've included the logs below.

Thanks
Matthew

2022/10/24 00:23:59.106| ctx: enter level  0: 'http://nintendo.com/'
2022/10/24 00:23:59.106| 11,3| http.cc(666) processReplyHeader:
processReplyHeader: key '19010000000000000C00000000000000'
2022/10/24 00:23:59.106| 11,2| http.cc(720) processReplyHeader: HTTP Server
conn294 local=172.25.0.3:57802 remote=159.203.14.9:1996 FIRSTUP_PARENT FD
26 flags=1
2022/10/24 00:23:59.106| 11,2| http.cc(721) processReplyHeader: HTTP Server
RESPONSE:
---------
HTTP/1.1 200 OK
x-powered-by: Express
content-type: text/html; charset=iso-8859-1
transfer-encoding:
date: Mon, 24 Oct 2022 00:23:57 GMT
connection: close

----------
2022/10/24 00:23:59.106| 55,3| HttpHeader.cc(882) getList: empty list
header: Transfer-Encoding(Transfer-Encoding[63])
2022/10/24 00:23:59.106| 55,2| HttpHeader.cc(559) parse: WARNING:
unsupported Transfer-Encoding used by client:
2022/10/24 00:23:59.106| ctx: exit level  0
2022/10/24 00:23:59.106| 20,3| store.cc(1673) reset: http://nintendo.com/
2022/10/24 00:23:59.107| 17,3| FwdState.cc(492) fail: ERR_INVALID_RESP "Bad
Gateway"
        http://nintendo.com/
2022/10/24 00:23:59.107| 17,3| FwdState.cc(533) unregister:
http://nintendo.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221024/4a082b13/attachment.htm>

From ankor2023 at gmail.com  Mon Oct 24 06:58:38 2022
From: ankor2023 at gmail.com (Andrey K)
Date: Mon, 24 Oct 2022 09:58:38 +0300
Subject: [squid-users] squid exiting on signal 6
In-Reply-To: <86ilkpp6lw.fsf@gmail.com>
References: <86mta1pijq.fsf@gmail.com>
 <8b36ba40-e531-b6dd-927b-e70b021efe09@treenet.co.nz>
 <86ilkpp6lw.fsf@gmail.com>
Message-ID: <CADJd0Y39U8PLW7KLpecpJYxt_V6wUeMJ-=SZr9sz1Hm6Rp0R3Q@mail.gmail.com>

Hello ludovit,

We experienced the similar problems of crashing squid with "signal 6 and
status 0" symptoms in the /var/log/messages. There were hundreds of crashes
per hour during the working hours. There were also many crashes during
night hours. And I had no idea what the cause of the problem was.
But your case shed light on the problem.

I captured network traffic and saw in the dump many requests to IPv6
addresses like this: POST http://[2001:67c:4e8:f004::a]:80/api HTTP/1.1\r\n
I think that the requests were generated by the Telegram application,
because neighboring similar requests were addressed to Telegram IPv4 pools:
POST http://91.105.192.100:80/api HTTP/1.1\r\n

To fix the problem somehow, I added the following rules to the very
beginning of the SQUID configuration:
  acl urldst_ipv6 url_regex ^http://\[
  http_access deny urldst_ipv6

Now it seems the problem has been solved.

I was wondering if this is a SQUID bug. In my opinion, SQUID should not
crash on requests to IPv6 resources, but should send a response with an
error, for example, Bad gateway 502, or Not Acceptable 406.


Kind regards,
    Ankor.


??, 12 ???. 2022 ?. ? 15:30, Ludovit Koren <ludovit.koren at gmail.com>:

> >>>>> Amos Jeffries <squid3 at treenet.co.nz> writes:
>
>     > On 12/10/22 21:12, Ludovit Koren wrote:
>     >> Hi,
>     >> I am running squid-5.7 on FreeBSD 12.3-STABLE r371879. Occasionally
>     >> I
>     >> get the following error:
>     >>
>
>     >> #3  0x000000080111fcb1 in __assert (func=<optimized out>,
> file=<optimized out>, line=<optimized out>, failedexpr=<optimized out>) at
> /usr/src/lib/libc/gen/assert.c:51
>     >> #4  0x0000000000698fcd in Ip::Address::getAddrInfo
> (this=0x861c04588, dst=<optimized out>, force=0) at Address.cc:663
>     >> #5  0x000000000068b732 in comm_openex (sock_type=sock_type at entry=1,
> proto=proto at entry=6, addr=..., flags=1, note=0x85ce42dc0
> "[fe80::21f:29ff:fe28:7017]") at comm.cc:347
>     > ...
>
>     >> The squid is compiled without IPv6 option, so I do not understand
>     >> why it
>     >> tries to reach IPv6 address.
>     >>
>
>     > A client is requiring connection to an IPv6 server. But Squid cannot
>     > convert that IPv6 address or use on an IPv4-only network. Lack of
> IPv6
>     > support also forbids IPv6 failover handling being used.
>
> So the solution is to reenable IPv6 in the squid, as well as in the OS
> network stack? Am I right?
>
> Thank you.
>
> Regards,
>
> lk
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221024/1a76db40/attachment.htm>

From wadie.lemrazzeq at capgemini.com  Mon Oct 24 15:48:09 2022
From: wadie.lemrazzeq at capgemini.com (LEMRAZZEQ, Wadie)
Date: Mon, 24 Oct 2022 15:48:09 +0000
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
Message-ID: <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>

I think this discussion had diverged from its subject 
So I refocus in our subject, gents

>I do not know exactly what you mean by "https proxy" in this context, but I suspect that you are using the wrong FireFox setting. The easily accessible "HTTPS proxy" setting in the "Configure Proxy Access to the Internet" dialog is _not_ what you >need! That setting configures a plain text HTTP proxy for handling HTTPS traffic. Very misleading, I know.

>You need a PAC file that tells FireFox to use an HTTPS proxy.

>See (again)
>https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection
>which refers to https://bugzilla.mozilla.org/show_bug.cgi?id=378637#c68

Indeed, I am aware of this bug discussion and I did apply the PAC script into network.proxy.autoconfig_url, and it did not work
And what's more misleading is that the bug is tagged resolved, as if starting from firefox 33, it supports https proxy out of the box
But anyway, my next step is to use a PAC file, since it is the legacy method, if this doesn't work either I'm gonna use stunnels

Thank you everyone for your insights
Regards,

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Rafael Akchurin
Sent: Thursday, October 20, 2022 7:34 AM
To: Grant Taylor; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] FW: Encrypted browser-Squid connection errors

***This mail has been sent by an external source***

The following line set in the Script Address box of the browser proxy configuration will help - no need for a PAC file for quick tests. Be sure to adjust the proxy name and port.

data:,function FindProxyForURL(u, h){return "HTTPS proxy.example.lan:8443";}

More info at https://webproxy.diladele.com/docs/network/secure_proxy/browsers/

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Grant Taylor
Sent: Thursday, October 20, 2022 2:39 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] FW: Encrypted browser-Squid connection errors

On 10/19/22 8:33 AM, Alex Rousskov wrote:
> I do not know exactly what you mean by "https proxy" in this context, 
> but I suspect that you are using the wrong FireFox setting. The easily 
> accessible "HTTPS proxy" setting in the "Configure Proxy Access to the 
> Internet" dialog is _not_ what you need! That setting configures a 
> plain text HTTP proxy for handling HTTPS traffic. Very misleading, I know.

+10 to the antiquated UI ~> worse UX.

> You need a PAC file that tells FireFox to use an HTTPS proxy.

I believe you can use the FoxyProxy add-on to manage this too.



--
Grant. . . .
unix || die

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

This message contains information that may be privileged or confidential and is the property of the Capgemini Group. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain, copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message.



From gtaylor at tnetconsulting.net  Mon Oct 24 16:08:08 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Mon, 24 Oct 2022 10:08:08 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <01bd9f1d-6b64-ce63-c07b-5bdd8c37c063@spamtrap.tnetconsulting.net>

On 10/24/22 9:48 AM, LEMRAZZEQ, Wadie wrote:
> But anyway, my next step is to use a PAC file, since it is the legacy 
> method, if this doesn't work either I'm gonna use stunnels

I have (a superset of) the following in my PAC file.

It is working perfectly fine for me across multiple browsers and 
multiple OSs.

function FindProxyForURL(url, host) {
	if (
		dnsDomainIs(host, "example.com") ||
		dnsDomainIs(host, "example.net") ||
		dnsDomainIs(host, "example.org") ||
		false
	) {
		return "DIRECT";
	} else {
		return "HTTPS 192.0.2.251:443; PROXY 192.0.2.251:80";
	}
}

N.B. I'm doing TLS Monkey in the Middle with a self signed cert 
installed as a root CA in my client systems.  --  Being able to filter 
HTTPS content is WONDERFUL.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221024/19e9e49f/attachment.bin>

From uhlar at fantomas.sk  Tue Oct 25 08:43:54 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 10:43:54 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <cd85f0b4-86bd-7a5c-a1ce-80a04ae08d20@spamtrap.tnetconsulting.net>
 <5441e900-91ea-5f22-9045-939dda9adc53@spamtrap.tnetconsulting.net>
Message-ID: <Y1ehyvnIdIFqw48T@fantomas.sk>

>On 10/21/22 2:25 AM, Matus UHLAR - fantomas wrote:
>>apparently this is a hack to be able to define proxy autoconfig in 
>>the location field.
>>
>>Since it has very restricted capabilities, it's apparently non-issue.
>>
>>I guess that you can only define FindProxyForURL() this way.

On 21.10.22 11:25, Grant Taylor wrote:
>From memory, the only effective difference between explicit proxy mode 
>and transparent proxy mode (from Squid's point of view) is the use of 
>the `CONNECT` vs `GET` et al, command and how the hostname is 
>specified.

if by "transparent" you mean "intercepting" proxy, that is incorrect


CONNECT is HTTP command designed for use with explicit HTTP proxy.

>>I think Adam Meyer also explained it nicely.
>
>Yes, Adam said that 3128 is a /convention/.

ok, there's no explicit need. And since there's no explicit need to use port 
80 for HTTP proxy, the convention is to use different port because of 
reasons stated before.

>>I repeat, FTP protocol does not support proxies and port 21 would be 
>>of low usage here.
>
>I remember reading things years ago where people would use a bog 
>standard FTP client to connect to an /FTP/ server acting as an /FTP/ 
>proxy.  I believe they then issues `OPEN` commands on the /FTP/ proxy 
>just like they did on their /FTP/ client.  --  My understanding was 
>that this had absolutely /nothing/ to do with /HTTP/, neither protocol 
>nor proxy daemon.  Nor was it telnet / rlogin / etc. to run a standard 
>ftp client on a bastion host.  Though that was also a solution at the 
>time.


On 21.10.22 11:51, Grant Taylor wrote:
>I knew that I had seen something about using an FTP proxy that wasn't 
>HTTP related.
>
>I encourage you to read ~/.ncftp/firewall for more details. 
>Conveniently copied below.
>
>I'd like to point out two things:
>
>1)  The syntax and ports used only reference FTP.
>2)  The 'NcFTP does NOT support HTTP proxies that do FTP, such as 
>"squid" or Netscape Proxy Server.  Why?  Because you have to 
>communicate with them using HTTP, and this is a FTP only program.'
>
>So ... yes, I am quite certain that there are FTP /proxies/ that are 
>NOT using HTTP.

These are the FTP protocol "hacks" I mentioned before.
The HTTP protocol was created with proxying in mind, FTP was not.
using specially crafted login name for connecting to anoter server is one of 
those hacks.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The 3 biggets disasters: Hiroshima 45, Tschernobyl 86, Windows 95


From uhlar at fantomas.sk  Tue Oct 25 08:50:03 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 10:50:03 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <Y1ejO821EAq4tibN@fantomas.sk>

On 24.10.22 15:48, LEMRAZZEQ, Wadie wrote:
>I think this discussion had diverged from its subject
>So I refocus in our subject, gents
>
>> I do not know exactly what you mean by "https proxy" in this context, but 
>> I suspect that you are using the wrong FireFox setting.  The easily 
>> accessible "HTTPS proxy" setting in the "Configure Proxy Access to the 
>> Internet" dialog is _not_ what you >need!  That setting configures a 
>> plain text HTTP proxy for handling HTTPS traffic.  Very misleading, I 
>> know.
>
>>You need a PAC file that tells FireFox to use an HTTPS proxy.
>
>>See (again)
>>https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection
>>which refers to https://bugzilla.mozilla.org/show_bug.cgi?id=378637#c68
>
>Indeed, I am aware of this bug discussion and I did apply the PAC script into network.proxy.autoconfig_url, and it did not work
>And what's more misleading is that the bug is tagged resolved, as if starting from firefox 33, it supports https proxy out of the box
>But anyway, my next step is to use a PAC file, since it is the legacy method

legacy?

>if this doesn't work either I'm gonna use stunnels

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Quantum mechanics: The dreams stuff is made of.


From uhlar at fantomas.sk  Tue Oct 25 09:03:25 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 11:03:25 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB61496C45B952998FCC050A49F5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <DBAPR02MB614945C56A9BB57130704BF0F52E9@DBAPR02MB6149.eurprd02.prod.outlook.com>
Message-ID: <Y1emXSOdxwqJRPDN@fantomas.sk>

>>I do not know exactly what you mean by "https proxy" in this context, but 
>> I suspect that you are using the wrong FireFox setting.  The easily 
>> accessible "HTTPS proxy" setting in the "Configure Proxy Access to the 
>> Internet" dialog is _not_ what you >need!  That setting configures a 
>> plain text HTTP proxy for handling HTTPS traffic.  Very misleading, I 
>> know.
>
>>You need a PAC file that tells FireFox to use an HTTPS proxy.
>
>>See (again)
>>https://wiki.squid-cache.org/Features/HTTPS#Encrypted_browser-Squid_connection
>>which refers to https://bugzilla.mozilla.org/show_bug.cgi?id=378637#c68

skip my previous e-mail

On 24.10.22 15:48, LEMRAZZEQ, Wadie wrote:
>Indeed, I am aware of this bug discussion and I did apply the PAC script 
> into network.proxy.autoconfig_url, and it did not work

perhaps syntax error in that script you have pasted?

I assume you pasted exactly text as mentioned on:
http://lists.squid-cache.org/pipermail/squid-users/2022-October/025315.html
or:
https://webproxy.diladele.com/docs/network/secure_proxy/browsers/


>And what's more misleading is that the bug is tagged resolved, as if 
> starting from firefox 33, it supports https proxy out of the box

yes, by using PAC script, or perhaps an extention that configures it 
instead. foxyproxy was mentioned iirc

>But anyway, my next step is to use a PAC file, since it is the legacy 
> method, if this doesn't work either I'm gonna use stunnels

I know nothing of autoconfig being legacy.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I intend to live forever - so far so good.


From squid3 at treenet.co.nz  Tue Oct 25 12:39:04 2022
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Wed, 26 Oct 2022 01:39:04 +1300
Subject: [squid-users] Empty transfer-encoding header causes 502 response
In-Reply-To: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>
References: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>
Message-ID: <5bbfac45bb521b494b86ee9dbc9c688c@treenet.co.nz>

On 2022-10-24 13:36, Matthew H wrote:
> Hi,
> 
> I'm using Squid to proxy HTTP requests to another proxy. I can see 
> squid
> sending the request to the parent and getting a response, but it sends 
> the
> client that initiated the request a 502 Bad Gateway response.

That is correct behaviour. Squid does not know how to decode the content 
for delivery.

> 
> On closer inspection it appears the parent proxy is sending an
> empty transfer-encoding header, and this is causing Squid to send a 
> 502. Is
> there any way to ignore this?


This MUST NOT be ignored. The server has explicitly indicated that the 
response content area is encoded, but not how. Squid cannot tell where 
the boundaries of the message content are, nor how to transform it for 
delivery to the client.

Amos


From rousskov at measurement-factory.com  Tue Oct 25 13:07:56 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Oct 2022 09:07:56 -0400
Subject: [squid-users] Empty transfer-encoding header causes 502 response
In-Reply-To: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>
References: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>
Message-ID: <42f81feb-2563-9ca7-38cc-e0114a0ca417@measurement-factory.com>

On 10/23/22 20:36, Matthew H wrote:
> Hi,
> 
> I'm using Squid to proxy HTTP requests to another proxy. I can see squid 
> sending the request to the parent and getting a response, but it sends 
> the client that initiated the request a 502 Bad Gateway response.
> 
> On closer inspection it appears the parent proxy is sending an 
> empty?transfer-encoding header, and this is causing Squid to send a 502. 

Do you know whether the response body was using chunked (or any other 
non-identity) encoding? I have already added your case to the list of 
known rejected responses[1], but it would be good to update that with 
the information on the actual response encoding.

[1] https://github.com/squid-cache/squid/pull/702#issuecomment-762459132

If the very first bytes of the response are "<html" or similar, then no 
encoding was probably applied. If you see what can be interpreted as a 
small hex number followed by a new line, then chunked encoding was 
probably applied (at least). If you cannot tell, or are not sure, feel 
free to share the response packet in libpcap format, captured with 
wireshark or "tcpdump -s0".


Thank you,

Alex.



> 2022/10/24 00:23:59.106| ctx: enter level ?0: 'http://nintendo.com/ 
> <http://nintendo.com/>'
> 2022/10/24 00:23:59.106| 11,3| http.cc(666) processReplyHeader: 
> processReplyHeader: key '19010000000000000C00000000000000'
> 2022/10/24 00:23:59.106| 11,2| http.cc(720) processReplyHeader: HTTP 
> Server conn294 local=172.25.0.3:57802 
> <http://172.25.0.3:57802/>?remote=159.203.14.9:1996 
> <http://159.203.14.9:1996/>?FIRSTUP_PARENT FD 26 flags=1
> 2022/10/24 00:23:59.106| 11,2| http.cc(721) processReplyHeader: HTTP 
> Server RESPONSE:
> ---------
> HTTP/1.1 200 OK
> x-powered-by: Express
> content-type: text/html; charset=iso-8859-1
> transfer-encoding:
> date: Mon, 24 Oct 2022 00:23:57 GMT
> connection: close
> 
> ----------
> 2022/10/24 00:23:59.106| 55,3| HttpHeader.cc(882) getList: empty list 
> header: Transfer-Encoding(Transfer-Encoding[63])
> 2022/10/24 00:23:59.106| 55,2| HttpHeader.cc(559) parse: WARNING: 
> unsupported Transfer-Encoding used by client:
> 2022/10/24 00:23:59.106| ctx: exit level ?0
> 2022/10/24 00:23:59.106| 20,3| store.cc(1673) reset: 
> http://nintendo.com/ <http://nintendo.com/>
> 2022/10/24 00:23:59.107| 17,3| FwdState.cc(492) fail: ERR_INVALID_RESP 
> "Bad Gateway"
> http://nintendo.com/ <http://nintendo.com/>
> 2022/10/24 00:23:59.107| 17,3| FwdState.cc(533) unregister: 
> http://nintendo.com/ <http://nintendo.com/>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From gtaylor at tnetconsulting.net  Tue Oct 25 15:47:53 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 09:47:53 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1ehyvnIdIFqw48T@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
Message-ID: <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>

On 10/25/22 2:43 AM, Matus UHLAR - fantomas wrote:
> if by "transparent" you mean "intercepting" proxy, that is incorrect

By "transparent" I mean using network techniques to force clients to use 
a proxy that aren't themselves aware that they are using a proxy.

> CONNECT is HTTP command designed for use with explicit HTTP proxy.

Agreed.

But what does Squid do differently after recognizing the request from 
the client; be it a GET, PUT, POST, or even a CONNECT; the former being 
transparent with the latter being explicit.  Squid will still proxy the 
request as it understands it dependent on configuration, ACLs, etc.

I currently maintain that there is little difference, other than the 
VERB used, between transparent and explicit proxy configuration.  Squid 
still largely does the same thing.

Or said another way, all Squid needed to do to be able to support both 
transparent and explicit was to understand the additional VERBs.  Much 
of the rest of the code was unchanged.

To me there is not a fundamental difference, beyond initial VERBs, for 
transparent and explicit configuration.  At least not anything like the 
differences between FTP, HTTP, and ICP.  Each of which are fundamentally 
different protocols.  Conversely transparent vs explicit is an extension 
of one protocol, namely HTTP.

> ok, there's no explicit need. And since there's no explicit need to use 
> port 80 for HTTP proxy, the convention is to use different port because 
> of reasons stated before.

So port 3128 is based on convention.  And that convention requires more 
explicit configuration in clients.  Okay.  So be it.

> These are the FTP protocol "hacks" I mentioned before.
> The HTTP protocol was created with proxying in mind, FTP was not.
> using specially crafted login name for connecting to anoter server is 
> one of those hacks.

Okay.

I (mis)took "hacks" to be things more severe like is typically done with 
proxifiers used with SOCKS servers, e.g. altering / overloading system 
library calls.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/952d9134/attachment.bin>

From uhlar at fantomas.sk  Tue Oct 25 16:18:33 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 18:18:33 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
Message-ID: <Y1gMWToZVwF8PVOy@fantomas.sk>

>On 10/25/22 2:43 AM, Matus UHLAR - fantomas wrote:
>>if by "transparent" you mean "intercepting" proxy, that is incorrect

On 25.10.22 09:47, Grant Taylor wrote:
>By "transparent" I mean using network techniques to force clients to 
>use a proxy that aren't themselves aware that they are using a proxy.

I prefer to explicitly state what one means by transparent because RFC2616 
has defined transparent proxy diferently:

       A
       "transparent proxy" is a proxy that does not modify the request or
       response beyond what is required for proxy authentication and
       identification.

term "interception proxy" better defines what happens here:

    Instead, an
    interception proxy filters or redirects outgoing TCP port 80 packets
    (and occasionally other common port traffic).

>>CONNECT is HTTP command designed for use with explicit HTTP proxy.
>
>Agreed.
>
>But what does Squid do differently after recognizing the request from 
>the client; be it a GET, PUT, POST, or even a CONNECT; the former 
>being transparent with the latter being explicit.  Squid will still 
>proxy the request as it understands it dependent on configuration, 
>ACLs, etc.

FYI, Intercepting proxy must use measures to avoid host header forgery:

https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
https://www.kb.cert.org/vuls/id/435052

squid must find out the original destination IP used and check, while in 
explicit mode it makes no sense.

>>These are the FTP protocol "hacks" I mentioned before.
>>The HTTP protocol was created with proxying in mind, FTP was not.
>>using specially crafted login name for connecting to anoter server 
>>is one of those hacks.
>
>Okay.
>
>I (mis)took "hacks" to be things more severe like is typically done 
>with proxifiers used with SOCKS servers, e.g. altering / overloading 
>system library calls.

this is a bit different kind of hacks.

Generally the SOCKS library know where/how to connect, socks wrappers (like 
socksify, tsocks, proxychains) are used to make other software use socks 
proxy even if it does not support it.

and of course socks is generic bidiretional tcp/udp proxy, which makes it 
possible to implement it near over any kind of communication.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
LSD will make your ECS screen display 16.7 million colors


From support at unveiltech.com  Tue Oct 25 16:22:17 2022
From: support at unveiltech.com (UnveilTech - Support)
Date: Tue, 25 Oct 2022 16:22:17 +0000
Subject: [squid-users] Squid 5: server_cert_fingerprint not working fine...
Message-ID: <3adf99e022084a76b0f18ffb1f3f605f@unveiltech.com>

Hello,


Here is the part of our squid.conf on Squid 5 :

...

acl my_cf1_list server_cert_fingerprint '/etc/squid5/CF1.txt'

ssl_bump peek all

ssl_bump terminate my_cf1_list

ssl_bump splice all

...



We're not sure about the ssl_bump keys and options to use here, to be honnest we've already spend hours to find the right way to make it working fine.



Here are some samples from the CF1.txt file:

# dayznews.biz

FB:EC:F7:AE:F4:BD:F4:85:68:C0:81:65:99:BA:7D:D3:FA:F8:51:74

# cdeveloper.cn

94:0A:C0:53:A0:E9:74:CE:91:12:6E:FD:06:57:08:58:B2:A5:76:10


1.       Is the server_cert_fingerprint working correctly or are there any bugs with the v5 ?

2.       Are the ssl_bump options/order correct ?

Any tips are welcome, thanks in advance...

Best regards,
Bye Fred
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/1922d0fa/attachment.htm>

From gtaylor at tnetconsulting.net  Tue Oct 25 16:56:06 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 10:56:06 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1gMWToZVwF8PVOy@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
Message-ID: <467c65b2-bd2e-8eb6-4b97-927b44ef982e@spamtrap.tnetconsulting.net>

On 10/25/22 10:18 AM, Matus UHLAR - fantomas wrote:
> I prefer to explicitly state what one means by transparent because 
> RFC2616 has defined transparent proxy diferently:

I do too.  I /thought/ that I was explicitly stating.  At least that was 
my intention.

Aside:  That's why I included my working definition.  So hopefully you 
would know what I meant even if I accidentally used the wrong term.

>> A "transparent proxy" is a proxy that does not modify the request 
>> or response beyond what is required for proxy authentication and 
>> identification.
> 
> term "interception proxy" better defines what happens here:
> 
>> Instead, an interception proxy filters or redirects outgoing TCP port 
>> 80 packets (and occasionally other common port traffic).

It seems as if I should (re)read RFC 2616 and refine my use of terms.

Based on the quoted sections, it seems to me like an intercepting proxy 
is a superset of a transparent proxy.

Aside:  I can see a conceptual way to not modify any of the TCP 
connection (source & destination IPs & ports) while still actively 
proxying the traffic.  --  I don't know if Squid supports this or not. 
But I do see conceptually what would be done.

> FYI, Intercepting proxy must use measures to avoid host header forgery:
> 
> https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery
> https://www.kb.cert.org/vuls/id/435052

I'll have to read those.

> squid must find out the original destination IP used and check, while in 
> explicit mode it makes no sense.

I'll have to think about that.  Probably more so after reading the links 
you provided.

Aside:  I've long been a fan of and preferred explicit client 
configuration to use a proxy.

> this is a bit different kind of hacks.
> 
> Generally the SOCKS library know where/how to connect, socks wrappers 
> (like socksify, tsocks, proxychains) are used to make other software use 
> socks proxy even if it does not support it.

Agreed.

> and of course socks is generic bidiretional tcp/udp proxy, which makes 
> it possible to implement it near over any kind of communication.

Yes, SOCKS is bidirectional.  However, inbound connections through it, 
e.g. FTP active connections, are time limited.  --  At least I'm not 
aware of any way to have a SOCKS proxy allow inbound traffic 
indefinitely a la. port forwarding in NAT or SSH remote port forwarding 
(assuming the real server is the SSH client).



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/fe25d625/attachment.bin>

From uhlar at fantomas.sk  Tue Oct 25 17:03:48 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 19:03:48 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <467c65b2-bd2e-8eb6-4b97-927b44ef982e@spamtrap.tnetconsulting.net>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <467c65b2-bd2e-8eb6-4b97-927b44ef982e@spamtrap.tnetconsulting.net>
Message-ID: <Y1gW9M6LUisFo78c@fantomas.sk>

>On 10/25/22 10:18 AM, Matus UHLAR - fantomas wrote:
>>I prefer to explicitly state what one means by transparent because 
>>RFC2616 has defined transparent proxy diferently:

On 25.10.22 10:56, Grant Taylor wrote:
>I do too.  I /thought/ that I was explicitly stating.  At least that 
>was my intention.
>
>Aside:  That's why I included my working definition.  So hopefully you 
>would know what I meant even if I accidentally used the wrong term.

I think intercepting is better, more precise.

>Based on the quoted sections, it seems to me like an intercepting 
>proxy is a superset of a transparent proxy.

those two are completely separate,
proxy may be intercepting and modify content (e.g. filter), including squid.

>Aside:  I've long been a fan of and preferred explicit client 
>configuration to use a proxy.

yes, especially PAC scripts are great to explicitly state what you need, 
including using socks for other than http(s)/ftp connections (direct 
smtp,imap,pop3 over socks)

>>and of course socks is generic bidiretional tcp/udp proxy, which 
>>makes it possible to implement it near over any kind of 
>>communication.
>
>Yes, SOCKS is bidirectional.  However, inbound connections through it, 
>e.g. FTP active connections, are time limited.  --  At least I'm not 
>aware of any way to have a SOCKS proxy allow inbound traffic 
>indefinitely a la. port forwarding in NAT or SSH remote port 
>forwarding (assuming the real server is the SSH client).

I guess PORT connections have to be allowed on the SOCKS server which is I'd 
say not common (can be dangerous)

passive connections are safe in case of ftp/ssl, where it's impossible to 
know for the proxy/firewall who connects where.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
There's a long-standing bug relating to the x86 architecture that
allows you to install Windows.   -- Matthew D. Fuller


From gtaylor at tnetconsulting.net  Tue Oct 25 18:14:52 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 12:14:52 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1gW9M6LUisFo78c@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <467c65b2-bd2e-8eb6-4b97-927b44ef982e@spamtrap.tnetconsulting.net>
 <Y1gW9M6LUisFo78c@fantomas.sk>
Message-ID: <b4f9bcaf-8c83-567b-f9e7-7b120a510aa2@spamtrap.tnetconsulting.net>

On 10/25/22 11:03 AM, Matus UHLAR - fantomas wrote:
> I think intercepting is better, more precise.

I think that Squid can be an interception proxy as it can filter / alter 
content.

I also think that Squid (as an interception proxy) can be used 
transparently.

> those two are completely separate,

I'm not yet convinced.

> proxy may be intercepting and modify content (e.g. filter), including 
> squid.

I guess it could be said that the transparency, or modification of 
content, is one aspect and that how the client connects to the proxy, 
explicit or implicit (network magic), could be another aspect.

            +-------------+--------+
            | transparent | opaque |
+----------+-------------+--------+
| explicit |      2      |   1    |
+----------+-------------+--------+
| implicit |      3      |   4    |
+----------+-------------+--------+

I believe that Squid can be either transparent and / or opaque depending 
on it's configuration.

I also believe that Squid can be either explicit and / or implicit via 
networking magic.

When I said that intercepting was a superset of transparent, I was 
including all four quadrants.

> yes, especially PAC scripts are great to explicitly state what you need, 
> including using socks for other than http(s)/ftp connections (direct 
> smtp,imap,pop3 over socks)

Yep.

> I guess PORT connections have to be allowed on the SOCKS server which is 
> I'd say not common (can be dangerous)

Yes, the PORT connection must be allowed.  But the problem that I found 
was that the PORT declaration has a timeout / finite time that they 
would wait for connections.  E.g. ten minutes in the example I was 
looking at.

What's more is that the PORT connections must be declared /per/ 
/expected/ /connection/.  They aren't a generic forward traffic from any 
Internet connected system into the SOCKS client.

> passive connections are safe in case of ftp/ssl, where it's impossible 
> to know for the proxy/firewall who connects where.

I don't think that it's impossible.  Rather it's just improbable.  It's 
technically possible to do TLS bump in the wire or other things like 
known keys (non-ephemeral / non-PFS) or sharing ephemeral / PFS keys 
from internal server with TLS monkey in the middle proxy.  Such is 
technically possible, just highly improbable.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/5210b330/attachment.bin>

From gtaylor at tnetconsulting.net  Tue Oct 25 18:52:26 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 12:52:26 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1gMWToZVwF8PVOy@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
Message-ID: <fd6fceae-8d92-0455-932d-22207fe21350@spamtrap.tnetconsulting.net>

On 10/25/22 10:18 AM, Matus UHLAR - fantomas wrote:
> term "interception proxy" better defines what happens here:
> 
> Instead, an interception proxy filters or redirects outgoing TCP port 
> 80 packets (and occasionally other common port traffic).

Where did you pull that quote from?  I don't see "interception" anywhere 
in RFC 2616.

Aside:  I'm thinking that we're having term collisions between "data 
transparency" and "network transparency".  Wherein a data transparent 
proxy doesn't modify the requested content and a network transparent 
proxy is a proxy that the client isn't aware that it's using.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/9a7f6424/attachment.bin>

From uhlar at fantomas.sk  Tue Oct 25 18:57:59 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 20:57:59 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <b4f9bcaf-8c83-567b-f9e7-7b120a510aa2@spamtrap.tnetconsulting.net>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <467c65b2-bd2e-8eb6-4b97-927b44ef982e@spamtrap.tnetconsulting.net>
 <Y1gW9M6LUisFo78c@fantomas.sk>
 <b4f9bcaf-8c83-567b-f9e7-7b120a510aa2@spamtrap.tnetconsulting.net>
Message-ID: <Y1gxt7xXve7L5YEy@fantomas.sk>

>On 10/25/22 11:03 AM, Matus UHLAR - fantomas wrote:
>>I think intercepting is better, more precise.

On 25.10.22 12:14, Grant Taylor wrote:
>I think that Squid can be an interception proxy as it can filter / 
>alter content.
>
>I also think that Squid (as an interception proxy) can be used 
>transparently.

intercepting connections and modifying content are two separate 
functionalities, and "transparent proxy" is in RFC2616 defined as "not doing 
the latter" while many people understand it as "doing the former".

That is why I prefer using "intercepting proxy" for case where connections 
between clients and servers intercepted by proxy, without it being 
configured in browsers.

>>those two are completely separate,
>
>I'm not yet convinced.

both functionalities described above are independent on each other, squid 
also supports both separately, which gives us four combinations.

>>proxy may be intercepting and modify content (e.g. filter), 
>>including squid.
>
>I guess it could be said that the transparency, or modification of 
>content, is one aspect and that how the client connects to the proxy, 
>explicit or implicit (network magic), could be another aspect.
>
>           +-------------+--------+
>           | transparent | opaque |
>+----------+-------------+--------+
>| explicit |      2      |   1    |
>+----------+-------------+--------+
>| implicit |      3      |   4    |
>+----------+-------------+--------+
>
>I believe that Squid can be either transparent and / or opaque 
>depending on it's configuration.

precisely, so what exactly aren't you convinced about? :-)

>I also believe that Squid can be either explicit and / or implicit via 
>networking magic.

>When I said that intercepting was a superset of transparent, I was 
>including all four quadrants.

I guess intercepting is what you have in second row, while transparent is 
the first column, that doesn't seem as superset to me.

>>I guess PORT connections have to be allowed on the SOCKS server 
>>which is I'd say not common (can be dangerous)
>
>Yes, the PORT connection must be allowed.  But the problem that I 
>found was that the PORT declaration has a timeout / finite time that 
>they would wait for connections.  E.g. ten minutes in the example I 
>was looking at.

Have you noticed this with SOCKS server?

I guess this applies for firewalls that will disable connections to the port 
later.  But the same applies for PASV connections and the reply when 
firewall at serer side is used.

>What's more is that the PORT connections must be declared /per/ 
>/expected/ /connection/.  They aren't a generic forward traffic from 
>any Internet connected system into the SOCKS client.
>
>>passive connections are safe in case of ftp/ssl, where it's 
>>impossible to know for the proxy/firewall who connects where.
>
>I don't think that it's impossible.  Rather it's just improbable.  

When ssl/tls is used between client and server, intermediate gateways and 
firewalls don't know what ports do endpoints agree on using PORT/PASV.

Unless they intercept SSL conneciton (which kinf od makes them FTP 
endpoints) or the client supports and issues FTP command "CCC" which is 
designed for this case.  I'm afraid not many FTP clients do that.

>It's technically possible to do TLS bump in the wire or other things 
>like known keys (non-ephemeral / non-PFS) or sharing ephemeral / PFS 
>keys from internal server with TLS monkey in the middle proxy.  Such 
>is technically possible, just highly improbable.

agree.

the workaround is to use static list of ports at server side and configure 
server firewall to statically allow connection to these ports (optionally 
NAT them).

however this is already not a SQUID issue.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I intend to live forever - so far so good.


From uhlar at fantomas.sk  Tue Oct 25 19:01:34 2022
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Oct 2022 21:01:34 +0200
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <fd6fceae-8d92-0455-932d-22207fe21350@spamtrap.tnetconsulting.net>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <fd6fceae-8d92-0455-932d-22207fe21350@spamtrap.tnetconsulting.net>
Message-ID: <Y1gyjggJ2BYUllu2@fantomas.sk>

>On 10/25/22 10:18 AM, Matus UHLAR - fantomas wrote:
>>term "interception proxy" better defines what happens here:
>>
>>Instead, an interception proxy filters or redirects outgoing TCP 
>>port 80 packets (and occasionally other common port traffic).

On 25.10.22 12:52, Grant Taylor wrote:
>Where did you pull that quote from?  I don't see "interception" 
>anywhere in RFC 2616.

sorry, this one is from 7230, section 2.3

>Aside:  I'm thinking that we're having term collisions between "data 
>transparency" and "network transparency".  Wherein a data transparent 
>proxy doesn't modify the requested content and a network transparent 
>proxy is a proxy that the client isn't aware that it's using.

If we don't use "data" and "network" in addition to "transparent", result is 
ambiguous.  "intercepting proxy" is not.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Support bacteria - they're the only culture some people have.


From gtaylor at tnetconsulting.net  Tue Oct 25 19:07:17 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 13:07:17 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1gxt7xXve7L5YEy@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <467c65b2-bd2e-8eb6-4b97-927b44ef982e@spamtrap.tnetconsulting.net>
 <Y1gW9M6LUisFo78c@fantomas.sk>
 <b4f9bcaf-8c83-567b-f9e7-7b120a510aa2@spamtrap.tnetconsulting.net>
 <Y1gxt7xXve7L5YEy@fantomas.sk>
Message-ID: <fe36f2a3-afea-5976-02af-ea5f96b50d8c@spamtrap.tnetconsulting.net>

On 10/25/22 12:57 PM, Matus UHLAR - fantomas wrote:
> That is why I prefer using "intercepting proxy" for case where 
> connections between clients and servers intercepted by proxy, without it 
> being configured in browsers.

Fair enough.

> precisely, so what exactly aren't you convinced about? :-)

The term "transparent" having multiple meanings.

I believe we were talking past each other and now are not.

> Have you noticed this with SOCKS server?

Yes, DANTE SOCKS server is exactly where I first read about the 
limitation that I'm talking about.  Subsequent reading of other SOCKS 
servers supported this limitation.

N.B. I'm specifically talking about how a SOCKS aware (FTP) client can 
ask that an external port be connected to the SOCKS client for a defined 
period of time (ten minutes in the examples I saw).  This is sufficient 
for most active FTP connections (presuming that the ftp client is also 
the socks client) as the data connection from the FTP server comes back 
to the SOCKS server ~> FTP client in short order.

> I guess this applies for firewalls that will disable connections to the 
> port later.? But the same applies for PASV connections and the reply 
> when firewall at serer side is used.

Agreed.

Aside:  I don't think I've ever seen SOCKS be used to front public 
services.  Rather I've only ever seen SOCKS used for (private) clients.

> When ssl/tls is used between client and server, intermediate gateways 
> and firewalls don't know what ports do endpoints agree on using PORT/PASV.
> 
> Unless they intercept SSL conneciton (which kind of makes them FTP 
> endpoints) or the client supports and issues FTP command "CCC" which is 
> designed for this case.? I'm afraid not many FTP clients do that.

Agreed.

I think this middle box behavior is far more common on HTTPS in larger 
data centers where the middle box is used to enforce compliance and the 
likes.

> agree.
> 
> the workaround is to use static list of ports at server side and 
> configure server firewall to statically allow connection to these ports 
> (optionally NAT them).

Yep.

> however this is already not a SQUID issue.

Agreed.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/fa705b4b/attachment.bin>

From gtaylor at tnetconsulting.net  Tue Oct 25 19:09:30 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 13:09:30 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1gyjggJ2BYUllu2@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <fd6fceae-8d92-0455-932d-22207fe21350@spamtrap.tnetconsulting.net>
 <Y1gyjggJ2BYUllu2@fantomas.sk>
Message-ID: <fde1e749-48f2-4861-6341-b72a11720664@spamtrap.tnetconsulting.net>

On 10/25/22 1:01 PM, Matus UHLAR - fantomas wrote:
> sorry, this one is from 7230, section 2.3

Thank you for the reference.

> If we don't use "data" and "network" in addition to "transparent", 
> result is ambiguous.? "intercepting proxy" is not.

Agreed.

It seems as if "transparent" in the context of proxies is as ambiguous 
as "secure" in the context of VPNs.

The former can be "data transparent" and / or "network transparent". 
The latter can be "privacy secure" and / or "integrity secure".  }:-)



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/0c5135e3/attachment.bin>

From squid at marrold.co.uk  Wed Oct 26 00:55:20 2022
From: squid at marrold.co.uk (Matthew H)
Date: Wed, 26 Oct 2022 01:55:20 +0100
Subject: [squid-users] Empty transfer-encoding header causes 502 response
In-Reply-To: <42f81feb-2563-9ca7-38cc-e0114a0ca417@measurement-factory.com>
References: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>
 <42f81feb-2563-9ca7-38cc-e0114a0ca417@measurement-factory.com>
Message-ID: <CAC-Lcd9dP-Li73jT=9aXfUPxVq-6kYVhMZa54xK_Q+_RkvqT8A@mail.gmail.com>

Hi all,

Thanks for the replies.

I have included the requested output from tcpdump below:

 tcpdump -A -s 0 -ni enp4s0 "host 159.203.14.9 and (((ip[2:2] -
((ip[0]&0xf)<<2)) - ((tcp[12]&
0xf0)>>2)) != 0)"
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on enp4s0, link-type EN10MB (Ethernet), snapshot length 262144
bytes

01:40:17.310479 IP 10.0.160.10.43426 > 159.203.14.9.1996: Flags [P.], seq
2955630477:2955630939, ack 2382737005, win 502, options [nop,nop,TS val
3000375654 ecr 1932743995], length 462
E....:@.?.7.
..
...     .....+W....m....Y......
...fs3U;GET http://nintendo.com/ HTTP/1.1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0)
Gecko/20100101 Firefox/106.0
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8
Accept-Language: en-GB,en;q=0.5
Accept-Encoding: gzip, deflate
Upgrade-Insecure-Requests: 1
Host: nintendo.com
Via: 1.1 dce3749b9671 (squid/5.6)
X-Forwarded-For: 10.0.130.210
Cache-Control: max-age=259200
Connection: keep-alive


01:40:18.957475 IP 159.203.14.9.1996 > 10.0.160.10.43426: Flags [P.], seq
1:1466, ack 462, win 114, options [nop,nop,TS val 1932744407 ecr
3000375654], length 1465
E....@@.3......
..
.......m.+Y[...r]......
s3V....fHTTP/1.1 200 OK
x-powered-by: Express
content-type: text/html; charset=iso-8859-1
transfer-encoding:
date: Wed, 26 Oct 2022 00:40:20 GMT
connection: close

<!DOCTYPE html PUBLIC "-//IETF//DTD HTML 2.0//EN"><html><head>
<!-- this automatically loads the hallway after 20 seconds -->
<meta http-equiv="refresh" content="20; url=
http://nintendo.com//./hallway/index.html">
<title>Nintendo Power Source</title>



On Tue, Oct 25, 2022 at 2:08 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 10/23/22 20:36, Matthew H wrote:
> > Hi,
> >
> > I'm using Squid to proxy HTTP requests to another proxy. I can see squid
> > sending the request to the parent and getting a response, but it sends
> > the client that initiated the request a 502 Bad Gateway response.
> >
> > On closer inspection it appears the parent proxy is sending an
> > empty transfer-encoding header, and this is causing Squid to send a 502.
>
> Do you know whether the response body was using chunked (or any other
> non-identity) encoding? I have already added your case to the list of
> known rejected responses[1], but it would be good to update that with
> the information on the actual response encoding.
>
> [1] https://github.com/squid-cache/squid/pull/702#issuecomment-762459132
>
> If the very first bytes of the response are "<html" or similar, then no
> encoding was probably applied. If you see what can be interpreted as a
> small hex number followed by a new line, then chunked encoding was
> probably applied (at least). If you cannot tell, or are not sure, feel
> free to share the response packet in libpcap format, captured with
> wireshark or "tcpdump -s0".
>
>
> Thank you,
>
> Alex.
>
>
>
> > 2022/10/24 00:23:59.106| ctx: enter level  0: 'http://nintendo.com/
> > <http://nintendo.com/>'
> > 2022/10/24 00:23:59.106| 11,3| http.cc(666) processReplyHeader:
> > processReplyHeader: key '19010000000000000C00000000000000'
> > 2022/10/24 00:23:59.106| 11,2| http.cc(720) processReplyHeader: HTTP
> > Server conn294 local=172.25.0.3:57802
> > <http://172.25.0.3:57802/> remote=159.203.14.9:1996
> > <http://159.203.14.9:1996/> FIRSTUP_PARENT FD 26 flags=1
> > 2022/10/24 00:23:59.106| 11,2| http.cc(721) processReplyHeader: HTTP
> > Server RESPONSE:
> > ---------
> > HTTP/1.1 200 OK
> > x-powered-by: Express
> > content-type: text/html; charset=iso-8859-1
> > transfer-encoding:
> > date: Mon, 24 Oct 2022 00:23:57 GMT
> > connection: close
> >
> > ----------
> > 2022/10/24 00:23:59.106| 55,3| HttpHeader.cc(882) getList: empty list
> > header: Transfer-Encoding(Transfer-Encoding[63])
> > 2022/10/24 00:23:59.106| 55,2| HttpHeader.cc(559) parse: WARNING:
> > unsupported Transfer-Encoding used by client:
> > 2022/10/24 00:23:59.106| ctx: exit level  0
> > 2022/10/24 00:23:59.106| 20,3| store.cc(1673) reset:
> > http://nintendo.com/ <http://nintendo.com/>
> > 2022/10/24 00:23:59.107| 17,3| FwdState.cc(492) fail: ERR_INVALID_RESP
> > "Bad Gateway"
> > http://nintendo.com/ <http://nintendo.com/>
> > 2022/10/24 00:23:59.107| 17,3| FwdState.cc(533) unregister:
> > http://nintendo.com/ <http://nintendo.com/>
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221026/4cb8c547/attachment.htm>

From admin at sneakerspace.co.uk  Wed Oct 26 01:27:01 2022
From: admin at sneakerspace.co.uk (Sneaker Space LTD)
Date: Wed, 26 Oct 2022 02:27:01 +0100
Subject: [squid-users] ACL based DNS server list
Message-ID: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>

Hello,

Is there a way to use specific DNS servers based on the user or connecting
IP address that is making the connection by using acls or any other method?
If so, can someone send an example.

Thanks,
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221026/f69112f5/attachment.htm>

From gtaylor at tnetconsulting.net  Wed Oct 26 02:23:27 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 20:23:27 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <fde1e749-48f2-4861-6341-b72a11720664@spamtrap.tnetconsulting.net>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
 <c9f13b06-b0ca-1f0b-d492-75589eab7fec@spamtrap.tnetconsulting.net>
 <Y1gMWToZVwF8PVOy@fantomas.sk>
 <fd6fceae-8d92-0455-932d-22207fe21350@spamtrap.tnetconsulting.net>
 <Y1gyjggJ2BYUllu2@fantomas.sk>
 <fde1e749-48f2-4861-6341-b72a11720664@spamtrap.tnetconsulting.net>
Message-ID: <86566c54-70c5-580d-55a3-5da7b0402895@spamtrap.tnetconsulting.net>

On 10/25/22 1:09 PM, Grant Taylor wrote:
> It seems as if "transparent" in the context of proxies is as ambiguous 
> as "secure" in the context of VPNs.
> 
> The former can be "data transparent" and / or "network transparent". The 
> latter can be "privacy secure" and / or "integrity secure".? }:-)

Oy vey.

For completeness -- I've continued reading -- RFC 1919: Classical verses 
Transparent IP Proxies ? 4 -- Transparent application proxies -- ? 3 
starts with:

"A transparent application proxy is often described as a system that 
appears like a packet filter to clients, and like a classical proxy to 
servers."

So as I read it, RFC 1919 ? 4 ? 3 supports "network transparency".

Then it continues with:

"Apart from this important concept, transparent and classical proxies 
can do similar access control checks and can offer an equivalent level 
of security/robustness/performance, at least as far as the proxy itself 
is concerned."

Which reads as if /network/ transparent proxies can be /data/ 
non-transparent.

Nomenclature and consistent definitions can be hard and can easily 
sideline discussions.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/a5b3c1e1/attachment.bin>

From gtaylor at tnetconsulting.net  Wed Oct 26 02:56:01 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Tue, 25 Oct 2022 20:56:01 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <Y1ehyvnIdIFqw48T@fantomas.sk>
References: <Y1ehyvnIdIFqw48T@fantomas.sk>
Message-ID: <dda3a340-3a55-21c8-5557-fc2eced20653@spamtrap.tnetconsulting.net>

On 10/25/22 2:43 AM, Matus UHLAR - fantomas wrote:
> These are the FTP protocol "hacks" I mentioned before.

FYI RFC 1919: Classical verses Transparent IP Proxies ? 4.1 -- 
Transparent proxy connection example -- describes the operation of an 
intercepting / (network) transparent FTP proxy that does not require any 
FTP protocol hacks.  }:-)



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221025/1f825288/attachment.bin>

From odhiambo at gmail.com  Wed Oct 26 07:08:11 2022
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Wed, 26 Oct 2022 10:08:11 +0300
Subject: [squid-users] ACL based DNS server list
In-Reply-To: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
References: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
Message-ID: <CAAdA2WOMKD=8CzCzc9d5i6WaNG6cP1bD-V3+xKomZiFqGbOLTw@mail.gmail.com>

On Wed, Oct 26, 2022 at 4:27 AM Sneaker Space LTD <admin at sneakerspace.co.uk>
wrote:

> Hello,
>
> Is there a way to use specific DNS servers based on the user or connecting
> IP address that is making the connection by using acls or any other method?
> If so, can someone send an example.
>

If you are using BIND, you can always use the "VIEWS" feature, but I think
this has to be done outside Squid.
However, nothing is impossible in this world except for changing the value
of Pi from 3.14-something  :)

-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft.", egrep -v '^$|^.*#' ?\_(?)_/? :-)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221026/586da06b/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 26 13:20:14 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 26 Oct 2022 09:20:14 -0400
Subject: [squid-users] Empty transfer-encoding header causes 502 response
In-Reply-To: <CAC-Lcd9dP-Li73jT=9aXfUPxVq-6kYVhMZa54xK_Q+_RkvqT8A@mail.gmail.com>
References: <CAC-Lcd_Vevs8qfan=05fnzLn9aw_KV8kB7FeX7VrOd9zBiRukA@mail.gmail.com>
 <42f81feb-2563-9ca7-38cc-e0114a0ca417@measurement-factory.com>
 <CAC-Lcd9dP-Li73jT=9aXfUPxVq-6kYVhMZa54xK_Q+_RkvqT8A@mail.gmail.com>
Message-ID: <c448aa28-617e-1ec5-a24e-39a757421cc9@measurement-factory.com>

On 10/25/22 20:55, Matthew H wrote:

> I have included the requested output from tcpdump below:

Thank you! This raw output is sufficient to determine that no transfer 
encoding was used by this buggy origin server. I have updated the GitHub 
comment/summary accordingly.

N.B. In the future, please consider sharing libpcap packet captures 
instead of raw tcpdump console output. It is not necessary to re-share 
anything now.

FWIW, I am not aware of any official Squid workarounds for this origin 
server bug. Some of the features Factory is currently working on will be 
useful here, but they are not yet ready for the official submission. One 
can remove the corresponding check from Squid source code, of course, 
but doing so will open modified Squid (and other HTTP agents) to serious 
security vulnerabilities, so I cannot recommend such a blunt workaround.

Alex.


>  ?tcpdump -A -s 0 -ni enp4s0 "host 159.203.14.9 and (((ip[2:2] - 
> ((ip[0]&0xf)<<2)) - ((tcp[12]&
> 0xf0)>>2)) != 0)"
> tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
> listening on enp4s0, link-type EN10MB (Ethernet), snapshot length 262144 
> bytes
> 
> 01:40:17.310479 IP 10.0.160.10.43426 > 159.203.14.9.1996: Flags [P.], 
> seq 2955630477:2955630939, ack 2382737005, win 502, options [nop,nop,TS 
> val 3000375654 ecr 1932743995], length 462
> E....:@.?.7.
> ..
> ... ? ? .....+W....m....Y......
> ...fs3U;GET http://nintendo.com/ <http://nintendo.com/> HTTP/1.1
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:106.0) 
> Gecko/20100101 Firefox/106.0
> Accept: 
> text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8
> Accept-Language: en-GB,en;q=0.5
> Accept-Encoding: gzip, deflate
> Upgrade-Insecure-Requests: 1
> Host: nintendo.com <http://nintendo.com>
> Via: 1.1 dce3749b9671 (squid/5.6)
> X-Forwarded-For: 10.0.130.210
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> 01:40:18.957475 IP 159.203.14.9.1996 > 10.0.160.10.43426: Flags [P.], 
> seq 1:1466, ack 462, win 114, options [nop,nop,TS val 1932744407 ecr 
> 3000375654], length 1465
> E....@@.3......
> ..
> .......m.+Y[...r]......
> s3V....fHTTP/1.1 200 OK
> x-powered-by: Express
> content-type: text/html; charset=iso-8859-1
> transfer-encoding:
> date: Wed, 26 Oct 2022 00:40:20 GMT
> connection: close
> 
> <!DOCTYPE html PUBLIC "-//IETF//DTD HTML 2.0//EN"><html><head>
> <!-- this automatically loads the hallway after 20 seconds -->
> <meta http-equiv="refresh" content="20; 
> url=http://nintendo.com//./hallway/index.html 
> <http://nintendo.com//./hallway/index.html>">
> <title>Nintendo Power Source</title>
> 
> 
> 
> On Tue, Oct 25, 2022 at 2:08 PM Alex Rousskov 
> <rousskov at measurement-factory.com 
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     On 10/23/22 20:36, Matthew H wrote:
>      > Hi,
>      >
>      > I'm using Squid to proxy HTTP requests to another proxy. I can
>     see squid
>      > sending the request to the parent and getting a response, but it
>     sends
>      > the client that initiated the request a 502 Bad Gateway response.
>      >
>      > On closer inspection it appears the parent proxy is sending an
>      > empty?transfer-encoding header, and this is causing Squid to send
>     a 502.
> 
>     Do you know whether the response body was using chunked (or any other
>     non-identity) encoding? I have already added your case to the list of
>     known rejected responses[1], but it would be good to update that with
>     the information on the actual response encoding.
> 
>     [1]
>     https://github.com/squid-cache/squid/pull/702#issuecomment-762459132
>     <https://github.com/squid-cache/squid/pull/702#issuecomment-762459132>
> 
>     If the very first bytes of the response are "<html" or similar, then no
>     encoding was probably applied. If you see what can be interpreted as a
>     small hex number followed by a new line, then chunked encoding was
>     probably applied (at least). If you cannot tell, or are not sure, feel
>     free to share the response packet in libpcap format, captured with
>     wireshark or "tcpdump -s0".
> 
> 
>     Thank you,
> 
>     Alex.
> 
> 
> 
>      > 2022/10/24 00:23:59.106| ctx: enter level ?0:
>     'http://nintendo.com/ <http://nintendo.com/>
>      > <http://nintendo.com/ <http://nintendo.com/>>'
>      > 2022/10/24 00:23:59.106| 11,3| http.cc(666) processReplyHeader:
>      > processReplyHeader: key '19010000000000000C00000000000000'
>      > 2022/10/24 00:23:59.106| 11,2| http.cc(720) processReplyHeader: HTTP
>      > Server conn294 local=172.25.0.3:57802 <http://172.25.0.3:57802>
>      > <http://172.25.0.3:57802/
>     <http://172.25.0.3:57802/>>?remote=159.203.14.9:1996
>     <http://159.203.14.9:1996>
>      > <http://159.203.14.9:1996/
>     <http://159.203.14.9:1996/>>?FIRSTUP_PARENT FD 26 flags=1
>      > 2022/10/24 00:23:59.106| 11,2| http.cc(721) processReplyHeader: HTTP
>      > Server RESPONSE:
>      > ---------
>      > HTTP/1.1 200 OK
>      > x-powered-by: Express
>      > content-type: text/html; charset=iso-8859-1
>      > transfer-encoding:
>      > date: Mon, 24 Oct 2022 00:23:57 GMT
>      > connection: close
>      >
>      > ----------
>      > 2022/10/24 00:23:59.106| 55,3| HttpHeader.cc(882) getList: empty
>     list
>      > header: Transfer-Encoding(Transfer-Encoding[63])
>      > 2022/10/24 00:23:59.106| 55,2| HttpHeader.cc(559) parse: WARNING:
>      > unsupported Transfer-Encoding used by client:
>      > 2022/10/24 00:23:59.106| ctx: exit level ?0
>      > 2022/10/24 00:23:59.106| 20,3| store.cc(1673) reset:
>      > http://nintendo.com/ <http://nintendo.com/> <http://nintendo.com/
>     <http://nintendo.com/>>
>      > 2022/10/24 00:23:59.107| 17,3| FwdState.cc(492) fail:
>     ERR_INVALID_RESP
>      > "Bad Gateway"
>      > http://nintendo.com/ <http://nintendo.com/> <http://nintendo.com/
>     <http://nintendo.com/>>
>      > 2022/10/24 00:23:59.107| 17,3| FwdState.cc(533) unregister:
>      > http://nintendo.com/ <http://nintendo.com/> <http://nintendo.com/
>     <http://nintendo.com/>>
>      >
>      > _______________________________________________
>      > squid-users mailing list
>      > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 



From rousskov at measurement-factory.com  Wed Oct 26 13:25:06 2022
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 26 Oct 2022 09:25:06 -0400
Subject: [squid-users] ACL based DNS server list
In-Reply-To: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
References: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
Message-ID: <fdde4d7e-6c6d-add0-5145-807eb2679d30@measurement-factory.com>

On 10/25/22 21:27, Sneaker Space LTD wrote:

> Is there a way to use specific?DNS servers based on the user or 
> connecting IP address that is making the connection by using?acls or any 
> other method? If so, can someone send an example.

One can write an external ACL helper that will use whatever DNS servers 
it wants. The helper can receive the user name or client-Squid 
connection IP address(es) as transaction input. The helper can make 
match/mismatch decisions and can annotate the transaction as needed.

There is currently no way to configure Squid to select DNS resolvers 
based on transaction properties.


HTH,

Alex.



From ifoolb at gmail.com  Wed Oct 26 16:43:08 2022
From: ifoolb at gmail.com (mingheng wang)
Date: Thu, 27 Oct 2022 00:43:08 +0800
Subject: [squid-users] Does Squid support client ssl termination?
Message-ID: <CAB-cPApGGo1zHR1hVdVUGGLnLA1Ciy3hF1J+9L3a6O1Ap=0r-A@mail.gmail.com>

Hello all,
  Since ssl_bump can generate self signed certificates on the fly, I wonder
if this setup is possible, or even just in theory:
clients with necessary root CA installed connect to a local Squid. With
ssl_bump and self signed certs, it always talks with the clients over
HTTPS, making clients believe their connections are secure; the local Squid
then forwards the connections to a parent Squid server, which however, will
only send data back in plain HTTP, i.e. in clear text, akin to a reverse
proxy with ssl termination to its proxied site.

  my goals are to cache data/modify requests even when connecting to https
only sites, while avoiding using self signed certs to encrypt connections
over the Internet, because this way, I can chain an https proxy with
trusted certs in between.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221027/bf676981/attachment.htm>

From gtaylor at tnetconsulting.net  Wed Oct 26 17:38:01 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Wed, 26 Oct 2022 11:38:01 -0600
Subject: [squid-users] Does Squid support client ssl termination?
In-Reply-To: <CAB-cPApGGo1zHR1hVdVUGGLnLA1Ciy3hF1J+9L3a6O1Ap=0r-A@mail.gmail.com>
References: <CAB-cPApGGo1zHR1hVdVUGGLnLA1Ciy3hF1J+9L3a6O1Ap=0r-A@mail.gmail.com>
Message-ID: <f1a6b704-38c0-321b-486c-1dfa01ed5c3c@spamtrap.tnetconsulting.net>

On 10/26/22 10:43 AM, mingheng wang wrote:
> Hello all,

Hi,

>  ? Since ssl_bump can generate self signed certificates on the fly, I 
> wonder if this setup is possible, or even just in theory:
> clients with necessary root CA installed connect to a local Squid. With 
> ssl_bump and self signed certs,

I'm with you so far.  I've got such a Monkey in the Middle here at the 
house.

> it always talks with the clients over HTTPS,

Please clarify / confirm if you're talking about HTTPS protection of the 
client to squid connection.  --  I ask because not all clients natively 
/ easily support HTTPS connection to Squid.

N.B. the connection between the client and Squid is completely 
independent of the connection between Squid and the next upstream server.

> making clients believe their connections are secure;

This is the biggest hang up for me.  --  I don't think that the HTTPS 
communications with Squid in and of itself will cause clients to think 
that an insecure site is actually secure.

My client doesn't show that it has a secure connection to neverssl.com 
which doesn't support HTTPS (by design) despite communicating with Squid 
via HTTPS.

> the local Squid then forwards the connections to a parent Squid server, 
> which however, will only send data back in plain HTTP, i.e. in clear 
> text, akin to a reverse proxy with ssl termination to its proxied site.

Okay.  I'm not sure why you would not have encryption on the downstream 
child Squid to the upstream parent Squid, but that's your choice.

>  ? my goals are to cache data/modify requests even when connecting to 
> https only sites,

Squid's TLS Monkey in the Middle should cache things without any 
problem.  So I don't see the need to do anything extra for this.

> while avoiding using self signed certs to encrypt connections over the 
> Internet,

I have no idea where the downstream child Squid is that's doing TLS 
MitM.  Nor do I have any idea where the upstream parent Squid is.  So I 
can't really comment about locality / Internet.

> because this way, I can chain an https proxy with trusted certs 
> in between.

"Trusted certs" is sort of ambiguous in this case as your TLS MitM 
/clients/ *trust* the root cert that the downstream child Squid is using.

I see no reason why you can't use similar methodology to protect the 
communications between the downstream child Squid to the upstream parent 
Squid.  --  Independent of who the cert used by the upstream parent 
Squid is from.

If the downstream child Squid has the root CA that signed the upstream 
parent Squid's TLS certificate in the downstream child Squid root CA 
store, then the connection between the two Squids is trusted.  Even if 
there are no public CAs involved.  }:-)



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221026/e0d0181d/attachment.bin>

From squid3 at treenet.co.nz  Sun Oct 30 12:59:00 2022
From: squid3 at treenet.co.nz (squid3 at treenet.co.nz)
Date: Mon, 31 Oct 2022 01:59:00 +1300
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <2f52e708-b1d8-ad6d-c9ea-62c3c5cf2965@spamtrap.tnetconsulting.net>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <6774a0af-b9bd-1aed-7474-1f439c05cd7c@measurement-factory.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>
 <0dc84627-5af0-78e1-9fe8-9a8329db92e1@spamtrap.tnetconsulting.net>
 <7b177298-2613-9215-2a99-b322e8b7c8e0@treenet.co.nz>
 <2f52e708-b1d8-ad6d-c9ea-62c3c5cf2965@spamtrap.tnetconsulting.net>
Message-ID: <19fa29f8ddf517b0e38d73a7e6b0ca55@treenet.co.nz>

On 2022-10-23 06:10, Grant Taylor wrote:
> On 10/21/22 11:30 PM, Amos Jeffries wrote:
>> Not just convention. AFAICT was formally registered with W3C, before 
>> everyone went to using IETF for registrations.
> 
> Please elaborate on what was formally registered.  I've only seen 3128 
> / 3129 be the default for Squid (and a few things emulating squid).  
> Other proxies of the time, namely Netscape's and Microsoft's 
> counterparts, tended to use 8080.
> 
> I'd genuinely like to learn more about and understand the history / 
> etymology / genesis of the 3128 / 3129.

Duane W. would be the best one to ask about the details.

What I know is that some 10-12 years ago I discovered an message by 
Duane mentioning that W3C had (given or accepted) port 3128 for Squid 
use. I've checked the squid-cache archives and not seeing the message.

Right now it looks like the W3C changed their systems and only track the 
standards documents. So I cannot reference their (outdated?) protocol 
registry :-{ . Also checked the squid-cache archives and not finding it 
email history. Sorry.


> 
>> FYI, discussion started ~30 years ago.
> 
> ACK
> 
>> The problem:
>> 
>> For bandwidth savings HTTP/1.0 defined different URL syntax for origin 
>> and relay/proxy requests. The form sent to an origin server lacks any 
>> information about the authority. That was expected to be known 
>> out-of-band by the origin itself.
>> 
>> HTTP/1.1 has attempted several different mechanisms to fix this over 
>> the years. None of them has been universally accepted, so the problem 
>> remains. The best we have is mandatory Host header which most (but 
>> sadly not all) clients and servers use.
>> 
>> HTTP/2 cements that design with mandatory ":authority" pseudo-header 
>> field. So the problem is "fixed"for native HTTP/2+ traffic. But until 
>> HTTP/1.0 and broken HTTP/1.1 clients are all gone the issue will still 
>> crop up.
> 
> I'm not entirely sure what you mean by "the authority".  I'm taking it 
> to mean the identity of the service that you are wanting content from. 
> The Host: header comment with HTTP/1.1 is what makes me think this.
> 

I mean "authority" as used by HTTP specification, which refers to 
https://www.rfc-editor.org/rfc/rfc3986#section-3.2


> My understanding is that neither HTTP/0.9 nor HTTP/1.0 had a Host: 
> header and that it was assumed that the IP address you were connecting 
> to conveyed the server that you were wanting to connect to.

Yes exactly. That is the source of the problem, perpetuated by the need 
to retain on-wire byte/octet backward compatibility until HTTP/2 changed 
to binary format.

Consider what the proxy has to do when (not if) the IP:port being 
connected to are that proxy's (eg localhost:80) and the URL is only a 
path ("/") on an origin server somewhere else. Does the "GET / HTTP/1.0" 
mean "http://example.com/" or "http://example.net/" ?


> 
>> More importantly the proxy hostname:port the client is opening TCP 
>> connections to may be different from the authority-info specified in 
>> the HTTP request message (or lack thereof).
> 
> My working understanding of what the authority is seems to still work 
> with this.
> 

The key point is that the proxy host:port and the origin host:port are 
two different authority and only the origin may be passed along in the 
URL (or URL+Host header). When the client uses port 80 and 443 thinking 
they are origin services it is *required* (per 
https://www.rfc-editor.org/rfc/rfc9112.html#name-origin-form) to omit 
the real origins info. Enter problems.


>> This crosses security boundaries and involves out-of-band information 
>> sources at all three endpoints involved in the transaction for the 
>> message semantics and protocol negotiations to work properly.
> 
> I feel like the nature of web traffic tends to frequently, but not 
> always, cross security / administrative boundaries.  As such, I don't 
> think that existence of proxies in the communications path alters 
> things much.
> 
> Please elaborate on what out-of-band information you are describing. 
> The most predominant thing that comes to mind, particularly with 
> HTTP/1.1 and HTTP/2 is name resolution -- ostensibly DNS -- to identify 
> the IP address to connect to.
> 

I refer to all the many ways the clients may be explicitly or implicitly 
configured to be aware that it is talking to a proxy - such that it 
explicitly avoids sending the problematic origin-form URLs.


>> What that text does not say is that when they are omitted by the 
>> **user** they are taken from configuration settings in the OS:
>> 
>>  ?* the environment variable name provides:
>>  ??? - the protocol name ("http" or "HTTPS", aka plain-text or 
>> encrypted)
>>  ??? - the expected protocol syntax/semantics ("proxy" aka 
>> forward-proxy)
>> 
>>  ?* the machine /etc/services configuration provides the default port 
>> for the named protocol.
> 
> Ergo the use of /default/ values when values are not specified.

The defaults though are tuned for origin server (or reverse-proxy) 
direct contact.
No Browser I know supports 
"http-alt://proxy.example.com?http://origin.example.net/index.html" 
URLs.


> 
> I feel like this in a round about way supports my stance that the 
> default ports are perfectly fine to use.
> 

... "at your own risk" they technically might be. So long as you only 
receive one of the three types of syntax there - port 80/443 being 
officially registered for origin / reverse-proxy syntax.


>> Attempting to use a reverse-proxy or origin server such a 
>> configuration may work for some messages, but **will** fail due to 
>> syntax or semantic errors on others.
> 
> I question the veracity of that statement.


It is based on experience. Squid used to be a lot more lenient and tried 
for decades to do the syntax auto-detection. The path from that to 
separate ports is littered with CVEs. Most notably the curse that keeps 
on giving: CVE-2009-0801, which is just the trigger issue for a whole 
nest of bad side effects.


Amos


From ngtech1ltd at gmail.com  Sun Oct 30 16:34:07 2022
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sun, 30 Oct 2022 18:34:07 +0200
Subject: [squid-users] ACL based DNS server list
In-Reply-To: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
References: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
Message-ID: <002701d8ec7d$6fbe28b0$4f3a7a10$@gmail.com>

Hey James,
 
No it?s not possible.
There is a possibility to run a single proxy per client however you should really try to make sense in doing so.
 
Eliezer
 
----
Eliezer Croitoru
NgTech, Tech Support
Mobile: +972-5-28704261
Email:  <mailto:ngtech1ltd at gmail.com> ngtech1ltd at gmail.com
Web:  <https://ngtech.co.il/> https://ngtech.co.il/
My-Tube:  <https://tube.ngtech.co.il/> https://tube.ngtech.co.il/
 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Sneaker Space LTD
Sent: Wednesday, 26 October 2022 4:27
To: squid-users at lists.squid-cache.org
Subject: [squid-users] ACL based DNS server list
 
Hello,
 
Is there a way to use specific DNS servers based on the user or connecting IP address that is making the connection by using acls or any other method? If so, can someone send an example.
 
Thanks,
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221030/b2de5385/attachment.htm>

From gtaylor at tnetconsulting.net  Sun Oct 30 17:00:10 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Sun, 30 Oct 2022 11:00:10 -0600
Subject: [squid-users] ACL based DNS server list
In-Reply-To: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
References: <CAGzU65pSapT51jQb4zbEu3aB7ecqpZ80GhTO0fzy1Zj0+JOiFw@mail.gmail.com>
Message-ID: <19aec77d-0dab-0e28-e536-d040b02311e7@spamtrap.tnetconsulting.net>

On 10/25/22 7:27 PM, Sneaker Space LTD wrote:
> Hello,

Hi,

> Is there a way to use specific?DNS servers based on the user or 
> connecting IP address that is making the connection by using?acls or any 
> other method? If so, can someone send an example.

"Any other method" covers a LOT of things.  Including things outside of 
Squid's domain.

You could probably do some things with networking such that different 
clients connected to different instances of Squid each configured to use 
different DNS servers.  --  This is a huge hole in the ground and can 
cover a LOT of things.  All of which are outside of Squid's domain.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221030/5e11edcf/attachment.bin>

From gtaylor at tnetconsulting.net  Mon Oct 31 22:38:09 2022
From: gtaylor at tnetconsulting.net (Grant Taylor)
Date: Mon, 31 Oct 2022 16:38:09 -0600
Subject: [squid-users] FW: Encrypted browser-Squid connection errors
In-Reply-To: <19fa29f8ddf517b0e38d73a7e6b0ca55@treenet.co.nz>
References: <DBAPR02MB6149665159D0E233B10A60CFF5249@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <DBAPR02MB6149F46E4378E1069758569EF5289@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <d168d00a-e5e2-1a13-5661-abe33c3397ef@measurement-factory.com>
 <DBAPR02MB6149F7F567889AC742B77519F52B9@DBAPR02MB6149.eurprd02.prod.outlook.com>
 <c76e6f82-f587-c1ab-8f7e-2b22c7badcf6@measurement-factory.com>
 <1a598226-ffe8-f536-6fff-9f1ae7920544@spamtrap.tnetconsulting.net>
 <AM8PR04MB77454AD896CB65626FA54E878F2A9@AM8PR04MB7745.eurprd04.prod.outlook.com>
 <1f0c07d5-74a0-c3b2-ced1-f4334d6dc66a@spamtrap.tnetconsulting.net>
 <Y1FuER/x8qizD34c@fantomas.sk>
 <263997a6-37bd-c9e8-0651-64301048b5af@spamtrap.tnetconsulting.net>
 <17b1bac2-eaa5-775a-95e5-6cc6ffb2692a@suse.de>
 <0dc84627-5af0-78e1-9fe8-9a8329db92e1@spamtrap.tnetconsulting.net>
 <7b177298-2613-9215-2a99-b322e8b7c8e0@treenet.co.nz>
 <2f52e708-b1d8-ad6d-c9ea-62c3c5cf2965@spamtrap.tnetconsulting.net>
 <19fa29f8ddf517b0e38d73a7e6b0ca55@treenet.co.nz>
Message-ID: <753b5a29-f2ac-1128-da8b-5c3b7ae7a043@spamtrap.tnetconsulting.net>

On 10/30/22 6:59 AM, squid3 at treenet.co.nz wrote:
> Duane W. would be the best one to ask about the details.
> 
> What I know is that some 10-12 years ago I discovered an message by 
> Duane mentioning that W3C had (given or accepted) port 3128 for Squid 
> use. I've checked the squid-cache archives and not seeing the message.
> 
> Right now it looks like the W3C changed their systems and only track the 
> standards documents. So I cannot reference their (outdated?) protocol 
> registry :-{ . Also checked the squid-cache archives and not finding it 
> email history. Sorry.

Did you by chance mean IANA?

I looked and 3128 is registered to something other than Squid.

Nor did their search bring anything up for Squid.

> I mean "authority" as used by HTTP specification, which refers to 
> https://www.rfc-editor.org/rfc/rfc3986#section-3.2
> 
> Yes exactly. That is the source of the problem, perpetuated by the need 
> to retain on-wire byte/octet backward compatibility until HTTP/2 changed 
> to binary format.
> 
> Consider what the proxy has to do when (not if) the IP:port being 
> connected to are that proxy's (eg localhost:80) and the URL is only a 
> path ("/") on an origin server somewhere else. Does the "GET / HTTP/1.0" 
> mean "http://example.com/" or "http://example.net/" ?

I would hope that it would return an error page, much like Squid does 
when it can't resolve a domain name or the connection times out.

> The key point is that the proxy host:port and the origin host:port are 
> two different authority and only the origin may be passed along in the 
> URL (or URL+Host header).

Agreed.

> When the client uses port 80 and 443 thinking 
> they are origin services it is *required* (per 
> https://www.rfc-editor.org/rfc/rfc9112.html#name-origin-form) to omit 
> the real origins info. Enter problems.

Why would a client (worth it's disk space) ever conflate the value of 
it's configured proxy as the origin server?

I can see a potential for confusion when using (network) transparent / 
intercepting proxies.

> I refer to all the many ways the clients may be explicitly or implicitly 
> configured to be aware that it is talking to a proxy - such that it 
> explicitly avoids sending the problematic origin-form URLs.

ACK

> The defaults though are tuned for origin server (or reverse-proxy) 
> direct contact.

I don't see how that precludes their use for (forward) proxy servers.

> No Browser I know supports 
> "http-alt://proxy.example.com?http://origin.example.net/index.html" URLs.

But I bet that many browsers would support:

    http://proxy.example.com:8080/?http://origin.example.net/index.html

Also, I'm talking about "http://" and "https://" using their default 
ports of 80 & 443.

> ... "at your own risk" they technically might be. So long as you only 
> receive one of the three types of syntax there - port 80/443 being 
> officially registered for origin / reverse-proxy syntax.

I've been using them without any known problem for multiple years across 
multiple platforms, clients, and versions thereof.  So I'll keep using 
it at my own risk.

> It is based on experience. Squid used to be a lot more lenient and tried 
> for decades to do the syntax auto-detection. The path from that to 
> separate ports is littered with CVEs. Most notably the curse that keeps 
> on giving: CVE-2009-0801, which is just the trigger issue for a whole 
> nest of bad side effects.

I wonder how much of that problematic history was related to HTTP/0.9 vs 
HTTP/1.0 vs HTTP/1.1 clients.

I similarly wonder how much HTTP/1.0, or even HTTP/0.9, protocol is used 
these days.

Also, there is the elephant in the room of we're talking about a proxy 
server which is frequently, but not always, on a dedicated system or IP. 
  As such, I have no problem predicating the use of the HTTP(80) and 
HTTPS(443) ports when there is no possible chance of confusion between 
forward proxy roles and origin server / reverse proxy roles.



-- 
Grant. . . .
unix || die

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4017 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20221031/84078341/attachment.bin>

