From ankor2023 at gmail.com  Thu Jun  1 09:20:45 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 1 Jun 2023 12:20:45 +0300
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
Message-ID: <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>

Hello, Alex,

Thanks for the help.

> The next step I would recommend is to study the very first cache miss
> _after_ the 500 or 200 concurrent threads test. Doing so may shed light
> on why Squid is refusing to serve that (presumably cached) object from
> the cache. I suspect that the object was marked for deletion earlier,
> but we should check before spending more time on more complex triage of
> concurrent cases. If you can share a (link to) compressed ALL,9
> cache.log from that single transaction against Squid v6, I may be able
> to help you with that step.
I cleared the rock cache, changed the squid.conf (added debug_options
ALL,9), restarted the squid, ran a test with 500 concurrent threads, and
dumped the rock cache to make sure that the URL is there:
$VAR1 = {
          '1' => {
                   'URL' => '
https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
',
                   'KEY_MD5' => 'e5eb10f0ab7d84ff9d3fd1e5a6d3eb9c',
                   'OBJSIZE' => 446985,
                   'STD_LFS' => {
                                  'lastref' => 'Thu Jun  1 11:29:01 2023',
                                  'timestamp' => 'Thu Jun  1 11:29:01 2023',
                                  'expires' => 'Thu Jun  1 17:29:01 2023',
                                  'refcount' => 3,
                                  'flags' => '0x4004',
                                  'swap_file_sz' => 0,
                                  'lastmod' => 'Wed Jun 29 16:09:14 2016'
                                },
                   'VERSION' => 'Thu Jun  1 11:29:01 2023'
                 }
        };
Then cleared the cache.log to have only a single transaction in the logs,
and finally ran curl.
Log file is uploaded to
https://drive.google.com/file/d/1uwbBVjWeDEHI95B6ArPZr5_pqkCr_h9P/view?usp=sharing


There are records in the log:
2023/06/01 11:30:34.556 kid7| 20,3| Controller.cc(429) peek:
E5EB10F0AB7D84FF9D3FD1E5A6D3EB9C
2023/06/01 11:30:34.556 kid7| 54,5| StoreMap.cc(443) openForReading:
opening entry with key E5EB10F0AB7D84FF9D3FD1E5A6D3EB9C for reading
transients_map
2023/06/01 11:30:34.556 kid7| 54,5| StoreMap.cc(455) openForReadingAt:
opening entry 11138 for reading transients_map
2023/06/01 11:30:34.556 kid7| 54,7| StoreMap.cc(467) openForReadingAt:
cannot open empty entry 11138 for reading transients_map
2023/06/01 11:30:34.556 kid7| 54,5| StoreMap.cc(443) openForReading:
opening entry with key E5EB10F0AB7D84FF9D3FD1E5A6D3EB9C for reading
cache_mem_map
2023/06/01 11:30:34.556 kid7| 54,5| StoreMap.cc(455) openForReadingAt:
opening entry 11138 for reading cache_mem_map
2023/06/01 11:30:34.556 kid7| 54,7| StoreMap.cc(474) openForReadingAt:
cannot open marked entry 11138 for reading cache_mem_map
2023/06/01 11:30:34.556 kid7| 54,5| StoreMap.cc(443) openForReading:
opening entry with key E5EB10F0AB7D84FF9D3FD1E5A6D3EB9C for reading
/data/squid.user/cache_map
2023/06/01 11:30:34.556 kid7| 54,5| StoreMap.cc(455) openForReadingAt:
opening entry 719406 for reading /data/squid.user/cache_map
2023/06/01 11:30:34.556 kid7| 54,7| StoreMap.cc(474) openForReadingAt:
cannot open marked entry 719406 for reading /data/squid.user/cache_map
2023/06/01 11:30:34.556 kid7| 20,6| Disks.cc(254) get: none of 1 cache_dirs
have E5EB10F0AB7D84FF9D3FD1E5A6D3EB9C
2023/06/01 11:30:34.556 kid7| 20,4| Controller.cc(463) peek: cannot locate
E5EB10F0AB7D84FF9D3FD1E5A6D3EB9C
2023/06/01 11:30:34.556 kid7| 85,7| client_side_reply.cc(1626)
detailStoreLookup: mismatch
2023/06/01 11:30:34.556 kid7| 85,3| client_side_reply.cc(1561)
identifyFoundObject: StoreEntry is NULL -  MISS
2023/06/01 11:30:34.556 kid7| 83,7| LogTags.cc(57) update: TAG_NONE to
TCP_MISS

The file that squid tried to read /data/squid.user/cache_map does not
exist. I use a cache_dir file /data/squid.user/cache/rock. I suppose that
file /data/squid.user/cache_map is a virtual entity.


Kind regards,
      Ankor

??, 31 ??? 2023??. ? 16:43, Alex Rousskov <rousskov at measurement-factory.com
>:

> On 5/31/23 02:56, Andrey K wrote:
>
> >  > Do you get close to 100% hit ratio if clients access these URLs
> >  > sequentially rather than concurrently? If not, then focus on that
> >  > problem before you open the collapsed forwarding Pandora box.
> > When I run curl sequentially like this:
> > for i in `seq 500`; do curl --tlsv1.2 -k   --proxy 0001vsg01:3131  -v
> > $URL  >/dev/null 2>&1; done
> > I get only the first request with a status TCP_MISS and all others with
> > TCP_MEM_HIT:
> >      Cnt Status            Parent
> >    499 TCP_MEM_HIT/200/- HIER_NONE/-
> >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>
> Excellent. This confirms that your Squid can successfully cache this
> object (in memory).
>
>
> > It is interesting to note that on both squid versions if I run a
> > separate curl after processing 500 or 200 concurrent threads, I get a
> > result with the status TCP_MISS/200
>
> The next step I would recommend is to study the very first cache miss
> _after_ the 500 or 200 concurrent threads test. Doing so may shed light
> on why Squid is refusing to serve that (presumably cached) object from
> the cache. I suspect that the object was marked for deletion earlier,
> but we should check before spending more time on more complex triage of
> concurrent cases. If you can share a (link to) compressed ALL,9
> cache.log from that single transaction against Squid v6, I may be able
> to help you with that step.
>
>
> Cheers,
>
> Alex.
>
>
> >  > What is your Squid version? Older Squids have more collapsed
> forwarding
> >  > bugs than newer ones. I recommend testing with Squid v6 or master/v7,
> at
> >  > least to confirm that the problem is still present in the latest
> >  > official code.
> > I run tests on SQUID 5.9.
> > We compiled 6.0.2 (with disabled delay-pools) and increased memory
> > parameters:
> >    cache_mem 2048 MB
> >    maximum_object_size_in_memory 10 MB
> > The complete configuration is shown below.
> >
> > Now on the version 6.0.2 we have the next results:
> > 500 threads -  Hit ratio 3.8%:
> >        3 TCP_CF_HIT/200/- HIER_NONE/-
> >        2 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >       16 TCP_HIT/200/- HIER_NONE/-
> >      467 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >       12 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >
> > 200 threads - 6%
> >        6 TCP_CF_HIT/200/- HIER_NONE/-
> >       10 TCP_HIT/200/- HIER_NONE/-
> >      176 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >        8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >
> > 50 threads - 82%
> >       30 TCP_CF_HIT/200/- HIER_NONE/-
> >       11 TCP_HIT/200/- HIER_NONE/-
> >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >        8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >
> > The results are slightly worse than they were on the version 5.9.
> > It is interesting to note that on both squid versions if I run a
> > separate curl after processing 500 or 200 concurrent threads, I get a
> > result with the status TCP_MISS/200, although the requested URL is
> > already in the rock cache (I can see it in the contents of the cache
> > using the utility I developed rock_cache_dump.pl
> > <http://rock_cache_dump.pl>:
> > $VAR1 = {
> >            '1' => {
> >                     'VERSION' => 'Wed May 31 09:18:05 2023',
> >                     'KEY_MD5' => 'e5eb10f0ab7d84ff9d3fd1e5a6d3eb9c',
> >                     'OBJSIZE' => 446985,
> >                     'STD_LFS' => {
> >                                    'lastref' => 'Wed May 31 09:18:05
> 2023',
> >                                    'flags' => '0x4004',
> >                                    'expires' => 'Wed May 31 15:18:05
> 2023',
> >                                    'swap_file_sz' => 0,
> >                                    'refcount' => 1,
> >                                    'lastmod' => 'Wed Jun 29 16:09:14
> 2016',
> >                                    'timestamp' => 'Wed May 31 09:18:05
> 2023'
> >                                  },
> >                     'URL' =>
> > '
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> >'
> >                   }
> >          };
> >
> > ).
> >
> >  > How much RAM does your server have? You are using default 256MB memory
> >  > cache (cache_mem). If you have spare memory, make your memory cache
> much
> >  > larger: A rock cache_dir cannot (yet) share the response _while_ the
> >  > response is being written to disk, so relying on cache_dir too much
> will
> >  > decrease your hit ratio, especially in a collapsed forwarding
> > environment.
> > The VM has 32 GB RAM. I configured cache_mem 2048 MB on the 6.0.2
> version.
> >
> >  > Is your Squid built with --enable-delay-pools? If yes, TCP_MISS does
> not
> >  > necessarily mean a cache miss (an old Squid bug), even if you do not
> use
> >  > any delay pools.
> > Yes, delay pools on the version 5.9 were enabled though we don't use
> > them. I disabled this feature on the 6.0.2 version.
> >
> >
> >  > Since you are trying to cache objects lager than 512KB, see
> >  > maximum_object_size_in_memory.
> > I configured maximum_object_size_in_memory 10 MB on the 6.0.2 version
> > (as videochunks are less than 7 MB).
> >
> >  > Consider making your test much longer (more sequential requests per
> >  > client/curl worker), to see whether the cache becomes "stable" after
> one
> >  > of the first transactions manages to fully cache the response. This
> may
> >  > not help with older Squids, but might help with newer ones. However,
> you
> >  > should not test using real origin servers (that you do not control)!
> > I don't have any of my own web servers for tests, so I choose some
> > resources on the public internet that have a robust infrastructure.
> > I will conduct the longer tests next week.
> >
> > Kind regards,
> >        Ankor.
> >
> > *squid.conf*
> > workers 21
> >
> > sslcrtd_program /data/squid.user/usr/lib/squid/security_file_certgen -s
> > /data/squid.user/var/lib/squid/ssl_db -M 20MB
> > sslcrtd_children 21
> >
> > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
> > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
> >
> > logfile_rotate 0
> > access_log daemon:/var/log/squid.user/access.log
> > logformat=extended-squid on-error=drop
> >
> > cache_peer parent_proxy  parent 3128 0
> > never_direct allow all
> >
> > cachemgr_passwd pass config
> >
> > acl PURGE method PURGE
> > http_access allow PURGE
> >
> > http_access allow all
> >
> > http_port 3131 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=20MB
> > tls-cert=/etc/squid.user/sslbump/bump.crt
> > tls-key=/etc/squid.user/sslbump/bump.key
> > sslproxy_cert_error allow all
> >
> > acl step1 at_step SslBump1
> > acl step2 at_step SslBump2
> > acl step3 at_step SslBump3
> >
> > ssl_bump peek step1
> > ssl_bump bump step2
> > ssl_bump bump step3
> >
> > cache_dir rock /data/squid.user/cache 20000 max-size=12000000
> > cache_swap_low 85
> > cache_swap_high 90
> >
> > collapsed_forwarding on
> > cache_mem 2048 MB
> > maximum_object_size_in_memory 10 MB
> >
> > pinger_enable off
> > max_filedesc 8192
> > shutdown_lifetime 5 seconds
> > netdb_filename none
> > log_icp_queries off
> >
> > via off
> > forwarded_for delete
> >
> > client_request_buffer_max_size 100 MB
> >
> > coredump_dir /data/squid.user/var/cache/squid
> >
> >
> >
> >
> > ??, 29 ??? 2023??. ? 23:17, Alex Rousskov
> > <rousskov at measurement-factory.com
> > <mailto:rousskov at measurement-factory.com>>:
> >
> >     On 5/29/23 10:43, Andrey K wrote:
> >
> >      > We need to configure a dedicated proxy server to provide caching
> of
> >      > online video broadcasts in order to reduce the load on the uplink
> >     proxy.
> >      > Hundreds of users will access the same video-chunks
> simultaneously.
> >      >
> >      > I developed a simple configuration for the test purposes (it is
> >     shown
> >      > below).
> >      > The *collapsed_forwarding* option is on.
> >
> >     Do you get close to 100% hit ratio if clients access these URLs
> >     sequentially rather than concurrently? If not, then focus on that
> >     problem before you open the collapsed forwarding Pandora box.
> >
> >     What is your Squid version? Older Squids have more collapsed
> forwarding
> >     bugs than newer ones. I recommend testing with Squid v6 or
> >     master/v7, at
> >     least to confirm that the problem is still present in the latest
> >     official code.
> >
> >     How much RAM does your server have? You are using default 256MB
> memory
> >     cache (cache_mem). If you have spare memory, make your memory cache
> >     much
> >     larger: A rock cache_dir cannot (yet) share the response _while_ the
> >     response is being written to disk, so relying on cache_dir too much
> >     will
> >     decrease your hit ratio, especially in a collapsed forwarding
> >     environment.
> >
> >     Is your Squid built with --enable-delay-pools? If yes, TCP_MISS does
> >     not
> >     necessarily mean a cache miss (an old Squid bug), even if you do not
> >     use
> >     any delay pools.
> >
> >     Since you are trying to cache objects lager than 512KB, see
> >     maximum_object_size_in_memory.
> >
> >     Consider making your test much longer (more sequential requests per
> >     client/curl worker), to see whether the cache becomes "stable" after
> >     one
> >     of the first transactions manages to fully cache the response. This
> may
> >     not help with older Squids, but might help with newer ones. However,
> >     you
> >     should not test using real origin servers (that you do not control)!
> >
> >
> >      > Could you clarify if this behavior of my squid is
> >      > a bug/misconfiguration, or if I'm running into server performance
> >      > limitations (squid is running on a VM with 22 cores)?
> >
> >     Most likely, reduction of hit ratio with increase of concurrency is
> >     _not_ a performance limitation.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >      > I selected a couple of cacheable resources in the internet for
> >     testing:
> >      >   - small size (~400 KB):
> >      >
> >
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> >>
> >      >   - large (~8 MB):
> >      >
> >
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> >>
> >      > To test simultaneous connections I am forking curl using a simple
> >     script
> >      > (it is also shown below).
> >      >
> >      > When I run a test (500 curl threads to
> >      >
> >
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>)
> I see lots of TCP_MISS/200 with FIRSTUP_PARENT/parent_proxy records in the
> logs.
> >      >
> >      > A simple analysis shows a low percentage of cache hits:
> >      > cat /var/log/squid.user/access.log| grep '2023-05-29 14' | grep
> >     pdf  |
> >      > awk '{print $5" " $10}' | sort | uniq -c
> >      >       24 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      457 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >       10 TCP_MISS/200/- HIER_NONE/-
> >      >        9 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > So the Hit ratio is about (500-457-9)*100/500=6.8%
> >      >
> >      > Almost the same situation we see when run 200 threads:
> >      > cat /var/log/squid.user/access.log| grep '2023-05-29 15:45' |
> >     grep pdf
> >      >   | awk '{print $5" " $10}' | sort | uniq -c
> >      >        4 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      140 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >       40 TCP_MISS/200/- HIER_NONE/-
> >      >       16 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > This time the Hit ratio is about (200-140-16)*100/500=21%
> >      >
> >      > With 50 threads the Hit ratio is 90%:
> >      > cat /var/log/squid.user/access.log| grep '2023-05-29 15:50' |
> >     grep pdf
> >      >   | awk '{print $5" " $10}' | sort | uniq -c
> >      >       27 TCP_CF_MISS/200/- HIER_NONE/-
> >      >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >       18 TCP_MISS/200/- HIER_NONE/-
> >      >        4 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > I thought that it should always be near 99% - only the first
> >     request to
> >      > an URL should be forwarded to the parent proxy and all subsequent
> >      > requests should be served from the cache.
> >      >
> >      > The situation is even worse with downloading a large file:
> >      > 500 threads (0.4%):
> >      > cat /var/log/squid.user/access.log| grep '2023-05-29 17:2' | grep
> >     pdf  |
> >      > awk '{print $5" " $10}' | sort | uniq -c
> >      >       10 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >        2 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      488 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > 200 threads (3%):
> >      > cat /var/log/squid.user/access.log| grep '2023-05-29 17:3' | grep
> >     pdf  |
> >      > awk '{print $5" " $10}' | sort | uniq -c
> >      >        9 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >        6 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      180 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >        5 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > 50 threads (98%):
> >      > cat /var/log/squid.user/access.log| grep '2023-05-29 17:36' |
> >     grep pdf
> >      >   | awk '{print $5" " $10}' | sort | uniq -c
> >      >       25 TCP_CF_HIT/200/- HIER_NONE/-
> >      >       12 TCP_CF_MISS/200/- HIER_NONE/-
> >      >       12 TCP_HIT/200/- HIER_NONE/-
> >      >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > Could you clarify if this behavior of my squid is a
> >      > bug/misconfiguration, or if I'm running into server performance
> >      > limitations (squid is running on a VM with 22 cores)?
> >      >
> >      > Kind regards,
> >      >       Ankor
> >      >
> >      >
> >      >
> >      > *squid.conf:*
> >      > workers 21
> >      >
> >      > sslcrtd_program
> >     /data/squid.user/usr/lib/squid/security_file_certgen -s
> >      > /data/squid.user/var/lib/squid/ssl_db -M 20MB
> >      > sslcrtd_children 21
> >      >
> >      > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
> >      > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
> >      >
> >      > logfile_rotate 0
> >      > access_log daemon:/var/log/squid.user/access.log
> >      > logformat=extended-squid on-error=drop
> >      >
> >      > cache_peer parent_proxy  parent 3128 0
> >      > never_direct allow all
> >      >
> >      > cachemgr_passwd pass config
> >      >
> >      > acl PURGE method PURGE
> >      > http_access allow PURGE
> >      >
> >      > http_access allow all
> >      >
> >      > http_port 3131 ssl-bump generate-host-certificates=on
> >      > dynamic_cert_mem_cache_size=20MB
> >      > tls-cert=/etc/squid.user/sslbump/bump.crt
> >      > tls-key=/etc/squid.user/sslbump/bump.key
> >      > sslproxy_cert_error allow all
> >      >
> >      > acl step1 at_step SslBump1
> >      > acl step2 at_step SslBump2
> >      > acl step3 at_step SslBump3
> >      >
> >      > ssl_bump peek step1
> >      > ssl_bump bump step2
> >      > ssl_bump bump step3
> >      >
> >      > cache_dir rock /data/squid.user/cache 20000 max-size=12000000
> >      > cache_swap_low 85
> >      > cache_swap_high 90
> >      >
> >      > *collapsed_forwarding on*
> >      >
> >      > pinger_enable off
> >      > max_filedesc 8192
> >      > shutdown_lifetime 5 seconds
> >      > netdb_filename none
> >      > log_icp_queries off
> >      > client_request_buffer_max_size 100 MB
> >      >
> >      > via off
> >      > forwarded_for delete
> >      >
> >      > coredump_dir /data/squid.user/var/cache/squid
> >      >
> >      > *curl_forker.sh:*
> >      > #!/bin/sh
> >      > N=100
> >      >
> >     URL=
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> >>
> >      >
> >      > if [[  -n $1 &&  $1 =~ help$  ]];
> >      > then
> >      >     echo "Usage: $0 [<cnt>] [<url>]"
> >      >     echo
> >      >     echo "Example: $0 10
> >      >
> >
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> >>";
> >      >     echo
> >      >     exit;
> >      > fi
> >      >
> >      > while [[ $# -gt 0 ]]
> >      > do
> >      >    if [[ $1 =~ ^[0-9]+$ ]]
> >      >    then
> >      >       N=$1
> >      >    else
> >      >       URL=$1
> >      >    fi
> >      >    shift
> >      > done
> >      >
> >      > echo $URL
> >      > echo $N threads
> >      >
> >      > for i in `seq $N`
> >      > do
> >      >    nohup curl --tlsv1.2 -k   --proxy 0001vsg01:3131  -v $URL
> >       >/dev/null
> >      >   2>&1 &
> >      > done
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >
> >      > _______________________________________________
> >      > squid-users mailing list
> >      > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >      > http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230601/285851c2/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun  1 16:15:06 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2023 12:15:06 -0400
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
 <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
Message-ID: <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>

On 6/1/23 05:20, Andrey K wrote:

>  > The next step I would recommend is to study the very first cache miss
>  > _after_ the 500 or 200 concurrent threads test. Doing so may shed light
>  > on why Squid is refusing to serve that (presumably cached) object from
>  > the cache. I suspect that the object was marked for deletion earlier

> openForReadingAt: cannot open marked entry 11138 for reading cache_mem_map
> openForReadingAt: cannot open marked entry 719406 for reading /data/squid.user/cache_map


The debugging log you have shared confirms that Squid deleted[1] the 
previously cached entry, from both caches (memory and disk). Now comes 
the hard part -- figuring out why Squid deleted that entry.

> I cleared the rock cache, changed the squid.conf (added debug_options
> ALL,9), restarted the squid, ran a test with 500 concurrent threads

Can you repeat this test and share a pointer to the corresponding 
compressed cache.log, containing those 500 (or fewer, as long as the 
problem is reproduced!) concurrent transactions. One or many of those 
concurrent transactions resulted in the unwanted entry deletion. The log 
may show what happened in that case.

FWIW, I do not recommend spending your time analyzing that huge log. 
Efficient analysis requires specialized knowledge in this case. I will 
share the results here, of course.


Thank you,

Alex.

[1] That entry deletion does not imply that all (or any) of the cached 
entry bytes are gone from the cache file on disk. It is likely that only 
the shared memory _index_ for that disk file was adjusted in your micro 
test. That index adjustment is enough for Squid to declare a cache miss.


> ??, 31 ??? 2023??. ? 16:43, Alex Rousskov:
> 
>     On 5/31/23 02:56, Andrey K wrote:
> 
>      >? > Do you get close to 100% hit ratio if clients access these URLs
>      >? > sequentially rather than concurrently? If not, then focus on that
>      >? > problem before you open the collapsed forwarding Pandora box.
>      > When I run curl sequentially?like this:
>      > for i in `seq 500`; do curl --tlsv1.2 -k ? --proxy 0001vsg01:3131
>      ?-v
>      > $URL??>/dev/null 2>&1; done
>      > I get only the first request with a status TCP_MISS?and all
>     others with
>      > TCP_MEM_HIT:
>      >? ? ? Cnt Status? ? ? ? ? ? Parent
>      >? ? 499 TCP_MEM_HIT/200/- HIER_NONE/-
>      >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> 
>     Excellent. This confirms that your Squid can successfully cache this
>     object (in memory).
> 
> 
>      > It is interesting to note that on both squid versions if I run a
>      > separate curl after processing 500 or 200 concurrent threads, I
>     get a
>      > result with the status TCP_MISS/200
> 
>     The next step I would recommend is to study the very first cache miss
>     _after_ the 500 or 200 concurrent threads test. Doing so may shed light
>     on why Squid is refusing to serve that (presumably cached) object from
>     the cache. I suspect that the object was marked for deletion earlier,
>     but we should check before spending more time on more complex triage of
>     concurrent cases. If you can share a (link to) compressed ALL,9
>     cache.log from that single transaction against Squid v6, I may be able
>     to help you with that step.
> 
> 
>     Cheers,
> 
>     Alex.
> 
> 
>      >? > What is your Squid version? Older Squids have more collapsed
>     forwarding
>      >? > bugs than newer ones. I recommend testing with Squid v6 or
>     master/v7, at
>      >? > least to confirm that the problem is still present in the latest
>      >? > official code.
>      > I run tests on SQUID 5.9.
>      > We compiled?6.0.2 (with disabled delay-pools) and increased memory
>      > parameters:
>      >? ? cache_mem 2048 MB
>      >? ? maximum_object_size_in_memory 10 MB
>      > The complete configuration is shown below.
>      >
>      > Now on the version 6.0.2 we have the next results:
>      > 500 threads -? Hit ratio 3.8%:
>      >? ? ? ? 3 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? ? 2 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? ?16 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? 467 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? ?12 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >
>      > 200 threads - 6%
>      >? ? ? ? 6 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? ?10 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? 176 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? ? 8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >
>      > 50 threads - 82%
>      >? ? ? ?30 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? ?11 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? ? 8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >
>      > The results are slightly worse than they were on the version 5.9.
>      > It is interesting to note that on both squid versions if I run a
>      > separate curl after processing 500 or 200 concurrent threads, I
>     get a
>      > result with the status TCP_MISS/200, although the requested URL is
>      > already in the rock cache (I can see it in the contents of the cache
>      > using the utility I developed rock_cache_dump.pl
>     <http://rock_cache_dump.pl>
>      > <http://rock_cache_dump.pl <http://rock_cache_dump.pl>>:
>      > $VAR1 = {
>      >? ? ? ? ? ? '1' => {
>      >? ? ? ? ? ? ? ? ? ? ?'VERSION' => 'Wed May 31 09:18:05 2023',
>      >? ? ? ? ? ? ? ? ? ? ?'KEY_MD5' => 'e5eb10f0ab7d84ff9d3fd1e5a6d3eb9c',
>      >? ? ? ? ? ? ? ? ? ? ?'OBJSIZE' => 446985,
>      >? ? ? ? ? ? ? ? ? ? ?'STD_LFS' => {
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'lastref' => 'Wed May 31
>     09:18:05 2023',
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'flags' => '0x4004',
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'expires' => 'Wed May 31
>     15:18:05 2023',
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'swap_file_sz' => 0,
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'refcount' => 1,
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'lastmod' => 'Wed Jun 29
>     16:09:14 2016',
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'timestamp' => 'Wed May 31
>     09:18:05 2023'
>      >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? },
>      >? ? ? ? ? ? ? ? ? ? ?'URL' =>
>      >
>     'https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>'
>      >? ? ? ? ? ? ? ? ? ?}
>      >? ? ? ? ? };
>      >
>      > ).
>      >
>      >? > How much RAM does your server have? You are using default
>     256MB memory
>      >? > cache (cache_mem). If you have spare memory, make your memory
>     cache much
>      >? > larger: A rock cache_dir cannot (yet) share the response
>     _while_ the
>      >? > response is being written to disk, so relying on cache_dir too
>     much will
>      >? > decrease your hit ratio, especially in a collapsed forwarding
>      > environment.
>      > The VM has 32 GB RAM. I configured cache_mem 2048 MB on the 6.0.2
>     version.
>      >
>      >? > Is your Squid built with --enable-delay-pools? If yes,
>     TCP_MISS does not
>      >? > necessarily mean a cache miss (an old Squid bug), even if you
>     do not use
>      >? > any delay pools.
>      > Yes, delay pools on the version 5.9 were enabled though?we don't use
>      > them. I disabled this feature on the 6.0.2 version.
>      >
>      >
>      >? > Since you are trying to cache objects lager than 512KB, see
>      >? > maximum_object_size_in_memory.
>      > I configured maximum_object_size_in_memory 10 MB?on the 6.0.2
>     version
>      > (as videochunks are less than 7 MB).
>      >
>      >? > Consider making your test much longer (more sequential
>     requests per
>      >? > client/curl worker), to see whether the cache becomes "stable"
>     after one
>      >? > of the first transactions manages to fully cache the response.
>     This may
>      >? > not help with older Squids, but might help with newer ones.
>     However, you
>      >? > should not test using real origin servers (that you do not
>     control)!
>      > I don't have any of my own web servers for tests, so I choose some
>      > resources on the public internet that have a robust infrastructure.
>      > I will conduct the longer tests next week.
>      >
>      > Kind regards,
>      >? ? ? ? Ankor.
>      >
>      > *squid.conf*
>      > workers 21
>      >
>      > sslcrtd_program
>     /data/squid.user/usr/lib/squid/security_file_certgen -s
>      > /data/squid.user/var/lib/squid/ssl_db -M 20MB
>      > sslcrtd_children 21
>      >
>      > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
>      > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
>      >
>      > logfile_rotate 0
>      > access_log daemon:/var/log/squid.user/access.log
>      > logformat=extended-squid on-error=drop
>      >
>      > cache_peer parent_proxy ?parent 3128 0
>      > never_direct allow all
>      >
>      > cachemgr_passwd pass config
>      >
>      > acl PURGE method PURGE
>      > http_access allow PURGE
>      >
>      > http_access allow all
>      >
>      > http_port 3131 ssl-bump generate-host-certificates=on
>      > dynamic_cert_mem_cache_size=20MB
>      > tls-cert=/etc/squid.user/sslbump/bump.crt
>      > tls-key=/etc/squid.user/sslbump/bump.key
>      > sslproxy_cert_error allow all
>      >
>      > acl step1 at_step SslBump1
>      > acl step2 at_step SslBump2
>      > acl step3 at_step SslBump3
>      >
>      > ssl_bump peek step1
>      > ssl_bump bump step2
>      > ssl_bump bump step3
>      >
>      > cache_dir rock /data/squid.user/cache 20000 max-size=12000000
>      > cache_swap_low 85
>      > cache_swap_high 90
>      >
>      > collapsed_forwarding on
>      > cache_mem 2048 MB
>      > maximum_object_size_in_memory 10 MB
>      >
>      > pinger_enable off
>      > max_filedesc 8192
>      > shutdown_lifetime 5 seconds
>      > netdb_filename none
>      > log_icp_queries off
>      >
>      > via off
>      > forwarded_for delete
>      >
>      > client_request_buffer_max_size 100 MB
>      >
>      > coredump_dir /data/squid.user/var/cache/squid
>      >
>      >
>      >
>      >
>      > ??, 29 ??? 2023??. ? 23:17, Alex Rousskov
>      > <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      > <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>>:
>      >
>      >? ? ?On 5/29/23 10:43, Andrey K wrote:
>      >
>      >? ? ? > We need to configure a dedicated proxy server to provide
>     caching of
>      >? ? ? > online video broadcasts in order to reduce the load on the
>     uplink
>      >? ? ?proxy.
>      >? ? ? > Hundreds of users will access the same video-chunks
>     simultaneously.
>      >? ? ? >
>      >? ? ? > I developed a simple configuration for the test purposes
>     (it is
>      >? ? ?shown
>      >? ? ? > below).
>      >? ? ? > The *collapsed_forwarding*?option is on.
>      >
>      >? ? ?Do you get close to 100% hit ratio if clients access these URLs
>      >? ? ?sequentially rather than concurrently? If not, then focus on that
>      >? ? ?problem before you open the collapsed forwarding Pandora box.
>      >
>      >? ? ?What is your Squid version? Older Squids have more collapsed
>     forwarding
>      >? ? ?bugs than newer ones. I recommend testing with Squid v6 or
>      >? ? ?master/v7, at
>      >? ? ?least to confirm that the problem is still present in the latest
>      >? ? ?official code.
>      >
>      >? ? ?How much RAM does your server have? You are using default
>     256MB memory
>      >? ? ?cache (cache_mem). If you have spare memory, make your memory
>     cache
>      >? ? ?much
>      >? ? ?larger: A rock cache_dir cannot (yet) share the response
>     _while_ the
>      >? ? ?response is being written to disk, so relying on cache_dir
>     too much
>      >? ? ?will
>      >? ? ?decrease your hit ratio, especially in a collapsed forwarding
>      >? ? ?environment.
>      >
>      >? ? ?Is your Squid built with --enable-delay-pools? If yes,
>     TCP_MISS does
>      >? ? ?not
>      >? ? ?necessarily mean a cache miss (an old Squid bug), even if you
>     do not
>      >? ? ?use
>      >? ? ?any delay pools.
>      >
>      >? ? ?Since you are trying to cache objects lager than 512KB, see
>      >? ? ?maximum_object_size_in_memory.
>      >
>      >? ? ?Consider making your test much longer (more sequential
>     requests per
>      >? ? ?client/curl worker), to see whether the cache becomes
>     "stable" after
>      >? ? ?one
>      >? ? ?of the first transactions manages to fully cache the
>     response. This may
>      >? ? ?not help with older Squids, but might help with newer ones.
>     However,
>      >? ? ?you
>      >? ? ?should not test using real origin servers (that you do not
>     control)!
>      >
>      >
>      >? ? ? > Could you clarify if this behavior of my squid is
>      >? ? ? > a bug/misconfiguration, or if I'm running into server
>     performance
>      >? ? ? > limitations (squid is running on a VM with 22 cores)?
>      >
>      >? ? ?Most likely, reduction of hit ratio with increase of
>     concurrency is
>      >? ? ?_not_ a performance limitation.
>      >
>      >
>      >? ? ?HTH,
>      >
>      >? ? ?Alex.
>      >
>      >
>      >? ? ? > I selected a couple of cacheable resources in the internet for
>      >? ? ?testing:
>      >? ? ? >? ?- small size (~400 KB):
>      >? ? ? >
>      >
>     https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>>
>      >? ? ? >? ?- large (~8 MB):
>      >? ? ? >
>      >
>     https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>>
>      >? ? ? > To test simultaneous connections I am forking curl using a
>     simple
>      >? ? ?script
>      >? ? ? > (it is also shown below).
>      >? ? ? >
>      >? ? ? > When I run a test (500 curl threads to
>      >? ? ? >
>      >
>     https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>>) I see lots of?TCP_MISS/200 with?FIRSTUP_PARENT/parent_proxy records in the logs.
>      >? ? ? >
>      >? ? ? > A simple analysis shows?a low percentage of cache hits:
>      >? ? ? > cat /var/log/squid.user/access.log| grep '2023-05-29 14' |
>     grep
>      >? ? ?pdf ?|
>      >? ? ? > awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? ?24 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? 457 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ?10 TCP_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 9 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > So the Hit ratio is about (500-457-9)*100/500=6.8%
>      >? ? ? >
>      >? ? ? > Almost the same situation we see when run 200 threads:
>      >? ? ? > cat /var/log/squid.user/access.log| grep '2023-05-29 15:45' |
>      >? ? ?grep pdf
>      >? ? ? >? ?| awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? ? 4 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? 140 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ?40 TCP_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? ?16 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > This time the Hit ratio is about (200-140-16)*100/500=21%
>      >? ? ? >
>      >? ? ? > With 50 threads the Hit ratio is 90%:
>      >? ? ? > cat /var/log/squid.user/access.log| grep '2023-05-29 15:50' |
>      >? ? ?grep pdf
>      >? ? ? >? ?| awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? ?27 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ?18 TCP_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 4 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > I thought that it should always be near 99% - only the first
>      >? ? ?request to
>      >? ? ? > an URL should be forwarded to the parent proxy and all
>     subsequent
>      >? ? ? > requests should be served from the cache.
>      >? ? ? >
>      >? ? ? > The situation is even worse with downloading a large file:
>      >? ? ? > 500 threads (0.4%):
>      >? ? ? > cat /var/log/squid.user/access.log| grep '2023-05-29 17:2'
>     | grep
>      >? ? ?pdf ?|
>      >? ? ? > awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? ?10 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ? 2 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? 488 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > 200 threads (3%):
>      >? ? ? > cat /var/log/squid.user/access.log| grep '2023-05-29 17:3'
>     | grep
>      >? ? ?pdf ?|
>      >? ? ? > awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? ? 9 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ? 6 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? 180 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ? 5 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > 50 threads (98%):
>      >? ? ? > cat /var/log/squid.user/access.log| grep '2023-05-29 17:36' |
>      >? ? ?grep pdf
>      >? ? ? >? ?| awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? ?25 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ?12 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? ?12 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > Could you clarify if this behavior of my squid is a
>      >? ? ? > bug/misconfiguration, or if I'm running into server
>     performance
>      >? ? ? > limitations (squid is running on a VM with 22 cores)?
>      >? ? ? >
>      >? ? ? > Kind regards,
>      >? ? ? >? ? ? ?Ankor
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > *squid.conf:*
>      >? ? ? > workers 21
>      >? ? ? >
>      >? ? ? > sslcrtd_program
>      >? ? ?/data/squid.user/usr/lib/squid/security_file_certgen -s
>      >? ? ? > /data/squid.user/var/lib/squid/ssl_db -M 20MB
>      >? ? ? > sslcrtd_children 21
>      >? ? ? >
>      >? ? ? > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
>      >? ? ? > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
>      >? ? ? >
>      >? ? ? > logfile_rotate 0
>      >? ? ? > access_log daemon:/var/log/squid.user/access.log
>      >? ? ? > logformat=extended-squid on-error=drop
>      >? ? ? >
>      >? ? ? > cache_peer parent_proxy ?parent 3128 0
>      >? ? ? > never_direct allow all
>      >? ? ? >
>      >? ? ? > cachemgr_passwd pass config
>      >? ? ? >
>      >? ? ? > acl PURGE method PURGE
>      >? ? ? > http_access allow PURGE
>      >? ? ? >
>      >? ? ? > http_access allow all
>      >? ? ? >
>      >? ? ? > http_port 3131 ssl-bump generate-host-certificates=on
>      >? ? ? > dynamic_cert_mem_cache_size=20MB
>      >? ? ? > tls-cert=/etc/squid.user/sslbump/bump.crt
>      >? ? ? > tls-key=/etc/squid.user/sslbump/bump.key
>      >? ? ? > sslproxy_cert_error allow all
>      >? ? ? >
>      >? ? ? > acl step1 at_step SslBump1
>      >? ? ? > acl step2 at_step SslBump2
>      >? ? ? > acl step3 at_step SslBump3
>      >? ? ? >
>      >? ? ? > ssl_bump peek step1
>      >? ? ? > ssl_bump bump step2
>      >? ? ? > ssl_bump bump step3
>      >? ? ? >
>      >? ? ? > cache_dir rock /data/squid.user/cache 20000 max-size=12000000
>      >? ? ? > cache_swap_low 85
>      >? ? ? > cache_swap_high 90
>      >? ? ? >
>      >? ? ? > *collapsed_forwarding on*
>      >? ? ? >
>      >? ? ? > pinger_enable off
>      >? ? ? > max_filedesc 8192
>      >? ? ? > shutdown_lifetime 5 seconds
>      >? ? ? > netdb_filename none
>      >? ? ? > log_icp_queries off
>      >? ? ? > client_request_buffer_max_size 100 MB
>      >? ? ? >
>      >? ? ? > via off
>      >? ? ? > forwarded_for delete
>      >? ? ? >
>      >? ? ? > coredump_dir /data/squid.user/var/cache/squid
>      >? ? ? >
>      >? ? ? > *curl_forker.sh:*
>      >? ? ? > #!/bin/sh
>      >? ? ? > N=100
>      >? ? ? >
>      >   
>      ?URL=https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>>
>      >? ? ? >
>      >? ? ? > if [[ ?-n $1 && ?$1 =~ help$ ?]];
>      >? ? ? > then
>      >? ? ? >? ? ?echo "Usage: $0 [<cnt>] [<url>]"
>      >? ? ? >? ? ?echo
>      >? ? ? >? ? ?echo "Example: $0 10
>      >? ? ? >
>      >
>     https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>>";
>      >? ? ? >? ? ?echo
>      >? ? ? >? ? ?exit;
>      >? ? ? > fi
>      >? ? ? >
>      >? ? ? > while [[ $# -gt 0 ]]
>      >? ? ? > do
>      >? ? ? >? ? if [[ $1 =~ ^[0-9]+$ ]]
>      >? ? ? >? ? then
>      >? ? ? >? ? ? ?N=$1
>      >? ? ? >? ? else
>      >? ? ? >? ? ? ?URL=$1
>      >? ? ? >? ? fi
>      >? ? ? >? ? shift
>      >? ? ? > done
>      >? ? ? >
>      >? ? ? > echo $URL
>      >? ? ? > echo $N threads
>      >? ? ? >
>      >? ? ? > for i in `seq $N`
>      >? ? ? > do
>      >? ? ? >? ? nohup curl --tlsv1.2 -k ? --proxy 0001vsg01:3131 ?-v $URL
>      >? ? ? ?>/dev/null
>      >? ? ? >? ?2>&1 &
>      >? ? ? > done
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > _______________________________________________
>      >? ? ? > squid-users mailing list
>      >? ? ? > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>      >? ? ? > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>      >
>      >? ? ?_______________________________________________
>      >? ? ?squid-users mailing list
>      > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>      > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>      >
> 


From ankor2023 at gmail.com  Fri Jun  2 07:29:21 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Fri, 2 Jun 2023 10:29:21 +0300
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
 <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
 <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>
Message-ID: <CADJd0Y2OsT-uZjHFMSUmCnGbAZq9FdbUhR6s4RfsfHdMUkbz2A@mail.gmail.com>

Hello, Alex,

Thank you very much.

> Can you repeat this test and share a pointer to the corresponding
> compressed cache.log, containing those 500 (or fewer, as long as the
> problem is reproduced!) concurrent transactions. One or many of those
> concurrent transactions resulted in the unwanted entry deletion. The log
> may show what happened in that case.
I cleared the rock cache, set the debug level, restarted squid, cleared the
cache.log, ran 500-threads test, waited for it to finish and launched curl
to make sure it returned TCP_MISS.
Then stopped squid to limit the cache.log file.
The link to the cache.log is
https://drive.google.com/file/d/1kC8oV8WAelsBYoZDoqnNsnyd7cfYOoDi/view?usp=sharing
I think the access.log file may be helpful for analyzing the problem too:
https://drive.google.com/file/d/1_fDd2mXgeIIHKdZPEavg50KUqTIR5Ltu/view?usp=sharing
The last record in the file (2023-06-02 09:52:48) is a single curl test
request.

The test statistics are:
Count   STATUS
     22 TCP_CF_HIT/200/- HIER_NONE/-
     72 TCP_HIT/200/- HIER_NONE/-
    404 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
      3 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy

Kind regards,
     Ankor.



??, 1 ???. 2023??. ? 19:15, Alex Rousskov <rousskov at measurement-factory.com
>:

> On 6/1/23 05:20, Andrey K wrote:
>
> >  > The next step I would recommend is to study the very first cache miss
> >  > _after_ the 500 or 200 concurrent threads test. Doing so may shed
> light
> >  > on why Squid is refusing to serve that (presumably cached) object from
> >  > the cache. I suspect that the object was marked for deletion earlier
>
> > openForReadingAt: cannot open marked entry 11138 for reading
> cache_mem_map
> > openForReadingAt: cannot open marked entry 719406 for reading
> /data/squid.user/cache_map
>
>
> The debugging log you have shared confirms that Squid deleted[1] the
> previously cached entry, from both caches (memory and disk). Now comes
> the hard part -- figuring out why Squid deleted that entry.
>
> > I cleared the rock cache, changed the squid.conf (added debug_options
> > ALL,9), restarted the squid, ran a test with 500 concurrent threads
>
> Can you repeat this test and share a pointer to the corresponding
> compressed cache.log, containing those 500 (or fewer, as long as the
> problem is reproduced!) concurrent transactions. One or many of those
> concurrent transactions resulted in the unwanted entry deletion. The log
> may show what happened in that case.
>
> FWIW, I do not recommend spending your time analyzing that huge log.
> Efficient analysis requires specialized knowledge in this case. I will
> share the results here, of course.
>
>
> Thank you,
>
> Alex.
>
> [1] That entry deletion does not imply that all (or any) of the cached
> entry bytes are gone from the cache file on disk. It is likely that only
> the shared memory _index_ for that disk file was adjusted in your micro
> test. That index adjustment is enough for Squid to declare a cache miss.
>
>
> > ??, 31 ??? 2023??. ? 16:43, Alex Rousskov:
> >
> >     On 5/31/23 02:56, Andrey K wrote:
> >
> >      >  > Do you get close to 100% hit ratio if clients access these URLs
> >      >  > sequentially rather than concurrently? If not, then focus on
> that
> >      >  > problem before you open the collapsed forwarding Pandora box.
> >      > When I run curl sequentially like this:
> >      > for i in `seq 500`; do curl --tlsv1.2 -k   --proxy 0001vsg01:3131
> >       -v
> >      > $URL  >/dev/null 2>&1; done
> >      > I get only the first request with a status TCP_MISS and all
> >     others with
> >      > TCP_MEM_HIT:
> >      >      Cnt Status            Parent
> >      >    499 TCP_MEM_HIT/200/- HIER_NONE/-
> >      >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >
> >     Excellent. This confirms that your Squid can successfully cache this
> >     object (in memory).
> >
> >
> >      > It is interesting to note that on both squid versions if I run a
> >      > separate curl after processing 500 or 200 concurrent threads, I
> >     get a
> >      > result with the status TCP_MISS/200
> >
> >     The next step I would recommend is to study the very first cache miss
> >     _after_ the 500 or 200 concurrent threads test. Doing so may shed
> light
> >     on why Squid is refusing to serve that (presumably cached) object
> from
> >     the cache. I suspect that the object was marked for deletion earlier,
> >     but we should check before spending more time on more complex triage
> of
> >     concurrent cases. If you can share a (link to) compressed ALL,9
> >     cache.log from that single transaction against Squid v6, I may be
> able
> >     to help you with that step.
> >
> >
> >     Cheers,
> >
> >     Alex.
> >
> >
> >      >  > What is your Squid version? Older Squids have more collapsed
> >     forwarding
> >      >  > bugs than newer ones. I recommend testing with Squid v6 or
> >     master/v7, at
> >      >  > least to confirm that the problem is still present in the
> latest
> >      >  > official code.
> >      > I run tests on SQUID 5.9.
> >      > We compiled 6.0.2 (with disabled delay-pools) and increased memory
> >      > parameters:
> >      >    cache_mem 2048 MB
> >      >    maximum_object_size_in_memory 10 MB
> >      > The complete configuration is shown below.
> >      >
> >      > Now on the version 6.0.2 we have the next results:
> >      > 500 threads -  Hit ratio 3.8%:
> >      >        3 TCP_CF_HIT/200/- HIER_NONE/-
> >      >        2 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >       16 TCP_HIT/200/- HIER_NONE/-
> >      >      467 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >       12 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > 200 threads - 6%
> >      >        6 TCP_CF_HIT/200/- HIER_NONE/-
> >      >       10 TCP_HIT/200/- HIER_NONE/-
> >      >      176 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >        8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > 50 threads - 82%
> >      >       30 TCP_CF_HIT/200/- HIER_NONE/-
> >      >       11 TCP_HIT/200/- HIER_NONE/-
> >      >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >        8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >
> >      > The results are slightly worse than they were on the version 5.9.
> >      > It is interesting to note that on both squid versions if I run a
> >      > separate curl after processing 500 or 200 concurrent threads, I
> >     get a
> >      > result with the status TCP_MISS/200, although the requested URL is
> >      > already in the rock cache (I can see it in the contents of the
> cache
> >      > using the utility I developed rock_cache_dump.pl
> >     <http://rock_cache_dump.pl>
> >      > <http://rock_cache_dump.pl <http://rock_cache_dump.pl>>:
> >      > $VAR1 = {
> >      >            '1' => {
> >      >                     'VERSION' => 'Wed May 31 09:18:05 2023',
> >      >                     'KEY_MD5' =>
> 'e5eb10f0ab7d84ff9d3fd1e5a6d3eb9c',
> >      >                     'OBJSIZE' => 446985,
> >      >                     'STD_LFS' => {
> >      >                                    'lastref' => 'Wed May 31
> >     09:18:05 2023',
> >      >                                    'flags' => '0x4004',
> >      >                                    'expires' => 'Wed May 31
> >     15:18:05 2023',
> >      >                                    'swap_file_sz' => 0,
> >      >                                    'refcount' => 1,
> >      >                                    'lastmod' => 'Wed Jun 29
> >     16:09:14 2016',
> >      >                                    'timestamp' => 'Wed May 31
> >     09:18:05 2023'
> >      >                                  },
> >      >                     'URL' =>
> >      >
> >     '
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> >>'
> >      >                   }
> >      >          };
> >      >
> >      > ).
> >      >
> >      >  > How much RAM does your server have? You are using default
> >     256MB memory
> >      >  > cache (cache_mem). If you have spare memory, make your memory
> >     cache much
> >      >  > larger: A rock cache_dir cannot (yet) share the response
> >     _while_ the
> >      >  > response is being written to disk, so relying on cache_dir too
> >     much will
> >      >  > decrease your hit ratio, especially in a collapsed forwarding
> >      > environment.
> >      > The VM has 32 GB RAM. I configured cache_mem 2048 MB on the 6.0.2
> >     version.
> >      >
> >      >  > Is your Squid built with --enable-delay-pools? If yes,
> >     TCP_MISS does not
> >      >  > necessarily mean a cache miss (an old Squid bug), even if you
> >     do not use
> >      >  > any delay pools.
> >      > Yes, delay pools on the version 5.9 were enabled though we don't
> use
> >      > them. I disabled this feature on the 6.0.2 version.
> >      >
> >      >
> >      >  > Since you are trying to cache objects lager than 512KB, see
> >      >  > maximum_object_size_in_memory.
> >      > I configured maximum_object_size_in_memory 10 MB on the 6.0.2
> >     version
> >      > (as videochunks are less than 7 MB).
> >      >
> >      >  > Consider making your test much longer (more sequential
> >     requests per
> >      >  > client/curl worker), to see whether the cache becomes "stable"
> >     after one
> >      >  > of the first transactions manages to fully cache the response.
> >     This may
> >      >  > not help with older Squids, but might help with newer ones.
> >     However, you
> >      >  > should not test using real origin servers (that you do not
> >     control)!
> >      > I don't have any of my own web servers for tests, so I choose some
> >      > resources on the public internet that have a robust
> infrastructure.
> >      > I will conduct the longer tests next week.
> >      >
> >      > Kind regards,
> >      >        Ankor.
> >      >
> >      > *squid.conf*
> >      > workers 21
> >      >
> >      > sslcrtd_program
> >     /data/squid.user/usr/lib/squid/security_file_certgen -s
> >      > /data/squid.user/var/lib/squid/ssl_db -M 20MB
> >      > sslcrtd_children 21
> >      >
> >      > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
> >      > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
> >      >
> >      > logfile_rotate 0
> >      > access_log daemon:/var/log/squid.user/access.log
> >      > logformat=extended-squid on-error=drop
> >      >
> >      > cache_peer parent_proxy  parent 3128 0
> >      > never_direct allow all
> >      >
> >      > cachemgr_passwd pass config
> >      >
> >      > acl PURGE method PURGE
> >      > http_access allow PURGE
> >      >
> >      > http_access allow all
> >      >
> >      > http_port 3131 ssl-bump generate-host-certificates=on
> >      > dynamic_cert_mem_cache_size=20MB
> >      > tls-cert=/etc/squid.user/sslbump/bump.crt
> >      > tls-key=/etc/squid.user/sslbump/bump.key
> >      > sslproxy_cert_error allow all
> >      >
> >      > acl step1 at_step SslBump1
> >      > acl step2 at_step SslBump2
> >      > acl step3 at_step SslBump3
> >      >
> >      > ssl_bump peek step1
> >      > ssl_bump bump step2
> >      > ssl_bump bump step3
> >      >
> >      > cache_dir rock /data/squid.user/cache 20000 max-size=12000000
> >      > cache_swap_low 85
> >      > cache_swap_high 90
> >      >
> >      > collapsed_forwarding on
> >      > cache_mem 2048 MB
> >      > maximum_object_size_in_memory 10 MB
> >      >
> >      > pinger_enable off
> >      > max_filedesc 8192
> >      > shutdown_lifetime 5 seconds
> >      > netdb_filename none
> >      > log_icp_queries off
> >      >
> >      > via off
> >      > forwarded_for delete
> >      >
> >      > client_request_buffer_max_size 100 MB
> >      >
> >      > coredump_dir /data/squid.user/var/cache/squid
> >      >
> >      >
> >      >
> >      >
> >      > ??, 29 ??? 2023??. ? 23:17, Alex Rousskov
> >      > <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>
> >      > <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>>>:
> >      >
> >      >     On 5/29/23 10:43, Andrey K wrote:
> >      >
> >      >      > We need to configure a dedicated proxy server to provide
> >     caching of
> >      >      > online video broadcasts in order to reduce the load on the
> >     uplink
> >      >     proxy.
> >      >      > Hundreds of users will access the same video-chunks
> >     simultaneously.
> >      >      >
> >      >      > I developed a simple configuration for the test purposes
> >     (it is
> >      >     shown
> >      >      > below).
> >      >      > The *collapsed_forwarding* option is on.
> >      >
> >      >     Do you get close to 100% hit ratio if clients access these
> URLs
> >      >     sequentially rather than concurrently? If not, then focus on
> that
> >      >     problem before you open the collapsed forwarding Pandora box.
> >      >
> >      >     What is your Squid version? Older Squids have more collapsed
> >     forwarding
> >      >     bugs than newer ones. I recommend testing with Squid v6 or
> >      >     master/v7, at
> >      >     least to confirm that the problem is still present in the
> latest
> >      >     official code.
> >      >
> >      >     How much RAM does your server have? You are using default
> >     256MB memory
> >      >     cache (cache_mem). If you have spare memory, make your memory
> >     cache
> >      >     much
> >      >     larger: A rock cache_dir cannot (yet) share the response
> >     _while_ the
> >      >     response is being written to disk, so relying on cache_dir
> >     too much
> >      >     will
> >      >     decrease your hit ratio, especially in a collapsed forwarding
> >      >     environment.
> >      >
> >      >     Is your Squid built with --enable-delay-pools? If yes,
> >     TCP_MISS does
> >      >     not
> >      >     necessarily mean a cache miss (an old Squid bug), even if you
> >     do not
> >      >     use
> >      >     any delay pools.
> >      >
> >      >     Since you are trying to cache objects lager than 512KB, see
> >      >     maximum_object_size_in_memory.
> >      >
> >      >     Consider making your test much longer (more sequential
> >     requests per
> >      >     client/curl worker), to see whether the cache becomes
> >     "stable" after
> >      >     one
> >      >     of the first transactions manages to fully cache the
> >     response. This may
> >      >     not help with older Squids, but might help with newer ones.
> >     However,
> >      >     you
> >      >     should not test using real origin servers (that you do not
> >     control)!
> >      >
> >      >
> >      >      > Could you clarify if this behavior of my squid is
> >      >      > a bug/misconfiguration, or if I'm running into server
> >     performance
> >      >      > limitations (squid is running on a VM with 22 cores)?
> >      >
> >      >     Most likely, reduction of hit ratio with increase of
> >     concurrency is
> >      >     _not_ a performance limitation.
> >      >
> >      >
> >      >     HTH,
> >      >
> >      >     Alex.
> >      >
> >      >
> >      >      > I selected a couple of cacheable resources in the internet
> for
> >      >     testing:
> >      >      >   - small size (~400 KB):
> >      >      >
> >      >
> >
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> >>>
> >      >      >   - large (~8 MB):
> >      >      >
> >      >
> >
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> >>>
> >      >      > To test simultaneous connections I am forking curl using a
> >     simple
> >      >     script
> >      >      > (it is also shown below).
> >      >      >
> >      >      > When I run a test (500 curl threads to
> >      >      >
> >      >
> >
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf
> <
> https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>>)
> I see lots of TCP_MISS/200 with FIRSTUP_PARENT/parent_proxy records in the
> logs.
> >      >      >
> >      >      > A simple analysis shows a low percentage of cache hits:
> >      >      > cat /var/log/squid.user/access.log| grep '2023-05-29 14' |
> >     grep
> >      >     pdf  |
> >      >      > awk '{print $5" " $10}' | sort | uniq -c
> >      >      >       24 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      >      457 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >       10 TCP_MISS/200/- HIER_NONE/-
> >      >      >        9 TCP_SWAPFAIL_MISS/200/200
> FIRSTUP_PARENT/parent_proxy
> >      >      >
> >      >      > So the Hit ratio is about (500-457-9)*100/500=6.8%
> >      >      >
> >      >      > Almost the same situation we see when run 200 threads:
> >      >      > cat /var/log/squid.user/access.log| grep '2023-05-29
> 15:45' |
> >      >     grep pdf
> >      >      >   | awk '{print $5" " $10}' | sort | uniq -c
> >      >      >        4 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      >      140 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >       40 TCP_MISS/200/- HIER_NONE/-
> >      >      >       16 TCP_SWAPFAIL_MISS/200/200
> FIRSTUP_PARENT/parent_proxy
> >      >      >
> >      >      > This time the Hit ratio is about (200-140-16)*100/500=21%
> >      >      >
> >      >      > With 50 threads the Hit ratio is 90%:
> >      >      > cat /var/log/squid.user/access.log| grep '2023-05-29
> 15:50' |
> >      >     grep pdf
> >      >      >   | awk '{print $5" " $10}' | sort | uniq -c
> >      >      >       27 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >       18 TCP_MISS/200/- HIER_NONE/-
> >      >      >        4 TCP_SWAPFAIL_MISS/200/200
> FIRSTUP_PARENT/parent_proxy
> >      >      >
> >      >      > I thought that it should always be near 99% - only the
> first
> >      >     request to
> >      >      > an URL should be forwarded to the parent proxy and all
> >     subsequent
> >      >      > requests should be served from the cache.
> >      >      >
> >      >      > The situation is even worse with downloading a large file:
> >      >      > 500 threads (0.4%):
> >      >      > cat /var/log/squid.user/access.log| grep '2023-05-29 17:2'
> >     | grep
> >      >     pdf  |
> >      >      > awk '{print $5" " $10}' | sort | uniq -c
> >      >      >       10 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >        2 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      >      488 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >
> >      >      > 200 threads (3%):
> >      >      > cat /var/log/squid.user/access.log| grep '2023-05-29 17:3'
> >     | grep
> >      >     pdf  |
> >      >      > awk '{print $5" " $10}' | sort | uniq -c
> >      >      >        9 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >        6 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      >      180 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >        5 TCP_SWAPFAIL_MISS/200/200
> FIRSTUP_PARENT/parent_proxy
> >      >      >
> >      >      > 50 threads (98%):
> >      >      > cat /var/log/squid.user/access.log| grep '2023-05-29
> 17:36' |
> >      >     grep pdf
> >      >      >   | awk '{print $5" " $10}' | sort | uniq -c
> >      >      >       25 TCP_CF_HIT/200/- HIER_NONE/-
> >      >      >       12 TCP_CF_MISS/200/- HIER_NONE/-
> >      >      >       12 TCP_HIT/200/- HIER_NONE/-
> >      >      >        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
> >      >      >
> >      >      > Could you clarify if this behavior of my squid is a
> >      >      > bug/misconfiguration, or if I'm running into server
> >     performance
> >      >      > limitations (squid is running on a VM with 22 cores)?
> >      >      >
> >      >      > Kind regards,
> >      >      >       Ankor
> >      >      >
> >      >      >
> >      >      >
> >      >      > *squid.conf:*
> >      >      > workers 21
> >      >      >
> >      >      > sslcrtd_program
> >      >     /data/squid.user/usr/lib/squid/security_file_certgen -s
> >      >      > /data/squid.user/var/lib/squid/ssl_db -M 20MB
> >      >      > sslcrtd_children 21
> >      >      >
> >      >      > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
> >      >      > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
> >      >      >
> >      >      > logfile_rotate 0
> >      >      > access_log daemon:/var/log/squid.user/access.log
> >      >      > logformat=extended-squid on-error=drop
> >      >      >
> >      >      > cache_peer parent_proxy  parent 3128 0
> >      >      > never_direct allow all
> >      >      >
> >      >      > cachemgr_passwd pass config
> >      >      >
> >      >      > acl PURGE method PURGE
> >      >      > http_access allow PURGE
> >      >      >
> >      >      > http_access allow all
> >      >      >
> >      >      > http_port 3131 ssl-bump generate-host-certificates=on
> >      >      > dynamic_cert_mem_cache_size=20MB
> >      >      > tls-cert=/etc/squid.user/sslbump/bump.crt
> >      >      > tls-key=/etc/squid.user/sslbump/bump.key
> >      >      > sslproxy_cert_error allow all
> >      >      >
> >      >      > acl step1 at_step SslBump1
> >      >      > acl step2 at_step SslBump2
> >      >      > acl step3 at_step SslBump3
> >      >      >
> >      >      > ssl_bump peek step1
> >      >      > ssl_bump bump step2
> >      >      > ssl_bump bump step3
> >      >      >
> >      >      > cache_dir rock /data/squid.user/cache 20000
> max-size=12000000
> >      >      > cache_swap_low 85
> >      >      > cache_swap_high 90
> >      >      >
> >      >      > *collapsed_forwarding on*
> >      >      >
> >      >      > pinger_enable off
> >      >      > max_filedesc 8192
> >      >      > shutdown_lifetime 5 seconds
> >      >      > netdb_filename none
> >      >      > log_icp_queries off
> >      >      > client_request_buffer_max_size 100 MB
> >      >      >
> >      >      > via off
> >      >      > forwarded_for delete
> >      >      >
> >      >      > coredump_dir /data/squid.user/var/cache/squid
> >      >      >
> >      >      > *curl_forker.sh:*
> >      >      > #!/bin/sh
> >      >      > N=100
> >      >      >
> >      >
> >       URL=
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> >>>
> >      >      >
> >      >      > if [[  -n $1 &&  $1 =~ help$  ]];
> >      >      > then
> >      >      >     echo "Usage: $0 [<cnt>] [<url>]"
> >      >      >     echo
> >      >      >     echo "Example: $0 10
> >      >      >
> >      >
> >
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> <
> https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf
> >>>";
> >      >      >     echo
> >      >      >     exit;
> >      >      > fi
> >      >      >
> >      >      > while [[ $# -gt 0 ]]
> >      >      > do
> >      >      >    if [[ $1 =~ ^[0-9]+$ ]]
> >      >      >    then
> >      >      >       N=$1
> >      >      >    else
> >      >      >       URL=$1
> >      >      >    fi
> >      >      >    shift
> >      >      > done
> >      >      >
> >      >      > echo $URL
> >      >      > echo $N threads
> >      >      >
> >      >      > for i in `seq $N`
> >      >      > do
> >      >      >    nohup curl --tlsv1.2 -k   --proxy 0001vsg01:3131  -v
> $URL
> >      >       >/dev/null
> >      >      >   2>&1 &
> >      >      > done
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      > _______________________________________________
> >      >      > squid-users mailing list
> >      >      > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >      >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>
> >      >      > http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >      >     <http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>>
> >      >
> >      >     _______________________________________________
> >      >     squid-users mailing list
> >      > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >      >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>
> >      > http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >      >     <http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>>
> >      >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230602/f50db762/attachment.htm>

From my.shellac at gmail.com  Fri Jun  2 13:05:46 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Fri, 2 Jun 2023 18:05:46 +0500
Subject: [squid-users] External ACL doesn't used
Message-ID: <CAFqyDwASh6uDDsoMAvPiWV25dB8kniCZtdGf+7QBm7SrfPZXHQ@mail.gmail.com>

Hello Guys!

Could you explain me case when the external acl couldn't to be run by
squid.

Last month all was pretty ok and I have using the annotation transaction
and my external acl script for assign the policy for proxy users (to send
users by different cache_peer)

Today I have just restart my squid and see that my external acl doesn't
using by squid.
I can see that squid loads it and it available, but once the request is
going that squid doesn't run this external acl script and as result I got
the error as squid can't choose which cache_peer need to use (as no answer
from external acl).

P.S. Squid 5.7, works inside docker.


Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230602/7c10f1b5/attachment.htm>

From my.shellac at gmail.com  Fri Jun  2 15:14:19 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Fri, 2 Jun 2023 20:14:19 +0500
Subject: [squid-users] External ACL doesn't used
In-Reply-To: <CAFqyDwASh6uDDsoMAvPiWV25dB8kniCZtdGf+7QBm7SrfPZXHQ@mail.gmail.com>
References: <CAFqyDwASh6uDDsoMAvPiWV25dB8kniCZtdGf+7QBm7SrfPZXHQ@mail.gmail.com>
Message-ID: <CAFqyDwA4OCa61SJoE=+ip=ctFP+=BQkc32P6sca+5a+LbjoPVQ@mail.gmail.com>

So.ok. Looks like this is misconfig....
I just restore from backup and now works well

??, 2 ???. 2023??. ? 18:05, Alexey?? Gruzdov <my.shellac at gmail.com>:

> Hello Guys!
>
> Could you explain me case when the external acl couldn't to be run by
> squid.
>
> Last month all was pretty ok and I have using the annotation transaction
> and my external acl script for assign the policy for proxy users (to send
> users by different cache_peer)
>
> Today I have just restart my squid and see that my external acl doesn't
> using by squid.
> I can see that squid loads it and it available, but once the request is
> going that squid doesn't run this external acl script and as result I got
> the error as squid can't choose which cache_peer need to use (as no answer
> from external acl).
>
> P.S. Squid 5.7, works inside docker.
>
>
> Thanks
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230602/d1360d38/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun  3 09:30:35 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 3 Jun 2023 21:30:35 +1200
Subject: [squid-users] External ACL doesn't used
In-Reply-To: <CAFqyDwA4OCa61SJoE=+ip=ctFP+=BQkc32P6sca+5a+LbjoPVQ@mail.gmail.com>
References: <CAFqyDwASh6uDDsoMAvPiWV25dB8kniCZtdGf+7QBm7SrfPZXHQ@mail.gmail.com>
 <CAFqyDwA4OCa61SJoE=+ip=ctFP+=BQkc32P6sca+5a+LbjoPVQ@mail.gmail.com>
Message-ID: <838f5767-5a1e-e810-1cbb-a26087256aa3@treenet.co.nz>

On 3/06/2023 3:14 am, Alexey?? Gruzdov wrote:
> So.ok. Looks like this is misconfig....
> I just restore from backup and now works well
>

Great to hear. I will answer your question below anyway to help avoid 
future issues...

> ??, 2 ???. 2023??. ? 18:05, Alexey?? Gruzdov:
>
>     Hello Guys!
>
>     Could you explain?me case when?the external acl couldn't to be run
>     by squid.
>

There are three cases when an "external" type ACL has troubles:

 ?1) when there are OS permission issues with the helper binary/script.

This can show up as either Squid not being allowed to run the helper, or 
as the helper existing (maybe "crashing") when it tries to use forbidden 
resources.

2) when the ACL is being checked in a "fast" group (aka synchronous) 
access check

The helper lookup is asynchronous, so does not work inn the synchronous 
checks. However there is a cache of previous helper checks which may 
have the result - so long as there is an identical previous lookup whose 
result has not yet reached its TTL, this cache can supply the answer. So 
external ACL can have the **appearance** of working in simple tests or 
some types of traffic.

3) when the ACL is used conditionally

Squid helpers are only started as-needed. Immediately after startup 
there may be traffic that goes through which does not need to check the 
external ACL, so the helper does not get started for a while. Also, as 
mentioned above there is the helper cache, so at time there may also be 
traffic that gets answered by that instead of waiting on the helper 
lookup. At times both of these may be having an effect, for example 
after a helper crash/exit or reconfigure of Squid.


HTH
Amos



From my.shellac at gmail.com  Sun Jun  4 12:30:06 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Sun, 4 Jun 2023 17:30:06 +0500
Subject: [squid-users] External ACL doesn't used
In-Reply-To: <838f5767-5a1e-e810-1cbb-a26087256aa3@treenet.co.nz>
References: <CAFqyDwASh6uDDsoMAvPiWV25dB8kniCZtdGf+7QBm7SrfPZXHQ@mail.gmail.com>
 <CAFqyDwA4OCa61SJoE=+ip=ctFP+=BQkc32P6sca+5a+LbjoPVQ@mail.gmail.com>
 <838f5767-5a1e-e810-1cbb-a26087256aa3@treenet.co.nz>
Message-ID: <CAFqyDwD6A6hG5NJ-J=vOL8Wb8aeGTd_wSR0Kh+hVSx=yQTDKgQ@mail.gmail.com>

Hello Amos!

Thank you very much for you explanation!

To be honest I didn?t get really what this issue was. This was really
strange.

Because ttl option of my external acl is 10 sec ( I really need this value )

Also I tried restart my squid docker and server at whole - and this didn?t
help. I saw in the log just silence of calling of my external helper ACL.
But this ext ACL helper must to call for each proxy request?..


Then I just to solved to restore from backup and got it working again. I
tend to think that it is possible to change the config - although it looks
doubtful?.


Ok!
Thanks again !

On Sat, 3 Jun 2023 at 14:30, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 3/06/2023 3:14 am, Alexey?? Gruzdov wrote:
> > So.ok. Looks like this is misconfig....
> > I just restore from backup and now works well
> >
>
> Great to hear. I will answer your question below anyway to help avoid
> future issues...
>
> > ??, 2 ???. 2023??. ? 18:05, Alexey?? Gruzdov:
> >
> >     Hello Guys!
> >
> >     Could you explain me case when the external acl couldn't to be run
> >     by squid.
> >
>
> There are three cases when an "external" type ACL has troubles:
>
>   1) when there are OS permission issues with the helper binary/script.
>
> This can show up as either Squid not being allowed to run the helper, or
> as the helper existing (maybe "crashing") when it tries to use forbidden
> resources.
>
> 2) when the ACL is being checked in a "fast" group (aka synchronous)
> access check
>
> The helper lookup is asynchronous, so does not work inn the synchronous
> checks. However there is a cache of previous helper checks which may
> have the result - so long as there is an identical previous lookup whose
> result has not yet reached its TTL, this cache can supply the answer. So
> external ACL can have the **appearance** of working in simple tests or
> some types of traffic.
>
> 3) when the ACL is used conditionally
>
> Squid helpers are only started as-needed. Immediately after startup
> there may be traffic that goes through which does not need to check the
> external ACL, so the helper does not get started for a while. Also, as
> mentioned above there is the helper cache, so at time there may also be
> traffic that gets answered by that instead of waiting on the helper
> lookup. At times both of these may be having an effect, for example
> after a helper crash/exit or reconfigure of Squid.
>
>
> HTH
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230604/63d282c5/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun  5 14:31:44 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 5 Jun 2023 10:31:44 -0400
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <CADJd0Y2OsT-uZjHFMSUmCnGbAZq9FdbUhR6s4RfsfHdMUkbz2A@mail.gmail.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
 <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
 <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>
 <CADJd0Y2OsT-uZjHFMSUmCnGbAZq9FdbUhR6s4RfsfHdMUkbz2A@mail.gmail.com>
Message-ID: <55a12ae0-9a7b-28e5-a8d1-03a0c2752cbe@measurement-factory.com>

On 6/2/23 03:29, Andrey K wrote:

>  > Can you repeat this test and share a pointer to the corresponding
>  > compressed cache.log, containing those 500 (or fewer, as long as the
>  > problem is reproduced!) concurrent transactions. One or many of those
>  > concurrent transactions resulted in the unwanted entry deletion. The log
>  > may show what happened in that case.

> I cleared the rock cache, set the debug level, restarted squid, cleared 
> the cache.log, ran 500-threads test, waited for it to finish and 
> launched curl to make sure it returned TCP_MISS.
> Then stopped squid to limit the cache.log file.


Thank you for sharing that log! I only had time to study a few misses. 
They all stemmed from the same sequence of events:

1. A collapsed request finds the corresponding entry in the cache.
2. Squid decides that this request should open the disk file.
3. The rock disk entry is still being written (i.e. "swapped out"),
    so the attempt to swap it in fails (TCP_SWAPFAIL_MISS).
4. The request goes to the origin server.
5. The fresh response deletes the existing cached entry.
6. When a subsequent request finds the cached entry marked for
    deletion, it declares a cache miss (TCP_MISS) and goes to step 4.

Disclaimer: The above sequence of events causes misses, but it may not 
be the only or even the primary cause. I do not have enough free time to 
rule out or confirm other causes (and order them by severity).


Squid can (and should) handle concurrent swapout/swapins better, and we 
may be able to estimate that improvement potential for your workload 
without significant development, but, for the next step, I suggest 
disabling cache_dir and testing whether your get substantially better 
results with memory cache alone. Shared memory cache also has periods 
where a being-written entry cannot be read, but, compared to the disk 
cache, those periods are much shorter IIRC. I would like to confirm that 
this simplified mode of operation works well for your workload before I 
suggest code changes that would rely, in part, on this mode.


Thank you,

Alex.


> ??, 1 ???. 2023??. ? 19:15, Alex Rousskov:
> 
>     On 6/1/23 05:20, Andrey K wrote:
> 
>      >? > The next step I would recommend is to study the very first
>     cache miss
>      >? > _after_ the 500 or 200 concurrent threads test. Doing so may
>     shed light
>      >? > on why Squid is refusing to serve that (presumably cached)
>     object from
>      >? > the cache. I suspect that the object was marked for deletion
>     earlier
> 
>      > openForReadingAt: cannot open marked entry 11138 for reading
>     cache_mem_map
>      > openForReadingAt: cannot open marked entry 719406 for reading
>     /data/squid.user/cache_map
> 
> 
>     The debugging log you have shared confirms that Squid deleted[1] the
>     previously cached entry, from both caches (memory and disk). Now comes
>     the hard part -- figuring out why Squid deleted that entry.
> 
>      > I cleared the rock cache, changed the squid.conf (added debug_options
>      > ALL,9), restarted the squid, ran a test with 500 concurrent threads
> 
>     Can you repeat this test and share a pointer to the corresponding
>     compressed cache.log, containing those 500 (or fewer, as long as the
>     problem is reproduced!) concurrent transactions. One or many of those
>     concurrent transactions resulted in the unwanted entry deletion. The
>     log
>     may show what happened in that case.
> 
>     FWIW, I do not recommend spending your time analyzing that huge log.
>     Efficient analysis requires specialized knowledge in this case. I will
>     share the results here, of course.
> 
> 
>     Thank you,
> 
>     Alex.
> 
>     [1] That entry deletion does not imply that all (or any) of the cached
>     entry bytes are gone from the cache file on disk. It is likely that
>     only
>     the shared memory _index_ for that disk file was adjusted in your micro
>     test. That index adjustment is enough for Squid to declare a cache miss.
> 
> 
>      > ??, 31 ??? 2023??. ? 16:43, Alex Rousskov:
>      >
>      >? ? ?On 5/31/23 02:56, Andrey K wrote:
>      >
>      >? ? ? >? > Do you get close to 100% hit ratio if clients access
>     these URLs
>      >? ? ? >? > sequentially rather than concurrently? If not, then
>     focus on that
>      >? ? ? >? > problem before you open the collapsed forwarding
>     Pandora box.
>      >? ? ? > When I run curl sequentially?like this:
>      >? ? ? > for i in `seq 500`; do curl --tlsv1.2 -k ? --proxy
>     0001vsg01:3131
>      >? ? ? ?-v
>      >? ? ? > $URL??>/dev/null 2>&1; done
>      >? ? ? > I get only the first request with a status TCP_MISS?and all
>      >? ? ?others with
>      >? ? ? > TCP_MEM_HIT:
>      >? ? ? >? ? ? Cnt Status? ? ? ? ? ? Parent
>      >? ? ? >? ? 499 TCP_MEM_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >
>      >? ? ?Excellent. This confirms that your Squid can successfully
>     cache this
>      >? ? ?object (in memory).
>      >
>      >
>      >? ? ? > It is interesting to note that on both squid versions if I
>     run a
>      >? ? ? > separate curl after processing 500 or 200 concurrent
>     threads, I
>      >? ? ?get a
>      >? ? ? > result with the status TCP_MISS/200
>      >
>      >? ? ?The next step I would recommend is to study the very first
>     cache miss
>      >? ? ?_after_ the 500 or 200 concurrent threads test. Doing so may
>     shed light
>      >? ? ?on why Squid is refusing to serve that (presumably cached)
>     object from
>      >? ? ?the cache. I suspect that the object was marked for deletion
>     earlier,
>      >? ? ?but we should check before spending more time on more complex
>     triage of
>      >? ? ?concurrent cases. If you can share a (link to) compressed ALL,9
>      >? ? ?cache.log from that single transaction against Squid v6, I
>     may be able
>      >? ? ?to help you with that step.
>      >
>      >
>      >? ? ?Cheers,
>      >
>      >? ? ?Alex.
>      >
>      >
>      >? ? ? >? > What is your Squid version? Older Squids have more
>     collapsed
>      >? ? ?forwarding
>      >? ? ? >? > bugs than newer ones. I recommend testing with Squid v6 or
>      >? ? ?master/v7, at
>      >? ? ? >? > least to confirm that the problem is still present in
>     the latest
>      >? ? ? >? > official code.
>      >? ? ? > I run tests on SQUID 5.9.
>      >? ? ? > We compiled?6.0.2 (with disabled delay-pools) and
>     increased memory
>      >? ? ? > parameters:
>      >? ? ? >? ? cache_mem 2048 MB
>      >? ? ? >? ? maximum_object_size_in_memory 10 MB
>      >? ? ? > The complete configuration is shown below.
>      >? ? ? >
>      >? ? ? > Now on the version 6.0.2 we have the next results:
>      >? ? ? > 500 threads -? Hit ratio 3.8%:
>      >? ? ? >? ? ? ? 3 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 2 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ?16 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? 467 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ?12 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > 200 threads - 6%
>      >? ? ? >? ? ? ? 6 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ?10 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? 176 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ? 8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > 50 threads - 82%
>      >? ? ? >? ? ? ?30 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ?11 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? ? 8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >
>      >? ? ? > The results are slightly worse than they were on the
>     version 5.9.
>      >? ? ? > It is interesting to note that on both squid versions if I
>     run a
>      >? ? ? > separate curl after processing 500 or 200 concurrent
>     threads, I
>      >? ? ?get a
>      >? ? ? > result with the status TCP_MISS/200, although the
>     requested URL is
>      >? ? ? > already in the rock cache (I can see it in the contents of
>     the cache
>      >? ? ? > using the utility I developed rock_cache_dump.pl
>     <http://rock_cache_dump.pl>
>      >? ? ?<http://rock_cache_dump.pl <http://rock_cache_dump.pl>>
>      >? ? ? > <http://rock_cache_dump.pl <http://rock_cache_dump.pl>
>     <http://rock_cache_dump.pl <http://rock_cache_dump.pl>>>:
>      >? ? ? > $VAR1 = {
>      >? ? ? >? ? ? ? ? ? '1' => {
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ?'VERSION' => 'Wed May 31 09:18:05 2023',
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ?'KEY_MD5' =>
>     'e5eb10f0ab7d84ff9d3fd1e5a6d3eb9c',
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ?'OBJSIZE' => 446985,
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ?'STD_LFS' => {
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'lastref' => 'Wed May 31
>      >? ? ?09:18:05 2023',
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'flags' => '0x4004',
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'expires' => 'Wed May 31
>      >? ? ?15:18:05 2023',
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'swap_file_sz' => 0,
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'refcount' => 1,
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'lastmod' => 'Wed Jun 29
>      >? ? ?16:09:14 2016',
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 'timestamp' => 'Wed May 31
>      >? ? ?09:18:05 2023'
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? },
>      >? ? ? >? ? ? ? ? ? ? ? ? ? ?'URL' =>
>      >? ? ? >
>      >   
>      ?'https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>>'
>      >? ? ? >? ? ? ? ? ? ? ? ? ?}
>      >? ? ? >? ? ? ? ? };
>      >? ? ? >
>      >? ? ? > ).
>      >? ? ? >
>      >? ? ? >? > How much RAM does your server have? You are using default
>      >? ? ?256MB memory
>      >? ? ? >? > cache (cache_mem). If you have spare memory, make your
>     memory
>      >? ? ?cache much
>      >? ? ? >? > larger: A rock cache_dir cannot (yet) share the response
>      >? ? ?_while_ the
>      >? ? ? >? > response is being written to disk, so relying on
>     cache_dir too
>      >? ? ?much will
>      >? ? ? >? > decrease your hit ratio, especially in a collapsed
>     forwarding
>      >? ? ? > environment.
>      >? ? ? > The VM has 32 GB RAM. I configured cache_mem 2048 MB on
>     the 6.0.2
>      >? ? ?version.
>      >? ? ? >
>      >? ? ? >? > Is your Squid built with --enable-delay-pools? If yes,
>      >? ? ?TCP_MISS does not
>      >? ? ? >? > necessarily mean a cache miss (an old Squid bug), even
>     if you
>      >? ? ?do not use
>      >? ? ? >? > any delay pools.
>      >? ? ? > Yes, delay pools on the version 5.9 were enabled though?we
>     don't use
>      >? ? ? > them. I disabled this feature on the 6.0.2 version.
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? > Since you are trying to cache objects lager than 512KB, see
>      >? ? ? >? > maximum_object_size_in_memory.
>      >? ? ? > I configured maximum_object_size_in_memory 10 MB?on the 6.0.2
>      >? ? ?version
>      >? ? ? > (as videochunks are less than 7 MB).
>      >? ? ? >
>      >? ? ? >? > Consider making your test much longer (more sequential
>      >? ? ?requests per
>      >? ? ? >? > client/curl worker), to see whether the cache becomes
>     "stable"
>      >? ? ?after one
>      >? ? ? >? > of the first transactions manages to fully cache the
>     response.
>      >? ? ?This may
>      >? ? ? >? > not help with older Squids, but might help with newer ones.
>      >? ? ?However, you
>      >? ? ? >? > should not test using real origin servers (that you do not
>      >? ? ?control)!
>      >? ? ? > I don't have any of my own web servers for tests, so I
>     choose some
>      >? ? ? > resources on the public internet that have a robust
>     infrastructure.
>      >? ? ? > I will conduct the longer tests next week.
>      >? ? ? >
>      >? ? ? > Kind regards,
>      >? ? ? >? ? ? ? Ankor.
>      >? ? ? >
>      >? ? ? > *squid.conf*
>      >? ? ? > workers 21
>      >? ? ? >
>      >? ? ? > sslcrtd_program
>      >? ? ?/data/squid.user/usr/lib/squid/security_file_certgen -s
>      >? ? ? > /data/squid.user/var/lib/squid/ssl_db -M 20MB
>      >? ? ? > sslcrtd_children 21
>      >? ? ? >
>      >? ? ? > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %>a
>      >? ? ? > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
>      >? ? ? >
>      >? ? ? > logfile_rotate 0
>      >? ? ? > access_log daemon:/var/log/squid.user/access.log
>      >? ? ? > logformat=extended-squid on-error=drop
>      >? ? ? >
>      >? ? ? > cache_peer parent_proxy ?parent 3128 0
>      >? ? ? > never_direct allow all
>      >? ? ? >
>      >? ? ? > cachemgr_passwd pass config
>      >? ? ? >
>      >? ? ? > acl PURGE method PURGE
>      >? ? ? > http_access allow PURGE
>      >? ? ? >
>      >? ? ? > http_access allow all
>      >? ? ? >
>      >? ? ? > http_port 3131 ssl-bump generate-host-certificates=on
>      >? ? ? > dynamic_cert_mem_cache_size=20MB
>      >? ? ? > tls-cert=/etc/squid.user/sslbump/bump.crt
>      >? ? ? > tls-key=/etc/squid.user/sslbump/bump.key
>      >? ? ? > sslproxy_cert_error allow all
>      >? ? ? >
>      >? ? ? > acl step1 at_step SslBump1
>      >? ? ? > acl step2 at_step SslBump2
>      >? ? ? > acl step3 at_step SslBump3
>      >? ? ? >
>      >? ? ? > ssl_bump peek step1
>      >? ? ? > ssl_bump bump step2
>      >? ? ? > ssl_bump bump step3
>      >? ? ? >
>      >? ? ? > cache_dir rock /data/squid.user/cache 20000 max-size=12000000
>      >? ? ? > cache_swap_low 85
>      >? ? ? > cache_swap_high 90
>      >? ? ? >
>      >? ? ? > collapsed_forwarding on
>      >? ? ? > cache_mem 2048 MB
>      >? ? ? > maximum_object_size_in_memory 10 MB
>      >? ? ? >
>      >? ? ? > pinger_enable off
>      >? ? ? > max_filedesc 8192
>      >? ? ? > shutdown_lifetime 5 seconds
>      >? ? ? > netdb_filename none
>      >? ? ? > log_icp_queries off
>      >? ? ? >
>      >? ? ? > via off
>      >? ? ? > forwarded_for delete
>      >? ? ? >
>      >? ? ? > client_request_buffer_max_size 100 MB
>      >? ? ? >
>      >? ? ? > coredump_dir /data/squid.user/var/cache/squid
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > ??, 29 ??? 2023??. ? 23:17, Alex Rousskov
>      >? ? ? > <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>
>      >? ? ? > <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>>>:
>      >? ? ? >
>      >? ? ? >? ? ?On 5/29/23 10:43, Andrey K wrote:
>      >? ? ? >
>      >? ? ? >? ? ? > We need to configure a dedicated proxy server to
>     provide
>      >? ? ?caching of
>      >? ? ? >? ? ? > online video broadcasts in order to reduce the load
>     on the
>      >? ? ?uplink
>      >? ? ? >? ? ?proxy.
>      >? ? ? >? ? ? > Hundreds of users will access the same video-chunks
>      >? ? ?simultaneously.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > I developed a simple configuration for the test
>     purposes
>      >? ? ?(it is
>      >? ? ? >? ? ?shown
>      >? ? ? >? ? ? > below).
>      >? ? ? >? ? ? > The *collapsed_forwarding*?option is on.
>      >? ? ? >
>      >? ? ? >? ? ?Do you get close to 100% hit ratio if clients access
>     these URLs
>      >? ? ? >? ? ?sequentially rather than concurrently? If not, then
>     focus on that
>      >? ? ? >? ? ?problem before you open the collapsed forwarding
>     Pandora box.
>      >? ? ? >
>      >? ? ? >? ? ?What is your Squid version? Older Squids have more
>     collapsed
>      >? ? ?forwarding
>      >? ? ? >? ? ?bugs than newer ones. I recommend testing with Squid v6 or
>      >? ? ? >? ? ?master/v7, at
>      >? ? ? >? ? ?least to confirm that the problem is still present in
>     the latest
>      >? ? ? >? ? ?official code.
>      >? ? ? >
>      >? ? ? >? ? ?How much RAM does your server have? You are using default
>      >? ? ?256MB memory
>      >? ? ? >? ? ?cache (cache_mem). If you have spare memory, make your
>     memory
>      >? ? ?cache
>      >? ? ? >? ? ?much
>      >? ? ? >? ? ?larger: A rock cache_dir cannot (yet) share the response
>      >? ? ?_while_ the
>      >? ? ? >? ? ?response is being written to disk, so relying on cache_dir
>      >? ? ?too much
>      >? ? ? >? ? ?will
>      >? ? ? >? ? ?decrease your hit ratio, especially in a collapsed
>     forwarding
>      >? ? ? >? ? ?environment.
>      >? ? ? >
>      >? ? ? >? ? ?Is your Squid built with --enable-delay-pools? If yes,
>      >? ? ?TCP_MISS does
>      >? ? ? >? ? ?not
>      >? ? ? >? ? ?necessarily mean a cache miss (an old Squid bug), even
>     if you
>      >? ? ?do not
>      >? ? ? >? ? ?use
>      >? ? ? >? ? ?any delay pools.
>      >? ? ? >
>      >? ? ? >? ? ?Since you are trying to cache objects lager than
>     512KB, see
>      >? ? ? >? ? ?maximum_object_size_in_memory.
>      >? ? ? >
>      >? ? ? >? ? ?Consider making your test much longer (more sequential
>      >? ? ?requests per
>      >? ? ? >? ? ?client/curl worker), to see whether the cache becomes
>      >? ? ?"stable" after
>      >? ? ? >? ? ?one
>      >? ? ? >? ? ?of the first transactions manages to fully cache the
>      >? ? ?response. This may
>      >? ? ? >? ? ?not help with older Squids, but might help with newer
>     ones.
>      >? ? ?However,
>      >? ? ? >? ? ?you
>      >? ? ? >? ? ?should not test using real origin servers (that you do not
>      >? ? ?control)!
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ? > Could you clarify if this behavior of my squid is
>      >? ? ? >? ? ? > a bug/misconfiguration, or if I'm running into server
>      >? ? ?performance
>      >? ? ? >? ? ? > limitations (squid is running on a VM with 22 cores)?
>      >? ? ? >
>      >? ? ? >? ? ?Most likely, reduction of hit ratio with increase of
>      >? ? ?concurrency is
>      >? ? ? >? ? ?_not_ a performance limitation.
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ?HTH,
>      >? ? ? >
>      >? ? ? >? ? ?Alex.
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ? > I selected a couple of cacheable resources in the
>     internet for
>      >? ? ? >? ? ?testing:
>      >? ? ? >? ? ? >? ?- small size (~400 KB):
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >
>     https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>>>
>      >? ? ? >? ? ? >? ?- large (~8 MB):
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >
>     https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>>>
>      >? ? ? >? ? ? > To test simultaneous connections I am forking curl
>     using a
>      >? ? ?simple
>      >? ? ? >? ? ?script
>      >? ? ? >? ? ? > (it is also shown below).
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > When I run a test (500 curl threads to
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >
>     https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf> <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf <https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf>>>>) I see lots of?TCP_MISS/200 with?FIRSTUP_PARENT/parent_proxy records in the logs.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > A simple analysis shows?a low percentage of cache hits:
>      >? ? ? >? ? ? > cat /var/log/squid.user/access.log| grep
>     '2023-05-29 14' |
>      >? ? ?grep
>      >? ? ? >? ? ?pdf ?|
>      >? ? ? >? ? ? > awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? >? ? ? ?24 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? 457 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >? ? ? ?10 TCP_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ? 9 TCP_SWAPFAIL_MISS/200/200
>     FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > So the Hit ratio is about (500-457-9)*100/500=6.8%
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > Almost the same situation we see when run 200 threads:
>      >? ? ? >? ? ? > cat /var/log/squid.user/access.log| grep
>     '2023-05-29 15:45' |
>      >? ? ? >? ? ?grep pdf
>      >? ? ? >? ? ? >? ?| awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? >? ? ? ? 4 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? 140 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >? ? ? ?40 TCP_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ?16 TCP_SWAPFAIL_MISS/200/200
>     FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > This time the Hit ratio is about
>     (200-140-16)*100/500=21%
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > With 50 threads the Hit ratio is 90%:
>      >? ? ? >? ? ? > cat /var/log/squid.user/access.log| grep
>     '2023-05-29 15:50' |
>      >? ? ? >? ? ?grep pdf
>      >? ? ? >? ? ? >? ?| awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? >? ? ? ?27 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >? ? ? ?18 TCP_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ? 4 TCP_SWAPFAIL_MISS/200/200
>     FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > I thought that it should always be near 99% - only
>     the first
>      >? ? ? >? ? ?request to
>      >? ? ? >? ? ? > an URL should be forwarded to the parent proxy and all
>      >? ? ?subsequent
>      >? ? ? >? ? ? > requests should be served from the cache.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > The situation is even worse with downloading a
>     large file:
>      >? ? ? >? ? ? > 500 threads (0.4%):
>      >? ? ? >? ? ? > cat /var/log/squid.user/access.log| grep
>     '2023-05-29 17:2'
>      >? ? ?| grep
>      >? ? ? >? ? ?pdf ?|
>      >? ? ? >? ? ? > awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? >? ? ? ?10 TCP_CF_MISS/200/200
>     FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >? ? ? ? 2 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? 488 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > 200 threads (3%):
>      >? ? ? >? ? ? > cat /var/log/squid.user/access.log| grep
>     '2023-05-29 17:3'
>      >? ? ?| grep
>      >? ? ? >? ? ?pdf ?|
>      >? ? ? >? ? ? > awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? >? ? ? ? 9 TCP_CF_MISS/200/200
>     FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >? ? ? ? 6 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? 180 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >? ? ? ? 5 TCP_SWAPFAIL_MISS/200/200
>     FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > 50 threads (98%):
>      >? ? ? >? ? ? > cat /var/log/squid.user/access.log| grep
>     '2023-05-29 17:36' |
>      >? ? ? >? ? ?grep pdf
>      >? ? ? >? ? ? >? ?| awk '{print $5" " $10}' | sort | uniq -c
>      >? ? ? >? ? ? >? ? ? ?25 TCP_CF_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ?12 TCP_CF_MISS/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ?12 TCP_HIT/200/- HIER_NONE/-
>      >? ? ? >? ? ? >? ? ? ? 1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > Could you clarify if this behavior of my squid is a
>      >? ? ? >? ? ? > bug/misconfiguration, or if I'm running into server
>      >? ? ?performance
>      >? ? ? >? ? ? > limitations (squid is running on a VM with 22 cores)?
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > Kind regards,
>      >? ? ? >? ? ? >? ? ? ?Ankor
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > *squid.conf:*
>      >? ? ? >? ? ? > workers 21
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > sslcrtd_program
>      >? ? ? >? ? ?/data/squid.user/usr/lib/squid/security_file_certgen -s
>      >? ? ? >? ? ? > /data/squid.user/var/lib/squid/ssl_db -M 20MB
>      >? ? ? >? ? ? > sslcrtd_children 21
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl|
>     %6tr %>a
>      >? ? ? >? ? ? > %Ss/%03>Hs/%<Hs %<st %rm %ru %un %Sh/%<A %mt %ea
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > logfile_rotate 0
>      >? ? ? >? ? ? > access_log daemon:/var/log/squid.user/access.log
>      >? ? ? >? ? ? > logformat=extended-squid on-error=drop
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > cache_peer parent_proxy ?parent 3128 0
>      >? ? ? >? ? ? > never_direct allow all
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > cachemgr_passwd pass config
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > acl PURGE method PURGE
>      >? ? ? >? ? ? > http_access allow PURGE
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > http_access allow all
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > http_port 3131 ssl-bump generate-host-certificates=on
>      >? ? ? >? ? ? > dynamic_cert_mem_cache_size=20MB
>      >? ? ? >? ? ? > tls-cert=/etc/squid.user/sslbump/bump.crt
>      >? ? ? >? ? ? > tls-key=/etc/squid.user/sslbump/bump.key
>      >? ? ? >? ? ? > sslproxy_cert_error allow all
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > acl step1 at_step SslBump1
>      >? ? ? >? ? ? > acl step2 at_step SslBump2
>      >? ? ? >? ? ? > acl step3 at_step SslBump3
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > ssl_bump peek step1
>      >? ? ? >? ? ? > ssl_bump bump step2
>      >? ? ? >? ? ? > ssl_bump bump step3
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > cache_dir rock /data/squid.user/cache 20000
>     max-size=12000000
>      >? ? ? >? ? ? > cache_swap_low 85
>      >? ? ? >? ? ? > cache_swap_high 90
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > *collapsed_forwarding on*
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > pinger_enable off
>      >? ? ? >? ? ? > max_filedesc 8192
>      >? ? ? >? ? ? > shutdown_lifetime 5 seconds
>      >? ? ? >? ? ? > netdb_filename none
>      >? ? ? >? ? ? > log_icp_queries off
>      >? ? ? >? ? ? > client_request_buffer_max_size 100 MB
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > via off
>      >? ? ? >? ? ? > forwarded_for delete
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > coredump_dir /data/squid.user/var/cache/squid
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > *curl_forker.sh:*
>      >? ? ? >? ? ? > #!/bin/sh
>      >? ? ? >? ? ? > N=100
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >     
>      ?URL=https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>>>
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > if [[ ?-n $1 && ?$1 =~ help$ ?]];
>      >? ? ? >? ? ? > then
>      >? ? ? >? ? ? >? ? ?echo "Usage: $0 [<cnt>] [<url>]"
>      >? ? ? >? ? ? >? ? ?echo
>      >? ? ? >? ? ? >? ? ?echo "Example: $0 10
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >
>     https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf> <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf <https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf>>>>";
>      >? ? ? >? ? ? >? ? ?echo
>      >? ? ? >? ? ? >? ? ?exit;
>      >? ? ? >? ? ? > fi
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > while [[ $# -gt 0 ]]
>      >? ? ? >? ? ? > do
>      >? ? ? >? ? ? >? ? if [[ $1 =~ ^[0-9]+$ ]]
>      >? ? ? >? ? ? >? ? then
>      >? ? ? >? ? ? >? ? ? ?N=$1
>      >? ? ? >? ? ? >? ? else
>      >? ? ? >? ? ? >? ? ? ?URL=$1
>      >? ? ? >? ? ? >? ? fi
>      >? ? ? >? ? ? >? ? shift
>      >? ? ? >? ? ? > done
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > echo $URL
>      >? ? ? >? ? ? > echo $N threads
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > for i in `seq $N`
>      >? ? ? >? ? ? > do
>      >? ? ? >? ? ? >? ? nohup curl --tlsv1.2 -k ? --proxy 0001vsg01:3131
>      ?-v $URL
>      >? ? ? >? ? ? ?>/dev/null
>      >? ? ? >? ? ? >? ?2>&1 &
>      >? ? ? >? ? ? > done
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > _______________________________________________
>      >? ? ? >? ? ? > squid-users mailing list
>      >? ? ? >? ? ? > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>      >? ? ? >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>>
>      >? ? ? >? ? ? > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>      >? ? ? >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>>
>      >? ? ? >
>      >? ? ? >? ? ?_______________________________________________
>      >? ? ? >? ? ?squid-users mailing list
>      >? ? ? > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>      >? ? ? >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>>
>      >? ? ? > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>      >? ? ? >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>>
>      >? ? ? >
>      >
> 


From ankor2023 at gmail.com  Tue Jun  6 13:07:33 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Tue, 6 Jun 2023 16:07:33 +0300
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <CADJd0Y3BBGLu1N60qvNzKToRNT-wN3c5_OhOKgQ8m4pFTgoJjg@mail.gmail.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
 <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
 <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>
 <CADJd0Y2OsT-uZjHFMSUmCnGbAZq9FdbUhR6s4RfsfHdMUkbz2A@mail.gmail.com>
 <55a12ae0-9a7b-28e5-a8d1-03a0c2752cbe@measurement-factory.com>
 <CADJd0Y3BBGLu1N60qvNzKToRNT-wN3c5_OhOKgQ8m4pFTgoJjg@mail.gmail.com>
Message-ID: <CADJd0Y2_5=bnbx57myZHpCTPxbtr6fBtYWBUAB_hEx-EZ1LHyA@mail.gmail.com>

Hello, Alex,

I have shortened the correspondence because it does not meet the size
requirements for the mailing list.

Thank you so much for your time, the analysis and recommendations.

I disabled the cache_dir and now squid works as expected -  there is only
one request to the original content server:
- on the small file:
      1 NONE_NONE/503/- HIER_NONE/-
      4 TCP_CF_HIT/200/- HIER_NONE/-
    128 TCP_HIT/200/- HIER_NONE/-
    366 TCP_MEM_HIT/200/- HIER_NONE/-
      1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy

- on the large file:
     17 TCP_CF_HIT/200/- HIER_NONE/-
    482 TCP_HIT/200/- HIER_NONE/-
      1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy

I think this configuration is perfect for caching online video broadcasts.
Chanks of video are required by clients simultaneously only for a short
period of time, so there is no need to save them to disk..
As my VM has 32 GB of RAM, I can configure a sufficient amount of
cache_mem, say 20000 MB to provide caching of video broadcasts.


Kind regards,
     Ankor.


>
> ??, 5 ???. 2023??. ? 17:31, Alex Rousskov <
> rousskov at measurement-factory.com>:
>
>> On 6/2/23 03:29, Andrey K wrote:
>>
>> >  > Can you repeat this test and share a pointer to the corresponding
>> >  > compressed cache.log, containing those 500 (or fewer, as long as the
>> >  > problem is reproduced!) concurrent transactions. One or many of those
>> >  > concurrent transactions resulted in the unwanted entry deletion. The
>> log
>> >  > may show what happened in that case.
>>
>> > I cleared the rock cache, set the debug level, restarted squid, cleared
>> > the cache.log, ran 500-threads test, waited for it to finish and
>> > launched curl to make sure it returned TCP_MISS.
>> > Then stopped squid to limit the cache.log file.
>>
>>
>> Thank you for sharing that log! I only had time to study a few misses.
>> They all stemmed from the same sequence of events:
>>
>> 1. A collapsed request finds the corresponding entry in the cache.
>> 2. Squid decides that this request should open the disk file.
>> 3. The rock disk entry is still being written (i.e. "swapped out"),
>>     so the attempt to swap it in fails (TCP_SWAPFAIL_MISS).
>> 4. The request goes to the origin server.
>> 5. The fresh response deletes the existing cached entry.
>> 6. When a subsequent request finds the cached entry marked for
>>     deletion, it declares a cache miss (TCP_MISS) and goes to step 4.
>>
>> Disclaimer: The above sequence of events causes misses, but it may not
>> be the only or even the primary cause. I do not have enough free time to
>> rule out or confirm other causes (and order them by severity).
>>
>>
>> Squid can (and should) handle concurrent swapout/swapins better, and we
>> may be able to estimate that improvement potential for your workload
>> without significant development, but, for the next step, I suggest
>> disabling cache_dir and testing whether your get substantially better
>> results with memory cache alone. Shared memory cache also has periods
>> where a being-written entry cannot be read, but, compared to the disk
>> cache, those periods are much shorter IIRC. I would like to confirm that
>> this simplified mode of operation works well for your workload before I
>> suggest code changes that would rely, in part, on this mode.
>>
>>
>> Thank you,
>>
>> Alex.
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230606/e2c4867b/attachment.htm>

From ankor2023 at gmail.com  Fri Jun  9 07:03:16 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Fri, 9 Jun 2023 10:03:16 +0300
Subject: [squid-users] Using tcp_outgoing_address with ACL
Message-ID: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>

Hello,

We use the tcp_outgoing_address feature to access some hosts using a
dedicated source IP address.

   acl domdst_SIProxy  dstdomain
"/data/squid.user/etc/squid/categories/domdst_SIProxy"
   tcp_outgoing_address 10.72.235.129 domdst_SIProxy

It works fine, but logs are flooded with warnings like this:
   2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is used in context
without an HTTP request. Assuming mismatch.

I found a similar case:
http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
where Amos suggested using a patch as a solution.
We have Squid Version 5.5. Is there a similar patch for our version, or can
we just ignore these messages?

Kind regards,
       Ankor.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230609/a307819d/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jun 12 07:53:53 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 12 Jun 2023 10:53:53 +0300
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
Message-ID: <001401d99d03$09206140$1b6123c0$@gmail.com>

Hey Ankor,

There is some missing context so I would be able to reproduce this issue.
Is this some kind of CONNECT request?

If you can describe in more technical details the setup and what client are you using,
Maybe couple sanitized log lines it would help to understand better the scenario.

Eliezer

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Andrey K
Sent: Friday, June 9, 2023 10:03
To: Squid Users <squid-users at lists.squid-cache.org>; Amos Jeffries <squid3 at treenet.co.nz>
Subject: [squid-users] Using tcp_outgoing_address with ACL

Hello, 

We use the tcp_outgoing_address feature to access some hosts using a dedicated source IP address.

   acl domdst_SIProxy  dstdomain "/data/squid.user/etc/squid/categories/domdst_SIProxy"
   tcp_outgoing_address 10.72.235.129 domdst_SIProxy

It works fine, but logs are flooded with warnings like this:
   2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is used in context without an HTTP request. Assuming mismatch.

I found a similar case: http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html where Amos suggested using a patch as a solution.
We have Squid Version 5.5. Is there a similar patch for our version, or can we just ignore these messages?

Kind regards,
       Ankor.



From ngtech1ltd at gmail.com  Mon Jun 12 08:08:04 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 12 Jun 2023 11:08:04 +0300
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <CADJd0Y2_5=bnbx57myZHpCTPxbtr6fBtYWBUAB_hEx-EZ1LHyA@mail.gmail.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
 <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
 <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>
 <CADJd0Y2OsT-uZjHFMSUmCnGbAZq9FdbUhR6s4RfsfHdMUkbz2A@mail.gmail.com>
 <55a12ae0-9a7b-28e5-a8d1-03a0c2752cbe@measurement-factory.com>
 <CADJd0Y3BBGLu1N60qvNzKToRNT-wN3c5_OhOKgQ8m4pFTgoJjg@mail.gmail.com>
 <CADJd0Y2_5=bnbx57myZHpCTPxbtr6fBtYWBUAB_hEx-EZ1LHyA@mail.gmail.com>
Message-ID: <001501d99d05$0429b5e0$0c7d21a0$@gmail.com>

Hey Ankor,

Thanks for sharing the scenario.
At the beginning I was thinking to myself: Why Squid? Is it the best choice for the scenario?
And after walking through my list of caching proxies, including couple I wrote myself I got to the conclusion:
Well.. Squid-Cache is simple to use and just works.
Compared to other caching mechanisms squid is so simple to configure and it really leaves dust
behind to all many other cache mechanisms.

Thanks,
Eliezer


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Andrey K
Sent: Tuesday, June 6, 2023 16:08
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Proxy server to support a large number of simultaneous requests

Hello, Alex,

I have shortened the correspondence because it does not meet the size requirements for the mailing list.

Thank you so much for your time, the analysis and recommendations.

I disabled the cache_dir and now squid works as expected -  there is only one request to the original content server:
- on the small file:
      1 NONE_NONE/503/- HIER_NONE/-
      4 TCP_CF_HIT/200/- HIER_NONE/-
    128 TCP_HIT/200/- HIER_NONE/-
    366 TCP_MEM_HIT/200/- HIER_NONE/-
      1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy

- on the large file:
     17 TCP_CF_HIT/200/- HIER_NONE/-
    482 TCP_HIT/200/- HIER_NONE/-
      1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy

I think this configuration is perfect for caching online video broadcasts. Chanks of video are required by clients simultaneously only for a short period of time, so there is no need to save them to disk..
As my VM has 32 GB of RAM, I can configure a sufficient amount of cache_mem, say 20000 MB to provide caching of video broadcasts.


Kind regards,
     Ankor.



??, 5 ???. 2023??. ? 17:31, Alex Rousskov <mailto:rousskov at measurement-factory.com>:
On 6/2/23 03:29, Andrey K wrote:

>  > Can you repeat this test and share a pointer to the corresponding
>  > compressed cache.log, containing those 500 (or fewer, as long as the
>  > problem is reproduced!) concurrent transactions. One or many of those
>  > concurrent transactions resulted in the unwanted entry deletion. The log
>  > may show what happened in that case.

> I cleared the rock cache, set the debug level, restarted squid, cleared 
> the cache.log, ran 500-threads test, waited for it to finish and 
> launched curl to make sure it returned TCP_MISS.
> Then stopped squid to limit the cache.log file.


Thank you for sharing that log! I only had time to study a few misses. 
They all stemmed from the same sequence of events:

1. A collapsed request finds the corresponding entry in the cache.
2. Squid decides that this request should open the disk file.
3. The rock disk entry is still being written (i.e. "swapped out"),
    so the attempt to swap it in fails (TCP_SWAPFAIL_MISS).
4. The request goes to the origin server.
5. The fresh response deletes the existing cached entry.
6. When a subsequent request finds the cached entry marked for
    deletion, it declares a cache miss (TCP_MISS) and goes to step 4.

Disclaimer: The above sequence of events causes misses, but it may not 
be the only or even the primary cause. I do not have enough free time to 
rule out or confirm other causes (and order them by severity).


Squid can (and should) handle concurrent swapout/swapins better, and we 
may be able to estimate that improvement potential for your workload 
without significant development, but, for the next step, I suggest 
disabling cache_dir and testing whether your get substantially better 
results with memory cache alone. Shared memory cache also has periods 
where a being-written entry cannot be read, but, compared to the disk 
cache, those periods are much shorter IIRC. I would like to confirm that 
this simplified mode of operation works well for your workload before I 
suggest code changes that would rely, in part, on this mode.


Thank you,

Alex.




From ngtech1ltd at gmail.com  Mon Jun 12 08:21:37 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 12 Jun 2023 11:21:37 +0300
Subject: [squid-users] TCP_TUNNEL/500 in squid logs in squid 5.9
In-Reply-To: <CALpWAcoD4NdJL78yU2o_9KvrKTc_jVLH0npDa-rE=j_BdDyOcg@mail.gmail.com>
References: <CALpWAcoD4NdJL78yU2o_9KvrKTc_jVLH0npDa-rE=j_BdDyOcg@mail.gmail.com>
Message-ID: <001701d99d06$e8b411a0$ba1c34e0$@gmail.com>

Hey Sachin,

What's the issue?
That the logs don't reflect the reality?

Thanks,
Eliezer


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of sachin gupta
Sent: Thursday, May 25, 2023 18:21
To: squid-users at lists.squid-cache.org
Subject: [squid-users] TCP_TUNNEL/500 in squid logs in squid 5.9

Hi All

We are migrating for squid 4.15 to squid 5.9. We are running our existing test suite to check if we pass our sanity testing.

For requests in transparent mode, though request passes and client get 200, in squid logs we are getting TCP_TUNNEL/500. We were not getting this issue with squid 4.15.

Client logs

curl -v https://origin/cache/0
*   Trying 10.80.96.68:443...
* TCP_NODELAY set
* Connected to origin (10.80.96.68) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-SHA
* ALPN, server did not agree to a protocol
* Server certificate:
*  subject: C=US; ST=CA; L=SF; O=SFDC; OU=0:ns.tester;1:mvp;2:mist51;3:na44;4:dev1; CN=origin
*  start date: Jul 26 06:59:41 2022 GMT
*  expire date: Jul 26 06:59:41 2023 GMT
*  subjectAltName: host "origin" matched cert's "origin"
*  issuer: C=US; ST=CA; L=SF; O=SFDC; OU=Edge; CN=ca
*  SSL certificate verify ok.
> GET /cache/0 HTTP/1.1
> Host: origin
> User-Agent: curl/7.67.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: origin
< Date: Thu, 25 May 2023 15:08:57 GMT
< Connection: close
< Content-Type: application/json
< Content-Length: 162
< Cache-Control: public, max-age=0
< Access-Control-Allow-Origin: *
< Access-Control-Allow-Credentials: true
< 
{"args":{},"headers":{"Accept":"*/*","Host":"origin","User-Agent":"curl/7.67.0","X-Origin-Server":"origin"},"origin":"10.80.96.3","url":"https://origin/cache/0"}
* Closing connection 0
* TLSv1.2 (OUT), TLS alert, close notify (256):

Squid access logs

[25/May/2023:15:08:57]      31 http://10.80.96.6:51028 - NONE_NONE/000 0 CONNECT http://10.80.96.68:443 tester HIER_NONE/- - - tester 746573746572 dagobah [-] - [-] - [-] - 0 0 - - [origin]
[25/May/2023:15:08:57]     40 http://10.80.96.6:51028 - TCP_TUNNEL/500 800 CONNECT origin:443 tester HIER_DIRECT/origin 10.80.96.68 - tester 746573746572 dagobah [-] - [-] - [-] - 1969 2769 4 33 [origin]

Can someone please help in this.

Regards
Sachin



From ngtech1ltd at gmail.com  Mon Jun 12 08:23:15 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 12 Jun 2023 11:23:15 +0300
Subject: [squid-users] Define Squid max connections limit
In-Reply-To: <CAG2Qp6u09jfFhUcDgEWq_S7ZTc=MQN6cV32C2sFR0YDfSyNSLA@mail.gmail.com>
References: <CAG2Qp6u09jfFhUcDgEWq_S7ZTc=MQN6cV32C2sFR0YDfSyNSLA@mail.gmail.com>
Message-ID: <001801d99d07$233d92b0$69b8b810$@gmail.com>

Hey Roberto,

How are you with this?
Still having issues?

Eliezer

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Roberto Carna
Sent: Wednesday, May 10, 2023 19:52
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Define Squid max connections limit

Dear all, is there any directive to define Squid max connections
limit, or do I have to increase the RAM and CPU instead?

Thanks a lot, regards!
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Mon Jun 12 08:25:48 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 12 Jun 2023 11:25:48 +0300
Subject: [squid-users] Squid Server on Android Devices?
In-Reply-To: <00da01d982b0$0bd69ed0$2383dc70$@graminsta.com.br>
References: <00da01d982b0$0bd69ed0$2383dc70$@graminsta.com.br>
Message-ID: <001901d99d07$7e8c5c00$7ba51400$@gmail.com>

Hey Marcelo,

Squid was not designed for Android devices and would need an expert to make it work on a simple Android device.
Squid-Cache is a server which provides services and it should be used on one.
If you wish to convert your S10 to a server that's another story and you are probably on the wrong place for that.

All The Bests,
Eliezer

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcelo Rodrigo - ProxyADS
Sent: Tuesday, May 9, 2023 22:54
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Server on Android Devices?


Hello.

Is it possible to run squid on Android devices?
I was thinking about using an android device as a peer.
I would like to run some tests. I got a Samsung S10 with Android 12 that already works with Ethernet Theatering and the USB-c/Ethernet cable.
Which Squid package should I use for that? This hardware is an ARM64.

I am thinking about root this device in order to run a SSH access to begin.

Any advices for this journey?

Marcelo




From sachin1.g at gmail.com  Mon Jun 12 14:26:36 2023
From: sachin1.g at gmail.com (sachin gupta)
Date: Mon, 12 Jun 2023 19:56:36 +0530
Subject: [squid-users] TCP_TUNNEL/500 in squid logs in squid 5.9
In-Reply-To: <001701d99d06$e8b411a0$ba1c34e0$@gmail.com>
References: <CALpWAcoD4NdJL78yU2o_9KvrKTc_jVLH0npDa-rE=j_BdDyOcg@mail.gmail.com>
 <001701d99d06$e8b411a0$ba1c34e0$@gmail.com>
Message-ID: <CALpWAcpCB=BNsQHTOVZoKbrr+cRkt-FuSWjp+akn2AbOdw_EZg@mail.gmail.com>

Hi

Further discussion happened on
https://bugs.squid-cache.org/show_bug.cgi?id=5274 and has more details.

But yes, the issue is the same for a connection just from squid logs: how
to know if a request has passed or failed.

Regards
Sachin

On Mon, Jun 12, 2023 at 1:51?PM <ngtech1ltd at gmail.com> wrote:

> Hey Sachin,
>
> What's the issue?
> That the logs don't reflect the reality?
>
> Thanks,
> Eliezer
>
>
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of sachin gupta
> Sent: Thursday, May 25, 2023 18:21
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] TCP_TUNNEL/500 in squid logs in squid 5.9
>
> Hi All
>
> We are migrating for squid 4.15 to squid 5.9. We are running our existing
> test suite to check if we pass our sanity testing.
>
> For requests in transparent mode, though request passes and client get
> 200, in squid logs we are getting TCP_TUNNEL/500. We were not getting this
> issue with squid 4.15.
>
> Client logs
>
> curl -v https://origin/cache/0
> *   Trying 10.80.96.68:443...
> * TCP_NODELAY set
> * Connected to origin (10.80.96.68) port 443 (#0)
> * ALPN, offering h2
> * ALPN, offering http/1.1
> * Cipher selection:
> ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
> * successfully set certificate verify locations:
> *   CAfile: /etc/pki/tls/certs/ca-bundle.crt
>   CApath: none
> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
> * TLSv1.2 (IN), TLS handshake, Server hello (2):
> * TLSv1.2 (IN), TLS handshake, Certificate (11):
> * TLSv1.2 (IN), TLS handshake, Server key exchange (12):
> * TLSv1.2 (IN), TLS handshake, Server finished (14):
> * TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
> * TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
> * TLSv1.2 (OUT), TLS handshake, Finished (20):
> * TLSv1.2 (IN), TLS change cipher, Change cipher spec (1):
> * TLSv1.2 (IN), TLS handshake, Finished (20):
> * SSL connection using TLSv1.2 / ECDHE-RSA-AES256-SHA
> * ALPN, server did not agree to a protocol
> * Server certificate:
> *  subject: C=US; ST=CA; L=SF; O=SFDC;
> OU=0:ns.tester;1:mvp;2:mist51;3:na44;4:dev1; CN=origin
> *  start date: Jul 26 06:59:41 2022 GMT
> *  expire date: Jul 26 06:59:41 2023 GMT
> *  subjectAltName: host "origin" matched cert's "origin"
> *  issuer: C=US; ST=CA; L=SF; O=SFDC; OU=Edge; CN=ca
> *  SSL certificate verify ok.
> > GET /cache/0 HTTP/1.1
> > Host: origin
> > User-Agent: curl/7.67.0
> > Accept: */*
> >
> * Mark bundle as not supporting multiuse
> < HTTP/1.1 200 OK
> < Server: origin
> < Date: Thu, 25 May 2023 15:08:57 GMT
> < Connection: close
> < Content-Type: application/json
> < Content-Length: 162
> < Cache-Control: public, max-age=0
> < Access-Control-Allow-Origin: *
> < Access-Control-Allow-Credentials: true
> <
>
> {"args":{},"headers":{"Accept":"*/*","Host":"origin","User-Agent":"curl/7.67.0","X-Origin-Server":"origin"},"origin":"10.80.96.3","url":"
> https://origin/cache/0"}
> * Closing connection 0
> * TLSv1.2 (OUT), TLS alert, close notify (256):
>
> Squid access logs
>
> [25/May/2023:15:08:57]      31 http://10.80.96.6:51028 - NONE_NONE/000 0
> CONNECT http://10.80.96.68:443 tester HIER_NONE/- - - tester 746573746572
> dagobah [-] - [-] - [-] - 0 0 - - [origin]
> [25/May/2023:15:08:57]     40 http://10.80.96.6:51028 - TCP_TUNNEL/500
> 800 CONNECT origin:443 tester HIER_DIRECT/origin 10.80.96.68 - tester
> 746573746572 dagobah [-] - [-] - [-] - 1969 2769 4 33 [origin]
>
> Can someone please help in this.
>
> Regards
> Sachin
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230612/78dddc6a/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun 15 00:13:34 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 15 Jun 2023 12:13:34 +1200
Subject: [squid-users] [squid-announce] Squid 6.0.3 beta is available
Message-ID: <bd6155ba-d4b8-28fb-dbb1-a9f50465b5f2@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the
availability of the Squid-6.0.3 beta release!


This release is a bug fix release resolving some issues
found in the prior Squid releases.

The major changes to be aware of:

  * Bug 5148: Log %Ss of failed tunnels as TCP_TUNNEL

When Squid failed to open a TCP connection while handling a
CONNECT request, Squid logged %Ss as NONE_NONE or TAG_NONE.
This Squid should now log TCP_TUNNEL for all CONNECT attempts.


  * Do not leak Security::CertErrors created in X509_verify_cert()

This bug shows up as a small and erratic memory leak when TLS
traffic is handled in production, which can disappear when
testing.


   All users of Squid-6 are encouraged to
   upgrade as soon as possible.

   All users of Squid-5 are encouraged to
   test this release and plan for upgrade as time permits.


See the ChangeLog for the full list of changes in this and
earlier releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v6/RELEASENOTES.html
when you are ready to make the switch to Squid-6

This new release can be downloaded from our HTTP or FTP servers

http://www.squid-cache.org/Versions/v6/
ftp://ftp.squid-cache.org/pub/squid/
ftp://ftp.squid-cache.org/pub/archive/6/

or the mirrors. For a list of mirror sites see

http://www.squid-cache.org/Download/http-mirrors.html
http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug
report.
https://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce


From ben.goz87 at gmail.com  Thu Jun 15 11:31:17 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 15 Jun 2023 14:31:17 +0300
Subject: [squid-users] Certificate error using using squid with tproxy
 configuration
Message-ID: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>

By the help of God.

Hi,
I'm using squid with tproxy including https interception configuration.

The squid version is:
$ /usr/local/squid/sbin/squid -v
Squid Cache: Version 7.0.0-VCS
Service Name: squid

This binary uses OpenSSL 3.0.2 15 Mar 2022. configure options:
 '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-icap-client'
'--enable-linux-netfilter'


And the tproxy configuration works perfectly using http without ssl,
But using ssl I'm getting in browser ssl error "ERR_SSL_PROTOCOL_ERROR"
And using curl I get the following output:

$ curl -iv https://www.google.com --cert ~/myCA.der
*   Trying 172.217.22.68:443...
* Connected to www.google.com (172.217.22.68) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* could not load PEM client certificate, OpenSSL error error:0480006C:PEM
routines::no start line, (no key found, wrong pass phrase, or wrong file
format?)
* Closing connection 0
curl: (58) could not load PEM client certificate, OpenSSL error
error:0480006C:PEM routines::no start line, (no key found, wrong pass
phrase, or wrong file format?)

Squid's configuration:
http_port 0.0.0.0:3130 tproxy ssl-bump \
  cert=/usr/local/squid/etc/ssl_cert/myCA.der \
  key=/usr/local/squid/etc/ssl_cert/myCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

iptables rule:
$ sudo iptables -t mangle -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DIVERT     tcp  --  anywhere             anywhere             socket
TPROXY     tcp  --  anywhere             anywhere             tcp dpt:http
TPROXY redirect 0.0.0.0:3129 mark 0x1/0x1
TPROXY     tcp  --  anywhere             anywhere             tcp dpt:https
TPROXY redirect 0.0.0.0:3130 mark 0x1/0x1

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination

Chain DIVERT (1 references)
target     prot opt source               destination
MARK       all  --  anywhere             anywhere             MARK set 0x1
ACCEPT     all  --  anywhere             anywhere

Did I miss something?

Thanks,
Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230615/7eef0174/attachment.htm>

From ben.goz87 at gmail.com  Thu Jun 15 11:51:40 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 15 Jun 2023 14:51:40 +0300
Subject: [squid-users] Certificate error using using squid with tproxy
 configuration
In-Reply-To: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>
References: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>
Message-ID: <CADAqQfz8h2PSu_7xSrQfc12LN9BxQXdyzeQBYXOTK9YN3_sd0Q@mail.gmail.com>

By the help of God

Update the squid.conf:
http_port 0.0.0.0:3128
http_port 0.0.0.0:3129 tproxy
http_port 0.0.0.0:3130 tproxy ssl-bump \
  cert=/usr/local/squid/etc/ssl_cert/myCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

# For squid 4.x
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB

acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all

Still the same issue.

??????? ??? ??, 15 ????? 2023 ?-14:31 ??? ?Ben Goz?? <?ben.goz87 at gmail.com
??>:?

> By the help of God.
>
> Hi,
> I'm using squid with tproxy including https interception configuration.
>
> The squid version is:
> $ /usr/local/squid/sbin/squid -v
> Squid Cache: Version 7.0.0-VCS
> Service Name: squid
>
> This binary uses OpenSSL 3.0.2 15 Mar 2022. configure options:
>  '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-icap-client'
> '--enable-linux-netfilter'
>
>
> And the tproxy configuration works perfectly using http without ssl,
> But using ssl I'm getting in browser ssl error "ERR_SSL_PROTOCOL_ERROR"
> And using curl I get the following output:
>
> $ curl -iv https://www.google.com --cert ~/myCA.der
> *   Trying 172.217.22.68:443...
> * Connected to www.google.com (172.217.22.68) port 443 (#0)
> * ALPN, offering h2
> * ALPN, offering http/1.1
> * could not load PEM client certificate, OpenSSL error error:0480006C:PEM
> routines::no start line, (no key found, wrong pass phrase, or wrong file
> format?)
> * Closing connection 0
> curl: (58) could not load PEM client certificate, OpenSSL error
> error:0480006C:PEM routines::no start line, (no key found, wrong pass
> phrase, or wrong file format?)
>
> Squid's configuration:
> http_port 0.0.0.0:3130 tproxy ssl-bump \
>   cert=/usr/local/squid/etc/ssl_cert/myCA.der \
>   key=/usr/local/squid/etc/ssl_cert/myCA.pem \
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
> iptables rule:
> $ sudo iptables -t mangle -L
> Chain PREROUTING (policy ACCEPT)
> target     prot opt source               destination
> DIVERT     tcp  --  anywhere             anywhere             socket
> TPROXY     tcp  --  anywhere             anywhere             tcp dpt:http
> TPROXY redirect 0.0.0.0:3129 mark 0x1/0x1
> TPROXY     tcp  --  anywhere             anywhere             tcp
> dpt:https TPROXY redirect 0.0.0.0:3130 mark 0x1/0x1
>
> Chain INPUT (policy ACCEPT)
> target     prot opt source               destination
>
> Chain FORWARD (policy ACCEPT)
> target     prot opt source               destination
>
> Chain OUTPUT (policy ACCEPT)
> target     prot opt source               destination
>
> Chain POSTROUTING (policy ACCEPT)
> target     prot opt source               destination
>
> Chain DIVERT (1 references)
> target     prot opt source               destination
> MARK       all  --  anywhere             anywhere             MARK set 0x1
> ACCEPT     all  --  anywhere             anywhere
>
> Did I miss something?
>
> Thanks,
> Ben
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230615/dfbb2130/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 15 13:08:14 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 15 Jun 2023 09:08:14 -0400
Subject: [squid-users] Certificate error using using squid with tproxy
 configuration
In-Reply-To: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>
References: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>
Message-ID: <64700b03-f0a7-bfb3-ddeb-a56184a61dbf@measurement-factory.com>

On 6/15/23 07:31, Ben Goz wrote:

> the tproxy configuration works perfectly using http without ssl,
> But using ssl I'm getting in browser ssl error "ERR_SSL_PROTOCOL_ERROR"


> http_port 0.0.0.0:3130 tproxy ...

This http_port is for plain text HTTP interception. The configuration 
needs an https_port (note the "s") dedicated to TLS interception instead.


> TPROXY ? ? tcp ?-- ?anywhere ? ? ? ? ? ? anywhere ? ? ? ? ? ? tcp 
> dpt:https TPROXY redirect 0.0.0.0:3130 mark 0x1/0x1

The above rule should redirect traffic to that https_port.


HTH,

Alex.



From ben.goz87 at gmail.com  Thu Jun 15 13:27:47 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 15 Jun 2023 16:27:47 +0300
Subject: [squid-users] Certificate error using using squid with tproxy
 configuration
In-Reply-To: <64700b03-f0a7-bfb3-ddeb-a56184a61dbf@measurement-factory.com>
References: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>
 <64700b03-f0a7-bfb3-ddeb-a56184a61dbf@measurement-factory.com>
Message-ID: <CADAqQfxGzi6r9V1AAeXBTPZaR4hNqjmjufpzRra+TuoVNxjU_A@mail.gmail.com>

By the help of God

The https interception guide in this link:
https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit#squid-configuration-file

is misleading as it uses http_port for ssl-bump and not https_port.




??????? ??? ??, 15 ????? 2023 ?-16:08 ??? ?Alex Rousskov?? <?
rousskov at measurement-factory.com??>:?

> On 6/15/23 07:31, Ben Goz wrote:
>
> > the tproxy configuration works perfectly using http without ssl,
> > But using ssl I'm getting in browser ssl error "ERR_SSL_PROTOCOL_ERROR"
>
>
> > http_port 0.0.0.0:3130 tproxy ...
>
> This http_port is for plain text HTTP interception. The configuration
> needs an https_port (note the "s") dedicated to TLS interception instead.
>
>
> > TPROXY     tcp  --  anywhere             anywhere             tcp
> > dpt:https TPROXY redirect 0.0.0.0:3130 mark 0x1/0x1
>
> The above rule should redirect traffic to that https_port.
>
>
> HTH,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230615/959a6873/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 15 14:39:29 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 15 Jun 2023 10:39:29 -0400
Subject: [squid-users] Certificate error using using squid with tproxy
 configuration
In-Reply-To: <CADAqQfxGzi6r9V1AAeXBTPZaR4hNqjmjufpzRra+TuoVNxjU_A@mail.gmail.com>
References: <CADAqQfw0Tk0JE0ppmWGKPGuYW_6Vnb90WKCu1GGvGKdfKZ0OSg@mail.gmail.com>
 <64700b03-f0a7-bfb3-ddeb-a56184a61dbf@measurement-factory.com>
 <CADAqQfxGzi6r9V1AAeXBTPZaR4hNqjmjufpzRra+TuoVNxjU_A@mail.gmail.com>
Message-ID: <37619ace-6155-b8e7-a3fa-d4b2a417c128@measurement-factory.com>

On 6/15/23 09:27, Ben Goz wrote:

> The https interception guide in this link:
> https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit#squid-configuration-file
> 
> is misleading 

I agree. That page should not use the word "intercept" when talking 
about HTTP CONNECT inspection and bumping -- CONNECT requests are not 
(normally) intercepted. Pull requests improving documentation welcome!


 > as it uses http_port for ssl-bump and not https_port.

Both directives support SslBump, but each works with a different kind of 
traffic.

Alex.


> ??????? ??? ??, 15 ????? 2023 ?-16:08 ??? ?Alex Rousskov?? 
> <?rousskov at measurement-factory.com 
> <mailto:rousskov at measurement-factory.com>??>:?
> 
>     On 6/15/23 07:31, Ben Goz wrote:
> 
>      > the tproxy configuration works perfectly using http without ssl,
>      > But using ssl I'm getting in browser ssl error
>     "ERR_SSL_PROTOCOL_ERROR"
> 
> 
>      > http_port 0.0.0.0:3130 <http://0.0.0.0:3130> tproxy ...
> 
>     This http_port is for plain text HTTP interception. The configuration
>     needs an https_port (note the "s") dedicated to TLS interception
>     instead.
> 
> 
>      > TPROXY ? ? tcp ?-- ?anywhere ? ? ? ? ? ? anywhere ? ? ? ? ? ? tcp
>      > dpt:https TPROXY redirect 0.0.0.0:3130 <http://0.0.0.0:3130> mark
>     0x1/0x1
> 
>     The above rule should redirect traffic to that https_port.
> 
> 
>     HTH,
> 
>     Alex.
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 



From Ralf.Hildebrandt at charite.de  Mon Jun 19 15:43:54 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 19 Jun 2023 17:43:54 +0200
Subject: [squid-users] ftp related crash in squid-6.0.0-20220905-r9358e99f9
Message-ID: <ZJB3usHKML99fdcd@charite.de>

Hi!

It seems I'm encountering an FTP related crash here; core.1678595 is
available for further debugging. 

I'm running squid-6.0.0-20220905-r9358e99f9 (since that's so far the
most stable version for me).

cache.log says:
===============

2023/06/19 17:31:10| conn3580032407 local=141.42.5.215:8080 remote=141.42.64.96:52710 FD 39958 flags=1: read/write failure: (110) Connection timed out
    current master transaction: master1645377270
2023/06/19 17:31:12| conn3572687242 local=141.42.5.215:8080 remote=172.29.144.96:51701 FD 41826 flags=1: read/write failure: (110) Connection timed out
    current master transaction: master1642214323
2023/06/19 17:31:17| FATAL: assertion failed: FwdState.cc:299: "!storedWholeReply_"
    current master transaction: master1650383954
    
This is the backtrace:
======================

Reading symbols from /usr/sbin/squid...
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

Program received signal SIGABRT, Aborted.
__GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
        set = {
          __val = {0, 115, 60684401525984, 11384875684165056000, 1, 
            140737488347504, 93825000028288, 11384875684165056000, 
            93824999874096, 140737488347504, 93825000028376, 140737488347536, 
            93824997547807, 93825140894448, 93826629547024, 140737344746339}
        }
        pid = <optimized out>
        tid = <optimized out>
        ret = <optimized out>
#1  0x00007ffff7615859 in __GI_abort () at abort.c:79
        save_stage = 1
        act = {
          __sigaction_handler = {
            sa_handler = 0x555555a65f1f,
            sa_sigaction = 0x555555a65f1f
          },
          sa_mask = {
            __val = {93825140894448, 93826629547024, 93824996975023, 155185, 
              1687188677, 0, 93826629547008, 93825491415392, 
              11384875684165056000, 111, 93825000028288, 93825000094136, 
              93824996978668, 93825000094136, 93824996974080, 93825000094136}
          },
          sa_flags = 1990456832,
          sa_restorer = 0x555555a6617d
        }
        sigs = {
          __val = {32, 0 <repeats 15 times>}
        }
#2  0x00005555559d9e8b in xassert (
    msg=msg at entry=0x555555a6617d "!storedWholeReply_", 
    file=file at entry=0x555555a65f1f "FwdState.cc", line=line at entry=299)
    at debug.cc:1266
        __FUNCTION__ = <optimized out>
#3  0x000055555570e9c2 in FwdState::completed (this=this at entry=0x55556c0a4748)
    at FwdState.cc:299
        __FUNCTION__ = "completed"
#4  0x00005555557128af in FwdState::complete (this=0x55556c0a4748)
    at FwdState.cc:589
        replyStatus = Http::scNone
        __FUNCTION__ = "complete"
#5  0x000055555587c796 in Client::completeForwarding (this=0x5555662702c8)
    at ../../src/base/RefCount.h:73
        __FUNCTION__ = "completeForwarding"
#6  0x0000555555882619 in Ftp::Gateway::completeForwarding (
    this=0x5555662702c8) at FtpGateway.cc:2651
        __FUNCTION__ = "completeForwarding"
#7  0x000055555587974b in Client::serverComplete2 (
    this=this at entry=0x5555662702c8) at Client.cc:214
        __FUNCTION__ = "serverComplete2"
#8  0x000055555587c3d4 in Client::serverComplete (this=0x5555662702c8)
    at Client.cc:198
        __FUNCTION__ = "serverComplete"
#9  0x000055555588154a in ftpReadQuit (ftpState=<optimized out>)
    at FtpGateway.cc:2288
No locals.
#10 0x00005555558816f6 in Ftp::Gateway::handleControlReply (
    this=0x5555662702c8) at FtpGateway.cc:1176
No locals.
#11 0x00005555558a15a5 in Ftp::Client::readControlReply (this=0x5555662702c8, 
    io=...) at FtpClient.cc:415
        __FUNCTION__ = "readControlReply"
        len = <optimized out>
#12 0x00005555558a7172 in CommCbMemFunT<Ftp::Client, CommIoCbParams>::doDial (
    this=0x5555ff888638) at ../../src/CommCalls.h:205
No locals.
#13 0x00005555558a6ee3 in JobDialer<Ftp::Client>::dial (this=0x5555ff888638, 
    call=...) at ../../src/base/AsyncJobCalls.h:170
        __FUNCTION__ = "dial"
#14 0x00005555558a7092 in AsyncCallT<CommCbMemFunT<Ftp::Client, CommIoCbParams> >::fire (this=<optimized out>) at ../../src/base/AsyncCall.h:143
No locals.
#15 0x00005555558d5952 in AsyncCall::make (this=0x5555ff888600)
    at AsyncCall.cc:44
        __FUNCTION__ = "make"
#16 0x00005555558d73d0 in AsyncCallQueue::fire (this=0x555555daff70)
    at ../../src/base/RefCount.h:73
        call = {
          p_ = 0x5555ff888600
        }
        made = true
        __FUNCTION__ = "fire"
#17 0x00005555557098cf in EventLoop::dispatchCalls (
    this=this at entry=0x7fffffffe940) at EventLoop.cc:144
        dispatchedSome = <optimized out>
#18 0x00005555557099c2 in EventLoop::runOnce (this=this at entry=0x7fffffffe940)
    at EventLoop.cc:121
        sawActivity = <optimized out>
        waitingEngine = 0x7fffffffe860
        __FUNCTION__ = <optimized out>
#19 0x0000555555709a2d in EventLoop::run (this=0x7fffffffe940)
    at EventLoop.cc:83
No locals.
#20 0x00005555557f9ad7 in SquidMain (argc=<optimized out>, 
    argv=<optimized out>) at main.cc:1691
        cmdLine = {
          argv_ = std::vector of length 3, capacity 3 = {
            0x555555cc3760 "/usr/sbin/squid", 0x555555cc3780 "-NsYC", 0x0},
          shortOptions_ = 0x555555cc3710 "CDFNRSYXa:d:f:hk:m::n:sl:u:vz?",
          longOptions_ = std::vector of length 5, capacity 8 = {{
              <option> = {
                name = 0x555555cc3860 "foreground",
                has_arg = 0,
                flag = 0x0,
                val = 2
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc3a60 "kid",
                has_arg = 1,
                flag = 0x0,
                val = 3
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc3a80 "help",
                has_arg = 0,
                flag = 0x0,
                val = 104
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc3aa0 "version",
                has_arg = 0,
                flag = 0x0,
                val = 118
              }, <No data fields>}, {
              <option> = {
                name = 0x0,
                has_arg = 0,
                flag = 0x0,
                val = 0
              }, <No data fields>}}
        }
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
        mainLoop = {
          errcount = 0,
          static Running = 0x7fffffffe940,
          last_loop = false,
          engines = std::vector of length 4, capacity 4 = {0x7fffffffe820, 
            0x555555c0dc80 <EventScheduler::_instance>, 0x7fffffffe840, 
            0x7fffffffe860},
          timeService = 0x7fffffffe880,
          primaryEngine = 0x7fffffffe860,
          loop_delay = 0,
          error = false,
          runOnceResult = false
        }
        signalEngine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b6ea38 <vtable for SignalEngine+16>
          }, <No data fields>}
        store_engine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b6ea10 <vtable for StoreRootEngine+16>
          }, <No data fields>}
        comm_engine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b7e138 <vtable for CommSelectEngine+16>
          }, <No data fields>}
        time_engine = {
          _vptr.Engine = 0x555555b8dad0 <vtable for Time::Engine+16>
        }
#21 0x00005555557fa0fa in SquidMainSafe (argv=0x7fffffffed48, argc=2)
    at main.cc:1381
        __FUNCTION__ = <optimized out>
        _dbg_level = <optimized out>
        _dbo = <optimized out>
#22 main (argc=2, argv=0x7fffffffed48) at main.cc:1369
No locals.
Saved corefile core.1678595

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From rousskov at measurement-factory.com  Mon Jun 19 18:21:40 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 19 Jun 2023 14:21:40 -0400
Subject: [squid-users] ftp related crash in
 squid-6.0.0-20220905-r9358e99f9
In-Reply-To: <ZJB3usHKML99fdcd@charite.de>
References: <ZJB3usHKML99fdcd@charite.de>
Message-ID: <deadd1c1-d705-eea9-0325-ce1b313716f8@measurement-factory.com>

On 6/19/23 11:43, Ralf Hildebrandt wrote:

> It seems I'm encountering an FTP related crash here; core.1678595 is
> available for further debugging.

Please post the (sanitized, if needed) output of the following gdb 
commands interrogating that Squid core file:


     set print pretty
     set print static-members off

     frame 3
     print this
     print *this
     print *entry
     print *entry->mem_obj

     frame 10
     print this
     print *this


Thank you,

Alex.


> I'm running squid-6.0.0-20220905-r9358e99f9 (since that's so far the
> most stable version for me).
> 
> cache.log says:
> ===============
> 
> 2023/06/19 17:31:10| conn3580032407 local=141.42.5.215:8080 remote=141.42.64.96:52710 FD 39958 flags=1: read/write failure: (110) Connection timed out
>      current master transaction: master1645377270
> 2023/06/19 17:31:12| conn3572687242 local=141.42.5.215:8080 remote=172.29.144.96:51701 FD 41826 flags=1: read/write failure: (110) Connection timed out
>      current master transaction: master1642214323
> 2023/06/19 17:31:17| FATAL: assertion failed: FwdState.cc:299: "!storedWholeReply_"
>      current master transaction: master1650383954
>      
> This is the backtrace:
> ======================
> 
> Reading symbols from /usr/sbin/squid...
> [Thread debugging using libthread_db enabled]
> Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
> 
> Program received signal SIGABRT, Aborted.
> __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
> #0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
>          set = {
>            __val = {0, 115, 60684401525984, 11384875684165056000, 1,
>              140737488347504, 93825000028288, 11384875684165056000,
>              93824999874096, 140737488347504, 93825000028376, 140737488347536,
>              93824997547807, 93825140894448, 93826629547024, 140737344746339}
>          }
>          pid = <optimized out>
>          tid = <optimized out>
>          ret = <optimized out>
> #1  0x00007ffff7615859 in __GI_abort () at abort.c:79
>          save_stage = 1
>          act = {
>            __sigaction_handler = {
>              sa_handler = 0x555555a65f1f,
>              sa_sigaction = 0x555555a65f1f
>            },
>            sa_mask = {
>              __val = {93825140894448, 93826629547024, 93824996975023, 155185,
>                1687188677, 0, 93826629547008, 93825491415392,
>                11384875684165056000, 111, 93825000028288, 93825000094136,
>                93824996978668, 93825000094136, 93824996974080, 93825000094136}
>            },
>            sa_flags = 1990456832,
>            sa_restorer = 0x555555a6617d
>          }
>          sigs = {
>            __val = {32, 0 <repeats 15 times>}
>          }
> #2  0x00005555559d9e8b in xassert (
>      msg=msg at entry=0x555555a6617d "!storedWholeReply_",
>      file=file at entry=0x555555a65f1f "FwdState.cc", line=line at entry=299)
>      at debug.cc:1266
>          __FUNCTION__ = <optimized out>
> #3  0x000055555570e9c2 in FwdState::completed (this=this at entry=0x55556c0a4748)
>      at FwdState.cc:299
>          __FUNCTION__ = "completed"
> #4  0x00005555557128af in FwdState::complete (this=0x55556c0a4748)
>      at FwdState.cc:589
>          replyStatus = Http::scNone
>          __FUNCTION__ = "complete"
> #5  0x000055555587c796 in Client::completeForwarding (this=0x5555662702c8)
>      at ../../src/base/RefCount.h:73
>          __FUNCTION__ = "completeForwarding"
> #6  0x0000555555882619 in Ftp::Gateway::completeForwarding (
>      this=0x5555662702c8) at FtpGateway.cc:2651
>          __FUNCTION__ = "completeForwarding"
> #7  0x000055555587974b in Client::serverComplete2 (
>      this=this at entry=0x5555662702c8) at Client.cc:214
>          __FUNCTION__ = "serverComplete2"
> #8  0x000055555587c3d4 in Client::serverComplete (this=0x5555662702c8)
>      at Client.cc:198
>          __FUNCTION__ = "serverComplete"
> #9  0x000055555588154a in ftpReadQuit (ftpState=<optimized out>)
>      at FtpGateway.cc:2288
> No locals.
> #10 0x00005555558816f6 in Ftp::Gateway::handleControlReply (
>      this=0x5555662702c8) at FtpGateway.cc:1176
> No locals.
> #11 0x00005555558a15a5 in Ftp::Client::readControlReply (this=0x5555662702c8,
>      io=...) at FtpClient.cc:415
>          __FUNCTION__ = "readControlReply"
>          len = <optimized out>
> #12 0x00005555558a7172 in CommCbMemFunT<Ftp::Client, CommIoCbParams>::doDial (
>      this=0x5555ff888638) at ../../src/CommCalls.h:205
> No locals.
> #13 0x00005555558a6ee3 in JobDialer<Ftp::Client>::dial (this=0x5555ff888638,
>      call=...) at ../../src/base/AsyncJobCalls.h:170
>          __FUNCTION__ = "dial"
> #14 0x00005555558a7092 in AsyncCallT<CommCbMemFunT<Ftp::Client, CommIoCbParams> >::fire (this=<optimized out>) at ../../src/base/AsyncCall.h:143
> No locals.
> #15 0x00005555558d5952 in AsyncCall::make (this=0x5555ff888600)
>      at AsyncCall.cc:44
>          __FUNCTION__ = "make"
> #16 0x00005555558d73d0 in AsyncCallQueue::fire (this=0x555555daff70)
>      at ../../src/base/RefCount.h:73
>          call = {
>            p_ = 0x5555ff888600
>          }
>          made = true
>          __FUNCTION__ = "fire"
> #17 0x00005555557098cf in EventLoop::dispatchCalls (
>      this=this at entry=0x7fffffffe940) at EventLoop.cc:144
>          dispatchedSome = <optimized out>
> #18 0x00005555557099c2 in EventLoop::runOnce (this=this at entry=0x7fffffffe940)
>      at EventLoop.cc:121
>          sawActivity = <optimized out>
>          waitingEngine = 0x7fffffffe860
>          __FUNCTION__ = <optimized out>
> #19 0x0000555555709a2d in EventLoop::run (this=0x7fffffffe940)
>      at EventLoop.cc:83
> No locals.
> #20 0x00005555557f9ad7 in SquidMain (argc=<optimized out>,
>      argv=<optimized out>) at main.cc:1691
>          cmdLine = {
>            argv_ = std::vector of length 3, capacity 3 = {
>              0x555555cc3760 "/usr/sbin/squid", 0x555555cc3780 "-NsYC", 0x0},
>            shortOptions_ = 0x555555cc3710 "CDFNRSYXa:d:f:hk:m::n:sl:u:vz?",
>            longOptions_ = std::vector of length 5, capacity 8 = {{
>                <option> = {
>                  name = 0x555555cc3860 "foreground",
>                  has_arg = 0,
>                  flag = 0x0,
>                  val = 2
>                }, <No data fields>}, {
>                <option> = {
>                  name = 0x555555cc3a60 "kid",
>                  has_arg = 1,
>                  flag = 0x0,
>                  val = 3
>                }, <No data fields>}, {
>                <option> = {
>                  name = 0x555555cc3a80 "help",
>                  has_arg = 0,
>                  flag = 0x0,
>                  val = 104
>                }, <No data fields>}, {
>                <option> = {
>                  name = 0x555555cc3aa0 "version",
>                  has_arg = 0,
>                  flag = 0x0,
>                  val = 118
>                }, <No data fields>}, {
>                <option> = {
>                  name = 0x0,
>                  has_arg = 0,
>                  flag = 0x0,
>                  val = 0
>                }, <No data fields>}}
>          }
>          WIN32_init_err = 0
>          __FUNCTION__ = "SquidMain"
>          mainLoop = {
>            errcount = 0,
>            static Running = 0x7fffffffe940,
>            last_loop = false,
>            engines = std::vector of length 4, capacity 4 = {0x7fffffffe820,
>              0x555555c0dc80 <EventScheduler::_instance>, 0x7fffffffe840,
>              0x7fffffffe860},
>            timeService = 0x7fffffffe880,
>            primaryEngine = 0x7fffffffe860,
>            loop_delay = 0,
>            error = false,
>            runOnceResult = false
>          }
>          signalEngine = {
>            <AsyncEngine> = {
>              _vptr.AsyncEngine = 0x555555b6ea38 <vtable for SignalEngine+16>
>            }, <No data fields>}
>          store_engine = {
>            <AsyncEngine> = {
>              _vptr.AsyncEngine = 0x555555b6ea10 <vtable for StoreRootEngine+16>
>            }, <No data fields>}
>          comm_engine = {
>            <AsyncEngine> = {
>              _vptr.AsyncEngine = 0x555555b7e138 <vtable for CommSelectEngine+16>
>            }, <No data fields>}
>          time_engine = {
>            _vptr.Engine = 0x555555b8dad0 <vtable for Time::Engine+16>
>          }
> #21 0x00005555557fa0fa in SquidMainSafe (argv=0x7fffffffed48, argc=2)
>      at main.cc:1381
>          __FUNCTION__ = <optimized out>
>          _dbg_level = <optimized out>
>          _dbo = <optimized out>
> #22 main (argc=2, argv=0x7fffffffed48) at main.cc:1369
> No locals.
> Saved corefile core.1678595
> 



From Ralf.Hildebrandt at charite.de  Mon Jun 19 20:44:08 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 19 Jun 2023 22:44:08 +0200
Subject: [squid-users] [ext] Re: ftp related crash in
 squid-6.0.0-20220905-r9358e99f9
In-Reply-To: <deadd1c1-d705-eea9-0325-ce1b313716f8@measurement-factory.com>
References: <ZJB3usHKML99fdcd@charite.de>
 <deadd1c1-d705-eea9-0325-ce1b313716f8@measurement-factory.com>
Message-ID: <ZJC+GNWAQfikbDUc@charite.de>

I hope I got it all cut&pasted

>     frame 3
#3  0x000055555570e9c2 in FwdState::completed (this=this at entry=0x55556c0a4748) at FwdState.cc:299
299            assert(!storedWholeReply_);

>     print this
$1 = (FwdState * const) 0x55556c0a4748

$2 = {
  <Lock> = {
    _vptr.Lock = 0x555555b6c3b8 <vtable for FwdState+88>,
    count_ = 2
  }, 
  <PeerSelectionInitiator> = {
    <CbdataParent> = {
      _vptr.CbdataParent = 0x555555b6c378 <vtable for FwdState+24>
    }, 
    members of PeerSelectionInitiator:
    subscribed = false
  }, 
  members of FwdState:
  entry = 0x55555e31aaf0,
  request = 0x55562827ab90,
  al = {
    p_ = 0x5555b6ecaec0
  },
  self = {
    p_ = 0x55556c0a4748
  },
  err = 0x0,
  clientConn = {
    p_ = 0x5555dafd6ea0
  },
  start_t = 1687188676,
  n_tries = 1,
  flags = {
    connected_okay = true,
    dont_retry = false,
    forward_completed = true,
    destinationsFound = true
  },
  transportWait = {
    <JobWaitBase> = {
      job_ = {
        cbc = 0x0,
        lock = 0x0
      },
      callback_ = {
        p_ = 0x0
      }
    }, 
    members of JobWait<HappyConnOpener>:
    typedJob_ = {
      cbc = 0x55566ccd1888,
      lock = 0x55566ccd1888
    }
  },
  encryptionWait = {
    <JobWaitBase> = {
      job_ = {
        cbc = 0x0,
        lock = 0x0
      },
      callback_ = {
        p_ = 0x0
      }
    }, 
    members of JobWait<Security::PeerConnector>:
    typedJob_ = {
      cbc = 0x0,
      lock = 0x0
    }
  },
  peerWait = {
    <JobWaitBase> = {
      job_ = {
        cbc = 0x0,
        lock = 0x0
      },
      callback_ = {
        p_ = 0x0
      }
    }, 
    members of JobWait<Http::Tunneler>:
    typedJob_ = {
      cbc = 0x0,
      lock = 0x0
    }
  },
  waitingForDispatched = false,
  destinations = {
    p_ = 0x555654304b50
  },
  serverConn = {
    p_ = 0x0
  },
  destinationReceipt = {
    connection_ = {
      p_ = 0x0
    },
    position_ = 18446744073709551615
  },
  closeHandler = {
    p_ = 0x0
  },
  pconnRace = FwdState::raceImpossible,
  storedWholeReply_ = 0x555555a94350 "ftpReadTransferDone code 226 or 250"
}

> print *entry

$3 = {
  <hash_link> = {
    key = 0x55556bfa3d50,
    next = 0x5555b6b68468
  }, 
  <Packable> = {
    _vptr.Packable = 0x555555b6ec50 <vtable for StoreEntry+16>
  }, 
  members of StoreEntry:
  mem_obj = 0x55559cd62ca0,
  repl = {
    data = 0x0
  },
  timestamp = -1,
  lastref = 1687188676,
  expires = -1,
  lastModified_ = -1,
  swap_file_sz = 0,
  refcount = 1,
  flags = 1216,
  swap_filen = -1,
  swap_dirn = -1,
  mem_status = NOT_IN_MEMORY,
  ping_status = PING_DONE,
  store_status = STORE_PENDING,
  swap_status = SWAPOUT_NONE,
  cachedESITree = {
    p_ = 0x0
  },
  lock_count = 3,
  shareableWhenPrivate = false,
  deferredProducer = {
    p_ = 0x0
  }
}

> print *entry->mem_obj

$4 = {
  appliedUpdates = false,
  method = {
    theMethod = Http::METHOD_GET,
    theImage = {
      id = {
        value = 99694914
      },
      store_ = {
        p_ = 0x555555cb57d0
      },
      off_ = 0,
      len_ = 0
    }
  },
  data_hdr = {
    inmem_hi = 0,
    nodes = {
      head = 0x0,
      elements = 0
    }
  },
  inmem_lo = 0,
  clients = {
    head = 0x55555acdbe30,
    tail = 0x55555acdbe30
  },
  nclients = 1,
  swapout = {
    queue_offset = 0,
    sio = {
      p_ = 0x0
    },
    decision = MemObject::SwapOut::swNeedsCheck
  },
  xitTable = {
    index = -1,
    io = Store::ioUndecided
  },
  memCache = {
    index = -1,
    offset = 0,
    io = Store::ioUndecided
  },
  request = {
    p_ = 0x55562827ab90
  },
  start_ping = {
    tv_sec = 0,
    tv_usec = 0
  },
  ping_reply_callback = 0x0,
  ircb_data = 0x0,
  abortCallback = {
    p_ = 0x5556543f35b0
  },
  repl = {
    data = 0x0
  },
  id = 105121620,
  object_sz = -1,
  swap_hdr_sz = 0,
  vary_headers = {
    id = {
      value = 99694915
    },
    store_ = {
      p_ = 0x555555cb57d0
    },
    off_ = 0,
    len_ = 0
  },
  reply_ = {
    p_ = 0x55563217e040
  },
  updatedReply_ = {
    p_ = 0x0
  },
  storeId_ = {
    size_ = 40,
    len_ = 35,
    buf_ = 0x5555df5d7a50 "ftp://ftp.nimblestorage.com/upload/"
  },
  logUri_ = {
    size_ = 40,
    len_ = 35,
    buf_ = 0x5555e78446a0 "ftp://ftp.nimblestorage.com/upload/"
  },
  deferredReads = {
    deferredReads = {
      head = {
        p_ = 0x0
      },
      tail = {
        p_ = 0x0
      },
      length = 0
    }
  }
}

> frame 10
#10 0x00005555558816f6 in Ftp::Gateway::handleControlReply (this=0x5555662702c8) at FtpGateway.cc:1176
1176	    FTP_SM_FUNCS[state] (this);

> print this
$5 = (Ftp::Gateway * const) 0x5555662702c8

> print *this
$6 = {
  <Ftp::Client> = {
    <Client> = {
      <Adaptation::Initiator> = {
        <AsyncJob> = {
          <CbdataParent> = {
            _vptr.CbdataParent = 0x555555b78ec8 <vtable for Ftp::Gateway+520>
          }, 
          members of AsyncJob:
          id = {
            value = 201681648
          },
          stopReason = 0x0,
          typeName = 0x555555a9355f "FtpStateData",
          inCall = {
            p_ = 0x5555ff888600
          },
          started_ = true,
          swanSang_ = false
        }, 
        members of Adaptation::Initiator:
        _vptr.Initiator = 0x555555b78cd8 <vtable for Ftp::Gateway+24>
      }, 
      <BodyProducer> = {
        members of BodyProducer:
        _vptr.BodyProducer = 0x555555b78e18 <vtable for Ftp::Gateway+344>
      }, 
      <BodyConsumer> = {
        members of BodyConsumer:
        _vptr.BodyConsumer = 0x555555b78e50 <vtable for Ftp::Gateway+400>
      }, 
      members of Client:
      completed = true,
      currentOffset = 0,
      responseBodyBuffer = 0x0,
      entry = 0x55555e31aaf0,
      fwd = {
        p_ = 0x55556c0a4748
      },
      request = {
        p_ = 0x55562827ab90
      },
      requestBodySource = {
        p_ = 0x0
      },
      requestSender = {
        p_ = 0x0
      },
      virginBodyDestination = {
        p_ = 0x0
      },
      adaptedHeadSource = {
        cbc = 0x0,
        lock = 0x0
      },
      adaptedBodySource = {
        p_ = 0x0
      },
      adaptationAccessCheckPending = false,
      startedAdaptation = false,
      receivedWholeAdaptedReply = false,
      receivedWholeRequestBody = false,
      doneWithFwd = 0x555555a92a12 "completeForwarding()",
      theVirginReply = 0x0,
      theFinalReply = 0x0
    }, 
    members of Ftp::Client:
    ctrl = {
      <Ftp::Channel> = {
        conn = {
          p_ = 0x0
        },
        listenConn = {
          p_ = 0x0
        },
        closer = {
          p_ = 0x0
        }
      }, 
      members of Ftp::CtrlChannel:
      buf = 0x5556084e8f30 "221 Goodbye.\r\nsend OK.\r\ntory listing.\r\n226 Directory send OK.\r\n####\r\n220-Welcome to the HPE NimbleStorage FTP Server.\r\n220-\r\n220-Please prepend files to be uploaded with your case number\r\n220-so that "...,
      size = 4096,
      offset = 0,
      message = 0x5555b1cb8b80,
      last_command = 0x5555a99543f0 "QUIT\r\n",
      last_reply = 0x5555bb4e65a0 "Goodbye.",
      replycode = 221
    },
    data = {
      <Ftp::Channel> = {
        conn = {
          p_ = 0x0
        },
        listenConn = {
          p_ = 0x0
        },
        closer = {
          p_ = 0x0
        }
      }, 
      members of Ftp::DataChannel:
      readBuf = 0x55555e493418,
      host = 0x5555bb42a4e0 "198.54.168.47",
      port = 6892,
      read_pending = false
    },
    ftp_state_t = Ftp::Client::BEGIN,
    state = 18,
    old_request = 0x0,
    old_reply = 0x0,
    dataConnWait = {
      <JobWaitBase> = {
        job_ = {
          cbc = 0x0,
          lock = 0x0
        },
        callback_ = {
          p_ = 0x0
        }
      }, 
      members of JobWait<Comm::ConnOpener>:
      typedJob_ = {
        cbc = 0x55557d0ec1b8,
        lock = 0x55557d0ec1b8
      }
    },
    shortenReadTimeout = false
  }, 
  members of Ftp::Gateway:
  user = "anonymous", '\000' <repeats 8182 times>,
  password = "Squid@", '\000' <repeats 8185 times>,
  password_url = 0,
  reply_hdr = 0x0,
  reply_hdr_state = 0,
  clean_url = {
    size_ = 0,
    len_ = 0,
    buf_ = 0x0
  },
  title_url = {
    size_ = 40,
    len_ = 35,
    buf_ = 0x5555d58c86b0 "ftp://ftp.nimblestorage.com/upload/"
  },
  base_href = {
    size_ = 40,
    len_ = 36,
    buf_ = 0x55559dd8c4f0 "ftp://ftp.nimblestorage.com/upload//"
  },
  conn_att = 0,
  login_att = 0,
  mdtm = -1,
  theSize = -1,
  pathcomps = 0x0,
  filepath = 0x0,
  dirpath = 0x5555e4136d90 "upload",
  restart_offset = 0,
  proxy_host = 0x0,
  list_width = 0,
  cwd_message = {
    size_ = 40,
    len_ = 36,
    buf_ = 0x5555c4d14f00 "\n250 Directory successfully changed."
  },
  old_filepath = 0x0,
  typecode = 0 '\000',
  listing = {
    <Packable> = {
      _vptr.Packable = 0x555555b6cab0 <vtable for MemBuf+16>
    }, 
    members of MemBuf:
    buf = 0x5555cc7050c0 "",
    size = 0,
    max_capacity = 2097152000,
    capacity = 2048,
    stolen = 0
  },
  flags = {
    pasv_supported = true,
    epsv_all_sent = false,
    pasv_only = false,
    pasv_failed = false,
    authenticated = false,
    tried_auth_anonymous = true,
    tried_auth_nopass = false,
    isdir = true,
    skip_whitespace = false,
    rest_supported = true,
    http_header_sent = false,
    tried_nlst = false,
    need_base_href = false,
    dir_slash = false,
    root_dir = false,
    no_dotdot = false,
    binary = false,
    try_slash_hack = false,
    put = false,
    put_mkdir = false,
    listformat_unknown = false,
    listing = true,
    completed_forwarding = true
  }
}

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netzwerk

Campus Benjamin Franklin (CBF)
Haus I | 1. OG | Raum 105
Hindenburgdamm 30 | D-12203 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From rousskov at measurement-factory.com  Mon Jun 19 22:10:28 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 19 Jun 2023 18:10:28 -0400
Subject: [squid-users] [ext] Re: ftp related crash in
 squid-6.0.0-20220905-r9358e99f9
In-Reply-To: <ZJC+GNWAQfikbDUc@charite.de>
References: <ZJB3usHKML99fdcd@charite.de>
 <deadd1c1-d705-eea9-0325-ce1b313716f8@measurement-factory.com>
 <ZJC+GNWAQfikbDUc@charite.de>
Message-ID: <cb13d9c5-06aa-0c8f-df0b-a7ee13b291f4@measurement-factory.com>

On 6/19/23 16:44, Ralf Hildebrandt wrote:
> I hope I got it all cut&pasted

This information was very useful, thank you!


> storedWholeReply_ = "ftpReadTransferDone code 226 or 250"

I suspect that the current Client::markParsedVirginReplyAsWhole() 
assumptions about received-vs-stored response data do not match our FTP 
code (in this particular use case). It is not clear to me whether 
Ftp::Gateway code is not storing any response headers at all this case 
(a bug) OR it is calling that method prematurely, before storing 
response headers (a different bug). Either way, FwdState gets a 
"complete" but empty Store entry, which is not supposed to happen 
(because every Store entry has HTTP response headers or equivalent), and 
Squid asserts.

Unfortunately, I do not remember enough about FTP code to quickly triage 
this further. I recommend filing a bug report with Squid bugzilla in 
hope that somebody will volunteer a bug fix (or, at least, a workaround).


Thank you,

Alex.


>>      frame 3
> #3  0x000055555570e9c2 in FwdState::completed (this=this at entry=0x55556c0a4748) at FwdState.cc:299
> 299            assert(!storedWholeReply_);
> 
>>      print this
> $1 = (FwdState * const) 0x55556c0a4748
> 
> $2 = {
>    <Lock> = {
>      _vptr.Lock = 0x555555b6c3b8 <vtable for FwdState+88>,
>      count_ = 2
>    },
>    <PeerSelectionInitiator> = {
>      <CbdataParent> = {
>        _vptr.CbdataParent = 0x555555b6c378 <vtable for FwdState+24>
>      },
>      members of PeerSelectionInitiator:
>      subscribed = false
>    },
>    members of FwdState:
>    entry = 0x55555e31aaf0,
>    request = 0x55562827ab90,
>    al = {
>      p_ = 0x5555b6ecaec0
>    },
>    self = {
>      p_ = 0x55556c0a4748
>    },
>    err = 0x0,
>    clientConn = {
>      p_ = 0x5555dafd6ea0
>    },
>    start_t = 1687188676,
>    n_tries = 1,
>    flags = {
>      connected_okay = true,
>      dont_retry = false,
>      forward_completed = true,
>      destinationsFound = true
>    },
>    transportWait = {
>      <JobWaitBase> = {
>        job_ = {
>          cbc = 0x0,
>          lock = 0x0
>        },
>        callback_ = {
>          p_ = 0x0
>        }
>      },
>      members of JobWait<HappyConnOpener>:
>      typedJob_ = {
>        cbc = 0x55566ccd1888,
>        lock = 0x55566ccd1888
>      }
>    },
>    encryptionWait = {
>      <JobWaitBase> = {
>        job_ = {
>          cbc = 0x0,
>          lock = 0x0
>        },
>        callback_ = {
>          p_ = 0x0
>        }
>      },
>      members of JobWait<Security::PeerConnector>:
>      typedJob_ = {
>        cbc = 0x0,
>        lock = 0x0
>      }
>    },
>    peerWait = {
>      <JobWaitBase> = {
>        job_ = {
>          cbc = 0x0,
>          lock = 0x0
>        },
>        callback_ = {
>          p_ = 0x0
>        }
>      },
>      members of JobWait<Http::Tunneler>:
>      typedJob_ = {
>        cbc = 0x0,
>        lock = 0x0
>      }
>    },
>    waitingForDispatched = false,
>    destinations = {
>      p_ = 0x555654304b50
>    },
>    serverConn = {
>      p_ = 0x0
>    },
>    destinationReceipt = {
>      connection_ = {
>        p_ = 0x0
>      },
>      position_ = 18446744073709551615
>    },
>    closeHandler = {
>      p_ = 0x0
>    },
>    pconnRace = FwdState::raceImpossible,
>    storedWholeReply_ = 0x555555a94350 "ftpReadTransferDone code 226 or 250"
> }
> 
>> print *entry
> 
> $3 = {
>    <hash_link> = {
>      key = 0x55556bfa3d50,
>      next = 0x5555b6b68468
>    },
>    <Packable> = {
>      _vptr.Packable = 0x555555b6ec50 <vtable for StoreEntry+16>
>    },
>    members of StoreEntry:
>    mem_obj = 0x55559cd62ca0,
>    repl = {
>      data = 0x0
>    },
>    timestamp = -1,
>    lastref = 1687188676,
>    expires = -1,
>    lastModified_ = -1,
>    swap_file_sz = 0,
>    refcount = 1,
>    flags = 1216,
>    swap_filen = -1,
>    swap_dirn = -1,
>    mem_status = NOT_IN_MEMORY,
>    ping_status = PING_DONE,
>    store_status = STORE_PENDING,
>    swap_status = SWAPOUT_NONE,
>    cachedESITree = {
>      p_ = 0x0
>    },
>    lock_count = 3,
>    shareableWhenPrivate = false,
>    deferredProducer = {
>      p_ = 0x0
>    }
> }
> 
>> print *entry->mem_obj
> 
> $4 = {
>    appliedUpdates = false,
>    method = {
>      theMethod = Http::METHOD_GET,
>      theImage = {
>        id = {
>          value = 99694914
>        },
>        store_ = {
>          p_ = 0x555555cb57d0
>        },
>        off_ = 0,
>        len_ = 0
>      }
>    },
>    data_hdr = {
>      inmem_hi = 0,
>      nodes = {
>        head = 0x0,
>        elements = 0
>      }
>    },
>    inmem_lo = 0,
>    clients = {
>      head = 0x55555acdbe30,
>      tail = 0x55555acdbe30
>    },
>    nclients = 1,
>    swapout = {
>      queue_offset = 0,
>      sio = {
>        p_ = 0x0
>      },
>      decision = MemObject::SwapOut::swNeedsCheck
>    },
>    xitTable = {
>      index = -1,
>      io = Store::ioUndecided
>    },
>    memCache = {
>      index = -1,
>      offset = 0,
>      io = Store::ioUndecided
>    },
>    request = {
>      p_ = 0x55562827ab90
>    },
>    start_ping = {
>      tv_sec = 0,
>      tv_usec = 0
>    },
>    ping_reply_callback = 0x0,
>    ircb_data = 0x0,
>    abortCallback = {
>      p_ = 0x5556543f35b0
>    },
>    repl = {
>      data = 0x0
>    },
>    id = 105121620,
>    object_sz = -1,
>    swap_hdr_sz = 0,
>    vary_headers = {
>      id = {
>        value = 99694915
>      },
>      store_ = {
>        p_ = 0x555555cb57d0
>      },
>      off_ = 0,
>      len_ = 0
>    },
>    reply_ = {
>      p_ = 0x55563217e040
>    },
>    updatedReply_ = {
>      p_ = 0x0
>    },
>    storeId_ = {
>      size_ = 40,
>      len_ = 35,
>      buf_ = 0x5555df5d7a50 "ftp://ftp.nimblestorage.com/upload/"
>    },
>    logUri_ = {
>      size_ = 40,
>      len_ = 35,
>      buf_ = 0x5555e78446a0 "ftp://ftp.nimblestorage.com/upload/"
>    },
>    deferredReads = {
>      deferredReads = {
>        head = {
>          p_ = 0x0
>        },
>        tail = {
>          p_ = 0x0
>        },
>        length = 0
>      }
>    }
> }
> 
>> frame 10
> #10 0x00005555558816f6 in Ftp::Gateway::handleControlReply (this=0x5555662702c8) at FtpGateway.cc:1176
> 1176	    FTP_SM_FUNCS[state] (this);
> 
>> print this
> $5 = (Ftp::Gateway * const) 0x5555662702c8
> 
>> print *this
> $6 = {
>    <Ftp::Client> = {
>      <Client> = {
>        <Adaptation::Initiator> = {
>          <AsyncJob> = {
>            <CbdataParent> = {
>              _vptr.CbdataParent = 0x555555b78ec8 <vtable for Ftp::Gateway+520>
>            },
>            members of AsyncJob:
>            id = {
>              value = 201681648
>            },
>            stopReason = 0x0,
>            typeName = 0x555555a9355f "FtpStateData",
>            inCall = {
>              p_ = 0x5555ff888600
>            },
>            started_ = true,
>            swanSang_ = false
>          },
>          members of Adaptation::Initiator:
>          _vptr.Initiator = 0x555555b78cd8 <vtable for Ftp::Gateway+24>
>        },
>        <BodyProducer> = {
>          members of BodyProducer:
>          _vptr.BodyProducer = 0x555555b78e18 <vtable for Ftp::Gateway+344>
>        },
>        <BodyConsumer> = {
>          members of BodyConsumer:
>          _vptr.BodyConsumer = 0x555555b78e50 <vtable for Ftp::Gateway+400>
>        },
>        members of Client:
>        completed = true,
>        currentOffset = 0,
>        responseBodyBuffer = 0x0,
>        entry = 0x55555e31aaf0,
>        fwd = {
>          p_ = 0x55556c0a4748
>        },
>        request = {
>          p_ = 0x55562827ab90
>        },
>        requestBodySource = {
>          p_ = 0x0
>        },
>        requestSender = {
>          p_ = 0x0
>        },
>        virginBodyDestination = {
>          p_ = 0x0
>        },
>        adaptedHeadSource = {
>          cbc = 0x0,
>          lock = 0x0
>        },
>        adaptedBodySource = {
>          p_ = 0x0
>        },
>        adaptationAccessCheckPending = false,
>        startedAdaptation = false,
>        receivedWholeAdaptedReply = false,
>        receivedWholeRequestBody = false,
>        doneWithFwd = 0x555555a92a12 "completeForwarding()",
>        theVirginReply = 0x0,
>        theFinalReply = 0x0
>      },
>      members of Ftp::Client:
>      ctrl = {
>        <Ftp::Channel> = {
>          conn = {
>            p_ = 0x0
>          },
>          listenConn = {
>            p_ = 0x0
>          },
>          closer = {
>            p_ = 0x0
>          }
>        },
>        members of Ftp::CtrlChannel:
>        buf = 0x5556084e8f30 "221 Goodbye.\r\nsend OK.\r\ntory listing.\r\n226 Directory send OK.\r\n####\r\n220-Welcome to the HPE NimbleStorage FTP Server.\r\n220-\r\n220-Please prepend files to be uploaded with your case number\r\n220-so that "...,
>        size = 4096,
>        offset = 0,
>        message = 0x5555b1cb8b80,
>        last_command = 0x5555a99543f0 "QUIT\r\n",
>        last_reply = 0x5555bb4e65a0 "Goodbye.",
>        replycode = 221
>      },
>      data = {
>        <Ftp::Channel> = {
>          conn = {
>            p_ = 0x0
>          },
>          listenConn = {
>            p_ = 0x0
>          },
>          closer = {
>            p_ = 0x0
>          }
>        },
>        members of Ftp::DataChannel:
>        readBuf = 0x55555e493418,
>        host = 0x5555bb42a4e0 "198.54.168.47",
>        port = 6892,
>        read_pending = false
>      },
>      ftp_state_t = Ftp::Client::BEGIN,
>      state = 18,
>      old_request = 0x0,
>      old_reply = 0x0,
>      dataConnWait = {
>        <JobWaitBase> = {
>          job_ = {
>            cbc = 0x0,
>            lock = 0x0
>          },
>          callback_ = {
>            p_ = 0x0
>          }
>        },
>        members of JobWait<Comm::ConnOpener>:
>        typedJob_ = {
>          cbc = 0x55557d0ec1b8,
>          lock = 0x55557d0ec1b8
>        }
>      },
>      shortenReadTimeout = false
>    },
>    members of Ftp::Gateway:
>    user = "anonymous", '\000' <repeats 8182 times>,
>    password = "Squid@", '\000' <repeats 8185 times>,
>    password_url = 0,
>    reply_hdr = 0x0,
>    reply_hdr_state = 0,
>    clean_url = {
>      size_ = 0,
>      len_ = 0,
>      buf_ = 0x0
>    },
>    title_url = {
>      size_ = 40,
>      len_ = 35,
>      buf_ = 0x5555d58c86b0 "ftp://ftp.nimblestorage.com/upload/"
>    },
>    base_href = {
>      size_ = 40,
>      len_ = 36,
>      buf_ = 0x55559dd8c4f0 "ftp://ftp.nimblestorage.com/upload//"
>    },
>    conn_att = 0,
>    login_att = 0,
>    mdtm = -1,
>    theSize = -1,
>    pathcomps = 0x0,
>    filepath = 0x0,
>    dirpath = 0x5555e4136d90 "upload",
>    restart_offset = 0,
>    proxy_host = 0x0,
>    list_width = 0,
>    cwd_message = {
>      size_ = 40,
>      len_ = 36,
>      buf_ = 0x5555c4d14f00 "\n250 Directory successfully changed."
>    },
>    old_filepath = 0x0,
>    typecode = 0 '\000',
>    listing = {
>      <Packable> = {
>        _vptr.Packable = 0x555555b6cab0 <vtable for MemBuf+16>
>      },
>      members of MemBuf:
>      buf = 0x5555cc7050c0 "",
>      size = 0,
>      max_capacity = 2097152000,
>      capacity = 2048,
>      stolen = 0
>    },
>    flags = {
>      pasv_supported = true,
>      epsv_all_sent = false,
>      pasv_only = false,
>      pasv_failed = false,
>      authenticated = false,
>      tried_auth_anonymous = true,
>      tried_auth_nopass = false,
>      isdir = true,
>      skip_whitespace = false,
>      rest_supported = true,
>      http_header_sent = false,
>      tried_nlst = false,
>      need_base_href = false,
>      dir_slash = false,
>      root_dir = false,
>      no_dotdot = false,
>      binary = false,
>      try_slash_hack = false,
>      put = false,
>      put_mkdir = false,
>      listformat_unknown = false,
>      listing = true,
>      completed_forwarding = true
>    }
> }
> 



From my.shellac at gmail.com  Tue Jun 20 04:35:04 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Tue, 20 Jun 2023 09:35:04 +0500
Subject: [squid-users] Insert requested URL to custom header in request
Message-ID: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>

Hello all !

I have web app on the server listens on port 8089. This app processes the
Get or Post requests, and also checks if there is specific header in
request. For example if I will do on the server request like:

curl -H ?X-my special header:https://example.com?
http://127.0.0.1:8089/send

I will get it working properly I will get answer from example.com over my
application

A questions:

1. How I can to add the specific header to request with put the value of
this header like requested URL ??? Something like ?X-my special header:
<requested url>

2. How I could to put the cache peer host as  originserver option as url ?
Something like

http://127.0.0.1:8089/send
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230620/aaed9143/attachment.htm>

From my.shellac at gmail.com  Wed Jun 21 12:04:26 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Wed, 21 Jun 2023 17:04:26 +0500
Subject: [squid-users] Insert requested URL to custom header in request
In-Reply-To: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
References: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
Message-ID: <CAFqyDwCRpJE8J+0QJmbgcPERtMg6ZbUBcsJ_XNswTm=ZNZG7Fg@mail.gmail.com>

Hello.
For now the most wanted answer is for question No 1.
Is that available to insert the requested user's url to request to
chache_peer ???

I tested   this

request_header_add  http://example.com  -  this works well. But I need to
put the requested URL for each user who send request to squid.

Something like this one:
request_header_add  %URL

Any ideas ?

Thank you !


??, 20 ???. 2023??. ? 09:35, Alexey?? Gruzdov <my.shellac at gmail.com>:

> Hello all !
>
> I have web app on the server listens on port 8089. This app processes the
> Get or Post requests, and also checks if there is specific header in
> request. For example if I will do on the server request like:
>
> curl -H ?X-my special header:https://example.com?
> http://127.0.0.1:8089/send
>
> I will get it working properly I will get answer from example.com over my
> application
>
> A questions:
>
> 1. How I can to add the specific header to request with put the value of
> this header like requested URL ??? Something like ?X-my special header:
> <requested url>
>
> 2. How I could to put the cache peer host as  originserver option as url
> ?  Something like
>
> http://127.0.0.1:8089/send
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230621/b222fde3/attachment.htm>

From rousskov at measurement-factory.com  Wed Jun 21 17:41:51 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 21 Jun 2023 13:41:51 -0400
Subject: [squid-users] Insert requested URL to custom header in request
In-Reply-To: <CAFqyDwCRpJE8J+0QJmbgcPERtMg6ZbUBcsJ_XNswTm=ZNZG7Fg@mail.gmail.com>
References: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
 <CAFqyDwCRpJE8J+0QJmbgcPERtMg6ZbUBcsJ_XNswTm=ZNZG7Fg@mail.gmail.com>
Message-ID: <8571c632-b645-9fe2-92b0-5fc72d6eddd0@measurement-factory.com>

On 6/21/23 08:04, Alexey?? Gruzdov wrote:

> I need to put the requested URL for each user who send 
> request to squid.

I believe you can use logformat %codes in quoted request_header_add 
header field values. For example:

     request_header_add X-Special-Header "%ru"

Be careful with URL value adaptation, encoding, and size though. There 
are important differences between %ru and %>ru codes. See 
squid.conf.documented for details.


HTH,

Alex.



> ??, 20 ???. 2023??. ? 09:35, Alexey?? Gruzdov <my.shellac at gmail.com 
> <mailto:my.shellac at gmail.com>>:
> 
>     Hello all !
> 
>     I have web app on the server listens on port 8089. This app
>     processes the Get or Post requests, and also checks if there is
>     specific header in request. For example if I will do on the server
>     request like:
> 
>     curl -H ?X-my special header:https://example.com
>     <https://example.com>? http://127.0.0.1:8089/send
>     <http://127.0.0.1:8089/send>
> 
>     I will get it working properly I will get answer from example.com
>     <http://example.com> over my application
> 
>     A questions:
> 
>     1. How I can to add the specific header to request with put the
>     value of this header like requested URL ??? Something like ?X-my
>     special header: <requested url>
> 
>     2. How I could to put the cache peer host as ?originserver option as
>     url ?? Something like
> 
>     http://127.0.0.1:8089/send <http://127.0.0.1:8089/send>
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From my.shellac at gmail.com  Wed Jun 21 18:42:08 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Wed, 21 Jun 2023 23:42:08 +0500
Subject: [squid-users] Insert requested URL to custom header in request
In-Reply-To: <8571c632-b645-9fe2-92b0-5fc72d6eddd0@measurement-factory.com>
References: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
 <CAFqyDwCRpJE8J+0QJmbgcPERtMg6ZbUBcsJ_XNswTm=ZNZG7Fg@mail.gmail.com>
 <8571c632-b645-9fe2-92b0-5fc72d6eddd0@measurement-factory.com>
Message-ID: <CAFqyDwDJr6Y7+y2cO7J4meuJeZ4JwF4zxpbEUadk=ZQoG8Wp1w@mail.gmail.com>

Thank you very much !!!
Works !!!



??, 21 ???. 2023??. ? 22:42, Alex Rousskov <rousskov at measurement-factory.com
>:

> On 6/21/23 08:04, Alexey?? Gruzdov wrote:
>
> > I need to put the requested URL for each user who send
> > request to squid.
>
> I believe you can use logformat %codes in quoted request_header_add
> header field values. For example:
>
>      request_header_add X-Special-Header "%ru"
>
> Be careful with URL value adaptation, encoding, and size though. There
> are important differences between %ru and %>ru codes. See
> squid.conf.documented for details.
>
>
> HTH,
>
> Alex.
>
>
>
> > ??, 20 ???. 2023??. ? 09:35, Alexey?? Gruzdov <my.shellac at gmail.com
> > <mailto:my.shellac at gmail.com>>:
> >
> >     Hello all !
> >
> >     I have web app on the server listens on port 8089. This app
> >     processes the Get or Post requests, and also checks if there is
> >     specific header in request. For example if I will do on the server
> >     request like:
> >
> >     curl -H ?X-my special header:https://example.com
> >     <https://example.com>? http://127.0.0.1:8089/send
> >     <http://127.0.0.1:8089/send>
> >
> >     I will get it working properly I will get answer from example.com
> >     <http://example.com> over my application
> >
> >     A questions:
> >
> >     1. How I can to add the specific header to request with put the
> >     value of this header like requested URL ??? Something like ?X-my
> >     special header: <requested url>
> >
> >     2. How I could to put the cache peer host as  originserver option as
> >     url ?  Something like
> >
> >     http://127.0.0.1:8089/send <http://127.0.0.1:8089/send>
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230621/4f0a5331/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 22 02:23:49 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 21 Jun 2023 22:23:49 -0400
Subject: [squid-users] Enable caching
In-Reply-To: <1171a8a3-bf15-67ca-1635-64f8ef425c57@measurement-factory.com>
References: <CADJd0Y1qBCX17D71jLtDFE2Dbxjf70_pT5yTJkpSJwnUMx=3zg@mail.gmail.com>
 <1171a8a3-bf15-67ca-1635-64f8ef425c57@measurement-factory.com>
Message-ID: <b7ed576f-8dbc-b706-ed09-a1c1d43a1dcb@measurement-factory.com>

On 4/5/23 09:27, Alex Rousskov wrote:
> On 4/5/23 06:07, Andrey K wrote:
> 
>> Previously, caching was disabled on our proxy servers. Now we need to 
>> cache some content (files about 10 MB in size).
>> So we changed the squid.conf:
> 
>> cache_dir ufs /data/squid/cache 32000 16 256 max-size=12000000
>>
>> We have 24 workers on each proxy.
> 
> UFS-based cache_dirs are not supported in multi-worker configurations 
> and, in most cases, should not be used in such configurations. The 
> combination will violate basic HTTP caching rules and may crash Squid 
> and/or corrupt responses.
> 
> 
>> We saw that some requests were taken from the cache, and some were not.
>> The documentation?says:
>> "In SMP configurations, cache_dir must not precede the workers option 
>> and should use configuration macros or conditionals to give each 
>> worker interested in disk caching a dedicated cache directory."
> 
> The official documentation quoted above is stale and very misleading in 
> modern Squids. Ignore it. I will try to find the time to post a PR to 
> fix this.

Done at https://github.com/squid-cache/squid/pull/1394

Alex.



>> So we switched to a rock cache_dir:
>> cache_dir rock /data/squid/cache 32000 max-size=12000000
>>
>> Now everything seems to be working fine in the test environment, but I 
>> found limitations on the RockStore 
>> (https://wiki.squid-cache.org/Features/RockStore:
>> "Objects larger than 32,000 bytes cannot be cached when cache_dirs are 
>> shared among workers."
> 
> The Feature/RockStore page is stale and can easily mislead. In general, 
> Feature/Foo wiki pages are often development-focused and get stale with 
> time. They cannot be reliably used as a Squid feature documentation.
> 
> 
>> Does this mean that?RockStore?is not suitable for caching large files?
> 
> No, it does not. Rock storage has evolved since that Feature page was 
> written. You can see the following wiki page discussing evolved rock 
> storage design, but that page probably has some stale info as well:
> https://wiki.squid-cache.org/Features/LargeRockStore
> 
> 
>> Should I switch back?to the UFS and configure 24 cache_dirs
> 
> If everything is "working fine", then you should not. Otherwise, I 
> recommend discussing specific problems before switching to that 
> unsupported and dangerous hack.





From ankor2023 at gmail.com  Thu Jun 22 08:59:44 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 22 Jun 2023 11:59:44 +0300
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <001401d99d03$09206140$1b6123c0$@gmail.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
 <001401d99d03$09206140$1b6123c0$@gmail.com>
Message-ID: <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>

Hello, Eliezer,

I reproduced the issue in the test environment.
I configured my squid with the debug_options: ALL,1 28,9
and ran the test curl from the same proxy host:
   curl -m 4 -k --tlsv1.2 --proxy-user 'user:pass' -s -o /dev/null -w
"%{http_code}"  --proxy localhost:3131 https://archive.org

The client got the 200-response and it works fine.

In the access.log the corresponding records are:
   2023-06-22 10:59:58|    747 127.0.0.1 NONE_NONE/200/- 0 CONNECT
archive.org:443 - HIER_DIRECT/archive.org - - - -
   2023-06-22 10:59:58|    201 127.0.0.1 TCP_MISS/200/200 3833 GET
https://archive.org/ - HIER_DIRECT/archive.org text/html - - -

The cache.log is available at the link:
https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing
There are three warnings there:
   grep WARN /tmp/acl.log
   2023/06/22 10:59:57.875 kid6| WARNING: domdst_SIProxy ACL is used in
context without an HTTP request. Assuming mismatch.
   2023/06/22 10:59:57.884 kid6| WARNING: domdst_SIProxy ACL is used in
context without an HTTP request. Assuming mismatch.
   2023/06/22 10:59:58.536 kid6| WARNING: domdst_SIProxy ACL is used in
context without an HTTP request. Assuming mismatch.

The domdst_SIProxy ACL is used only to change the outgoing address for
specific domains:
   acl domdst_SIProxy  dstdomain
"/data/squid.user/etc/squid/categories/domdst_SIProxy"
   tcp_outgoing_address 10.72.235.184 domdst_SIProxy
The test URL https://archive.org is not in the domdst_SIProxy list.

Squid is configured with an SSL-Bump feature, if it matters.

I think we could ignore these warnings as squid works perfectly, but maybe
there is a workaround to suppress logs flooding?

Kind regards,
       Ankor.










??, 12 ???. 2023??. ? 10:54, <ngtech1ltd at gmail.com>:

> Hey Ankor,
>
> There is some missing context so I would be able to reproduce this issue.
> Is this some kind of CONNECT request?
>
> If you can describe in more technical details the setup and what client
> are you using,
> Maybe couple sanitized log lines it would help to understand better the
> scenario.
>
> Eliezer
>
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Andrey K
> Sent: Friday, June 9, 2023 10:03
> To: Squid Users <squid-users at lists.squid-cache.org>; Amos Jeffries <
> squid3 at treenet.co.nz>
> Subject: [squid-users] Using tcp_outgoing_address with ACL
>
> Hello,
>
> We use the tcp_outgoing_address feature to access some hosts using a
> dedicated source IP address.
>
>    acl domdst_SIProxy  dstdomain
> "/data/squid.user/etc/squid/categories/domdst_SIProxy"
>    tcp_outgoing_address 10.72.235.129 domdst_SIProxy
>
> It works fine, but logs are flooded with warnings like this:
>    2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is used in
> context without an HTTP request. Assuming mismatch.
>
> I found a similar case:
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
> where Amos suggested using a patch as a solution.
> We have Squid Version 5.5. Is there a similar patch for our version, or
> can we just ignore these messages?
>
> Kind regards,
>        Ankor.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230622/15868ae0/attachment.htm>

From ankor2023 at gmail.com  Thu Jun 22 09:07:55 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 22 Jun 2023 12:07:55 +0300
Subject: [squid-users] Proxy server to support a large number of
 simultaneous requests
In-Reply-To: <001501d99d05$0429b5e0$0c7d21a0$@gmail.com>
References: <CADJd0Y3ofecE0XsFBX+YG2P1HuMT9CCHoRQVE1=K8risZLhmkA@mail.gmail.com>
 <4f506ba3-c1aa-feab-8cbd-8b86861364c9@measurement-factory.com>
 <CADJd0Y16ZsmBCFzeKM4RSCDs0hi+AuJRu=iqgayfe4FEBmqQEA@mail.gmail.com>
 <509b99ce-346e-441f-2fa0-cd6cd1c915ef@measurement-factory.com>
 <CADJd0Y0Lto=gbxt2yJ=ULPB1O=gEhDTeX5fOd-Z0s02942dThQ@mail.gmail.com>
 <6087ae32-c719-d2da-170a-7116554e1421@measurement-factory.com>
 <CADJd0Y2OsT-uZjHFMSUmCnGbAZq9FdbUhR6s4RfsfHdMUkbz2A@mail.gmail.com>
 <55a12ae0-9a7b-28e5-a8d1-03a0c2752cbe@measurement-factory.com>
 <CADJd0Y3BBGLu1N60qvNzKToRNT-wN3c5_OhOKgQ8m4pFTgoJjg@mail.gmail.com>
 <CADJd0Y2_5=bnbx57myZHpCTPxbtr6fBtYWBUAB_hEx-EZ1LHyA@mail.gmail.com>
 <001501d99d05$0429b5e0$0c7d21a0$@gmail.com>
Message-ID: <CADJd0Y3YRXJ55pUr0zJvwO5JxfEUY3tgPF9+aVMkcXKHowzFeg@mail.gmail.com>

Hello,  Eliezer,

Thank you for the comment.
I tried squid for this scenario only in the test environment.
I will share the results after conducting a real video broadcast.

Kind regards,
       Ankor

??, 12 ???. 2023??. ? 11:08, <ngtech1ltd at gmail.com>:

> Hey Ankor,
>
> Thanks for sharing the scenario.
> At the beginning I was thinking to myself: Why Squid? Is it the best
> choice for the scenario?
> And after walking through my list of caching proxies, including couple I
> wrote myself I got to the conclusion:
> Well.. Squid-Cache is simple to use and just works.
> Compared to other caching mechanisms squid is so simple to configure and
> it really leaves dust
> behind to all many other cache mechanisms.
>
> Thanks,
> Eliezer
>
>
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Andrey K
> Sent: Tuesday, June 6, 2023 16:08
> To: Alex Rousskov <rousskov at measurement-factory.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Proxy server to support a large number of
> simultaneous requests
>
> Hello, Alex,
>
> I have shortened the correspondence because it does not meet the size
> requirements for the mailing list.
>
> Thank you so much for your time, the analysis and recommendations.
>
> I disabled the cache_dir and now squid works as expected -  there is only
> one request to the original content server:
> - on the small file:
>       1 NONE_NONE/503/- HIER_NONE/-
>       4 TCP_CF_HIT/200/- HIER_NONE/-
>     128 TCP_HIT/200/- HIER_NONE/-
>     366 TCP_MEM_HIT/200/- HIER_NONE/-
>       1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>
> - on the large file:
>      17 TCP_CF_HIT/200/- HIER_NONE/-
>     482 TCP_HIT/200/- HIER_NONE/-
>       1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
>
> I think this configuration is perfect for caching online video broadcasts.
> Chanks of video are required by clients simultaneously only for a short
> period of time, so there is no need to save them to disk..
> As my VM has 32 GB of RAM, I can configure a sufficient amount of
> cache_mem, say 20000 MB to provide caching of video broadcasts.
>
>
> Kind regards,
>      Ankor.
>
>
>
> ??, 5 ???. 2023??. ? 17:31, Alex Rousskov <mailto:
> rousskov at measurement-factory.com>:
> On 6/2/23 03:29, Andrey K wrote:
>
> >  > Can you repeat this test and share a pointer to the corresponding
> >  > compressed cache.log, containing those 500 (or fewer, as long as the
> >  > problem is reproduced!) concurrent transactions. One or many of those
> >  > concurrent transactions resulted in the unwanted entry deletion. The
> log
> >  > may show what happened in that case.
>
> > I cleared the rock cache, set the debug level, restarted squid, cleared
> > the cache.log, ran 500-threads test, waited for it to finish and
> > launched curl to make sure it returned TCP_MISS.
> > Then stopped squid to limit the cache.log file.
>
>
> Thank you for sharing that log! I only had time to study a few misses.
> They all stemmed from the same sequence of events:
>
> 1. A collapsed request finds the corresponding entry in the cache.
> 2. Squid decides that this request should open the disk file.
> 3. The rock disk entry is still being written (i.e. "swapped out"),
>     so the attempt to swap it in fails (TCP_SWAPFAIL_MISS).
> 4. The request goes to the origin server.
> 5. The fresh response deletes the existing cached entry.
> 6. When a subsequent request finds the cached entry marked for
>     deletion, it declares a cache miss (TCP_MISS) and goes to step 4.
>
> Disclaimer: The above sequence of events causes misses, but it may not
> be the only or even the primary cause. I do not have enough free time to
> rule out or confirm other causes (and order them by severity).
>
>
> Squid can (and should) handle concurrent swapout/swapins better, and we
> may be able to estimate that improvement potential for your workload
> without significant development, but, for the next step, I suggest
> disabling cache_dir and testing whether your get substantially better
> results with memory cache alone. Shared memory cache also has periods
> where a being-written entry cannot be read, but, compared to the disk
> cache, those periods are much shorter IIRC. I would like to confirm that
> this simplified mode of operation works well for your workload before I
> suggest code changes that would rely, in part, on this mode.
>
>
> Thank you,
>
> Alex.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230622/fec96b06/attachment.htm>

From ankor2023 at gmail.com  Thu Jun 22 09:08:55 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 22 Jun 2023 12:08:55 +0300
Subject: [squid-users] Enable caching
In-Reply-To: <b7ed576f-8dbc-b706-ed09-a1c1d43a1dcb@measurement-factory.com>
References: <CADJd0Y1qBCX17D71jLtDFE2Dbxjf70_pT5yTJkpSJwnUMx=3zg@mail.gmail.com>
 <1171a8a3-bf15-67ca-1635-64f8ef425c57@measurement-factory.com>
 <b7ed576f-8dbc-b706-ed09-a1c1d43a1dcb@measurement-factory.com>
Message-ID: <CADJd0Y31NX4rcf3_Ao1vrXoFSKUmKz7jLAKy9HTSsEu8agjKZw@mail.gmail.com>

Hello, Alex,

Thank you very much!

Kind regards,
      Ankor

??, 22 ???. 2023??. ? 05:23, Alex Rousskov <rousskov at measurement-factory.com
>:

> On 4/5/23 09:27, Alex Rousskov wrote:
> > On 4/5/23 06:07, Andrey K wrote:
> >
> >> Previously, caching was disabled on our proxy servers. Now we need to
> >> cache some content (files about 10 MB in size).
> >> So we changed the squid.conf:
> >
> >> cache_dir ufs /data/squid/cache 32000 16 256 max-size=12000000
> >>
> >> We have 24 workers on each proxy.
> >
> > UFS-based cache_dirs are not supported in multi-worker configurations
> > and, in most cases, should not be used in such configurations. The
> > combination will violate basic HTTP caching rules and may crash Squid
> > and/or corrupt responses.
> >
> >
> >> We saw that some requests were taken from the cache, and some were not.
> >> The documentation says:
> >> "In SMP configurations, cache_dir must not precede the workers option
> >> and should use configuration macros or conditionals to give each
> >> worker interested in disk caching a dedicated cache directory."
> >
> > The official documentation quoted above is stale and very misleading in
> > modern Squids. Ignore it. I will try to find the time to post a PR to
> > fix this.
>
> Done at https://github.com/squid-cache/squid/pull/1394
>
> Alex.
>
>
>
> >> So we switched to a rock cache_dir:
> >> cache_dir rock /data/squid/cache 32000 max-size=12000000
> >>
> >> Now everything seems to be working fine in the test environment, but I
> >> found limitations on the RockStore
> >> (https://wiki.squid-cache.org/Features/RockStore:
> >> "Objects larger than 32,000 bytes cannot be cached when cache_dirs are
> >> shared among workers."
> >
> > The Feature/RockStore page is stale and can easily mislead. In general,
> > Feature/Foo wiki pages are often development-focused and get stale with
> > time. They cannot be reliably used as a Squid feature documentation.
> >
> >
> >> Does this mean that RockStore is not suitable for caching large files?
> >
> > No, it does not. Rock storage has evolved since that Feature page was
> > written. You can see the following wiki page discussing evolved rock
> > storage design, but that page probably has some stale info as well:
> > https://wiki.squid-cache.org/Features/LargeRockStore
> >
> >
> >> Should I switch back to the UFS and configure 24 cache_dirs
> >
> > If everything is "working fine", then you should not. Otherwise, I
> > recommend discussing specific problems before switching to that
> > unsupported and dangerous hack.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230622/0baacb75/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 22 17:11:08 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 22 Jun 2023 13:11:08 -0400
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
 <001401d99d03$09206140$1b6123c0$@gmail.com>
 <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>
Message-ID: <12bd5681-0bb9-bb59-a3b0-3f527594ee22@measurement-factory.com>

On 6/22/23 04:59, Andrey K wrote:

> I reproduced the issue in the test environment.
> I configured my squid with the debug_options: ALL,1 28,9
> and ran the test curl from the same proxy host:
>  ? ?curl -m 4 -k --tlsv1.2 --proxy-user 'user:pass' -s -o /dev/null -w 
> "%{http_code}" ?--proxy localhost:3131 https://archive.org 
> <https://archive.org>
> 
> The client got the 200-response and it works fine.
> 
> In the access.log the corresponding records are:
>  ? ?2023-06-22 10:59:58| ? ?747 127.0.0.1 NONE_NONE/200/- 0 CONNECT 
> archive.org:443 <http://archive.org:443> - HIER_DIRECT/archive.org 
> <http://archive.org> - - - -
>  ? ?2023-06-22 10:59:58| ? ?201 127.0.0.1 TCP_MISS/200/200 3833 GET 
> https://archive.org/ <https://archive.org/> - HIER_DIRECT/archive.org 
> <http://archive.org> text/html - - -
> 
> The cache.log is available at the link: 
> https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing

> There are three warnings there:
>  ? ?grep WARN /tmp/acl.log
>  ? ?2023/06/22 10:59:57.875 kid6| WARNING: domdst_SIProxy ACL is used in 
> context without an HTTP request. Assuming mismatch.
>  ? ?2023/06/22 10:59:57.884 kid6| WARNING: domdst_SIProxy ACL is used in 
> context without an HTTP request. Assuming mismatch.
>  ? ?2023/06/22 10:59:58.536 kid6| WARNING: domdst_SIProxy ACL is used in 
> context without an HTTP request. Assuming mismatch.

The shared log is not detailed enough for me to pinpoint the problem, 
but there are several places in Squid code where tcp_outgoing_address is 
used without a request. Some of those places look like Squid bugs to me. 
Some look legitimate. Again, I cannot tell whether your Squid is hitting 
one of those places; if you want more definitive answers, please share a 
compressed ALL,9 log while reproducing the problem with that curl 
transaction:

https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

> The domdst_SIProxy ACL?is used only to change the outgoing address for 
> specific domains

> The test URL https://archive.org is not in the domdst_SIProxy list.

That fact does not matter -- the warnings are printed (and the directive 
is ignored) _before_ Squid checks the configured/listed values.


> I think we could ignore these warnings as squid works perfectly

I do not think we have enough information to reach that "works 
perfectly" conclusion. At the very least, you should test with a domain 
that should match domdst_SIProxy rather than one that should not match 
(and does not match, but for the wrong reason).


> maybe there is a workaround?to suppress logs flooding?

If you do not want to know what is actually going on (e.g., whether 
there is a Squid bug or misconfiguration here), then you can use a "has" 
ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address context 
from request-free evaluations. Here is an untested sketch:

     acl hasRequest has request

     # If Squid has no request access (possibly due to Squid bugs),
     # then do not use 10.72.235.184, even if domdst_SIProxy would
     # have matched if Squid had access to the request.
     tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy


Otherwise, consider sharing an ALL,9 log as discussed above.


HTH,

Alex.


> ??, 12 ???. 2023??. ? 10:54, <ngtech1ltd at gmail.com 
> <mailto:ngtech1ltd at gmail.com>>:
> 
>     Hey Ankor,
> 
>     There is some missing context so I would be able to reproduce this
>     issue.
>     Is this some kind of CONNECT request?
> 
>     If you can describe in more technical details the setup and what
>     client are you using,
>     Maybe couple sanitized log lines it would help to understand better
>     the scenario.
> 
>     Eliezer
> 
>     From: squid-users <squid-users-bounces at lists.squid-cache.org
>     <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of
>     Andrey K
>     Sent: Friday, June 9, 2023 10:03
>     To: Squid Users <squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>; Amos Jeffries
>     <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>
>     Subject: [squid-users] Using tcp_outgoing_address with ACL
> 
>     Hello,
> 
>     We use the tcp_outgoing_address feature to access some hosts using a
>     dedicated source IP address.
> 
>      ? ?acl domdst_SIProxy? dstdomain
>     "/data/squid.user/etc/squid/categories/domdst_SIProxy"
>      ? ?tcp_outgoing_address 10.72.235.129 domdst_SIProxy
> 
>     It works fine, but logs are flooded with warnings like this:
>      ? ?2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is used in
>     context without an HTTP request. Assuming mismatch.
> 
>     I found a similar case:
>     http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html <http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html> where Amos suggested using a patch as a solution.
>     We have Squid Version 5.5. Is there a similar patch for our version,
>     or can we just ignore these messages?
> 
>     Kind regards,
>      ? ? ? ?Ankor.
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From optimkaga at gmail.com  Fri Jun 23 07:45:55 2023
From: optimkaga at gmail.com (kaga optim)
Date: Fri, 23 Jun 2023 16:45:55 +0900
Subject: [squid-users] About Squid settings How can I restrict Docker
 container access with Squid and refer to the host name written in
 /etc/hosts instead of the external DNS?
Message-ID: <CAPua2S_=BZuNf9DFcvyhZJ1+X_skdu9_BWJ1U2H9qOq=8gophQ@mail.gmail.com>

Hello,We become indebted to.

If anyone is familiar with setting up squid in a linux environment, please
let me know.

I want to access the URL of the actual site in the test environment that I
do not want to connect to the production environment.
Specifically, I want to be able to control it with a file like /etc/hosts.

I am building a docker container and
For access on the docker container server
I want it to be a successful move.

----
Example: AAA.optim.co.jp is actually registered in DNS, so
The IP address of the production AAA.optim.co.jp site is referenced.
Therefore, I am trying to rewrite the hosts file on the docker container
server.

### For AAA.optim.co.jp, set to refer to your own server
$ docker exec -it ci_app_1 bash
# vi /etc/hosts
172.17.0.1 AAA.optim.co.jp


However, when I actually hit the URL, I get a 503.
$ docker exec -it ci_app_1 bash
# curl -G -I "https://AAA.optim.co.jp/~"

HTTP/1.1 200 Connection established

HTTP/1.1 503 Service Unavailable
Server: nginx
Date: Tue, 23 May 2023 08:04:30 GMT
Content-Type: application/json; charset=utf-8
Status: 503 Service Unavailable
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Cache-Control: no-cache
X-Request-Id: c690bd75-750b-4985-937f-2ac566d3b09f
X-Runtime: 0.004765
X-Cache: MISS from ip-10-0-3-217.ap-northeast-1.compute.internal
X-Cache-Lookup: MISS from ip-10-0-3-217.ap-northeast-1.compute.internal:3128
Via: 1.1 ip-10-0-3-217.ap-northeast-1.compute.internal (squid/3.5.20)
Connection: keep-alive

If you look at the Squid logs, it goes to see the production site address
registered in DNS.
1684829160.806 135 172.17.0.2 TAG_NONE/200 0 CONNECT AAA.optim.co.jp:443 -
HIER_DIRECT/AAA.AAA.AAA.AAA -
1684829160.838 28 172.17.0.2 TCP_MISS/503 600 HEAD https://AAA.optim.co.jp/?
<https://aaa.optim.co.jp/%EF%BD%9E> -
HIER_DIRECT/AAA.AAA.AAA.AAAapplication/json

----

what i want to do

?If you want to access the https://AAA.optim.co.jp/ site via Squid, you
want to refer to /etc/hosts of your own host instead of DNS.
In short, in case of access to a specific domain, can't it be separated so
that it refers to the hosts file without using DNS?
Is it possible to divide the reference destination of name resolution for
each domain in Squid? about it.

We apologize for the inconvenience, but thank you in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230623/63cc074b/attachment.htm>

From ankor2023 at gmail.com  Fri Jun 23 12:05:16 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Fri, 23 Jun 2023 15:05:16 +0300
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <12bd5681-0bb9-bb59-a3b0-3f527594ee22@measurement-factory.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
 <001401d99d03$09206140$1b6123c0$@gmail.com>
 <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>
 <12bd5681-0bb9-bb59-a3b0-3f527594ee22@measurement-factory.com>
Message-ID: <CADJd0Y36VGPJFFgt56bk=3Ngs2bNAbrEYaSxgWL5y=Pi9qJymQ@mail.gmail.com>

Hello, Alex,

Thanks for your comments.

>  if you want more definitive answers, please share a
> compressed ALL,9 log while reproducing the problem with that curl
> transaction:

A link to the uploaded ALL,9 log is:
https://drive.google.com/file/d/1kC8oV8WAelsBYoZDoqnNsnyd7cfYOoDi/view?usp=sharing

>> I think we could ignore these warnings as squid works perfectly

> I do not think we have enough information to reach that "works
> perfectly" conclusion. At the very least, you should test with a domain
> that should match domdst_SIProxy rather than one that should not match
> (and does not match, but for the wrong reason).

This is a production system and it works as expected: for domains from the
domdst_SIProxy ACL it uses correct tcp_outgoing_address: 10.72.235.184

> If you do not want to know what is actually going on (e.g., whether
> there is a Squid bug or misconfiguration here), then you can use a "has"
> ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address context
> from request-free evaluations. Here is an untested sketch:
>
>     acl hasRequest has request
>
>     # If Squid has no request access (possibly due to Squid bugs),
>     # then do not use 10.72.235.184, even if domdst_SIProxy would
>     # have matched if Squid had access to the request.
>     tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy

Thank you, Alex, I will try this workaround after you have time to take a
look at ALL,9 log.

Kind regards,
     Ankor.


??, 22 ???. 2023??. ? 20:11, Alex Rousskov <rousskov at measurement-factory.com
>:

> On 6/22/23 04:59, Andrey K wrote:
>
> > I reproduced the issue in the test environment.
> > I configured my squid with the debug_options: ALL,1 28,9
> > and ran the test curl from the same proxy host:
> >     curl -m 4 -k --tlsv1.2 --proxy-user 'user:pass' -s -o /dev/null -w
> > "%{http_code}"  --proxy localhost:3131 https://archive.org
> > <https://archive.org>
> >
> > The client got the 200-response and it works fine.
> >
> > In the access.log the corresponding records are:
> >     2023-06-22 10:59:58|    747 127.0.0.1 NONE_NONE/200/- 0 CONNECT
> > archive.org:443 <http://archive.org:443> - HIER_DIRECT/archive.org
> > <http://archive.org> - - - -
> >     2023-06-22 10:59:58|    201 127.0.0.1 TCP_MISS/200/200 3833 GET
> > https://archive.org/ <https://archive.org/> - HIER_DIRECT/archive.org
> > <http://archive.org> text/html - - -
> >
> > The cache.log is available at the link:
> >
> https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing
>
> > There are three warnings there:
> >     grep WARN /tmp/acl.log
> >     2023/06/22 10:59:57.875 kid6| WARNING: domdst_SIProxy ACL is used in
> > context without an HTTP request. Assuming mismatch.
> >     2023/06/22 10:59:57.884 kid6| WARNING: domdst_SIProxy ACL is used in
> > context without an HTTP request. Assuming mismatch.
> >     2023/06/22 10:59:58.536 kid6| WARNING: domdst_SIProxy ACL is used in
> > context without an HTTP request. Assuming mismatch.
>
> The shared log is not detailed enough for me to pinpoint the problem,
> but there are several places in Squid code where tcp_outgoing_address is
> used without a request. Some of those places look like Squid bugs to me.
> Some look legitimate. Again, I cannot tell whether your Squid is hitting
> one of those places; if you want more definitive answers, please share a
> compressed ALL,9 log while reproducing the problem with that curl
> transaction:
>
>
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
>
> > The domdst_SIProxy ACL is used only to change the outgoing address for
> > specific domains
>
> > The test URL https://archive.org is not in the domdst_SIProxy list.
>
> That fact does not matter -- the warnings are printed (and the directive
> is ignored) _before_ Squid checks the configured/listed values.
>
>
> > I think we could ignore these warnings as squid works perfectly
>
> I do not think we have enough information to reach that "works
> perfectly" conclusion. At the very least, you should test with a domain
> that should match domdst_SIProxy rather than one that should not match
> (and does not match, but for the wrong reason).
>
>
> > maybe there is a workaround to suppress logs flooding?
>
> If you do not want to know what is actually going on (e.g., whether
> there is a Squid bug or misconfiguration here), then you can use a "has"
> ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address context
> from request-free evaluations. Here is an untested sketch:
>
>      acl hasRequest has request
>
>      # If Squid has no request access (possibly due to Squid bugs),
>      # then do not use 10.72.235.184, even if domdst_SIProxy would
>      # have matched if Squid had access to the request.
>      tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
>
>
> Otherwise, consider sharing an ALL,9 log as discussed above.
>
>
> HTH,
>
> Alex.
>
>
> > ??, 12 ???. 2023??. ? 10:54, <ngtech1ltd at gmail.com
> > <mailto:ngtech1ltd at gmail.com>>:
> >
> >     Hey Ankor,
> >
> >     There is some missing context so I would be able to reproduce this
> >     issue.
> >     Is this some kind of CONNECT request?
> >
> >     If you can describe in more technical details the setup and what
> >     client are you using,
> >     Maybe couple sanitized log lines it would help to understand better
> >     the scenario.
> >
> >     Eliezer
> >
> >     From: squid-users <squid-users-bounces at lists.squid-cache.org
> >     <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of
> >     Andrey K
> >     Sent: Friday, June 9, 2023 10:03
> >     To: Squid Users <squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>; Amos Jeffries
> >     <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>
> >     Subject: [squid-users] Using tcp_outgoing_address with ACL
> >
> >     Hello,
> >
> >     We use the tcp_outgoing_address feature to access some hosts using a
> >     dedicated source IP address.
> >
> >         acl domdst_SIProxy  dstdomain
> >     "/data/squid.user/etc/squid/categories/domdst_SIProxy"
> >         tcp_outgoing_address 10.72.235.129 domdst_SIProxy
> >
> >     It works fine, but logs are flooded with warnings like this:
> >         2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is used in
> >     context without an HTTP request. Assuming mismatch.
> >
> >     I found a similar case:
> >
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
> <
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html>
> where Amos suggested using a patch as a solution.
> >     We have Squid Version 5.5. Is there a similar patch for our version,
> >     or can we just ignore these messages?
> >
> >     Kind regards,
> >             Ankor.
> >
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230623/fd3ed01a/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 24 12:20:58 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 25 Jun 2023 00:20:58 +1200
Subject: [squid-users] About Squid settings How can I restrict Docker
 container access with Squid and refer to the host name written in
 /etc/hosts instead of the external DNS?
In-Reply-To: <CAPua2S_=BZuNf9DFcvyhZJ1+X_skdu9_BWJ1U2H9qOq=8gophQ@mail.gmail.com>
References: <CAPua2S_=BZuNf9DFcvyhZJ1+X_skdu9_BWJ1U2H9qOq=8gophQ@mail.gmail.com>
Message-ID: <f92a0477-81a6-1a72-754e-aa6b26204ff8@treenet.co.nz>

On 23/06/23 19:45, kaga optim wrote:
> 
> I want to access the URL of the actual site in the test environment that 
> I do not want to connect to the production environment.
> Specifically, I want to be able to control it with a file like /etc/hosts.
> 

Looking ahead at the end of your message it seems that you have 
misinterpreted some advice you have been given elsewhere:

 > what i want to do
 >
 > ?If you want to access the https://AAA.optim.co.jp/
 > <https://AAA.optim.co.jp/> site via Squid, you want to refer to
 > /etc/hosts of your own host instead of DNS.
 > In short, in case of access to a specific domain, can't it be separated
 > so that it refers to the hosts file without using DNS?
 > Is it possible to divide the reference destination of name resolution
 > for each domain in Squid? about it.


FYI, the /etc/hosts file is part of your machines DNS configuration. So 
the answer to those questions are both "NO".
  However, they are wrong questions to be asking.




> ----
> Example: AAA.optim.co.jp <http://AAA.optim.co.jp> is actually registered 
> in DNS, so
> The IP address of the production AAA.optim.co.jp 
> <http://AAA.optim.co.jp> site is referenced.
> Therefore, I am trying to rewrite the hosts file on the docker container 
> server.
> 
> ### For AAA.optim.co.jp <http://AAA.optim.co.jp>, set to refer to your 
> own server
> $ docker exec -it ci_app_1 bash
> # vi /etc/hosts
> 172.17.0.1 AAA.optim.co.jp <http://AAA.optim.co.jp>
> 

Place that change in the /etc/hosts file of whatever machine you will be 
using to contact Squid. Not in the Squid docker container.

The IP address in that /etc/hosts file should be the Squid listening 
address/port.

FWIW, The entire (and only) purpose of that /etc/hosts record is to make 
your machines Browser (and etc) reach Squid instead of the Internet 
production server.


The Squid wiki has the squid.conf settings you need:

  For port-80 (http://) see 
<https://wiki.squid-cache.org/ConfigExamples/Reverse/BasicAccelerator>.
  For port-443 (https://) see 
<https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting>


HTH
Amos


From rousskov at measurement-factory.com  Mon Jun 26 13:49:47 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 26 Jun 2023 09:49:47 -0400
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <CADJd0Y36VGPJFFgt56bk=3Ngs2bNAbrEYaSxgWL5y=Pi9qJymQ@mail.gmail.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
 <001401d99d03$09206140$1b6123c0$@gmail.com>
 <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>
 <12bd5681-0bb9-bb59-a3b0-3f527594ee22@measurement-factory.com>
 <CADJd0Y36VGPJFFgt56bk=3Ngs2bNAbrEYaSxgWL5y=Pi9qJymQ@mail.gmail.com>
Message-ID: <3426b1f8-d26a-4280-751c-e621c617448e@measurement-factory.com>

On 6/23/23 08:05, Andrey K wrote:

> A link to the uploaded ALL,9 log is: ...

Your Squid is suffering from a bug in its ICAP client implementation: 
AFAICT, all ICAP transactions will trigger these WARNINGS if 
tcp_outgoing_address rules (the ones these ICAP transactions can reach) 
use a request-dependent ACL (e.g., dstdomain).

I filed bug #5280 at https://bugs.squid-cache.org/show_bug.cgi?id=5280

The workaround suggested in my previous response should avoid these 
WARNINGS for the affected ICAP transactions. However, it will also 
effectively disable these WARNINGS for other transactions without 
requests (if any). There may be a way to be more selective, but I cannot 
suggest something specific right now. FWIW, the "transaction_initiator" 
ACL cannot help here because it needs access to the request as well.


HTH,

Alex.


>>> I think we could ignore these warnings as squid works perfectly
> 
>  > I do not think we have enough information to reach that "works
>  > perfectly" conclusion. At the very least, you should test with a domain
>  > that should match domdst_SIProxy rather than one that should not match
>  > (and does not match, but for the wrong reason).
> 
> This is a production system and it works as expected: for domains from 
> the domdst_SIProxy ACL it uses correct tcp_outgoing_address: 10.72.235.184
> 
>  > If you do not want to know what is actually going on (e.g., whether
>  > there is a Squid bug or misconfiguration here), then you can use a "has"
>  > ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address context
>  > from request-free evaluations. Here is an untested sketch:
>  >
>  >? ? ?acl hasRequest has request
>  >
>  >? ? ?# If Squid has no request access (possibly due to Squid bugs),
>  >? ? ?# then do not use 10.72.235.184, even if domdst_SIProxy would
>  >? ? ?# have matched if Squid had access to the request.
>  >? ? ?tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
> 
> Thank you, Alex, I will try this workaround after you have time to take 
> a look at ALL,9 log.
> 
> Kind regards,
>  ? ? ?Ankor.
> 
> 
> ??, 22 ???. 2023??. ? 20:11, Alex Rousskov 
> <rousskov at measurement-factory.com 
> <mailto:rousskov at measurement-factory.com>>:
> 
>     On 6/22/23 04:59, Andrey K wrote:
> 
>      > I reproduced the issue in the test environment.
>      > I configured my squid with the debug_options: ALL,1 28,9
>      > and ran the test curl from the same proxy host:
>      >? ? ?curl -m 4 -k --tlsv1.2 --proxy-user 'user:pass' -s -o
>     /dev/null -w
>      > "%{http_code}" ?--proxy localhost:3131 https://archive.org
>     <https://archive.org>
>      > <https://archive.org <https://archive.org>>
>      >
>      > The client got the 200-response and it works fine.
>      >
>      > In the access.log the corresponding records are:
>      >? ? ?2023-06-22 10:59:58| ? ?747 127.0.0.1 NONE_NONE/200/- 0 CONNECT
>      > archive.org:443 <http://archive.org:443> <http://archive.org:443
>     <http://archive.org:443>> - HIER_DIRECT/archive.org
>     <http://archive.org>
>      > <http://archive.org <http://archive.org>> - - - -
>      >? ? ?2023-06-22 10:59:58| ? ?201 127.0.0.1 TCP_MISS/200/200 3833 GET
>      > https://archive.org/ <https://archive.org/> <https://archive.org/
>     <https://archive.org/>> - HIER_DIRECT/archive.org <http://archive.org>
>      > <http://archive.org <http://archive.org>> text/html - - -
>      >
>      > The cache.log is available at the link:
>      >
>     https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing <https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing>
> 
>      > There are three warnings there:
>      >? ? ?grep WARN /tmp/acl.log
>      >? ? ?2023/06/22 10:59:57.875 kid6| WARNING: domdst_SIProxy ACL is
>     used in
>      > context without an HTTP request. Assuming mismatch.
>      >? ? ?2023/06/22 10:59:57.884 kid6| WARNING: domdst_SIProxy ACL is
>     used in
>      > context without an HTTP request. Assuming mismatch.
>      >? ? ?2023/06/22 10:59:58.536 kid6| WARNING: domdst_SIProxy ACL is
>     used in
>      > context without an HTTP request. Assuming mismatch.
> 
>     The shared log is not detailed enough for me to pinpoint the problem,
>     but there are several places in Squid code where
>     tcp_outgoing_address is
>     used without a request. Some of those places look like Squid bugs to
>     me.
>     Some look legitimate. Again, I cannot tell whether your Squid is
>     hitting
>     one of those places; if you want more definitive answers, please
>     share a
>     compressed ALL,9 log while reproducing the problem with that curl
>     transaction:
> 
>     https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
>      > The domdst_SIProxy ACL?is used only to change the outgoing
>     address for
>      > specific domains
> 
>      > The test URL https://archive.org <https://archive.org> is not in
>     the domdst_SIProxy list.
> 
>     That fact does not matter -- the warnings are printed (and the
>     directive
>     is ignored) _before_ Squid checks the configured/listed values.
> 
> 
>      > I think we could ignore these warnings as squid works perfectly
> 
>     I do not think we have enough information to reach that "works
>     perfectly" conclusion. At the very least, you should test with a domain
>     that should match domdst_SIProxy rather than one that should not match
>     (and does not match, but for the wrong reason).
> 
> 
>      > maybe there is a workaround?to suppress logs flooding?
> 
>     If you do not want to know what is actually going on (e.g., whether
>     there is a Squid bug or misconfiguration here), then you can use a
>     "has"
>     ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address context
>     from request-free evaluations. Here is an untested sketch:
> 
>      ? ? ?acl hasRequest has request
> 
>      ? ? ?# If Squid has no request access (possibly due to Squid bugs),
>      ? ? ?# then do not use 10.72.235.184, even if domdst_SIProxy would
>      ? ? ?# have matched if Squid had access to the request.
>      ? ? ?tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
> 
> 
>     Otherwise, consider sharing an ALL,9 log as discussed above.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>      > ??, 12 ???. 2023??. ? 10:54, <ngtech1ltd at gmail.com
>     <mailto:ngtech1ltd at gmail.com>
>      > <mailto:ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>>>:
>      >
>      >? ? ?Hey Ankor,
>      >
>      >? ? ?There is some missing context so I would be able to reproduce
>     this
>      >? ? ?issue.
>      >? ? ?Is this some kind of CONNECT request?
>      >
>      >? ? ?If you can describe in more technical details the setup and what
>      >? ? ?client are you using,
>      >? ? ?Maybe couple sanitized log lines it would help to understand
>     better
>      >? ? ?the scenario.
>      >
>      >? ? ?Eliezer
>      >
>      >? ? ?From: squid-users <squid-users-bounces at lists.squid-cache.org
>     <mailto:squid-users-bounces at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users-bounces at lists.squid-cache.org
>     <mailto:squid-users-bounces at lists.squid-cache.org>>> On Behalf Of
>      >? ? ?Andrey K
>      >? ? ?Sent: Friday, June 9, 2023 10:03
>      >? ? ?To: Squid Users <squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>>; Amos Jeffries
>      >? ? ?<squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>
>     <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>
>      >? ? ?Subject: [squid-users] Using tcp_outgoing_address with ACL
>      >
>      >? ? ?Hello,
>      >
>      >? ? ?We use the tcp_outgoing_address feature to access some hosts
>     using a
>      >? ? ?dedicated source IP address.
>      >
>      >? ? ? ? ?acl domdst_SIProxy? dstdomain
>      >? ? ?"/data/squid.user/etc/squid/categories/domdst_SIProxy"
>      >? ? ? ? ?tcp_outgoing_address 10.72.235.129 domdst_SIProxy
>      >
>      >? ? ?It works fine, but logs are flooded with warnings like this:
>      >? ? ? ? ?2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is
>     used in
>      >? ? ?context without an HTTP request. Assuming mismatch.
>      >
>      >? ? ?I found a similar case:
>      >
>     http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html <http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html> <http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html <http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html>> where Amos suggested using a patch as a solution.
>      >? ? ?We have Squid Version 5.5. Is there a similar patch for our
>     version,
>      >? ? ?or can we just ignore these messages?
>      >
>      >? ? ?Kind regards,
>      >? ? ? ? ? ? ?Ankor.
>      >
>      >? ? ?_______________________________________________
>      >? ? ?squid-users mailing list
>      > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >? ? ?<mailto:squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>>
>      > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>      >? ? ?<http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>>
>      >
>      >
>      > _______________________________________________
>      > squid-users mailing list
>      > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 



From ankor2023 at gmail.com  Mon Jun 26 14:11:28 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Mon, 26 Jun 2023 17:11:28 +0300
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <3426b1f8-d26a-4280-751c-e621c617448e@measurement-factory.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
 <001401d99d03$09206140$1b6123c0$@gmail.com>
 <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>
 <12bd5681-0bb9-bb59-a3b0-3f527594ee22@measurement-factory.com>
 <CADJd0Y36VGPJFFgt56bk=3Ngs2bNAbrEYaSxgWL5y=Pi9qJymQ@mail.gmail.com>
 <3426b1f8-d26a-4280-751c-e621c617448e@measurement-factory.com>
Message-ID: <CADJd0Y1K02eoeE0WmGPWOd4Yi_0i8OMFBG5=smbUP9ReS_LNLg@mail.gmail.com>

Hello, Alex,

Thank you very much!

I will try the suggested workaround and share results.

Kind regards,
      Ankor.




??, 26 ???. 2023??. ? 16:49, Alex Rousskov <rousskov at measurement-factory.com
>:

> On 6/23/23 08:05, Andrey K wrote:
>
> > A link to the uploaded ALL,9 log is: ...
>
> Your Squid is suffering from a bug in its ICAP client implementation:
> AFAICT, all ICAP transactions will trigger these WARNINGS if
> tcp_outgoing_address rules (the ones these ICAP transactions can reach)
> use a request-dependent ACL (e.g., dstdomain).
>
> I filed bug #5280 at https://bugs.squid-cache.org/show_bug.cgi?id=5280
>
> The workaround suggested in my previous response should avoid these
> WARNINGS for the affected ICAP transactions. However, it will also
> effectively disable these WARNINGS for other transactions without
> requests (if any). There may be a way to be more selective, but I cannot
> suggest something specific right now. FWIW, the "transaction_initiator"
> ACL cannot help here because it needs access to the request as well.
>
>
> HTH,
>
> Alex.
>
>
> >>> I think we could ignore these warnings as squid works perfectly
> >
> >  > I do not think we have enough information to reach that "works
> >  > perfectly" conclusion. At the very least, you should test with a
> domain
> >  > that should match domdst_SIProxy rather than one that should not match
> >  > (and does not match, but for the wrong reason).
> >
> > This is a production system and it works as expected: for domains from
> > the domdst_SIProxy ACL it uses correct tcp_outgoing_address:
> 10.72.235.184
> >
> >  > If you do not want to know what is actually going on (e.g., whether
> >  > there is a Squid bug or misconfiguration here), then you can use a
> "has"
> >  > ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address context
> >  > from request-free evaluations. Here is an untested sketch:
> >  >
> >  >     acl hasRequest has request
> >  >
> >  >     # If Squid has no request access (possibly due to Squid bugs),
> >  >     # then do not use 10.72.235.184, even if domdst_SIProxy would
> >  >     # have matched if Squid had access to the request.
> >  >     tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
> >
> > Thank you, Alex, I will try this workaround after you have time to take
> > a look at ALL,9 log.
> >
> > Kind regards,
> >       Ankor.
> >
> >
> > ??, 22 ???. 2023??. ? 20:11, Alex Rousskov
> > <rousskov at measurement-factory.com
> > <mailto:rousskov at measurement-factory.com>>:
> >
> >     On 6/22/23 04:59, Andrey K wrote:
> >
> >      > I reproduced the issue in the test environment.
> >      > I configured my squid with the debug_options: ALL,1 28,9
> >      > and ran the test curl from the same proxy host:
> >      >     curl -m 4 -k --tlsv1.2 --proxy-user 'user:pass' -s -o
> >     /dev/null -w
> >      > "%{http_code}"  --proxy localhost:3131 https://archive.org
> >     <https://archive.org>
> >      > <https://archive.org <https://archive.org>>
> >      >
> >      > The client got the 200-response and it works fine.
> >      >
> >      > In the access.log the corresponding records are:
> >      >     2023-06-22 10:59:58|    747 127.0.0.1 NONE_NONE/200/- 0
> CONNECT
> >      > archive.org:443 <http://archive.org:443> <http://archive.org:443
> >     <http://archive.org:443>> - HIER_DIRECT/archive.org
> >     <http://archive.org>
> >      > <http://archive.org <http://archive.org>> - - - -
> >      >     2023-06-22 10:59:58|    201 127.0.0.1 TCP_MISS/200/200 3833
> GET
> >      > https://archive.org/ <https://archive.org/> <https://archive.org/
> >     <https://archive.org/>> - HIER_DIRECT/archive.org <
> http://archive.org>
> >      > <http://archive.org <http://archive.org>> text/html - - -
> >      >
> >      > The cache.log is available at the link:
> >      >
> >
> https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing
> <
> https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing
> >
> >
> >      > There are three warnings there:
> >      >     grep WARN /tmp/acl.log
> >      >     2023/06/22 10:59:57.875 kid6| WARNING: domdst_SIProxy ACL is
> >     used in
> >      > context without an HTTP request. Assuming mismatch.
> >      >     2023/06/22 10:59:57.884 kid6| WARNING: domdst_SIProxy ACL is
> >     used in
> >      > context without an HTTP request. Assuming mismatch.
> >      >     2023/06/22 10:59:58.536 kid6| WARNING: domdst_SIProxy ACL is
> >     used in
> >      > context without an HTTP request. Assuming mismatch.
> >
> >     The shared log is not detailed enough for me to pinpoint the problem,
> >     but there are several places in Squid code where
> >     tcp_outgoing_address is
> >     used without a request. Some of those places look like Squid bugs to
> >     me.
> >     Some look legitimate. Again, I cannot tell whether your Squid is
> >     hitting
> >     one of those places; if you want more definitive answers, please
> >     share a
> >     compressed ALL,9 log while reproducing the problem with that curl
> >     transaction:
> >
> >
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> >
> >
> >      > The domdst_SIProxy ACL is used only to change the outgoing
> >     address for
> >      > specific domains
> >
> >      > The test URL https://archive.org <https://archive.org> is not in
> >     the domdst_SIProxy list.
> >
> >     That fact does not matter -- the warnings are printed (and the
> >     directive
> >     is ignored) _before_ Squid checks the configured/listed values.
> >
> >
> >      > I think we could ignore these warnings as squid works perfectly
> >
> >     I do not think we have enough information to reach that "works
> >     perfectly" conclusion. At the very least, you should test with a
> domain
> >     that should match domdst_SIProxy rather than one that should not
> match
> >     (and does not match, but for the wrong reason).
> >
> >
> >      > maybe there is a workaround to suppress logs flooding?
> >
> >     If you do not want to know what is actually going on (e.g., whether
> >     there is a Squid bug or misconfiguration here), then you can use a
> >     "has"
> >     ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address
> context
> >     from request-free evaluations. Here is an untested sketch:
> >
> >           acl hasRequest has request
> >
> >           # If Squid has no request access (possibly due to Squid bugs),
> >           # then do not use 10.72.235.184, even if domdst_SIProxy would
> >           # have matched if Squid had access to the request.
> >           tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
> >
> >
> >     Otherwise, consider sharing an ALL,9 log as discussed above.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >      > ??, 12 ???. 2023??. ? 10:54, <ngtech1ltd at gmail.com
> >     <mailto:ngtech1ltd at gmail.com>
> >      > <mailto:ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>>>:
> >      >
> >      >     Hey Ankor,
> >      >
> >      >     There is some missing context so I would be able to reproduce
> >     this
> >      >     issue.
> >      >     Is this some kind of CONNECT request?
> >      >
> >      >     If you can describe in more technical details the setup and
> what
> >      >     client are you using,
> >      >     Maybe couple sanitized log lines it would help to understand
> >     better
> >      >     the scenario.
> >      >
> >      >     Eliezer
> >      >
> >      >     From: squid-users <squid-users-bounces at lists.squid-cache.org
> >     <mailto:squid-users-bounces at lists.squid-cache.org>
> >      >     <mailto:squid-users-bounces at lists.squid-cache.org
> >     <mailto:squid-users-bounces at lists.squid-cache.org>>> On Behalf Of
> >      >     Andrey K
> >      >     Sent: Friday, June 9, 2023 10:03
> >      >     To: Squid Users <squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >      >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>>; Amos Jeffries
> >      >     <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>
> >     <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>
> >      >     Subject: [squid-users] Using tcp_outgoing_address with ACL
> >      >
> >      >     Hello,
> >      >
> >      >     We use the tcp_outgoing_address feature to access some hosts
> >     using a
> >      >     dedicated source IP address.
> >      >
> >      >         acl domdst_SIProxy  dstdomain
> >      >     "/data/squid.user/etc/squid/categories/domdst_SIProxy"
> >      >         tcp_outgoing_address 10.72.235.129 domdst_SIProxy
> >      >
> >      >     It works fine, but logs are flooded with warnings like this:
> >      >         2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is
> >     used in
> >      >     context without an HTTP request. Assuming mismatch.
> >      >
> >      >     I found a similar case:
> >      >
> >
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
> <
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html>
> <
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
> <
> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html>>
> where Amos suggested using a patch as a solution.
> >      >     We have Squid Version 5.5. Is there a similar patch for our
> >     version,
> >      >     or can we just ignore these messages?
> >      >
> >      >     Kind regards,
> >      >             Ankor.
> >      >
> >      >     _______________________________________________
> >      >     squid-users mailing list
> >      > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >      >     <mailto:squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>>
> >      > http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >      >     <http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>>
> >      >
> >      >
> >      > _______________________________________________
> >      > squid-users mailing list
> >      > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >      > http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230626/2ea10e53/attachment.htm>

From robertkwild at gmail.com  Mon Jun 26 19:25:24 2023
From: robertkwild at gmail.com (robert k Wild)
Date: Mon, 26 Jun 2023 20:25:24 +0100
Subject: [squid-users] make URL bypass squid proxy
Message-ID: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>

hi all,

i have set up squid for url whitelisting and no intercept SSL (see below)

https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts

but some websites i want the client to bypass the squid proxy and go
straight to the website as i think this is why a url isnt working even when
i add the url to both files ie urlwhite and no intercept SSL



thanks,
rob

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230626/dca9fe75/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jun 26 22:35:26 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Tue, 27 Jun 2023 01:35:26 +0300
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
Message-ID: <001501d9a87e$81662140$843263c0$@gmail.com>

Hey Robert,
 
I am not sure what forward proxy setup you have there.
A simple forward proxy?
What tool are you using for whitelisting?
You can use an external acl helper to allow dynamic updates of the whitelists or
to periodic update your lists and reload.
It will depend on the size of your lists.
What OS are you using for your squid proxy?
 
More details will help us help you.
 
Eliezer
 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of robert k Wild
Sent: Monday, June 26, 2023 22:25
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] make URL bypass squid proxy
 
hi all,
 
i have set up squid for url whitelisting and no intercept SSL (see below)
 
https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts
 
but some websites i want the client to bypass the squid proxy and go straight to the website as i think this is why a url isnt working even when i add the url to both files ie urlwhite and no intercept SSL
 
 
 
thanks,
rob

-- 
Regards, 

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230627/790e6d81/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jun 26 22:40:33 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Tue, 27 Jun 2023 01:40:33 +0300
Subject: [squid-users] Enable caching
In-Reply-To: <CADJd0Y31NX4rcf3_Ao1vrXoFSKUmKz7jLAKy9HTSsEu8agjKZw@mail.gmail.com>
References: <CADJd0Y1qBCX17D71jLtDFE2Dbxjf70_pT5yTJkpSJwnUMx=3zg@mail.gmail.com>
 <1171a8a3-bf15-67ca-1635-64f8ef425c57@measurement-factory.com>
 <b7ed576f-8dbc-b706-ed09-a1c1d43a1dcb@measurement-factory.com>
 <CADJd0Y31NX4rcf3_Ao1vrXoFSKUmKz7jLAKy9HTSsEu8agjKZw@mail.gmail.com>
Message-ID: <001a01d9a87f$3899d960$a9cd8c20$@gmail.com>

I added the script to a gist just to keep it reachable:
 
https://gist.github.com/elico/dfccc0905bc223c68c483e5074a6484a
 
Eliezer
 
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Andrey K
Sent: Thursday, June 22, 2023 12:09
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Enable caching
 
Hello, Alex,
 
Thank you very much!
 
Kind regards,
      Ankor
 
??, 22 ???. 2023??. ? 05:23, Alex Rousskov <rousskov at measurement-factory.com <mailto:rousskov at measurement-factory.com> >:
On 4/5/23 09:27, Alex Rousskov wrote:
> On 4/5/23 06:07, Andrey K wrote:
> 
>> Previously, caching was disabled on our proxy servers. Now we need to 
>> cache some content (files about 10 MB in size).
>> So we changed the squid.conf:
> 
>> cache_dir ufs /data/squid/cache 32000 16 256 max-size=12000000
>>
>> We have 24 workers on each proxy.
> 
> UFS-based cache_dirs are not supported in multi-worker configurations 
> and, in most cases, should not be used in such configurations. The 
> combination will violate basic HTTP caching rules and may crash Squid 
> and/or corrupt responses.
> 
> 
>> We saw that some requests were taken from the cache, and some were not.
>> The documentation says:
>> "In SMP configurations, cache_dir must not precede the workers option 
>> and should use configuration macros or conditionals to give each 
>> worker interested in disk caching a dedicated cache directory."
> 
> The official documentation quoted above is stale and very misleading in 
> modern Squids. Ignore it. I will try to find the time to post a PR to 
> fix this.

Done at https://github.com/squid-cache/squid/pull/1394

Alex.



>> So we switched to a rock cache_dir:
>> cache_dir rock /data/squid/cache 32000 max-size=12000000
>>
>> Now everything seems to be working fine in the test environment, but I 
>> found limitations on the RockStore 
>> (https://wiki.squid-cache.org/Features/RockStore:
>> "Objects larger than 32,000 bytes cannot be cached when cache_dirs are 
>> shared among workers."
> 
> The Feature/RockStore page is stale and can easily mislead. In general, 
> Feature/Foo wiki pages are often development-focused and get stale with 
> time. They cannot be reliably used as a Squid feature documentation.
> 
> 
>> Does this mean that RockStore is not suitable for caching large files?
> 
> No, it does not. Rock storage has evolved since that Feature page was 
> written. You can see the following wiki page discussing evolved rock 
> storage design, but that page probably has some stale info as well:
> https://wiki.squid-cache.org/Features/LargeRockStore
> 
> 
>> Should I switch back to the UFS and configure 24 cache_dirs
> 
> If everything is "working fine", then you should not. Otherwise, I 
> recommend discussing specific problems before switching to that 
> unsupported and dangerous hack.



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230627/4f9db0ee/attachment.htm>

From my.shellac at gmail.com  Tue Jun 27 04:37:26 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Tue, 27 Jun 2023 09:37:26 +0500
Subject: [squid-users] Insert requested URL to custom header in request
In-Reply-To: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
References: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
Message-ID: <CAFqyDwBakONHV8K82kC-VmXkYBiBD0p2p_Qwv7kRvFVHcVk1DA@mail.gmail.com>

Hello again !
Thanks your help !

I think one more question:


Like you can know I have the run app and it?s available by

curl -H ?X-my-header:https://example.com?  http://127.0.0.1:8089
<http://127.0.0.1:8089/send>

that means  I say to my app to go to url that put in header and I get the
correct answer.

But I need to use a squid at front of this app. And I added cache_peer as
this my internal app and used option request_header_add - and this chain
works well, but for http only.
And doesn?t for https. Because for https uses TCP Connect from squid to
requested url, but my app doesn?t support Connect method and just drops
incoming.
is there some way to force squid to make something like regular get / post
request for https ( or may be just forward or bypass), or just I will need
to add the http proxy support to my app ?
Any ideas ?



Thanks
Alexey

On Tue, 20 Jun 2023 at 09:35, Alexey?? Gruzdov <my.shellac at gmail.com> wrote:

> Hello all !
>
> I have web app on the server listens on port 8089. This app processes the
> Get or Post requests, and also checks if there is specific header in
> request. For example if I will do on the server request like:
>
> curl -H ?X-my special header:https://example.com?
> http://127.0.0.1:8089/send
>
> I will get it working properly I will get answer from example.com over my
> application
>
> A questions:
>
> 1. How I can to add the specific header to request with put the value of
> this header like requested URL ??? Something like ?X-my special header:
> <requested url>
>
> 2. How I could to put the cache peer host as  originserver option as url
> ?  Something like
>
> http://127.0.0.1:8089/send
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230627/eaf30679/attachment.htm>

From robertkwild at gmail.com  Tue Jun 27 06:36:20 2023
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 27 Jun 2023 07:36:20 +0100
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <001501d9a87e$81662140$843263c0$@gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
 <001501d9a87e$81662140$843263c0$@gmail.com>
Message-ID: <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>

 Hi Eliezer,

this is a snippet of my whitelist and no intercept SSL config

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex
"/usr/local/squid/etc/interceptssl.txt"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all
#
#SSL Bump
http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB
#
#deny up MIME types
acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
#deny URL links
acl url_links url_regex "/usr/local/squid/etc/linksurl.txt"
#
#allow special URL paths
acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
#
#deny down MIME types
acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
http_reply_access allow special_url
http_reply_access deny downmime
#http_access deny upmime
#http_access deny url_links
#
#HTTP_HTTPS whitelist websites
acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
#
http_access allow activation whitelist
http_access deny all

so basically no SSL interception

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex
"/usr/local/squid/etc/interceptssl.txt"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all

and whitelisting

#HTTP_HTTPS whitelist websites
acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"

in both txt files ie

/usr/local/squid/etc/interceptssl.txt
/usr/local/squid/etc/urlwhite.txt

i have a URL that first i have to whitelist and then if i want squid not to
inspect the url traffic i put it in the SSL interception (i do this as some
websites dont like MITM )

but even putting the URL in question in both files im still having issues
with this website ie its still being detected that its passing through a
proxy

thanks,
rob

On Mon, 26 Jun 2023 at 23:35, <ngtech1ltd at gmail.com> wrote:

> Hey Robert,
>
>
>
> I am not sure what forward proxy setup you have there.
>
> A simple forward proxy?
>
> What tool are you using for whitelisting?
>
> You can use an external acl helper to allow dynamic updates of the
> whitelists or
> to periodic update your lists and reload.
> It will depend on the size of your lists.
> What OS are you using for your squid proxy?
>
>
>
> More details will help us help you.
>
>
>
> Eliezer
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *robert k Wild
> *Sent:* Monday, June 26, 2023 22:25
> *To:* Squid Users <squid-users at lists.squid-cache.org>
> *Subject:* [squid-users] make URL bypass squid proxy
>
>
>
> hi all,
>
>
>
> i have set up squid for url whitelisting and no intercept SSL (see below)
>
>
>
> https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts
>
>
>
> but some websites i want the client to bypass the squid proxy and go
> straight to the website as i think this is why a url isnt working even when
> i add the url to both files ie urlwhite and no intercept SSL
>
>
>
>
>
>
>
> thanks,
>
> rob
>
>
> --
>
> Regards,
>
> Robert K Wild.
>


-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230627/6ba16129/attachment.htm>

From ankor2023 at gmail.com  Tue Jun 27 12:55:56 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Tue, 27 Jun 2023 15:55:56 +0300
Subject: [squid-users] Using tcp_outgoing_address with ACL
In-Reply-To: <CADJd0Y1K02eoeE0WmGPWOd4Yi_0i8OMFBG5=smbUP9ReS_LNLg@mail.gmail.com>
References: <CADJd0Y17sCcU-zhaUohngN6JhystUVhoD6gJYc94=pM3FMWQhQ@mail.gmail.com>
 <001401d99d03$09206140$1b6123c0$@gmail.com>
 <CADJd0Y1x1YZEeYmBSOkBJ_G5zuiMySYsxct0HgaT7i1X9C5p3g@mail.gmail.com>
 <12bd5681-0bb9-bb59-a3b0-3f527594ee22@measurement-factory.com>
 <CADJd0Y36VGPJFFgt56bk=3Ngs2bNAbrEYaSxgWL5y=Pi9qJymQ@mail.gmail.com>
 <3426b1f8-d26a-4280-751c-e621c617448e@measurement-factory.com>
 <CADJd0Y1K02eoeE0WmGPWOd4Yi_0i8OMFBG5=smbUP9ReS_LNLg@mail.gmail.com>
Message-ID: <CADJd0Y0yk7Capg1XYWfi5aOPzBRp3zLWUA+siYD751tEAtp7+Q@mail.gmail.com>

Hello, Alex,

The suggested workaround works correctly.
Thank you very much!

Kind regards
      Ankor.

??, 26 ???. 2023??. ? 17:11, Andrey K <ankor2023 at gmail.com>:

> Hello, Alex,
>
> Thank you very much!
>
> I will try the suggested workaround and share results.
>
> Kind regards,
>       Ankor.
>
>
>
>
> ??, 26 ???. 2023??. ? 16:49, Alex Rousskov <
> rousskov at measurement-factory.com>:
>
>> On 6/23/23 08:05, Andrey K wrote:
>>
>> > A link to the uploaded ALL,9 log is: ...
>>
>> Your Squid is suffering from a bug in its ICAP client implementation:
>> AFAICT, all ICAP transactions will trigger these WARNINGS if
>> tcp_outgoing_address rules (the ones these ICAP transactions can reach)
>> use a request-dependent ACL (e.g., dstdomain).
>>
>> I filed bug #5280 at https://bugs.squid-cache.org/show_bug.cgi?id=5280
>>
>> The workaround suggested in my previous response should avoid these
>> WARNINGS for the affected ICAP transactions. However, it will also
>> effectively disable these WARNINGS for other transactions without
>> requests (if any). There may be a way to be more selective, but I cannot
>> suggest something specific right now. FWIW, the "transaction_initiator"
>> ACL cannot help here because it needs access to the request as well.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>> >>> I think we could ignore these warnings as squid works perfectly
>> >
>> >  > I do not think we have enough information to reach that "works
>> >  > perfectly" conclusion. At the very least, you should test with a
>> domain
>> >  > that should match domdst_SIProxy rather than one that should not
>> match
>> >  > (and does not match, but for the wrong reason).
>> >
>> > This is a production system and it works as expected: for domains from
>> > the domdst_SIProxy ACL it uses correct tcp_outgoing_address:
>> 10.72.235.184
>> >
>> >  > If you do not want to know what is actually going on (e.g., whether
>> >  > there is a Squid bug or misconfiguration here), then you can use a
>> "has"
>> >  > ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address
>> context
>> >  > from request-free evaluations. Here is an untested sketch:
>> >  >
>> >  >     acl hasRequest has request
>> >  >
>> >  >     # If Squid has no request access (possibly due to Squid bugs),
>> >  >     # then do not use 10.72.235.184, even if domdst_SIProxy would
>> >  >     # have matched if Squid had access to the request.
>> >  >     tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
>> >
>> > Thank you, Alex, I will try this workaround after you have time to take
>> > a look at ALL,9 log.
>> >
>> > Kind regards,
>> >       Ankor.
>> >
>> >
>> > ??, 22 ???. 2023??. ? 20:11, Alex Rousskov
>> > <rousskov at measurement-factory.com
>> > <mailto:rousskov at measurement-factory.com>>:
>> >
>> >     On 6/22/23 04:59, Andrey K wrote:
>> >
>> >      > I reproduced the issue in the test environment.
>> >      > I configured my squid with the debug_options: ALL,1 28,9
>> >      > and ran the test curl from the same proxy host:
>> >      >     curl -m 4 -k --tlsv1.2 --proxy-user 'user:pass' -s -o
>> >     /dev/null -w
>> >      > "%{http_code}"  --proxy localhost:3131 https://archive.org
>> >     <https://archive.org>
>> >      > <https://archive.org <https://archive.org>>
>> >      >
>> >      > The client got the 200-response and it works fine.
>> >      >
>> >      > In the access.log the corresponding records are:
>> >      >     2023-06-22 10:59:58|    747 127.0.0.1 NONE_NONE/200/- 0
>> CONNECT
>> >      > archive.org:443 <http://archive.org:443> <http://archive.org:443
>> >     <http://archive.org:443>> - HIER_DIRECT/archive.org
>> >     <http://archive.org>
>> >      > <http://archive.org <http://archive.org>> - - - -
>> >      >     2023-06-22 10:59:58|    201 127.0.0.1 TCP_MISS/200/200 3833
>> GET
>> >      > https://archive.org/ <https://archive.org/> <
>> https://archive.org/
>> >     <https://archive.org/>> - HIER_DIRECT/archive.org <
>> http://archive.org>
>> >      > <http://archive.org <http://archive.org>> text/html - - -
>> >      >
>> >      > The cache.log is available at the link:
>> >      >
>> >
>> https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing
>> <
>> https://drive.google.com/file/d/12xQch5nHAzijAh4PxZV4mZzjviYX7l7B/view?usp=sharing
>> >
>> >
>> >      > There are three warnings there:
>> >      >     grep WARN /tmp/acl.log
>> >      >     2023/06/22 10:59:57.875 kid6| WARNING: domdst_SIProxy ACL is
>> >     used in
>> >      > context without an HTTP request. Assuming mismatch.
>> >      >     2023/06/22 10:59:57.884 kid6| WARNING: domdst_SIProxy ACL is
>> >     used in
>> >      > context without an HTTP request. Assuming mismatch.
>> >      >     2023/06/22 10:59:58.536 kid6| WARNING: domdst_SIProxy ACL is
>> >     used in
>> >      > context without an HTTP request. Assuming mismatch.
>> >
>> >     The shared log is not detailed enough for me to pinpoint the
>> problem,
>> >     but there are several places in Squid code where
>> >     tcp_outgoing_address is
>> >     used without a request. Some of those places look like Squid bugs to
>> >     me.
>> >     Some look legitimate. Again, I cannot tell whether your Squid is
>> >     hitting
>> >     one of those places; if you want more definitive answers, please
>> >     share a
>> >     compressed ALL,9 log while reproducing the problem with that curl
>> >     transaction:
>> >
>> >
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
>> <
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
>> >
>> >
>> >      > The domdst_SIProxy ACL is used only to change the outgoing
>> >     address for
>> >      > specific domains
>> >
>> >      > The test URL https://archive.org <https://archive.org> is not in
>> >     the domdst_SIProxy list.
>> >
>> >     That fact does not matter -- the warnings are printed (and the
>> >     directive
>> >     is ignored) _before_ Squid checks the configured/listed values.
>> >
>> >
>> >      > I think we could ignore these warnings as squid works perfectly
>> >
>> >     I do not think we have enough information to reach that "works
>> >     perfectly" conclusion. At the very least, you should test with a
>> domain
>> >     that should match domdst_SIProxy rather than one that should not
>> match
>> >     (and does not match, but for the wrong reason).
>> >
>> >
>> >      > maybe there is a workaround to suppress logs flooding?
>> >
>> >     If you do not want to know what is actually going on (e.g., whether
>> >     there is a Squid bug or misconfiguration here), then you can use a
>> >     "has"
>> >     ACL to protect your domdst_SIProxy ACL in tcp_outgoing_address
>> context
>> >     from request-free evaluations. Here is an untested sketch:
>> >
>> >           acl hasRequest has request
>> >
>> >           # If Squid has no request access (possibly due to Squid bugs),
>> >           # then do not use 10.72.235.184, even if domdst_SIProxy would
>> >           # have matched if Squid had access to the request.
>> >           tcp_outgoing_address 10.72.235.184 hasRequest domdst_SIProxy
>> >
>> >
>> >     Otherwise, consider sharing an ALL,9 log as discussed above.
>> >
>> >
>> >     HTH,
>> >
>> >     Alex.
>> >
>> >
>> >      > ??, 12 ???. 2023??. ? 10:54, <ngtech1ltd at gmail.com
>> >     <mailto:ngtech1ltd at gmail.com>
>> >      > <mailto:ngtech1ltd at gmail.com <mailto:ngtech1ltd at gmail.com>>>:
>> >      >
>> >      >     Hey Ankor,
>> >      >
>> >      >     There is some missing context so I would be able to reproduce
>> >     this
>> >      >     issue.
>> >      >     Is this some kind of CONNECT request?
>> >      >
>> >      >     If you can describe in more technical details the setup and
>> what
>> >      >     client are you using,
>> >      >     Maybe couple sanitized log lines it would help to understand
>> >     better
>> >      >     the scenario.
>> >      >
>> >      >     Eliezer
>> >      >
>> >      >     From: squid-users <squid-users-bounces at lists.squid-cache.org
>> >     <mailto:squid-users-bounces at lists.squid-cache.org>
>> >      >     <mailto:squid-users-bounces at lists.squid-cache.org
>> >     <mailto:squid-users-bounces at lists.squid-cache.org>>> On Behalf Of
>> >      >     Andrey K
>> >      >     Sent: Friday, June 9, 2023 10:03
>> >      >     To: Squid Users <squid-users at lists.squid-cache.org
>> >     <mailto:squid-users at lists.squid-cache.org>
>> >      >     <mailto:squid-users at lists.squid-cache.org
>> >     <mailto:squid-users at lists.squid-cache.org>>>; Amos Jeffries
>> >      >     <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>
>> >     <mailto:squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz>>>
>> >      >     Subject: [squid-users] Using tcp_outgoing_address with ACL
>> >      >
>> >      >     Hello,
>> >      >
>> >      >     We use the tcp_outgoing_address feature to access some hosts
>> >     using a
>> >      >     dedicated source IP address.
>> >      >
>> >      >         acl domdst_SIProxy  dstdomain
>> >      >     "/data/squid.user/etc/squid/categories/domdst_SIProxy"
>> >      >         tcp_outgoing_address 10.72.235.129 domdst_SIProxy
>> >      >
>> >      >     It works fine, but logs are flooded with warnings like this:
>> >      >         2023/06/09 08:30:07 kid2| WARNING: domdst_SIProxy ACL is
>> >     used in
>> >      >     context without an HTTP request. Assuming mismatch.
>> >      >
>> >      >     I found a similar case:
>> >      >
>> >
>> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
>> <
>> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html>
>> <
>> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html
>> <
>> http://lists.squid-cache.org/pipermail/squid-users/2015-January/001629.html>>
>> where Amos suggested using a patch as a solution.
>> >      >     We have Squid Version 5.5. Is there a similar patch for our
>> >     version,
>> >      >     or can we just ignore these messages?
>> >      >
>> >      >     Kind regards,
>> >      >             Ankor.
>> >      >
>> >      >     _______________________________________________
>> >      >     squid-users mailing list
>> >      > squid-users at lists.squid-cache.org
>> >     <mailto:squid-users at lists.squid-cache.org>
>> >      >     <mailto:squid-users at lists.squid-cache.org
>> >     <mailto:squid-users at lists.squid-cache.org>>
>> >      > http://lists.squid-cache.org/listinfo/squid-users
>> >     <http://lists.squid-cache.org/listinfo/squid-users>
>> >      >     <http://lists.squid-cache.org/listinfo/squid-users
>> >     <http://lists.squid-cache.org/listinfo/squid-users>>
>> >      >
>> >      >
>> >      > _______________________________________________
>> >      > squid-users mailing list
>> >      > squid-users at lists.squid-cache.org
>> >     <mailto:squid-users at lists.squid-cache.org>
>> >      > http://lists.squid-cache.org/listinfo/squid-users
>> >     <http://lists.squid-cache.org/listinfo/squid-users>
>> >
>> >     _______________________________________________
>> >     squid-users mailing list
>> >     squid-users at lists.squid-cache.org
>> >     <mailto:squid-users at lists.squid-cache.org>
>> >     http://lists.squid-cache.org/listinfo/squid-users
>> >     <http://lists.squid-cache.org/listinfo/squid-users>
>> >
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230627/21df16aa/attachment.htm>

From robertkwild at gmail.com  Tue Jun 27 20:29:19 2023
From: robertkwild at gmail.com (robert k Wild)
Date: Tue, 27 Jun 2023 21:29:19 +0100
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
 <001501d9a87e$81662140$843263c0$@gmail.com>
 <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
Message-ID: <CAGU_Ci+E97baqX40AiL4MVhhUSCvL-PMOBh8E-57AkJXLhif_g@mail.gmail.com>

Ok I've literally commented out "http deny all" so the proxy isn't blocking
anything and allowing everything

http_access allow activation whitelist
#http_access deny all

And still it's not allowing this specific URL to go through the proxy

activate.redshift3d.com

Well it is but it isn't, as it's an activation URL it isn't activating the
app via the proxy, as soon as I pop the pc on the internet, it activates
the app

Any ideas guys?

Thanks,
Rob

On Tue, 27 Jun 2023, 07:36 robert k Wild, <robertkwild at gmail.com> wrote:

> Hi Eliezer,
>
> this is a snippet of my whitelist and no intercept SSL config
>
> #SSL Interception
> acl DiscoverSNIHost at_step SslBump1
> acl NoSSLIntercept ssl::server_name_regex
> "/usr/local/squid/etc/interceptssl.txt"
> ssl_bump peek DiscoverSNIHost
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
> #
> #SSL Bump
> http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
> #
> #deny up MIME types
> acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
> #
> #deny URL links
> acl url_links url_regex "/usr/local/squid/etc/linksurl.txt"
> #
> #allow special URL paths
> acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
> #
> #deny down MIME types
> acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
> #
> http_reply_access allow special_url
> http_reply_access deny downmime
> #http_access deny upmime
> #http_access deny url_links
> #
> #HTTP_HTTPS whitelist websites
> acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
> #
> http_access allow activation whitelist
> http_access deny all
>
> so basically no SSL interception
>
> #SSL Interception
> acl DiscoverSNIHost at_step SslBump1
> acl NoSSLIntercept ssl::server_name_regex
> "/usr/local/squid/etc/interceptssl.txt"
> ssl_bump peek DiscoverSNIHost
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
>
> and whitelisting
>
> #HTTP_HTTPS whitelist websites
> acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
>
> in both txt files ie
>
> /usr/local/squid/etc/interceptssl.txt
> /usr/local/squid/etc/urlwhite.txt
>
> i have a URL that first i have to whitelist and then if i want squid not
> to inspect the url traffic i put it in the SSL interception (i do this as
> some websites dont like MITM )
>
> but even putting the URL in question in both files im still having issues
> with this website ie its still being detected that its passing through a
> proxy
>
> thanks,
> rob
>
> On Mon, 26 Jun 2023 at 23:35, <ngtech1ltd at gmail.com> wrote:
>
>> Hey Robert,
>>
>>
>>
>> I am not sure what forward proxy setup you have there.
>>
>> A simple forward proxy?
>>
>> What tool are you using for whitelisting?
>>
>> You can use an external acl helper to allow dynamic updates of the
>> whitelists or
>> to periodic update your lists and reload.
>> It will depend on the size of your lists.
>> What OS are you using for your squid proxy?
>>
>>
>>
>> More details will help us help you.
>>
>>
>>
>> Eliezer
>>
>>
>>
>> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
>> Behalf Of *robert k Wild
>> *Sent:* Monday, June 26, 2023 22:25
>> *To:* Squid Users <squid-users at lists.squid-cache.org>
>> *Subject:* [squid-users] make URL bypass squid proxy
>>
>>
>>
>> hi all,
>>
>>
>>
>> i have set up squid for url whitelisting and no intercept SSL (see below)
>>
>>
>>
>> https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts
>>
>>
>>
>> but some websites i want the client to bypass the squid proxy and go
>> straight to the website as i think this is why a url isnt working even when
>> i add the url to both files ie urlwhite and no intercept SSL
>>
>>
>>
>>
>>
>>
>>
>> thanks,
>>
>> rob
>>
>>
>> --
>>
>> Regards,
>>
>> Robert K Wild.
>>
>
>
> --
> Regards,
>
> Robert K Wild.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230627/06fb3499/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 27 20:43:27 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Jun 2023 16:43:27 -0400
Subject: [squid-users] Insert requested URL to custom header in request
In-Reply-To: <CAFqyDwBakONHV8K82kC-VmXkYBiBD0p2p_Qwv7kRvFVHcVk1DA@mail.gmail.com>
References: <CAFqyDwBp3wc+QL5hOY=y0YuR4bAAkxARP+09aodt3-6wgEHZJA@mail.gmail.com>
 <CAFqyDwBakONHV8K82kC-VmXkYBiBD0p2p_Qwv7kRvFVHcVk1DA@mail.gmail.com>
Message-ID: <870b0f35-30d7-4ff0-a6ca-281a427a6c81@measurement-factory.com>

On 6/27/23 00:37, Alexey?? Gruzdov wrote:
> Hello again !
> Thanks your help !
> 
> I think one more question:
> 
> 
> Like you can know I have the run app and it?s available by
> 
> curl -H ?X-my-header:https://example.com <https://example.com/>? 
> http://127.0.0.1:8089 <http://127.0.0.1:8089/send>
> 
> that means ?I say to my app to go to url that put in header and I get 
> the correct answer.
> 
> But I need to use a squid at front of this app. And I added cache_peer 
> as this my internal app and used option request_header_add - and this 
> chain works well, but for http only.
> And doesn?t for https. Because for https uses TCP Connect from squid to 
> requested url, but my app doesn?t support Connect method and just drops 
> incoming.
> is there some way to force squid to make something like regular get / 
> post request for https ( or may be just forward or bypass), or just I 
> will need to add the http proxy support to my app ?

If you configure your cache_peer with an originserver flag, Squid will 
treat that cache peer as an origin server and, hence, will not try to 
open a CONNECT tunnel through it.

Alex.


> On Tue, 20 Jun 2023 at 09:35, Alexey?? Gruzdov <my.shellac at gmail.com 
> <mailto:my.shellac at gmail.com>> wrote:
> 
>     Hello all !
> 
>     I have web app on the server listens on port 8089. This app
>     processes the Get or Post requests, and also checks if there is
>     specific header in request. For example if I will do on the server
>     request like:
> 
>     curl -H ?X-my special header:https://example.com
>     <https://example.com>? http://127.0.0.1:8089/send
>     <http://127.0.0.1:8089/send>
> 
>     I will get it working properly I will get answer from example.com
>     <http://example.com> over my application
> 
>     A questions:
> 
>     1. How I can to add the specific header to request with put the
>     value of this header like requested URL ??? Something like ?X-my
>     special header: <requested url>
> 
>     2. How I could to put the cache peer host as ?originserver option as
>     url ?? Something like
> 
>     http://127.0.0.1:8089/send <http://127.0.0.1:8089/send>
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Jun 27 20:51:10 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Jun 2023 16:51:10 -0400
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <CAGU_Ci+E97baqX40AiL4MVhhUSCvL-PMOBh8E-57AkJXLhif_g@mail.gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
 <001501d9a87e$81662140$843263c0$@gmail.com>
 <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
 <CAGU_Ci+E97baqX40AiL4MVhhUSCvL-PMOBh8E-57AkJXLhif_g@mail.gmail.com>
Message-ID: <b34086dc-81e3-e4ab-d9ea-c7bd4709ca24@measurement-factory.com>

On 6/27/23 16:29, robert k Wild wrote:
> Ok I've literally commented out "http deny all" so the proxy isn't 
> blocking anything and allowing everything
> 
> http_access allow activation whitelist
> #http_access deny all
> 
> And still it's not allowing this specific URL to go through the proxy
> 
> activate.redshift3d.com <http://activate.redshift3d.com>
> 
> Well it is but it isn't, as it's an activation URL it isn't activating 
> the app via the proxy, as soon as I pop the pc on the internet, it 
> activates the app
> 
> Any ideas guys?

If you have not already, restore the "deny all" rule and make sure that 
everything works if you do not bump traffic. Use just "http_port 3128" 
if you have to, without the ssl-bump flag and related ssl_bump rules.

Once the above is working, I would check whether your app trusts your CA 
certificate (/usr/local/squid/etc/ssl_cert/myCA.pem). If you have not 
done anything about that trust on the app side, then that app will not 
trust it, and all bumped transactions will fail because the app will 
refuse to receive TLS traffic related to that certificate.

Add %err_code/%err_detail fields to your access.log using the logformat 
and access_log directives. They may help identify failed transactions.


HTH,

Alex.


> On Tue, 27 Jun 2023, 07:36 robert k Wild, <robertkwild at gmail.com 
> <mailto:robertkwild at gmail.com>> wrote:
> 
>     Hi Eliezer,
> 
>     this is a snippet of my whitelist and no intercept SSL config
> 
>     #SSL Interception
>     acl DiscoverSNIHost at_step SslBump1
>     acl NoSSLIntercept ssl::server_name_regex
>     "/usr/local/squid/etc/interceptssl.txt"
>     ssl_bump peek DiscoverSNIHost
>     ssl_bump splice NoSSLIntercept
>     ssl_bump bump all
>     #
>     #SSL Bump
>     http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem
>     generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>     sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
>     /var/lib/ssl_db -M 4MB
>     #
>     #deny up MIME types
>     acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
>     #
>     #deny URL links
>     acl url_links url_regex "/usr/local/squid/etc/linksurl.txt"
>     #
>     #allow special URL paths
>     acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
>     #
>     #deny down MIME types
>     acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
>     #
>     http_reply_access allow special_url
>     http_reply_access deny downmime
>     #http_access deny upmime
>     #http_access deny url_links
>     #
>     #HTTP_HTTPS whitelist websites
>     acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
>     #
>     http_access allow activation whitelist
>     http_access deny all
> 
>     so basically no SSL interception
> 
>     #SSL Interception
>     acl DiscoverSNIHost at_step SslBump1
>     acl NoSSLIntercept ssl::server_name_regex
>     "/usr/local/squid/etc/interceptssl.txt"
>     ssl_bump peek DiscoverSNIHost
>     ssl_bump splice NoSSLIntercept
>     ssl_bump bump all
> 
>     and whitelisting
> 
>     #HTTP_HTTPS whitelist websites
>     acl whitelist ssl::server_name_regex
>     "/usr/local/squid/etc/urlwhite.txt"
> 
>     in both txt files ie
> 
>     /usr/local/squid/etc/interceptssl.txt
>     /usr/local/squid/etc/urlwhite.txt
> 
>     i have a URL that first i have to whitelist and then if i want squid
>     not to inspect the url traffic i put it in the SSL interception (i
>     do this as some websites dont like MITM )
> 
>     but even putting the URL in question in both files im still having
>     issues with this website ie its still being detected that its
>     passing through a proxy
> 
>     thanks,
>     rob
> 
>     On Mon, 26 Jun 2023 at 23:35, <ngtech1ltd at gmail.com
>     <mailto:ngtech1ltd at gmail.com>> wrote:
> 
>         Hey Robert,____
> 
>         __ __
> 
>         I am not sure what forward proxy setup you have there.____
> 
>         A simple forward proxy?____
> 
>         What tool are you using for whitelisting?____
> 
>         You can use an external acl helper to allow dynamic updates of
>         the whitelists or
>         to periodic update your lists and reload.
>         It will depend on the size of your lists.
>         What OS are you using for your squid proxy?____
> 
>         __ __
> 
>         More details will help us help you.____
> 
>         __ __
> 
>         Eliezer____
> 
>         __ __
> 
>         *From:*squid-users <squid-users-bounces at lists.squid-cache.org
>         <mailto:squid-users-bounces at lists.squid-cache.org>> *On Behalf
>         Of *robert k Wild
>         *Sent:* Monday, June 26, 2023 22:25
>         *To:* Squid Users <squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>>
>         *Subject:* [squid-users] make URL bypass squid proxy____
> 
>         __ __
> 
>         hi all,____
> 
>         __ __
> 
>         i have set up squid for url whitelisting and no intercept SSL
>         (see below)____
> 
>         __ __
> 
>         https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts <https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts>____
> 
>         __ __
> 
>         but some websites i want the client to bypass the squid proxy
>         and go straight to the website as i think this is why a url isnt
>         working even when i add the url to both files ie urlwhite and no
>         intercept SSL____
> 
>         __ __
> 
>         __ __
> 
>         __ __
> 
>         thanks,____
> 
>         rob____
> 
> 
>         -- ____
> 
>         Regards,
> 
>         Robert K Wild.____
> 
> 
> 
>     -- 
>     Regards,
> 
>     Robert K Wild.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Wed Jun 28 16:02:53 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Wed, 28 Jun 2023 19:02:53 +0300
Subject: [squid-users] HSTS in browsers summary, help wanted.
Message-ID: <001701d9a9da$00e606e0$02b214a0$@gmail.com>

Hey Everyone,

I am testing Squid 5.9 and 6.0.3 now and I am trying to understand what might go wrong in the client side with SSL Bump.
I have a nice setup which works with a mysql DB and it can be recreated with vagrant in a very simple manner on-top of all EL8 based Distros.
(Alma, Rocky, CentOS, Oracle, RHEL, Fedora).

There are a set of helpers which runs in the background and do the heavy lifting to make the setup more dynamic.

Since I am using an existing DESKTOP there is HSTS history in the browsers:
- Edge
- Chrome
- Firefox

I have added the Root CA certificate to both Windows trusted root ca's store and into firefox certificates store.

For many sites like bing... the HSTS warning is popping out.
In edge I can disable HSTS but I don't know how to clean the HSTS cache in Edge and in other browsers.
Any help would be usefull.

Thanks,
Eliezer

* I will post later on the Vagrant sources.



From rafael.akchurin at diladele.com  Wed Jun 28 16:10:08 2023
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 28 Jun 2023 16:10:08 +0000
Subject: [squid-users] HSTS in browsers summary, help wanted.
In-Reply-To: <001701d9a9da$00e606e0$02b214a0$@gmail.com>
References: <001701d9a9da$00e606e0$02b214a0$@gmail.com>
Message-ID: <AM8PR04MB77450F390314E3CEDE31EE288F24A@AM8PR04MB7745.eurprd04.prod.outlook.com>

Hello Eliezer,

Please be sure to clean up the mimicked cert storage of Squid after changing the Root CA for sslbump (if you use one).

Best regards,
Rafael
Diladele B.V.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of ngtech1ltd at gmail.com
Sent: Wednesday, June 28, 2023 6:03 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] HSTS in browsers summary, help wanted.

Hey Everyone,

I am testing Squid 5.9 and 6.0.3 now and I am trying to understand what might go wrong in the client side with SSL Bump.
I have a nice setup which works with a mysql DB and it can be recreated with vagrant in a very simple manner on-top of all EL8 based Distros.
(Alma, Rocky, CentOS, Oracle, RHEL, Fedora).

There are a set of helpers which runs in the background and do the heavy lifting to make the setup more dynamic.

Since I am using an existing DESKTOP there is HSTS history in the browsers:
- Edge
- Chrome
- Firefox

I have added the Root CA certificate to both Windows trusted root ca's store and into firefox certificates store.

For many sites like bing... the HSTS warning is popping out.
In edge I can disable HSTS but I don't know how to clean the HSTS cache in Edge and in other browsers.
Any help would be usefull.

Thanks,
Eliezer

* I will post later on the Vagrant sources.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From ngtech1ltd at gmail.com  Thu Jun 29 00:38:53 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Thu, 29 Jun 2023 03:38:53 +0300
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
 <001501d9a87e$81662140$843263c0$@gmail.com>
 <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
Message-ID: <003601d9aa22$1505c550$3f114ff0$@gmail.com>

Hey Rob,

The first thing is to allow the domain in the http_acces just to be sure and use a basic deny all bottom line.
Let me try to simplify your squid.conf
In a link:
https://gist.github.com/elico/b49f4a28d4b5db5ba882b10d40872d5e

In plain text:
## START OF FILE
# SSL Interception  basic rules
acl DiscoverSNIHost at_step SslBump1

acl NoSSLInterceptRegEx ssl::server_name_regex (^|.*\.)redshift3d\.com$
acl NoSSLInterceptRegExFile ssl::server_name_regex "/usr/local/squid/etc/no-intercept-ssl-regex.txt"

acl NoSSLInterceptDstDom ssl::server_name .redshift3d.com
acl NoSSLInterceptDstDomFile ssl::server_name "/usr/local/squid/etc/no-intercept-ssl-dstdom.txt"

## Any of will test what ever rule match first in a first match/hit fasion
acl NoSSLInterceptAnyOf any-of NoSSLInterceptDstDom NoSSLInterceptDstDomFile NoSSLInterceptRegEx NoSSLInterceptRegExFile

ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLInterceptAnyOf
ssl_bump bump all

#SSL Bump port
http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s /var/lib/ssl_db -M 4MB

## http_access acls, will apply on incomming requests and not on responses
acl special_url_regex url_regex https?://(^|.*\.)redshift3d\.com\/
acl special_url_regex_file url_regex "/usr/local/squid/etc/special_url_regex.txt"

acl special_url_dst_dom dstdomain .redshift3d.com
acl special_url_dst_dom_file dstdomain "/usr/local/squid/etc/special_url_dstdom.txt"

acl special_url_any_of any-of special_url_dst_dom special_url_dst_dom_file special_url_regex special_url_regex_file

acl localnet src 192.168.0.0/16
acl localnet src 10.0.0.0/8

http_access allow localnet special_url_any_of
http_access deny all
## END OF FILE

 Once the above will work try to add other http_access rule like reply access rules

Let me know what happens,
Eliezer

From: robert k Wild <robertkwild at gmail.com> 
Sent: Tuesday, June 27, 2023 09:36
To: ngtech1ltd at gmail.com
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] make URL bypass squid proxy

Hi Eliezer,

this is a snippet of my whitelist and no intercept SSL config

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/usr/local/squid/etc/interceptssl.txt"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all
#
#SSL Bump
http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s /var/lib/ssl_db -M 4MB
#
#deny up MIME types
acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
#deny URL links
acl url_links url_regex "/usr/local/squid/etc/linksurl.txt"
#
#allow special URL paths
acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
#
#deny down MIME types
acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
http_reply_access allow special_url
http_reply_access deny downmime
#http_access deny upmime
#http_access deny url_links
#
#HTTP_HTTPS whitelist websites
acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
#
http_access allow activation whitelist
http_access deny all

so basically no SSL interception

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/usr/local/squid/etc/interceptssl.txt"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all 

and whitelisting

#HTTP_HTTPS whitelist websites
acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt" 

in both txt files ie

/usr/local/squid/etc/interceptssl.txt 
/usr/local/squid/etc/urlwhite.txt 

i have a URL that first i have to whitelist and then if i want squid not to inspect the url traffic i put it in the SSL interception (i do this as some websites dont like MITM )

but even putting the URL in question in both files im still having issues with this website ie its still being detected that its passing through a proxy

thanks,
rob

On Mon, 26 Jun 2023 at 23:35, <mailto:ngtech1ltd at gmail.com> wrote:
Hey Robert,
 
I am not sure what forward proxy setup you have there.
A simple forward proxy?
What tool are you using for whitelisting?
You can use an external acl helper to allow dynamic updates of the whitelists or
to periodic update your lists and reload.
It will depend on the size of your lists.
What OS are you using for your squid proxy?
 
More details will help us help you.
 
Eliezer
 
From: squid-users <mailto:squid-users-bounces at lists.squid-cache.org> On Behalf Of robert k Wild
Sent: Monday, June 26, 2023 22:25
To: Squid Users <mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] make URL bypass squid proxy
 
hi all,
 
i have set up squid for url whitelisting and no intercept SSL (see below)
 
https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts
 
but some websites i want the client to bypass the squid proxy and go straight to the website as i think this is why a url isnt working even when i add the url to both files ie urlwhite and no intercept SSL
 
 
 
thanks,
rob

-- 
Regards, 

Robert K Wild.


-- 
Regards, 

Robert K Wild.



From ngtech1ltd at gmail.com  Thu Jun 29 02:20:57 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Thu, 29 Jun 2023 05:20:57 +0300
Subject: [squid-users] Theoretically speaking about a proxy service
Message-ID: <003901d9aa30$57696f60$063c4e20$@gmail.com>

Hey Everybody,

I have seen couple free proxy providers like:
Urban vpn
Nord vpn
Clearvpn

And couple other proxy services.

A long time ago I wrote the article:
A Proxy for each Internet user! The future!

https://www1.ngtech.co.il/wpe/2016/05/02/proxy-per-internet-user-is-it-realistic/

And I was just wondering to myself a thing or two about http proxies.

Most of the VPN services use and support OpenVPN, wireguard and other vpn services on the route level.
These are simple and needs some kinds of "smart" CGNAT to operate and are cheaper than a http proxy since the it works in the lower
level of the connection.
For example, you can give a static private IP to the client in your system and apply all the relevant routing and NAT rules and the connection
will be initiated automatically with the relevant external IP.
Also, if you need an IP address you can just spin an "exit" node on any public cloud and add it into the pool of routes.

But there is another option, the proxy way of things.
Either socks or plain HTTP Proxy..

But let start with a proxy to simplify things.

Let say I want to spin couple squid "exit" nodes and I would like to have a frontend that will route traffic based on authentication details.
I have seen an answer which is un-verified since 2013 at:
https://access.redhat.com/solutions/259903

To make it all work we first need to assume that 
bever_direct allow all

will force all CONNECT requests to a cache_peer (since there aren't too many plain http services else then MS updates and couple others).

There is also another problem, how do we route clients based on credentials from a frontend to the backend exit nodes / cache peers?

There are couple issues in this kinds of setup.
Since the client connects to the proxy service in plain text it can be intercepted so we will assume that the user can access some securely to the proxy.
IE Wireguard or OpenVPN or SSTP or other IPSEC based solution which or any other alternative method like a Trusted network...

The next step in this setup is securing the connections between the proxies.
For this we need to use some kind of network of connection between the Hub or Hubs to the exit nodes.
If both the HUB and the exit node has a public IP address behind a 1:1 nat and can communicate directly they can use Wireguard or OpenVPN to secure their connections.
There are couple other things that need to be sorted and these are the provisioning of the exit nodes and their registration and status check each.
Any of the HUBs need to be able to handle couple of these tasks with a bit of automation and couple uuid generators.

I wanted to build such a tiny setup but I lack couple things for the specs for such a system.
I have seen this nice post:
* https://www.blackhatworld.com/seo/developer-needed-to-build-scripts-to-create-proxies-using-haproxy-or-squid-advanced-expertise-required.1300167/

So I am trying to mimic a WWW net.
The first thing is to have two to three ipconfig.io nodes which will have a very tiny foot print that I will use to test the setup.
The next thing is the basic WWW net ie couple sites with BGP each will have a /24(?) CIDR behind them and a central /24(?) for all of them.
Since it's a lab it's preferable that all these will have a very small resources foot print.
We can use a simple containers network and use the next piece of software:
* https://github.com/georgyo/ifconfig.io
* https://hub.docker.com/r/elicro/ifconfig.io

For the tests we might need a root CA but not really relevant since -k is good enough for most basic tests with curl since... we assume the connection is secured already.

Networks the we can use, private only(?):
192.168.0.0/16
10.0.0.0/8
172.16. 0.0/12

We can use also use CGNAT cidr:
100.64.0.0/10

* https://www.rfc-editor.org/rfc/rfc6598

And just for theses who need:
* https://www.ngtech.co.il/ipcalc/
* https://hub.docker.com/r/elicro/ipcalc


So we will need first one central hub for automation registry and management.
It will use couple internal CIDRs and couple 1:1 nat address spaces.

The end result should be couple tiny clients that will run couple curl tests with usename and password that will be the routing vector for the setup.
So we will have one main HUB and this hub will have 1 port that will listen to all proxy requests with username and passwords.
So basically we need an office and an internet connection, an idea and all the automation tools to implement it.
Currently AWS and many other providers have enough automation tools that can remove some of the heavy lifting off the table.
So now for the DB and registration system.
For each exit node we need a uuid and couple specific services.
* health check
* external ip verification
* registration against the hub
* VPN to the central HUB? (complexity.. but flexibility for the NAT connection tracking limit of the OFFICE/Proxy IP)

In the central office we need let say port 10000 a http proxy in port which will be port forwarded to a single squid proxy server with a floating ip and redundant server.
If we would have a secure channel between the proxies and the central office it will be much simple to register new proxies 
(Assuming each proxy receives the uuid and registration and VPN details in it's cloud-init or any other initialization method)

So we would have a DB which will hold a uuid and configuration details prepared before for the registration and health checks and status.

The squid.conf of the proxy should be created dynamically since there are changes in the network....
Unless we assume a specific capacity and an internal connection between the HUB and the proxy.
If we assume an internal connection between the HUB and the proxies we can dedicate a cidr for the proxies.
Then we can create a pretty "static" squid.conf (a big one..) and we can change the configuration in the DB so
helpers will help us decide which proxy is up or down and which of the static cache_peers a user name and password will use.

What do you think about this? How will it work?
Squid can handle this kind of load with couple workers and couple scripts but to create such a setup, it?s a bit of a job.
Let say I will assume a network of proxies with 10 proxies which will spin up and down, how will it work????
How much resources are required to run test such a setup?

I believe a demo can all be done on a linux network namespaces on a single node setup but it's not like real world...
What OS will you use in such a setup?
These days any linux OS requires at-least 512 MB of RAM to spin nicely so I assume an Alpine based setup would be nice but...
It's not like RHEL systems, There are scripts that should be written and supervised to be used (compared to systemd) etc...

Let me know if the script I wrote seems reasonable enough.

( 6.0.3 here I'm coming, here since 3.2 beta )

Eliezer



From robertkwild at gmail.com  Thu Jun 29 09:18:26 2023
From: robertkwild at gmail.com (robert k Wild)
Date: Thu, 29 Jun 2023 10:18:26 +0100
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <003601d9aa22$1505c550$3f114ff0$@gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
 <001501d9a87e$81662140$843263c0$@gmail.com>
 <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
 <003601d9aa22$1505c550$3f114ff0$@gmail.com>
Message-ID: <CAGU_CiKqgxK0ejJoL1F0n5YF3y-4JY2OJbpSJe7TUK6sskRKzw@mail.gmail.com>

very clever, so you bunch all the acls up

acl NoSSLInterceptAnyOf any-of NoSSLInterceptDstDom
NoSSLInterceptDstDomFile NoSSLInterceptRegEx NoSSLInterceptRegExFile

the key word is "any-of" ie if the url hits any one do that first

what about instead of making it

ssl::server_name_regex

i make it

*dstdom_regex*

On Thu, 29 Jun 2023 at 01:38, <ngtech1ltd at gmail.com> wrote:

> Hey Rob,
>
> The first thing is to allow the domain in the http_acces just to be sure
> and use a basic deny all bottom line.
> Let me try to simplify your squid.conf
> In a link:
> https://gist.github.com/elico/b49f4a28d4b5db5ba882b10d40872d5e
>
> In plain text:
> ## START OF FILE
> # SSL Interception  basic rules
> acl DiscoverSNIHost at_step SslBump1
>
> acl NoSSLInterceptRegEx ssl::server_name_regex (^|.*\.)redshift3d\.com$
> acl NoSSLInterceptRegExFile ssl::server_name_regex
> "/usr/local/squid/etc/no-intercept-ssl-regex.txt"
>
> acl NoSSLInterceptDstDom ssl::server_name .redshift3d.com
> acl NoSSLInterceptDstDomFile ssl::server_name
> "/usr/local/squid/etc/no-intercept-ssl-dstdom.txt"
>
> ## Any of will test what ever rule match first in a first match/hit fasion
> acl NoSSLInterceptAnyOf any-of NoSSLInterceptDstDom
> NoSSLInterceptDstDomFile NoSSLInterceptRegEx NoSSLInterceptRegExFile
>
> ssl_bump peek DiscoverSNIHost
> ssl_bump splice NoSSLInterceptAnyOf
> ssl_bump bump all
>
> #SSL Bump port
> http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
>
> ## http_access acls, will apply on incomming requests and not on responses
> acl special_url_regex url_regex https?://(^|.*\.)redshift3d\.com\/
> acl special_url_regex_file url_regex
> "/usr/local/squid/etc/special_url_regex.txt"
>
> acl special_url_dst_dom dstdomain .redshift3d.com
> acl special_url_dst_dom_file dstdomain
> "/usr/local/squid/etc/special_url_dstdom.txt"
>
> acl special_url_any_of any-of special_url_dst_dom special_url_dst_dom_file
> special_url_regex special_url_regex_file
>
> acl localnet src 192.168.0.0/16
> acl localnet src 10.0.0.0/8
>
> http_access allow localnet special_url_any_of
> http_access deny all
> ## END OF FILE
>
>  Once the above will work try to add other http_access rule like reply
> access rules
>
> Let me know what happens,
> Eliezer
>
> From: robert k Wild <robertkwild at gmail.com>
> Sent: Tuesday, June 27, 2023 09:36
> To: ngtech1ltd at gmail.com
> Cc: Squid Users <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] make URL bypass squid proxy
>
> Hi Eliezer,
>
> this is a snippet of my whitelist and no intercept SSL config
>
> #SSL Interception
> acl DiscoverSNIHost at_step SslBump1
> acl NoSSLIntercept ssl::server_name_regex
> "/usr/local/squid/etc/interceptssl.txt"
> ssl_bump peek DiscoverSNIHost
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
> #
> #SSL Bump
> http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
> #
> #deny up MIME types
> acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
> #
> #deny URL links
> acl url_links url_regex "/usr/local/squid/etc/linksurl.txt"
> #
> #allow special URL paths
> acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
> #
> #deny down MIME types
> acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
> #
> http_reply_access allow special_url
> http_reply_access deny downmime
> #http_access deny upmime
> #http_access deny url_links
> #
> #HTTP_HTTPS whitelist websites
> acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
> #
> http_access allow activation whitelist
> http_access deny all
>
> so basically no SSL interception
>
> #SSL Interception
> acl DiscoverSNIHost at_step SslBump1
> acl NoSSLIntercept ssl::server_name_regex
> "/usr/local/squid/etc/interceptssl.txt"
> ssl_bump peek DiscoverSNIHost
> ssl_bump splice NoSSLIntercept
> ssl_bump bump all
>
> and whitelisting
>
> #HTTP_HTTPS whitelist websites
> acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
>
> in both txt files ie
>
> /usr/local/squid/etc/interceptssl.txt
> /usr/local/squid/etc/urlwhite.txt
>
> i have a URL that first i have to whitelist and then if i want squid not
> to inspect the url traffic i put it in the SSL interception (i do this as
> some websites dont like MITM )
>
> but even putting the URL in question in both files im still having issues
> with this website ie its still being detected that its passing through a
> proxy
>
> thanks,
> rob
>
> On Mon, 26 Jun 2023 at 23:35, <mailto:ngtech1ltd at gmail.com> wrote:
> Hey Robert,
>
> I am not sure what forward proxy setup you have there.
> A simple forward proxy?
> What tool are you using for whitelisting?
> You can use an external acl helper to allow dynamic updates of the
> whitelists or
> to periodic update your lists and reload.
> It will depend on the size of your lists.
> What OS are you using for your squid proxy?
>
> More details will help us help you.
>
> Eliezer
>
> From: squid-users <mailto:squid-users-bounces at lists.squid-cache.org> On
> Behalf Of robert k Wild
> Sent: Monday, June 26, 2023 22:25
> To: Squid Users <mailto:squid-users at lists.squid-cache.org>
> Subject: [squid-users] make URL bypass squid proxy
>
> hi all,
>
> i have set up squid for url whitelisting and no intercept SSL (see below)
>
> https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts
>
> but some websites i want the client to bypass the squid proxy and go
> straight to the website as i think this is why a url isnt working even when
> i add the url to both files ie urlwhite and no intercept SSL
>
>
>
> thanks,
> rob
>
> --
> Regards,
>
> Robert K Wild.
>
>
> --
> Regards,
>
> Robert K Wild.
>
>

-- 
Regards,

Robert K Wild.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230629/2fb91e6c/attachment.htm>

From ngtech1ltd at gmail.com  Thu Jun 29 10:40:20 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Thu, 29 Jun 2023 13:40:20 +0300
Subject: [squid-users] make URL bypass squid proxy
In-Reply-To: <CAGU_CiKqgxK0ejJoL1F0n5YF3y-4JY2OJbpSJe7TUK6sskRKzw@mail.gmail.com>
References: <CAGU_CiJaVNkM7g28rSmXg+QVDRcLa05vEsNEpAe1+dMOdAjjPg@mail.gmail.com>
 <001501d9a87e$81662140$843263c0$@gmail.com>
 <CAGU_CiK5EXtYuK4dPod=sLvUHGWr63WeLdKW9k45RLjt_=e=Bg@mail.gmail.com>
 <003601d9aa22$1505c550$3f114ff0$@gmail.com>
 <CAGU_CiKqgxK0ejJoL1F0n5YF3y-4JY2OJbpSJe7TUK6sskRKzw@mail.gmail.com>
Message-ID: <005101d9aa76$1a7cce30$4f766a90$@gmail.com>

Hey Rob,

it?s a great question.
I will assume you are using squid 5.x since it?s the stable one.
There is a configuration reference documentation at:
http://www.squid-cache.org/Versions/v5/cfgman/

And the relevant one is ?acl?:
http://www.squid-cache.org/Versions/v5/cfgman/acl.html

In the config we the next options:
* dstdom_regex
* dstdomain
* ssl::server_name
* ssl::server_name_regex

Each one of them is different in two things.
The dstdomain is happening when the http request verification is done, usually in the http_access part.
So for a CONNECT request in your case it should work but will only work for http_access rules ie allow or deny the connection to being proxied by the proxy.
All this happens before the bump stage.
The dstdom_regex is doing the same but with another "lookup" mechanism.
Just to illustrate we will use the domain www.example.com.

With dstdomain .example.com which is a wildcard domain the www.example .com will match since what it does it..
Take apart the .example.com into .com and .example and example (logically, not the code actually does..)
Then when a request for www.example-2.com arrives the dstdomain acl checks the strings one by one ie:
com
example-2
www

So the "com" would match and since it has a dot "." as a prefix it continues to the next part of the domain name ie:
example-2

and it will try to match it to "example" from the ".example.com'.
then it will fail and there for it will declare the specific definition in the acl as "do not match.
But for www.example .com it would be different:

Squid will find that the .com is in the "com" part of www.example.com and will continue to the next part ie
"example" and it will match.
Then it will see the dot "." which means that all subdomains are a part of this rule ie both example.com and any other domain
under .example.com will match so... www.example.com and www2.example.com and w.example.com will match and this is compared to
example.com

with dstdomain example.com it will only match a full match of example.com but not www.example.com and it's pointless to define both:
- example.com
- .example.com

In the same acl file.
Squid will compare it to the basic proxy url request and the Host: header inside of it.
(Amos might remember which one of these wins or which one is the main one)

So that's how a dstdomain test works.
The dstdom_regex is applied on the same "part" of the proxy request ie request url and Host header but...
Tries to match it with a regex that is a much more "CPU" intensive but can do magic if you know how to use it right.
The pros are that you can use one regex to match about 100 domains however dstdomain is much faster in many cases.
To test regex in general I love:
https://rubular.com/

and it's very simple to put a line with the url or the domain itself and write the pattern and then get instant result.
There are many sites which does this exact same thing.
You need: activate.redshift3d.com  for the test.

For the server_name dstdom and regex the difference is on what part of the "known" information on the request done.
Ie it's not on the known plain text proxy request but a "peek" into the client or server TLS part of the connection bumping.
It's possible on TLS 1.2 with ease but 1.3 with encryption it's a whole other story.
(Alex and his team is more involved in this part then me)

It's a bit more complex in the server_name part since from what I remember the server_name can be the SNI itself or parts of the
certificate of the server.
In the certificate of the server in many cases there is no real domain but a catch all ie multidomain which is for example:
*.example.com

The ssl::server_name is like dstdomain so .example.com should match both SNI with www.example.com and the *.example.com
If it doesn't then it?s a bug..

About the ssl::server_name_regex you will need to test and match the regex against couple possibilities that are in the certificate ie:
*.example.com
activation.example.com

etc...

The current certificate I see is:
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            c0:71:02:fc:1f:e5:64:5b:0d:d6:ae:c8:1a:17:e6:80
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: C = GB, ST = Greater Manchester, L = Salford, O = Sectigo Limited, CN = Sectigo RSA Domain Validation Secure Server CA
        Validity
            Not Before: Dec 14 00:00:00 2022 GMT
            Not After : Jan 12 23:59:59 2024 GMT
        Subject: CN = activate.redshift3d.com
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                RSA Public-Key: (2048 bit)
                Modulus:
                    00:cd:f9:bd:58:a4:7a:16:3f:23:12:9d:e0:1e:39:
                    a0:ef:66:ae:b3:ae:32:5c:69:9e:cc:89:83:fb:a2:
                    e0:c7:9d:86:aa:e6:9b:b3:d9:0c:bb:35:36:2f:cf:
                    be:ec:1e:62:ca:d8:ab:16:66:6e:00:8d:f7:42:3d:
                    5b:d6:bf:a0:30:a1:c2:6f:5e:69:76:ef:0c:38:73:
                    d2:e8:42:b8:83:04:a3:2c:da:22:a4:c4:13:71:38:
                    36:00:1b:e7:b3:f0:fe:6e:59:17:11:e7:0a:81:3e:
                    04:b2:ad:e0:61:c1:15:f3:9a:36:33:24:a6:f1:0d:
                    b6:e6:32:91:34:54:7c:da:40:14:72:06:25:91:78:
                    75:07:84:62:47:7b:3f:05:60:e6:35:d3:27:55:1e:
                    ff:23:82:18:09:f5:8a:0f:a6:cb:02:bc:d6:09:98:
                    54:5b:10:e3:d2:09:4e:31:2a:75:42:29:00:86:9b:
                    f9:78:2f:fd:99:9e:2e:10:3e:bc:84:f5:9b:3a:3f:
                    fa:d9:b6:20:c0:2e:86:f3:de:14:be:3a:1a:05:30:
                    3b:00:e6:50:8d:4c:13:a3:97:dc:f4:03:9e:84:31:
                    0d:23:f2:02:50:6d:53:a5:30:03:b0:fd:f2:46:64:
                    49:22:a4:d1:5a:00:fb:78:37:fc:51:b1:f0:6e:41:
                    3b:5d
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Authority Key Identifier:
                keyid:8D:8C:5E:C4:54:AD:8A:E1:77:E9:9B:F9:9B:05:E1:B8:01:8D:61:E1

            X509v3 Subject Key Identifier:
                5D:BC:D9:F2:C4:99:83:5E:C0:3A:F1:BF:FE:F2:E3:92:92:F1:F0:29
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Extended Key Usage:
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Certificate Policies:
                Policy: 1.3.6.1.4.1.6449.1.2.2.7
                  CPS: https://sectigo.com/CPS
                Policy: 2.23.140.1.2.1

            Authority Information Access:
                CA Issuers - URI:http://crt.sectigo.com/SectigoRSADomainValidationSecureServerCA.crt
                OCSP - URI:http://ocsp.sectigo.com

            X509v3 Subject Alternative Name:
                DNS:activate.redshift3d.com, DNS:www.activate.redshift3d.com
            CT Precertificate SCTs:
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : 76:FF:88:3F:0A:B6:FB:95:51:C2:61:CC:F5:87:BA:34:
                                B4:A4:CD:BB:29:DC:68:42:0A:9F:E6:67:4C:5A:3A:74
                    Timestamp : Dec 14 15:34:39.227 2022 GMT
                    Extensions: none
                    Signature : ecdsa-with-SHA256
                                30:44:02:20:6E:D0:96:68:F3:07:A0:5F:DD:C5:B2:61:
                                2F:51:6B:06:4A:3C:7F:9B:DA:08:6D:5A:31:0D:B8:0B:
                                83:7D:D6:37:02:20:64:66:3F:A2:40:87:78:9E:C8:90:
                                7C:EE:7C:77:3C:BB:43:C1:9F:52:54:F1:04:85:B6:AC:
                                6B:F2:36:8E:E3:03
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : 3B:53:77:75:3E:2D:B9:80:4E:8B:30:5B:06:FE:40:3B:
                                67:D8:4F:C3:F4:C7:BD:00:0D:2D:72:6F:E1:FA:D4:17
                    Timestamp : Dec 14 15:34:39.153 2022 GMT
                    Extensions: none
                    Signature : ecdsa-with-SHA256
                                30:44:02:20:69:BA:F2:DD:32:B3:48:0C:26:E3:FB:F0:
                                DD:4E:5E:10:95:1E:B4:59:5B:67:64:C4:F2:40:7C:A8:
                                62:35:71:FE:02:20:13:15:41:26:E5:9C:DB:34:DA:D7:
                                B9:CB:B3:1A:4E:33:C7:46:7F:D9:93:45:25:7F:DE:A3:
                                72:EB:07:03:DB:C5
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : EE:CD:D0:64:D5:DB:1A:CE:C5:5C:B7:9D:B4:CD:13:A2:
                                32:87:46:7C:BC:EC:DE:C3:51:48:59:46:71:1F:B5:9B
                    Timestamp : Dec 14 15:34:39.124 2022 GMT
                    Extensions: none
                    Signature : ecdsa-with-SHA256
                                30:45:02:20:2C:E2:85:9C:A6:54:1B:1C:31:E5:F8:37:
                                E9:CD:09:8B:D8:26:29:E4:C7:65:94:9C:FF:32:D2:41:
                                CD:16:A3:51:02:21:00:A0:2F:C3:F7:A6:55:3B:21:EB:
                                9B:CA:6E:4E:07:A2:8C:40:4B:E2:27:D6:82:44:0F:09:
                                C9:F7:7D:1B:72:6F:13
    Signature Algorithm: sha256WithRSAEncryption
         25:bd:bb:de:57:c0:7f:07:5e:18:62:2e:0b:d3:03:54:a7:45:
         ab:c6:1f:e2:f6:58:ff:6e:8e:6b:4f:09:9a:87:66:32:81:7f:
         f4:35:4f:7e:65:e5:6a:04:d6:62:62:ff:d9:3a:f2:6f:19:ba:
         fa:e6:35:0e:2a:44:5c:3b:ee:9d:97:72:05:86:0c:4c:01:c1:
         f0:8c:21:c1:c4:84:54:d8:a8:05:25:18:72:db:f7:53:9b:f1:
         13:d6:0b:bc:92:6e:01:e3:fd:de:a1:45:e9:29:37:e1:2e:64:
         36:b4:4d:38:c1:60:02:6a:17:3d:87:a2:5f:33:3b:86:eb:0d:
         cc:dd:fa:d4:43:58:50:43:e7:b7:ec:0a:4f:86:72:15:e5:30:
         c9:bb:5f:0b:83:9c:26:6f:60:49:dd:1a:7c:92:45:45:4e:b5:
         ce:cd:64:8c:12:83:e9:3d:5c:6b:65:97:75:99:4c:66:eb:d0:
         3a:ca:18:62:8a:08:07:16:ab:09:66:bd:65:43:94:00:d9:79:
         3e:84:b4:60:7d:7e:f9:09:3c:fe:2d:ad:98:94:17:0c:24:8f:
         e1:a2:74:b6:3b:68:c0:01:f9:67:e8:b9:d2:6a:65:9e:99:a3:
         4a:5f:39:31:ae:c1:59:02:7b:ef:db:b2:94:06:f8:1a:74:c1:
         d7:5b:5b:6a

So the DNS names it will check are:
        Subject: CN = activate.redshift3d.com
                DNS:activate.redshift3d.com, DNS:www.activate.redshift3d.com

So to summarize the checks of ssl::server_name/ will be done on:
* activate.redshift3d.com
* activate.redshift3d.com
* www.activate.redshift3d.com

So . redshift3d.com ssl::server_name should match the certificate.
If for any reason it doesn't work you can try ssl::server_name_regex with something like:
(^|\.)activate\.redshift3d\.com$

Or just to verify if there is a bug in squid code try:
(^|\.)activate\.redshift3d\.com

Now, the splice should be able to take into account also dstdomain and dstdom_regex but it should match them only if they exist in a plain text
form like in any simple forward proxy CONNECT request.
If for any reason it doesn?t work we should investigate what might cause this issue.

I hope the scroll I wrote make sense to you and with hopes it will clear out the doubts about the wiki article you mentioned.
I believe this is considered a summary of the subject and if Alex and others might think so it can be converted into an example article in the wiki.

Let me know if this makes sense and resolve the issue.

Yours,
Eliezer

From: robert k Wild <robertkwild at gmail.com> 
Sent: Thursday, June 29, 2023 12:18
To: ngtech1ltd at gmail.com
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] make URL bypass squid proxy

very clever, so you bunch all the acls up 

acl NoSSLInterceptAnyOf any-of NoSSLInterceptDstDom NoSSLInterceptDstDomFile NoSSLInterceptRegEx NoSSLInterceptRegExFile 

the key word is "any-of" ie if the url hits any one do that first

what about instead of making it 

ssl::server_name_regex 

i make it

dstdom_regex

On Thu, 29 Jun 2023 at 01:38, <mailto:ngtech1ltd at gmail.com> wrote:
Hey Rob,

The first thing is to allow the domain in the http_acces just to be sure and use a basic deny all bottom line.
Let me try to simplify your squid.conf
In a link:
https://gist.github.com/elico/b49f4a28d4b5db5ba882b10d40872d5e

In plain text:
## START OF FILE
# SSL Interception  basic rules
acl DiscoverSNIHost at_step SslBump1

acl NoSSLInterceptRegEx ssl::server_name_regex (^|.*\.)redshift3d\.com$
acl NoSSLInterceptRegExFile ssl::server_name_regex "/usr/local/squid/etc/no-intercept-ssl-regex.txt"

acl NoSSLInterceptDstDom ssl::server_name .redshift3d.com
acl NoSSLInterceptDstDomFile ssl::server_name "/usr/local/squid/etc/no-intercept-ssl-dstdom.txt"

## Any of will test what ever rule match first in a first match/hit fasion
acl NoSSLInterceptAnyOf any-of NoSSLInterceptDstDom NoSSLInterceptDstDomFile NoSSLInterceptRegEx NoSSLInterceptRegExFile

ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLInterceptAnyOf
ssl_bump bump all

#SSL Bump port
http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s /var/lib/ssl_db -M 4MB

## http_access acls, will apply on incomming requests and not on responses
acl special_url_regex url_regex https?://(^|.*\.)redshift3d\.com\/
acl special_url_regex_file url_regex "/usr/local/squid/etc/special_url_regex.txt"

acl special_url_dst_dom dstdomain .redshift3d.com
acl special_url_dst_dom_file dstdomain "/usr/local/squid/etc/special_url_dstdom.txt"

acl special_url_any_of any-of special_url_dst_dom special_url_dst_dom_file special_url_regex special_url_regex_file

acl localnet src http://192.168.0.0/16
acl localnet src http://10.0.0.0/8

http_access allow localnet special_url_any_of
http_access deny all
## END OF FILE

 Once the above will work try to add other http_access rule like reply access rules

Let me know what happens,
Eliezer

From: robert k Wild <mailto:robertkwild at gmail.com> 
Sent: Tuesday, June 27, 2023 09:36
To: mailto:ngtech1ltd at gmail.com
Cc: Squid Users <mailto:squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] make URL bypass squid proxy

Hi Eliezer,

this is a snippet of my whitelist and no intercept SSL config

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/usr/local/squid/etc/interceptssl.txt"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all
#
#SSL Bump
http_port 3128 ssl-bump cert=/usr/local/squid/etc/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s /var/lib/ssl_db -M 4MB
#
#deny up MIME types
acl upmime req_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
#deny URL links
acl url_links url_regex "/usr/local/squid/etc/linksurl.txt"
#
#allow special URL paths
acl special_url url_regex "/usr/local/squid/etc/urlspecial.txt"
#
#deny down MIME types
acl downmime rep_mime_type "/usr/local/squid/etc/mimedeny.txt"
#
http_reply_access allow special_url
http_reply_access deny downmime
#http_access deny upmime
#http_access deny url_links
#
#HTTP_HTTPS whitelist websites
acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt"
#
http_access allow activation whitelist
http_access deny all

so basically no SSL interception

#SSL Interception
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex "/usr/local/squid/etc/interceptssl.txt"
ssl_bump peek DiscoverSNIHost
ssl_bump splice NoSSLIntercept
ssl_bump bump all 

and whitelisting

#HTTP_HTTPS whitelist websites
acl whitelist ssl::server_name_regex "/usr/local/squid/etc/urlwhite.txt" 

in both txt files ie

/usr/local/squid/etc/interceptssl.txt 
/usr/local/squid/etc/urlwhite.txt 

i have a URL that first i have to whitelist and then if i want squid not to inspect the url traffic i put it in the SSL interception (i do this as some websites dont like MITM )

but even putting the URL in question in both files im still having issues with this website ie its still being detected that its passing through a proxy

thanks,
rob

On Mon, 26 Jun 2023 at 23:35, <mailto:mailto:ngtech1ltd at gmail.com> wrote:
Hey Robert,

I am not sure what forward proxy setup you have there.
A simple forward proxy?
What tool are you using for whitelisting?
You can use an external acl helper to allow dynamic updates of the whitelists or
to periodic update your lists and reload.
It will depend on the size of your lists.
What OS are you using for your squid proxy?

More details will help us help you.

Eliezer

From: squid-users <mailto:mailto:squid-users-bounces at lists.squid-cache.org> On Behalf Of robert k Wild
Sent: Monday, June 26, 2023 22:25
To: Squid Users <mailto:mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] make URL bypass squid proxy

hi all,

i have set up squid for url whitelisting and no intercept SSL (see below)

https://wiki.squid-cache.org/ConfigExamples/Caching/AdobeProducts

but some websites i want the client to bypass the squid proxy and go straight to the website as i think this is why a url isnt working even when i add the url to both files ie urlwhite and no intercept SSL



thanks,
rob

-- 
Regards, 

Robert K Wild.


-- 
Regards, 

Robert K Wild.


-- 
Regards, 

Robert K Wild.



From ben.goz87 at gmail.com  Thu Jun 29 11:37:02 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Thu, 29 Jun 2023 14:37:02 +0300
Subject: [squid-users] QUIC - squid tproxy with c-icap content filtering
 server
Message-ID: <CADAqQfwRWE871ejV3g4AbjFXz250KAygbybN25o2rdzx75DdRw@mail.gmail.com>

By the help of God.

My squid machine is configured using tproxy and c-icap content filtering
server.
the http and https traffic redirected with iptables rules to squid ports.

What additional configurations (on squid or iptables rule) should be done
to support QUIC protocol so I can redirect the web content received to
c-icap server?

Thanks,
Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20230629/086ff24f/attachment.htm>

From squid.org at bloms.de  Fri Jun 30 09:55:44 2023
From: squid.org at bloms.de (Dieter Bloms)
Date: Fri, 30 Jun 2023 11:55:44 +0200
Subject: [squid-users] trickeling support in squid as icap client
Message-ID: <20230630095544.wrlyaujecj4v2yxf@bloms.de>

Hello,

we are currently using the Squid with an ICAP virus scanner, which is capable of trickling.
There are many manufacturers who support the ICAP protocol but not trickling.

Therefore, in my opinion, it would make sense if squid supported trickeling as ICAP client.

Then you could use any ICAP virus scanner independent from trickling support of the scanner.

What do you think about the idea?

-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From squid3 at treenet.co.nz  Fri Jun 30 11:18:15 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 30 Jun 2023 23:18:15 +1200
Subject: [squid-users] QUIC - squid tproxy with c-icap content filtering
 server
In-Reply-To: <CADAqQfwRWE871ejV3g4AbjFXz250KAygbybN25o2rdzx75DdRw@mail.gmail.com>
References: <CADAqQfwRWE871ejV3g4AbjFXz250KAygbybN25o2rdzx75DdRw@mail.gmail.com>
Message-ID: <88894abb-a363-444b-c91c-e10c1251ca1b@treenet.co.nz>

On 29/06/23 23:37, Ben Goz wrote:
> By the help of God.
> 
> My squid machine is configured using tproxy and c-icap content filtering 
> server.
> the http and https traffic redirected with iptables rules to squid ports.
> 
> What additional?configurations (on squid or iptables rule) should be 
> done to support QUIC protocol so I can redirect the web content received 
> to c-icap server?


Squid does not yet support HTTP/3 or QUIC protocol.

Some initial work is underway in 
https://github.com/squid-cache/squid/pull/919, but progress is slow.

Amos


