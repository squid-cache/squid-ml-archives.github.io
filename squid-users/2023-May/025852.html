<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Proxy server to support a large number of simultaneous requests
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Proxy%20server%20to%20support%20a%20large%20number%20of%0A%20simultaneous%20requests&In-Reply-To=%3CCADJd0Y16ZsmBCFzeKM4RSCDs0hi%2BAuJRu%3Diqgayfe4FEBmqQEA%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="025850.html">
   <LINK REL="Next"  HREF="025853.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Proxy server to support a large number of simultaneous requests</H1>
    <B>Andrey K</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Proxy%20server%20to%20support%20a%20large%20number%20of%0A%20simultaneous%20requests&In-Reply-To=%3CCADJd0Y16ZsmBCFzeKM4RSCDs0hi%2BAuJRu%3Diqgayfe4FEBmqQEA%40mail.gmail.com%3E"
       TITLE="[squid-users] Proxy server to support a large number of simultaneous requests">ankor2023 at gmail.com
       </A><BR>
    <I>Wed May 31 06:56:28 UTC 2023</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="025850.html">[squid-users] Proxy server to support a large number of simultaneous requests
</A></li>
        <LI>Next message (by thread): <A HREF="025853.html">[squid-users] Proxy server to support a large number of simultaneous requests
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#25852">[ date ]</a>
              <a href="thread.html#25852">[ thread ]</a>
              <a href="subject.html#25852">[ subject ]</a>
              <a href="author.html#25852">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hello, Alex,

Thank you for the recommendations.

&gt;<i> Do you get close to 100% hit ratio if clients access these URLs
</I>&gt;<i> sequentially rather than concurrently? If not, then focus on that
</I>&gt;<i> problem before you open the collapsed forwarding Pandora box.
</I>When I run curl sequentially like this:
for i in `seq 500`; do curl --tlsv1.2 -k   --proxy 0001vsg01:3131  -v
$URL  &gt;/dev/null 2&gt;&amp;1; done
I get only the first request with a status TCP_MISS and all others with
TCP_MEM_HIT:
    Cnt Status            Parent
    499 TCP_MEM_HIT/200/- HIER_NONE/-
      1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy


&gt;<i> What is your Squid version? Older Squids have more collapsed forwarding
</I>&gt;<i> bugs than newer ones. I recommend testing with Squid v6 or master/v7, at
</I>&gt;<i> least to confirm that the problem is still present in the latest
</I>&gt;<i> official code.
</I>I run tests on SQUID 5.9.
We compiled 6.0.2 (with disabled delay-pools) and increased memory
parameters:
  cache_mem 2048 MB
  maximum_object_size_in_memory 10 MB
The complete configuration is shown below.

Now on the version 6.0.2 we have the next results:
500 threads -  Hit ratio 3.8%:
      3 TCP_CF_HIT/200/- HIER_NONE/-
      2 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
     16 TCP_HIT/200/- HIER_NONE/-
    467 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
     12 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy

200 threads - 6%
      6 TCP_CF_HIT/200/- HIER_NONE/-
     10 TCP_HIT/200/- HIER_NONE/-
    176 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
      8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy

50 threads - 82%
     30 TCP_CF_HIT/200/- HIER_NONE/-
     11 TCP_HIT/200/- HIER_NONE/-
      1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
      8 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy

The results are slightly worse than they were on the version 5.9.
It is interesting to note that on both squid versions if I run a separate
curl after processing 500 or 200 concurrent threads, I get a result with
the status  TCP_MISS/200, although the requested URL is already in the rock
cache (I can see it in the contents of the cache using the utility I
developed rock_cache_dump.pl:
$VAR1 = {
          '1' =&gt; {
                   'VERSION' =&gt; 'Wed May 31 09:18:05 2023',
                   'KEY_MD5' =&gt; 'e5eb10f0ab7d84ff9d3fd1e5a6d3eb9c',
                   'OBJSIZE' =&gt; 446985,
                   'STD_LFS' =&gt; {
                                  'lastref' =&gt; 'Wed May 31 09:18:05 2023',
                                  'flags' =&gt; '0x4004',
                                  'expires' =&gt; 'Wed May 31 15:18:05 2023',
                                  'swap_file_sz' =&gt; 0,
                                  'refcount' =&gt; 1,
                                  'lastmod' =&gt; 'Wed Jun 29 16:09:14 2016',
                                  'timestamp' =&gt; 'Wed May 31 09:18:05 2023'
                                },
                   'URL' =&gt; '
<A HREF="https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf">https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf</A>
'
                 }
        };

).

&gt;<i> How much RAM does your server have? You are using default 256MB memory
</I>&gt;<i> cache (cache_mem). If you have spare memory, make your memory cache much
</I>&gt;<i> larger: A rock cache_dir cannot (yet) share the response _while_ the
</I>&gt;<i> response is being written to disk, so relying on cache_dir too much will
</I>&gt;<i> decrease your hit ratio, especially in a collapsed forwarding environment.
</I>The VM has 32 GB RAM. I configured cache_mem 2048 MB on the 6.0.2 version.

&gt;<i> Is your Squid built with --enable-delay-pools? If yes, TCP_MISS does not
</I>&gt;<i> necessarily mean a cache miss (an old Squid bug), even if you do not use
</I>&gt;<i> any delay pools.
</I>Yes, delay pools on the version 5.9 were enabled though we don't use them.
I disabled this feature on the 6.0.2 version.


&gt;<i> Since you are trying to cache objects lager than 512KB, see
</I>&gt;<i> maximum_object_size_in_memory.
</I>I configured  maximum_object_size_in_memory 10 MB on the 6.0.2 version (as
video chunks are less than 7 MB).

&gt;<i> Consider making your test much longer (more sequential requests per
</I>&gt;<i> client/curl worker), to see whether the cache becomes &quot;stable&quot; after one
</I>&gt;<i> of the first transactions manages to fully cache the response. This may
</I>&gt;<i> not help with older Squids, but might help with newer ones. However, you
</I>&gt;<i> should not test using real origin servers (that you do not control)!
</I>I don't have any of my own web servers for tests, so I choose some
resources on the public internet that have a robust infrastructure.
I will conduct the longer tests next week.

Kind regards,
      Ankor.

*squid.conf*
workers 21

sslcrtd_program /data/squid.user/usr/lib/squid/security_file_certgen -s
/data/squid.user/var/lib/squid/ssl_db -M 20MB
sslcrtd_children 21

logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %&gt;a %Ss/%03&gt;Hs/%&lt;Hs
%&lt;st %rm %ru %un %Sh/%&lt;A %mt %ea

logfile_rotate 0
access_log daemon:/var/log/squid.user/access.log logformat=extended-squid
on-error=drop

cache_peer parent_proxy  parent 3128 0
never_direct allow all

cachemgr_passwd pass config

acl PURGE method PURGE
http_access allow PURGE

http_access allow all

http_port 3131 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=20MB tls-cert=/etc/squid.user/sslbump/bump.crt
tls-key=/etc/squid.user/sslbump/bump.key
sslproxy_cert_error allow all

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1
ssl_bump bump step2
ssl_bump bump step3

cache_dir rock /data/squid.user/cache 20000 max-size=12000000
cache_swap_low 85
cache_swap_high 90

collapsed_forwarding on
cache_mem 2048 MB
maximum_object_size_in_memory 10 MB

pinger_enable off
max_filedesc 8192
shutdown_lifetime 5 seconds
netdb_filename none
log_icp_queries off

via off
forwarded_for delete

client_request_buffer_max_size 100 MB

coredump_dir /data/squid.user/var/cache/squid




&#1087;&#1085;, 29 &#1084;&#1072;&#1103; 2023&#8239;&#1075;. &#1074; 23:17, Alex Rousskov &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">rousskov at measurement-factory.com</A>
&gt;:<i>
</I>
&gt;<i> On 5/29/23 10:43, Andrey K wrote:
</I>&gt;<i>
</I>&gt;<i> &gt; We need to configure a dedicated proxy server to provide caching of
</I>&gt;<i> &gt; online video broadcasts in order to reduce the load on the uplink proxy.
</I>&gt;<i> &gt; Hundreds of users will access the same video-chunks simultaneously.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; I developed a simple configuration for the test purposes (it is shown
</I>&gt;<i> &gt; below).
</I>&gt;<i> &gt; The *collapsed_forwarding* option is on.
</I>&gt;<i>
</I>&gt;<i> Do you get close to 100% hit ratio if clients access these URLs
</I>&gt;<i> sequentially rather than concurrently? If not, then focus on that
</I>&gt;<i> problem before you open the collapsed forwarding Pandora box.
</I>&gt;<i>
</I>&gt;<i> What is your Squid version? Older Squids have more collapsed forwarding
</I>&gt;<i> bugs than newer ones. I recommend testing with Squid v6 or master/v7, at
</I>&gt;<i> least to confirm that the problem is still present in the latest
</I>&gt;<i> official code.
</I>&gt;<i>
</I>&gt;<i> How much RAM does your server have? You are using default 256MB memory
</I>&gt;<i> cache (cache_mem). If you have spare memory, make your memory cache much
</I>&gt;<i> larger: A rock cache_dir cannot (yet) share the response _while_ the
</I>&gt;<i> response is being written to disk, so relying on cache_dir too much will
</I>&gt;<i> decrease your hit ratio, especially in a collapsed forwarding environment.
</I>&gt;<i>
</I>&gt;<i> Is your Squid built with --enable-delay-pools? If yes, TCP_MISS does not
</I>&gt;<i> necessarily mean a cache miss (an old Squid bug), even if you do not use
</I>&gt;<i> any delay pools.
</I>&gt;<i>
</I>&gt;<i> Since you are trying to cache objects lager than 512KB, see
</I>&gt;<i> maximum_object_size_in_memory.
</I>&gt;<i>
</I>&gt;<i> Consider making your test much longer (more sequential requests per
</I>&gt;<i> client/curl worker), to see whether the cache becomes &quot;stable&quot; after one
</I>&gt;<i> of the first transactions manages to fully cache the response. This may
</I>&gt;<i> not help with older Squids, but might help with newer ones. However, you
</I>&gt;<i> should not test using real origin servers (that you do not control)!
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> &gt; Could you clarify if this behavior of my squid is
</I>&gt;<i> &gt; a bug/misconfiguration, or if I'm running into server performance
</I>&gt;<i> &gt; limitations (squid is running on a VM with 22 cores)?
</I>&gt;<i>
</I>&gt;<i> Most likely, reduction of hit ratio with increase of concurrency is
</I>&gt;<i> _not_ a performance limitation.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> HTH,
</I>&gt;<i>
</I>&gt;<i> Alex.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> &gt; I selected a couple of cacheable resources in the internet for testing:
</I>&gt;<i> &gt;   - small size (~400 KB):
</I>&gt;<i> &gt;
</I>&gt;<i> <A HREF="https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf">https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf</A>
</I>&gt;<i> &lt;
</I>&gt;<i> <A HREF="https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf">https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf</A>
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;   - large (~8 MB):
</I>&gt;<i> &gt;
</I>&gt;<i> <A HREF="https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf">https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf</A>
</I>&gt;<i> &lt;
</I>&gt;<i> <A HREF="https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf">https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf</A>
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; To test simultaneous connections I am forking curl using a simple script
</I>&gt;<i> &gt; (it is also shown below).
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; When I run a test (500 curl threads to
</I>&gt;<i> &gt;
</I>&gt;<i> <A HREF="https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf">https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf</A>
</I>&gt;<i> &lt;
</I>&gt;<i> <A HREF="https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf">https://ia800406.us.archive.org/13/items/romeo-y-julieta-texto-completo/Romeo%20y%20Julieta%20-%20William%20Shakespeare.pdf</A>&gt;)
</I>&gt;<i> I see lots of TCP_MISS/200 with FIRSTUP_PARENT/parent_proxy records in the
</I>&gt;<i> logs.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; A simple analysis shows a low percentage of cache hits:
</I>&gt;<i> &gt; cat /var/log/squid.user/access.log| grep '2023-05-29 14' | grep pdf  |
</I>&gt;<i> &gt; awk '{print $5&quot; &quot; $10}' | sort | uniq -c
</I>&gt;<i> &gt;       24 TCP_CF_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;      457 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;       10 TCP_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;        9 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; So the Hit ratio is about (500-457-9)*100/500=6.8%
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Almost the same situation we see when run 200 threads:
</I>&gt;<i> &gt; cat /var/log/squid.user/access.log| grep '2023-05-29 15:45' | grep pdf
</I>&gt;<i> &gt;   | awk '{print $5&quot; &quot; $10}' | sort | uniq -c
</I>&gt;<i> &gt;        4 TCP_CF_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;      140 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;       40 TCP_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;       16 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; This time the Hit ratio is about (200-140-16)*100/500=21%
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; With 50 threads the Hit ratio is 90%:
</I>&gt;<i> &gt; cat /var/log/squid.user/access.log| grep '2023-05-29 15:50' | grep pdf
</I>&gt;<i> &gt;   | awk '{print $5&quot; &quot; $10}' | sort | uniq -c
</I>&gt;<i> &gt;       27 TCP_CF_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;       18 TCP_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;        4 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; I thought that it should always be near 99% - only the first request to
</I>&gt;<i> &gt; an URL should be forwarded to the parent proxy and all subsequent
</I>&gt;<i> &gt; requests should be served from the cache.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; The situation is even worse with downloading a large file:
</I>&gt;<i> &gt; 500 threads (0.4%):
</I>&gt;<i> &gt; cat /var/log/squid.user/access.log| grep '2023-05-29 17:2' | grep pdf  |
</I>&gt;<i> &gt; awk '{print $5&quot; &quot; $10}' | sort | uniq -c
</I>&gt;<i> &gt;       10 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;        2 TCP_CF_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;      488 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; 200 threads (3%):
</I>&gt;<i> &gt; cat /var/log/squid.user/access.log| grep '2023-05-29 17:3' | grep pdf  |
</I>&gt;<i> &gt; awk '{print $5&quot; &quot; $10}' | sort | uniq -c
</I>&gt;<i> &gt;        9 TCP_CF_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;        6 TCP_CF_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;      180 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;        5 TCP_SWAPFAIL_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; 50 threads (98%):
</I>&gt;<i> &gt; cat /var/log/squid.user/access.log| grep '2023-05-29 17:36' | grep pdf
</I>&gt;<i> &gt;   | awk '{print $5&quot; &quot; $10}' | sort | uniq -c
</I>&gt;<i> &gt;       25 TCP_CF_HIT/200/- HIER_NONE/-
</I>&gt;<i> &gt;       12 TCP_CF_MISS/200/- HIER_NONE/-
</I>&gt;<i> &gt;       12 TCP_HIT/200/- HIER_NONE/-
</I>&gt;<i> &gt;        1 TCP_MISS/200/200 FIRSTUP_PARENT/parent_proxy
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Could you clarify if this behavior of my squid is a
</I>&gt;<i> &gt; bug/misconfiguration, or if I'm running into server performance
</I>&gt;<i> &gt; limitations (squid is running on a VM with 22 cores)?
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Kind regards,
</I>&gt;<i> &gt;       Ankor
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; *squid.conf:*
</I>&gt;<i> &gt; workers 21
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; sslcrtd_program /data/squid.user/usr/lib/squid/security_file_certgen -s
</I>&gt;<i> &gt; /data/squid.user/var/lib/squid/ssl_db -M 20MB
</I>&gt;<i> &gt; sslcrtd_children 21
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; logformat extended-squid %{%Y-%m-%d %H:%M:%S}tl| %6tr %&gt;a
</I>&gt;<i> &gt; %Ss/%03&gt;Hs/%&lt;Hs %&lt;st %rm %ru %un %Sh/%&lt;A %mt %ea
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; logfile_rotate 0
</I>&gt;<i> &gt; access_log daemon:/var/log/squid.user/access.log
</I>&gt;<i> &gt; logformat=extended-squid on-error=drop
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_peer parent_proxy  parent 3128 0
</I>&gt;<i> &gt; never_direct allow all
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cachemgr_passwd pass config
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl PURGE method PURGE
</I>&gt;<i> &gt; http_access allow PURGE
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_access allow all
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; http_port 3131 ssl-bump generate-host-certificates=on
</I>&gt;<i> &gt; dynamic_cert_mem_cache_size=20MB
</I>&gt;<i> &gt; tls-cert=/etc/squid.user/sslbump/bump.crt
</I>&gt;<i> &gt; tls-key=/etc/squid.user/sslbump/bump.key
</I>&gt;<i> &gt; sslproxy_cert_error allow all
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; acl step1 at_step SslBump1
</I>&gt;<i> &gt; acl step2 at_step SslBump2
</I>&gt;<i> &gt; acl step3 at_step SslBump3
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; ssl_bump peek step1
</I>&gt;<i> &gt; ssl_bump bump step2
</I>&gt;<i> &gt; ssl_bump bump step3
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; cache_dir rock /data/squid.user/cache 20000 max-size=12000000
</I>&gt;<i> &gt; cache_swap_low 85
</I>&gt;<i> &gt; cache_swap_high 90
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; *collapsed_forwarding on*
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; pinger_enable off
</I>&gt;<i> &gt; max_filedesc 8192
</I>&gt;<i> &gt; shutdown_lifetime 5 seconds
</I>&gt;<i> &gt; netdb_filename none
</I>&gt;<i> &gt; log_icp_queries off
</I>&gt;<i> &gt; client_request_buffer_max_size 100 MB
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; via off
</I>&gt;<i> &gt; forwarded_for delete
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; coredump_dir /data/squid.user/var/cache/squid
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; *curl_forker.sh:*
</I>&gt;<i> &gt; #!/bin/sh
</I>&gt;<i> &gt; N=100
</I>&gt;<i> &gt; URL=
</I>&gt;<i> <A HREF="https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf">https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf</A>
</I>&gt;<i> &lt;
</I>&gt;<i> <A HREF="https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf">https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf</A>
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; if [[  -n $1 &amp;&amp;  $1 =~ help$  ]];
</I>&gt;<i> &gt; then
</I>&gt;<i> &gt;     echo &quot;Usage: $0 [&lt;cnt&gt;] [&lt;url&gt;]&quot;
</I>&gt;<i> &gt;     echo
</I>&gt;<i> &gt;     echo &quot;Example: $0 10
</I>&gt;<i> &gt;
</I>&gt;<i> <A HREF="https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf">https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf</A>
</I>&gt;<i> &lt;
</I>&gt;<i> <A HREF="https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf">https://ia600601.us.archive.org/10/items/Linux-Journal-2015-01/Linux-Journal-2015-01.pdf</A>
</I>&gt;<i> &gt;&quot;;
</I>&gt;<i> &gt;     echo
</I>&gt;<i> &gt;     exit;
</I>&gt;<i> &gt; fi
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; while [[ $# -gt 0 ]]
</I>&gt;<i> &gt; do
</I>&gt;<i> &gt;    if [[ $1 =~ ^[0-9]+$ ]]
</I>&gt;<i> &gt;    then
</I>&gt;<i> &gt;       N=$1
</I>&gt;<i> &gt;    else
</I>&gt;<i> &gt;       URL=$1
</I>&gt;<i> &gt;    fi
</I>&gt;<i> &gt;    shift
</I>&gt;<i> &gt; done
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; echo $URL
</I>&gt;<i> &gt; echo $N threads
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; for i in `seq $N`
</I>&gt;<i> &gt; do
</I>&gt;<i> &gt;    nohup curl --tlsv1.2 -k   --proxy 0001vsg01:3131  -v $URL  &gt;/dev/null
</I>&gt;<i> &gt;   2&gt;&amp;1 &amp;
</I>&gt;<i> &gt; done
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; _______________________________________________
</I>&gt;<i> &gt; squid-users mailing list
</I>&gt;<i> &gt; <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;<i> &gt; <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> squid-users mailing list
</I>&gt;<i> <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
</I>&gt;<i> <A HREF="http://lists.squid-cache.org/listinfo/squid-users">http://lists.squid-cache.org/listinfo/squid-users</A>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.squid-cache.org/pipermail/squid-users/attachments/20230531/fa233276/attachment.htm">http://lists.squid-cache.org/pipermail/squid-users/attachments/20230531/fa233276/attachment.htm</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="025850.html">[squid-users] Proxy server to support a large number of simultaneous requests
</A></li>
	<LI>Next message (by thread): <A HREF="025853.html">[squid-users] Proxy server to support a large number of simultaneous requests
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#25852">[ date ]</a>
              <a href="thread.html#25852">[ thread ]</a>
              <a href="subject.html#25852">[ subject ]</a>
              <a href="author.html#25852">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
