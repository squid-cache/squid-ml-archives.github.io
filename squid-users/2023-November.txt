From rousskov at measurement-factory.com  Wed Nov  1 20:39:20 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 1 Nov 2023 16:39:20 -0400
Subject: [squid-users] Get IP of denied request
In-Reply-To: <c5df1856-0750-4259-9dd9-c01ccf90cc30@web.de>
References: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>
 <95cddc1f-73e9-431d-8ba0-e92a09a95487@measurement-factory.com>
 <107552eb-8406-455a-88f0-6d1bb3a2d328@web.de>
 <e3d70ac4-a680-4de8-bfd9-4e6fd0698fa2@measurement-factory.com>
 <c5df1856-0750-4259-9dd9-c01ccf90cc30@web.de>
Message-ID: <9eebab85-2b64-4ec6-b656-16fabc4ac0f8@measurement-factory.com>

On 2023-10-30 13:08, magri at web.de wrote:
> Am 27.10.23 um 16:22 schrieb Alex Rousskov:
>> 1. Enhance Squid to resolve transaction destination address once (on
>> first use/need). Remember/reuse resolved IP addresses. Log them via some
>> new %resolved_dst and %dst_resolution_detail codes.
>>
>> This improvement will help address a few use cases unrelated to this
>> discussion, but it will _not_ tell you which of the resolved addresses
>> actually matched which ACL. You will be able to guess in many cases, but
>> there will be exceptions.
>>
>>
>> 2. Add a Squid feature where an evaluated ACL can be configured to
>> annotate the transaction with the information about that evaluation.
>>
>> To start with, we can support annotations for _matched_ dst ACLs. For
>> example:
>>
>> ???? # When matches, sets the following master transaction annotations:
>> ???? # * badDestination_input: used destination address (IP or name)
>> ???? # * badDestination_match: the matched destination IP
>> ???? # * badDestination_value: the matched ACL parameter value
>> ???? # * badDestination_ips: resolved destination IP(s)
>> ???? # * badDestination_errors: DNS resolution and other error(s)
>> ???? # * badDestination_matches: number of matches (this transaction)
>> ???? # * badDestination_evals: number of evaluations (this transaction)
>> ???? acl badDestination dst --on-match=annotate 127.0.0.0/24 10.0.0.1
>>
>> If the same ACL matches more than once, the last(?) match wins, but the
>> aclfoo_matches annotation can be used to detect these cases. The
>> aclfoo_evals annotation can be used to detect whether this ACL was
>> "reached" at all.
>>
>> If really needed, we can support turning individual annotations on and
>> off, but I doubt that complexity is worth associated performance
>> savings. After all, most ACLs will only match once per transaction
>> lifetime (for correctly written configurations). Access.log will be
>> configured to only log annotations of interest to the admin, of course.
>>
>>
>> The above approach can be extended to provide ACL debugging aid:
>>
>> ???? # Dumps every mismatch information to cache.log at level 1
>> ???? acl goodDestination dst --on-mismatch=log 127.0.0.0/24
>>
>> ???? # Dumps every evaluation information to cache.log at level 1
>> ???? acl redDestination dst --on-eval=log 127.0.0.0/24
>>
>>
>> 3. Add a Squid feature where Squid (optionally) maintains an internal
>> database of recent ACL evaluation history and makes that information
>> accessible via cache manager queries like "which ACLs matched
>> transaction X?" (where X is logged %master_xaction ID).
>>
>>
>> The three sketched options are not mutually exclusive, of course. All
>> require non-trivial code changes.



> Let me first ask some questions for clarification:
> - Does squid cache all ips from dns responses with multiple ips or only
> the one it uses for the request?

All.


> - If it caches more than one ip - does squid use more than one of these
> ips (e.g. as fallback or round robin) inside a single transaction or for
> multiple transactions?

A single transaction will go through several single-name IPs after 
certain transaction failures, but not all failures are treated the same.

Multiple transactions may use multiple single-name IPs due to 
round-robin rotation of cached IPs and other factors.


> Supposing squid uses only a single dst ip inside a single transaction,
> your first option would be sufficient for our purpose!

In my first option, logged %resolved_dst (and the corresponding master 
transaction metadata) may contain multiple IPs (i.e. all resolved ones). 
AFAICT, if any one of those IPs match a "dst" ACL address, then that 
"dst" ACL should match, even if the transaction does not actually use 
the matched address: The "dst" ACL matches based on request-target info, 
not the _use_ of that info.

One of the difficulties here is to decide whether a transaction should 
wait for both IPv4 and IPv6 addresses to be resolved:

If a transaction proceeds with one family of addresses, then there will 
be no delays associated with waiting for the second one. However, in 
that case, Squid may declare a "dst" ACL mismatch even though there 
would have been a match if we waited for the second DNS answer! It feels 
like for correctness sake, we must wait if a "dst" ACL needs checking or 
a similar decision has to be made...


> Resolving the ip only once and storing it inside the transaction would
> also avoid ambiguous cases where different ips could theoretically be
> used in different acls or worse in acls and the real connection.

Yes, that is one of the positive side effects of that option.


> It could also improve the performance of acl evaluation because after
> the resolution of the dst ip all following dst acls would be evaluated
> fast(?).

A speedup is possible in some corner cases, but, in most cases, there 
will be no difference in this area because "all following dst ACLs" 
should "immediately" use _cached_ IP(s) in the current code AFAICT.


> Your second option sounds really nice for debugging purposes but IMHO
> these annotations should be optional because even if the performance
> penalty may(?) be neglectable they increase the size of the transaction
> object and this could accumulate if many acls are in use.
> Hence I would propose this as optional extension to the first option.

Yes, the second feature is optional (and orthogonal): If one does not 
add "--on-match=annotate" to a "dst" ACL declaration, then they do not 
get those annotations (and their overheads).


> Your third option seems not feasible for us, because the delay between a
> failed request and reporting of the failure often takes more than a day.
> The needed history would be quite excessive (with over 80 million
> requests a day).

Agreed.

Alex.



>>>>> Is there any way to get the ip logged that was used in the dst-acl
>>>>> aside
>>>>> from debug logging? Maybe through some annotation mechanism?
>>>>>
>>>>> Squid version is 6.2, as 6.4 crashes with assertion errors here, too.
>>>>>
>>>>> thanks,
>>>>> Martin
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From andre.bolinhas at articatech.com  Thu Nov  2 00:08:47 2023
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Thu, 2 Nov 2023 00:08:47 +0000
Subject: [squid-users] Cache NTLM Authenticaion
In-Reply-To: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
References: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
Message-ID: <d2475cfe-e22d-402e-9d6a-5d6409cbd480@articatech.com>

Anyone can help me with this please.
I just want to know if it's possible or not, and if it's possible how to.

Best regards

On 27/10/2023 02:08, Andre Bolinhas wrote:
>
> Hi
>
> It's possible squid cache NTLM authentication from users?
>
> My goal is to store the credentials in cache in order to reduce the 
> request to Active Directory.
>
> I'm trying guide from this squid : auth_param configuration directive 
> (squid-cache.org) <http://www.squid-cache.org/Doc/config/auth_param/> 
> but there is no information relative to cache the authentication / 
> credentials.
>
> Also, in NTLM did you recommend to use the keep_alive option?
>
> Best regards
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/e6c32013/attachment.htm>

From koloschin at zmnh.uni-hamburg.de  Thu Nov  2 10:23:34 2023
From: koloschin at zmnh.uni-hamburg.de (Siegfried Koloschin)
Date: Thu, 2 Nov 2023 11:23:34 +0100
Subject: [squid-users] Bug in squid-6.4, msg25035.html
Message-ID: <64d353ee-e003-4267-984e-5dd5667962e1@zmnh.uni-hamburg.de>

just to confirm this bug: After upgrading to 6.4, squid also crashes 
under Redhat 8 and Debian 11, unfortunately only on the production 
systems with a lot of traffic; on the test system with little traffic, 
squid-6.4 runs without any problems.

2023/10/30 10:15:25 kid1| storeLateRelease: released 11990 objects
2023/10/30 10:15:26 kid1| FATAL: assertion failed: stmem.cc:98: 
"lowestOffset () <= target_offset"
 ??? current master transaction: master518

Sigi







-- 
Pflichtangaben gem?? Gesetz ?ber elektronische Handelsregister und Genossenschaftsregister sowie das Unternehmensregister (EHUG):

Universit?tsklinikum Hamburg-Eppendorf
K?rperschaft des ?ffentlichen Rechts
Gerichtsstand: Hamburg

Vorstandsmitglieder:
Prof. Dr. Christian Gerloff (Vorstandsvorsitzender)
Prof. Dr. Blanche Schwappach-Pignataro
Joachim Pr?l?
Marya Verdel


From admin at eduaicta.ro  Thu Nov  2 12:05:57 2023
From: admin at eduaicta.ro (Avram-Teodor Berindeie)
Date: Thu, 2 Nov 2023 14:05:57 +0200
Subject: [squid-users] Bug in squid-6.4, msg25035.html
Message-ID: <CAOsGNXz-hOafpJOX=-YMiM9XiZi-uFP5dG94iwwOsc3z1H_FHQ@mail.gmail.com>

Fixed here <https://bugs.squid-cache.org/show_bug.cgi?id=5309>.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/af2ae51e/attachment.htm>

From bpk678 at gmail.com  Thu Nov  2 17:46:41 2023
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 2 Nov 2023 13:46:41 -0400
Subject: [squid-users] log_db_daemon errors
Message-ID: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>

list members,

i am trying to log to a mariadb database, and cannot get the 
log_db_daemon script working.? i think i have everything setup, but an 
error is being thrown when i try to run the script manually.

/usr/lib64/squid/log_db_daemon /database:3306/squid/access_log/brendan/pass

Connecting... dsn='DBI:mysql:database=squid:database:3306', 
username='brendan', password='...' at /usr/lib64/squid/log_db_daemon 
line 399.
Use of uninitialized value $DBI::errstr in concatenation (.) or string 
at /usr/lib64/squid/log_db_daemon line 403.
Cannot connect to database:? at /usr/lib64/squid/log_db_daemon line 403.

i have no idea what is going wrong, but i cannot get any more detail 
about what is missing or malformed.? any ideas?

thanks,

brendan



From Robert.Zenz at bonsaimind.org  Thu Nov  2 18:10:59 2023
From: Robert.Zenz at bonsaimind.org (Robert 'Bobby' Zenz)
Date: Thu, 2 Nov 2023 19:10:59 +0100
Subject: [squid-users] log_db_daemon errors
In-Reply-To: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
Message-ID: <20231102191059.4449f97f@Dagon>

> Use of uninitialized value $DBI::errstr in concatenation (.) or
> string at /usr/lib64/squid/log_db_daemon line 403.

You're trying to use an uninitialized variable when outputting(?) the
error message. Fix that first. I'm guessing you're using the `errstr`
function wrong there, see the official documentation for hints:
https://metacpan.org/pod/DBD::MariaDB

> Cannot connect to database:? at /usr/lib64/squid/log_db_daemon line
> 403.

And then you should see what error you're actually getting here. My
guess is that it will be a permission issue. User not allowed to
connect from this host, or process not allowed to access the socket or
something similar.


From Robert.Zenz at bonsaimind.org  Thu Nov  2 18:14:30 2023
From: Robert.Zenz at bonsaimind.org (Robert 'Bobby' Zenz)
Date: Thu, 2 Nov 2023 19:14:30 +0100
Subject: [squid-users] log_db_daemon errors
In-Reply-To: <20231102191059.4449f97f@Dagon>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <20231102191059.4449f97f@Dagon>
Message-ID: <20231102191430.1ccbf84f@Dagon>

> > Use of uninitialized value $DBI::errstr in concatenation (.) or
> > string at /usr/lib64/squid/log_db_daemon line 403.  
> 
> You're trying to use an uninitialized variable when outputting(?) the
> error message. Fix that first. I'm guessing you're using the `errstr`
> function wrong there, see the official documentation for hints:
> https://metacpan.org/pod/DBD::MariaDB
> 
> > Cannot connect to database:? at /usr/lib64/squid/log_db_daemon line
> > 403.  
> 
> And then you should see what error you're actually getting here. My
> guess is that it will be a permission issue. User not allowed to
> connect from this host, or process not allowed to access the socket or
> something similar.

My apologies, I missed that that might not be a script you've written.
I guess it is a ready-made script?


From bpk678 at gmail.com  Thu Nov  2 18:41:40 2023
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 2 Nov 2023 14:41:40 -0400
Subject: [squid-users] log_db_daemon errors
In-Reply-To: <20231102191430.1ccbf84f@Dagon>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <20231102191059.4449f97f@Dagon> <20231102191430.1ccbf84f@Dagon>
Message-ID: <c6f7522c-ec47-4a68-9d62-f245777b8c73@gmail.com>

On 11/2/23 2:14 PM, Robert 'Bobby' Zenz wrote:
>>> Use of uninitialized value $DBI::errstr in concatenation (.) or
>>> string at /usr/lib64/squid/log_db_daemon line 403.
>> You're trying to use an uninitialized variable when outputting(?) the
>> error message. Fix that first. I'm guessing you're using the `errstr`
>> function wrong there, see the official documentation for hints:
>> https://metacpan.org/pod/DBD::MariaDB
>>
>>> Cannot connect to database:? at /usr/lib64/squid/log_db_daemon line
>>> 403.
>> And then you should see what error you're actually getting here. My
>> guess is that it will be a permission issue. User not allowed to
>> connect from this host, or process not allowed to access the socket or
>> something similar.
> My apologies, I missed that that might not be a script you've written.
> I guess it is a ready-made script?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

yes, this is the script packaged with squid from the fedora repos.? i 
will try to correct the script, which i believe may be victim to newer 
syntax in an updated perl version or something like that.? we'll see 
what comes of it...

thanks,

brendan



From gkinkie at gmail.com  Thu Nov  2 18:49:32 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Thu, 2 Nov 2023 18:49:32 +0000
Subject: [squid-users] log_db_daemon errors
In-Reply-To: <c6f7522c-ec47-4a68-9d62-f245777b8c73@gmail.com>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <20231102191059.4449f97f@Dagon> <20231102191430.1ccbf84f@Dagon>
 <c6f7522c-ec47-4a68-9d62-f245777b8c73@gmail.com>
Message-ID: <CA+Y8hcPKSg3-5zHVW+8bpn45x+gLYFXd=LHNoxbjfb0iSkDWpQ@mail.gmail.com>

Hi Robert,
  are you sure that you have the required packages on your system?
You'll need perl-DBD-MariaDB and what it depends on



On Thu, Nov 2, 2023 at 6:41?PM Brendan Kearney <bpk678 at gmail.com> wrote:

> On 11/2/23 2:14 PM, Robert 'Bobby' Zenz wrote:
> >>> Use of uninitialized value $DBI::errstr in concatenation (.) or
> >>> string at /usr/lib64/squid/log_db_daemon line 403.
> >> You're trying to use an uninitialized variable when outputting(?) the
> >> error message. Fix that first. I'm guessing you're using the `errstr`
> >> function wrong there, see the official documentation for hints:
> >> https://metacpan.org/pod/DBD::MariaDB
> >>
> >>> Cannot connect to database:  at /usr/lib64/squid/log_db_daemon line
> >>> 403.
> >> And then you should see what error you're actually getting here. My
> >> guess is that it will be a permission issue. User not allowed to
> >> connect from this host, or process not allowed to access the socket or
> >> something similar.
> > My apologies, I missed that that might not be a script you've written.
> > I guess it is a ready-made script?
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
>
> yes, this is the script packaged with squid from the fedora repos.  i
> will try to correct the script, which i believe may be victim to newer
> syntax in an updated perl version or something like that.  we'll see
> what comes of it...
>
> thanks,
>
> brendan
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/e9bc51c3/attachment.htm>

From bpk678 at gmail.com  Thu Nov  2 18:51:41 2023
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 2 Nov 2023 14:51:41 -0400
Subject: [squid-users] log_db_daemon errors
In-Reply-To: <CA+Y8hcPKSg3-5zHVW+8bpn45x+gLYFXd=LHNoxbjfb0iSkDWpQ@mail.gmail.com>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <20231102191059.4449f97f@Dagon> <20231102191430.1ccbf84f@Dagon>
 <c6f7522c-ec47-4a68-9d62-f245777b8c73@gmail.com>
 <CA+Y8hcPKSg3-5zHVW+8bpn45x+gLYFXd=LHNoxbjfb0iSkDWpQ@mail.gmail.com>
Message-ID: <8e4f2fd3-67f6-5658-1855-1cbc9dc7de6f@gmail.com>

On 11/2/23 2:49 PM, Francesco Chemolli wrote:
> Hi Robert,
> ? are you sure that you have the required packages on your system?
> You'll need perl-DBD-MariaDB and what it depends on
>
>
>
> On Thu, Nov 2, 2023 at 6:41?PM Brendan Kearney <bpk678 at gmail.com> wrote:
>
>     On 11/2/23 2:14 PM, Robert 'Bobby' Zenz wrote:
>     >>> Use of uninitialized value $DBI::errstr in concatenation (.) or
>     >>> string at /usr/lib64/squid/log_db_daemon line 403.
>     >> You're trying to use an uninitialized variable when
>     outputting(?) the
>     >> error message. Fix that first. I'm guessing you're using the
>     `errstr`
>     >> function wrong there, see the official documentation for hints:
>     >> https://metacpan.org/pod/DBD::MariaDB
>     >>
>     >>> Cannot connect to database:? at /usr/lib64/squid/log_db_daemon
>     line
>     >>> 403.
>     >> And then you should see what error you're actually getting here. My
>     >> guess is that it will be a permission issue. User not allowed to
>     >> connect from this host, or process not allowed to access the
>     socket or
>     >> something similar.
>     > My apologies, I missed that that might not be a script you've
>     written.
>     > I guess it is a ready-made script?
>     > _______________________________________________
>     > squid-users mailing list
>     > squid-users at lists.squid-cache.org
>     > https://lists.squid-cache.org/listinfo/squid-users
>
>     yes, this is the script packaged with squid from the fedora repos.? i
>     will try to correct the script, which i believe may be victim to
>     newer
>     syntax in an updated perl version or something like that. we'll see
>     what comes of it...
>
>     thanks,
>
>     brendan
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     https://lists.squid-cache.org/listinfo/squid-users
>
>
>
> -- 
> ? ? Francesco

got that...

[root at server3 bin]# rpm -qa |grep perl |grep -i maria

perl-DBD-MariaDB-1.22-4.fc38.x86_64
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/75158c98/attachment.htm>

From gkinkie at gmail.com  Thu Nov  2 18:58:43 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Thu, 2 Nov 2023 18:58:43 +0000
Subject: [squid-users] Cache NTLM Authenticaion
In-Reply-To: <d2475cfe-e22d-402e-9d6a-5d6409cbd480@articatech.com>
References: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
 <d2475cfe-e22d-402e-9d6a-5d6409cbd480@articatech.com>
Message-ID: <CA+Y8hcMU-Ni96TQ6hmwH5MmmJP2K5UhmbEpEG8o6qO+ia0NNiA@mail.gmail.com>

Hi Andre,
  in short: it's not possible in modern Windows environment, especially if
backed by Active Directory.


On Thu, Nov 2, 2023 at 12:09?AM Andre Bolinhas <
andre.bolinhas at articatech.com> wrote:

> Anyone can help me with this please.
> I just want to know if it's possible or not, and if it's possible how to.
>
> Best regards
> On 27/10/2023 02:08, Andre Bolinhas wrote:
>
> Hi
>
> It's possible squid cache NTLM authentication from users?
>
> My goal is to store the credentials in cache in order to reduce the
> request to Active Directory.
>
> I'm trying guide from this squid : auth_param configuration directive
> (squid-cache.org) <http://www.squid-cache.org/Doc/config/auth_param/> but
> there is no information relative to cache the authentication / credentials.
>
> Also, in NTLM did you recommend to use the keep_alive option?
>
> Best regards
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/ebb0507c/attachment.htm>

From bpk678 at gmail.com  Thu Nov  2 19:04:44 2023
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 2 Nov 2023 15:04:44 -0400
Subject: [squid-users] log_db_daemon errors
In-Reply-To: <8e4f2fd3-67f6-5658-1855-1cbc9dc7de6f@gmail.com>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <20231102191059.4449f97f@Dagon> <20231102191430.1ccbf84f@Dagon>
 <c6f7522c-ec47-4a68-9d62-f245777b8c73@gmail.com>
 <CA+Y8hcPKSg3-5zHVW+8bpn45x+gLYFXd=LHNoxbjfb0iSkDWpQ@mail.gmail.com>
 <8e4f2fd3-67f6-5658-1855-1cbc9dc7de6f@gmail.com>
Message-ID: <213dda60-8516-b561-60a4-0d64e8a42d25@gmail.com>

On 11/2/23 2:51 PM, Brendan Kearney wrote:
> On 11/2/23 2:49 PM, Francesco Chemolli wrote:
>> Hi Robert,
>> ? are you sure that you have the required packages on your system?
>> You'll need perl-DBD-MariaDB and what it depends on
>>
>>
>>
>> On Thu, Nov 2, 2023 at 6:41?PM Brendan Kearney <bpk678 at gmail.com> wrote:
>>
>>     On 11/2/23 2:14 PM, Robert 'Bobby' Zenz wrote:
>>     >>> Use of uninitialized value $DBI::errstr in concatenation (.) or
>>     >>> string at /usr/lib64/squid/log_db_daemon line 403.
>>     >> You're trying to use an uninitialized variable when
>>     outputting(?) the
>>     >> error message. Fix that first. I'm guessing you're using the
>>     `errstr`
>>     >> function wrong there, see the official documentation for hints:
>>     >> https://metacpan.org/pod/DBD::MariaDB
>>     >>
>>     >>> Cannot connect to database:? at
>>     /usr/lib64/squid/log_db_daemon line
>>     >>> 403.
>>     >> And then you should see what error you're actually getting
>>     here. My
>>     >> guess is that it will be a permission issue. User not allowed to
>>     >> connect from this host, or process not allowed to access the
>>     socket or
>>     >> something similar.
>>     > My apologies, I missed that that might not be a script you've
>>     written.
>>     > I guess it is a ready-made script?
>>     > _______________________________________________
>>     > squid-users mailing list
>>     > squid-users at lists.squid-cache.org
>>     > https://lists.squid-cache.org/listinfo/squid-users
>>
>>     yes, this is the script packaged with squid from the fedora
>>     repos.? i
>>     will try to correct the script, which i believe may be victim to
>>     newer
>>     syntax in an updated perl version or something like that. we'll see
>>     what comes of it...
>>
>>     thanks,
>>
>>     brendan
>>
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org
>>     https://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>> -- 
>> ? ? Francesco
>
> got that...
>
> [root at server3 bin]# rpm -qa |grep perl |grep -i maria
>
> perl-DBD-MariaDB-1.22-4.fc38.x86_64

original script:

# perform db connection
my $dsn = "DBI:mysql:database=$database" . ($host ne "localhost" ? 
":$host" : "");
my $dbh;
my $sth;
eval {
 ??? warn "Connecting... dsn='$dsn', username='$user', password='...'";
 ??? $dbh = DBI->connect($dsn, $user, $pass, { AutoCommit => 1, 
RaiseError => 1, PrintError => 1 });
 ??? };
if ($EVAL_ERROR) {
 ??? die "Cannot connect to database: $DBI::errstr";
}

hacked up, but seemingly working, mods:

# perform db connection
 ??? # my $dsn = "DBI:mysql:database=$database" . ($host ne "localhost" 
? ":$host" : "");
my $dsn = "DBI:MariaDB:database=$database;host=$host";
my $dbh;
my $sth;
eval {
 ??????? # warn "Connecting... dsn='$dsn', username='$user', 
password='...'";
 ??? $dbh = DBI->connect($dsn, $user, $pass, { AutoCommit => 1, 
RaiseError => 1, PrintError => 1 });
 ??? };
if ($EVAL_ERROR) {
 ??????? # die "Cannot connect to database: $DBI::errstr";
 ??????? die;
}

i am by far not a developer, so i cannot say what should be in the 
script.? brute forcing it got me to the mods shown above.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/db9b7dce/attachment.htm>

From jose.rodriguez at cenpalab.cu  Thu Nov  2 19:14:24 2023
From: jose.rodriguez at cenpalab.cu (jose.rodriguez at cenpalab.cu)
Date: Thu, 02 Nov 2023 15:14:24 -0400
Subject: [squid-users] [DMARC] log_db_daemon errors
In-Reply-To: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
Message-ID: <27febb30136503fd806be7015baa1f2f@cenpalab.cu>

On 2023-11-02 13:46, Brendan Kearney wrote:
> list members,
> 
> i am trying to log to a mariadb database, and cannot get the 
> log_db_daemon script working.? i think i have everything setup, but an 
> error is being thrown when i try to run the script manually.
> 
> /usr/lib64/squid/log_db_daemon 
> /database:3306/squid/access_log/brendan/pass
> 
> Connecting... dsn='DBI:mysql:database=squid:database:3306', 
> username='brendan', password='...' at /usr/lib64/squid/log_db_daemon 
> line 399.


(Replied without looking and it did not go to the list, but to the 
personal email, so will repeat it for completeness...)


That DSN seems wrong, as far as I can find it should look like this:

DBI:mysql:database=$database;host=$hostname;port=$port

Something is not being 'fed' right to the script?

Regards,
Joe1962



From andre.bolinhas at articatech.com  Thu Nov  2 23:11:36 2023
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Thu, 2 Nov 2023 23:11:36 +0000
Subject: [squid-users] Cache NTLM Authenticaion
In-Reply-To: <CA+Y8hcMU-Ni96TQ6hmwH5MmmJP2K5UhmbEpEG8o6qO+ia0NNiA@mail.gmail.com>
References: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
 <d2475cfe-e22d-402e-9d6a-5d6409cbd480@articatech.com>
 <CA+Y8hcMU-Ni96TQ6hmwH5MmmJP2K5UhmbEpEG8o6qO+ia0NNiA@mail.gmail.com>
Message-ID: <5c3389f5-66a1-4027-829f-e0990c122ab7@articatech.com>

Hi Francesco Chemolli

Many thanks for your reply.

In your opinion, keep alive should be used within NTLM, it increases 
performance?

Best regards

On 02/11/2023 18:58, Francesco Chemolli wrote:
> Hi Andre,
> ? in short: it's not possible in modern Windows environment, 
> especially if backed by Active Directory.
>
>
> On Thu, Nov 2, 2023 at 12:09?AM Andre Bolinhas 
> <andre.bolinhas at articatech.com> wrote:
>
>     Anyone can help me with this please.
>     I just want to know if it's possible or not, and if it's possible
>     how to.
>
>     Best regards
>
>     On 27/10/2023 02:08, Andre Bolinhas wrote:
>>
>>     Hi
>>
>>     It's possible squid cache NTLM authentication from users?
>>
>>     My goal is to store the credentials in cache in order to reduce
>>     the request to Active Directory.
>>
>>     I'm trying guide from this squid : auth_param configuration
>>     directive (squid-cache.org)
>>     <http://www.squid-cache.org/Doc/config/auth_param/> but there is
>>     no information relative to cache the authentication / credentials.
>>
>>     Also, in NTLM did you recommend to use the keep_alive option?
>>
>>     Best regards
>>
>>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     https://lists.squid-cache.org/listinfo/squid-users
>
>
>
> -- 
> ? ? Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/5beb2b8e/attachment.htm>

From gkinkie at gmail.com  Thu Nov  2 23:32:38 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Thu, 2 Nov 2023 23:32:38 +0000
Subject: [squid-users] Cache NTLM Authenticaion
In-Reply-To: <5c3389f5-66a1-4027-829f-e0990c122ab7@articatech.com>
References: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
 <d2475cfe-e22d-402e-9d6a-5d6409cbd480@articatech.com>
 <CA+Y8hcMU-Ni96TQ6hmwH5MmmJP2K5UhmbEpEG8o6qO+ia0NNiA@mail.gmail.com>
 <5c3389f5-66a1-4027-829f-e0990c122ab7@articatech.com>
Message-ID: <CA+Y8hcPVeoYLV_17Pb0gSpnSzsrwyP1GV-Xe300DC4kF1xzNdA@mail.gmail.com>

Hi Andre,
  More than that. Without keep-alive, NTLM over HTTP will not work at all.

@mobile


On Thu, 2 Nov 2023 at 23:11, Andre Bolinhas <andre.bolinhas at articatech.com>
wrote:

> Hi Francesco Chemolli
>
> Many thanks for your reply.
>
> In your opinion, keep alive should be used within NTLM, it increases
> performance?
>
> Best regards
> On 02/11/2023 18:58, Francesco Chemolli wrote:
>
> Hi Andre,
>   in short: it's not possible in modern Windows environment, especially if
> backed by Active Directory.
>
>
> On Thu, Nov 2, 2023 at 12:09?AM Andre Bolinhas <
> andre.bolinhas at articatech.com> wrote:
>
>> Anyone can help me with this please.
>> I just want to know if it's possible or not, and if it's possible how to.
>>
>> Best regards
>> On 27/10/2023 02:08, Andre Bolinhas wrote:
>>
>> Hi
>>
>> It's possible squid cache NTLM authentication from users?
>>
>> My goal is to store the credentials in cache in order to reduce the
>> request to Active Directory.
>>
>> I'm trying guide from this squid : auth_param configuration directive
>> (squid-cache.org) <http://www.squid-cache.org/Doc/config/auth_param/>
>> but there is no information relative to cache the authentication /
>> credentials.
>>
>> Also, in NTLM did you recommend to use the keep_alive option?
>>
>> Best regards
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> --
>     Francesco
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/bb2dceb6/attachment.htm>

From andre.bolinhas at articatech.com  Thu Nov  2 23:57:20 2023
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Thu, 2 Nov 2023 23:57:20 +0000
Subject: [squid-users] Cache NTLM Authenticaion
In-Reply-To: <CA+Y8hcPVeoYLV_17Pb0gSpnSzsrwyP1GV-Xe300DC4kF1xzNdA@mail.gmail.com>
References: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
 <d2475cfe-e22d-402e-9d6a-5d6409cbd480@articatech.com>
 <CA+Y8hcMU-Ni96TQ6hmwH5MmmJP2K5UhmbEpEG8o6qO+ia0NNiA@mail.gmail.com>
 <5c3389f5-66a1-4027-829f-e0990c122ab7@articatech.com>
 <CA+Y8hcPVeoYLV_17Pb0gSpnSzsrwyP1GV-Xe300DC4kF1xzNdA@mail.gmail.com>
Message-ID: <1749873f-2af2-45c1-93d7-2dbf3125520d@articatech.com>

Hi Francesco Chemolli

oohhh, strange, I always use keep_alived off and authentication/NTLM 
works perfectly.

auth_param ntlm program /usr/bin/ntlm_auth --domain=ARTICATECH2012.LAB 
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 20 startup=5 idle=1 concurrency=0 queue-size=80 
on-persistent-overload=ERR
auth_param ntlm keep_alive off

I just want to know if keep_alive on increase performance of 
squid/authentication and if not create issues on POST/PUT requests.

Best regards

On 02/11/2023 23:32, Francesco Chemolli wrote:
> Hi Andre,
> ? More than that. Without keep-alive, NTLM over HTTP will not work at all.
>
> @mobile
>
>
> On Thu, 2 Nov 2023 at 23:11, Andre Bolinhas 
> <andre.bolinhas at articatech.com> wrote:
>
>     Hi Francesco Chemolli
>
>     Many thanks for your reply.
>
>     In your opinion, keep alive should be used within NTLM, it
>     increases performance?
>
>     Best regards
>
>     On 02/11/2023 18:58, Francesco Chemolli wrote:
>>     Hi Andre,
>>     ? in short: it's not possible in modern Windows environment,
>>     especially if backed by Active Directory.
>>
>>
>>     On Thu, Nov 2, 2023 at 12:09?AM Andre Bolinhas
>>     <andre.bolinhas at articatech.com> wrote:
>>
>>         Anyone can help me with this please.
>>         I just want to know if it's possible or not, and if it's
>>         possible how to.
>>
>>         Best regards
>>
>>         On 27/10/2023 02:08, Andre Bolinhas wrote:
>>>
>>>         Hi
>>>
>>>         It's possible squid cache NTLM authentication from users?
>>>
>>>         My goal is to store the credentials in cache in order to
>>>         reduce the request to Active Directory.
>>>
>>>         I'm trying guide from this squid : auth_param configuration
>>>         directive (squid-cache.org)
>>>         <http://www.squid-cache.org/Doc/config/auth_param/> but
>>>         there is no information relative to cache the authentication
>>>         / credentials.
>>>
>>>         Also, in NTLM did you recommend to use the keep_alive option?
>>>
>>>         Best regards
>>>
>>>
>>         _______________________________________________
>>         squid-users mailing list
>>         squid-users at lists.squid-cache.org
>>         https://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
>>     -- 
>>     ? ? Francesco
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231102/e46373c3/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov  3 12:27:17 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Nov 2023 01:27:17 +1300
Subject: [squid-users] [DMARC] log_db_daemon errors
In-Reply-To: <27febb30136503fd806be7015baa1f2f@cenpalab.cu>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <27febb30136503fd806be7015baa1f2f@cenpalab.cu>
Message-ID: <7a3a648f-b32d-4f66-9517-97b4599d1e4e@treenet.co.nz>

On 3/11/23 08:14, jose.rodriguez wrote:
> On 2023-11-02 13:46, Brendan Kearney wrote:
>> list members,
>>
>> i am trying to log to a mariadb database, and cannot get the 
>> log_db_daemon script working.? i think i have everything setup, but an 
>> error is being thrown when i try to run the script manually.
>>
>> /usr/lib64/squid/log_db_daemon 
>> /database:3306/squid/access_log/brendan/pass
>>
>> Connecting... dsn='DBI:mysql:database=squid:database:3306', 
>> username='brendan', password='...' at /usr/lib64/squid/log_db_daemon 
>> line 399.
> 
> 
> (Replied without looking and it did not go to the list, but to the 
> personal email, so will repeat it for completeness...)
> 
> 
> That DSN seems wrong, as far as I can find it should look like this:
> 
> DBI:mysql:database=$database;host=$hostname;port=$port
> 
> Something is not being 'fed' right to the script?
> 

Thank you for the catch. I have now opened this to fix it:
<https://github.com/squid-cache/squid/pull/1570>


Cheers
Amos


From bpk678 at gmail.com  Fri Nov  3 16:40:55 2023
From: bpk678 at gmail.com (Brendan Kearney)
Date: Fri, 3 Nov 2023 12:40:55 -0400
Subject: [squid-users] [DMARC] log_db_daemon errors
In-Reply-To: <7a3a648f-b32d-4f66-9517-97b4599d1e4e@treenet.co.nz>
References: <9a5b00b4-af3c-46e5-70ba-deb6d8f6f8cb@gmail.com>
 <27febb30136503fd806be7015baa1f2f@cenpalab.cu>
 <7a3a648f-b32d-4f66-9517-97b4599d1e4e@treenet.co.nz>
Message-ID: <ffd0d8a9-0878-d5ab-212a-1567d9929b08@gmail.com>

On 11/3/23 8:27 AM, Amos Jeffries wrote:
> On 3/11/23 08:14, jose.rodriguez wrote:
>> On 2023-11-02 13:46, Brendan Kearney wrote:
>>> list members,
>>>
>>> i am trying to log to a mariadb database, and cannot get the 
>>> log_db_daemon script working.? i think i have everything setup, but 
>>> an error is being thrown when i try to run the script manually.
>>>
>>> /usr/lib64/squid/log_db_daemon 
>>> /database:3306/squid/access_log/brendan/pass
>>>
>>> Connecting... dsn='DBI:mysql:database=squid:database:3306', 
>>> username='brendan', password='...' at /usr/lib64/squid/log_db_daemon 
>>> line 399.
>>
>>
>> (Replied without looking and it did not go to the list, but to the 
>> personal email, so will repeat it for completeness...)
>>
>>
>> That DSN seems wrong, as far as I can find it should look like this:
>>
>> DBI:mysql:database=$database;host=$hostname;port=$port
>>
>> Something is not being 'fed' right to the script?
>>
>
> Thank you for the catch. I have now opened this to fix it:
> <https://github.com/squid-cache/squid/pull/1570>
>
>
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

in reading the description in the man page/perl script, i find that the 
only supported log format is the native squid format.? i have a custom 
log format that i use to log via syslog, and wonder what limitations 
exist in trying to expand the capability of the log_db_daemon.? i have 
the custom log format, and corresponding table structure for it.? is the 
effort involved more than just adding columns to the table, then 
updating the @db_fields and insert routine?

thanks,

brendan



From dbq343 at gmx.de  Sat Nov  4 07:53:44 2023
From: dbq343 at gmx.de (Stefan Meurer)
Date: Sat, 4 Nov 2023 08:53:44 +0100
Subject: [squid-users] access.log - POST requests
Message-ID: <df1777bc-bf57-4a7b-87c2-a922229995b6@gmx.de>

Hello,

is there a way to remove out all POST requests from access.log file?


Best regards

Steve


From squid3 at treenet.co.nz  Sat Nov  4 10:11:35 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Nov 2023 23:11:35 +1300
Subject: [squid-users] access.log - POST requests
In-Reply-To: <df1777bc-bf57-4a7b-87c2-a922229995b6@gmx.de>
References: <df1777bc-bf57-4a7b-87c2-a922229995b6@gmx.de>
Message-ID: <f4629ee1-82be-4863-b272-cceb19ecbb51@treenet.co.nz>

On 4/11/23 20:53, Stefan Meurer wrote:
> Hello,
> 
> is there a way to remove out all POST requests from access.log file?
> 

   acl POST method POST
   access_log stdio:/var/log/squid/access.log format=squid !POST

Cheers
Amos


From chrmet at web.de  Mon Nov  6 10:45:02 2023
From: chrmet at web.de (Christian Metzger)
Date: Mon, 6 Nov 2023 11:45:02 +0100
Subject: [squid-users] Squid: blocking all requests to plain ip addresses
References: <trinity-be72cf63-b3f4-4ac9-8344-024fefe428f9-1699263708853@3c-app-webde-bap23>
Message-ID: <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>

Hello,
is the above feature available, if yes how to configure it?
This feature should be available in all modi of no-, white- and blacklisting.
This feature is important for security and it's available in big commercial proxies.
Best regards, Chris
?


From gkinkie at gmail.com  Mon Nov  6 11:35:33 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 6 Nov 2023 11:35:33 +0000
Subject: [squid-users] Squid: blocking all requests to plain ip addresses
In-Reply-To: <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>
References: <trinity-be72cf63-b3f4-4ac9-8344-024fefe428f9-1699263708853@3c-app-webde-bap23>
 <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>
Message-ID: <CA+Y8hcNtDa+eNqEhJhfnm_K4kecffxjGOg68-gwPM4t8NG_rMw@mail.gmail.com>

Hi Christian,
  What you're aiming to do should be easily doable via an url_regex ACL

On Mon, Nov 6, 2023 at 10:45?AM Christian Metzger <chrmet at web.de> wrote:

> Hello,
> is the above feature available, if yes how to configure it?
> This feature should be available in all modi of no-, white- and
> blacklisting.
> This feature is important for security and it's available in big
> commercial proxies.
> Best regards, Chris
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231106/800da632/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Nov  6 11:40:12 2023
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 6 Nov 2023 12:40:12 +0100
Subject: [squid-users] Squid: blocking all requests to plain ip addresses
In-Reply-To: <CA+Y8hcNtDa+eNqEhJhfnm_K4kecffxjGOg68-gwPM4t8NG_rMw@mail.gmail.com>
References: <trinity-be72cf63-b3f4-4ac9-8344-024fefe428f9-1699263708853@3c-app-webde-bap23>
 <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>
 <CA+Y8hcNtDa+eNqEhJhfnm_K4kecffxjGOg68-gwPM4t8NG_rMw@mail.gmail.com>
Message-ID: <202311061240.12680.Antony.Stone@squid.open.source.it>

On Monday 06 November 2023 at 12:35:33, Francesco Chemolli wrote:

> Hi Christian,
>   What you're aiming to do should be easily doable via an url_regex ACL

https://wiki.squid-cache.org/ConfigExamples/Chat/Skype contains an example of a 
regex to match IP addresses which may also point you in a helpful direction.


Antony.

> On Mon, Nov 6, 2023 at 10:45?AM Christian Metzger wrote:
> > Hello,
> > is the above feature available, if yes how to configure it?
> > This feature should be available in all modi of no-, white- and
> > blacklisting.
> > This feature is important for security and it's available in big
> > commercial proxies.
> > Best regards, Chris

-- 
Is it venison for dinner again?  Oh deer.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ngtech1ltd at gmail.com  Mon Nov  6 11:43:13 2023
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 6 Nov 2023 13:43:13 +0200
Subject: [squid-users] Squid: blocking all requests to plain ip addresses
In-Reply-To: <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>
References: <trinity-be72cf63-b3f4-4ac9-8344-024fefe428f9-1699263708853@3c-app-webde-bap23>
 <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>
Message-ID: <CABA8h=SmwKntrD3O4Okh=S101PJLbJMNJY0AwEWPjnVKH+q6HQ@mail.gmail.com>

Do you need to block access to all plain ip addresses or specific ones?
What if you will want to allow specific ones but deny all the others?

Eliezer

?????? ??? ??, 6 ????? 2023, 12:45, ??? Christian Metzger ?<chrmet at web.de>:

> Hello,
> is the above feature available, if yes how to configure it?
> This feature should be available in all modi of no-, white- and
> blacklisting.
> This feature is important for security and it's available in big
> commercial proxies.
> Best regards, Chris
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231106/48d0da87/attachment.htm>

From chrmet at web.de  Mon Nov  6 11:51:12 2023
From: chrmet at web.de (Christian Metzger)
Date: Mon, 6 Nov 2023 12:51:12 +0100
Subject: [squid-users] Squid: blocking all requests to plain ip addresses
In-Reply-To: <202311061240.12680.Antony.Stone@squid.open.source.it>
References: <trinity-be72cf63-b3f4-4ac9-8344-024fefe428f9-1699263708853@3c-app-webde-bap23>
 <trinity-7b82f4d2-0228-4a18-9f98-5b2b8ec43d04-1699267502793@3c-app-webde-bap23>
 <CA+Y8hcNtDa+eNqEhJhfnm_K4kecffxjGOg68-gwPM4t8NG_rMw@mail.gmail.com>
 <202311061240.12680.Antony.Stone@squid.open.source.it>
Message-ID: <trinity-f452392b-d30a-498e-be50-67935a486d73-1699271472662@3c-app-webde-bap23>

Hi Antony,
very much thanks for your quick help.
Yes "dstdom_regex" looks right.
Best regards, Chris 
?
?

Gesendet:?Montag, 06. November 2023 um 12:40 Uhr
Von:?"Antony Stone" <Antony.Stone at squid.open.source.it>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] Squid: blocking all requests to plain ip addresses
On Monday 06 November 2023 at 12:35:33, Francesco Chemolli wrote:

> Hi Christian,
> What you're aiming to do should be easily doable via an url_regex ACL

https://wiki.squid-cache.org/ConfigExamples/Chat/Skype contains an example of a
regex to match IP addresses which may also point you in a helpful direction.


Antony.

> On Mon, Nov 6, 2023 at 10:45?AM Christian Metzger wrote:
> > Hello,
> > is the above feature available, if yes how to configure it?
> > This feature should be available in all modi of no-, white- and
> > blacklisting.
> > This feature is important for security and it's available in big
> > commercial proxies.
> > Best regards, Chris

--
Is it venison for dinner again? Oh deer.

Please reply to the list;
please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users[https://lists.squid-cache.org/listinfo/squid-users]


From squid at borrill.org.uk  Fri Nov 10 10:46:09 2023
From: squid at borrill.org.uk (Stephen Borrill)
Date: Fri, 10 Nov 2023 10:46:09 +0000
Subject: [squid-users] 6.x gives frequent connection to peer failed -
 spurious?
Message-ID: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>

For reasons I won't go into, we are running two copies of squid. One 
(main squid) is client-facing and uses the other (peer squid) as the 
upstream cache_peer which is a non-caching fetcher.

Main squid is configured like this:

cache_peer 127.0.0.1 parent 8123 0 no-query no-digest no-netdb-exchange 
default name=127.0.0.1:8123
cache_peer_access 127.0.0.1:8123 allow all

Peer squid is configured like this:

unique_hostname webfilter.squidnc
pid_filename /var/run/squidnc.pid
http_port 127.0.0.1:8123
icp_port 0
snmp_port 0
no_cache deny all
cache_access_log none
cache_store_log none
cache_log /usr/local/squid/logs/nocache.log
cache_effective_user nobody
cache_effective_group wheel
logfile_rotate 0
http_access allow localhost
http_access deny all
cache_mgr nocache
hosts_file none
cache_mem 10 MB
cache_dir ufs /usr/local/squid/nocache 10 1 1 no-store
always_direct allow all

With 6.x (currently 6.5) there are very frequent (every 10 seconds or 
so) messages like:
2023/11/10 10:25:43 kid1| ERROR: Connection to 127.0.0.1:8123 failed
     current master transaction: master3692

With 4.x there were no such messages.

By comparing to the peer squid logs, these seems to tally with DNS failures:
peer_select.cc(479) resolveSelected: PeerSelector1688 found all 0 
destinations for bugzilla.tucasi.com:443

Full ALL,2 log at the time of the reported connection failure:

2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(214) doAccept: New 
connection on FD 17
2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(316) acceptNext: 
connection on conn3 local=127.0.0.1:8123 remote=[::] FD 17 flags=9
2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1332) 
parseHttpRequest: HTTP Client conn13206 local=127.0.0.1:8123 
remote=127.0.0.1:57843 FD 147 flags=1
2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1336) 
parseHttpRequest: HTTP Client REQUEST:
2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
ALLOWED; last ACL checked: localhost
2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(683) 
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
ALLOWED; last ACL checked: localhost
2023/11/10 10:25:43.162 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: bugzilla.tucasi.com:443' via bugzilla.tucasi.com
2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector1526 found all 0 destinations for bugzilla.tucasi.com:443
2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(480) resolveSelected: 
   always_direct = ALLOWED
2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(481) resolveSelected: 
    never_direct = DENIED
2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(482) resolveSelected: 
        timedout = 0
2023/11/10 10:25:43.163 kid1| 4,2| errorpage.cc(1397) buildBody: No 
existing error page language negotiated for ERR_DNS_FAIL. Using default 
error file.
2023/11/10 10:25:43.163 kid1| 33,2| client_side.cc(617) swanSong: 
conn13206 local=127.0.0.1:8123 remote=127.0.0.1:57843 flags=1

If my analysis is correct why is this logged as a connection failure and 
do I need to worry about it beyond it filing up the logs needlessly?

My concern is that this could lead to the parent being incorrectly 
declared DEAD thus impacting other traffic:

2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
     current master transaction: master4581234

-- 
Stephen


From rafael.akchurin at diladele.com  Sun Nov 12 07:34:17 2023
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sun, 12 Nov 2023 07:34:17 +0000
Subject: [squid-users] Ubuntu 22.04 LTS repository for Squid 6.5 (rebuilt
 from sources in Debian unstable)
Message-ID: <AM8PR04MB7745FB34C8B45BB03F0DA12F8FACA@AM8PR04MB7745.eurprd04.prod.outlook.com>

Hello everyone,

Online repository with latest Squid 6.5 (rebuilt from sources in Debian unstable) for Ubuntu 22.04 LTS 64-bit is available at https://squid65.diladele.com/.
Github repo https://github.com/diladele/squid-ubuntu/tree/master/src/ubuntu22 contains all the scripts we used to make this compilation.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add new repo
echo "deb https://squid65.diladele.com/ubuntu/ jammy main" \
    > /etc/apt/sources.list.d/squid65.diladele.com.list

# and install
apt-get update && apt-get install -y \
    squid-common \
    squid-openssl \
    squidclient \
    libecap3 libecap3-dev

Hope you will find this useful.

Best regards,
Rafael Akchurin
Diladele B.V.

P.S. This version of Squid will now be part of Web Safety 9.0 coming out in early 2024. Please, if you have time and interested in Admin UI for Squid and ICAP web filtering, consider using the latest development version at https://www.diladele.com/download.html - the preconfigured appliance can be easily deployed in VMware ESXi/vSphere, Microsoft Hyper-V, Microsoft Azure and Amazon AWS.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231112/7d843ff5/attachment.htm>

From service.mv at gmail.com  Sun Nov 12 14:00:20 2023
From: service.mv at gmail.com (Service MV)
Date: Sun, 12 Nov 2023 11:00:20 -0300
Subject: [squid-users] Ubuntu 22.04 LTS repository for Squid 6.5
 (rebuilt from sources in Debian unstable)
In-Reply-To: <AM8PR04MB7745FB34C8B45BB03F0DA12F8FACA@AM8PR04MB7745.eurprd04.prod.outlook.com>
References: <AM8PR04MB7745FB34C8B45BB03F0DA12F8FACA@AM8PR04MB7745.eurprd04.prod.outlook.com>
Message-ID: <CA+d==oFFKNE=oeWHKsvJSys_hvu2ZRVE4GW7mmm4WNFr4CidhQ@mail.gmail.com>

Excelent news.

Thank you very much

El dom, 12 de nov de 2023, 04:34, Rafael Akchurin <
rafael.akchurin at diladele.com> escribi?:

> Hello everyone,
>
>
>
> Online repository with latest Squid 6.5 (rebuilt from sources in Debian
> unstable) for Ubuntu 22.04 LTS 64-bit is available at
> https://squid65.diladele.com/.
>
> Github repo
> https://github.com/diladele/squid-ubuntu/tree/master/src/ubuntu22
> contains all the scripts we used to make this compilation.
>
>
>
> Here are simple instructions how to use the repo. For more information see
> readme at https://github.com/diladele/squid-ubuntu .
>
>
>
> # add diladele apt key
>
> wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key
> add -
>
>
>
> # add new repo
>
> echo "deb https://squid65.diladele.com/ubuntu/ jammy main" \
>
>     > /etc/apt/sources.list.d/squid65.diladele.com.list
>
>
>
> # and install
>
> apt-get update && apt-get install -y \
>
>     squid-common \
>
>     squid-openssl \
>
>     squidclient \
>
>     libecap3 libecap3-dev
>
>
>
> Hope you will find this useful.
>
>
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
>
>
> P.S. This version of Squid will now be part of Web Safety 9.0 coming out
> in early 2024. Please, if you have time and interested in Admin UI for
> Squid and ICAP web filtering, consider using the latest development version
> at https://www.diladele.com/download.html - the preconfigured appliance
> can be easily deployed in VMware ESXi/vSphere, Microsoft Hyper-V, Microsoft
> Azure and Amazon AWS.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231112/da8c6a7f/attachment.htm>

From saieshwar.ak03 at gmail.com  Sun Nov 12 20:35:16 2023
From: saieshwar.ak03 at gmail.com (Sai Eshwar)
Date: Sun, 12 Nov 2023 15:35:16 -0500
Subject: [squid-users] mime.conf path
Message-ID: <CAHk47hYn_6Y1z+b0os=UQhBoM0HP6Viu+zT5Q+0k_DZOhY=eEg@mail.gmail.com>

Hello, I am trying to install squid on CentOS without root privilege
following the information present at
https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen

after running squid, I get the following error:
FATAL: Unable to open configuration file: /etc/squid/squid.conf: (2) No
such file or directory,
which is resolved by running squid with -f ~/centos/etc/squid/squid.conf,
but now I get the following error:
FATAL: MIME Config Table /etc/squid/mime.conf: (2) No such file or directory
How can I specify mime.conf path to squid?

Thanks,
Sai Eshwar.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231112/adbcf4fe/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 13 04:01:00 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Nov 2023 17:01:00 +1300
Subject: [squid-users] mime.conf path
In-Reply-To: <CAHk47hYn_6Y1z+b0os=UQhBoM0HP6Viu+zT5Q+0k_DZOhY=eEg@mail.gmail.com>
References: <CAHk47hYn_6Y1z+b0os=UQhBoM0HP6Viu+zT5Q+0k_DZOhY=eEg@mail.gmail.com>
Message-ID: <245e27d3-c20a-444d-ac0d-9ff2af24c7e0@treenet.co.nz>

On 13/11/23 09:35, Sai Eshwar wrote:
> Hello, I am trying to install squid on CentOS without root privilege 
> following the information present at 
> https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen <https://stackoverflow.com/questions/36651091/how-to-install-packages-in-linux-centos-without-root-user-with-automatic-depen>
> 
> after running squid, I get the following error:
> FATAL: Unable to open configuration file: /etc/squid/squid.conf: (2) No 
> such file or directory,
> which is resolved by running squid with -f 
> ~/centos/etc/squid/squid.conf, but now I get the following error:
> FATAL: MIME Config Table /etc/squid/mime.conf: (2) No such file or directory
> How can I specify mime.conf path to squid?
> 

Add to your squid.conf:

   mime_table ~/centos/etc/squid/mime.conf


There are likely other paths you need to change if you are going to 
continue and try to run it like this. Squid is designed to be run as 
root and auto-chroot itself into a lower user account - default 
"nobody", or whatever the "./configure --with-default-user=" option was 
set to.

If/when Squid produces more of these messages you can usually search in 
this list to find the directives related 
<http://www.squid-cache.org/Doc/config/>


PS. You may find creating a chroot for Squid to run inside to be easier 
that explicitly configuring every path.

HTH
Amos


From rousskov at measurement-factory.com  Wed Nov 15 21:55:54 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 15 Nov 2023 16:55:54 -0500
Subject: [squid-users] 6.x gives frequent connection to peer failed -
 spurious?
In-Reply-To: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>
References: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>
Message-ID: <fc40c8a1-e988-40c9-82db-bd70426f6c99@measurement-factory.com>

On 2023-11-10 05:46, Stephen Borrill wrote:

> With 6.x (currently 6.5) there are very frequent (every 10 seconds or 
> so) messages like:
> 2023/11/10 10:25:43 kid1| ERROR: Connection to 127.0.0.1:8123 failed


 > why is this logged as a connection failure

The current error wording is too assuming and, in your case, evidently 
misleading. The phrase "Connection to X failed" should be changed to 
something more general like "Cannot contact cache_peer X" or "Cannot 
communicate with cache_peer X".

CachePeer::countFailure() patches welcome.


 > do I need to worry about it beyond it filing up the logs needlessly?

In short, "yes".

I cannot accurately assess your specific needs, but, in most 
environments, one should indeed worry that their cache_peer server names 
cannot be reliably resolved because failed resolution attempts waste 
Squid resources and increase transaction response time. Moreover, if 
these failures are frequent enough (relative to peer usage attempts), 
the affected cache_peer will be marked as DEAD (as you have mentioned):

 > 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123


HTH,

Alex.



> 
> With 4.x there were no such messages.
> 
> By comparing to the peer squid logs, these seems to tally with DNS 
> failures:
> peer_select.cc(479) resolveSelected: PeerSelector1688 found all 0 
> destinations for bugzilla.tucasi.com:443
> 
> Full ALL,2 log at the time of the reported connection failure:
> 
> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(214) doAccept: New 
> connection on FD 17
> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(316) acceptNext: 
> connection on conn3 local=127.0.0.1:8123 remote=[::] FD 17 flags=9
> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1332) 
> parseHttpRequest: HTTP Client conn13206 local=127.0.0.1:8123 
> remote=127.0.0.1:57843 FD 147 flags=1
> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1336) 
> parseHttpRequest: HTTP Client REQUEST:
> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
> ALLOWED; last ACL checked: localhost
> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(683) 
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
> ALLOWED; last ACL checked: localhost
> 2023/11/10 10:25:43.162 kid1| 44,2| peer_select.cc(460) resolveSelected: 
> Find IP destination for: bugzilla.tucasi.com:443' via bugzilla.tucasi.com
> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(479) resolveSelected: 
> PeerSelector1526 found all 0 destinations for bugzilla.tucasi.com:443
> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(480) resolveSelected: 
>  ? always_direct = ALLOWED
> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(481) resolveSelected: 
>  ?? never_direct = DENIED
> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(482) resolveSelected: 
>  ?????? timedout = 0
> 2023/11/10 10:25:43.163 kid1| 4,2| errorpage.cc(1397) buildBody: No 
> existing error page language negotiated for ERR_DNS_FAIL. Using default 
> error file.
> 2023/11/10 10:25:43.163 kid1| 33,2| client_side.cc(617) swanSong: 
> conn13206 local=127.0.0.1:8123 remote=127.0.0.1:57843 flags=1
> 
> If my analysis is correct why is this logged as a connection failure and 
> do I need to worry about it beyond it filing up the logs needlessly?
> 
> My concern is that this could lead to the parent being incorrectly 
> declared DEAD thus impacting other traffic:
> 
> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
>  ??? current master transaction: master4581234
> 



From ankor2023 at gmail.com  Thu Nov 16 12:48:48 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 16 Nov 2023 15:48:48 +0300
Subject: [squid-users] Kerberos pac ResourceGroups parsing
Message-ID: <CADJd0Y1CEG1DAWYV1f0tVAOkL4XfRER5XdeDa0Pg7Q3WrewpVw@mail.gmail.com>

Hello,

I found that negotiate_kerberos_auth helper does not see domain local AD
groups.
As it turned out, helper parses only GroupIds and ExtraSids pac-blocks,
while the information about domain local groups is placed in the
ResourceGropIds pac-block.
I have slightly patched the negotiate_kerberos_pac.cc to
implement ResourceGropIds-block parsing.
Maybe it will be useful for the community.
This patch can be included in future Squid-releases.

Kind regards,
   Ankor.

The patch for the
file src/auth/negotiate/kerberos/negotiate_kerberos_pac.cc below:

@@ -362,6 +362,123 @@
     return ad_groups;
 }

+
+char *
+get_resource_group_domain_sid(uint32_t ResourceGroupDomainSid){
+
+    if (ResourceGroupDomainSid!= 0) {
+        uint8_t rev;
+        uint64_t idauth;
+        char dli[256];
+        char *ag;
+        int l;
+
+        align(4);
+
+        uint32_t nauth = get4byt();
+
+        size_t length = 1+1+6+nauth*4;
+
+            ag=(char *)xcalloc((length+1)*sizeof(char),1);
+            // the first byte is a length of the SID
+            ag[0] = (char) length;
+            memcpy((void *)&ag[1],(const void*)&p[bpos],1);
+            memcpy((void *)&ag[2],(const void*)&p[bpos+1],1);
+            ag[2] = ag[2]+1;
+            memcpy((void *)&ag[3],(const void*)&p[bpos+2],6+nauth*4);
+
+
+
+        /* mainly for debug only */
+        rev = get1byt();
+        bpos = bpos + 1; /*nsub*/
+        idauth = get6byt_be();
+
+        snprintf(dli,sizeof(dli),"S-%d-%lu",rev,(long unsigned int)idauth);
+        for ( l=0; l<(int)nauth; l++ ) {
+            uint32_t sauth;
+            sauth = get4byt();
+            snprintf((char
*)&dli[strlen(dli)],sizeof(dli)-strlen(dli),"-%u",sauth);
+        }
+        debug((char *) "%s| %s: INFO: Got ResourceGroupDomainSid %s\n",
LogTime(), PROGRAM, dli);
+        return ag;
+    }
+
+    return NULL;
+}
+
+char *
+get_resource_groups(char *ad_groups, char *resource_group_domain_sid,
uint32_t ResourceGroupIds, uint32_t ResourceGroupCount){
+    size_t group_domain_sid_len = resource_group_domain_sid[0];
+    char *ag;
+    size_t length;
+
+    resource_group_domain_sid++; //now it points to the actual data
+
+
+    if (ResourceGroupIds!= 0) {
+        uint32_t ngroup;
+        int l;
+
+        align(4);
+        ngroup = get4byt();
+        if ( ngroup != ResourceGroupCount) {
+            debug((char *) "%s| %s: ERROR: Group encoding error =>
ResourceGroupCount: %d Array size: %d\n",
+                  LogTime(), PROGRAM, ResourceGroupCount, ngroup);
+            return NULL;
+        }
+        debug((char *) "%s| %s: INFO: Found %d Resource Group rids\n",
LogTime(), PROGRAM, ResourceGroupCount);
+
+        //make a group template which begins with the ResourceGroupDomainID
+        length = group_domain_sid_len+4;  //+4 for a rid
+        ag=(char *)xcalloc(length*sizeof(char),1);
+        memcpy((void *)ag,(const void*)resource_group_domain_sid,
group_domain_sid_len);
+
+
+        for ( l=0; l<(int)ResourceGroupCount; l++) {
+            uint32_t sauth;
+            memcpy((void *)&ag[group_domain_sid_len],(const
void*)&p[bpos],4);
+
+            if (!pstrcat(ad_groups," group=")) {
+                debug((char *) "%s| %s: WARN: Too many groups ! size > %d
: %s\n",
+                      LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE, ad_groups);
+               xfree(ag);
+               return NULL;
+            }
+
+
+            struct base64_encode_ctx ctx;
+            base64_encode_init(&ctx);
+            const uint32_t expectedSz = base64_encode_len(length) +1 /*
terminator */;
+            char *b64buf = static_cast<char *>(xcalloc(expectedSz, 1));
+            size_t blen = base64_encode_update(&ctx, b64buf, length,
reinterpret_cast<uint8_t*>(ag));
+            blen += base64_encode_final(&ctx, b64buf+blen);
+            b64buf[expectedSz-1] = '\0';
+            if (!pstrcat(ad_groups, reinterpret_cast<char*>(b64buf))) {
+                debug((char *) "%s| %s: WARN: Too many groups ! size > %d
: %s\n",
+                      LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE, ad_groups);
+               xfree(ag);
+               xfree(b64buf);
+               return NULL;
+            }
+            xfree(b64buf);
+
+
+
+            sauth = get4byt();
+            debug((char *) "%s| %s: Info: Got rid: %u\n", LogTime(),
PROGRAM, sauth);
+            /* attribute */
+            bpos = bpos+4;
+        }
+
+        xfree(ag);
+       return ad_groups;
+    }
+
+    return NULL;
+}
+
+
 char *
 get_ad_groups(char *ad_groups, krb5_context context, krb5_pac pac)
 {
@@ -379,14 +496,14 @@
     uint32_t LogonDomainId=0;
     uint32_t SidCount=0;
     uint32_t ExtraSids=0;
-    /*
     uint32_t ResourceGroupDomainSid=0;
     uint32_t ResourceGroupCount=0;
     uint32_t ResourceGroupIds=0;
-    */
     char **Rids=NULL;
     int l=0;

+    char * resource_group_domain_sid=NULL;
+
     if (!ad_groups) {
         debug((char *) "%s| %s: ERR: No space to store groups\n",
               LogTime(), PROGRAM);
@@ -454,11 +571,11 @@
     bpos = bpos+40;
     SidCount = get4byt();
     ExtraSids = get4byt();
-    /* 4 bytes ResourceGroupDomainSid
-     * 4 bytes ResourceGroupCount
-     * 4 bytes ResourceGroupIds
-     */
-    bpos = bpos+12;
+
+    ResourceGroupDomainSid = get4byt();
+    ResourceGroupCount = get4byt();
+    ResourceGroupIds = get4byt();
+
     /*
      * Read all data from structure => Now check pointers
      */
@@ -483,7 +600,15 @@
     if ((ad_groups = getextrasids(ad_groups,ExtraSids,SidCount))==NULL)
         goto k5clean;

+    resource_group_domain_sid =
get_resource_group_domain_sid(ResourceGroupDomainSid);
+    if(resource_group_domain_sid && ResourceGroupCount &&
ResourceGroupIds){
+        get_resource_groups(ad_groups, resource_group_domain_sid,
ResourceGroupIds, ResourceGroupCount);
+    }
+
     debug((char *) "%s| %s: INFO: Read %d of %d bytes \n", LogTime(),
PROGRAM, bpos, (int)ad_data->length);
+
+    if(resource_group_domain_sid) xfree(resource_group_domain_sid);
+
     if (Rids) {
         for ( l=0; l<(int)GroupCount; l++) {
             xfree(Rids[l]);
@@ -493,6 +618,8 @@
     krb5_free_data(context, ad_data);
     return ad_groups;
 k5clean:
+    if(resource_group_domain_sid) xfree(resource_group_domain_sid);
+
     if (Rids) {
         for ( l=0; l<(int)GroupCount; l++) {
             xfree(Rids[l]);
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231116/46994f54/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov 16 14:01:09 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Nov 2023 09:01:09 -0500
Subject: [squid-users] Kerberos pac ResourceGroups parsing
In-Reply-To: <CADJd0Y1CEG1DAWYV1f0tVAOkL4XfRER5XdeDa0Pg7Q3WrewpVw@mail.gmail.com>
References: <CADJd0Y1CEG1DAWYV1f0tVAOkL4XfRER5XdeDa0Pg7Q3WrewpVw@mail.gmail.com>
Message-ID: <04aa8846-c50e-4ff0-9ae3-dc78b8905f3c@measurement-factory.com>

On 2023-11-16 07:48, Andrey K wrote:

> I have slightly patched the?negotiate_kerberos_pac.cc to 
> implement?ResourceGropIds-block parsing.

Please consider posting tested changes as a GitHub Pull Request:
https://wiki.squid-cache.org/MergeProcedure#pull-request


Thank you,

Alex.


> Maybe it will be useful for the community.
> This patch can be included in future Squid-releases.
> 
> Kind regards,
>  ? ?Ankor.
> 
> The patch for the 
> file?src/auth/negotiate/kerberos/negotiate_kerberos_pac.cc below:
> 
> @@ -362,6 +362,123 @@
>  ? ? ?return ad_groups;
>  ?}
> 
> +
> +char *
> +get_resource_group_domain_sid(uint32_t ResourceGroupDomainSid){
> +
> + ? ?if (ResourceGroupDomainSid!= 0) {
> + ? ? ? ?uint8_t rev;
> + ? ? ? ?uint64_t idauth;
> + ? ? ? ?char dli[256];
> + ? ? ? ?char *ag;
> + ? ? ? ?int l;
> +
> + ? ? ? ?align(4);
> +
> + ? ? ? ?uint32_t nauth = get4byt();
> +
> + ? ? ? ?size_t length = 1+1+6+nauth*4;
> +
> + ? ? ? ? ? ?ag=(char *)xcalloc((length+1)*sizeof(char),1);
> + ? ? ? ? ? ?// the first byte is a length of the SID
> + ? ? ? ? ? ?ag[0] = (char) length;
> + ? ? ? ? ? ?memcpy((void *)&ag[1],(const void*)&p[bpos],1);
> + ? ? ? ? ? ?memcpy((void *)&ag[2],(const void*)&p[bpos+1],1);
> + ? ? ? ? ? ?ag[2] = ag[2]+1;
> + ? ? ? ? ? ?memcpy((void *)&ag[3],(const void*)&p[bpos+2],6+nauth*4);
> +
> +
> +
> + ? ? ? ?/* mainly for debug only */
> + ? ? ? ?rev = get1byt();
> + ? ? ? ?bpos = bpos + 1; /*nsub*/
> + ? ? ? ?idauth = get6byt_be();
> +
> + ? ? ? ?snprintf(dli,sizeof(dli),"S-%d-%lu",rev,(long unsigned int)idauth);
> + ? ? ? ?for ( l=0; l<(int)nauth; l++ ) {
> + ? ? ? ? ? ?uint32_t sauth;
> + ? ? ? ? ? ?sauth = get4byt();
> + ? ? ? ? ? ?snprintf((char 
> *)&dli[strlen(dli)],sizeof(dli)-strlen(dli),"-%u",sauth);
> + ? ? ? ?}
> + ? ? ? ?debug((char *) "%s| %s: INFO: Got ResourceGroupDomainSid %s\n", 
> LogTime(), PROGRAM, dli);
> + ? ? ? ?return ag;
> + ? ?}
> +
> + ? ?return NULL;
> +}
> +
> +char *
> +get_resource_groups(char *ad_groups, char *resource_group_domain_sid, 
> uint32_t ResourceGroupIds, uint32_t ResourceGroupCount){
> + ? ?size_t group_domain_sid_len = resource_group_domain_sid[0];
> + ? ?char *ag;
> + ? ?size_t length;
> +
> + ? ?resource_group_domain_sid++; //now it points to the actual data
> +
> +
> + ? ?if (ResourceGroupIds!= 0) {
> + ? ? ? ?uint32_t ngroup;
> + ? ? ? ?int l;
> +
> + ? ? ? ?align(4);
> + ? ? ? ?ngroup = get4byt();
> + ? ? ? ?if ( ngroup != ResourceGroupCount) {
> + ? ? ? ? ? ?debug((char *) "%s| %s: ERROR: Group encoding error => 
> ResourceGroupCount: %d Array size: %d\n",
> + ? ? ? ? ? ? ? ? ?LogTime(), PROGRAM, ResourceGroupCount, ngroup);
> + ? ? ? ? ? ?return NULL;
> + ? ? ? ?}
> + ? ? ? ?debug((char *) "%s| %s: INFO: Found %d Resource Group rids\n", 
> LogTime(), PROGRAM, ResourceGroupCount);
> +
> + ? ? ? ?//make a group template which begins with the ResourceGroupDomainID
> + ? ? ? ?length = group_domain_sid_len+4; ?//+4 for a rid
> + ? ? ? ?ag=(char *)xcalloc(length*sizeof(char),1);
> + ? ? ? ?memcpy((void *)ag,(const void*)resource_group_domain_sid, 
> group_domain_sid_len);
> +
> +
> + ? ? ? ?for ( l=0; l<(int)ResourceGroupCount; l++) {
> + ? ? ? ? ? ?uint32_t sauth;
> + ? ? ? ? ? ?memcpy((void *)&ag[group_domain_sid_len],(const 
> void*)&p[bpos],4);
> +
> + ? ? ? ? ? ?if (!pstrcat(ad_groups," group=")) {
> + ? ? ? ? ? ? ? ?debug((char *) "%s| %s: WARN: Too many groups ! size > 
> %d : %s\n",
> + ? ? ? ? ? ? ? ? ? ? ?LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE, ad_groups);
> + ? ? ? ? ? ? ? xfree(ag);
> + ? ? ? ? ? ? ? return NULL;
> + ? ? ? ? ? ?}
> +
> +
> + ? ? ? ? ? ?struct base64_encode_ctx ctx;
> + ? ? ? ? ? ?base64_encode_init(&ctx);
> + ? ? ? ? ? ?const uint32_t expectedSz = base64_encode_len(length) +1 /* 
> terminator */;
> + ? ? ? ? ? ?char *b64buf = static_cast<char *>(xcalloc(expectedSz, 1));
> + ? ? ? ? ? ?size_t blen = base64_encode_update(&ctx, b64buf, length, 
> reinterpret_cast<uint8_t*>(ag));
> + ? ? ? ? ? ?blen += base64_encode_final(&ctx, b64buf+blen);
> + ? ? ? ? ? ?b64buf[expectedSz-1] = '\0';
> + ? ? ? ? ? ?if (!pstrcat(ad_groups, reinterpret_cast<char*>(b64buf))) {
> + ? ? ? ? ? ? ? ?debug((char *) "%s| %s: WARN: Too many groups ! size > 
> %d : %s\n",
> + ? ? ? ? ? ? ? ? ? ? ?LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE, ad_groups);
> + ? ? ? ? ? ? ? xfree(ag);
> + ? ? ? ? ? ? ? xfree(b64buf);
> + ? ? ? ? ? ? ? return NULL;
> + ? ? ? ? ? ?}
> + ? ? ? ? ? ?xfree(b64buf);
> +
> +
> +
> + ? ? ? ? ? ?sauth = get4byt();
> + ? ? ? ? ? ?debug((char *) "%s| %s: Info: Got rid: %u\n", LogTime(), 
> PROGRAM, sauth);
> + ? ? ? ? ? ?/* attribute */
> + ? ? ? ? ? ?bpos = bpos+4;
> + ? ? ? ?}
> +
> + ? ? ? ?xfree(ag);
> + ? ? ? return ad_groups;
> + ? ?}
> +
> + ? ?return NULL;
> +}
> +
> +
>  ?char *
>  ?get_ad_groups(char *ad_groups, krb5_context context, krb5_pac pac)
>  ?{
> @@ -379,14 +496,14 @@
>  ? ? ?uint32_t LogonDomainId=0;
>  ? ? ?uint32_t SidCount=0;
>  ? ? ?uint32_t ExtraSids=0;
> - ? ?/*
>  ? ? ?uint32_t ResourceGroupDomainSid=0;
>  ? ? ?uint32_t ResourceGroupCount=0;
>  ? ? ?uint32_t ResourceGroupIds=0;
> - ? ?*/
>  ? ? ?char **Rids=NULL;
>  ? ? ?int l=0;
> 
> + ? ?char * resource_group_domain_sid=NULL;
> +
>  ? ? ?if (!ad_groups) {
>  ? ? ? ? ?debug((char *) "%s| %s: ERR: No space to store groups\n",
>  ? ? ? ? ? ? ? ?LogTime(), PROGRAM);
> @@ -454,11 +571,11 @@
>  ? ? ?bpos = bpos+40;
>  ? ? ?SidCount = get4byt();
>  ? ? ?ExtraSids = get4byt();
> - ? ?/* 4 bytes ResourceGroupDomainSid
> - ? ? * 4 bytes ResourceGroupCount
> - ? ? * 4 bytes ResourceGroupIds
> - ? ? */
> - ? ?bpos = bpos+12;
> +
> + ? ?ResourceGroupDomainSid = get4byt();
> + ? ?ResourceGroupCount = get4byt();
> + ? ?ResourceGroupIds = get4byt();
> +
>  ? ? ?/*
>  ? ? ? * Read all data from structure => Now check pointers
>  ? ? ? */
> @@ -483,7 +600,15 @@
>  ? ? ?if ((ad_groups = getextrasids(ad_groups,ExtraSids,SidCount))==NULL)
>  ? ? ? ? ?goto k5clean;
> 
> + ? ?resource_group_domain_sid = 
> get_resource_group_domain_sid(ResourceGroupDomainSid);
> + ? ?if(resource_group_domain_sid && ResourceGroupCount && 
> ResourceGroupIds){
> + ? ? ? ?get_resource_groups(ad_groups, resource_group_domain_sid, 
> ResourceGroupIds, ResourceGroupCount);
> + ? ?}
> +
>  ? ? ?debug((char *) "%s| %s: INFO: Read %d of %d bytes \n", LogTime(), 
> PROGRAM, bpos, (int)ad_data->length);
> +
> + ? ?if(resource_group_domain_sid) xfree(resource_group_domain_sid);
> +
>  ? ? ?if (Rids) {
>  ? ? ? ? ?for ( l=0; l<(int)GroupCount; l++) {
>  ? ? ? ? ? ? ?xfree(Rids[l]);
> @@ -493,6 +618,8 @@
>  ? ? ?krb5_free_data(context, ad_data);
>  ? ? ?return ad_groups;
>  ?k5clean:
> + ? ?if(resource_group_domain_sid) xfree(resource_group_domain_sid);
> +
>  ? ? ?if (Rids) {
>  ? ? ? ? ?for ( l=0; l<(int)GroupCount; l++) {
>  ? ? ? ? ? ? ?xfree(Rids[l]);
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From squid at borrill.org.uk  Tue Nov 21 13:38:06 2023
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 21 Nov 2023 13:38:06 +0000
Subject: [squid-users] 6.x gives frequent connection to peer failed -
 spurious?
In-Reply-To: <fc40c8a1-e988-40c9-82db-bd70426f6c99@measurement-factory.com>
References: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>
 <fc40c8a1-e988-40c9-82db-bd70426f6c99@measurement-factory.com>
Message-ID: <1fd61adc-2b97-4cdc-92a4-f8a0a7c575d2@borrill.org.uk>

On 15/11/2023 21:55, Alex Rousskov wrote:
> On 2023-11-10 05:46, Stephen Borrill wrote:
> 
>> With 6.x (currently 6.5) there are very frequent (every 10 seconds or 
>> so) messages like:
>> 2023/11/10 10:25:43 kid1| ERROR: Connection to 127.0.0.1:8123 failed
> 
> 
>> why is this logged as a connection failure
> 
> The current error wording is too assuming and, in your case, evidently 
> misleading. The phrase "Connection to X failed" should be changed to 
> something more general like "Cannot contact cache_peer X" or "Cannot 
> communicate with cache_peer X".
> 
> CachePeer::countFailure() patches welcome.

But the point is that it _can_ communicate with the peer, but the peer 
itself can't service the request. The peer returning 503 shouldn't be 
logged as a connection failure

>  > do I need to worry about it beyond it filing up the logs needlessly?
> 
> In short, "yes".
> 
> I cannot accurately assess your specific needs, but, in most 
> environments, one should indeed worry that their cache_peer server names 
> cannot be reliably resolved because failed resolution attempts waste 
> Squid resources and increase transaction response time. Moreover, if 
> these failures are frequent enough (relative to peer usage attempts), 
> the affected cache_peer will be marked as DEAD (as you have mentioned):
> 
>  > 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123

Problem seems to be easily reproducible:

1# env https_proxy=http://127.0.0.1:8084 curl https://www.invalid.domain/
curl: (56) CONNECT tunnel failed, response 503
2# grep invalid /usr/local/squid/logs/access.log|tail -1
1700573429.015      4 127.0.0.1:8084 TCP_TUNNEL/503 0 CONNECT 
www.invalid.domain:443 - FIRSTUP_PARENT/127.0.0.1:8123 -
3# date -r 1700573429 '+%Y/%m/%d %H:%M:%S'
2023/11/21 13:30:29
4# grep '2023/11/21 13:30:29' /usr/local/squid/logs/cache.log
2023/11/21 13:30:29 kid1| ERROR: Connection to 127.0.0.1:8123 failed

>> With 4.x there were no such messages.
>>
>> By comparing to the peer squid logs, these seems to tally with DNS 
>> failures:
>> peer_select.cc(479) resolveSelected: PeerSelector1688 found all 0 
>> destinations for bugzilla.tucasi.com:443
>>
>> Full ALL,2 log at the time of the reported connection failure:
>>
>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(214) doAccept: New 
>> connection on FD 17
>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(316) acceptNext: 
>> connection on conn3 local=127.0.0.1:8123 remote=[::] FD 17 flags=9
>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1332) 
>> parseHttpRequest: HTTP Client conn13206 local=127.0.0.1:8123 
>> remote=127.0.0.1:57843 FD 147 flags=1
>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1336) 
>> parseHttpRequest: HTTP Client REQUEST:
>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
>> ALLOWED; last ACL checked: localhost
>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(683) 
>> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
>> ALLOWED; last ACL checked: localhost
>> 2023/11/10 10:25:43.162 kid1| 44,2| peer_select.cc(460) 
>> resolveSelected: Find IP destination for: bugzilla.tucasi.com:443' via 
>> bugzilla.tucasi.com
>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(479) 
>> resolveSelected: PeerSelector1526 found all 0 destinations for 
>> bugzilla.tucasi.com:443
>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(480) 
>> resolveSelected: ?? always_direct = ALLOWED
>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(481) 
>> resolveSelected: ??? never_direct = DENIED
>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(482) 
>> resolveSelected: ??????? timedout = 0
>> 2023/11/10 10:25:43.163 kid1| 4,2| errorpage.cc(1397) buildBody: No 
>> existing error page language negotiated for ERR_DNS_FAIL. Using 
>> default error file.
>> 2023/11/10 10:25:43.163 kid1| 33,2| client_side.cc(617) swanSong: 
>> conn13206 local=127.0.0.1:8123 remote=127.0.0.1:57843 flags=1
>>
>> If my analysis is correct why is this logged as a connection failure 
>> and do I need to worry about it beyond it filing up the logs needlessly?
>>
>> My concern is that this could lead to the parent being incorrectly 
>> declared DEAD thus impacting other traffic:
>>
>> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
>> ???? current master transaction: master4581234
>>
> 



From rousskov at measurement-factory.com  Tue Nov 21 15:55:13 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Nov 2023 10:55:13 -0500
Subject: [squid-users] 6.x gives frequent connection to peer failed -
 spurious?
In-Reply-To: <1fd61adc-2b97-4cdc-92a4-f8a0a7c575d2@borrill.org.uk>
References: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>
 <fc40c8a1-e988-40c9-82db-bd70426f6c99@measurement-factory.com>
 <1fd61adc-2b97-4cdc-92a4-f8a0a7c575d2@borrill.org.uk>
Message-ID: <802c46b3-69a3-40b2-9ccd-f1bedb57961a@measurement-factory.com>

On 2023-11-21 08:38, Stephen Borrill wrote:
> On 15/11/2023 21:55, Alex Rousskov wrote:
>> On 2023-11-10 05:46, Stephen Borrill wrote:
>>
>>> With 6.x (currently 6.5) there are very frequent (every 10 seconds or 
>>> so) messages like:
>>> 2023/11/10 10:25:43 kid1| ERROR: Connection to 127.0.0.1:8123 failed
>>
>>
>>> why is this logged as a connection failure
>>
>> The current error wording is too assuming and, in your case, evidently 
>> misleading. The phrase "Connection to X failed" should be changed to 
>> something more general like "Cannot contact cache_peer X" or "Cannot 
>> communicate with cache_peer X".
>>
>> CachePeer::countFailure() patches welcome.

> But the point is that it _can_ communicate with the peer, but the peer 
> itself can't service the request. The peer returning 503 shouldn't be 
> logged as a connection failure


My bad. I missed the fact that the described DNS error happens at a 
_peer_ Squid. Sorry.


Currently, Squid v6 treats most CONNECT-to-peer errors as a sign of a 
broken peer. In 2022, 4xx errors were excluded from that set[1]. At that 
time, we also proposed to make that decision configurable using a new 
cache_peer_fault directive[2], but the new directive was blocked as an 
"overkill"[3], so we hard-coded 4xx exclusion instead.

Going forward, you have several options, including these two:

1. Convince others that Squid should treat all 503 CONNECT errors from 
peers as it already treats all 4xx errors. Hard-code that new logic.

2. Convince others that cache_peer_fault or a similar directive is a 
good idea rather than an overkill. Resurrect its implementation[2].


[1] 
https://github.com/squid-cache/squid/commit/022dbabd89249f839d1861aa87c1ab9e1a008a47

[2] 
https://github.com/squid-cache/squid/commit/25431f18f2f5e796b8704c85fc51f93b6cc2a73d

[3] https://github.com/squid-cache/squid/pull/1166#issuecomment-1295806530


HTH,

Alex.


>> ?> do I need to worry about it beyond it filing up the logs needlessly?
>>
>> In short, "yes".
>>
>> I cannot accurately assess your specific needs, but, in most 
>> environments, one should indeed worry that their cache_peer server 
>> names cannot be reliably resolved because failed resolution attempts 
>> waste Squid resources and increase transaction response time. 
>> Moreover, if these failures are frequent enough (relative to peer 
>> usage attempts), the affected cache_peer will be marked as DEAD (as 
>> you have mentioned):
>>
>> ?> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
> 
> Problem seems to be easily reproducible:
> 
> 1# env https_proxy=http://127.0.0.1:8084 curl https://www.invalid.domain/
> curl: (56) CONNECT tunnel failed, response 503
> 2# grep invalid /usr/local/squid/logs/access.log|tail -1
> 1700573429.015????? 4 127.0.0.1:8084 TCP_TUNNEL/503 0 CONNECT 
> www.invalid.domain:443 - FIRSTUP_PARENT/127.0.0.1:8123 -
> 3# date -r 1700573429 '+%Y/%m/%d %H:%M:%S'
> 2023/11/21 13:30:29
> 4# grep '2023/11/21 13:30:29' /usr/local/squid/logs/cache.log
> 2023/11/21 13:30:29 kid1| ERROR: Connection to 127.0.0.1:8123 failed
> 
>>> With 4.x there were no such messages.
>>>
>>> By comparing to the peer squid logs, these seems to tally with DNS 
>>> failures:
>>> peer_select.cc(479) resolveSelected: PeerSelector1688 found all 0 
>>> destinations for bugzilla.tucasi.com:443
>>>
>>> Full ALL,2 log at the time of the reported connection failure:
>>>
>>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(214) doAccept: New 
>>> connection on FD 17
>>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(316) acceptNext: 
>>> connection on conn3 local=127.0.0.1:8123 remote=[::] FD 17 flags=9
>>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1332) 
>>> parseHttpRequest: HTTP Client conn13206 local=127.0.0.1:8123 
>>> remote=127.0.0.1:57843 FD 147 flags=1
>>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1336) 
>>> parseHttpRequest: HTTP Client REQUEST:
>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
>>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
>>> ALLOWED; last ACL checked: localhost
>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(683) 
>>> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
>>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 is 
>>> ALLOWED; last ACL checked: localhost
>>> 2023/11/10 10:25:43.162 kid1| 44,2| peer_select.cc(460) 
>>> resolveSelected: Find IP destination for: bugzilla.tucasi.com:443' 
>>> via bugzilla.tucasi.com
>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector1526 found all 0 destinations for 
>>> bugzilla.tucasi.com:443
>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(480) 
>>> resolveSelected: ?? always_direct = ALLOWED
>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(481) 
>>> resolveSelected: ??? never_direct = DENIED
>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(482) 
>>> resolveSelected: ??????? timedout = 0
>>> 2023/11/10 10:25:43.163 kid1| 4,2| errorpage.cc(1397) buildBody: No 
>>> existing error page language negotiated for ERR_DNS_FAIL. Using 
>>> default error file.
>>> 2023/11/10 10:25:43.163 kid1| 33,2| client_side.cc(617) swanSong: 
>>> conn13206 local=127.0.0.1:8123 remote=127.0.0.1:57843 flags=1
>>>
>>> If my analysis is correct why is this logged as a connection failure 
>>> and do I need to worry about it beyond it filing up the logs needlessly?
>>>
>>> My concern is that this could lead to the parent being incorrectly 
>>> declared DEAD thus impacting other traffic:
>>>
>>> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
>>> ???? current master transaction: master4581234
>>>
>>



From miles.wy.1 at gmail.com  Tue Nov 21 18:01:30 2023
From: miles.wy.1 at gmail.com (Wen Yue)
Date: Wed, 22 Nov 2023 02:01:30 +0800
Subject: [squid-users] What's this 'errorno=104' error?
Message-ID: <CAJZuUJAOGXcadztXXoXU+Ni+6JFUC0vVNEUa8pYtgsmgTsz+4g@mail.gmail.com>

I configured Squid6.3 as a MITM proxy and used Chrome to browse web pages
through this Squid proxy, such as twitter.com. However, I noticed these
error messages in the cache.log:

...
2023/11/22 01:33:38 kid1| ERROR: system call failure while accepting a TLS
connection on conn8925690 local=10.0.0.5:3128 remote=171.221.64.188:33454
FD 12 flags=1: SQUID_TLS_ERR_ACCEPT+TLS_IO_ERR=5+errno=104
    current master transaction: master128
2023/11/22 01:33:50 kid1| ERROR: system call failure while accepting a TLS
connection on conn8925909 local=10.0.0.5:3128 remote=171.221.64.188:33481
FD 42 flags=1: SQUID_TLS_ERR_ACCEPT+TLS_IO_ERR=5+errno=104
    current master transaction: master128
2023/11/22 01:33:50 kid1| ERROR: system call failure while accepting a TLS
connection on conn8925915 local=10.0.0.5:3128 remote=171.221.64.188:33484
FD 45 flags=1: SQUID_TLS_ERR_ACCEPT+TLS_IO_ERR=5+errno=104
    current master transaction: master128
2023/11/22 01:33:51 kid1| ERROR: system call failure while accepting a TLS
connection on conn8925931 local=10.0.0.5:3128 remote=171.221.64.188:33488
FD 43 flags=1: SQUID_TLS_ERR_ACCEPT+TLS_IO_ERR=5+errno=104
    current master transaction: master128
...

At the same time, the web pages are not displayed correctly. This error
does not occur when browsing all websites, only for a certain subset of
websites. I haven't found any obvious patterns regarding which websites
have issues and which do not.

And here is my squid.conf:


> debug_options ALL,1
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN)
> acl localnet src 10.0.0.0/8 # RFC 1918 local private network (LAN)
> acl localnet src 100.64.0.0/10 # RFC 6598 shared address space (CGN)
> acl localnet src 169.254.0.0/16 # RFC 3927 link-local (directly plugged)
> machines
> acl localnet src 172.16.0.0/12 # RFC 1918 local private network (LAN)
> acl localnet src 192.168.0.0/16 # RFC 1918 local private network (LAN)
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10       # RFC 4291 link-local (directly plugged)
> machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> acl bump_exclude_domains ssl::server_name
> "/opt/squid/var/bump_exclude_domains.conf"
> acl bump_exclude_ips dst "/opt/squid/var/bump_exclude_ips.conf"
>
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> http_access allow localhost
>
> http_access deny to_localhost
>
> http_access deny to_linklocal
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> http_port 3128 ssl-bump tls-cert=/opt/squid/var/localCA/myCA.crt
> tls-key=/opt/squid/var/localCA/myCA.key generate-host-certificates=on
> dynamic_cert_mem_cache_size=8MB
> tls-dh=prime256v1:/opt/squid/var/localCA/dhparam.pem
> sslpassword_program /opt/squid/var/localCA/keypass.sh
> # Uncomment and adjust the following to add a disk cache directory.
> cache_dir aufs /opt/squid/var/cache/squid 10240 256 256 max-size=5242880
>
> # Leave coredumps in the first cache dir
> coredump_dir /opt/squid/var/cache/squid
>
> sslcrtd_program /opt/squid/libexec/security_file_certgen -s
> /opt/squid/var/localCA/ssl_db -M 5120MB
> sslcrtd_children 32 startup=5 idle=1
>
> # at step 1 we're peeking at client TLS-request in order to find the "SNI"
> ssl_bump peek step1 all
> # peeking of bump_excludes at step2, leading to splice at step3 for them
> ssl_bump peek step2 bump_exclude_domains
> ssl_bump peek step2 bump_exclude_ips
> ssl_bump splice step3 bump_exclude_domains
> ssl_bump splice step3 bump_exclude_ips
> # staring for others, leading to bump at step3
> ssl_bump stare step2 all
> ssl_bump bump step3 all
>
>
> sslproxy_cert_error allow all
> tls_outgoing_options options=ALL flags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN
>
>
> forwarded_for transparent
>
> via off
>
> on_unsupported_protocol respond all
>
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
>


Does anyone know what could be causing this problem, and how may I get rid
of it? thanks!

-- 
Regards.
Wen Yue
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231122/fd09d2c0/attachment.htm>

From ankor2023 at gmail.com  Wed Nov 22 04:05:49 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Wed, 22 Nov 2023 07:05:49 +0300
Subject: [squid-users] Kerberos pac ResourceGroups parsing
In-Reply-To: <04aa8846-c50e-4ff0-9ae3-dc78b8905f3c@measurement-factory.com>
References: <CADJd0Y1CEG1DAWYV1f0tVAOkL4XfRER5XdeDa0Pg7Q3WrewpVw@mail.gmail.com>
 <04aa8846-c50e-4ff0-9ae3-dc78b8905f3c@measurement-factory.com>
Message-ID: <CADJd0Y2VJ=-6E=EodJwTcditJ9cP268v5J+OwC+htaaA3+o=Lw@mail.gmail.com>

Hello, Alex,

I have posted a PR: https://github.com/squid-cache/squid/pull/1597

This is my first contribution to open source. Could you please verify if
everything is OK.

Kind regards,
    Ankor.


??, 16 ????. 2023??. ? 17:01, Alex Rousskov <
rousskov at measurement-factory.com>:

> On 2023-11-16 07:48, Andrey K wrote:
>
> > I have slightly patched the negotiate_kerberos_pac.cc to
> > implement ResourceGropIds-block parsing.
>
> Please consider posting tested changes as a GitHub Pull Request:
> https://wiki.squid-cache.org/MergeProcedure#pull-request
>
>
> Thank you,
>
> Alex.
>
>
> > Maybe it will be useful for the community.
> > This patch can be included in future Squid-releases.
> >
> > Kind regards,
> >     Ankor.
> >
> > The patch for the
> > file src/auth/negotiate/kerberos/negotiate_kerberos_pac.cc below:
> >
> > @@ -362,6 +362,123 @@
> >       return ad_groups;
> >   }
> >
> > +
> > +char *
> > +get_resource_group_domain_sid(uint32_t ResourceGroupDomainSid){
> > +
> > +    if (ResourceGroupDomainSid!= 0) {
> > +        uint8_t rev;
> > +        uint64_t idauth;
> > +        char dli[256];
> > +        char *ag;
> > +        int l;
> > +
> > +        align(4);
> > +
> > +        uint32_t nauth = get4byt();
> > +
> > +        size_t length = 1+1+6+nauth*4;
> > +
> > +            ag=(char *)xcalloc((length+1)*sizeof(char),1);
> > +            // the first byte is a length of the SID
> > +            ag[0] = (char) length;
> > +            memcpy((void *)&ag[1],(const void*)&p[bpos],1);
> > +            memcpy((void *)&ag[2],(const void*)&p[bpos+1],1);
> > +            ag[2] = ag[2]+1;
> > +            memcpy((void *)&ag[3],(const void*)&p[bpos+2],6+nauth*4);
> > +
> > +
> > +
> > +        /* mainly for debug only */
> > +        rev = get1byt();
> > +        bpos = bpos + 1; /*nsub*/
> > +        idauth = get6byt_be();
> > +
> > +        snprintf(dli,sizeof(dli),"S-%d-%lu",rev,(long unsigned
> int)idauth);
> > +        for ( l=0; l<(int)nauth; l++ ) {
> > +            uint32_t sauth;
> > +            sauth = get4byt();
> > +            snprintf((char
> > *)&dli[strlen(dli)],sizeof(dli)-strlen(dli),"-%u",sauth);
> > +        }
> > +        debug((char *) "%s| %s: INFO: Got ResourceGroupDomainSid %s\n",
> > LogTime(), PROGRAM, dli);
> > +        return ag;
> > +    }
> > +
> > +    return NULL;
> > +}
> > +
> > +char *
> > +get_resource_groups(char *ad_groups, char *resource_group_domain_sid,
> > uint32_t ResourceGroupIds, uint32_t ResourceGroupCount){
> > +    size_t group_domain_sid_len = resource_group_domain_sid[0];
> > +    char *ag;
> > +    size_t length;
> > +
> > +    resource_group_domain_sid++; //now it points to the actual data
> > +
> > +
> > +    if (ResourceGroupIds!= 0) {
> > +        uint32_t ngroup;
> > +        int l;
> > +
> > +        align(4);
> > +        ngroup = get4byt();
> > +        if ( ngroup != ResourceGroupCount) {
> > +            debug((char *) "%s| %s: ERROR: Group encoding error =>
> > ResourceGroupCount: %d Array size: %d\n",
> > +                  LogTime(), PROGRAM, ResourceGroupCount, ngroup);
> > +            return NULL;
> > +        }
> > +        debug((char *) "%s| %s: INFO: Found %d Resource Group rids\n",
> > LogTime(), PROGRAM, ResourceGroupCount);
> > +
> > +        //make a group template which begins with the
> ResourceGroupDomainID
> > +        length = group_domain_sid_len+4;  //+4 for a rid
> > +        ag=(char *)xcalloc(length*sizeof(char),1);
> > +        memcpy((void *)ag,(const void*)resource_group_domain_sid,
> > group_domain_sid_len);
> > +
> > +
> > +        for ( l=0; l<(int)ResourceGroupCount; l++) {
> > +            uint32_t sauth;
> > +            memcpy((void *)&ag[group_domain_sid_len],(const
> > void*)&p[bpos],4);
> > +
> > +            if (!pstrcat(ad_groups," group=")) {
> > +                debug((char *) "%s| %s: WARN: Too many groups ! size >
> > %d : %s\n",
> > +                      LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE,
> ad_groups);
> > +               xfree(ag);
> > +               return NULL;
> > +            }
> > +
> > +
> > +            struct base64_encode_ctx ctx;
> > +            base64_encode_init(&ctx);
> > +            const uint32_t expectedSz = base64_encode_len(length) +1 /*
> > terminator */;
> > +            char *b64buf = static_cast<char *>(xcalloc(expectedSz, 1));
> > +            size_t blen = base64_encode_update(&ctx, b64buf, length,
> > reinterpret_cast<uint8_t*>(ag));
> > +            blen += base64_encode_final(&ctx, b64buf+blen);
> > +            b64buf[expectedSz-1] = '\0';
> > +            if (!pstrcat(ad_groups, reinterpret_cast<char*>(b64buf))) {
> > +                debug((char *) "%s| %s: WARN: Too many groups ! size >
> > %d : %s\n",
> > +                      LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE,
> ad_groups);
> > +               xfree(ag);
> > +               xfree(b64buf);
> > +               return NULL;
> > +            }
> > +            xfree(b64buf);
> > +
> > +
> > +
> > +            sauth = get4byt();
> > +            debug((char *) "%s| %s: Info: Got rid: %u\n", LogTime(),
> > PROGRAM, sauth);
> > +            /* attribute */
> > +            bpos = bpos+4;
> > +        }
> > +
> > +        xfree(ag);
> > +       return ad_groups;
> > +    }
> > +
> > +    return NULL;
> > +}
> > +
> > +
> >   char *
> >   get_ad_groups(char *ad_groups, krb5_context context, krb5_pac pac)
> >   {
> > @@ -379,14 +496,14 @@
> >       uint32_t LogonDomainId=0;
> >       uint32_t SidCount=0;
> >       uint32_t ExtraSids=0;
> > -    /*
> >       uint32_t ResourceGroupDomainSid=0;
> >       uint32_t ResourceGroupCount=0;
> >       uint32_t ResourceGroupIds=0;
> > -    */
> >       char **Rids=NULL;
> >       int l=0;
> >
> > +    char * resource_group_domain_sid=NULL;
> > +
> >       if (!ad_groups) {
> >           debug((char *) "%s| %s: ERR: No space to store groups\n",
> >                 LogTime(), PROGRAM);
> > @@ -454,11 +571,11 @@
> >       bpos = bpos+40;
> >       SidCount = get4byt();
> >       ExtraSids = get4byt();
> > -    /* 4 bytes ResourceGroupDomainSid
> > -     * 4 bytes ResourceGroupCount
> > -     * 4 bytes ResourceGroupIds
> > -     */
> > -    bpos = bpos+12;
> > +
> > +    ResourceGroupDomainSid = get4byt();
> > +    ResourceGroupCount = get4byt();
> > +    ResourceGroupIds = get4byt();
> > +
> >       /*
> >        * Read all data from structure => Now check pointers
> >        */
> > @@ -483,7 +600,15 @@
> >       if ((ad_groups = getextrasids(ad_groups,ExtraSids,SidCount))==NULL)
> >           goto k5clean;
> >
> > +    resource_group_domain_sid =
> > get_resource_group_domain_sid(ResourceGroupDomainSid);
> > +    if(resource_group_domain_sid && ResourceGroupCount &&
> > ResourceGroupIds){
> > +        get_resource_groups(ad_groups, resource_group_domain_sid,
> > ResourceGroupIds, ResourceGroupCount);
> > +    }
> > +
> >       debug((char *) "%s| %s: INFO: Read %d of %d bytes \n", LogTime(),
> > PROGRAM, bpos, (int)ad_data->length);
> > +
> > +    if(resource_group_domain_sid) xfree(resource_group_domain_sid);
> > +
> >       if (Rids) {
> >           for ( l=0; l<(int)GroupCount; l++) {
> >               xfree(Rids[l]);
> > @@ -493,6 +618,8 @@
> >       krb5_free_data(context, ad_data);
> >       return ad_groups;
> >   k5clean:
> > +    if(resource_group_domain_sid) xfree(resource_group_domain_sid);
> > +
> >       if (Rids) {
> >           for ( l=0; l<(int)GroupCount; l++) {
> >               xfree(Rids[l]);
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231122/ba1f52b2/attachment.htm>

From david.komanek at natur.cuni.cz  Wed Nov 22 10:03:56 2023
From: david.komanek at natur.cuni.cz (David Komanek)
Date: Wed, 22 Nov 2023 11:03:56 +0100
Subject: [squid-users] how to avoid use http/1.0 between squid and the target
Message-ID: <3d203fcd-707d-4020-9150-1409bda9d89e@natur.cuni.cz>

Hello,

I have a strange problem (definitely some kind of my own ignorance) :

If I try to access anything on the site https://www.samba.org WITHOUT 
proxy, my browser negotiate happily for http/2 protocol and receives all 
the data. For?http://www.samba.org WITHOUT proxy it starts with http/1.1 
which is auto-redirected from http to https and continues with http/2. 
So far so good.

But WITH proxy, it happens that squid is using http/1.0. The remote site 
is blocking this protocol, requiring at least http/1.1 (confirmed by the 
samba.org website maintainer), so the site remains inaccessible. But 
this is the only site where I have been able to encounter this problem. 
If I connect WITH proxy to other sites, squid is using http/1.1 as expected.

So, I'm lost here, unable to find the reason, why http/1.1 couldn't be 
used by squid in some rare cases. What am I missing here? I am not aware 
of any configuration directives which could cause this.

browsers: chrome, firefox (both updated)
squid: freebsd package (now version 6.5, but the I had the same problem 
with 5.9 before)

Thanks in advance for some hints here.

Best regards,

 ? David Komanek
 ? Charles University in Prague
 ? Faculty of Science




From squid at borrill.org.uk  Wed Nov 22 10:15:13 2023
From: squid at borrill.org.uk (Stephen Borrill)
Date: Wed, 22 Nov 2023 10:15:13 +0000
Subject: [squid-users] 6.x gives frequent connection to peer failed -
 spurious?
In-Reply-To: <802c46b3-69a3-40b2-9ccd-f1bedb57961a@measurement-factory.com>
References: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>
 <fc40c8a1-e988-40c9-82db-bd70426f6c99@measurement-factory.com>
 <1fd61adc-2b97-4cdc-92a4-f8a0a7c575d2@borrill.org.uk>
 <802c46b3-69a3-40b2-9ccd-f1bedb57961a@measurement-factory.com>
Message-ID: <ace4b905-44ef-47d5-98cf-f1c5714fb5be@borrill.org.uk>

On 21/11/2023 15:55, Alex Rousskov wrote:
> On 2023-11-21 08:38, Stephen Borrill wrote:
>> On 15/11/2023 21:55, Alex Rousskov wrote:
>>> On 2023-11-10 05:46, Stephen Borrill wrote:
>>>
>>>> With 6.x (currently 6.5) there are very frequent (every 10 seconds 
>>>> or so) messages like:
>>>> 2023/11/10 10:25:43 kid1| ERROR: Connection to 127.0.0.1:8123 failed
>>>
>>>
>>>> why is this logged as a connection failure
>>>
>>> The current error wording is too assuming and, in your case, 
>>> evidently misleading. The phrase "Connection to X failed" should be 
>>> changed to something more general like "Cannot contact cache_peer X" 
>>> or "Cannot communicate with cache_peer X".
>>>
>>> CachePeer::countFailure() patches welcome.
> 
>> But the point is that it _can_ communicate with the peer, but the peer 
>> itself can't service the request. The peer returning 503 shouldn't be 
>> logged as a connection failure
> 
> 
> My bad. I missed the fact that the described DNS error happens at a 
> _peer_ Squid. Sorry.
> 
> 
> Currently, Squid v6 treats most CONNECT-to-peer errors as a sign of a 
> broken peer. In 2022, 4xx errors were excluded from that set[1]. At that 
> time, we also proposed to make that decision configurable using a new 
> cache_peer_fault directive[2], but the new directive was blocked as an 
> "overkill"[3], so we hard-coded 4xx exclusion instead.
> 
> Going forward, you have several options, including these two:
> 
> 1. Convince others that Squid should treat all 503 CONNECT errors from 
> peers as it already treats all 4xx errors. Hard-code that new logic.
> 
> 2. Convince others that cache_peer_fault or a similar directive is a 
> good idea rather than an overkill. Resurrect its implementation[2].
> 
> 
> [1] 
> https://github.com/squid-cache/squid/commit/022dbabd89249f839d1861aa87c1ab9e1a008a47
> 
> [2] 
> https://github.com/squid-cache/squid/commit/25431f18f2f5e796b8704c85fc51f93b6cc2a73d
> 
> [3] https://github.com/squid-cache/squid/pull/1166#issuecomment-1295806530


2) seems sensible. Especially in the case where you have a single 
cache_peer and cannot go direct. No benefit to marking it as dead.

However, I'm currently running with 1) as per below and this stops the 
non-existent domains counting against the peer (which surely opens it to 
a DoS attack):

--- src/CachePeer.cc.orig       2023-11-22 08:30:17.524266325 +0000
+++ src/CachePeer.cc    2023-11-22 08:31:05.394052184 +0000
@@ -71,7 +71,7 @@
  void
  CachePeer::noteFailure(const Http::StatusCode code)
  {
-    if (Http::Is4xx(code))
+    if (Http::Is4xx(code) || code == Http::scServiceUnavailable)
          return; // this failure is not our fault

      countFailure();


>>> ?> do I need to worry about it beyond it filing up the logs needlessly?
>>>
>>> In short, "yes".
>>>
>>> I cannot accurately assess your specific needs, but, in most 
>>> environments, one should indeed worry that their cache_peer server 
>>> names cannot be reliably resolved because failed resolution attempts 
>>> waste Squid resources and increase transaction response time. 
>>> Moreover, if these failures are frequent enough (relative to peer 
>>> usage attempts), the affected cache_peer will be marked as DEAD (as 
>>> you have mentioned):
>>>
>>> ?> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
>>
>> Problem seems to be easily reproducible:
>>
>> 1# env https_proxy=http://127.0.0.1:8084 curl https://www.invalid.domain/
>> curl: (56) CONNECT tunnel failed, response 503
>> 2# grep invalid /usr/local/squid/logs/access.log|tail -1
>> 1700573429.015????? 4 127.0.0.1:8084 TCP_TUNNEL/503 0 CONNECT 
>> www.invalid.domain:443 - FIRSTUP_PARENT/127.0.0.1:8123 -
>> 3# date -r 1700573429 '+%Y/%m/%d %H:%M:%S'
>> 2023/11/21 13:30:29
>> 4# grep '2023/11/21 13:30:29' /usr/local/squid/logs/cache.log
>> 2023/11/21 13:30:29 kid1| ERROR: Connection to 127.0.0.1:8123 failed
>>
>>>> With 4.x there were no such messages.
>>>>
>>>> By comparing to the peer squid logs, these seems to tally with DNS 
>>>> failures:
>>>> peer_select.cc(479) resolveSelected: PeerSelector1688 found all 0 
>>>> destinations for bugzilla.tucasi.com:443
>>>>
>>>> Full ALL,2 log at the time of the reported connection failure:
>>>>
>>>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(214) doAccept: New 
>>>> connection on FD 17
>>>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(316) acceptNext: 
>>>> connection on conn3 local=127.0.0.1:8123 remote=[::] FD 17 flags=9
>>>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1332) 
>>>> parseHttpRequest: HTTP Client conn13206 local=127.0.0.1:8123 
>>>> remote=127.0.0.1:57843 FD 147 flags=1
>>>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1336) 
>>>> parseHttpRequest: HTTP Client REQUEST:
>>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
>>>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 
>>>> is ALLOWED; last ACL checked: localhost
>>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(683) 
>>>> clientAccessCheck2: No adapted_http_access configuration. default: 
>>>> ALLOW
>>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707) 
>>>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443 
>>>> is ALLOWED; last ACL checked: localhost
>>>> 2023/11/10 10:25:43.162 kid1| 44,2| peer_select.cc(460) 
>>>> resolveSelected: Find IP destination for: bugzilla.tucasi.com:443' 
>>>> via bugzilla.tucasi.com
>>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(479) 
>>>> resolveSelected: PeerSelector1526 found all 0 destinations for 
>>>> bugzilla.tucasi.com:443
>>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(480) 
>>>> resolveSelected: ?? always_direct = ALLOWED
>>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(481) 
>>>> resolveSelected: ??? never_direct = DENIED
>>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(482) 
>>>> resolveSelected: ??????? timedout = 0
>>>> 2023/11/10 10:25:43.163 kid1| 4,2| errorpage.cc(1397) buildBody: No 
>>>> existing error page language negotiated for ERR_DNS_FAIL. Using 
>>>> default error file.
>>>> 2023/11/10 10:25:43.163 kid1| 33,2| client_side.cc(617) swanSong: 
>>>> conn13206 local=127.0.0.1:8123 remote=127.0.0.1:57843 flags=1
>>>>
>>>> If my analysis is correct why is this logged as a connection failure 
>>>> and do I need to worry about it beyond it filing up the logs 
>>>> needlessly?
>>>>
>>>> My concern is that this could lead to the parent being incorrectly 
>>>> declared DEAD thus impacting other traffic:
>>>>
>>>> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
>>>> ???? current master transaction: master4581234
>>>>
>>>
> 



From squid3 at treenet.co.nz  Wed Nov 22 12:44:30 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2023 01:44:30 +1300
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <3d203fcd-707d-4020-9150-1409bda9d89e@natur.cuni.cz>
References: <3d203fcd-707d-4020-9150-1409bda9d89e@natur.cuni.cz>
Message-ID: <ec400901-951e-4112-ac53-ae8cd3351301@treenet.co.nz>

On 22/11/23 23:03, David Komanek wrote:
> Hello,
> 
> I have a strange problem (definitely some kind of my own ignorance) :
> 
> If I try to access anything on the site https://www.samba.org WITHOUT 
> proxy, my browser negotiate happily for http/2 protocol and receives all 
> the data. For?http://www.samba.org WITHOUT proxy it starts with http/1.1 
> which is auto-redirected from http to https and continues with http/2. 
> So far so good.
> 
> But WITH proxy, it happens that squid is using http/1.0.

That is odd. Squid should always be sending requests as HTTP/1.1.

Have a look at the debug level "11,2" cache.log records to see if Squid 
is actually sending 1.0 or if it is just relaying CONNECT requests with 
possibly HTTP/1.0 inside.


HTH
Amos


From squid3 at treenet.co.nz  Wed Nov 22 12:58:03 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2023 01:58:03 +1300
Subject: [squid-users] What's this 'errorno=104' error?
In-Reply-To: <CAJZuUJAOGXcadztXXoXU+Ni+6JFUC0vVNEUa8pYtgsmgTsz+4g@mail.gmail.com>
References: <CAJZuUJAOGXcadztXXoXU+Ni+6JFUC0vVNEUa8pYtgsmgTsz+4g@mail.gmail.com>
Message-ID: <e9ce5af4-b27d-48ac-9d2c-73c5f1525d18@treenet.co.nz>

On 22/11/23 07:01, Wen Yue wrote:
> I configured Squid6.3 as a MITM proxy and used Chrome to browse web 
> pages through this Squid proxy, such as twitter.com. However,
> I noticed these error messages in the 
> cache.log:
> 
> ...
> 2023/11/22 01:33:38 kid1| ERROR: system call failure while accepting a 
> TLS connection on conn8925690 local=10.0.0.5:3128 <http://10.0.0.5:3128> 
> remote=171.221.64.188:33454 <http://171.221.64.188:33454> FD 12 flags=1: 
> SQUID_TLS_ERR_ACCEPT+TLS_IO_ERR=5+errno=104


Depends on the OS the proxy was built for.

Assuming Linux it would be POSIX "ECONNRESET" meaning "Connection reset 
by peer".


HTH
Amos


From rousskov at measurement-factory.com  Wed Nov 22 16:22:13 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 22 Nov 2023 11:22:13 -0500
Subject: [squid-users] Kerberos pac ResourceGroups parsing
In-Reply-To: <CADJd0Y2VJ=-6E=EodJwTcditJ9cP268v5J+OwC+htaaA3+o=Lw@mail.gmail.com>
References: <CADJd0Y1CEG1DAWYV1f0tVAOkL4XfRER5XdeDa0Pg7Q3WrewpVw@mail.gmail.com>
 <04aa8846-c50e-4ff0-9ae3-dc78b8905f3c@measurement-factory.com>
 <CADJd0Y2VJ=-6E=EodJwTcditJ9cP268v5J+OwC+htaaA3+o=Lw@mail.gmail.com>
Message-ID: <90fe73bb-20b8-4cb5-9634-1a1ebb1aba24@measurement-factory.com>

On 2023-11-21 23:05, Andrey K wrote:

> I have posted a PR: https://github.com/squid-cache/squid/pull/1597 
> 
> This is my first contribution to open source. Could you please verify if 
> everything is OK.

Thank you for posting that pull request! Let's continue this 
conversation on GitHub since squid-users mailing list is not meant for 
code reviews.

Alex.


> ??, 16 ????. 2023??. ? 17:01, Alex Rousskov:
> 
>     On 2023-11-16 07:48, Andrey K wrote:
> 
>      > I have slightly patched the?negotiate_kerberos_pac.cc to
>      > implement?ResourceGropIds-block parsing.
> 
>     Please consider posting tested changes as a GitHub Pull Request:
>     https://wiki.squid-cache.org/MergeProcedure#pull-request
>     <https://wiki.squid-cache.org/MergeProcedure#pull-request>
> 
> 
>     Thank you,
> 
>     Alex.
> 
> 
>      > Maybe it will be useful for the community.
>      > This patch can be included in future Squid-releases.
>      >
>      > Kind regards,
>      >? ? ?Ankor.
>      >
>      > The patch for the
>      > file?src/auth/negotiate/kerberos/negotiate_kerberos_pac.cc below:
>      >
>      > @@ -362,6 +362,123 @@
>      >? ? ? ?return ad_groups;
>      >? ?}
>      >
>      > +
>      > +char *
>      > +get_resource_group_domain_sid(uint32_t ResourceGroupDomainSid){
>      > +
>      > + ? ?if (ResourceGroupDomainSid!= 0) {
>      > + ? ? ? ?uint8_t rev;
>      > + ? ? ? ?uint64_t idauth;
>      > + ? ? ? ?char dli[256];
>      > + ? ? ? ?char *ag;
>      > + ? ? ? ?int l;
>      > +
>      > + ? ? ? ?align(4);
>      > +
>      > + ? ? ? ?uint32_t nauth = get4byt();
>      > +
>      > + ? ? ? ?size_t length = 1+1+6+nauth*4;
>      > +
>      > + ? ? ? ? ? ?ag=(char *)xcalloc((length+1)*sizeof(char),1);
>      > + ? ? ? ? ? ?// the first byte is a length of the SID
>      > + ? ? ? ? ? ?ag[0] = (char) length;
>      > + ? ? ? ? ? ?memcpy((void *)&ag[1],(const void*)&p[bpos],1);
>      > + ? ? ? ? ? ?memcpy((void *)&ag[2],(const void*)&p[bpos+1],1);
>      > + ? ? ? ? ? ?ag[2] = ag[2]+1;
>      > + ? ? ? ? ? ?memcpy((void *)&ag[3],(const
>     void*)&p[bpos+2],6+nauth*4);
>      > +
>      > +
>      > +
>      > + ? ? ? ?/* mainly for debug only */
>      > + ? ? ? ?rev = get1byt();
>      > + ? ? ? ?bpos = bpos + 1; /*nsub*/
>      > + ? ? ? ?idauth = get6byt_be();
>      > +
>      > + ? ? ? ?snprintf(dli,sizeof(dli),"S-%d-%lu",rev,(long unsigned
>     int)idauth);
>      > + ? ? ? ?for ( l=0; l<(int)nauth; l++ ) {
>      > + ? ? ? ? ? ?uint32_t sauth;
>      > + ? ? ? ? ? ?sauth = get4byt();
>      > + ? ? ? ? ? ?snprintf((char
>      > *)&dli[strlen(dli)],sizeof(dli)-strlen(dli),"-%u",sauth);
>      > + ? ? ? ?}
>      > + ? ? ? ?debug((char *) "%s| %s: INFO: Got ResourceGroupDomainSid
>     %s\n",
>      > LogTime(), PROGRAM, dli);
>      > + ? ? ? ?return ag;
>      > + ? ?}
>      > +
>      > + ? ?return NULL;
>      > +}
>      > +
>      > +char *
>      > +get_resource_groups(char *ad_groups, char
>     *resource_group_domain_sid,
>      > uint32_t ResourceGroupIds, uint32_t ResourceGroupCount){
>      > + ? ?size_t group_domain_sid_len = resource_group_domain_sid[0];
>      > + ? ?char *ag;
>      > + ? ?size_t length;
>      > +
>      > + ? ?resource_group_domain_sid++; //now it points to the actual data
>      > +
>      > +
>      > + ? ?if (ResourceGroupIds!= 0) {
>      > + ? ? ? ?uint32_t ngroup;
>      > + ? ? ? ?int l;
>      > +
>      > + ? ? ? ?align(4);
>      > + ? ? ? ?ngroup = get4byt();
>      > + ? ? ? ?if ( ngroup != ResourceGroupCount) {
>      > + ? ? ? ? ? ?debug((char *) "%s| %s: ERROR: Group encoding error =>
>      > ResourceGroupCount: %d Array size: %d\n",
>      > + ? ? ? ? ? ? ? ? ?LogTime(), PROGRAM, ResourceGroupCount, ngroup);
>      > + ? ? ? ? ? ?return NULL;
>      > + ? ? ? ?}
>      > + ? ? ? ?debug((char *) "%s| %s: INFO: Found %d Resource Group
>     rids\n",
>      > LogTime(), PROGRAM, ResourceGroupCount);
>      > +
>      > + ? ? ? ?//make a group template which begins with the
>     ResourceGroupDomainID
>      > + ? ? ? ?length = group_domain_sid_len+4; ?//+4 for a rid
>      > + ? ? ? ?ag=(char *)xcalloc(length*sizeof(char),1);
>      > + ? ? ? ?memcpy((void *)ag,(const void*)resource_group_domain_sid,
>      > group_domain_sid_len);
>      > +
>      > +
>      > + ? ? ? ?for ( l=0; l<(int)ResourceGroupCount; l++) {
>      > + ? ? ? ? ? ?uint32_t sauth;
>      > + ? ? ? ? ? ?memcpy((void *)&ag[group_domain_sid_len],(const
>      > void*)&p[bpos],4);
>      > +
>      > + ? ? ? ? ? ?if (!pstrcat(ad_groups," group=")) {
>      > + ? ? ? ? ? ? ? ?debug((char *) "%s| %s: WARN: Too many groups !
>     size >
>      > %d : %s\n",
>      > + ? ? ? ? ? ? ? ? ? ? ?LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE,
>     ad_groups);
>      > + ? ? ? ? ? ? ? xfree(ag);
>      > + ? ? ? ? ? ? ? return NULL;
>      > + ? ? ? ? ? ?}
>      > +
>      > +
>      > + ? ? ? ? ? ?struct base64_encode_ctx ctx;
>      > + ? ? ? ? ? ?base64_encode_init(&ctx);
>      > + ? ? ? ? ? ?const uint32_t expectedSz =
>     base64_encode_len(length) +1 /*
>      > terminator */;
>      > + ? ? ? ? ? ?char *b64buf = static_cast<char
>     *>(xcalloc(expectedSz, 1));
>      > + ? ? ? ? ? ?size_t blen = base64_encode_update(&ctx, b64buf,
>     length,
>      > reinterpret_cast<uint8_t*>(ag));
>      > + ? ? ? ? ? ?blen += base64_encode_final(&ctx, b64buf+blen);
>      > + ? ? ? ? ? ?b64buf[expectedSz-1] = '\0';
>      > + ? ? ? ? ? ?if (!pstrcat(ad_groups,
>     reinterpret_cast<char*>(b64buf))) {
>      > + ? ? ? ? ? ? ? ?debug((char *) "%s| %s: WARN: Too many groups !
>     size >
>      > %d : %s\n",
>      > + ? ? ? ? ? ? ? ? ? ? ?LogTime(), PROGRAM, MAX_PAC_GROUP_SIZE,
>     ad_groups);
>      > + ? ? ? ? ? ? ? xfree(ag);
>      > + ? ? ? ? ? ? ? xfree(b64buf);
>      > + ? ? ? ? ? ? ? return NULL;
>      > + ? ? ? ? ? ?}
>      > + ? ? ? ? ? ?xfree(b64buf);
>      > +
>      > +
>      > +
>      > + ? ? ? ? ? ?sauth = get4byt();
>      > + ? ? ? ? ? ?debug((char *) "%s| %s: Info: Got rid: %u\n",
>     LogTime(),
>      > PROGRAM, sauth);
>      > + ? ? ? ? ? ?/* attribute */
>      > + ? ? ? ? ? ?bpos = bpos+4;
>      > + ? ? ? ?}
>      > +
>      > + ? ? ? ?xfree(ag);
>      > + ? ? ? return ad_groups;
>      > + ? ?}
>      > +
>      > + ? ?return NULL;
>      > +}
>      > +
>      > +
>      >? ?char *
>      >? ?get_ad_groups(char *ad_groups, krb5_context context, krb5_pac pac)
>      >? ?{
>      > @@ -379,14 +496,14 @@
>      >? ? ? ?uint32_t LogonDomainId=0;
>      >? ? ? ?uint32_t SidCount=0;
>      >? ? ? ?uint32_t ExtraSids=0;
>      > - ? ?/*
>      >? ? ? ?uint32_t ResourceGroupDomainSid=0;
>      >? ? ? ?uint32_t ResourceGroupCount=0;
>      >? ? ? ?uint32_t ResourceGroupIds=0;
>      > - ? ?*/
>      >? ? ? ?char **Rids=NULL;
>      >? ? ? ?int l=0;
>      >
>      > + ? ?char * resource_group_domain_sid=NULL;
>      > +
>      >? ? ? ?if (!ad_groups) {
>      >? ? ? ? ? ?debug((char *) "%s| %s: ERR: No space to store groups\n",
>      >? ? ? ? ? ? ? ? ?LogTime(), PROGRAM);
>      > @@ -454,11 +571,11 @@
>      >? ? ? ?bpos = bpos+40;
>      >? ? ? ?SidCount = get4byt();
>      >? ? ? ?ExtraSids = get4byt();
>      > - ? ?/* 4 bytes ResourceGroupDomainSid
>      > - ? ? * 4 bytes ResourceGroupCount
>      > - ? ? * 4 bytes ResourceGroupIds
>      > - ? ? */
>      > - ? ?bpos = bpos+12;
>      > +
>      > + ? ?ResourceGroupDomainSid = get4byt();
>      > + ? ?ResourceGroupCount = get4byt();
>      > + ? ?ResourceGroupIds = get4byt();
>      > +
>      >? ? ? ?/*
>      >? ? ? ? * Read all data from structure => Now check pointers
>      >? ? ? ? */
>      > @@ -483,7 +600,15 @@
>      >? ? ? ?if ((ad_groups =
>     getextrasids(ad_groups,ExtraSids,SidCount))==NULL)
>      >? ? ? ? ? ?goto k5clean;
>      >
>      > + ? ?resource_group_domain_sid =
>      > get_resource_group_domain_sid(ResourceGroupDomainSid);
>      > + ? ?if(resource_group_domain_sid && ResourceGroupCount &&
>      > ResourceGroupIds){
>      > + ? ? ? ?get_resource_groups(ad_groups, resource_group_domain_sid,
>      > ResourceGroupIds, ResourceGroupCount);
>      > + ? ?}
>      > +
>      >? ? ? ?debug((char *) "%s| %s: INFO: Read %d of %d bytes \n",
>     LogTime(),
>      > PROGRAM, bpos, (int)ad_data->length);
>      > +
>      > + ? ?if(resource_group_domain_sid) xfree(resource_group_domain_sid);
>      > +
>      >? ? ? ?if (Rids) {
>      >? ? ? ? ? ?for ( l=0; l<(int)GroupCount; l++) {
>      >? ? ? ? ? ? ? ?xfree(Rids[l]);
>      > @@ -493,6 +618,8 @@
>      >? ? ? ?krb5_free_data(context, ad_data);
>      >? ? ? ?return ad_groups;
>      >? ?k5clean:
>      > + ? ?if(resource_group_domain_sid) xfree(resource_group_domain_sid);
>      > +
>      >? ? ? ?if (Rids) {
>      >? ? ? ? ? ?for ( l=0; l<(int)GroupCount; l++) {
>      >? ? ? ? ? ? ? ?xfree(Rids[l]);
>      >
>      > _______________________________________________
>      > squid-users mailing list
>      > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      > https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
> 



From mohsen at pahlevanzadeh.net  Wed Nov 22 23:33:13 2023
From: mohsen at pahlevanzadeh.net (Mohsen Pahlevanzadeh)
Date: Thu, 23 Nov 2023 03:03:13 +0330
Subject: [squid-users] UFS or ext4
Message-ID: <d26f0499-8e7f-4aba-8ff5-471c903d5d6d@pahlevanzadeh.net>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231123/8e7a0aba/attachment.htm>

From uhlar at fantomas.sk  Thu Nov 23 08:46:05 2023
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 23 Nov 2023 09:46:05 +0100
Subject: [squid-users] UFS or ext4
In-Reply-To: <d26f0499-8e7f-4aba-8ff5-471c903d5d6d@pahlevanzadeh.net>
References: <d26f0499-8e7f-4aba-8ff5-471c903d5d6d@pahlevanzadeh.net>
Message-ID: <ZV8RTYnBPaAWplkC@fantomas.sk>

On 23.11.23 03:03, Mohsen Pahlevanzadeh wrote:
>   I need to install a www cache, However? I don't know use UFS or ext4?

The underlying filesystem does not matter much.

the cache_dir option can be ufs, aufs (better), diskd or properly set up 
rock storage.

https://wiki.squid-cache.org/Features/RockStore


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Emacs is a complicated operating system without good text editor.


From ml at netfence.it  Thu Nov 23 10:05:04 2023
From: ml at netfence.it (Andrea Venturoli)
Date: Thu, 23 Nov 2023 11:05:04 +0100
Subject: [squid-users] Intercepted connections are not bumped
Message-ID: <59683041-f333-4182-9e91-0bc5474abdc1@netfence.it>

Hello.

I've got the following config:

> ...
> http_port 8080 ssl-bump cert=/usr/local/etc/squid/proxyCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> https_port 3129 intercept ssl-bump cert=/usr/local/etc/squid/proxyCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> ...
> acl step1 at_step SslBump1
> ssl_bump splice !bumphosts
> ssl_bump splice splicedom
> ssl_bump peek step1
> ssl_bump bump all
> ...

So I've got port 8080 where proxy-aware client connect and 3129, which 
is feeded intercepted https connection by ipfw.

Problem is: if a client connects explicitly via proxy (port 8080) it 
gets SSLBumped; if a client simply connects to its destination https 
port (so directed to 3129) it is tunneled.

Anything wrong in my config?
I think it worked in the past: has anything changed in this regard with 
Squid 6?

  bye & Thanks
	av.


From david.komanek at natur.cuni.cz  Mon Nov 27 09:21:18 2023
From: david.komanek at natur.cuni.cz (David Komanek)
Date: Mon, 27 Nov 2023 10:21:18 +0100
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
References: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
Message-ID: <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>


> Date: Thu, 23 Nov 2023 01:44:30 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] how to avoid use http/1.0 between squid and
> 	the target
> Message-ID: <ec400901-951e-4112-ac53-ae8cd3351301 at treenet.co.nz>
> Content-Type: text/plain; charset=UTF-8; format=flowed
>
> On 22/11/23 23:03, David Komanek wrote:
>> Hello,
>>
>> I have a strange problem (definitely some kind of my own ignorance) :
>>
>> If I try to access anything on the site https://www.samba.org WITHOUT
>> proxy, my browser negotiate happily for http/2 protocol and receives all
>> the data. For?http://www.samba.org WITHOUT proxy it starts with http/1.1
>> which is auto-redirected from http to https and continues with http/2.
>> So far so good.
>>
>> But WITH proxy, it happens that squid is using http/1.0.
> That is odd. Squid should always be sending requests as HTTP/1.1.
>
> Have a look at the debug level "11,2" cache.log records to see if Squid
> is actually sending 1.0 or if it is just relaying CONNECT requests with
> possibly HTTP/1.0 inside.
>
Hello,

here are the debug logs (IP addresses redacted) after connection attempt 
to https://samba.org/ :

----------
2023/11/27 09:58:07.345 kid1| 11,2| client_side.cc(1332) 
parseHttpRequest: HTTP Client conn21570 local=195.113.x.y:3128 
remote=10.10.a.b:53868 FD 666 flags=1
2023/11/27 09:58:07.345 kid1| 11,2| client_side.cc(1336) 
parseHttpRequest: HTTP Client REQUEST:
---------
CONNECT samba.org:443 HTTP/1.1
Host: samba.org:443
Proxy-Connection: keep-alive
User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, 
like Gecko) Chrome/119.0.0.0 Safari/537.36


----------
2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(273) sendStartOfMessage: 
HTTP Client conn21576 local=195.113.x.y:3128 remote=10.10.a.b:16730 FD 
1267 flags=1
2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(274) sendStartOfMessage: 
HTTP Client REPLY:
---------
HTTP/1.1 400 Bad Request
Server: squid/6.5
Mime-Version: 1.0
Date: Mon, 27 Nov 2023 08:58:07 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3363
X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
Cache-Status: pteryx.natur.cuni.cz
Via: 1.1 pteryx.natur.cuni.cz (squid/6.5)
Connection: close

So, it seems it's not true that squid is using http/1.0, but the guy on 
the other side told me so. According to the log, do you think I can 
somehow make it working or is it definitely problem on the samba.org 
webserver?

Thanks again,

 ? David


> HTH
> Amos
>


From ottmihkel at mac.com  Mon Nov 27 09:38:00 2023
From: ottmihkel at mac.com (Mihkel Tammepuu)
Date: Mon, 27 Nov 2023 11:38:00 +0200
Subject: [squid-users] Https from sibling peers does not work
Message-ID: <5A07D46E-89D9-457C-A8C3-6B154F4B4B1A@mac.com>

Hello!
I am trying to set up a sibling cluster of 4 Squid instances. The purpose of the cluster is redundancy AND sharing cache disk space.
Everything seems to work fine with http, but with https I cannot see requests being forwarded to siblings.
Interestingly, when using HTCP, the siblings do get HTCP_CLR requests, but not HTCP_TST requests and https content is NOT loaded from sibling even if it?s clearly present there.
I?m of course using SSL Bump, content from origin servers works fine. I?ve tried Squid 6.5 and 5.9 with same results.
What might be wrong? Any way to fix it?

Best regards,
Mihkel Tammepuu

From squid3 at treenet.co.nz  Mon Nov 27 09:40:00 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Nov 2023 22:40:00 +1300
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>
References: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
 <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>
Message-ID: <0a1b1097-9c02-4d54-a8dd-69a694b72fd8@treenet.co.nz>

On 27/11/23 22:21, David Komanek wrote:
> here are the debug logs (IP addresses redacted) after connection attempt 
> to https://samba.org/ :
> 
...
> 2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(274) sendStartOfMessage: 
> HTTP Client REPLY:
> ---------
> HTTP/1.1 400 Bad Request
> Server: squid/6.5
> Mime-Version: 1.0
> Date: Mon, 27 Nov 2023 08:58:07 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3363
> X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
> Cache-Status: pteryx.natur.cuni.cz
> Via: 1.1 pteryx.natur.cuni.cz (squid/6.5)
> Connection: close
> 
> So, it seems it's not true that squid is using http/1.0, but the guy on 
> the other side told me so. According to the log, do you think I can 
> somehow make it working or is it definitely problem on the samba.org 
> webserver?


That ERR_PROTOCOL_UNKNOWN indicates that your proxy is trying to 
SSL-Bump the CONNECT tunnel and not understanding the protocol inside 
the TLS layer - which is expected if that protocol is HTTP/2.


For now you should be able to use 
<http://www.squid-cache.org/Doc/config/on_unsupported_protocol/> to 
allow these tunnels. Alternatively use the "splice" action to explicitly 
bypass the SSL-Bump process.


HTH
Amos


From david.komanek at natur.cuni.cz  Mon Nov 27 10:05:09 2023
From: david.komanek at natur.cuni.cz (David Komanek)
Date: Mon, 27 Nov 2023 11:05:09 +0100
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <0a1b1097-9c02-4d54-a8dd-69a694b72fd8@treenet.co.nz>
References: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
 <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>
 <0a1b1097-9c02-4d54-a8dd-69a694b72fd8@treenet.co.nz>
Message-ID: <27d8051e-2e97-46fa-80cd-519735820683@natur.cuni.cz>


On 11/27/23 10:40, Amos Jeffries wrote:
> On 27/11/23 22:21, David Komanek wrote:
>> here are the debug logs (IP addresses redacted) after connection 
>> attempt to https://samba.org/ :
>>
> ...
>> 2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(274) 
>> sendStartOfMessage: HTTP Client REPLY:
>> ---------
>> HTTP/1.1 400 Bad Request
>> Server: squid/6.5
>> Mime-Version: 1.0
>> Date: Mon, 27 Nov 2023 08:58:07 GMT
>> Content-Type: text/html;charset=utf-8
>> Content-Length: 3363
>> X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
>> Cache-Status: pteryx.natur.cuni.cz
>> Via: 1.1 pteryx.natur.cuni.cz (squid/6.5)
>> Connection: close
>>
>> So, it seems it's not true that squid is using http/1.0, but the guy 
>> on the other side told me so. According to the log, do you think I 
>> can somehow make it working or is it definitely problem on the 
>> samba.org webserver?
>
>
> That ERR_PROTOCOL_UNKNOWN indicates that your proxy is trying to 
> SSL-Bump the CONNECT tunnel and not understanding the protocol inside 
> the TLS layer - which is expected if that protocol is HTTP/2.
>
>
> For now you should be able to use 
> <http://www.squid-cache.org/Doc/config/on_unsupported_protocol/> to 
> allow these tunnels. Alternatively use the "splice" action to 
> explicitly bypass the SSL-Bump process.


Thank you for the quick response. So I should add

acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN
on_unsupported_protocol tunnel foreignProtocol

to the squid.conf, right?


Still, I don't understand, why is this case handled by my browsers (or 
squid?) differently from usual HTTPS traffic to other sites. I suppose 
that plenty of sites are accepting HTTP/2 nowadays. A huge lack of 
knowledge on my side :-)


Sincerely,

 ? David




From squid3 at treenet.co.nz  Mon Nov 27 10:11:04 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Nov 2023 23:11:04 +1300
Subject: [squid-users] Intercepted connections are not bumped
In-Reply-To: <59683041-f333-4182-9e91-0bc5474abdc1@netfence.it>
References: <59683041-f333-4182-9e91-0bc5474abdc1@netfence.it>
Message-ID: <e1510cf2-92c3-4645-a80f-01e96b5bd9dc@treenet.co.nz>

On 23/11/23 23:05, Andrea Venturoli wrote:
> Hello.
> 
> I've got the following config:
> 
>> ...
>> http_port 8080 ssl-bump cert=/usr/local/etc/squid/proxyCA.pem 
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> https_port 3129 intercept ssl-bump 
>> cert=/usr/local/etc/squid/proxyCA.pem generate-host-certificates=on 
>> dynamic_cert_mem_cache_size=4MB
>> ...
>> acl step1 at_step SslBump1
>> ssl_bump splice !bumphosts
>> ssl_bump splice splicedom
>> ssl_bump peek step1
>> ssl_bump bump all
>> ...
> 
> So I've got port 8080 where proxy-aware client connect and 3129, which 
> is feeded intercepted https connection by ipfw.
> 
> Problem is: if a client connects explicitly via proxy (port 8080) it 
> gets SSLBumped; if a client simply connects to its destination https 
> port (so directed to 3129) it is tunneled.
> 
> Anything wrong in my config?


FYI, Intercepted traffic first gets interpreted as a CONNECT tunnel to 
the TCP dst-IP:port and processed by http_access to see if the client is 
allowed to make that type of connection.

To guess based on the info provided above I suspect that the 
fake-CONNECT raw-IP does not match your "bumphosts" ACL test. Causing 
that "ssl_bump splice !bumphosts" to occur.

That behaviour is why we typically recommend doing "peek" first, then 
the splice checks can be based on whatever TLS SNI value is found.


For further assistance please also show your http_access and ACL config 
lines. They will be needed for a better analysis of what is going on.




> I think it worked in the past: has anything changed in this regard with 
> Squid 6?


Changed since what version? Over time a lot of small changes can add up 
to large differences.


HTH
Amos


From squid3 at treenet.co.nz  Mon Nov 27 10:29:23 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Nov 2023 23:29:23 +1300
Subject: [squid-users] Https from sibling peers does not work
In-Reply-To: <5A07D46E-89D9-457C-A8C3-6B154F4B4B1A@mac.com>
References: <5A07D46E-89D9-457C-A8C3-6B154F4B4B1A@mac.com>
Message-ID: <8783c476-7389-4700-a872-fcb12a03a649@treenet.co.nz>

On 27/11/23 22:38, Mihkel Tammepuu wrote:
> Hello!
> I am trying to set up a sibling cluster of 4 Squid instances. The purpose of the cluster is redundancy AND sharing cache disk space.


FWIW, if these are running on the same machine you may find SMP workers 
with rock type cache_dir easier to manage and more efficient with the 
caching than a traditional cluster.



> Everything seems to work fine with http, but with https I cannot see requests being forwarded to siblings.
> Interestingly, when using HTCP, the siblings do get HTCP_CLR requests, but not HTCP_TST requests and https content is NOT loaded from sibling even if it?s clearly present there.
> I?m of course using SSL Bump, content from origin servers works fine. I?ve tried Squid 6.5 and 5.9 with same results.
> What might be wrong? Any way to fix it?
> 

I assume/suspect you have the traditional cache_peer setup without TLS 
between them.

Squid intentionally does not send decrypted HTTPS traffic over non-TLS 
connections. That includes your cache_peer.

Try adding the "tls" option to your cache_peer lines and ensure they all 
use https_port listening in forward-proxy mode to receive that traffic.


If you need more assistance, please show what your config is. We will 
need the specific details of that to see if any other changes are useful 
and/or advise on further troubleshooting.


HTH
Amos


From squid3 at treenet.co.nz  Mon Nov 27 10:36:43 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Nov 2023 23:36:43 +1300
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <27d8051e-2e97-46fa-80cd-519735820683@natur.cuni.cz>
References: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
 <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>
 <0a1b1097-9c02-4d54-a8dd-69a694b72fd8@treenet.co.nz>
 <27d8051e-2e97-46fa-80cd-519735820683@natur.cuni.cz>
Message-ID: <29243557-64e2-4979-95bf-e3213122c12b@treenet.co.nz>


On 27/11/23 23:05, David Komanek wrote:
> 
> On 11/27/23 10:40, Amos Jeffries wrote:
>> On 27/11/23 22:21, David Komanek wrote:
>>> here are the debug logs (IP addresses redacted) after connection 
>>> attempt to https://samba.org/ :
>>>
>> ...
>>> 2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(274) 
>>> sendStartOfMessage: HTTP Client REPLY:
>>> ---------
>>> HTTP/1.1 400 Bad Request
>>> Server: squid/6.5
>>> Mime-Version: 1.0
>>> Date: Mon, 27 Nov 2023 08:58:07 GMT
>>> Content-Type: text/html;charset=utf-8
>>> Content-Length: 3363
>>> X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
>>> Cache-Status: pteryx.natur.cuni.cz
>>> Via: 1.1 pteryx.natur.cuni.cz (squid/6.5)
>>> Connection: close
>>>
>>> So, it seems it's not true that squid is using http/1.0, but the guy 
>>> on the other side told me so. According to the log, do you think I 
>>> can somehow make it working or is it definitely problem on the 
>>> samba.org webserver?
>>
>>
>> That ERR_PROTOCOL_UNKNOWN indicates that your proxy is trying to 
>> SSL-Bump the CONNECT tunnel and not understanding the protocol inside 
>> the TLS layer - which is expected if that protocol is HTTP/2.
>>
>>
>> For now you should be able to use 
>> <http://www.squid-cache.org/Doc/config/on_unsupported_protocol/> to 
>> allow these tunnels. Alternatively use the "splice" action to 
>> explicitly bypass the SSL-Bump process.
> 
> 
> Thank you for the quick response. So I should add
> 
> acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN
> on_unsupported_protocol tunnel foreignProtocol
> 
> to the squid.conf, right?

At the point the error exists is too late AFAIK.

I was thinking something like:
   acl foo dstdomain samba.org
   on_unsupported_protocol tunnel foo



> 
> Still, I don't understand, why is this case handled by my browsers (or 
> squid?) differently from usual HTTPS traffic to other sites. I suppose 
> that plenty of sites are accepting HTTP/2 nowadays. A huge lack of 
> knowledge on my side :-)

I'm not clear exactly why you see this only now, and only with 
samba.org. Squid not supporting HTTP/2 yet is a big part of the problem 
though.


Cheers
Amos


From david.komanek at natur.cuni.cz  Mon Nov 27 15:46:50 2023
From: david.komanek at natur.cuni.cz (David Komanek)
Date: Mon, 27 Nov 2023 16:46:50 +0100
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <29243557-64e2-4979-95bf-e3213122c12b@treenet.co.nz>
References: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
 <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>
 <0a1b1097-9c02-4d54-a8dd-69a694b72fd8@treenet.co.nz>
 <27d8051e-2e97-46fa-80cd-519735820683@natur.cuni.cz>
 <29243557-64e2-4979-95bf-e3213122c12b@treenet.co.nz>
Message-ID: <3900b379-a903-4a01-8dd5-883449a4be91@natur.cuni.cz>


On 11/27/23 11:36, Amos Jeffries wrote:
>
> On 27/11/23 23:05, David Komanek wrote:
>>
>> On 11/27/23 10:40, Amos Jeffries wrote:
>>> On 27/11/23 22:21, David Komanek wrote:
>>>> here are the debug logs (IP addresses redacted) after connection 
>>>> attempt to https://samba.org/ :
>>>>
>>> ...
>>>> 2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(274) 
>>>> sendStartOfMessage: HTTP Client REPLY:
>>>> ---------
>>>> HTTP/1.1 400 Bad Request
>>>> Server: squid/6.5
>>>> Mime-Version: 1.0
>>>> Date: Mon, 27 Nov 2023 08:58:07 GMT
>>>> Content-Type: text/html;charset=utf-8
>>>> Content-Length: 3363
>>>> X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
>>>> Cache-Status: pteryx.natur.cuni.cz
>>>> Via: 1.1 pteryx.natur.cuni.cz (squid/6.5)
>>>> Connection: close
>>>>
>>>> So, it seems it's not true that squid is using http/1.0, but the 
>>>> guy on the other side told me so. According to the log, do you 
>>>> think I can somehow make it working or is it definitely problem on 
>>>> the samba.org webserver?
>>>
>>>
>>> That ERR_PROTOCOL_UNKNOWN indicates that your proxy is trying to 
>>> SSL-Bump the CONNECT tunnel and not understanding the protocol 
>>> inside the TLS layer - which is expected if that protocol is HTTP/2.
>>>
>>>
>>> For now you should be able to use 
>>> <http://www.squid-cache.org/Doc/config/on_unsupported_protocol/> to 
>>> allow these tunnels. Alternatively use the "splice" action to 
>>> explicitly bypass the SSL-Bump process.
>>
>>
>> Thank you for the quick response. So I should add
>>
>> acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN
>> on_unsupported_protocol tunnel foreignProtocol
>>
>> to the squid.conf, right?

doesn't work

> At the point the error exists is too late AFAIK.
>
> I was thinking something like:
> ? acl foo dstdomain samba.org
> ? on_unsupported_protocol tunnel foo
>
doesn't work either


Redards,
David

>
>>
>> Still, I don't understand, why is this case handled by my browsers 
>> (or squid?) differently from usual HTTPS traffic to other sites. I 
>> suppose that plenty of sites are accepting HTTP/2 nowadays. A huge 
>> lack of knowledge on my side :-)
>
> I'm not clear exactly why you see this only now, and only with 
> samba.org. Squid not supporting HTTP/2 yet is a big part of the 
> problem though.
>
>
> Cheers
> Amos


From ml at netfence.it  Mon Nov 27 15:59:11 2023
From: ml at netfence.it (Andrea Venturoli)
Date: Mon, 27 Nov 2023 16:59:11 +0100
Subject: [squid-users] Intercepted connections are not bumped
In-Reply-To: <e1510cf2-92c3-4645-a80f-01e96b5bd9dc@treenet.co.nz>
References: <59683041-f333-4182-9e91-0bc5474abdc1@netfence.it>
 <e1510cf2-92c3-4645-a80f-01e96b5bd9dc@treenet.co.nz>
Message-ID: <6ba031f3-ac28-4142-a4de-e23c84b55109@netfence.it>

On 11/27/23 11:11, Amos Jeffries wrote:


First off, thanks for answering.



> For further assistance please also show your http_access and ACL config 
> lines. They will be needed for a better analysis of what is going on.

I'll start from here.
It's quite long, but a reduced example is:

acl localnet src 10.1.2.0/24
acl bumphosts src 10.1.2.18
acl SSL_ports port 443
acl SSL_ports port 563 801 3001 8443 19996 19997
acl Safe_ports port 80          # http
acl Safe_ports port 800
acl ftptraffic myportname ftpport
acl fetched_certificate transaction_initiator certificate-fetching
acl splicedom ssl::server_name_regex -i "/usr/local/etc/squid/nobumpsites"
acl step1 at_step SslBump1
ssl_bump splice !bumphosts
ssl_bump splice splicedom
ssl_bump peek step1
ssl_bump bump all
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny to_localhost
adaptation_access service_req deny ftptraffic
adaptation_access service_resp deny ftptraffic
http_access allow localnet
http_access allow localhost


For the sake of an example, let's say I connect from 10.1.2.18 to 
www.google.com.



> FYI, Intercepted traffic first gets interpreted as a CONNECT tunnel to 
> the TCP dst-IP:port and processed by http_access to see if the client is 
> allowed to make that type of connection.

Fine.
Traffic is in fact allowed.



> To guess based on the info provided above I suspect that the 
> fake-CONNECT raw-IP does not match your "bumphosts" ACL test. Causing 
> that "ssl_bump splice !bumphosts" to occur.

Not sure I understand what you mean: is raw-IP the source (in my case 
10.1.2.18) or the destination IP (142.251.209.36)?

"bumphosts" ACLs are local clients (those that SSLBump should be applied 
to): 10.1.2.18 is in this list (in fact it gets SSLBump if explicitly 
using the proxy).



This is what I see in the logs for an intercepted connection (after it's 
closed):

> 1701100166.601   2203 10.1.2.18 TCP_TUNNEL/500 6622 CONNECT 142.251.209.36:443 - ORIGINAL_DST/142.251.209.36 -



This is what I see using a proxy-aware application:

> 1701100243.374    172 10.1.2.18 TCP_MISS/200 49333 GET https://www.google.com/? - HIER_DIRECT/142.251.209.36 text/html





> That behaviour is why we typically recommend doing "peek" first, then 
> the splice checks can be based on whatever TLS SNI value is found.

I don't think it should matter: neither www.google.com nor 
142.251.209.36 are in any ACL.
Or did I understand wrong?
Is this needed for intercepted SSLBump?



>> I think it worked in the past: has anything changed in this regard 
>> with Squid 6?
> 
> 
> Changed since what version? Over time a lot of small changes can add up 
> to large differences.

I first noticed this on 6.4.
Unfortunately I don't remember which version I was using at the time I 
set this up, maybe 5.x, maybe even 4.x.



  bye & Thanks
	av.


From david.komanek at natur.cuni.cz  Mon Nov 27 16:31:24 2023
From: david.komanek at natur.cuni.cz (David Komanek)
Date: Mon, 27 Nov 2023 17:31:24 +0100
Subject: [squid-users] how to avoid use http/1.0 between squid and the
 target
In-Reply-To: <29243557-64e2-4979-95bf-e3213122c12b@treenet.co.nz>
References: <mailman.3.1700740802.1017745.squid-users@lists.squid-cache.org>
 <001714af-94cf-4210-a920-0fc9032412e5@natur.cuni.cz>
 <0a1b1097-9c02-4d54-a8dd-69a694b72fd8@treenet.co.nz>
 <27d8051e-2e97-46fa-80cd-519735820683@natur.cuni.cz>
 <29243557-64e2-4979-95bf-e3213122c12b@treenet.co.nz>
Message-ID: <6890b3ef-74f9-46ca-b07b-3d6c4b7b048e@natur.cuni.cz>


On 11/27/23 11:36, Amos Jeffries wrote:
>
> On 27/11/23 23:05, David Komanek wrote:
>>
>> On 11/27/23 10:40, Amos Jeffries wrote:
>>> On 27/11/23 22:21, David Komanek wrote:
>>>> here are the debug logs (IP addresses redacted) after connection 
>>>> attempt to https://samba.org/ :
>>>>
>>> ...
>>>> 2023/11/27 09:58:07.370 kid1| 11,2| Stream.cc(274) 
>>>> sendStartOfMessage: HTTP Client REPLY:
>>>> ---------
>>>> HTTP/1.1 400 Bad Request
>>>> Server: squid/6.5
>>>> Mime-Version: 1.0
>>>> Date: Mon, 27 Nov 2023 08:58:07 GMT
>>>> Content-Type: text/html;charset=utf-8
>>>> Content-Length: 3363
>>>> X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
>>>> Cache-Status: pteryx.natur.cuni.cz
>>>> Via: 1.1 pteryx.natur.cuni.cz (squid/6.5)
>>>> Connection: close
>>>>
>>>> So, it seems it's not true that squid is using http/1.0, but the 
>>>> guy on the other side told me so. According to the log, do you 
>>>> think I can somehow make it working or is it definitely problem on 
>>>> the samba.org webserver?
>>>
>>>
>>> That ERR_PROTOCOL_UNKNOWN indicates that your proxy is trying to 
>>> SSL-Bump the CONNECT tunnel and not understanding the protocol 
>>> inside the TLS layer - which is expected if that protocol is HTTP/2.
>>>
>>>
>>> For now you should be able to use 
>>> <http://www.squid-cache.org/Doc/config/on_unsupported_protocol/> to 
>>> allow these tunnels. Alternatively use the "splice" action to 
>>> explicitly bypass the SSL-Bump process.
>>
>>
>> Thank you for the quick response. So I should add
>>
>> acl foreignProtocol squid_error ERR_PROTOCOL_UNKNOWN
>> on_unsupported_protocol tunnel foreignProtocol
>>
>> to the squid.conf, right?
>
> At the point the error exists is too late AFAIK.
>
> I was thinking something like:
> ? acl foo dstdomain samba.org
> ? on_unsupported_protocol tunnel foo
>
>
>
>>
>> Still, I don't understand, why is this case handled by my browsers 
>> (or squid?) differently from usual HTTPS traffic to other sites. I 
>> suppose that plenty of sites are accepting HTTP/2 nowadays. A huge 
>> lack of knowledge on my side :-)
>
> I'm not clear exactly why you see this only now, and only with 
> samba.org. Squid not supporting HTTP/2 yet is a big part of the 
> problem though.
>
>
> Cheers
> Amos


Hello,

I managed to google some options for curl useful in this context, and it 
is quite interesting:

working: curl -vvvv --http2 -x cache.my.domain:3128 https://www.samba.org/

working: curl -vvvv --http1.1 -x cache.my.domain:3128 https://www.samba.org/

rejected by samba.org: curl -vvvv --http1.0 -x cache.my.domain:3128 
https://www.samba.org/
 ??? this returns a simple html page with code 403:
 ????????? <html><body><h1>403 Forbidden</h1>
 ????????? Request forbidden by administrative rules.
 ???????? </body></html>

not working: chrome, firefox via proxy
 ?? chrome returns "ERR_CONNECTION_CLOSED"
 ?? firefox returns "PR_END_OF_FILE_ERROR"

So, it seems to me, there squid doesn't like something with the 
heavy-duty browsers in this case. Even if I disable http/2 in firefox, 
it makes no difference for me. I'm really confused.

Best regards,
David




From mario.theodoridis at regify.com  Tue Nov 28 10:29:48 2023
From: mario.theodoridis at regify.com (Mario Theodoridis)
Date: Tue, 28 Nov 2023 11:29:48 +0100
Subject: [squid-users] SSL Virtual Hosting Problem
Message-ID: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>

Hello everyone,

i'm trying to use squid as a TLS virtual hosting proxy on a system with 
a public IP in front of several internal systems running TLS web servers.

I would like to proxy the incoming connections to the appropriate 
backend servers based on the hostname using SNI.

I'm using the following config to just try this with 1 backend to test 
with and fail already

Here the config:

http_port 3128
debug_options ALL,2
pinger_enable off
shutdown_lifetime 1 second
https_port 0.0.0.0:443 tproxy ssl-bump tls-cert=/root/dummy.pem
acl tlspls ssl::server_name_regex -i test\.regify\.com
cache_peer test.de.regify.com parent 443 0 proxy-only originserver 
no-digest no-netdb-exchange name=test
ssl_bump peek all
ssl_bump splice all
http_access allow all
cache_peer_access test allow all


Starting squid gives me the following:

2023/11/28 11:13:21.919| 1,2| main.cc(1619) SquidMain: Doing post-config 
initialization
2023/11/28 11:13:21.919| 1,2| main.cc(1621) SquidMain: running 
RegisteredRunner::finalizeConfig
2023/11/28 11:13:21.919| Created PID file (/run/squid.pid)
2023/11/28 11:13:21.921| 1,2| main.cc(1453) StartUsingConfig: running 
RegisteredRunner::claimMemoryNeeds
2023/11/28 11:13:21.921| 1,2| main.cc(1454) StartUsingConfig: running 
RegisteredRunner::useConfig
2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1619) SquidMain: Doing 
post-config initialization
2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1621) SquidMain: running 
RegisteredRunner::finalizeConfig
2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1453) StartUsingConfig: 
running RegisteredRunner::claimMemoryNeeds
2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1454) StartUsingConfig: 
running RegisteredRunner::useConfig
2023/11/28 11:13:21.988 kid1| Current Directory is /
2023/11/28 11:13:21.988 kid1| Creating missing swap directories
2023/11/28 11:13:21.988 kid1| No cache_dir stores are configured.
2023/11/28 11:13:21.992| 1,2| main.cc(2051) watch_child: running 
RegisteredRunner::finishShutdown
2023/11/28 11:13:21.992| Removing PID file (/run/squid.pid)
2023/11/28 11:13:22.063| 1,2| main.cc(1619) SquidMain: Doing post-config 
initialization
2023/11/28 11:13:22.063| 1,2| main.cc(1621) SquidMain: running 
RegisteredRunner::finalizeConfig
2023/11/28 11:13:22.063| Created PID file (/run/squid.pid)
2023/11/28 11:13:22.066| 1,2| main.cc(1453) StartUsingConfig: running 
RegisteredRunner::claimMemoryNeeds
2023/11/28 11:13:22.066| 1,2| main.cc(1454) StartUsingConfig: running 
RegisteredRunner::useConfig
2023/11/28 11:13:22.131 kid1| 1,2| main.cc(1619) SquidMain: Doing 
post-config initialization
2023/11/28 11:13:22.132 kid1| 1,2| main.cc(1621) SquidMain: running 
RegisteredRunner::finalizeConfig
2023/11/28 11:13:22.132 kid1| 1,2| main.cc(1453) StartUsingConfig: 
running RegisteredRunner::claimMemoryNeeds
2023/11/28 11:13:22.132 kid1| 1,2| main.cc(1454) StartUsingConfig: 
running RegisteredRunner::useConfig
2023/11/28 11:13:22.132 kid1| Current Directory is /
2023/11/28 11:13:22.132 kid1| Starting Squid Cache version 4.13 for 
x86_64-pc-linux-gnu...
2023/11/28 11:13:22.132 kid1| Service Name: squid
2023/11/28 11:13:22.132 kid1| Process ID 2863502
2023/11/28 11:13:22.132 kid1| Process Roles: worker
2023/11/28 11:13:22.132 kid1| With 1024 file descriptors available
2023/11/28 11:13:22.132 kid1| Initializing IP Cache...
2023/11/28 11:13:22.135 kid1| 78,2| dns_internal.cc(1570) Init: 
idnsInit: attempt open DNS socket to: 0.0.0.0
2023/11/28 11:13:22.135 kid1| DNS Socket created at 0.0.0.0, FD 5
2023/11/28 11:13:22.135 kid1| Adding domain de.regify.com from 
/etc/resolv.conf
2023/11/28 11:13:22.135 kid1| Adding nameserver 192.168.1.1 from 
/etc/resolv.conf
2023/11/28 11:13:22.135 kid1| helperOpenServers: Starting 5/32 
'security_file_certgen' processes
2023/11/28 11:13:22.164 kid1| 46,2| Format.cc(71) parse: got definition 
'%>a/%>A %un %>rm myip=%la myport=%lp'
2023/11/28 11:13:22.165 kid1| 46,2| Format.cc(71) parse: got definition 
'%>a/%>A %un %>rm myip=%la myport=%lp'
2023/11/28 11:13:22.165 kid1| Logfile: opening log 
daemon:/var/log/squid/access.log
2023/11/28 11:13:22.165 kid1| Logfile Daemon: opening log 
/var/log/squid/access.log
2023/11/28 11:13:22.194 kid1| 71,2| store_digest.cc(96) 
storeDigestCalcCap: have: 0, want 0 entries; limits: [1, 0]
2023/11/28 11:13:22.194 kid1| 70,2| CacheDigest.cc(46) init: capacity: 1 
entries, bpe: ; size: 1 bytes
2023/11/28 11:13:22.194 kid1| Local cache digest enabled; 
rebuild/rewrite every 3600/3600 sec
2023/11/28 11:13:22.194 kid1| Store logging disabled
2023/11/28 11:13:22.194 kid1| Swap maxSize 0 + 262144 KB, estimated 
20164 objects
2023/11/28 11:13:22.194 kid1| Target number of buckets: 1008
2023/11/28 11:13:22.194 kid1| Using 8192 Store buckets
2023/11/28 11:13:22.194 kid1| Max Mem? size: 262144 KB
2023/11/28 11:13:22.194 kid1| Max Swap size: 0 KB
2023/11/28 11:13:22.194 kid1| Using Least Load store dir selection
2023/11/28 11:13:22.194 kid1| Current Directory is /
2023/11/28 11:13:22.194 kid1| Finished loading MIME types and icons.
2023/11/28 11:13:22.332 kid1| 80,2| wccp.cc(113) wccpConnectionOpen: 
WCCPv1 disabled.
2023/11/28 11:13:22.332 kid1| 80,2| wccp2.cc(959) wccp2ConnectionOpen: 
WCCPv2 Disabled. No IPv4 Router(s) configured.
2023/11/28 11:13:22.332 kid1| 33,2| AsyncCall.cc(25) AsyncCall: The 
AsyncCall clientListenerConnectionOpened constructed, 
this=0x5636c42036d0 [call18]
2023/11/28 11:13:22.333 kid1| 33,2| AsyncCall.cc(92) ScheduleCall: 
StartListening.cc(59) will call 
clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] FD 22 
flags=9, err=0, HTTP Socket port=0x5636c4203730) [call18]
2023/11/28 11:13:22.333 kid1| 33,2| AsyncCall.cc(25) AsyncCall: The 
AsyncCall clientListenerConnectionOpened constructed, 
this=0x5636c420ca50 [call20]
2023/11/28 11:13:22.337 kid1| 33,2| AsyncCall.cc(92) ScheduleCall: 
StartListening.cc(59) will call 
clientListenerConnectionOpened(local=0.0.0.0:443 remote=[::] FD 23 
flags=25, err=0, HTTPS Socket port=0x5636c420cab0) [call20]
2023/11/28 11:13:22.337 kid1| HTCP Disabled.
2023/11/28 11:13:22.337 kid1| Squid plugin modules loaded: 0
2023/11/28 11:13:22.337 kid1| Adaptation support is off.
2023/11/28 11:13:22.338 kid1| 93,2| Config.cc(224) FinalizeEach: 
Initialized 0 message adaptation services
2023/11/28 11:13:22.338 kid1| 93,2| Config.cc(224) FinalizeEach: 
Initialized 0 message adaptation service groups
2023/11/28 11:13:22.338 kid1| 93,2| Config.cc(224) FinalizeEach: 
Initialized 0 message adaptation access rules
2023/11/28 11:13:22.339 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: 
entering clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] 
FD 22 flags=9, err=0, HTTP Socket port=0x5636c4203730)
2023/11/28 11:13:22.339 kid1| 33,2| AsyncCall.cc(37) make: make call 
clientListenerConnectionOpened [call18]
2023/11/28 11:13:22.339 kid1| Accepting HTTP Socket connections at 
local=0.0.0.0:3128 remote=[::] FD 22 flags=9
2023/11/28 11:13:22.346 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: 
leaving clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] FD 
22 flags=9, err=0, HTTP Socket port=0x5636c4203730)
2023/11/28 11:13:22.346 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: 
entering clientListenerConnectionOpened(local=0.0.0.0:443 remote=[::] FD 
23 flags=25, err=0, HTTPS Socket port=0x5636c420cab0)
2023/11/28 11:13:22.346 kid1| 33,2| AsyncCall.cc(37) make: make call 
clientListenerConnectionOpened [call20]
2023/11/28 11:13:22.346 kid1| Accepting TPROXY intercepted SSL bumped 
HTTPS Socket connections at local=0.0.0.0:443 remote=[::] FD 23 flags=25
2023/11/28 11:13:22.352 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: 
leaving clientListenerConnectionOpened(local=0.0.0.0:443 remote=[::] FD 
23 flags=25, err=0, HTTPS Socket port=0x5636c420cab0)
2023/11/28 11:13:22.352 kid1| Configuring Parent test.de.regify.com/443/0
2023/11/28 11:13:22.353 kid1| 15,2| neighbors.cc(1198) peerDNSConfigure: 
--> IP address #0: 192.168.1.122
2023/11/28 11:13:22.368 kid1| 15,2| neighbors.cc(1272) 
peerConnectSucceded: TCP connection to test.de.regify.com/443 succeeded
2023/11/28 11:13:23 kid1| storeLateRelease: released 0 objects


Then when i call curl -k https://test.regify.com/

i get

The requested URL could not be retrieved

And the log has the following:


2023/11/28 11:15:05.467 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New 
connection on FD 23
2023/11/28 11:15:05.467 kid1| 5,2| TcpAcceptor.cc(312) acceptNext: 
connection on local=0.0.0.0:443 remote=[::] FD 23 flags=25
2023/11/28 11:15:05.467 kid1| 17,2| QosConfig.cc(125) 
getNfmarkFromConnection: QOS: Failed to retrieve connection mark: (-1) 
(2) No such file or directory (Destination 192.168.1.132:443, source 
192.168.1.124:60690)
2023/11/28 11:15:05.468 kid1| 33,2| client_side.cc(2742) 
httpsSslBumpAccessCheckDone: sslBump action peekneeded for 
local=192.168.1.132:443 remote=192.168.1.124:60690 FD 11 flags=17
2023/11/28 11:15:05.468 kid1| 33,2| client_side.cc(3418) 
fakeAConnectRequest: fake a CONNECT request to force connState to tunnel 
for ssl-bump
2023/11/28 11:15:05.468 kid1| 85,2| client_side_request.cc(751) 
clientAccessCheckDone: The request CONNECT 192.168.1.132:443 is ALLOWED; 
last ACL checked: all
2023/11/28 11:15:05.468 kid1| 85,2| client_side_request.cc(729) 
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2023/11/28 11:15:05.468 kid1| 85,2| client_side_request.cc(751) 
clientAccessCheckDone: The request CONNECT 192.168.1.132:443 is ALLOWED; 
last ACL checked: all
2023/11/28 11:15:05.483 kid1| 17,2| FwdState.cc(142) FwdState: 
Forwarding client request local=192.168.1.132:443 
remote=192.168.1.124:60690 FD 11 flags=17, url=192.168.1.132:443
2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(316) 
peerSelectDnsPaths: Found sources for '192.168.1.132:443'
2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(317) 
peerSelectDnsPaths:?? always_direct = DENIED
2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(318) 
peerSelectDnsPaths:??? never_direct = DENIED
2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(324) 
peerSelectDnsPaths:??? ORIGINAL_DST = local=192.168.1.124 
remote=192.168.1.132:443 flags=25
2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(331) 
peerSelectDnsPaths:??????? timedout = 0
2023/11/28 11:16:05.433 kid1| 4,2| errorpage.cc(1259) BuildContent: No 
existing error page language negotiated for ERR_CONNECT_FAIL. Using 
default error file.
2023/11/28 11:16:05.433 kid1| 20,2| store.cc(985) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2023/11/28 11:16:05.433 kid1| 20,2| store.cc(985) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2023/11/28 11:16:05.463 kid1| 83,2| client_side.cc(2675) 
clientNegotiateSSL: New session 0x5636c4227330 on FD 11 
(192.168.1.124:60690)
2023/11/28 11:16:05.464 kid1| 11,2| client_side.cc(1306) 
parseHttpRequest: HTTP Client local=192.168.1.132:443 
remote=192.168.1.124:60690 FD 11 flags=17
2023/11/28 11:16:05.464 kid1| 11,2| client_side.cc(1307) 
parseHttpRequest: HTTP Client REQUEST:
---------
GET / HTTP/1.1
Host: test.regify.com
User-Agent: curl/7.74.0
Accept: */*


----------
2023/11/28 11:16:05.464 kid1| 88,2| client_side_reply.cc(2062) 
processReplyAccessResult: The reply for GET https://test.regify.com/ is 
ALLOWED, because it matched (access_log daemon:/var/log/squid/access.log 
line)
2023/11/28 11:16:05.464 kid1| 11,2| Stream.cc(271) sendStartOfMessage: 
HTTP Client local=192.168.1.132:443 remote=192.168.1.124:60690 FD 11 
flags=17
2023/11/28 11:16:05.464 kid1| 11,2| Stream.cc(272) sendStartOfMessage: 
HTTP Client REPLY:
---------
HTTP/1.1 503 Service Unavailable
Server: squid/4.13
Mime-Version: 1.0
Date: Tue, 28 Nov 2023 10:16:05 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3487
X-Squid-Error: ERR_CONNECT_FAIL 110
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from proxy
X-Cache-Lookup: NONE from proxy:3128
Via: 1.1 proxy (squid/4.13)
Connection: close


----------
2023/11/28 11:16:05.464 kid1| 20,2| store.cc(985) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2023/11/28 11:16:05.464 kid1| 33,2| client_side.cc(895) kick: 
local=192.168.1.132:443 remote=192.168.1.124:60690 flags=17 Connection 
was closed
2023/11/28 11:16:05.464 kid1| 33,2| client_side.cc(586) swanSong: 
local=192.168.1.132:443 remote=192.168.1.124:60690 flags=17
2023/11/28 11:16:05.465 kid1| 20,2| store.cc(985) checkCachable: 
StoreEntry::checkCachable: NO: not cachable

I've been reading the squid docs and other internet resources, but am 
failing to figure out why this is not working.

Any clue sticks would be appreciated.

Also appreciated would be advise on where to find this documented.


-- 
Mit Freundlichen Gr??en / Kind regards

Mario Theodoridis

regify GmbH
R?merstrasse 39 | D-78183 H?fingen-Behla
Amtsgericht Freiburg HRB 709343
Telefon: +49 771 8978 4238



From squid3 at treenet.co.nz  Tue Nov 28 13:57:32 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Nov 2023 02:57:32 +1300
Subject: [squid-users] SSL Virtual Hosting Problem
In-Reply-To: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>
References: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>
Message-ID: <f68d6369-1886-448d-a808-4fffe0e75cbd@treenet.co.nz>

On 28/11/23 23:29, Mario Theodoridis wrote:
> Hello everyone,
> 
> i'm trying to use squid as a TLS virtual hosting proxy on a system with 
> a public IP in front of several internal systems running TLS web servers.
> 
> I would like to proxy the incoming connections to the appropriate 
> backend servers based on the hostname using SNI.
> 
> I'm using the following config to just try this with 1 backend to test 
> with and fail already
> 
> Here the config:
> 
> http_port 3128
> debug_options ALL,2
> pinger_enable off
> shutdown_lifetime 1 second
> https_port 0.0.0.0:443 tproxy ssl-bump tls-cert=/root/dummy.pem

That should be:

   https_port 443 accel defaultsite=example.com \
     tls-cert=/etc/squid/example.com.pem

The PEM file needs to be valid for all the domains served.


> acl tlspls ssl::server_name_regex -i test\.regify\.com
> cache_peer test.de.regify.com parent 443 0 proxy-only originserver 
> no-digest no-netdb-exchange name=test

Missing "tls" option to enable TLS when talking to this peer.


> ssl_bump peek all
> ssl_bump splice all
> http_access allow all
> cache_peer_access test allow all

I appreciate this is a test. But be sure to keep the default Squid 
security rules ("deny !Safe_ports" etc) and only allow the hosted 
domains instead of "all". These DoS and attack protections are 
particularly important on a reverse-proxy where the general public has 
access.

FYI; "test what you will use" is important for proxies. One of the 
"irrelevant" config details may kill your real-world production setup 
where testing works fine without any security.


> 
> ...
> I've been reading the squid docs and other internet resources, but am 
> failing to figure out why this is not working.
> 
> Any clue sticks would be appreciated.
> 
> Also appreciated would be advise on where to find this documented.
> 

The Squid wiki ConfigExamples section has all the typical configuration 
types and a few of the more uncommon ones as well.
The one you are needing is 
<https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting>


Cheers
Amos


From rousskov at measurement-factory.com  Tue Nov 28 14:24:18 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 28 Nov 2023 09:24:18 -0500
Subject: [squid-users] SSL Virtual Hosting Problem
In-Reply-To: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>
References: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>
Message-ID: <f79f3020-f40a-4da0-9fed-39cc8f48bd51@measurement-factory.com>

On 2023-11-28 05:29, Mario Theodoridis wrote:
> Hello everyone,
> 
> i'm trying to use squid as a TLS virtual hosting proxy on a system with 
> a public IP in front of several internal systems running TLS web servers.
> 
> I would like to proxy the incoming connections to the appropriate 
> backend servers based on the hostname using SNI.
> 
> I'm using the following config to just try this with 1 backend to test 
> with and fail already
> 
> Here the config:
> 
> http_port 3128
> debug_options ALL,2
> pinger_enable off
> shutdown_lifetime 1 second
> https_port 0.0.0.0:443 tproxy ssl-bump tls-cert=/root/dummy.pem
> acl tlspls ssl::server_name_regex -i test\.regify\.com
> cache_peer test.de.regify.com parent 443 0 proxy-only originserver 
> no-digest no-netdb-exchange name=test
> ssl_bump peek all
> ssl_bump splice all
> http_access allow all
> cache_peer_access test allow all


It sounds like you want all traffic to go to the configured cache_peer, 
but the above configuration has no rules specifying that request routing 
requirement. Try adding something like

     never_direct allow all
     always_direct deny all

FWIW, cache_peer_access gives permission to access a peer if that peer 
is being considered by request routing rules; it is not a requirement to 
consider a peer.


 > Also appreciated would be advise on where to find this documented.

While all squid.conf directives are documented, I am not aware of any 
high-quality web page dedicated to explaining overall request routing to 
Squid admins.


HTH,

Alex.



> Starting squid gives me the following:
> 
> 2023/11/28 11:13:21.919| 1,2| main.cc(1619) SquidMain: Doing post-config 
> initialization
> 2023/11/28 11:13:21.919| 1,2| main.cc(1621) SquidMain: running 
> RegisteredRunner::finalizeConfig
> 2023/11/28 11:13:21.919| Created PID file (/run/squid.pid)
> 2023/11/28 11:13:21.921| 1,2| main.cc(1453) StartUsingConfig: running 
> RegisteredRunner::claimMemoryNeeds
> 2023/11/28 11:13:21.921| 1,2| main.cc(1454) StartUsingConfig: running 
> RegisteredRunner::useConfig
> 2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1619) SquidMain: Doing 
> post-config initialization
> 2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1621) SquidMain: running 
> RegisteredRunner::finalizeConfig
> 2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1453) StartUsingConfig: 
> running RegisteredRunner::claimMemoryNeeds
> 2023/11/28 11:13:21.988 kid1| 1,2| main.cc(1454) StartUsingConfig: 
> running RegisteredRunner::useConfig
> 2023/11/28 11:13:21.988 kid1| Current Directory is /
> 2023/11/28 11:13:21.988 kid1| Creating missing swap directories
> 2023/11/28 11:13:21.988 kid1| No cache_dir stores are configured.
> 2023/11/28 11:13:21.992| 1,2| main.cc(2051) watch_child: running 
> RegisteredRunner::finishShutdown
> 2023/11/28 11:13:21.992| Removing PID file (/run/squid.pid)
> 2023/11/28 11:13:22.063| 1,2| main.cc(1619) SquidMain: Doing post-config 
> initialization
> 2023/11/28 11:13:22.063| 1,2| main.cc(1621) SquidMain: running 
> RegisteredRunner::finalizeConfig
> 2023/11/28 11:13:22.063| Created PID file (/run/squid.pid)
> 2023/11/28 11:13:22.066| 1,2| main.cc(1453) StartUsingConfig: running 
> RegisteredRunner::claimMemoryNeeds
> 2023/11/28 11:13:22.066| 1,2| main.cc(1454) StartUsingConfig: running 
> RegisteredRunner::useConfig
> 2023/11/28 11:13:22.131 kid1| 1,2| main.cc(1619) SquidMain: Doing 
> post-config initialization
> 2023/11/28 11:13:22.132 kid1| 1,2| main.cc(1621) SquidMain: running 
> RegisteredRunner::finalizeConfig
> 2023/11/28 11:13:22.132 kid1| 1,2| main.cc(1453) StartUsingConfig: 
> running RegisteredRunner::claimMemoryNeeds
> 2023/11/28 11:13:22.132 kid1| 1,2| main.cc(1454) StartUsingConfig: 
> running RegisteredRunner::useConfig
> 2023/11/28 11:13:22.132 kid1| Current Directory is /
> 2023/11/28 11:13:22.132 kid1| Starting Squid Cache version 4.13 for 
> x86_64-pc-linux-gnu...
> 2023/11/28 11:13:22.132 kid1| Service Name: squid
> 2023/11/28 11:13:22.132 kid1| Process ID 2863502
> 2023/11/28 11:13:22.132 kid1| Process Roles: worker
> 2023/11/28 11:13:22.132 kid1| With 1024 file descriptors available
> 2023/11/28 11:13:22.132 kid1| Initializing IP Cache...
> 2023/11/28 11:13:22.135 kid1| 78,2| dns_internal.cc(1570) Init: 
> idnsInit: attempt open DNS socket to: 0.0.0.0
> 2023/11/28 11:13:22.135 kid1| DNS Socket created at 0.0.0.0, FD 5
> 2023/11/28 11:13:22.135 kid1| Adding domain de.regify.com from 
> /etc/resolv.conf
> 2023/11/28 11:13:22.135 kid1| Adding nameserver 192.168.1.1 from 
> /etc/resolv.conf
> 2023/11/28 11:13:22.135 kid1| helperOpenServers: Starting 5/32 
> 'security_file_certgen' processes
> 2023/11/28 11:13:22.164 kid1| 46,2| Format.cc(71) parse: got definition 
> '%>a/%>A %un %>rm myip=%la myport=%lp'
> 2023/11/28 11:13:22.165 kid1| 46,2| Format.cc(71) parse: got definition 
> '%>a/%>A %un %>rm myip=%la myport=%lp'
> 2023/11/28 11:13:22.165 kid1| Logfile: opening log 
> daemon:/var/log/squid/access.log
> 2023/11/28 11:13:22.165 kid1| Logfile Daemon: opening log 
> /var/log/squid/access.log
> 2023/11/28 11:13:22.194 kid1| 71,2| store_digest.cc(96) 
> storeDigestCalcCap: have: 0, want 0 entries; limits: [1, 0]
> 2023/11/28 11:13:22.194 kid1| 70,2| CacheDigest.cc(46) init: capacity: 1 
> entries, bpe: ; size: 1 bytes
> 2023/11/28 11:13:22.194 kid1| Local cache digest enabled; 
> rebuild/rewrite every 3600/3600 sec
> 2023/11/28 11:13:22.194 kid1| Store logging disabled
> 2023/11/28 11:13:22.194 kid1| Swap maxSize 0 + 262144 KB, estimated 
> 20164 objects
> 2023/11/28 11:13:22.194 kid1| Target number of buckets: 1008
> 2023/11/28 11:13:22.194 kid1| Using 8192 Store buckets
> 2023/11/28 11:13:22.194 kid1| Max Mem? size: 262144 KB
> 2023/11/28 11:13:22.194 kid1| Max Swap size: 0 KB
> 2023/11/28 11:13:22.194 kid1| Using Least Load store dir selection
> 2023/11/28 11:13:22.194 kid1| Current Directory is /
> 2023/11/28 11:13:22.194 kid1| Finished loading MIME types and icons.
> 2023/11/28 11:13:22.332 kid1| 80,2| wccp.cc(113) wccpConnectionOpen: 
> WCCPv1 disabled.
> 2023/11/28 11:13:22.332 kid1| 80,2| wccp2.cc(959) wccp2ConnectionOpen: 
> WCCPv2 Disabled. No IPv4 Router(s) configured.
> 2023/11/28 11:13:22.332 kid1| 33,2| AsyncCall.cc(25) AsyncCall: The 
> AsyncCall clientListenerConnectionOpened constructed, 
> this=0x5636c42036d0 [call18]
> 2023/11/28 11:13:22.333 kid1| 33,2| AsyncCall.cc(92) ScheduleCall: 
> StartListening.cc(59) will call 
> clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] FD 22 
> flags=9, err=0, HTTP Socket port=0x5636c4203730) [call18]
> 2023/11/28 11:13:22.333 kid1| 33,2| AsyncCall.cc(25) AsyncCall: The 
> AsyncCall clientListenerConnectionOpened constructed, 
> this=0x5636c420ca50 [call20]
> 2023/11/28 11:13:22.337 kid1| 33,2| AsyncCall.cc(92) ScheduleCall: 
> StartListening.cc(59) will call 
> clientListenerConnectionOpened(local=0.0.0.0:443 remote=[::] FD 23 
> flags=25, err=0, HTTPS Socket port=0x5636c420cab0) [call20]
> 2023/11/28 11:13:22.337 kid1| HTCP Disabled.
> 2023/11/28 11:13:22.337 kid1| Squid plugin modules loaded: 0
> 2023/11/28 11:13:22.337 kid1| Adaptation support is off.
> 2023/11/28 11:13:22.338 kid1| 93,2| Config.cc(224) FinalizeEach: 
> Initialized 0 message adaptation services
> 2023/11/28 11:13:22.338 kid1| 93,2| Config.cc(224) FinalizeEach: 
> Initialized 0 message adaptation service groups
> 2023/11/28 11:13:22.338 kid1| 93,2| Config.cc(224) FinalizeEach: 
> Initialized 0 message adaptation access rules
> 2023/11/28 11:13:22.339 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: 
> entering clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] 
> FD 22 flags=9, err=0, HTTP Socket port=0x5636c4203730)
> 2023/11/28 11:13:22.339 kid1| 33,2| AsyncCall.cc(37) make: make call 
> clientListenerConnectionOpened [call18]
> 2023/11/28 11:13:22.339 kid1| Accepting HTTP Socket connections at 
> local=0.0.0.0:3128 remote=[::] FD 22 flags=9
> 2023/11/28 11:13:22.346 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: 
> leaving clientListenerConnectionOpened(local=0.0.0.0:3128 remote=[::] FD 
> 22 flags=9, err=0, HTTP Socket port=0x5636c4203730)
> 2023/11/28 11:13:22.346 kid1| 33,2| AsyncCallQueue.cc(55) fireNext: 
> entering clientListenerConnectionOpened(local=0.0.0.0:443 remote=[::] FD 
> 23 flags=25, err=0, HTTPS Socket port=0x5636c420cab0)
> 2023/11/28 11:13:22.346 kid1| 33,2| AsyncCall.cc(37) make: make call 
> clientListenerConnectionOpened [call20]
> 2023/11/28 11:13:22.346 kid1| Accepting TPROXY intercepted SSL bumped 
> HTTPS Socket connections at local=0.0.0.0:443 remote=[::] FD 23 flags=25
> 2023/11/28 11:13:22.352 kid1| 33,2| AsyncCallQueue.cc(57) fireNext: 
> leaving clientListenerConnectionOpened(local=0.0.0.0:443 remote=[::] FD 
> 23 flags=25, err=0, HTTPS Socket port=0x5636c420cab0)
> 2023/11/28 11:13:22.352 kid1| Configuring Parent test.de.regify.com/443/0
> 2023/11/28 11:13:22.353 kid1| 15,2| neighbors.cc(1198) peerDNSConfigure: 
> --> IP address #0: 192.168.1.122
> 2023/11/28 11:13:22.368 kid1| 15,2| neighbors.cc(1272) 
> peerConnectSucceded: TCP connection to test.de.regify.com/443 succeeded
> 2023/11/28 11:13:23 kid1| storeLateRelease: released 0 objects
> 
> 
> Then when i call curl -k https://test.regify.com/
> 
> i get
> 
> The requested URL could not be retrieved
> 
> And the log has the following:
> 
> 
> 2023/11/28 11:15:05.467 kid1| 5,2| TcpAcceptor.cc(224) doAccept: New 
> connection on FD 23
> 2023/11/28 11:15:05.467 kid1| 5,2| TcpAcceptor.cc(312) acceptNext: 
> connection on local=0.0.0.0:443 remote=[::] FD 23 flags=25
> 2023/11/28 11:15:05.467 kid1| 17,2| QosConfig.cc(125) 
> getNfmarkFromConnection: QOS: Failed to retrieve connection mark: (-1) 
> (2) No such file or directory (Destination 192.168.1.132:443, source 
> 192.168.1.124:60690)
> 2023/11/28 11:15:05.468 kid1| 33,2| client_side.cc(2742) 
> httpsSslBumpAccessCheckDone: sslBump action peekneeded for 
> local=192.168.1.132:443 remote=192.168.1.124:60690 FD 11 flags=17
> 2023/11/28 11:15:05.468 kid1| 33,2| client_side.cc(3418) 
> fakeAConnectRequest: fake a CONNECT request to force connState to tunnel 
> for ssl-bump
> 2023/11/28 11:15:05.468 kid1| 85,2| client_side_request.cc(751) 
> clientAccessCheckDone: The request CONNECT 192.168.1.132:443 is ALLOWED; 
> last ACL checked: all
> 2023/11/28 11:15:05.468 kid1| 85,2| client_side_request.cc(729) 
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2023/11/28 11:15:05.468 kid1| 85,2| client_side_request.cc(751) 
> clientAccessCheckDone: The request CONNECT 192.168.1.132:443 is ALLOWED; 
> last ACL checked: all
> 2023/11/28 11:15:05.483 kid1| 17,2| FwdState.cc(142) FwdState: 
> Forwarding client request local=192.168.1.132:443 
> remote=192.168.1.124:60690 FD 11 flags=17, url=192.168.1.132:443
> 2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(316) 
> peerSelectDnsPaths: Found sources for '192.168.1.132:443'
> 2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(317) 
> peerSelectDnsPaths:?? always_direct = DENIED
> 2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(318) 
> peerSelectDnsPaths:??? never_direct = DENIED
> 2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(324) 
> peerSelectDnsPaths:??? ORIGINAL_DST = local=192.168.1.124 
> remote=192.168.1.132:443 flags=25
> 2023/11/28 11:15:05.483 kid1| 44,2| peer_select.cc(331) 
> peerSelectDnsPaths:??????? timedout = 0
> 2023/11/28 11:16:05.433 kid1| 4,2| errorpage.cc(1259) BuildContent: No 
> existing error page language negotiated for ERR_CONNECT_FAIL. Using 
> default error file.
> 2023/11/28 11:16:05.433 kid1| 20,2| store.cc(985) checkCachable: 
> StoreEntry::checkCachable: NO: not cachable
> 2023/11/28 11:16:05.433 kid1| 20,2| store.cc(985) checkCachable: 
> StoreEntry::checkCachable: NO: not cachable
> 2023/11/28 11:16:05.463 kid1| 83,2| client_side.cc(2675) 
> clientNegotiateSSL: New session 0x5636c4227330 on FD 11 
> (192.168.1.124:60690)
> 2023/11/28 11:16:05.464 kid1| 11,2| client_side.cc(1306) 
> parseHttpRequest: HTTP Client local=192.168.1.132:443 
> remote=192.168.1.124:60690 FD 11 flags=17
> 2023/11/28 11:16:05.464 kid1| 11,2| client_side.cc(1307) 
> parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET / HTTP/1.1
> Host: test.regify.com
> User-Agent: curl/7.74.0
> Accept: */*
> 
> 
> ----------
> 2023/11/28 11:16:05.464 kid1| 88,2| client_side_reply.cc(2062) 
> processReplyAccessResult: The reply for GET https://test.regify.com/ is 
> ALLOWED, because it matched (access_log daemon:/var/log/squid/access.log 
> line)
> 2023/11/28 11:16:05.464 kid1| 11,2| Stream.cc(271) sendStartOfMessage: 
> HTTP Client local=192.168.1.132:443 remote=192.168.1.124:60690 FD 11 
> flags=17
> 2023/11/28 11:16:05.464 kid1| 11,2| Stream.cc(272) sendStartOfMessage: 
> HTTP Client REPLY:
> ---------
> HTTP/1.1 503 Service Unavailable
> Server: squid/4.13
> Mime-Version: 1.0
> Date: Tue, 28 Nov 2023 10:16:05 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3487
> X-Squid-Error: ERR_CONNECT_FAIL 110
> Vary: Accept-Language
> Content-Language: en
> X-Cache: MISS from proxy
> X-Cache-Lookup: NONE from proxy:3128
> Via: 1.1 proxy (squid/4.13)
> Connection: close
> 
> 
> ----------
> 2023/11/28 11:16:05.464 kid1| 20,2| store.cc(985) checkCachable: 
> StoreEntry::checkCachable: NO: not cachable
> 2023/11/28 11:16:05.464 kid1| 33,2| client_side.cc(895) kick: 
> local=192.168.1.132:443 remote=192.168.1.124:60690 flags=17 Connection 
> was closed
> 2023/11/28 11:16:05.464 kid1| 33,2| client_side.cc(586) swanSong: 
> local=192.168.1.132:443 remote=192.168.1.124:60690 flags=17
> 2023/11/28 11:16:05.465 kid1| 20,2| store.cc(985) checkCachable: 
> StoreEntry::checkCachable: NO: not cachable
> 
> I've been reading the squid docs and other internet resources, but am 
> failing to figure out why this is not working.
> 
> Any clue sticks would be appreciated.
> 
> Also appreciated would be advise on where to find this documented.
> 
> 



From mario.theodoridis at regify.com  Tue Nov 28 15:58:18 2023
From: mario.theodoridis at regify.com (Mario Theodoridis)
Date: Tue, 28 Nov 2023 16:58:18 +0100
Subject: [squid-users] SSL Virtual Hosting Problem
In-Reply-To: <f68d6369-1886-448d-a808-4fffe0e75cbd@treenet.co.nz>
References: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>
 <f68d6369-1886-448d-a808-4fffe0e75cbd@treenet.co.nz>
Message-ID: <31de1e7d-7f9e-5ce5-3af4-df3aeb56b05f@regify.com>

Thank you Amos and Alex,

this is a config i managed to get working for http and https


acl SSL_ports port 443
acl Safe_ports port 80????????? # http
acl Safe_ports port 443???????? # https

# listeners
https_port 0.0.0.0:443 accel defaultsite=regify.com \
 ??? tls-cert=/etc/ssl/certs/regify.com.pem \
 ??? tls-key=/etc/ssl/private/regify.com.key
http_port 0.0.0.0:80 accel

# incoming
http_access deny !Safe_ports
http_access deny manager


# plain
acl vplain dstdomain -n plain.regify.com
http_access allow vplain
cache_peer plain.de.regify.com parent 80 0 \
 ??? proxy-only originserver no-digest no-netdb-exchange name=plain
cache_peer_access plain allow vplain
cache_peer_access plain deny all

# test1
acl stest1 ssl::server_name test1.regify.com
http_access allow stest1
cache_peer test1.de.regify.com parent 443 0 tls ssldomain=test1.regify.com \
 ??? proxy-only originserver no-digest no-netdb-exchange name=test1
cache_peer_access test1 allow stest1
cache_peer_access test1 deny all

# test2
acl stest2 ssl::server_name test2.regify.com
http_access allow stest2
cache_peer test1.de.regify.com parent 443 0 tls ssldomain=test2.regify.com \
 ??? proxy-only originserver no-digest no-netdb-exchange name=test2
cache_peer_access test2 allow stest2
cache_peer_access test2 deny all

# fallback
http_access deny all



Mit Freundlichen Gr??en / Kind regards

Mario Theodoridis

regify GmbH
R?merstrasse 39 | D-78183 H?fingen-Behla
Amtsgericht Freiburg HRB 709343
Telefon: +49 771 8978 4238

On 28/11/23 14:57, Amos Jeffries wrote:
> On 28/11/23 23:29, Mario Theodoridis wrote:
>> Hello everyone,
>>
>> i'm trying to use squid as a TLS virtual hosting proxy on a system 
>> with a public IP in front of several internal systems running TLS web 
>> servers.
>>
>> I would like to proxy the incoming connections to the appropriate 
>> backend servers based on the hostname using SNI.
>>
>> I'm using the following config to just try this with 1 backend to 
>> test with and fail already
>>
>> Here the config:
>>
>> http_port 3128
>> debug_options ALL,2
>> pinger_enable off
>> shutdown_lifetime 1 second
>> https_port 0.0.0.0:443 tproxy ssl-bump tls-cert=/root/dummy.pem
>
> That should be:
>
> ? https_port 443 accel defaultsite=example.com \
> ??? tls-cert=/etc/squid/example.com.pem
>
> The PEM file needs to be valid for all the domains served.
>
>
>> acl tlspls ssl::server_name_regex -i test\.regify\.com
>> cache_peer test.de.regify.com parent 443 0 proxy-only originserver 
>> no-digest no-netdb-exchange name=test
>
> Missing "tls" option to enable TLS when talking to this peer.
>
>
>> ssl_bump peek all
>> ssl_bump splice all
>> http_access allow all
>> cache_peer_access test allow all
>
> I appreciate this is a test. But be sure to keep the default Squid 
> security rules ("deny !Safe_ports" etc) and only allow the hosted 
> domains instead of "all". These DoS and attack protections are 
> particularly important on a reverse-proxy where the general public has 
> access.
>
> FYI; "test what you will use" is important for proxies. One of the 
> "irrelevant" config details may kill your real-world production setup 
> where testing works fine without any security.
>
>
>>
>> ...
>> I've been reading the squid docs and other internet resources, but am 
>> failing to figure out why this is not working.
>>
>> Any clue sticks would be appreciated.
>>
>> Also appreciated would be advise on where to find this documented.
>>
>
> The Squid wiki ConfigExamples section has all the typical 
> configuration types and a few of the more uncommon ones as well.
> The one you are needing is 
> <https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting>
>
>
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From ngtech1ltd at gmail.com  Wed Nov 29 06:51:14 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Wed, 29 Nov 2023 08:51:14 +0200
Subject: [squid-users] Squid 6.2 with WCCP
In-Reply-To: <9c8e62a1-eab0-4c27-bb60-ec0df25f0223@treenet.co.nz>
References: <LO0P265MB5503918F5857C242D659CF91F01EA@LO0P265MB5503.GBRP265.PROD.OUTLOOK.COM>
 <a41412c6-936e-c6f1-d2fd-a204e71c1dc8@measurement-factory.com>
 <60aa4884-6bd5-481b-9995-52636dda4d1f@treenet.co.nz>
 <000401d9e488$359ce460$a0d6ad20$@gmail.com>
 <9c8e62a1-eab0-4c27-bb60-ec0df25f0223@treenet.co.nz>
Message-ID: <00a801da2290$728673a0$57935ae0$@gmail.com>

Thanks,

So an enterprise level Cisco.
The main issue is the upgrades for these...
Anyone knows if the newest Cisco devices still works with WCCP?

Eliezer

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, September 12, 2023 8:20 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 6.2 with WCCP

On 11/09/23 20:16, ngtech1ltd wrote:
> Hey,
> 
> What is required for testing the wccp code?


At minimum a Router or Switch with WCCPv2, plus separate machines for 
client and proxy.

Ideally;
  * at least two router/switch to test the changed code handling 
multiple routers.
  * ability to test both Mask and GRE assignment methods.
  * ability to test a mix of the capability and security settings in WCCPv2.


> I can try to get a Cisco device for a basic testing.
> Is there a specific bug report we can follow on this issue or maybe we should follow on the PR?
> 

Test results in the PR please.

Cheers
Amos


> Eliezer
> 
> -----Original Message-----
> From: Amos Jeffries
> Sent: Tuesday, August 22, 2023 15:16
> 
> On 22/08/23 01:34, Alex Rousskov wrote:
>> On 8/21/23 05:06, Callum Haywood wrote:
>>>
>>> Does anyone understand what is causing these errors? Are there any
>>> known issues or patches in progress?
>>
>> A few years ago, several serious problems were discovered in WCCP code,
>> including security vulnerabilities:
>>
>> https://github.com/squid-cache/squid/security/advisories/GHSA-rgf3-9v3p-qp82
>>
>> Some of the WCCP bugs were fixed without testing; developers fixing
>> those bugs could not easily test WCCP. Some of the old WCCP bugs
>> remained and some of the new fixes were buggy.
>>
>> Today, WCCP code remains problematic. If your customers rely on WCCP,
>> consider investing into revamping that neglected and buggy feature.
>>
> 
> This PR <https://github.com/squid-cache/squid/pull/970> has some
> progress towards a fix of those. See the latest comment (currently Sept
> 2022) for issues that still need to be resolved before that PR is ready
> for merge attempt.
> 
> The major issue remains the ability to test.
> 
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users



From Ziert at pdv-sachsen.net  Wed Nov 29 14:38:51 2023
From: Ziert at pdv-sachsen.net (Ziert, Norman)
Date: Wed, 29 Nov 2023 14:38:51 +0000
Subject: [squid-users] =?utf-8?q?reconfigure_drops_in_memory_caches_for_e?=
 =?utf-8?q?xternal=5Facl=5Ftype?=
Message-ID: <e9edfdfa17da43bfbdbfdb132cb6ea2e@pdv-sachsen.net>

Hello,


in the very recent past I stumbled over that a "squid -k reconfigure" drops in memory caches for external_acl_type helpers, wich in my case leads to a massive query burst against local winbind (ext_wbinfo_group_acl) and infact the active directory domaincontrollers. This also leads to an massive impact in servicetime to endusers, where after a reconfigure the time needed to fullfill takes a recent time by group membership authorization within squid until the external_acl cache is "rewarmed".


I have veriefied that behavior back to v5.2.


I want to ask if this is a wanted behavior to squid or should I file a bug-report on this?


Maybe I discovered a topic wich is connected to this discussion: https://wiki.squid-cache.org/Features/HotConf



Best regards


Norman
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231129/e052dcca/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov 29 15:33:21 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 29 Nov 2023 10:33:21 -0500
Subject: [squid-users] reconfigure drops in memory caches for
 external_acl_type
In-Reply-To: <e9edfdfa17da43bfbdbfdb132cb6ea2e@pdv-sachsen.net>
References: <e9edfdfa17da43bfbdbfdb132cb6ea2e@pdv-sachsen.net>
Message-ID: <6baeb8e7-6583-4a61-8e78-72f352d790a1@measurement-factory.com>

On 2023-11-29 09:38, Ziert, Norman wrote:

> in the very recent past I stumbled over that a "squid -k reconfigure" 
> drops in memory caches for external_acl_type helpers, wich in my case 
> leads to a massive query burst against local winbind 
> (ext_wbinfo_group_acl) and infact the active directory 
> domaincontrollers. This also leads to an massive impact in servicetime 
> to endusers, where after a reconfigure the time needed to fullfill takes 
> a recent time by group membership authorization within squid until the 
> external_acl cache is "rewarmed".
> 
> 
> I have veriefied that behavior back to v5.2.
> 
> 
> I want to ask if this is a wanted behavior to squid or should I file a 
> bug-report on this?


This behavior is usually unwanted, but there is no particular need for a 
bug report IMHO: We know that this and similar reconfiguration side 
effects are usually unwanted. We are actively working on avoiding 
various unwanted reconfiguration side effects -- the "smooth 
reconfiguration" project has merged about 25 pull requests in recent 
months (some of them were very complex/substantial -- we need to payoff 
a lot of very old technical debt first!), and more are coming. We will 
solve this problem in the foreseeable future.


> Maybe I discovered a topic wich is connected to this discussion: 
> https://wiki.squid-cache.org/Features/HotConf 

Yes, that old wiki page has some relevant pieces, but we have moved far 
ahead of those old discussions since then and have solved complex 
architectural problems with functional, deployment-tested code that 
supports smooth reconfiguration in some typical cases. We are actively 
working on polishing, merging, and enhancing those improvements.


HTH,

Alex.



From ankor2023 at gmail.com  Thu Nov 30 03:29:51 2023
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 30 Nov 2023 06:29:51 +0300
Subject: [squid-users] reconfigure drops in memory caches for
 external_acl_type
In-Reply-To: <6baeb8e7-6583-4a61-8e78-72f352d790a1@measurement-factory.com>
References: <e9edfdfa17da43bfbdbfdb132cb6ea2e@pdv-sachsen.net>
 <6baeb8e7-6583-4a61-8e78-72f352d790a1@measurement-factory.com>
Message-ID: <CADJd0Y17-4Cr_Cg1N7684Gwx3C6S1BdMtqi3t8DT0ySqOmu_8g@mail.gmail.com>

Hello, Norman,

I faced the problem too.
For myself, I modified the authorisation script (ext_wbinfo_group_acl - it
is a simple Perl code) and cache some user groups membership in a Memcache.
The script tries first of all to get the information from the cache, and if
it is not there, then get it from winbind.
Squid reconfigurations do not affect the information in the memcache.

Kind regards,
       Ankor


??, 29 ????. 2023??. ? 18:33, Alex Rousskov <
rousskov at measurement-factory.com>:

> On 2023-11-29 09:38, Ziert, Norman wrote:
>
> > in the very recent past I stumbled over that a "squid -k reconfigure"
> > drops in memory caches for external_acl_type helpers, wich in my case
> > leads to a massive query burst against local winbind
> > (ext_wbinfo_group_acl) and infact the active directory
> > domaincontrollers. This also leads to an massive impact in servicetime
> > to endusers, where after a reconfigure the time needed to fullfill takes
> > a recent time by group membership authorization within squid until the
> > external_acl cache is "rewarmed".
> >
> >
> > I have veriefied that behavior back to v5.2.
> >
> >
> > I want to ask if this is a wanted behavior to squid or should I file a
> > bug-report on this?
>
>
> This behavior is usually unwanted, but there is no particular need for a
> bug report IMHO: We know that this and similar reconfiguration side
> effects are usually unwanted. We are actively working on avoiding
> various unwanted reconfiguration side effects -- the "smooth
> reconfiguration" project has merged about 25 pull requests in recent
> months (some of them were very complex/substantial -- we need to payoff
> a lot of very old technical debt first!), and more are coming. We will
> solve this problem in the foreseeable future.
>
>
> > Maybe I discovered a topic wich is connected to this discussion:
> > https://wiki.squid-cache.org/Features/HotConf
>
> Yes, that old wiki page has some relevant pieces, but we have moved far
> ahead of those old discussions since then and have solved complex
> architectural problems with functional, deployment-tested code that
> supports smooth reconfiguration in some typical cases. We are actively
> working on polishing, merging, and enhancing those improvements.
>
>
> HTH,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231130/74ee2a02/attachment.htm>

From mika at rii.fr  Thu Nov 30 09:22:29 2023
From: mika at rii.fr (MIKA)
Date: Thu, 30 Nov 2023 10:22:29 +0100
Subject: [squid-users] Module c-icap help
Message-ID: <D2C457D1-8638-4659-B953-E86B233559CF@rii.fr>


Hello everyone,
Thank you again for all the work you were able to do on this project.
I try to control the cookies with squid but it's impossible. the c-icap module in the squid.conf file does not seem to work because the c-icap server does not seem to work.
Can you help me please ?
thanks in advance


From squid3 at treenet.co.nz  Thu Nov 30 10:03:23 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Nov 2023 23:03:23 +1300
Subject: [squid-users] Module c-icap help
In-Reply-To: <D2C457D1-8638-4659-B953-E86B233559CF@rii.fr>
References: <D2C457D1-8638-4659-B953-E86B233559CF@rii.fr>
Message-ID: <758af944-6ad6-4a50-81b0-646c5467c67f@treenet.co.nz>

On 30/11/23 22:22, MIKA wrote:
> 
> Hello everyone,
> Thank you again for all the work you were able to do on this project.
> I try to control the cookies with squid but it's impossible. the c-icap module in the squid.conf file does not seem to work because the c-icap server does not seem to work.
> Can you help me please ?


Please describe with a bit more details what setup you have, what you 
expect it to be doing, and what you see it actually doing.


Cheers
Amos


From mario.theodoridis at regify.com  Thu Nov 30 15:55:18 2023
From: mario.theodoridis at regify.com (Mario Theodoridis)
Date: Thu, 30 Nov 2023 16:55:18 +0100
Subject: [squid-users] SSL Virtual Hosting Problem
In-Reply-To: <31de1e7d-7f9e-5ce5-3af4-df3aeb56b05f@regify.com>
References: <03a7ad35-a700-c02d-cba3-0f163c1014f5@regify.com>
 <f68d6369-1886-448d-a808-4fffe0e75cbd@treenet.co.nz>
 <31de1e7d-7f9e-5ce5-3af4-df3aeb56b05f@regify.com>
Message-ID: <ba4870c8-6ba0-12aa-52ef-d43e2c5b1172@regify.com>

I do have one more problem at this point.

Using openssl i can work with what i have below, but i cannot add a 2nd 
certificate

https_port 0.0.0.0:443 accel defaultsite=regify.com \
 ??? tls-cert=/etc/ssl/certs/regify.com.pem \
 ??? tls-cert=/etc/ssl/certs/foo.com.pem

gives me

ERROR: OpenSSL does not support multiple server certificates. Ignoring 
addional cert= parameters.


If i instead use gnutls, i get dinged for using ssl::server

FATAL: Bungled /etc/squid/squid.conf line 29: acl stest1 
ssl::server_name test1.regify.com

is there a way to get the SNI host with gnutls?

http://www.squid-cache.org/Doc/config/acl/ did not answer that for me.

Alternatively, can i get openssl to cope with multiple certs somehow?


Mit Freundlichen Gr??en / Kind regards

Mario Theodoridis

regify GmbH
R?merstrasse 39 | D-78183 H?fingen-Behla
Amtsgericht Freiburg HRB 709343
Telefon: +49 771 8978 4238

On 28/11/23 16:58, Mario Theodoridis wrote:
> Thank you Amos and Alex,
>
> this is a config i managed to get working for http and https
>
>
> acl SSL_ports port 443
> acl Safe_ports port 80????????? # http
> acl Safe_ports port 443???????? # https
>
> # listeners
> https_port 0.0.0.0:443 accel defaultsite=regify.com \
> ??? tls-cert=/etc/ssl/certs/regify.com.pem \
> ??? tls-key=/etc/ssl/private/regify.com.key
> http_port 0.0.0.0:80 accel
>
> # incoming
> http_access deny !Safe_ports
> http_access deny manager
>
>
> # plain
> acl vplain dstdomain -n plain.regify.com
> http_access allow vplain
> cache_peer plain.de.regify.com parent 80 0 \
> ??? proxy-only originserver no-digest no-netdb-exchange name=plain
> cache_peer_access plain allow vplain
> cache_peer_access plain deny all
>
> # test1
> acl stest1 ssl::server_name test1.regify.com
> http_access allow stest1
> cache_peer test1.de.regify.com parent 443 0 tls 
> ssldomain=test1.regify.com \
> ??? proxy-only originserver no-digest no-netdb-exchange name=test1
> cache_peer_access test1 allow stest1
> cache_peer_access test1 deny all
>
> # test2
> acl stest2 ssl::server_name test2.regify.com
> http_access allow stest2
> cache_peer test1.de.regify.com parent 443 0 tls 
> ssldomain=test2.regify.com \
> ??? proxy-only originserver no-digest no-netdb-exchange name=test2
> cache_peer_access test2 allow stest2
> cache_peer_access test2 deny all
>
> # fallback
> http_access deny all
>
>
>
> Mit Freundlichen Gr??en / Kind regards
>
> Mario Theodoridis
>
> regify GmbH
> R?merstrasse 39 | D-78183 H?fingen-Behla
> Amtsgericht Freiburg HRB 709343
> Telefon: +49 771 8978 4238
>
> On 28/11/23 14:57, Amos Jeffries wrote:
>> On 28/11/23 23:29, Mario Theodoridis wrote:
>>> Hello everyone,
>>>
>>> i'm trying to use squid as a TLS virtual hosting proxy on a system 
>>> with a public IP in front of several internal systems running TLS 
>>> web servers.
>>>
>>> I would like to proxy the incoming connections to the appropriate 
>>> backend servers based on the hostname using SNI.
>>>
>>> I'm using the following config to just try this with 1 backend to 
>>> test with and fail already
>>>
>>> Here the config:
>>>
>>> http_port 3128
>>> debug_options ALL,2
>>> pinger_enable off
>>> shutdown_lifetime 1 second
>>> https_port 0.0.0.0:443 tproxy ssl-bump tls-cert=/root/dummy.pem
>>
>> That should be:
>>
>> ? https_port 443 accel defaultsite=example.com \
>> ??? tls-cert=/etc/squid/example.com.pem
>>
>> The PEM file needs to be valid for all the domains served.
>>
>>
>>> acl tlspls ssl::server_name_regex -i test\.regify\.com
>>> cache_peer test.de.regify.com parent 443 0 proxy-only originserver 
>>> no-digest no-netdb-exchange name=test
>>
>> Missing "tls" option to enable TLS when talking to this peer.
>>
>>
>>> ssl_bump peek all
>>> ssl_bump splice all
>>> http_access allow all
>>> cache_peer_access test allow all
>>
>> I appreciate this is a test. But be sure to keep the default Squid 
>> security rules ("deny !Safe_ports" etc) and only allow the hosted 
>> domains instead of "all". These DoS and attack protections are 
>> particularly important on a reverse-proxy where the general public 
>> has access.
>>
>> FYI; "test what you will use" is important for proxies. One of the 
>> "irrelevant" config details may kill your real-world production setup 
>> where testing works fine without any security.
>>
>>
>>>
>>> ...
>>> I've been reading the squid docs and other internet resources, but 
>>> am failing to figure out why this is not working.
>>>
>>> Any clue sticks would be appreciated.
>>>
>>> Also appreciated would be advise on where to find this documented.
>>>
>>
>> The Squid wiki ConfigExamples section has all the typical 
>> configuration types and a few of the more uncommon ones as well.
>> The one you are needing is 
>> <https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting>
>>
>>
>> Cheers
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



