From dave at killthe.net  Sun Oct  1 21:28:13 2023
From: dave at killthe.net (Dave Blanchard)
Date: Sun, 1 Oct 2023 16:28:13 -0500
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <ZRfACbyYspQxpqlp@charite.de>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
Message-ID: <20231001162813.b80ff55792136466d00825a5@killthe.net>

Squid's user friendliness could use a major overhaul. I absolutely despise programs which are designed this way. It just silently fails on startup with no obvious reason or explanation given, even if one enables debug output to find out why. Instead you have to get online and search, ask questions in a mailing list, etc to find out you have to type in some obscure commands. 

"Too few ssl_crtd processes are running" and "FATAL: The ssl_crtd helpers are crashing too rapidly, need help!" are some of the dumbest error messages ever conceived. Reminds me of every web site these days with their "Oops something went wrong lol" errors. 

Dave


From bud_miljkovic at trimble.com  Mon Oct  2 03:09:45 2023
From: bud_miljkovic at trimble.com (Bud Miljkovic)
Date: Mon, 2 Oct 2023 16:09:45 +1300
Subject: [squid-users] Squid 3.5.25 outgoing interface?
Message-ID: <CAPO8noLDE++bemH8Zc27CzB1JPWjS_o024Wrqjze1a7bTDY7fw@mail.gmail.com>

Does Squid have a configuration directive to forward the processed TCP
traffic to one of the target's existing network interfaces?

Or to put in another way, does Squid have a way to route the processed TCP
traffic to one of the target's existing network interfaces, rather than to
an IP address?

Buda

-- 
Budimir Miljkovi? BSc E | He
Senior Development Engineer
Civil Construction Field Systems
Trimble

11-17 Birmingham Drive, Christchurch, Canterbury, 8024
New Zealand
+64 3 963-5550 Direct
+64 21 419-024 Mobile

www.trimble.com

This email may contain confidential information that is intended only for
the listed recipient(s) of this email. Any unauthorized review, use,
disclosure or distribution is prohibited. If you believe you have received
this email in error, please immediately delete this email and any
attachments, and inform me via reply email.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231002/bf664b3a/attachment.htm>

From uhlar at fantomas.sk  Mon Oct  2 06:46:23 2023
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 2 Oct 2023 08:46:23 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <20231001162813.b80ff55792136466d00825a5@killthe.net>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
Message-ID: <ZRpnP4K9Mhj2c+Lf@fantomas.sk>

On 01.10.23 16:28, Dave Blanchard wrote:
> Squid's user friendliness could use a major overhaul.  I absolutely despise 
> programs which are designed this way.  It just silently fails on startup 
> with no obvious reason or explanation given, even if one enables debug 
> output to find out why.  Instead you have to get online and search, ask 
> questions in a mailing list, etc to find out you have to type in some 
> obscure commands.

is there nothing in cache_log file?

> "Too few ssl_crtd processes are running" and "FATAL: The ssl_crtd helpers 
> are crashing too rapidly, need help!" are some of the dumbest error 
> messages ever conceived.

There is nothing dumb about this. You have configured squid to start 
ssl_crtd processes and they crash. The only thing squid can tell you that 
there is something wrong with them. 

> Reminds me of every web site these days with 
> their "Oops something went wrong lol" errors.

The same applies to web clients, what do you expect squid to do, display 
"squid admin screwed up the ssl_crtd configuration"? 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Save the whales. Collect the whole set.


From uhlar at fantomas.sk  Mon Oct  2 06:48:24 2023
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 2 Oct 2023 08:48:24 +0200
Subject: [squid-users] Squid 3.5.25 outgoing interface?
In-Reply-To: <CAPO8noLDE++bemH8Zc27CzB1JPWjS_o024Wrqjze1a7bTDY7fw@mail.gmail.com>
References: <CAPO8noLDE++bemH8Zc27CzB1JPWjS_o024Wrqjze1a7bTDY7fw@mail.gmail.com>
Message-ID: <ZRpnuExmdnKQTkLi@fantomas.sk>

On 02.10.23 16:09, Bud Miljkovic wrote:
>Does Squid have a configuration directive to forward the processed TCP
>traffic to one of the target's existing network interfaces?

No, with squid you can only configure IP address the outgoing requests will 
go from.

>Or to put in another way, does Squid have a way to route the processed TCP
>traffic to one of the target's existing network interfaces, rather than to
>an IP address?

No, the OS kernel (or IP stack) does the routing, squid has no word here.

You can configure outgoing IP and configure the kernel to forward traffic 
using another interface/route.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Have you got anything without Spam in it?
- Well, there's Spam egg sausage and Spam, that's not got much Spam in it.


From dave at killthe.net  Mon Oct  2 06:53:10 2023
From: dave at killthe.net (Dave Blanchard)
Date: Mon, 2 Oct 2023 01:53:10 -0500
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <ZRpnP4K9Mhj2c+Lf@fantomas.sk>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <ZRpnP4K9Mhj2c+Lf@fantomas.sk>
Message-ID: <20231002015310.f9ef5d4c3cc18e4eeb98d488@killthe.net>

If I have to explain correct UI design to you, then there's just no hope for you, is there? 

How hard is it to print a MEANINGFUL ERROR MESSAGE?


From uhlar at fantomas.sk  Mon Oct  2 06:59:47 2023
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 2 Oct 2023 08:59:47 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <20231002015310.f9ef5d4c3cc18e4eeb98d488@killthe.net>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <ZRpnP4K9Mhj2c+Lf@fantomas.sk>
 <20231002015310.f9ef5d4c3cc18e4eeb98d488@killthe.net>
Message-ID: <ZRpqY3xN46j19/nl@fantomas.sk>

On 02.10.23 01:53, Dave Blanchard wrote:
>If I have to explain correct UI design to you, then there's just no hope for you, is there?

squit is a proxy SERVER, it does NOT have an UI.

>How hard is it to print a MEANINGFUL ERROR MESSAGE?

The message is meaningful enough: the "ssl_crtd" process you have configured 
to create certificates for you is crashing.

I have asked and you have not answered: What's in the cache_log file?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese.


From dave at killthe.net  Mon Oct  2 07:07:05 2023
From: dave at killthe.net (Dave Blanchard)
Date: Mon, 2 Oct 2023 02:07:05 -0500
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <ZRpqY3xN46j19/nl@fantomas.sk>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <ZRpnP4K9Mhj2c+Lf@fantomas.sk>
 <20231002015310.f9ef5d4c3cc18e4eeb98d488@killthe.net>
 <ZRpqY3xN46j19/nl@fantomas.sk>
Message-ID: <20231002020705.82022f2f1446c667d3dae602@killthe.net>

I'm in no mood for your bullshit. Pretending as if the daemon has successfully launched but just silently failing with no warning, then requiring me to jump through hoops to dig up some completely meaningless error message that I have to Google for is OBVIOUSLY NOT how it's done. 

Every other piece of software on the planet knows better, so maybe get a clue.

There have been countless books written on proper user interface design; try reading one. 



From uhlar at fantomas.sk  Mon Oct  2 07:16:14 2023
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 2 Oct 2023 09:16:14 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <20231002020705.82022f2f1446c667d3dae602@killthe.net>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <ZRpnP4K9Mhj2c+Lf@fantomas.sk>
 <20231002015310.f9ef5d4c3cc18e4eeb98d488@killthe.net>
 <ZRpqY3xN46j19/nl@fantomas.sk>
 <20231002020705.82022f2f1446c667d3dae602@killthe.net>
Message-ID: <ZRpuPhm9Ps6iszeK@fantomas.sk>

On 02.10.23 02:07, Dave Blanchard wrote:
>I'm in no mood for your bullshit.

GTFO then, go and pay for software with support.
I am just user and this is user forum, not paid support line.

>Pretending as if the daemon has successfully launched but just silently failing with no warning, then requiring me to jump through hoops to dig up some completely meaningless error message that I have to Google for is OBVIOUSLY NOT how it's done.
>
>Every other piece of software on the planet knows better, so maybe get a clue.
>
>There have been countless books written on proper user interface design; try reading one.

I am not a squid developer, you idiot.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
If Barbie is so popular, why do you have to buy her friends?


From gkinkie at gmail.com  Mon Oct  2 07:19:34 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 2 Oct 2023 09:19:34 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <20231002020705.82022f2f1446c667d3dae602@killthe.net>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <ZRpnP4K9Mhj2c+Lf@fantomas.sk>
 <20231002015310.f9ef5d4c3cc18e4eeb98d488@killthe.net>
 <ZRpqY3xN46j19/nl@fantomas.sk>
 <20231002020705.82022f2f1446c667d3dae602@killthe.net>
Message-ID: <CA+Y8hcNZGn19vop4xpATJd-bGqqe59hZnK9k1G0TwzUoWS6Khw@mail.gmail.com>

Please, everyone, let's keep the discussion civil; insulting other
participants will not achieve anything good for anyone.

On the merit of the conversation: Squid is the result of the work and
passion of countless people (including myself and many other participants
to this forum) who contribute their time and expertise for free, in the
hope that it will be helpful to others.
There is no expectation placed anywhere on anyone that they contribute
anything for free, be it code, advice, documentation, or support.

There is one authoritative source of information about the behaviour of the
software, and that's the source code, which is freely available.
Improvements to Squid's behaviour are always welcome, under the form of
pull requests.
In fact, many valuable code contributions happened because someone was
dissatisfied with the behaviour of Squid (or any other Free/Open Source
Software) in their use case.

Again, please keep it civil; harassment of any kind to anyone is unwelcome
here.

Francesco Chemolli
Squid Software Foundation

On Mon, Oct 2, 2023 at 9:04?AM Dave Blanchard <dave at killthe.net> wrote:

> I'm in no mood for your bullshit. Pretending as if the daemon has
> successfully launched but just silently failing with no warning, then
> requiring me to jump through hoops to dig up some completely meaningless
> error message that I have to Google for is OBVIOUSLY NOT how it's done.
>
> Every other piece of software on the planet knows better, so maybe get a
> clue.
>
> There have been countless books written on proper user interface design;
> try reading one.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231002/90b85e8b/attachment.htm>

From squid3 at treenet.co.nz  Mon Oct  2 08:43:21 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 2 Oct 2023 21:43:21 +1300
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <20231001162813.b80ff55792136466d00825a5@killthe.net>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
Message-ID: <231b4d52-4318-43e8-a9ca-8083afd46480@treenet.co.nz>

On 2/10/23 10:28, Dave Blanchard wrote:
> Squid's user friendliness could use a major overhaul.

Agreed. As one of the people trying to do that for the past decade ... 
any suggestions of better wording are welcome.


> I absolutely despise programs which are designed this way.

Ah, there we have part of the problem, Squid is not exactly designed. As 
Francesco mentioned these days it is a collection of 40+ years of Ad Hoc 
community contributions.

The group of long-term developers (we call ourselves "core team") have 
tried to guide/wrangle some coherency out of that - some variable success.


> It just silently fails on startup with no obvious reason or explanation given,

That is false. Squid writes as much information as it can about the 
problem to log, stderr, and if possible the system message log. There is 
nothing else a process like Squid can do.

> even if one enables debug output to find out why.

If you are not seeing information detailing the problem in the above 
mentioned logs/outputs. Then the problem is either:

  1) something broken in the ability to write those log/output.

This is a major problem. How does one report about problems when the 
reporting method is broken? Sorry.

Using multiple outputs during startup helps reduce this but it is 
possible that all of them have been forbidden. Further troubleshooting 
can be done by running Squid with "squid -k parse" - the output to 
stderr should be printed to the shell for admin to view.


  2) Squid has no known information to display




> Instead you have to get online and search, ask questions in a mailing list, etc to find out you have to type in some obscure commands.
> 

Hmm. That is the normal process for any software error which you do not 
already fully understand.

For those we believe to be clear enough already we try to document 
troubleshooting procedures in the Squid wiki 
<https://wiki.squid-cache.org/>. Suggestions for better organization 
and/or missing texts of that content are always welcome.


> "Too few ssl_crtd processes are running" 

  ... means *exactly* what it says.

Note: this is not an error message. It is part of the information Squid 
knows about the problem. It is listed to help inform you about what was 
going on when the error occured. It may be helpful to your troubleshooting.

Squid needs to use some "ssl_crtd" helpers. There are not enough of them 
running. ... Squid is about to start some to use. Then ...


> and "FATAL: The ssl_crtd helpers are crashing too rapidly, need help!"


... also means exactly what it says.


"ssl_crtd" is a binary separate from Squid. We humans know that it is 
one of the Squid Project bundled helpers, but Squid itself cannot tell.

Squid is not getting anything out of the helper. Not even a message 
saying what went wrong for it. That would have appeared as messages in 
your log between the two above lines .. .and what Matus was asking about.

Squid tries approximately 10 times to recover the helper before the 
FATAL message is produced and Squid itself exits instead of staying in 
the infinite loop of "start helper...start helper...start helper..."



> are some of the dumbest error messages ever conceived. Reminds me of every web site these days with their "Oops something went wrong lol" errors.

This one is more inline with the classic "Keyboard not found. Press any 
key to continue".

It seems dumb, until you actually understand what it means. One must 
plug in keyboard (aks solve the problem) before the machine can be used.

Or for Squid ... only by researching and fixing the problem will you get 
any better info about the problem.



FYI: Documentation about ssl_crtd can be found at 
<https://wiki.squid-cache.org/Features/DynamicSslCert>.
  The page need updating to detail this and other errors the helper 
produces and how to solve them. Any volunteers?


HTH
Amos


From Ralf.Hildebrandt at charite.de  Mon Oct  2 09:01:41 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 2 Oct 2023 11:01:41 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <231b4d52-4318-43e8-a9ca-8083afd46480@treenet.co.nz>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <231b4d52-4318-43e8-a9ca-8083afd46480@treenet.co.nz>
Message-ID: <ZRqG9UOvGe/fv36w@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:
> On 2/10/23 10:28, Dave Blanchard wrote:
> > Squid's user friendliness could use a major overhaul.
> 
> Agreed. As one of the people trying to do that for the past decade ... any
> suggestions of better wording are welcome.

But you have to admit this: I read the error message (I'm no devel, just a
mere admin), which CLEARLY stated what was wrong (the DB needs to be
initialized). 

This is so much better than windows, which usually says something like
"A generic error occured: -34", which tells you nothing. At all.

> That is false. Squid writes as much information as it can about the problem
> to log, stderr, and if possible the system message log. There is nothing
> else a process like Squid can do.

Squid COULD have initialized the DB itself. 
That's the criticism I'm willing to allow.
If it KNOWS what's wrong, why not "fix" it itself.

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From stu.lists at spacehopper.org  Mon Oct  2 09:26:39 2023
From: stu.lists at spacehopper.org (Stuart Henderson)
Date: Mon, 2 Oct 2023 09:26:39 -0000 (UTC)
Subject: [squid-users] [ext] Squid quits while starting?!
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <231b4d52-4318-43e8-a9ca-8083afd46480@treenet.co.nz>
 <ZRqG9UOvGe/fv36w@charite.de>
Message-ID: <slrnuhl36f.eit.stu.lists@naiad.spacehopper.org>

On 2023-10-02, Ralf Hildebrandt <Ralf.Hildebrandt at charite.de> wrote:
> Squid COULD have initialized the DB itself. 
> That's the criticism I'm willing to allow.
> If it KNOWS what's wrong, why not "fix" it itself.

In the cache db case: it _doesn't_ know what is wrong. Perhaps it is
indeed because the DB hasn't been initialised (though it may not have
access permissions to create the db anyway), but it could equally be
that a partition is not mounted - refusing to start at least makes it
obvious that there's a problem.




From Ralf.Hildebrandt at charite.de  Mon Oct  2 09:30:36 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 2 Oct 2023 11:30:36 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <slrnuhl36f.eit.stu.lists@naiad.spacehopper.org>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <231b4d52-4318-43e8-a9ca-8083afd46480@treenet.co.nz>
 <ZRqG9UOvGe/fv36w@charite.de>
 <slrnuhl36f.eit.stu.lists@naiad.spacehopper.org>
Message-ID: <ZRqNvBk+YK8OxT9X@charite.de>

* Stuart Henderson <stu.lists at spacehopper.org>:

> In the cache db case: it _doesn't_ know what is wrong. Perhaps it is
> indeed because the DB hasn't been initialised (though it may not have
> access permissions to create the db anyway)

r it might be the wrong path.

>, but it could equally be
> that a partition is not mounted - refusing to start at least makes it
> obvious that there's a problem.

Yup.

I'm always joking: "Professional(n.): User who can read & understand error messages"

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From uhlar at fantomas.sk  Mon Oct  2 10:03:39 2023
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 2 Oct 2023 12:03:39 +0200
Subject: [squid-users] [ext] Squid quits while starting?!
In-Reply-To: <ZRqG9UOvGe/fv36w@charite.de>
References: <CAPO8noJ9R8Tf3z1yp4gR3ameZOWO69F==Hgq9mtx3uP_k0aB6Q@mail.gmail.com>
 <ZRfACbyYspQxpqlp@charite.de>
 <20231001162813.b80ff55792136466d00825a5@killthe.net>
 <231b4d52-4318-43e8-a9ca-8083afd46480@treenet.co.nz>
 <ZRqG9UOvGe/fv36w@charite.de>
Message-ID: <ZRqVe6WeVg49l5Bg@fantomas.sk>

>* Amos Jeffries <squid3 at treenet.co.nz>:
>> That is false. Squid writes as much information as it can about the problem
>> to log, stderr, and if possible the system message log. There is nothing
>> else a process like Squid can do.

On 02.10.23 11:01, Ralf Hildebrandt wrote:
>Squid COULD have initialized the DB itself.
>That's the criticism I'm willing to allow.
>If it KNOWS what's wrong, why not "fix" it itself.

crtd, not squid, unless there's reason not to do that.

Perhaps squid could log the error crtd sent, perhapt this could be enhanced 
to all helpers.

so the OP doesn't need to find it out themselves.
https://lists.squid-cache.org/pipermail/squid-users/2023-September/026164.html


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Despite the cost of living, have you noticed how popular it remains?


From fgiorgetti at gmail.com  Mon Oct  2 15:02:55 2023
From: fgiorgetti at gmail.com (Fernando Giorgetti)
Date: Mon, 2 Oct 2023 12:02:55 -0300
Subject: [squid-users] TLS passthrough
In-Reply-To: <dd47accd-50c6-4515-9f89-a430de327308@treenet.co.nz>
References: <CADxZ=iVSpT0VYTZS0ZhnnQNP_1mg+PZNzGmWc9cR3mYnos1p_g@mail.gmail.com>
 <CADxZ=iX=0fpqyv9L8koUPuCCDuA3j8iEqdizqLPcFsgBSQuEJw@mail.gmail.com>
 <e1c49be1-597a-47e7-96e3-5f036d3024a5@measurement-factory.com>
 <CADxZ=iV2sfFOYXWC=gRa_d53M2GK6WjXt_P+aNK6cFCUoSdpig@mail.gmail.com>
 <4d494271-f038-4209-b99a-bf8e08cbc42c@measurement-factory.com>
 <CADxZ=iXBfUBS3WHf66QW94=8bKX0r-8i1QtWJNrpAZ6Z4Cm_2g@mail.gmail.com>
 <00c74d5d-7b8f-470e-aa63-1e87d7224f57@measurement-factory.com>
 <CADxZ=iVq79jruOgXA6TBw0guVSzeN=mZwLF33LVyCd1e2UTBJQ@mail.gmail.com>
 <6b73642d-0148-4c1e-a822-3d88a3f02cb7@measurement-factory.com>
 <CADxZ=iWxMA3avGOtYsAqg8K1_ti7asfAev=Htb3+_xicChYgtw@mail.gmail.com>
 <38e0a961-318d-4b05-99d3-e824480712a1@measurement-factory.com>
 <CADxZ=iWJ_g8KQ1wYoah-EcLXuAx1MgXqJhCFoJ8xfg3t6GZmug@mail.gmail.com>
 <13dfcdf2-b207-49ae-8b1e-9b30d25ead10@measurement-factory.com>
 <CADxZ=iVv0FLTTKNng0fQ961SU7UeOUrdHbk9-P1ixDeEUX-=mA@mail.gmail.com>
 <dd47accd-50c6-4515-9f89-a430de327308@treenet.co.nz>
Message-ID: <CADxZ=iVMCJ-_zDWU7xVdvs1OM68cAwijjqShmuBt64AWkzp1fw@mail.gmail.com>

Thank you Amos and Rafael,

Using the LinuxDnat approach worked great as well.

On Sat, Sep 30, 2023 at 5:18?AM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 30/09/23 11:06, Fernando Giorgetti wrote:
> > If someone has already done that, with the client running in a different
> > machine, I would love to know how.
>
>
> There are several ways;
>
>   1) run Squid on the gateway router for your network, or
>
>   2) place Squid in a DMZ between the LAN gateway and WAN gateway.
>
>   3) setup a custom route+gateway for port 80 and 443 LAN traffic as the
> Squid machine. Excluding traffic from that machine itself.
>
>
> >
> > In case Squid runs on the same machine used as a network gateway to the
> > client machine, I suppose the config would be similar, but if it's not
> > running on the same machine used as the gateway, then it would be nice
> > to see how.
> >
>
> That would be (1). See
> <https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat> for
> how to configure the gateway router running Squid.
>
> The configuration difference between the at-source (aka, on client
> machine) you are/were using is just some iptables rules.
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231002/57620584/attachment.htm>

From david at articatech.com  Mon Oct  2 19:32:11 2023
From: david at articatech.com (David Touzeau)
Date: Mon, 2 Oct 2023 21:32:11 +0200
Subject: [squid-users] Unable to start Squid 6.3 "earlyMessages->size() <
 1000"
Message-ID: <06dcd751-ab0e-6509-fe3b-41fc53573881@articatech.com>


Hi

Since Squid 6.x we have this strange behavior on acl dst
Many warnings is generated

2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.226' 
from the ACL named 'GlobalWhitelistDSTNet'
2023/10/02 20:18:50| WARNING: (B) '64.34.72.226' is a subnetwork of (A) 
'64.34.72.226'
2023/10/02 20:18:50| WARNING: because of this '64.34.72.226' is ignored 
to keep splay tree searching predictable
2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.226' 
from the ACL named 'GlobalWhitelistDSTNet'


(B) '*64.34.72.226*' is a subnetwork of (A) '*64.34.72.226*' --> Sure, 
this is the IP address.

You should probably remove '64.34.72.226' from the ACL named 
'GlobalWhitelistDSTNet' --> Why this is only the IP address in the acl ???


2023/10/02 20:18:50| WARNING: (B) '64.34.72.230' is a subnetwork of (A) 
'64.34.72.230'
2023/10/02 20:18:50| WARNING: because of this '64.34.72.230' is ignored 
to keep splay tree searching predictable
2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.230' 
from the ACL named 'GlobalWhitelistDSTNet'
2023/10/02 20:18:50| WARNING: (B) '64.34.72.230' is a subnetwork of (A) 
'64.34.72.230'
2023/10/02 20:18:50| WARNING: because of this '64.34.72.230' is ignored 
to keep splay tree searching predictable
2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.230' 
from the ACL named 'GlobalWhitelistDSTNet'
2023/10/02 20:18:50| WARNING: (B) '64.34.72.232' is a subnetwork of (A) 
'64.34.72.232'

According to all warning, Squid won't start with this error

*2023/10/02 20:20:09| FATAL: assertion failed: debug.cc:606: 
"earlyMessages->size() < 1000"**
**Aborted*

How to avoid this ??

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231002/2ffd7ef5/attachment.htm>

From rousskov at measurement-factory.com  Mon Oct  2 20:01:19 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 2 Oct 2023 16:01:19 -0400
Subject: [squid-users] Unable to start Squid 6.3 "earlyMessages->size()
 < 1000"
In-Reply-To: <06dcd751-ab0e-6509-fe3b-41fc53573881@articatech.com>
References: <06dcd751-ab0e-6509-fe3b-41fc53573881@articatech.com>
Message-ID: <a9b7800e-653f-4c84-8ce8-7188c389dccc@measurement-factory.com>


> Since Squid 6.x we have this strange behavior on acl dst
> Many warnings is generated
> 
> 2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.226' from the ACL named 'GlobalWhitelistDSTNet'
> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.226' is a subnetwork of (A) '64.34.72.226'
> 2023/10/02 20:18:50| WARNING: because of this '64.34.72.226' is ignored to keep splay tree searching predictable

> (B) '*64.34.72.226*' is a subnetwork of (A) '*64.34.72.226*' --> Sure, 
> this is the IP address.

Is it possible that you have two 64.34.72.226 entries in that 
GlobalWhitelistDSTNet ACL? Perhaps in another included configuration 
file or something like that?


> You should probably remove '64.34.72.226' from the ACL named 
> 'GlobalWhitelistDSTNet' --> Why this is only the IP address in the acl ???

Squid thinks that there is more than one copy of 64.34.72.226 address in 
GlobalWhitelistDSTNet ACL. It could be Squid bug, of course. Please 
share a configuration that reproduces the issue or a pointer to 
compressed "squid -N -X -d9 ..." output while reproducing the problem.


>> 2023/10/02 20:20:09| FATAL: assertion failed: debug.cc:606: "earlyMessages->size() < 1000"
>> Aborted

This assert is a side effect of the above ACL problem/bug - you probably 
have many IPs in that ACL and the corresponding WARNINGs exceed Squid 
hard-coded message accumulation limit. Now that we know how a broken(*) 
configuration can produce so many early cache.log messages, we should 
probably modify Squid to quit without asserting, but let's focus on the 
root cause of your problems -- those WARNING messages.

(*) I am not implying that _your_ configuration is broken.


Cheers,

Alex.


> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.230' is a subnetwork of (A) 
> '64.34.72.230'
> 2023/10/02 20:18:50| WARNING: because of this '64.34.72.230' is ignored 
> to keep splay tree searching predictable
> 2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.230' 
> from the ACL named 'GlobalWhitelistDSTNet'
> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.230' is a subnetwork of (A) 
> '64.34.72.230'
> 2023/10/02 20:18:50| WARNING: because of this '64.34.72.230' is ignored 
> to keep splay tree searching predictable
> 2023/10/02 20:18:50| WARNING: You should probably remove '64.34.72.230' 
> from the ACL named 'GlobalWhitelistDSTNet'
> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.232' is a subnetwork of (A) 
> '64.34.72.232'
> 
> According to all warning, Squid won't start with this error
> 
> *2023/10/02 20:20:09| FATAL: assertion failed: debug.cc:606: 
> "earlyMessages->size() < 1000"**
> **Aborted*
> 
> How to avoid this ??
> 
> -- 
> David Touzeau - Artica Tech France
> Development team, level 3 support
> ----------------------------------
> P: +33 6 58 44 69 46
> www:https://wiki.articatech.com
> www:http://articatech.net  
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From david at articatech.com  Mon Oct  2 23:12:08 2023
From: david at articatech.com (David Touzeau)
Date: Tue, 3 Oct 2023 01:12:08 +0200
Subject: [squid-users] Unable to start Squid 6.3 "earlyMessages->size()
 < 1000"
In-Reply-To: <a9b7800e-653f-4c84-8ce8-7188c389dccc@measurement-factory.com>
References: <06dcd751-ab0e-6509-fe3b-41fc53573881@articatech.com>
 <a9b7800e-653f-4c84-8ce8-7188c389dccc@measurement-factory.com>
Message-ID: <f7dd3b91-7624-8cd1-5fea-0d195b2ddf2a@articatech.com>

Thank you, you've enlightened me;
I had the GlobalWhitelistDSTNet directive declared twice in two 
different includes
This meant that an identical Acl declared in two different places would 
contradict each other on the same addresses and generate mass warnings.

On 02/10/2023 22:01, Alex Rousskov wrote:
>
>> Since Squid 6.x we have this strange behavior on acl dst
>> Many warnings is generated
>>
>> 2023/10/02 20:18:50| WARNING: You should probably remove 
>> '64.34.72.226' from the ACL named 'GlobalWhitelistDSTNet'
>> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.226' is a subnetwork of 
>> (A) '64.34.72.226'
>> 2023/10/02 20:18:50| WARNING: because of this '64.34.72.226' is 
>> ignored to keep splay tree searching predictable
>
>> (B) '*64.34.72.226*' is a subnetwork of (A) '*64.34.72.226*' --> 
>> Sure, this is the IP address.
>
> Is it possible that you have two 64.34.72.226 entries in that 
> GlobalWhitelistDSTNet ACL? Perhaps in another included configuration 
> file or something like that?
>
>
>> You should probably remove '64.34.72.226' from the ACL named 
>> 'GlobalWhitelistDSTNet' --> Why this is only the IP address in the 
>> acl ???
>
> Squid thinks that there is more than one copy of 64.34.72.226 address 
> in GlobalWhitelistDSTNet ACL. It could be Squid bug, of course. Please 
> share a configuration that reproduces the issue or a pointer to 
> compressed "squid -N -X -d9 ..." output while reproducing the problem.
>
>
>>> 2023/10/02 20:20:09| FATAL: assertion failed: debug.cc:606: 
>>> "earlyMessages->size() < 1000"
>>> Aborted
>
> This assert is a side effect of the above ACL problem/bug - you 
> probably have many IPs in that ACL and the corresponding WARNINGs 
> exceed Squid hard-coded message accumulation limit. Now that we know 
> how a broken(*) configuration can produce so many early cache.log 
> messages, we should probably modify Squid to quit without asserting, 
> but let's focus on the root cause of your problems -- those WARNING 
> messages.
>
> (*) I am not implying that _your_ configuration is broken.
>
>
> Cheers,
>
> Alex.
>
>
>> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.230' is a subnetwork of 
>> (A) '64.34.72.230'
>> 2023/10/02 20:18:50| WARNING: because of this '64.34.72.230' is 
>> ignored to keep splay tree searching predictable
>> 2023/10/02 20:18:50| WARNING: You should probably remove 
>> '64.34.72.230' from the ACL named 'GlobalWhitelistDSTNet'
>> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.230' is a subnetwork of 
>> (A) '64.34.72.230'
>> 2023/10/02 20:18:50| WARNING: because of this '64.34.72.230' is 
>> ignored to keep splay tree searching predictable
>> 2023/10/02 20:18:50| WARNING: You should probably remove 
>> '64.34.72.230' from the ACL named 'GlobalWhitelistDSTNet'
>> 2023/10/02 20:18:50| WARNING: (B) '64.34.72.232' is a subnetwork of 
>> (A) '64.34.72.232'
>>
>> According to all warning, Squid won't start with this error
>>
>> *2023/10/02 20:20:09| FATAL: assertion failed: debug.cc:606: 
>> "earlyMessages->size() < 1000"**
>> **Aborted*
>>
>> How to avoid this ??
>>
>> -- 
>> David Touzeau - Artica Tech France
>> Development team, level 3 support
>> ----------------------------------
>> P: +33 6 58 44 69 46
>> www:https://wiki.articatech.com
>> www:http://articatech.net
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231003/9b33eb27/attachment.htm>

From ludovit.koren at gmail.com  Thu Oct  5 06:30:57 2023
From: ludovit.koren at gmail.com (Ludovit Koren)
Date: Thu, 05 Oct 2023 08:30:57 +0200
Subject: [squid-users] squid 5.9 Kerberos authentication problem
Message-ID: <86edi97f8e.fsf@gmail.com>


Hello,

I am using squid 5.9 with AD Kerberos authentication and could not solve
the problem of sending incorrect request according to client
configuration followed by the correct one, i.e.:

1695983264.808      0 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983264.834     21 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -

There are some web servers which are not working even when the correct
request follows afterwards.

Anyone experienced this behavior? Any specific settings of Windows
clients?


Thank you very much.

Regards,

lk



From squid3 at treenet.co.nz  Thu Oct  5 12:58:37 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Oct 2023 01:58:37 +1300
Subject: [squid-users] squid 5.9 Kerberos authentication problem
In-Reply-To: <86edi97f8e.fsf@gmail.com>
References: <86edi97f8e.fsf@gmail.com>
Message-ID: <e7b203ed-a893-464b-a615-bc9e97a28ccf@treenet.co.nz>

On 5/10/23 19:30, Ludovit Koren wrote:
> 
> Hello,
> 
> I am using squid 5.9 with AD Kerberos authentication and could not solve
> the problem of sending incorrect request according to client
> configuration followed by the correct one, i.e.:
> 
> 1695983264.808      0 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
> 1695983264.834     21 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
> 

This looks fine to me. The first request is sent without credentials, 
then the second contains the correct ones using the correct 
authentication scheme.

TL;DR ... I would only be worried if there were sequences 2+ of these 
407 before the final 200 status.


CONNECT tunnels are typically opened on a brand new TCP connection. Also 
the Negotiate authentication scheme used for Kerberos requires a unique 
token per-connection which is only received in the first HTTP response 
from Squid to client. Meaning the full 2-stage authentication process is 
needed every time for it to figure out how it is supposed to 
authenticate on that TCP connection.

Compared to other HTTP requests which often re-use an already 
authenticated TCP connection. So they get away with assuming the offered 
authentication schemes, and/or Kerberos token, will remain unchanged. 
Allowing the negotiation stage to be skipped.


If you are worried; you can try running the testing/troubleshooting 
checks detailed at 
<https://wiki.squid-cache.org/Features/NegotiateAuthentication#testing>


> There are some web servers which are not working even when the correct
> request follows afterwards.
> 

The TCP connection between client and Squid is different (and 
independent) from the TCP connections between Squid and servers.
The authentication you are using is only between client and Squid, it 
has nothing to do with web servers.


Cheers
Amos


From ludovit.koren at gmail.com  Thu Oct  5 17:15:08 2023
From: ludovit.koren at gmail.com (Ludovit Koren)
Date: Thu, 05 Oct 2023 19:15:08 +0200
Subject: [squid-users] squid 5.9 Kerberos authentication problem
In-Reply-To: <e7b203ed-a893-464b-a615-bc9e97a28ccf@treenet.co.nz> (Amos
 Jeffries's message of "Fri, 6 Oct 2023 01:58:37 +1300")
References: <86edi97f8e.fsf@gmail.com>
 <e7b203ed-a893-464b-a615-bc9e97a28ccf@treenet.co.nz>
Message-ID: <86a5sx6ler.fsf@gmail.com>

>>>>> Amos Jeffries <squid3 at treenet.co.nz> writes:

    > On 5/10/23 19:30, Ludovit Koren wrote:
    >> Hello,
    >> I am using squid 5.9 with AD Kerberos authentication and could not
    >> solve
    >> the problem of sending incorrect request according to client
    >> configuration followed by the correct one, i.e.:
    >> 1695983264.808      0 x.y.z TCP_DENIED/407 4135 CONNECT
    >> th.bing.com:443 - HIER_NONE/- text/html
    >> 1695983264.834     21 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
    >> 

    > This looks fine to me. The first request is sent without credentials,
    > then the second contains the correct ones using the correct 
    > authentication scheme.

ok, this is little bit longer output:

1695983167.151      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.154      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.155      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.155      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.163      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.837      0 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983167.842      1 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983167.873     27 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983168.440      0 x.y.z TCP_DENIED/407 4139 CONNECT www.bing.com:443 - HIER_NONE/- text/html
1695983181.337 103937 x.y.z TCP_TUNNEL/200 7562 CONNECT browser.events.data.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983181.367     15 x.y.z TCP_DENIED/407 4254 CONNECT assets.msn.com:443 - HIER_NONE/- text/html
1695983181.367     28 x.y.z TCP_DENIED/407 4254 CONNECT assets.msn.com:443 - HIER_NONE/- text/html
1695983181.504     24 x.y.z TCP_DENIED/407 4306 CONNECT browser.events.data.msn.com:443 - HIER_NONE/- text/html
1695983181.504     26 x.y.z TCP_DENIED/407 4306 CONNECT browser.events.data.msn.com:443 - HIER_NONE/- text/html
1695983181.559      0 x.y.z TCP_DENIED/407 4306 CONNECT browser.events.data.msn.com:443 - HIER_NONE/- text/html
1695983181.662      5 x.y.z TCP_DENIED/407 4234 CONNECT c.msn.com:443 - HIER_NONE/- text/html
1695983181.847      5 x.y.z TCP_DENIED/407 4238 CONNECT c.bing.com:443 - HIER_NONE/- text/html
1695983182.031     12 x.y.z TCP_DENIED/407 4242 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983194.404      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983200.151      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983201.409  20034 x.y.z TCP_TUNNEL/200 4166 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983202.682 131076 x.y.z TCP_TUNNEL/200 6868 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983205.701      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983221.409      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983233.498   5002 x.y.z TCP_DENIED/407 4139 CONNECT www.bing.com:443 - HIER_NONE/- text/html
1695983241.717  77946 x.y.z TCP_TUNNEL/200 7596 CONNECT edge.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.717  76955 x.y.z TCP_TUNNEL/200 58182 CONNECT ntp.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.717  76795 x.y.z TCP_TUNNEL/200 8279 CONNECT api.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  76731 x.y.z TCP_TUNNEL/200 110495 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  74275 x.y.z TCP_TUNNEL/200 6756 CONNECT edge.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.718  76590 x.y.z TCP_TUNNEL/200 42623 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  73876 x.y.z TCP_TUNNEL/200 95997 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  74645 x.y.z TCP_TUNNEL/200 18121 CONNECT business.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  18104 x.y.z TCP_TUNNEL/200 59080 CONNECT edge.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.719  18234 x.y.z TCP_TUNNEL/200 7147 CONNECT go.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.719  76826 x.y.z TCP_TUNNEL/200 7443 CONNECT deff.nelreports.net:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74620 x.y.z TCP_TUNNEL/200 8392 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74562 x.y.z TCP_TUNNEL/200 4477 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74552 x.y.z TCP_TUNNEL/200 4476 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74547 x.y.z TCP_TUNNEL/200 11278 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74552 x.y.z TCP_TUNNEL/200 66247 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -


In the gw1.ris.datacentrum.sk, there is authentication on the site
inside SSL. It is not working. As soon as I exclude
gw1.ris.datacentrum.sk from the authentication in squid, it starts
working.

    > TL;DR ... I would only be worried if there were sequences 2+ of these
    > 407 before the final 200 status.


    > CONNECT tunnels are typically opened on a brand new TCP
    > connection. Also the Negotiate authentication scheme used for Kerberos
    > requires a unique token per-connection which is only received in the
    > first HTTP response from Squid to client. Meaning the full 2-stage
    > authentication process is needed every time for it to figure out how
    > it is supposed to authenticate on that TCP connection.

    > Compared to other HTTP requests which often re-use an already
    > authenticated TCP connection. So they get away with assuming the
    > offered authentication schemes, and/or Kerberos token, will remain
    > unchanged. Allowing the negotiation stage to be skipped.


    > If you are worried; you can try running the testing/troubleshooting
    > checks detailed at 
    > <https://wiki.squid-cache.org/Features/NegotiateAuthentication#testing>


I have no access to a windows machine. As soon as I get an access to a
windows machine, I can do the tests.


    >> There are some web servers which are not working even when the correct
    >> request follows afterwards.
    >> 

    > The TCP connection between client and Squid is different (and
    > independent) from the TCP connections between Squid and servers.
    > The authentication you are using is only between client and Squid, it
    > has nothing to do with web servers.

Yes, I understand this.

The problem is, the client cannot access the above mentioned site with
authentication in squid.


Regards,

lk


From andre.bolinhas at articatech.com  Mon Oct  9 04:03:04 2023
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Mon, 9 Oct 2023 05:03:04 +0100
Subject: [squid-users] Squid 4.17 read/write failure
Message-ID: <d62f282d-1e38-45f4-93eb-3d2139adefc3@articatech.com>

Hi

I'm using squid 4.17 and sometimes the internet surf drop for a while 
and then back again, i manage to catch this error on cache.log

2023/10/09 10:05:43 kid5| local=10.30.2.34:3128 
remote=10.188.100.99:50047 FD 3997 flags=1: read/write failure: (110) 
Connection timed out
2023/10/09 10:05:47 kid2| local=10.30.2.34:3128 
remote=10.188.100.99:50048 FD 8163 flags=1: read/write failure: (110) 
Connection timed out
2023/10/09 10:06:05 kid5| local=10.30.2.34:3128 
remote=168.111.22.10:54191 FD 15965 flags=1: read/write failure: (110) 
Connection timed out
2023/10/09 10:06:07 kid5| local=10.30.2.34:3128 remote=10.43.2.251:56103 
FD 16874 flags=1: read/write failure: (32) Broken pipe
2023/10/09 10:06:14 kid4| local=10.132.18.57:8684 
remote=23.49.60.193:443 FD 17055 flags=1: read/write failure: (32) 
Broken pipe
2023/10/09 10:06:49 kid1| local=10.132.18.57:57088 
remote=23.49.60.193:443 FD 13945 flags=1: read/write failure: (32) 
Broken pipe

...


Can you help to understand this issue?
This is a problem on Squid or Squid box or something related with 
network, firewall, ISP??

Best regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231009/6a0370ca/attachment.htm>

From ludovit.koren at gmail.com  Tue Oct 10 09:23:27 2023
From: ludovit.koren at gmail.com (Ludovit Koren)
Date: Tue, 10 Oct 2023 11:23:27 +0200
Subject: [squid-users] squid 5.9 Kerberos authentication problem
Message-ID: <86leca4yr4.fsf@gmail.com>



Hi,

I am sorry to bother you once again, but I sent you and described just
the problem you were talking about and did not get any answer.

In the logged output you can see several 407 and afterwards 200 error
code just as you described, that the state you worry about.

Is there any solution to this?

Regards,

lk


>>>>> Amos Jeffries <squid3 at treenet.co.nz> writes:

    > On 5/10/23 19:30, Ludovit Koren wrote:
    >> Hello,
    >> I am using squid 5.9 with AD Kerberos authentication and could not
    >> solve
    >> the problem of sending incorrect request according to client
    >> configuration followed by the correct one, i.e.:
    >> 1695983264.808      0 x.y.z TCP_DENIED/407 4135 CONNECT
    >> th.bing.com:443 - HIER_NONE/- text/html
    >> 1695983264.834     21 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
    >> 

    > This looks fine to me. The first request is sent without credentials,
    > then the second contains the correct ones using the correct 
    > authentication scheme.

ok, this is little bit longer output:

1695983167.151      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.154      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.155      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.155      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.163      0 x.y.z TCP_DENIED/407 4179 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983167.837      0 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983167.842      1 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983167.873     27 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983168.440      0 x.y.z TCP_DENIED/407 4139 CONNECT www.bing.com:443 - HIER_NONE/- text/html
1695983181.337 103937 x.y.z TCP_TUNNEL/200 7562 CONNECT browser.events.data.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983181.367     15 x.y.z TCP_DENIED/407 4254 CONNECT assets.msn.com:443 - HIER_NONE/- text/html
1695983181.367     28 x.y.z TCP_DENIED/407 4254 CONNECT assets.msn.com:443 - HIER_NONE/- text/html
1695983181.504     24 x.y.z TCP_DENIED/407 4306 CONNECT browser.events.data.msn.com:443 - HIER_NONE/- text/html
1695983181.504     26 x.y.z TCP_DENIED/407 4306 CONNECT browser.events.data.msn.com:443 - HIER_NONE/- text/html
1695983181.559      0 x.y.z TCP_DENIED/407 4306 CONNECT browser.events.data.msn.com:443 - HIER_NONE/- text/html
1695983181.662      5 x.y.z TCP_DENIED/407 4234 CONNECT c.msn.com:443 - HIER_NONE/- text/html
1695983181.847      5 x.y.z TCP_DENIED/407 4238 CONNECT c.bing.com:443 - HIER_NONE/- text/html
1695983182.031     12 x.y.z TCP_DENIED/407 4242 CONNECT th.bing.com:443 - HIER_NONE/- text/html
1695983194.404      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983200.151      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983201.409  20034 x.y.z TCP_TUNNEL/200 4166 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983202.682 131076 x.y.z TCP_TUNNEL/200 6868 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983205.701      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983221.409      0 x.y.z TCP_DENIED/407 3952 CONNECT gw1.ris.datacentrum.sk:443 - HIER_NONE/- text/html
1695983233.498   5002 x.y.z TCP_DENIED/407 4139 CONNECT www.bing.com:443 - HIER_NONE/- text/html
1695983241.717  77946 x.y.z TCP_TUNNEL/200 7596 CONNECT edge.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.717  76955 x.y.z TCP_TUNNEL/200 58182 CONNECT ntp.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.717  76795 x.y.z TCP_TUNNEL/200 8279 CONNECT api.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  76731 x.y.z TCP_TUNNEL/200 110495 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  74275 x.y.z TCP_TUNNEL/200 6756 CONNECT edge.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.718  76590 x.y.z TCP_TUNNEL/200 42623 CONNECT assets.msn.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  73876 x.y.z TCP_TUNNEL/200 95997 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  74645 x.y.z TCP_TUNNEL/200 18121 CONNECT business.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.718  18104 x.y.z TCP_TUNNEL/200 59080 CONNECT edge.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.719  18234 x.y.z TCP_TUNNEL/200 7147 CONNECT go.microsoft.com:443 - FIRSTUP_PARENT/squid-parent -
1695983241.719  76826 x.y.z TCP_TUNNEL/200 7443 CONNECT deff.nelreports.net:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74620 x.y.z TCP_TUNNEL/200 8392 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74562 x.y.z TCP_TUNNEL/200 4477 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74552 x.y.z TCP_TUNNEL/200 4476 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74547 x.y.z TCP_TUNNEL/200 11278 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -
1695983241.719  74552 x.y.z TCP_TUNNEL/200 66247 CONNECT gw1.ris.datacentrum.sk:443 name at domain FIRSTUP_PARENT/squid-parent -


In the gw1.ris.datacentrum.sk, there is authentication on the site
inside SSL. It is not working. As soon as I exclude
gw1.ris.datacentrum.sk from the authentication in squid, it starts
working.

    > TL;DR ... I would only be worried if there were sequences 2+ of these
    > 407 before the final 200 status.


    > CONNECT tunnels are typically opened on a brand new TCP
    > connection. Also the Negotiate authentication scheme used for Kerberos
    > requires a unique token per-connection which is only received in the
    > first HTTP response from Squid to client. Meaning the full 2-stage
    > authentication process is needed every time for it to figure out how
    > it is supposed to authenticate on that TCP connection.

    > Compared to other HTTP requests which often re-use an already
    > authenticated TCP connection. So they get away with assuming the
    > offered authentication schemes, and/or Kerberos token, will remain
    > unchanged. Allowing the negotiation stage to be skipped.


    > If you are worried; you can try running the testing/troubleshooting
    > checks detailed at 
    > <https://wiki.squid-cache.org/Features/NegotiateAuthentication#testing>


I have no access to a windows machine. As soon as I get an access to a
windows machine, I can do the tests.


    >> There are some web servers which are not working even when the correct
    >> request follows afterwards.
    >> 

    > The TCP connection between client and Squid is different (and
    > independent) from the TCP connections between Squid and servers.
    > The authentication you are using is only between client and Squid, it
    > has nothing to do with web servers.

Yes, I understand this.

The problem is, the client cannot access the above mentioned site with
authentication in squid.


Regards,

lk



From squid3 at treenet.co.nz  Wed Oct 11 03:31:20 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Oct 2023 16:31:20 +1300
Subject: [squid-users] squid 5.9 Kerberos authentication problem
In-Reply-To: <86leca4yr4.fsf@gmail.com>
References: <86leca4yr4.fsf@gmail.com>
Message-ID: <322b5831-3f9b-4542-800e-006ccd02f152@treenet.co.nz>

On 10/10/23 22:23, Ludovit Koren wrote:
> 
> 
> Hi,
> 
> I am sorry to bother you once again, but I sent you and described just
> the problem you were talking about and did not get any answer.
> 

Sorry about that. Following up on the original thread in a short while.


PS. Normally no answer means nobody currently has a solution. In this 
case there is more troubleshooting that case be done to investigate, I 
just ran out of time to write it up for you.


Cheers
Amos


From andre.bolinhas at articatech.com  Thu Oct 12 11:42:41 2023
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Thu, 12 Oct 2023 12:42:41 +0100
Subject: [squid-users] Vey slow navigation
Message-ID: <5f0e1d4b-4896-413e-8b22-f4960967bf55@articatech.com>

Hi

I'm using Squid and sometimes my users are unable to access to internet 
or the internet access is very slow.

The error returned from the browser is

?This site can't be reached ? took too long?

On cache.log i get this errors very frequently

2023/10/12 10:23:49 kid5| local=10.30.2.33:3128 
remote=10.188.150.131:53305 FD 3379 flags=1: read/write failure: (32) 
Broken pipe
2023/10/12 10:23:53 kid4| local=10.30.2.33:3128 
remote=172.161.26.109:55123 FD 7784 flags=1: read/write failure: (32) 
Broken pipe
2023/10/12 10:24:11 kid1| local=10.30.2.33:3128 
remote=10.188.120.13:54408 FD 3116 flags=1: read/write failure: (32) 
Broken pipe
2023/10/12 10:24:26 kid4| local=10.30.2.33:3128 
remote=198.101.5.73:57952 FD 11987 flags=1: read/write failure: (32) 
Broken pipe

...

2023/10/12 10:47:29 kid2| local=10.30.2.33:3128 remote=167.2.22.36:64301 
FD 18774 flags=1: read/write failure: (110) Connection timed out
2023/10/12 10:50:28 kid2| local=10.30.2.33:3128 
remote=10.188.100.153:62550 FD 4240 flags=1: read/write failure: (110) 
Connection timed out
2023/10/12 10:50:30 kid4| local=10.30.2.33:3128 
remote=172.16.109.181:55468 FD 19204 flags=1: read/write failure: (110) 
Connection timed out

..


Can you help me to understand this?
It's a problem on squid box or something related with network / isp?

Best regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231012/265ceebe/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Oct 12 11:50:04 2023
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 12 Oct 2023 13:50:04 +0200
Subject: [squid-users] Vey slow navigation
In-Reply-To: <5f0e1d4b-4896-413e-8b22-f4960967bf55@articatech.com>
References: <5f0e1d4b-4896-413e-8b22-f4960967bf55@articatech.com>
Message-ID: <202310121350.04953.Antony.Stone@squid.open.source.it>

On Thursday 12 October 2023 at 13:42:41, Andre Bolinhas wrote:

> Hi
> 
> I'm using Squid and sometimes my users are unable to access to internet
> or the internet access is very slow.

Have you tried accessing the same sites (preferably at the same time) from a 
machine which does not use Squid?

I would start from there to identify whether Squid is causing the problem.


Antony.

-- 
Tinned food was developed for the British Navy in 1813.

The tin opener was not invented until 1858.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Ralf.Hildebrandt at charite.de  Thu Oct 12 11:53:02 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 12 Oct 2023 13:53:02 +0200
Subject: [squid-users] Squid Caching Proxy Security Audit: 55
 vulnerabilities and 35 0days
Message-ID: <ZSfeHoXnOZ+MEuMB@charite.de>

This caught my attention:
https://github.com/MegaManSec/Squid-Security-Audit
-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From squid3 at treenet.co.nz  Thu Oct 12 16:04:45 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Oct 2023 05:04:45 +1300
Subject: [squid-users] squid 5.9 Kerberos authentication problem
In-Reply-To: <86a5sx6ler.fsf@gmail.com>
References: <86edi97f8e.fsf@gmail.com>
 <e7b203ed-a893-464b-a615-bc9e97a28ccf@treenet.co.nz>
 <86a5sx6ler.fsf@gmail.com>
Message-ID: <be2f6fe3-c71f-4c28-907c-eeee287c4406@treenet.co.nz>

On 6/10/23 06:15, Ludovit Koren wrote:
>>>>>> Amos Jeffries writes:
> 
>      > On 5/10/23 19:30, Ludovit Koren wrote:
>      >> Hello,
>      >> I am using squid 5.9 with AD Kerberos authentication and could not
>      >> solve
>      >> the problem of sending incorrect request according to client
>      >> configuration followed by the correct one, i.e.:
>      >> 1695983264.808      0 x.y.z TCP_DENIED/407 4135 CONNECT
>      >> th.bing.com:443 - HIER_NONE/- text/html
>      >> 1695983264.834     21 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -
>      >>
> 
>      > This looks fine to me. The first request is sent without credentials,
>      > then the second contains the correct ones using the correct
>      > authentication scheme.
> 
> ok, this is little bit longer output:


> 1695983167.837      0 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
> 1695983167.842      1 x.y.z TCP_DENIED/407 4135 CONNECT th.bing.com:443 - HIER_NONE/- text/html
> 1695983167.873     27 x.y.z TCP_TUNNEL/200 6080 CONNECT th.bing.com:443 name at domain FIRSTUP_PARENT/squid-parent -

Taking this set of th.bing.com requests as clearly a bunch related they 
look like an NTLM or Negotiate/NTLM authentication sequence.


The rest of the log entries are a little too spread out with a mix of 
domains to tell where the connections are.

Also, the 200 status CONNECT tunnels in this log extract were all 
running from a time before the first line of the log snippet. So we 
cannot see how they reached 200 status.


> 
> In the gw1.ris.datacentrum.sk, there is authentication on the site
> inside SSL. It is not working.

FYI, "inside SSL" is just opaque bytes to Squid. Any failure there is 
between the client and server at the other end of the CONNECT tunnel. 
Nothing to do with this Squid.


> As soon as I exclude
> gw1.ris.datacentrum.sk from the authentication in squid, it starts
> working.

That is an indication that the client software is unable to handle 
authentication on the CONNECT tunnel properly.



For better troubleshooting there are several steps to take:

* making a custom log format and a debug log for your Squid would be 
useful to get more details about each transaction.

  I suggest adding this to your squid.conf:

  logformat debug %ts.%03tu %6tr %>a cid=%>p_%lp_%ssl::bump_mode \
     %Ss/%03>Hs %<st %rm %ru \
     user=%[un/login=%[ul/token=%[credentials %Sh/%<a/%03<Hs

  access_log /var/log/squid/debug.log debug


The "cid=" entry should be a semi-unique value per TCP connection. It is 
not true unique since ports get re-used, but should be reliable enough 
to separate overlapping connections with duplicate request URLs.

The user=/login=/token= part should allow you to see what/why the 407 is 
occuring. You can investigate the token value with this tool 
<https://gist.github.com/aseering/829a2270b72345a1dc42> to see if it is 
truly a Negotiate/Kerberos token vs a Negotiate/NTLM one.



If you need more assistance, I/we will need to see your squid.conf (in 
full but without the "#" comment lines) and the output trace from that 
debug.log.

HTH
Amos


From ngtech1ltd at gmail.com  Thu Oct 12 18:58:35 2023
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Thu, 12 Oct 2023 21:58:35 +0300
Subject: [squid-users] Vey slow navigation
In-Reply-To: <5f0e1d4b-4896-413e-8b22-f4960967bf55@articatech.com>
References: <5f0e1d4b-4896-413e-8b22-f4960967bf55@articatech.com>
Message-ID: <000001d9fd3e$1b32bd10$51983730$@gmail.com>

Hey Andre,

The issue can be caused by couple technical reasons.
If we want to find one of the reasons we first need to understand the setup.
Lets start with the ISP part of the picture:
Where the public IP is residing? On the Squid box or on a NAT gateway?
```
$ ip route show
```

Also, did you made any fine tunning to the OS networking stack?
What OS are you using?
What version of squid? Is it self compiled or pre-packaged binary?
```
squid -v
```

Also, do you have any dummy website locally you can try to verify which works or not?
You can try to install nginx or apache on the squid box and to try accessing the local ip or domain on port 80 and see what happens.
Also, is this squid a simple forward or intercept proxy?

With the above details I believe we can start thinking about the technical options of the issue.

Eliezer


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Andre Bolinhas
Sent: Thursday, October 12, 2023 14:43
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Vey slow navigation

Hi
I'm using Squid and sometimes my users are unable to access to internet or the internet access is very slow.
The error returned from the browser is
?This site can't be reached ? took too long?
On cache.log i get this errors very frequently
2023/10/12 10:23:49 kid5| local=10.30.2.33:3128 remote=10.188.150.131:53305 FD 3379 flags=1: read/write failure: (32) Broken pipe
2023/10/12 10:23:53 kid4| local=10.30.2.33:3128 remote=172.161.26.109:55123 FD 7784 flags=1: read/write failure: (32) Broken pipe
2023/10/12 10:24:11 kid1| local=10.30.2.33:3128 remote=10.188.120.13:54408 FD 3116 flags=1: read/write failure: (32) Broken pipe
2023/10/12 10:24:26 kid4| local=10.30.2.33:3128 remote=198.101.5.73:57952 FD 11987 flags=1: read/write failure: (32) Broken pipe
...
2023/10/12 10:47:29 kid2| local=10.30.2.33:3128 remote=167.2.22.36:64301 FD 18774 flags=1: read/write failure: (110) Connection timed out
2023/10/12 10:50:28 kid2| local=10.30.2.33:3128 remote=10.188.100.153:62550 FD 4240 flags=1: read/write failure: (110) Connection timed out
2023/10/12 10:50:30 kid4| local=10.30.2.33:3128 remote=172.16.109.181:55468 FD 19204 flags=1: read/write failure: (110) Connection timed out
..

Can you help me to understand this?
It's a problem on squid box or something related with network / isp?
Best regards



From rousskov at measurement-factory.com  Fri Oct 13 13:52:55 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 13 Oct 2023 09:52:55 -0400
Subject: [squid-users] RFC: Irreplaceable squidclient features
Message-ID: <374c8fb2-3d34-4dd0-b111-14d3a2d2696e@measurement-factory.com>

Hello,

     Francesco and I would like to remove squidclient tool from Squid so 
that we can divert resources to more important areas[1]. As far as we 
can tell, all essential squidclient functionality can be obtained via 
well-known command-line clients like curl, wget, nc, s_client, etc. For 
example, `mgr:foo` shortcuts can be replaced with URLs like 
`http://localhost:3128/squid-internal-mgr/foo` (in a script or shell 
alias if needed).

If you routinely use squidclient _and_ do not think you can replace it 
with another client, please respond and detail your use case!

We will collect use cases until the end of October 2023. If we then 
remove squidclient as currently planned, then that removal will _not_ 
affect Squid v6 and earlier releases, of course -- those releases will 
keep squidclient.


Thank you,

Alex.
P.S. This email thread is _not_ the right place to discuss squidclient 
_problems_, including Squid bug 5283[2]. Please focus on squidclient 
features that you use and consider irreplaceable.

[1] https://github.com/squid-cache/squid/pull/1514
[2] https://bugs.squid-cache.org/show_bug.cgi?id=5283


From squid.org at bloms.de  Fri Oct 13 15:19:33 2023
From: squid.org at bloms.de (Dieter Bloms)
Date: Fri, 13 Oct 2023 17:19:33 +0200
Subject: [squid-users] 2 year old security bugs not fixed?
Message-ID: <20231013151933.wcxrlifortdu227d@bloms.de>

Hello,

I stumbled across this page
https://joshua.hu/squid-security-audit-35-0days-45-exploits and wonder
if all these security holes are really still there.

Can someone from the developers give a status?

Thank you very much.

-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From squid3 at treenet.co.nz  Fri Oct 13 16:49:28 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Oct 2023 05:49:28 +1300
Subject: [squid-users] 2 year old security bugs not fixed?
In-Reply-To: <20231013151933.wcxrlifortdu227d@bloms.de>
References: <20231013151933.wcxrlifortdu227d@bloms.de>
Message-ID: <f2ec0432-8662-4617-9ed4-4cba5c68bf6b@treenet.co.nz>

On 14/10/23 04:19, Dieter Bloms wrote:
> Hello,
> 
> I stumbled across this page
> https://joshua.hu/squid-security-audit-35-0days-45-exploits and wonder
> if all these security holes are really still there.
> 
> Can someone from the developers give a status?
> 
> Thank you very much.
> 


We continue to close the vulnerabilities we can. In the order we deem 
most urgent based on what we know of common use cases for Squid.

Some issues listed are missing their fix references, so the situation is 
(slightly) better than first appearances.  Right now I am going through 
the list again cross-checking his given titles against our security team 
records to make sure all of them have had the appropriate triage done 
and get his CVE references updated.



To quote the article:

"
The Squid Team have been helpful and supportive during the process of 
reporting these issues. However, they are effectively understaffed, and 
simply do not have the resources to fix the discovered issues. Hammering 
them with demands to fix the issues won?t get far.
"

If anyone wishes to help please volunteer in squid-dev or squid-bugs 
mailing lists. <https://wiki.squid-cache.org/DeveloperResources/> has 
all the starter info.



Amos


From jtaylor.debian at googlemail.com  Fri Oct 13 20:01:02 2023
From: jtaylor.debian at googlemail.com (Julian Taylor)
Date: Fri, 13 Oct 2023 22:01:02 +0200
Subject: [squid-users] very poor performance of rock cache ipc
Message-ID: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>

Hello,
When using squid for caching using the rock cache_dir setting the 
performance is pretty poor with multiple workers.
The reason for this is due to the very high number of systemcalls 
involved in the IPC between the disker and workers.

You can reproduce this very easily with a simple setup with following 
configuration in the current git HEAD and older versions:

maximum_object_size 8 GB
cache_dir rock /cachedir/cache 1024
cache_peer some.host parent 80 3130 default no-query no-digest
http_port 3128

Now download a larger file from some.host through the cache so it cached 
and repeat.

curl --proxy localhost:3128  http://some.host/file >  /dev/null

The download of the cached file from the local machine will be performed 
with a very low rate, on my not ancient machine 35mb/s with everything 
is being cached in memory.

If you check what is happening in the disker you see that it reads a 
4112 byte ipc message from the worker, performs a read of 4KiB size then 
opens a new socket to notifies the worker, does 4 fcntl calls on the 
socket and then sends a 4112 byte (2 x86 pages) size ipc message and 
then closes the socket, this repeats for every 4KiB read and you have 
the same thing in the receiving worker side.

Here an strace of one chunk of the request in the disker:

21:49:28 epoll_wait(7, [{events=EPOLLIN, data={u32=26, u64=26}}], 65536, 
827) = 1 <0.000013>
21:49:28 recvmsg(26, {msg_name=0x557d7c4f06b8, msg_namelen=110 => 0, 
msg_iov=[{iov_base="\7\0\0\0\0\0\0\0\4\0\0\0\0\0\0\0\2\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 
iov_len=4112}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, 
MSG_DONTWAIT) = 4112 <0.000027>
21:49:28 pread64(19, 
"\266E\337\37\374\201b\215\240\310`\216\366\242\350\210\215\22\377zu\302\244Tb\317\255K\10\"p\327"..., 
4096, 10747944) = 4096 <0.000015>
21:49:28 socket(AF_UNIX, SOCK_DGRAM, 0) = 11 <0.000021>
21:49:28 fcntl(11, F_GETFD)             = 0 <0.000011>
21:49:28 fcntl(11, F_SETFD, FD_CLOEXEC) = 0 <0.000011>
21:49:28 fcntl(11, F_GETFL)             = 0x2 (flags O_RDWR) <0.000011>
21:49:28 fcntl(11, F_SETFL, O_RDWR|O_NONBLOCK) = 0 <0.000012>
21:49:28 epoll_ctl(7, EPOLL_CTL_ADD, 11, 
{events=EPOLLOUT|EPOLLERR|EPOLLHUP, data={u32=11, u64=11}}) = 0 <0.000023>
21:49:28 epoll_wait(7, [{events=EPOLLOUT, data={u32=11, u64=11}}], 
65536, 826) = 1 <0.000015>
21:49:28 sendmsg(11, {msg_name={sa_family=AF_UNIX, 
sun_path="/tmp/local/var/run/squid/squid-kid-2.ipc"}, msg_namelen=42, 
msg_iov=[{iov_base="\7\0\0\0\0\0\0\0\4\0\0\0\0\0\0\0\3\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 
iov_len=4112}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, 
MSG_NOSIGNAL) = 4112 <0.000022>
21:49:28 epoll_ctl(7, EPOLL_CTL_DEL, 11, 0x7ffef63da174) = 0 <0.000014>
21:49:28 close(11)                      = 0 <0.000018>


Pocking around a bit in the code I have found that by increasing the 
HTTP_REQBUF_SZ in src/http/forward.h to 32KiB also affects the read size 
on the disker making it 8 times more efficient which is ok (but not great).
(This does not work the same anymore with 
https://github.com/squid-cache/squid/pull/1335 recently added to 6.x 
backports, but the 4KiB issue remains in current master)

This problem is very noticeable on large objects but the extrem overhead 
per disk cache request should affect most disk cached objects.

Is it necessary to have these read chunks so small and the processes 
opening and closing sockets for every single request instead of reusing 
an open socket?
At least the 4 fcntl calls could be removed/reduced to 1 though that 
only gains 10-30% compared to 800% of increasing the read size.
Reducing the 4112 byte ipc message with only has 4 bytes of data to 
lower values also results in measurable improvements (though dangerous 
as squid crashes if its too low and receives cachemanager requests which 
seem to be around 600 bytes in length).

If the small chunk sizes are needed for certain use cases I would love a 
configuration flag to set it to higher values (higher even that the 
current maximum of mem::pagessize 32KiB) if that fits the use case. In 
the case I noticed this the average object size in the cache was in the 
megabyte range.

Currently without recompiling squid using the rock cache (the only one 
supported for SMP) utilizing modern hardware with 10G or more network 
and SSD disks does not seem feasible unless I missed some configuration 
option which may help here.

Cheers,
Julian


From rousskov at measurement-factory.com  Sat Oct 14 15:40:51 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 14 Oct 2023 11:40:51 -0400
Subject: [squid-users] very poor performance of rock cache ipc
In-Reply-To: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>
References: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>
Message-ID: <0e161590-206f-4d51-9fd0-1a767f0a1793@measurement-factory.com>

On 2023-10-13 16:01, Julian Taylor wrote:

> When using squid for caching using the rock cache_dir setting the 
> performance is pretty poor with multiple workers.
> The reason for this is due to the very high number of systemcalls 
> involved in the IPC between the disker and workers.

Please allow me to rephrase your conclusion to better match (expected) 
reality and avoid misunderstanding:

By design, a mostly idle SMP Squid should use a lot more system calls 
per disk cache hit than a busy SMP Squid would:

* Mostly idle Squid: Every disk I/O may require a few IPC messages.
* Busy Squid: Bugs notwithstanding, disk I/Os require no IPC messages.


In your single-request test, you are observing the expected effects 
described in the first bullet. That does not imply those effects are 
"good" or "desirable" in your use case, of course. It only means that 
SMP Squid was no optimized for that use case; SMP rock design was 
explicitly targeting the opposite use case (i.e. a busy Squid).

Roughly speaking, here, "busy" means "there are always some messages in 
the disk I/O queue [maintained by Squid in shared memory]".


You may wonder how it is possible that an increase in I/O work results 
in decrease (and, hopefully, elimination) of related IPC messages. 
Roughly speaking, a worker must send an IPC "you have a new I/O request" 
message only when its worker->disker queue is empty. If the queue is not 
empty, then there is no reason to send an IPC message to wake up disker 
because disker will see the new message when dequeuing the previous one. 
Same for the opposite direction: disker->worker...


 > Is it necessary to have these read chunks so small

It is not. Disk I/O size should be at least the system I/O page size, 
but it can be larger. The optimal I/O size is probably very dependent on 
traffic patterns. IIRC, Squid I/O size is at most one Squid page 
(SM_PAGE_SIZE or 4KB).

FWIW, I suspect there are significant inefficiencies in disk I/O related 
request alignment: The code does not attempt to read from and write to 
disk page boundaries, probably resulting in multiple low-level disk I/Os 
per one Squid 4KB I/O in some (many?) cases. With modern non-rotational 
storage these effects are probably less pronounced, but they probably 
still exist.

BTW, please note that, IIRC, workers and diskers do not send HTTP bytes 
using IPC messages. Those IPC messages only carry small metainformation 
about I/O. HTTP bytes are stored in shared memory pages. I do not recall 
why the corresponding disk I/O IPC messages are so big, but it is 
probably just a code simplification (because larger IPC messages are 
needed for cache manager queries).


HTH,

Alex.


> You can reproduce this very easily with a simple setup with following 
> configuration in the current git HEAD and older versions:
> 
> maximum_object_size 8 GB
> cache_dir rock /cachedir/cache 1024
> cache_peer some.host parent 80 3130 default no-query no-digest
> http_port 3128
> 
> Now download a larger file from some.host through the cache so it cached 
> and repeat.
> 
> curl --proxy localhost:3128? http://some.host/file >? /dev/null
> 
> The download of the cached file from the local machine will be performed 
> with a very low rate, on my not ancient machine 35mb/s with everything 
> is being cached in memory.
> 
> If you check what is happening in the disker you see that it reads a 
> 4112 byte ipc message from the worker, performs a read of 4KiB size then 
> opens a new socket to notifies the worker, does 4 fcntl calls on the 
> socket and then sends a 4112 byte (2 x86 pages) size ipc message and 
> then closes the socket, this repeats for every 4KiB read and you have 
> the same thing in the receiving worker side.
> 
> Here an strace of one chunk of the request in the disker:
> 
> 21:49:28 epoll_wait(7, [{events=EPOLLIN, data={u32=26, u64=26}}], 65536, 
> 827) = 1 <0.000013>
> 21:49:28 recvmsg(26, {msg_name=0x557d7c4f06b8, msg_namelen=110 => 0, 
> msg_iov=[{iov_base="\7\0\0\0\0\0\0\0\4\0\0\0\0\0\0\0\2\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., iov_len=4112}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, MSG_DONTWAIT) = 4112 <0.000027>
> 21:49:28 pread64(19, 
> "\266E\337\37\374\201b\215\240\310`\216\366\242\350\210\215\22\377zu\302\244Tb\317\255K\10\"p\327"..., 4096, 10747944) = 4096 <0.000015>
> 21:49:28 socket(AF_UNIX, SOCK_DGRAM, 0) = 11 <0.000021>
> 21:49:28 fcntl(11, F_GETFD)???????????? = 0 <0.000011>
> 21:49:28 fcntl(11, F_SETFD, FD_CLOEXEC) = 0 <0.000011>
> 21:49:28 fcntl(11, F_GETFL)???????????? = 0x2 (flags O_RDWR) <0.000011>
> 21:49:28 fcntl(11, F_SETFL, O_RDWR|O_NONBLOCK) = 0 <0.000012>
> 21:49:28 epoll_ctl(7, EPOLL_CTL_ADD, 11, 
> {events=EPOLLOUT|EPOLLERR|EPOLLHUP, data={u32=11, u64=11}}) = 0 <0.000023>
> 21:49:28 epoll_wait(7, [{events=EPOLLOUT, data={u32=11, u64=11}}], 
> 65536, 826) = 1 <0.000015>
> 21:49:28 sendmsg(11, {msg_name={sa_family=AF_UNIX, 
> sun_path="/tmp/local/var/run/squid/squid-kid-2.ipc"}, msg_namelen=42, 
> msg_iov=[{iov_base="\7\0\0\0\0\0\0\0\4\0\0\0\0\0\0\0\3\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., iov_len=4112}], msg_iovlen=1, msg_controllen=0, msg_flags=0}, MSG_NOSIGNAL) = 4112 <0.000022>
> 21:49:28 epoll_ctl(7, EPOLL_CTL_DEL, 11, 0x7ffef63da174) = 0 <0.000014>
> 21:49:28 close(11)????????????????????? = 0 <0.000018>
> 
> 
> Pocking around a bit in the code I have found that by increasing the 
> HTTP_REQBUF_SZ in src/http/forward.h to 32KiB also affects the read size 
> on the disker making it 8 times more efficient which is ok (but not great).
> (This does not work the same anymore with 
> https://github.com/squid-cache/squid/pull/1335 recently added to 6.x 
> backports, but the 4KiB issue remains in current master)
> 
> This problem is very noticeable on large objects but the extrem overhead 
> per disk cache request should affect most disk cached objects.
> 
> Is it necessary to have these read chunks so small and the processes 
> opening and closing sockets for every single request instead of reusing 
> an open socket?
> At least the 4 fcntl calls could be removed/reduced to 1 though that 
> only gains 10-30% compared to 800% of increasing the read size.
> Reducing the 4112 byte ipc message with only has 4 bytes of data to 
> lower values also results in measurable improvements (though dangerous 
> as squid crashes if its too low and receives cachemanager requests which 
> seem to be around 600 bytes in length).
> 
> If the small chunk sizes are needed for certain use cases I would love a 
> configuration flag to set it to higher values (higher even that the 
> current maximum of mem::pagessize 32KiB) if that fits the use case. In 
> the case I noticed this the average object size in the cache was in the 
> megabyte range.
> 
> Currently without recompiling squid using the rock cache (the only one 
> supported for SMP) utilizing modern hardware with 10G or more network 
> and SSD disks does not seem feasible unless I missed some configuration 
> option which may help here.
> 
> Cheers,
> Julian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From jtaylor.debian at googlemail.com  Sat Oct 14 16:04:00 2023
From: jtaylor.debian at googlemail.com (Julian Taylor)
Date: Sat, 14 Oct 2023 18:04:00 +0200
Subject: [squid-users] very poor performance of rock cache ipc
In-Reply-To: <0e161590-206f-4d51-9fd0-1a767f0a1793@measurement-factory.com>
References: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>
 <0e161590-206f-4d51-9fd0-1a767f0a1793@measurement-factory.com>
Message-ID: <8279acb8-45c0-4a26-a080-16ea52176283@googlemail.com>

On 14.10.23 17:40, Alex Rousskov wrote:
> On 2023-10-13 16:01, Julian Taylor wrote:
> 
>> When using squid for caching using the rock cache_dir setting the 
>> performance is pretty poor with multiple workers.
>> The reason for this is due to the very high number of systemcalls 
>> involved in the IPC between the disker and workers.
> 
> Please allow me to rephrase your conclusion to better match (expected) 
> reality and avoid misunderstanding:
> 
> By design, a mostly idle SMP Squid should use a lot more system calls 
> per disk cache hit than a busy SMP Squid would:
> 
> * Mostly idle Squid: Every disk I/O may require a few IPC messages.
> * Busy Squid: Bugs notwithstanding, disk I/Os require no IPC messages.
> 
> 
> In your single-request test, you are observing the expected effects 
> described in the first bullet. That does not imply those effects are 
> "good" or "desirable" in your use case, of course. It only means that 
> SMP Squid was no optimized for that use case; SMP rock design was 
> explicitly targeting the opposite use case (i.e. a busy Squid).

The reproducer uses as single request, the same very thing can be 
observed on a very busy squid and workaround improves both the single 
request case and the actual heavy loaded production squid in the same way.

The hardware involved has a 10G card, not ssds but lots of ram so it has 
a very high page cache hit rate and the squid is very busy, so much it 
is overloaded by system cpu usage in default configuration with the rock 
cache. The network or disk bandwidth is barely ever utilized more than 
10% with all 8 cpus busy on system load.
The only way to get the squid to utilize the machine is to increase the 
IO size via the request buffer change or not use the rock cache. UFS 
cache works ok in comparison, but requires multiple independent squid 
instances as it does not support SMP.

Increasing the IO size to 32KiB as I mentioned does allow the squid 
workers to utilize a good 60% of the hardware network and disk capabilities.

> 
> Roughly speaking, here, "busy" means "there are always some messages in 
> the disk I/O queue [maintained by Squid in shared memory]".
> 
> 
> You may wonder how it is possible that an increase in I/O work results 
> in decrease (and, hopefully, elimination) of related IPC messages. 
> Roughly speaking, a worker must send an IPC "you have a new I/O request" 
> message only when its worker->disker queue is empty. If the queue is not 
> empty, then there is no reason to send an IPC message to wake up disker 
> because disker will see the new message when dequeuing the previous one. 
> Same for the opposite direction: disker->worker...

This is probably true if you have slow disks and are actually IO bound, 
but on fast disks or high page cache hit rate you essential see this ipc 
ping pong and very little actual work being done.

> 
> 
>  > Is it necessary to have these read chunks so small
> 
> It is not. Disk I/O size should be at least the system I/O page size, 
> but it can be larger. The optimal I/O size is probably very dependent on 
> traffic patterns. IIRC, Squid I/O size is at most one Squid page 
> (SM_PAGE_SIZE or 4KB).
> 
> FWIW, I suspect there are significant inefficiencies in disk I/O related 
> request alignment: The code does not attempt to read from and write to 
> disk page boundaries, probably resulting in multiple low-level disk I/Os 
> per one Squid 4KB I/O in some (many?) cases. With modern non-rotational 
> storage these effects are probably less pronounced, but they probably 
> still exist.
The kernel drivers will mostly handle this for you if multiple requests 
are available, but this is also almost irrelevant with current hardware, 
typically it will be so fast software overhead will make it hard to 
utilize modern large disk arrays properly you probably need to look at 
other approaches like io_ring to get rid of the classical read/write 
systemcall overhead dominating your performance.


From rousskov at measurement-factory.com  Sun Oct 15 03:42:39 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 14 Oct 2023 23:42:39 -0400
Subject: [squid-users] very poor performance of rock cache ipc
In-Reply-To: <8279acb8-45c0-4a26-a080-16ea52176283@googlemail.com>
References: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>
 <0e161590-206f-4d51-9fd0-1a767f0a1793@measurement-factory.com>
 <8279acb8-45c0-4a26-a080-16ea52176283@googlemail.com>
Message-ID: <baae1a3e-330c-4022-8fdd-c55dafcafae1@measurement-factory.com>

On 2023-10-14 12:04, Julian Taylor wrote:
> On 14.10.23 17:40, Alex Rousskov wrote:
>> On 2023-10-13 16:01, Julian Taylor wrote:
>>
>>> When using squid for caching using the rock cache_dir setting the 
>>> performance is pretty poor with multiple workers.
>>> The reason for this is due to the very high number of systemcalls 
>>> involved in the IPC between the disker and workers.
>>
>> Please allow me to rephrase your conclusion to better match (expected) 
>> reality and avoid misunderstanding:
>>
>> By design, a mostly idle SMP Squid should use a lot more system calls 
>> per disk cache hit than a busy SMP Squid would:
>>
>> * Mostly idle Squid: Every disk I/O may require a few IPC messages.
>> * Busy Squid: Bugs notwithstanding, disk I/Os require no IPC messages.
>>
>>
>> In your single-request test, you are observing the expected effects 
>> described in the first bullet. That does not imply those effects are 
>> "good" or "desirable" in your use case, of course. It only means that 
>> SMP Squid was no optimized for that use case; SMP rock design was 
>> explicitly targeting the opposite use case (i.e. a busy Squid).
> 
> The reproducer uses as single request, the same very thing can be 
> observed on a very busy squid

If a busy Squid sends lots of IPC messages between worker and disker, 
then either there is a Squid bug we do not know about OR that disker is 
just not as busy as one might expect it to be.

In Squid v6+, you can observe disker queues using mgr:store_queues cache 
manager report. In your environment, do those queues always have lots of 
requests when Squid is busy? Feel free to share (a pointer to) a 
representative sample of those reports from your busy Squid.

N.B. Besides worker-disker IPC messages, there are also worker-worker 
cache synchronization IPC messages. They also have the same "do not send 
IPC messages if the queue has some pending items already" optimization.


> and workaround improves both the single 
> request case and the actual heavy loaded production squid in the same way.

FWIW, I do not think that observation contradicts anything I have said.


> The hardware involved has a 10G card, not ssds but lots of ram so it has 
> a very high page cache hit rate and the squid is very busy, so much it 
> is overloaded by system cpu usage in default configuration with the rock 
> cache. The network or disk bandwidth is barely ever utilized more than 
> 10% with all 8 cpus busy on system load.

The above facts suggest that the disk is just not used much OR there is 
a bug somewhere. Slower (for any reason, including CPU overload) IPC 
messages should lead to longer queues and the disappearance of "your 
queue is no longer empty!" IPC messages.


> The only way to get the squid to utilize the machine is to increase the 
> IO size via the request buffer change or not use the rock cache. UFS 
> cache works ok in comparison, but requires multiple independent squid 
> instances as it does not support SMP.
> 
> Increasing the IO size to 32KiB as I mentioned does allow the squid 
> workers to utilize a good 60% of the hardware network and disk 
> capabilities.

Please note that I am not disputing this observation. Unfortunately, it 
does not help me guess where the actual/core problem or bottleneck is. 
Hopefully, cache manager mgr:store_queues report will shed some light.


>> Roughly speaking, here, "busy" means "there are always some messages 
>> in the disk I/O queue [maintained by Squid in shared memory]".
>>
>> You may wonder how it is possible that an increase in I/O work results 
>> in decrease (and, hopefully, elimination) of related IPC messages. 
>> Roughly speaking, a worker must send an IPC "you have a new I/O 
>> request" message only when its worker->disker queue is empty. If the 
>> queue is not empty, then there is no reason to send an IPC message to 
>> wake up disker because disker will see the new message when dequeuing 
>> the previous one. Same for the opposite direction: disker->worker...

> This is probably true if you have slow disks and are actually IO bound, 
> but on fast disks or high page cache hit rate you essential see this ipc 
> ping pong and very little actual work being done.

AFAICT, "too slow" IPC messages should result in non-empty queues and, 
hence, no IPC messages at all. For this logic to work, it does not 
matter whether the system is I/O bound or not, whether disks are "slow" 
or not.


>> ?> Is it necessary to have these read chunks so small
>>
>> It is not. Disk I/O size should be at least the system I/O page size, 
>> but it can be larger. The optimal I/O size is probably very dependent 
>> on traffic patterns. IIRC, Squid I/O size is at most one Squid page 
>> (SM_PAGE_SIZE or 4KB).
>>
>> FWIW, I suspect there are significant inefficiencies in disk I/O 
>> related request alignment: The code does not attempt to read from and 
>> write to disk page boundaries, probably resulting in multiple 
>> low-level disk I/Os per one Squid 4KB I/O in some (many?) cases. With 
>> modern non-rotational storage these effects are probably less 
>> pronounced, but they probably still exist.

> The kernel drivers will mostly handle this for you if multiple requests 
> are available, but this is also almost irrelevant with current hardware, 
> typically it will be so fast software overhead will make it hard to 
> utilize modern large disk arrays properly

I doubt doing twice as many low-level disk I/Os (due to wrong alignment) 
is likely to be irrelevant, but we do not need to agree on that to make 
progress: Clearly, excessive low-level disk I/Os is not the bottleneck 
in your current environment.


> you probably need to look at 
> other approaches like io_ring to get rid of the classical read/write 
> systemcall overhead dominating your performance.

Yes, but those things are complementary (i.e. not mutually exclusive).


Cheers,

Alex.



From bud_miljkovic at trimble.com  Mon Oct 16 03:41:30 2023
From: bud_miljkovic at trimble.com (Bud Miljkovic)
Date: Mon, 16 Oct 2023 16:41:30 +1300
Subject: [squid-users] Fwd: Squid does not pass HTTPS traffic transparently
In-Reply-To: <CAPO8noLLk5aPNt-EThg1FUSFhgLQBgz7Vf0nw+nSp0SHC8iEhg@mail.gmail.com>
References: <CAPO8noLLk5aPNt-EThg1FUSFhgLQBgz7Vf0nw+nSp0SHC8iEhg@mail.gmail.com>
Message-ID: <CAPO8noLawUvm4vBHEWd1yYuTOKg-TbnErad_6j2Oh3y6tpkA6A@mail.gmail.com>

Resending it without an image

On Mon, Oct 16, 2023 at 1:59?PM Bud Miljkovic <bud_miljkovic at trimble.com>
wrote:

> Here is my system configuration
>
-
> The setup and the problem
>
>    - The HW box tries to establish an HTTPS transparent connection with a
>    server located within Internet.
>    - It uses the Local Server and send its request via eth0 interface.
>    - The request is Pre-routed from eth0, port 443, to the Transparent
>    Squid proxy (v3.5.25), listening at port 3129.
>    - For testing purposes, the Squid proxy is configured to pass only the
>    HTTPStraffic transparently via the eth1 interface, using sing the
>    `tcp_outgoing_address <ip_addr>` directive.  Please see the attached
>    squid-ota.conf file.
>    - While testing, I am monitoring the eth1 output via tcpdump and I get
>    the following:
>    # tcpdump -i eth1 port 443 -n -X -q
>    tcpdump: verbose output suppressed, use -v or -vv for full protocol
>    decode
>    listening on eth1, link-type EN10MB (Ethernet), capture size 262144
>    bytes
>    -  But nothing is detected!?
>    - From the above it appears that there is no an eth1 output at port
>    443?
>
> I have attached the printouts of the `iptables -nvL` and `iptables -nvL -t
> nat`
>  commands.
>
> Can someone check ut what I have done here and perhaps suggest what could
> be
> wrong in here.
>
> Cheers,
> Bud
> --
> Budimir Miljkovi? BSc E | He
> Senior Development Engineer
> Civil Construction Field Systems
> Trimble
>
> 11-17 Birmingham Drive, Christchurch, Canterbury, 8024
> New Zealand
> +64 3 963-5550 Direct
> +64 21 419-024 Mobile
>
> www.trimble.com
>
> This email may contain confidential information that is intended only for
> the listed recipient(s) of this email. Any unauthorized review, use,
> disclosure or distribution is prohibited. If you believe you have received
> this email in error, please immediately delete this email and any
> attachments, and inform me via reply email.
>


-- 
Budimir Miljkovi? BSc E | He
Senior Development Engineer
Civil Construction Field Systems
Trimble

11-17 Birmingham Drive, Christchurch, Canterbury, 8024
New Zealand
+64 3 963-5550 Direct
+64 21 419-024 Mobile

www.trimble.com

This email may contain confidential information that is intended only for
the listed recipient(s) of this email. Any unauthorized review, use,
disclosure or distribution is prohibited. If you believe you have received
this email in error, please immediately delete this email and any
attachments, and inform me via reply email.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231016/b3de9138/attachment.htm>
-------------- next part --------------
Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         
 8827  680K ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    7   438 ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            icmptype 8 ctstate NEW
    2   138 ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0           
    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate INVALID
1218K  299M APP_RULES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW
1218K  299M OS_RULES   all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW
  134 28053 REJECT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable
14014  841K REJECT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            reject-with tcp-reset
    0     0 REJECT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            reject-with icmp-proto-unreachable

Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  wlan1  wlan1   0.0.0.0/0            0.0.0.0/0           
    9   559 REJECT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            reject-with icmp-host-unreachable

Chain OUTPUT (policy ACCEPT 39073 packets, 2757K bytes)
 pkts bytes target     prot opt in     out     source               destination         
  125 11932 ACCEPT     all  --  *      *       10.3.19.92           0.0.0.0/0           

Chain APP_RULES (1 references)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            tcp dpt:20
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            tcp dpt:21
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80

Chain DEV_RULES (2 references)
 pkts bytes target     prot opt in     out     source               destination         
    5   300 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:22
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:1534
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:2345
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            udp dpt:1534
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            udp dpt:2345

Chain EXTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source               destination         
1190K  298M DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0           

Chain INTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source               destination         
13930  794K ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            udp dpt:53
    8  2540 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            udp dpt:67
    1   328 ACCEPT     udp  --  *      *       0.0.0.0/0            0.0.0.0/0            udp dpt:68
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80

Chain OS_RULES (1 references)
 pkts bytes target     prot opt in     out     source               destination         
28092 1666K DEV_RULES  all  --  eth0   *       0.0.0.0/0            0.0.0.0/0           
    0     0 DEV_RULES  all  --  wlan1  *       0.0.0.0/0            0.0.0.0/0           
28087 1666K INTERNAL_RULES  all  --  eth0   *       0.0.0.0/0            0.0.0.0/0           
    0     0 INTERNAL_RULES  all  --  wlan1  *       0.0.0.0/0            0.0.0.0/0           
1190K  298M EXTERNAL_RULES  all  --  eth1   *       0.0.0.0/0            0.0.0.0/0           
    0     0 EXTERNAL_RULES  all  --  wlan0  *       0.0.0.0/0            0.0.0.0/0           

-------------- next part --------------
Chain PREROUTING (policy ACCEPT 1234K packets, 306M bytes)
 pkts bytes target     prot opt in     out     source               destination         
   96  5760 REDIRECT   tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            tcp dpt:443 redir ports 3129
13943  837K REDIRECT   tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            tcp dpt:80 redir ports 3128

Chain INPUT (policy ACCEPT 13972 packets, 798K bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain OUTPUT (policy ACCEPT 62 packets, 4650 bytes)
 pkts bytes target     prot opt in     out     source               destination         

Chain POSTROUTING (policy ACCEPT 14103 packets, 566K bytes)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 MASQUERADE  all  --  *      eth1    192.168.168.0/24     0.0.0.0/0           
    0     0 MASQUERADE  all  --  *      eth1    192.168.192.0/24     0.0.0.0/0           
    0     0 MASQUERADE  all  --  *      wlan0   192.168.168.0/24     0.0.0.0/0           
    0     0 MASQUERADE  all  --  *      wlan0   192.168.192.0/24     0.0.0.0/0           


From bud_miljkovic at trimble.com  Mon Oct 16 18:51:46 2023
From: bud_miljkovic at trimble.com (Bud Miljkovic)
Date: Tue, 17 Oct 2023 07:51:46 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy does not work!
Message-ID: <CAPO8noKQ-y-6+a0QaXeKH4db2Zwghx2nO3snppV0H7=65QFoDg@mail.gmail.com>

Let me try one more time.


Here is my system configuration:

{HW-Box} --> Local Server{ (eth0[port 444]) -----+

   |
          +-----------------------------------------------------+
          |
          |
          +-----> ([3129] Transparent Squid proxy) ---> (eth1[port443]) }--+

                                  |

 +------------------------------------------------------- ---+
                                     |
                                     +->--{ INTERNET Server }

The setup and the problem:
   - The HW box tries to establish an HTTPS transparent connection with a
server located within Internet.

   - It uses the Local Server and send its request via eth0 interface.

   - The request is Pre-routed from eth0, port 443, to the Transparent
Squid proxy (v3.5.25), listening at port 3129.

   - For testing purposes, the Squid proxy is configured to pass only the
HTTPS traffic transparently via the eth1 interface, using sing the
`tcp_outgoing_address <ip_addr>` directive.  Please see the squid-ota.conf
file content below.

   - While testing, I am monitoring the eth1 output via tcpdump and I get
the following:

     # tcpdump -i eth1 port 443 -n -X -q -w tcp_dump_24
       tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture
size 262144 bytes
       0 packets captured
       1 packet received by filter
       0 packets dropped by kernel
       3 packets dropped by interface

   - But nothing is detected!?

   - From the above it appears that there is no eth1 output at port 443?

I have included the printouts of the `iptables -nvL` and `iptables -nvL -t
nat` commands.

Can someone tell me what I have done wrong here and perhaps suggest a
solution?


Cheers,
Bud


=========================
Squid configuration file:

# 1) Visible hostname
visible_hostname ctct-r2

# 2) Initialize SSL database first
sslcrtd_program /usr/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

# 3) Listen to incoming HTTP traffic
http_port 3128

# 4) Block all HTTP traffic
http_access deny all

# 5) Listen for incoming HTTPS traffic and intercept it
https_port 3129 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

# 6) Pass the SSL (HTTPS) traffic trasparently throught
ssl_bump splice all

# Do not use caching
# cache_dir ufs /var/volatile/log/squid/logs 100 16 256

# 7) Send out all HTTPS traffic to destination server via given IP address
tcp_outgoing_address 10.3.19.92

===============================================
# iptables -nvL

Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
 8827  680K ACCEPT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate RELATED,ESTABLISHED
    7   438 ACCEPT     icmp --  *      *       0.0.0.0/0
0.0.0.0/0            icmptype 8 ctstate NEW
    2   138 ACCEPT     all  --  lo     *       0.0.0.0/0
0.0.0.0/0
    0     0 DROP       all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate INVALID
1218K  299M APP_RULES  all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate NEW
1218K  299M OS_RULES   all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate NEW
  134 28053 REJECT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-port-unreachable
14014  841K REJECT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with tcp-reset
    0     0 REJECT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-proto-unreachable

Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  wlan1  wlan1   0.0.0.0/0
0.0.0.0/0
    9   559 REJECT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-host-unreachable

Chain OUTPUT (policy ACCEPT 39073 packets, 2757K bytes)
 pkts bytes target     prot opt in     out     source
destination
  125 11932 ACCEPT     all  --  *      *       10.3.19.92
0.0.0.0/0

Chain APP_RULES (1 references)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:20
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:21
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80

Chain DEV_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
    5   300 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:22
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:1534
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:2345
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:1534
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:2345

Chain EXTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
1190K  298M DROP       all  --  *      *       0.0.0.0/0
0.0.0.0/0

Chain INTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
13930  794K ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:53
    8  2540 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:67
    1   328 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:68
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80

Chain OS_RULES (1 references)
 pkts bytes target     prot opt in     out     source
destination
28092 1666K DEV_RULES  all  --  eth0   *       0.0.0.0/0
0.0.0.0/0
    0     0 DEV_RULES  all  --  wlan1  *       0.0.0.0/0
0.0.0.0/0
28087 1666K INTERNAL_RULES  all  --  eth0   *       0.0.0.0/0
0.0.0.0/0
    0     0 INTERNAL_RULES  all  --  wlan1  *       0.0.0.0/0
0.0.0.0/0
1190K  298M EXTERNAL_RULES  all  --  eth1   *       0.0.0.0/0
0.0.0.0/0
    0     0 EXTERNAL_RULES  all  --  wlan0  *       0.0.0.0/0
0.0.0.0/0

=====================================================
iptables -nvL -t nat

Chain PREROUTING (policy ACCEPT 1234K packets, 306M bytes)
 pkts bytes target     prot opt in     out     source
destination
   96  5760 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443 redir ports 3129
13943  837K REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 redir ports 3128

Chain INPUT (policy ACCEPT 13972 packets, 798K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 62 packets, 4650 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 14103 packets, 566K bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 MASQUERADE  all  --  *      eth1    192.168.168.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      eth1    192.168.192.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      wlan0   192.168.168.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      wlan0   192.168.192.0/24
0.0.0.0/0
==================================================

-- 
Budimir Miljkovi? BSc E | He
Senior Development Engineer
Civil Construction Field Systems
Trimble

11-17 Birmingham Drive, Christchurch, Canterbury, 8024
New Zealand
+64 3 963-5550 Direct
+64 21 419-024 Mobile

www.trimble.com

This email may contain confidential information that is intended only for
the listed recipient(s) of this email. Any unauthorized review, use,
disclosure or distribution is prohibited. If you believe you have received
this email in error, please immediately delete this email and any
attachments, and inform me via reply email.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231017/f15c4145/attachment.htm>

From squid3 at treenet.co.nz  Mon Oct 16 19:35:20 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 17 Oct 2023 08:35:20 +1300
Subject: [squid-users] Transparent HTTPS Squid proxy does not work!
In-Reply-To: <CAPO8noKQ-y-6+a0QaXeKH4db2Zwghx2nO3snppV0H7=65QFoDg@mail.gmail.com>
References: <CAPO8noKQ-y-6+a0QaXeKH4db2Zwghx2nO3snppV0H7=65QFoDg@mail.gmail.com>
Message-ID: <d3ac9ced-5528-495d-a050-383678650b74@treenet.co.nz>


I think your problem is the NAT table rules. You are missing some 
critical exceptions to let squid make the outbound tunnels.


On 17/10/23 07:51, Bud Miljkovic wrote:
> Let me try one more time.
> 
> 
> Here is my system configuration:
> 
> {HW-Box} --> Local Server{ (eth0[port 444]) -----+
>                                                                          
>  ? ?? |
>  ? ? ? ? ? +-----------------------------------------------------+
>  ? ? ? ? ? |
>  ? ? ? ? ? |
>  ? ? ? ? ? +-----> ([3129] Transparent Squid proxy) ---> (eth1[port443]) 
> }--+
>                                                                          
>  ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ?? |
>                                      
>  ?+------------------------------------------------------- ---+
>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?|
>  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?+->--{ INTERNET Server }
> 
> The setup and the problem:
>  ? ?- The HW box tries to establish an HTTPS transparent connection with 
> a server located within Internet.
> 

Drop the word "transparent" from this thinking. It is a connection.


>  ? ?- It uses the Local Server and send its request via eth0 interface.
> 
>  ? ?- The request is Pre-routed from eth0, port 443, to the Transparent 
> Squid proxy (v3.5.25), listening at port 3129.
> 

Correction. NAT'ed, not "routed".  The distinction is important and 
impacts which type of configuration will work and which will guarantee 
errors.

  * NAT is only working when performed on the machine running Squid.

  * "routed" can be done from a remote machine to the Squid machine. But 
does need additional NAT or TPROXY on the Squid machine.


>  ? ?- For testing purposes, the Squid proxy is configured to pass only 
> the HTTPS traffic transparently via the eth1 interface, using sing the 
> `tcp_outgoing_address <ip_addr>` directive.? Please see the 
> squid-ota.conf file content below.


To be clear FTR; Squid *cannot* guarantee a particular interface. Only 
the OS can decide that.

Your Squid is setting the TCP src-IP on outbound packets as a *Hint* 
(albeit a strong one) for the OS to use in its routing choices.


> 
>  ? ?- While testing, I am monitoring the eth1 output via tcpdump and I 
> get the following:
> 
>  ? ? ?# tcpdump -i eth1 port 443 -n -X -q -w tcp_dump_24
>  ? ? ? ?tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture 
> size 262144 bytes
>  ? ? ? ?0 packets captured
>  ? ? ? ?1 packet received by filter
>  ? ? ? ?0 packets dropped by kernel
>  ? ? ? ?3 packets dropped by interface
> 
>  ? ?- But nothing is detected!?
> 

Nod.

FWIW, I have had mixed success with tcpdump when NAT and MASQUERADE are 
happening on the machine. It plugs in at the lowest layer somewhere 
around the point packets are actually going to/from the network. So 
packets with internal temporary values in them may not show up in the dump.

When NAT, MASQUERADE and routing are manipulating things the iptables 
packet counters seem to be a lot more reliable indicator of what traffic 
is (not) going through, even if the dump shows less than expected.



>  ? ?- From the above it appears that there is no eth1 output at port 443?
> 

Possibly. Maybe.


> I have included the printouts of the `iptables -nvL` and `iptables -nvL 
> -t nat` commands.
> 
> Can someone tell me what I have done wrong here and perhaps suggest a 
> solution?
> 
> 
> Cheers,
> Bud
> 
> 
> =========================
> Squid configuration file:
> 
> # 1) Visible hostname
> visible_hostname ctct-r2
> 
> # 2) Initialize SSL database first
> sslcrtd_program /usr/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
> 
> # 3) Listen to incoming HTTP traffic
> http_port 3128
> 
> # 4) Block all HTTP traffic
> http_access deny all


Ah, This will block the CONNECT tunnels.

"CONNECT" is an HTTP request method. This (4) rule will both reject the 
CONNECT tunnel being handled, and/or will prevent the "splice" action 
from being allowed to pass it through Squid in its form as a "CONNECT" 
message.


Please keep the basic security rules (http_access deny ...) provided in 
the default squid.conf for your version. You can see them at 
<https://wiki.squid-cache.org/Releases/Squid-3.5#squid-35-default-config>

The rules you have for your local policy should go after the line 
"INSERT YOUR OWN RULE(S) HERE".

  You may remove the existing "http_access allow ..." lines as needed if 
you do not want them to happen.


> 
> # 5) Listen for incoming HTTPS traffic and intercept it
> https_port 3129 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> # 6) Pass the SSL (HTTPS) traffic trasparently throught
> ssl_bump splice all
> 
> # Do not use caching
> # cache_dir ufs /var/volatile/log/squid/logs 100 16 256
> 
> # 7) Send out all HTTPS traffic to destination server via given IP address
> tcp_outgoing_address 10.3.19.92
> 

Again this is not limited to one protocol (HTTPS). **All** TCP outgoing 
packets from your Squid will use this regardless of their protocol.



> =====================================================
> iptables -nvL -t nat
> 
> Chain PREROUTING (policy ACCEPT 1234K packets, 306M bytes)
>  ?pkts bytes target ? ? prot opt in ? ? out ? ? source               
> destination
>  ? ?96 ?5760 REDIRECT ? tcp ?-- ?eth0 ? * 0.0.0.0/0 
> 0.0.0.0/0 ? ? ? ? ? ?tcp dpt:443 redir ports 3129
> 13943 ?837K REDIRECT ? tcp ?-- ?eth0 ? * 0.0.0.0/0 
> 0.0.0.0/0 ? ? ? ? ? ?tcp dpt:80 redir ports 3128
> 

Per <https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

This table should contain an ACCEPT for Squid outbound connections 
before the REDIRECT. Like so:


   iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
   iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 443 -j ACCEPT

   iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT ...
   iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT ...


You do not list the "mangle" table. But check that you have the rule 
there protecting the port as well:

   iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP



HTH
Amos


From jtaylor.debian at googlemail.com  Mon Oct 16 20:24:48 2023
From: jtaylor.debian at googlemail.com (Julian Taylor)
Date: Mon, 16 Oct 2023 22:24:48 +0200
Subject: [squid-users] very poor performance of rock cache ipc
In-Reply-To: <baae1a3e-330c-4022-8fdd-c55dafcafae1@measurement-factory.com>
References: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>
 <0e161590-206f-4d51-9fd0-1a767f0a1793@measurement-factory.com>
 <8279acb8-45c0-4a26-a080-16ea52176283@googlemail.com>
 <baae1a3e-330c-4022-8fdd-c55dafcafae1@measurement-factory.com>
Message-ID: <d7cff248-ab65-4a23-803e-ddea2372a885@googlemail.com>

On 15.10.23 05:42, Alex Rousskov wrote:
> On 2023-10-14 12:04, Julian Taylor wrote:
>> On 14.10.23 17:40, Alex Rousskov wrote:
>>> On 2023-10-13 16:01, Julian Taylor wrote:
>>>
>>
>> The reproducer uses as single request, the same very thing can be 
>> observed on a very busy squid
> 
> If a busy Squid sends lots of IPC messages between worker and disker, 
> then either there is a Squid bug we do not know about OR that disker is 
> just not as busy as one might expect it to be.
> 
> In Squid v6+, you can observe disker queues using mgr:store_queues cache 
> manager report. In your environment, do those queues always have lots of 
> requests when Squid is busy? Feel free to share (a pointer to) a 
> representative sample of those reports from your busy Squid.
> 
> N.B. Besides worker-disker IPC messages, there are also worker-worker 
> cache synchronization IPC messages. They also have the same "do not send 
> IPC messages if the queue has some pending items already" optimization.
> 


I checked the queues running with the configuration from my initial mail 
with workers increase and the queues are generally low, around 1-10 
items in the queue when sending around 100 parallel requests reading 
about 100mb data files. Here is a sample: https://dpaste.com/8SLNRW5F8
Also with the higher request rate than the single curl the majority of 
work throughput was more than doubled by increasing the blocksize.

How are the queues supposed to look like on a busy squid that is not 
spending a large portion of its time doing notify IPC?

Increasing the parallel requests does decrease the amount of overhead 
but its still pretty large, I measured about 10%-30% cpu overhead with 
100 parallel requests served from cache in the worker and disker
Here a snipped of a profile:
--22.34%--JobDialer<AsyncJob>::dial(AsyncCall&)
    |
    |--21.19%--Ipc::UdsSender::start()
    |       |
    |        --21.13%--Ipc::UdsSender::write()
    |           |
    |           |--16.12%--Ipc::UdsOp::conn()
    |           |          |
    |           |           --15.84%--comm_open_uds(int, int, 
sockaddr_un*, int)
    |           |                |--1.70%--commSetCloseOnExec(int)
    |           |                 --1.56%--commSetNonBlocking(int)
   ...
--12.98%--comm_close_complete(int)

Clearing and constructing the large Ipc::TypedMsgHdr is also very 
noticeable.

That the overhead and maximum throughput is so low for not so busy 
squids (say 1-10 requests per second but requests on average > 1MiB) is 
imo also a reason for concern and could be improved.

If I understand the way it works correctly e.g. the worker when it gets 
a request splits it into 4k blocks and enqueues read requests into the 
ipc queue and if the queue is empty it emits a notify ipc so the disker 
starts popping from the queue.

On large requests that are answered immediately from the disker the 
problem seems to be that the queue is mostly empty and it sends an ipc 
ping pong for each 4k block.

So my though was when the request is larger than 4k enqueue multiple 
pending reads in the worker and only notify after a certain amount has 
been added to the queue, vice versa in the disker.
So I messed around a bit trying to reduce the notifications by delaying 
the Notify call in src/DiskIO/IpcIo/IpcIoFile.cc for larger requests but 
it ended up blocking after the first queue push with no notify. If I 
understand the queue correctly this is due to the reader requires a 
notify to initially start and and simply pushing multiple read requests 
onto the queue without notifying will not work as trivially as I hoped.

Is this approach feasible or am I misunderstanding how it works?


I also tried to add reusing of the IPC connection between calls so the 
major source of overhead,tearing down and reestablishing the connection, 
is removed but that also turned out difficult as the connections are 
closed in various places and the general complexity of the code.


From rousskov at measurement-factory.com  Tue Oct 17 03:26:50 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Oct 2023 23:26:50 -0400
Subject: [squid-users] very poor performance of rock cache ipc
In-Reply-To: <d7cff248-ab65-4a23-803e-ddea2372a885@googlemail.com>
References: <b428c0db-33e6-410c-8d90-94b9b84a0d91@googlemail.com>
 <0e161590-206f-4d51-9fd0-1a767f0a1793@measurement-factory.com>
 <8279acb8-45c0-4a26-a080-16ea52176283@googlemail.com>
 <baae1a3e-330c-4022-8fdd-c55dafcafae1@measurement-factory.com>
 <d7cff248-ab65-4a23-803e-ddea2372a885@googlemail.com>
Message-ID: <4f23cc52-02db-4df1-b13f-5734209e7560@measurement-factory.com>

On 2023-10-16 16:24, Julian Taylor wrote:
> On 15.10.23 05:42, Alex Rousskov wrote:
>> On 2023-10-14 12:04, Julian Taylor wrote:
>>> On 14.10.23 17:40, Alex Rousskov wrote:
>>>> On 2023-10-13 16:01, Julian Taylor wrote:
>>>>
>>>
>>> The reproducer uses as single request, the same very thing can be 
>>> observed on a very busy squid
>>
>> If a busy Squid sends lots of IPC messages between worker and disker, 
>> then either there is a Squid bug we do not know about OR that disker 
>> is just not as busy as one might expect it to be.
>>
>> In Squid v6+, you can observe disker queues using mgr:store_queues 
>> cache manager report. In your environment, do those queues always have 
>> lots of requests when Squid is busy? Feel free to share (a pointer to) 
>> a representative sample of those reports from your busy Squid.
>>
>> N.B. Besides worker-disker IPC messages, there are also worker-worker 
>> cache synchronization IPC messages. They also have the same "do not 
>> send IPC messages if the queue has some pending items already" 
>> optimization.

> I checked the queues running with the configuration from my initial mail 
> with workers increase and the queues are generally low, around 1-10 
> items in the queue when sending around 100 parallel requests reading 
> about 100mb data files. Here is a sample: https://dpaste.com/8SLNRW5F8
> Also with the higher request rate than the single curl the majority of 
> work throughput was more than doubled by increasing the blocksize.
> 
> How are the queues supposed to look like on a busy squid that is not 
> spending a large portion of its time doing notify IPC?

The queues are supposed to look "not empty" -- a non-empty queue does 
not result in IPC notifications. Needless to say, the further away from 
"empty" the queues are, the lesser the chance they will become empty 
when cache manager report is _not_ "looking" at them.


> Increasing the parallel requests does decrease the amount of overhead 
> but its still pretty large, I measured about 10%-30% cpu overhead with 
> 100 parallel requests served from cache in the worker and disker
> Here a snipped of a profile:
> --22.34%--JobDialer<AsyncJob>::dial(AsyncCall&)
>  ?? |
>  ?? |--21.19%--Ipc::UdsSender::start()
>  ?? |?????? |
>  ?? |??????? --21.13%--Ipc::UdsSender::write()
>  ?? |?????????? |
>  ?? |?????????? |--16.12%--Ipc::UdsOp::conn()
>  ?? |?????????? |????????? |
>  ?? |?????????? |?????????? --15.84%--comm_open_uds(int, int, 
> sockaddr_un*, int)
>  ?? |?????????? |??????????????? |--1.70%--commSetCloseOnExec(int)
>  ?? |?????????? |???????????????? --1.56%--commSetNonBlocking(int)
>  ? ...
> --12.98%--comm_close_complete(int)
> 
> Clearing and constructing the large Ipc::TypedMsgHdr is also very 
> noticeable.
> 
> That the overhead and maximum throughput is so low for not so busy 
> squids (say 1-10 requests per second but requests on average > 1MiB) is 
> imo also a reason for concern and could be improved.

I agree.


> If I understand the way it works correctly e.g. the worker when it gets 
> a request splits it into 4k blocks and enqueues read requests into the 
> ipc queue and if the queue is empty it emits a notify ipc so the disker 
> starts popping from the queue.

Yes, at some level of abstraction, the above summary is not wrong. 
However, please keep in mind that, for a single HTTP transaction, most 
of the disk read requests are queued by worker, read by disk, and 
received by worker one read request at a time. There is no disk read 
"prefetching" (yet?).


> On large requests that are answered immediately from the disker the 
> problem seems to be that the queue is mostly empty and it sends an ipc 
> ping pong for each 4k block.

Due to lack of prefetching, the total size of the HTTP response does not 
really affect the queue length. Only the transaction concurrency level 
does; on average, that is determined by mean response time multiplied by 
the I/O request rate from a particular worker to a particular disker.


> So my though was when the request is larger than 4k enqueue multiple 
> pending reads in the worker and only notify after a certain amount has 
> been added to the queue, vice versa in the disker.

> So I messed around a bit trying to reduce the notifications by delaying 
> the Notify call in src/DiskIO/IpcIo/IpcIoFile.cc for larger requests but 
> it ended up blocking after the first queue push with no notify. If I 
> understand the queue correctly this is due to the reader requires a 
> notify to initially start and and simply pushing multiple read requests 
> onto the queue without notifying will not work

You are correct.


> Is this approach feasible or am I misunderstanding how it works?

Prefetching is feasible in principle, but is not easy to implement well 
and will probably require configuration options (because it will slow 
down busy Squids that do not have the time to prefetch but may not know 
that).

I would consider increasing I/O size (and shared memory page size) 
instead, at least as the first step. Doing so well is not trivial 
either, but may be easier and beneficial to more use cases.


> I also tried to add reusing of the IPC connection between calls so the 
> major source of overhead,tearing down and reestablishing the connection, 
> is removed but that also turned out difficult as the connections are 
> closed in various places and the general complexity of the code.

Yes, that would be nice. Reusing sockets is especially difficult to get 
right with startup/bootstrapping, reconfigurations, and kid 
death/restarts problems in mind. On the other hand, it is probably much 
easier to optimize this than to implement disk hit "prefetching".

There may be some other, more effcient IPC notification mechanisms 
available on your OS that Squid can be enhanced to support. I have not 
surveyed what is available these days.


HTH,

Alex.



From my.shellac at gmail.com  Tue Oct 17 11:48:20 2023
From: my.shellac at gmail.com (=?UTF-8?B?QWxleGV50Y/RgCBHcnV6ZG92?=)
Date: Tue, 17 Oct 2023 16:48:20 +0500
Subject: [squid-users] external IP from DB
Message-ID: <CAFqyDwChj3XSAim_pyFiDZvGu4ySEYxGumo1kjM_g_Jmfg_xVQ@mail.gmail.com>

Hello guys!
Thank you for your support.

Could you explain me one more  things:

Case1:
1. I have a squid 6.2 and a lot of IP addresses on the server (assigned on
the virtual dummy interfaces)
2. I use the specific acl policy rules to pass the users with  different
external IP to the internet

For example this is acl like :
                "acl 1.2.3.4  localip 1.2.3.4"
 and then
                tcp_outgoing_address 1.2.3.4 1.2.3.4
That means if to connect to proxy on the  1.2.3.4  you will be passed to
the internet with source ip like 1.2.3.4

A Question:  Is the way to transfer these policy to DB ?  Something like
this:
        user connects to the 1.2.3.4 by proxy request, then squid looks to
DB (by ext_acl ) if there is ip 1.2.3.4 there and if it does  - that this
acl will be triggered and ip will be used as outgoing_ip for this
transaction.


Case2:
Also I use the random mechanism of squid to rotate the external IP of the
users. This works well. But I would to transfer this policy to the DB also.
May be somehow to use the ext_acl that will get the answer like an IP
address and use this IP in the tcp_ougoing address option.
Is there way to implement that?

Thank you

Alex Gru
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231017/c6c88d13/attachment.htm>

From bud_miljkovic at trimble.com  Wed Oct 18 03:32:21 2023
From: bud_miljkovic at trimble.com (Bud Miljkovic)
Date: Wed, 18 Oct 2023 16:32:21 +1300
Subject: [squid-users] Squid not working properly ...
Message-ID: <CAPO8noKY7qf9xiDhX5ee8Sz1O1b64JW5xcJtFwFBiJKNJrpj3g@mail.gmail.com>

I have now made some changes based on suggestions from the community and
would like a second look from the more experienced people.
Here is my squid configuration file:
----------------
visible_hostname ctct-r2
# 2) Initialize SSL database first
sslcrtd_program /usr/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
# 3) An ACL named 'whitelist'
acl whitelist dstdomain '/etc/squid/whitelist.ota'
# 4) Allow whitelisted URLs through
http_access allow whitelist
# 5) Listen to incoming HTTP traffic
http_port 3128
# 6) Block the rest
http_access deny all
# 7) Listen for incoming HTTPS traffic and intercept it
https_port 3129 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
# 8) Pass the SSL (HTTPS) traffic transparently through
ssl_bump splice all
# 9) Send out all HTTPS traffic to destination server via given IP address
tcp_outgoing_address 10.3.19.150
-------------------
And here are the iptables' settings:
NAT table:
# iptables -nvL -t nat
Chain PREROUTING (policy ACCEPT 9094 packets, 1823K bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     tcp  --  *      *       10.3.19.150
0.0.0.0/0            tcp dpt:80
    0     0 ACCEPT     tcp  --  *      *       10.3.19.150
0.0.0.0/0            tcp dpt:443
    0     0 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443 redir ports 3129
    0     0 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 redir ports 3128

Chain INPUT (policy ACCEPT 1 packets, 70 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 9 packets, 627 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 9 packets, 627 bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 MASQUERADE  all  --  *      eth1    192.168.168.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      eth1    192.168.192.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      wlan0   192.168.168.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      wlan0   192.168.192.0/24
0.0.0.0/0
------------------------------
Mangle table:
# iptables -nvL -t mangle
Chain PREROUTING (policy ACCEPT 12117 packets, 2382K bytes)
 pkts bytes target     prot opt in     out     source
destination
   16   960 DROP       tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443

Chain INPUT (policy ACCEPT 11861 packets, 2319K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 451 packets, 47694 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 451 packets, 47694 bytes)
 pkts bytes target     prot opt in     out     source
destination
-----------------------------
Routing table:
# iptables -nvL
Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
 3843  304K ACCEPT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate RELATED,ESTABLISHED
    1    59 ACCEPT     icmp --  *      *       0.0.0.0/0
0.0.0.0/0            icmptype 8 ctstate NEW
   33  2285 ACCEPT     all  --  lo     *       0.0.0.0/0
0.0.0.0/0
    0     0 DROP       all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate INVALID
91160   17M APP_RULES  all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate NEW
91160   17M OS_RULES   all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate NEW
   15  3195 REJECT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-port-unreachable
   75  4508 REJECT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with tcp-reset
    0     0 REJECT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-proto-unreachable

Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  wlan1  wlan1   0.0.0.0/0
0.0.0.0/0
    7   739 REJECT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-host-unreachable

Chain OUTPUT (policy ACCEPT 523 packets, 54506 bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     tcp  --  *      eth1    10.3.19.150
0.0.0.0/0       <<<--------------

Chain APP_RULES (1 references)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:20
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:21
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80

Chain DEV_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
    6   360 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:22
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:1534
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:2345
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:1534
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:2345

Chain EXTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
90961   17M DROP       all  --  *      *       0.0.0.0/0
0.0.0.0/0

Chain INTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
   95  5676 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:53
    5  1592 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:67
    1   328 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:68
    2   120 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80

Chain OS_RULES (1 references)
 pkts bytes target     prot opt in     out     source
destination
  199 15779 DEV_RULES  all  --  eth0   *       0.0.0.0/0
0.0.0.0/0
    0     0 DEV_RULES  all  --  wlan1  *       0.0.0.0/0
0.0.0.0/0
  193 15419 INTERNAL_RULES  all  --  eth0   *       0.0.0.0/0
0.0.0.0/0
    0     0 INTERNAL_RULES  all  --  wlan1  *       0.0.0.0/0
0.0.0.0/0
90961   17M EXTERNAL_RULES  all  --  eth1   *       0.0.0.0/0
0.0.0.0/0
    0     0 EXTERNAL_RULES  all  --  wlan0  *       0.0.0.0/0
0.0.0.0/0
------------------------------------------------

I am now getting something related to my testing expectation but there
problems
like following:
```
15:05:58.464105 IP (tos 0x0, ttl 64, id 33640, offset 0, flags [DF], proto
UDP (17), length 70)
    10.3.19.150.55834 > 10.3.30.20.domain: [udp sum ok] 31312+ A?
api.globalota.limios.net. (42)
....
15:05:58.810877 IP (tos 0xc0, ttl 64, id 32951, offset 0, flags [none],
proto ICMP (1), length 209)
    10.3.19.150 > 10.3.0.124: ICMP 10.3.19.150 udp port 55834 unreachable,
length 189
------------------------------------- Any hint would be appreciated
--------------------
Also, there is a configuration in the chain OUTPUT policy marked with the
"<<<-------------" string above,
which I am not sure about.

Cheers,
Buda


-- 

11-17 Birmingham Drive, Christchurch, Canterbury, 8024
New Zealand
+64 3 963-5550 Direct
+64 21 419-024 Mobile

www.trimble.com

This email may contain confidential information that is intended only for
the listed recipient(s) of this email. Any unauthorized review, use,
disclosure or distribution is prohibited. If you believe you have received
this email in error, please immediately delete this email and any
attachments, and inform me via reply email.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231018/e3981bdb/attachment.htm>

From ben.goz87 at gmail.com  Wed Oct 18 12:21:13 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Wed, 18 Oct 2023 15:21:13 +0300
Subject: [squid-users] Spliced domains tunnel connect is very slow
Message-ID: <CADAqQfz+b4NgEHjOSj9zFfNivJnUxH+KHAnODKjgfUf6_eoK7Q@mail.gmail.com>

By the help of God.

Hi,
I saw in my access log a traces that shows that spliced URLs tunneling is
very slowly:

18/Oct/2023:15:18:50 +0300 240841 192.168.3.98 TCP_TUNNEL/200 6225 CONNECT
beacons2.gvt2.com:443 - HIER_DIRECT/172.217.0.67 - beacons2.gvt2.com -
splice -
18/Oct/2023:15:18:50 +0300    680 192.168.3.173 TCP_TUNNEL/500 4977 CONNECT
mobile.events.data.microsoft.com:443 - HIER_DIRECT/13.89.178.26 -
mobile.events.data.microsoft.com - splice -
18/Oct/2023:15:18:51 +0300 127307 192.168.3.97 TCP_TUNNEL/500 3101 CONNECT
array612.prod.do.dsp.mp.microsoft.com:443 - HIER_DIRECT/20.54.24.148 -
array612.prod.do.dsp.mp.microsoft.com - splice -
18/Oct/2023:15:18:51 +0300    741 192.168.3.73 TCP_TUNNEL/200 4978 CONNECT
v10.events.data.microsoft.com:443 - HIER_DIRECT/52.182.143.211 -
v10.events.data.microsoft.com - splice -
18/Oct/2023:15:18:51 +0300      1 192.168.84.150 NONE_NONE/000 0 CONNECT
104.46.162.224:443 - HIER_NONE/- - v10.vortex-win.data.microsoft.com -
splice -
18/Oct/2023:15:18:51 +0300    956 192.168.2.78 TCP_TUNNEL/200 4979 CONNECT
v10.events.data.microsoft.com:443 - HIER_DIRECT/52.182.143.211 -
v10.events.data.microsoft.com - splice -
18/Oct/2023:15:18:51 +0300 156658 192.168.3.110 TCP_TUNNEL/200 2293 CONNECT
ssl.gstatic.com:443 - HIER_DIRECT/172.217.22.35 - ssl.gstatic.com - splice -
18/Oct/2023:15:18:51 +0300 251723 192.168.3.110 TCP_TUNNEL/200 4428 CONNECT
ssl.gstatic.com:443 - HIER_DIRECT/142.251.142.195 - ssl.gstatic.com -
splice -
18/Oct/2023:15:18:51 +0300 165983 192.168.3.110 TCP_TUNNEL/200 5572 CONNECT
clientservices.googleapis.com:443 - HIER_DIRECT/172.217.22.99 -
clientservices.googleapis.com - splice -

This is my squid configurations:

acl NoSSLInterceptRegexp_always ssl::server_name --client-requested
 "/usr/local/squid/etc/splice.list"
acl alwaysBump ssl::server_name --client-requested storage.googleapis.com
youtubei.googleapis.com www.eset.com eset.com safebrowsing.googleapis.com
play.google.com
on_unsupported_protocol tunnel
acl DiscoverSNIHost at_step SslBump1
ssl_bump peek DiscoverSNIHost
ssl_bump bump alwaysBump   -  Used to bumd certain subdomains before the
whole domain is bumped.
ssl_bump splice NoSSLInterceptRegexp_always
ssl_bump stare all



Other CONNECT requests are served noramly.
Is this issue could be a root cause for the generally slow internet?

Thanks,
Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231018/d8f35a1f/attachment.htm>

From bud_miljkovic at trimble.com  Thu Oct 19 18:17:13 2023
From: bud_miljkovic at trimble.com (Bud Miljkovic)
Date: Fri, 20 Oct 2023 07:17:13 +1300
Subject: [squid-users] How to configure a transparent, pass-all, Squid proxy?
Message-ID: <CAPO8noLAKQm2O4EOxc88uHQGFoZFjMu3Psa1MDY3MaLv6mrsuA@mail.gmail.com>

I am new to Squid and I'm trying to use it in a simple test case of a
pass-all transparent proxy.

My configuration is: Web-browser->Local_Server{eth0/port-443->(Transparent
Proxy)->port-443/eth1}->{Internet}

Squid version: 3.5.25

Below are the squid.conf file content, iptables -nvL and iptables -nvL -t
nat command outputs.

When Squid is running, I expect to be able to browse to all websites.
However, access to all websites is blocked?!
*squid.conf* file content:

# 1) Visible hostname
visible_hostname ctct-r2

# Debugging
debug_options ALL,1 33,2 28,9

# Enable log
access_log daemon:/var/log/squid/access.log squid

# 2) Initialize SSL database
sslcrtd_program /usr/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

# Do not use caching
# cache_dir ufs /var/volatile/log/squid/logs 100 16 256

# 3) Listen to incoming HTTP traffic
http_port 3128

# 4) Listen for incoming HTTPS traffic and intercept it
https_port 3129 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

# 5) Pass the SSL (HTTPS) traffic transparently through
ssl_bump splice all

# 6) Allow all HTTP traffic
http_access allow all

# 7) Send out all traffic to Internet via given IP address
tcp_outgoing_address 10.3.19.150
-----------


*# iptables -vnL*Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
 1467  121K ACCEPT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate RELATED,ESTABLISHED
    1    59 ACCEPT     icmp --  *      *       0.0.0.0/0
0.0.0.0/0            icmptype 8 ctstate NEW
    0     0 ACCEPT     all  --  lo     *       0.0.0.0/0
0.0.0.0/0
    0     0 DROP       all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate INVALID
83243   15M APP_RULES  all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate NEW
83243   15M OS_RULES   all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate NEW
   15  3195 REJECT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-port-unreachable
   64  3840 REJECT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with tcp-reset
    0     0 REJECT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-proto-unreachable

Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  wlan1  wlan1   0.0.0.0/0
0.0.0.0/0
    7   651 REJECT     all  --  *      *       0.0.0.0/0
0.0.0.0/0            reject-with icmp-host-unreachable

Chain OUTPUT (policy ACCEPT 915 packets, 82175 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain APP_RULES (1 references)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:20
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:21
    0     0 ACCEPT     tcp  --  eth1   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80

Chain DEV_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
    2   120 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:22
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:1534
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:2345
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:1534
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:2345

Chain EXTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
83158   15M DROP       all  --  *      *       0.0.0.0/0
0.0.0.0/0

Chain INTERNAL_RULES (2 references)
 pkts bytes target     prot opt in     out     source
destination
    4   269 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:53
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:67
    0     0 ACCEPT     udp  --  *      *       0.0.0.0/0
0.0.0.0/0            udp dpt:68
    0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80

Chain OS_RULES (1 references)
 pkts bytes target     prot opt in     out     source
destination
   85  7424 DEV_RULES  all  --  eth0   *       0.0.0.0/0
0.0.0.0/0
    0     0 DEV_RULES  all  --  wlan1  *       0.0.0.0/0
0.0.0.0/0
   83  7304 INTERNAL_RULES  all  --  eth0   *       0.0.0.0/0
  0.0.0.0/0
    0     0 INTERNAL_RULES  all  --  wlan1  *       0.0.0.0/0
  0.0.0.0/0
83158   15M EXTERNAL_RULES  all  --  eth1   *       0.0.0.0/0
  0.0.0.0/0
    0     0 EXTERNAL_RULES  all  --  wlan0  *       0.0.0.0/0
  0.0.0.0/0

*------------------*
*# iptables -vnL -t nat*Chain PREROUTING (policy ACCEPT 55227 packets,
10M bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 ACCEPT     tcp  --  *      *       10.3.19.150
0.0.0.0/0            tcp dpt:80
    0     0 ACCEPT     tcp  --  *      *       10.3.19.150
0.0.0.0/0            tcp dpt:443
   21  1260 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:443 redir ports 3129
    0     0 REDIRECT   tcp  --  eth0   *       0.0.0.0/0
0.0.0.0/0            tcp dpt:80 redir ports 3128

Chain INPUT (policy ACCEPT 4 packets, 508 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 8 packets, 532 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination
    0     0 MASQUERADE  all  --  *      eth1    192.168.168.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      eth1    192.168.192.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      wlan0   192.168.168.0/24
0.0.0.0/0
    0     0 MASQUERADE  all  --  *      wlan0   192.168.192.0/24
0.0.0.0/0
   29  1372 MASQUERADE  all  --  *      *       0.0.0.0/0
0.0.0.0/0
--------------

-- 
Budimir Miljkovi? BSc E | He
Senior Development Engineer
Civil Construction Field Systems
Trimble

11-17 Birmingham Drive, Christchurch, Canterbury, 8024
New Zealand
+64 3 963-5550 Direct
+64 21 419-024 Mobile

www.trimble.com

This email may contain confidential information that is intended only for
the listed recipient(s) of this email. Any unauthorized review, use,
disclosure or distribution is prohibited. If you believe you have received
this email in error, please immediately delete this email and any
attachments, and inform me via reply email.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231020/038b0a18/attachment.htm>

From squid3 at treenet.co.nz  Thu Oct 19 23:19:40 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Oct 2023 12:19:40 +1300
Subject: [squid-users] How to configure a transparent, pass-all,
 Squid proxy?
In-Reply-To: <CAPO8noLAKQm2O4EOxc88uHQGFoZFjMu3Psa1MDY3MaLv6mrsuA@mail.gmail.com>
References: <CAPO8noLAKQm2O4EOxc88uHQGFoZFjMu3Psa1MDY3MaLv6mrsuA@mail.gmail.com>
Message-ID: <31add224-c114-44e9-be08-58e6fe3d2ef4@treenet.co.nz>

On 20/10/23 07:17, Bud Miljkovic wrote:
> Chain EXTERNAL_RULES (2 references) pkts bytes target prot opt in out 
> source destination 83158 15M DROP all -- * * 0.0.0.0/0 
> <http://0.0.0.0/0> 0.0.0.0/0 <http://0.0.0.0/0>


FYI,  All of the traffic leaving the machine is being dropped by your 
iptables rules.


Amos


From squid3 at treenet.co.nz  Fri Oct 20 03:27:00 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Oct 2023 16:27:00 +1300
Subject: [squid-users] Spliced domains tunnel connect is very slow
In-Reply-To: <CADAqQfz+b4NgEHjOSj9zFfNivJnUxH+KHAnODKjgfUf6_eoK7Q@mail.gmail.com>
References: <CADAqQfz+b4NgEHjOSj9zFfNivJnUxH+KHAnODKjgfUf6_eoK7Q@mail.gmail.com>
Message-ID: <bdddff9c-bcc5-4805-b614-747bfd2dcbc3@treenet.co.nz>

On 19/10/23 01:21, Ben Goz wrote:
> By the help of God.
> 
> Hi,
> I saw in my access log a traces that shows that spliced URLs tunneling 
> is very slowly:

Please clarify what you mean by "slow" ?

  How have you determined speed ?
  What speed are you expecting / would you call non-slow ?



FYI, Several things to be aware of:

  1) CONNECT tunnel is not a simple thing with a constant "speed" of 
transfer. It represents and entire set of tunneled messages (or other 
opaque data) over indefinite timespan. Each of those messages has its 
own "speed" of transfer, with possible empty periods of 0 bytes 
transferred between.

  2) the SSL-Bump procedure may pause a CONNECT tunnel during TLS 
handshake and/or validation process to asynchronously fetch missing 
certificate details, and/or validate other data with ACLs, etc.
   Each of these subsidiary transactions may add indefinite effects on 
timing of the 'bumped' CONNECT tunnel.

  3) modern networking systems utilize "Happy Eyeballs" algorithms 
wherein they may open multiple TCP connections to various (or same) 
services in parallel and only utilize the fastest to connect. This can 
result in CONNECT tunnel being initiated and unused - either closed 
immediately or left open waiting activity for long periods.


So, as you should be able to see the log snippet shows some details 
about tunnels duration of use, you cannot tell "speed" from these logs.


For example:
> 
> 18/Oct/2023:15:18:50 +0300 240841 192.168.3.98 TCP_TUNNEL/200 6225 
> CONNECT beacons2.gvt2.com:443 <http://beacons2.gvt2.com:443> - 
> HIER_DIRECT/172.217.0.67 <http://172.217.0.67> - beacons2.gvt2.com 
> <http://beacons2.gvt2.com> - splice -


Tunnel was _open_ for 240 seconds. 6225 bytes transferred.

Those bytes may have been transferred in the first 1 milliseconds of the 
tunnel being open. Then Squid leaving it open waiting for further uses 
which never came.

... "slow" at 1.4 GB/sec.

  ... or it could have been "slow" at 10 bytes/sec the whole time. One 
cannot tell.


> 18/Oct/2023:15:18:50 +0300 ? ?680 192.168.3.173 TCP_TUNNEL/500 4977 
> CONNECT mobile.events.data.microsoft.com:443 
> <http://mobile.events.data.microsoft.com:443> - HIER_DIRECT/13.89.178.26 
> <http://13.89.178.26> - mobile.events.data.microsoft.com 
> <http://mobile.events.data.microsoft.com> - splice -

This tunnel was never open at all. It was *rejected*.
"speed" in that case was 3 KB/sec.


> 18/Oct/2023:15:18:51 +0300 127307 192.168.3.97 TCP_TUNNEL/500 3101 
> CONNECT array612.prod.do.dsp.mp.microsoft.com:443 
> <http://array612.prod.do.dsp.mp.microsoft.com:443> - 
> HIER_DIRECT/20.54.24.148 <http://20.54.24.148> - 
> array612.prod.do.dsp.mp.microsoft.com 
> <http://array612.prod.do.dsp.mp.microsoft.com> - splice -

Tunnel was _open_ for 127 seconds. 3101 bytes transferred.

Those bytes may have been transferred in the first 1 milliseconds of the 
tunnel being open. Then Squid leaving it open waiting for further uses 
which never came.

  ... "slow" at 376 MB/sec.

  ... or it could have been "slow" at 25 bytes/sec the whole time. One 
cannot tell.



> This is my squid configurations:
> 
> acl NoSSLInterceptRegexp_always ssl::server_name --client-requested 
>  ?"/usr/local/squid/etc/splice.list"

> acl alwaysBump ssl::server_name --client-requested 
> storage.googleapis.com <http://storage.googleapis.com> 
> youtubei.googleapis.com <http://youtubei.googleapis.com> www.eset.com 
> <http://www.eset.com> eset.com <http://eset.com> 
> safebrowsing.googleapis.com <http://safebrowsing.googleapis.com> 
> play.google.com <http://play.google.com>
> on_unsupported_protocol tunnel
> acl DiscoverSNIHost at_step SslBump1
> ssl_bump peek DiscoverSNIHost
> ssl_bump bump alwaysBump?? -? Used to bumd certain subdomains before the 
> whole domain is bumped.
> ssl_bump splice NoSSLInterceptRegexp_always
> ssl_bump stare all
> 
> 
> 
> Other CONNECT requests are served noramly.
> Is this issue could be a root cause for the generally slow internet?
> 
> Thanks,
> Ben
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From ben.goz87 at gmail.com  Sat Oct 21 19:35:09 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Sat, 21 Oct 2023 22:35:09 +0300
Subject: [squid-users] Spliced domains tunnel connect is very slow
In-Reply-To: <bdddff9c-bcc5-4805-b614-747bfd2dcbc3@treenet.co.nz>
References: <CADAqQfz+b4NgEHjOSj9zFfNivJnUxH+KHAnODKjgfUf6_eoK7Q@mail.gmail.com>
 <bdddff9c-bcc5-4805-b614-747bfd2dcbc3@treenet.co.nz>
Message-ID: <CADAqQfwvBZLhY-aoGoBEfEun8knMimYp2pPjj-maejb5aVS2hg@mail.gmail.com>

By the help of God.

I'll clarify more when I'm looking at chrome's Internet Tools network
traffic of one of the users that several
Requests are timed out and at the same time I see in Squid's access log a
trace that according to your explanations
was opened for more that 60 secs for the timed out url at client side. This
issue is observed only for certain urls while the other urls are working as
expected.

I asked the question very generally but I think I should say that the
question is regarding TPROXY setup with ssl-bump
And eventually an icap service. The weird thing here is that for many other
URLs this setup works fine.
So currently I don't have any idea how to work it out.

If more information about my setup is needed please let me know.

Thanks,
Ben



??????? ??? ??, 20 ????? 2023 ?-6:27 ??? ?Amos Jeffries?? <?
squid3 at treenet.co.nz??>:?

> On 19/10/23 01:21, Ben Goz wrote:
> > By the help of God.
> >
> > Hi,
> > I saw in my access log a traces that shows that spliced URLs tunneling
> > is very slowly:
>
> Please clarify what you mean by "slow" ?
>
>   How have you determined speed ?
>   What speed are you expecting / would you call non-slow ?
>
>
>
> FYI, Several things to be aware of:
>
>   1) CONNECT tunnel is not a simple thing with a constant "speed" of
> transfer. It represents and entire set of tunneled messages (or other
> opaque data) over indefinite timespan. Each of those messages has its
> own "speed" of transfer, with possible empty periods of 0 bytes
> transferred between.
>
>   2) the SSL-Bump procedure may pause a CONNECT tunnel during TLS
> handshake and/or validation process to asynchronously fetch missing
> certificate details, and/or validate other data with ACLs, etc.
>    Each of these subsidiary transactions may add indefinite effects on
> timing of the 'bumped' CONNECT tunnel.
>
>   3) modern networking systems utilize "Happy Eyeballs" algorithms
> wherein they may open multiple TCP connections to various (or same)
> services in parallel and only utilize the fastest to connect. This can
> result in CONNECT tunnel being initiated and unused - either closed
> immediately or left open waiting activity for long periods.
>
>
> So, as you should be able to see the log snippet shows some details
> about tunnels duration of use, you cannot tell "speed" from these logs.
>
>
> For example:
> >
> > 18/Oct/2023:15:18:50 +0300 240841 192.168.3.98 TCP_TUNNEL/200 6225
> > CONNECT beacons2.gvt2.com:443 <http://beacons2.gvt2.com:443> -
> > HIER_DIRECT/172.217.0.67 <http://172.217.0.67> - beacons2.gvt2.com
> > <http://beacons2.gvt2.com> - splice -
>
>
> Tunnel was _open_ for 240 seconds. 6225 bytes transferred.
>
> Those bytes may have been transferred in the first 1 milliseconds of the
> tunnel being open. Then Squid leaving it open waiting for further uses
> which never came.
>
> ... "slow" at 1.4 GB/sec.
>
>   ... or it could have been "slow" at 10 bytes/sec the whole time. One
> cannot tell.
>
>
> > 18/Oct/2023:15:18:50 +0300    680 192.168.3.173 TCP_TUNNEL/500 4977
> > CONNECT mobile.events.data.microsoft.com:443
> > <http://mobile.events.data.microsoft.com:443> - HIER_DIRECT/13.89.178.26
> > <http://13.89.178.26> - mobile.events.data.microsoft.com
> > <http://mobile.events.data.microsoft.com> - splice -
>
> This tunnel was never open at all. It was *rejected*.
> "speed" in that case was 3 KB/sec.
>
>
> > 18/Oct/2023:15:18:51 +0300 127307 192.168.3.97 TCP_TUNNEL/500 3101
> > CONNECT array612.prod.do.dsp.mp.microsoft.com:443
> > <http://array612.prod.do.dsp.mp.microsoft.com:443> -
> > HIER_DIRECT/20.54.24.148 <http://20.54.24.148> -
> > array612.prod.do.dsp.mp.microsoft.com
> > <http://array612.prod.do.dsp.mp.microsoft.com> - splice -
>
> Tunnel was _open_ for 127 seconds. 3101 bytes transferred.
>
> Those bytes may have been transferred in the first 1 milliseconds of the
> tunnel being open. Then Squid leaving it open waiting for further uses
> which never came.
>
>   ... "slow" at 376 MB/sec.
>
>   ... or it could have been "slow" at 25 bytes/sec the whole time. One
> cannot tell.
>
>
>
> > This is my squid configurations:
> >
> > acl NoSSLInterceptRegexp_always ssl::server_name --client-requested
> >   "/usr/local/squid/etc/splice.list"
>
> > acl alwaysBump ssl::server_name --client-requested
> > storage.googleapis.com <http://storage.googleapis.com>
> > youtubei.googleapis.com <http://youtubei.googleapis.com> www.eset.com
> > <http://www.eset.com> eset.com <http://eset.com>
> > safebrowsing.googleapis.com <http://safebrowsing.googleapis.com>
> > play.google.com <http://play.google.com>
> > on_unsupported_protocol tunnel
> > acl DiscoverSNIHost at_step SslBump1
> > ssl_bump peek DiscoverSNIHost
> > ssl_bump bump alwaysBump   -  Used to bumd certain subdomains before the
> > whole domain is bumped.
> > ssl_bump splice NoSSLInterceptRegexp_always
> > ssl_bump stare all
> >
> >
> >
> > Other CONNECT requests are served noramly.
> > Is this issue could be a root cause for the generally slow internet?
> >
> > Thanks,
> > Ben
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231021/c4e57e33/attachment.htm>

From ben.goz87 at gmail.com  Sun Oct 22 09:46:27 2023
From: ben.goz87 at gmail.com (Ben Goz)
Date: Sun, 22 Oct 2023 12:46:27 +0300
Subject: [squid-users] Spliced domains tunnel connect is very slow
In-Reply-To: <CADAqQfwvBZLhY-aoGoBEfEun8knMimYp2pPjj-maejb5aVS2hg@mail.gmail.com>
References: <CADAqQfz+b4NgEHjOSj9zFfNivJnUxH+KHAnODKjgfUf6_eoK7Q@mail.gmail.com>
 <bdddff9c-bcc5-4805-b614-747bfd2dcbc3@treenet.co.nz>
 <CADAqQfwvBZLhY-aoGoBEfEun8knMimYp2pPjj-maejb5aVS2hg@mail.gmail.com>
Message-ID: <CADAqQfz90tBEuPAJW4LQEYpF-k=-J2TxMUwfFO4E8POTsHDW9Q@mail.gmail.com>

By the help of God.

I think I can pinpoint the problem but I'm not sure, It looks like the
issue is between c-icap and squid communication.
>From squid logs I can see lots of traces:
2023/10/22 12:31:50.654 kid1| 93,3| ModXact.cc(556) readMore: not reading
because ICAP reply pipe is full
2023/10/22 12:31:50.654 kid1| 93,3| ModXact.cc(556) readMore: not reading
because ICAP reply pipe is full
2023/10/22 12:31:50.654 kid1| 93,3| AsyncJob.cc(161) callEnd:
Adaptation::Icap::ModXact status out: [FD 319;rp(2)S(2)/RwP(ieof) job2845

And on c-icap logs I can see that squid closing connection:
Sun Oct 22 12:38:07 2023, 709785/139749352670784, Error while sending rest
responce or client closed the connection

Those traces are coming from a relatively high debug level so I'm not sure
if it's pointing to an issue but it is worth thinking about.
To me it sounds not good if c-icap needs to send back data to squid and
while it's sending the session is closed.
What do you think?



??????? ???, 21 ????? 2023 ?-22:35 ??? ?Ben Goz?? <?ben.goz87 at gmail.com??>:?

> By the help of God.
>
> I'll clarify more when I'm looking at chrome's Internet Tools network
> traffic of one of the users that several
> Requests are timed out and at the same time I see in Squid's access log a
> trace that according to your explanations
> was opened for more that 60 secs for the timed out url at client side.
> This issue is observed only for certain urls while the other urls are
> working as expected.
>
> I asked the question very generally but I think I should say that the
> question is regarding TPROXY setup with ssl-bump
> And eventually an icap service. The weird thing here is that for many
> other URLs this setup works fine.
> So currently I don't have any idea how to work it out.
>
> If more information about my setup is needed please let me know.
>
> Thanks,
> Ben
>
>
>
> ??????? ??? ??, 20 ????? 2023 ?-6:27 ??? ?Amos Jeffries?? <?
> squid3 at treenet.co.nz??>:?
>
>> On 19/10/23 01:21, Ben Goz wrote:
>> > By the help of God.
>> >
>> > Hi,
>> > I saw in my access log a traces that shows that spliced URLs tunneling
>> > is very slowly:
>>
>> Please clarify what you mean by "slow" ?
>>
>>   How have you determined speed ?
>>   What speed are you expecting / would you call non-slow ?
>>
>>
>>
>> FYI, Several things to be aware of:
>>
>>   1) CONNECT tunnel is not a simple thing with a constant "speed" of
>> transfer. It represents and entire set of tunneled messages (or other
>> opaque data) over indefinite timespan. Each of those messages has its
>> own "speed" of transfer, with possible empty periods of 0 bytes
>> transferred between.
>>
>>   2) the SSL-Bump procedure may pause a CONNECT tunnel during TLS
>> handshake and/or validation process to asynchronously fetch missing
>> certificate details, and/or validate other data with ACLs, etc.
>>    Each of these subsidiary transactions may add indefinite effects on
>> timing of the 'bumped' CONNECT tunnel.
>>
>>   3) modern networking systems utilize "Happy Eyeballs" algorithms
>> wherein they may open multiple TCP connections to various (or same)
>> services in parallel and only utilize the fastest to connect. This can
>> result in CONNECT tunnel being initiated and unused - either closed
>> immediately or left open waiting activity for long periods.
>>
>>
>> So, as you should be able to see the log snippet shows some details
>> about tunnels duration of use, you cannot tell "speed" from these logs.
>>
>>
>> For example:
>> >
>> > 18/Oct/2023:15:18:50 +0300 240841 192.168.3.98 TCP_TUNNEL/200 6225
>> > CONNECT beacons2.gvt2.com:443 <http://beacons2.gvt2.com:443> -
>> > HIER_DIRECT/172.217.0.67 <http://172.217.0.67> - beacons2.gvt2.com
>> > <http://beacons2.gvt2.com> - splice -
>>
>>
>> Tunnel was _open_ for 240 seconds. 6225 bytes transferred.
>>
>> Those bytes may have been transferred in the first 1 milliseconds of the
>> tunnel being open. Then Squid leaving it open waiting for further uses
>> which never came.
>>
>> ... "slow" at 1.4 GB/sec.
>>
>>   ... or it could have been "slow" at 10 bytes/sec the whole time. One
>> cannot tell.
>>
>>
>> > 18/Oct/2023:15:18:50 +0300    680 192.168.3.173 TCP_TUNNEL/500 4977
>> > CONNECT mobile.events.data.microsoft.com:443
>> > <http://mobile.events.data.microsoft.com:443> - HIER_DIRECT/
>> 13.89.178.26
>> > <http://13.89.178.26> - mobile.events.data.microsoft.com
>> > <http://mobile.events.data.microsoft.com> - splice -
>>
>> This tunnel was never open at all. It was *rejected*.
>> "speed" in that case was 3 KB/sec.
>>
>>
>> > 18/Oct/2023:15:18:51 +0300 127307 192.168.3.97 TCP_TUNNEL/500 3101
>> > CONNECT array612.prod.do.dsp.mp.microsoft.com:443
>> > <http://array612.prod.do.dsp.mp.microsoft.com:443> -
>> > HIER_DIRECT/20.54.24.148 <http://20.54.24.148> -
>> > array612.prod.do.dsp.mp.microsoft.com
>> > <http://array612.prod.do.dsp.mp.microsoft.com> - splice -
>>
>> Tunnel was _open_ for 127 seconds. 3101 bytes transferred.
>>
>> Those bytes may have been transferred in the first 1 milliseconds of the
>> tunnel being open. Then Squid leaving it open waiting for further uses
>> which never came.
>>
>>   ... "slow" at 376 MB/sec.
>>
>>   ... or it could have been "slow" at 25 bytes/sec the whole time. One
>> cannot tell.
>>
>>
>>
>> > This is my squid configurations:
>> >
>> > acl NoSSLInterceptRegexp_always ssl::server_name --client-requested
>> >   "/usr/local/squid/etc/splice.list"
>>
>> > acl alwaysBump ssl::server_name --client-requested
>> > storage.googleapis.com <http://storage.googleapis.com>
>> > youtubei.googleapis.com <http://youtubei.googleapis.com> www.eset.com
>> > <http://www.eset.com> eset.com <http://eset.com>
>> > safebrowsing.googleapis.com <http://safebrowsing.googleapis.com>
>> > play.google.com <http://play.google.com>
>> > on_unsupported_protocol tunnel
>> > acl DiscoverSNIHost at_step SslBump1
>> > ssl_bump peek DiscoverSNIHost
>> > ssl_bump bump alwaysBump   -  Used to bumd certain subdomains before
>> the
>> > whole domain is bumped.
>> > ssl_bump splice NoSSLInterceptRegexp_always
>> > ssl_bump stare all
>> >
>> >
>> >
>> > Other CONNECT requests are served noramly.
>> > Is this issue could be a root cause for the generally slow internet?
>> >
>> > Thanks,
>> > Ben
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > https://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231022/db9b41ce/attachment.htm>

From Ralf.Hildebrandt at charite.de  Sun Oct 22 13:52:57 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Sun, 22 Oct 2023 15:52:57 +0200
Subject: [squid-users] Security advisories pointing to Squid 6.4,
 but no download (yer)?
Message-ID: <ZTUpOYWJhjCU5PV5@charite.de>

Hi!

The recent four Security advisories are pointing to Squid 6.4, but I'm
not seeing that one for download yet...

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From gkinkie at gmail.com  Sun Oct 22 16:45:55 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Sun, 22 Oct 2023 18:45:55 +0200
Subject: [squid-users] Security advisories pointing to Squid 6.4,
 but no download (yer)?
In-Reply-To: <ZTUpOYWJhjCU5PV5@charite.de>
References: <ZTUpOYWJhjCU5PV5@charite.de>
Message-ID: <CA+Y8hcNesdUzLVqA_=3QJq-bJwYGyxEfoS6MwMHLOLA3sOpJnw@mail.gmail.com>

Hi Ralf,
   It might be some delay in propagating to the mirrors. I see 6.4 is
available at http://static.squid-cache.org/Versions/v6/ .


On Sun, Oct 22, 2023 at 3:53?PM Ralf Hildebrandt <
Ralf.Hildebrandt at charite.de> wrote:

> Hi!
>
> The recent four Security advisories are pointing to Squid 6.4, but I'm
> not seeing that one for download yet...
>
> --
> Ralf Hildebrandt
> Charit? - Universit?tsmedizin Berlin
> Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
> Invalidenstra?e 120/121 | D-10115 Berlin
>
> Tel. +49 30 450 570 155
> ralf.hildebrandt at charite.de
> https://www.charite.de
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231022/d744b10b/attachment.htm>

From Raja.Rajasekaran at Honeywell.com  Mon Oct 23 03:20:43 2023
From: Raja.Rajasekaran at Honeywell.com (Rajasekaran, Raja)
Date: Mon, 23 Oct 2023 03:20:43 +0000
Subject: [squid-users] Build Issue on Ubuntu 22.04 for Squid version 6.4
In-Reply-To: <BLAPR07MB83532495B9E9131DC5657C799DD8A@BLAPR07MB8353.namprd07.prod.outlook.com>
References: <BLAPR07MB83532495B9E9131DC5657C799DD8A@BLAPR07MB8353.namprd07.prod.outlook.com>
Message-ID: <BLAPR07MB8353171E6898D001D7C31F769DD8A@BLAPR07MB8353.namprd07.prod.outlook.com>

Hi All

I'm trying to compile and install deb package of squid 6.4, I am using the Ubuntu server 22.04.02 as my build machine.

While running make command, I get the following error. Could you anyone help me, figure it out.

lto-wrapper: warning: using serial compilation of 100 LTRANS jobs
acl/BoolOps.cc: In member function '__ct_base ':
acl/BoolOps.cc:20:5: error: '__builtin_strlen' reading 1 or more bytes from a region of size 0 [-Werror=stringop-overread]
   20 |     Must(strlen(acl->name) <= sizeof(name)-2);
      |     ^
lto1: all warnings being treated as errors
lto-wrapper: fatal error: g++ returned 1 exit status
compilation terminated.
/usr/bin/ld: error: lto-wrapper failed
collect2: error: ld returned 1 exit status
libtool: link: rm -f ".libs/squidS.o"
make[3]: *** [Makefile:5220: squid] Error 1
make[3]: Leaving directory '/home/coeadmin/squid-6.4-build1/squid-6.4/src'
make[2]: *** [Makefile:6004: all-recursive] Error 1
make[2]: Leaving directory '/home/coeadmin/squid-6.4-build1/squid-6.4/src'
make[1]: *** [Makefile:5004: all] Error 2
make[1]: Leaving directory '/home/coeadmin/squid-6.4-build1/squid-6.4/src'
make: *** [Makefile:599: all-recursive] Error 1


Steps I followed.


  1.  Ran build dep to install build dependencies.
  2.  Executed configure command with following options.

--datadir=/usr/share/squid \

                                --sysconfdir=/etc/squid \

                                --libexecdir=/usr/lib/squid \

                                --mandir=/usr/share/man \

                                --enable-inline \

                                --disable-arch-native \

                                --enable-async-io=8 \

                                --enable-storeio="ufs,aufs,diskd,rock" \

                                --enable-removal-policies="lru,heap" \

                                --enable-delay-pools \

                                --enable-cache-digests \

                                --enable-icap-client \

                                --enable-follow-x-forwarded-for \

                                --enable-auth-basic="DB,fake,getpwnam,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB" \

                                --enable-auth-digest="file,LDAP" \

                                --enable-auth-negotiate="kerberos,wrapper" \

                                --enable-auth-ntlm="fake,SMB_LM" \

                                --enable-external-acl-helpers="file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group" \

                                --enable-security-cert-validators="fake" \

                                --enable-storeid-rewrite-helpers="file" \

                                --enable-url-rewrite-helpers="fake" \

                                --enable-eui \

                                --enable-esi \

                                --enable-icmp \

                                --enable-zph-qos \

                                --enable-ecap \

                                --disable-translation \

                                --with-swapdir=/var/spool/squid \

                                --with-logdir=/var/log/squid \

                                --with-pidfile=/run/squid.pid \

                                --with-filedescriptors=65536 \

                                --with-large-files \

                                --with-default-user=proxy \

                                --enable-linux-netfilter --with-systemd \

                                --with-openssl \

                                --enable-ssl-crtd



  1.  executed make, which resulted in above error.

Please let me know if more information is required
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231023/04b2ac3d/attachment.htm>

From Ralf.Hildebrandt at charite.de  Mon Oct 23 08:09:26 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 23 Oct 2023 10:09:26 +0200
Subject: [squid-users] [ext] Re: Security advisories pointing to Squid
 6.4, but no download (yer)?
In-Reply-To: <CA+Y8hcNesdUzLVqA_=3QJq-bJwYGyxEfoS6MwMHLOLA3sOpJnw@mail.gmail.com>
References: <ZTUpOYWJhjCU5PV5@charite.de>
 <CA+Y8hcNesdUzLVqA_=3QJq-bJwYGyxEfoS6MwMHLOLA3sOpJnw@mail.gmail.com>
Message-ID: <ZTYqNisuQvHYHYPn@charite.de>

* Francesco Chemolli <gkinkie at gmail.com>:
> Hi Ralf,
>    It might be some delay in propagating to the mirrors. I see 6.4 is
> available at http://static.squid-cache.org/Versions/v6/ .

Yep, working now (not on the v6 mirror, though)

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From Ralf.Hildebrandt at charite.de  Mon Oct 23 11:39:52 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Mon, 23 Oct 2023 13:39:52 +0200
Subject: [squid-users] Squid 6.4 assertion errors: FATAL: assertion failed:
 stmem.cc:98: "lowestOffset () <= target_offset" current master transaction:
 master655 (backtrace)]
Message-ID: <ZTZbiDV9ZEXvt/d8@charite.de>

I upgraded from 6.3 to 6.4 today and it INSTANTLY began crashing
(frequently!)

The Log messages in cache.log:
...
2023/10/23 09:57:21| Beginning Validation Procedure
2023/10/23 09:57:21| Completed Validation Procedure
    Validated 237163 Entries
    store_swap_size = 29491032.00 KB
2023/10/23 09:57:21| Recv recv: (111) Connection refused
2023/10/23 09:57:21| Closing Pinger socket on FD 25
2023/10/23 09:57:22| storeLateRelease: released 0 objects
2023/10/23 09:57:46| FATAL: assertion failed: stmem.cc:98: "lowestOffset () <= target_offset"
    current master transaction: master655
   
Backtraces (core files mentioned in the bt are available for further debugging):
================================================================================

Reading symbols from /usr/sbin/squid...
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

Program received signal SIGABRT, Aborted.
__GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
        set = {
          __val = {0, 115, 59597774791904, 8260741391549598720, 1, 
            8260741391549598720, 93825000044944, 140737488346944, 
            93825000045032, 140737488346992, 93824997712448, 93824995977164, 
            93825059434008, 140737344738147, 206158430240, 140737488346848}
        }
        pid = <optimized out>
        tid = <optimized out>
        ret = <optimized out>
#1  0x00007ffff7613859 in __GI_abort () at abort.c:79
        save_stage = 1
        act = {
          __sigaction_handler = {
            sa_handler = 0x55555956ae18,
            sa_sigaction = 0x55555956ae18
          },
          sa_mask = {
            __val = {93824997144201, 47, 1698047866, 972957, 0, 
              93824997779200, 93825059434008, 93825059130880, 
              8260741391549598720, 114, 93825000044944, 93825000113656, 
              93824997147760, 93825000113656, 93824997143224, 93825000113656}
          },
          sa_flags = 1085148160,
          sa_restorer = 0x555555a8e4e0
        }
        sigs = {
          __val = {32, 0 <repeats 15 times>}
        }
#2  0x0000555555a03343 in xassert (
    msg=msg at entry=0x555555a8e4e0 "lowestOffset () <= target_offset", 
    file=file at entry=0x555555a8e240 "stmem.cc", line=line at entry=98)
    at debug.cc:1269
        __FUNCTION__ = <optimized out>
#3  0x000055555582a2ea in mem_hdr::freeDataUpto (
    this=this at entry=0x55555957ca08, target_offset=target_offset at entry=1048576)
    at stmem.cc:83
        __FUNCTION__ = "freeDataUpto"
        theStart = 0x5555595d57e0
#4  0x00005555557477f1 in MemObject::trimUnSwappable (this=0x55555957c9e0)
    at MemObject.cc:400
        new_mem_lo = 1048576
#5  0x0000555555831b69 in StoreEntry::trimMemory (
    this=this at entry=0x5555589d7d80, 
    preserveSwappable=preserveSwappable at entry=false) at store.cc:1816
        __FUNCTION__ = "trimMemory"
#6  0x0000555555a55765 in Store::Controller::memoryOut (this=<optimized out>, 
    e=..., preserveSwappable=<optimized out>) at Controller.cc:600
        keepInLocalMemory = false
        __FUNCTION__ = "memoryOut"
#7  0x00005555558419ff in StoreEntry::swapOut (this=0x5555589d7d80)
    at store_swapout.cc:168
        weAreOrMayBeSwappingOut = false
        __FUNCTION__ = "swapOut"
        lowest_offset = <optimized out>
        swapout_maxsize = <optimized out>
#8  0x000055555583bd2f in StoreEntry::invokeHandlers (
    this=this at entry=0x5555589d7d80) at store_client.cc:834
        __FUNCTION__ = "invokeHandlers"
        i = <optimized out>
        sc = <optimized out>
        nx = <optimized out>
        node = <optimized out>
        savedContext = {
          p_ = 0x5555558e67cc
     <UnitFileNameHashCacher(char const*, FileNameHasher*)>
        }
#9  0x0000555555832aac in StoreEntry::write (this=0x5555589d7d80, 
    writeBuffer=...) at store.cc:784
        __FUNCTION__ = "write"
#10 0x000055555588804c in Client::storeReplyBody (
    this=this at entry=0x555558d1e3d8, 
    data=data at entry=0x555559599540 "7\030\253\\W\027\306\377\273EM\265\326\236b\226\071\343pSv\331\342\022\030\266Y\n\020\066\314\344+\031N\346\367\205\276C#6N\227\254l\016rn@\262\367\017\212\070\367\307\350\223\360yK\236\237\a\002V\201\316\310\372\225\263+\254R\\\t\267\255\327\234\334\375\233\265\322\235\215I\214\355g\243\031\344\afth\262D\210\204\276\b@\363g\251\353\334e\034\240\217`q\022mV\243\063uB\345\332\341\233\256\333V\261;^.\352\027\365k\273\362\341g\220\002\341\237\240o\254z\373\066\211\036\016YgN\311\240\251\070\273\341\023\252\033e\020\365\200\026\316\331\ntT\017\246\217\251\346\203d\372\361\250\371b\335\310\354\062\361J\225\060\223"..., len=len at entry=7140) at ../../src/StoreIOBuffer.h:23
No locals.
#11 0x000055555588808b in Client::addVirginReplyBody (
    this=this at entry=0x555558d1e3d8, 
    data=0x555559599540 "7\030\253\\W\027\306\377\273EM\265\326\236b\226\071\343pSv\331\342\022\030\266Y\n\020\066\314\344+\031N\346\367\205\276C#6N\227\254l\016rn@\262\367\017\212\070\367\307\350\223\360yK\236\237\a\002V\201\316\310\372\225\263+\254R\\\t\267\255\327\234\334\375\233\265\322\235\215I\214\355g\243\031\344\afth\262D\210\204\276\b@\363g\251\353\334e\034\240\217`q\022mV\243\063uB\345\332\341\233\256\333V\261;^.\352\027\365k\273\362\341g\220\002\341\237\240o\254z\373\066\211\036\016YgN\311\240\251\070\273\341\023\252\033e\020\365\200\026\316\331\ntT\017\246\217\251\346\203d\372\361\250\371b\335\310\354\062\361J\225\060\223"..., len=len at entry=7140) at Client.cc:1049
No locals.
#12 0x00005555557e9dd6 in HttpStateData::writeReplyBody (
    this=this at entry=0x555558d1e3d8) at http.cc:1415
        data = <optimized out>
        len = 7140
        clen = 0
        parsedWhole = <optimized out>
#13 0x00005555557ede3a in HttpStateData::processReplyBody (
    this=0x555558d1e3d8) at http.cc:1498
        __FUNCTION__ = "processReplyBody"
#14 0x00005555557f1015 in HttpStateData::processReply (
    this=this at entry=0x555558d1e3d8) at http.cc:1295
        __FUNCTION__ = "processReply"
#15 0x00005555557f208a in HttpStateData::readReply (this=0x555558d1e3d8, 
    io=...) at http.cc:1265
        __FUNCTION__ = "readReply"
        rd = {
          <CommCommonCbParams> = {
            data = 0x555558d1e3d8,
            conn = {
              p_ = 0x555559314a80
            },
            flag = Comm::OK,
            xerrno = 0,
            fd = -1
          }, 
          members of CommIoCbParams:
          buf = 0x0,
          size = 7140
        }
#16 0x00005555557f782c in CommCbMemFunT<HttpStateData, CommIoCbParams>::doDial
    (this=0x555559293c48) at CommCalls.h:190
No locals.
#17 0x00005555557f748f in JobDialer<HttpStateData>::dial (
    this=0x555559293c48, call=...) at base/AsyncJobCalls.h:170
        __FUNCTION__ = "dial"
#18 0x00005555557f773e in AsyncCallT<CommCbMemFunT<HttpStateData, CommIoCbParams> >::fire (this=<optimized out>) at ../src/base/AsyncCall.h:147
No locals.
#19 0x00005555558e4f4e in AsyncCall::make (this=0x555559293c10)
    at AsyncCall.cc:44
        __FUNCTION__ = "make"
#20 0x00005555558e69cc in AsyncCallQueue::fire (this=0x555555d9b410)
    at ../../src/base/RefCount.h:73
        call = {
          p_ = 0x555559293c10
        }
        made = true
        __FUNCTION__ = "fire"
#21 0x0000555555710e93 in EventLoop::dispatchCalls (
    this=this at entry=0x7fffffffe940) at EventLoop.cc:144
        dispatchedSome = <optimized out>
#22 0x0000555555710f86 in EventLoop::runOnce (this=this at entry=0x7fffffffe940)
    at EventLoop.cc:121
        sawActivity = <optimized out>
        waitingEngine = 0x7fffffffe860
        __FUNCTION__ = <optimized out>
#23 0x0000555555710ff1 in EventLoop::run (this=0x7fffffffe940)
    at EventLoop.cc:83
No locals.
#24 0x0000555555805a58 in SquidMain (argc=<optimized out>, 
    argv=<optimized out>) at main.cc:1710
        cmdLine = {
          argv_ = std::vector of length 3, capacity 3 = {
            0x555555cc7870 "/usr/sbin/squid", 0x555555cc7890 "-NsYC", 0x0},
          shortOptions_ = 0x555555cc7820 "CDFNRSYXa:d:f:hk:m::n:sl:u:vz?",
          longOptions_ = std::vector of length 5, capacity 8 = {{
              <option> = {
                name = 0x555555cc7970 "foreground",
                has_arg = 0,
                flag = 0x0,
                val = 2
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc7b70 "kid",
                has_arg = 1,
                flag = 0x0,
                val = 3
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc7b90 "help",
                has_arg = 0,
                flag = 0x0,
                val = 104
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc7bb0 "version",
                has_arg = 0,
                flag = 0x0,
                val = 118
              }, <No data fields>}, {
              <option> = {
                name = 0x0,
                has_arg = 0,
                flag = 0x0,
                val = 0
              }, <No data fields>}}
        }
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
        mainLoop = {
          errcount = 0,
          static Running = 0x7fffffffe940,
          last_loop = false,
          engines = std::vector of length 4, capacity 4 = {0x7fffffffe820, 
            0x555555c13760 <EventScheduler::_instance>, 0x7fffffffe840, 
            0x7fffffffe860},
          timeService = 0x7fffffffe880,
          primaryEngine = 0x7fffffffe860,
          loop_delay = 0,
          error = false,
          runOnceResult = false
        }
        signalEngine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b75138 <vtable for SignalEngine+16>
          }, <No data fields>}
        store_engine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b75110 <vtable for StoreRootEngine+16>
          }, <No data fields>}
        comm_engine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b84c08 <vtable for CommSelectEngine+16>
          }, <No data fields>}
        time_engine = {
          _vptr.Engine = 0x555555b93ac8 <vtable for Time::Engine+16>
        }
#25 0x000055555580603a in SquidMainSafe (argv=0x7fffffffed48, argc=2)
    at main.cc:1353
        __FUNCTION__ = <optimized out>
        _dbg_level = <optimized out>
        _dbo = <optimized out>
#26 main (argc=2, argv=0x7fffffffed48) at main.cc:1341
No locals.
Saved corefile core.4053617
A debugging session is active.

	Inferior 1 [process 4053617] will be killed.

Quit anyway? (y or n) [answered Y; input not from terminal]


and yet another crash:
======================

Reading symbols from /usr/sbin/squid...
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

Program received signal SIGABRT, Aborted.
__GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
        set = {
          __val = {0, 115, 60701581386976, 3007948225109730816, 1, 
            3007948225109730816, 93825000044944, 140737488346944, 
            93825000045032, 140737488346992, 93824997712448, 93824995977164, 
            93825145171800, 140737344738147, 206158430240, 140737488346848}
        }
        pid = <optimized out>
        tid = <optimized out>
        ret = <optimized out>
#1  0x00007ffff7613859 in __GI_abort () at abort.c:79
        save_stage = 1
        act = {
          __sigaction_handler = {
            sa_handler = 0x55555e72ef58,
            sa_sigaction = 0x55555e72ef58
          },
          sa_mask = {
            __val = {93824997144201, 47, 1698047947, 163403, 0, 
              93824997779200, 93825145171800, 93825150253856, 
              3007948225109730816, 115, 93825000044944, 93825000113656, 
              93824997147760, 93825000113656, 93824997143224, 93825000113656}
          },
          sa_flags = 1416537600,
          sa_restorer = 0x555555a8e4e0
        }
        sigs = {
          __val = {32, 0 <repeats 15 times>}
        }
#2  0x0000555555a03343 in xassert (
    msg=msg at entry=0x555555a8e4e0 "lowestOffset () <= target_offset", 
    file=file at entry=0x555555a8e240 "stmem.cc", line=line at entry=98)
    at debug.cc:1269
        __FUNCTION__ = <optimized out>
#3  0x000055555582a2ea in mem_hdr::freeDataUpto (
    this=this at entry=0x55555eb698e8, target_offset=target_offset at entry=2097152)
    at stmem.cc:83
        __FUNCTION__ = "freeDataUpto"
        theStart = 0x55555ec07a70
#4  0x00005555557477f1 in MemObject::trimUnSwappable (this=0x55555eb698c0)
    at MemObject.cc:400
        new_mem_lo = 2097152
#5  0x0000555555831b69 in StoreEntry::trimMemory (
    this=this at entry=0x555558989360, 
    preserveSwappable=preserveSwappable at entry=false) at store.cc:1816
        __FUNCTION__ = "trimMemory"
#6  0x0000555555a55765 in Store::Controller::memoryOut (this=<optimized out>, 
    e=..., preserveSwappable=<optimized out>) at Controller.cc:600
        keepInLocalMemory = false
        __FUNCTION__ = "memoryOut"
#7  0x00005555558419ff in StoreEntry::swapOut (this=0x555558989360)
    at store_swapout.cc:168
        weAreOrMayBeSwappingOut = false
        __FUNCTION__ = "swapOut"
        lowest_offset = <optimized out>
        swapout_maxsize = <optimized out>
#8  0x000055555583bd2f in StoreEntry::invokeHandlers (
    this=this at entry=0x555558989360) at store_client.cc:834
        __FUNCTION__ = "invokeHandlers"
        i = <optimized out>
        sc = <optimized out>
        nx = <optimized out>
        node = <optimized out>
        savedContext = {
          p_ = 0x5555558e67cc
     <UnitFileNameHashCacher(char const*, FileNameHasher*)>
        }
#9  0x0000555555832aac in StoreEntry::write (this=0x555558989360, 
    writeBuffer=...) at store.cc:784
        __FUNCTION__ = "write"
#10 0x000055555588804c in Client::storeReplyBody (
    this=this at entry=0x555558aee1f8, 
    data=data at entry=0x55555ec07ba0 "\245N\313\343Q\334M8t\365\356\341\f\177O\257\242\376\240\031\341\317\022Gh\243\324\034\211\273\317\037\f\353XQN\034\325\312\355\330\337\251y\021\335L\266N\342\071<7\215\230\234\300\225\343\316\320\221\271\316\020IT\335f\"I\302}\236\301\206}\325\r\243\232\336\247\v\277\360H\352\026@\005\356\227\204\267\206\036\361\245\357y?k\310T\324\023h\325\304#t\vB\352\273\005\253\331a\027`+\217!i\331=\374\356V\310\224\327\336\336A\035>\322\377\250q\350\207\v\030:\341\n\f]\311'?w^$\vg\024\067\374\263'\352\347)p\230\024\217\036\270b\023\320\025\066\021Z7\230\217\006n\300\314a,\304.w\337\367\332\304\333o"..., len=len at entry=12080) at ../../src/StoreIOBuffer.h:23
No locals.
#11 0x000055555588808b in Client::addVirginReplyBody (
    this=this at entry=0x555558aee1f8, 
    data=0x55555ec07ba0 "\245N\313\343Q\334M8t\365\356\341\f\177O\257\242\376\240\031\341\317\022Gh\243\324\034\211\273\317\037\f\353XQN\034\325\312\355\330\337\251y\021\335L\266N\342\071<7\215\230\234\300\225\343\316\320\221\271\316\020IT\335f\"I\302}\236\301\206}\325\r\243\232\336\247\v\277\360H\352\026@\005\356\227\204\267\206\036\361\245\357y?k\310T\324\023h\325\304#t\vB\352\273\005\253\331a\027`+\217!i\331=\374\356V\310\224\327\336\336A\035>\322\377\250q\350\207\v\030:\341\n\f]\311'?w^$\vg\024\067\374\263'\352\347)p\230\024\217\036\270b\023\320\025\066\021Z7\230\217\006n\300\314a,\304.w\337\367\332\304\333o"..., 
    len=len at entry=12080) at Client.cc:1049
No locals.
#12 0x00005555557e9dd6 in HttpStateData::writeReplyBody (
    this=this at entry=0x555558aee1f8) at http.cc:1415
        data = <optimized out>
        len = 12080
        clen = 93825148988456
        parsedWhole = <optimized out>
#13 0x00005555557ede3a in HttpStateData::processReplyBody (
    this=0x555558aee1f8) at http.cc:1498
        __FUNCTION__ = "processReplyBody"
#14 0x00005555557f1015 in HttpStateData::processReply (
    this=this at entry=0x555558aee1f8) at http.cc:1295
        __FUNCTION__ = "processReply"
#15 0x00005555557f208a in HttpStateData::readReply (this=0x555558aee1f8, 
    io=...) at http.cc:1265
        __FUNCTION__ = "readReply"
        rd = {
          <CommCommonCbParams> = {
            data = 0x555558aee1f8,
            conn = {
              p_ = 0x55555eb588d0
            },
            flag = Comm::OK,
            xerrno = 0,
            fd = -1
          }, 
          members of CommIoCbParams:
          buf = 0x0,
          size = 12080
        }
#16 0x00005555557f782c in CommCbMemFunT<HttpStateData, CommIoCbParams>::doDial
    (this=0x55555eadc0a8) at CommCalls.h:190
No locals.
#17 0x00005555557f748f in JobDialer<HttpStateData>::dial (
    this=0x55555eadc0a8, call=...) at base/AsyncJobCalls.h:170
        __FUNCTION__ = "dial"
#18 0x00005555557f773e in AsyncCallT<CommCbMemFunT<HttpStateData, CommIoCbParams> >::fire (this=<optimized out>) at ../src/base/AsyncCall.h:147
No locals.
#19 0x00005555558e4f4e in AsyncCall::make (this=0x55555eadc070)
    at AsyncCall.cc:44
        __FUNCTION__ = "make"
#20 0x00005555558e69cc in AsyncCallQueue::fire (this=0x555555d9b410)
    at ../../src/base/RefCount.h:73
        call = {
          p_ = 0x55555eadc070
        }
        made = true
        __FUNCTION__ = "fire"
#21 0x0000555555710e93 in EventLoop::dispatchCalls (
    this=this at entry=0x7fffffffe940) at EventLoop.cc:144
        dispatchedSome = <optimized out>
#22 0x0000555555710f86 in EventLoop::runOnce (this=this at entry=0x7fffffffe940)
    at EventLoop.cc:121
        sawActivity = <optimized out>
        waitingEngine = 0x7fffffffe860
        __FUNCTION__ = <optimized out>
#23 0x0000555555710ff1 in EventLoop::run (this=0x7fffffffe940)
    at EventLoop.cc:83
No locals.
#24 0x0000555555805a58 in SquidMain (argc=<optimized out>, 
    argv=<optimized out>) at main.cc:1710
        cmdLine = {
          argv_ = std::vector of length 3, capacity 3 = {
            0x555555cc7870 "/usr/sbin/squid", 0x555555cc7890 "-NsYC", 0x0},
          shortOptions_ = 0x555555cc7820 "CDFNRSYXa:d:f:hk:m::n:sl:u:vz?",
          longOptions_ = std::vector of length 5, capacity 8 = {{
              <option> = {
                name = 0x555555cc7970 "foreground",
                has_arg = 0,
                flag = 0x0,
                val = 2
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc7b70 "kid",
                has_arg = 1,
                flag = 0x0,
                val = 3
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc7b90 "help",
                has_arg = 0,
                flag = 0x0,
                val = 104
              }, <No data fields>}, {
              <option> = {
                name = 0x555555cc7bb0 "version",
                has_arg = 0,
                flag = 0x0,
                val = 118
              }, <No data fields>}, {
              <option> = {
                name = 0x0,
                has_arg = 0,
                flag = 0x0,
                val = 0
              }, <No data fields>}}
        }
        WIN32_init_err = 0
        __FUNCTION__ = "SquidMain"
        mainLoop = {
          errcount = 0,
          static Running = 0x7fffffffe940,
          last_loop = false,
          engines = std::vector of length 4, capacity 4 = {0x7fffffffe820, 
            0x555555c13760 <EventScheduler::_instance>, 0x7fffffffe840, 
            0x7fffffffe860},
          timeService = 0x7fffffffe880,
          primaryEngine = 0x7fffffffe860,
          loop_delay = 0,
          error = false,
          runOnceResult = false
        }
        signalEngine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b75138 <vtable for SignalEngine+16>
          }, <No data fields>}
        store_engine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b75110 <vtable for StoreRootEngine+16>
          }, <No data fields>}
        comm_engine = {
          <AsyncEngine> = {
            _vptr.AsyncEngine = 0x555555b84c08 <vtable for CommSelectEngine+16>
          }, <No data fields>}
        time_engine = {
          _vptr.Engine = 0x555555b93ac8 <vtable for Time::Engine+16>
        }
#25 0x000055555580603a in SquidMainSafe (argv=0x7fffffffed48, argc=2)
    at main.cc:1353
        __FUNCTION__ = <optimized out>
        _dbg_level = <optimized out>
        _dbo = <optimized out>
#26 main (argc=2, argv=0x7fffffffed48) at main.cc:1341
No locals.
Saved corefile core.4053842
A debugging session is active.

	Inferior 1 [process 4053842] will be killed.

Quit anyway? (y or n) [answered Y; input not from terminal]

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From squid at borrill.org.uk  Tue Oct 24 09:01:21 2023
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 24 Oct 2023 10:01:21 +0100
Subject: [squid-users] Squid 6.4 assertion errors: FATAL: assertion
 failed: stmem.cc:98: "lowestOffset () <= target_offset" current master
 transaction: master655 (backtrace)]
Message-ID: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>

On Mon Oct 23 11:39:52 UTC 2023 Ralf Hildebrandt wrote:

> I upgraded from 6.3 to 6.4 today and it INSTANTLY began crashing
> (frequently!)
> 
> The Log messages in cache.log:
> ...
> 2023/10/23 09:57:21| Beginning Validation Procedure
> 2023/10/23 09:57:21| Completed Validation Procedure
>     Validated 237163 Entries
>     store_swap_size = 29491032.00 KB
> 2023/10/23 09:57:21| Recv recv: (111) Connection refused
> 2023/10/23 09:57:21| Closing Pinger socket on FD 25
> 2023/10/23 09:57:22| storeLateRelease: released 0 objects
> 2023/10/23 09:57:46| FATAL: assertion failed: stmem.cc:98: "lowestOffset () <= target_offset"
>     current master transaction: master655

I'll add a "me too" to this. 6.3 reliable, 6.4 crashes and this is under 
_very_ low load. NetBSD 9.3_STABLE.

-- 
Stephen


From Ralf.Hildebrandt at charite.de  Tue Oct 24 09:26:46 2023
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 24 Oct 2023 11:26:46 +0200
Subject: [squid-users] [ext] Re: Squid 6.4 assertion errors: FATAL:
 assertion failed: stmem.cc:98: "lowestOffset () <= target_offset" current
 master transaction: master655 (backtrace)]
In-Reply-To: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>
References: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>
Message-ID: <ZTeN1mEL9Alp6V7O@charite.de>

> I'll add a "me too" to this. 6.3 reliable, 6.4 crashes and this is under
> _very_ low load. NetBSD 9.3_STABLE.

You can check the debugging recommendation in 
https://bugs.squid-cache.org/show_bug.cgi?id=5309

I'll try 6.4 on my test proxy now (with very low to no load at all),
and will also try 7.0/master

-- 
Ralf Hildebrandt
Charit? - Universit?tsmedizin Berlin
Gesch?ftsbereich IT | Abteilung Netz | Netzwerk-Administration
Invalidenstra?e 120/121 | D-10115 Berlin

Tel. +49 30 450 570 155
ralf.hildebrandt at charite.de
https://www.charite.de


From admin at eduaicta.ro  Tue Oct 24 12:06:45 2023
From: admin at eduaicta.ro (Avram-Teodor Berindeie)
Date: Tue, 24 Oct 2023 15:06:45 +0300
Subject: [squid-users] Squid 6.4 assertion errors: FATAL: assertion failed:
 stmem.cc:98: "lowestOffset () <= target_offset" current master transaction:
 master655 (backtrace)]
Message-ID: <CAOsGNXwByxAs5HGvoa764dCW9dM83zDwnwW6ChyBF4q8ypUpCg@mail.gmail.com>

I'll also add a "me too" to this. Squid 6.4 crashes frequently.

Slackware64-current
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231024/a31a0f37/attachment.htm>

From squid3 at treenet.co.nz  Tue Oct 24 17:02:48 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 25 Oct 2023 06:02:48 +1300
Subject: [squid-users] [ext] Re: Squid 6.4 assertion errors: FATAL:
 assertion failed: stmem.cc:98: "lowestOffset () <= target_offset" current
 master transaction: master655 (backtrace)]
In-Reply-To: <ZTeN1mEL9Alp6V7O@charite.de>
References: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>
 <ZTeN1mEL9Alp6V7O@charite.de>
Message-ID: <1fac5be7-01b2-4b97-ae70-466379b9e9ee@treenet.co.nz>

On 24/10/23 22:26, Ralf Hildebrandt wrote:
>> I'll add a "me too" to this. 6.3 reliable, 6.4 crashes and this is under
>> _very_ low load. NetBSD 9.3_STABLE.
> 
> You can check the debugging recommendation in
> https://bugs.squid-cache.org/show_bug.cgi?id=5309
> 
> I'll try 6.4 on my test proxy now (with very low to no load at all),
> and will also try 7.0/master
> 

FTR; current workaround is to reverse this patch from 6.4:
 
https://github.com/squid-cache/squid/commit/a27bf4b84da23594150c7a86a23435df0b35b988

That is a partial removal of the SQUID-2003:2 vulnerability fix. I hope 
we can have this corrected and an updated fix in the scheduled 6.5 
release on Nov 5.


HTH
Amos


From stu at spacehopper.org  Tue Oct 24 18:21:00 2023
From: stu at spacehopper.org (Stuart Henderson)
Date: Tue, 24 Oct 2023 18:21:00 -0000 (UTC)
Subject: [squid-users] Security advisories pointing to Squid 6.4,
 but no download (yer)?
References: <ZTUpOYWJhjCU5PV5@charite.de>
 <CA+Y8hcNesdUzLVqA_=3QJq-bJwYGyxEfoS6MwMHLOLA3sOpJnw@mail.gmail.com>
Message-ID: <slrnujg2oc.2kh6.stu.lists@naiad.spacehopper.org>

On 2023-10-22, Francesco Chemolli <gkinkie at gmail.com> wrote:
>    It might be some delay in propagating to the mirrors. I see 6.4 is
> available at http://static.squid-cache.org/Versions/v6/ .

Currently the site with the only AAAA for www.squid-cache.org does not
carry the 6.4 release, so fetching with IPv6 preferred fails from there
(at least with download tools that don't fallback to the other address
family after 404).




From stu at spacehopper.org  Tue Oct 24 22:34:34 2023
From: stu at spacehopper.org (Stuart Henderson)
Date: Tue, 24 Oct 2023 22:34:34 -0000 (UTC)
Subject: [squid-users] [ext] Re: Squid 6.4 assertion errors: FATAL:
 assertion failed: stmem.cc:98: "lowestOffset () <= target_offset" current
 master transaction: master655 (backtrace)]
References: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>
 <ZTeN1mEL9Alp6V7O@charite.de>
 <1fac5be7-01b2-4b97-ae70-466379b9e9ee@treenet.co.nz>
Message-ID: <slrnujghjq.2kh6.stu.lists@naiad.spacehopper.org>

On 2023-10-24, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 24/10/23 22:26, Ralf Hildebrandt wrote:
>>> I'll add a "me too" to this. 6.3 reliable, 6.4 crashes and this is under
>>> _very_ low load. NetBSD 9.3_STABLE.
>> 
>> You can check the debugging recommendation in
>> https://bugs.squid-cache.org/show_bug.cgi?id=5309
>> 
>> I'll try 6.4 on my test proxy now (with very low to no load at all),
>> and will also try 7.0/master
>> 
>
> FTR; current workaround is to reverse this patch from 6.4:
>  
> https://github.com/squid-cache/squid/commit/a27bf4b84da23594150c7a86a23435df0b35b988

Unfortunately other commits on top touch some of the same files so the
simple methods of applying the above with patch -R, or 'git revert',
need more fix-up.




From admin at eduaicta.ro  Wed Oct 25 06:12:52 2023
From: admin at eduaicta.ro (Avram-Teodor Berindeie)
Date: Wed, 25 Oct 2023 09:12:52 +0300
Subject: [squid-users] [ext] Re: Squid 6.4 assertion errors: FATAL:
 assertion failed: stmem.cc:98: "lowestOffset () <= target_offset" current
 master transaction: master655 (backtrace)]
In-Reply-To: <CAOsGNXwByxAs5HGvoa764dCW9dM83zDwnwW6ChyBF4q8ypUpCg@mail.gmail.com>
References: <CAOsGNXwByxAs5HGvoa764dCW9dM83zDwnwW6ChyBF4q8ypUpCg@mail.gmail.com>
Message-ID: <CAOsGNXwLyHCp+WzmNCf05OafNmtfJ2ORwEBpnt_Wy+2Rn-YBDA@mail.gmail.com>

If it is not possible to revert the commits, what should be done?
Should I go back to Squid 6.3 (6.4 was pulled from the site)?
http://www.squid-cache.org/Versions/v6/
I have clients where Squid cannot be easily disabled and are now without
internet connection.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231025/e0239b96/attachment.htm>

From gkinkie at gmail.com  Wed Oct 25 19:35:42 2023
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Wed, 25 Oct 2023 21:35:42 +0200
Subject: [squid-users] Build Issue on Ubuntu 22.04 for Squid version 6.4
In-Reply-To: <BLAPR07MB8353171E6898D001D7C31F769DD8A@BLAPR07MB8353.namprd07.prod.outlook.com>
References: <BLAPR07MB83532495B9E9131DC5657C799DD8A@BLAPR07MB8353.namprd07.prod.outlook.com>
 <BLAPR07MB8353171E6898D001D7C31F769DD8A@BLAPR07MB8353.namprd07.prod.outlook.com>
Message-ID: <CA+Y8hcMt8BpG92LbPtTkCD3ZT54O_ttLUF_AEgCYS7ktVkpcPw@mail.gmail.com>

Hi Raja,
   I tried reproducing your error and could not.
What version of gcc are you using?


On Mon, Oct 23, 2023 at 5:20?AM Rajasekaran, Raja <
Raja.Rajasekaran at honeywell.com> wrote:

> Hi All
>
>
>
> I?m trying to compile and install deb package of squid 6.4, I am using the
> Ubuntu server 22.04.02 as my build machine.
>
>
>
> While running make command, I get the following error. Could you anyone
> help me, figure it out.
>
>
>
> lto-wrapper: warning: using serial compilation of 100 LTRANS jobs
>
> acl/BoolOps.cc: In member function '__ct_base ':
>
> acl/BoolOps.cc:20:5: error: '__builtin_strlen' reading 1 or more bytes
> from a region of size 0 [-Werror=stringop-overread]
>
>    20 |     Must(strlen(acl->name) <= sizeof(name)-2);
>
>       |     ^
>
> lto1: all warnings being treated as errors
>
> lto-wrapper: fatal error: g++ returned 1 exit status
>
> compilation terminated.
>
> /usr/bin/ld: error: lto-wrapper failed
>
> collect2: error: ld returned 1 exit status
>
> libtool: link: rm -f ".libs/squidS.o"
>
> make[3]: *** [Makefile:5220: squid] Error 1
>
> make[3]: Leaving directory '/home/coeadmin/squid-6.4-build1/squid-6.4/src'
>
> make[2]: *** [Makefile:6004: all-recursive] Error 1
>
> make[2]: Leaving directory '/home/coeadmin/squid-6.4-build1/squid-6.4/src'
>
> make[1]: *** [Makefile:5004: all] Error 2
>
> make[1]: Leaving directory '/home/coeadmin/squid-6.4-build1/squid-6.4/src'
>
> make: *** [Makefile:599: all-recursive] Error 1
>
>
>
>
>
> Steps I followed.
>
>
>
>    1. Ran build dep to install build dependencies.
>    2. Executed configure command with following options.
>
> --datadir=/usr/share/squid \
>
>                                 --sysconfdir=/etc/squid \
>
>                                 --libexecdir=/usr/lib/squid \
>
>                                 --mandir=/usr/share/man \
>
>                                 --enable-inline \
>
>                                 --disable-arch-native \
>
>                                 --enable-async-io=8 \
>
>                                 --enable-storeio="ufs,aufs,diskd,rock" \
>
>                                 --enable-removal-policies="lru,heap" \
>
>                                 --enable-delay-pools \
>
>                                 --enable-cache-digests \
>
>                                 --enable-icap-client \
>
>                                 --enable-follow-x-forwarded-for \
>
>
> --enable-auth-basic="DB,fake,getpwnam,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB" \
>
>                                 --enable-auth-digest="file,LDAP" \
>
>                                 --enable-auth-negotiate="kerberos,wrapper"
> \
>
>                                 --enable-auth-ntlm="fake,SMB_LM" \
>
>
> --enable-external-acl-helpers="file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group"
> \
>
>                                 --enable-security-cert-validators="fake" \
>
>                                 --enable-storeid-rewrite-helpers="file" \
>
>                                 --enable-url-rewrite-helpers="fake" \
>
>                                 --enable-eui \
>
>                                 --enable-esi \
>
>                                 --enable-icmp \
>
>                                 --enable-zph-qos \
>
>                                 --enable-ecap \
>
>                                 --disable-translation \
>
>                                 --with-swapdir=/var/spool/squid \
>
>                                 --with-logdir=/var/log/squid \
>
>                                 --with-pidfile=/run/squid.pid \
>
>                                 --with-filedescriptors=65536 \
>
>                                 --with-large-files \
>
>                                 --with-default-user=proxy \
>
>                                 --enable-linux-netfilter --with-systemd \
>
>                                 --with-openssl \
>
>                                 --enable-ssl-crtd
>
>
>
>    1. executed make, which resulted in above error.
>
>
>
> Please let me know if more information is required
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231025/4fc2fc72/attachment.htm>

From magri at web.de  Thu Oct 26 12:37:43 2023
From: magri at web.de (magri at web.de)
Date: Thu, 26 Oct 2023 14:37:43 +0200
Subject: [squid-users] Get IP of denied request
Message-ID: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>

Hi list,

TL;DR: is there a way to get/log the resolved ip of a denied request?


We have a rather large ip based malware blacklist (dst acl) and
sometimes a destination is blocked inadvertantly because of a false
positive entry in this list.
This happens most often with CDNs where the ips of a destination change
often and even move between different sites.

Because of this rapid change it's difficult to determine the blocked ip
in hindsight when analyzing access problems and makes it impossible to
correct the blacklist.

For normal requests the resolved and accessed ip is be logged with %<a,
but that doesn't happen when the request is denied.

Is there any way to get the ip logged that was used in the dst-acl aside
from debug logging? Maybe through some annotation mechanism?

Squid version is 6.2, as 6.4 crashes with assertion errors here, too.

thanks,
Martin



From TMueller at pdv-sachsen.net  Thu Oct 26 13:27:44 2023
From: TMueller at pdv-sachsen.net (=?utf-8?B?TcO8bGxlciwgVGhvbWFz?=)
Date: Thu, 26 Oct 2023 13:27:44 +0000
Subject: [squid-users] fallback from kerberos sso to basic auth
Message-ID: <15a3394f95b8654cb26f4201b1cade61b1cf003e.camel@pdv-sachsen.net>

Hi,

does anyone knows wether is this scenario possible?

setup:

- squid 5.x?
- host is domain joined (winbind)
- kerberos is configured and working
- squid uses as auth params kerberos/ntlm - ntlm - basic (ldap) in this
order
- clients are all domain joined
- every user is a domain user
- some users are member of group "internet", some others are not
- permission is set to group members (internet)?

challenge:
- the users without group membership (internet) should be forced to use
basic auth (ldap) to give alternative user credentials (with group
membership internet)

Some tries with auth_schemes were not succesful.

Customer use in its old setup (Microsoft Forefront TMG) with this
feature (someone had built a workaround with a helper script in the
past.)
I had never seen this, but the behaviour of it.

-- 
Mit freundlichen Gr??en

Thomas M?ller 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231026/0a1d4c9e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4649 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231026/0a1d4c9e/attachment.bin>

From rousskov at measurement-factory.com  Thu Oct 26 19:11:01 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Oct 2023 15:11:01 -0400
Subject: [squid-users] Get IP of denied request
In-Reply-To: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>
References: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>
Message-ID: <95cddc1f-73e9-431d-8ba0-e92a09a95487@measurement-factory.com>

On 2023-10-26 08:37, magri at web.de wrote:

> TL;DR: is there a way to get/log the resolved ip of a denied request?

TLDR: Bugs notwithstanding, use %<a.


> We have a rather large ip based malware blacklist (dst acl) and
> sometimes a destination is blocked inadvertantly because of a false
> positive entry in this list.
> This happens most often with CDNs where the ips of a destination change
> often and even move between different sites.
> 
> Because of this rapid change it's difficult to determine the blocked ip
> in hindsight when analyzing access problems and makes it impossible to
> correct the blacklist.
> 
> For normal requests the resolved and accessed ip is be logged with %<a,
> but that doesn't happen when the request is denied.


If a request was denied by a dst ACL based on its successfully resolved 
destination IP address but %<a was logged as "-", then it is a Squid bug 
that should be fixed IMO. Meanwhile, you can annotate every dst match 
and log that annotation. Here is an untested sketch:

     acl matchDst1 dst 127.0.0.1
     acl markDst1 note matched=127.0.0.1
     acl all-of dst1 matchDst1 markDst1
     http_access deny dst1

     acl matchDst2 dst 127.0.0.2
     acl markDst2 note matched=127.0.0.2
     acl all-of dst2 matchDst2 markDst2
     http_access deny dst2

     logformat myFormat ... matched_dst=%note{matched}
     access_log ...


The same thing with fewer lines (but with fewer ways to group dst1 and 
dst2 with other ACLs):

     acl matchDst1 dst 127.0.0.1
     acl markDst1 note matched=127.0.0.1
     http_access deny matchDst1 markDst1

     acl matchDst2 dst 127.0.0.2
     acl markDst2 note matched=127.0.0.2
     http_access deny matchDst2 markDst2

     logformat myFormat ... matched_dst=%note{matched}
     access_log ...

For long dst lists, the above approach will require scripting the 
generation of the corresponding squid.conf portions or include files, of 
course.


If a request was denied by a dst ACL because its destination IP address 
could not be resolved, then %<a should be logged as "-". I cannot think 
of a way to distinguish this case from other cases where %<a is "-". It 
feels like address resolution failures should be available via 
%err_detail, but I doubt Squid code populates that information in these 
cases. Another problem to fix!


HTH,

Alex.



> Is there any way to get the ip logged that was used in the dst-acl aside
> from debug logging? Maybe through some annotation mechanism?
> 
> Squid version is 6.2, as 6.4 crashes with assertion errors here, too.
> 
> thanks,
> Martin
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From andre.bolinhas at articatech.com  Fri Oct 27 01:08:51 2023
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Fri, 27 Oct 2023 02:08:51 +0100
Subject: [squid-users] Cache NTLM Authenticaion
Message-ID: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>

Hi

It's possible squid cache NTLM authentication from users?

My goal is to store the credentials in cache in order to reduce the 
request to Active Directory.

I'm trying guide from this squid : auth_param configuration directive 
(squid-cache.org) <http://www.squid-cache.org/Doc/config/auth_param/> 
but there is no information relative to cache the authentication / 
credentials.

Also, in NTLM did you recommend to use the keep_alive option?

Best regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20231027/8afa6179/attachment.htm>

From squid3 at treenet.co.nz  Fri Oct 27 08:28:28 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Oct 2023 21:28:28 +1300
Subject: [squid-users] Cache NTLM Authenticaion
In-Reply-To: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
References: <b287aad8-f4d3-4a24-afb6-763083425098@articatech.com>
Message-ID: <1451fd0d-c7eb-45fd-bd17-eca1d495fb5d@treenet.co.nz>

On 27/10/23 14:08, Andre Bolinhas wrote:
> Hi
> 
> It's possible squid cache NTLM authentication from users?
> 

NTLM tokens are unique per TCP connection. So no, caching is a pointless 
waste of CPU and memory. The best that can be done already is.


> My goal is to store the credentials in cache in order to reduce the 
> request to Active Directory.
> 

The only way to do that is to reduce unique TCP connections between 
clients and Squid.

Check that 
<http://www.squid-cache.org/Doc/config/client_persistent_connections/> 
directive is either absent or turned "on" explicitly.



> I'm trying guide from this squid : auth_param configuration directive 
> (squid-cache.org) <http://www.squid-cache.org/Doc/config/auth_param/> 
> but there is no information relative to cache the authentication / 
> credentials.
> 
> Also, in NTLM did you recommend to use the keep_alive option?

If it works, yes. Though be aware it only affects the initial request of 
the NTLM handshake.


Cheers
Amos


From magri at web.de  Fri Oct 27 11:14:56 2023
From: magri at web.de (magri at web.de)
Date: Fri, 27 Oct 2023 13:14:56 +0200
Subject: [squid-users] Get IP of denied request
In-Reply-To: <95cddc1f-73e9-431d-8ba0-e92a09a95487@measurement-factory.com>
References: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>
 <95cddc1f-73e9-431d-8ba0-e92a09a95487@measurement-factory.com>
Message-ID: <107552eb-8406-455a-88f0-6d1bb3a2d328@web.de>

Hi Alex,


Am 26.10.23 um 21:11 schrieb Alex Rousskov:
> On 2023-10-26 08:37, magri at web.de wrote:
>
>> TL;DR: is there a way to get/log the resolved ip of a denied request?
>
> TLDR: Bugs notwithstanding, use %<a.
>

%<a doesn't work :-(

Tested with attached minimal-squid.conf:
- config blacklists the ip of www.example.org
- logformat uses %<a

Test-Call:
http_proxy=http://127.0.0.1:3128 https_proxy=http://127.0.0.1:3128 wget
http://www.example.org
--2023-10-27 09:52:27--  http://www.example.org/
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response... 403 Forbidden
2023-10-27 09:52:31 ERROR 403: Forbidden.

Log contains:
1698393151.066   3903 127.0.0.1 TCP_DENIED/- 3889 GET
http://www.example.org/ - HIER_NONE/- text/html HTTP/1.1 37030 182 "-"
"Wget/1.21.3" ERR_ACCESS_DENIED "-" 53

>
>> We have a rather large ip based malware blacklist (dst acl) and
>> sometimes a destination is blocked inadvertantly because of a false
>> positive entry in this list.
>> This happens most often with CDNs where the ips of a destination change
>> often and even move between different sites.
>>
>> Because of this rapid change it's difficult to determine the blocked ip
>> in hindsight when analyzing access problems and makes it impossible to
>> correct the blacklist.
>>
>> For normal requests the resolved and accessed ip is be logged with %<a,
>> but that doesn't happen when the request is denied.
>
>
> If a request was denied by a dst ACL based on its successfully resolved
> destination IP address but %<a was logged as "-", then it is a Squid bug
> that should be fixed IMO. Meanwhile, you can annotate every dst match
> and log that annotation. Here is an untested sketch:
>
>  ??? acl matchDst1 dst 127.0.0.1
>  ??? acl markDst1 note matched=127.0.0.1
>  ??? acl all-of dst1 matchDst1 markDst1
>  ??? http_access deny dst1
>
>  ??? acl matchDst2 dst 127.0.0.2
>  ??? acl markDst2 note matched=127.0.0.2
>  ??? acl all-of dst2 matchDst2 markDst2
>  ??? http_access deny dst2
>
>  ??? logformat myFormat ... matched_dst=%note{matched}
>  ??? access_log ...
>
>
> The same thing with fewer lines (but with fewer ways to group dst1 and
> dst2 with other ACLs):
>
>  ??? acl matchDst1 dst 127.0.0.1
>  ??? acl markDst1 note matched=127.0.0.1
>  ??? http_access deny matchDst1 markDst1
>
>  ??? acl matchDst2 dst 127.0.0.2
>  ??? acl markDst2 note matched=127.0.0.2
>  ??? http_access deny matchDst2 markDst2
>
>  ??? logformat myFormat ... matched_dst=%note{matched}
>  ??? access_log ...
>
> For long dst lists, the above approach will require scripting the
> generation of the corresponding squid.conf portions or include files, of
> course.
>

I don't think this scales to blacklists with 6-digit count sizes and it
also doesn't work for blacklisted networks :-(
I hoped there would be a way to get the ip as some kind of variable like
the header fields in logformat.

>
> If a request was denied by a dst ACL because its destination IP address
> could not be resolved, then %<a should be logged as "-". I cannot think
> of a way to distinguish this case from other cases where %<a is "-". It
> feels like address resolution failures should be available via
> %err_detail, but I doubt Squid code populates that information in these
> cases. Another problem to fix!

I'm not familiar with the code but from staring at it I get that '%<a'
is fetched from hier.tcpServer->remote and this isn't necessarily the ip
of the resolved URI FQDN (e.g. when using a parent proxy).

There doesn't seem to be any code that stores or reuses a once resolved
ip in acl/DestinationIP.cc (at least if a request exists).
As far as I understand it for every dst ACL the ip is fetched
(asyncronously) from the ipcache and compared to the ACL but never
stored for later use.

Any ideas?
Martin

>
>
> HTH,
>
> Alex.
>
>
>
>> Is there any way to get the ip logged that was used in the dst-acl aside
>> from debug logging? Maybe through some annotation mechanism?
>>
>> Squid version is 6.2, as 6.4 crashes with assertion errors here, too.
>>
>> thanks,
>> Martin
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
http_port 3128

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443

acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

acl www_example_org dst 93.184.216.34

http_access deny www_example_org
http_access allow all

never_direct deny all
always_direct allow all

logformat debug_squid %ts.%03tu %6tr %>a %Ss/%<Hs %<st %rm %ru %un %Sh/%<a %mt HTTP/%rv %>p %>st "%{Referer}>h" "%{User-Agent}>h" %err_code "%err_detail" %master_xaction
access_log daemon:/var/log/squid/test.log logformat=debug_squid
cache_log /var/log/squid/cache.log
debug_options ALL,2

From rousskov at measurement-factory.com  Fri Oct 27 14:22:44 2023
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 27 Oct 2023 10:22:44 -0400
Subject: [squid-users] Get IP of denied request
In-Reply-To: <107552eb-8406-455a-88f0-6d1bb3a2d328@web.de>
References: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>
 <95cddc1f-73e9-431d-8ba0-e92a09a95487@measurement-factory.com>
 <107552eb-8406-455a-88f0-6d1bb3a2d328@web.de>
Message-ID: <e3d70ac4-a680-4de8-bfd9-4e6fd0698fa2@measurement-factory.com>

On 2023-10-27 07:14, magri at web.de wrote:
> Am 26.10.23 um 21:11 schrieb Alex Rousskov:
>> On 2023-10-26 08:37, magri at web.de wrote:
>>> TL;DR: is there a way to get/log the resolved ip of a denied request?
>>
>> TLDR: Bugs notwithstanding, use %<a.

> %<a doesn't work :-(

Sorry, my first response was wrong: As you have correctly explained, %<a 
is the destination address of the used next hop connection while dst ACL 
uses an address derived from the request-target. The two addresses do 
not have to match and, more importantly, it is wrong to expect a dst ACL 
to update/store that "used next hop address" information when the ACL 
itself does not open or reuse a connection to that address.


>> ???? acl matchDst1 dst 127.0.0.1
>> ???? acl markDst1 note matched=127.0.0.1
>> ???? http_access deny matchDst1 markDst1
>>
>> ???? acl matchDst2 dst 127.0.0.2
>> ???? acl markDst2 note matched=127.0.0.2
>> ???? http_access deny matchDst2 markDst2
>>
>> ???? logformat myFormat ... matched_dst=%note{matched}
>> ???? access_log ...
>>
>> For long dst lists, the above approach will require scripting the
>> generation of the corresponding squid.conf portions or include files, of
>> course.

> I don't think this scales to blacklists with 6-digit count sizes 

I think that depends on the definition of "to scale".


> and it also doesn't work for blacklisted networks :-(

Agreed. This approach can only log which dst ACL data matched, and that 
data may be different from the IP.


> I hoped there would be a way to get the ip as some kind of variable like
> the header fields in logformat.

Yes, I know. I was just documenting an existing workaround for cases 
where it is usable.


> Any ideas?

I have three:

1. Enhance Squid to resolve transaction destination address once (on 
first use/need). Remember/reuse resolved IP addresses. Log them via some 
new %resolved_dst and %dst_resolution_detail codes.

This improvement will help address a few use cases unrelated to this 
discussion, but it will _not_ tell you which of the resolved addresses 
actually matched which ACL. You will be able to guess in many cases, but 
there will be exceptions.


2. Add a Squid feature where an evaluated ACL can be configured to 
annotate the transaction with the information about that evaluation.

To start with, we can support annotations for _matched_ dst ACLs. For 
example:

     # When matches, sets the following master transaction annotations:
     # * badDestination_input: used destination address (IP or name)
     # * badDestination_match: the matched destination IP
     # * badDestination_value: the matched ACL parameter value
     # * badDestination_ips: resolved destination IP(s)
     # * badDestination_errors: DNS resolution and other error(s)
     # * badDestination_matches: number of matches (this transaction)
     # * badDestination_evals: number of evaluations (this transaction)
     acl badDestination dst --on-match=annotate 127.0.0.0/24 10.0.0.1

If the same ACL matches more than once, the last(?) match wins, but the 
aclfoo_matches annotation can be used to detect these cases. The 
aclfoo_evals annotation can be used to detect whether this ACL was 
"reached" at all.

If really needed, we can support turning individual annotations on and 
off, but I doubt that complexity is worth associated performance 
savings. After all, most ACLs will only match once per transaction 
lifetime (for correctly written configurations). Access.log will be 
configured to only log annotations of interest to the admin, of course.


The above approach can be extended to provide ACL debugging aid:

     # Dumps every mismatch information to cache.log at level 1
     acl goodDestination dst --on-mismatch=log 127.0.0.0/24

     # Dumps every evaluation information to cache.log at level 1
     acl redDestination dst --on-eval=log 127.0.0.0/24


3. Add a Squid feature where Squid (optionally) maintains an internal 
database of recent ACL evaluation history and makes that information 
accessible via cache manager queries like "which ACLs matched 
transaction X?" (where X is logged %master_xaction ID).


The three sketched options are not mutually exclusive, of course. All 
require non-trivial code changes.

Would any of the above options address your needs? Any preferences or 
spec adjustments?



Thank you,

Alex.


>>> Is there any way to get the ip logged that was used in the dst-acl aside
>>> from debug logging? Maybe through some annotation mechanism?
>>>
>>> Squid version is 6.2, as 6.4 crashes with assertion errors here, too.
>>>
>>> thanks,
>>> Martin



From magri at web.de  Mon Oct 30 17:08:17 2023
From: magri at web.de (magri at web.de)
Date: Mon, 30 Oct 2023 18:08:17 +0100
Subject: [squid-users] Get IP of denied request
In-Reply-To: <e3d70ac4-a680-4de8-bfd9-4e6fd0698fa2@measurement-factory.com>
References: <c5115934-5936-49b0-8a3e-d4231689dcbc@web.de>
 <95cddc1f-73e9-431d-8ba0-e92a09a95487@measurement-factory.com>
 <107552eb-8406-455a-88f0-6d1bb3a2d328@web.de>
 <e3d70ac4-a680-4de8-bfd9-4e6fd0698fa2@measurement-factory.com>
Message-ID: <c5df1856-0750-4259-9dd9-c01ccf90cc30@web.de>



Am 27.10.23 um 16:22 schrieb Alex Rousskov:
> On 2023-10-27 07:14, magri at web.de wrote:
>> Am 26.10.23 um 21:11 schrieb Alex Rousskov:
>>> On 2023-10-26 08:37, magri at web.de wrote:
>>>> TL;DR: is there a way to get/log the resolved ip of a denied request?
>>>
>>> TLDR: Bugs notwithstanding, use %<a.
>
>> %<a doesn't work :-(
>
> Sorry, my first response was wrong: As you have correctly explained, %<a
> is the destination address of the used next hop connection while dst ACL
> uses an address derived from the request-target. The two addresses do
> not have to match and, more importantly, it is wrong to expect a dst ACL
> to update/store that "used next hop address" information when the ACL
> itself does not open or reuse a connection to that address.
>
>
>>> ???? acl matchDst1 dst 127.0.0.1
>>> ???? acl markDst1 note matched=127.0.0.1
>>> ???? http_access deny matchDst1 markDst1
>>>
>>> ???? acl matchDst2 dst 127.0.0.2
>>> ???? acl markDst2 note matched=127.0.0.2
>>> ???? http_access deny matchDst2 markDst2
>>>
>>> ???? logformat myFormat ... matched_dst=%note{matched}
>>> ???? access_log ...
>>>
>>> For long dst lists, the above approach will require scripting the
>>> generation of the corresponding squid.conf portions or include files, of
>>> course.
>
>> I don't think this scales to blacklists with 6-digit count sizes
>
> I think that depends on the definition of "to scale".
>
>
>> and it also doesn't work for blacklisted networks :-(
>
> Agreed. This approach can only log which dst ACL data matched, and that
> data may be different from the IP.
>
>
>> I hoped there would be a way to get the ip as some kind of variable like
>> the header fields in logformat.
>
> Yes, I know. I was just documenting an existing workaround for cases
> where it is usable.
>
>
>> Any ideas?
>
> I have three:
>
> 1. Enhance Squid to resolve transaction destination address once (on
> first use/need). Remember/reuse resolved IP addresses. Log them via some
> new %resolved_dst and %dst_resolution_detail codes.
>
> This improvement will help address a few use cases unrelated to this
> discussion, but it will _not_ tell you which of the resolved addresses
> actually matched which ACL. You will be able to guess in many cases, but
> there will be exceptions.
>
>
> 2. Add a Squid feature where an evaluated ACL can be configured to
> annotate the transaction with the information about that evaluation.
>
> To start with, we can support annotations for _matched_ dst ACLs. For
> example:
>
>  ??? # When matches, sets the following master transaction annotations:
>  ??? # * badDestination_input: used destination address (IP or name)
>  ??? # * badDestination_match: the matched destination IP
>  ??? # * badDestination_value: the matched ACL parameter value
>  ??? # * badDestination_ips: resolved destination IP(s)
>  ??? # * badDestination_errors: DNS resolution and other error(s)
>  ??? # * badDestination_matches: number of matches (this transaction)
>  ??? # * badDestination_evals: number of evaluations (this transaction)
>  ??? acl badDestination dst --on-match=annotate 127.0.0.0/24 10.0.0.1
>
> If the same ACL matches more than once, the last(?) match wins, but the
> aclfoo_matches annotation can be used to detect these cases. The
> aclfoo_evals annotation can be used to detect whether this ACL was
> "reached" at all.
>
> If really needed, we can support turning individual annotations on and
> off, but I doubt that complexity is worth associated performance
> savings. After all, most ACLs will only match once per transaction
> lifetime (for correctly written configurations). Access.log will be
> configured to only log annotations of interest to the admin, of course.
>
>
> The above approach can be extended to provide ACL debugging aid:
>
>  ??? # Dumps every mismatch information to cache.log at level 1
>  ??? acl goodDestination dst --on-mismatch=log 127.0.0.0/24
>
>  ??? # Dumps every evaluation information to cache.log at level 1
>  ??? acl redDestination dst --on-eval=log 127.0.0.0/24
>
>
> 3. Add a Squid feature where Squid (optionally) maintains an internal
> database of recent ACL evaluation history and makes that information
> accessible via cache manager queries like "which ACLs matched
> transaction X?" (where X is logged %master_xaction ID).
>
>
> The three sketched options are not mutually exclusive, of course. All
> require non-trivial code changes.
>
> Would any of the above options address your needs? Any preferences or
> spec adjustments?

Very nice and detailed options!

Let me first ask some questions for clarification:
- Does squid cache all ips from dns responses with multiple ips or only
the one it uses for the request?
- If it caches more than one ip - does squid use more than one of these
ips (e.g. as fallback or round robin) inside a single transaction or for
multiple transactions?

Supposing squid uses only a single dst ip inside a single transaction,
your first option would be sufficient for our purpose!

Resolving the ip only once and storing it inside the transaction would
also avoid ambiguous cases where different ips could theoretically be
used in different acls or worse in acls and the real connection.
It could also improve the performance of acl evaluation because after
the resolution of the dst ip all following dst acls would be evaluated
fast(?).

Before the introduction of annotations we hat to use some acls twice
(for http_access and dedicated logs for this acl) but that's not
necessary anymore. We still have several dst acls that could benefit
from storing the once resolved dst ip.

Your second option sounds really nice for debugging purposes but IMHO
these annotations should be optional because even if the performance
penalty may(?) be neglectable they increase the size of the transaction
object and this could accumulate if many acls are in use.
Hence I would propose this as optional extension to the first option.

Your third option seems not feasible for us, because the delay between a
failed request and reporting of the failure often takes more than a day.
The needed history would be quite excessive (with over 80 million
requests a day).

Thank you,
Martin

>
>
>
> Thank you,
>
> Alex.
>
>
>>>> Is there any way to get the ip logged that was used in the dst-acl
>>>> aside
>>>> from debug logging? Maybe through some annotation mechanism?
>>>>
>>>> Squid version is 6.2, as 6.4 crashes with assertion errors here, too.
>>>>
>>>> thanks,
>>>> Martin
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From dougs at dawnsign.com  Mon Oct 30 18:55:41 2023
From: dougs at dawnsign.com (Doug Sampson)
Date: Mon, 30 Oct 2023 18:55:41 +0000
Subject: [squid-users] Squid 6.4 assertion errors: FATAL: assertion
 failed: stmem.cc:98: "lowestOffset () <= target_offset" current master
 transaction: master655 (backtrace)]
In-Reply-To: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>
References: <62081987-016d-4b20-aaf3-dc48fcf8266c@borrill.org.uk>
Message-ID: <SA1PR17MB469168F8B0761D4124C0B3E4D0A1A@SA1PR17MB4691.namprd17.prod.outlook.com>

> On Mon Oct 23 11:39:52 UTC 2023 Ralf Hildebrandt wrote:
> 
> > I upgraded from 6.3 to 6.4 today and it INSTANTLY began crashing
> > (frequently!)
> >
> > The Log messages in cache.log:
> > ...
> > 2023/10/23 09:57:21| Beginning Validation Procedure
> > 2023/10/23 09:57:21| Completed Validation Procedure
> >     Validated 237163 Entries
> >     store_swap_size = 29491032.00 KB
> > 2023/10/23 09:57:21| Recv recv: (111) Connection refused
> > 2023/10/23 09:57:21| Closing Pinger socket on FD 25
> > 2023/10/23 09:57:22| storeLateRelease: released 0 objects
> > 2023/10/23 09:57:46| FATAL: assertion failed: stmem.cc:98:
> "lowestOffset () <= target_offset"
> >     current master transaction: master655
> 
> I'll add a "me too" to this. 6.3 reliable, 6.4 crashes and this is under
> _very_ low load. NetBSD 9.3_STABLE.
> 
> --
> Stephen

Am seeing multiple crashes upon upgrading from 6.3 to 6.4 last Friday on a FreeBSD 13.2-RELEASE machine. Same error message the one above plus multiple pinger processes. Currently at 322 pinger processes...

~Doug


From squid at borrill.org.uk  Tue Oct 31 13:08:29 2023
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 31 Oct 2023 13:08:29 +0000
Subject: [squid-users] Disable IPV6 for certain destinations only?
Message-ID: <c27710ea-6055-44c1-b10c-6868fb6d862b@borrill.org.uk>

On 18th April 2023 Alex Rousskov wrote:
> On 4/18/23 03:38, Ralf Hildebrandt wrote:
> 
>> We're using squid-6, currently v4 only. The use case for us is mostly
>> our users using our proxy to retrieve full text publications of
>> several thousand medical journals... via IPv4.
>> 
>> The publishers "know" our IPv4 range for the proxies and allow us to
>> download freely. What they don't (yet) know is our ipv6 range.
>> 
>> Thus arises the need to "fall back" to ipv4 in the unlikely case some
>> publisher already has ipv6, we connect via ipv6 and suddenly are not
>> allowed to download the publications.
>> 
>> Is there an acl for that kind of need?
> 
> I will rephrase your question to avoid the distraction of "acl":
> 
>    How can I configure Squid to try IPv4 if IPv6 fails?
> 
> The answer depends on how IPv6 fails:
> 
> 1. If IPv6 fails at DNS resolution time (i.e. the DNS resolver does not 
> respond with a usable address to a AAAA query), then Squid will 
> automatically use IPv4 (i.e. the DNS resolver address in an A response). 
> There is nothing to configure.
> 
> 2. If IPv6 fails at TCP connection establishment time, then Squid will 
> automatically use an IPv4 connection. There is nothing to configure 
> (although there are a few Happy Eyeballs configuration options that you 
> can tune).
> 
> 3. If IPv6 fails at TLS connection establishment time, then, IIRC, #2 
> applies unless SslBump is involved. Squid will not retry failed TLS 
> connections that are subject to SslBump IIRC.
> 
> 4. If IPv6 fails at HTTP request time, then Squid will retry in _some_ 
> cases. See [1] for a long list of conditions; you are probably mostly 
> interested in the last four or five bullets, but keep in mind that the 
> list is of cases where Squid does _not_ re-forward the failed request.
> 
> [1] 
> https://wiki.squid-cache.org/SquidFaq/InnerWorkings#when-does-squid-re-forward-a-client-request
> 
> You can also replace your DNS resolver with a custom one (that drops 
> AAAA answers) or, as Adam has suggested, with hard-coded IPv4-only 
> /etc/hosts entries.

On a machine with no IPv6 connection to the Internet and bearing in mind 2 and 3 above,
what is going on in the following situation? The host is being resolved to IPv6,
the connection is failing with (presumably) no route to host and the client's connection is rejected
with a 503 error:


peer_select.cc(373) checkAlwaysDirectDone: ALLOWED
peer_select.cc(379) checkAlwaysDirectDone: direct = DIRECT_YES (always_direct allow)
peer_select.cc(612) selectMore: CONNECT forcesafesearch.google.com
peer_select.cc(1102) addSelection: adding HIER_DIRECT#forcesafesearch.google.com
peer_select.cc(460) resolveSelected: Find IP destination for: forcesafesearch.google.com:443' via forcesafesearch.google.com
Address.cc(389) lookupHostIP: Given Non-IP 'forcesafesearch.google.com': hostname or servname not provided or not known
peer_select.cc(1174) handlePath: PeerSelector33640 found conn271011 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 for forcesafesearch.google.com:443
peer_select.cc(1180) handlePath:   always_direct = ALLOWED
peer_select.cc(1181) handlePath:    never_direct = DENIED
peer_select.cc(1182) handlePath:        timedout = 0
peer_select.cc(479) resolveSelected: PeerSelector33640 found all 1 destinations for forcesafesearch.google.com:443
peer_select.cc(480) resolveSelected:   always_direct = ALLOWED
peer_select.cc(481) resolveSelected:    never_direct = DENIED
peer_select.cc(482) resolveSelected:        timedout = 0
client_side.cc(1953) clientParseRequests: Not parsing new requests, as this request may need the connection
FwdState.cc(1568) GetMarkingsToServer: from [::] tos 0 netfilter mark 0
ConnOpener.cc(42) ConnOpener: will connect to conn271013 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1 with 60 timeout
comm.cc(372) comm_openex: comm_openex: Attempt open socket for: [::]
comm.cc(414) comm_openex: comm_openex: Opened socket conn271014 local=[::] remote=[::] FD 1218 flags=1 : family=24, type=1, protocol=6
fd.cc(168) fd_open: fd_open() FD 1218 forcesafesearch.google.com
ConnOpener.cc(312) createFd: conn271013 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1 will timeout in 60
comm.cc(844) _comm_close: start closing FD 1218 by ConnOpener.cc:232
comm.cc(580) commUnsetFdTimeout: Remove timeout for FD 1218
HappyConnOpener.cc(522) sendFailure: conn271013 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1 @0
fd.cc(93) fd_close: fd_close FD 1218 forcesafesearch.google.com
tunnel.cc(1364) sendError: aborting transaction for HappyConnOpener gave up
errorpage.cc(751) errorSend: conn271010 local=127.0.0.1:8123 remote=127.0.0.1:56146 FD 1217 flags=1, err=ERR_CONNECT_FAIL

-- 
Stephen



From squid at borrill.org.uk  Tue Oct 31 13:28:03 2023
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 31 Oct 2023 13:28:03 +0000
Subject: [squid-users] Disable IPV6 for certain destinations only?
In-Reply-To: <c27710ea-6055-44c1-b10c-6868fb6d862b@borrill.org.uk>
References: <c27710ea-6055-44c1-b10c-6868fb6d862b@borrill.org.uk>
Message-ID: <ba25f62e-9e9b-4afc-ad0d-d96383462e75@borrill.org.uk>

On 31/10/2023 13:08, Stephen Borrill wrote:
> On 18th April 2023 Alex Rousskov wrote:
>> On 4/18/23 03:38, Ralf Hildebrandt wrote:
>>
>>> We're using squid-6, currently v4 only. The use case for us is mostly
>>> our users using our proxy to retrieve full text publications of
>>> several thousand medical journals... via IPv4.
>>>
>>> The publishers "know" our IPv4 range for the proxies and allow us to
>>> download freely. What they don't (yet) know is our ipv6 range.
>>>
>>> Thus arises the need to "fall back" to ipv4 in the unlikely case some
>>> publisher already has ipv6, we connect via ipv6 and suddenly are not
>>> allowed to download the publications.
>>>
>>> Is there an acl for that kind of need?
>>
>> I will rephrase your question to avoid the distraction of "acl":
>>
>> ?? How can I configure Squid to try IPv4 if IPv6 fails?
>>
>> The answer depends on how IPv6 fails:
>>
>> 1. If IPv6 fails at DNS resolution time (i.e. the DNS resolver does not respond with a usable address to a AAAA query), then Squid will automatically use IPv4 (i.e. the DNS resolver address in an A response). There is nothing to configure.
>>
>> 2. If IPv6 fails at TCP connection establishment time, then Squid will automatically use an IPv4 connection. There is nothing to configure (although there are a few Happy Eyeballs configuration options that you can tune).
>>
>> 3. If IPv6 fails at TLS connection establishment time, then, IIRC, #2 applies unless SslBump is involved. Squid will not retry failed TLS connections that are subject to SslBump IIRC.
>>
>> 4. If IPv6 fails at HTTP request time, then Squid will retry in _some_ cases. See [1] for a long list of conditions; you are probably mostly interested in the last four or five bullets, but keep in mind that the list is of cases where Squid does _not_ re-forward the failed request.
>>
>> [1] https://wiki.squid-cache.org/SquidFaq/InnerWorkings#when-does-squid-re-forward-a-client-request
>>
>> You can also replace your DNS resolver with a custom one (that drops AAAA answers) or, as Adam has suggested, with hard-coded IPv4-only /etc/hosts entries.
> 
> On a machine with no IPv6 connection to the Internet and bearing in mind 2 and 3 above,
> what is going on in the following situation? The host is being resolved to IPv6,
> the connection is failing with (presumably) no route to host and the client's connection is rejected
> with a 503 error:
> 
> 
> peer_select.cc(373) checkAlwaysDirectDone: ALLOWED
> peer_select.cc(379) checkAlwaysDirectDone: direct = DIRECT_YES (always_direct allow)
> peer_select.cc(612) selectMore: CONNECT forcesafesearch.google.com
> peer_select.cc(1102) addSelection: adding HIER_DIRECT#forcesafesearch.google.com
> peer_select.cc(460) resolveSelected: Find IP destination for: forcesafesearch.google.com:443' via forcesafesearch.google.com
> Address.cc(389) lookupHostIP: Given Non-IP 'forcesafesearch.google.com': hostname or servname not provided or not known
> peer_select.cc(1174) handlePath: PeerSelector33640 found conn271011 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 for forcesafesearch.google.com:443
> peer_select.cc(1180) handlePath:?? always_direct = ALLOWED
> peer_select.cc(1181) handlePath:??? never_direct = DENIED
> peer_select.cc(1182) handlePath:??????? timedout = 0
> peer_select.cc(479) resolveSelected: PeerSelector33640 found all 1 destinations for forcesafesearch.google.com:443
> peer_select.cc(480) resolveSelected:?? always_direct = ALLOWED
> peer_select.cc(481) resolveSelected:??? never_direct = DENIED
> peer_select.cc(482) resolveSelected:??????? timedout = 0
> client_side.cc(1953) clientParseRequests: Not parsing new requests, as this request may need the connection
> FwdState.cc(1568) GetMarkingsToServer: from [::] tos 0 netfilter mark 0
> ConnOpener.cc(42) ConnOpener: will connect to conn271013 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1 with 60 timeout
> comm.cc(372) comm_openex: comm_openex: Attempt open socket for: [::]
> comm.cc(414) comm_openex: comm_openex: Opened socket conn271014 local=[::] remote=[::] FD 1218 flags=1 : family=24, type=1, protocol=6
> fd.cc(168) fd_open: fd_open() FD 1218 forcesafesearch.google.com
> ConnOpener.cc(312) createFd: conn271013 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1 will timeout in 60
> comm.cc(844) _comm_close: start closing FD 1218 by ConnOpener.cc:232
> comm.cc(580) commUnsetFdTimeout: Remove timeout for FD 1218
> HappyConnOpener.cc(522) sendFailure: conn271013 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1 @0
> fd.cc(93) fd_close: fd_close FD 1218 forcesafesearch.google.com
> tunnel.cc(1364) sendError: aborting transaction for HappyConnOpener gave up
> errorpage.cc(751) errorSend: conn271010 local=127.0.0.1:8123 remote=127.0.0.1:56146 FD 1217 flags=1, err=ERR_CONNECT_FAIL

After restarting (not reloading), I get the following and it will work for while before stopping again:

peer_select.cc(1174) handlePath: PeerSelector1666 found conn12948 local=0.0.0.0 remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #1 for forcesafesearch.google.com:443
peer_select.cc(1180) handlePath:   always_direct = ALLOWED
peer_select.cc(1181) handlePath:    never_direct = DENIED
peer_select.cc(1182) handlePath:        timedout = 0
peer_select.cc(1174) handlePath: PeerSelector1666 found conn12949 local=[::] remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #2 for forcesafesearch.google.com:443
peer_select.cc(1180) handlePath:   always_direct = ALLOWED
peer_select.cc(1181) handlePath:    never_direct = DENIED
peer_select.cc(1182) handlePath:        timedout = 0
peer_select.cc(479) resolveSelected: PeerSelector1666 found all 2 destinations for forcesafesearch.google.com:443

This is a newly occurring problem after upgrade from 4.x to 6.3.

-- 
Stephen



From squid3 at treenet.co.nz  Sat Oct 21 21:57:35 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Oct 2023 21:57:35 -0000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2023:3 Denial of
 Service in HTTP Digest Authentication
Message-ID: <cb0fd982-a098-4262-8646-41f62ba57ff1@treenet.co.nz>

__________________________________________________________________

   Squid Proxy Cache Security Update Advisory SQUID-2023:3
__________________________________________________________________

Advisory ID:       | SQUID-2023:3
Date:              | October 22, 2023
Summary:           | Denial of Service in HTTP Digest Authentication
Affected versions: | Squid 3.2 -> 3.5.28
                    | Squid 4.x -> 4.16
                    | Squid 5.x -> 5.9
                    | Squid 6.x -> 6.3
Fixed in version:  | Squid 6.4
__________________________________________________________________

Problem Description:

  Due to a buffer overflow bug Squid is vulnerable to a Denial of
  Service attack against HTTP Digest Authentication

__________________________________________________________________

Severity:

  This problem allows a remote client to perform buffer overflow
  attack writing up to 2 MB of arbitrary data to heap memory
  when Squid is configured to accept HTTP Digest Authentication.

  On machines with advanced memory protections this will result
  in a Denial of Service against all users of the Squid proxy.

CVSS Score of 9.9
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:L/I:L/A:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid version 6.4.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 5:
  <http://www.squid-cache.org/Versions/v5/SQUID-2023_3.patch>

Squid 6:
  <http://www.squid-cache.org/Versions/v6/SQUID-2023_3.patch>

  If you are using a prepackaged version of Squid then please refer
  to the package vendor for availability information on updated
  packages.

__________________________________________________________________

Determining if your version is vulnerable:

  Squid older than 5.0.5 have not been tested and should be assumed
  to be vulnerable.

  All Squid-5.x up to and including 5.9 are vulnerable.

  All Squid-6.x up to and including 6.3 are vulnerable.

__________________________________________________________________

Workaround:

   Disable HTTP Digest authentication until Squid can be
   upgraded or patched.

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If you install and build Squid from the original Squid sources
  then the <squid-users at lists.squid-cache.org> mailing list is your
  primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <https://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  This vulnerability was discovered by Joshua Rogers of Opera
  Software.

  Fixed by Alex Bason.

__________________________________________________________________

Revision history:

  2021-03-22 00:59:20 UTC Initial Report
  2023-10-13 17:31:11 UTC Patch Published
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Sat Oct 21 21:57:38 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Oct 2023 21:57:38 -0000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2023:2 Multiple
 issues in HTTP response caching
Message-ID: <0e092d9f-d1b0-4c9e-aad3-ee684e71a8c1@treenet.co.nz>

__________________________________________________________________

   Squid Proxy Cache Security Update Advisory SQUID-2023:2
__________________________________________________________________

Advisory ID:       | SQUID-2023:2
Date:              | October 22, 2023
Summary:           | Multiple issues in HTTP response caching.
Affected versions: | Squid 2.x -> 2.7.STABLE9
                    | Squid 3.x -> 3.5.28
                    | Squid 4.x -> 4.16
                    | Squid 5.x -> 5.9
                    | Squid 6.x -> 6.3
Fixed in version:  | Squid 6.4
__________________________________________________________________

Problem Description:

  Due to an Improper Handling of Structural Elements
  bug Squid is vulnerable to a Denial of Service
  attack against HTTP and HTTPS clients.

  Due to an Incomplete Filtering of Special Elements
  bug Squid is vulnerable to a Denial of Service
  attack against HTTP and HTTPS clients.

__________________________________________________________________

Severity:

  The limits applied for validation of HTTP Response headers are
  applied before caching. Different limits may be in place at the
  later cache HIT usage of that response.

  The limits applied for validation of HTTP Response headers are
  applied to each received server response. Squid may grow a cached
  HTTP Response header with HTTP 304 updates beyond the configured
  maximum header size.

  Subsequent parsing to de-serialize a large header from disk cache
  can stall or crash the worker process. Resulting in Denial of
  Service to all clients using the proxy.

CVSS Score of 9.6
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:H/A:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid version 6.4.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 6:
  <http://www.squid-cache.org/Versions/v6/SQUID-2023_2.patch>

  If you are using a prepackaged version of Squid then please refer
  to the package vendor for availability information on updated
  packages.

__________________________________________________________________

Determining if your version is vulnerable:

  Squid older than v5 have not been tested and are presumed
  vulnerable.

  Squid v5.x up to and including 5.9 are vulnerable.

  Squid v6.x up to and including 6.3 are vulnerable.

__________________________________________________________________

Workaround:

  Disable disk caching by removing all cache_dir directives from
  squid.conf.

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If you install and build Squid from the original Squid sources
  then the <squid-users at lists.squid-cache.org> mailing list is your
  primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <https://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  This vulnerability was independently discovered by Joshua Rogers
  of Opera Software and by The Measurement Factory.

  Fixed by The Measurement Factory.

__________________________________________________________________

Revision history:

2019-09-11: Initial report of header growth caused by HTTP 304.
2021-03-04: Initial report of caching of huge response headers.
2023-04-28 02:40:03 UTC Initial patches released.

_________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Sat Oct 21 21:57:40 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Oct 2023 21:57:40 -0000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2023:1
 Request/Response smuggling in HTTP(S) and ICAP
Message-ID: <88ff5ad4-f523-4f55-8da8-a2f25395d3f3@treenet.co.nz>

__________________________________________________________________

Squid Proxy Cache Security Update Advisory SQUID-2023:1
__________________________________________________________________


Advisory ID:       | SQUID-2023:1
Date:              | October 22, 2023
Summary:           | Request/Response smuggling in HTTP(S) and ICAP
Affected versions: | Squid 2.6.STABLE10  -> 2.7.STABLE9
                    | Squid 3.x -> 3.5.28
                    | Squid 4.x -> 4.16
                    | Squid 5.x -> 5.9
                    | Squid 6.x -> 6.3
Fixed in version:  | Squid 6.4
__________________________________________________________________

Problem Description:

  Due to chunked decoder lenience Squid is vulnerable to
  Request/Response smuggling attacks when parsing HTTP/1.1
  and ICAP messages.

__________________________________________________________________

Severity:

  This problem allows a remote attacker to perform
  Request/Response smuggling past firewall and frontend security
  systems when the upstream server interprets the chunked encoding
  syntax differently from Squid.

  This attack is limited to the HTTP/1.1 and ICAP protocols which
  support receiving Transfer-Encoding:chunked.

CVSS Score of 9.3
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:L/A:N&version=3.1>

__________________________________________________________________

Updated Packages:

#This bug is fixed by Squid version 6.4.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 5:
  <http://www.squid-cache.org/Versions/v5/SQUID-2023_1.patch>

Squid 6:
  <http://www.squid-cache.org/Versions/v6/SQUID-2023_1.patch>

  If you are using a prepackaged version of Squid then please refer
  to the package vendor for availability information on updated
  packages.

__________________________________________________________________

Determining if your version is vulnerable:

  Squid older than 5.1 have not been tested and should be
  assumed to be vulnerable.

  All Squid-5.x up to and including 5.9 are vulnerable.

  All Squid-6.x up to and including 6.3 are vulnerable.

__________________________________________________________________

Workaround:

  * ICAP issues can be reduced by ensuring only trusted ICAP
    services are used, with TLS encrypted connections
    (ICAPS extension).

  *  There is no workaround for the HTTP Request Smuggling issue.

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If you install and build Squid from the original Squid sources
  then the <squid-users at lists.squid-cache.org> mailing list is your
  primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <https://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  This vulnerability was discovered by Keran Mu and Jianjun Chen,
  from Tsinghua University and Zhongguancun Laboratory.

  Fixed by Amos Jeffries of Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

  2023-09-01 04:34:00 UTC Initial Report
  2023-10-01 08:43:00 UTC Patch Available
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-announce


From squid3 at treenet.co.nz  Sat Oct 21 21:57:43 2023
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Oct 2023 21:57:43 -0000
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2023:5 Denial of
 Service in FTP
Message-ID: <304e9d98-394d-47b6-8955-63aab32c2fe3@treenet.co.nz>

__________________________________________________________________

   Squid Proxy Cache Security Update Advisory SQUID-2023:5
__________________________________________________________________

Advisory ID:       | SQUID-2023:5
Date:              | October 22, 2023
Summary:           | Denial of Service in FTP
Affected versions: | Squid 5.0.3 -> 5.9
                    | Squid 6.x -> 6.3
Fixed in version:  | Squid 6.4
__________________________________________________________________

Problem Description:

  Due to an Incorrect Conversion between Numeric Types
  bug Squid is vulnerable to a Denial of Service
  attack against FTP Native Relay input validation.

  Due to an Incorrect Conversion between Numeric Types
  bug Squid is vulnerable to a Denial of Service
  attack against ftp:// URL validation and access control.

__________________________________________________________________

Severity:

  This problem allows a remote client to perform Denial of Service
  when sending ftp:// URLs in HTTP Request messages or constructing
  ftp:// URLs from FTP Native input.

  This issue is triggered during access control security checks,
  meaning clients may not have been permitted to use the proxy yet.

  FTP support is always enabled and cannot be disabled completely.

CVSS Score of 8.6
<https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:H&version=3.1>

__________________________________________________________________

Updated Packages:

This bug is fixed by Squid version 6.4.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 6:
  <http://www.squid-cache.org/Versions/v6/SQUID-2023_5.patch>

  If you are using a prepackaged version of Squid then please refer
  to the package vendor for availability information on updated
  packages.

__________________________________________________________________

Determining if your version is vulnerable:

  Squid older than 5.0.3 are not vulnerable.

  All Squid-5.0.4 up to and including 5.6 are vulnerable.

  All Squid-6.x up to and including 6.3 are vulnerable.

__________________________________________________________________

Workaround:

  * The FTP Native Relay input validation vector can be secured by
    removing all ftp_port directives from squid.conf.

  * There are no workarounds to avoid the ftp:// URL validation and
    access control vector.

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If you install and build Squid from the original Squid sources
  then the <squid-users at lists.squid-cache.org> mailing list is your
  primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <https://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  <squid-bugs at lists.squid-cache.org> mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  This vulnerability was discovered by Joshua Rogers of Opera
  Software.

  Fixed by The Measurement Factory.

__________________________________________________________________

Revision history:

  2023-10-12 11:53:02 UTC Initial Report
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-announce


