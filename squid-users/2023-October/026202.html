<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] very poor performance of rock cache ipc
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20very%20poor%20performance%20of%20rock%20cache%20ipc&In-Reply-To=%3C8279acb8-45c0-4a26-a080-16ea52176283%40googlemail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="026201.html">
   <LINK REL="Next"  HREF="026203.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] very poor performance of rock cache ipc</H1>
    <B>Julian Taylor</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20very%20poor%20performance%20of%20rock%20cache%20ipc&In-Reply-To=%3C8279acb8-45c0-4a26-a080-16ea52176283%40googlemail.com%3E"
       TITLE="[squid-users] very poor performance of rock cache ipc">jtaylor.debian at googlemail.com
       </A><BR>
    <I>Sat Oct 14 16:04:00 UTC 2023</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="026201.html">[squid-users] very poor performance of rock cache ipc
</A></li>
        <LI>Next message (by thread): <A HREF="026203.html">[squid-users] very poor performance of rock cache ipc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26202">[ date ]</a>
              <a href="thread.html#26202">[ thread ]</a>
              <a href="subject.html#26202">[ subject ]</a>
              <a href="author.html#26202">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 14.10.23 17:40, Alex Rousskov wrote:
&gt;<i> On 2023-10-13 16:01, Julian Taylor wrote:
</I>&gt;<i> 
</I>&gt;&gt;<i> When using squid for caching using the rock cache_dir setting the 
</I>&gt;&gt;<i> performance is pretty poor with multiple workers.
</I>&gt;&gt;<i> The reason for this is due to the very high number of systemcalls 
</I>&gt;&gt;<i> involved in the IPC between the disker and workers.
</I>&gt;<i> 
</I>&gt;<i> Please allow me to rephrase your conclusion to better match (expected) 
</I>&gt;<i> reality and avoid misunderstanding:
</I>&gt;<i> 
</I>&gt;<i> By design, a mostly idle SMP Squid should use a lot more system calls 
</I>&gt;<i> per disk cache hit than a busy SMP Squid would:
</I>&gt;<i> 
</I>&gt;<i> * Mostly idle Squid: Every disk I/O may require a few IPC messages.
</I>&gt;<i> * Busy Squid: Bugs notwithstanding, disk I/Os require no IPC messages.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> In your single-request test, you are observing the expected effects 
</I>&gt;<i> described in the first bullet. That does not imply those effects are 
</I>&gt;<i> &quot;good&quot; or &quot;desirable&quot; in your use case, of course. It only means that 
</I>&gt;<i> SMP Squid was no optimized for that use case; SMP rock design was 
</I>&gt;<i> explicitly targeting the opposite use case (i.e. a busy Squid).
</I>
The reproducer uses as single request, the same very thing can be 
observed on a very busy squid and workaround improves both the single 
request case and the actual heavy loaded production squid in the same way.

The hardware involved has a 10G card, not ssds but lots of ram so it has 
a very high page cache hit rate and the squid is very busy, so much it 
is overloaded by system cpu usage in default configuration with the rock 
cache. The network or disk bandwidth is barely ever utilized more than 
10% with all 8 cpus busy on system load.
The only way to get the squid to utilize the machine is to increase the 
IO size via the request buffer change or not use the rock cache. UFS 
cache works ok in comparison, but requires multiple independent squid 
instances as it does not support SMP.

Increasing the IO size to 32KiB as I mentioned does allow the squid 
workers to utilize a good 60% of the hardware network and disk capabilities.

&gt;<i> 
</I>&gt;<i> Roughly speaking, here, &quot;busy&quot; means &quot;there are always some messages in 
</I>&gt;<i> the disk I/O queue [maintained by Squid in shared memory]&quot;.
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> You may wonder how it is possible that an increase in I/O work results 
</I>&gt;<i> in decrease (and, hopefully, elimination) of related IPC messages. 
</I>&gt;<i> Roughly speaking, a worker must send an IPC &quot;you have a new I/O request&quot; 
</I>&gt;<i> message only when its worker-&gt;disker queue is empty. If the queue is not 
</I>&gt;<i> empty, then there is no reason to send an IPC message to wake up disker 
</I>&gt;<i> because disker will see the new message when dequeuing the previous one. 
</I>&gt;<i> Same for the opposite direction: disker-&gt;worker...
</I>
This is probably true if you have slow disks and are actually IO bound, 
but on fast disks or high page cache hit rate you essential see this ipc 
ping pong and very little actual work being done.

&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i>  &gt; Is it necessary to have these read chunks so small
</I>&gt;<i> 
</I>&gt;<i> It is not. Disk I/O size should be at least the system I/O page size, 
</I>&gt;<i> but it can be larger. The optimal I/O size is probably very dependent on 
</I>&gt;<i> traffic patterns. IIRC, Squid I/O size is at most one Squid page 
</I>&gt;<i> (SM_PAGE_SIZE or 4KB).
</I>&gt;<i> 
</I>&gt;<i> FWIW, I suspect there are significant inefficiencies in disk I/O related 
</I>&gt;<i> request alignment: The code does not attempt to read from and write to 
</I>&gt;<i> disk page boundaries, probably resulting in multiple low-level disk I/Os 
</I>&gt;<i> per one Squid 4KB I/O in some (many?) cases. With modern non-rotational 
</I>&gt;<i> storage these effects are probably less pronounced, but they probably 
</I>&gt;<i> still exist.
</I>The kernel drivers will mostly handle this for you if multiple requests 
are available, but this is also almost irrelevant with current hardware, 
typically it will be so fast software overhead will make it hard to 
utilize modern large disk arrays properly you probably need to look at 
other approaches like io_ring to get rid of the classical read/write 
systemcall overhead dominating your performance.

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="026201.html">[squid-users] very poor performance of rock cache ipc
</A></li>
	<LI>Next message (by thread): <A HREF="026203.html">[squid-users] very poor performance of rock cache ipc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26202">[ date ]</a>
              <a href="thread.html#26202">[ thread ]</a>
              <a href="subject.html#26202">[ subject ]</a>
              <a href="author.html#26202">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
