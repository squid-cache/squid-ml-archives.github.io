<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] very poor performance of rock cache ipc
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20very%20poor%20performance%20of%20rock%20cache%20ipc&In-Reply-To=%3Cbaae1a3e-330c-4022-8fdd-c55dafcafae1%40measurement-factory.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="026202.html">
   <LINK REL="Next"  HREF="026207.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] very poor performance of rock cache ipc</H1>
    <B>Alex Rousskov</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20very%20poor%20performance%20of%20rock%20cache%20ipc&In-Reply-To=%3Cbaae1a3e-330c-4022-8fdd-c55dafcafae1%40measurement-factory.com%3E"
       TITLE="[squid-users] very poor performance of rock cache ipc">rousskov at measurement-factory.com
       </A><BR>
    <I>Sun Oct 15 03:42:39 UTC 2023</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="026202.html">[squid-users] very poor performance of rock cache ipc
</A></li>
        <LI>Next message (by thread): <A HREF="026207.html">[squid-users] very poor performance of rock cache ipc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26203">[ date ]</a>
              <a href="thread.html#26203">[ thread ]</a>
              <a href="subject.html#26203">[ subject ]</a>
              <a href="author.html#26203">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 2023-10-14 12:04, Julian Taylor wrote:
&gt;<i> On 14.10.23 17:40, Alex Rousskov wrote:
</I>&gt;&gt;<i> On 2023-10-13 16:01, Julian Taylor wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> When using squid for caching using the rock cache_dir setting the 
</I>&gt;&gt;&gt;<i> performance is pretty poor with multiple workers.
</I>&gt;&gt;&gt;<i> The reason for this is due to the very high number of systemcalls 
</I>&gt;&gt;&gt;<i> involved in the IPC between the disker and workers.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Please allow me to rephrase your conclusion to better match (expected) 
</I>&gt;&gt;<i> reality and avoid misunderstanding:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> By design, a mostly idle SMP Squid should use a lot more system calls 
</I>&gt;&gt;<i> per disk cache hit than a busy SMP Squid would:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> * Mostly idle Squid: Every disk I/O may require a few IPC messages.
</I>&gt;&gt;<i> * Busy Squid: Bugs notwithstanding, disk I/Os require no IPC messages.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> In your single-request test, you are observing the expected effects 
</I>&gt;&gt;<i> described in the first bullet. That does not imply those effects are 
</I>&gt;&gt;<i> &quot;good&quot; or &quot;desirable&quot; in your use case, of course. It only means that 
</I>&gt;&gt;<i> SMP Squid was no optimized for that use case; SMP rock design was 
</I>&gt;&gt;<i> explicitly targeting the opposite use case (i.e. a busy Squid).
</I>&gt;<i> 
</I>&gt;<i> The reproducer uses as single request, the same very thing can be 
</I>&gt;<i> observed on a very busy squid
</I>
If a busy Squid sends lots of IPC messages between worker and disker, 
then either there is a Squid bug we do not know about OR that disker is 
just not as busy as one might expect it to be.

In Squid v6+, you can observe disker queues using mgr:store_queues cache 
manager report. In your environment, do those queues always have lots of 
requests when Squid is busy? Feel free to share (a pointer to) a 
representative sample of those reports from your busy Squid.

N.B. Besides worker-disker IPC messages, there are also worker-worker 
cache synchronization IPC messages. They also have the same &quot;do not send 
IPC messages if the queue has some pending items already&quot; optimization.


&gt;<i> and workaround improves both the single 
</I>&gt;<i> request case and the actual heavy loaded production squid in the same way.
</I>
FWIW, I do not think that observation contradicts anything I have said.


&gt;<i> The hardware involved has a 10G card, not ssds but lots of ram so it has 
</I>&gt;<i> a very high page cache hit rate and the squid is very busy, so much it 
</I>&gt;<i> is overloaded by system cpu usage in default configuration with the rock 
</I>&gt;<i> cache. The network or disk bandwidth is barely ever utilized more than 
</I>&gt;<i> 10% with all 8 cpus busy on system load.
</I>
The above facts suggest that the disk is just not used much OR there is 
a bug somewhere. Slower (for any reason, including CPU overload) IPC 
messages should lead to longer queues and the disappearance of &quot;your 
queue is no longer empty!&quot; IPC messages.


&gt;<i> The only way to get the squid to utilize the machine is to increase the 
</I>&gt;<i> IO size via the request buffer change or not use the rock cache. UFS 
</I>&gt;<i> cache works ok in comparison, but requires multiple independent squid 
</I>&gt;<i> instances as it does not support SMP.
</I>&gt;<i> 
</I>&gt;<i> Increasing the IO size to 32KiB as I mentioned does allow the squid 
</I>&gt;<i> workers to utilize a good 60% of the hardware network and disk 
</I>&gt;<i> capabilities.
</I>
Please note that I am not disputing this observation. Unfortunately, it 
does not help me guess where the actual/core problem or bottleneck is. 
Hopefully, cache manager mgr:store_queues report will shed some light.


&gt;&gt;<i> Roughly speaking, here, &quot;busy&quot; means &quot;there are always some messages 
</I>&gt;&gt;<i> in the disk I/O queue [maintained by Squid in shared memory]&quot;.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> You may wonder how it is possible that an increase in I/O work results 
</I>&gt;&gt;<i> in decrease (and, hopefully, elimination) of related IPC messages. 
</I>&gt;&gt;<i> Roughly speaking, a worker must send an IPC &quot;you have a new I/O 
</I>&gt;&gt;<i> request&quot; message only when its worker-&gt;disker queue is empty. If the 
</I>&gt;&gt;<i> queue is not empty, then there is no reason to send an IPC message to 
</I>&gt;&gt;<i> wake up disker because disker will see the new message when dequeuing 
</I>&gt;&gt;<i> the previous one. Same for the opposite direction: disker-&gt;worker...
</I>
&gt;<i> This is probably true if you have slow disks and are actually IO bound, 
</I>&gt;<i> but on fast disks or high page cache hit rate you essential see this ipc 
</I>&gt;<i> ping pong and very little actual work being done.
</I>
AFAICT, &quot;too slow&quot; IPC messages should result in non-empty queues and, 
hence, no IPC messages at all. For this logic to work, it does not 
matter whether the system is I/O bound or not, whether disks are &quot;slow&quot; 
or not.


&gt;&gt;<i> &#160;&gt; Is it necessary to have these read chunks so small
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> It is not. Disk I/O size should be at least the system I/O page size, 
</I>&gt;&gt;<i> but it can be larger. The optimal I/O size is probably very dependent 
</I>&gt;&gt;<i> on traffic patterns. IIRC, Squid I/O size is at most one Squid page 
</I>&gt;&gt;<i> (SM_PAGE_SIZE or 4KB).
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> FWIW, I suspect there are significant inefficiencies in disk I/O 
</I>&gt;&gt;<i> related request alignment: The code does not attempt to read from and 
</I>&gt;&gt;<i> write to disk page boundaries, probably resulting in multiple 
</I>&gt;&gt;<i> low-level disk I/Os per one Squid 4KB I/O in some (many?) cases. With 
</I>&gt;&gt;<i> modern non-rotational storage these effects are probably less 
</I>&gt;&gt;<i> pronounced, but they probably still exist.
</I>
&gt;<i> The kernel drivers will mostly handle this for you if multiple requests 
</I>&gt;<i> are available, but this is also almost irrelevant with current hardware, 
</I>&gt;<i> typically it will be so fast software overhead will make it hard to 
</I>&gt;<i> utilize modern large disk arrays properly
</I>
I doubt doing twice as many low-level disk I/Os (due to wrong alignment) 
is likely to be irrelevant, but we do not need to agree on that to make 
progress: Clearly, excessive low-level disk I/Os is not the bottleneck 
in your current environment.


&gt;<i> you probably need to look at 
</I>&gt;<i> other approaches like io_ring to get rid of the classical read/write 
</I>&gt;<i> systemcall overhead dominating your performance.
</I>
Yes, but those things are complementary (i.e. not mutually exclusive).


Cheers,

Alex.


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="026202.html">[squid-users] very poor performance of rock cache ipc
</A></li>
	<LI>Next message (by thread): <A HREF="026207.html">[squid-users] very poor performance of rock cache ipc
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26203">[ date ]</a>
              <a href="thread.html#26203">[ thread ]</a>
              <a href="subject.html#26203">[ subject ]</a>
              <a href="author.html#26203">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
