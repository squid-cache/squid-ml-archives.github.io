<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] very poor performance of rock cache ipc
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20very%20poor%20performance%20of%20rock%20cache%20ipc&In-Reply-To=%3C4f23cc52-02db-4df1-b13f-5734209e7560%40measurement-factory.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="026207.html">
   <LINK REL="Next"  HREF="026204.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] very poor performance of rock cache ipc</H1>
    <B>Alex Rousskov</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20very%20poor%20performance%20of%20rock%20cache%20ipc&In-Reply-To=%3C4f23cc52-02db-4df1-b13f-5734209e7560%40measurement-factory.com%3E"
       TITLE="[squid-users] very poor performance of rock cache ipc">rousskov at measurement-factory.com
       </A><BR>
    <I>Tue Oct 17 03:26:50 UTC 2023</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="026207.html">[squid-users] very poor performance of rock cache ipc
</A></li>
        <LI>Next message (by thread): <A HREF="026204.html">[squid-users] Fwd: Squid does not pass HTTPS traffic transparently
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26208">[ date ]</a>
              <a href="thread.html#26208">[ thread ]</a>
              <a href="subject.html#26208">[ subject ]</a>
              <a href="author.html#26208">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 2023-10-16 16:24, Julian Taylor wrote:
&gt;<i> On 15.10.23 05:42, Alex Rousskov wrote:
</I>&gt;&gt;<i> On 2023-10-14 12:04, Julian Taylor wrote:
</I>&gt;&gt;&gt;<i> On 14.10.23 17:40, Alex Rousskov wrote:
</I>&gt;&gt;&gt;&gt;<i> On 2023-10-13 16:01, Julian Taylor wrote:
</I>&gt;&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> The reproducer uses as single request, the same very thing can be 
</I>&gt;&gt;&gt;<i> observed on a very busy squid
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> If a busy Squid sends lots of IPC messages between worker and disker, 
</I>&gt;&gt;<i> then either there is a Squid bug we do not know about OR that disker 
</I>&gt;&gt;<i> is just not as busy as one might expect it to be.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> In Squid v6+, you can observe disker queues using mgr:store_queues 
</I>&gt;&gt;<i> cache manager report. In your environment, do those queues always have 
</I>&gt;&gt;<i> lots of requests when Squid is busy? Feel free to share (a pointer to) 
</I>&gt;&gt;<i> a representative sample of those reports from your busy Squid.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> N.B. Besides worker-disker IPC messages, there are also worker-worker 
</I>&gt;&gt;<i> cache synchronization IPC messages. They also have the same &quot;do not 
</I>&gt;&gt;<i> send IPC messages if the queue has some pending items already&quot; 
</I>&gt;&gt;<i> optimization.
</I>
&gt;<i> I checked the queues running with the configuration from my initial mail 
</I>&gt;<i> with workers increase and the queues are generally low, around 1-10 
</I>&gt;<i> items in the queue when sending around 100 parallel requests reading 
</I>&gt;<i> about 100mb data files. Here is a sample: <A HREF="https://dpaste.com/8SLNRW5F8">https://dpaste.com/8SLNRW5F8</A>
</I>&gt;<i> Also with the higher request rate than the single curl the majority of 
</I>&gt;<i> work throughput was more than doubled by increasing the blocksize.
</I>&gt;<i> 
</I>&gt;<i> How are the queues supposed to look like on a busy squid that is not 
</I>&gt;<i> spending a large portion of its time doing notify IPC?
</I>
The queues are supposed to look &quot;not empty&quot; -- a non-empty queue does 
not result in IPC notifications. Needless to say, the further away from 
&quot;empty&quot; the queues are, the lesser the chance they will become empty 
when cache manager report is _not_ &quot;looking&quot; at them.


&gt;<i> Increasing the parallel requests does decrease the amount of overhead 
</I>&gt;<i> but its still pretty large, I measured about 10%-30% cpu overhead with 
</I>&gt;<i> 100 parallel requests served from cache in the worker and disker
</I>&gt;<i> Here a snipped of a profile:
</I>&gt;<i> --22.34%--JobDialer&lt;AsyncJob&gt;::dial(AsyncCall&amp;)
</I>&gt;<i>  &#160;&#160; |
</I>&gt;<i>  &#160;&#160; |--21.19%--Ipc::UdsSender::start()
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160; |
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160; --21.13%--Ipc::UdsSender::write()
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |--16.12%--Ipc::UdsOp::conn()
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; --15.84%--comm_open_uds(int, int, 
</I>&gt;<i> sockaddr_un*, int)
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |--1.70%--commSetCloseOnExec(int)
</I>&gt;<i>  &#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; |&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; --1.56%--commSetNonBlocking(int)
</I>&gt;<i>  &#160; ...
</I>&gt;<i> --12.98%--comm_close_complete(int)
</I>&gt;<i> 
</I>&gt;<i> Clearing and constructing the large Ipc::TypedMsgHdr is also very 
</I>&gt;<i> noticeable.
</I>&gt;<i> 
</I>&gt;<i> That the overhead and maximum throughput is so low for not so busy 
</I>&gt;<i> squids (say 1-10 requests per second but requests on average &gt; 1MiB) is 
</I>&gt;<i> imo also a reason for concern and could be improved.
</I>
I agree.


&gt;<i> If I understand the way it works correctly e.g. the worker when it gets 
</I>&gt;<i> a request splits it into 4k blocks and enqueues read requests into the 
</I>&gt;<i> ipc queue and if the queue is empty it emits a notify ipc so the disker 
</I>&gt;<i> starts popping from the queue.
</I>
Yes, at some level of abstraction, the above summary is not wrong. 
However, please keep in mind that, for a single HTTP transaction, most 
of the disk read requests are queued by worker, read by disk, and 
received by worker one read request at a time. There is no disk read 
&quot;prefetching&quot; (yet?).


&gt;<i> On large requests that are answered immediately from the disker the 
</I>&gt;<i> problem seems to be that the queue is mostly empty and it sends an ipc 
</I>&gt;<i> ping pong for each 4k block.
</I>
Due to lack of prefetching, the total size of the HTTP response does not 
really affect the queue length. Only the transaction concurrency level 
does; on average, that is determined by mean response time multiplied by 
the I/O request rate from a particular worker to a particular disker.


&gt;<i> So my though was when the request is larger than 4k enqueue multiple 
</I>&gt;<i> pending reads in the worker and only notify after a certain amount has 
</I>&gt;<i> been added to the queue, vice versa in the disker.
</I>
&gt;<i> So I messed around a bit trying to reduce the notifications by delaying 
</I>&gt;<i> the Notify call in src/DiskIO/IpcIo/IpcIoFile.cc for larger requests but 
</I>&gt;<i> it ended up blocking after the first queue push with no notify. If I 
</I>&gt;<i> understand the queue correctly this is due to the reader requires a 
</I>&gt;<i> notify to initially start and and simply pushing multiple read requests 
</I>&gt;<i> onto the queue without notifying will not work
</I>
You are correct.


&gt;<i> Is this approach feasible or am I misunderstanding how it works?
</I>
Prefetching is feasible in principle, but is not easy to implement well 
and will probably require configuration options (because it will slow 
down busy Squids that do not have the time to prefetch but may not know 
that).

I would consider increasing I/O size (and shared memory page size) 
instead, at least as the first step. Doing so well is not trivial 
either, but may be easier and beneficial to more use cases.


&gt;<i> I also tried to add reusing of the IPC connection between calls so the 
</I>&gt;<i> major source of overhead,tearing down and reestablishing the connection, 
</I>&gt;<i> is removed but that also turned out difficult as the connections are 
</I>&gt;<i> closed in various places and the general complexity of the code.
</I>
Yes, that would be nice. Reusing sockets is especially difficult to get 
right with startup/bootstrapping, reconfigurations, and kid 
death/restarts problems in mind. On the other hand, it is probably much 
easier to optimize this than to implement disk hit &quot;prefetching&quot;.

There may be some other, more effcient IPC notification mechanisms 
available on your OS that Squid can be enhanced to support. I have not 
surveyed what is available these days.


HTH,

Alex.


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="026207.html">[squid-users] very poor performance of rock cache ipc
</A></li>
	<LI>Next message (by thread): <A HREF="026204.html">[squid-users] Fwd: Squid does not pass HTTPS traffic transparently
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26208">[ date ]</a>
              <a href="thread.html#26208">[ thread ]</a>
              <a href="subject.html#26208">[ subject ]</a>
              <a href="author.html#26208">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
