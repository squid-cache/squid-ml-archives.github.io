From squid3 at treenet.co.nz  Thu Feb  1 04:50:13 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 1 Feb 2024 17:50:13 +1300
Subject: [squid-users] Is Squid 6 production ready?
In-Reply-To: <CABZyDWEtZzM5vLZX8u79VeUjeei_4n-N+rLBuCcsLi0yyNmFvQ@mail.gmail.com>
References: <CABZyDWHB_Jj3wYF242WSOvHQQop2RmpAH-_1+ho+Cr5VvRBG0Q@mail.gmail.com>
 <d8faeb35-9b42-49ab-9862-c304d6a09a3b@treenet.co.nz>
 <CABZyDWEtZzM5vLZX8u79VeUjeei_4n-N+rLBuCcsLi0yyNmFvQ@mail.gmail.com>
Message-ID: <134057a7-fbbe-442f-8bc7-e86504dcdf3f@treenet.co.nz>

On 1/02/24 11:22, Miha Miha wrote:
>> On 10/01/24 12:18, Miha Miha wrote:
>>> Release note of latest Squid 6.6 says: "...not deemed ready for
>>> production use..."  For comparison Squid 5.1 was 'ready'. When v6 is
>>> expected to be ready for prod systems?
>>
>> On Fri, Jan 12, 2024 at 3:37?PM Amos Jeffries wrote:
>> Sorry, that is an oversight in the release notes text. Removing it now.
>>
>> Squid 6 is production ready.
> 
> Hi Amos,
> I still see the 6.6 release note unchanged. Could you please adjust.
> 


The page is auto-generated from the release source code. It is too late 
to change the 6.6 package. The documentation has been updated already 
for 6.7 release.

Cheers
Amos


From squid.org at bloms.de  Thu Feb  1 12:15:11 2024
From: squid.org at bloms.de (Dieter Bloms)
Date: Thu, 1 Feb 2024 13:15:11 +0100
Subject: [squid-users] does the logging of cache.log support the log modules
 like daemon, syslog, udp ...
Message-ID: <20240201121511.isvoljnxpzj3zzlz@bloms.de>

Hello,

I would like to run the squid in a Kubernetes environment.
I can simply send the access.log outside the container with the syslog module.
I have tried it with the cache.log, but unfortunately I don't see any log entries from the cache.log. The access.log lines are transmitted:

--snip--
# send the logs to rsyslog (rsyslog will forward the logs to external syslog server)
access_log syslog:local1.info keyvalue
cache_log syslog:local2.info
--snip--

Is it possible to send the cache.logs to the syslog socket /dev/log ?

-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From rousskov at measurement-factory.com  Thu Feb  1 16:33:32 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Feb 2024 11:33:32 -0500
Subject: [squid-users] does the logging of cache.log support the log
 modules like daemon, syslog, udp ...
In-Reply-To: <20240201121511.isvoljnxpzj3zzlz@bloms.de>
References: <20240201121511.isvoljnxpzj3zzlz@bloms.de>
Message-ID: <769d143a-10ff-4cf8-805f-bd3c74398bf8@measurement-factory.com>

On 2024-02-01 07:15, Dieter Bloms wrote:

> Is it possible to send the cache.logs to the syslog socket /dev/log ?

cache_log does not have access_log's concept of logging modules.

* To send level-0/1 cache.log messages to syslog, use "squid -s ..." or 
"squid -l... ...". By default, syslog is only used for a few special 
messages that are not printed to cache.log (e.g., "Exiting due to 
repeated, frequent failures") and these level-0 cache.log messages:

     FATAL: Received Segment Violation...dying.
     FATAL: Received Bus Error...dying.
     FATAL: Received signal ... ...dying.
     ERROR: Squid BUG: ...

and

     FATAL: <assertion text or a similar failure message>
     Squid Cache (Version ...): Terminated abnormally


* To send level-X (and more important) cache.log messages to standard 
error stream, use "squid -dX ...". That stderr output can be redirected 
as needed using shell output redirection mechanisms, of course. By 
default, modern Squids do not log to stderr in most cases.


The above two options do not tell Squid what cache.log messages to emit. 
They only affect which emitted cache.log messages to copy to syslog 
and/or stderr. To tell Squid what cache.log messages to emit, see "squid 
-X ...", debug_options, and cache_log_message. By default, Squid emits 
level-0/1 messages in most cases.


If the above information is not in Squid wiki, please consider 
submitting a pull request that adds (a polished version of) it:
https://github.com/squid-cache/squid-cache.github.io/pulls


HTH,

Alex.



From robin.carlisle at framestore.com  Thu Feb  1 17:03:38 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Thu, 1 Feb 2024 17:03:38 +0000
Subject: [squid-users] stale-if-error returning a 502
Message-ID: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>

Hi, I am having trouble with stale-if-error response.  I am making calls
using curl to an API (under my control) on Amazon AWS.  Config and details
below ...


# /etc/squid/squid.conf :

acl to_aws dstdomain .amazonaws.com

acl from_local src localhost

http_access allow to_aws

http_access allow from_local

cache allow all

cache_dir ufs /var/cache/squid 1024 16 256

http_port 3129 ssl-bump cert=/etc/squid/maul.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/lib/squid/security_file_certgen -s
/var/lib/squid/ssl_db -M 4MB

acl step1 at_step SslBump1

ssl_bump bump step1

ssl_bump bump all

sslproxy_cert_error deny all

cache_store_log stdio:/var/log/squid/store.log

logfile_rotate 0

shutdown_lifetime 3 seconds

# /usr/bin/proxy-test :

#!/bin/bash

curl --proxy http://localhost:3129 \

  --cacert /etc/squid/stuff.pem \

  -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json" \

  -H "Authorization: token MYTOKEN" \

  -H "Content-Type: application/json" \

  --output "/tmp/stuff.json"


Tests  ..........

At this point in time the network cable is unattached.  Squid returns the
cached object it got when the network was online earlier. The Age of this
object is just still under the max_age of 3600.     Previously I was using
offline_mode but I found that it did not try to revalidate from the origin
after the object expired (defined via max-age response).   My understanding
is that stale-if-error should work under my circumstances.

# /var/log/squid/access.log

1706799404.440      6 127.0.0.1 NONE_NONE/200 0 CONNECT
stuff.amazonaws.com:443 - HIER_NONE/- -

1706799404.440      0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
https://stuff.amazonaws.com/stuff.json - HIER_NONE/- application/json

# extract from /usr/bin/proxy-test

< HTTP/1.1 200 OK

< Date: Thu, 01 Feb 2024 13:57:11 GMT

< Content-Type: application/json

< Content-Length: 20134

< x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2

< Last-Modified: 2024-02-01T13:00:45.000Z

< Access-Control-Allow-Origin: *

< x-amz-apigw-id: SdZwpG7qiYcERUQ=

< Cache-Control: public, max-age=3600, stale-if-error=31536000

< ETag: "cec102b43372840737ab773c2e77858b"

< X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6

< Age: 3573

< X-Cache: HIT from labs-maul-st-31

< X-Cache-Lookup: HIT from labs-maul-st-31:3129

< Via: 1.1 labs-maul-st-31 (squid/5.7)

< Connection: keep-alive



Below .. the curl script executes again.  The Age has gone over the max-age
so squid attempted to refresh from the origin.  The machine is still
offline so the refresh failed.   I expected that the stale-if-error
response would instruct squid to return the cached object as a 200.

# /var/log/squid/access.log

1706799434.464      5 127.0.0.1 NONE_NONE/200 0 CONNECT
stuff.amazonaws.com:443 - HIER_NONE/- -

1706799434.464      0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235 GET
https://stuff.amazonaws.com/stuff.json - HIER_NONE/- text/html

# extract from /usr/bin/proxy-test

< HTTP/1.1 502 Bad Gateway

< Server: squid/5.7

< Mime-Version: 1.0

< Date: Thu, 01 Feb 2024 14:57:14 GMT

< Content-Type: text/html;charset=utf-8

< Content-Length: 3853

< X-Squid-Error: ERR_READ_ERROR 0

< Vary: Accept-Language

< Content-Language: en

< X-Cache: MISS from labs-maul-st-31

< X-Cache-Lookup: HIT from labs-maul-st-31:3129

< Via: 1.1 labs-maul-st-31 (squid/5.7)

< Connection: close


Hope someone can help me with this.  All the best,

Robin Carlisle
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240201/5e93b7bd/attachment.htm>

From rousskov at measurement-factory.com  Thu Feb  1 18:27:49 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Feb 2024 13:27:49 -0500
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
Message-ID: <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>

On 2024-02-01 12:03, Robin Carlisle wrote:
> Hi, I am having trouble with stale-if-error response.

If I am interpreting Squid code correctly, in primary use cases:

* without a Cache-Control:stale-if-error=X in the original response, 
Squid sends a stale object if revalidation results in a 5xx error;

* with a Cache-Control:stale-if-error=X and object age at most X, Squid 
sends a stale object if revalidation results in a 5xx error;

* with a Cache-Control:stale-if-error=X and object age exceeding X, 
Squid forwards the 5xx error response if revalidation results in a 5xx 
error;

In other words, stale-if-error=X turns on a "fail on validation errors" 
behavior for stale objects older than X. It has no other effects.

In your test case, the stale objects are much younger than 
stale-if-error value (e.g., Age~=3601 vs. stale-if-error=31536000). 
Thus, stale-if-error should have no relevant effect.

Something else is probably preventing your Squid from serving the stale 
response when facing a 5xx error. I do not know what that something is.

I recommend sharing (privately if you need to protect sensitive info) a 
pointer to a compressed ALL,9 cache.log collected while reproducing the 
problem (using two transactions similar to the ones you have shared 
below -- a successful stale hit and a problematic one): 
https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

Alternatively, you can try to study cache.log yourself after setting 
debug_options to ALL,3. Searching for "refresh" and "handleIMSReply" may 
yield enough clues.


HTH,

Alex.




> # /etc/squid/squid.conf :
> 
> acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
> 
> acl from_local src localhost
> 
> http_access allow to_aws
> 
> http_access allow from_local
> 
> cache allow all
> 
> cache_dir ufs /var/cache/squid 1024 16 256
> 
> http_port 3129 ssl-bump cert=/etc/squid/maul.pem 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> sslcrtd_program /usr/lib/squid/security_file_certgen -s 
> /var/lib/squid/ssl_db -M 4MB
> 
> acl step1 at_step SslBump1
> 
> ssl_bump bump step1
> 
> ssl_bump bump all
> 
> sslproxy_cert_error deny all
> 
> cache_store_log stdio:/var/log/squid/store.log
> 
> logfile_rotate 0
> 
> shutdown_lifetime 3 seconds
> 
> 
> # /usr/bin/proxy-test :
> 
> #!/bin/bash
> 
> curl --proxy http://localhost:3129 <http://localhost:3129> \
> 
>  ??--cacert /etc/squid/stuff.pem \
> 
>  ??-v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json 
> <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>" \
> 
>  ??-H "Authorization: token MYTOKEN" \
> 
>  ??-H "Content-Type: application/json" \
> 
>  ??--output "/tmp/stuff.json"
> 
> 
> 
> Tests? ..........
> 
> 
> At this point in time the network cable is unattached.? Squid returns 
> the cached object it got when the network was online earlier. The Age of 
> this object is just still under the max_age of 3600. ? ? Previously I 
> was using offline_mode but I found that it did not try to revalidate 
> from the origin after the object expired (defined via max-age response). 
>  ? My understanding is that stale-if-error should work under my 
> circumstances.
> 
> 
> # /var/log/squid/access.log
> 
> 1706799404.440? ? ? 6 127.0.0.1 NONE_NONE/200 0 CONNECT 
> stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - HIER_NONE/- -
> 
> 1706799404.440? ? ? 0 127.0.0.1 TCP_MEM_HIT/200 20726 GET 
> https://stuff.amazonaws.com/stuff.json 
> <https://stuff.amazonaws.com/stuff.json> - HIER_NONE/- application/json
> 
> 
> # extract from /usr/bin/proxy-test
> 
> < HTTP/1.1 200 OK
> 
> < Date: Thu, 01 Feb 2024 13:57:11 GMT
> 
> < Content-Type: application/json
> 
> < Content-Length: 20134
> 
> < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
> 
> < Last-Modified: 2024-02-01T13:00:45.000Z
> 
> < Access-Control-Allow-Origin: *
> 
> < x-amz-apigw-id: SdZwpG7qiYcERUQ=
> 
> < Cache-Control: public, max-age=3600, stale-if-error=31536000
> 
> < ETag: "cec102b43372840737ab773c2e77858b"
> 
> < X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6
> 
> < Age: 3573
> 
> < X-Cache: HIT from labs-maul-st-31
> 
> < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> 
> < Via: 1.1 labs-maul-st-31 (squid/5.7)
> 
> < Connection: keep-alive
> 
> 
> 
> 
> Below .. the curl script executes again.? The Age has gone over the 
> max-age so squid attempted to refresh from the origin.? The machine is 
> still offline so the refresh failed. ? I expected that the 
> stale-if-error response would instruct squid to return the cached object 
> as a 200.
> 
> 
> # /var/log/squid/access.log
> 
> 1706799434.464? ? ? 5 127.0.0.1 NONE_NONE/200 0 CONNECT 
> stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - HIER_NONE/- -
> 
> 1706799434.464? ? ? 0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235 GET 
> https://stuff.amazonaws.com/stuff.json 
> <https://stuff.amazonaws.com/stuff.json> - HIER_NONE/- text/html
> 
> 
> # extract from /usr/bin/proxy-test
> 
> < HTTP/1.1 502 Bad Gateway
> 
> < Server: squid/5.7
> 
> < Mime-Version: 1.0
> 
> < Date: Thu, 01 Feb 2024 14:57:14 GMT
> 
> < Content-Type: text/html;charset=utf-8
> 
> < Content-Length: 3853
> 
> < X-Squid-Error: ERR_READ_ERROR 0
> 
> < Vary: Accept-Language
> 
> < Content-Language: en
> 
> < X-Cache: MISS from labs-maul-st-31
> 
> < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> 
> < Via: 1.1 labs-maul-st-31 (squid/5.7)
> 
> < Connection: close
> 
> 
> 
> Hope someone can help me with this.? All the best,
> 
> 
> Robin Carlisle
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From robin.carlisle at framestore.com  Fri Feb  2 11:20:33 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Fri, 2 Feb 2024 11:20:33 +0000
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
Message-ID: <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>

Hi, thanks for your reply.

I have been looking at :
https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control






*The stale-if-error response directive indicates that the cache can reuse a
stale response when an upstream server generates an error, or when the
error is generated locally. Here, an error is considered any response with
a status code of 500, 502, 503, or 504.Cache-Control: max-age=604800,
stale-if-error=86400In the example above, the response is fresh for 7 days
(604800s). Afterwards, it becomes stale, but can be used for an extra 1 day
(86400s) when an error is encountered.After the stale-if-error period
passes, the client will receive any error generated*

Given what you have said and what the above docs say - I am still confused
as it looks like (in my test cases) the cached response can be used for
3600 secs (this works), after which the cached response can still be used
for an additional 31536000 seconds on an error (this doesnt work).

I am going to dig into the error logging you suggested to see if I can make
sense of that - and will send on if I can't.

Thanks v much for your help again,

Robin





On Thu, 1 Feb 2024 at 18:27, Alex Rousskov <rousskov at measurement-factory.com>
wrote:

> On 2024-02-01 12:03, Robin Carlisle wrote:
> > Hi, I am having trouble with stale-if-error response.
>
> If I am interpreting Squid code correctly, in primary use cases:
>
> * without a Cache-Control:stale-if-error=X in the original response,
> Squid sends a stale object if revalidation results in a 5xx error;
>
> * with a Cache-Control:stale-if-error=X and object age at most X, Squid
> sends a stale object if revalidation results in a 5xx error;
>
> * with a Cache-Control:stale-if-error=X and object age exceeding X,
> Squid forwards the 5xx error response if revalidation results in a 5xx
> error;
>
> In other words, stale-if-error=X turns on a "fail on validation errors"
> behavior for stale objects older than X. It has no other effects.
>
> In your test case, the stale objects are much younger than
> stale-if-error value (e.g., Age~=3601 vs. stale-if-error=31536000).
> Thus, stale-if-error should have no relevant effect.
>
> Something else is probably preventing your Squid from serving the stale
> response when facing a 5xx error. I do not know what that something is.
>
> I recommend sharing (privately if you need to protect sensitive info) a
> pointer to a compressed ALL,9 cache.log collected while reproducing the
> problem (using two transactions similar to the ones you have shared
> below -- a successful stale hit and a problematic one):
>
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
>
> Alternatively, you can try to study cache.log yourself after setting
> debug_options to ALL,3. Searching for "refresh" and "handleIMSReply" may
> yield enough clues.
>
>
> HTH,
>
> Alex.
>
>
>
>
> > # /etc/squid/squid.conf :
> >
> > acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
> >
> > acl from_local src localhost
> >
> > http_access allow to_aws
> >
> > http_access allow from_local
> >
> > cache allow all
> >
> > cache_dir ufs /var/cache/squid 1024 16 256
> >
> > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
> > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> >
> > sslcrtd_program /usr/lib/squid/security_file_certgen -s
> > /var/lib/squid/ssl_db -M 4MB
> >
> > acl step1 at_step SslBump1
> >
> > ssl_bump bump step1
> >
> > ssl_bump bump all
> >
> > sslproxy_cert_error deny all
> >
> > cache_store_log stdio:/var/log/squid/store.log
> >
> > logfile_rotate 0
> >
> > shutdown_lifetime 3 seconds
> >
> >
> > # /usr/bin/proxy-test :
> >
> > #!/bin/bash
> >
> > curl --proxy http://localhost:3129 <http://localhost:3129> \
> >
> >    --cacert /etc/squid/stuff.pem \
> >
> >    -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>" \
> >
> >    -H "Authorization: token MYTOKEN" \
> >
> >    -H "Content-Type: application/json" \
> >
> >    --output "/tmp/stuff.json"
> >
> >
> >
> > Tests  ..........
> >
> >
> > At this point in time the network cable is unattached.  Squid returns
> > the cached object it got when the network was online earlier. The Age of
> > this object is just still under the max_age of 3600.     Previously I
> > was using offline_mode but I found that it did not try to revalidate
> > from the origin after the object expired (defined via max-age response).
> >    My understanding is that stale-if-error should work under my
> > circumstances.
> >
> >
> > # /var/log/squid/access.log
> >
> > 1706799404.440      6 127.0.0.1 NONE_NONE/200 0 CONNECT
> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - HIER_NONE/- -
> >
> > 1706799404.440      0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
> > https://stuff.amazonaws.com/stuff.json
> > <https://stuff.amazonaws.com/stuff.json> - HIER_NONE/- application/json
> >
> >
> > # extract from /usr/bin/proxy-test
> >
> > < HTTP/1.1 200 OK
> >
> > < Date: Thu, 01 Feb 2024 13:57:11 GMT
> >
> > < Content-Type: application/json
> >
> > < Content-Length: 20134
> >
> > < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
> >
> > < Last-Modified: 2024-02-01T13:00:45.000Z
> >
> > < Access-Control-Allow-Origin: *
> >
> > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
> >
> > < Cache-Control: public, max-age=3600, stale-if-error=31536000
> >
> > < ETag: "cec102b43372840737ab773c2e77858b"
> >
> > < X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6
> >
> > < Age: 3573
> >
> > < X-Cache: HIT from labs-maul-st-31
> >
> > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >
> > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >
> > < Connection: keep-alive
> >
> >
> >
> >
> > Below .. the curl script executes again.  The Age has gone over the
> > max-age so squid attempted to refresh from the origin.  The machine is
> > still offline so the refresh failed.   I expected that the
> > stale-if-error response would instruct squid to return the cached object
> > as a 200.
> >
> >
> > # /var/log/squid/access.log
> >
> > 1706799434.464      5 127.0.0.1 NONE_NONE/200 0 CONNECT
> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - HIER_NONE/- -
> >
> > 1706799434.464      0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235 GET
> > https://stuff.amazonaws.com/stuff.json
> > <https://stuff.amazonaws.com/stuff.json> - HIER_NONE/- text/html
> >
> >
> > # extract from /usr/bin/proxy-test
> >
> > < HTTP/1.1 502 Bad Gateway
> >
> > < Server: squid/5.7
> >
> > < Mime-Version: 1.0
> >
> > < Date: Thu, 01 Feb 2024 14:57:14 GMT
> >
> > < Content-Type: text/html;charset=utf-8
> >
> > < Content-Length: 3853
> >
> > < X-Squid-Error: ERR_READ_ERROR 0
> >
> > < Vary: Accept-Language
> >
> > < Content-Language: en
> >
> > < X-Cache: MISS from labs-maul-st-31
> >
> > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >
> > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >
> > < Connection: close
> >
> >
> >
> > Hope someone can help me with this.  All the best,
> >
> >
> > Robin Carlisle
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240202/ca911108/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb  2 15:40:52 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Feb 2024 10:40:52 -0500
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <5337955.387429.1705673338435@mail.yahoo.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
 <929108967.11249363.1704896472206@mail.yahoo.com>
 <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>
 <1781216472.758279.1705069266312@mail.yahoo.com>
 <3b0de845-eff2-485a-9528-9e714f06c0b1@measurement-factory.com>
 <5337955.387429.1705673338435@mail.yahoo.com>
Message-ID: <1e27f874-7d16-42c6-b6da-793753a47ef4@measurement-factory.com>

On 2024-01-19 09:08, Arun Kumar wrote:
> Sorry, due to organization policy not possible to upload the debug logs.

I really doubt your organization prohibits sharing information with 
trusted parties. It is up to you whether to make me (or any other Squid 
developer who is willing to help you) such a party.


> Anything to look specifically in the debug logs?

Yes, of course, but I, personally, do not have enough free time to help 
you navigate debugging logs via email. That is why I am suggesting 
sharing those logs with me (while making that sharing comply with any 
organizational policies you need to comply with, of course). This is the 
best I can offer. If that is not good enough, I hope that others can 
offer more/different help.


Good luck,

Alex.


> Also please suggest if we can tweak the below sslbump configuration, to 
> make the chunked transfer work seamless.
> 
> /http_port tcpkeepalive=60,30,3 ssl-bump generate-host-certificates=on 
> dynamic_cert_mem_cache_size=20MB tls-cert=<pem file> tls-key=<key file> 
> cipher=... options=NO_TLSv1,... tls_dh=prime256v1:<dhparm.pem>/
> /
> /
> /ssl_bump stare all/
> 
> PS: Any documentations/video available to understand the 
> bump/stare/peek/splice better? Not understanding much from the 
> squid-cache.org contents.
> 
> On Friday, January 12, 2024 at 02:10:40 PM EST, Alex Rousskov 
> <rousskov at measurement-factory.com> wrote:
> 
> On 2024-01-12 09:21, Arun Kumar wrote:
>  > On Wednesday, January 10, 2024 at 11:09:48 AM EST, Alex Rousskov wrote:
>  >
>  >
>  > On 2024-01-10 09:21, Arun Kumar wrote:
>  >? >> i) Retry seems to fetch one chunk of the response and not the 
> complete.
>  >? >> ii) Enabling sslbump and turning ICAP off, not helping.
>  >? >> iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)
>  >
>  >? >GCC v7 has insufficient C++17 support. I recommend installing GCC v9 or
>  > better and then trying with Squid v6.6 or newer.
>  >
>  > Arun: Compiled Squid 6.6 with gcc 11.4 and still seeing the same issue.
> 
> Glad you were able to upgrade to Squid v6.6!
> 
> 
>  >? > FWIW, if the problem persists in Squid v6, sharing debugging logs 
> would
>  > be the next recommended step.
>  >
>  > Arun: /debug_options ALL,6 /giving too much log. Any particular option
>  > we can use to debug this issue?
> 
> 
> Please share[^1] a pointer to compressed ALL,9 cache.log collected while
> reproducing the problem with Squid v6.6:
> 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
> Debugging logs are for developers. Developers can deal with large
> volumes of debugging information. You can use services like DropBox to
> share large compressed logs. Said that, the better you can isolate the
> problem/traffic, the higher are the chances that a developer will (have
> the time to) find the answer to your question in the noisy log.
> 
> [^1]: Please feel free to share privately if needed, especially if you
> are using sensitive configuration or transactions.
> 
> Alex.
> 
> 
>  >? > Also want to point out that, squid connects to another non-squid proxy
>  >? > to reach internet.
>  >? > cache_peer <proxy_url> parent <port> 0 no-query default
>  >? >
>  >? > On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov wrote:
>  >? >
>  >? >
>  >? > On 2024-01-09 11:51, Zhang, Jinshu wrote:
>  >? >
>  >? >? > Client got below response headers and body. Masked few details.
>  >? >
>  >? > Thank you.
>  >? >
>  >? >
>  >? >? > Retry seems to fetch data remaining.
>  >? >
>  >? > I would expect a successful retry to fetch the entire response, 
> not just
>  >? > the remaining bytes, but perhaps that is what you meant. Thank you for
>  >? > sharing this info.
>  >? >
>  >? >
>  >? >? > Want to point out that removing sslbump everything is working fine,
>  >? >? > but we wanted to keep it for ICAP scanning.
>  >? >
>  >? > What if you keep SslBump enabled but disable any ICAP analysis
>  >? > ("icap_enable off")? This test may tell us if the problem is between
>  >? > Squid and the origin server or Squid and the ICAP service...
>  >? >
>  >? >
>  >? >? > We tried compiling 6.x in Amazon linux, using latest gcc, but 
> facing
>  >? > similar error -
>  >? >
>  > 
> https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html> <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>> <[squid-users] compile error in squid v6.1 <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>>>
>  >? >
>  >? > What is the "latest gcc" version in your environment? I suspect it is
>  >? > not the latest GCC version available to folks running Amazon 
> Linux, but
>  >? > you may need to install some packages to get a more recent GCC 
> version.
>  >? > Unfortunately, I cannot give specific instructions for Amazon Linux
>  >? > right now.
>  >? >
>  >? >
>  >? > HTH,
>  >? >
>  >? > Alex.
>  >? >
>  >? >
>  >? >? > HTTP/1.1 200 OK
>  >? >? > Date: Tue, 09 Jan 2024 15:41:33 GMT
>  >? >? > Server: Apache/mod_perl/2.0.10 Perl
>  >? >? > Content-Type: application/download
>  >? >? > X-Cache: MISS from ip-x-y-z
>  >? >? > Transfer-Encoding: chunked
>  >? >? > Via: xxx (ICAP)
>  >? >? > Connection: keep-alive
>  >? >? >
>  >? >? > 1000
>  >? >? > File-Id: xyz.zip
>  >? >? > Local-Path: x/y/z.txt
>  >? >? > Content-Size: 2967
>  >? >? > < binary content >
>  >? >? >
>  >? >? >
>  >? >? > Access log(1st attempt):
>  >? >? > 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>  >? > FIRSTUP_PARENT/10.x.y.z -
>  >? >? > 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST
>  >? > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> <https://a.b.com/xyz <https://a.b.com/xyz>
>  > <https://a.b.com/xyz <https://a.b.com/xyz>>> - FIRSTUP_PARENT/10.x.y.z
>  >? > application/download
>  >? >? >
>  >? >? > Retry after 5 mins:
>  >? >? > 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>  >? > FIRSTUP_PARENT/10.x.y.z -
>  >? >? > 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST
>  >? > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> <https://a.b.com/xyz <https://a.b.com/xyz>
>  > <https://a.b.com/xyz <https://a.b.com/xyz>>> - FIRSTUP_PARENT/10.x.y.z
>  >? > application/download
>  >? >? >
>  >? >? > Jinshu Zhang
>  >? >? >
>  >? >? >
>  >? >? > Fannie Mae Confidential
>  >? >? > -----Original Message-----
>  >? >? > From: squid-users <squid-users-bounces at lists.squid-cache.org 
> <mailto:squid-users-bounces at lists.squid-cache.org>
>  > <mailto:squid-users-bounces at lists.squid-cache.org>
>  >? > <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Alex
>  >? > Rousskov
>  >? >? > Sent: Tuesday, January 9, 2024 9:53 AM
>  >? >? > To: squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > <mailto:squid-users at lists.squid-cache.org>
>  >? > <mailto:squid-users at lists.squid-cache.org>
>  >? >? > Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
>  >? >? >
>  >? >? >
>  >? >? > On 2024-01-09 09:13, Arun Kumar wrote:
>  >? >? >
>  >? >? >> I have compiled/installed squid v5.8 in Amazon Linux and
>  > configured it
>  >? >? >> with sslbump option. Squid is used as proxy to get response from
>  > https
>  >? >? >> site. When the https site sends chunked response, it appears 
> that the
>  >? >? >> first response comes but it get stuck and doesn't receive the full
>  >? >? >> response. Appreciate any help.
>  >? >? >? ? There were some recent chunking-related changes in Squid, 
> but none
>  >? > of them is likely to be responsible for the problems you are 
> describing
>  >? > unless the origin server response is very special/unusual.
>  >? >? >
>  >? >? > Does the client in this test get the HTTP response header? Some 
> HTTP
>  >? > response body bytes?
>  >? >? >
>  >? >? > To triage the problem, I recommend sharing the corresponding
>  >? > access.log records (at least). Seeing debugging of the problematic
>  >? > transaction may be very useful (but avoid using production 
> security keys
>  >? > and other sensitive information in such tests):
>  >? >? >
>  >? >
>  > 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>> <Sending Bug Reports to the Squid Team <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>>
>  >? >? >
>  >? >? > Please note that Squid v5 is not officially supported and has more
>  >? > known security vulnerabilities than Squid v6. You should be using
>  > Squid v6.
>  >? >? >
>  >? >? >
>  >? >? > HTH,
>  >? >? >
>  >? >? > Alex.
>  >? >? >
> 



From yvain.payen at tessi.fr  Fri Feb  2 16:00:26 2024
From: yvain.payen at tessi.fr (Yvain PAYEN)
Date: Fri, 2 Feb 2024 16:00:26 +0000
Subject: [squid-users] external icap issue with squid 5 and higher
Message-ID: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>

Hi Squid users,

I have an issue with an external icap service I have to use (from Forcepoint).
This service is working great with squid v3 and v4.
Starting v5 (v6 also tested) the service only work with plain text http requests, all requests for https content are allowed even if the website should be denied.

We are searching in the debug logs but don't find anything interesting by now.

My first question is : do you know if a big change in the icap code happened between v4 and v5 ?
My second question : How can I trace only icap debug logs to understand the issue ?

Service is setup like this :
icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap bypass=1


Regards,

Yvain PAYEN

P?le Op?rations & Technologies
Equipe Infrastructure syst?me
T. +33 (0)5 57 57 01 85 (Poste 1185)
M. +33 (0)7 87 30 34 01

Absent tous les mercredi

Tessi France
Immeuble Cassiop?e
1-3 avenue des Satellites
33185 Le Haillan

yvain.payen at tessi.fr<mailto:yvain.payen at tessi.fr>
www.tessi.eu
[cid:image001.png at 01DA55F7.B0BA8DB0]
Pensez ? l'environnement avant d'imprimer cet e-mail.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240202/fe047a61/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 1692 bytes
Desc: image001.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240202/fe047a61/attachment.png>

From rousskov at measurement-factory.com  Fri Feb  2 16:19:10 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Feb 2024 11:19:10 -0500
Subject: [squid-users] external icap issue with squid 5 and higher
In-Reply-To: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
References: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
Message-ID: <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>

On 2024-02-02 11:00, Yvain PAYEN wrote:
> Hi Squid users,
> 
> I have an issue with an external icap service I have to use (from 
> Forcepoint).
> 
> This service is working great with squid v3 and v4.
> 
> Starting v5 (v6 also tested) the service only work with plain text http 
> requests, all requests for https content are allowed even if the website 
> should be denied.

Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your 
service supposed to analyze plain HTTP CONNECT requests?

With Squid v6, does your ICAP service actually receive expected 
"requests for https content" for analysis from Squid? Or does Squid 
allow them without contacting the ICAP service with those requests? You 
can check service logs and/or enable icap.log in Squid to answer these 
high-level questions (see icap_log).


> My first question is : do you know if a big change in the icap code 
> happened between v4 and v5 ?

I do not recall, unfortunately; it was too long ago. Please keep in mind 
that your problems may not be triggered by ICAP code changes (if any).


> My second question : How can I trace only icap debug logs

ICAP code uses debug section 93. See debug_options directive and 
docs/debug-sections.txt.


HTH,

Alex.



> Service is setup like this :
> 
> icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap bypass=1
> 
> Regards,
> 
> *Yvain PAYEN*
> 
> *
> **P?le Op?rations & Technologies
> *Equipe Infrastructure syst?me
> T. +33 (0)5 57 57 01 85 (Poste 1185)
> 
> M. +33 (0)7 87 30 34 01
> 
> Absent tous les mercredi
> 
> 
> Tessi France
> Immeuble Cassiop?e
> 
> 1-3 avenue des Satellites
> 33185 Le Haillan
> 
> 
> *yvain.payen at tessi.fr <mailto:yvain.payen at tessi.fr>
> www.tessi.eu <www.tessi.eu>
> ***
> Pensez ? l'environnement avant d'imprimer cet e-mail.**
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From yvain.payen at tessi.fr  Fri Feb  2 17:04:36 2024
From: yvain.payen at tessi.fr (Yvain PAYEN)
Date: Fri, 2 Feb 2024 17:04:36 +0000
Subject: [squid-users] external icap issue with squid 5 and higher
In-Reply-To: <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>
References: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>
Message-ID: <DB9PR07MB9809B35CC2CA5B48794626169C422@DB9PR07MB9809.eurprd07.prod.outlook.com>

> Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your service supposed to analyze plain HTTP CONNECT requests?

We don't use ssl_bump, icap service only analyze HTTP CONNECT requests

I just compared the debug logs between http and https request and the icap service is contacted in both case and reply in both case with a deny.
But just after the deny reply in https log I found theses lines : 

	2024/02/02 17:40:41.943 kid1| 93,3| ../../../src/base/AsyncJobCalls.h(177) dial: Adaptation::Icap::Xaction::noteCommRead threw exception: check failed: 	readBuf.isEmpty()
    		exception location: ModXact.cc(1219) stopParsing
	2024/02/02 17:40:41.943 kid1| 93,3| ModXact.cc(679) callException: bypassing 0x558f358fdae0*2 exception: check failed: readBuf.isEmpty()
    		exception location: ModXact.cc(1219) stopParsing  [FD 17;rp(1)S(2)YG/Rw job17]
	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(720) disableBypass: will never start bypass because already started to bypass
	2024/02/02 17:40:41.943 kid1| 93,5| Xaction.cc(127) disableRepeats: Adaptation::Icap::ModXact still cannot be repeated because preparing to echo content [FD 	17;rp(1)S(2)G/Rw job17]
	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(724) disableBypass: not protecting group bypass because preparing to echo content
	2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_SAT to ICAP_ECHO
	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(962) prepEchoing: cloning virgin message 0x558f358ff040
	2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_ECHO to ICAP_ERR_OTHER
	2024/02/02 17:40:41.943 kid1| 93,4| ServiceRep.cc(97) noteFailure:  failure 1 out of 10 allowed in 0sec [up,fail1]
	2024/02/02 17:40:41.943 kid1| 93,2| AsyncJob.cc(130) callException: check failed: !adapted.header
   		 exception location: ModXact.cc(971) prepEchoing
	2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(85) mustStop: Adaptation::Icap::ModXact will stop, reason: exception
	2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(140) callEnd: Adaptation::Icap::Xaction::noteCommRead(conn8 local=X.X.X.X:46704 remote=X.X.X.X:1344 FD 	17 flags=1, data=0x558f358fe888) ends job [FD 17;rp(1)S(2)/StoppedRw job17]
	2024/02/02 17:40:41.943 kid1| 93,5| ModXact.cc(1295) swanSong: swan sings [FD 17;rp(1)S(2)/StoppedRw job17]
	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(616) stopSending: Enter stop sending 
	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(619) stopSending: Proceed with stop sending

It seems to bypass because something gone wrong.

Yvain PAYEN

P?le Op?rations & Technologies
Equipe Infrastructure syst?me
T. +33 (0)5 57 57 01 85 (Poste 1185)
M. +33 (0)7 87 30 34 01

Absent tous les mercredi

Tessi France
Immeuble Cassiop?e
1-3 avenue des Satellites
33185 Le Haillan

Pensez ? l'environnement avant d'imprimer cet e-mail.

-----Message d'origine-----
De?: squid-users <squid-users-bounces at lists.squid-cache.org> De la part de Alex Rousskov
Envoy??: vendredi 2 f?vrier 2024 17:19
??: squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] external icap issue with squid 5 and higher

? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le contenu est fiable.  ?



On 2024-02-02 11:00, Yvain PAYEN wrote:
> Hi Squid users,
>
> I have an issue with an external icap service I have to use (from 
> Forcepoint).
>
> This service is working great with squid v3 and v4.
>
> Starting v5 (v6 also tested) the service only work with plain text 
> http requests, all requests for https content are allowed even if the 
> website should be denied.

Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your service supposed to analyze plain HTTP CONNECT requests?

With Squid v6, does your ICAP service actually receive expected "requests for https content" for analysis from Squid? Or does Squid allow them without contacting the ICAP service with those requests? You can check service logs and/or enable icap.log in Squid to answer these high-level questions (see icap_log).


> My first question is : do you know if a big change in the icap code 
> happened between v4 and v5 ?

I do not recall, unfortunately; it was too long ago. Please keep in mind that your problems may not be triggered by ICAP code changes (if any).


> My second question : How can I trace only icap debug logs

ICAP code uses debug section 93. See debug_options directive and docs/debug-sections.txt.


HTH,

Alex.



> Service is setup like this :
>
> icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap 
> bypass=1
>
> Regards,
>
> *Yvain PAYEN*
>
> *
> **P?le Op?rations & Technologies
> *Equipe Infrastructure syst?me
> T. +33 (0)5 57 57 01 85 (Poste 1185)
>
> M. +33 (0)7 87 30 34 01
>
> Absent tous les mercredi
>
>
> Tessi France
> Immeuble Cassiop?e
>
> 1-3 avenue des Satellites
> 33185 Le Haillan
>
>
> *yvain.payen at tessi.fr <mailto:yvain.payen at tessi.fr> www.tessi.eu 
> <www.tessi.eu>
> ***
> Pensez ? l'environnement avant d'imprimer cet e-mail.**
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Fri Feb  2 17:45:08 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Feb 2024 12:45:08 -0500
Subject: [squid-users] external icap issue with squid 5 and higher
In-Reply-To: <DB9PR07MB9809B35CC2CA5B48794626169C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
References: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>
 <DB9PR07MB9809B35CC2CA5B48794626169C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
Message-ID: <28771f6c-7ba7-4376-9ae9-cdcfcadaa5f3@measurement-factory.com>

On 2024-02-02 12:04, Yvain PAYEN wrote:

> We don't use ssl_bump, icap service only analyze HTTP CONNECT requests

Great, that simplifies things a lot.


> Adaptation::Icap::Xaction::noteCommRead threw exception: > check failed: readBuf.isEmpty()> exception location: ModXact.cc(1219) 
stopParsing

It looks like Squid found some leftovers after the ICAP response that 
Squid (thought it) had fully parsed. I do not yet know whether that ICAP 
response was malformed or Squid is buggy.

Can you share raw ICAP response bytes (preferrably in libpcap or similar 
raw packet capture format) collected by tcpdump, wireshark, or a similar 
tool? You can obfuscate/convert that ICAP response to text as needed, 
but if those extra bytes get lost in those conversions, then we would 
not be able to tell what those bytes are (e.g., they may contain 
whitespace characters that get easily lost).


Thank you,

Alex.


> 	2024/02/02 17:40:41.943 kid1| 93,3| ModXact.cc(679) callException: bypassing 0x558f358fdae0*2 exception: check failed: readBuf.isEmpty()
>      		exception location: ModXact.cc(1219) stopParsing  [FD 17;rp(1)S(2)YG/Rw job17]
> 	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(720) disableBypass: will never start bypass because already started to bypass
> 	2024/02/02 17:40:41.943 kid1| 93,5| Xaction.cc(127) disableRepeats: Adaptation::Icap::ModXact still cannot be repeated because preparing to echo content [FD 	17;rp(1)S(2)G/Rw job17]
> 	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(724) disableBypass: not protecting group bypass because preparing to echo content
> 	2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_SAT to ICAP_ECHO
> 	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(962) prepEchoing: cloning virgin message 0x558f358ff040
> 	2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_ECHO to ICAP_ERR_OTHER
> 	2024/02/02 17:40:41.943 kid1| 93,4| ServiceRep.cc(97) noteFailure:  failure 1 out of 10 allowed in 0sec [up,fail1]
> 	2024/02/02 17:40:41.943 kid1| 93,2| AsyncJob.cc(130) callException: check failed: !adapted.header
>     		 exception location: ModXact.cc(971) prepEchoing
> 	2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(85) mustStop: Adaptation::Icap::ModXact will stop, reason: exception
> 	2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(140) callEnd: Adaptation::Icap::Xaction::noteCommRead(conn8 local=X.X.X.X:46704 remote=X.X.X.X:1344 FD 	17 flags=1, data=0x558f358fe888) ends job [FD 17;rp(1)S(2)/StoppedRw job17]
> 	2024/02/02 17:40:41.943 kid1| 93,5| ModXact.cc(1295) swanSong: swan sings [FD 17;rp(1)S(2)/StoppedRw job17]
> 	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(616) stopSending: Enter stop sending
> 	2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(619) stopSending: Proceed with stop sending
> 
> It seems to bypass because something gone wrong.
> 
> Yvain PAYEN
> 
> P?le Op?rations & Technologies
> Equipe Infrastructure syst?me
> T. +33 (0)5 57 57 01 85 (Poste 1185)
> M. +33 (0)7 87 30 34 01
> 
> Absent tous les mercredi
> 
> Tessi France
> Immeuble Cassiop?e
> 1-3 avenue des Satellites
> 33185 Le Haillan
> 
> Pensez ? l'environnement avant d'imprimer cet e-mail.
> 
> -----Message d'origine-----
> De?: squid-users <squid-users-bounces at lists.squid-cache.org> De la part de Alex Rousskov
> Envoy??: vendredi 2 f?vrier 2024 17:19
> ??: squid-users at lists.squid-cache.org
> Objet?: Re: [squid-users] external icap issue with squid 5 and higher
> 
> ? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le contenu est fiable.  ?
> 
> 
> 
> On 2024-02-02 11:00, Yvain PAYEN wrote:
>> Hi Squid users,
>>
>> I have an issue with an external icap service I have to use (from
>> Forcepoint).
>>
>> This service is working great with squid v3 and v4.
>>
>> Starting v5 (v6 also tested) the service only work with plain text
>> http requests, all requests for https content are allowed even if the
>> website should be denied.
> 
> Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your service supposed to analyze plain HTTP CONNECT requests?
> 
> With Squid v6, does your ICAP service actually receive expected "requests for https content" for analysis from Squid? Or does Squid allow them without contacting the ICAP service with those requests? You can check service logs and/or enable icap.log in Squid to answer these high-level questions (see icap_log).
> 
> 
>> My first question is : do you know if a big change in the icap code
>> happened between v4 and v5 ?
> 
> I do not recall, unfortunately; it was too long ago. Please keep in mind that your problems may not be triggered by ICAP code changes (if any).
> 
> 
>> My second question : How can I trace only icap debug logs
> 
> ICAP code uses debug section 93. See debug_options directive and docs/debug-sections.txt.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
>> Service is setup like this :
>>
>> icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap
>> bypass=1
>>
>> Regards,
>>
>> *Yvain PAYEN*
>>
>> *
>> **P?le Op?rations & Technologies
>> *Equipe Infrastructure syst?me
>> T. +33 (0)5 57 57 01 85 (Poste 1185)
>>
>> M. +33 (0)7 87 30 34 01
>>
>> Absent tous les mercredi
>>
>>
>> Tessi France
>> Immeuble Cassiop?e
>>
>> 1-3 avenue des Satellites
>> 33185 Le Haillan
>>
>>
>> *yvain.payen at tessi.fr <mailto:yvain.payen at tessi.fr> www.tessi.eu
>> <www.tessi.eu>
>> ***
>> Pensez ? l'environnement avant d'imprimer cet e-mail.**
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From yvain.payen at tessi.fr  Fri Feb  2 22:36:07 2024
From: yvain.payen at tessi.fr (Yvain PAYEN)
Date: Fri, 2 Feb 2024 22:36:07 +0000
Subject: [squid-users] external icap issue with squid 5 and higher
In-Reply-To: <28771f6c-7ba7-4376-9ae9-cdcfcadaa5f3@measurement-factory.com>
References: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>
 <DB9PR07MB9809B35CC2CA5B48794626169C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <28771f6c-7ba7-4376-9ae9-cdcfcadaa5f3@measurement-factory.com>
Message-ID: <DB9PR07MB9809A70EBB9433723BB45BE39C422@DB9PR07MB9809.eurprd07.prod.outlook.com>

I just sent you a Onedrive link to 2 pcap files, one for http request and one for https request.

Regards,

Yvain PAYEN

P?le Op?rations & Technologies
Equipe Infrastructure syst?me
T. +33 (0)5 57 57 01 85 (Poste 1185)
M. +33 (0)7 87 30 34 01

Absent tous les mercredi

Tessi France
Immeuble Cassiop?e
1-3 avenue des Satellites
33185 Le Haillan

Pensez ? l'environnement avant d'imprimer cet e-mail.

-----Message d'origine-----
De?: Alex Rousskov <rousskov at measurement-factory.com> 
Envoy??: vendredi 2 f?vrier 2024 18:45
??: Yvain PAYEN <yvain.payen at tessi.fr>; squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] external icap issue with squid 5 and higher

? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le contenu est fiable.  ?



On 2024-02-02 12:04, Yvain PAYEN wrote:

> We don't use ssl_bump, icap service only analyze HTTP CONNECT requests

Great, that simplifies things a lot.


> Adaptation::Icap::Xaction::noteCommRead threw exception: > check 
> failed: readBuf.isEmpty()> exception location: ModXact.cc(1219)
stopParsing

It looks like Squid found some leftovers after the ICAP response that Squid (thought it) had fully parsed. I do not yet know whether that ICAP response was malformed or Squid is buggy.

Can you share raw ICAP response bytes (preferrably in libpcap or similar raw packet capture format) collected by tcpdump, wireshark, or a similar tool? You can obfuscate/convert that ICAP response to text as needed, but if those extra bytes get lost in those conversions, then we would not be able to tell what those bytes are (e.g., they may contain whitespace characters that get easily lost).


Thank you,

Alex.


>       2024/02/02 17:40:41.943 kid1| 93,3| ModXact.cc(679) callException: bypassing 0x558f358fdae0*2 exception: check failed: readBuf.isEmpty()
>               exception location: ModXact.cc(1219) stopParsing  [FD 17;rp(1)S(2)YG/Rw job17]
>       2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(720) disableBypass: will never start bypass because already started to bypass
>       2024/02/02 17:40:41.943 kid1| 93,5| Xaction.cc(127) disableRepeats: Adaptation::Icap::ModXact still cannot be repeated because preparing to echo content [FD    17;rp(1)S(2)G/Rw job17]
>       2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(724) disableBypass: not protecting group bypass because preparing to echo content
>       2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_SAT to ICAP_ECHO
>       2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(962) prepEchoing: cloning virgin message 0x558f358ff040
>       2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_ECHO to ICAP_ERR_OTHER
>       2024/02/02 17:40:41.943 kid1| 93,4| ServiceRep.cc(97) noteFailure:  failure 1 out of 10 allowed in 0sec [up,fail1]
>       2024/02/02 17:40:41.943 kid1| 93,2| AsyncJob.cc(130) callException: check failed: !adapted.header
>                exception location: ModXact.cc(971) prepEchoing
>       2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(85) mustStop: Adaptation::Icap::ModXact will stop, reason: exception
>       2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(140) callEnd: Adaptation::Icap::Xaction::noteCommRead(conn8 local=X.X.X.X:46704 remote=X.X.X.X:1344 FD  17 flags=1, data=0x558f358fe888) ends job [FD 17;rp(1)S(2)/StoppedRw job17]
>       2024/02/02 17:40:41.943 kid1| 93,5| ModXact.cc(1295) swanSong: swan sings [FD 17;rp(1)S(2)/StoppedRw job17]
>       2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(616) stopSending: Enter stop sending
>       2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(619) stopSending: 
> Proceed with stop sending
>
> It seems to bypass because something gone wrong.
>
> Yvain PAYEN
>
> P?le Op?rations & Technologies
> Equipe Infrastructure syst?me
> T. +33 (0)5 57 57 01 85 (Poste 1185)
> M. +33 (0)7 87 30 34 01
>
> Absent tous les mercredi
>
> Tessi France
> Immeuble Cassiop?e
> 1-3 avenue des Satellites
> 33185 Le Haillan
>
> Pensez ? l'environnement avant d'imprimer cet e-mail.
>
> -----Message d'origine-----
> De : squid-users <squid-users-bounces at lists.squid-cache.org> De la 
> part de Alex Rousskov Envoy? : vendredi 2 f?vrier 2024 17:19 ? : 
> squid-users at lists.squid-cache.org Objet : Re: [squid-users] external 
> icap issue with squid 5 and higher
>
> ? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez 
> pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le 
> contenu est fiable.  ?
>
>
>
> On 2024-02-02 11:00, Yvain PAYEN wrote:
>> Hi Squid users,
>>
>> I have an issue with an external icap service I have to use (from 
>> Forcepoint).
>>
>> This service is working great with squid v3 and v4.
>>
>> Starting v5 (v6 also tested) the service only work with plain text 
>> http requests, all requests for https content are allowed even if the 
>> website should be denied.
>
> Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your service supposed to analyze plain HTTP CONNECT requests?
>
> With Squid v6, does your ICAP service actually receive expected "requests for https content" for analysis from Squid? Or does Squid allow them without contacting the ICAP service with those requests? You can check service logs and/or enable icap.log in Squid to answer these high-level questions (see icap_log).
>
>
>> My first question is : do you know if a big change in the icap code 
>> happened between v4 and v5 ?
>
> I do not recall, unfortunately; it was too long ago. Please keep in mind that your problems may not be triggered by ICAP code changes (if any).
>
>
>> My second question : How can I trace only icap debug logs
>
> ICAP code uses debug section 93. See debug_options directive and docs/debug-sections.txt.
>
>
> HTH,
>
> Alex.
>
>
>
>> Service is setup like this :
>>
>> icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap
>> bypass=1
>>
>> Regards,
>>
>> *Yvain PAYEN*
>>
>> *
>> **P?le Op?rations & Technologies
>> *Equipe Infrastructure syst?me
>> T. +33 (0)5 57 57 01 85 (Poste 1185)
>>
>> M. +33 (0)7 87 30 34 01
>>
>> Absent tous les mercredi
>>
>>
>> Tessi France
>> Immeuble Cassiop?e
>>
>> 1-3 avenue des Satellites
>> 33185 Le Haillan
>>
>>
>> *yvain.payen at tessi.fr <mailto:yvain.payen at tessi.fr> www.tessi.eu 
>> <www.tessi.eu>
>> ***
>> Pensez ? l'environnement avant d'imprimer cet e-mail.**
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Sat Feb  3 03:31:56 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Feb 2024 22:31:56 -0500
Subject: [squid-users] external icap issue with squid 5 and higher
In-Reply-To: <DB9PR07MB9809A70EBB9433723BB45BE39C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
References: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>
 <DB9PR07MB9809B35CC2CA5B48794626169C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <28771f6c-7ba7-4376-9ae9-cdcfcadaa5f3@measurement-factory.com>
 <DB9PR07MB9809A70EBB9433723BB45BE39C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
Message-ID: <8a28ed63-7bec-4d87-89f1-71b010b3235b@measurement-factory.com>

On 2024-02-02 17:36, Yvain PAYEN wrote:

> I just sent you a Onedrive link to 2 pcap files, one for http request
> and one for https request.

Thank you. The ICAP service you are using is sending a malformed ICAP 
response to Squid. That ICAP response promises that there will be no 
HTTP body in the encapsulated HTTP message:

     Encapsulated: res-hdr=0, null-body=524

... but the service does send a body after HTTP headers. That HTTP body 
contains an HTML resource explaining that the CONNECT message was 
blocked and redirecting the user to blockpage.cgi, but that content does 
not really matter here. What matters is that there are some bytes after 
the encapsulated HTTP header. There should be no such bytes (or the ICAP 
Encapsulated header should have res-body=184 instead of null-body=524).

The null-body offset in the ICAP Encapsulated header is wrong. It should 
be 184 bytes (i.e. the size of the encapsulated HTTP response header), 
not 524 bytes. FWIW, 524 is the sum of the encapsulated HTTP response 
header (184 bytes) and the encapsulated HTTP response body (340 bytes). 
It sounds like the ICAP service thinks that it is encapsulating an HTTP 
response header, but it is actually encapsulating the whole HTTP response.

Since this is an ICAP framing error(s), Squid rejects the whole 
transaction and bypasses the ICAP service (as configured).

To fix this, fix the ICAP service configuration (or code).

It is also possible to modify Squid code to ignore these errors, but I 
do not recommend that, and a hard-coded or rigid tolerance code like 
that should not be accepted by the Squid Project for the official inclusion.


The ICAP response in the "http request" capture does not have this 
problem. It contains an encapsulated HTTP 302 Moved header without any 
encapsulated HTTP body. That encapsulation matches the ICAP Encapsulated 
header.


HTH,

Alex.



> -----Message d'origine-----
> De?: Alex Rousskov <rousskov at measurement-factory.com>
> Envoy??: vendredi 2 f?vrier 2024 18:45
> ??: Yvain PAYEN <yvain.payen at tessi.fr>; squid-users at lists.squid-cache.org
> Objet?: Re: [squid-users] external icap issue with squid 5 and higher
> 
> ? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le contenu est fiable.  ?
> 
> 
> 
> On 2024-02-02 12:04, Yvain PAYEN wrote:
> 
>> We don't use ssl_bump, icap service only analyze HTTP CONNECT requests
> 
> Great, that simplifies things a lot.
> 
> 
>> Adaptation::Icap::Xaction::noteCommRead threw exception: > check
>> failed: readBuf.isEmpty()> exception location: ModXact.cc(1219)
> stopParsing
> 
> It looks like Squid found some leftovers after the ICAP response that Squid (thought it) had fully parsed. I do not yet know whether that ICAP response was malformed or Squid is buggy.
> 
> Can you share raw ICAP response bytes (preferrably in libpcap or similar raw packet capture format) collected by tcpdump, wireshark, or a similar tool? You can obfuscate/convert that ICAP response to text as needed, but if those extra bytes get lost in those conversions, then we would not be able to tell what those bytes are (e.g., they may contain whitespace characters that get easily lost).
> 
> 
> Thank you,
> 
> Alex.
> 
> 
>>        2024/02/02 17:40:41.943 kid1| 93,3| ModXact.cc(679) callException: bypassing 0x558f358fdae0*2 exception: check failed: readBuf.isEmpty()
>>                exception location: ModXact.cc(1219) stopParsing  [FD 17;rp(1)S(2)YG/Rw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(720) disableBypass: will never start bypass because already started to bypass
>>        2024/02/02 17:40:41.943 kid1| 93,5| Xaction.cc(127) disableRepeats: Adaptation::Icap::ModXact still cannot be repeated because preparing to echo content [FD    17;rp(1)S(2)G/Rw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(724) disableBypass: not protecting group bypass because preparing to echo content
>>        2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_SAT to ICAP_ECHO
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(962) prepEchoing: cloning virgin message 0x558f358ff040
>>        2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_ECHO to ICAP_ERR_OTHER
>>        2024/02/02 17:40:41.943 kid1| 93,4| ServiceRep.cc(97) noteFailure:  failure 1 out of 10 allowed in 0sec [up,fail1]
>>        2024/02/02 17:40:41.943 kid1| 93,2| AsyncJob.cc(130) callException: check failed: !adapted.header
>>                 exception location: ModXact.cc(971) prepEchoing
>>        2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(85) mustStop: Adaptation::Icap::ModXact will stop, reason: exception
>>        2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(140) callEnd: Adaptation::Icap::Xaction::noteCommRead(conn8 local=X.X.X.X:46704 remote=X.X.X.X:1344 FD  17 flags=1, data=0x558f358fe888) ends job [FD 17;rp(1)S(2)/StoppedRw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,5| ModXact.cc(1295) swanSong: swan sings [FD 17;rp(1)S(2)/StoppedRw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(616) stopSending: Enter stop sending
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(619) stopSending:
>> Proceed with stop sending
>>
>> It seems to bypass because something gone wrong.
>>
>> Yvain PAYEN
>>
>> P?le Op?rations & Technologies
>> Equipe Infrastructure syst?me
>> T. +33 (0)5 57 57 01 85 (Poste 1185)
>> M. +33 (0)7 87 30 34 01
>>
>> Absent tous les mercredi
>>
>> Tessi France
>> Immeuble Cassiop?e
>> 1-3 avenue des Satellites
>> 33185 Le Haillan
>>
>> Pensez ? l'environnement avant d'imprimer cet e-mail.
>>
>> -----Message d'origine-----
>> De : squid-users <squid-users-bounces at lists.squid-cache.org> De la
>> part de Alex Rousskov Envoy? : vendredi 2 f?vrier 2024 17:19 ? :
>> squid-users at lists.squid-cache.org Objet : Re: [squid-users] external
>> icap issue with squid 5 and higher
>>
>> ? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez
>> pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le
>> contenu est fiable.  ?
>>
>>
>>
>> On 2024-02-02 11:00, Yvain PAYEN wrote:
>>> Hi Squid users,
>>>
>>> I have an issue with an external icap service I have to use (from
>>> Forcepoint).
>>>
>>> This service is working great with squid v3 and v4.
>>>
>>> Starting v5 (v6 also tested) the service only work with plain text
>>> http requests, all requests for https content are allowed even if the
>>> website should be denied.
>>
>> Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your service supposed to analyze plain HTTP CONNECT requests?
>>
>> With Squid v6, does your ICAP service actually receive expected "requests for https content" for analysis from Squid? Or does Squid allow them without contacting the ICAP service with those requests? You can check service logs and/or enable icap.log in Squid to answer these high-level questions (see icap_log).
>>
>>
>>> My first question is : do you know if a big change in the icap code
>>> happened between v4 and v5 ?
>>
>> I do not recall, unfortunately; it was too long ago. Please keep in mind that your problems may not be triggered by ICAP code changes (if any).
>>
>>
>>> My second question : How can I trace only icap debug logs
>>
>> ICAP code uses debug section 93. See debug_options directive and docs/debug-sections.txt.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>> Service is setup like this :
>>>
>>> icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap
>>> bypass=1
>>>
>>> Regards,
>>>
>>> *Yvain PAYEN*
>>>
>>> *
>>> **P?le Op?rations & Technologies
>>> *Equipe Infrastructure syst?me
>>> T. +33 (0)5 57 57 01 85 (Poste 1185)
>>>
>>> M. +33 (0)7 87 30 34 01
>>>
>>> Absent tous les mercredi
>>>
>>>
>>> Tessi France
>>> Immeuble Cassiop?e
>>>
>>> 1-3 avenue des Satellites
>>> 33185 Le Haillan
>>>
>>>
>>> *yvain.payen at tessi.fr <mailto:yvain.payen at tessi.fr> www.tessi.eu
>>> <www.tessi.eu>
>>> ***
>>> Pensez ? l'environnement avant d'imprimer cet e-mail.**
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
> 



From yvain.payen at tessi.fr  Mon Feb  5 09:38:11 2024
From: yvain.payen at tessi.fr (Yvain PAYEN)
Date: Mon, 5 Feb 2024 09:38:11 +0000
Subject: [squid-users] external icap issue with squid 5 and higher
In-Reply-To: <8a28ed63-7bec-4d87-89f1-71b010b3235b@measurement-factory.com>
References: <DB9PR07MB9809EEA844CB5E49E9B48E079C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <df73225d-d576-4c01-89c0-f1f63bf88e25@measurement-factory.com>
 <DB9PR07MB9809B35CC2CA5B48794626169C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <28771f6c-7ba7-4376-9ae9-cdcfcadaa5f3@measurement-factory.com>
 <DB9PR07MB9809A70EBB9433723BB45BE39C422@DB9PR07MB9809.eurprd07.prod.outlook.com>
 <8a28ed63-7bec-4d87-89f1-71b010b3235b@measurement-factory.com>
Message-ID: <DB9PR07MB9809F5E68F411A1C1C5E240C9C472@DB9PR07MB9809.eurprd07.prod.outlook.com>

Hello Alex,

Thank you so much for your time and this detailed analysis.

We will get back to the Forcepoint editor and ask for a correction.

Best regards,

Yvain PAYEN

P?le Op?rations & Technologies
Equipe Infrastructure syst?me
T. +33 (0)5 57 57 01 85 (Poste 1185)
M. +33 (0)7 87 30 34 01

Absent tous les mercredi

Tessi France
Immeuble Cassiop?e
1-3 avenue des Satellites
33185 Le Haillan

Pensez ? l'environnement avant d'imprimer cet e-mail.

-----Message d'origine-----
De?: Alex Rousskov <rousskov at measurement-factory.com> 
Envoy??: samedi 3 f?vrier 2024 04:32
??: Yvain PAYEN <yvain.payen at tessi.fr>; squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] external icap issue with squid 5 and higher

? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le contenu est fiable.  ?



On 2024-02-02 17:36, Yvain PAYEN wrote:

> I just sent you a Onedrive link to 2 pcap files, one for http request 
> and one for https request.

Thank you. The ICAP service you are using is sending a malformed ICAP response to Squid. That ICAP response promises that there will be no HTTP body in the encapsulated HTTP message:

     Encapsulated: res-hdr=0, null-body=524

... but the service does send a body after HTTP headers. That HTTP body contains an HTML resource explaining that the CONNECT message was blocked and redirecting the user to blockpage.cgi, but that content does not really matter here. What matters is that there are some bytes after the encapsulated HTTP header. There should be no such bytes (or the ICAP Encapsulated header should have res-body=184 instead of null-body=524).

The null-body offset in the ICAP Encapsulated header is wrong. It should be 184 bytes (i.e. the size of the encapsulated HTTP response header), not 524 bytes. FWIW, 524 is the sum of the encapsulated HTTP response header (184 bytes) and the encapsulated HTTP response body (340 bytes).
It sounds like the ICAP service thinks that it is encapsulating an HTTP response header, but it is actually encapsulating the whole HTTP response.

Since this is an ICAP framing error(s), Squid rejects the whole transaction and bypasses the ICAP service (as configured).

To fix this, fix the ICAP service configuration (or code).

It is also possible to modify Squid code to ignore these errors, but I do not recommend that, and a hard-coded or rigid tolerance code like that should not be accepted by the Squid Project for the official inclusion.


The ICAP response in the "http request" capture does not have this problem. It contains an encapsulated HTTP 302 Moved header without any encapsulated HTTP body. That encapsulation matches the ICAP Encapsulated header.


HTH,

Alex.



> -----Message d'origine-----
> De : Alex Rousskov <rousskov at measurement-factory.com>
> Envoy? : vendredi 2 f?vrier 2024 18:45 ? : Yvain PAYEN 
> <yvain.payen at tessi.fr>; squid-users at lists.squid-cache.org Objet : Re: 
> [squid-users] external icap issue with squid 5 and higher
>
> ? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez 
> pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le 
> contenu est fiable.  ?
>
>
>
> On 2024-02-02 12:04, Yvain PAYEN wrote:
>
>> We don't use ssl_bump, icap service only analyze HTTP CONNECT 
>> requests
>
> Great, that simplifies things a lot.
>
>
>> Adaptation::Icap::Xaction::noteCommRead threw exception: > check
>> failed: readBuf.isEmpty()> exception location: ModXact.cc(1219)
> stopParsing
>
> It looks like Squid found some leftovers after the ICAP response that Squid (thought it) had fully parsed. I do not yet know whether that ICAP response was malformed or Squid is buggy.
>
> Can you share raw ICAP response bytes (preferrably in libpcap or similar raw packet capture format) collected by tcpdump, wireshark, or a similar tool? You can obfuscate/convert that ICAP response to text as needed, but if those extra bytes get lost in those conversions, then we would not be able to tell what those bytes are (e.g., they may contain whitespace characters that get easily lost).
>
>
> Thank you,
>
> Alex.
>
>
>>        2024/02/02 17:40:41.943 kid1| 93,3| ModXact.cc(679) callException: bypassing 0x558f358fdae0*2 exception: check failed: readBuf.isEmpty()
>>                exception location: ModXact.cc(1219) stopParsing  [FD 17;rp(1)S(2)YG/Rw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(720) disableBypass: will never start bypass because already started to bypass
>>        2024/02/02 17:40:41.943 kid1| 93,5| Xaction.cc(127) disableRepeats: Adaptation::Icap::ModXact still cannot be repeated because preparing to echo content [FD    17;rp(1)S(2)G/Rw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(724) disableBypass: not protecting group bypass because preparing to echo content
>>        2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_SAT to ICAP_ECHO
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(962) prepEchoing: cloning virgin message 0x558f358ff040
>>        2024/02/02 17:40:41.943 kid1| 93,3| Xaction.cc(564) setOutcome: WARNING: resetting outcome: from ICAP_ECHO to ICAP_ERR_OTHER
>>        2024/02/02 17:40:41.943 kid1| 93,4| ServiceRep.cc(97) noteFailure:  failure 1 out of 10 allowed in 0sec [up,fail1]
>>        2024/02/02 17:40:41.943 kid1| 93,2| AsyncJob.cc(130) callException: check failed: !adapted.header
>>                 exception location: ModXact.cc(971) prepEchoing
>>        2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(85) mustStop: Adaptation::Icap::ModXact will stop, reason: exception
>>        2024/02/02 17:40:41.943 kid1| 93,5| AsyncJob.cc(140) callEnd: Adaptation::Icap::Xaction::noteCommRead(conn8 local=X.X.X.X:46704 remote=X.X.X.X:1344 FD  17 flags=1, data=0x558f358fe888) ends job [FD 17;rp(1)S(2)/StoppedRw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,5| ModXact.cc(1295) swanSong: swan sings [FD 17;rp(1)S(2)/StoppedRw job17]
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(616) stopSending: Enter stop sending
>>        2024/02/02 17:40:41.943 kid1| 93,7| ModXact.cc(619) stopSending:
>> Proceed with stop sending
>>
>> It seems to bypass because something gone wrong.
>>
>> Yvain PAYEN
>>
>> P?le Op?rations & Technologies
>> Equipe Infrastructure syst?me
>> T. +33 (0)5 57 57 01 85 (Poste 1185)
>> M. +33 (0)7 87 30 34 01
>>
>> Absent tous les mercredi
>>
>> Tessi France
>> Immeuble Cassiop?e
>> 1-3 avenue des Satellites
>> 33185 Le Haillan
>>
>> Pensez ? l'environnement avant d'imprimer cet e-mail.
>>
>> -----Message d'origine-----
>> De : squid-users <squid-users-bounces at lists.squid-cache.org> De la 
>> part de Alex Rousskov Envoy? : vendredi 2 f?vrier 2024 17:19 ? :
>> squid-users at lists.squid-cache.org Objet : Re: [squid-users] external 
>> icap issue with squid 5 and higher
>>
>> ? FR : Ce message provient de l'ext?rieur de l'organisation. N'ouvrez 
>> pas de liens ou de pi?ces jointes ? moins que vous ne sachiez que le 
>> contenu est fiable.  ?
>>
>>
>>
>> On 2024-02-02 11:00, Yvain PAYEN wrote:
>>> Hi Squid users,
>>>
>>> I have an issue with an external icap service I have to use (from 
>>> Forcepoint).
>>>
>>> This service is working great with squid v3 and v4.
>>>
>>> Starting v5 (v6 also tested) the service only work with plain text 
>>> http requests, all requests for https content are allowed even if 
>>> the website should be denied.
>>
>> Do you use ssl_bump rules to decode affected HTTPS traffic? Or is your service supposed to analyze plain HTTP CONNECT requests?
>>
>> With Squid v6, does your ICAP service actually receive expected "requests for https content" for analysis from Squid? Or does Squid allow them without contacting the ICAP service with those requests? You can check service logs and/or enable icap.log in Squid to answer these high-level questions (see icap_log).
>>
>>
>>> My first question is : do you know if a big change in the icap code 
>>> happened between v4 and v5 ?
>>
>> I do not recall, unfortunately; it was too long ago. Please keep in mind that your problems may not be triggered by ICAP code changes (if any).
>>
>>
>>> My second question : How can I trace only icap debug logs
>>
>> ICAP code uses debug section 93. See debug_options directive and docs/debug-sections.txt.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>> Service is setup like this :
>>>
>>> icap_service service_req reqmod_precache icap://10.1.1.1:1344/icap
>>> bypass=1
>>>
>>> Regards,
>>>
>>> *Yvain PAYEN*
>>>
>>> *
>>> **P?le Op?rations & Technologies
>>> *Equipe Infrastructure syst?me
>>> T. +33 (0)5 57 57 01 85 (Poste 1185)
>>>
>>> M. +33 (0)7 87 30 34 01
>>>
>>> Absent tous les mercredi
>>>
>>>
>>> Tessi France
>>> Immeuble Cassiop?e
>>>
>>> 1-3 avenue des Satellites
>>> 33185 Le Haillan
>>>
>>>
>>> *yvain.payen at tessi.fr <mailto:yvain.payen at tessi.fr> www.tessi.eu 
>>> <www.tessi.eu>
>>> ***
>>> Pensez ? l'environnement avant d'imprimer cet e-mail.**
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>


From rob at sput.nl  Mon Feb  5 16:01:29 2024
From: rob at sput.nl (Rob van der Putten)
Date: Mon, 5 Feb 2024 17:01:29 +0100
Subject: [squid-users] New Squid prefers IPv4
Message-ID: <upr0op$k9b$1@ciao.gmane.io>

Hi there


After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from 
61% to less then 1%.
Any ideas?


Regards,
Rob



From squid.org at bloms.de  Mon Feb  5 16:16:57 2024
From: squid.org at bloms.de (Dieter Bloms)
Date: Mon, 5 Feb 2024 17:16:57 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <upr0op$k9b$1@ciao.gmane.io>
References: <upr0op$k9b$1@ciao.gmane.io>
Message-ID: <20240205161657.ofbcchhi3sfcfd42@bloms.de>

Hello Rob,

On Mon, Feb 05, Rob van der Putten wrote:

> After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from 61% to
> less then 1%.
> Any ideas?

yes, since squid5 the happy eyeball algorithm as described in rfc 8305
is used.
If your ipv4 connectivity is better than ipv6 than ipv4 is used.

-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From rob at sput.nl  Mon Feb  5 16:32:51 2024
From: rob at sput.nl (Rob van der Putten)
Date: Mon, 5 Feb 2024 17:32:51 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <20240205161657.ofbcchhi3sfcfd42@bloms.de>
References: <upr0op$k9b$1@ciao.gmane.io>
 <20240205161657.ofbcchhi3sfcfd42@bloms.de>
Message-ID: <upr2jj$nm1$1@ciao.gmane.io>

Hi there


On 05/02/2024 17:16, Dieter Bloms wrote:

> On Mon, Feb 05, Rob van der Putten wrote:
> 
>> After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from 61% to
>> less then 1%.
>> Any ideas?
> 
> yes, since squid5 the happy eyeball algorithm as described in rfc 8305
> is used.
> If your ipv4 connectivity is better than ipv6 than ipv4 is used.

I'm not quite sure how this is established. It prefers IPv4 even when 
the IPv6 ping is slightly smaller.


Regards,
Rob




From Antony.Stone at squid.open.source.it  Mon Feb  5 17:32:43 2024
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 5 Feb 2024 18:32:43 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <upr2jj$nm1$1@ciao.gmane.io>
References: <upr0op$k9b$1@ciao.gmane.io>
 <20240205161657.ofbcchhi3sfcfd42@bloms.de> <upr2jj$nm1$1@ciao.gmane.io>
Message-ID: <202402051832.43200.Antony.Stone@squid.open.source.it>

On Monday 05 February 2024 at 17:32:51, Rob van der Putten wrote:

> Hi there
> 
> On 05/02/2024 17:16, Dieter Bloms wrote:
> > On Mon, Feb 05, Rob van der Putten wrote:
> >> After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from
> >> 61% to less then 1%.
> >> Any ideas?
> > 
> > yes, since squid5 the happy eyeball algorithm as described in rfc 8305
> > is used.
> > If your ipv4 connectivity is better than ipv6 than ipv4 is used.
> 
> I'm not quite sure how this is established. It prefers IPv4 even when
> the IPv6 ping is slightly smaller.

I believe ping (ICMP) timings are irrelevant.  The client (squid in this case) 
does a DNS lookup for the hostname's A and AAAA records, then makes two 
simultaneous HTTP connections to the server (one IPv4, on IPv6) and whichever 
one responds first *by HTTP* is then regarded as being the best way to route 
traffic thereafter.

So, if you want to understand how this is doing what it is, I suggest you 
perform a packet capture of HTTP traffic and look at the requests and the 
response timings.


Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Mon Feb  5 18:17:31 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 5 Feb 2024 13:17:31 -0500
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <upr2jj$nm1$1@ciao.gmane.io>
References: <upr0op$k9b$1@ciao.gmane.io>
 <20240205161657.ofbcchhi3sfcfd42@bloms.de> <upr2jj$nm1$1@ciao.gmane.io>
Message-ID: <c1057249-ef91-4c13-a231-ea700b558cdd@measurement-factory.com>

On 2024-02-05 11:32, Rob van der Putten wrote:
> On 05/02/2024 17:16, Dieter Bloms wrote:
>> On Mon, Feb 05, Rob van der Putten wrote:
>>> After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from 
>>> 61% to less then 1%. Any ideas?
>>
>> yes, since squid5 the happy eyeball algorithm as described in rfc 8305
>> is used. If your ipv4 connectivity is better than ipv6 than ipv4 is used.
> 
> I'm not quite sure how this is established.

See RFC 8305 for the general approach, search squid.conf.documented for 
"Happy Eyeballs" to find relevant configuration directives, and see the 
following Squid commit message for a subset of Squid implementation caveats:

https://github.com/squid-cache/squid/commit/5562295321debdf33b59f772bce846bf6dd33c26


Antony is correct that ICMP is pretty much irrelevant here. A brief 
algorithm description in Antony's response is easy to misinterpret, but 
it can be used as a rough approximation of what is actually going on.

AFAICT, we do not have a good understanding of how the implemented 
algorithm actually behaves in various deployment environments. If you 
believe your IPv6 connectivity is better than your IPv4 connectivity, 
you may want to investigate why your Squid favors IPv4.


HTH,

Alex.



From speedy67 at chez.com  Mon Feb  5 21:33:20 2024
From: speedy67 at chez.com (speedy67)
Date: Mon, 5 Feb 2024 22:33:20 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <c1057249-ef91-4c13-a231-ea700b558cdd@measurement-factory.com>
References: <upr0op$k9b$1@ciao.gmane.io>
 <20240205161657.ofbcchhi3sfcfd42@bloms.de> <upr2jj$nm1$1@ciao.gmane.io>
 <c1057249-ef91-4c13-a231-ea700b558cdd@measurement-factory.com>
Message-ID: <f5d09734-7fb2-553f-01c3-bf7edacd40a6@chez.com>

Spam detection software, running on the system "master.squid-cache.org",
has identified this incoming email as possible spam.  The original
message has been attached to this so you can view it or label
similar future email.  If you have any questions, see
the administrator of that system for details.

Content preview:  Hello, I'm using Squid 3.5.24 (indluded in Synology DSM 6)
   and I've an issue with time acl. All works fine except some websites like
   myhordes.de. Once the user connected to this kind of website, the time acl
   [...] 

Content analysis details:   (5.5 points, 5.0 required)

 pts rule name              description
---- ---------------------- --------------------------------------------------
 3.6 RCVD_IN_PBL            RBL: Received via a relay in Spamhaus PBL
                            [82.64.172.59 listed in zen.spamhaus.org]
 0.0 SPF_HELO_NONE          SPF: HELO does not publish an SPF Record
 0.7 SPF_NEUTRAL            SPF: sender does not match SPF record (neutral)
-0.0 T_SCC_BODY_TEXT_LINE   No description available.
 1.3 RDNS_NONE              Delivered to internal network by a host with no rDNS
 0.0 UNPARSEABLE_RELAY      Informational: message has unparseable relay
                            lines
-0.0 NICE_REPLY_A           Looks like a legit reply (A)


-------------- next part --------------
An embedded message was scrubbed...
From: speedy67 <speedy67 at chez.com>
Subject: Re: [squid-users] New Squid prefers IPv4
Date: Mon, 5 Feb 2024 22:33:20 +0100
Size: 1579
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240205/e4192c67/attachment.eml>

From rob at sput.nl  Tue Feb  6 15:16:24 2024
From: rob at sput.nl (Rob van der Putten)
Date: Tue, 6 Feb 2024 16:16:24 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <202402051832.43200.Antony.Stone@squid.open.source.it>
References: <upr0op$k9b$1@ciao.gmane.io>
 <20240205161657.ofbcchhi3sfcfd42@bloms.de> <upr2jj$nm1$1@ciao.gmane.io>
 <202402051832.43200.Antony.Stone@squid.open.source.it>
Message-ID: <uptig8$qs$1@ciao.gmane.io>

Hi there


On 05/02/2024 18:32, Antony Stone wrote:

> On Monday 05 February 2024 at 17:32:51, Rob van der Putten wrote:
> 
>>
>> On 05/02/2024 17:16, Dieter Bloms wrote:
>>> On Mon, Feb 05, Rob van der Putten wrote:
>>>> After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from
>>>> 61% to less then 1%.
>>>> Any ideas?
>>>
>>> yes, since squid5 the happy eyeball algorithm as described in rfc 8305
>>> is used.
>>> If your ipv4 connectivity is better than ipv6 than ipv4 is used.
>>
>> I'm not quite sure how this is established. It prefers IPv4 even when
>> the IPv6 ping is slightly smaller.
> 
> I believe ping (ICMP) timings are irrelevant.  The client (squid in this case)
> does a DNS lookup for the hostname's A and AAAA records,

A before AAAA. Bind responds within the same millisecond.

> then makes two
> simultaneous HTTP connections to the server (one IPv4, on IPv6) and whichever
> one responds first *by HTTP* is then regarded as being the best way to route
> traffic thereafter.

I do not see Squid opening two connections simultaneously and then 
closing one. It's just one connection.

> So, if you want to understand how this is doing what it is, I suggest you
> perform a packet capture of HTTP traffic and look at the requests and the
> response timings.


Regards,
Rob





From Antony.Stone at squid.open.source.it  Tue Feb  6 15:24:34 2024
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 6 Feb 2024 16:24:34 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <uptig8$qs$1@ciao.gmane.io>
References: <upr0op$k9b$1@ciao.gmane.io>
 <202402051832.43200.Antony.Stone@squid.open.source.it>
 <uptig8$qs$1@ciao.gmane.io>
Message-ID: <202402061624.34300.Antony.Stone@squid.open.source.it>

On Tuesday 06 February 2024 at 16:16:24, Rob van der Putten wrote:

> Hi there
> 
> On 05/02/2024 18:32, Antony Stone wrote:
> > 
> > I believe ping (ICMP) timings are irrelevant.  The client (squid in this
> > case) does a DNS lookup for the hostname's A and AAAA records,
> 
> A before AAAA. Bind responds within the same millisecond.

I think the simultaneity of these lookups is unimportant.

> > then makes two simultaneous HTTP connections to the server (one IPv4, on
> > IPv6) and whichever one responds first *by HTTP* is then regarded as being
> > the best way to route traffic thereafter.
> 
> I do not see Squid opening two connections simultaneously and then
> closing one. It's just one connection.

Are you sure this is not because Squid has already made earlier connections to 
this name, decided that IPv4 is better, and continues to use that when you are 
now testing it?

I would expect you to have to start from an "undecided" Squid setup (I have no 
idea where it keeps this informatin for later use, though) to find out whether 
this is what's going on.


Antony.

-- 
1960s: Let's build a network which can withstand a nuclear war!
1970s: Hm, that looks good, we'll run it on TCP/IP.
1980s: Nice, how about letting everyone join?
1990s: Hey, you can make money out of this!
2000s: Oh, you can lose it, too.
2010s: Alright, let's just plug absolutely everything into it.
2020s: Meh, my lightswitch is now connected to my lamp via China.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Tue Feb  6 16:39:24 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 6 Feb 2024 11:39:24 -0500
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <uptig8$qs$1@ciao.gmane.io>
References: <upr0op$k9b$1@ciao.gmane.io>
 <20240205161657.ofbcchhi3sfcfd42@bloms.de> <upr2jj$nm1$1@ciao.gmane.io>
 <202402051832.43200.Antony.Stone@squid.open.source.it>
 <uptig8$qs$1@ciao.gmane.io>
Message-ID: <e6200c67-c41e-421a-a5f3-e4139618e295@measurement-factory.com>

On 2024-02-06 10:16, Rob van der Putten wrote:
> On 05/02/2024 18:32, Antony Stone wrote:
> 
>> On Monday 05 February 2024 at 17:32:51, Rob van der Putten wrote:
>>
>>>
>>> On 05/02/2024 17:16, Dieter Bloms wrote:
>>>> On Mon, Feb 05, Rob van der Putten wrote:
>>>>> After upgrading Squid from 3 to 5 the percentage of IPv6 reduced from
>>>>> 61% to less then 1%.
>>>>> Any ideas?
>>>>
>>>> yes, since squid5 the happy eyeball algorithm as described in rfc 8305
>>>> is used.
>>>> If your ipv4 connectivity is better than ipv6 than ipv4 is used.
>>>
>>> I'm not quite sure how this is established. It prefers IPv4 even when
>>> the IPv6 ping is slightly smaller.
>>
>> I believe ping (ICMP) timings are irrelevant.? The client (squid in 
>> this case) does a DNS lookup for the hostname's A and AAAA records,
> 
> A before AAAA. Bind responds within the same millisecond.

If Squid sends two DNS queries, then the first DNS answer seen/processed 
by Squid will normally trigger the first (called "primary") TCP 
connection establishment attempt. A "spare" connection attempt may or 
may not happen a bit later. DNS cache and persistent connections may 
play their natural role.


>> then makes two
>> simultaneous HTTP connections to the server (one IPv4, on IPv6) and 
>> whichever
>> one responds first *by HTTP* is then regarded as being the best way to 
>> route traffic thereafter.

> I do not see Squid opening two connections simultaneously and then 
> closing one. It's just one connection.

What you see matches Squid code (and the Happy Eyeballs RFC/intent). As 
I said in my earlier response, it is easy to misinterpret Antony's 
high0-level summary. Please do not use it for low-level triage. See my 
response for details.


HTH,

Alex.



From rob at sput.nl  Tue Feb  6 20:18:31 2024
From: rob at sput.nl (Rob van der Putten)
Date: Tue, 6 Feb 2024 21:18:31 +0100
Subject: [squid-users] New Squid prefers IPv4
In-Reply-To: <202402061624.34300.Antony.Stone@squid.open.source.it>
References: <upr0op$k9b$1@ciao.gmane.io>
 <202402051832.43200.Antony.Stone@squid.open.source.it>
 <uptig8$qs$1@ciao.gmane.io>
 <202402061624.34300.Antony.Stone@squid.open.source.it>
Message-ID: <upu46n$ko1$1@ciao.gmane.io>

Hi there


On 06/02/2024 16:24, Antony Stone wrote:

> On Tuesday 06 February 2024 at 16:16:24, Rob van der Putten wrote:
> 
>>
>> On 05/02/2024 18:32, Antony Stone wrote:
>>>
>>> I believe ping (ICMP) timings are irrelevant.  The client (squid in this
>>> case) does a DNS lookup for the hostname's A and AAAA records,
>>
>> A before AAAA. Bind responds within the same millisecond.
> 
> I think the simultaneity of these lookups is unimportant.
> 
>>> then makes two simultaneous HTTP connections to the server (one IPv4, on
>>> IPv6) and whichever one responds first *by HTTP* is then regarded as being
>>> the best way to route traffic thereafter.
>>
>> I do not see Squid opening two connections simultaneously and then
>> closing one. It's just one connection.
> 
> Are you sure this is not because Squid has already made earlier connections to
> this name, decided that IPv4 is better, and continues to use that when you are
> now testing it?
> 
> I would expect you to have to start from an "undecided" Squid setup (I have no
> idea where it keeps this informatin for later use, though) to find out whether
> this is what's going on.

I think i finally have figured this out.
The answer is more or less in netdb.state. I couldn't find the file 
format, so I lookup it up in the source (net_db.cc). It's;

network
pings_sent
pings_recv
hops
rtt
next_ping_time
last_use_time
host name(s)

Squid does send pings. But only once per five minutes per host. Both 
IPv4 and IPv6 (I checked with tcpdump). And then decides on the ping 
results. At least, that's what the contents of netdb.state suggests.


Regards,
Rob




From misho.98118 at gmail.com  Tue Feb  6 20:34:22 2024
From: misho.98118 at gmail.com (Miha Miha)
Date: Tue, 6 Feb 2024 22:34:22 +0200
Subject: [squid-users] Is Squid 6 production ready?
In-Reply-To: <134057a7-fbbe-442f-8bc7-e86504dcdf3f@treenet.co.nz>
References: <CABZyDWHB_Jj3wYF242WSOvHQQop2RmpAH-_1+ho+Cr5VvRBG0Q@mail.gmail.com>
 <d8faeb35-9b42-49ab-9862-c304d6a09a3b@treenet.co.nz>
 <CABZyDWEtZzM5vLZX8u79VeUjeei_4n-N+rLBuCcsLi0yyNmFvQ@mail.gmail.com>
 <134057a7-fbbe-442f-8bc7-e86504dcdf3f@treenet.co.nz>
Message-ID: <CABZyDWHbtGSA8MLsoqRuxK3_i5YcvNrSVNQniXjroAaxc6dxRA@mail.gmail.com>

> >> On 10/01/24 12:18, Miha Miha wrote:
> >>> Release note of latest Squid 6.6 says: "...not deemed ready for
> >>> production use..."  For comparison Squid 5.1 was 'ready'. When v6 is
> >>> expected to be ready for prod systems?
> >>
> >> On Fri, Jan 12, 2024 at 3:37?PM Amos Jeffries wrote:
> >> Sorry, that is an oversight in the release notes text. Removing it now.
> >>
> >> Squid 6 is production ready.
> >
> > On 1/02/24 11:22, Miha Miha wrote:
> > Hi Amos,
> > I still see the 6.6 release note unchanged. Could you please adjust.
> >
>On Thu, Feb 1, 2024 at 6:50?AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> The page is auto-generated from the release source code. It is too late
> to change the 6.6 package. The documentation has been updated already
> for 6.7 release.
>
> Cheers
> Amos

Hi Amos,

FYI new release note still says: "...release of Squid-6.7 for testing."

Sorry for being so punctual,
Mihail

On Thu, Feb 1, 2024 at 6:50?AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> On 1/02/24 11:22, Miha Miha wrote:
> >> On 10/01/24 12:18, Miha Miha wrote:
> >>> Release note of latest Squid 6.6 says: "...not deemed ready for
> >>> production use..."  For comparison Squid 5.1 was 'ready'. When v6 is
> >>> expected to be ready for prod systems?
> >>
> >> On Fri, Jan 12, 2024 at 3:37?PM Amos Jeffries wrote:
> >> Sorry, that is an oversight in the release notes text. Removing it now.
> >>
> >> Squid 6 is production ready.
> >
> > Hi Amos,
> > I still see the 6.6 release note unchanged. Could you please adjust.
> >
>
>
> The page is auto-generated from the release source code. It is too late
> to change the 6.6 package. The documentation has been updated already
> for 6.7 release.
>
> Cheers
> Amos


From misho.98118 at gmail.com  Tue Feb  6 21:17:54 2024
From: misho.98118 at gmail.com (Miha Miha)
Date: Tue, 6 Feb 2024 23:17:54 +0200
Subject: [squid-users] Can't verify the signature of squid-6.7.tar.gz
Message-ID: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>

Hi there,

When I try to verify the authenticity of squid-6.7.tar.gz file I get:

gpg: Signature made Sun 04 Feb 2024 08:22:04 PM EET using RSA key ID F48F8EAB
gpg: Can't check signature: No public key

I don't see a key with ID F48F8EAB in the public chain in pgp.asc file
Could you please double check?

Regards,
Mihail


From gkinkie at gmail.com  Tue Feb  6 22:55:00 2024
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 6 Feb 2024 22:55:00 +0000
Subject: [squid-users] Can't verify the signature of squid-6.7.tar.gz
In-Reply-To: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>
References: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>
Message-ID: <CA+Y8hcPOxq3_R+3kCL6Pv0ov3jCe15u7xS8VaUcMtu6AuXBr6g@mail.gmail.com>

Hi,
  apologies, that's on me, I had uploaded the wrong version of a file. It
should be okay now, can you recheck?


On Tue, 6 Feb 2024 at 21:18, Miha Miha <misho.98118 at gmail.com> wrote:

> Hi there,
>
> When I try to verify the authenticity of squid-6.7.tar.gz file I get:
>
> gpg: Signature made Sun 04 Feb 2024 08:22:04 PM EET using RSA key ID
> F48F8EAB
> gpg: Can't check signature: No public key
>
> I don't see a key with ID F48F8EAB in the public chain in pgp.asc file
> Could you please double check?
>
> Regards,
> Mihail
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240206/e4921b60/attachment.htm>

From misho.98118 at gmail.com  Wed Feb  7 13:19:42 2024
From: misho.98118 at gmail.com (Miha Miha)
Date: Wed, 7 Feb 2024 15:19:42 +0200
Subject: [squid-users] Can't verify the signature of squid-6.7.tar.gz
In-Reply-To: <CA+Y8hcPOxq3_R+3kCL6Pv0ov3jCe15u7xS8VaUcMtu6AuXBr6g@mail.gmail.com>
References: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>
 <CA+Y8hcPOxq3_R+3kCL6Pv0ov3jCe15u7xS8VaUcMtu6AuXBr6g@mail.gmail.com>
Message-ID: <CABZyDWH0EJ2vnxjaDbwwJ8vP4k7uGMvtpcScOmgQnfCLzQFpFQ@mail.gmail.com>

Hi Francesco,

I still get an issue, although a slightly different one:

#gpg --verify squid-6.7.tar.gz.asc squid-6.7.tar.gz
gpg: Signature made Tue 06 Feb 2024 10:51:28 PM EET using ? key ID FEF6E865
gpg: Can't check signature: Invalid public key algorithm

When I try to import the public keys (pgp.asc file) I see:

#gpg --import pgp.asc

...
gpg: key FEF6E865: no valid user IDs
gpg: this may be caused by a missing self-signature
...

All the rest keys have an user and e-mail.

When I list the imported pub keys with   gpg --list-keys I see
multiple keys, but not the FEF6E865

May be the pub key hasn't been properly imported?

Regards,
Mihail

On Wed, Feb 7, 2024 at 12:55?AM Francesco Chemolli <gkinkie at gmail.com> wrote:
>
> Hi,
>   apologies, that's on me, I had uploaded the wrong version of a file. It should be okay now, can you recheck?
>
>
> On Tue, 6 Feb 2024 at 21:18, Miha Miha <misho.98118 at gmail.com> wrote:
>>
>> Hi there,
>>
>> When I try to verify the authenticity of squid-6.7.tar.gz file I get:
>>
>> gpg: Signature made Sun 04 Feb 2024 08:22:04 PM EET using RSA key ID F48F8EAB
>> gpg: Can't check signature: No public key
>>
>> I don't see a key with ID F48F8EAB in the public chain in pgp.asc file
>> Could you please double check?
>>
>> Regards,
>> Mihail
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users


From robin.carlisle at framestore.com  Wed Feb  7 18:45:22 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Wed, 7 Feb 2024 18:45:22 +0000
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
Message-ID: <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>

Hi,

I have just started my enhanced logging journey and have a small snippet
below that might illuminate the issue ...

*2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507)
handleIMSReply: origin replied with error 502, forwarding to client due to
fail_on_validation_err*

A few lines below in the log it looks like squid sent :-















*2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
HTTP Client REPLY:---------HTTP/1.1 502 Bad GatewayServer:
squid/5.7Mime-Version: 1.0Date: Wed, 07 Feb 2024 17:06:39 GMTContent-Type:
text/html;charset=utf-8Content-Length: 3853X-Squid-Error: ERR_READ_ERROR
0Vary: Accept-LanguageContent-Language: enX-Cache: MISS from
labs-maul-st-15X-Cache-Lookup: HIT from labs-maul-st-15:3129Via: 1.1
labs-maul-st-15 (squid/5.7)Connection: close*


The rest of the logs are quite large and contain URLs I cannot put here.
 The logs were generated with debug_options to ALL,3.

Any ideas?   Or should I generate more detailed logs and send them
privately?

Thanks again,

Robin




On Fri, 2 Feb 2024 at 11:20, Robin Carlisle <robin.carlisle at framestore.com>
wrote:

> Hi, thanks for your reply.
>
> I have been looking at :
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control
>
>
>
>
>
>
> *The stale-if-error response directive indicates that the cache can reuse
> a stale response when an upstream server generates an error, or when the
> error is generated locally. Here, an error is considered any response with
> a status code of 500, 502, 503, or 504.Cache-Control: max-age=604800,
> stale-if-error=86400In the example above, the response is fresh for 7 days
> (604800s). Afterwards, it becomes stale, but can be used for an extra 1 day
> (86400s) when an error is encountered.After the stale-if-error period
> passes, the client will receive any error generated*
>
> Given what you have said and what the above docs say - I am still confused
> as it looks like (in my test cases) the cached response can be used for
> 3600 secs (this works), after which the cached response can still be used
> for an additional 31536000 seconds on an error (this doesnt work).
>
> I am going to dig into the error logging you suggested to see if I can
> make sense of that - and will send on if I can't.
>
> Thanks v much for your help again,
>
> Robin
>
>
>
>
>
> On Thu, 1 Feb 2024 at 18:27, Alex Rousskov <
> rousskov at measurement-factory.com> wrote:
>
>> On 2024-02-01 12:03, Robin Carlisle wrote:
>> > Hi, I am having trouble with stale-if-error response.
>>
>> If I am interpreting Squid code correctly, in primary use cases:
>>
>> * without a Cache-Control:stale-if-error=X in the original response,
>> Squid sends a stale object if revalidation results in a 5xx error;
>>
>> * with a Cache-Control:stale-if-error=X and object age at most X, Squid
>> sends a stale object if revalidation results in a 5xx error;
>>
>> * with a Cache-Control:stale-if-error=X and object age exceeding X,
>> Squid forwards the 5xx error response if revalidation results in a 5xx
>> error;
>>
>> In other words, stale-if-error=X turns on a "fail on validation errors"
>> behavior for stale objects older than X. It has no other effects.
>>
>> In your test case, the stale objects are much younger than
>> stale-if-error value (e.g., Age~=3601 vs. stale-if-error=31536000).
>> Thus, stale-if-error should have no relevant effect.
>>
>> Something else is probably preventing your Squid from serving the stale
>> response when facing a 5xx error. I do not know what that something is.
>>
>> I recommend sharing (privately if you need to protect sensitive info) a
>> pointer to a compressed ALL,9 cache.log collected while reproducing the
>> problem (using two transactions similar to the ones you have shared
>> below -- a successful stale hit and a problematic one):
>>
>> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
>>
>> Alternatively, you can try to study cache.log yourself after setting
>> debug_options to ALL,3. Searching for "refresh" and "handleIMSReply" may
>> yield enough clues.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>
>> > # /etc/squid/squid.conf :
>> >
>> > acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
>> >
>> > acl from_local src localhost
>> >
>> > http_access allow to_aws
>> >
>> > http_access allow from_local
>> >
>> > cache allow all
>> >
>> > cache_dir ufs /var/cache/squid 1024 16 256
>> >
>> > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
>> > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> >
>> > sslcrtd_program /usr/lib/squid/security_file_certgen -s
>> > /var/lib/squid/ssl_db -M 4MB
>> >
>> > acl step1 at_step SslBump1
>> >
>> > ssl_bump bump step1
>> >
>> > ssl_bump bump all
>> >
>> > sslproxy_cert_error deny all
>> >
>> > cache_store_log stdio:/var/log/squid/store.log
>> >
>> > logfile_rotate 0
>> >
>> > shutdown_lifetime 3 seconds
>> >
>> >
>> > # /usr/bin/proxy-test :
>> >
>> > #!/bin/bash
>> >
>> > curl --proxy http://localhost:3129 <http://localhost:3129> \
>> >
>> >    --cacert /etc/squid/stuff.pem \
>> >
>> >    -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>> > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>" \
>> >
>> >    -H "Authorization: token MYTOKEN" \
>> >
>> >    -H "Content-Type: application/json" \
>> >
>> >    --output "/tmp/stuff.json"
>> >
>> >
>> >
>> > Tests  ..........
>> >
>> >
>> > At this point in time the network cable is unattached.  Squid returns
>> > the cached object it got when the network was online earlier. The Age
>> of
>> > this object is just still under the max_age of 3600.     Previously I
>> > was using offline_mode but I found that it did not try to revalidate
>> > from the origin after the object expired (defined via max-age
>> response).
>> >    My understanding is that stale-if-error should work under my
>> > circumstances.
>> >
>> >
>> > # /var/log/squid/access.log
>> >
>> > 1706799404.440      6 127.0.0.1 NONE_NONE/200 0 CONNECT
>> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - HIER_NONE/-
>> -
>> >
>> > 1706799404.440      0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
>> > https://stuff.amazonaws.com/stuff.json
>> > <https://stuff.amazonaws.com/stuff.json> - HIER_NONE/- application/json
>> >
>> >
>> > # extract from /usr/bin/proxy-test
>> >
>> > < HTTP/1.1 200 OK
>> >
>> > < Date: Thu, 01 Feb 2024 13:57:11 GMT
>> >
>> > < Content-Type: application/json
>> >
>> > < Content-Length: 20134
>> >
>> > < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
>> >
>> > < Last-Modified: 2024-02-01T13:00:45.000Z
>> >
>> > < Access-Control-Allow-Origin: *
>> >
>> > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
>> >
>> > < Cache-Control: public, max-age=3600, stale-if-error=31536000
>> >
>> > < ETag: "cec102b43372840737ab773c2e77858b"
>> >
>> > < X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6
>> >
>> > < Age: 3573
>> >
>> > < X-Cache: HIT from labs-maul-st-31
>> >
>> > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>> >
>> > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>> >
>> > < Connection: keep-alive
>> >
>> >
>> >
>> >
>> > Below .. the curl script executes again.  The Age has gone over the
>> > max-age so squid attempted to refresh from the origin.  The machine is
>> > still offline so the refresh failed.   I expected that the
>> > stale-if-error response would instruct squid to return the cached
>> object
>> > as a 200.
>> >
>> >
>> > # /var/log/squid/access.log
>> >
>> > 1706799434.464      5 127.0.0.1 NONE_NONE/200 0 CONNECT
>> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - HIER_NONE/-
>> -
>> >
>> > 1706799434.464      0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235 GET
>> > https://stuff.amazonaws.com/stuff.json
>> > <https://stuff.amazonaws.com/stuff.json> - HIER_NONE/- text/html
>> >
>> >
>> > # extract from /usr/bin/proxy-test
>> >
>> > < HTTP/1.1 502 Bad Gateway
>> >
>> > < Server: squid/5.7
>> >
>> > < Mime-Version: 1.0
>> >
>> > < Date: Thu, 01 Feb 2024 14:57:14 GMT
>> >
>> > < Content-Type: text/html;charset=utf-8
>> >
>> > < Content-Length: 3853
>> >
>> > < X-Squid-Error: ERR_READ_ERROR 0
>> >
>> > < Vary: Accept-Language
>> >
>> > < Content-Language: en
>> >
>> > < X-Cache: MISS from labs-maul-st-31
>> >
>> > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>> >
>> > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>> >
>> > < Connection: close
>> >
>> >
>> >
>> > Hope someone can help me with this.  All the best,
>> >
>> >
>> > Robin Carlisle
>> >
>> >
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > https://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240207/f60b12ab/attachment.htm>

From rousskov at measurement-factory.com  Wed Feb  7 18:58:03 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 7 Feb 2024 13:58:03 -0500
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
Message-ID: <9916bad1-0422-48e0-befe-f902c7ee7228@measurement-factory.com>

On 2024-02-07 13:45, Robin Carlisle wrote:
> I have just started my enhanced logging journey and have a small snippet 
> below that might illuminate the issue ...
> 
> /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507) 
> handleIMSReply: origin replied with error 502, forwarding to client due 
> to fail_on_validation_err/

This confirms that Squid considers the cached response stale and, hence, 
applies the last bullet logic from my earlier summary ("object age 
exceeding X"). We still do not know why.


> should I generate more detailed logs and send them privately?

Privately sharing a pointer to the current (or, better, ALL,9) 
compressed logs while reproducing the problem is (still) the best way 
forward IMO.


Cheers,

Alex.


> On Fri, 2 Feb 2024 at 11:20, Robin Carlisle wrote:
> 
>     Hi, thanks for your reply.
> 
>     I have been looking at :
>     https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>
> 
>     /The stale-if-error response directive indicates that the cache can
>     reuse a stale response when an upstream server generates an error,
>     or when the error is generated locally. Here, an error is considered
>     any response with a status code of 500, 502, 503, or 504.
> 
>     Cache-Control: max-age=604800, stale-if-error=86400
>     In the example above, the response is fresh for 7 days (604800s).
>     Afterwards, it becomes stale, but can be used for an extra 1 day
>     (86400s) when an error is encountered.
> 
>     After the stale-if-error period passes, the client will receive any
>     error generated/
> 
>     Given what you have said and what the above docs say - I am still
>     confused as it looks like (in my test cases) the cached response can
>     be used for 3600 secs (this works), after which the cached response
>     can still be used for an additional 31536000 seconds on an error
>     (this doesnt work).
> 
>     I am going to dig into the error logging?you suggested to see if I
>     can make sense of that - and will send on if I can't.
> 
>     Thanks v much for your help again,
> 
>     Robin
> 
> 
> 
> 
> 
>     On Thu, 1 Feb 2024 at 18:27, Alex Rousskov wrote:
> 
>         On 2024-02-01 12:03, Robin Carlisle wrote:
>          > Hi, I am having trouble with stale-if-error response.
> 
>         If I am interpreting Squid code correctly, in primary use cases:
> 
>         * without a Cache-Control:stale-if-error=X in the original
>         response,
>         Squid sends a stale object if revalidation results in a 5xx error;
> 
>         * with a Cache-Control:stale-if-error=X and object age at most
>         X, Squid
>         sends a stale object if revalidation results in a 5xx error;
> 
>         * with a Cache-Control:stale-if-error=X and object age exceeding X,
>         Squid forwards the 5xx error response if revalidation results in
>         a 5xx
>         error;
> 
>         In other words, stale-if-error=X turns on a "fail on validation
>         errors"
>         behavior for stale objects older than X. It has no other effects.
> 
>         In your test case, the stale objects are much younger than
>         stale-if-error value (e.g., Age~=3601 vs. stale-if-error=31536000).
>         Thus, stale-if-error should have no relevant effect.
> 
>         Something else is probably preventing your Squid from serving
>         the stale
>         response when facing a 5xx error. I do not know what that
>         something is.
> 
>         I recommend sharing (privately if you need to protect sensitive
>         info) a
>         pointer to a compressed ALL,9 cache.log collected while
>         reproducing the
>         problem (using two transactions similar to the ones you have shared
>         below -- a successful stale hit and a problematic one):
>         https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
>         Alternatively, you can try to study cache.log yourself after
>         setting
>         debug_options to ALL,3. Searching for "refresh" and
>         "handleIMSReply" may
>         yield enough clues.
> 
> 
>         HTH,
> 
>         Alex.
> 
> 
> 
> 
>          > # /etc/squid/squid.conf :
>          >
>          > acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
>         <http://amazonaws.com <http://amazonaws.com>>
>          >
>          > acl from_local src localhost
>          >
>          > http_access allow to_aws
>          >
>          > http_access allow from_local
>          >
>          > cache allow all
>          >
>          > cache_dir ufs /var/cache/squid 1024 16 256
>          >
>          > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
>          > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>          >
>          > sslcrtd_program /usr/lib/squid/security_file_certgen -s
>          > /var/lib/squid/ssl_db -M 4MB
>          >
>          > acl step1 at_step SslBump1
>          >
>          > ssl_bump bump step1
>          >
>          > ssl_bump bump all
>          >
>          > sslproxy_cert_error deny all
>          >
>          > cache_store_log stdio:/var/log/squid/store.log
>          >
>          > logfile_rotate 0
>          >
>          > shutdown_lifetime 3 seconds
>          >
>          >
>          > # /usr/bin/proxy-test :
>          >
>          > #!/bin/bash
>          >
>          > curl --proxy http://localhost:3129 <http://localhost:3129>
>         <http://localhost:3129 <http://localhost:3129>> \
>          >
>          >? ??--cacert /etc/squid/stuff.pem \
>          >
>          >? ??-v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>          > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>" \
>          >
>          >? ??-H "Authorization: token MYTOKEN" \
>          >
>          >? ??-H "Content-Type: application/json" \
>          >
>          >? ??--output "/tmp/stuff.json"
>          >
>          >
>          >
>          > Tests? ..........
>          >
>          >
>          > At this point in time the network cable is unattached.? Squid
>         returns
>          > the cached object it got when the network was online earlier.
>         The Age of
>          > this object is just still under the max_age of 3600.    
>         Previously I
>          > was using offline_mode but I found that it did not try to
>         revalidate
>          > from the origin after the object expired (defined via max-age
>         response).
>          >? ? My understanding is that stale-if-error should work under my
>          > circumstances.
>          >
>          >
>          > # /var/log/squid/access.log
>          >
>          > 1706799404.440? ? ? 6 127.0.0.1 NONE_NONE/200 0 CONNECT
>          > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>
>         <http://stuff.amazonaws.com:443
>         <http://stuff.amazonaws.com:443>> - HIER_NONE/- -
>          >
>          > 1706799404.440? ? ? 0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
>          > https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>
>          > <https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>> - HIER_NONE/-
>         application/json
>          >
>          >
>          > # extract from /usr/bin/proxy-test
>          >
>          > < HTTP/1.1 200 OK
>          >
>          > < Date: Thu, 01 Feb 2024 13:57:11 GMT
>          >
>          > < Content-Type: application/json
>          >
>          > < Content-Length: 20134
>          >
>          > < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
>          >
>          > < Last-Modified: 2024-02-01T13:00:45.000Z
>          >
>          > < Access-Control-Allow-Origin: *
>          >
>          > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
>          >
>          > < Cache-Control: public, max-age=3600, stale-if-error=31536000
>          >
>          > < ETag: "cec102b43372840737ab773c2e77858b"
>          >
>          > < X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6
>          >
>          > < Age: 3573
>          >
>          > < X-Cache: HIT from labs-maul-st-31
>          >
>          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>          >
>          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>          >
>          > < Connection: keep-alive
>          >
>          >
>          >
>          >
>          > Below .. the curl script executes again.? The Age has gone
>         over the
>          > max-age so squid attempted to refresh from the origin.? The
>         machine is
>          > still offline so the refresh failed. ? I expected that the
>          > stale-if-error response would instruct squid to return the
>         cached object
>          > as a 200.
>          >
>          >
>          > # /var/log/squid/access.log
>          >
>          > 1706799434.464? ? ? 5 127.0.0.1 NONE_NONE/200 0 CONNECT
>          > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>
>         <http://stuff.amazonaws.com:443
>         <http://stuff.amazonaws.com:443>> - HIER_NONE/- -
>          >
>          > 1706799434.464? ? ? 0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235
>         GET
>          > https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>
>          > <https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>> - HIER_NONE/- text/html
>          >
>          >
>          > # extract from /usr/bin/proxy-test
>          >
>          > < HTTP/1.1 502 Bad Gateway
>          >
>          > < Server: squid/5.7
>          >
>          > < Mime-Version: 1.0
>          >
>          > < Date: Thu, 01 Feb 2024 14:57:14 GMT
>          >
>          > < Content-Type: text/html;charset=utf-8
>          >
>          > < Content-Length: 3853
>          >
>          > < X-Squid-Error: ERR_READ_ERROR 0
>          >
>          > < Vary: Accept-Language
>          >
>          > < Content-Language: en
>          >
>          > < X-Cache: MISS from labs-maul-st-31
>          >
>          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>          >
>          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>          >
>          > < Connection: close
>          >
>          >
>          >
>          > Hope someone can help me with this.? All the best,
>          >
>          >
>          > Robin Carlisle
>          >
>          >
>          >
>          > _______________________________________________
>          > squid-users mailing list
>          > squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>          > https://lists.squid-cache.org/listinfo/squid-users
>         <https://lists.squid-cache.org/listinfo/squid-users>
> 
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         https://lists.squid-cache.org/listinfo/squid-users
>         <https://lists.squid-cache.org/listinfo/squid-users>
> 



From squid3 at treenet.co.nz  Thu Feb  8 04:52:48 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Feb 2024 17:52:48 +1300
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
Message-ID: <24212877-baf5-4e01-bafd-ea0a6fc2457c@treenet.co.nz>

On 8/02/24 07:45, Robin Carlisle wrote:
> Hi,
> 
> I have just started my enhanced logging journey and have a small snippet 
> below that might illuminate the issue ...
> 
> /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507) 
> handleIMSReply: origin replied with error 502, forwarding to client due 
> to fail_on_validation_err/
> 

Please check the log for the earlier 22,3 line saying "checking 
freshness of URI: <test-URL>".

All the 22,3 lines between there and your found 88,3 line will tell the 
story of why refresh was done. That will give an hint about why the 
fail_on_validation_err flag was set.


Cheers
Amos


From squid3 at treenet.co.nz  Thu Feb  8 05:58:44 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Feb 2024 18:58:44 +1300
Subject: [squid-users] Can't verify the signature of squid-6.7.tar.gz
In-Reply-To: <CABZyDWH0EJ2vnxjaDbwwJ8vP4k7uGMvtpcScOmgQnfCLzQFpFQ@mail.gmail.com>
References: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>
 <CA+Y8hcPOxq3_R+3kCL6Pv0ov3jCe15u7xS8VaUcMtu6AuXBr6g@mail.gmail.com>
 <CABZyDWH0EJ2vnxjaDbwwJ8vP4k7uGMvtpcScOmgQnfCLzQFpFQ@mail.gmail.com>
Message-ID: <0e21989d-ab3e-472b-80fc-23be5aad9928@treenet.co.nz>


On 8/02/24 02:19, Miha Miha wrote:
> Hi Francesco,
> 
> I still get an issue, although a slightly different one:
> 
> #gpg --verify squid-6.7.tar.gz.asc squid-6.7.tar.gz
> gpg: Signature made Tue 06 Feb 2024 10:51:28 PM EET using ? key ID FEF6E865
> gpg: Can't check signature: Invalid public key algorithm
> 


The error mentions algorithm, so also check the ciphers/algorithms 
supported by your GPG agent. The new key uses the EDDSA cipher instead 
of typical RSA.



> When I try to import the public keys (pgp.asc file) I see:
> 
> #gpg --import pgp.asc
> 
> ...
> gpg: key FEF6E865: no valid user IDs
> gpg: this may be caused by a missing self-signature
> ...
> 
> All the rest keys have an user and e-mail.
> 
> When I list the imported pub keys with   gpg --list-keys I see
> multiple keys, but not the FEF6E865
> 
> May be the pub key hasn't been properly imported?
> 

Please check the contents of squid-6.7.tar.gz.asc. The full key ID 
should be provided there (FEF6E865 is one of its short-forms).

If you have any doubts about the keyring (pgp.asc file), you can try to 
fetch a fresh copy of it from <http://master.squid-cache.org/pgp.asc>



FTR; this is what I get working from a clean /tmp/squid pseudo-chroot 
directory to avoid my actual trusted+known keys:

## mkdir /tmp/squid

## wget http://master.squid-cache.org/pgp.asc

## gpg --homedir /tmp/squid --import pgp.asc
gpg: WARNING: unsafe permissions on homedir '/tmp/squid'
gpg: keybox '/tmp/squid/pubring.kbx' created
gpg: key B268E706FF5CF463: 1 duplicate signature removed
gpg: key B268E706FF5CF463: 4 signatures not checked due to missing keys
gpg: /tmp/squid/trustdb.gpg: trustdb created
gpg: key B268E706FF5CF463: public key "Amos Jeffries 
<amos at treenet.co.nz>" imported
gpg: key 4250AB432402F2F8: 1 signature not checked due to a missing key
gpg: key 4250AB432402F2F8: public key "Duane Wessels 
<wessels at squid-cache.org>" imported
gpg: key E75E90C039CC33DB: 202 signatures not checked due to missing keys
gpg: key E75E90C039CC33DB: public key "Henrik Nordstrom 
<henrik at henriknordstrom.net>" imported
gpg: key 867BF9A9FBD3EB8E: 605 signatures not checked due to missing keys
gpg: key 867BF9A9FBD3EB8E: public key "Robert Collins 
<robertc at robertcollins.net>" imported
gpg: key CD6DBF8EF3B17D3E: 1 signature not checked due to a missing key
gpg: key CD6DBF8EF3B17D3E: public key "Amos Jeffries (Squid Signing Key) 
<squid3 at treenet.co.nz>" imported
gpg: key 28F85029FEF6E865: public key "Francesco Chemolli (code signing 
key) <kinkie at squid-cache.org>" imported
gpg: key 3AEBEC6EC66648FD: public key "Francesco Chemolli (kinkie) 
<kinkie at kinkie.it>" imported
gpg: Total number processed: 7
gpg:               imported: 7
gpg: no ultimately trusted keys found

## wget http://master.squid-cache.org/Versions/v6/squid-6.7.tar.gz
## wget http://master.squid-cache.org/Versions/v6/squid-6.7.tar.gz.asc

## gpg --homedir /tmp/squid --verify squid-6.7.tar.gz.asc squid-6.7.tar.gz
gpg: WARNING: unsafe permissions on homedir '/tmp/squid'
gpg: Signature made Wed 07 Feb 2024 09:51:28 NZDT
gpg:                using EDDSA key 29B4B1F7CE03D1B1DED22F3028F85029FEF6E865
gpg: Good signature from "Francesco Chemolli (code signing key) 
<kinkie at squid-cache.org>" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the 
owner.
Primary key fingerprint: 29B4 B1F7 CE03 D1B1 DED2  2F30 28F8 5029 FEF6 E865



HTH
Amos


From rousskov at measurement-factory.com  Thu Feb  8 17:42:03 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 8 Feb 2024 12:42:03 -0500
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
Message-ID: <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>

Hi Robin,

     AFAICT from the logs you have privately shared and your squid.conf 
that you have posted earlier, your Squid overwrites 
stale-if-error=31536000 in the response with "refresh_pattern 
max-stale=0" default. That 0 value is wrong. The correct value should be 
taken from max_stale directive that defaults to 1 week, not zero:

     refresh_pattern
     ...
     max-stale=NN provide a maximum staleness factor. Squid won't
     serve objects more stale than this even if it failed to
     validate the object. Default: use the max_stale global limit.

This wrong default is a Squid bug AFAICT. I posted an _untested_ fix as 
Squid PR 1664: https://github.com/squid-cache/squid/pull/1664

If possible, please test the corresponding patch:
https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch

AFAICT, you can also work around that bug by configuring an explicit 
refresh_pattern rule with an explicit max-stale option (see 
squid.conf.documented for examples). I have not tested that theory either.


HTH,

Alex.


On 2024-02-07 13:45, Robin Carlisle wrote:
> Hi,
> 
> I have just started my enhanced logging journey and have a small snippet 
> below that might illuminate the issue ...
> 
> /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507) 
> handleIMSReply: origin replied with error 502, forwarding to client due 
> to fail_on_validation_err/
> 
> A few lines below in the log it looks like squid sent :-
> 
> /2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280) sendStartOfMessage: 
> HTTP Client REPLY:
> ---------
> HTTP/1.1 502 Bad Gateway
> Server: squid/5.7
> Mime-Version: 1.0
> Date: Wed, 07 Feb 2024 17:06:39 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3853
> X-Squid-Error: ERR_READ_ERROR 0
> Vary: Accept-Language
> Content-Language: en
> X-Cache: MISS from labs-maul-st-15
> X-Cache-Lookup: HIT from labs-maul-st-15:3129
> Via: 1.1 labs-maul-st-15 (squid/5.7)
> Connection: close/
> 
> 
> The rest of the logs are quite large and contain URLs I cannot put 
> here.? ?The logs were generated with debug_options to ALL,3.
> 
> Any ideas?? ?Or should I generate more detailed logs and send them 
> privately?
> 
> Thanks again,
> 
> Robin
> 
> 
> 
> 
> On Fri, 2 Feb 2024 at 11:20, Robin Carlisle 
> <robin.carlisle at framestore.com <mailto:robin.carlisle at framestore.com>> 
> wrote:
> 
>     Hi, thanks for your reply.
> 
>     I have been looking at :
>     https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>
> 
>     /The stale-if-error response directive indicates that the cache can
>     reuse a stale response when an upstream server generates an error,
>     or when the error is generated locally. Here, an error is considered
>     any response with a status code of 500, 502, 503, or 504.
> 
>     Cache-Control: max-age=604800, stale-if-error=86400
>     In the example above, the response is fresh for 7 days (604800s).
>     Afterwards, it becomes stale, but can be used for an extra 1 day
>     (86400s) when an error is encountered.
> 
>     After the stale-if-error period passes, the client will receive any
>     error generated/
> 
>     Given what you have said and what the above docs say - I am still
>     confused as it looks like (in my test cases) the cached response can
>     be used for 3600 secs (this works), after which the cached response
>     can still be used for an additional 31536000 seconds on an error
>     (this doesnt work).
> 
>     I am going to dig into the error logging?you suggested to see if I
>     can make sense of that - and will send on if I can't.
> 
>     Thanks v much for your help again,
> 
>     Robin
> 
> 
> 
> 
> 
>     On Thu, 1 Feb 2024 at 18:27, Alex Rousskov
>     <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>> wrote:
> 
>         On 2024-02-01 12:03, Robin Carlisle wrote:
>          > Hi, I am having trouble with stale-if-error response.
> 
>         If I am interpreting Squid code correctly, in primary use cases:
> 
>         * without a Cache-Control:stale-if-error=X in the original
>         response,
>         Squid sends a stale object if revalidation results in a 5xx error;
> 
>         * with a Cache-Control:stale-if-error=X and object age at most
>         X, Squid
>         sends a stale object if revalidation results in a 5xx error;
> 
>         * with a Cache-Control:stale-if-error=X and object age exceeding X,
>         Squid forwards the 5xx error response if revalidation results in
>         a 5xx
>         error;
> 
>         In other words, stale-if-error=X turns on a "fail on validation
>         errors"
>         behavior for stale objects older than X. It has no other effects.
> 
>         In your test case, the stale objects are much younger than
>         stale-if-error value (e.g., Age~=3601 vs. stale-if-error=31536000).
>         Thus, stale-if-error should have no relevant effect.
> 
>         Something else is probably preventing your Squid from serving
>         the stale
>         response when facing a 5xx error. I do not know what that
>         something is.
> 
>         I recommend sharing (privately if you need to protect sensitive
>         info) a
>         pointer to a compressed ALL,9 cache.log collected while
>         reproducing the
>         problem (using two transactions similar to the ones you have shared
>         below -- a successful stale hit and a problematic one):
>         https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
>         Alternatively, you can try to study cache.log yourself after
>         setting
>         debug_options to ALL,3. Searching for "refresh" and
>         "handleIMSReply" may
>         yield enough clues.
> 
> 
>         HTH,
> 
>         Alex.
> 
> 
> 
> 
>          > # /etc/squid/squid.conf :
>          >
>          > acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
>         <http://amazonaws.com <http://amazonaws.com>>
>          >
>          > acl from_local src localhost
>          >
>          > http_access allow to_aws
>          >
>          > http_access allow from_local
>          >
>          > cache allow all
>          >
>          > cache_dir ufs /var/cache/squid 1024 16 256
>          >
>          > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
>          > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>          >
>          > sslcrtd_program /usr/lib/squid/security_file_certgen -s
>          > /var/lib/squid/ssl_db -M 4MB
>          >
>          > acl step1 at_step SslBump1
>          >
>          > ssl_bump bump step1
>          >
>          > ssl_bump bump all
>          >
>          > sslproxy_cert_error deny all
>          >
>          > cache_store_log stdio:/var/log/squid/store.log
>          >
>          > logfile_rotate 0
>          >
>          > shutdown_lifetime 3 seconds
>          >
>          >
>          > # /usr/bin/proxy-test :
>          >
>          > #!/bin/bash
>          >
>          > curl --proxy http://localhost:3129 <http://localhost:3129>
>         <http://localhost:3129 <http://localhost:3129>> \
>          >
>          >? ??--cacert /etc/squid/stuff.pem \
>          >
>          >? ??-v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>          > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>" \
>          >
>          >? ??-H "Authorization: token MYTOKEN" \
>          >
>          >? ??-H "Content-Type: application/json" \
>          >
>          >? ??--output "/tmp/stuff.json"
>          >
>          >
>          >
>          > Tests? ..........
>          >
>          >
>          > At this point in time the network cable is unattached.? Squid
>         returns
>          > the cached object it got when the network was online earlier.
>         The Age of
>          > this object is just still under the max_age of 3600.    
>         Previously I
>          > was using offline_mode but I found that it did not try to
>         revalidate
>          > from the origin after the object expired (defined via max-age
>         response).
>          >? ? My understanding is that stale-if-error should work under my
>          > circumstances.
>          >
>          >
>          > # /var/log/squid/access.log
>          >
>          > 1706799404.440? ? ? 6 127.0.0.1 NONE_NONE/200 0 CONNECT
>          > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>
>         <http://stuff.amazonaws.com:443
>         <http://stuff.amazonaws.com:443>> - HIER_NONE/- -
>          >
>          > 1706799404.440? ? ? 0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
>          > https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>
>          > <https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>> - HIER_NONE/-
>         application/json
>          >
>          >
>          > # extract from /usr/bin/proxy-test
>          >
>          > < HTTP/1.1 200 OK
>          >
>          > < Date: Thu, 01 Feb 2024 13:57:11 GMT
>          >
>          > < Content-Type: application/json
>          >
>          > < Content-Length: 20134
>          >
>          > < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
>          >
>          > < Last-Modified: 2024-02-01T13:00:45.000Z
>          >
>          > < Access-Control-Allow-Origin: *
>          >
>          > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
>          >
>          > < Cache-Control: public, max-age=3600, stale-if-error=31536000
>          >
>          > < ETag: "cec102b43372840737ab773c2e77858b"
>          >
>          > < X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6
>          >
>          > < Age: 3573
>          >
>          > < X-Cache: HIT from labs-maul-st-31
>          >
>          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>          >
>          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>          >
>          > < Connection: keep-alive
>          >
>          >
>          >
>          >
>          > Below .. the curl script executes again.? The Age has gone
>         over the
>          > max-age so squid attempted to refresh from the origin.? The
>         machine is
>          > still offline so the refresh failed. ? I expected that the
>          > stale-if-error response would instruct squid to return the
>         cached object
>          > as a 200.
>          >
>          >
>          > # /var/log/squid/access.log
>          >
>          > 1706799434.464? ? ? 5 127.0.0.1 NONE_NONE/200 0 CONNECT
>          > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>
>         <http://stuff.amazonaws.com:443
>         <http://stuff.amazonaws.com:443>> - HIER_NONE/- -
>          >
>          > 1706799434.464? ? ? 0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235
>         GET
>          > https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>
>          > <https://stuff.amazonaws.com/stuff.json
>         <https://stuff.amazonaws.com/stuff.json>> - HIER_NONE/- text/html
>          >
>          >
>          > # extract from /usr/bin/proxy-test
>          >
>          > < HTTP/1.1 502 Bad Gateway
>          >
>          > < Server: squid/5.7
>          >
>          > < Mime-Version: 1.0
>          >
>          > < Date: Thu, 01 Feb 2024 14:57:14 GMT
>          >
>          > < Content-Type: text/html;charset=utf-8
>          >
>          > < Content-Length: 3853
>          >
>          > < X-Squid-Error: ERR_READ_ERROR 0
>          >
>          > < Vary: Accept-Language
>          >
>          > < Content-Language: en
>          >
>          > < X-Cache: MISS from labs-maul-st-31
>          >
>          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>          >
>          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>          >
>          > < Connection: close
>          >
>          >
>          >
>          > Hope someone can help me with this.? All the best,
>          >
>          >
>          > Robin Carlisle
>          >
>          >
>          >
>          > _______________________________________________
>          > squid-users mailing list
>          > squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>          > https://lists.squid-cache.org/listinfo/squid-users
>         <https://lists.squid-cache.org/listinfo/squid-users>
> 
>         _______________________________________________
>         squid-users mailing list
>         squid-users at lists.squid-cache.org
>         <mailto:squid-users at lists.squid-cache.org>
>         https://lists.squid-cache.org/listinfo/squid-users
>         <https://lists.squid-cache.org/listinfo/squid-users>
> 



From ngtech1ltd at gmail.com  Fri Feb  9 03:41:22 2024
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Fri, 9 Feb 2024 05:41:22 +0200
Subject: [squid-users] Squid as an education tool
Message-ID: <005401da5b09$da87b110$8f971330$@gmail.com>

Hey Everybody,

I am just releasing the latest 6.7 RPMs and binaries while running couple tests and I was wondering if this was done.
As I am looking at proxy, in most cases it's being used as a policy enforcer rather than an education tool.
I believe in education as one of the top priorities compared to enforcing policies.
The nature of policies depends on the environment and the risks but eventually understanding the meaning of the policy
gives a lot to the cooperation of the user or an employee.

I have yet to see a solution like the next:
Each user has a profile/user which when receiving a policy block will be prompted with an option to allow temporarily 
the specific site or domain.
Also, I have not seen an implementation which allows the user to disable or lower the policy strictness for a short period of time.

I am looking for such implementations if those exist already to run education sessions with teenagers.

Thanks,
Eliezer  



From marcus.kool at urlfilterdb.com  Fri Feb  9 09:23:27 2024
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 9 Feb 2024 09:23:27 +0000
Subject: [squid-users] Squid as an education tool
In-Reply-To: <005401da5b09$da87b110$8f971330$@gmail.com>
References: <005401da5b09$da87b110$8f971330$@gmail.com>
Message-ID: <12e2c972-f64d-457d-ba23-f12d3eab8c41@urlfilterdb.com>

Hi Eliezer,

I am not aware of a tool that has all functionality that you seek so you probably have to make it yourself.
I know that you are already familiar with ufdbGuard for Squid to block access, but you can also use ufdbGuard for temporary access by including a time-restricted whitelist in the configuration file 
and doing a reload of the ufdbGuard configuration.? The reload does not interrupt the function of the web proxy or ufdbGuard itself.

Marcus

On 09/02/2024 03:41, ngtech1ltd at gmail.com wrote:
> Hey Everybody,
>
> I am just releasing the latest 6.7 RPMs and binaries while running couple tests and I was wondering if this was done.
> As I am looking at proxy, in most cases it's being used as a policy enforcer rather than an education tool.
> I believe in education as one of the top priorities compared to enforcing policies.
> The nature of policies depends on the environment and the risks but eventually understanding the meaning of the policy
> gives a lot to the cooperation of the user or an employee.
>
> I have yet to see a solution like the next:
> Each user has a profile/user which when receiving a policy block will be prompted with an option to allow temporarily
> the specific site or domain.
> Also, I have not seen an implementation which allows the user to disable or lower the policy strictness for a short period of time.
>
> I am looking for such implementations if those exist already to run education sessions with teenagers.
>
> Thanks,
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From gkinkie at gmail.com  Fri Feb  9 10:00:06 2024
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Fri, 9 Feb 2024 10:00:06 +0000
Subject: [squid-users] Squid as an education tool
In-Reply-To: <12e2c972-f64d-457d-ba23-f12d3eab8c41@urlfilterdb.com>
References: <005401da5b09$da87b110$8f971330$@gmail.com>
 <12e2c972-f64d-457d-ba23-f12d3eab8c41@urlfilterdb.com>
Message-ID: <CA+Y8hcOVjaO79RV-qmn8LPw-iBxXzmfkdQezJa82vt5kE_gGOg@mail.gmail.com>

Hi Eliezer, Marcus,
  what you describe seems very similar to a captive portal, just with a
very dynamic allowlist policy.
I'm confident that it can be implemented with Squid, a few helpers, and a
side webserver plus a small website.
In fact, it would probably be a nice project to release to the community if
it were built to be generic enough

On Fri, Feb 9, 2024 at 9:23?AM Marcus Kool <marcus.kool at urlfilterdb.com>
wrote:

> Hi Eliezer,
>
> I am not aware of a tool that has all functionality that you seek so you
> probably have to make it yourself.
> I know that you are already familiar with ufdbGuard for Squid to block
> access, but you can also use ufdbGuard for temporary access by including a
> time-restricted whitelist in the configuration file
> and doing a reload of the ufdbGuard configuration.  The reload does not
> interrupt the function of the web proxy or ufdbGuard itself.
>
> Marcus
>
> On 09/02/2024 03:41, ngtech1ltd at gmail.com wrote:
> > Hey Everybody,
> >
> > I am just releasing the latest 6.7 RPMs and binaries while running
> couple tests and I was wondering if this was done.
> > As I am looking at proxy, in most cases it's being used as a policy
> enforcer rather than an education tool.
> > I believe in education as one of the top priorities compared to
> enforcing policies.
> > The nature of policies depends on the environment and the risks but
> eventually understanding the meaning of the policy
> > gives a lot to the cooperation of the user or an employee.
> >
> > I have yet to see a solution like the next:
> > Each user has a profile/user which when receiving a policy block will be
> prompted with an option to allow temporarily
> > the specific site or domain.
> > Also, I have not seen an implementation which allows the user to disable
> or lower the policy strictness for a short period of time.
> >
> > I am looking for such implementations if those exist already to run
> education sessions with teenagers.
> >
> > Thanks,
> > Eliezer
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240209/623ee92c/attachment.htm>

From robin.carlisle at framestore.com  Fri Feb  9 13:53:54 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Fri, 9 Feb 2024 13:53:54 +0000
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
 <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>
Message-ID: <CANOuv9pZXMoHH-MU0m3vhi35C-Xu4sbONGsCDBcweAmq5BukaA@mail.gmail.com>

Hi,
Thanks for the info Alex.  Patching code and building is a little beyond me
tbh, especially as I would need this as a debian package to deploy to many
machines.  With that in mind I am trying the config workaround approach.
 Below is the config snippet I have added.    I made the assumption that
for the *refresh_pattern, max-stale=NN  *config, the NN is in minutes as
per the rest of that config directive.
I am testing this right now

# this should allow stale objects up to 1 year if allowed by Cache-Control
repsonse headers ...

# ... setting both options just in case

max_stale 525600 minutes

refresh_pattern . 0  20% 4320 max-stale=525600


Thanks again for your help


Robin




On Thu, 8 Feb 2024 at 17:42, Alex Rousskov <rousskov at measurement-factory.com>
wrote:

> Hi Robin,
>
>      AFAICT from the logs you have privately shared and your squid.conf
> that you have posted earlier, your Squid overwrites
> stale-if-error=31536000 in the response with "refresh_pattern
> max-stale=0" default. That 0 value is wrong. The correct value should be
> taken from max_stale directive that defaults to 1 week, not zero:
>
>      refresh_pattern
>      ...
>      max-stale=NN provide a maximum staleness factor. Squid won't
>      serve objects more stale than this even if it failed to
>      validate the object. Default: use the max_stale global limit.
>
> This wrong default is a Squid bug AFAICT. I posted an _untested_ fix as
> Squid PR 1664: https://github.com/squid-cache/squid/pull/1664
>
> If possible, please test the corresponding patch:
>
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch
>
> AFAICT, you can also work around that bug by configuring an explicit
> refresh_pattern rule with an explicit max-stale option (see
> squid.conf.documented for examples). I have not tested that theory either.
>
>
> HTH,
>
> Alex.
>
>
> On 2024-02-07 13:45, Robin Carlisle wrote:
> > Hi,
> >
> > I have just started my enhanced logging journey and have a small snippet
> > below that might illuminate the issue ...
> >
> > /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507)
> > handleIMSReply: origin replied with error 502, forwarding to client due
> > to fail_on_validation_err/
> >
> > A few lines below in the log it looks like squid sent :-
> >
> > /2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280) sendStartOfMessage:
> > HTTP Client REPLY:
> > ---------
> > HTTP/1.1 502 Bad Gateway
> > Server: squid/5.7
> > Mime-Version: 1.0
> > Date: Wed, 07 Feb 2024 17:06:39 GMT
> > Content-Type: text/html;charset=utf-8
> > Content-Length: 3853
> > X-Squid-Error: ERR_READ_ERROR 0
> > Vary: Accept-Language
> > Content-Language: en
> > X-Cache: MISS from labs-maul-st-15
> > X-Cache-Lookup: HIT from labs-maul-st-15:3129
> > Via: 1.1 labs-maul-st-15 (squid/5.7)
> > Connection: close/
> >
> >
> > The rest of the logs are quite large and contain URLs I cannot put
> > here.   The logs were generated with debug_options to ALL,3.
> >
> > Any ideas?   Or should I generate more detailed logs and send them
> > privately?
> >
> > Thanks again,
> >
> > Robin
> >
> >
> >
> >
> > On Fri, 2 Feb 2024 at 11:20, Robin Carlisle
> > <robin.carlisle at framestore.com <mailto:robin.carlisle at framestore.com>>
> > wrote:
> >
> >     Hi, thanks for your reply.
> >
> >     I have been looking at :
> >
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>
> >
> >     /The stale-if-error response directive indicates that the cache can
> >     reuse a stale response when an upstream server generates an error,
> >     or when the error is generated locally. Here, an error is considered
> >     any response with a status code of 500, 502, 503, or 504.
> >
> >     Cache-Control: max-age=604800, stale-if-error=86400
> >     In the example above, the response is fresh for 7 days (604800s).
> >     Afterwards, it becomes stale, but can be used for an extra 1 day
> >     (86400s) when an error is encountered.
> >
> >     After the stale-if-error period passes, the client will receive any
> >     error generated/
> >
> >     Given what you have said and what the above docs say - I am still
> >     confused as it looks like (in my test cases) the cached response can
> >     be used for 3600 secs (this works), after which the cached response
> >     can still be used for an additional 31536000 seconds on an error
> >     (this doesnt work).
> >
> >     I am going to dig into the error logging you suggested to see if I
> >     can make sense of that - and will send on if I can't.
> >
> >     Thanks v much for your help again,
> >
> >     Robin
> >
> >
> >
> >
> >
> >     On Thu, 1 Feb 2024 at 18:27, Alex Rousskov
> >     <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>> wrote:
> >
> >         On 2024-02-01 12:03, Robin Carlisle wrote:
> >          > Hi, I am having trouble with stale-if-error response.
> >
> >         If I am interpreting Squid code correctly, in primary use cases:
> >
> >         * without a Cache-Control:stale-if-error=X in the original
> >         response,
> >         Squid sends a stale object if revalidation results in a 5xx
> error;
> >
> >         * with a Cache-Control:stale-if-error=X and object age at most
> >         X, Squid
> >         sends a stale object if revalidation results in a 5xx error;
> >
> >         * with a Cache-Control:stale-if-error=X and object age exceeding
> X,
> >         Squid forwards the 5xx error response if revalidation results in
> >         a 5xx
> >         error;
> >
> >         In other words, stale-if-error=X turns on a "fail on validation
> >         errors"
> >         behavior for stale objects older than X. It has no other effects.
> >
> >         In your test case, the stale objects are much younger than
> >         stale-if-error value (e.g., Age~=3601 vs.
> stale-if-error=31536000).
> >         Thus, stale-if-error should have no relevant effect.
> >
> >         Something else is probably preventing your Squid from serving
> >         the stale
> >         response when facing a 5xx error. I do not know what that
> >         something is.
> >
> >         I recommend sharing (privately if you need to protect sensitive
> >         info) a
> >         pointer to a compressed ALL,9 cache.log collected while
> >         reproducing the
> >         problem (using two transactions similar to the ones you have
> shared
> >         below -- a successful stale hit and a problematic one):
> >
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> >
> >
> >         Alternatively, you can try to study cache.log yourself after
> >         setting
> >         debug_options to ALL,3. Searching for "refresh" and
> >         "handleIMSReply" may
> >         yield enough clues.
> >
> >
> >         HTH,
> >
> >         Alex.
> >
> >
> >
> >
> >          > # /etc/squid/squid.conf :
> >          >
> >          > acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
> >         <http://amazonaws.com <http://amazonaws.com>>
> >          >
> >          > acl from_local src localhost
> >          >
> >          > http_access allow to_aws
> >          >
> >          > http_access allow from_local
> >          >
> >          > cache allow all
> >          >
> >          > cache_dir ufs /var/cache/squid 1024 16 256
> >          >
> >          > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
> >          > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> >          >
> >          > sslcrtd_program /usr/lib/squid/security_file_certgen -s
> >          > /var/lib/squid/ssl_db -M 4MB
> >          >
> >          > acl step1 at_step SslBump1
> >          >
> >          > ssl_bump bump step1
> >          >
> >          > ssl_bump bump all
> >          >
> >          > sslproxy_cert_error deny all
> >          >
> >          > cache_store_log stdio:/var/log/squid/store.log
> >          >
> >          > logfile_rotate 0
> >          >
> >          > shutdown_lifetime 3 seconds
> >          >
> >          >
> >          > # /usr/bin/proxy-test :
> >          >
> >          > #!/bin/bash
> >          >
> >          > curl --proxy http://localhost:3129 <http://localhost:3129>
> >         <http://localhost:3129 <http://localhost:3129>> \
> >          >
> >          >    --cacert /etc/squid/stuff.pem \
> >          >
> >          >    -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >          > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>" \
> >          >
> >          >    -H "Authorization: token MYTOKEN" \
> >          >
> >          >    -H "Content-Type: application/json" \
> >          >
> >          >    --output "/tmp/stuff.json"
> >          >
> >          >
> >          >
> >          > Tests  ..........
> >          >
> >          >
> >          > At this point in time the network cable is unattached.  Squid
> >         returns
> >          > the cached object it got when the network was online earlier.
> >         The Age of
> >          > this object is just still under the max_age of 3600.
> >         Previously I
> >          > was using offline_mode but I found that it did not try to
> >         revalidate
> >          > from the origin after the object expired (defined via max-age
> >         response).
> >          >    My understanding is that stale-if-error should work under
> my
> >          > circumstances.
> >          >
> >          >
> >          > # /var/log/squid/access.log
> >          >
> >          > 1706799404.440      6 127.0.0.1 NONE_NONE/200 0 CONNECT
> >          > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>
> >         <http://stuff.amazonaws.com:443
> >         <http://stuff.amazonaws.com:443>> - HIER_NONE/- -
> >          >
> >          > 1706799404.440      0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
> >          > https://stuff.amazonaws.com/stuff.json
> >         <https://stuff.amazonaws.com/stuff.json>
> >          > <https://stuff.amazonaws.com/stuff.json
> >         <https://stuff.amazonaws.com/stuff.json>> - HIER_NONE/-
> >         application/json
> >          >
> >          >
> >          > # extract from /usr/bin/proxy-test
> >          >
> >          > < HTTP/1.1 200 OK
> >          >
> >          > < Date: Thu, 01 Feb 2024 13:57:11 GMT
> >          >
> >          > < Content-Type: application/json
> >          >
> >          > < Content-Length: 20134
> >          >
> >          > < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
> >          >
> >          > < Last-Modified: 2024-02-01T13:00:45.000Z
> >          >
> >          > < Access-Control-Allow-Origin: *
> >          >
> >          > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
> >          >
> >          > < Cache-Control: public, max-age=3600, stale-if-error=31536000
> >          >
> >          > < ETag: "cec102b43372840737ab773c2e77858b"
> >          >
> >          > < X-Amzn-Trace-Id: Root=1-65bba337-292be751134161b03555cdd6
> >          >
> >          > < Age: 3573
> >          >
> >          > < X-Cache: HIT from labs-maul-st-31
> >          >
> >          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >          >
> >          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >          >
> >          > < Connection: keep-alive
> >          >
> >          >
> >          >
> >          >
> >          > Below .. the curl script executes again.  The Age has gone
> >         over the
> >          > max-age so squid attempted to refresh from the origin.  The
> >         machine is
> >          > still offline so the refresh failed.   I expected that the
> >          > stale-if-error response would instruct squid to return the
> >         cached object
> >          > as a 200.
> >          >
> >          >
> >          > # /var/log/squid/access.log
> >          >
> >          > 1706799434.464      5 127.0.0.1 NONE_NONE/200 0 CONNECT
> >          > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>
> >         <http://stuff.amazonaws.com:443
> >         <http://stuff.amazonaws.com:443>> - HIER_NONE/- -
> >          >
> >          > 1706799434.464      0 127.0.0.1 TCP_REFRESH_FAIL_ERR/502 4235
> >         GET
> >          > https://stuff.amazonaws.com/stuff.json
> >         <https://stuff.amazonaws.com/stuff.json>
> >          > <https://stuff.amazonaws.com/stuff.json
> >         <https://stuff.amazonaws.com/stuff.json>> - HIER_NONE/-
> text/html
> >          >
> >          >
> >          > # extract from /usr/bin/proxy-test
> >          >
> >          > < HTTP/1.1 502 Bad Gateway
> >          >
> >          > < Server: squid/5.7
> >          >
> >          > < Mime-Version: 1.0
> >          >
> >          > < Date: Thu, 01 Feb 2024 14:57:14 GMT
> >          >
> >          > < Content-Type: text/html;charset=utf-8
> >          >
> >          > < Content-Length: 3853
> >          >
> >          > < X-Squid-Error: ERR_READ_ERROR 0
> >          >
> >          > < Vary: Accept-Language
> >          >
> >          > < Content-Language: en
> >          >
> >          > < X-Cache: MISS from labs-maul-st-31
> >          >
> >          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >          >
> >          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >          >
> >          > < Connection: close
> >          >
> >          >
> >          >
> >          > Hope someone can help me with this.  All the best,
> >          >
> >          >
> >          > Robin Carlisle
> >          >
> >          >
> >          >
> >          > _______________________________________________
> >          > squid-users mailing list
> >          > squid-users at lists.squid-cache.org
> >         <mailto:squid-users at lists.squid-cache.org>
> >          > https://lists.squid-cache.org/listinfo/squid-users
> >         <https://lists.squid-cache.org/listinfo/squid-users>
> >
> >         _______________________________________________
> >         squid-users mailing list
> >         squid-users at lists.squid-cache.org
> >         <mailto:squid-users at lists.squid-cache.org>
> >         https://lists.squid-cache.org/listinfo/squid-users
> >         <https://lists.squid-cache.org/listinfo/squid-users>
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240209/9c8ae641/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb  9 14:31:08 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 9 Feb 2024 09:31:08 -0500
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9pZXMoHH-MU0m3vhi35C-Xu4sbONGsCDBcweAmq5BukaA@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
 <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>
 <CANOuv9pZXMoHH-MU0m3vhi35C-Xu4sbONGsCDBcweAmq5BukaA@mail.gmail.com>
Message-ID: <a479a52a-2044-47d1-b49a-2c4c98a90edb@measurement-factory.com>

On 2024-02-09 08:53, Robin Carlisle wrote:

> I am trying the config workaround approach.

Please keep us posted on your progress.

> ?Below is the config snippet I have added.? ? I made the 
> assumption that for the /refresh_pattern, max-stale=NN /config, the NN 
> is in minutes as per the rest of that config directive.

That assumption is natural but incorrect: Unlike the anonymous 
positional min and max parameters (that use minutes), refresh_pattern 
max-stale=NN uses seconds. Documentation improvements are welcome.

Said that, the workaround should still prevent the application of the 
broken default refresh_pattern max-stale=0 rule, so you should still see 
positive results for the first NN seconds of the response age.

Instead of specifying max-stale=NN, consider adding refresh_pattern 
rules recommended by squid.conf.documented (and included in 
squid.cond.default). Those rules do not have max-stale options at all, 
and, hence, Squid will use (explicit or default) max_stale directive 
instead.

HTH,

Alex.


> I am testing this right now
> 
> # this should allow stale objects up to 1 year if allowed by 
> Cache-Control repsonseheaders ...
> 
> # ... setting both options just in case
> 
> max_stale 525600 minutes
> 
> refresh_pattern . 0? 20% 4320 max-stale=525600
> 
> 
> Thanks again for your help
> 
> 
> Robin
> 
> 
> 
> 
> On Thu, 8 Feb 2024 at 17:42, Alex Rousskov 
> <rousskov at measurement-factory.com 
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     Hi Robin,
> 
>      ? ? ?AFAICT from the logs you have privately shared and your
>     squid.conf
>     that you have posted earlier, your Squid overwrites
>     stale-if-error=31536000 in the response with "refresh_pattern
>     max-stale=0" default. That 0 value is wrong. The correct value
>     should be
>     taken from max_stale directive that defaults to 1 week, not zero:
> 
>      ? ? ?refresh_pattern
>      ? ? ?...
>      ? ? ?max-stale=NN provide a maximum staleness factor. Squid won't
>      ? ? ?serve objects more stale than this even if it failed to
>      ? ? ?validate the object. Default: use the max_stale global limit.
> 
>     This wrong default is a Squid bug AFAICT. I posted an _untested_ fix as
>     Squid PR 1664: https://github.com/squid-cache/squid/pull/1664
>     <https://github.com/squid-cache/squid/pull/1664>
> 
>     If possible, please test the corresponding patch:
>     https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch <https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch>
> 
>     AFAICT, you can also work around that bug by configuring an explicit
>     refresh_pattern rule with an explicit max-stale option (see
>     squid.conf.documented for examples). I have not tested that theory
>     either.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     On 2024-02-07 13:45, Robin Carlisle wrote:
>      > Hi,
>      >
>      > I have just started my enhanced logging journey and have a small
>     snippet
>      > below that might illuminate the issue ...
>      >
>      > /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507)
>      > handleIMSReply: origin replied with error 502, forwarding to
>     client due
>      > to fail_on_validation_err/
>      >
>      > A few lines below in the log it looks like squid sent :-
>      >
>      > /2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280)
>     sendStartOfMessage:
>      > HTTP Client REPLY:
>      > ---------
>      > HTTP/1.1 502 Bad Gateway
>      > Server: squid/5.7
>      > Mime-Version: 1.0
>      > Date: Wed, 07 Feb 2024 17:06:39 GMT
>      > Content-Type: text/html;charset=utf-8
>      > Content-Length: 3853
>      > X-Squid-Error: ERR_READ_ERROR 0
>      > Vary: Accept-Language
>      > Content-Language: en
>      > X-Cache: MISS from labs-maul-st-15
>      > X-Cache-Lookup: HIT from labs-maul-st-15:3129
>      > Via: 1.1 labs-maul-st-15 (squid/5.7)
>      > Connection: close/
>      >
>      >
>      > The rest of the logs are quite large and contain URLs I cannot put
>      > here.? ?The logs were generated with debug_options to ALL,3.
>      >
>      > Any ideas?? ?Or should I generate more detailed logs and send them
>      > privately?
>      >
>      > Thanks again,
>      >
>      > Robin
>      >
>      >
>      >
>      >
>      > On Fri, 2 Feb 2024 at 11:20, Robin Carlisle
>      > <robin.carlisle at framestore.com
>     <mailto:robin.carlisle at framestore.com>
>     <mailto:robin.carlisle at framestore.com
>     <mailto:robin.carlisle at framestore.com>>>
>      > wrote:
>      >
>      >? ? ?Hi, thanks for your reply.
>      >
>      >? ? ?I have been looking at :
>      >
>     https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control> <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>>
>      >
>      >? ? ?/The stale-if-error response directive indicates that the
>     cache can
>      >? ? ?reuse a stale response when an upstream server generates an
>     error,
>      >? ? ?or when the error is generated locally. Here, an error is
>     considered
>      >? ? ?any response with a status code of 500, 502, 503, or 504.
>      >
>      >? ? ?Cache-Control: max-age=604800, stale-if-error=86400
>      >? ? ?In the example above, the response is fresh for 7 days (604800s).
>      >? ? ?Afterwards, it becomes stale, but can be used for an extra 1 day
>      >? ? ?(86400s) when an error is encountered.
>      >
>      >? ? ?After the stale-if-error period passes, the client will
>     receive any
>      >? ? ?error generated/
>      >
>      >? ? ?Given what you have said and what the above docs say - I am still
>      >? ? ?confused as it looks like (in my test cases) the cached
>     response can
>      >? ? ?be used for 3600 secs (this works), after which the cached
>     response
>      >? ? ?can still be used for an additional 31536000 seconds on an error
>      >? ? ?(this doesnt work).
>      >
>      >? ? ?I am going to dig into the error logging?you suggested to see
>     if I
>      >? ? ?can make sense of that - and will send on if I can't.
>      >
>      >? ? ?Thanks v much for your help again,
>      >
>      >? ? ?Robin
>      >
>      >
>      >
>      >
>      >
>      >? ? ?On Thu, 1 Feb 2024 at 18:27, Alex Rousskov
>      >? ? ?<rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>      >
>      >? ? ? ? ?On 2024-02-01 12:03, Robin Carlisle wrote:
>      >? ? ? ? ? > Hi, I am having trouble with stale-if-error response.
>      >
>      >? ? ? ? ?If I am interpreting Squid code correctly, in primary use
>     cases:
>      >
>      >? ? ? ? ?* without a Cache-Control:stale-if-error=X in the original
>      >? ? ? ? ?response,
>      >? ? ? ? ?Squid sends a stale object if revalidation results in a
>     5xx error;
>      >
>      >? ? ? ? ?* with a Cache-Control:stale-if-error=X and object age at
>     most
>      >? ? ? ? ?X, Squid
>      >? ? ? ? ?sends a stale object if revalidation results in a 5xx error;
>      >
>      >? ? ? ? ?* with a Cache-Control:stale-if-error=X and object age
>     exceeding X,
>      >? ? ? ? ?Squid forwards the 5xx error response if revalidation
>     results in
>      >? ? ? ? ?a 5xx
>      >? ? ? ? ?error;
>      >
>      >? ? ? ? ?In other words, stale-if-error=X turns on a "fail on
>     validation
>      >? ? ? ? ?errors"
>      >? ? ? ? ?behavior for stale objects older than X. It has no other
>     effects.
>      >
>      >? ? ? ? ?In your test case, the stale objects are much younger than
>      >? ? ? ? ?stale-if-error value (e.g., Age~=3601 vs.
>     stale-if-error=31536000).
>      >? ? ? ? ?Thus, stale-if-error should have no relevant effect.
>      >
>      >? ? ? ? ?Something else is probably preventing your Squid from serving
>      >? ? ? ? ?the stale
>      >? ? ? ? ?response when facing a 5xx error. I do not know what that
>      >? ? ? ? ?something is.
>      >
>      >? ? ? ? ?I recommend sharing (privately if you need to protect
>     sensitive
>      >? ? ? ? ?info) a
>      >? ? ? ? ?pointer to a compressed ALL,9 cache.log collected while
>      >? ? ? ? ?reproducing the
>      >? ? ? ? ?problem (using two transactions similar to the ones you
>     have shared
>      >? ? ? ? ?below -- a successful stale hit and a problematic one):
>      >
>     https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>
>      >
>      >? ? ? ? ?Alternatively, you can try to study cache.log yourself after
>      >? ? ? ? ?setting
>      >? ? ? ? ?debug_options to ALL,3. Searching for "refresh" and
>      >? ? ? ? ?"handleIMSReply" may
>      >? ? ? ? ?yield enough clues.
>      >
>      >
>      >? ? ? ? ?HTH,
>      >
>      >? ? ? ? ?Alex.
>      >
>      >
>      >
>      >
>      >? ? ? ? ? > # /etc/squid/squid.conf :
>      >? ? ? ? ? >
>      >? ? ? ? ? > acl to_aws dstdomain .amazonaws.com
>     <http://amazonaws.com> <http://amazonaws.com <http://amazonaws.com>>
>      >? ? ? ? ?<http://amazonaws.com <http://amazonaws.com>
>     <http://amazonaws.com <http://amazonaws.com>>>
>      >? ? ? ? ? >
>      >? ? ? ? ? > acl from_local src localhost
>      >? ? ? ? ? >
>      >? ? ? ? ? > http_access allow to_aws
>      >? ? ? ? ? >
>      >? ? ? ? ? > http_access allow from_local
>      >? ? ? ? ? >
>      >? ? ? ? ? > cache allow all
>      >? ? ? ? ? >
>      >? ? ? ? ? > cache_dir ufs /var/cache/squid 1024 16 256
>      >? ? ? ? ? >
>      >? ? ? ? ? > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
>      >? ? ? ? ? > generate-host-certificates=on
>     dynamic_cert_mem_cache_size=4MB
>      >? ? ? ? ? >
>      >? ? ? ? ? > sslcrtd_program /usr/lib/squid/security_file_certgen -s
>      >? ? ? ? ? > /var/lib/squid/ssl_db -M 4MB
>      >? ? ? ? ? >
>      >? ? ? ? ? > acl step1 at_step SslBump1
>      >? ? ? ? ? >
>      >? ? ? ? ? > ssl_bump bump step1
>      >? ? ? ? ? >
>      >? ? ? ? ? > ssl_bump bump all
>      >? ? ? ? ? >
>      >? ? ? ? ? > sslproxy_cert_error deny all
>      >? ? ? ? ? >
>      >? ? ? ? ? > cache_store_log stdio:/var/log/squid/store.log
>      >? ? ? ? ? >
>      >? ? ? ? ? > logfile_rotate 0
>      >? ? ? ? ? >
>      >? ? ? ? ? > shutdown_lifetime 3 seconds
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > # /usr/bin/proxy-test :
>      >? ? ? ? ? >
>      >? ? ? ? ? > #!/bin/bash
>      >? ? ? ? ? >
>      >? ? ? ? ? > curl --proxy http://localhost:3129
>     <http://localhost:3129> <http://localhost:3129 <http://localhost:3129>>
>      >? ? ? ? ?<http://localhost:3129 <http://localhost:3129>
>     <http://localhost:3129 <http://localhost:3129>>> \
>      >? ? ? ? ? >
>      >? ? ? ? ? >? ??--cacert /etc/squid/stuff.pem \
>      >? ? ? ? ? >
>      >? ? ? ? ? >? ??-v
>     "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>      >? ? ? ? ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>
>      >? ? ? ? ? > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>      >? ? ? ? ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>>" \
>      >? ? ? ? ? >
>      >? ? ? ? ? >? ??-H "Authorization: token MYTOKEN" \
>      >? ? ? ? ? >
>      >? ? ? ? ? >? ??-H "Content-Type: application/json" \
>      >? ? ? ? ? >
>      >? ? ? ? ? >? ??--output "/tmp/stuff.json"
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > Tests? ..........
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > At this point in time the network cable is
>     unattached.? Squid
>      >? ? ? ? ?returns
>      >? ? ? ? ? > the cached object it got when the network was online
>     earlier.
>      >? ? ? ? ?The Age of
>      >? ? ? ? ? > this object is just still under the max_age of 3600.
>      >? ? ? ? ?Previously I
>      >? ? ? ? ? > was using offline_mode but I found that it did not try to
>      >? ? ? ? ?revalidate
>      >? ? ? ? ? > from the origin after the object expired (defined via
>     max-age
>      >? ? ? ? ?response).
>      >? ? ? ? ? >? ? My understanding is that stale-if-error should work
>     under my
>      >? ? ? ? ? > circumstances.
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > # /var/log/squid/access.log
>      >? ? ? ? ? >
>      >? ? ? ? ? > 1706799404.440? ? ? 6 127.0.0.1 NONE_NONE/200 0 CONNECT
>      >? ? ? ? ? > stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443> <http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>
>      >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>> - HIER_NONE/- -
>      >? ? ? ? ? >
>      >? ? ? ? ? > 1706799404.440? ? ? 0 127.0.0.1 TCP_MEM_HIT/200 20726 GET
>      >? ? ? ? ? > https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>
>      >? ? ? ? ? > <https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>> - HIER_NONE/-
>      >? ? ? ? ?application/json
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > # extract from /usr/bin/proxy-test
>      >? ? ? ? ? >
>      >? ? ? ? ? > < HTTP/1.1 200 OK
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Date: Thu, 01 Feb 2024 13:57:11 GMT
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Content-Type: application/json
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Content-Length: 20134
>      >? ? ? ? ? >
>      >? ? ? ? ? > < x-amzn-RequestId: 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Last-Modified: 2024-02-01T13:00:45.000Z
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Access-Control-Allow-Origin: *
>      >? ? ? ? ? >
>      >? ? ? ? ? > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Cache-Control: public, max-age=3600,
>     stale-if-error=31536000
>      >? ? ? ? ? >
>      >? ? ? ? ? > < ETag: "cec102b43372840737ab773c2e77858b"
>      >? ? ? ? ? >
>      >? ? ? ? ? > < X-Amzn-Trace-Id:
>     Root=1-65bba337-292be751134161b03555cdd6
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Age: 3573
>      >? ? ? ? ? >
>      >? ? ? ? ? > < X-Cache: HIT from labs-maul-st-31
>      >? ? ? ? ? >
>      >? ? ? ? ? > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Connection: keep-alive
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > Below .. the curl script executes again.? The Age has gone
>      >? ? ? ? ?over the
>      >? ? ? ? ? > max-age so squid attempted to refresh from the
>     origin.? The
>      >? ? ? ? ?machine is
>      >? ? ? ? ? > still offline so the refresh failed. ? I expected that the
>      >? ? ? ? ? > stale-if-error response would instruct squid to return the
>      >? ? ? ? ?cached object
>      >? ? ? ? ? > as a 200.
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > # /var/log/squid/access.log
>      >? ? ? ? ? >
>      >? ? ? ? ? > 1706799434.464? ? ? 5 127.0.0.1 NONE_NONE/200 0 CONNECT
>      >? ? ? ? ? > stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443> <http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>
>      >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>> - HIER_NONE/- -
>      >? ? ? ? ? >
>      >? ? ? ? ? > 1706799434.464? ? ? 0 127.0.0.1
>     TCP_REFRESH_FAIL_ERR/502 4235
>      >? ? ? ? ?GET
>      >? ? ? ? ? > https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>
>      >? ? ? ? ? > <https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>> - HIER_NONE/- text/html
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > # extract from /usr/bin/proxy-test
>      >? ? ? ? ? >
>      >? ? ? ? ? > < HTTP/1.1 502 Bad Gateway
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Server: squid/5.7
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Mime-Version: 1.0
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Date: Thu, 01 Feb 2024 14:57:14 GMT
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Content-Type: text/html;charset=utf-8
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Content-Length: 3853
>      >? ? ? ? ? >
>      >? ? ? ? ? > < X-Squid-Error: ERR_READ_ERROR 0
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Vary: Accept-Language
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Content-Language: en
>      >? ? ? ? ? >
>      >? ? ? ? ? > < X-Cache: MISS from labs-maul-st-31
>      >? ? ? ? ? >
>      >? ? ? ? ? > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>      >? ? ? ? ? >
>      >? ? ? ? ? > < Connection: close
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > Hope someone can help me with this.? All the best,
>      >? ? ? ? ? >
>      >? ? ? ? ? >
>      >? ? ? ? ? > Robin Carlisle


From ngtech1ltd at gmail.com  Sat Feb 10 21:53:03 2024
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sat, 10 Feb 2024 23:53:03 +0200
Subject: [squid-users] Squid as an education tool
In-Reply-To: <CA+Y8hcOVjaO79RV-qmn8LPw-iBxXzmfkdQezJa82vt5kE_gGOg@mail.gmail.com>
References: <005401da5b09$da87b110$8f971330$@gmail.com>
 <12e2c972-f64d-457d-ba23-f12d3eab8c41@urlfilterdb.com>
 <CA+Y8hcOVjaO79RV-qmn8LPw-iBxXzmfkdQezJa82vt5kE_gGOg@mail.gmail.com>
Message-ID: <007701da5c6b$86963d20$93c2b760$@gmail.com>

Hey Francesco and others,

First thanks of the direction.

I was thinking about using generic tools that are available as possible.
Also, in education there is a whole thing about it not being an intercept proxy (with or without bump) so
it simplifies some of the aspects of the setup.

I would try to write the general specs of the project from my point of view.
Since the goal is educating and not enforcing a policy we can start by defining the age of the kids as low as 6-5 or even lower.
Due to the age of the kids there is a baseline policy that must be enforced ie couple of standard and known categories.
With this in mind we need a DB setup that will host these categories and will be performant enough for high load ie Schools.
Since the law in most if not all countries on earth prohibit nudity to a degree and also prohibit the demonstration of reproductive activities
in both animals and humans, it's pretty clear that any violation of these should only be possible only by professional staff that is allowed by the law
to open these doors for very very special cases (which I know to exist).
There are also other activities and categories which are known to be harmful for specific ages which should be blocked by policy.
We can divide the level of filtering policy to domains, urls and content inside a page or a dynamic app which is either embedded or sources in another method.
Domains and urls are the most known and is commonly filtered and many tools are available to enforce and block these.
There is some issue is with systems and sites which are not based on static content and others which are based on content which is 
streams inside Websocket or another method such as content that is chunked over multiple urls or customized requests and responses.

On this specific project I want to address only the basics which are: domains and maybe urls.
Due to the above fact and the fact that the internet is far ahead of 1985 or 2000 the depth of the education session is restricted 
the proxy will be used only to demonstrate that there are bad actors on the Internet.
There are also other categories that probably many would like to add into the list such as malware sites.

I believe that the right way is to use a forward proxy which will use usernames to authenticate and identify the user.
This will make the whole setup a bit simpler to build and it is based on that the kids or teenagers are actively participating
in the setup and agreed to the terms of use based on their trust in the teachers and parents.
We also need to show some trust to the kids to allow them to be open in the session.

>From my point of view the architecture should be something like this:
* Proxy
* DB (SQL or another)
* Users Web portal (app)
* Admins Web Portal (app)
* Blockpages (static content with a touch of JS)
* A set of external helpers (auth, dstdomain matcher, time limit, dns rbl checker)
* Audit system

The assumption is that only authenticated users can use the proxy ie no username no internet.. even for windows and AV updates.
We also assume that the admins of the proxy do not need to override the basic polices because they have access to unrestricted internet.
Authentication can be done using the existing tools with a MySQL DB which can be integrated with the web portal( not AD or LDAP..)
The DB for the dstdomain/url blacklists should be fast enough to allow almost real time updates to the degree of TTL such as 5 to 10 seconds.
Every domain which should be blocked by the policy is a "must bump" one while if it allowed by the policy a "no bump" should be applied.
There are couple layers of block and whitelists (first match from left to right):
  Top-level(never allowed), , campus wide customized blacklist (for testing), campus wide customized whitelist(for testing), user customized blacklist, user customized whitelist, campus wide blacklist, campus wide whitelist

The user can manage his lists via the web portal but not the top level and campus lists.
There is also a section in the web portal which allows the user to contact the content administrators about any non user customized
lists such as the top level and the campus wide.
The expectation from the content administrators is to really understand the user interaction with them and to not just enforce the policy.
There is also a requirement from the content admin to have above the average technical knowledge about how internet works.
It includes both IP level and application level such as how TLS and firewall piercing.
The expectation is that all changes in any of the lists will be logged in the audit log.
Also, any "action" in the web portal will be logged in the audit log.
The audit is required by law to prevent from bad actions to be done in un supervised manner.
Due to this the Proxy structure and config is set and cannot be changed by anyone, even the sys admins.
To allow the system to be effective the only option to access the DB is using an audited web portal.
Since the structure of the DBs is pretty simple we can simplify the access to it VIA a very simple API.
The API should include both single entries action(add/modify) of entries and also bulk actions(for big lists).

I believe that such a setup can be implemented with containers and in a HA architecture. 

The actual DB for the lists which I have considered are:
* MySQL/MariaDB
* PostgreSQL
* MSSql
* SquidGuard
* ufdbguard
* SquidBlocker
* DNS Rbl

The limitations of SquidGuard and ufdbguard and DNS Rbl services is that they need to recompile the lists for usage.
For lists which should be re-compiled every one hour or so we can create a CI CD pipeline which includes a compilation
of the lists DB on a dedicated system and publish the precompiled files in a public storage ie s3 compatible or git.
A list change check can be done every 5 minutes for emergency updates but will only be updated periodically every 1 hour.
The above idea can work with both ufdbguard and SquidGuard or any DNS RBL system.

As for the user and campus dynamic lists, these should be stored and managed on a DB such as key-value or any other SQL
which doesn't require compilation to begin with.
If the dynamic lists DB will be small enough per user or campus it would be possible to use a ttl of 5-15 seconds on the dstdomain
external helper to reduce the number of times the "slow" queries against the DB will happen.
The other option is to use some kinds of RAM caching service such as Memcached or redis and to cache the response per 
domain per user for 300 Seconds ie the user "eliezer" response for "www.example.com" will be stored as: "eliezer://www.example.com"
and if the lists are small enough it would probably be simple to even trigger a prefix cleanup for all "eliezer://" namespace.
Currently all the Lists DB that I know about do not allow a query to know if a dstdomain is in a category or a set of categories.
With such a service we can divide the user lists to 2 separate searches/steps.
* customized dstdomain list match
* customized dstdomain to set of categories match

It can work for both white and black lists.

Working with pre-compiled lists or a fast enough service would allow the system to work fast enough and probably scale.

I have all the squid knowledge required for such a system but I need some help with the other moving parts.

I am open for any comments and suggestions about the setup technical or other aspects.

Thanks,
Eliezer Croitoru
ngtech1ltd at mgail.com
+972-5-28704261


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Francesco Chemolli
Sent: Friday, February 9, 2024 12:00 PM
To: Marcus Kool <marcus.kool at urlfilterdb.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid as an education tool

Hi Eliezer, Marcus,
  what you describe seems very similar to a captive portal, just with a very dynamic allowlist policy.
I'm confident that it can be implemented with Squid, a few helpers, and a side webserver plus a small website.
In fact, it would probably be a nice project to release to the community if it were built to be generic enough

On Fri, Feb 9, 2024 at 9:23?AM Marcus Kool <mailto:marcus.kool at urlfilterdb.com> wrote:
Hi Eliezer,

I am not aware of a tool that has all functionality that you seek so you probably have to make it yourself.
I know that you are already familiar with ufdbGuard for Squid to block access, but you can also use ufdbGuard for temporary access by including a time-restricted whitelist in the configuration file 
and doing a reload of the ufdbGuard configuration.  The reload does not interrupt the function of the web proxy or ufdbGuard itself.

Marcus

On 09/02/2024 03:41, mailto:ngtech1ltd at gmail.com wrote:
> Hey Everybody,
>
> I am just releasing the latest 6.7 RPMs and binaries while running couple tests and I was wondering if this was done.
> As I am looking at proxy, in most cases it's being used as a policy enforcer rather than an education tool.
> I believe in education as one of the top priorities compared to enforcing policies.
> The nature of policies depends on the environment and the risks but eventually understanding the meaning of the policy
> gives a lot to the cooperation of the user or an employee.
>
> I have yet to see a solution like the next:
> Each user has a profile/user which when receiving a policy block will be prompted with an option to allow temporarily
> the specific site or domain.
> Also, I have not seen an implementation which allows the user to disable or lower the policy strictness for a short period of time.
>
> I am looking for such implementations if those exist already to run education sessions with teenagers.
>
> Thanks,
> Eliezer
>
> _______________________________________________
> squid-users mailing list
> mailto:squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users



-- 
    Francesco



From ngtech1ltd at gmail.com  Sun Feb 11 20:24:43 2024
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Sun, 11 Feb 2024 22:24:43 +0200
Subject: [squid-users] Basic Squid-Cache docker containers
Message-ID: <00b201da5d28$59c700a0$0d5501e0$@gmail.com>

Hey Everyone,

As a part of the project I am currently working on I needed a basic squid-cache container.
I have looked for these in Docker hub and wasn't able to find such a container image with the newest version of squid.

Due to this I have created 3 containers:
Alma8 based
Debian 12 Based
Ubuntu 22.04 based

The images can be found at:
https://hub.docker.com/u/elicro

and are:
elicro/almasquid:6.7
or 
elicro/almasquid:latest

elicro/debiansquid:6.7
or
elicro/debiansquid:latest

elicro/ubuntusquid:6.7
or
elicro/ubuntusquid:latest

I will be happy to hear any response regarding these containers.

Yours,
Eliezer



From david at articatech.com  Mon Feb 12 11:30:30 2024
From: david at articatech.com (David Touzeau)
Date: Mon, 12 Feb 2024 12:30:30 +0100
Subject: [squid-users] Squid as an education tool
In-Reply-To: <007701da5c6b$86963d20$93c2b760$@gmail.com>
References: <005401da5b09$da87b110$8f971330$@gmail.com>
 <12e2c972-f64d-457d-ba23-f12d3eab8c41@urlfilterdb.com>
 <CA+Y8hcOVjaO79RV-qmn8LPw-iBxXzmfkdQezJa82vt5kE_gGOg@mail.gmail.com>
 <007701da5c6b$86963d20$93c2b760$@gmail.com>
Message-ID: <3e8f493d-2298-4393-b6b6-02ae17d3e22b@articatech.com>

Hi,

I can speak for the French part.

We work with a lot of schools and despite having produced similar 
features, these are not used by school administrators.

We have therefore abandoned this initiative:

- Lack of time
- Internet pedagogy is not a priority in the face of the educational 
abyss to be undertaken.
- For small schools, what teachers want is access to specific Web sites, 
and whitelist/deny all modes is favored. Squid ACLs meet this need.
- For medium-sized schools (over 12 years old), students are all 
equipped with smartphones and are not connected to the wifi network 
provided by the school.
This leaves the teaching stations, which are subject to the same 
restrictions as for small schools.
- In some cases, the Internet is open and only phishing, malware, porn, 
SafeSearch categories are forbidden, which is enough for teaching staff 
to be legally excluded.

However, I don't have any specific knowledge of universities or private 
schools.



Le 10/02/2024 ? 22:53, ngtech1ltd at gmail.com a ?crit?:
> Hey Francesco and others,
>
> First thanks of the direction.
>
> I was thinking about using generic tools that are available as possible.
> Also, in education there is a whole thing about it not being an intercept proxy (with or without bump) so
> it simplifies some of the aspects of the setup.
>
> I would try to write the general specs of the project from my point of view.
> Since the goal is educating and not enforcing a policy we can start by defining the age of the kids as low as 6-5 or even lower.
> Due to the age of the kids there is a baseline policy that must be enforced ie couple of standard and known categories.
> With this in mind we need a DB setup that will host these categories and will be performant enough for high load ie Schools.
> Since the law in most if not all countries on earth prohibit nudity to a degree and also prohibit the demonstration of reproductive activities
> in both animals and humans, it's pretty clear that any violation of these should only be possible only by professional staff that is allowed by the law
> to open these doors for very very special cases (which I know to exist).
> There are also other activities and categories which are known to be harmful for specific ages which should be blocked by policy.
> We can divide the level of filtering policy to domains, urls and content inside a page or a dynamic app which is either embedded or sources in another method.
> Domains and urls are the most known and is commonly filtered and many tools are available to enforce and block these.
> There is some issue is with systems and sites which are not based on static content and others which are based on content which is
> streams inside Websocket or another method such as content that is chunked over multiple urls or customized requests and responses.
>
> On this specific project I want to address only the basics which are: domains and maybe urls.
> Due to the above fact and the fact that the internet is far ahead of 1985 or 2000 the depth of the education session is restricted
> the proxy will be used only to demonstrate that there are bad actors on the Internet.
> There are also other categories that probably many would like to add into the list such as malware sites.
>
> I believe that the right way is to use a forward proxy which will use usernames to authenticate and identify the user.
> This will make the whole setup a bit simpler to build and it is based on that the kids or teenagers are actively participating
> in the setup and agreed to the terms of use based on their trust in the teachers and parents.
> We also need to show some trust to the kids to allow them to be open in the session.
>
>  From my point of view the architecture should be something like this:
> * Proxy
> * DB (SQL or another)
> * Users Web portal (app)
> * Admins Web Portal (app)
> * Blockpages (static content with a touch of JS)
> * A set of external helpers (auth, dstdomain matcher, time limit, dns rbl checker)
> * Audit system
>
> The assumption is that only authenticated users can use the proxy ie no username no internet.. even for windows and AV updates.
> We also assume that the admins of the proxy do not need to override the basic polices because they have access to unrestricted internet.
> Authentication can be done using the existing tools with a MySQL DB which can be integrated with the web portal( not AD or LDAP..)
> The DB for the dstdomain/url blacklists should be fast enough to allow almost real time updates to the degree of TTL such as 5 to 10 seconds.
> Every domain which should be blocked by the policy is a "must bump" one while if it allowed by the policy a "no bump" should be applied.
> There are couple layers of block and whitelists (first match from left to right):
>    Top-level(never allowed), , campus wide customized blacklist (for testing), campus wide customized whitelist(for testing), user customized blacklist, user customized whitelist, campus wide blacklist, campus wide whitelist
>
> The user can manage his lists via the web portal but not the top level and campus lists.
> There is also a section in the web portal which allows the user to contact the content administrators about any non user customized
> lists such as the top level and the campus wide.
> The expectation from the content administrators is to really understand the user interaction with them and to not just enforce the policy.
> There is also a requirement from the content admin to have above the average technical knowledge about how internet works.
> It includes both IP level and application level such as how TLS and firewall piercing.
> The expectation is that all changes in any of the lists will be logged in the audit log.
> Also, any "action" in the web portal will be logged in the audit log.
> The audit is required by law to prevent from bad actions to be done in un supervised manner.
> Due to this the Proxy structure and config is set and cannot be changed by anyone, even the sys admins.
> To allow the system to be effective the only option to access the DB is using an audited web portal.
> Since the structure of the DBs is pretty simple we can simplify the access to it VIA a very simple API.
> The API should include both single entries action(add/modify) of entries and also bulk actions(for big lists).
>
> I believe that such a setup can be implemented with containers and in a HA architecture.
>
> The actual DB for the lists which I have considered are:
> * MySQL/MariaDB
> * PostgreSQL
> * MSSql
> * SquidGuard
> * ufdbguard
> * SquidBlocker
> * DNS Rbl
>
> The limitations of SquidGuard and ufdbguard and DNS Rbl services is that they need to recompile the lists for usage.
> For lists which should be re-compiled every one hour or so we can create a CI CD pipeline which includes a compilation
> of the lists DB on a dedicated system and publish the precompiled files in a public storage ie s3 compatible or git.
> A list change check can be done every 5 minutes for emergency updates but will only be updated periodically every 1 hour.
> The above idea can work with both ufdbguard and SquidGuard or any DNS RBL system.
>
> As for the user and campus dynamic lists, these should be stored and managed on a DB such as key-value or any other SQL
> which doesn't require compilation to begin with.
> If the dynamic lists DB will be small enough per user or campus it would be possible to use a ttl of 5-15 seconds on the dstdomain
> external helper to reduce the number of times the "slow" queries against the DB will happen.
> The other option is to use some kinds of RAM caching service such as Memcached or redis and to cache the response per
> domain per user for 300 Seconds ie the user "eliezer" response for "www.example.com" will be stored as: "eliezer://www.example.com"
> and if the lists are small enough it would probably be simple to even trigger a prefix cleanup for all "eliezer://" namespace.
> Currently all the Lists DB that I know about do not allow a query to know if a dstdomain is in a category or a set of categories.
> With such a service we can divide the user lists to 2 separate searches/steps.
> * customized dstdomain list match
> * customized dstdomain to set of categories match
>
> It can work for both white and black lists.
>
> Working with pre-compiled lists or a fast enough service would allow the system to work fast enough and probably scale.
>
> I have all the squid knowledge required for such a system but I need some help with the other moving parts.
>
> I am open for any comments and suggestions about the setup technical or other aspects.
>
> Thanks,
> Eliezer Croitoru
> ngtech1ltd at mgail.com
> +972-5-28704261
>
>
> From: squid-users<squid-users-bounces at lists.squid-cache.org>  On Behalf Of Francesco Chemolli
> Sent: Friday, February 9, 2024 12:00 PM
> To: Marcus Kool<marcus.kool at urlfilterdb.com>
> Cc:squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid as an education tool
>
> Hi Eliezer, Marcus,
>    what you describe seems very similar to a captive portal, just with a very dynamic allowlist policy.
> I'm confident that it can be implemented with Squid, a few helpers, and a side webserver plus a small website.
> In fact, it would probably be a nice project to release to the community if it were built to be generic enough
>
> On Fri, Feb 9, 2024 at 9:23?AM Marcus Kool<mailto:marcus.kool at urlfilterdb.com>  wrote:
> Hi Eliezer,
>
> I am not aware of a tool that has all functionality that you seek so you probably have to make it yourself.
> I know that you are already familiar with ufdbGuard for Squid to block access, but you can also use ufdbGuard for temporary access by including a time-restricted whitelist in the configuration file
> and doing a reload of the ufdbGuard configuration.  The reload does not interrupt the function of the web proxy or ufdbGuard itself.
>
> Marcus
>
> On 09/02/2024 03:41,mailto:ngtech1ltd at gmail.com  wrote:
>> Hey Everybody,
>>
>> I am just releasing the latest 6.7 RPMs and binaries while running couple tests and I was wondering if this was done.
>> As I am looking at proxy, in most cases it's being used as a policy enforcer rather than an education tool.
>> I believe in education as one of the top priorities compared to enforcing policies.
>> The nature of policies depends on the environment and the risks but eventually understanding the meaning of the policy
>> gives a lot to the cooperation of the user or an employee.
>>
>> I have yet to see a solution like the next:
>> Each user has a profile/user which when receiving a policy block will be prompted with an option to allow temporarily
>> the specific site or domain.
>> Also, I have not seen an implementation which allows the user to disable or lower the policy strictness for a short period of time.
>>
>> I am looking for such implementations if those exist already to run education sessions with teenagers.
>>
>> Thanks,
>> Eliezer
>>
>> _______________________________________________
>> squid-users mailing list
>> mailto:squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> mailto:squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
>
>

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240212/08076a2d/attachment.htm>

From squid at borrill.org.uk  Mon Feb 12 11:46:56 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Mon, 12 Feb 2024 11:46:56 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
Message-ID: <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>

On 16/01/2024 14:37, Alex Rousskov wrote:
> On 2024-01-16 06:01, Stephen Borrill wrote:
>> The problem is no different with 6.6. Is there any more debugging I 
>> can provide, Alex?
> 
> Yes, but I need to give you a patch that adds that (temporary) debugging 
> first (assuming I fail to reproduce the problem in the lab). The ball is 
> on my side (unless somebody else steps in). Unfortunately, I do not have 
> any free time for any of that right now. If you do not hear from me 
> sooner, please ping me again on or after February 8, 2024.

PING!

I will get 6.7 compiled up so we can add debugging to it quickly. It 
would be good if we could get something in place this week as it is 
school holidays next week in the UK and so there will be little 
opportunity to test until afterwards.

>> On 10/01/2024 12:40, Stephen Borrill wrote:
>>> On 09/01/2024 15:42, Alex Rousskov wrote:
>>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected 
>>>>>>>> via IPv4-only periodically fails to connect to the destination 
>>>>>>>> and then requires a restart to fix it (reload is not sufficient).
>>>>>>>>
>>>>>>>> The problem appears to be that a host that has one address each 
>>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go missing as 
>>>>>>>> a destination. On closer inspection, this appears to happen when 
>>>>>>>> the IPv6 address (not the IPv4) address is marked as bad.
>>>>
>>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>>>> 216.239.38.120 #1/2-0
>>>>
>>>>
>>>> Thank you for sharing more debugging info!
>>>
>>> The following seemed odd to. It finds an IPv4 address (this host does 
>>> not have IPv6), puts it in the cache and then says "No DNS records":
>>>
>>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>> schoolbase.online
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
>>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
>>> 'schoolbase.online'
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
>>> answers for schoolbase.online
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>> 20.54.32.34 in [no cached IPs]
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>> 20.54.32.34 in [no cached IPs]
>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use 
>>> first 69 from RR TTL 69
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
>>> schoolbase.online #1 20.54.32.34
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> 20.54.32.34
>>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector72389 found conn564274 local=0.0.0.0 
>>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for 
>>> schoolbase.online:443
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: 
>>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) 
>>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>> 0x1b7381f38? lookup_err=No DNS records
>>>
>>> It seemed to happen about the same time as the other failure, so 
>>> perhaps another symptom of the same.
>>>
>>>> The above log line is self-contradictory AFAICT: It says that the 
>>>> cache has both IPv6-looking and IPv4-looking address at the same 
>>>> cache position (0) and, judging by the corresponding code, those two 
>>>> IP addresses are equal. This is not possible (for those specific IP 
>>>> address values). The subsequent Squid behavior can be explained by 
>>>> this (unexplained) conflict.
>>>>
>>>> I assume you are running official Squid v6.5 code.
>>>
>>> Yes, compiled from source on NetBSD. I have the patch I refer to here 
>>> applied too:
>>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
>>>
>>>> I can suggest the following two steps for going forward:
>>>>
>>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
>>>
>>> I have just upgraded to 6.6.
>>>
>>>> 2. If the problem is still there, patch the latest Squid v6 to add 
>>>> more debugging in hope to explain what is going on. This may take a 
>>>> few iterations, and it will take me some time to produce the 
>>>> necessary debugging patch.
>>>
>>> Unfortunately, I don't have a test case that will cause the problem 
>>> so I need to run this at a customer's production site that is 
>>> particularly affected by it. Luckily, the problem recurs pretty quickly.
>>>
>>> Here's a run with 6.6 where the number of destinations drops from 2 
>>> to 1 before reverting. Not seen this before - usually once it has 
>>> dropped to 1 (the IPv6 address), it stays there until a restart (and 
>>> this did happen about a minute after this log fragment). Happy to 
>>> test out any debugging patch.
>>>
>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>> forcesafesearch.google.com
>>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>> provided or not known
>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>> 'forcesafesearch.google.com'
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> [2001:4860:4802:32::78]
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300176 found conn2388484 local=[::] 
>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>> #1 for forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> 216.239.38.120
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300176 found conn2388485 local=0.0.0.0 
>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>> 0x12208e038
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector300176 found all 2 destinations for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) 
>>> resolveSelected: ?? always_direct = ALLOWED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) 
>>> resolveSelected: ??? never_direct = DENIED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) 
>>> resolveSelected: ??????? timedout = 0
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>> 216.239.38.120:443 of forcesafesearch.google.com
>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>> provided or not known
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) 
>>> resolveSelected: Find IP destination for: 
>>> forcesafesearch.google.com:443' via forcesafesearch.google.com
>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>> forcesafesearch.google.com
>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>> provided or not known
>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>> 'forcesafesearch.google.com'
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> [2001:4860:4802:32::78]
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300177 found conn2388493 local=[::] 
>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>> #1 for forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> 216.239.38.120
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300177 found conn2388494 local=0.0.0.0 
>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>> 0x12208e038
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector300177 found all 2 destinations for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) 
>>> resolveSelected: ?? always_direct = ALLOWED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) 
>>> resolveSelected: ??? never_direct = DENIED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) 
>>> resolveSelected: ??????? timedout = 0
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
> 

-- 
Dr. Stephen Borrill, Director and Solutions Architect
Precedence Technologies Ltd              T: +44 (0) 1223 359 900
Technology House, 36a Union Lane         E: sborrill at precedence.co.uk
Cambridge, CB4 1QB, United Kingdom       W: http://www.precedence.co.uk/
Limited company registered in England and Wales. Company number 3725626



From ngtech1ltd at gmail.com  Mon Feb 12 11:48:54 2024
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 12 Feb 2024 13:48:54 +0200
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
 <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>
Message-ID: <CABA8h=Sq7ceHBOwvX0rxfZ8OJKJ8Lz0KOz=KNKWaQ9+hc7_XbQ@mail.gmail.com>

What distro are you using?

?????? ??? ??, 12 ????? 2024, 13:47, ??? Stephen Borrill ?<
squid at borrill.org.uk>:

> On 16/01/2024 14:37, Alex Rousskov wrote:
> > On 2024-01-16 06:01, Stephen Borrill wrote:
> >> The problem is no different with 6.6. Is there any more debugging I
> >> can provide, Alex?
> >
> > Yes, but I need to give you a patch that adds that (temporary) debugging
> > first (assuming I fail to reproduce the problem in the lab). The ball is
> > on my side (unless somebody else steps in). Unfortunately, I do not have
> > any free time for any of that right now. If you do not hear from me
> > sooner, please ping me again on or after February 8, 2024.
>
> PING!
>
> I will get 6.7 compiled up so we can add debugging to it quickly. It
> would be good if we could get something in place this week as it is
> school holidays next week in the UK and so there will be little
> opportunity to test until afterwards.
>
> >> On 10/01/2024 12:40, Stephen Borrill wrote:
> >>> On 09/01/2024 15:42, Alex Rousskov wrote:
> >>>> On 2024-01-09 05:56, Stephen Borrill wrote:
> >>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
> >>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
> >>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
> >>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected
> >>>>>>>> via IPv4-only periodically fails to connect to the destination
> >>>>>>>> and then requires a restart to fix it (reload is not sufficient).
> >>>>>>>>
> >>>>>>>> The problem appears to be that a host that has one address each
> >>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go missing as
> >>>>>>>> a destination. On closer inspection, this appears to happen when
> >>>>>>>> the IPv6 address (not the IPv4) address is marked as bad.
> >>>>
> >>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in
> >>>>> 216.239.38.120 #1/2-0
> >>>>
> >>>>
> >>>> Thank you for sharing more debugging info!
> >>>
> >>> The following seemed odd to. It finds an IPv4 address (this host does
> >>> not have IPv6), puts it in the cache and then says "No DNS records":
> >>>
> >>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname:
> >>> schoolbase.online
> >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease:
> >>> ipcacheRelease: Releasing entry for 'schoolbase.online'
> >>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670)
> >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for
> >>> 'schoolbase.online'
> >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1
> >>> answers for schoolbase.online
> >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:  no
> >>> 20.54.32.34 in [no cached IPs]
> >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:  no
> >>> 20.54.32.34 in [no cached IPs]
> >>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use
> >>> first 69 from RR TTL 69
> >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood:
> >>> schoolbase.online #1 20.54.32.34
> >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp:
> >>> 20.54.32.34
> >>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath:
> >>> PeerSelector72389 found conn564274 local=0.0.0.0
> >>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for
> >>> schoolbase.online:443
> >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError:
> >>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
> >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586)
> >>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
> >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback:
> >>> 0x1b7381f38  lookup_err=No DNS records
> >>>
> >>> It seemed to happen about the same time as the other failure, so
> >>> perhaps another symptom of the same.
> >>>
> >>>> The above log line is self-contradictory AFAICT: It says that the
> >>>> cache has both IPv6-looking and IPv4-looking address at the same
> >>>> cache position (0) and, judging by the corresponding code, those two
> >>>> IP addresses are equal. This is not possible (for those specific IP
> >>>> address values). The subsequent Squid behavior can be explained by
> >>>> this (unexplained) conflict.
> >>>>
> >>>> I assume you are running official Squid v6.5 code.
> >>>
> >>> Yes, compiled from source on NetBSD. I have the patch I refer to here
> >>> applied too:
> >>>
> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
> >>>
> >>>> I can suggest the following two steps for going forward:
> >>>>
> >>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
> >>>
> >>> I have just upgraded to 6.6.
> >>>
> >>>> 2. If the problem is still there, patch the latest Squid v6 to add
> >>>> more debugging in hope to explain what is going on. This may take a
> >>>> few iterations, and it will take me some time to produce the
> >>>> necessary debugging patch.
> >>>
> >>> Unfortunately, I don't have a test case that will cause the problem
> >>> so I need to run this at a customer's production site that is
> >>> particularly affected by it. Luckily, the problem recurs pretty
> quickly.
> >>>
> >>> Here's a run with 6.6 where the number of destinations drops from 2
> >>> to 1 before reverting. Not seen this before - usually once it has
> >>> dropped to 1 (the IPv6 address), it stays there until a restart (and
> >>> this did happen about a minute after this log fragment). Happy to
> >>> test out any debugging patch.
> >>>
> >>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname:
> >>> forcesafesearch.google.com
> >>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP:
> >>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not
> >>> provided or not known
> >>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657)
> >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for
> >>> 'forcesafesearch.google.com'
> >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp:
> >>> [2001:4860:4802:32::78]
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath:
> >>> PeerSelector300176 found conn2388484 local=[::]
> >>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination
> >>> #1 for forcesafesearch.google.com:443
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath:
> >>> always_direct = ALLOWED
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath:
> >>> never_direct = DENIED
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath:
> >>> timedout = 0
> >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp:
> >>> 216.239.38.120
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath:
> >>> PeerSelector300176 found conn2388485 local=0.0.0.0
> >>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for
> >>> forcesafesearch.google.com:443
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath:
> >>> always_direct = ALLOWED
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath:
> >>> never_direct = DENIED
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath:
> >>> timedout = 0
> >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback:
> >>> 0x12208e038
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479)
> >>> resolveSelected: PeerSelector300176 found all 2 destinations for
> >>> forcesafesearch.google.com:443
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480)
> >>> resolveSelected:    always_direct = ALLOWED
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481)
> >>> resolveSelected:     never_direct = DENIED
> >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482)
> >>> resolveSelected:         timedout = 0
> >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have:
> >>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
> >>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad:
> >>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
> >>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have:
> >>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
> >>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking:
> >>> 216.239.38.120:443 of forcesafesearch.google.com
> >>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP:
> >>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not
> >>> provided or not known
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460)
> >>> resolveSelected: Find IP destination for:
> >>> forcesafesearch.google.com:443' via forcesafesearch.google.com
> >>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname:
> >>> forcesafesearch.google.com
> >>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP:
> >>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not
> >>> provided or not known
> >>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657)
> >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for
> >>> 'forcesafesearch.google.com'
> >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp:
> >>> [2001:4860:4802:32::78]
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath:
> >>> PeerSelector300177 found conn2388493 local=[::]
> >>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination
> >>> #1 for forcesafesearch.google.com:443
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath:
> >>> always_direct = ALLOWED
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath:
> >>> never_direct = DENIED
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath:
> >>> timedout = 0
> >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp:
> >>> 216.239.38.120
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath:
> >>> PeerSelector300177 found conn2388494 local=0.0.0.0
> >>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for
> >>> forcesafesearch.google.com:443
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath:
> >>> always_direct = ALLOWED
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath:
> >>> never_direct = DENIED
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath:
> >>> timedout = 0
> >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback:
> >>> 0x12208e038
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479)
> >>> resolveSelected: PeerSelector300177 found all 2 destinations for
> >>> forcesafesearch.google.com:443
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480)
> >>> resolveSelected:    always_direct = ALLOWED
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481)
> >>> resolveSelected:     never_direct = DENIED
> >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482)
> >>> resolveSelected:         timedout = 0
> >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have:
> >>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
> >>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad:
> >>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
> >>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have:
> >>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
> >>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking:
> >>> 216.239.38.120:443 of forcesafesearch.google.com
> >>>
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> https://lists.squid-cache.org/listinfo/squid-users
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
> >
>
> --
> Dr. Stephen Borrill, Director and Solutions Architect
> Precedence Technologies Ltd              T: +44 (0) 1223 359 900
> Technology House, 36a Union Lane         E: sborrill at precedence.co.uk
> Cambridge, CB4 1QB, United Kingdom       W: http://www.precedence.co.uk/
> Limited company registered in England and Wales. Company number 3725626
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240212/5cb8d6a0/attachment.htm>

From squid at borrill.org.uk  Mon Feb 12 12:02:02 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Mon, 12 Feb 2024 12:02:02 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <CABA8h=Sq7ceHBOwvX0rxfZ8OJKJ8Lz0KOz=KNKWaQ9+hc7_XbQ@mail.gmail.com>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
 <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>
 <CABA8h=Sq7ceHBOwvX0rxfZ8OJKJ8Lz0KOz=KNKWaQ9+hc7_XbQ@mail.gmail.com>
Message-ID: <2bff1730-8595-4224-a89d-f79f9f592321@borrill.org.uk>

On 12/02/2024 11:48, NgTech LTD wrote:
> What distro are you using?

NetBSD 9.2_STABLE and building with gcc 8.5.0

> ?????? ??? ??, 12 ????? 2024, 13:47, ??? Stephen Borrill 
> ?<squid at borrill.org.uk <mailto:squid at borrill.org.uk>>:
> 
>     On 16/01/2024 14:37, Alex Rousskov wrote:
>      > On 2024-01-16 06:01, Stephen Borrill wrote:
>      >> The problem is no different with 6.6. Is there any more debugging I
>      >> can provide, Alex?
>      >
>      > Yes, but I need to give you a patch that adds that (temporary)
>     debugging
>      > first (assuming I fail to reproduce the problem in the lab). The
>     ball is
>      > on my side (unless somebody else steps in). Unfortunately, I do
>     not have
>      > any free time for any of that right now. If you do not hear from me
>      > sooner, please ping me again on or after February 8, 2024.
> 
>     PING!
> 
>     I will get 6.7 compiled up so we can add debugging to it quickly. It
>     would be good if we could get something in place this week as it is
>     school holidays next week in the UK and so there will be little
>     opportunity to test until afterwards.
> 
>      >> On 10/01/2024 12:40, Stephen Borrill wrote:
>      >>> On 09/01/2024 15:42, Alex Rousskov wrote:
>      >>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>      >>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>      >>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>      >>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>      >>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5)
>     connected
>      >>>>>>>> via IPv4-only periodically fails to connect to the
>     destination
>      >>>>>>>> and then requires a restart to fix it (reload is not
>     sufficient).
>      >>>>>>>>
>      >>>>>>>> The problem appears to be that a host that has one address
>     each
>      >>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go
>     missing as
>      >>>>>>>> a destination. On closer inspection, this appears to
>     happen when
>      >>>>>>>> the IPv6 address (not the IPv4) address is marked as bad.
>      >>>>
>      >>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in
>      >>>>> 216.239.38.120 #1/2-0
>      >>>>
>      >>>>
>      >>>> Thank you for sharing more debugging info!
>      >>>
>      >>> The following seemed odd to. It finds an IPv4 address (this
>     host does
>      >>> not have IPv6), puts it in the cache and then says "No DNS
>     records":
>      >>>
>      >>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617)
>     nbgethostbyname:
>      >>> schoolbase.online
>      >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313)
>     ipcacheRelease:
>      >>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>      >>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670)
>      >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for
>      >>> 'schoolbase.online'
>      >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480)
>     ipcacheParse: 1
>      >>> answers for schoolbase.online
>      >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no
>      >>> 20.54.32.34 in [no cached IPs]
>      >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no
>      >>> 20.54.32.34 in [no cached IPs]
>      >>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use
>      >>> first 69 from RR TTL 69
>      >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood:
>      >>> schoolbase.online #1 20.54.32.34
>      >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp:
>      >>> 20.54.32.34
>      >>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174)
>     handlePath:
>      >>> PeerSelector72389 found conn564274 local=0.0.0.0
>      >>> remote=20.54.32.34:443 <http://20.54.32.34:443> HIER_DIRECT
>     flags=1, destination #1 for
>      >>> schoolbase.online:443
>      >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError:
>      >>> ERROR: DNS failure while resolving schoolbase.online: No DNS
>     records
>      >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586)
>      >>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>      >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback:
>      >>> 0x1b7381f38? lookup_err=No DNS records
>      >>>
>      >>> It seemed to happen about the same time as the other failure, so
>      >>> perhaps another symptom of the same.
>      >>>
>      >>>> The above log line is self-contradictory AFAICT: It says that the
>      >>>> cache has both IPv6-looking and IPv4-looking address at the same
>      >>>> cache position (0) and, judging by the corresponding code,
>     those two
>      >>>> IP addresses are equal. This is not possible (for those
>     specific IP
>      >>>> address values). The subsequent Squid behavior can be
>     explained by
>      >>>> this (unexplained) conflict.
>      >>>>
>      >>>> I assume you are running official Squid v6.5 code.
>      >>>
>      >>> Yes, compiled from source on NetBSD. I have the patch I refer
>     to here
>      >>> applied too:
>      >>>
>     https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html <https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html>
>      >>>
>      >>>> I can suggest the following two steps for going forward:
>      >>>>
>      >>>> 1. Upgrade to the latest Squid v6 in hope that the problem
>     goes away.
>      >>>
>      >>> I have just upgraded to 6.6.
>      >>>
>      >>>> 2. If the problem is still there, patch the latest Squid v6 to
>     add
>      >>>> more debugging in hope to explain what is going on. This may
>     take a
>      >>>> few iterations, and it will take me some time to produce the
>      >>>> necessary debugging patch.
>      >>>
>      >>> Unfortunately, I don't have a test case that will cause the
>     problem
>      >>> so I need to run this at a customer's production site that is
>      >>> particularly affected by it. Luckily, the problem recurs pretty
>     quickly.
>      >>>
>      >>> Here's a run with 6.6 where the number of destinations drops
>     from 2
>      >>> to 1 before reverting. Not seen this before - usually once it has
>      >>> dropped to 1 (the IPv6 address), it stays there until a restart
>     (and
>      >>> this did happen about a minute after this log fragment). Happy to
>      >>> test out any debugging patch.
>      >>>
>      >>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617)
>     nbgethostbyname:
>      >>> forcesafesearch.google.com <http://forcesafesearch.google.com>
>      >>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP:
>      >>> Given Non-IP 'forcesafesearch.google.com
>     <http://forcesafesearch.google.com>': hostname or servname not
>      >>> provided or not known
>      >>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657)
>      >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for
>      >>> 'forcesafesearch.google.com <http://forcesafesearch.google.com>'
>      >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp:
>      >>> [2001:4860:4802:32::78]
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174)
>     handlePath:
>      >>> PeerSelector300176 found conn2388484 local=[::]
>      >>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1,
>     destination
>      >>> #1 for forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180)
>     handlePath:
>      >>> always_direct = ALLOWED
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181)
>     handlePath:
>      >>> never_direct = DENIED
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182)
>     handlePath:
>      >>> timedout = 0
>      >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp:
>      >>> 216.239.38.120
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174)
>     handlePath:
>      >>> PeerSelector300176 found conn2388485 local=0.0.0.0
>      >>> remote=216.239.38.120:443 <http://216.239.38.120:443>
>     HIER_DIRECT flags=1, destination #2 for
>      >>> forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180)
>     handlePath:
>      >>> always_direct = ALLOWED
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181)
>     handlePath:
>      >>> never_direct = DENIED
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182)
>     handlePath:
>      >>> timedout = 0
>      >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback:
>      >>> 0x12208e038
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479)
>      >>> resolveSelected: PeerSelector300176 found all 2 destinations for
>      >>> forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480)
>      >>> resolveSelected: ?? always_direct = ALLOWED
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481)
>      >>> resolveSelected: ??? never_direct = DENIED
>      >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482)
>      >>> resolveSelected: ??????? timedout = 0
>      >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have:
>      >>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>      >>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad:
>      >>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>     <http://forcesafesearch.google.com>
>      >>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have:
>      >>> 216.239.38.120:443 <http://216.239.38.120:443> at 0 in
>     [2001:4860:4802:32::78] #2/2-1
>      >>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055)
>     forgetMarking:
>      >>> 216.239.38.120:443 <http://216.239.38.120:443> of
>     forcesafesearch.google.com <http://forcesafesearch.google.com>
>      >>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP:
>      >>> Given Non-IP 'forcesafesearch.google.com
>     <http://forcesafesearch.google.com>': hostname or servname not
>      >>> provided or not known
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460)
>      >>> resolveSelected: Find IP destination for:
>      >>> forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>' via
>     forcesafesearch.google.com <http://forcesafesearch.google.com>
>      >>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617)
>     nbgethostbyname:
>      >>> forcesafesearch.google.com <http://forcesafesearch.google.com>
>      >>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP:
>      >>> Given Non-IP 'forcesafesearch.google.com
>     <http://forcesafesearch.google.com>': hostname or servname not
>      >>> provided or not known
>      >>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657)
>      >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for
>      >>> 'forcesafesearch.google.com <http://forcesafesearch.google.com>'
>      >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp:
>      >>> [2001:4860:4802:32::78]
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174)
>     handlePath:
>      >>> PeerSelector300177 found conn2388493 local=[::]
>      >>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1,
>     destination
>      >>> #1 for forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180)
>     handlePath:
>      >>> always_direct = ALLOWED
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181)
>     handlePath:
>      >>> never_direct = DENIED
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182)
>     handlePath:
>      >>> timedout = 0
>      >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp:
>      >>> 216.239.38.120
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174)
>     handlePath:
>      >>> PeerSelector300177 found conn2388494 local=0.0.0.0
>      >>> remote=216.239.38.120:443 <http://216.239.38.120:443>
>     HIER_DIRECT flags=1, destination #2 for
>      >>> forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180)
>     handlePath:
>      >>> always_direct = ALLOWED
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181)
>     handlePath:
>      >>> never_direct = DENIED
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182)
>     handlePath:
>      >>> timedout = 0
>      >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback:
>      >>> 0x12208e038
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479)
>      >>> resolveSelected: PeerSelector300177 found all 2 destinations for
>      >>> forcesafesearch.google.com:443
>     <http://forcesafesearch.google.com:443>
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480)
>      >>> resolveSelected: ?? always_direct = ALLOWED
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481)
>      >>> resolveSelected: ??? never_direct = DENIED
>      >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482)
>      >>> resolveSelected: ??????? timedout = 0
>      >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have:
>      >>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>      >>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad:
>      >>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>     <http://forcesafesearch.google.com>
>      >>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have:
>      >>> 216.239.38.120:443 <http://216.239.38.120:443> at 0 in
>     [2001:4860:4802:32::78] #2/2-1
>      >>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055)
>     forgetMarking:
>      >>> 216.239.38.120:443 <http://216.239.38.120:443> of
>     forcesafesearch.google.com <http://forcesafesearch.google.com>
>      >>>
>      >>
>      >> _______________________________________________
>      >> squid-users mailing list
>      >> squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      >> https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
>      >
>      > _______________________________________________
>      > squid-users mailing list
>      > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>      > https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
>      >
> 
>     -- 
>     Dr. Stephen Borrill, Director and Solutions Architect
>     Precedence Technologies Ltd? ? ? ? ? ? ? T: +44 (0) 1223 359 900
>     Technology House, 36a Union Lane? ? ? ? ?E:
>     sborrill at precedence.co.uk <mailto:sborrill at precedence.co.uk>
>     Cambridge, CB4 1QB, United Kingdom? ? ? ?W:
>     http://www.precedence.co.uk/ <http://www.precedence.co.uk/>
>     Limited company registered in England and Wales. Company number 3725626
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
> 



From robin.carlisle at framestore.com  Mon Feb 12 15:13:04 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Mon, 12 Feb 2024 15:13:04 +0000
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <a479a52a-2044-47d1-b49a-2c4c98a90edb@measurement-factory.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
 <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>
 <CANOuv9pZXMoHH-MU0m3vhi35C-Xu4sbONGsCDBcweAmq5BukaA@mail.gmail.com>
 <a479a52a-2044-47d1-b49a-2c4c98a90edb@measurement-factory.com>
Message-ID: <CANOuv9pgkg9i5qS6ZA6vnpELTTPdwViF_g3Em7VvDuuU-_zk=w@mail.gmail.com>

Hi,

I have been having success so far with the config workaround.. config
snippet :-


*max_stale 31536000 secondsrefresh_pattern . 0  20% 4320 max-stale=31536000*

When an object has expired due to max-age and the PC is offline (ethernet
unplugged), squid attempts an origin refresh and gives me :

 * 0 ::1 TCP_REFRESH_FAIL_OLD/200 35965 GET
https://widgets.api.labs.dev.framestoresignage.com/api/v1/instagram/labs/posts.json
<https://widgets.api.labs.dev.framestoresignage.com/api/v1/instagram/labs/posts.json>
- HIER_NONE/- application/json*

Previously it had been passing the 502 through to the client application.

I am continuing to test this - but it looks like I have a working solution.

Thanks again for all your help on this,

Robin




On Fri, 9 Feb 2024 at 14:31, Alex Rousskov <rousskov at measurement-factory.com>
wrote:

> On 2024-02-09 08:53, Robin Carlisle wrote:
>
> > I am trying the config workaround approach.
>
> Please keep us posted on your progress.
>
> >  Below is the config snippet I have added.    I made the
> > assumption that for the /refresh_pattern, max-stale=NN /config, the NN
> > is in minutes as per the rest of that config directive.
>
> That assumption is natural but incorrect: Unlike the anonymous
> positional min and max parameters (that use minutes), refresh_pattern
> max-stale=NN uses seconds. Documentation improvements are welcome.
>
> Said that, the workaround should still prevent the application of the
> broken default refresh_pattern max-stale=0 rule, so you should still see
> positive results for the first NN seconds of the response age.
>
> Instead of specifying max-stale=NN, consider adding refresh_pattern
> rules recommended by squid.conf.documented (and included in
> squid.cond.default). Those rules do not have max-stale options at all,
> and, hence, Squid will use (explicit or default) max_stale directive
> instead.
>
> HTH,
>
> Alex.
>
>
> > I am testing this right now
> >
> > # this should allow stale objects up to 1 year if allowed by
> > Cache-Control repsonseheaders ...
> >
> > # ... setting both options just in case
> >
> > max_stale 525600 minutes
> >
> > refresh_pattern . 0  20% 4320 max-stale=525600
> >
> >
> > Thanks again for your help
> >
> >
> > Robin
> >
> >
> >
> >
> > On Thu, 8 Feb 2024 at 17:42, Alex Rousskov
> > <rousskov at measurement-factory.com
> > <mailto:rousskov at measurement-factory.com>> wrote:
> >
> >     Hi Robin,
> >
> >           AFAICT from the logs you have privately shared and your
> >     squid.conf
> >     that you have posted earlier, your Squid overwrites
> >     stale-if-error=31536000 in the response with "refresh_pattern
> >     max-stale=0" default. That 0 value is wrong. The correct value
> >     should be
> >     taken from max_stale directive that defaults to 1 week, not zero:
> >
> >           refresh_pattern
> >           ...
> >           max-stale=NN provide a maximum staleness factor. Squid won't
> >           serve objects more stale than this even if it failed to
> >           validate the object. Default: use the max_stale global limit.
> >
> >     This wrong default is a Squid bug AFAICT. I posted an _untested_ fix
> as
> >     Squid PR 1664: https://github.com/squid-cache/squid/pull/1664
> >     <https://github.com/squid-cache/squid/pull/1664>
> >
> >     If possible, please test the corresponding patch:
> >
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch
> <
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch
> >
> >
> >     AFAICT, you can also work around that bug by configuring an explicit
> >     refresh_pattern rule with an explicit max-stale option (see
> >     squid.conf.documented for examples). I have not tested that theory
> >     either.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >     On 2024-02-07 13:45, Robin Carlisle wrote:
> >      > Hi,
> >      >
> >      > I have just started my enhanced logging journey and have a small
> >     snippet
> >      > below that might illuminate the issue ...
> >      >
> >      > /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507)
> >      > handleIMSReply: origin replied with error 502, forwarding to
> >     client due
> >      > to fail_on_validation_err/
> >      >
> >      > A few lines below in the log it looks like squid sent :-
> >      >
> >      > /2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280)
> >     sendStartOfMessage:
> >      > HTTP Client REPLY:
> >      > ---------
> >      > HTTP/1.1 502 Bad Gateway
> >      > Server: squid/5.7
> >      > Mime-Version: 1.0
> >      > Date: Wed, 07 Feb 2024 17:06:39 GMT
> >      > Content-Type: text/html;charset=utf-8
> >      > Content-Length: 3853
> >      > X-Squid-Error: ERR_READ_ERROR 0
> >      > Vary: Accept-Language
> >      > Content-Language: en
> >      > X-Cache: MISS from labs-maul-st-15
> >      > X-Cache-Lookup: HIT from labs-maul-st-15:3129
> >      > Via: 1.1 labs-maul-st-15 (squid/5.7)
> >      > Connection: close/
> >      >
> >      >
> >      > The rest of the logs are quite large and contain URLs I cannot put
> >      > here.   The logs were generated with debug_options to ALL,3.
> >      >
> >      > Any ideas?   Or should I generate more detailed logs and send them
> >      > privately?
> >      >
> >      > Thanks again,
> >      >
> >      > Robin
> >      >
> >      >
> >      >
> >      >
> >      > On Fri, 2 Feb 2024 at 11:20, Robin Carlisle
> >      > <robin.carlisle at framestore.com
> >     <mailto:robin.carlisle at framestore.com>
> >     <mailto:robin.carlisle at framestore.com
> >     <mailto:robin.carlisle at framestore.com>>>
> >      > wrote:
> >      >
> >      >     Hi, thanks for your reply.
> >      >
> >      >     I have been looking at :
> >      >
> >
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control> <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>>
> >      >
> >      >     /The stale-if-error response directive indicates that the
> >     cache can
> >      >     reuse a stale response when an upstream server generates an
> >     error,
> >      >     or when the error is generated locally. Here, an error is
> >     considered
> >      >     any response with a status code of 500, 502, 503, or 504.
> >      >
> >      >     Cache-Control: max-age=604800, stale-if-error=86400
> >      >     In the example above, the response is fresh for 7 days
> (604800s).
> >      >     Afterwards, it becomes stale, but can be used for an extra 1
> day
> >      >     (86400s) when an error is encountered.
> >      >
> >      >     After the stale-if-error period passes, the client will
> >     receive any
> >      >     error generated/
> >      >
> >      >     Given what you have said and what the above docs say - I am
> still
> >      >     confused as it looks like (in my test cases) the cached
> >     response can
> >      >     be used for 3600 secs (this works), after which the cached
> >     response
> >      >     can still be used for an additional 31536000 seconds on an
> error
> >      >     (this doesnt work).
> >      >
> >      >     I am going to dig into the error logging you suggested to see
> >     if I
> >      >     can make sense of that - and will send on if I can't.
> >      >
> >      >     Thanks v much for your help again,
> >      >
> >      >     Robin
> >      >
> >      >
> >      >
> >      >
> >      >
> >      >     On Thu, 1 Feb 2024 at 18:27, Alex Rousskov
> >      >     <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>
> >      >     <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>>> wrote:
> >      >
> >      >         On 2024-02-01 12:03, Robin Carlisle wrote:
> >      >          > Hi, I am having trouble with stale-if-error response.
> >      >
> >      >         If I am interpreting Squid code correctly, in primary use
> >     cases:
> >      >
> >      >         * without a Cache-Control:stale-if-error=X in the original
> >      >         response,
> >      >         Squid sends a stale object if revalidation results in a
> >     5xx error;
> >      >
> >      >         * with a Cache-Control:stale-if-error=X and object age at
> >     most
> >      >         X, Squid
> >      >         sends a stale object if revalidation results in a 5xx
> error;
> >      >
> >      >         * with a Cache-Control:stale-if-error=X and object age
> >     exceeding X,
> >      >         Squid forwards the 5xx error response if revalidation
> >     results in
> >      >         a 5xx
> >      >         error;
> >      >
> >      >         In other words, stale-if-error=X turns on a "fail on
> >     validation
> >      >         errors"
> >      >         behavior for stale objects older than X. It has no other
> >     effects.
> >      >
> >      >         In your test case, the stale objects are much younger than
> >      >         stale-if-error value (e.g., Age~=3601 vs.
> >     stale-if-error=31536000).
> >      >         Thus, stale-if-error should have no relevant effect.
> >      >
> >      >         Something else is probably preventing your Squid from
> serving
> >      >         the stale
> >      >         response when facing a 5xx error. I do not know what that
> >      >         something is.
> >      >
> >      >         I recommend sharing (privately if you need to protect
> >     sensitive
> >      >         info) a
> >      >         pointer to a compressed ALL,9 cache.log collected while
> >      >         reproducing the
> >      >         problem (using two transactions similar to the ones you
> >     have shared
> >      >         below -- a successful stale hit and a problematic one):
> >      >
> >
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> >>
> >      >
> >      >         Alternatively, you can try to study cache.log yourself
> after
> >      >         setting
> >      >         debug_options to ALL,3. Searching for "refresh" and
> >      >         "handleIMSReply" may
> >      >         yield enough clues.
> >      >
> >      >
> >      >         HTH,
> >      >
> >      >         Alex.
> >      >
> >      >
> >      >
> >      >
> >      >          > # /etc/squid/squid.conf :
> >      >          >
> >      >          > acl to_aws dstdomain .amazonaws.com
> >     <http://amazonaws.com> <http://amazonaws.com <http://amazonaws.com>>
> >      >         <http://amazonaws.com <http://amazonaws.com>
> >     <http://amazonaws.com <http://amazonaws.com>>>
> >      >          >
> >      >          > acl from_local src localhost
> >      >          >
> >      >          > http_access allow to_aws
> >      >          >
> >      >          > http_access allow from_local
> >      >          >
> >      >          > cache allow all
> >      >          >
> >      >          > cache_dir ufs /var/cache/squid 1024 16 256
> >      >          >
> >      >          > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
> >      >          > generate-host-certificates=on
> >     dynamic_cert_mem_cache_size=4MB
> >      >          >
> >      >          > sslcrtd_program /usr/lib/squid/security_file_certgen -s
> >      >          > /var/lib/squid/ssl_db -M 4MB
> >      >          >
> >      >          > acl step1 at_step SslBump1
> >      >          >
> >      >          > ssl_bump bump step1
> >      >          >
> >      >          > ssl_bump bump all
> >      >          >
> >      >          > sslproxy_cert_error deny all
> >      >          >
> >      >          > cache_store_log stdio:/var/log/squid/store.log
> >      >          >
> >      >          > logfile_rotate 0
> >      >          >
> >      >          > shutdown_lifetime 3 seconds
> >      >          >
> >      >          >
> >      >          > # /usr/bin/proxy-test :
> >      >          >
> >      >          > #!/bin/bash
> >      >          >
> >      >          > curl --proxy http://localhost:3129
> >     <http://localhost:3129> <http://localhost:3129 <
> http://localhost:3129>>
> >      >         <http://localhost:3129 <http://localhost:3129>
> >     <http://localhost:3129 <http://localhost:3129>>> \
> >      >          >
> >      >          >    --cacert /etc/squid/stuff.pem \
> >      >          >
> >      >          >    -v
> >     "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >      >         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>
> >      >          > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >      >         <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>>" \
> >      >          >
> >      >          >    -H "Authorization: token MYTOKEN" \
> >      >          >
> >      >          >    -H "Content-Type: application/json" \
> >      >          >
> >      >          >    --output "/tmp/stuff.json"
> >      >          >
> >      >          >
> >      >          >
> >      >          > Tests  ..........
> >      >          >
> >      >          >
> >      >          > At this point in time the network cable is
> >     unattached.  Squid
> >      >         returns
> >      >          > the cached object it got when the network was online
> >     earlier.
> >      >         The Age of
> >      >          > this object is just still under the max_age of 3600.
> >      >         Previously I
> >      >          > was using offline_mode but I found that it did not try
> to
> >      >         revalidate
> >      >          > from the origin after the object expired (defined via
> >     max-age
> >      >         response).
> >      >          >    My understanding is that stale-if-error should work
> >     under my
> >      >          > circumstances.
> >      >          >
> >      >          >
> >      >          > # /var/log/squid/access.log
> >      >          >
> >      >          > 1706799404.440      6 127.0.0.1 NONE_NONE/200 0 CONNECT
> >      >          > stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443> <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>
> >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>> - HIER_NONE/- -
> >      >          >
> >      >          > 1706799404.440      0 127.0.0.1 TCP_MEM_HIT/200 20726
> GET
> >      >          > https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>
> >      >          > <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>> - HIER_NONE/-
> >      >         application/json
> >      >          >
> >      >          >
> >      >          > # extract from /usr/bin/proxy-test
> >      >          >
> >      >          > < HTTP/1.1 200 OK
> >      >          >
> >      >          > < Date: Thu, 01 Feb 2024 13:57:11 GMT
> >      >          >
> >      >          > < Content-Type: application/json
> >      >          >
> >      >          > < Content-Length: 20134
> >      >          >
> >      >          > < x-amzn-RequestId:
> 3a2d3b26-df73-4b30-88cb-1a9268fa0df2
> >      >          >
> >      >          > < Last-Modified: 2024-02-01T13:00:45.000Z
> >      >          >
> >      >          > < Access-Control-Allow-Origin: *
> >      >          >
> >      >          > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
> >      >          >
> >      >          > < Cache-Control: public, max-age=3600,
> >     stale-if-error=31536000
> >      >          >
> >      >          > < ETag: "cec102b43372840737ab773c2e77858b"
> >      >          >
> >      >          > < X-Amzn-Trace-Id:
> >     Root=1-65bba337-292be751134161b03555cdd6
> >      >          >
> >      >          > < Age: 3573
> >      >          >
> >      >          > < X-Cache: HIT from labs-maul-st-31
> >      >          >
> >      >          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >      >          >
> >      >          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >      >          >
> >      >          > < Connection: keep-alive
> >      >          >
> >      >          >
> >      >          >
> >      >          >
> >      >          > Below .. the curl script executes again.  The Age has
> gone
> >      >         over the
> >      >          > max-age so squid attempted to refresh from the
> >     origin.  The
> >      >         machine is
> >      >          > still offline so the refresh failed.   I expected that
> the
> >      >          > stale-if-error response would instruct squid to return
> the
> >      >         cached object
> >      >          > as a 200.
> >      >          >
> >      >          >
> >      >          > # /var/log/squid/access.log
> >      >          >
> >      >          > 1706799434.464      5 127.0.0.1 NONE_NONE/200 0 CONNECT
> >      >          > stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443> <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>
> >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>> - HIER_NONE/- -
> >      >          >
> >      >          > 1706799434.464      0 127.0.0.1
> >     TCP_REFRESH_FAIL_ERR/502 4235
> >      >         GET
> >      >          > https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>
> >      >          > <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>> - HIER_NONE/- text/html
> >      >          >
> >      >          >
> >      >          > # extract from /usr/bin/proxy-test
> >      >          >
> >      >          > < HTTP/1.1 502 Bad Gateway
> >      >          >
> >      >          > < Server: squid/5.7
> >      >          >
> >      >          > < Mime-Version: 1.0
> >      >          >
> >      >          > < Date: Thu, 01 Feb 2024 14:57:14 GMT
> >      >          >
> >      >          > < Content-Type: text/html;charset=utf-8
> >      >          >
> >      >          > < Content-Length: 3853
> >      >          >
> >      >          > < X-Squid-Error: ERR_READ_ERROR 0
> >      >          >
> >      >          > < Vary: Accept-Language
> >      >          >
> >      >          > < Content-Language: en
> >      >          >
> >      >          > < X-Cache: MISS from labs-maul-st-31
> >      >          >
> >      >          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >      >          >
> >      >          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >      >          >
> >      >          > < Connection: close
> >      >          >
> >      >          >
> >      >          >
> >      >          > Hope someone can help me with this.  All the best,
> >      >          >
> >      >          >
> >      >          > Robin Carlisle
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240212/4e6c92eb/attachment.htm>

From rousskov at measurement-factory.com  Mon Feb 12 16:06:22 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Feb 2024 11:06:22 -0500
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <CANOuv9pgkg9i5qS6ZA6vnpELTTPdwViF_g3Em7VvDuuU-_zk=w@mail.gmail.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
 <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>
 <CANOuv9pZXMoHH-MU0m3vhi35C-Xu4sbONGsCDBcweAmq5BukaA@mail.gmail.com>
 <a479a52a-2044-47d1-b49a-2c4c98a90edb@measurement-factory.com>
 <CANOuv9pgkg9i5qS6ZA6vnpELTTPdwViF_g3Em7VvDuuU-_zk=w@mail.gmail.com>
Message-ID: <dc725a10-4204-45f4-9b23-96f8421ce842@measurement-factory.com>

On 2024-02-12 10:13, Robin Carlisle wrote:

> I have been having success so far with the config workaround.. config 
> snippet :-
> 
> /max_stale 31536000 seconds
> refresh_pattern . 0 ?20% 4320 max-stale=31536000/
> 
> When an object has expired due to max-age and the PC is offline 
> (ethernet unplugged), squid attempts an origin refresh and gives me :
> 
> /0 ::1 TCP_REFRESH_FAIL_OLD/200 35965 GET 
> https://widgets.api.labs.dev.framestoresignage.com/api/v1/instagram/labs/posts.json <https://widgets.api.labs.dev.framestoresignage.com/api/v1/instagram/labs/posts.json> - HIER_NONE/- application/json/
> 
> Previously it had been passing the 502 through to the client application.

Glad this workaround helps. Just keep in mind that the configuration 
snippet above changes max-stale for _all_ responses.


> I am continuing to test this - but it looks like I have a working solution.

Meanwhile, the fix for the underlying Squid bug was officially accepted 
and should become a part of v6.8 release (at least).


Thank you,

Alex.


> On Fri, 9 Feb 2024 at 14:31, Alex Rousskov wrote:
> 
>     On 2024-02-09 08:53, Robin Carlisle wrote:
> 
>      > I am trying the config workaround approach.
> 
>     Please keep us posted on your progress.
> 
>      > ?Below is the config snippet I have added.? ? I made the
>      > assumption that for the /refresh_pattern, max-stale=NN /config,
>     the NN
>      > is in minutes as per the rest of that config directive.
> 
>     That assumption is natural but incorrect: Unlike the anonymous
>     positional min and max parameters (that use minutes), refresh_pattern
>     max-stale=NN uses seconds. Documentation improvements are welcome.
> 
>     Said that, the workaround should still prevent the application of the
>     broken default refresh_pattern max-stale=0 rule, so you should still
>     see
>     positive results for the first NN seconds of the response age.
> 
>     Instead of specifying max-stale=NN, consider adding refresh_pattern
>     rules recommended by squid.conf.documented (and included in
>     squid.cond.default). Those rules do not have max-stale options at all,
>     and, hence, Squid will use (explicit or default) max_stale directive
>     instead.
> 
>     HTH,
> 
>     Alex.
> 
> 
>      > I am testing this right now
>      >
>      > # this should allow stale objects up to 1 year if allowed by
>      > Cache-Control repsonseheaders ...
>      >
>      > # ... setting both options just in case
>      >
>      > max_stale 525600 minutes
>      >
>      > refresh_pattern . 0? 20% 4320 max-stale=525600
>      >
>      >
>      > Thanks again for your help
>      >
>      >
>      > Robin
>      >
>      >
>      >
>      >
>      > On Thu, 8 Feb 2024 at 17:42, Alex Rousskov
>      > <rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      > <mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>> wrote:
>      >
>      >? ? ?Hi Robin,
>      >
>      >? ? ? ? ? ?AFAICT from the logs you have privately shared and your
>      >? ? ?squid.conf
>      >? ? ?that you have posted earlier, your Squid overwrites
>      >? ? ?stale-if-error=31536000 in the response with "refresh_pattern
>      >? ? ?max-stale=0" default. That 0 value is wrong. The correct value
>      >? ? ?should be
>      >? ? ?taken from max_stale directive that defaults to 1 week, not zero:
>      >
>      >? ? ? ? ? ?refresh_pattern
>      >? ? ? ? ? ?...
>      >? ? ? ? ? ?max-stale=NN provide a maximum staleness factor. Squid
>     won't
>      >? ? ? ? ? ?serve objects more stale than this even if it failed to
>      >? ? ? ? ? ?validate the object. Default: use the max_stale global
>     limit.
>      >
>      >? ? ?This wrong default is a Squid bug AFAICT. I posted an
>     _untested_ fix as
>      >? ? ?Squid PR 1664: https://github.com/squid-cache/squid/pull/1664
>     <https://github.com/squid-cache/squid/pull/1664>
>      >? ? ?<https://github.com/squid-cache/squid/pull/1664
>     <https://github.com/squid-cache/squid/pull/1664>>
>      >
>      >? ? ?If possible, please test the corresponding patch:
>      >
>     https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch <https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch> <https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch <https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch>>
>      >
>      >? ? ?AFAICT, you can also work around that bug by configuring an
>     explicit
>      >? ? ?refresh_pattern rule with an explicit max-stale option (see
>      >? ? ?squid.conf.documented for examples). I have not tested that
>     theory
>      >? ? ?either.
>      >
>      >
>      >? ? ?HTH,
>      >
>      >? ? ?Alex.
>      >
>      >
>      >? ? ?On 2024-02-07 13:45, Robin Carlisle wrote:
>      >? ? ? > Hi,
>      >? ? ? >
>      >? ? ? > I have just started my enhanced logging journey and have a
>     small
>      >? ? ?snippet
>      >? ? ? > below that might illuminate the issue ...
>      >? ? ? >
>      >? ? ? > /2024/02/07 17:06:39.212 kid1| 88,3| client_side_reply.cc(507)
>      >? ? ? > handleIMSReply: origin replied with error 502, forwarding to
>      >? ? ?client due
>      >? ? ? > to fail_on_validation_err/
>      >? ? ? >
>      >? ? ? > A few lines below in the log it looks like squid sent :-
>      >? ? ? >
>      >? ? ? > /2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280)
>      >? ? ?sendStartOfMessage:
>      >? ? ? > HTTP Client REPLY:
>      >? ? ? > ---------
>      >? ? ? > HTTP/1.1 502 Bad Gateway
>      >? ? ? > Server: squid/5.7
>      >? ? ? > Mime-Version: 1.0
>      >? ? ? > Date: Wed, 07 Feb 2024 17:06:39 GMT
>      >? ? ? > Content-Type: text/html;charset=utf-8
>      >? ? ? > Content-Length: 3853
>      >? ? ? > X-Squid-Error: ERR_READ_ERROR 0
>      >? ? ? > Vary: Accept-Language
>      >? ? ? > Content-Language: en
>      >? ? ? > X-Cache: MISS from labs-maul-st-15
>      >? ? ? > X-Cache-Lookup: HIT from labs-maul-st-15:3129
>      >? ? ? > Via: 1.1 labs-maul-st-15 (squid/5.7)
>      >? ? ? > Connection: close/
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > The rest of the logs are quite large and contain URLs I
>     cannot put
>      >? ? ? > here.? ?The logs were generated with debug_options to ALL,3.
>      >? ? ? >
>      >? ? ? > Any ideas?? ?Or should I generate more detailed logs and
>     send them
>      >? ? ? > privately?
>      >? ? ? >
>      >? ? ? > Thanks again,
>      >? ? ? >
>      >? ? ? > Robin
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > On Fri, 2 Feb 2024 at 11:20, Robin Carlisle
>      >? ? ? > <robin.carlisle at framestore.com
>     <mailto:robin.carlisle at framestore.com>
>      >? ? ?<mailto:robin.carlisle at framestore.com
>     <mailto:robin.carlisle at framestore.com>>
>      >? ? ?<mailto:robin.carlisle at framestore.com
>     <mailto:robin.carlisle at framestore.com>
>      >? ? ?<mailto:robin.carlisle at framestore.com
>     <mailto:robin.carlisle at framestore.com>>>>
>      >? ? ? > wrote:
>      >? ? ? >
>      >? ? ? >? ? ?Hi, thanks for your reply.
>      >? ? ? >
>      >? ? ? >? ? ?I have been looking at :
>      >? ? ? >
>      >
>     https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control> <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>> <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control> <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>>>
>      >? ? ? >
>      >? ? ? >? ? ?/The stale-if-error response directive indicates that the
>      >? ? ?cache can
>      >? ? ? >? ? ?reuse a stale response when an upstream server
>     generates an
>      >? ? ?error,
>      >? ? ? >? ? ?or when the error is generated locally. Here, an error is
>      >? ? ?considered
>      >? ? ? >? ? ?any response with a status code of 500, 502, 503, or 504.
>      >? ? ? >
>      >? ? ? >? ? ?Cache-Control: max-age=604800, stale-if-error=86400
>      >? ? ? >? ? ?In the example above, the response is fresh for 7 days
>     (604800s).
>      >? ? ? >? ? ?Afterwards, it becomes stale, but can be used for an
>     extra 1 day
>      >? ? ? >? ? ?(86400s) when an error is encountered.
>      >? ? ? >
>      >? ? ? >? ? ?After the stale-if-error period passes, the client will
>      >? ? ?receive any
>      >? ? ? >? ? ?error generated/
>      >? ? ? >
>      >? ? ? >? ? ?Given what you have said and what the above docs say -
>     I am still
>      >? ? ? >? ? ?confused as it looks like (in my test cases) the cached
>      >? ? ?response can
>      >? ? ? >? ? ?be used for 3600 secs (this works), after which the cached
>      >? ? ?response
>      >? ? ? >? ? ?can still be used for an additional 31536000 seconds
>     on an error
>      >? ? ? >? ? ?(this doesnt work).
>      >? ? ? >
>      >? ? ? >? ? ?I am going to dig into the error logging?you suggested
>     to see
>      >? ? ?if I
>      >? ? ? >? ? ?can make sense of that - and will send on if I can't.
>      >? ? ? >
>      >? ? ? >? ? ?Thanks v much for your help again,
>      >? ? ? >
>      >? ? ? >? ? ?Robin
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ?On Thu, 1 Feb 2024 at 18:27, Alex Rousskov
>      >? ? ? >? ? ?<rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>
>      >? ? ? >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>
>      >? ? ?<mailto:rousskov at measurement-factory.com
>     <mailto:rousskov at measurement-factory.com>>>> wrote:
>      >? ? ? >
>      >? ? ? >? ? ? ? ?On 2024-02-01 12:03, Robin Carlisle wrote:
>      >? ? ? >? ? ? ? ? > Hi, I am having trouble with stale-if-error
>     response.
>      >? ? ? >
>      >? ? ? >? ? ? ? ?If I am interpreting Squid code correctly, in
>     primary use
>      >? ? ?cases:
>      >? ? ? >
>      >? ? ? >? ? ? ? ?* without a Cache-Control:stale-if-error=X in the
>     original
>      >? ? ? >? ? ? ? ?response,
>      >? ? ? >? ? ? ? ?Squid sends a stale object if revalidation results
>     in a
>      >? ? ?5xx error;
>      >? ? ? >
>      >? ? ? >? ? ? ? ?* with a Cache-Control:stale-if-error=X and object
>     age at
>      >? ? ?most
>      >? ? ? >? ? ? ? ?X, Squid
>      >? ? ? >? ? ? ? ?sends a stale object if revalidation results in a
>     5xx error;
>      >? ? ? >
>      >? ? ? >? ? ? ? ?* with a Cache-Control:stale-if-error=X and object age
>      >? ? ?exceeding X,
>      >? ? ? >? ? ? ? ?Squid forwards the 5xx error response if revalidation
>      >? ? ?results in
>      >? ? ? >? ? ? ? ?a 5xx
>      >? ? ? >? ? ? ? ?error;
>      >? ? ? >
>      >? ? ? >? ? ? ? ?In other words, stale-if-error=X turns on a "fail on
>      >? ? ?validation
>      >? ? ? >? ? ? ? ?errors"
>      >? ? ? >? ? ? ? ?behavior for stale objects older than X. It has no
>     other
>      >? ? ?effects.
>      >? ? ? >
>      >? ? ? >? ? ? ? ?In your test case, the stale objects are much
>     younger than
>      >? ? ? >? ? ? ? ?stale-if-error value (e.g., Age~=3601 vs.
>      >? ? ?stale-if-error=31536000).
>      >? ? ? >? ? ? ? ?Thus, stale-if-error should have no relevant effect.
>      >? ? ? >
>      >? ? ? >? ? ? ? ?Something else is probably preventing your Squid
>     from serving
>      >? ? ? >? ? ? ? ?the stale
>      >? ? ? >? ? ? ? ?response when facing a 5xx error. I do not know
>     what that
>      >? ? ? >? ? ? ? ?something is.
>      >? ? ? >
>      >? ? ? >? ? ? ? ?I recommend sharing (privately if you need to protect
>      >? ? ?sensitive
>      >? ? ? >? ? ? ? ?info) a
>      >? ? ? >? ? ? ? ?pointer to a compressed ALL,9 cache.log collected
>     while
>      >? ? ? >? ? ? ? ?reproducing the
>      >? ? ? >? ? ? ? ?problem (using two transactions similar to the
>     ones you
>      >? ? ?have shared
>      >? ? ? >? ? ? ? ?below -- a successful stale hit and a problematic
>     one):
>      >? ? ? >
>      >
>     https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>>
>      >? ? ? >
>      >? ? ? >? ? ? ? ?Alternatively, you can try to study cache.log
>     yourself after
>      >? ? ? >? ? ? ? ?setting
>      >? ? ? >? ? ? ? ?debug_options to ALL,3. Searching for "refresh" and
>      >? ? ? >? ? ? ? ?"handleIMSReply" may
>      >? ? ? >? ? ? ? ?yield enough clues.
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ? ? ?HTH,
>      >? ? ? >
>      >? ? ? >? ? ? ? ?Alex.
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >? ? ? ? ? > # /etc/squid/squid.conf :
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > acl to_aws dstdomain .amazonaws.com
>     <http://amazonaws.com>
>      >? ? ?<http://amazonaws.com <http://amazonaws.com>>
>     <http://amazonaws.com <http://amazonaws.com> <http://amazonaws.com
>     <http://amazonaws.com>>>
>      >? ? ? >? ? ? ? ?<http://amazonaws.com <http://amazonaws.com>
>     <http://amazonaws.com <http://amazonaws.com>>
>      >? ? ?<http://amazonaws.com <http://amazonaws.com>
>     <http://amazonaws.com <http://amazonaws.com>>>>
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > acl from_local src localhost
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > http_access allow to_aws
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > http_access allow from_local
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > cache allow all
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > cache_dir ufs /var/cache/squid 1024 16 256
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
>      >? ? ? >? ? ? ? ? > generate-host-certificates=on
>      >? ? ?dynamic_cert_mem_cache_size=4MB
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > sslcrtd_program
>     /usr/lib/squid/security_file_certgen -s
>      >? ? ? >? ? ? ? ? > /var/lib/squid/ssl_db -M 4MB
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > acl step1 at_step SslBump1
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > ssl_bump bump step1
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > ssl_bump bump all
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > sslproxy_cert_error deny all
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > cache_store_log stdio:/var/log/squid/store.log
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > logfile_rotate 0
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > shutdown_lifetime 3 seconds
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > # /usr/bin/proxy-test :
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > #!/bin/bash
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > curl --proxy http://localhost:3129
>     <http://localhost:3129>
>      >? ? ?<http://localhost:3129 <http://localhost:3129>>
>     <http://localhost:3129 <http://localhost:3129>
>     <http://localhost:3129 <http://localhost:3129>>>
>      >? ? ? >? ? ? ? ?<http://localhost:3129 <http://localhost:3129>
>     <http://localhost:3129 <http://localhost:3129>>
>      >? ? ?<http://localhost:3129 <http://localhost:3129>
>     <http://localhost:3129 <http://localhost:3129>>>> \
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >? ??--cacert /etc/squid/stuff.pem \
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >? ??-v
>      >? ? ?"https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>
>      >? ? ? >       
>      ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>>
>      >? ? ? >? ? ? ? ? >
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>
>      >? ? ? >       
>      ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/api/v1/stuff/stuff.json
>     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>>>" \
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >? ??-H "Authorization: token MYTOKEN" \
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >? ??-H "Content-Type: application/json" \
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >? ??--output "/tmp/stuff.json"
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > Tests? ..........
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > At this point in time the network cable is
>      >? ? ?unattached.? Squid
>      >? ? ? >? ? ? ? ?returns
>      >? ? ? >? ? ? ? ? > the cached object it got when the network was
>     online
>      >? ? ?earlier.
>      >? ? ? >? ? ? ? ?The Age of
>      >? ? ? >? ? ? ? ? > this object is just still under the max_age of
>     3600.
>      >? ? ? >? ? ? ? ?Previously I
>      >? ? ? >? ? ? ? ? > was using offline_mode but I found that it did
>     not try to
>      >? ? ? >? ? ? ? ?revalidate
>      >? ? ? >? ? ? ? ? > from the origin after the object expired
>     (defined via
>      >? ? ?max-age
>      >? ? ? >? ? ? ? ?response).
>      >? ? ? >? ? ? ? ? >? ? My understanding is that stale-if-error
>     should work
>      >? ? ?under my
>      >? ? ? >? ? ? ? ? > circumstances.
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > # /var/log/squid/access.log
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > 1706799404.440? ? ? 6 127.0.0.1 NONE_NONE/200 0
>     CONNECT
>      >? ? ? >? ? ? ? ? > stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>> <http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>>
>      >? ? ? >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>>
>      >? ? ? >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>>> - HIER_NONE/- -
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > 1706799404.440? ? ? 0 127.0.0.1 TCP_MEM_HIT/200
>     20726 GET
>      >? ? ? >? ? ? ? ? > https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>
>      >? ? ? >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>>
>      >? ? ? >? ? ? ? ? > <https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>
>      >? ? ? >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>>> - HIER_NONE/-
>      >? ? ? >? ? ? ? ?application/json
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > # extract from /usr/bin/proxy-test
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < HTTP/1.1 200 OK
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Date: Thu, 01 Feb 2024 13:57:11 GMT
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Content-Type: application/json
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Content-Length: 20134
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < x-amzn-RequestId:
>     3a2d3b26-df73-4b30-88cb-1a9268fa0df2
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Last-Modified: 2024-02-01T13:00:45.000Z
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Access-Control-Allow-Origin: *
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Cache-Control: public, max-age=3600,
>      >? ? ?stale-if-error=31536000
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < ETag: "cec102b43372840737ab773c2e77858b"
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < X-Amzn-Trace-Id:
>      >? ? ?Root=1-65bba337-292be751134161b03555cdd6
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Age: 3573
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < X-Cache: HIT from labs-maul-st-31
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Connection: keep-alive
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > Below .. the curl script executes again.? The
>     Age has gone
>      >? ? ? >? ? ? ? ?over the
>      >? ? ? >? ? ? ? ? > max-age so squid attempted to refresh from the
>      >? ? ?origin.? The
>      >? ? ? >? ? ? ? ?machine is
>      >? ? ? >? ? ? ? ? > still offline so the refresh failed. ? I
>     expected that the
>      >? ? ? >? ? ? ? ? > stale-if-error response would instruct squid to
>     return the
>      >? ? ? >? ? ? ? ?cached object
>      >? ? ? >? ? ? ? ? > as a 200.
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > # /var/log/squid/access.log
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > 1706799434.464? ? ? 5 127.0.0.1 NONE_NONE/200 0
>     CONNECT
>      >? ? ? >? ? ? ? ? > stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>> <http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>>
>      >? ? ? >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443>>
>      >? ? ? >? ? ? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>
>      >? ? ?<http://stuff.amazonaws.com:443
>     <http://stuff.amazonaws.com:443>>>> - HIER_NONE/- -
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > 1706799434.464? ? ? 0 127.0.0.1
>      >? ? ?TCP_REFRESH_FAIL_ERR/502 4235
>      >? ? ? >? ? ? ? ?GET
>      >? ? ? >? ? ? ? ? > https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>
>      >? ? ? >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>>
>      >? ? ? >? ? ? ? ? > <https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>
>      >? ? ? >? ? ? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>
>      >? ? ?<https://stuff.amazonaws.com/stuff.json
>     <https://stuff.amazonaws.com/stuff.json>>>> - HIER_NONE/- text/html
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > # extract from /usr/bin/proxy-test
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < HTTP/1.1 502 Bad Gateway
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Server: squid/5.7
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Mime-Version: 1.0
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Date: Thu, 01 Feb 2024 14:57:14 GMT
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Content-Type: text/html;charset=utf-8
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Content-Length: 3853
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < X-Squid-Error: ERR_READ_ERROR 0
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Vary: Accept-Language
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Content-Language: en
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < X-Cache: MISS from labs-maul-st-31
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Via: 1.1 labs-maul-st-31 (squid/5.7)
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > < Connection: close
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > Hope someone can help me with this.? All the best,
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? >
>      >? ? ? >? ? ? ? ? > Robin Carlisle
> 



From robin.carlisle at framestore.com  Mon Feb 12 16:08:32 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Mon, 12 Feb 2024 16:08:32 +0000
Subject: [squid-users] stale-if-error returning a 502
In-Reply-To: <dc725a10-4204-45f4-9b23-96f8421ce842@measurement-factory.com>
References: <CANOuv9oUdha+v1oUTwY+kE1xV9bidp_G3ynyiqOfsAdXqWB3Dw@mail.gmail.com>
 <7c8590f0-8c3f-43f1-b150-0c5c37a7e403@measurement-factory.com>
 <CANOuv9rsG+A+MN77E4AadTFm4OxzJcMkmZ4FQWuTTi8vJvzaLA@mail.gmail.com>
 <CANOuv9q5F5-8253r0qY96sCJX2drnVshun2o0EomXE2FCtdZ2Q@mail.gmail.com>
 <6b0bc1fb-87df-4dda-b4da-eef3a5749050@measurement-factory.com>
 <CANOuv9pZXMoHH-MU0m3vhi35C-Xu4sbONGsCDBcweAmq5BukaA@mail.gmail.com>
 <a479a52a-2044-47d1-b49a-2c4c98a90edb@measurement-factory.com>
 <CANOuv9pgkg9i5qS6ZA6vnpELTTPdwViF_g3Em7VvDuuU-_zk=w@mail.gmail.com>
 <dc725a10-4204-45f4-9b23-96f8421ce842@measurement-factory.com>
Message-ID: <CANOuv9ogsnzW-yxNERBdo-+GweZ45swZSyPD2_OuiEeOgvxksw@mail.gmail.com>

Thanks for clarification on max-stale, although it is unintentionally ideal
for my use-case.
Best,
Robin


On Mon, 12 Feb 2024 at 16:06, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2024-02-12 10:13, Robin Carlisle wrote:
>
> > I have been having success so far with the config workaround.. config
> > snippet :-
> >
> > /max_stale 31536000 seconds
> > refresh_pattern . 0  20% 4320 max-stale=31536000/
> >
> > When an object has expired due to max-age and the PC is offline
> > (ethernet unplugged), squid attempts an origin refresh and gives me :
> >
> > /0 ::1 TCP_REFRESH_FAIL_OLD/200 35965 GET
> >
> https://widgets.api.labs.dev.framestoresignage.com/api/v1/instagram/labs/posts.json
> <
> https://widgets.api.labs.dev.framestoresignage.com/api/v1/instagram/labs/posts.json>
> - HIER_NONE/- application/json/
> >
> > Previously it had been passing the 502 through to the client application.
>
> Glad this workaround helps. Just keep in mind that the configuration
> snippet above changes max-stale for _all_ responses.
>
>
> > I am continuing to test this - but it looks like I have a working
> solution.
>
> Meanwhile, the fix for the underlying Squid bug was officially accepted
> and should become a part of v6.8 release (at least).
>
>
> Thank you,
>
> Alex.
>
>
> > On Fri, 9 Feb 2024 at 14:31, Alex Rousskov wrote:
> >
> >     On 2024-02-09 08:53, Robin Carlisle wrote:
> >
> >      > I am trying the config workaround approach.
> >
> >     Please keep us posted on your progress.
> >
> >      >  Below is the config snippet I have added.    I made the
> >      > assumption that for the /refresh_pattern, max-stale=NN /config,
> >     the NN
> >      > is in minutes as per the rest of that config directive.
> >
> >     That assumption is natural but incorrect: Unlike the anonymous
> >     positional min and max parameters (that use minutes), refresh_pattern
> >     max-stale=NN uses seconds. Documentation improvements are welcome.
> >
> >     Said that, the workaround should still prevent the application of the
> >     broken default refresh_pattern max-stale=0 rule, so you should still
> >     see
> >     positive results for the first NN seconds of the response age.
> >
> >     Instead of specifying max-stale=NN, consider adding refresh_pattern
> >     rules recommended by squid.conf.documented (and included in
> >     squid.cond.default). Those rules do not have max-stale options at
> all,
> >     and, hence, Squid will use (explicit or default) max_stale directive
> >     instead.
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >      > I am testing this right now
> >      >
> >      > # this should allow stale objects up to 1 year if allowed by
> >      > Cache-Control repsonseheaders ...
> >      >
> >      > # ... setting both options just in case
> >      >
> >      > max_stale 525600 minutes
> >      >
> >      > refresh_pattern . 0  20% 4320 max-stale=525600
> >      >
> >      >
> >      > Thanks again for your help
> >      >
> >      >
> >      > Robin
> >      >
> >      >
> >      >
> >      >
> >      > On Thu, 8 Feb 2024 at 17:42, Alex Rousskov
> >      > <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>
> >      > <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>>> wrote:
> >      >
> >      >     Hi Robin,
> >      >
> >      >           AFAICT from the logs you have privately shared and your
> >      >     squid.conf
> >      >     that you have posted earlier, your Squid overwrites
> >      >     stale-if-error=31536000 in the response with "refresh_pattern
> >      >     max-stale=0" default. That 0 value is wrong. The correct value
> >      >     should be
> >      >     taken from max_stale directive that defaults to 1 week, not
> zero:
> >      >
> >      >           refresh_pattern
> >      >           ...
> >      >           max-stale=NN provide a maximum staleness factor. Squid
> >     won't
> >      >           serve objects more stale than this even if it failed to
> >      >           validate the object. Default: use the max_stale global
> >     limit.
> >      >
> >      >     This wrong default is a Squid bug AFAICT. I posted an
> >     _untested_ fix as
> >      >     Squid PR 1664: https://github.com/squid-cache/squid/pull/1664
> >     <https://github.com/squid-cache/squid/pull/1664>
> >      >     <https://github.com/squid-cache/squid/pull/1664
> >     <https://github.com/squid-cache/squid/pull/1664>>
> >      >
> >      >     If possible, please test the corresponding patch:
> >      >
> >
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch
> <
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch>
> <
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch
> <
> https://github.com/squid-cache/squid/commit/571973589b5a46d458311f8b60dcb83032fd5cec.patch
> >>
> >      >
> >      >     AFAICT, you can also work around that bug by configuring an
> >     explicit
> >      >     refresh_pattern rule with an explicit max-stale option (see
> >      >     squid.conf.documented for examples). I have not tested that
> >     theory
> >      >     either.
> >      >
> >      >
> >      >     HTH,
> >      >
> >      >     Alex.
> >      >
> >      >
> >      >     On 2024-02-07 13:45, Robin Carlisle wrote:
> >      >      > Hi,
> >      >      >
> >      >      > I have just started my enhanced logging journey and have a
> >     small
> >      >     snippet
> >      >      > below that might illuminate the issue ...
> >      >      >
> >      >      > /2024/02/07 17:06:39.212 kid1| 88,3|
> client_side_reply.cc(507)
> >      >      > handleIMSReply: origin replied with error 502, forwarding
> to
> >      >     client due
> >      >      > to fail_on_validation_err/
> >      >      >
> >      >      > A few lines below in the log it looks like squid sent :-
> >      >      >
> >      >      > /2024/02/07 17:06:39.212 kid1| 11,2| Stream.cc(280)
> >      >     sendStartOfMessage:
> >      >      > HTTP Client REPLY:
> >      >      > ---------
> >      >      > HTTP/1.1 502 Bad Gateway
> >      >      > Server: squid/5.7
> >      >      > Mime-Version: 1.0
> >      >      > Date: Wed, 07 Feb 2024 17:06:39 GMT
> >      >      > Content-Type: text/html;charset=utf-8
> >      >      > Content-Length: 3853
> >      >      > X-Squid-Error: ERR_READ_ERROR 0
> >      >      > Vary: Accept-Language
> >      >      > Content-Language: en
> >      >      > X-Cache: MISS from labs-maul-st-15
> >      >      > X-Cache-Lookup: HIT from labs-maul-st-15:3129
> >      >      > Via: 1.1 labs-maul-st-15 (squid/5.7)
> >      >      > Connection: close/
> >      >      >
> >      >      >
> >      >      > The rest of the logs are quite large and contain URLs I
> >     cannot put
> >      >      > here.   The logs were generated with debug_options to
> ALL,3.
> >      >      >
> >      >      > Any ideas?   Or should I generate more detailed logs and
> >     send them
> >      >      > privately?
> >      >      >
> >      >      > Thanks again,
> >      >      >
> >      >      > Robin
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      > On Fri, 2 Feb 2024 at 11:20, Robin Carlisle
> >      >      > <robin.carlisle at framestore.com
> >     <mailto:robin.carlisle at framestore.com>
> >      >     <mailto:robin.carlisle at framestore.com
> >     <mailto:robin.carlisle at framestore.com>>
> >      >     <mailto:robin.carlisle at framestore.com
> >     <mailto:robin.carlisle at framestore.com>
> >      >     <mailto:robin.carlisle at framestore.com
> >     <mailto:robin.carlisle at framestore.com>>>>
> >      >      > wrote:
> >      >      >
> >      >      >     Hi, thanks for your reply.
> >      >      >
> >      >      >     I have been looking at :
> >      >      >
> >      >
> >
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control> <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>>
> <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control> <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control <
> https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control>>>
> >      >      >
> >      >      >     /The stale-if-error response directive indicates that
> the
> >      >     cache can
> >      >      >     reuse a stale response when an upstream server
> >     generates an
> >      >     error,
> >      >      >     or when the error is generated locally. Here, an error
> is
> >      >     considered
> >      >      >     any response with a status code of 500, 502, 503, or
> 504.
> >      >      >
> >      >      >     Cache-Control: max-age=604800, stale-if-error=86400
> >      >      >     In the example above, the response is fresh for 7 days
> >     (604800s).
> >      >      >     Afterwards, it becomes stale, but can be used for an
> >     extra 1 day
> >      >      >     (86400s) when an error is encountered.
> >      >      >
> >      >      >     After the stale-if-error period passes, the client will
> >      >     receive any
> >      >      >     error generated/
> >      >      >
> >      >      >     Given what you have said and what the above docs say -
> >     I am still
> >      >      >     confused as it looks like (in my test cases) the cached
> >      >     response can
> >      >      >     be used for 3600 secs (this works), after which the
> cached
> >      >     response
> >      >      >     can still be used for an additional 31536000 seconds
> >     on an error
> >      >      >     (this doesnt work).
> >      >      >
> >      >      >     I am going to dig into the error logging you suggested
> >     to see
> >      >     if I
> >      >      >     can make sense of that - and will send on if I can't.
> >      >      >
> >      >      >     Thanks v much for your help again,
> >      >      >
> >      >      >     Robin
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >     On Thu, 1 Feb 2024 at 18:27, Alex Rousskov
> >      >      >     <rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>
> >      >     <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>>
> >      >      >     <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>
> >      >     <mailto:rousskov at measurement-factory.com
> >     <mailto:rousskov at measurement-factory.com>>>> wrote:
> >      >      >
> >      >      >         On 2024-02-01 12:03, Robin Carlisle wrote:
> >      >      >          > Hi, I am having trouble with stale-if-error
> >     response.
> >      >      >
> >      >      >         If I am interpreting Squid code correctly, in
> >     primary use
> >      >     cases:
> >      >      >
> >      >      >         * without a Cache-Control:stale-if-error=X in the
> >     original
> >      >      >         response,
> >      >      >         Squid sends a stale object if revalidation results
> >     in a
> >      >     5xx error;
> >      >      >
> >      >      >         * with a Cache-Control:stale-if-error=X and object
> >     age at
> >      >     most
> >      >      >         X, Squid
> >      >      >         sends a stale object if revalidation results in a
> >     5xx error;
> >      >      >
> >      >      >         * with a Cache-Control:stale-if-error=X and object
> age
> >      >     exceeding X,
> >      >      >         Squid forwards the 5xx error response if
> revalidation
> >      >     results in
> >      >      >         a 5xx
> >      >      >         error;
> >      >      >
> >      >      >         In other words, stale-if-error=X turns on a "fail
> on
> >      >     validation
> >      >      >         errors"
> >      >      >         behavior for stale objects older than X. It has no
> >     other
> >      >     effects.
> >      >      >
> >      >      >         In your test case, the stale objects are much
> >     younger than
> >      >      >         stale-if-error value (e.g., Age~=3601 vs.
> >      >     stale-if-error=31536000).
> >      >      >         Thus, stale-if-error should have no relevant
> effect.
> >      >      >
> >      >      >         Something else is probably preventing your Squid
> >     from serving
> >      >      >         the stale
> >      >      >         response when facing a 5xx error. I do not know
> >     what that
> >      >      >         something is.
> >      >      >
> >      >      >         I recommend sharing (privately if you need to
> protect
> >      >     sensitive
> >      >      >         info) a
> >      >      >         pointer to a compressed ALL,9 cache.log collected
> >     while
> >      >      >         reproducing the
> >      >      >         problem (using two transactions similar to the
> >     ones you
> >      >     have shared
> >      >      >         below -- a successful stale hit and a problematic
> >     one):
> >      >      >
> >      >
> >
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> <
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> >>>
> >      >      >
> >      >      >         Alternatively, you can try to study cache.log
> >     yourself after
> >      >      >         setting
> >      >      >         debug_options to ALL,3. Searching for "refresh" and
> >      >      >         "handleIMSReply" may
> >      >      >         yield enough clues.
> >      >      >
> >      >      >
> >      >      >         HTH,
> >      >      >
> >      >      >         Alex.
> >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      >          > # /etc/squid/squid.conf :
> >      >      >          >
> >      >      >          > acl to_aws dstdomain .amazonaws.com
> >     <http://amazonaws.com>
> >      >     <http://amazonaws.com <http://amazonaws.com>>
> >     <http://amazonaws.com <http://amazonaws.com> <http://amazonaws.com
> >     <http://amazonaws.com>>>
> >      >      >         <http://amazonaws.com <http://amazonaws.com>
> >     <http://amazonaws.com <http://amazonaws.com>>
> >      >     <http://amazonaws.com <http://amazonaws.com>
> >     <http://amazonaws.com <http://amazonaws.com>>>>
> >      >      >          >
> >      >      >          > acl from_local src localhost
> >      >      >          >
> >      >      >          > http_access allow to_aws
> >      >      >          >
> >      >      >          > http_access allow from_local
> >      >      >          >
> >      >      >          > cache allow all
> >      >      >          >
> >      >      >          > cache_dir ufs /var/cache/squid 1024 16 256
> >      >      >          >
> >      >      >          > http_port 3129 ssl-bump cert=/etc/squid/maul.pem
> >      >      >          > generate-host-certificates=on
> >      >     dynamic_cert_mem_cache_size=4MB
> >      >      >          >
> >      >      >          > sslcrtd_program
> >     /usr/lib/squid/security_file_certgen -s
> >      >      >          > /var/lib/squid/ssl_db -M 4MB
> >      >      >          >
> >      >      >          > acl step1 at_step SslBump1
> >      >      >          >
> >      >      >          > ssl_bump bump step1
> >      >      >          >
> >      >      >          > ssl_bump bump all
> >      >      >          >
> >      >      >          > sslproxy_cert_error deny all
> >      >      >          >
> >      >      >          > cache_store_log stdio:/var/log/squid/store.log
> >      >      >          >
> >      >      >          > logfile_rotate 0
> >      >      >          >
> >      >      >          > shutdown_lifetime 3 seconds
> >      >      >          >
> >      >      >          >
> >      >      >          > # /usr/bin/proxy-test :
> >      >      >          >
> >      >      >          > #!/bin/bash
> >      >      >          >
> >      >      >          > curl --proxy http://localhost:3129
> >     <http://localhost:3129>
> >      >     <http://localhost:3129 <http://localhost:3129>>
> >     <http://localhost:3129 <http://localhost:3129>
> >     <http://localhost:3129 <http://localhost:3129>>>
> >      >      >         <http://localhost:3129 <http://localhost:3129>
> >     <http://localhost:3129 <http://localhost:3129>>
> >      >     <http://localhost:3129 <http://localhost:3129>
> >     <http://localhost:3129 <http://localhost:3129>>>> \
> >      >      >          >
> >      >      >          >    --cacert /etc/squid/stuff.pem \
> >      >      >          >
> >      >      >          >    -v
> >      >     "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >      >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>
> >      >      >
> >       <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >      >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>>
> >      >      >          >
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >      >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>
> >      >      >
> >       <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>
> >      >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> >     <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>>>>" \
> >      >      >          >
> >      >      >          >    -H "Authorization: token MYTOKEN" \
> >      >      >          >
> >      >      >          >    -H "Content-Type: application/json" \
> >      >      >          >
> >      >      >          >    --output "/tmp/stuff.json"
> >      >      >          >
> >      >      >          >
> >      >      >          >
> >      >      >          > Tests  ..........
> >      >      >          >
> >      >      >          >
> >      >      >          > At this point in time the network cable is
> >      >     unattached.  Squid
> >      >      >         returns
> >      >      >          > the cached object it got when the network was
> >     online
> >      >     earlier.
> >      >      >         The Age of
> >      >      >          > this object is just still under the max_age of
> >     3600.
> >      >      >         Previously I
> >      >      >          > was using offline_mode but I found that it did
> >     not try to
> >      >      >         revalidate
> >      >      >          > from the origin after the object expired
> >     (defined via
> >      >     max-age
> >      >      >         response).
> >      >      >          >    My understanding is that stale-if-error
> >     should work
> >      >     under my
> >      >      >          > circumstances.
> >      >      >          >
> >      >      >          >
> >      >      >          > # /var/log/squid/access.log
> >      >      >          >
> >      >      >          > 1706799404.440      6 127.0.0.1 NONE_NONE/200 0
> >     CONNECT
> >      >      >          > stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>> <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>>
> >      >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443 <
> http://stuff.amazonaws.com:443>>
> >      >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>>> - HIER_NONE/- -
> >      >      >          >
> >      >      >          > 1706799404.440      0 127.0.0.1 TCP_MEM_HIT/200
> >     20726 GET
> >      >      >          > https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>
> >      >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>>
> >      >      >          > <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>
> >      >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>>> - HIER_NONE/-
> >      >      >         application/json
> >      >      >          >
> >      >      >          >
> >      >      >          > # extract from /usr/bin/proxy-test
> >      >      >          >
> >      >      >          > < HTTP/1.1 200 OK
> >      >      >          >
> >      >      >          > < Date: Thu, 01 Feb 2024 13:57:11 GMT
> >      >      >          >
> >      >      >          > < Content-Type: application/json
> >      >      >          >
> >      >      >          > < Content-Length: 20134
> >      >      >          >
> >      >      >          > < x-amzn-RequestId:
> >     3a2d3b26-df73-4b30-88cb-1a9268fa0df2
> >      >      >          >
> >      >      >          > < Last-Modified: 2024-02-01T13:00:45.000Z
> >      >      >          >
> >      >      >          > < Access-Control-Allow-Origin: *
> >      >      >          >
> >      >      >          > < x-amz-apigw-id: SdZwpG7qiYcERUQ=
> >      >      >          >
> >      >      >          > < Cache-Control: public, max-age=3600,
> >      >     stale-if-error=31536000
> >      >      >          >
> >      >      >          > < ETag: "cec102b43372840737ab773c2e77858b"
> >      >      >          >
> >      >      >          > < X-Amzn-Trace-Id:
> >      >     Root=1-65bba337-292be751134161b03555cdd6
> >      >      >          >
> >      >      >          > < Age: 3573
> >      >      >          >
> >      >      >          > < X-Cache: HIT from labs-maul-st-31
> >      >      >          >
> >      >      >          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >      >      >          >
> >      >      >          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >      >      >          >
> >      >      >          > < Connection: keep-alive
> >      >      >          >
> >      >      >          >
> >      >      >          >
> >      >      >          >
> >      >      >          > Below .. the curl script executes again.  The
> >     Age has gone
> >      >      >         over the
> >      >      >          > max-age so squid attempted to refresh from the
> >      >     origin.  The
> >      >      >         machine is
> >      >      >          > still offline so the refresh failed.   I
> >     expected that the
> >      >      >          > stale-if-error response would instruct squid to
> >     return the
> >      >      >         cached object
> >      >      >          > as a 200.
> >      >      >          >
> >      >      >          >
> >      >      >          > # /var/log/squid/access.log
> >      >      >          >
> >      >      >          > 1706799434.464      5 127.0.0.1 NONE_NONE/200 0
> >     CONNECT
> >      >      >          > stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>> <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>>
> >      >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443 <
> http://stuff.amazonaws.com:443>>
> >      >      >         <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>
> >      >     <http://stuff.amazonaws.com:443
> >     <http://stuff.amazonaws.com:443>>>> - HIER_NONE/- -
> >      >      >          >
> >      >      >          > 1706799434.464      0 127.0.0.1
> >      >     TCP_REFRESH_FAIL_ERR/502 4235
> >      >      >         GET
> >      >      >          > https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>
> >      >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>>
> >      >      >          > <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>
> >      >      >         <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>
> >      >     <https://stuff.amazonaws.com/stuff.json
> >     <https://stuff.amazonaws.com/stuff.json>>>> - HIER_NONE/- text/html
> >      >      >          >
> >      >      >          >
> >      >      >          > # extract from /usr/bin/proxy-test
> >      >      >          >
> >      >      >          > < HTTP/1.1 502 Bad Gateway
> >      >      >          >
> >      >      >          > < Server: squid/5.7
> >      >      >          >
> >      >      >          > < Mime-Version: 1.0
> >      >      >          >
> >      >      >          > < Date: Thu, 01 Feb 2024 14:57:14 GMT
> >      >      >          >
> >      >      >          > < Content-Type: text/html;charset=utf-8
> >      >      >          >
> >      >      >          > < Content-Length: 3853
> >      >      >          >
> >      >      >          > < X-Squid-Error: ERR_READ_ERROR 0
> >      >      >          >
> >      >      >          > < Vary: Accept-Language
> >      >      >          >
> >      >      >          > < Content-Language: en
> >      >      >          >
> >      >      >          > < X-Cache: MISS from labs-maul-st-31
> >      >      >          >
> >      >      >          > < X-Cache-Lookup: HIT from labs-maul-st-31:3129
> >      >      >          >
> >      >      >          > < Via: 1.1 labs-maul-st-31 (squid/5.7)
> >      >      >          >
> >      >      >          > < Connection: close
> >      >      >          >
> >      >      >          >
> >      >      >          >
> >      >      >          > Hope someone can help me with this.  All the
> best,
> >      >      >          >
> >      >      >          >
> >      >      >          > Robin Carlisle
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240212/08561b82/attachment.htm>

From ngtech1ltd at gmail.com  Mon Feb 12 18:22:56 2024
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Mon, 12 Feb 2024 20:22:56 +0200
Subject: [squid-users] Anyone build Squid for on multiarch ie arm and arm64?
Message-ID: <00d101da5de0$80d45ef0$827d1cd0$@gmail.com>

I have couple RouterOS devices which supports containers with the next CPU arches:
? x86_64
? arm64
? armv6
? armv7

And I was wondering if someone bothered compiling squid containers for these arches?

I know that there are packages for Debian and Ubuntu but these are not 6.x squid but rather 5.x.
I am almost sure that publishing a container for these would benefit someone but not sure about it.

Thanks,
Eliezer




From speedy67 at chez.com  Mon Feb 12 22:40:54 2024
From: speedy67 at chez.com (speedy67 at chez.com)
Date: Mon, 12 Feb 2024 23:40:54 +0100
Subject: [squid-users] Unable to filter javascript exchanges
Message-ID: <580c26a4fa3fd6bf415a712a256bd0f2@chez.com>

Hello,

I'm using Squid 3.5.24 (indluded in Synology DSM 6) and I've an issue 
with time acl. All works fine except some websites like myhordes.de. 
Once the user connected to this kind of website, the time acl has no 
effect while the web page is not reloaded. All datas sent and received 
by the javascript scripts continue going thru the proxy server without 
any filter.

Thx a lot for any idea.

Regards,
Speedy


From squid3 at treenet.co.nz  Tue Feb 13 10:26:11 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Feb 2024 23:26:11 +1300
Subject: [squid-users] Anyone build Squid for on multiarch ie arm and
 arm64?
In-Reply-To: <00d101da5de0$80d45ef0$827d1cd0$@gmail.com>
References: <00d101da5de0$80d45ef0$827d1cd0$@gmail.com>
Message-ID: <5e43dd6f-b7ad-41c0-b2d6-8f13c636011d@treenet.co.nz>

On 13/02/24 07:22, ngtech1ltd wrote:
> I have couple RouterOS devices which supports containers with the next CPU arches:
> ? x86_64
> ? arm64
> ? armv6
> ? armv7
> 
> And I was wondering if someone bothered compiling squid containers for these arches?
> 
> I know that there are packages for Debian and Ubuntu but these are not 6.x squid but rather 5.x.

Debian packages of Squid are up to 6.x if you base the container on the 
"Testing" repository.

FWIW; I'm not sure if publishing built Docker containers is much use 
compared to providing the docker configuration file + extras needed to 
build the container as-needed.


Amos


From gkinkie at gmail.com  Tue Feb 13 10:34:56 2024
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 13 Feb 2024 17:34:56 +0700
Subject: [squid-users] Anyone build Squid for on multiarch ie arm and
 arm64?
In-Reply-To: <00d101da5de0$80d45ef0$827d1cd0$@gmail.com>
References: <00d101da5de0$80d45ef0$827d1cd0$@gmail.com>
Message-ID: <CA+Y8hcNb3yho8P7ymsZoz_jfbmSpZQ8KLDnDCNV4jSyBuVrXUQ@mail.gmail.com>

As part of our regular build testing, we we test each Squid patch against
amd64 (x86_64), arm64, armv7, and i386 (all on Linux), against a range of
distributions.
We do not release the artifacts, but it would be surprising if anyone had
major difficulties in building on a specific CPU architecture.

Francesco

On Tue, Feb 13, 2024 at 1:23?AM <ngtech1ltd at gmail.com> wrote:

> I have couple RouterOS devices which supports containers with the next CPU
> arches:
> ? x86_64
> ? arm64
> ? armv6
> ? armv7
>
> And I was wondering if someone bothered compiling squid containers for
> these arches?
>
> I know that there are packages for Debian and Ubuntu but these are not 6.x
> squid but rather 5.x.
> I am almost sure that publishing a container for these would benefit
> someone but not sure about it.
>
> Thanks,
> Eliezer
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240213/87e4f1c3/attachment.htm>

From squid at borrill.org.uk  Wed Feb 14 16:01:59 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Wed, 14 Feb 2024 16:01:59 +0000
Subject: [squid-users] Error files removed from 6.7
Message-ID: <2357eb2f-0706-473d-8cb6-b23e9fc9e1f7@borrill.org.uk>

I see the translations of error messages have been removed from 6.7 
compared to 6.6 (and earlier), but I see no mention of this in the 
changelog:
https://github.com/squid-cache/squid/blob/552c2ceef220f3bbcdbedf194eae419fc791098e/ChangeLog

Was this change intentional and, if so, why isn't it documented?

% tar -tzf /usr/pkgsrc/distfiles/squid-6.6.tar.xz | grep 
'squid-6\../errors/.*/ERR' | wc -l
     1974
% tar -tzf /usr/pkgsrc/distfiles/squid-6.7.tar.xz | grep 
'squid-6\../errors/.*/ERR' | wc -l
       42

-- 
Stephen


From squid3 at treenet.co.nz  Wed Feb 14 17:20:09 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 15 Feb 2024 06:20:09 +1300
Subject: [squid-users] Error files removed from 6.7
In-Reply-To: <2357eb2f-0706-473d-8cb6-b23e9fc9e1f7@borrill.org.uk>
References: <2357eb2f-0706-473d-8cb6-b23e9fc9e1f7@borrill.org.uk>
Message-ID: <28fe23c6-3dba-4575-b51e-815f88f03a77@treenet.co.nz>


On 15/02/24 05:01, Stephen Borrill wrote:
> I see the translations of error messages have been removed from 6.7 
> compared to 6.6 (and earlier), but I see no mention of this in the 
> changelog:
> https://github.com/squid-cache/squid/blob/552c2ceef220f3bbcdbedf194eae419fc791098e/ChangeLog
> 
> Was this change intentional and, if so, why isn't it documented?

No it was not intentional, hopefully will be fixed with the next release 
due on 3rd March.

As a workaround the files from 6.6 can be used, or the latest langpack 
available separately at <http://www.squid-cache.org/Versions/langpack/>


Cheers
Amos


From edreamer322 at gmail.com  Fri Feb 16 02:30:13 2024
From: edreamer322 at gmail.com (Eternal Dreamer)
Date: Fri, 16 Feb 2024 05:30:13 +0300
Subject: [squid-users] Squid Segment Violation with authorization
Message-ID: <CAFw4dKp7ZxypkBtaenbo6E2bUN-AUOCuHeQ3HyJiK-73=Kz70Q@mail.gmail.com>

 Hi!
When I'm trying to send curl request with provided basic
proxy-authorization credentials through my proxy I see Segment Violation
error in my logs and empty reply from server. Command is:
curl -v --proxy-basic --proxy-user login:password --proxy
http://192.168.3.19:8080 https://google.com

In squid.conf I have 3 directives:

http_access allow some_acl
http_access allow some_acl some_acl_user_auth some_special_domain
http_all_port http_all_proto
http_access allow some_acl some_acl_user_auth some_special_domain CONNECT
https_port

If I comment first one authorization works fine and it looks good. But with
all lines I even can't authorize to special domains without Segment
Violation error.
I've tried to use different versions of squid from 3.5 to 7.0.
Squid before v5.0.1 ignores Proxy-Authorization header when it's not needed
and works fine with this configuration.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240216/5393f7d4/attachment.htm>

From squid3 at treenet.co.nz  Fri Feb 16 04:39:38 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 16 Feb 2024 17:39:38 +1300
Subject: [squid-users] Squid Segment Violation with authorization
In-Reply-To: <CAFw4dKp7ZxypkBtaenbo6E2bUN-AUOCuHeQ3HyJiK-73=Kz70Q@mail.gmail.com>
References: <CAFw4dKp7ZxypkBtaenbo6E2bUN-AUOCuHeQ3HyJiK-73=Kz70Q@mail.gmail.com>
Message-ID: <24c09c6b-7da5-4fac-917b-5106231b72e6@treenet.co.nz>

On 16/02/24 15:30, Eternal Dreamer wrote:
> Hi!
> When I'm trying to send curl request with provided basic 
> proxy-authorization credentials through my proxy I see Segment Violation 
> error in my logs and empty reply from server. Command is:
> curl -v --proxy-basic --proxy-user login:password --proxy 
> http://192.168.3.19:8080 <http://192.168.3.19:8080> https://google.com 
> <http://google.com>
> 
> In squid.conf I have 3 directives:
> 
> http_access allow some_acl
> http_access allow some_acl some_acl_user_auth some_special_domain 
> http_all_port http_all_proto
> http_access allow some_acl some_acl_user_auth some_special_domain 
> CONNECT https_port
> 
> If I comment first one authorization works fine and it looks good.


Authorize or Authenticate?

Different things and you are mixing them up in these rules.


> But 
> with all lines I even can't authorize to special domains without Segment 
> Violation error.


The issue is likely somewhere else in what you have configured Squid to 
do. The initial "allow some_acl" line *authorizes* access, without 
*authenticating*. Resulting in there being no credentials for anything 
that Squid needs to do later.


If it helps this arrangement is clearer and does almost the same thing:

  http_access allow some_acl
  http_access deny !some_special_domain
  http_access deny !some_acl_user_auth
  http_access allow CONNECT https_port
  http_access allow http_all_port http_all_proto



> I've tried to use different versions of squid from 3.5 to 7.0.
> Squid before v5.0.1 ignores Proxy-Authorization header when it's not 
> needed and works fine with this configuration.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From squid at borrill.org.uk  Mon Feb 19 13:48:59 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Mon, 19 Feb 2024 13:48:59 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <2bff1730-8595-4224-a89d-f79f9f592321@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
 <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>
 <CABA8h=Sq7ceHBOwvX0rxfZ8OJKJ8Lz0KOz=KNKWaQ9+hc7_XbQ@mail.gmail.com>
 <2bff1730-8595-4224-a89d-f79f9f592321@borrill.org.uk>
Message-ID: <95bcd873-2bc8-4de1-8693-8da17df20d53@borrill.org.uk>

On 12/02/2024 12:02, Stephen Borrill wrote:
> On 12/02/2024 11:48, NgTech LTD wrote:
>> What distro are you using?
> 
> NetBSD 9.2_STABLE and building with gcc 8.5.0

I will stick with 6.6 for the debugging due to the missing files in 6.7 
(I'm one of the maintainers of the pkgsrc port for squid and we're not 
going to adjust the port just for this one broken release tarball).

>> ?????? ??? ??, 12 ????? 2024, 13:47, ??? Stephen Borrill 
>> ?<squid at borrill.org.uk <mailto:squid at borrill.org.uk>>:
>>
>> ??? On 16/01/2024 14:37, Alex Rousskov wrote:
>> ???? > On 2024-01-16 06:01, Stephen Borrill wrote:
>> ???? >> The problem is no different with 6.6. Is there any more 
>> debugging I
>> ???? >> can provide, Alex?
>> ???? >
>> ???? > Yes, but I need to give you a patch that adds that (temporary)
>> ??? debugging
>> ???? > first (assuming I fail to reproduce the problem in the lab). The
>> ??? ball is
>> ???? > on my side (unless somebody else steps in). Unfortunately, I do
>> ??? not have
>> ???? > any free time for any of that right now. If you do not hear 
>> from me
>> ???? > sooner, please ping me again on or after February 8, 2024.
>>
>> ??? PING!
>>
>> ??? I will get 6.7 compiled up so we can add debugging to it quickly. It
>> ??? would be good if we could get something in place this week as it is
>> ??? school holidays next week in the UK and so there will be little
>> ??? opportunity to test until afterwards.
>>
>> ???? >> On 10/01/2024 12:40, Stephen Borrill wrote:
>> ???? >>> On 09/01/2024 15:42, Alex Rousskov wrote:
>> ???? >>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>> ???? >>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>> ???? >>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>> ???? >>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>> ???? >>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5)
>> ??? connected
>> ???? >>>>>>>> via IPv4-only periodically fails to connect to the
>> ??? destination
>> ???? >>>>>>>> and then requires a restart to fix it (reload is not
>> ??? sufficient).
>> ???? >>>>>>>>
>> ???? >>>>>>>> The problem appears to be that a host that has one address
>> ??? each
>> ???? >>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go
>> ??? missing as
>> ???? >>>>>>>> a destination. On closer inspection, this appears to
>> ??? happen when
>> ???? >>>>>>>> the IPv6 address (not the IPv4) address is marked as bad.
>> ???? >>>>
>> ???? >>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in
>> ???? >>>>> 216.239.38.120 #1/2-0
>> ???? >>>>
>> ???? >>>>
>> ???? >>>> Thank you for sharing more debugging info!
>> ???? >>>
>> ???? >>> The following seemed odd to. It finds an IPv4 address (this
>> ??? host does
>> ???? >>> not have IPv6), puts it in the cache and then says "No DNS
>> ??? records":
>> ???? >>>
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617)
>> ??? nbgethostbyname:
>> ???? >>> schoolbase.online
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313)
>> ??? ipcacheRelease:
>> ???? >>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670)
>> ???? >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for
>> ???? >>> 'schoolbase.online'
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480)
>> ??? ipcacheParse: 1
>> ???? >>> answers for schoolbase.online
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no
>> ???? >>> 20.54.32.34 in [no cached IPs]
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no
>> ???? >>> 20.54.32.34 in [no cached IPs]
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) 
>> updateTtl: use
>> ???? >>> first 69 from RR TTL 69
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood:
>> ???? >>> schoolbase.online #1 20.54.32.34
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp:
>> ???? >>> 20.54.32.34
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174)
>> ??? handlePath:
>> ???? >>> PeerSelector72389 found conn564274 local=0.0.0.0
>> ???? >>> remote=20.54.32.34:443 <http://20.54.32.34:443> HIER_DIRECT
>> ??? flags=1, destination #1 for
>> ???? >>> schoolbase.online:443
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError:
>> ???? >>> ERROR: DNS failure while resolving schoolbase.online: No DNS
>> ??? records
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586)
>> ???? >>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 
>> #1/1-0
>> ???? >>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) 
>> finalCallback:
>> ???? >>> 0x1b7381f38? lookup_err=No DNS records
>> ???? >>>
>> ???? >>> It seemed to happen about the same time as the other failure, so
>> ???? >>> perhaps another symptom of the same.
>> ???? >>>
>> ???? >>>> The above log line is self-contradictory AFAICT: It says 
>> that the
>> ???? >>>> cache has both IPv6-looking and IPv4-looking address at the 
>> same
>> ???? >>>> cache position (0) and, judging by the corresponding code,
>> ??? those two
>> ???? >>>> IP addresses are equal. This is not possible (for those
>> ??? specific IP
>> ???? >>>> address values). The subsequent Squid behavior can be
>> ??? explained by
>> ???? >>>> this (unexplained) conflict.
>> ???? >>>>
>> ???? >>>> I assume you are running official Squid v6.5 code.
>> ???? >>>
>> ???? >>> Yes, compiled from source on NetBSD. I have the patch I refer
>> ??? to here
>> ???? >>> applied too:
>> ???? >>>
>>     
>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html <https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html>
>> ???? >>>
>> ???? >>>> I can suggest the following two steps for going forward:
>> ???? >>>>
>> ???? >>>> 1. Upgrade to the latest Squid v6 in hope that the problem
>> ??? goes away.
>> ???? >>>
>> ???? >>> I have just upgraded to 6.6.
>> ???? >>>
>> ???? >>>> 2. If the problem is still there, patch the latest Squid v6 to
>> ??? add
>> ???? >>>> more debugging in hope to explain what is going on. This may
>> ??? take a
>> ???? >>>> few iterations, and it will take me some time to produce the
>> ???? >>>> necessary debugging patch.
>> ???? >>>
>> ???? >>> Unfortunately, I don't have a test case that will cause the
>> ??? problem
>> ???? >>> so I need to run this at a customer's production site that is
>> ???? >>> particularly affected by it. Luckily, the problem recurs pretty
>> ??? quickly.
>> ???? >>>
>> ???? >>> Here's a run with 6.6 where the number of destinations drops
>> ??? from 2
>> ???? >>> to 1 before reverting. Not seen this before - usually once it 
>> has
>> ???? >>> dropped to 1 (the IPv6 address), it stays there until a restart
>> ??? (and
>> ???? >>> this did happen about a minute after this log fragment). 
>> Happy to
>> ???? >>> test out any debugging patch.
>> ???? >>>
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617)
>> ??? nbgethostbyname:
>> ???? >>> forcesafesearch.google.com <http://forcesafesearch.google.com>
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) 
>> lookupHostIP:
>> ???? >>> Given Non-IP 'forcesafesearch.google.com
>> ??? <http://forcesafesearch.google.com>': hostname or servname not
>> ???? >>> provided or not known
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657)
>> ???? >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for
>> ???? >>> 'forcesafesearch.google.com <http://forcesafesearch.google.com>'
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp:
>> ???? >>> [2001:4860:4802:32::78]
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174)
>> ??? handlePath:
>> ???? >>> PeerSelector300176 found conn2388484 local=[::]
>> ???? >>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1,
>> ??? destination
>> ???? >>> #1 for forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180)
>> ??? handlePath:
>> ???? >>> always_direct = ALLOWED
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181)
>> ??? handlePath:
>> ???? >>> never_direct = DENIED
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182)
>> ??? handlePath:
>> ???? >>> timedout = 0
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp:
>> ???? >>> 216.239.38.120
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174)
>> ??? handlePath:
>> ???? >>> PeerSelector300176 found conn2388485 local=0.0.0.0
>> ???? >>> remote=216.239.38.120:443 <http://216.239.38.120:443>
>> ??? HIER_DIRECT flags=1, destination #2 for
>> ???? >>> forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180)
>> ??? handlePath:
>> ???? >>> always_direct = ALLOWED
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181)
>> ??? handlePath:
>> ???? >>> never_direct = DENIED
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182)
>> ??? handlePath:
>> ???? >>> timedout = 0
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) 
>> finalCallback:
>> ???? >>> 0x12208e038
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479)
>> ???? >>> resolveSelected: PeerSelector300176 found all 2 destinations for
>> ???? >>> forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480)
>> ???? >>> resolveSelected: ?? always_direct = ALLOWED
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481)
>> ???? >>> resolveSelected: ??? never_direct = DENIED
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482)
>> ???? >>> resolveSelected: ??????? timedout = 0
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have:
>> ???? >>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] 
>> #2/2-0
>> ???? >>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad:
>> ???? >>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>> ??? <http://forcesafesearch.google.com>
>> ???? >>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have:
>> ???? >>> 216.239.38.120:443 <http://216.239.38.120:443> at 0 in
>> ??? [2001:4860:4802:32::78] #2/2-1
>> ???? >>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055)
>> ??? forgetMarking:
>> ???? >>> 216.239.38.120:443 <http://216.239.38.120:443> of
>> ??? forcesafesearch.google.com <http://forcesafesearch.google.com>
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) 
>> lookupHostIP:
>> ???? >>> Given Non-IP 'forcesafesearch.google.com
>> ??? <http://forcesafesearch.google.com>': hostname or servname not
>> ???? >>> provided or not known
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460)
>> ???? >>> resolveSelected: Find IP destination for:
>> ???? >>> forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>' via
>> ??? forcesafesearch.google.com <http://forcesafesearch.google.com>
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617)
>> ??? nbgethostbyname:
>> ???? >>> forcesafesearch.google.com <http://forcesafesearch.google.com>
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) 
>> lookupHostIP:
>> ???? >>> Given Non-IP 'forcesafesearch.google.com
>> ??? <http://forcesafesearch.google.com>': hostname or servname not
>> ???? >>> provided or not known
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657)
>> ???? >>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for
>> ???? >>> 'forcesafesearch.google.com <http://forcesafesearch.google.com>'
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp:
>> ???? >>> [2001:4860:4802:32::78]
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174)
>> ??? handlePath:
>> ???? >>> PeerSelector300177 found conn2388493 local=[::]
>> ???? >>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1,
>> ??? destination
>> ???? >>> #1 for forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180)
>> ??? handlePath:
>> ???? >>> always_direct = ALLOWED
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181)
>> ??? handlePath:
>> ???? >>> never_direct = DENIED
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182)
>> ??? handlePath:
>> ???? >>> timedout = 0
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp:
>> ???? >>> 216.239.38.120
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174)
>> ??? handlePath:
>> ???? >>> PeerSelector300177 found conn2388494 local=0.0.0.0
>> ???? >>> remote=216.239.38.120:443 <http://216.239.38.120:443>
>> ??? HIER_DIRECT flags=1, destination #2 for
>> ???? >>> forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180)
>> ??? handlePath:
>> ???? >>> always_direct = ALLOWED
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181)
>> ??? handlePath:
>> ???? >>> never_direct = DENIED
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182)
>> ??? handlePath:
>> ???? >>> timedout = 0
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) 
>> finalCallback:
>> ???? >>> 0x12208e038
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479)
>> ???? >>> resolveSelected: PeerSelector300177 found all 2 destinations for
>> ???? >>> forcesafesearch.google.com:443
>> ??? <http://forcesafesearch.google.com:443>
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480)
>> ???? >>> resolveSelected: ?? always_direct = ALLOWED
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481)
>> ???? >>> resolveSelected: ??? never_direct = DENIED
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482)
>> ???? >>> resolveSelected: ??????? timedout = 0
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have:
>> ???? >>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] 
>> #2/2-0
>> ???? >>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad:
>> ???? >>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>> ??? <http://forcesafesearch.google.com>
>> ???? >>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have:
>> ???? >>> 216.239.38.120:443 <http://216.239.38.120:443> at 0 in
>> ??? [2001:4860:4802:32::78] #2/2-1
>> ???? >>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055)
>> ??? forgetMarking:
>> ???? >>> 216.239.38.120:443 <http://216.239.38.120:443> of
>> ??? forcesafesearch.google.com <http://forcesafesearch.google.com>
>> ???? >>>
>> ???? >>
>> ???? >> _______________________________________________
>> ???? >> squid-users mailing list
>> ???? >> squid-users at lists.squid-cache.org
>> ??? <mailto:squid-users at lists.squid-cache.org>
>> ???? >> https://lists.squid-cache.org/listinfo/squid-users
>> ??? <https://lists.squid-cache.org/listinfo/squid-users>
>> ???? >
>> ???? > _______________________________________________
>> ???? > squid-users mailing list
>> ???? > squid-users at lists.squid-cache.org
>> ??? <mailto:squid-users at lists.squid-cache.org>
>> ???? > https://lists.squid-cache.org/listinfo/squid-users
>> ??? <https://lists.squid-cache.org/listinfo/squid-users>
>> ???? >
>>
>> ??? -- ??? Dr. Stephen Borrill, Director and Solutions Architect
>> ??? Precedence Technologies Ltd? ? ? ? ? ? ? T: +44 (0) 1223 359 900
>> ??? Technology House, 36a Union Lane? ? ? ? ?E:
>> ??? sborrill at precedence.co.uk <mailto:sborrill at precedence.co.uk>
>> ??? Cambridge, CB4 1QB, United Kingdom? ? ? ?W:
>> ??? http://www.precedence.co.uk/ <http://www.precedence.co.uk/>
>> ??? Limited company registered in England and Wales. Company number 
>> 3725626
>>
>> ??? _______________________________________________
>> ??? squid-users mailing list
>> ??? squid-users at lists.squid-cache.org
>> ??? <mailto:squid-users at lists.squid-cache.org>
>> ??? https://lists.squid-cache.org/listinfo/squid-users
>> ??? <https://lists.squid-cache.org/listinfo/squid-users>
>>
> 



From shorvath at npsh.hu  Tue Feb 20 07:15:07 2024
From: shorvath at npsh.hu (=?UTF-8?B?U3ppbMOhcmQgSG9ydsOhdGg=?=)
Date: Tue, 20 Feb 2024 08:15:07 +0100
Subject: [squid-users] Squid delay_access with external acl
Message-ID: <65D4517B020000230004796F@groupwise.npsh.hu>

Good Day!

I try to make limitation bandwidth for some user group. I have an external acl which get the users from ldap database server. In
the old version of config we blocked the internet with http_access deny GROUP, but now i try to allow the internet which has
limited bandwidth. I know that the delay_access work with only fast ACL and external acl or proxy_auth acl are slow. I already
tried some opportunity but i couldn't solve.

Maybe have you any solution for this? Or any idea how can limitation the bandwidth for some user? I need use the username (e-mail
address format) because that use to login to the proxy.

Version: Squid Cache: Version 5.6

 

Thank you so much and i am waiting for your answer!

Have a good day!

Br,
Szilard Horvath 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240220/bacc19c2/attachment.htm>

From fab at votreservice.com  Tue Feb 20 08:06:48 2024
From: fab at votreservice.com (Dsant)
Date: Tue, 20 Feb 2024 09:06:48 +0100
Subject: [squid-users] Google recaptcha use
Message-ID: <2424a0b4-4ead-4cc2-c62a-7a79e3a21d44@votreservice.com>

Hello, I set up a squid proxy, I want to allow some sites, Google 
recaptcha and block everything else.

acl mydest dstdomain .projet-voltaire.fr
http_access allow mydest
acl? google_recaptcha url_regex ^www.google.com/recaptcha/$
http_access allow google_recaptcha
http_access deny all

The captcha is not showing. A syntax error ?

Thanks.




From gkinkie at gmail.com  Tue Feb 20 08:14:30 2024
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 20 Feb 2024 15:14:30 +0700
Subject: [squid-users] Squid delay_access with external acl
In-Reply-To: <65D4517B020000230004796F@groupwise.npsh.hu>
References: <65D4517B020000230004796F@groupwise.npsh.hu>
Message-ID: <CA+Y8hcMkoAz2cuxtqd0Y_7JKfhMBzN+zYbTZJWfXR6DfjdtQTg@mail.gmail.com>

Hello Szil?rd,
  quoting from the squid wiki
<https://wiki.squid-cache.org/SquidFaq/SquidAcl#fast-and-slow-acls> :

"A possible workaround which can mitigate the effect of this characteristic
consists in exploiting caching, by setting some ?useless? ACL checks in
slow clauses, so that subsequent fast clauses may have a cached result to
evaluate against."

In other words, a simplified example like:

acl users ext_user foo bar gazonk
http_access allow users all  # always allow, verify and cache cache user
auth
delay_access 3 allow users

should do the trick


On Tue, Feb 20, 2024 at 2:15?PM Szil?rd Horv?th <shorvath at npsh.hu> wrote:

> Good Day!
>
> I try to make limitation bandwidth for some user group. I have an external
> acl which get the users from ldap database server. In the old version of
> config we blocked the internet with http_access deny GROUP, but now i try
> to allow the internet which has limited bandwidth. I know that the
> delay_access work with only fast ACL and external acl or proxy_auth acl are
> slow. I already tried some opportunity but i couldn't solve.
>
> Maybe have you any solution for this? Or any idea how can limitation the
> bandwidth for some user? I need use the username (e-mail address format)
> because that use to login to the proxy.
>
> Version: Squid Cache: Version 5.6
>
>
>
> Thank you so much and i am waiting for your answer!
>
> Have a good day!
>
> Br,
> Szilard Horvath
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240220/32972595/attachment.htm>

From squid at borrill.org.uk  Tue Feb 20 08:40:57 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 20 Feb 2024 08:40:57 +0000
Subject: [squid-users] Google recaptcha use
In-Reply-To: <2424a0b4-4ead-4cc2-c62a-7a79e3a21d44@votreservice.com>
References: <2424a0b4-4ead-4cc2-c62a-7a79e3a21d44@votreservice.com>
Message-ID: <ae7064d8-5531-4a03-811a-ff6a00ede05f@borrill.org.uk>

On 20/02/2024 08:06, Dsant wrote:
> Hello, I set up a squid proxy, I want to allow some sites, Google 
> recaptcha and block everything else.
> 
> acl mydest dstdomain .projet-voltaire.fr
> http_access allow mydest
> acl? google_recaptcha url_regex ^www.google.com/recaptcha/$
> http_access allow google_recaptcha
> http_access deny all
> 
> The captcha is not showing. A syntax error ?

www.google.com is an HTTPS site. This means that from the point of view 
of the proxy, only the hostname is visible (i.e. www.google.com) and so 
your regex can never match. Look in your logs, you will see:

CONNECT www.google.com

and not:

GET http://www.google.com/recaptcha/

The only way round this is to use ssl_bump to intercept and decrypt the 
traffic so that the HTTP request is visible. This is, however, not for 
the faint-hearted and will require a CA certificate to be installed on 
each client machine.

-- 
Stephen




From fab at votreservice.com  Tue Feb 20 15:41:47 2024
From: fab at votreservice.com (Dsant)
Date: Tue, 20 Feb 2024 16:41:47 +0100
Subject: [squid-users] Google recaptcha use
In-Reply-To: <ae7064d8-5531-4a03-811a-ff6a00ede05f@borrill.org.uk>
References: <2424a0b4-4ead-4cc2-c62a-7a79e3a21d44@votreservice.com>
 <ae7064d8-5531-4a03-811a-ff6a00ede05f@borrill.org.uk>
Message-ID: <e380a993-fee6-ec6f-8405-dc6a3ac526d5@votreservice.com>

Solved !

I had to add more than only google.com/recaptcha/ ...

So do : tail -f /var/log/squid/access.log

I had to add (not all mandatory) : .ireby.fr .mozilla.org 
.callnowbutton.com .googleapis.com .consentmanager.net 
.googletagmanager.com .gstatic.com

Thanks a lot.

Dsant from France


On 2/20/24 09:40, Stephen Borrill wrote:
> On 20/02/2024 08:06, Dsant wrote:
>> Hello, I set up a squid proxy, I want to allow some sites, Google 
>> recaptcha and block everything else.
>>
>> acl mydest dstdomain .projet-voltaire.fr
>> http_access allow mydest
>> acl? google_recaptcha url_regex ^www.google.com/recaptcha/$
>> http_access allow google_recaptcha
>> http_access deny all
>>
>> The captcha is not showing. A syntax error ?
>
> www.google.com is an HTTPS site. This means that from the point of 
> view of the proxy, only the hostname is visible (i.e. www.google.com) 
> and so your regex can never match. Look in your logs, you will see:
>
> CONNECT www.google.com
>
> and not:
>
> GET http://www.google.com/recaptcha/
>
> The only way round this is to use ssl_bump to intercept and decrypt 
> the traffic so that the HTTP request is visible. This is, however, not 
> for the faint-hearted and will require a CA certificate to be 
> installed on each client machine.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240220/81a73297/attachment.htm>

From rousskov at measurement-factory.com  Tue Feb 20 15:51:50 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 20 Feb 2024 10:51:50 -0500
Subject: [squid-users] Squid delay_access with external acl
In-Reply-To: <CA+Y8hcMkoAz2cuxtqd0Y_7JKfhMBzN+zYbTZJWfXR6DfjdtQTg@mail.gmail.com>
References: <65D4517B020000230004796F@groupwise.npsh.hu>
 <CA+Y8hcMkoAz2cuxtqd0Y_7JKfhMBzN+zYbTZJWfXR6DfjdtQTg@mail.gmail.com>
Message-ID: <8ef8e83a-346d-4ce1-bde3-88c1c31f14d4@measurement-factory.com>

On 2024-02-20 03:14, Francesco Chemolli wrote:

> acl users ext_user foo bar gazonk
> http_access allow users all? # always allow

The above does not always allow. What you meant it probably this:

# This rule never matches. It is used for its side effect:
# The rule evaluates users ACL, caching evaluation result.
http_access allow users !all


> delay_access 3 allow users
> 
> should do the trick

... but sometimes will not. Wiki recommendation to "exploit caching" is 
an ugly outdated hack that should be avoided. The correct solution these 
days is to use annotate_transaction ACL to mark the transaction 
accordingly. Here is an untested sketch:

     acl fromUserThatShouldBeLimited ext_user ...
     acl markAsLimited annotate_transaction limited=yes
     acl markedAsLimited note limited yes

     # This rule never matches; used for its annotation side effect.
     http_access allow fromUserThatShouldBeLimited markAsLimited !all

     delay_access 3 allow markedAsLimited

HTH,

Alex.



> On Tue, Feb 20, 2024 at 2:15?PM Szil?rd Horv?th wrote:
> 
>     Good Day!
> 
>     I try to make limitation bandwidth for some user group. I have an
>     external acl which get the users from ldap database server. In the
>     old version of config we blocked the internet with http_access deny
>     GROUP, but now i try to allow the internet which has limited
>     bandwidth. I know that the delay_access work with only fast ACL and
>     external acl or proxy_auth acl are slow. I already tried some
>     opportunity but i couldn't solve.
> 
>     Maybe have you any solution for this? Or any idea how can limitation
>     the bandwidth for some user? I need use the username (e-mail address
>     format) because that use to login to the proxy.
> 
>     Version: Squid Cache: Version 5.6
> 
>     Thank you so much and i am waiting for your answer!
> 
>     Have a good day!
> 
>     Br,
>     Szilard Horvath
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> -- 
>  ? ? Francesco
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Feb 20 19:55:18 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 20 Feb 2024 14:55:18 -0500
Subject: [squid-users] Unable to filter javascript exchanges
In-Reply-To: <580c26a4fa3fd6bf415a712a256bd0f2@chez.com>
References: <580c26a4fa3fd6bf415a712a256bd0f2@chez.com>
Message-ID: <369004ee-dff0-409b-8875-5e608ce11f04@measurement-factory.com>

On 2024-02-12 17:40, speedy67 at chez.com wrote:

> I'm using Squid 3.5.24 (indluded in Synology DSM 6) and I've an issue 
> with time acl. All works fine except some websites like myhordes.de. 
> Once the user connected to this kind of website, the time acl has no 
> effect while the web page is not reloaded. All datas sent and received 
> by the javascript scripts continue going thru the proxy server without 
> any filter.

Squid does not normally evaluate ACLs while tunneling traffic: Various 
directives are checked at the tunnel establishment time and after the 
tunnel is closed, but not when bytes are shoveled back and forth between 
a TCP client and a TCP server.

The same can be said about processing (large) HTTP message bodies.

If your use case involves CONNECT tunnels, intercepted (but not bumped) 
TLS connections, or very large/slow HTTP messages, then you need to 
enhance Squid to apply some [time-related] checks "in the middle of a 
[long] transaction".

https://wiki.squid-cache.org/SquidFaq/AboutSquid#how-to-add-a-new-squid-feature-enhance-of-fix-something

N.B. Squid v3 is very buggy and has not been supported by the Squid 
Project for many years. Please upgrade to Squid v6 or later. The upgrade 
itself will not add a "check directive X when tunneling for a long time" 
feature though.


HTH,

Alex.



From rousskov at measurement-factory.com  Tue Feb 20 21:12:45 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 20 Feb 2024 16:12:45 -0500
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
 <12ddc619-122b-4417-89ed-7e0fe8963f1d@borrill.org.uk>
Message-ID: <7256eb27-12fc-4fed-98ef-0ccc5f9bb498@measurement-factory.com>

On 2024-02-12 06:46, Stephen Borrill wrote:
> On 16/01/2024 14:37, Alex Rousskov wrote:
>> On 2024-01-16 06:01, Stephen Borrill wrote:
>>> The problem is no different with 6.6. Is there any more debugging I 
>>> can provide, Alex?
>>
>> Yes, but I need to give you a patch that adds that (temporary) 
>> debugging first (assuming I fail to reproduce the problem in the lab). 
>> The ball is on my side (unless somebody else steps in). Unfortunately, 
>> I do not have any free time for any of that right now. If you do not 
>> hear from me sooner, please ping me again on or after February 8, 2024.

> PING!

I reproduced this bug and posted a minimal master/v7 fix for the 
official review: https://github.com/squid-cache/squid/pull/1691

Please test the corresponding patch; it applies to Squid v5 and v6:

https://github.com/squid-cache/squid/commit/7d255a72131217d30af3653cec10452fa53289c3.patch


Thank you,

Alex.


> I will get 6.7 compiled up so we can add debugging to it quickly. It 
> would be good if we could get something in place this week as it is 
> school holidays next week in the UK and so there will be little 
> opportunity to test until afterwards.
> 
>>> On 10/01/2024 12:40, Stephen Borrill wrote:
>>>> On 09/01/2024 15:42, Alex Rousskov wrote:
>>>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected 
>>>>>>>>> via IPv4-only periodically fails to connect to the destination 
>>>>>>>>> and then requires a restart to fix it (reload is not sufficient).
>>>>>>>>>
>>>>>>>>> The problem appears to be that a host that has one address each 
>>>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go missing 
>>>>>>>>> as a destination. On closer inspection, this appears to happen 
>>>>>>>>> when the IPv6 address (not the IPv4) address is marked as bad.
>>>>>
>>>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>>>>> 216.239.38.120 #1/2-0
>>>>>
>>>>>
>>>>> Thank you for sharing more debugging info!
>>>>
>>>> The following seemed odd to. It finds an IPv4 address (this host 
>>>> does not have IPv6), puts it in the cache and then says "No DNS 
>>>> records":
>>>>
>>>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>>> schoolbase.online
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
>>>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
>>>> 'schoolbase.online'
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
>>>> answers for schoolbase.online
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>>> 20.54.32.34 in [no cached IPs]
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>>> 20.54.32.34 in [no cached IPs]
>>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use 
>>>> first 69 from RR TTL 69
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
>>>> schoolbase.online #1 20.54.32.34
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> 20.54.32.34
>>>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector72389 found conn564274 local=0.0.0.0 
>>>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for 
>>>> schoolbase.online:443
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: 
>>>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) 
>>>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>> 0x1b7381f38? lookup_err=No DNS records
>>>>
>>>> It seemed to happen about the same time as the other failure, so 
>>>> perhaps another symptom of the same.
>>>>
>>>>> The above log line is self-contradictory AFAICT: It says that the 
>>>>> cache has both IPv6-looking and IPv4-looking address at the same 
>>>>> cache position (0) and, judging by the corresponding code, those 
>>>>> two IP addresses are equal. This is not possible (for those 
>>>>> specific IP address values). The subsequent Squid behavior can be 
>>>>> explained by this (unexplained) conflict.
>>>>>
>>>>> I assume you are running official Squid v6.5 code.
>>>>
>>>> Yes, compiled from source on NetBSD. I have the patch I refer to 
>>>> here applied too:
>>>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
>>>>
>>>>> I can suggest the following two steps for going forward:
>>>>>
>>>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
>>>>
>>>> I have just upgraded to 6.6.
>>>>
>>>>> 2. If the problem is still there, patch the latest Squid v6 to add 
>>>>> more debugging in hope to explain what is going on. This may take a 
>>>>> few iterations, and it will take me some time to produce the 
>>>>> necessary debugging patch.
>>>>
>>>> Unfortunately, I don't have a test case that will cause the problem 
>>>> so I need to run this at a customer's production site that is 
>>>> particularly affected by it. Luckily, the problem recurs pretty 
>>>> quickly.
>>>>
>>>> Here's a run with 6.6 where the number of destinations drops from 2 
>>>> to 1 before reverting. Not seen this before - usually once it has 
>>>> dropped to 1 (the IPv6 address), it stays there until a restart (and 
>>>> this did happen about a minute after this log fragment). Happy to 
>>>> test out any debugging patch.
>>>>
>>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>>> forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>> provided or not known
>>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>>> 'forcesafesearch.google.com'
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> [2001:4860:4802:32::78]
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300176 found conn2388484 local=[::] 
>>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>>> #1 for forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> 216.239.38.120
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300176 found conn2388485 local=0.0.0.0 
>>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>> 0x12208e038
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) 
>>>> resolveSelected: PeerSelector300176 found all 2 destinations for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) 
>>>> resolveSelected: ?? always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) 
>>>> resolveSelected: ??? never_direct = DENIED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) 
>>>> resolveSelected: ??????? timedout = 0
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
>>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
>>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>> provided or not known
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) 
>>>> resolveSelected: Find IP destination for: 
>>>> forcesafesearch.google.com:443' via forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>>> forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>> provided or not known
>>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>>> 'forcesafesearch.google.com'
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> [2001:4860:4802:32::78]
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300177 found conn2388493 local=[::] 
>>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>>> #1 for forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> 216.239.38.120
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300177 found conn2388494 local=0.0.0.0 
>>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>> 0x12208e038
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) 
>>>> resolveSelected: PeerSelector300177 found all 2 destinations for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) 
>>>> resolveSelected: ?? always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) 
>>>> resolveSelected: ??? never_direct = DENIED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) 
>>>> resolveSelected: ??????? timedout = 0
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
>>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
>>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
> 



From misho.98118 at gmail.com  Wed Feb 21 21:14:55 2024
From: misho.98118 at gmail.com (Miha Miha)
Date: Wed, 21 Feb 2024 23:14:55 +0200
Subject: [squid-users] Can't verify the signature of squid-6.7.tar.gz
In-Reply-To: <0e21989d-ab3e-472b-80fc-23be5aad9928@treenet.co.nz>
References: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>
 <CA+Y8hcPOxq3_R+3kCL6Pv0ov3jCe15u7xS8VaUcMtu6AuXBr6g@mail.gmail.com>
 <CABZyDWH0EJ2vnxjaDbwwJ8vP4k7uGMvtpcScOmgQnfCLzQFpFQ@mail.gmail.com>
 <0e21989d-ab3e-472b-80fc-23be5aad9928@treenet.co.nz>
Message-ID: <CABZyDWHbgmM8_UnuNde_Bj8jBM+QJr0Q8_OFA48PK9uQA5j+oQ@mail.gmail.com>

Hi Amos,

It took me some time to check and verify.
I'm posting my findings here just to complete the thread.

Regarding this one:

> On 8/02/24 02:19, Miha Miha wrote:
> > Hi Francesco,
> >
> > I still get an issue, although a slightly different one:
> >
> > #gpg --verify squid-6.7.tar.gz.asc squid-6.7.tar.gz
> > gpg: Signature made Tue 06 Feb 2024 10:51:28 PM EET using ? key ID FEF6E865
> > gpg: Can't check signature: Invalid public key algorithm
>
>On Thu, Feb 8, 2024 at 7:58?AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>
> The error mentions algorithm, so also check the ciphers/algorithms
> supported by your GPG agent. The new key uses the EDDSA cipher instead
> of typical RSA.

Indeed, the problem is with my gpg agent - gpg (GnuPG) 2.0.22 which
doesn't support EDDSA. My system is CentOS7 and it sticks to GnuPG
2.0.x

I did another test on Amazon Linux with gpg (GnuPG) 2.3.7 (it supports
EDDSA) and there I was able to verify the package with the given pub
key.

All questions are clarified. Thank you!

Regards,
Mihail


From ed-squid-cache at s5h.net  Sat Feb 24 17:26:22 2024
From: ed-squid-cache at s5h.net (Ed)
Date: Sat, 24 Feb 2024 17:26:22 +0000
Subject: [squid-users] ICAP response to avoid backend
Message-ID: <Zdomvr0FjGOBbMmg@s5h.net>

Hi,

Does anyone know a way to tell squid in accelerator mode to not go to 
the backend peers?

There's a couple of requests I'd like to avoid handing off to the 
backend, but, if the data is already a HIT, then I'd like squid to send 
that, just sometimes I don't want it to send the request to the backend.

My thinking is to set a header in the reqmod stage, or set the response 
status there in a way that tells squid to not continue to the backend, 
or to disable for this request only, if the cache already has the 
request stored, then send that response instead, or return a 500-style 
response.

In varnish land this is doable in the vcl_miss hook, but I don't know 
how to do that in squid.

Hope someone else may have walked a similar path previously.

Thanks in advance for your time,
Ed



From ed-squid-cache at s5h.net  Sun Feb 25 17:52:54 2024
From: ed-squid-cache at s5h.net (Ed)
Date: Sun, 25 Feb 2024 17:52:54 +0000
Subject: [squid-users] ICAP response to avoid backend
In-Reply-To: <Zdomvr0FjGOBbMmg@s5h.net>
References: <Zdomvr0FjGOBbMmg@s5h.net>
Message-ID: <Zdt-drh8fVpEiSTP@s5h.net>

On 2024-02-24 17:26+0000, Ed wrote:
> In varnish land this is doable in the vcl_miss hook, but I don't know 
> how to do that in squid.

I think I found a way, but maybe there's a better method - I'd like to 
the cache_peer_access to apply to all backends, but this does seem to do 
what I was after:

  acl bad_foo req_header ICAPHEADER -i foobar
  cache_peer_access server_1 deny bad_foo

Ed


From squid3 at treenet.co.nz  Mon Feb 26 23:20:14 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 27 Feb 2024 12:20:14 +1300
Subject: [squid-users] Can't verify the signature of squid-6.7.tar.gz
In-Reply-To: <CABZyDWHbgmM8_UnuNde_Bj8jBM+QJr0Q8_OFA48PK9uQA5j+oQ@mail.gmail.com>
References: <CABZyDWHAbNSaGwOh49LOUPTFcf5dnC=kiQ4ve=GnazGAVh2XXw@mail.gmail.com>
 <CA+Y8hcPOxq3_R+3kCL6Pv0ov3jCe15u7xS8VaUcMtu6AuXBr6g@mail.gmail.com>
 <CABZyDWH0EJ2vnxjaDbwwJ8vP4k7uGMvtpcScOmgQnfCLzQFpFQ@mail.gmail.com>
 <0e21989d-ab3e-472b-80fc-23be5aad9928@treenet.co.nz>
 <CABZyDWHbgmM8_UnuNde_Bj8jBM+QJr0Q8_OFA48PK9uQA5j+oQ@mail.gmail.com>
Message-ID: <30fa7de7-9ec8-4314-a7e3-ac6d934f8bc2@treenet.co.nz>

Excellent news.

Thank you for the feedback on the solution.


Cheers
Amos

On 22/02/24 10:14, Miha Miha wrote:
> Hi Amos,
> 
> It took me some time to check and verify.
> I'm posting my findings here just to complete the thread.
> 
> Regarding this one:
> 
>> On 8/02/24 02:19, Miha Miha wrote:
>>> Hi Francesco,
>>>
>>> I still get an issue, although a slightly different one:
>>>
>>> #gpg --verify squid-6.7.tar.gz.asc squid-6.7.tar.gz
>>> gpg: Signature made Tue 06 Feb 2024 10:51:28 PM EET using ? key ID FEF6E865
>>> gpg: Can't check signature: Invalid public key algorithm
>>
>> On Thu, Feb 8, 2024 at 7:58?AM Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>
>> The error mentions algorithm, so also check the ciphers/algorithms
>> supported by your GPG agent. The new key uses the EDDSA cipher instead
>> of typical RSA.
> 
> Indeed, the problem is with my gpg agent - gpg (GnuPG) 2.0.22 which
> doesn't support EDDSA. My system is CentOS7 and it sticks to GnuPG
> 2.0.x
> 
> I did another test on Amazon Linux with gpg (GnuPG) 2.3.7 (it supports
> EDDSA) and there I was able to verify the package with the given pub
> key.
> 
> All questions are clarified. Thank you!
> 
> Regards,
> Mihail


From squid3 at treenet.co.nz  Mon Feb 26 23:38:34 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 27 Feb 2024 12:38:34 +1300
Subject: [squid-users] ICAP response to avoid backend
In-Reply-To: <Zdt-drh8fVpEiSTP@s5h.net>
References: <Zdomvr0FjGOBbMmg@s5h.net> <Zdt-drh8fVpEiSTP@s5h.net>
Message-ID: <5f19cdff-f6af-402c-8a51-35af0252f350@treenet.co.nz>

On 26/02/24 06:52, Ed wrote:
> On 2024-02-24 17:26+0000, Ed wrote:
>> In varnish land this is doable in the vcl_miss hook, but I don't know
>> how to do that in squid.
> 
> I think I found a way, but maybe there's a better method - I'd like to
> the cache_peer_access to apply to all backends, but this does seem to do
> what I was after:
> 
>    acl bad_foo req_header ICAPHEADER -i foobar
>    cache_peer_access server_1 deny bad_foo
> 

Assuming that an ICAP service is controlling whether the peers are to be 
used that is the correct way.

However, if you have an ICAP service controlling whether a peer can be 
used consider having the ICAP service just send Squid the final 
response. There is a relatively huge amount of complexity, both in the 
config and what Squid has to do slowing the transaction down just for 
this maybe-a-HIT behaviour.


Alternatives to "cache_peer_access .. deny bad_foo" are:

A) "always_direct allow bad_foo",
   If you want the request to be served, but using servers from a DNS 
lookup instead of the configured cache_peer.

B) "miss_access deny bad_foo",
   If you do not want the cache MISS to be answered at all.


It has been a while since I tested it, but IIRC with miss_access a 
"deny_info" line may be used to change the default 403 error status into 
another in the 200-599 status range. Which includes redirects, 
retry-after, empty responses, and template pages responses ... whichever 
suits your need best.


Cheers
Amos


From ed-squid-cache at s5h.net  Tue Feb 27 09:23:06 2024
From: ed-squid-cache at s5h.net (Ed Neville)
Date: Tue, 27 Feb 2024 09:23:06 +0000
Subject: [squid-users] ICAP response to avoid backend
In-Reply-To: <5f19cdff-f6af-402c-8a51-35af0252f350@treenet.co.nz>
References: <Zdomvr0FjGOBbMmg@s5h.net> <Zdt-drh8fVpEiSTP@s5h.net>
 <5f19cdff-f6af-402c-8a51-35af0252f350@treenet.co.nz>
Message-ID: <Zd2p-j6eNgRu166c@s5h.net>

On 2024-02-27 12:38+1300, Amos Jeffries wrote:
> On 26/02/24 06:52, Ed wrote:
> > 
> >    acl bad_foo req_header ICAPHEADER -i foobar
> >    cache_peer_access server_1 deny bad_foo

> Assuming that an ICAP service is controlling whether the peers are to 
> be used that is the correct way.
> 
> However, if you have an ICAP service controlling whether a peer can be 
> used consider having the ICAP service just send Squid the final 
> response. There is a relatively huge amount of complexity, both in the 
> config and what Squid has to do slowing the transaction down just for 
> this maybe-a-HIT behaviour.

Yes, it would be best if the traffic wasn't there sometimes :) 

> Alternatives to "cache_peer_access .. deny bad_foo" are:
> ...
> B) "miss_access deny bad_foo",

I think your suggestion of B suits best, thank you very much as that 
saves me using multiple cache_peer_access lines. Something didn't feel 
right with what I was doing, and asking for an alternative was the right 
thing to do.

Many thanks,
Ed



From ml at netfence.it  Tue Feb 27 15:36:56 2024
From: ml at netfence.it (Andrea Venturoli)
Date: Tue, 27 Feb 2024 16:36:56 +0100
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
Message-ID: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>

Hello.

I'm having trouble accessing cachemgr with squidclient.

As a test, I've added the following to my squid.conf as the first 
http_access line:
> http_access manager

(I know this is dangerous and I've removed it after the test).


Opening "http://10.1.2.39:8080/squid-internal-mgr/info" from a client, I 
see all the stats.
However, squidclient still gets an access denied error:
> # squidclient -vv -p 8080 -h 10.1.2.39 mgr:info
> verbosity level set to 2
> Request:
> GET http://10.1.2.39:8080/squid-internal-mgr/info HTTP/1.0
> Host: 10.1.2.39:8080
> User-Agent: squidclient/6.6
> Accept: */*
> Connection: close
> 
> 
> .
> Transport detected: IPv4-only
> Resolving 10.1.2.39 ...
> Connecting... 10.1.2.39 (10.1.2.39:8080)
> Connected to: 10.1.2.39 (10.1.2.39:8080)
> Sending HTTP request ... 
> done.
> HTTP/1.1 403 Forbidden
> Server: squid
> Mime-Version: 1.0
> Date: Tue, 27 Feb 2024 15:33:55 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 3691
> X-Squid-Error: ERR_ACCESS_DENIED 0
> Vary: Accept-Language
> Content-Language: en
> Cache-Status: proxy2.ventu;fwd=miss;detail=mismatch
> Via: 1.1 proxy2.ventu (squid), 1.1 proxy2.ventu (squid)
> Cache-Status: proxy2.ventu;fwd=miss;detail=no-cache
> Connection: close

This happens indifferently if I run it on the cache host itself or from 
the same client where the browser works.

In cache.log I see:
> 2024/02/27 16:34:48 kid1| WARNING: Forwarding loop detected for:
> GET /squid-internal-mgr/info HTTP/1.1
> Host: proxy2.ventu:8080
> User-Agent: squidclient/6.6
> Accept: */*
> Via: 1.0 proxy2.ventu (squid)
> X-Forwarded-For: 10.1.2.18
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
>     current master transaction: master2562

Does this mean Squid is connecting to itself as a proxy in order to 
connect to himself?
I removed all "*proxy*" env vars and tried running squidclient again, 
but there was no difference.

Any hint?
Is there a way to get more debugging info from Squid on this?

  bye & Thanks
	av.


From rousskov at measurement-factory.com  Tue Feb 27 17:02:54 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Feb 2024 12:02:54 -0500
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
In-Reply-To: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
References: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
Message-ID: <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>

On 2024-02-27 10:36, Andrea Venturoli wrote:

> I'm having trouble accessing cachemgr with squidclient.

You are suffering from one or several known problems[1,2] related to 
cache manager changes in v6+ code. Without going into complicated 
details, I recommend that you replace deprecated squidclient with curl, 
wget, or another popular client of your choice _and_ then use the URL 
host name (or IP address) and other client configuration parameters that 
"work" in your specific Squid environment. You may need to adjust them 
later, but at least you will have a temporary workaround.

AFAIK[1], a Squid developer is working on improving this ugly situation, 
but that work takes time (and will not resurrect squidclient support in 
future Squid versions).


HTH,

Alex.

[1] https://bugs.squid-cache.org/show_bug.cgi?id=5283
[2] 
https://lists.squid-cache.org/pipermail/squid-users/2023-August/026023.html

> As a test, I've added the following to my squid.conf as the first 
> http_access line:
>> http_access manager
> 
> (I know this is dangerous and I've removed it after the test).
> 
> 
> Opening "http://10.1.2.39:8080/squid-internal-mgr/info" from a client, I 
> see all the stats.
> However, squidclient still gets an access denied error:
>> # squidclient -vv -p 8080 -h 10.1.2.39 mgr:info
>> verbosity level set to 2
>> Request:
>> GET http://10.1.2.39:8080/squid-internal-mgr/info HTTP/1.0
>> Host: 10.1.2.39:8080
>> User-Agent: squidclient/6.6
>> Accept: */*
>> Connection: close
>>
>>
>> .
>> Transport detected: IPv4-only
>> Resolving 10.1.2.39 ...
>> Connecting... 10.1.2.39 (10.1.2.39:8080)
>> Connected to: 10.1.2.39 (10.1.2.39:8080)
>> Sending HTTP request ... done.
>> HTTP/1.1 403 Forbidden
>> Server: squid
>> Mime-Version: 1.0
>> Date: Tue, 27 Feb 2024 15:33:55 GMT
>> Content-Type: text/html;charset=utf-8
>> Content-Length: 3691
>> X-Squid-Error: ERR_ACCESS_DENIED 0
>> Vary: Accept-Language
>> Content-Language: en
>> Cache-Status: proxy2.ventu;fwd=miss;detail=mismatch
>> Via: 1.1 proxy2.ventu (squid), 1.1 proxy2.ventu (squid)
>> Cache-Status: proxy2.ventu;fwd=miss;detail=no-cache
>> Connection: close
> 
> This happens indifferently if I run it on the cache host itself or from 
> the same client where the browser works.
> 
> In cache.log I see:
>> 2024/02/27 16:34:48 kid1| WARNING: Forwarding loop detected for:
>> GET /squid-internal-mgr/info HTTP/1.1
>> Host: proxy2.ventu:8080
>> User-Agent: squidclient/6.6
>> Accept: */*
>> Via: 1.0 proxy2.ventu (squid)
>> X-Forwarded-For: 10.1.2.18
>> Cache-Control: max-age=259200
>> Connection: keep-alive
>>
>>
>> ??? current master transaction: master2562
> 
> Does this mean Squid is connecting to itself as a proxy in order to 
> connect to himself?
> I removed all "*proxy*" env vars and tried running squidclient again, 
> but there was no difference.
> 
> Any hint?
> Is there a way to get more debugging info from Squid on this?
> 
>  ?bye & Thanks
>  ????av.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From ml at netfence.it  Wed Feb 28 07:35:46 2024
From: ml at netfence.it (Andrea Venturoli)
Date: Wed, 28 Feb 2024 08:35:46 +0100
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
In-Reply-To: <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>
References: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
 <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>
Message-ID: <f340b88c-2159-487e-be4d-ca6ad8ccbd5f@netfence.it>

On 2/27/24 18:02, Alex Rousskov wrote:

Hello and thanks for answering.



> You are suffering from one or several known problems[1,2] related to 
> cache manager changes in v6+ code. Without going into complicated 
> details, I recommend that you replace deprecated squidclient with curl, 
> wget, or another popular client of your choice _and_ then use the URL 
> host name (or IP address) and other client configuration parameters that 
> "work" in your specific Squid environment. You may need to adjust them 
> later, but at least you will have a temporary workaround.

I vaguely remembered squidclient deprecation (although I searched for it 
and could not find official info on the site).

WRT to moving to curl/wget/whatever, is there any documentation I can use?

  bye & Thanks
	av.


From ngtech1ltd at gmail.com  Wed Feb 28 10:04:54 2024
From: ngtech1ltd at gmail.com (ngtech1ltd at gmail.com)
Date: Wed, 28 Feb 2024 12:04:54 +0200
Subject: [squid-users] Squid Docker container
Message-ID: <000001da6a2d$94d1c920$be755b60$@gmail.com>

I started working on the docker containers of squid-cache these days.
The first one is at:
https://hub.docker.com/r/elicro/debian12squid/tags

but it's not ready to use as is yet, just the build steps for now with the binaries in place.
I need to add the supervisord damon and maybe couple other things.
These containers will not be a plain Squid as is but rather with all the tools I use daily like ruby and couple other packages.

My goals are:
* debian
* ubuntu
* AlmaLinux
* CentOS
* Oracle Linux
* Alpine linux
And maybe opensuse but not really sure about that yet.

The architectures I plan to build these containers for are:
* x86_64
* arm/v6
* arm/v7
* arm64

The above is since these are the most commonly used in containers platforms such as docker/podman/other.
My original plan was to build a container with the binaries but, It's so simple to build and publish with docker with the multi stages of containers so I don't care
to just let the CPU and RAM work couple more minutes in real so couple other platforms will benefit from my time.

Any recommendations and comments are more then welcome.

Eliezer



From gkinkie at gmail.com  Wed Feb 28 11:51:55 2024
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Wed, 28 Feb 2024 18:51:55 +0700
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
In-Reply-To: <f340b88c-2159-487e-be4d-ca6ad8ccbd5f@netfence.it>
References: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
 <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>
 <f340b88c-2159-487e-be4d-ca6ad8ccbd5f@netfence.it>
Message-ID: <CA+Y8hcNmWV52i86cme8_nkEbu=DjQrxyS+Nh8fE8GY1NnZKrXQ@mail.gmail.com>

>
>
> I vaguely remembered squidclient deprecation (although I searched for it
> and could not find official info on the site).
>
> WRT to moving to curl/wget/whatever, is there any documentation I can use?
>

Hi Andrea,
  there's https://wiki.squid-cache.org/Features/CacheManager/Index ,
although it could probably be more explicit

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240228/6115f280/attachment.htm>

From ml at netfence.it  Wed Feb 28 13:05:22 2024
From: ml at netfence.it (Andrea Venturoli)
Date: Wed, 28 Feb 2024 14:05:22 +0100
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
In-Reply-To: <CA+Y8hcNmWV52i86cme8_nkEbu=DjQrxyS+Nh8fE8GY1NnZKrXQ@mail.gmail.com>
References: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
 <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>
 <f340b88c-2159-487e-be4d-ca6ad8ccbd5f@netfence.it>
 <CA+Y8hcNmWV52i86cme8_nkEbu=DjQrxyS+Nh8fE8GY1NnZKrXQ@mail.gmail.com>
Message-ID: <31e8e078-0ce4-43e6-a2bf-2858497dfc2e@netfence.it>

On 2/28/24 12:51, Francesco Chemolli wrote:

> Hi Andrea,
>  ? there's https://wiki.squid-cache.org/Features/CacheManager/Index 
> <https://wiki.squid-cache.org/Features/CacheManager/Index> ,
> although it could probably be more explicit

Hello and thanks.

I had seen that document before posting, but, possibly due to my 
ignorance, I cannot understand how to use it.
For example I see some endpoints listed under the SMP chapter (e.g.
curl http://localhost:8080/squid-internal-mgr/info), but I guess that's 
not a complete list.
Does such a list exist? Where?

I'm in need to purge some objects from the cache and I always used 
something like:
> squidclient mgr:objects | grep -i somesite | grep GET | sed "s/.*GET //"rgs -n 1 squidclient -m PURGE

What could be an equivalent using curl/wget?

  bye & Thanks
	av.




From gkinkie at gmail.com  Wed Feb 28 13:52:38 2024
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Wed, 28 Feb 2024 20:52:38 +0700
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
In-Reply-To: <31e8e078-0ce4-43e6-a2bf-2858497dfc2e@netfence.it>
References: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
 <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>
 <f340b88c-2159-487e-be4d-ca6ad8ccbd5f@netfence.it>
 <CA+Y8hcNmWV52i86cme8_nkEbu=DjQrxyS+Nh8fE8GY1NnZKrXQ@mail.gmail.com>
 <31e8e078-0ce4-43e6-a2bf-2858497dfc2e@netfence.it>
Message-ID: <CA+Y8hcMCpoGkuH8mX_iSKN9mJ4-bht0FQe-7xMuj7OqxovDKJw@mail.gmail.com>

On Wed, Feb 28, 2024 at 8:05?PM Andrea Venturoli <ml at netfence.it> wrote:

> On 2/28/24 12:51, Francesco Chemolli wrote:
>
> > Hi Andrea,
> >    there's https://wiki.squid-cache.org/Features/CacheManager/Index
> > <https://wiki.squid-cache.org/Features/CacheManager/Index> ,
> > although it could probably be more explicit
>
> Hello and thanks.
>
> I had seen that document before posting, but, possibly due to my
> ignorance, I cannot understand how to use it.
> For example I see some endpoints listed under the SMP chapter (e.g.
> curl http://localhost:8080/squid-internal-mgr/info), but I guess that's
> not a complete list.
> Does such a list exist? Where?
>
> I'm in need to purge some objects from the cache and I always used
> something like:
> > squidclient mgr:objects | grep -i somesite | grep GET | sed "s/.*GET
> //"rgs -n 1 squidclient -m PURGE
>

Hi Andrea,
  just replace

squidclient mgr:objects

with

curl --silent --user squid_cachemgr_user:squd_cachemgr_password
http://squid.host.name:3128/squid-internal-mgr/objects

(and of course replace port 3128 with whatever port you're using for Squid)
Everything else is the same as previously.

Also, the same applies to all other cachemgr reports:

curl --silent --user squid_cachemgr_user:squd_cachemgr_password
http://squid.host.name:3128/squid-internal-mgr/menu

will give you the list of available subpages; replace menu with the subpage
name to access any




>
> What could be an equivalent using curl/wget?
>
>   bye & Thanks
>         av.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240228/47ef7c4b/attachment.htm>

From rousskov at measurement-factory.com  Wed Feb 28 14:31:08 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 28 Feb 2024 09:31:08 -0500
Subject: [squid-users] squidclient ERR_ACCESS_DENIED
In-Reply-To: <CA+Y8hcMCpoGkuH8mX_iSKN9mJ4-bht0FQe-7xMuj7OqxovDKJw@mail.gmail.com>
References: <e2c0a2fe-345b-470a-bc48-55e842cbf102@netfence.it>
 <42217c6c-fc12-4939-a753-5673aac77df6@measurement-factory.com>
 <f340b88c-2159-487e-be4d-ca6ad8ccbd5f@netfence.it>
 <CA+Y8hcNmWV52i86cme8_nkEbu=DjQrxyS+Nh8fE8GY1NnZKrXQ@mail.gmail.com>
 <31e8e078-0ce4-43e6-a2bf-2858497dfc2e@netfence.it>
 <CA+Y8hcMCpoGkuH8mX_iSKN9mJ4-bht0FQe-7xMuj7OqxovDKJw@mail.gmail.com>
Message-ID: <7c187ebc-0e30-44a2-b49d-93044427893f@measurement-factory.com>

On 2024-02-28 08:52, Francesco Chemolli wrote:

> just replace
> 
> squidclient mgr:objects
> 
> with
> 
> curl --silent --user squid_cachemgr_user:squd_cachemgr_password 
> http://squid.host.name:3128/squid-internal-mgr/objects 

Neither is required for basic cases, but it is better, IMHO, to use 
--no-progress-meter instead of error-hiding --silent.

One only needs --user when accessing password-protected reports.

The biggest difficulty in this conversion is with guessing what hostname 
a modern Squid will recognize as its own. And the correct guess is 
likely to change when we fix the remaining bugs.


Cheers,

Alex.


> (and of course replace port 3128 with whatever port you're using for Squid)
> Everything else is the same as previously.
> 
> Also, the same applies to all other cachemgr reports:
> 
> curl --silent --user squid_cachemgr_user:squd_cachemgr_password 
> http://squid.host.name:3128/squid-internal-mgr/menu 
> <http://squid.host.name:3128/squid-internal-mgr/menu>
> 
> will give you the list of available subpages; replace menuwith the 
> subpage name to access any
> 
> 
> 
>     What could be an equivalent using curl/wget?
> 
>      ? bye & Thanks
>      ? ? ? ? av.
> 
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     https://lists.squid-cache.org/listinfo/squid-users
>     <https://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> -- 
>  ? ? Francesco
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



