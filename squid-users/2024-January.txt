From kinkie at squid-cache.org  Fri Jan  5 14:00:22 2024
From: kinkie at squid-cache.org (Francesco Chemolli)
Date: Fri, 5 Jan 2024 14:00:22 +0000
Subject: [squid-users] [NOC] Squid Binary Package on RedHat Enterprise
 Linus
In-Reply-To: <DB8P191MB111076AD19EF6AB520546853DB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
References: <DB8P191MB111076AD19EF6AB520546853DB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CA+Y8hcMCvysV7v1XKF8pzEZ2KB6YQqvR7HoCCGXD+rSKCNo+CA@mail.gmail.com>

Hi Anil,
   Red Hat takes the Squid sources that we provide, possibly adds a number
of patches on top to facilitate integration with their environment, cover
security vulnerabilities or add features that their customers want,
distributes and provides support for it. The version number matches the
version number distributed by the Squid project.
You can find the actual details by looking at the source RPM - anyone
purchasing RHEL should have access to the source according to the
conditions of the Squid GNU GPL license.

On Fri, Jan 5, 2024 at 1:50?PM Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
wrote:

> Hi All,
>
>
>
> Apologies for spamming.
>
>
>
> I would like to understand difference between RHEL supplied squid with
> actual squid?
>
>
>
> I see RHEL 7/8 supplying Squid 3.5.20-17/4.15-6 versions as part of binary
> and how are they different from Squid version (latest 6.6)?  Does the RHEL
> version number specific to RHEL development or it matches to squid versions
> one-to-one?
>
>
>
> Thanks
>
> Anil
>
>
>
> INTERNAL
>
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> * ******************************************************************* This
> e-mail is confidential. It may also be legally privileged. If you are not
> the addressee you may not copy, forward, disclose or use any part of it. If
> you have received this message in error, please delete it and all copies
> from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
> ******************************************************************* "SAVE
> PAPER - THINK BEFORE YOU PRINT!" *
>
> _______________________________________________
> NOC mailing list
> NOC at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/noc
>


-- 
    Francesco Chemolli
    Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240105/f640865f/attachment.htm>

From kinkie at squid-cache.org  Fri Jan  5 14:13:57 2024
From: kinkie at squid-cache.org (Francesco Chemolli)
Date: Fri, 5 Jan 2024 14:13:57 +0000
Subject: [squid-users] EXTERNAL: Re: [NOC] Squid Binary Package on
 RedHat Enterprise Linus
In-Reply-To: <DB8P191MB11104F7EA88A4D18DF48B56BDB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
References: <DB8P191MB111076AD19EF6AB520546853DB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
 <CA+Y8hcMCvysV7v1XKF8pzEZ2KB6YQqvR7HoCCGXD+rSKCNo+CA@mail.gmail.com>
 <DB8P191MB11104F7EA88A4D18DF48B56BDB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CA+Y8hcNK3H3MCDwg=xBrKezbKt=BmRN9JLDh0DOjCqEMquAR4Q@mail.gmail.com>

Hi Anil,
  yes, that's correct.
It is to be expected that distributions lag behind Project releases, they
need time to package, test, and train.

On Fri, Jan 5, 2024 at 2:12?PM Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
wrote:

> Thanks, Francesco, for the quick reply.
>
>
>
> Squid project having a latest stable release 6.6 however latest Red Hat
> latest OS 9 has a binary squid version of 5.5-6.
>
>
>
> This concludes that Red Hat is yet to release the 6.6 equivalent latest
> release and we may be missing certain features of 6.6 in RHEL version?
>
>
>
> Thanks & Regards
>
> Anil
>
>
>
> *From:* Francesco Chemolli <kinkie at squid-cache.org>
> *Sent:* Friday, January 5, 2024 7:30 PM
> *To:* Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* EXTERNAL: Re: [NOC] Squid Binary Package on RedHat Enterprise
> Linus
>
>
>
> Hi Anil, Red Hat takes the Squid sources that we provide, possibly adds a
> number of patches on top to facilitate integration with their environment,
> cover security vulnerabilities or add features that their customers want,
> distributes and provides
>
> ZjQcmQRYFpfptBannerStart
>
> *This Message Is From an Untrusted Sender *
>
> You have not previously corresponded with this sender.
>
> ZjQcmQRYFpfptBannerEnd
>
> Hi Anil,
>
>    Red Hat takes the Squid sources that we provide, possibly adds a number
> of patches on top to facilitate integration with their environment, cover
> security vulnerabilities or add features that their customers want,
> distributes and provides support for it. The version number matches the
> version number distributed by the Squid project.
>
> You can find the actual details by looking at the source RPM - anyone
> purchasing RHEL should have access to the source according to the
> conditions of the Squid GNU GPL license.
>
>
>
> On Fri, Jan 5, 2024 at 1:50?PM Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
> wrote:
>
> Hi All,
>
>
>
> Apologies for spamming.
>
>
>
> I would like to understand difference between RHEL supplied squid with
> actual squid?
>
>
>
> I see RHEL 7/8 supplying Squid 3.5.20-17/4.15-6 versions as part of binary
> and how are they different from Squid version (latest 6.6)?  Does the RHEL
> version number specific to RHEL development or it matches to squid versions
> one-to-one?
>
>
>
> Thanks
>
> Anil
>
>
>
>
>
> INTERNAL
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> * ******************************************************************* This
> e-mail is confidential. It may also be legally privileged. If you are not
> the addressee you may not copy, forward, disclose or use any part of it. If
> you have received this message in error, please delete it and all copies
> from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
> ******************************************************************* "SAVE
> PAPER - THINK BEFORE YOU PRINT!" *
>
>
>
> _______________________________________________
> NOC mailing list
> NOC at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/noc
> <https://urldefense.com/v3/__https:/lists.squid-cache.org/listinfo/noc__;!!LSAcJDlP!zwexufZeQxIUypBa5zAI6E-nLuWTywHLn9I2wec2YgP05KMoZRMC-pnVEqEubDVHFiOWzJzgdgUg5SjS3d3ofMGGcEU$>
>
>
>
>
> --
>
>     Francesco Chemolli
>     Squid Software Foundation
> ------------------------------
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
> * ****************************************************************** This
> message originated from the Internet. Its originator may or may not be who
> they claim to be and the information contained in the message and any
> attachments may or may not be accurate.
> *******************************************************************
>
>
>
>
>
> PUBLIC
>
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> * ******************************************************************* This
> e-mail is confidential. It may also be legally privileged. If you are not
> the addressee you may not copy, forward, disclose or use any part of it. If
> you have received this message in error, please delete it and all copies
> from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
> ******************************************************************* "SAVE
> PAPER - THINK BEFORE YOU PRINT!" *
>
>

-- 
    Francesco Chemolli
    Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240105/14438070/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 31859 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240105/14438070/attachment.png>

From kinkie at squid-cache.org  Fri Jan  5 14:47:41 2024
From: kinkie at squid-cache.org (Francesco Chemolli)
Date: Fri, 5 Jan 2024 14:47:41 +0000
Subject: [squid-users] EXTERNAL: Re: Re: [NOC] Squid Binary Package on
 RedHat Enterprise Linus
In-Reply-To: <DB8P191MB11104828DD37302ED76BD7D0DB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
References: <DB8P191MB111076AD19EF6AB520546853DB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
 <CA+Y8hcMCvysV7v1XKF8pzEZ2KB6YQqvR7HoCCGXD+rSKCNo+CA@mail.gmail.com>
 <DB8P191MB11104F7EA88A4D18DF48B56BDB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
 <CA+Y8hcNK3H3MCDwg=xBrKezbKt=BmRN9JLDh0DOjCqEMquAR4Q@mail.gmail.com>
 <DB8P191MB11104828DD37302ED76BD7D0DB662@DB8P191MB1110.EURP191.PROD.OUTLOOK.COM>
Message-ID: <CA+Y8hcODCN82rKtmDghUdiWH_E2qpp_82kKT1vpVK5f7pmL0FQ@mail.gmail.com>

Hi,
  Red Hat has the freedom to change the product they distribute from what
the Project releases. They might have fixes that the project has not yet
covered in the same version (or add new bugs, however unlikely this might
be). This is all reflected in the source RPM

On Fri, Jan 5, 2024 at 2:36?PM Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
wrote:

> Great. One last query,
>
>
>
> How should I look at vulnerability thing, do RHEL squid pose same
> vulnerabilities as relative to squid project versions and vice-versa? Or
> RHEL can independently fix Squid vulnerabilities?
>
>
>
> Thanks & Regards
>
> Anil
>
>
>
> *From:* Francesco Chemolli <kinkie at squid-cache.org>
> *Sent:* Friday, January 5, 2024 7:44 PM
> *To:* Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
> *Cc:* Francesco Chemolli <kinkie at squid-cache.org>;
> squid-users at lists.squid-cache.org
> *Subject:* EXTERNAL: Re: Re: [NOC] Squid Binary Package on RedHat
> Enterprise Linus
>
>
>
> Hi Anil, yes, that's correct. It is to be expected that distributions lag
> behind Project releases, they need time to package, test, and train. On
> Fri, Jan 5, 2024 at 2: 12 PM Anil Kumar1 K <anil. kumar1. k@ hsbc. co.
> in> wrote: Thanks,
>
> ZjQcmQRYFpfptBannerStart
>
> *This Message Is From an Untrusted Sender *
>
> You have not previously corresponded with this sender.
>
> ZjQcmQRYFpfptBannerEnd
>
> Hi Anil,
>
>   yes, that's correct.
>
> It is to be expected that distributions lag behind Project releases, they
> need time to package, test, and train.
>
>
>
> On Fri, Jan 5, 2024 at 2:12?PM Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
> wrote:
>
> Thanks, Francesco, for the quick reply.
>
>
>
> Squid project having a latest stable release 6.6 however latest Red Hat
> latest OS 9 has a binary squid version of 5.5-6.
>
>
>
> This concludes that Red Hat is yet to release the 6.6 equivalent latest
> release and we may be missing certain features of 6.6 in RHEL version?
>
>
>
> Thanks & Regards
>
> Anil
>
>
>
> *From:* Francesco Chemolli <kinkie at squid-cache.org>
> *Sent:* Friday, January 5, 2024 7:30 PM
> *To:* Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* EXTERNAL: Re: [NOC] Squid Binary Package on RedHat Enterprise
> Linus
>
>
>
> Hi Anil, Red Hat takes the Squid sources that we provide, possibly adds a
> number of patches on top to facilitate integration with their environment,
> cover security vulnerabilities or add features that their customers want,
> distributes and provides
>
> ZjQcmQRYFpfptBannerStart
>
> *This Message Is From an Untrusted Sender *
>
> You have not previously corresponded with this sender.
>
> ZjQcmQRYFpfptBannerEnd
>
> Hi Anil,
>
>    Red Hat takes the Squid sources that we provide, possibly adds a number
> of patches on top to facilitate integration with their environment, cover
> security vulnerabilities or add features that their customers want,
> distributes and provides support for it. The version number matches the
> version number distributed by the Squid project.
>
> You can find the actual details by looking at the source RPM - anyone
> purchasing RHEL should have access to the source according to the
> conditions of the Squid GNU GPL license.
>
>
>
> On Fri, Jan 5, 2024 at 1:50?PM Anil Kumar1 K <anil.kumar1.k at hsbc.co.in>
> wrote:
>
> Hi All,
>
>
>
> Apologies for spamming.
>
>
>
> I would like to understand difference between RHEL supplied squid with
> actual squid?
>
>
>
> I see RHEL 7/8 supplying Squid 3.5.20-17/4.15-6 versions as part of binary
> and how are they different from Squid version (latest 6.6)?  Does the RHEL
> version number specific to RHEL development or it matches to squid versions
> one-to-one?
>
>
>
> Thanks
>
> Anil
>
>
>
>
>
> INTERNAL
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> * ******************************************************************* This
> e-mail is confidential. It may also be legally privileged. If you are not
> the addressee you may not copy, forward, disclose or use any part of it. If
> you have received this message in error, please delete it and all copies
> from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
> ******************************************************************* "SAVE
> PAPER - THINK BEFORE YOU PRINT!" *
>
>
>
> _______________________________________________
> NOC mailing list
> NOC at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/noc
> <https://urldefense.com/v3/__https:/lists.squid-cache.org/listinfo/noc__;!!LSAcJDlP!zwexufZeQxIUypBa5zAI6E-nLuWTywHLn9I2wec2YgP05KMoZRMC-pnVEqEubDVHFiOWzJzgdgUg5SjS3d3ofMGGcEU$>
>
>
>
>
> --
>
>     Francesco Chemolli
>     Squid Software Foundation
> ------------------------------
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
> * ****************************************************************** This
> message originated from the Internet. Its originator may or may not be who
> they claim to be and the information contained in the message and any
> attachments may or may not be accurate.
> *******************************************************************
>
>
>
>
>
> PUBLIC
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> * ******************************************************************* This
> e-mail is confidential. It may also be legally privileged. If you are not
> the addressee you may not copy, forward, disclose or use any part of it. If
> you have received this message in error, please delete it and all copies
> from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
> ******************************************************************* "SAVE
> PAPER - THINK BEFORE YOU PRINT!" *
>
>
>
>
>
>
> --
>
>     Francesco Chemolli
>     Squid Software Foundation
> ------------------------------
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
> * ****************************************************************** This
> message originated from the Internet. Its originator may or may not be who
> they claim to be and the information contained in the message and any
> attachments may or may not be accurate.
> *******************************************************************
>
>
>
>
>
> PUBLIC
>
> ------------------------------
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> * ******************************************************************* This
> e-mail is confidential. It may also be legally privileged. If you are not
> the addressee you may not copy, forward, disclose or use any part of it. If
> you have received this message in error, please delete it and all copies
> from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
> ******************************************************************* "SAVE
> PAPER - THINK BEFORE YOU PRINT!" *
>
>

-- 
    Francesco Chemolli
    Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240105/cecac74b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 31859 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240105/cecac74b/attachment.png>

From squid at borrill.org.uk  Mon Jan  8 13:31:24 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Mon, 8 Jan 2024 13:31:24 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
Message-ID: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>

I'm trying to determine why squid 6.x (seen with 6.5) connected via 
IPv4-only periodically fails to connect to the destination and then 
requires a restart to fix it (reload is not sufficient).

The problem appears to be that a host that has one address each of IPv4 
and IPv6 occasionally has its IPv4 address go missing as a destination. 
On closer inspection, this appears to happen when the IPv6 address (not 
the IPv4) address is marked as bad. A log fragment is as follows:

2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: clientservices.googleapis.com:443' via 
clientservices.googleapis.com
2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector82284 found conn696198 local=0.0.0.0 
remote=142.250.187.227:443 HIER_DIRECT flags=1, destination #1 for 
clientservices.googleapis.com:443
2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector82284 found conn696199 local=[::] 
remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, destination 
#2 for clientservices.googleapis.com:443
2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector82284 found all 2 destinations for 
clientservices.googleapis.com:443
2024/01/08 13:18:40.245 kid1| 14,2| ipcache.cc(1031) markAsBad: 
[2a00:1450:4009:820::2003]:443 of clientservices.googleapis.com
2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(946) seekNewGood: 
succeeded for clientservices.googleapis.com: [2a00:1450:4009:820::2003] 
#2/2-1
2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(978) restoreGoodness: 
cleared all IPs for clientservices.googleapis.com; now back to 
[2a00:1450:4009:820::2003] #2/2-1
2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'clientservices.googleapis.com': hostname or servname not 
provided or not known
2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: clientservices.googleapis.com:443' via 
clientservices.googleapis.com
2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'clientservices.googleapis.com': hostname or servname not 
provided or not known
2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector82372 found conn697148 local=[::] 
remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, destination 
#1 for clientservices.googleapis.com:443
2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector82372 found all 1 destinations for 
clientservices.googleapis.com:443


This shows two subsequent connection attempts to 
clientservices.googleapis.com. The first one has both IPv4 and IPv6 
destinations. The IPv6 address is passed to markAsBad. After that the 
IPv4 address is not listed as a destination.

Note that there have been many connections to 
clientservices.googleapis.com prior to this where markAsBad was not 
called, even though IPv6 connectivity was never available.

-- 
Stephen


From rousskov at measurement-factory.com  Tue Jan  9 03:41:52 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 8 Jan 2024 22:41:52 -0500
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
Message-ID: <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>

On 2024-01-08 08:31, Stephen Borrill wrote:
> I'm trying to determine why squid 6.x (seen with 6.5) connected via 
> IPv4-only periodically fails to connect to the destination and then 
> requires a restart to fix it (reload is not sufficient).
> 
> The problem appears to be that a host that has one address each of IPv4 
> and IPv6 occasionally has its IPv4 address go missing as a destination. 
> On closer inspection, this appears to happen when the IPv6 address (not 
> the IPv4) address is marked as bad. A log fragment is as follows:
> 
> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(460) resolveSelected: 
> Find IP destination for: clientservices.googleapis.com:443' via 
> clientservices.googleapis.com
> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector82284 found conn696198 local=0.0.0.0 
> remote=142.250.187.227:443 HIER_DIRECT flags=1, destination #1 for 
> clientservices.googleapis.com:443
> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector82284 found conn696199 local=[::] 
> remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, destination 
> #2 for clientservices.googleapis.com:443
> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(479) resolveSelected: 
> PeerSelector82284 found all 2 destinations for 
> clientservices.googleapis.com:443
> 2024/01/08 13:18:40.245 kid1| 14,2| ipcache.cc(1031) markAsBad: 
> [2a00:1450:4009:820::2003]:443 of clientservices.googleapis.com
> 2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(946) seekNewGood: 
> succeeded for clientservices.googleapis.com: [2a00:1450:4009:820::2003] 
> #2/2-1
> 2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(978) restoreGoodness: 
> cleared all IPs for clientservices.googleapis.com; now back to 
> [2a00:1450:4009:820::2003] #2/2-1
> 2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
> Non-IP 'clientservices.googleapis.com': hostname or servname not 
> provided or not known
> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(460) resolveSelected: 
> Find IP destination for: clientservices.googleapis.com:443' via 
> clientservices.googleapis.com
> 2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
> Non-IP 'clientservices.googleapis.com': hostname or servname not 
> provided or not known
> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector82372 found conn697148 local=[::] 
> remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, destination 
> #1 for clientservices.googleapis.com:443
> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(479) resolveSelected: 
> PeerSelector82372 found all 1 destinations for 
> clientservices.googleapis.com:443
> 
> 
> This shows two subsequent connection attempts to 
> clientservices.googleapis.com. The first one has both IPv4 and IPv6 
> destinations. The IPv6 address is passed to markAsBad. 

Yes.


> After that the IPv4 address is not listed as a destination.

I do not see that. I see IPv6 address being selected as the first 
destination (instead of the IPv4 address).

I cannot explain why that happens though. Moreover, a combination of 
certain lines in your debug output near "seekNewGood" do not make sense 
to me -- I do not see how it is possible for Squid to display those 
exact debugging details, but I am probably missing something. Can you 
retest and repost similar lines with 14,9 (or at least 14,7) added to 
your debug_options (or share those lines privately; the more lines you 
can share, the better)?


> Note that there have been many connections to 
> clientservices.googleapis.com prior to this where markAsBad was not 
> called, even though IPv6 connectivity was never available.

No markAsBad() is probably normal if Squid did not try to establish an 
IPv6 connection or did not wait long enough to know the result of that 
attempt. However, that does not explain why Squid selected an IPv6 
address as the next "good" address right after marking that IPv6 address 
as bad (at "restoreGoodness" line) when there was another good IP 
address available. It is as if Squid stored two identical IPv6 addresses 
(and not IPv4 ones), but that should not happen either.

Alex.



From squid at borrill.org.uk  Tue Jan  9 09:51:01 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 9 Jan 2024 09:51:01 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
Message-ID: <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>

On 09/01/2024 03:41, Alex Rousskov wrote:
> On 2024-01-08 08:31, Stephen Borrill wrote:
>> I'm trying to determine why squid 6.x (seen with 6.5) connected via 
>> IPv4-only periodically fails to connect to the destination and then 
>> requires a restart to fix it (reload is not sufficient).
>>
>> The problem appears to be that a host that has one address each of 
>> IPv4 and IPv6 occasionally has its IPv4 address go missing as a 
>> destination. On closer inspection, this appears to happen when the 
>> IPv6 address (not the IPv4) address is marked as bad. A log fragment 
>> is as follows:
>>
>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(460) 
>> resolveSelected: Find IP destination for: 
>> clientservices.googleapis.com:443' via clientservices.googleapis.com
>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector82284 found conn696198 local=0.0.0.0 
>> remote=142.250.187.227:443 HIER_DIRECT flags=1, destination #1 for 
>> clientservices.googleapis.com:443
>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector82284 found conn696199 local=[::] 
>> remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, destination 
>> #2 for clientservices.googleapis.com:443
>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(479) 
>> resolveSelected: PeerSelector82284 found all 2 destinations for 
>> clientservices.googleapis.com:443
>> 2024/01/08 13:18:40.245 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>> [2a00:1450:4009:820::2003]:443 of clientservices.googleapis.com
>> 2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(946) seekNewGood: 
>> succeeded for clientservices.googleapis.com: 
>> [2a00:1450:4009:820::2003] #2/2-1
>> 2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(978) restoreGoodness: 
>> cleared all IPs for clientservices.googleapis.com; now back to 
>> [2a00:1450:4009:820::2003] #2/2-1
>> 2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: 
>> Given Non-IP 'clientservices.googleapis.com': hostname or servname not 
>> provided or not known
>> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(460) 
>> resolveSelected: Find IP destination for: 
>> clientservices.googleapis.com:443' via clientservices.googleapis.com
>> 2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: 
>> Given Non-IP 'clientservices.googleapis.com': hostname or servname not 
>> provided or not known
>> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector82372 found conn697148 local=[::] 
>> remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, destination 
>> #1 for clientservices.googleapis.com:443
>> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(479) 
>> resolveSelected: PeerSelector82372 found all 1 destinations for 
>> clientservices.googleapis.com:443
>>
>>
>> This shows two subsequent connection attempts to 
>> clientservices.googleapis.com. The first one has both IPv4 and IPv6 
>> destinations. The IPv6 address is passed to markAsBad. 
> 
> Yes.
> 
> 
>> After that the IPv4 address is not listed as a destination.
> 
> I do not see that. I see IPv6 address being selected as the first 
> destination (instead of the IPv4 address).

My reading of the the log files is that all possible destinations are 
listed, e.g. "found all 18 destinations for www.googleapis.com:443". 
Prior to markAsBad being called both IP addresses for 
clientservices.googleapis.com are shown, afterwards the IPv4 is not 
listed. It isn't just that IPv6 is being selected as the first 
destination, it is the ONLY destination as the IPv4 address is no longer 
a candidate.


> I cannot explain why that happens though. Moreover, a combination of 
> certain lines in your debug output near "seekNewGood" do not make sense 
> to me -- I do not see how it is possible for Squid to display those 
> exact debugging details, but I am probably missing something. Can you 
> retest and repost similar lines with 14,9 (or at least 14,7) added to 
> your debug_options (or share those lines privately; the more lines you 
> can share, the better)?

Thanks, I have started a new run with 14,7 and will pass it on privately 
unless I can distil out the relevant lines to be small enough for the list.

>> Note that there have been many connections to 
>> clientservices.googleapis.com prior to this where markAsBad was not 
>> called, even though IPv6 connectivity was never available.
> 
> No markAsBad() is probably normal if Squid did not try to establish an 
> IPv6 connection or did not wait long enough to know the result of that 
> attempt. However, that does not explain why Squid selected an IPv6 
> address as the next "good" address right after marking that IPv6 address 
> as bad (at "restoreGoodness" line) when there was another good IP 
> address available. It is as if Squid stored two identical IPv6 addresses 
> (and not IPv4 ones), but that should not happen either.

This is tangentially related to this thread too:
https://lists.squid-cache.org/pipermail/squid-users/2023-November/026266.html

Once only the IPv6 address is being used, then it returns 503 for that 
host and thus can quickly get marked as dead by a downstream squid 
meaning it does not get used at all (and if it's the only peer all 
access stops).

-- 
Stephen



From squid at borrill.org.uk  Tue Jan  9 10:56:16 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 9 Jan 2024 10:56:16 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
Message-ID: <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>

On 09/01/2024 09:51, Stephen Borrill wrote:
> On 09/01/2024 03:41, Alex Rousskov wrote:
>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>> I'm trying to determine why squid 6.x (seen with 6.5) connected via 
>>> IPv4-only periodically fails to connect to the destination and then 
>>> requires a restart to fix it (reload is not sufficient).
>>>
>>> The problem appears to be that a host that has one address each of 
>>> IPv4 and IPv6 occasionally has its IPv4 address go missing as a 
>>> destination. On closer inspection, this appears to happen when the 
>>> IPv6 address (not the IPv4) address is marked as bad. A log fragment 
>>> is as follows:
>>>
>>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(460) 
>>> resolveSelected: Find IP destination for: 
>>> clientservices.googleapis.com:443' via clientservices.googleapis.com
>>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector82284 found conn696198 local=0.0.0.0 
>>> remote=142.250.187.227:443 HIER_DIRECT flags=1, destination #1 for 
>>> clientservices.googleapis.com:443
>>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector82284 found conn696199 local=[::] 
>>> remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, 
>>> destination #2 for clientservices.googleapis.com:443
>>> 2024/01/08 13:18:39.974 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector82284 found all 2 destinations for 
>>> clientservices.googleapis.com:443
>>> 2024/01/08 13:18:40.245 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>> [2a00:1450:4009:820::2003]:443 of clientservices.googleapis.com
>>> 2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(946) seekNewGood: 
>>> succeeded for clientservices.googleapis.com: 
>>> [2a00:1450:4009:820::2003] #2/2-1
>>> 2024/01/08 13:18:40.245 kid1| 14,3| ipcache.cc(978) restoreGoodness: 
>>> cleared all IPs for clientservices.googleapis.com; now back to 
>>> [2a00:1450:4009:820::2003] #2/2-1
>>> 2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'clientservices.googleapis.com': hostname or servname 
>>> not provided or not known
>>> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(460) 
>>> resolveSelected: Find IP destination for: 
>>> clientservices.googleapis.com:443' via clientservices.googleapis.com
>>> 2024/01/08 13:18:42.065 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'clientservices.googleapis.com': hostname or servname 
>>> not provided or not known
>>> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector82372 found conn697148 local=[::] 
>>> remote=[2a00:1450:4009:820::2003]:443 HIER_DIRECT flags=1, 
>>> destination #1 for clientservices.googleapis.com:443
>>> 2024/01/08 13:18:42.065 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector82372 found all 1 destinations for 
>>> clientservices.googleapis.com:443
>>>
>>>
>>> This shows two subsequent connection attempts to 
>>> clientservices.googleapis.com. The first one has both IPv4 and IPv6 
>>> destinations. The IPv6 address is passed to markAsBad. 
>>
>> Yes.
>>
>>
>>> After that the IPv4 address is not listed as a destination.
>>
>> I do not see that. I see IPv6 address being selected as the first 
>> destination (instead of the IPv4 address).
> 
> My reading of the the log files is that all possible destinations are 
> listed, e.g. "found all 18 destinations for www.googleapis.com:443". 
> Prior to markAsBad being called both IP addresses for 
> clientservices.googleapis.com are shown, afterwards the IPv4 is not 
> listed. It isn't just that IPv6 is being selected as the first 
> destination, it is the ONLY destination as the IPv4 address is no longer 
> a candidate.
> 
> 
>> I cannot explain why that happens though. Moreover, a combination of 
>> certain lines in your debug output near "seekNewGood" do not make 
>> sense to me -- I do not see how it is possible for Squid to display 
>> those exact debugging details, but I am probably missing something. 
>> Can you retest and repost similar lines with 14,9 (or at least 14,7) 
>> added to your debug_options (or share those lines privately; the more 
>> lines you can share, the better)?
> 
> Thanks, I have started a new run with 14,7 and will pass it on privately 
> unless I can distil out the relevant lines to be small enough for the list.

A distilled version is here. Let me know if you want the full log.

This looks suspicious to me:
have: [2001:4860:4802:32::78]:443 at 0 in 216.239.38.120 #1/2-0

2024/01/09 09:34:34.102 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
forcesafesearch.google.com
2024/01/09 09:34:34.102 kid1| 14,4| ipcache.cc(657) 
ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
'forcesafesearch.google.com'
2024/01/09 09:34:34.102 kid1| 14,7| ipcache.cc(253) forwardIp: 
216.239.38.120
2024/01/09 09:34:34.102 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector128957 found conn1010793 local=0.0.0.0 
remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #1 for 
forcesafesearch.google.com:443
2024/01/09 09:34:34.102 kid1| 14,7| ipcache.cc(253) forwardIp: 
[2001:4860:4802:32::78]
2024/01/09 09:34:34.102 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector128957 found conn1010794 local=[::] 
remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #2 
for forcesafesearch.google.com:443
2024/01/09 09:34:34.102 kid1| 14,7| ipcache.cc(236) finalCallback: 
0x18f816d38
2024/01/09 09:34:34.102 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector128957 found all 2 destinations for 
forcesafesearch.google.com:443
2024/01/09 09:34:34.352 kid1| 14,7| ipcache.cc(990) have: 
[2001:4860:4802:32::78]:443 at 0 in 216.239.38.120 #1/2-0
2024/01/09 09:34:34.352 kid1| 14,2| ipcache.cc(1031) markAsBad: 
[2001:4860:4802:32::78]:443 of forcesafesearch.google.com
2024/01/09 09:34:34.352 kid1| 14,3| ipcache.cc(946) seekNewGood: 
succeeded for forcesafesearch.google.com: [2001:4860:4802:32::78] #2/2-1
2024/01/09 09:34:34.352 kid1| 14,3| ipcache.cc(978) restoreGoodness: 
cleared all IPs for forcesafesearch.google.com; now back to 
[2001:4860:4802:32::78] #2/2-1

2024/01/09 09:34:35.683 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
forcesafesearch.google.com
2024/01/09 09:34:35.683 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
or not known
2024/01/09 09:34:35.683 kid1| 14,4| ipcache.cc(657) 
ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
'forcesafesearch.google.com'
2024/01/09 09:34:35.683 kid1| 14,7| ipcache.cc(253) forwardIp: 
[2001:4860:4802:32::78]
2024/01/09 09:34:35.683 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector128974 found conn1010956 local=[::] 
remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
for forcesafesearch.google.com:443
2024/01/09 09:34:35.683 kid1| 14,7| ipcache.cc(236) finalCallback: 
0x18f816d38
2024/01/09 09:34:35.683 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector128974 found all 1 destinations for 
forcesafesearch.google.com:443
2024/01/09 09:34:35.683 kid1| 14,7| ipcache.cc(990) have: 
[2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-1


>>> Note that there have been many connections to 
>>> clientservices.googleapis.com prior to this where markAsBad was not 
>>> called, even though IPv6 connectivity was never available.
>>
>> No markAsBad() is probably normal if Squid did not try to establish an 
>> IPv6 connection or did not wait long enough to know the result of that 
>> attempt. However, that does not explain why Squid selected an IPv6 
>> address as the next "good" address right after marking that IPv6 
>> address as bad (at "restoreGoodness" line) when there was another good 
>> IP address available. It is as if Squid stored two identical IPv6 
>> addresses (and not IPv4 ones), but that should not happen either.
> 
> This is tangentially related to this thread too:
> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026266.html
> 
> Once only the IPv6 address is being used, then it returns 503 for that 
> host and thus can quickly get marked as dead by a downstream squid 
> meaning it does not get used at all (and if it's the only peer all 
> access stops).
> 



From s_p_arun at yahoo.com  Tue Jan  9 14:13:03 2024
From: s_p_arun at yahoo.com (Arun Kumar)
Date: Tue, 9 Jan 2024 14:13:03 +0000 (UTC)
Subject: [squid-users] chunked transfer over sslbump
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
Message-ID: <1868843187.239667.1704809583107@mail.yahoo.com>

Hello Squid Community,
I have compiled/installed squid v5.8 in Amazon Linux and configured it with sslbump option. Squid is used as proxy to get response from https site. When the https site sends chunked response, it appears that the first response comes but it get stuck and doesn't receive the full response. Appreciate any help.
Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240109/ed6a880f/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan  9 14:52:40 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Jan 2024 09:52:40 -0500
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <1868843187.239667.1704809583107@mail.yahoo.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
Message-ID: <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>

On 2024-01-09 09:13, Arun Kumar wrote:

> I have compiled/installed squid v5.8 in Amazon Linux and configured it 
> with sslbump option. Squid is used as proxy to get response from https 
> site. When the https site sends chunked response, it appears that the 
> first response comes but it get stuck and doesn't receive the full 
> response. Appreciate any help.
  There were some recent chunking-related changes in Squid, but none of 
them is likely to be responsible for the problems you are describing 
unless the origin server response is very special/unusual.

Does the client in this test get the HTTP response header? Some HTTP 
response body bytes?

To triage the problem, I recommend sharing the corresponding access.log 
records (at least). Seeing debugging of the problematic transaction may 
be very useful (but avoid using production security keys and other 
sensitive information in such tests):
https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

Please note that Squid v5 is not officially supported and has more known 
security vulnerabilities than Squid v6. You should be using Squid v6.


HTH,

Alex.



From rousskov at measurement-factory.com  Tue Jan  9 15:42:15 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Jan 2024 10:42:15 -0500
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
Message-ID: <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>

On 2024-01-09 05:56, Stephen Borrill wrote:
> On 09/01/2024 09:51, Stephen Borrill wrote:
>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected via 
>>>> IPv4-only periodically fails to connect to the destination and then 
>>>> requires a restart to fix it (reload is not sufficient).
>>>>
>>>> The problem appears to be that a host that has one address each of 
>>>> IPv4 and IPv6 occasionally has its IPv4 address go missing as a 
>>>> destination. On closer inspection, this appears to happen when the 
>>>> IPv6 address (not the IPv4) address is marked as bad.

> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 216.239.38.120 #1/2-0


Thank you for sharing more debugging info!

The above log line is self-contradictory AFAICT: It says that the cache 
has both IPv6-looking and IPv4-looking address at the same cache 
position (0) and, judging by the corresponding code, those two IP 
addresses are equal. This is not possible (for those specific IP address 
values). The subsequent Squid behavior can be explained by this 
(unexplained) conflict.

I assume you are running official Squid v6.5 code.

I can suggest the following two steps for going forward:

1. Upgrade to the latest Squid v6 in hope that the problem goes away.

2. If the problem is still there, patch the latest Squid v6 to add more 
debugging in hope to explain what is going on. This may take a few 
iterations, and it will take me some time to produce the necessary 
debugging patch.


HTH,

Alex.


>>>> Note that there have been many connections to 
>>>> clientservices.googleapis.com prior to this where markAsBad was not 
>>>> called, even though IPv6 connectivity was never available.
>>>
>>> No markAsBad() is probably normal if Squid did not try to establish 
>>> an IPv6 connection or did not wait long enough to know the result of 
>>> that attempt. However, that does not explain why Squid selected an 
>>> IPv6 address as the next "good" address right after marking that IPv6 
>>> address as bad (at "restoreGoodness" line) when there was another 
>>> good IP address available. It is as if Squid stored two identical 
>>> IPv6 addresses (and not IPv4 ones), but that should not happen either.
>>
>> This is tangentially related to this thread too:
>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026266.html
>>
>> Once only the IPv6 address is being used, then it returns 503 for that 
>> host and thus can quickly get marked as dead by a downstream squid 
>> meaning it does not get used at all (and if it's the only peer all 
>> access stops).
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From jinshu_zhang at fanniemae.com  Tue Jan  9 16:51:15 2024
From: jinshu_zhang at fanniemae.com (Zhang, Jinshu)
Date: Tue, 9 Jan 2024 16:51:15 +0000
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
Message-ID: <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>

Hi Alex, good morning. Thank you for your reply. I work together with Arun on this issue. Here is some more detail.

Client got below response headers and body. Masked few details.
Retry seems to fetch data remaining.
Want to point out that removing sslbump everything is working fine, but we wanted to keep it for ICAP scanning.
We tried compiling 6.x in Amazon linux, using latest gcc, but facing similar error - https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html

HTTP/1.1 200 OK
Date: Tue, 09 Jan 2024 15:41:33 GMT
Server: Apache/mod_perl/2.0.10 Perl
Content-Type: application/download
X-Cache: MISS from ip-x-y-z
Transfer-Encoding: chunked
Via: xxx (ICAP)
Connection: keep-alive

1000
File-Id: xyz.zip
Local-Path: x/y/z.txt
Content-Size: 2967
< binary content >


Access log(1st attempt):
1704814893.695    138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - FIRSTUP_PARENT/10.x.y.z -
1704814900.491   6779 172.17.0.2 TCP_MISS/200 138996535 POST https://a.b.com/xyz - FIRSTUP_PARENT/10.x.y.z application/download

Retry after 5 mins:
1704815201.530    189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - FIRSTUP_PARENT/10.x.y.z -
1704815208.438   6896 x.y.0.2 TCP_MISS/200 138967930 POST https://a.b.com/xyz - FIRSTUP_PARENT/10.x.y.z application/download

Jinshu Zhang


Fannie Mae Confidential
-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
Sent: Tuesday, January 9, 2024 9:53 AM
To: squid-users at lists.squid-cache.org
Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump


On 2024-01-09 09:13, Arun Kumar wrote:

> I have compiled/installed squid v5.8 in Amazon Linux and configured it
> with sslbump option. Squid is used as proxy to get response from https
> site. When the https site sends chunked response, it appears that the
> first response comes but it get stuck and doesn't receive the full
> response. Appreciate any help.
  There were some recent chunking-related changes in Squid, but none of them is likely to be responsible for the problems you are describing unless the origin server response is very special/unusual.

Does the client in this test get the HTTP response header? Some HTTP response body bytes?

To triage the problem, I recommend sharing the corresponding access.log records (at least). Seeing debugging of the problematic transaction may be very useful (but avoid using production security keys and other sensitive information in such tests):
https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

Please note that Squid v5 is not officially supported and has more known security vulnerabilities than Squid v6. You should be using Squid v6.


HTH,

Alex.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Jan  9 19:18:04 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Jan 2024 14:18:04 -0500
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
Message-ID: <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>

On 2024-01-09 11:51, Zhang, Jinshu wrote:

> Client got below response headers and body. Masked few details.

Thank you.


> Retry seems to fetch data remaining.

I would expect a successful retry to fetch the entire response, not just 
the remaining bytes, but perhaps that is what you meant. Thank you for 
sharing this info.


> Want to point out that removing sslbump everything is working fine,
> but we wanted to keep it for ICAP scanning.

What if you keep SslBump enabled but disable any ICAP analysis 
("icap_enable off")? This test may tell us if the problem is between 
Squid and the origin server or Squid and the ICAP service...


> We tried compiling 6.x in Amazon linux, using latest gcc, but facing similar error - https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html

What is the "latest gcc" version in your environment? I suspect it is 
not the latest GCC version available to folks running Amazon Linux, but 
you may need to install some packages to get a more recent GCC version. 
Unfortunately, I cannot give specific instructions for Amazon Linux 
right now.


HTH,

Alex.


> HTTP/1.1 200 OK
> Date: Tue, 09 Jan 2024 15:41:33 GMT
> Server: Apache/mod_perl/2.0.10 Perl
> Content-Type: application/download
> X-Cache: MISS from ip-x-y-z
> Transfer-Encoding: chunked
> Via: xxx (ICAP)
> Connection: keep-alive
> 
> 1000
> File-Id: xyz.zip
> Local-Path: x/y/z.txt
> Content-Size: 2967
> < binary content >
> 
> 
> Access log(1st attempt):
> 1704814893.695    138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - FIRSTUP_PARENT/10.x.y.z -
> 1704814900.491   6779 172.17.0.2 TCP_MISS/200 138996535 POST https://a.b.com/xyz - FIRSTUP_PARENT/10.x.y.z application/download
> 
> Retry after 5 mins:
> 1704815201.530    189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - FIRSTUP_PARENT/10.x.y.z -
> 1704815208.438   6896 x.y.0.2 TCP_MISS/200 138967930 POST https://a.b.com/xyz - FIRSTUP_PARENT/10.x.y.z application/download
> 
> Jinshu Zhang
> 
> 
> Fannie Mae Confidential
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
> Sent: Tuesday, January 9, 2024 9:53 AM
> To: squid-users at lists.squid-cache.org
> Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
> 
> 
> On 2024-01-09 09:13, Arun Kumar wrote:
> 
>> I have compiled/installed squid v5.8 in Amazon Linux and configured it
>> with sslbump option. Squid is used as proxy to get response from https
>> site. When the https site sends chunked response, it appears that the
>> first response comes but it get stuck and doesn't receive the full
>> response. Appreciate any help.
>    There were some recent chunking-related changes in Squid, but none of them is likely to be responsible for the problems you are describing unless the origin server response is very special/unusual.
> 
> Does the client in this test get the HTTP response header? Some HTTP response body bytes?
> 
> To triage the problem, I recommend sharing the corresponding access.log records (at least). Seeing debugging of the problematic transaction may be very useful (but avoid using production security keys and other sensitive information in such tests):
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> 
> Please note that Squid v5 is not officially supported and has more known security vulnerabilities than Squid v6. You should be using Squid v6.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From misho.98118 at gmail.com  Tue Jan  9 23:18:13 2024
From: misho.98118 at gmail.com (Miha Miha)
Date: Wed, 10 Jan 2024 01:18:13 +0200
Subject: [squid-users] Is Squid 6 production ready?
Message-ID: <CABZyDWHB_Jj3wYF242WSOvHQQop2RmpAH-_1+ho+Cr5VvRBG0Q@mail.gmail.com>

Hi there!

Release note of latest Squid 6.6 says: "...not deemed ready for
production use..."  For comparison Squid 5.1 was 'ready'. When v6 is
expected to be ready for prod systems?

Regards,
Mihail Mihaylov


From jzhu at proofpoint.com  Wed Jan 10 00:32:42 2024
From: jzhu at proofpoint.com (John Zhu)
Date: Wed, 10 Jan 2024 00:32:42 +0000
Subject: [squid-users] ICAP too many errors and suspensions
Message-ID: <FCCF1B00-22FC-4BB7-9A18-8C7FC04EAFBE@proofpoint.com>

Hi, Alex,

We have the same ?suspension? issue when ?too many failure?.  See this links.
https://www.mail-archive.com/squid-users at lists.squid-cache.org/msg22187.html
>> 14:24:24 kid1| suspending ICAP service for too many failures
>> 14:24:24 kid1| essential ICAP service is suspended:


We tried enable the debug_options ALL,1 93,7

But have not reproduced suspensions and did not find the root cause.


Checking the source code, can we simply comment out the lines:
        scheduleUpdate(squid_curtime + TheConfig.service_revival_delay);
        announceStatusChange("suspended", true);


void Adaptation::Icap::ServiceRep::suspend(const char *reason)
{
    if (isSuspended) {
        debugs(93,4, "keeping suspended, also for " << reason);
    } else {
        isSuspended = reason;
        debugs(93, DBG_IMPORTANT, "suspending ICAP service for " << reason);
        scheduleUpdate(squid_curtime + TheConfig.service_revival_delay);
        announceStatusChange("suspended", true);
    }
}




void Adaptation::Icap::ServiceRep::noteFailure()
{
    const int failures = theSessionFailures.count(1);
    debugs(93,4, " failure " << failures << " out of " <<
           TheConfig.service_failure_limit << " allowed in " <<
           TheConfig.oldest_service_failure << "sec " << status());

    if (isSuspended)
        return;

    if (TheConfig.service_failure_limit >= 0 &&
            failures > TheConfig.service_failure_limit)
        suspend("too many failures");

    // TODO: Should bypass setting affect how much Squid tries to talk to
    // the ICAP service that is currently unusable and is likely to remain
    // so for some time? The current code says "no". Perhaps the answer
    // should be configurable.
}

Regards,

John Zhu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240110/7ce4f66c/attachment.htm>

From squid at borrill.org.uk  Wed Jan 10 12:40:57 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Wed, 10 Jan 2024 12:40:57 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
Message-ID: <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>

On 09/01/2024 15:42, Alex Rousskov wrote:
> On 2024-01-09 05:56, Stephen Borrill wrote:
>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected via 
>>>>> IPv4-only periodically fails to connect to the destination and then 
>>>>> requires a restart to fix it (reload is not sufficient).
>>>>>
>>>>> The problem appears to be that a host that has one address each of 
>>>>> IPv4 and IPv6 occasionally has its IPv4 address go missing as a 
>>>>> destination. On closer inspection, this appears to happen when the 
>>>>> IPv6 address (not the IPv4) address is marked as bad.
> 
>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>> 216.239.38.120 #1/2-0
> 
> 
> Thank you for sharing more debugging info!

The following seemed odd to. It finds an IPv4 address (this host does 
not have IPv6), puts it in the cache and then says "No DNS records":

2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
schoolbase.online
2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
ipcacheRelease: Releasing entry for 'schoolbase.online'
2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
'schoolbase.online'
2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
answers for schoolbase.online
2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:  no 
20.54.32.34 in [no cached IPs]
2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:  no 
20.54.32.34 in [no cached IPs]
2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use first 
69 from RR TTL 69
2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
schoolbase.online #1 20.54.32.34
2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 20.54.32.34
2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector72389 found conn564274 local=0.0.0.0 remote=20.54.32.34:443 
HIER_DIRECT flags=1, destination #1 for schoolbase.online:443
2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: ERROR: 
DNS failure while resolving schoolbase.online: No DNS records
2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) ipcacheHandleReply: 
done with schoolbase.online: 20.54.32.34 #1/1-0
2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
0x1b7381f38  lookup_err=No DNS records

It seemed to happen about the same time as the other failure, so perhaps 
another symptom of the same.

> The above log line is self-contradictory AFAICT: It says that the cache 
> has both IPv6-looking and IPv4-looking address at the same cache 
> position (0) and, judging by the corresponding code, those two IP 
> addresses are equal. This is not possible (for those specific IP address 
> values). The subsequent Squid behavior can be explained by this 
> (unexplained) conflict.
> 
> I assume you are running official Squid v6.5 code.

Yes, compiled from source on NetBSD. I have the patch I refer to here 
applied too:
https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html

> I can suggest the following two steps for going forward:
> 
> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.

I have just upgraded to 6.6.

> 2. If the problem is still there, patch the latest Squid v6 to add more 
> debugging in hope to explain what is going on. This may take a few 
> iterations, and it will take me some time to produce the necessary 
> debugging patch.

Unfortunately, I don't have a test case that will cause the problem so I 
need to run this at a customer's production site that is particularly 
affected by it. Luckily, the problem recurs pretty quickly.

Here's a run with 6.6 where the number of destinations drops from 2 to 1 
before reverting. Not seen this before - usually once it has dropped to 
1 (the IPv6 address), it stays there until a restart (and this did 
happen about a minute after this log fragment). Happy to test out any 
debugging patch.

2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
forcesafesearch.google.com
2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
or not known
2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
'forcesafesearch.google.com'
2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
[2001:4860:4802:32::78]
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector300176 found conn2388484 local=[::] 
remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
for forcesafesearch.google.com:443
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
always_direct = ALLOWED
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
never_direct = DENIED
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
    timedout = 0
2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
216.239.38.120
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector300176 found conn2388485 local=0.0.0.0 
remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
forcesafesearch.google.com:443
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
always_direct = ALLOWED
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
never_direct = DENIED
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
    timedout = 0
2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
0x12208e038
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector300176 found all 2 destinations for 
forcesafesearch.google.com:443
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) resolveSelected: 
   always_direct = ALLOWED
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) resolveSelected: 
    never_direct = DENIED
2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) resolveSelected: 
        timedout = 0
2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
[2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
[2001:4860:4802:32::78]:443 of forcesafesearch.google.com
2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
216.239.38.120:443 of forcesafesearch.google.com
2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
or not known
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) resolveSelected: 
Find IP destination for: forcesafesearch.google.com:443' via 
forcesafesearch.google.com
2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
forcesafesearch.google.com
2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
or not known
2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
'forcesafesearch.google.com'
2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
[2001:4860:4802:32::78]
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector300177 found conn2388493 local=[::] 
remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
for forcesafesearch.google.com:443
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
always_direct = ALLOWED
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
never_direct = DENIED
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
    timedout = 0
2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
216.239.38.120
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector300177 found conn2388494 local=0.0.0.0 
remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
forcesafesearch.google.com:443
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
always_direct = ALLOWED
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
never_direct = DENIED
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
    timedout = 0
2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
0x12208e038
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) resolveSelected: 
PeerSelector300177 found all 2 destinations for 
forcesafesearch.google.com:443
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) resolveSelected: 
   always_direct = ALLOWED
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) resolveSelected: 
    never_direct = DENIED
2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) resolveSelected: 
        timedout = 0
2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
[2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
[2001:4860:4802:32::78]:443 of forcesafesearch.google.com
2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
216.239.38.120:443 of forcesafesearch.google.com

-- 
Stephen



From s_p_arun at yahoo.com  Wed Jan 10 14:21:12 2024
From: s_p_arun at yahoo.com (Arun Kumar)
Date: Wed, 10 Jan 2024 14:21:12 +0000 (UTC)
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
Message-ID: <929108967.11249363.1704896472206@mail.yahoo.com>

 i) Retry seems to fetch one chunk of the response and not the complete.ii) Enabling sslbump and turning ICAP off, not helping.?iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)
Also want to point out that, squid connects to another non-squid proxy to reach internet.cache_peer <proxy_url> parent <port> 0 no-query default
    On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov <rousskov at measurement-factory.com> wrote:  
 
 On 2024-01-09 11:51, Zhang, Jinshu wrote:

> Client got below response headers and body. Masked few details.

Thank you.


> Retry seems to fetch data remaining.

I would expect a successful retry to fetch the entire response, not just 
the remaining bytes, but perhaps that is what you meant. Thank you for 
sharing this info.


> Want to point out that removing sslbump everything is working fine,
> but we wanted to keep it for ICAP scanning.

What if you keep SslBump enabled but disable any ICAP analysis 
("icap_enable off")? This test may tell us if the problem is between 
Squid and the origin server or Squid and the ICAP service...


> We tried compiling 6.x in Amazon linux, using latest gcc, but facing similar error - https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html

What is the "latest gcc" version in your environment? I suspect it is 
not the latest GCC version available to folks running Amazon Linux, but 
you may need to install some packages to get a more recent GCC version. 
Unfortunately, I cannot give specific instructions for Amazon Linux 
right now.


HTH,

Alex.


> HTTP/1.1 200 OK
> Date: Tue, 09 Jan 2024 15:41:33 GMT
> Server: Apache/mod_perl/2.0.10 Perl
> Content-Type: application/download
> X-Cache: MISS from ip-x-y-z
> Transfer-Encoding: chunked
> Via: xxx (ICAP)
> Connection: keep-alive
> 
> 1000
> File-Id: xyz.zip
> Local-Path: x/y/z.txt
> Content-Size: 2967
> < binary content >
> 
> 
> Access log(1st attempt):
> 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - FIRSTUP_PARENT/10.x.y.z -
> 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST https://a.b.com/xyz - FIRSTUP_PARENT/10.x.y.z application/download
> 
> Retry after 5 mins:
> 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - FIRSTUP_PARENT/10.x.y.z -
> 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST https://a.b.com/xyz - FIRSTUP_PARENT/10.x.y.z application/download
> 
> Jinshu Zhang
> 
> 
> Fannie Mae Confidential
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex Rousskov
> Sent: Tuesday, January 9, 2024 9:53 AM
> To: squid-users at lists.squid-cache.org
> Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
> 
> 
> On 2024-01-09 09:13, Arun Kumar wrote:
> 
>> I have compiled/installed squid v5.8 in Amazon Linux and configured it
>> with sslbump option. Squid is used as proxy to get response from https
>> site. When the https site sends chunked response, it appears that the
>> first response comes but it get stuck and doesn't receive the full
>> response. Appreciate any help.
>? ? There were some recent chunking-related changes in Squid, but none of them is likely to be responsible for the problems you are describing unless the origin server response is very special/unusual.
> 
> Does the client in this test get the HTTP response header? Some HTTP response body bytes?
> 
> To triage the problem, I recommend sharing the corresponding access.log records (at least). Seeing debugging of the problematic transaction may be very useful (but avoid using production security keys and other sensitive information in such tests):
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction
> 
> Please note that Squid v5 is not officially supported and has more known security vulnerabilities than Squid v6. You should be using Squid v6.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240110/d1743ba6/attachment.htm>

From rousskov at measurement-factory.com  Wed Jan 10 14:46:50 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jan 2024 09:46:50 -0500
Subject: [squid-users] ICAP too many errors and suspensions
In-Reply-To: <FCCF1B00-22FC-4BB7-9A18-8C7FC04EAFBE@proofpoint.com>
References: <FCCF1B00-22FC-4BB7-9A18-8C7FC04EAFBE@proofpoint.com>
Message-ID: <71791178-e8f6-4e2c-b7ae-e0b1b65f845e@measurement-factory.com>

On 2024-01-09 19:32, John Zhu wrote:

> We have the same ?suspension? issue when ?too many failure?.

To clarify, you have a "failure" issue. Suspension after 
icap_service_failure_limit is normal/expected.


> https://www.mail-archive.com/squid-users at lists.squid-cache.org/msg22187.html

FWIW, AFAICT, the original problem was attributed to ClamAV service 
timing out Squid ICAP connection attempts while reloading its virus 
definitions:

https://lists.squid-cache.org/pipermail/squid-users/2021-February/023293.html


> 14:24:24 kid1| suspending ICAP service for too many failures 
> 14:24:24 kid1| essential ICAP service is suspended:

> We tried enable the debug_options ALL,1 93,7
> 
> But have not reproduced suspensions and did not find the root cause.

Please note that it is probably enough to reproduce a single failure; 
reproducing suspensions is better, but can be more difficult, and is 
probably less important. If you cannot see individual failures until the 
service is suspended, then use "icap_service_failure_limit 1" during 
your test, so that the service is suspended after the _first_ failure.

If "ALL,1 93,7" debugging prevents you from reproducing the problem, try 
debug_options set to "ALL,1 93,5", then "ALL,1 93,4", etc.


> Checking the source code, can we simply comment out the lines:
> 
> scheduleUpdate(squid_curtime+ TheConfig.service_revival_delay);
> announceStatusChange("suspended", true);

I have to decline this opportunity to discuss Squid source code 
modifications on the squid-users mailing list. If you want to disable 
service suspensions without understanding why ICAP transactions fail, 
then use a very large icap_service_failure_limit in squid.conf.


HTH,

Alex.



From rousskov at measurement-factory.com  Wed Jan 10 16:09:45 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jan 2024 11:09:45 -0500
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <929108967.11249363.1704896472206@mail.yahoo.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
 <929108967.11249363.1704896472206@mail.yahoo.com>
Message-ID: <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>

On 2024-01-10 09:21, Arun Kumar wrote:
> i) Retry seems to fetch one chunk of the response and not the complete.
> ii) Enabling sslbump and turning ICAP off, not helping.
> iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)

GCC v7 has insufficient C++17 support. I recommend installing GCC v9 or 
better and then trying with Squid v6.6 or newer.

FWIW, if the problem persists in Squid v6, sharing debugging logs would 
be the next recommended step.


HTH,

Alex.


> Also want to point out that, squid connects to another non-squid proxy 
> to reach internet.
> /cache_peer <proxy_url> parent <port> 0 no-query default/
> 
> On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov wrote:
> 
> 
> On 2024-01-09 11:51, Zhang, Jinshu wrote:
> 
>  > Client got below response headers and body. Masked few details.
> 
> Thank you.
> 
> 
>  > Retry seems to fetch data remaining.
> 
> I would expect a successful retry to fetch the entire response, not just
> the remaining bytes, but perhaps that is what you meant. Thank you for
> sharing this info.
> 
> 
>  > Want to point out that removing sslbump everything is working fine,
>  > but we wanted to keep it for ICAP scanning.
> 
> What if you keep SslBump enabled but disable any ICAP analysis
> ("icap_enable off")? This test may tell us if the problem is between
> Squid and the origin server or Squid and the ICAP service...
> 
> 
>  > We tried compiling 6.x in Amazon linux, using latest gcc, but facing 
> similar error - 
> https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>
> 
> What is the "latest gcc" version in your environment? I suspect it is
> not the latest GCC version available to folks running Amazon Linux, but
> you may need to install some packages to get a more recent GCC version.
> Unfortunately, I cannot give specific instructions for Amazon Linux
> right now.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>  > HTTP/1.1 200 OK
>  > Date: Tue, 09 Jan 2024 15:41:33 GMT
>  > Server: Apache/mod_perl/2.0.10 Perl
>  > Content-Type: application/download
>  > X-Cache: MISS from ip-x-y-z
>  > Transfer-Encoding: chunked
>  > Via: xxx (ICAP)
>  > Connection: keep-alive
>  >
>  > 1000
>  > File-Id: xyz.zip
>  > Local-Path: x/y/z.txt
>  > Content-Size: 2967
>  > < binary content >
>  >
>  >
>  > Access log(1st attempt):
>  > 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - 
> FIRSTUP_PARENT/10.x.y.z -
>  > 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST 
> https://a.b.com/xyz <https://a.b.com/xyz> - FIRSTUP_PARENT/10.x.y.z 
> application/download
>  >
>  > Retry after 5 mins:
>  > 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - 
> FIRSTUP_PARENT/10.x.y.z -
>  > 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST 
> https://a.b.com/xyz <https://a.b.com/xyz> - FIRSTUP_PARENT/10.x.y.z 
> application/download
>  >
>  > Jinshu Zhang
>  >
>  >
>  > Fannie Mae Confidential
>  > -----Original Message-----
>  > From: squid-users <squid-users-bounces at lists.squid-cache.org 
> <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Alex 
> Rousskov
>  > Sent: Tuesday, January 9, 2024 9:53 AM
>  > To: squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
>  >
>  >
>  > On 2024-01-09 09:13, Arun Kumar wrote:
>  >
>  >> I have compiled/installed squid v5.8 in Amazon Linux and configured it
>  >> with sslbump option. Squid is used as proxy to get response from https
>  >> site. When the https site sends chunked response, it appears that the
>  >> first response comes but it get stuck and doesn't receive the full
>  >> response. Appreciate any help.
>  >? ? There were some recent chunking-related changes in Squid, but none 
> of them is likely to be responsible for the problems you are describing 
> unless the origin server response is very special/unusual.
>  >
>  > Does the client in this test get the HTTP response header? Some HTTP 
> response body bytes?
>  >
>  > To triage the problem, I recommend sharing the corresponding 
> access.log records (at least). Seeing debugging of the problematic 
> transaction may be very useful (but avoid using production security keys 
> and other sensitive information in such tests):
>  > 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
>  >
>  > Please note that Squid v5 is not officially supported and has more 
> known security vulnerabilities than Squid v6. You should be using Squid v6.
>  >
>  >
>  > HTH,
>  >
>  > Alex.
>  >
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>  >
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>



From dwd at fnal.gov  Wed Jan 10 21:48:05 2024
From: dwd at fnal.gov (Dave Dykstra)
Date: Wed, 10 Jan 2024 21:48:05 +0000
Subject: [squid-users] Is a workaround for SQUID-2023:9 to disable TRACE
 requests?
Message-ID: <ZZ8QlFzR0bjObn5s@fnal.gov>

We currently are unable to upgrade to squid6 due to a serious problem we found with collapsed_forwarding (https://bugs.squid-cache.org/show_bug.cgi?id=5332), and our applications need collapsed_forwarding for reasonable performance.

So we want to build a version of squid5 with as many vulnerabilities patched as we can.  All the posted 2023 vulnerabilities we care about include squid5 patches except one: https://github.com/squid-cache/squid/security/advisories/GHSA-rj5h-46j6-q2g5.  That is listed only as being patched in version 6.0.1, which is not an option.  

I'm pretty sure based on the "Patches Released" date listed at the bottom of the advisory that this was fixed in https://github.com/squid-cache/squid/pull/1127.  A further corroboration is that Joshua's vulnerability list at
    https://megamansec.github.io/Squid-Security-Audit/
lists that GHSA as a fix for "Use-After-Free in TRACE requests" and the description at
    https://megamansec.github.io/Squid-Security-Audit/trace-uaf.html
points to a bit of code that was deleted in the above PR.

So, my question is: since Joshua said the vulnerability was in the TRACE request, is another workaround to disable TRACE requests rather than disabling collapsed_forwarding?  That's something we can do, where disabling collapsed_forwarding is not something we can do.

Dave

From rousskov at measurement-factory.com  Wed Jan 10 22:23:57 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jan 2024 17:23:57 -0500
Subject: [squid-users] Is a workaround for SQUID-2023:9 to disable TRACE
 requests?
In-Reply-To: <ZZ8QlFzR0bjObn5s@fnal.gov>
References: <ZZ8QlFzR0bjObn5s@fnal.gov>
Message-ID: <4f471eeb-0163-4825-a174-ef2eea65e034@measurement-factory.com>

On 2024-01-10 16:48, Dave Dykstra wrote:

> https://github.com/squid-cache/squid/security/advisories/GHSA-rj5h-46j6-q2g5.  

> ... is another workaround to disable TRACE requests ...?

AFAICT, denying TRACE requests will not allow TRACE transactions to 
reach the problematic code related to that Advisory (under the typical 
conditions you probably care about). However, please note that the same 
or similar bugs can probably be triggered using other requests, under 
other conditions.

In other words, if you just want protection against a script kiddie 
blindly following "Use-After-Free in TRACE Requests" instructions on how 
to kill Squid, then denying TRACE requests should be sufficient. If you 
want protection from somebody who understands the underlying problem and 
spends the time on finding other ways to exploit it, then denying TRACE 
requests (or even disabling collapsed forwarding) may not be enough IMO.


HTH,

Alex.



From squid3 at treenet.co.nz  Fri Jan 12 13:37:02 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jan 2024 02:37:02 +1300
Subject: [squid-users] Is Squid 6 production ready?
In-Reply-To: <CABZyDWHB_Jj3wYF242WSOvHQQop2RmpAH-_1+ho+Cr5VvRBG0Q@mail.gmail.com>
References: <CABZyDWHB_Jj3wYF242WSOvHQQop2RmpAH-_1+ho+Cr5VvRBG0Q@mail.gmail.com>
Message-ID: <d8faeb35-9b42-49ab-9862-c304d6a09a3b@treenet.co.nz>

On 10/01/24 12:18, Miha Miha wrote:
> Release note of latest Squid 6.6 says: "...not deemed ready for
> production use..."  For comparison Squid 5.1 was 'ready'. When v6 is
> expected to be ready for prod systems?
> 

Sorry, that is an oversight in the release notes text. Removing it now.

Squid 6 is production ready.


Cheers
Amos


From s_p_arun at yahoo.com  Fri Jan 12 14:21:06 2024
From: s_p_arun at yahoo.com (Arun Kumar)
Date: Fri, 12 Jan 2024 14:21:06 +0000 (UTC)
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
 <929108967.11249363.1704896472206@mail.yahoo.com>
 <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>
Message-ID: <1781216472.758279.1705069266312@mail.yahoo.com>

 On Wednesday, January 10, 2024 at 11:09:48 AM EST, Alex Rousskov <rousskov at measurement-factory.com> wrote:
 
 
 On 2024-01-10 09:21, Arun Kumar wrote:
>> i) Retry seems to fetch one chunk of the response and not the complete.
>> ii) Enabling sslbump and turning ICAP off, not helping.
>> iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)

>GCC v7 has insufficient C++17 support. I recommend installing GCC v9 or 
better and then trying with Squid v6.6 or newer.

Arun: Compiled Squid 6.6 with gcc 11.4 and still seeing the same issue.
> FWIW, if the problem persists in Squid v6, sharing debugging logs would 
be the next recommended step.

Arun: debug_options ALL,6 giving too much log. Any particular option we can use to debug this issue?

>HTH,

>Alex.


> Also want to point out that, squid connects to another non-squid proxy 
> to reach internet.
> /cache_peer <proxy_url> parent <port> 0 no-query default/
> 
> On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov wrote:
> 
> 
> On 2024-01-09 11:51, Zhang, Jinshu wrote:
> 
>? > Client got below response headers and body. Masked few details.
> 
> Thank you.
> 
> 
>? > Retry seems to fetch data remaining.
> 
> I would expect a successful retry to fetch the entire response, not just
> the remaining bytes, but perhaps that is what you meant. Thank you for
> sharing this info.
> 
> 
>? > Want to point out that removing sslbump everything is working fine,
>? > but we wanted to keep it for ICAP scanning.
> 
> What if you keep SslBump enabled but disable any ICAP analysis
> ("icap_enable off")? This test may tell us if the problem is between
> Squid and the origin server or Squid and the ICAP service...
> 
> 
>? > We tried compiling 6.x in Amazon linux, using latest gcc, but facing 
> similar error - 
> https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <[squid-users] compile error in squid v6.1>
> 
> What is the "latest gcc" version in your environment? I suspect it is
> not the latest GCC version available to folks running Amazon Linux, but
> you may need to install some packages to get a more recent GCC version.
> Unfortunately, I cannot give specific instructions for Amazon Linux
> right now.
> 
> 
> HTH,
> 
> Alex.
> 
> 
>? > HTTP/1.1 200 OK
>? > Date: Tue, 09 Jan 2024 15:41:33 GMT
>? > Server: Apache/mod_perl/2.0.10 Perl
>? > Content-Type: application/download
>? > X-Cache: MISS from ip-x-y-z
>? > Transfer-Encoding: chunked
>? > Via: xxx (ICAP)
>? > Connection: keep-alive
>? >
>? > 1000
>? > File-Id: xyz.zip
>? > Local-Path: x/y/z.txt
>? > Content-Size: 2967
>? > < binary content >
>? >
>? >
>? > Access log(1st attempt):
>? > 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - 
> FIRSTUP_PARENT/10.x.y.z -
>? > 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST 
> https://a.b.com/xyz <https://a.b.com/xyz> - FIRSTUP_PARENT/10.x.y.z 
> application/download
>? >
>? > Retry after 5 mins:
>? > 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 - 
> FIRSTUP_PARENT/10.x.y.z -
>? > 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST 
> https://a.b.com/xyz <https://a.b.com/xyz> - FIRSTUP_PARENT/10.x.y.z 
> application/download
>? >
>? > Jinshu Zhang
>? >
>? >
>? > Fannie Mae Confidential
>? > -----Original Message-----
>? > From: squid-users <squid-users-bounces at lists.squid-cache.org 
> <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Alex 
> Rousskov
>? > Sent: Tuesday, January 9, 2024 9:53 AM
>? > To: squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
>? >
>? >
>? > On 2024-01-09 09:13, Arun Kumar wrote:
>? >
>? >> I have compiled/installed squid v5.8 in Amazon Linux and configured it
>? >> with sslbump option. Squid is used as proxy to get response from https
>? >> site. When the https site sends chunked response, it appears that the
>? >> first response comes but it get stuck and doesn't receive the full
>? >> response. Appreciate any help.
>? >? ? There were some recent chunking-related changes in Squid, but none 
> of them is likely to be responsible for the problems you are describing 
> unless the origin server response is very special/unusual.
>? >
>? > Does the client in this test get the HTTP response header? Some HTTP 
> response body bytes?
>? >
>? > To triage the problem, I recommend sharing the corresponding 
> access.log records (at least). Seeing debugging of the problematic 
> transaction may be very useful (but avoid using production security keys 
> and other sensitive information in such tests):
>? > 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <Sending Bug Reports to the Squid Team>
>? >
>? > Please note that Squid v5 is not officially supported and has more 
> known security vulnerabilities than Squid v6. You should be using Squid v6.
>? >
>? >
>? > HTH,
>? >
>? > Alex.
>? >
>? > _______________________________________________
>? > squid-users mailing list
>? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? >
>? > _______________________________________________
>? > squid-users mailing list
>? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> https://lists.squid-cache.org/listinfo/squid-users 
> <squid-users Info Page>



| 
| 
|  | 
squid-users Info Page


 |

 |

 |





| 
| 
|  | 
Sending Bug Reports to the Squid Team

Squid Web Cache documentation
 |

 |

 |





| 
| 
|  | 
[squid-users] compile error in squid v6.1


 |

 |

 |



  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240112/65d55764/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 12 19:10:37 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Jan 2024 14:10:37 -0500
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <1781216472.758279.1705069266312@mail.yahoo.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
 <929108967.11249363.1704896472206@mail.yahoo.com>
 <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>
 <1781216472.758279.1705069266312@mail.yahoo.com>
Message-ID: <3b0de845-eff2-485a-9528-9e714f06c0b1@measurement-factory.com>

On 2024-01-12 09:21, Arun Kumar wrote:
> On Wednesday, January 10, 2024 at 11:09:48 AM EST, Alex Rousskov wrote:
> 
> 
> On 2024-01-10 09:21, Arun Kumar wrote:
>  >> i) Retry seems to fetch one chunk of the response and not the complete.
>  >> ii) Enabling sslbump and turning ICAP off, not helping.
>  >> iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)
> 
>  >GCC v7 has insufficient C++17 support. I recommend installing GCC v9 or
> better and then trying with Squid v6.6 or newer.
> 
> Arun: Compiled Squid 6.6 with gcc 11.4 and still seeing the same issue.

Glad you were able to upgrade to Squid v6.6!


>  > FWIW, if the problem persists in Squid v6, sharing debugging logs would
> be the next recommended step.
> 
> Arun: /debug_options ALL,6 /giving too much log. Any particular option 
> we can use to debug this issue?


Please share[^1] a pointer to compressed ALL,9 cache.log collected while 
reproducing the problem with Squid v6.6:

https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

Debugging logs are for developers. Developers can deal with large 
volumes of debugging information. You can use services like DropBox to 
share large compressed logs. Said that, the better you can isolate the 
problem/traffic, the higher are the chances that a developer will (have 
the time to) find the answer to your question in the noisy log.

[^1]: Please feel free to share privately if needed, especially if you 
are using sensitive configuration or transactions.

Alex.


>  > Also want to point out that, squid connects to another non-squid proxy
>  > to reach internet.
>  > cache_peer <proxy_url> parent <port> 0 no-query default
>  >
>  > On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov wrote:
>  >
>  >
>  > On 2024-01-09 11:51, Zhang, Jinshu wrote:
>  >
>  >? > Client got below response headers and body. Masked few details.
>  >
>  > Thank you.
>  >
>  >
>  >? > Retry seems to fetch data remaining.
>  >
>  > I would expect a successful retry to fetch the entire response, not just
>  > the remaining bytes, but perhaps that is what you meant. Thank you for
>  > sharing this info.
>  >
>  >
>  >? > Want to point out that removing sslbump everything is working fine,
>  >? > but we wanted to keep it for ICAP scanning.
>  >
>  > What if you keep SslBump enabled but disable any ICAP analysis
>  > ("icap_enable off")? This test may tell us if the problem is between
>  > Squid and the origin server or Squid and the ICAP service...
>  >
>  >
>  >? > We tried compiling 6.x in Amazon linux, using latest gcc, but facing
>  > similar error -
>  > 
> https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html> <[squid-users] compile error in squid v6.1 <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>>
>  >
>  > What is the "latest gcc" version in your environment? I suspect it is
>  > not the latest GCC version available to folks running Amazon Linux, but
>  > you may need to install some packages to get a more recent GCC version.
>  > Unfortunately, I cannot give specific instructions for Amazon Linux
>  > right now.
>  >
>  >
>  > HTH,
>  >
>  > Alex.
>  >
>  >
>  >? > HTTP/1.1 200 OK
>  >? > Date: Tue, 09 Jan 2024 15:41:33 GMT
>  >? > Server: Apache/mod_perl/2.0.10 Perl
>  >? > Content-Type: application/download
>  >? > X-Cache: MISS from ip-x-y-z
>  >? > Transfer-Encoding: chunked
>  >? > Via: xxx (ICAP)
>  >? > Connection: keep-alive
>  >? >
>  >? > 1000
>  >? > File-Id: xyz.zip
>  >? > Local-Path: x/y/z.txt
>  >? > Content-Size: 2967
>  >? > < binary content >
>  >? >
>  >? >
>  >? > Access log(1st attempt):
>  >? > 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>  > FIRSTUP_PARENT/10.x.y.z -
>  >? > 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST
>  > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> - FIRSTUP_PARENT/10.x.y.z
>  > application/download
>  >? >
>  >? > Retry after 5 mins:
>  >? > 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>  > FIRSTUP_PARENT/10.x.y.z -
>  >? > 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST
>  > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> - FIRSTUP_PARENT/10.x.y.z
>  > application/download
>  >? >
>  >? > Jinshu Zhang
>  >? >
>  >? >
>  >? > Fannie Mae Confidential
>  >? > -----Original Message-----
>  >? > From: squid-users <squid-users-bounces at lists.squid-cache.org 
> <mailto:squid-users-bounces at lists.squid-cache.org>
>  > <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Alex
>  > Rousskov
>  >? > Sent: Tuesday, January 9, 2024 9:53 AM
>  >? > To: squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > <mailto:squid-users at lists.squid-cache.org>
>  >? > Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
>  >? >
>  >? >
>  >? > On 2024-01-09 09:13, Arun Kumar wrote:
>  >? >
>  >? >> I have compiled/installed squid v5.8 in Amazon Linux and 
> configured it
>  >? >> with sslbump option. Squid is used as proxy to get response from 
> https
>  >? >> site. When the https site sends chunked response, it appears that the
>  >? >> first response comes but it get stuck and doesn't receive the full
>  >? >> response. Appreciate any help.
>  >? >? ? There were some recent chunking-related changes in Squid, but none
>  > of them is likely to be responsible for the problems you are describing
>  > unless the origin server response is very special/unusual.
>  >? >
>  >? > Does the client in this test get the HTTP response header? Some HTTP
>  > response body bytes?
>  >? >
>  >? > To triage the problem, I recommend sharing the corresponding
>  > access.log records (at least). Seeing debugging of the problematic
>  > transaction may be very useful (but avoid using production security keys
>  > and other sensitive information in such tests):
>  >? >
>  > 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <Sending Bug Reports to the Squid Team <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>
>  >? >
>  >? > Please note that Squid v5 is not officially supported and has more
>  > known security vulnerabilities than Squid v6. You should be using 
> Squid v6.
>  >? >
>  >? >
>  >? > HTH,
>  >? >
>  >? > Alex.
>  >? >
>  >? > _______________________________________________
>  >? > squid-users mailing list
>  >? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > <mailto:squid-users at lists.squid-cache.org>
>  >? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>  > <https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>>
>  >? >
>  >? > _______________________________________________
>  >? > squid-users mailing list
>  >? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>  > <mailto:squid-users at lists.squid-cache.org>
>  >? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>  > <https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>>
>  >
>  > _______________________________________________
>  > squid-users mailing list
>  > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org> 
> <mailto:squid-users at lists.squid-cache.org>
> 
>  > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>  > <squid-users Info Page 
> <https://lists.squid-cache.org/listinfo/squid-users>>
> 
> 
> 	
> 
> 
>     squid-users Info Page
> 
> <https://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> 	
> 
> 
>     Sending Bug Reports to the Squid Team
> 
> Squid Web Cache documentation
> 
> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
> 
> 
> 	
> 
> 
>     [squid-users] compile error in squid v6.1
> 
> <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>
> 
> 



From squid at borrill.org.uk  Tue Jan 16 11:01:08 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 16 Jan 2024 11:01:08 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
Message-ID: <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>

The problem is no different with 6.6. Is there any more debugging I can 
provide, Alex?

On 10/01/2024 12:40, Stephen Borrill wrote:
> On 09/01/2024 15:42, Alex Rousskov wrote:
>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected 
>>>>>> via IPv4-only periodically fails to connect to the destination and 
>>>>>> then requires a restart to fix it (reload is not sufficient).
>>>>>>
>>>>>> The problem appears to be that a host that has one address each of 
>>>>>> IPv4 and IPv6 occasionally has its IPv4 address go missing as a 
>>>>>> destination. On closer inspection, this appears to happen when the 
>>>>>> IPv6 address (not the IPv4) address is marked as bad.
>>
>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>> 216.239.38.120 #1/2-0
>>
>>
>> Thank you for sharing more debugging info!
> 
> The following seemed odd to. It finds an IPv4 address (this host does 
> not have IPv6), puts it in the cache and then says "No DNS records":
> 
> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
> schoolbase.online
> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
> ipcacheRelease: Releasing entry for 'schoolbase.online'
> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
> 'schoolbase.online'
> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
> answers for schoolbase.online
> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
> 20.54.32.34 in [no cached IPs]
> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
> 20.54.32.34 in [no cached IPs]
> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use first 
> 69 from RR TTL 69
> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
> schoolbase.online #1 20.54.32.34
> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 20.54.32.34
> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector72389 found conn564274 local=0.0.0.0 remote=20.54.32.34:443 
> HIER_DIRECT flags=1, destination #1 for schoolbase.online:443
> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: ERROR: 
> DNS failure while resolving schoolbase.online: No DNS records
> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) ipcacheHandleReply: 
> done with schoolbase.online: 20.54.32.34 #1/1-0
> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
> 0x1b7381f38? lookup_err=No DNS records
> 
> It seemed to happen about the same time as the other failure, so perhaps 
> another symptom of the same.
> 
>> The above log line is self-contradictory AFAICT: It says that the 
>> cache has both IPv6-looking and IPv4-looking address at the same cache 
>> position (0) and, judging by the corresponding code, those two IP 
>> addresses are equal. This is not possible (for those specific IP 
>> address values). The subsequent Squid behavior can be explained by 
>> this (unexplained) conflict.
>>
>> I assume you are running official Squid v6.5 code.
> 
> Yes, compiled from source on NetBSD. I have the patch I refer to here 
> applied too:
> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
> 
>> I can suggest the following two steps for going forward:
>>
>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
> 
> I have just upgraded to 6.6.
> 
>> 2. If the problem is still there, patch the latest Squid v6 to add 
>> more debugging in hope to explain what is going on. This may take a 
>> few iterations, and it will take me some time to produce the necessary 
>> debugging patch.
> 
> Unfortunately, I don't have a test case that will cause the problem so I 
> need to run this at a customer's production site that is particularly 
> affected by it. Luckily, the problem recurs pretty quickly.
> 
> Here's a run with 6.6 where the number of destinations drops from 2 to 1 
> before reverting. Not seen this before - usually once it has dropped to 
> 1 (the IPv6 address), it stays there until a restart (and this did 
> happen about a minute after this log fragment). Happy to test out any 
> debugging patch.
> 
> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
> forcesafesearch.google.com
> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
> Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
> or not known
> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
> 'forcesafesearch.google.com'
> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
> [2001:4860:4802:32::78]
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector300176 found conn2388484 local=[::] 
> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
> for forcesafesearch.google.com:443
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
> always_direct = ALLOWED
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
> never_direct = DENIED
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath:    
> timedout = 0
> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
> 216.239.38.120
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector300176 found conn2388485 local=0.0.0.0 
> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
> forcesafesearch.google.com:443
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
> always_direct = ALLOWED
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
> never_direct = DENIED
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath:    
> timedout = 0
> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
> 0x12208e038
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) resolveSelected: 
> PeerSelector300176 found all 2 destinations for 
> forcesafesearch.google.com:443
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) resolveSelected: 
>  ? always_direct = ALLOWED
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) resolveSelected: 
>  ?? never_direct = DENIED
> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) resolveSelected: 
>  ?????? timedout = 0
> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
> 216.239.38.120:443 of forcesafesearch.google.com
> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
> Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
> or not known
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) resolveSelected: 
> Find IP destination for: forcesafesearch.google.com:443' via 
> forcesafesearch.google.com
> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
> forcesafesearch.google.com
> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: Given 
> Non-IP 'forcesafesearch.google.com': hostname or servname not provided 
> or not known
> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
> 'forcesafesearch.google.com'
> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
> [2001:4860:4802:32::78]
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector300177 found conn2388493 local=[::] 
> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
> for forcesafesearch.google.com:443
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
> always_direct = ALLOWED
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
> never_direct = DENIED
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath:    
> timedout = 0
> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
> 216.239.38.120
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector300177 found conn2388494 local=0.0.0.0 
> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
> forcesafesearch.google.com:443
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
> always_direct = ALLOWED
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
> never_direct = DENIED
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath:    
> timedout = 0
> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
> 0x12208e038
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) resolveSelected: 
> PeerSelector300177 found all 2 destinations for 
> forcesafesearch.google.com:443
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) resolveSelected: 
>  ? always_direct = ALLOWED
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) resolveSelected: 
>  ?? never_direct = DENIED
> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) resolveSelected: 
>  ?????? timedout = 0
> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
> 216.239.38.120:443 of forcesafesearch.google.com
> 



From rousskov at measurement-factory.com  Tue Jan 16 14:37:54 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jan 2024 09:37:54 -0500
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
Message-ID: <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>

On 2024-01-16 06:01, Stephen Borrill wrote:
> The problem is no different with 6.6. Is there any more debugging I can 
> provide, Alex?

Yes, but I need to give you a patch that adds that (temporary) debugging 
first (assuming I fail to reproduce the problem in the lab). The ball is 
on my side (unless somebody else steps in). Unfortunately, I do not have 
any free time for any of that right now. If you do not hear from me 
sooner, please ping me again on or after February 8, 2024.


Thank you,

Alex.

> On 10/01/2024 12:40, Stephen Borrill wrote:
>> On 09/01/2024 15:42, Alex Rousskov wrote:
>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected 
>>>>>>> via IPv4-only periodically fails to connect to the destination 
>>>>>>> and then requires a restart to fix it (reload is not sufficient).
>>>>>>>
>>>>>>> The problem appears to be that a host that has one address each 
>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go missing as 
>>>>>>> a destination. On closer inspection, this appears to happen when 
>>>>>>> the IPv6 address (not the IPv4) address is marked as bad.
>>>
>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>>> 216.239.38.120 #1/2-0
>>>
>>>
>>> Thank you for sharing more debugging info!
>>
>> The following seemed odd to. It finds an IPv4 address (this host does 
>> not have IPv6), puts it in the cache and then says "No DNS records":
>>
>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>> schoolbase.online
>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
>> 'schoolbase.online'
>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
>> answers for schoolbase.online
>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>> 20.54.32.34 in [no cached IPs]
>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>> 20.54.32.34 in [no cached IPs]
>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use 
>> first 69 from RR TTL 69
>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
>> schoolbase.online #1 20.54.32.34
>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 
>> 20.54.32.34
>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector72389 found conn564274 local=0.0.0.0 
>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for 
>> schoolbase.online:443
>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: 
>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) 
>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
>> 0x1b7381f38? lookup_err=No DNS records
>>
>> It seemed to happen about the same time as the other failure, so 
>> perhaps another symptom of the same.
>>
>>> The above log line is self-contradictory AFAICT: It says that the 
>>> cache has both IPv6-looking and IPv4-looking address at the same 
>>> cache position (0) and, judging by the corresponding code, those two 
>>> IP addresses are equal. This is not possible (for those specific IP 
>>> address values). The subsequent Squid behavior can be explained by 
>>> this (unexplained) conflict.
>>>
>>> I assume you are running official Squid v6.5 code.
>>
>> Yes, compiled from source on NetBSD. I have the patch I refer to here 
>> applied too:
>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
>>
>>> I can suggest the following two steps for going forward:
>>>
>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
>>
>> I have just upgraded to 6.6.
>>
>>> 2. If the problem is still there, patch the latest Squid v6 to add 
>>> more debugging in hope to explain what is going on. This may take a 
>>> few iterations, and it will take me some time to produce the 
>>> necessary debugging patch.
>>
>> Unfortunately, I don't have a test case that will cause the problem so 
>> I need to run this at a customer's production site that is 
>> particularly affected by it. Luckily, the problem recurs pretty quickly.
>>
>> Here's a run with 6.6 where the number of destinations drops from 2 to 
>> 1 before reverting. Not seen this before - usually once it has dropped 
>> to 1 (the IPv6 address), it stays there until a restart (and this did 
>> happen about a minute after this log fragment). Happy to test out any 
>> debugging patch.
>>
>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>> forcesafesearch.google.com
>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: 
>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>> provided or not known
>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>> 'forcesafesearch.google.com'
>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>> [2001:4860:4802:32::78]
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector300176 found conn2388484 local=[::] 
>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
>> for forcesafesearch.google.com:443
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>> always_direct = ALLOWED
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>> never_direct = DENIED
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>> timedout = 0
>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>> 216.239.38.120
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector300176 found conn2388485 local=0.0.0.0 
>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>> forcesafesearch.google.com:443
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>> always_direct = ALLOWED
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>> never_direct = DENIED
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>> timedout = 0
>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
>> 0x12208e038
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) 
>> resolveSelected: PeerSelector300176 found all 2 destinations for 
>> forcesafesearch.google.com:443
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) 
>> resolveSelected: ?? always_direct = ALLOWED
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) 
>> resolveSelected: ??? never_direct = DENIED
>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) 
>> resolveSelected: ??????? timedout = 0
>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>> 216.239.38.120:443 of forcesafesearch.google.com
>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>> provided or not known
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) 
>> resolveSelected: Find IP destination for: 
>> forcesafesearch.google.com:443' via forcesafesearch.google.com
>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>> forcesafesearch.google.com
>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>> provided or not known
>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>> 'forcesafesearch.google.com'
>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>> [2001:4860:4802:32::78]
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector300177 found conn2388493 local=[::] 
>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination #1 
>> for forcesafesearch.google.com:443
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>> always_direct = ALLOWED
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>> never_direct = DENIED
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>> timedout = 0
>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>> 216.239.38.120
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>> PeerSelector300177 found conn2388494 local=0.0.0.0 
>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>> forcesafesearch.google.com:443
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>> always_direct = ALLOWED
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>> never_direct = DENIED
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>> timedout = 0
>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
>> 0x12208e038
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) 
>> resolveSelected: PeerSelector300177 found all 2 destinations for 
>> forcesafesearch.google.com:443
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) 
>> resolveSelected: ?? always_direct = ALLOWED
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) 
>> resolveSelected: ??? never_direct = DENIED
>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) 
>> resolveSelected: ??????? timedout = 0
>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>> 216.239.38.120:443 of forcesafesearch.google.com
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From squid at borrill.org.uk  Tue Jan 16 14:43:03 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 16 Jan 2024 14:43:03 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
Message-ID: <313179e2-b3ab-4924-8fb8-4b90905a55d6@borrill.org.uk>

On 16/01/2024 14:37, Alex Rousskov wrote:
> On 2024-01-16 06:01, Stephen Borrill wrote:
>> The problem is no different with 6.6. Is there any more debugging I 
>> can provide, Alex?
> 
> Yes, but I need to give you a patch that adds that (temporary) debugging 
> first (assuming I fail to reproduce the problem in the lab). The ball is 
> on my side (unless somebody else steps in). Unfortunately, I do not have 
> any free time for any of that right now. If you do not hear from me 
> sooner, please ping me again on or after February 8, 2024.

Thanks. In the meantime, I have created a local DNS entry for 
forcesafesearch.google.com that only returns the A record. I think that 
should work around it (for that site, but not others).

>> On 10/01/2024 12:40, Stephen Borrill wrote:
>>> On 09/01/2024 15:42, Alex Rousskov wrote:
>>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected 
>>>>>>>> via IPv4-only periodically fails to connect to the destination 
>>>>>>>> and then requires a restart to fix it (reload is not sufficient).
>>>>>>>>
>>>>>>>> The problem appears to be that a host that has one address each 
>>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go missing as 
>>>>>>>> a destination. On closer inspection, this appears to happen when 
>>>>>>>> the IPv6 address (not the IPv4) address is marked as bad.
>>>>
>>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>>>> 216.239.38.120 #1/2-0
>>>>
>>>>
>>>> Thank you for sharing more debugging info!
>>>
>>> The following seemed odd to. It finds an IPv4 address (this host does 
>>> not have IPv6), puts it in the cache and then says "No DNS records":
>>>
>>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>> schoolbase.online
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
>>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
>>> 'schoolbase.online'
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
>>> answers for schoolbase.online
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>> 20.54.32.34 in [no cached IPs]
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>> 20.54.32.34 in [no cached IPs]
>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use 
>>> first 69 from RR TTL 69
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
>>> schoolbase.online #1 20.54.32.34
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> 20.54.32.34
>>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector72389 found conn564274 local=0.0.0.0 
>>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for 
>>> schoolbase.online:443
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: 
>>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) 
>>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>> 0x1b7381f38? lookup_err=No DNS records
>>>
>>> It seemed to happen about the same time as the other failure, so 
>>> perhaps another symptom of the same.
>>>
>>>> The above log line is self-contradictory AFAICT: It says that the 
>>>> cache has both IPv6-looking and IPv4-looking address at the same 
>>>> cache position (0) and, judging by the corresponding code, those two 
>>>> IP addresses are equal. This is not possible (for those specific IP 
>>>> address values). The subsequent Squid behavior can be explained by 
>>>> this (unexplained) conflict.
>>>>
>>>> I assume you are running official Squid v6.5 code.
>>>
>>> Yes, compiled from source on NetBSD. I have the patch I refer to here 
>>> applied too:
>>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
>>>
>>>> I can suggest the following two steps for going forward:
>>>>
>>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
>>>
>>> I have just upgraded to 6.6.
>>>
>>>> 2. If the problem is still there, patch the latest Squid v6 to add 
>>>> more debugging in hope to explain what is going on. This may take a 
>>>> few iterations, and it will take me some time to produce the 
>>>> necessary debugging patch.
>>>
>>> Unfortunately, I don't have a test case that will cause the problem 
>>> so I need to run this at a customer's production site that is 
>>> particularly affected by it. Luckily, the problem recurs pretty quickly.
>>>
>>> Here's a run with 6.6 where the number of destinations drops from 2 
>>> to 1 before reverting. Not seen this before - usually once it has 
>>> dropped to 1 (the IPv6 address), it stays there until a restart (and 
>>> this did happen about a minute after this log fragment). Happy to 
>>> test out any debugging patch.
>>>
>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>> forcesafesearch.google.com
>>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>> provided or not known
>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>> 'forcesafesearch.google.com'
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> [2001:4860:4802:32::78]
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300176 found conn2388484 local=[::] 
>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>> #1 for forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> 216.239.38.120
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300176 found conn2388485 local=0.0.0.0 
>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>> 0x12208e038
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector300176 found all 2 destinations for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) 
>>> resolveSelected: ?? always_direct = ALLOWED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) 
>>> resolveSelected: ??? never_direct = DENIED
>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) 
>>> resolveSelected: ??????? timedout = 0
>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>> 216.239.38.120:443 of forcesafesearch.google.com
>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>> provided or not known
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) 
>>> resolveSelected: Find IP destination for: 
>>> forcesafesearch.google.com:443' via forcesafesearch.google.com
>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>> forcesafesearch.google.com
>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>> provided or not known
>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>> 'forcesafesearch.google.com'
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> [2001:4860:4802:32::78]
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300177 found conn2388493 local=[::] 
>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>> #1 for forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>> 216.239.38.120
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>> PeerSelector300177 found conn2388494 local=0.0.0.0 
>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>> always_direct = ALLOWED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>> never_direct = DENIED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>> timedout = 0
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>> 0x12208e038
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) 
>>> resolveSelected: PeerSelector300177 found all 2 destinations for 
>>> forcesafesearch.google.com:443
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) 
>>> resolveSelected: ?? always_direct = ALLOWED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) 
>>> resolveSelected: ??? never_direct = DENIED
>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) 
>>> resolveSelected: ??????? timedout = 0
>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
> 



From squid at borrill.org.uk  Tue Jan 16 15:44:30 2024
From: squid at borrill.org.uk (Stephen Borrill)
Date: Tue, 16 Jan 2024 15:44:30 +0000
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <313179e2-b3ab-4924-8fb8-4b90905a55d6@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
 <313179e2-b3ab-4924-8fb8-4b90905a55d6@borrill.org.uk>
Message-ID: <9e1e97ce-5967-4945-88dd-7a24dcb67540@borrill.org.uk>

On 16/01/2024 14:43, Stephen Borrill wrote:
> On 16/01/2024 14:37, Alex Rousskov wrote:
>> On 2024-01-16 06:01, Stephen Borrill wrote:
>>> The problem is no different with 6.6. Is there any more debugging I 
>>> can provide, Alex?
>>
>> Yes, but I need to give you a patch that adds that (temporary) 
>> debugging first (assuming I fail to reproduce the problem in the lab). 
>> The ball is on my side (unless somebody else steps in). Unfortunately, 
>> I do not have any free time for any of that right now. If you do not 
>> hear from me sooner, please ping me again on or after February 8, 2024.
> 
> Thanks. In the meantime, I have created a local DNS entry for 
> forcesafesearch.google.com that only returns the A record. I think that 
> should work around it (for that site, but not others).

Huh, it appears not to work around it properly. See error of "no DNS 
records" when it has literally just found the address in the cache.

2024/01/16 15:40:06.409 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
forcesafesearch.google.com
2024/01/16 15:40:06.409 kid1| 14,4| ipcache.cc(657) 
ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
'forcesafesearch.google.com'
2024/01/16 15:40:06.409 kid1| 14,7| ipcache.cc(253) forwardIp: 
216.239.38.120
2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1174) handlePath: 
PeerSelector260781 found conn1888968 local=0.0.0.0 
remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #1 for 
forcesafesearch.google.com:443
2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1180) handlePath: 
always_direct = ALLOWED
2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1181) handlePath: 
never_direct = DENIED
2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1182) handlePath: 
    timedout = 0
2024/01/16 15:40:06.409 kid1| 14,7| ipcache.cc(236) finalCallback: 
0x189fb5e38  lookup_err=No DNS records

>>> On 10/01/2024 12:40, Stephen Borrill wrote:
>>>> On 09/01/2024 15:42, Alex Rousskov wrote:
>>>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) connected 
>>>>>>>>> via IPv4-only periodically fails to connect to the destination 
>>>>>>>>> and then requires a restart to fix it (reload is not sufficient).
>>>>>>>>>
>>>>>>>>> The problem appears to be that a host that has one address each 
>>>>>>>>> of IPv4 and IPv6 occasionally has its IPv4 address go missing 
>>>>>>>>> as a destination. On closer inspection, this appears to happen 
>>>>>>>>> when the IPv6 address (not the IPv4) address is marked as bad.
>>>>>
>>>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>>>>> 216.239.38.120 #1/2-0
>>>>>
>>>>>
>>>>> Thank you for sharing more debugging info!
>>>>
>>>> The following seemed odd to. It finds an IPv4 address (this host 
>>>> does not have IPv6), puts it in the cache and then says "No DNS 
>>>> records":
>>>>
>>>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>>> schoolbase.online
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
>>>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
>>>> 'schoolbase.online'
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
>>>> answers for schoolbase.online
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>>> 20.54.32.34 in [no cached IPs]
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>>> 20.54.32.34 in [no cached IPs]
>>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use 
>>>> first 69 from RR TTL 69
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
>>>> schoolbase.online #1 20.54.32.34
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> 20.54.32.34
>>>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector72389 found conn564274 local=0.0.0.0 
>>>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for 
>>>> schoolbase.online:443
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: 
>>>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) 
>>>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>> 0x1b7381f38? lookup_err=No DNS records
>>>>
>>>> It seemed to happen about the same time as the other failure, so 
>>>> perhaps another symptom of the same.
>>>>
>>>>> The above log line is self-contradictory AFAICT: It says that the 
>>>>> cache has both IPv6-looking and IPv4-looking address at the same 
>>>>> cache position (0) and, judging by the corresponding code, those 
>>>>> two IP addresses are equal. This is not possible (for those 
>>>>> specific IP address values). The subsequent Squid behavior can be 
>>>>> explained by this (unexplained) conflict.
>>>>>
>>>>> I assume you are running official Squid v6.5 code.
>>>>
>>>> Yes, compiled from source on NetBSD. I have the patch I refer to 
>>>> here applied too:
>>>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
>>>>
>>>>> I can suggest the following two steps for going forward:
>>>>>
>>>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
>>>>
>>>> I have just upgraded to 6.6.
>>>>
>>>>> 2. If the problem is still there, patch the latest Squid v6 to add 
>>>>> more debugging in hope to explain what is going on. This may take a 
>>>>> few iterations, and it will take me some time to produce the 
>>>>> necessary debugging patch.
>>>>
>>>> Unfortunately, I don't have a test case that will cause the problem 
>>>> so I need to run this at a customer's production site that is 
>>>> particularly affected by it. Luckily, the problem recurs pretty 
>>>> quickly.
>>>>
>>>> Here's a run with 6.6 where the number of destinations drops from 2 
>>>> to 1 before reverting. Not seen this before - usually once it has 
>>>> dropped to 1 (the IPv6 address), it stays there until a restart (and 
>>>> this did happen about a minute after this log fragment). Happy to 
>>>> test out any debugging patch.
>>>>
>>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>>> forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>> provided or not known
>>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>>> 'forcesafesearch.google.com'
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> [2001:4860:4802:32::78]
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300176 found conn2388484 local=[::] 
>>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>>> #1 for forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> 216.239.38.120
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300176 found conn2388485 local=0.0.0.0 
>>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>> 0x12208e038
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) 
>>>> resolveSelected: PeerSelector300176 found all 2 destinations for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) 
>>>> resolveSelected: ?? always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) 
>>>> resolveSelected: ??? never_direct = DENIED
>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) 
>>>> resolveSelected: ??????? timedout = 0
>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
>>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
>>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>> provided or not known
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) 
>>>> resolveSelected: Find IP destination for: 
>>>> forcesafesearch.google.com:443' via forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
>>>> forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>> provided or not known
>>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>>> 'forcesafesearch.google.com'
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> [2001:4860:4802:32::78]
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300177 found conn2388493 local=[::] 
>>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>>> #1 for forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>> 216.239.38.120
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) handlePath: 
>>>> PeerSelector300177 found conn2388494 local=0.0.0.0 
>>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) handlePath: 
>>>> always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) handlePath: 
>>>> never_direct = DENIED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) handlePath: 
>>>> timedout = 0
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>> 0x12208e038
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) 
>>>> resolveSelected: PeerSelector300177 found all 2 destinations for 
>>>> forcesafesearch.google.com:443
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) 
>>>> resolveSelected: ?? always_direct = ALLOWED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) 
>>>> resolveSelected: ??? never_direct = DENIED
>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) 
>>>> resolveSelected: ??????? timedout = 0
>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
>>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
>>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
> 



From rousskov at measurement-factory.com  Tue Jan 16 16:47:54 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jan 2024 11:47:54 -0500
Subject: [squid-users] IPv4 addresses go missing - markAsBad wrong?
In-Reply-To: <9e1e97ce-5967-4945-88dd-7a24dcb67540@borrill.org.uk>
References: <4c66d013-f51b-4116-93b1-338b4e4523ca@borrill.org.uk>
 <399501d1-e42a-4d18-b26d-c044409cf292@measurement-factory.com>
 <82d5d410-3940-4f82-893c-a28653b9d50a@borrill.org.uk>
 <917b9fdc-7998-4d95-873d-35ca19824cff@borrill.org.uk>
 <2f9b9a7a-88d4-4bd0-901b-4898473d74c6@measurement-factory.com>
 <53dcacda-d684-408c-8747-e72f4cddb24b@borrill.org.uk>
 <3636309c-e22f-41ac-8d70-6b019dc7fccb@borrill.org.uk>
 <51f3d57e-0288-4c68-9d78-3751541543bf@measurement-factory.com>
 <313179e2-b3ab-4924-8fb8-4b90905a55d6@borrill.org.uk>
 <9e1e97ce-5967-4945-88dd-7a24dcb67540@borrill.org.uk>
Message-ID: <8573ee4c-e01d-4efc-83d8-6059c082e5ac@measurement-factory.com>

On 2024-01-16 10:44, Stephen Borrill wrote:
> On 16/01/2024 14:43, Stephen Borrill wrote:
>> I have created a local DNS entry for 
>> forcesafesearch.google.com that only returns the A record. I think 
>> that should work around it (for that site, but not others).

> Huh, it appears not to work around it properly. See error of "no DNS 
> records" when it has literally just found the address in the cache.

These level-7 debugging records are meant for developers. The snippet 
below is not as self-contradictory as it may appear to a casual 
observer. It implies that the transaction hit a cached _set_ of DNS 
lookups. That set was previously formed from a usable DNS A response 
record (216.239.38.120) and an empty DNS AAAA response ("No DNS records").

Alex.


> 2024/01/16 15:40:06.409 kid1| 14,4| ipcache.cc(617) nbgethostbyname: 
> forcesafesearch.google.com
> 2024/01/16 15:40:06.409 kid1| 14,4| ipcache.cc(657) 
> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
> 'forcesafesearch.google.com'
> 2024/01/16 15:40:06.409 kid1| 14,7| ipcache.cc(253) forwardIp: 
> 216.239.38.120
> 2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1174) handlePath: 
> PeerSelector260781 found conn1888968 local=0.0.0.0 
> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #1 for 
> forcesafesearch.google.com:443
> 2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1180) handlePath: 
> always_direct = ALLOWED
> 2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1181) handlePath: 
> never_direct = DENIED
> 2024/01/16 15:40:06.409 kid1| 44,2| peer_select.cc(1182) handlePath:    
> timedout = 0
> 2024/01/16 15:40:06.409 kid1| 14,7| ipcache.cc(236) finalCallback: 
> 0x189fb5e38? lookup_err=No DNS records
> 
>>>> On 10/01/2024 12:40, Stephen Borrill wrote:
>>>>> On 09/01/2024 15:42, Alex Rousskov wrote:
>>>>>> On 2024-01-09 05:56, Stephen Borrill wrote:
>>>>>>> On 09/01/2024 09:51, Stephen Borrill wrote:
>>>>>>>> On 09/01/2024 03:41, Alex Rousskov wrote:
>>>>>>>>> On 2024-01-08 08:31, Stephen Borrill wrote:
>>>>>>>>>> I'm trying to determine why squid 6.x (seen with 6.5) 
>>>>>>>>>> connected via IPv4-only periodically fails to connect to the 
>>>>>>>>>> destination and then requires a restart to fix it (reload is 
>>>>>>>>>> not sufficient).
>>>>>>>>>>
>>>>>>>>>> The problem appears to be that a host that has one address 
>>>>>>>>>> each of IPv4 and IPv6 occasionally has its IPv4 address go 
>>>>>>>>>> missing as a destination. On closer inspection, this appears 
>>>>>>>>>> to happen when the IPv6 address (not the IPv4) address is 
>>>>>>>>>> marked as bad.
>>>>>>
>>>>>>> ipcache.cc(990) have: [2001:4860:4802:32::78]:443 at 0 in 
>>>>>>> 216.239.38.120 #1/2-0
>>>>>>
>>>>>>
>>>>>> Thank you for sharing more debugging info!
>>>>>
>>>>> The following seemed odd to. It finds an IPv4 address (this host 
>>>>> does not have IPv6), puts it in the cache and then says "No DNS 
>>>>> records":
>>>>>
>>>>> 2024/01/09 12:31:24.020 kid1| 14,4| ipcache.cc(617) 
>>>>> nbgethostbyname: schoolbase.online
>>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(313) ipcacheRelease: 
>>>>> ipcacheRelease: Releasing entry for 'schoolbase.online'
>>>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(670) 
>>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: MISS for 
>>>>> 'schoolbase.online'
>>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(480) ipcacheParse: 1 
>>>>> answers for schoolbase.online
>>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>>>> 20.54.32.34 in [no cached IPs]
>>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(995) have:? no 
>>>>> 20.54.32.34 in [no cached IPs]
>>>>> 2024/01/09 12:31:24.020 kid1| 14,5| ipcache.cc(549) updateTtl: use 
>>>>> first 69 from RR TTL 69
>>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(535) addGood: 
>>>>> schoolbase.online #1 20.54.32.34
>>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>>> 20.54.32.34
>>>>> 2024/01/09 12:31:24.020 kid1| 44,2| peer_select.cc(1174) 
>>>>> handlePath: PeerSelector72389 found conn564274 local=0.0.0.0 
>>>>> remote=20.54.32.34:443 HIER_DIRECT flags=1, destination #1 for 
>>>>> schoolbase.online:443
>>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(459) latestError: 
>>>>> ERROR: DNS failure while resolving schoolbase.online: No DNS records
>>>>> 2024/01/09 12:31:24.020 kid1| 14,3| ipcache.cc(586) 
>>>>> ipcacheHandleReply: done with schoolbase.online: 20.54.32.34 #1/1-0
>>>>> 2024/01/09 12:31:24.020 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>>> 0x1b7381f38? lookup_err=No DNS records
>>>>>
>>>>> It seemed to happen about the same time as the other failure, so 
>>>>> perhaps another symptom of the same.
>>>>>
>>>>>> The above log line is self-contradictory AFAICT: It says that the 
>>>>>> cache has both IPv6-looking and IPv4-looking address at the same 
>>>>>> cache position (0) and, judging by the corresponding code, those 
>>>>>> two IP addresses are equal. This is not possible (for those 
>>>>>> specific IP address values). The subsequent Squid behavior can be 
>>>>>> explained by this (unexplained) conflict.
>>>>>>
>>>>>> I assume you are running official Squid v6.5 code.
>>>>>
>>>>> Yes, compiled from source on NetBSD. I have the patch I refer to 
>>>>> here applied too:
>>>>> https://lists.squid-cache.org/pipermail/squid-users/2023-November/026279.html
>>>>>
>>>>>> I can suggest the following two steps for going forward:
>>>>>>
>>>>>> 1. Upgrade to the latest Squid v6 in hope that the problem goes away.
>>>>>
>>>>> I have just upgraded to 6.6.
>>>>>
>>>>>> 2. If the problem is still there, patch the latest Squid v6 to add 
>>>>>> more debugging in hope to explain what is going on. This may take 
>>>>>> a few iterations, and it will take me some time to produce the 
>>>>>> necessary debugging patch.
>>>>>
>>>>> Unfortunately, I don't have a test case that will cause the problem 
>>>>> so I need to run this at a customer's production site that is 
>>>>> particularly affected by it. Luckily, the problem recurs pretty 
>>>>> quickly.
>>>>>
>>>>> Here's a run with 6.6 where the number of destinations drops from 2 
>>>>> to 1 before reverting. Not seen this before - usually once it has 
>>>>> dropped to 1 (the IPv6 address), it stays there until a restart 
>>>>> (and this did happen about a minute after this log fragment). Happy 
>>>>> to test out any debugging patch.
>>>>>
>>>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(617) 
>>>>> nbgethostbyname: forcesafesearch.google.com
>>>>> 2024/01/10 11:55:49.849 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>>> provided or not known
>>>>> 2024/01/10 11:55:49.849 kid1| 14,4| ipcache.cc(657) 
>>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>>>> 'forcesafesearch.google.com'
>>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>>> [2001:4860:4802:32::78]
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) 
>>>>> handlePath: PeerSelector300176 found conn2388484 local=[::] 
>>>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>>>> #1 for forcesafesearch.google.com:443
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) 
>>>>> handlePath: always_direct = ALLOWED
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) 
>>>>> handlePath: never_direct = DENIED
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) 
>>>>> handlePath: timedout = 0
>>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>>> 216.239.38.120
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1174) 
>>>>> handlePath: PeerSelector300176 found conn2388485 local=0.0.0.0 
>>>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>>>> forcesafesearch.google.com:443
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1180) 
>>>>> handlePath: always_direct = ALLOWED
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1181) 
>>>>> handlePath: never_direct = DENIED
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(1182) 
>>>>> handlePath: timedout = 0
>>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>>> 0x12208e038
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(479) 
>>>>> resolveSelected: PeerSelector300176 found all 2 destinations for 
>>>>> forcesafesearch.google.com:443
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(480) 
>>>>> resolveSelected: ?? always_direct = ALLOWED
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(481) 
>>>>> resolveSelected: ??? never_direct = DENIED
>>>>> 2024/01/10 11:55:49.849 kid1| 44,2| peer_select.cc(482) 
>>>>> resolveSelected: ??????? timedout = 0
>>>>> 2024/01/10 11:55:49.849 kid1| 14,7| ipcache.cc(990) have: 
>>>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>>>> 2024/01/10 11:55:49.849 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>>>> 2024/01/10 11:55:49.855 kid1| 14,7| ipcache.cc(990) have: 
>>>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>>>> 2024/01/10 11:55:49.855 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>>> provided or not known
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(460) 
>>>>> resolveSelected: Find IP destination for: 
>>>>> forcesafesearch.google.com:443' via forcesafesearch.google.com
>>>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(617) 
>>>>> nbgethostbyname: forcesafesearch.google.com
>>>>> 2024/01/10 11:55:49.877 kid1| 14,3| Address.cc(389) lookupHostIP: 
>>>>> Given Non-IP 'forcesafesearch.google.com': hostname or servname not 
>>>>> provided or not known
>>>>> 2024/01/10 11:55:49.877 kid1| 14,4| ipcache.cc(657) 
>>>>> ipcache_nbgethostbyname_: ipcache_nbgethostbyname: HIT for 
>>>>> 'forcesafesearch.google.com'
>>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>>> [2001:4860:4802:32::78]
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) 
>>>>> handlePath: PeerSelector300177 found conn2388493 local=[::] 
>>>>> remote=[2001:4860:4802:32::78]:443 HIER_DIRECT flags=1, destination 
>>>>> #1 for forcesafesearch.google.com:443
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) 
>>>>> handlePath: always_direct = ALLOWED
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) 
>>>>> handlePath: never_direct = DENIED
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) 
>>>>> handlePath: timedout = 0
>>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(253) forwardIp: 
>>>>> 216.239.38.120
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1174) 
>>>>> handlePath: PeerSelector300177 found conn2388494 local=0.0.0.0 
>>>>> remote=216.239.38.120:443 HIER_DIRECT flags=1, destination #2 for 
>>>>> forcesafesearch.google.com:443
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1180) 
>>>>> handlePath: always_direct = ALLOWED
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1181) 
>>>>> handlePath: never_direct = DENIED
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(1182) 
>>>>> handlePath: timedout = 0
>>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(236) finalCallback: 
>>>>> 0x12208e038
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(479) 
>>>>> resolveSelected: PeerSelector300177 found all 2 destinations for 
>>>>> forcesafesearch.google.com:443
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(480) 
>>>>> resolveSelected: ?? always_direct = ALLOWED
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(481) 
>>>>> resolveSelected: ??? never_direct = DENIED
>>>>> 2024/01/10 11:55:49.877 kid1| 44,2| peer_select.cc(482) 
>>>>> resolveSelected: ??????? timedout = 0
>>>>> 2024/01/10 11:55:49.877 kid1| 14,7| ipcache.cc(990) have: 
>>>>> [2001:4860:4802:32::78]:443 at 0 in [2001:4860:4802:32::78] #2/2-0
>>>>> 2024/01/10 11:55:49.877 kid1| 14,2| ipcache.cc(1031) markAsBad: 
>>>>> [2001:4860:4802:32::78]:443 of forcesafesearch.google.com
>>>>> 2024/01/10 11:55:49.882 kid1| 14,7| ipcache.cc(990) have: 
>>>>> 216.239.38.120:443 at 0 in [2001:4860:4802:32::78] #2/2-1
>>>>> 2024/01/10 11:55:49.882 kid1| 14,2| ipcache.cc(1055) forgetMarking: 
>>>>> 216.239.38.120:443 of forcesafesearch.google.com
>>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> https://lists.squid-cache.org/listinfo/squid-users
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://lists.squid-cache.org/listinfo/squid-users
>>>
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From robin.carlisle at framestore.com  Thu Jan 18 14:53:32 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Thu, 18 Jan 2024 14:53:32 +0000
Subject: [squid-users] offline mode not working for me
Message-ID: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>

Hi, Hoping someone can help me with this issue that I have been struggling
with for days now.   I am setting up squid on an ubuntu PC to forward HTTPS
requests to an API and an s3 bucket under my control on amazon AWS.  The
reason I am setting up the proxy is two-fold...

1) To reduce costs from AWS.
2) To provide content to the client on the ubuntu PC if there is a
networking issue somewhere in between the ubuntu PC and AWS.

Item 1 is going well so far.   Item 2 is not going well.   Setup details ...

*# squid - setup cache folder*
mkdir -p /var/cache/squid
chown -R proxy:proxy  /var/cache/squid

*# ssl - generate key*
apt --yes install squid-openssl libnss3-tools
openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \
  -subj "/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com" \
  -keyout /etc/squid/stuff.pem -out /etc/squid/stuff.pem
chown root:proxy /etc/squid/stuff.pem
chmod 644  /etc/squid/stuff.pem

*# ssl - ssl DB*
mkdir -p /var/lib/squid
rm -rf /var/lib/squid/ssl_db
/usr/lib/squid/security_file_certgen -c -s /var/lib/squid/ssl_db -M 4MB
chown -R proxy:proxy /var/lib/squid/ssl_db

*# /etc/squid/squid.conf :*
acl to_aws dstdomain .amazonaws.com
acl from_local src localhost
http_access allow to_aws
http_access allow from_local
cache allow all
cache_dir ufs /var/cache/squid 1024 16 256
offline_mode on
http_port 3129 ssl-bump cert=/etc/squid/stuff.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/lib/squid/security_file_certgen -s
/var/lib/squid/ssl_db -M 4MB
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
sslproxy_cert_error deny all
cache_store_log stdio:/var/log/squid/store.log
logfile_rotate 0

*# /usr/bin/proxy-test :*
#!/bin/bash
curl --proxy http://localhost:3129 \
  --cacert /etc/squid/stuff.pem \
  -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json" \
  -H "Authorization: token MYTOKEN" \
  -H "Content-Type: application/json" \
  --output "/tmp/stuff.json"



When network connectivity is GOOD, everything works well and I get cache
HITS ...

*# /var/log/squid/access.log*
1705587538.837    238 127.0.0.1 NONE_NONE/200 0 CONNECT
stuff.amazonaws.com:443 - HIER_DIRECT/3.136.246.238 -
1705587538.838      0 127.0.0.1 TCP_MEM_HIT/200 32818 GET
https://stuff.amazonaws.com/api/v1/stuff/stuff.json - HIER_NONE/-
application/json

*# extract from /usr/bin/proxy-test output*
< HTTP/1.1 200 OK
< Date: Thu, 18 Jan 2024 13:38:01 GMT
< Content-Type: application/json
< Content-Length: 32187
< x-amzn-RequestId: 8afba80e-6df7-4d5b-a34b-a70bd9b54380
< Last-Modified: 2024-01-03T11:23:19.000Z
< Access-Control-Allow-Origin: *
< x-amz-apigw-id: RvN1CF2_iYcEokA=
< Cache-Control: max-age=2147483648,public,stale-if-error
< ETag: "53896156c4e8e26933188a092c4e40f1"
< X-Amzn-Trace-Id: Root=1-65a929b9-3bd3285934151c1a2495481a
< Age: 2578
< Warning: 110 squid/5.7 "Response is stale"
< X-Cache: HIT from ubuntu-pc
< X-Cache-Lookup: HIT from ubuntu-pc:3129
< Via: 1.1 ubuntu-pc (squid/5.7)
< Connection: keep-alive


When network connectivity is BAD, I get errors and a cache MISS.   In this
test case I unplugged the ethernet cable from the back on the ubuntu-pc ...

*# /var/log/squid/access.log*
1705588717.420     11 127.0.0.1 NONE_NONE/200 0 CONNECT
stuff.amazonaws.com:443 - HIER_DIRECT/3.135.162.228 -
1705588717.420      0 127.0.0.1 NONE_NONE/503 4087 GET
https://stuff.amazonaws.com/api/v1/stuff/stuff.json - HIER_NONE/- text/html

*# extract from /usr/bin/proxy-test output*
< HTTP/1.1 503 Service Unavailable
< Server: squid/5.7
< Mime-Version: 1.0
< Date: Thu, 18 Jan 2024 14:38:37 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 3692
< X-Squid-Error: ERR_CONNECT_FAIL 101
< Vary: Accept-Language
< Content-Language: en
< X-Cache: MISS from ubuntu-pc
< X-Cache-Lookup: NONE from ubuntu-pc:3129
< Via: 1.1 ubuntu-pc (squid/5.7)
< Connection: close

I have also seen it error in a different way with a 502 but with the same
ultimate result.

My expectation/hope is that squid would return the cached object on any
network failure in between ubuntu-pc and the AWS endpoint - and continue to
return this cached object forever.   Is this something squid can do?   It
would seem that offline_mode should do this?

Hope you can help,

Robin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240118/ac10bf0d/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 18 16:03:22 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jan 2024 11:03:22 -0500
Subject: [squid-users] offline mode not working for me
In-Reply-To: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
References: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
Message-ID: <697ba41e-73cc-440d-a5bb-71338f6b2a14@measurement-factory.com>

On 2024-01-18 09:53, Robin Carlisle wrote:

> My expectation/hope is that squid would return the cached object on
> any network failure in between ubuntu-pc and the AWS endpoint - and
> continue to return this cached object forever.   Is this something
> squid can do? It would seem that offline_mode should do this?

Yes and yes. The reason you are getting errors are not related to cache 
hits or misses. Those errors happen _before_ Squid gets the requested 
resource URL and looks up that resource in Squid cache.

> ssl_bump peek step1
> ssl_bump bump all 

To get that URL (in your configuration), Squid must bump the connection. 
To bump the connection at step2, Squid must contact the origin server. 
When the cable is unplugged, Squid obviously cannot do that: The attempt 
to open a Squid-AWS connection fails.

 > .../200 0 CONNECT stuff.amazonaws.com:443 - HIER_DIRECT
 > .../503 4087 GET https://stuff.amazonaws.com/api/... - HIER_NONE

Squid reports bumping errors to the client using HTTP responses. To do 
that, Squid remembers the error response, bumps the client connection, 
receives GET from the client on that bumped connection, and sends that 
error response to the client. This is why you see both CONNECT/200 and 
GET/503 access.log records. Note that Squid does not check whether the 
received GET request would have been a cache hit in this case -- the 
response to that request has been preordained by the earlier bumping 
failure.


Solution candidates to consider include:

* Stop bumping: https_port 443 cert=/etc/squid/stuff.pem

Configure Squid as (a reverse HTTPS proxy for) the AWS service. Use 
https_port. No SslBump rules/options! The client would think that it is 
sending HTTPS requests directly to the service. Squid will forward 
client requests to the service. If this works (and I do not have enough 
information to know that this will work in your specific environment), 
then you will get a much simpler setup.


* Bump at step1, before Squid contacts AWS: ssl_bump bump all

Bugs notwithstanding, there will be no Squid-AWS connection for cache 
hits. The resulting certificate will not be based on AWS service info, 
but it looks like your client is ignorant enough to ignore related 
certificate problems.


HTH,

Alex.


> Hi, Hoping someone can help me with this issue that I have been 
> struggling with for days now. ? I am setting up squid on an ubuntu PC to 
> forward HTTPS requests to an API and an s3 bucket under my control on 
> amazon AWS.? The reason I am setting up the proxy is two-fold...
> 
> 1) To reduce costs from AWS.
> 2) To provide content to the client on the ubuntu PC if there is a 
> networking issue somewhere in between the ubuntu PC and AWS.
> 
> Item 1 is going well so far. ? Item 2 is not going well. ? Setup details ...
> 
> *# squid - setup cache folder*
> mkdir -p /var/cache/squid
> chown -R proxy:proxy ?/var/cache/squid
> 
> *# ssl - generate key*
> apt --yes install squid-openssl libnss3-tools
> openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \
>  ? -subj "/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com 
> <http://www.example.com>" \
>  ? -keyout /etc/squid/stuff.pem -out /etc/squid/stuff.pem
> chown root:proxy /etc/squid/stuff.pem
> chmod 644 ?/etc/squid/stuff.pem
> 
> *# ssl - ssl DB*
> mkdir -p /var/lib/squid
> rm -rf /var/lib/squid/ssl_db
> /usr/lib/squid/security_file_certgen -c -s /var/lib/squid/ssl_db -M 4MB
> chown -R proxy:proxy /var/lib/squid/ssl_db
> 
> *# /etc/squid/squid.conf :*
> acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
> acl from_local src localhost
> http_access allow to_aws
> http_access allow from_local
> cache allow all
> cache_dir ufs /var/cache/squid 1024 16 256
> offline_mode on
> http_port 3129 ssl-bump cert=/etc/squid/stuff.pem 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/lib/squid/security_file_certgen -s 
> /var/lib/squid/ssl_db -M 4MB
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all
> sslproxy_cert_error deny all
> cache_store_log stdio:/var/log/squid/store.log
> logfile_rotate 0
> 
> *# /usr/bin/proxy-test :*
> #!/bin/bash
> curl --proxy http://localhost:3129 <http://localhost:3129> \
>  ? --cacert /etc/squid/stuff.pem \
>  ? -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json 
> <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>" \
>  ? -H "Authorization: token MYTOKEN" \
>  ? -H "Content-Type: application/json" \
>  ? --output "/tmp/stuff.json"
> 
> 
> 
> When network connectivity is GOOD, everything works well and I get cache 
> HITS ...
> 
> *# /var/log/squid/access.log*
> 1705587538.837 ? ?238 127.0.0.1 NONE_NONE/200 0 CONNECT 
> stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - 
> HIER_DIRECT/3.136.246.238 <http://3.136.246.238> -
> 1705587538.838 ? ? ?0 127.0.0.1 TCP_MEM_HIT/200 32818 GET 
> https://stuff.amazonaws.com/api/v1/stuff/stuff.json 
> <https://stuff.amazonaws.com/api/v1/stuff/stuff.json> - HIER_NONE/- 
> application/json
> 
> *# extract from /usr/bin/proxy-test output*
> < HTTP/1.1 200 OK
> < Date: Thu, 18 Jan 2024 13:38:01 GMT
> < Content-Type: application/json
> < Content-Length: 32187
> < x-amzn-RequestId: 8afba80e-6df7-4d5b-a34b-a70bd9b54380
> < Last-Modified: 2024-01-03T11:23:19.000Z
> < Access-Control-Allow-Origin: *
> < x-amz-apigw-id: RvN1CF2_iYcEokA=
> < Cache-Control: max-age=2147483648,public,stale-if-error
> < ETag: "53896156c4e8e26933188a092c4e40f1"
> < X-Amzn-Trace-Id: Root=1-65a929b9-3bd3285934151c1a2495481a
> < Age: 2578
> < Warning: 110 squid/5.7 "Response is stale"
> < X-Cache: HIT from ubuntu-pc
> < X-Cache-Lookup: HIT from ubuntu-pc:3129
> < Via: 1.1 ubuntu-pc (squid/5.7)
> < Connection: keep-alive
> 
> 
> When network connectivity is BAD, I get errors and a cache MISS. ? In 
> this test case I unplugged the ethernet cable from the back on the 
> ubuntu-pc ...
> 
> *# /var/log/squid/access.log*
> 1705588717.420 ? ? 11 127.0.0.1 NONE_NONE/200 0 CONNECT 
> stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - 
> HIER_DIRECT/3.135.162.228 <http://3.135.162.228> -
> 1705588717.420 ? ? ?0 127.0.0.1 NONE_NONE/503 4087 GET 
> https://stuff.amazonaws.com/api/v1/stuff/stuff.json 
> <https://stuff.amazonaws.com/api/v1/stuff/stuff.json> - HIER_NONE/- 
> text/html
> 
> *# extract from /usr/bin/proxy-test output*
> < HTTP/1.1 503 Service Unavailable
> < Server: squid/5.7
> < Mime-Version: 1.0
> < Date: Thu, 18 Jan 2024 14:38:37 GMT
> < Content-Type: text/html;charset=utf-8
> < Content-Length: 3692
> < X-Squid-Error: ERR_CONNECT_FAIL 101
> < Vary: Accept-Language
> < Content-Language: en
> < X-Cache: MISS from ubuntu-pc
> < X-Cache-Lookup: NONE from ubuntu-pc:3129
> < Via: 1.1 ubuntu-pc (squid/5.7)
> < Connection: close
> 
> I have also seen it error in a different way with a 502 but with the 
> same ultimate result.
> 
> My expectation/hope is that squid would return the cached object on any 
> network failure in between ubuntu-pc and the AWS endpoint - and continue 
> to return this cached object forever.? ?Is this something squid can do? 
>  ? It would seem that offline_mode should do this?
> 
> Hope you can help,
> 
> Robin
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jan 18 16:24:34 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jan 2024 05:24:34 +1300
Subject: [squid-users] offline mode not working for me
In-Reply-To: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
References: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
Message-ID: <e69d9a88-dd31-4421-a63f-158e4e244688@treenet.co.nz>

On 19/01/24 03:53, Robin Carlisle wrote:
> Hi, Hoping someone can help me with this issue that I have been 
> struggling with for days now. ? I am setting up squid on an ubuntu PC to 
> forward HTTPS requests to an API and an s3 bucket under my control on 
> amazon AWS.? The reason I am setting up the proxy is two-fold...
> 
> 1) To reduce costs from AWS.
> 2) To provide content to the client on the ubuntu PC if there is a 
> networking issue somewhere in between the ubuntu PC and AWS.
> 
> Item 1 is going well so far. ? Item 2 is not going well. ? Setup details ...
> 
...

> 
> When network connectivity is BAD, I get errors and a cache MISS. ? In 
> this test case I unplugged the ethernet cable from the back on the 
> ubuntu-pc ...
> 
> *# /var/log/squid/access.log*
> 1705588717.420 ? ? 11 127.0.0.1 NONE_NONE/200 0 CONNECT 
> stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> - 
> HIER_DIRECT/3.135.162.228 <http://3.135.162.228> -
> 1705588717.420 ? ? ?0 127.0.0.1 NONE_NONE/503 4087 GET 
> https://stuff.amazonaws.com/api/v1/stuff/stuff.json 
> <https://stuff.amazonaws.com/api/v1/stuff/stuff.json> - HIER_NONE/- 
> text/html
> 
> *# extract from /usr/bin/proxy-test output*
> < HTTP/1.1 503 Service Unavailable
> < Server: squid/5.7
> < Mime-Version: 1.0
> < Date: Thu, 18 Jan 2024 14:38:37 GMT
> < Content-Type: text/html;charset=utf-8
> < Content-Length: 3692
> < X-Squid-Error: ERR_CONNECT_FAIL 101
> < Vary: Accept-Language
> < Content-Language: en
> < X-Cache: MISS from ubuntu-pc
> < X-Cache-Lookup: NONE from ubuntu-pc:3129
> < Via: 1.1 ubuntu-pc (squid/5.7)
> < Connection: close
> 
> I have also seen it error in a different way with a 502 but with the 
> same ultimate result.
> 
> My expectation/hope is that squid would return the cached object on any 
> network failure in between ubuntu-pc and the AWS endpoint - and continue 
> to return this cached object forever.? ?Is this something squid can do? 
>  ? It would seem that offline_mode should do this?
> 


FYI,  offline_mode is not a guarantee that a URL will always HIT. It is 
simply a form of "greedy" caching - where Squid will take actions to 
ensure that full-size objects are fetched whenever it lacks one, and 
serve things as stale HITs when a) it is not specifically prohibited, 
and b) a refresh/fetch is not working.


The URL you are testing with should meet your expected behaviour due to 
the "Cache-Control: public, stale-of-error" header alone.
   Regardless of offline_mode configuration.


That said, getting a 5xx response when there is an object already in 
cache seems like something is buggy to me.

A high level cache.log will be needed to figure out what is going on 
(see https://wiki.squid-cache.org/SquidFaq/BugReporting#full-debug-output).
Be aware this list does not permit large posts so please provide a link 
to download in your reply not attachment.


Cheers
Amos


From rafael.akchurin at diladele.com  Fri Jan 19 12:27:57 2024
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 19 Jan 2024 12:27:57 +0000
Subject: [squid-users] Ubuntu 22.04 LTS repository for Squid 6.6 (rebuilt
 from sources in Debian unstable)
Message-ID: <AM8PR04MB7745FC737CCA3F32499A29EC8F702@AM8PR04MB7745.eurprd04.prod.outlook.com>

Hello everyone,

Online repository with latest Squid 6.6 (rebuilt from sources in Debian unstable) for Ubuntu 22.04 LTS 64-bit is available at https://squid66.diladele.com/.
Github repo https://github.com/diladele/squid-ubuntu/tree/master/src/ubuntu22 contains all the scripts we used to make this compilation.

Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - https://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add new repo
echo "deb https://squid66.diladele.com/ubuntu/ jammy main" \
    > /etc/apt/sources.list.d/squid66.diladele.com.list

# and install
apt-get update && apt-get install -y \
    squid-common \
    squid-openssl \
    squidclient \
    libecap3 libecap3-dev

This version of Squid will now be part of Web Safety 9.0 coming out in March 2024.  If you have some spare time and are interested in Admin UI for Squid and ICAP web filtering, consider downloading an appliance for VMware ESXi/vSphere<https://packages.diladele.com/websafety-va/9.0/websafety.zip> or Microsoft Hyper-V<https://packages.diladele.com/websafety-va/9.0/websafety-hyperv.zip> or even deploy directly on Microsoft Azure<https://azuremarketplace.microsoft.com/en-us/marketplace/apps/diladele.websafety> and Amazon AWS<https://aws.amazon.com/marketplace/pp/prodview-ixvbzugrltcqq>.

Hope you will find this useful.

Best regards,
Rafael Akchurin
Diladele B.V.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: winmail.dat
Type: application/ms-tnef
Size: 18004 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240119/38c96716/attachment.bin>

From robin.carlisle at framestore.com  Fri Jan 19 13:05:23 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Fri, 19 Jan 2024 13:05:23 +0000
Subject: [squid-users] offline mode not working for me
In-Reply-To: <697ba41e-73cc-440d-a5bb-71338f6b2a14@measurement-factory.com>
References: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
 <697ba41e-73cc-440d-a5bb-71338f6b2a14@measurement-factory.com>
Message-ID: <CANOuv9qeniLNrrj5YM69f3qoCS8Zi_fb14HEpYiUyggTG4cOVQ@mail.gmail.com>

Hi, thanks so much for the detailed response.  I chose to test option 2
from your recommendations as I am new to squid and I do not understand how
to set it up as a reverse proxy anyway.  I made the change to my squid.conf
:


#ssl_bump peek step1

ssl_bump bump step1

ssl_bump bump all


This made it work - which is great news.   My curl requests now are
satisfied by the cache when the pc is offline!


I do have 1 followup question which I think is unrelated, let me know if
etiquette demands I create a new post for this.  When I test using
chromium browser, chromium sends OPTION requests - which I think is
something to do with CORS.   These always cause cache MISS  from squid,.. I
think because the return code is 204...?


1705669236.776    113 ::1 TCP_MISS/204 680 OPTIONS
https://stuff.amazonaws.com/api/v1/stuff/stuff.json - HIER_DIRECT/
3.135.146.17 application/json


I can prevent my chromium instance from making these (pointless?) OPTIONS
calls using the following args, but I would rather not have to do this.


--disable-web-security  --disable-features=IsolateOrigins,site-per-process


Any way I can get squid to cache these calls?


Thanks again and all the best,


Robin





On Thu, 18 Jan 2024 at 16:03, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 2024-01-18 09:53, Robin Carlisle wrote:
>
> > My expectation/hope is that squid would return the cached object on
> > any network failure in between ubuntu-pc and the AWS endpoint - and
> > continue to return this cached object forever.   Is this something
> > squid can do? It would seem that offline_mode should do this?
>
> Yes and yes. The reason you are getting errors are not related to cache
> hits or misses. Those errors happen _before_ Squid gets the requested
> resource URL and looks up that resource in Squid cache.
>
> > ssl_bump peek step1
> > ssl_bump bump all
>
> To get that URL (in your configuration), Squid must bump the connection.
> To bump the connection at step2, Squid must contact the origin server.
> When the cable is unplugged, Squid obviously cannot do that: The attempt
> to open a Squid-AWS connection fails.
>
>  > .../200 0 CONNECT stuff.amazonaws.com:443 - HIER_DIRECT
>  > .../503 4087 GET https://stuff.amazonaws.com/api/... - HIER_NONE
>
> Squid reports bumping errors to the client using HTTP responses. To do
> that, Squid remembers the error response, bumps the client connection,
> receives GET from the client on that bumped connection, and sends that
> error response to the client. This is why you see both CONNECT/200 and
> GET/503 access.log records. Note that Squid does not check whether the
> received GET request would have been a cache hit in this case -- the
> response to that request has been preordained by the earlier bumping
> failure.
>
>
> Solution candidates to consider include:
>
> * Stop bumping: https_port 443 cert=/etc/squid/stuff.pem
>
> Configure Squid as (a reverse HTTPS proxy for) the AWS service. Use
> https_port. No SslBump rules/options! The client would think that it is
> sending HTTPS requests directly to the service. Squid will forward
> client requests to the service. If this works (and I do not have enough
> information to know that this will work in your specific environment),
> then you will get a much simpler setup.
>
>
> * Bump at step1, before Squid contacts AWS: ssl_bump bump all
>
> Bugs notwithstanding, there will be no Squid-AWS connection for cache
> hits. The resulting certificate will not be based on AWS service info,
> but it looks like your client is ignorant enough to ignore related
> certificate problems.
>
>
> HTH,
>
> Alex.
>
>
> > Hi, Hoping someone can help me with this issue that I have been
> > struggling with for days now.   I am setting up squid on an ubuntu PC to
> > forward HTTPS requests to an API and an s3 bucket under my control on
> > amazon AWS.  The reason I am setting up the proxy is two-fold...
> >
> > 1) To reduce costs from AWS.
> > 2) To provide content to the client on the ubuntu PC if there is a
> > networking issue somewhere in between the ubuntu PC and AWS.
> >
> > Item 1 is going well so far.   Item 2 is not going well.   Setup details
> ...
> >
> > *# squid - setup cache folder*
> > mkdir -p /var/cache/squid
> > chown -R proxy:proxy  /var/cache/squid
> >
> > *# ssl - generate key*
> > apt --yes install squid-openssl libnss3-tools
> > openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \
> >    -subj "/C=US/ST=Denial/L=Springfield/O=Dis/CN=www.example.com
> > <http://www.example.com>" \
> >    -keyout /etc/squid/stuff.pem -out /etc/squid/stuff.pem
> > chown root:proxy /etc/squid/stuff.pem
> > chmod 644  /etc/squid/stuff.pem
> >
> > *# ssl - ssl DB*
> > mkdir -p /var/lib/squid
> > rm -rf /var/lib/squid/ssl_db
> > /usr/lib/squid/security_file_certgen -c -s /var/lib/squid/ssl_db -M 4MB
> > chown -R proxy:proxy /var/lib/squid/ssl_db
> >
> > *# /etc/squid/squid.conf :*
> > acl to_aws dstdomain .amazonaws.com <http://amazonaws.com>
> > acl from_local src localhost
> > http_access allow to_aws
> > http_access allow from_local
> > cache allow all
> > cache_dir ufs /var/cache/squid 1024 16 256
> > offline_mode on
> > http_port 3129 ssl-bump cert=/etc/squid/stuff.pem
> > generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> > sslcrtd_program /usr/lib/squid/security_file_certgen -s
> > /var/lib/squid/ssl_db -M 4MB
> > acl step1 at_step SslBump1
> > ssl_bump peek step1
> > ssl_bump bump all
> > sslproxy_cert_error deny all
> > cache_store_log stdio:/var/log/squid/store.log
> > logfile_rotate 0
> >
> > *# /usr/bin/proxy-test :*
> > #!/bin/bash
> > curl --proxy http://localhost:3129 <http://localhost:3129> \
> >    --cacert /etc/squid/stuff.pem \
> >    -v "https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json>" \
> >    -H "Authorization: token MYTOKEN" \
> >    -H "Content-Type: application/json" \
> >    --output "/tmp/stuff.json"
> >
> >
> >
> > When network connectivity is GOOD, everything works well and I get cache
> > HITS ...
> >
> > *# /var/log/squid/access.log*
> > 1705587538.837    238 127.0.0.1 NONE_NONE/200 0 CONNECT
> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> -
> > HIER_DIRECT/3.136.246.238 <http://3.136.246.238> -
> > 1705587538.838      0 127.0.0.1 TCP_MEM_HIT/200 32818 GET
> > https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json> - HIER_NONE/-
> > application/json
> >
> > *# extract from /usr/bin/proxy-test output*
> > < HTTP/1.1 200 OK
> > < Date: Thu, 18 Jan 2024 13:38:01 GMT
> > < Content-Type: application/json
> > < Content-Length: 32187
> > < x-amzn-RequestId: 8afba80e-6df7-4d5b-a34b-a70bd9b54380
> > < Last-Modified: 2024-01-03T11:23:19.000Z
> > < Access-Control-Allow-Origin: *
> > < x-amz-apigw-id: RvN1CF2_iYcEokA=
> > < Cache-Control: max-age=2147483648,public,stale-if-error
> > < ETag: "53896156c4e8e26933188a092c4e40f1"
> > < X-Amzn-Trace-Id: Root=1-65a929b9-3bd3285934151c1a2495481a
> > < Age: 2578
> > < Warning: 110 squid/5.7 "Response is stale"
> > < X-Cache: HIT from ubuntu-pc
> > < X-Cache-Lookup: HIT from ubuntu-pc:3129
> > < Via: 1.1 ubuntu-pc (squid/5.7)
> > < Connection: keep-alive
> >
> >
> > When network connectivity is BAD, I get errors and a cache MISS.   In
> > this test case I unplugged the ethernet cable from the back on the
> > ubuntu-pc ...
> >
> > *# /var/log/squid/access.log*
> > 1705588717.420     11 127.0.0.1 NONE_NONE/200 0 CONNECT
> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> -
> > HIER_DIRECT/3.135.162.228 <http://3.135.162.228> -
> > 1705588717.420      0 127.0.0.1 NONE_NONE/503 4087 GET
> > https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json> - HIER_NONE/-
> > text/html
> >
> > *# extract from /usr/bin/proxy-test output*
> > < HTTP/1.1 503 Service Unavailable
> > < Server: squid/5.7
> > < Mime-Version: 1.0
> > < Date: Thu, 18 Jan 2024 14:38:37 GMT
> > < Content-Type: text/html;charset=utf-8
> > < Content-Length: 3692
> > < X-Squid-Error: ERR_CONNECT_FAIL 101
> > < Vary: Accept-Language
> > < Content-Language: en
> > < X-Cache: MISS from ubuntu-pc
> > < X-Cache-Lookup: NONE from ubuntu-pc:3129
> > < Via: 1.1 ubuntu-pc (squid/5.7)
> > < Connection: close
> >
> > I have also seen it error in a different way with a 502 but with the
> > same ultimate result.
> >
> > My expectation/hope is that squid would return the cached object on any
> > network failure in between ubuntu-pc and the AWS endpoint - and continue
> > to return this cached object forever.   Is this something squid can do?
> >    It would seem that offline_mode should do this?
> >
> > Hope you can help,
> >
> > Robin
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > https://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240119/06298d11/attachment.htm>

From robin.carlisle at framestore.com  Fri Jan 19 13:06:07 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Fri, 19 Jan 2024 13:06:07 +0000
Subject: [squid-users] offline mode not working for me
In-Reply-To: <e69d9a88-dd31-4421-a63f-158e4e244688@treenet.co.nz>
References: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
 <e69d9a88-dd31-4421-a63f-158e4e244688@treenet.co.nz>
Message-ID: <CANOuv9p7s8uY5GKgsSBtGU-6JrGUeyxbBNZ6qok_ufaH7N+1Og@mail.gmail.com>

Thanks for the explanations Amos, much appreciated.

On Thu, 18 Jan 2024 at 16:24, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 19/01/24 03:53, Robin Carlisle wrote:
> > Hi, Hoping someone can help me with this issue that I have been
> > struggling with for days now.   I am setting up squid on an ubuntu PC to
> > forward HTTPS requests to an API and an s3 bucket under my control on
> > amazon AWS.  The reason I am setting up the proxy is two-fold...
> >
> > 1) To reduce costs from AWS.
> > 2) To provide content to the client on the ubuntu PC if there is a
> > networking issue somewhere in between the ubuntu PC and AWS.
> >
> > Item 1 is going well so far.   Item 2 is not going well.   Setup details
> ...
> >
> ...
>
> >
> > When network connectivity is BAD, I get errors and a cache MISS.   In
> > this test case I unplugged the ethernet cable from the back on the
> > ubuntu-pc ...
> >
> > *# /var/log/squid/access.log*
> > 1705588717.420     11 127.0.0.1 NONE_NONE/200 0 CONNECT
> > stuff.amazonaws.com:443 <http://stuff.amazonaws.com:443> -
> > HIER_DIRECT/3.135.162.228 <http://3.135.162.228> -
> > 1705588717.420      0 127.0.0.1 NONE_NONE/503 4087 GET
> > https://stuff.amazonaws.com/api/v1/stuff/stuff.json
> > <https://stuff.amazonaws.com/api/v1/stuff/stuff.json> - HIER_NONE/-
> > text/html
> >
> > *# extract from /usr/bin/proxy-test output*
> > < HTTP/1.1 503 Service Unavailable
> > < Server: squid/5.7
> > < Mime-Version: 1.0
> > < Date: Thu, 18 Jan 2024 14:38:37 GMT
> > < Content-Type: text/html;charset=utf-8
> > < Content-Length: 3692
> > < X-Squid-Error: ERR_CONNECT_FAIL 101
> > < Vary: Accept-Language
> > < Content-Language: en
> > < X-Cache: MISS from ubuntu-pc
> > < X-Cache-Lookup: NONE from ubuntu-pc:3129
> > < Via: 1.1 ubuntu-pc (squid/5.7)
> > < Connection: close
> >
> > I have also seen it error in a different way with a 502 but with the
> > same ultimate result.
> >
> > My expectation/hope is that squid would return the cached object on any
> > network failure in between ubuntu-pc and the AWS endpoint - and continue
> > to return this cached object forever.   Is this something squid can do?
> >    It would seem that offline_mode should do this?
> >
>
>
> FYI,  offline_mode is not a guarantee that a URL will always HIT. It is
> simply a form of "greedy" caching - where Squid will take actions to
> ensure that full-size objects are fetched whenever it lacks one, and
> serve things as stale HITs when a) it is not specifically prohibited,
> and b) a refresh/fetch is not working.
>
>
> The URL you are testing with should meet your expected behaviour due to
> the "Cache-Control: public, stale-of-error" header alone.
>    Regardless of offline_mode configuration.
>
>
> That said, getting a 5xx response when there is an object already in
> cache seems like something is buggy to me.
>
> A high level cache.log will be needed to figure out what is going on
> (see https://wiki.squid-cache.org/SquidFaq/BugReporting#full-debug-output
> ).
> Be aware this list does not permit large posts so please provide a link
> to download in your reply not attachment.
>
>
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240119/a42920d9/attachment.htm>

From s_p_arun at yahoo.com  Fri Jan 19 14:08:58 2024
From: s_p_arun at yahoo.com (Arun Kumar)
Date: Fri, 19 Jan 2024 14:08:58 +0000 (UTC)
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <3b0de845-eff2-485a-9528-9e714f06c0b1@measurement-factory.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
 <929108967.11249363.1704896472206@mail.yahoo.com>
 <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>
 <1781216472.758279.1705069266312@mail.yahoo.com>
 <3b0de845-eff2-485a-9528-9e714f06c0b1@measurement-factory.com>
Message-ID: <5337955.387429.1705673338435@mail.yahoo.com>

 Sorry, due to organization policy not possible to upload the debug logs. Anything to look specifically in the debug logs?Also please suggest if we can tweak the below sslbump configuration, to make the chunked transfer work seamless.
http_port tcpkeepalive=60,30,3 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB tls-cert=<pem file> tls-key=<key file> cipher=... options=NO_TLSv1,... tls_dh=prime256v1:<dhparm.pem>
ssl_bump stare all

PS: Any documentations/video available to understand the bump/stare/peek/splice better? Not understanding much from the squid-cache.org contents.
    On Friday, January 12, 2024 at 02:10:40 PM EST, Alex Rousskov <rousskov at measurement-factory.com> wrote:  
 On 2024-01-12 09:21, Arun Kumar wrote:
> On Wednesday, January 10, 2024 at 11:09:48 AM EST, Alex Rousskov wrote:
> 
> 
> On 2024-01-10 09:21, Arun Kumar wrote:
>? >> i) Retry seems to fetch one chunk of the response and not the complete.
>? >> ii) Enabling sslbump and turning ICAP off, not helping.
>? >> iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)
> 
>? >GCC v7 has insufficient C++17 support. I recommend installing GCC v9 or
> better and then trying with Squid v6.6 or newer.
> 
> Arun: Compiled Squid 6.6 with gcc 11.4 and still seeing the same issue.

Glad you were able to upgrade to Squid v6.6!


>? > FWIW, if the problem persists in Squid v6, sharing debugging logs would
> be the next recommended step.
> 
> Arun: /debug_options ALL,6 /giving too much log. Any particular option 
> we can use to debug this issue?


Please share[^1] a pointer to compressed ALL,9 cache.log collected while 
reproducing the problem with Squid v6.6:

https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

Debugging logs are for developers. Developers can deal with large 
volumes of debugging information. You can use services like DropBox to 
share large compressed logs. Said that, the better you can isolate the 
problem/traffic, the higher are the chances that a developer will (have 
the time to) find the answer to your question in the noisy log.

[^1]: Please feel free to share privately if needed, especially if you 
are using sensitive configuration or transactions.

Alex.


>? > Also want to point out that, squid connects to another non-squid proxy
>? > to reach internet.
>? > cache_peer <proxy_url> parent <port> 0 no-query default
>? >
>? > On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov wrote:
>? >
>? >
>? > On 2024-01-09 11:51, Zhang, Jinshu wrote:
>? >
>? >? > Client got below response headers and body. Masked few details.
>? >
>? > Thank you.
>? >
>? >
>? >? > Retry seems to fetch data remaining.
>? >
>? > I would expect a successful retry to fetch the entire response, not just
>? > the remaining bytes, but perhaps that is what you meant. Thank you for
>? > sharing this info.
>? >
>? >
>? >? > Want to point out that removing sslbump everything is working fine,
>? >? > but we wanted to keep it for ICAP scanning.
>? >
>? > What if you keep SslBump enabled but disable any ICAP analysis
>? > ("icap_enable off")? This test may tell us if the problem is between
>? > Squid and the origin server or Squid and the ICAP service...
>? >
>? >
>? >? > We tried compiling 6.x in Amazon linux, using latest gcc, but facing
>? > similar error -
>? > 
> https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html> <[squid-users] compile error in squid v6.1 <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>>
>? >
>? > What is the "latest gcc" version in your environment? I suspect it is
>? > not the latest GCC version available to folks running Amazon Linux, but
>? > you may need to install some packages to get a more recent GCC version.
>? > Unfortunately, I cannot give specific instructions for Amazon Linux
>? > right now.
>? >
>? >
>? > HTH,
>? >
>? > Alex.
>? >
>? >
>? >? > HTTP/1.1 200 OK
>? >? > Date: Tue, 09 Jan 2024 15:41:33 GMT
>? >? > Server: Apache/mod_perl/2.0.10 Perl
>? >? > Content-Type: application/download
>? >? > X-Cache: MISS from ip-x-y-z
>? >? > Transfer-Encoding: chunked
>? >? > Via: xxx (ICAP)
>? >? > Connection: keep-alive
>? >? >
>? >? > 1000
>? >? > File-Id: xyz.zip
>? >? > Local-Path: x/y/z.txt
>? >? > Content-Size: 2967
>? >? > < binary content >
>? >? >
>? >? >
>? >? > Access log(1st attempt):
>? >? > 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>? > FIRSTUP_PARENT/10.x.y.z -
>? >? > 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST
>? > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> - FIRSTUP_PARENT/10.x.y.z
>? > application/download
>? >? >
>? >? > Retry after 5 mins:
>? >? > 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>? > FIRSTUP_PARENT/10.x.y.z -
>? >? > 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST
>? > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> - FIRSTUP_PARENT/10.x.y.z
>? > application/download
>? >? >
>? >? > Jinshu Zhang
>? >? >
>? >? >
>? >? > Fannie Mae Confidential
>? >? > -----Original Message-----
>? >? > From: squid-users <squid-users-bounces at lists.squid-cache.org 
> <mailto:squid-users-bounces at lists.squid-cache.org>
>? > <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Alex
>? > Rousskov
>? >? > Sent: Tuesday, January 9, 2024 9:53 AM
>? >? > To: squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > <mailto:squid-users at lists.squid-cache.org>
>? >? > Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
>? >? >
>? >? >
>? >? > On 2024-01-09 09:13, Arun Kumar wrote:
>? >? >
>? >? >> I have compiled/installed squid v5.8 in Amazon Linux and 
> configured it
>? >? >> with sslbump option. Squid is used as proxy to get response from 
> https
>? >? >> site. When the https site sends chunked response, it appears that the
>? >? >> first response comes but it get stuck and doesn't receive the full
>? >? >> response. Appreciate any help.
>? >? >? ? There were some recent chunking-related changes in Squid, but none
>? > of them is likely to be responsible for the problems you are describing
>? > unless the origin server response is very special/unusual.
>? >? >
>? >? > Does the client in this test get the HTTP response header? Some HTTP
>? > response body bytes?
>? >? >
>? >? > To triage the problem, I recommend sharing the corresponding
>? > access.log records (at least). Seeing debugging of the problematic
>? > transaction may be very useful (but avoid using production security keys
>? > and other sensitive information in such tests):
>? >? >
>? > 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <Sending Bug Reports to the Squid Team <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>
>? >? >
>? >? > Please note that Squid v5 is not officially supported and has more
>? > known security vulnerabilities than Squid v6. You should be using 
> Squid v6.
>? >? >
>? >? >
>? >? > HTH,
>? >? >
>? >? > Alex.
>? >? >
>? >? > _______________________________________________
>? >? > squid-users mailing list
>? >? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > <mailto:squid-users at lists.squid-cache.org>
>? >? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? > <https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>>
>? >? >
>? >? > _______________________________________________
>? >? > squid-users mailing list
>? >? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > <mailto:squid-users at lists.squid-cache.org>
>? >? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? > <https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>>
>? >
>? > _______________________________________________
>? > squid-users mailing list
>? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org> 
> <mailto:squid-users at lists.squid-cache.org>
> 
>? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? > <squid-users Info Page 
> <https://lists.squid-cache.org/listinfo/squid-users>>
> 
> 
> ??? 
> 
> 
>? ? squid-users Info Page
> 
> <https://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> ??? 
> 
> 
>? ? Sending Bug Reports to the Squid Team
> 
> Squid Web Cache documentation
> 
> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
> 
> 
> ??? 
> 
> 
>? ? [squid-users] compile error in squid v6.1
> 
> <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>
> 
> 

  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240119/93b3a456/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 20 12:17:43 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jan 2024 01:17:43 +1300
Subject: [squid-users] offline mode not working for me
In-Reply-To: <CANOuv9qeniLNrrj5YM69f3qoCS8Zi_fb14HEpYiUyggTG4cOVQ@mail.gmail.com>
References: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
 <697ba41e-73cc-440d-a5bb-71338f6b2a14@measurement-factory.com>
 <CANOuv9qeniLNrrj5YM69f3qoCS8Zi_fb14HEpYiUyggTG4cOVQ@mail.gmail.com>
Message-ID: <ffb2b616-1bef-4b39-9823-e60f76745105@treenet.co.nz>

On 20/01/24 02:05, Robin Carlisle wrote:
> 
> I do have 1 followup question which I think is unrelated, let me know if 
> etiquette?demands I create a new post for this. When I test using 
> chromium?browser, chromium sends OPTION requests- which I think is 
> something to do with CORS.? ?These always cause cache MISS from squid,.. 
> I think because the return code is 204...?
> 

No, the reason is HTTP specification (RFC 9110 section 9.3.7):
    "Responses to the OPTIONS method are not cacheable."

If these actually are CORS (might be several other things also), then 
there is important differences in the response headers per-visitor. 
These cannot be cached, and Squid does not know how to correctly 
generate for those headers. So having Squid auto-respond is not a good idea.


Cheers
Amos


From robin.carlisle at framestore.com  Sun Jan 21 09:54:14 2024
From: robin.carlisle at framestore.com (Robin Carlisle)
Date: Sun, 21 Jan 2024 09:54:14 +0000
Subject: [squid-users] offline mode not working for me
In-Reply-To: <ffb2b616-1bef-4b39-9823-e60f76745105@treenet.co.nz>
References: <CANOuv9pS+wLZ64Cu5OphFi0GRB=TJmwd+fpWMyH6rVR=d2vMiQ@mail.gmail.com>
 <697ba41e-73cc-440d-a5bb-71338f6b2a14@measurement-factory.com>
 <CANOuv9qeniLNrrj5YM69f3qoCS8Zi_fb14HEpYiUyggTG4cOVQ@mail.gmail.com>
 <ffb2b616-1bef-4b39-9823-e60f76745105@treenet.co.nz>
Message-ID: <CANOuv9qWjB18-EcVQAmoj+4fNdnKq_d=y1wz8hOgRfOuN3TgGQ@mail.gmail.com>

Ah OK - understood.  Thanks for the explanation.

Robin

On Sat, 20 Jan 2024 at 12:17, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/01/24 02:05, Robin Carlisle wrote:
> >
> > I do have 1 followup question which I think is unrelated, let me know if
> > etiquette demands I create a new post for this. When I test using
> > chromium browser, chromium sends OPTION requests- which I think is
> > something to do with CORS.   These always cause cache MISS from squid,..
> > I think because the return code is 204...?
> >
>
> No, the reason is HTTP specification (RFC 9110 section 9.3.7):
>     "Responses to the OPTIONS method are not cacheable."
>
> If these actually are CORS (might be several other things also), then
> there is important differences in the response headers per-visitor.
> These cannot be cached, and Squid does not know how to correctly
> generate for those headers. So having Squid auto-respond is not a good
> idea.
>
>
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240121/cf101939/attachment.htm>

From alexcoomans at gmail.com  Mon Jan 22 21:28:09 2024
From: alexcoomans at gmail.com (Alex Coomans)
Date: Mon, 22 Jan 2024 13:28:09 -0800
Subject: [squid-users] CONNECT Response Headers
Message-ID: <CABY02KA63m+OKNEzZRO534ERtzb9Jh1wG_q3w-pXKnhThuWnsw@mail.gmail.com>

Hey y'all,

I'd like to be able to set headers on the response sent to a CONNECT
request, but the documentation notes reply_header_add does not work for
that - is there another option or a way to achieve this without needing to
MITM the TLS?

Thanks,
Alex
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240122/b14e110d/attachment.htm>

From s_p_arun at yahoo.com  Tue Jan 23 15:58:57 2024
From: s_p_arun at yahoo.com (Arun Kumar)
Date: Tue, 23 Jan 2024 15:58:57 +0000 (UTC)
Subject: [squid-users] chunked transfer over sslbump
In-Reply-To: <5337955.387429.1705673338435@mail.yahoo.com>
References: <1868843187.239667.1704809583107.ref@mail.yahoo.com>
 <1868843187.239667.1704809583107@mail.yahoo.com>
 <20f4cd69-32f3-428e-81ed-638d3572e0fc@measurement-factory.com>
 <DM6PR06MB44255DE559D846418218AB77F16A2@DM6PR06MB4425.namprd06.prod.outlook.com>
 <be7385fb-efee-499a-b9ee-f3e78a60edbd@measurement-factory.com>
 <929108967.11249363.1704896472206@mail.yahoo.com>
 <df95af22-f03e-48e9-a9a8-9588df436395@measurement-factory.com>
 <1781216472.758279.1705069266312@mail.yahoo.com>
 <3b0de845-eff2-485a-9528-9e714f06c0b1@measurement-factory.com>
 <5337955.387429.1705673338435@mail.yahoo.com>
Message-ID: <1735873709.2107679.1706025537809@mail.yahoo.com>

Appreciate if you can provide any insights.


Sent from Yahoo Mail for iPhone


On Friday, January 19, 2024, 9:08 AM, Arun Kumar <s_p_arun at yahoo.com> wrote:

 Sorry, due to organization policy not possible to upload the debug logs. Anything to look specifically in the debug logs?Also please suggest if we can tweak the below sslbump configuration, to make the chunked transfer work seamless.
http_port tcpkeepalive=60,30,3 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB tls-cert=<pem file> tls-key=<key file> cipher=... options=NO_TLSv1,... tls_dh=prime256v1:<dhparm.pem>
ssl_bump stare all

PS: Any documentations/video available to understand the bump/stare/peek/splice better? Not understanding much from the squid-cache.org contents.
    On Friday, January 12, 2024 at 02:10:40 PM EST, Alex Rousskov <rousskov at measurement-factory.com> wrote:  
 On 2024-01-12 09:21, Arun Kumar wrote:
> On Wednesday, January 10, 2024 at 11:09:48 AM EST, Alex Rousskov wrote:
> 
> 
> On 2024-01-10 09:21, Arun Kumar wrote:
>? >> i) Retry seems to fetch one chunk of the response and not the complete.
>? >> ii) Enabling sslbump and turning ICAP off, not helping.
>? >> iii)? gcc version is 7.3.1 (Red Hat 7.3.1-17)
> 
>? >GCC v7 has insufficient C++17 support. I recommend installing GCC v9 or
> better and then trying with Squid v6.6 or newer.
> 
> Arun: Compiled Squid 6.6 with gcc 11.4 and still seeing the same issue.

Glad you were able to upgrade to Squid v6.6!


>? > FWIW, if the problem persists in Squid v6, sharing debugging logs would
> be the next recommended step.
> 
> Arun: /debug_options ALL,6 /giving too much log. Any particular option 
> we can use to debug this issue?


Please share[^1] a pointer to compressed ALL,9 cache.log collected while 
reproducing the problem with Squid v6.6:

https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction

Debugging logs are for developers. Developers can deal with large 
volumes of debugging information. You can use services like DropBox to 
share large compressed logs. Said that, the better you can isolate the 
problem/traffic, the higher are the chances that a developer will (have 
the time to) find the answer to your question in the noisy log.

[^1]: Please feel free to share privately if needed, especially if you 
are using sensitive configuration or transactions.

Alex.


>? > Also want to point out that, squid connects to another non-squid proxy
>? > to reach internet.
>? > cache_peer <proxy_url> parent <port> 0 no-query default
>? >
>? > On Tuesday, January 9, 2024 at 02:18:14 PM EST, Alex Rousskov wrote:
>? >
>? >
>? > On 2024-01-09 11:51, Zhang, Jinshu wrote:
>? >
>? >? > Client got below response headers and body. Masked few details.
>? >
>? > Thank you.
>? >
>? >
>? >? > Retry seems to fetch data remaining.
>? >
>? > I would expect a successful retry to fetch the entire response, not just
>? > the remaining bytes, but perhaps that is what you meant. Thank you for
>? > sharing this info.
>? >
>? >
>? >? > Want to point out that removing sslbump everything is working fine,
>? >? > but we wanted to keep it for ICAP scanning.
>? >
>? > What if you keep SslBump enabled but disable any ICAP analysis
>? > ("icap_enable off")? This test may tell us if the problem is between
>? > Squid and the origin server or Squid and the ICAP service...
>? >
>? >
>? >? > We tried compiling 6.x in Amazon linux, using latest gcc, but facing
>? > similar error -
>? > 
> https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html> <[squid-users] compile error in squid v6.1 <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>>
>? >
>? > What is the "latest gcc" version in your environment? I suspect it is
>? > not the latest GCC version available to folks running Amazon Linux, but
>? > you may need to install some packages to get a more recent GCC version.
>? > Unfortunately, I cannot give specific instructions for Amazon Linux
>? > right now.
>? >
>? >
>? > HTH,
>? >
>? > Alex.
>? >
>? >
>? >? > HTTP/1.1 200 OK
>? >? > Date: Tue, 09 Jan 2024 15:41:33 GMT
>? >? > Server: Apache/mod_perl/2.0.10 Perl
>? >? > Content-Type: application/download
>? >? > X-Cache: MISS from ip-x-y-z
>? >? > Transfer-Encoding: chunked
>? >? > Via: xxx (ICAP)
>? >? > Connection: keep-alive
>? >? >
>? >? > 1000
>? >? > File-Id: xyz.zip
>? >? > Local-Path: x/y/z.txt
>? >? > Content-Size: 2967
>? >? > < binary content >
>? >? >
>? >? >
>? >? > Access log(1st attempt):
>? >? > 1704814893.695? ? 138 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>? > FIRSTUP_PARENT/10.x.y.z -
>? >? > 1704814900.491? 6779 172.17.0.2 TCP_MISS/200 138996535 POST
>? > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> - FIRSTUP_PARENT/10.x.y.z
>? > application/download
>? >? >
>? >? > Retry after 5 mins:
>? >? > 1704815201.530? ? 189 x.y.0.2 NONE_NONE/200 0 CONNECT a.b.com:443 -
>? > FIRSTUP_PARENT/10.x.y.z -
>? >? > 1704815208.438? 6896 x.y.0.2 TCP_MISS/200 138967930 POST
>? > https://a.b.com/xyz <https://a.b.com/xyz> <https://a.b.com/xyz 
> <https://a.b.com/xyz>> - FIRSTUP_PARENT/10.x.y.z
>? > application/download
>? >? >
>? >? > Jinshu Zhang
>? >? >
>? >? >
>? >? > Fannie Mae Confidential
>? >? > -----Original Message-----
>? >? > From: squid-users <squid-users-bounces at lists.squid-cache.org 
> <mailto:squid-users-bounces at lists.squid-cache.org>
>? > <mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Alex
>? > Rousskov
>? >? > Sent: Tuesday, January 9, 2024 9:53 AM
>? >? > To: squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > <mailto:squid-users at lists.squid-cache.org>
>? >? > Subject: [EXTERNAL] Re: [squid-users] chunked transfer over sslbump
>? >? >
>? >? >
>? >? > On 2024-01-09 09:13, Arun Kumar wrote:
>? >? >
>? >? >> I have compiled/installed squid v5.8 in Amazon Linux and 
> configured it
>? >? >> with sslbump option. Squid is used as proxy to get response from 
> https
>? >? >> site. When the https site sends chunked response, it appears that the
>? >? >> first response comes but it get stuck and doesn't receive the full
>? >? >> response. Appreciate any help.
>? >? >? ? There were some recent chunking-related changes in Squid, but none
>? > of them is likely to be responsible for the problems you are describing
>? > unless the origin server response is very special/unusual.
>? >? >
>? >? > Does the client in this test get the HTTP response header? Some HTTP
>? > response body bytes?
>? >? >
>? >? > To triage the problem, I recommend sharing the corresponding
>? > access.log records (at least). Seeing debugging of the problematic
>? > transaction may be very useful (but avoid using production security keys
>? > and other sensitive information in such tests):
>? >? >
>? > 
> https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction> <Sending Bug Reports to the Squid Team <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>>
>? >? >
>? >? > Please note that Squid v5 is not officially supported and has more
>? > known security vulnerabilities than Squid v6. You should be using 
> Squid v6.
>? >? >
>? >? >
>? >? > HTH,
>? >? >
>? >? > Alex.
>? >? >
>? >? > _______________________________________________
>? >? > squid-users mailing list
>? >? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > <mailto:squid-users at lists.squid-cache.org>
>? >? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? > <https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>>
>? >? >
>? >? > _______________________________________________
>? >? > squid-users mailing list
>? >? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org>
>? > <mailto:squid-users at lists.squid-cache.org>
>? >? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? > <https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>>
>? >
>? > _______________________________________________
>? > squid-users mailing list
>? > squid-users at lists.squid-cache.org 
> <mailto:squid-users at lists.squid-cache.org> 
> <mailto:squid-users at lists.squid-cache.org>
> 
>? > https://lists.squid-cache.org/listinfo/squid-users 
> <https://lists.squid-cache.org/listinfo/squid-users>
>? > <squid-users Info Page 
> <https://lists.squid-cache.org/listinfo/squid-users>>
> 
> 
> ??? 
> 
> 
>? ? squid-users Info Page
> 
> <https://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> ??? 
> 
> 
>? ? Sending Bug Reports to the Squid Team
> 
> Squid Web Cache documentation
> 
> <https://wiki.squid-cache.org/SquidFaq/BugReporting#debugging-a-single-transaction>
> 
> 
> 
> ??? 
> 
> 
>? ? [squid-users] compile error in squid v6.1
> 
> <https://lists.squid-cache.org/pipermail/squid-users/2023-July/026016.html>
> 
> 

  


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240123/b9840c4f/attachment.htm>

From andre.bolinhas at articatech.com  Mon Jan 29 12:05:41 2024
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Mon, 29 Jan 2024 12:05:41 +0000
Subject: [squid-users] Squid - Your cache is running out of filedescriptors
Message-ID: <80e9c751-3279-4296-850d-e56dcbedaddc@articatech.com>

Hi

I'm running Squid 5.9, and sometimes I have an issue that cause a proxy 
malfunction.
I have set the max_filedescriptors to 159514 and it's works fine for a 
few hours or day, but then suddenly, Squid run out of filedescriptors.
2024/01/29 10:33:47 kid3| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:33:53 kid2| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:33:55 kid5| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:33:57 kid1| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:33:58 kid4| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:03 kid3| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:09 kid2| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:11 kid5| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:13 kid1| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:14 kid4| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:19 kid3| WARNING! Your cache is running out of 
filedescriptors
2024/01/29 10:34:25 kid2| WARNING! Your cache is running out of 
filedescriptors
 ??? listening port: MyPortNameID1

Checking mgr:info inside squidclient I can see that squid only set 4096 
filedescriptors per kid instead 15951.

Can you help me to understand why Squid losses filedescriptors and don't 
start with the correct filedescriptors values set in configuration file?

Best regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240129/d4669e39/attachment.htm>

From andre.bolinhas at articatech.com  Mon Jan 29 12:09:41 2024
From: andre.bolinhas at articatech.com (Andre Bolinhas)
Date: Mon, 29 Jan 2024 12:09:41 +0000
Subject: [squid-users] Squid - Queue overflow
Message-ID: <53662869-3e81-43b4-a61d-d0be33cbc33f@articatech.com>

Hi

I'm getting this error in cache.log

2024/01/29 14:33:03 kid5| ERROR: Collapsed forwarding queue overflow for 
kid1 at 1024 items
 ??? current master transaction: master2163155
2024/01/29 14:33:03 kid5| ERROR: Collapsed forwarding queue overflow for 
kid2 at 1024 items
 ??? current master transaction: master2163155
2024/01/29 14:33:03 kid5| ERROR: Collapsed forwarding queue overflow for 
kid3 at 1024 items
 ??? current master transaction: master2163155
2024/01/29 14:33:03 kid5| ERROR: Collapsed forwarding queue overflow for 
kid4 at 1024 items
 ??? current master transaction: master2163155
2024/01/29 14:33:03 kid5| ERROR: Collapsed forwarding queue overflow for 
kid5 at 1024 items
 ??? current master transaction: master2163155

This leads Squid stops filtering or check any of the ACL rules, allowing 
users to navigate to all websites without any kind of filtering or control.

Can you help to understand and correct this issue please.

Best regatds
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240129/51c042ac/attachment.htm>

From stu.lists at spacehopper.org  Mon Jan 29 12:18:09 2024
From: stu.lists at spacehopper.org (Stuart Henderson)
Date: Mon, 29 Jan 2024 12:18:09 -0000 (UTC)
Subject: [squid-users] Squid - Your cache is running out of
 filedescriptors
References: <80e9c751-3279-4296-850d-e56dcbedaddc@articatech.com>
Message-ID: <slrnurf5s1.2ivf.stu.lists@naiad.spacehopper.org>

On 2024-01-29, Andre Bolinhas <andre.bolinhas at articatech.com> wrote:
> I'm running Squid 5.9, and sometimes I have an issue that cause a proxy 
> malfunction.
> I have set the max_filedescriptors to 159514 and it's works fine for a 
> few hours or day, but then suddenly, Squid run out of filedescriptors.
...
> Checking mgr:info inside squidclient I can see that squid only set 4096 
> filedescriptors per kid instead 15951.
>
> Can you help me to understand why Squid losses filedescriptors and don't 
> start with the correct filedescriptors values set in configuration file?

You may be running into OS limits - either kernel limits, or per-process
limits. For example, on BSDs you would often need to adjust /etc/login.conf,
and possibly also a sysctl.




From amajer at suse.de  Mon Jan 29 12:50:30 2024
From: amajer at suse.de (Adam Majer)
Date: Mon, 29 Jan 2024 13:50:30 +0100
Subject: [squid-users] Security advisories are not accessible
Message-ID: <5ac93a8b-a735-441d-9a72-1f6df7090436@suse.de>

Hi,

http://www.squid-cache.org/Versions/v6/ lists security advisories link as

    https://github.com/squid-cache/squid/security/advisories

But going there "There aren?t any published security advisories". There 
are links to individual patches.

Thanks,
- Adam


From squid3 at treenet.co.nz  Mon Jan 29 13:18:17 2024
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jan 2024 02:18:17 +1300
Subject: [squid-users] Security advisories are not accessible
In-Reply-To: <5ac93a8b-a735-441d-9a72-1f6df7090436@suse.de>
References: <5ac93a8b-a735-441d-9a72-1f6df7090436@suse.de>
Message-ID: <2ddccdb5-a6fe-4782-b067-946683fe7845@treenet.co.nz>

Thanks for the notice.

This appears to be a github issue that has been occuring to many other 
projects for at least 5hrs now. For now we can only hope that it gets 
resolved soon


Cheers
Amos

On 30/01/24 01:50, Adam Majer wrote:
> Hi,
> 
> http://www.squid-cache.org/Versions/v6/ lists security advisories link as
> 
>  ?? https://github.com/squid-cache/squid/security/advisories
> 
> But going there "There aren?t any published security advisories". There 
> are links to individual patches.
> 
> Thanks,
> - Adam
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From tommy.brunn at klarna.com  Mon Jan 29 13:48:06 2024
From: tommy.brunn at klarna.com (Tommy Brunn)
Date: Mon, 29 Jan 2024 14:48:06 +0100
Subject: [squid-users] 6.x gives frequent connection to peer failed -
 spurious?
In-Reply-To: <ace4b905-44ef-47d5-98cf-f1c5714fb5be@borrill.org.uk>
References: <3cf29e5a-a84b-4ff1-91b9-66c6782891b1@borrill.org.uk>
 <fc40c8a1-e988-40c9-82db-bd70426f6c99@measurement-factory.com>
 <1fd61adc-2b97-4cdc-92a4-f8a0a7c575d2@borrill.org.uk>
 <802c46b3-69a3-40b2-9ccd-f1bedb57961a@measurement-factory.com>
 <ace4b905-44ef-47d5-98cf-f1c5714fb5be@borrill.org.uk>
Message-ID: <CAARumMnQZEFAAsxzzsXuf7Of4L+60dGQigrz04aEkenj-u4sSg@mail.gmail.com>

Thank you for sharing your patch to work around this issue. We deploy
Squid in a very similar configuration to what you describe, and we
recently had a problem where a destination was down, causing the peer
squid to be marked as dead even though the peer Squid was just fine
and it was just the upstream destination that was unavailable. The
proposed `cache_peer_fault` directive sounds like it would be a
perfect fit for our case, but in the absence of such a directive we'll
be applying a similar patch to what you shared.

On Wed, Nov 22, 2023 at 11:15?AM Stephen Borrill <squid at borrill.org.uk> wrote:
>
> On 21/11/2023 15:55, Alex Rousskov wrote:
> > On 2023-11-21 08:38, Stephen Borrill wrote:
> >> On 15/11/2023 21:55, Alex Rousskov wrote:
> >>> On 2023-11-10 05:46, Stephen Borrill wrote:
> >>>
> >>>> With 6.x (currently 6.5) there are very frequent (every 10 seconds
> >>>> or so) messages like:
> >>>> 2023/11/10 10:25:43 kid1| ERROR: Connection to 127.0.0.1:8123 failed
> >>>
> >>>
> >>>> why is this logged as a connection failure
> >>>
> >>> The current error wording is too assuming and, in your case,
> >>> evidently misleading. The phrase "Connection to X failed" should be
> >>> changed to something more general like "Cannot contact cache_peer X"
> >>> or "Cannot communicate with cache_peer X".
> >>>
> >>> CachePeer::countFailure() patches welcome.
> >
> >> But the point is that it _can_ communicate with the peer, but the peer
> >> itself can't service the request. The peer returning 503 shouldn't be
> >> logged as a connection failure
> >
> >
> > My bad. I missed the fact that the described DNS error happens at a
> > _peer_ Squid. Sorry.
> >
> >
> > Currently, Squid v6 treats most CONNECT-to-peer errors as a sign of a
> > broken peer. In 2022, 4xx errors were excluded from that set[1]. At that
> > time, we also proposed to make that decision configurable using a new
> > cache_peer_fault directive[2], but the new directive was blocked as an
> > "overkill"[3], so we hard-coded 4xx exclusion instead.
> >
> > Going forward, you have several options, including these two:
> >
> > 1. Convince others that Squid should treat all 503 CONNECT errors from
> > peers as it already treats all 4xx errors. Hard-code that new logic.
> >
> > 2. Convince others that cache_peer_fault or a similar directive is a
> > good idea rather than an overkill. Resurrect its implementation[2].
> >
> >
> > [1]
> > https://github.com/squid-cache/squid/commit/022dbabd89249f839d1861aa87c1ab9e1a008a47
> >
> > [2]
> > https://github.com/squid-cache/squid/commit/25431f18f2f5e796b8704c85fc51f93b6cc2a73d
> >
> > [3] https://github.com/squid-cache/squid/pull/1166#issuecomment-1295806530
>
>
> 2) seems sensible. Especially in the case where you have a single
> cache_peer and cannot go direct. No benefit to marking it as dead.
>
> However, I'm currently running with 1) as per below and this stops the
> non-existent domains counting against the peer (which surely opens it to
> a DoS attack):
>
> --- src/CachePeer.cc.orig       2023-11-22 08:30:17.524266325 +0000
> +++ src/CachePeer.cc    2023-11-22 08:31:05.394052184 +0000
> @@ -71,7 +71,7 @@
>   void
>   CachePeer::noteFailure(const Http::StatusCode code)
>   {
> -    if (Http::Is4xx(code))
> +    if (Http::Is4xx(code) || code == Http::scServiceUnavailable)
>           return; // this failure is not our fault
>
>       countFailure();
>
>
> >>>  > do I need to worry about it beyond it filing up the logs needlessly?
> >>>
> >>> In short, "yes".
> >>>
> >>> I cannot accurately assess your specific needs, but, in most
> >>> environments, one should indeed worry that their cache_peer server
> >>> names cannot be reliably resolved because failed resolution attempts
> >>> waste Squid resources and increase transaction response time.
> >>> Moreover, if these failures are frequent enough (relative to peer
> >>> usage attempts), the affected cache_peer will be marked as DEAD (as
> >>> you have mentioned):
> >>>
> >>>  > 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
> >>
> >> Problem seems to be easily reproducible:
> >>
> >> 1# env https_proxy=http://127.0.0.1:8084 curl https://www.invalid.domain/
> >> curl: (56) CONNECT tunnel failed, response 503
> >> 2# grep invalid /usr/local/squid/logs/access.log|tail -1
> >> 1700573429.015      4 127.0.0.1:8084 TCP_TUNNEL/503 0 CONNECT
> >> www.invalid.domain:443 - FIRSTUP_PARENT/127.0.0.1:8123 -
> >> 3# date -r 1700573429 '+%Y/%m/%d %H:%M:%S'
> >> 2023/11/21 13:30:29
> >> 4# grep '2023/11/21 13:30:29' /usr/local/squid/logs/cache.log
> >> 2023/11/21 13:30:29 kid1| ERROR: Connection to 127.0.0.1:8123 failed
> >>
> >>>> With 4.x there were no such messages.
> >>>>
> >>>> By comparing to the peer squid logs, these seems to tally with DNS
> >>>> failures:
> >>>> peer_select.cc(479) resolveSelected: PeerSelector1688 found all 0
> >>>> destinations for bugzilla.tucasi.com:443
> >>>>
> >>>> Full ALL,2 log at the time of the reported connection failure:
> >>>>
> >>>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(214) doAccept: New
> >>>> connection on FD 17
> >>>> 2023/11/10 10:25:43.162 kid1| 5,2| TcpAcceptor.cc(316) acceptNext:
> >>>> connection on conn3 local=127.0.0.1:8123 remote=[::] FD 17 flags=9
> >>>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1332)
> >>>> parseHttpRequest: HTTP Client conn13206 local=127.0.0.1:8123
> >>>> remote=127.0.0.1:57843 FD 147 flags=1
> >>>> 2023/11/10 10:25:43.162 kid1| 11,2| client_side.cc(1336)
> >>>> parseHttpRequest: HTTP Client REQUEST:
> >>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707)
> >>>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443
> >>>> is ALLOWED; last ACL checked: localhost
> >>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(683)
> >>>> clientAccessCheck2: No adapted_http_access configuration. default:
> >>>> ALLOW
> >>>> 2023/11/10 10:25:43.162 kid1| 85,2| client_side_request.cc(707)
> >>>> clientAccessCheckDone: The request CONNECT bugzilla.tucasi.com:443
> >>>> is ALLOWED; last ACL checked: localhost
> >>>> 2023/11/10 10:25:43.162 kid1| 44,2| peer_select.cc(460)
> >>>> resolveSelected: Find IP destination for: bugzilla.tucasi.com:443'
> >>>> via bugzilla.tucasi.com
> >>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(479)
> >>>> resolveSelected: PeerSelector1526 found all 0 destinations for
> >>>> bugzilla.tucasi.com:443
> >>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(480)
> >>>> resolveSelected:    always_direct = ALLOWED
> >>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(481)
> >>>> resolveSelected:     never_direct = DENIED
> >>>> 2023/11/10 10:25:43.163 kid1| 44,2| peer_select.cc(482)
> >>>> resolveSelected:         timedout = 0
> >>>> 2023/11/10 10:25:43.163 kid1| 4,2| errorpage.cc(1397) buildBody: No
> >>>> existing error page language negotiated for ERR_DNS_FAIL. Using
> >>>> default error file.
> >>>> 2023/11/10 10:25:43.163 kid1| 33,2| client_side.cc(617) swanSong:
> >>>> conn13206 local=127.0.0.1:8123 remote=127.0.0.1:57843 flags=1
> >>>>
> >>>> If my analysis is correct why is this logged as a connection failure
> >>>> and do I need to worry about it beyond it filing up the logs
> >>>> needlessly?
> >>>>
> >>>> My concern is that this could lead to the parent being incorrectly
> >>>> declared DEAD thus impacting other traffic:
> >>>>
> >>>> 2023/11/09 08:55:22 kid1| Detected DEAD Parent: 127.0.0.1:8123
> >>>>      current master transaction: master4581234
> >>>>
> >>>
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Mon Jan 29 14:31:06 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Jan 2024 09:31:06 -0500
Subject: [squid-users] CONNECT Response Headers
In-Reply-To: <CABY02KA63m+OKNEzZRO534ERtzb9Jh1wG_q3w-pXKnhThuWnsw@mail.gmail.com>
References: <CABY02KA63m+OKNEzZRO534ERtzb9Jh1wG_q3w-pXKnhThuWnsw@mail.gmail.com>
Message-ID: <39c7215f-619b-465d-a487-6d70ff2832b6@measurement-factory.com>

On 2024-01-22 16:28, Alex Coomans wrote:

> I'd like to be able to set headers on the response sent to a CONNECT 
> request, but the documentation notes reply_header_add does not work for 
> that - is there another option or a way to achieve this without needing 
> to MITM?the TLS?

AFAICT, Squid does not have code that can customize headers of a regular 
200 (Connection established) response to a CONNECT requests (with or 
without MitM). This functionality is rarely needed (because most clients 
tend to ignore most CONNECT response headers), but quality pull requests 
adding that functionality are welcomed.


HTH,

Alex.



From adm.snd at gmail.com  Mon Jan 29 19:31:42 2024
From: adm.snd at gmail.com (=?UTF-8?Q?SND_Tecnologia_=2D_Administra=C3=A7=C3=A3o?=)
Date: Mon, 29 Jan 2024 16:31:42 -0300
Subject: [squid-users] FATAL: getpwnam failed to find userid for effective
 user 'squid'
Message-ID: <CAOX8345_MCn_YH2vgf3j1sp6MFsSCG38Wt-wdZ1eL_OCq9BB3A@mail.gmail.com>

Hey guys

I'm a newbie.
Sorry if this is a common problem.
Please, provide me the a way to resolve.

I'm getting the following message when starting squid:


root at fwhcr:/etc/squid# systemctl stop squid.service
root at fwhcr:/etc/squid# systemctl start squid.service
Job for squid.service failed because the control process exited with error
code.
See "systemctl status squid.service" and "journalctl -xeu squid.service"
for details.
root at fwhcr:/etc/squid# systemctl status squid.service
? squid.service - Squid Web Proxy Server
    Loaded: loaded (/lib/systemd/system/squid.service; enabled; preset:
enabled)
    Active: failed (Result: exit-code) since Mon 2024-01-29 16:27:10 -03;
4s ago
  Duration: 2h 59min 59.188s
      Docs: man:squid(8)
   Process: 525742 ExecStartPre=/usr/sbin/squid --foreground -z (code=exited,
status=1/FAILURE)
       CPU: 22ms

jan 29 16:27:10 fwhcr systemd[1]: Starting squid.service - Squid Web Proxy
Server...
jan 29 16:27:10 fwhcr squid[525742]: 2024/01/29 16:27:10| FATAL: getpwnam
failed to find userid for effective user 'squid'
jan 29 16:27:10 fwhcr squid[525742]: 2024/01/29 16:27:10| Squid Cache
(Version 5.7): Terminated abnormally.
jan 29 16:27:10 fwhcr squid[525742]: CPU Usage: 0.021 seconds = 0.015 user
+ 0.005 sys
jan 29 16:27:10 fwhcr squid[525742]: Maximum Resident Size: 40496 KB
jan 29 16:27:10 fwhcr squid[525742]: Page faults with physical i/o: 0
jan 29 16:27:10 fwhcr squid[525742]: FATAL: getpwnam failed to find userid
for effective user 'squid'
jan 29 16:27:10 fwhcr systemd[1]: squid.service: Control process exited,
code=exited, status=1/FAILURE
jan 29 16:27:10 fwhcr systemd[1]: squid.service: Failed with result
'exit-code'.
jan 29 16:27:10 fwhcr systemd[1]: Failed to start squid.service - Squid Web
Proxy Server.


SQUID.CONF
LINE 39:

access_log daemon:/var/log/squid/access.log squid
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240129/eea43c40/attachment.htm>

From uhlar at fantomas.sk  Tue Jan 30 10:36:14 2024
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 30 Jan 2024 11:36:14 +0100
Subject: [squid-users] FATAL: getpwnam failed to find userid for
 effective user 'squid'
In-Reply-To: <CAOX8345_MCn_YH2vgf3j1sp6MFsSCG38Wt-wdZ1eL_OCq9BB3A@mail.gmail.com>
References: <CAOX8345_MCn_YH2vgf3j1sp6MFsSCG38Wt-wdZ1eL_OCq9BB3A@mail.gmail.com>
Message-ID: <ZbjRHr8b1jL3JRXa@fantomas.sk>

On 29.01.24 16:31, SND Tecnologia - Administra??o wrote:
>I'm getting the following message when starting squid:
[...]
>root at fwhcr:/etc/squid# systemctl status squid.service
[...]
>jan 29 16:27:10 fwhcr systemd[1]: Starting squid.service - Squid Web Proxy
>Server...
>jan 29 16:27:10 fwhcr squid[525742]: 2024/01/29 16:27:10| FATAL: getpwnam
>failed to find userid for effective user 'squid'

looks like your config file sets up squid to run under 'squid' user either 
in your squid config (cache_effective_user directive) or as default.

and user 'squid' does not exist on your system

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Quantum mechanics: The dreams stuff is made of.


From david at articatech.com  Tue Jan 30 15:49:30 2024
From: david at articatech.com (David Touzeau)
Date: Tue, 30 Jan 2024 16:49:30 +0100
Subject: [squid-users] Long Group TAG in access.log when using kerberos
Message-ID: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>


Hi when using Kerberos with Squid when in access log a long Group tags:

I would like to know how to disable Squid to grab groups suring 
authentication verification and in other way, how to decode Group value

example of an access.log

|1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443 CONNECT 
eu-mobile.events.data.microsoft.com:443 leblud 
HIER_DIRECT/13.69.239.72:443 - mac="00:00:00:00:00:00" 
user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A 
ua="-" exterr="-|-"|

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240130/f14a5fd1/attachment.htm>

From adm.snd at gmail.com  Tue Jan 30 18:32:23 2024
From: adm.snd at gmail.com (=?UTF-8?Q?SND_Tecnologia_=2D_Administra=C3=A7=C3=A3o?=)
Date: Tue, 30 Jan 2024 15:32:23 -0300
Subject: [squid-users] FATAL: getpwnam failed to find userid for
 effective user 'squid'
In-Reply-To: <ZbjRHr8b1jL3JRXa@fantomas.sk>
References: <CAOX8345_MCn_YH2vgf3j1sp6MFsSCG38Wt-wdZ1eL_OCq9BB3A@mail.gmail.com>
 <ZbjRHr8b1jL3JRXa@fantomas.sk>
Message-ID: <CAOX8344u-jYCCTKzK4MKWibzc4V5XdMsbnssYrBHgaYUb611AA@mail.gmail.com>

Thank you very much...

My friend insisted on putting the directive twice in the script and when I
went to help I only changed it in one place


Em ter., 30 de jan. de 2024 ?s 07:36, Matus UHLAR - fantomas <
uhlar at fantomas.sk> escreveu:

> On 29.01.24 16:31, SND Tecnologia - Administra??o wrote:
> >I'm getting the following message when starting squid:
> [...]
> >root at fwhcr:/etc/squid# systemctl status squid.service
> [...]
> >jan 29 16:27:10 fwhcr systemd[1]: Starting squid.service - Squid Web Proxy
> >Server...
> >jan 29 16:27:10 fwhcr squid[525742]: 2024/01/29 16:27:10| FATAL: getpwnam
> >failed to find userid for effective user 'squid'
>
> looks like your config file sets up squid to run under 'squid' user either
> in your squid config (cache_effective_user directive) or as default.
>
> and user 'squid' does not exist on your system
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Quantum mechanics: The dreams stuff is made of.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240130/322026dd/attachment.htm>

From ankor2023 at gmail.com  Wed Jan 31 09:01:39 2024
From: ankor2023 at gmail.com (Andrey K)
Date: Wed, 31 Jan 2024 12:01:39 +0300
Subject: [squid-users] Long Group TAG in access.log when using kerberos
In-Reply-To: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
References: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
Message-ID: <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>

Hello, David,

group values in your logs are BASE64-encoded binary AD-groups SIDs.
You can try to decode them by a simple perl script sid-reader.pl (see
below):

echo  AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==  | base64 -d | perl
sid-reader.pl

And finally convert SID to a group name:
wbinfo -s S-01-5-21-407062282-1694779757-312552118-71814

Kind regards,
      Ankor


*sid-reader.pl <http://sid-reader.pl>:*
#!/usr/bin/perl
#https://lists.samba.org/archive/linux/2005-September/014301.html

my $binary_sid;
my @parts;
while(<>){
  push @parts, $_;
}
  $binary_sid = join('', @parts);

  my($sid_rev, $num_auths, $id1, $id2, @ids) =
                unpack("H2 H2 n N V*", $binary_sid);
  my $sid_string = join("-", "S", $sid_rev, ($id1<<32)+$id2, @ids);
  print "$sid_string\n";


??, 30 ???. 2024??. ? 18:49, David Touzeau <david at articatech.com>:

>
> Hi when using Kerberos with Squid when in access log a long Group tags:
>
> I would like to know how to disable Squid to grab groups suring
> authentication verification and in other way, how to decode Group value
>
> example of an access.log
>
> 1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443 CONNECT
> eu-mobile.events.data.microsoft.com:443 leblud HIER_DIRECT/
> 13.69.239.72:443 - mac="00:00:00:00:00:00"
> user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A
> ua="-" exterr="-|-"
>
> --
> David Touzeau - Artica Tech France
> Development team, level 3 support
> ----------------------------------
> P: +33 6 58 44 69 46
> www: https://wiki.articatech.com
> www: http://articatech.net
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240131/1bd503a5/attachment.htm>

From david at articatech.com  Wed Jan 31 12:51:06 2024
From: david at articatech.com (David Touzeau)
Date: Wed, 31 Jan 2024 13:51:06 +0100
Subject: [squid-users] Long Group TAG in access.log when using kerberos
In-Reply-To: <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>
References: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
 <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>
Message-ID: <1686b64e-6481-4e48-9cc0-d6f1839a3979@articatech.com>

Anyway to remove these entries from the log ?

Le 31/01/2024 ? 10:01, Andrey K a ?crit?:
> Hello, David,
>
> group values in your logs are BASE64-encoded binary AD-groups SIDs.
> You can try to decode them by a simple perl script sid-reader.pl 
> <http://sid-reader.pl> (see below):
>
> echo AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==? | base64 -d | perl 
> sid-reader.pl <http://sid-reader.pl>
>
> And finally convert SID to a group name:
> wbinfo -s S-01-5-21-407062282-1694779757-312552118-71814
>
> Kind?regards,
> ? ? ? Ankor
>
>
> *sid-reader.pl <http://sid-reader.pl>:*
> #!/usr/bin/perl
> #https://lists.samba.org/archive/linux/2005-September/014301.html
>
> my $binary_sid;
> my @parts;
> while(<>){
> ? push @parts, $_;
> }
> ? $binary_sid = join('', @parts);
>
> ? my($sid_rev, $num_auths, $id1, $id2, @ids) =
> ? ? ? ? ? ? ? ? unpack("H2 H2 n N V*", $binary_sid);
> ? my $sid_string = join("-", "S", $sid_rev, ($id1<<32)+$id2, @ids);
> ? print "$sid_string\n";
>
>
> ??, 30 ???. 2024??. ? 18:49, David Touzeau <david at articatech.com>:
>
>
>     Hi when using Kerberos with Squid when in access log a long Group
>     tags:
>
>     I would like to know how to disable Squid to grab groups suring
>     authentication verification and in other way, how to decode Group
>     value
>
>     example of an access.log
>
>     |1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443 CONNECT
>     eu-mobile.events.data.microsoft.com:443
>     <http://eu-mobile.events.data.microsoft.com:443> leblud
>     HIER_DIRECT/13.69.239.72:443 <http://13.69.239.72:443> -
>     mac="00:00:00:00:00:00"
>     user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A
>     ua="-" exterr="-|-"|
>
>     -- 
>     David Touzeau - Artica Tech France
>     Development team, level 3 support
>     ----------------------------------
>     P: +33 6 58 44 69 46
>     www:https://wiki.articatech.com
>     www:http://articatech.net  
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     https://lists.squid-cache.org/listinfo/squid-users
>

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240131/c8d68755/attachment.htm>

From ankor2023 at gmail.com  Wed Jan 31 13:36:29 2024
From: ankor2023 at gmail.com (Andrey K)
Date: Wed, 31 Jan 2024 16:36:29 +0300
Subject: [squid-users] Long Group TAG in access.log when using kerberos
In-Reply-To: <1686b64e-6481-4e48-9cc0-d6f1839a3979@articatech.com>
References: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
 <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>
 <1686b64e-6481-4e48-9cc0-d6f1839a3979@articatech.com>
Message-ID: <CADJd0Y0b_07i6sXUU6AcF4b2LB_AjaF1ksMbt0=fJ2jXv9BCjQ@mail.gmail.com>

Hello, David,

> Anyway to remove these entries from the log ?
I think you should correct logformat directive in your squid configuration
to disable annotations logging (%note):
http://www.squid-cache.org/Doc/config/logformat/

Kind regards,
      Ankor.





??, 31 ???. 2024??. ? 15:51, David Touzeau <david at articatech.com>:

> Anyway to remove these entries from the log ?
>
> Le 31/01/2024 ? 10:01, Andrey K a ?crit :
>
> Hello, David,
>
> group values in your logs are BASE64-encoded binary AD-groups SIDs.
> You can try to decode them by a simple perl script sid-reader.pl (see
> below):
>
> echo  AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==  | base64 -d | perl
> sid-reader.pl
>
> And finally convert SID to a group name:
> wbinfo -s S-01-5-21-407062282-1694779757-312552118-71814
>
> Kind regards,
>       Ankor
>
>
> *sid-reader.pl <http://sid-reader.pl>:*
> #!/usr/bin/perl
> #https://lists.samba.org/archive/linux/2005-September/014301.html
>
> my $binary_sid;
> my @parts;
> while(<>){
>   push @parts, $_;
> }
>   $binary_sid = join('', @parts);
>
>   my($sid_rev, $num_auths, $id1, $id2, @ids) =
>                 unpack("H2 H2 n N V*", $binary_sid);
>   my $sid_string = join("-", "S", $sid_rev, ($id1<<32)+$id2, @ids);
>   print "$sid_string\n";
>
>
> ??, 30 ???. 2024??. ? 18:49, David Touzeau <david at articatech.com>:
>
>>
>> Hi when using Kerberos with Squid when in access log a long Group tags:
>>
>> I would like to know how to disable Squid to grab groups suring
>> authentication verification and in other way, how to decode Group value
>>
>> example of an access.log
>>
>> 1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443 CONNECT
>> eu-mobile.events.data.microsoft.com:443 leblud HIER_DIRECT/
>> 13.69.239.72:443 - mac="00:00:00:00:00:00"
>> user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A
>> ua="-" exterr="-|-"
>>
>> --
>> David Touzeau - Artica Tech France
>> Development team, level 3 support
>> ----------------------------------
>> P: +33 6 58 44 69 46
>> www: https://wiki.articatech.com
>> www: http://articatech.net
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>
> --
> David Touzeau - Artica Tech France
> Development team, level 3 support
> ----------------------------------
> P: +33 6 58 44 69 46
> www: https://wiki.articatech.com
> www: http://articatech.net
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240131/a2dd5aa1/attachment.htm>

From david at articatech.com  Wed Jan 31 14:23:29 2024
From: david at articatech.com (David Touzeau)
Date: Wed, 31 Jan 2024 15:23:29 +0100
Subject: [squid-users] Long Group TAG in access.log when using kerberos
In-Reply-To: <CADJd0Y0b_07i6sXUU6AcF4b2LB_AjaF1ksMbt0=fJ2jXv9BCjQ@mail.gmail.com>
References: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
 <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>
 <1686b64e-6481-4e48-9cc0-d6f1839a3979@articatech.com>
 <CADJd0Y0b_07i6sXUU6AcF4b2LB_AjaF1ksMbt0=fJ2jXv9BCjQ@mail.gmail.com>
Message-ID: <80cfb8d3-0149-4526-bbe6-29c46eb35bda@articatech.com>





Hi %note is used by our external_acls and for log other tokens
And we use also Group as token.
it can disabled by direcly removing source kerberos code before 
compiling but i would like to know if there is another way

Le 31/01/2024 ? 14:36, Andrey K a ?crit?:
> Hello, David,
>
> > Anyway to remove these entries from the log ?
> I think you should correct logformat directive in your squid 
> configuration to disable annotations logging (%note): 
> http://www.squid-cache.org/Doc/config/logformat/
>
> Kind regards,
> ? ? ? Ankor.
>
>
>
>
>
> ??, 31 ???. 2024??. ? 15:51, David Touzeau <david at articatech.com>:
>
>     Anyway to remove these entries from the log ?
>
>     Le 31/01/2024 ? 10:01, Andrey K a ?crit?:
>>     Hello, David,
>>
>>     group values in your logs are BASE64-encoded binary AD-groups SIDs.
>>     You can try to decode them by a simple perl script sid-reader.pl
>>     <http://sid-reader.pl> (see below):
>>
>>     echo AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==? | base64 -d | perl
>>     sid-reader.pl <http://sid-reader.pl>
>>
>>     And finally convert SID to a group name:
>>     wbinfo -s S-01-5-21-407062282-1694779757-312552118-71814
>>
>>     Kind?regards,
>>     ? ? ? Ankor
>>
>>
>>     *sid-reader.pl <http://sid-reader.pl>:*
>>     #!/usr/bin/perl
>>     #https://lists.samba.org/archive/linux/2005-September/014301.html
>>
>>     my $binary_sid;
>>     my @parts;
>>     while(<>){
>>     ? push @parts, $_;
>>     }
>>     ? $binary_sid = join('', @parts);
>>
>>     ? my($sid_rev, $num_auths, $id1, $id2, @ids) =
>>     ? ? ? ? ? ? ? ? unpack("H2 H2 n N V*", $binary_sid);
>>     ? my $sid_string = join("-", "S", $sid_rev, ($id1<<32)+$id2, @ids);
>>     ? print "$sid_string\n";
>>
>>
>>     ??, 30 ???. 2024??. ? 18:49, David Touzeau <david at articatech.com>:
>>
>>
>>         Hi when using Kerberos with Squid when in access log a long
>>         Group tags:
>>
>>         I would like to know how to disable Squid to grab groups
>>         suring authentication verification and in other way, how to
>>         decode Group value
>>
>>         example of an access.log
>>
>>         |1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443
>>         CONNECT eu-mobile.events.data.microsoft.com:443
>>         <http://eu-mobile.events.data.microsoft.com:443> leblud
>>         HIER_DIRECT/13.69.239.72:443 <http://13.69.239.72:443> -
>>         mac="00:00:00:00:00:00"
>>         user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A
>>         ua="-" exterr="-|-"|
>>
>>         -- 
>>         David Touzeau - Artica Tech France
>>         Development team, level 3 support
>>         ----------------------------------
>>         P: +33 6 58 44 69 46
>>         www:https://wiki.articatech.com
>>         www:http://articatech.net  
>>
>>         _______________________________________________
>>         squid-users mailing list
>>         squid-users at lists.squid-cache.org
>>         https://lists.squid-cache.org/listinfo/squid-users
>>
>
>     -- 
>     David Touzeau - Artica Tech France
>     Development team, level 3 support
>     ----------------------------------
>     P: +33 6 58 44 69 46
>     www:https://wiki.articatech.com
>     www:http://articatech.net  
>

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240131/3e152915/attachment.htm>

From rousskov at measurement-factory.com  Wed Jan 31 16:43:18 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 Jan 2024 11:43:18 -0500
Subject: [squid-users] Long Group TAG in access.log when using kerberos
In-Reply-To: <80cfb8d3-0149-4526-bbe6-29c46eb35bda@articatech.com>
References: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
 <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>
 <1686b64e-6481-4e48-9cc0-d6f1839a3979@articatech.com>
 <CADJd0Y0b_07i6sXUU6AcF4b2LB_AjaF1ksMbt0=fJ2jXv9BCjQ@mail.gmail.com>
 <80cfb8d3-0149-4526-bbe6-29c46eb35bda@articatech.com>
Message-ID: <ed5c55f2-0be1-4087-a6fd-175eea58a570@measurement-factory.com>

On 2024-01-31 09:23, David Touzeau wrote:

> Hi %note is used by our external_acls and for log other tokens
> And we use also Group as token.
> it can disabled by direcly removing source kerberos code before 
> compiling but i would like to know if there is another way

In most cases, one does not have to (and does not really want to) log 
_all_ transaction annotations. It is possible to specify annotations 
that should be logged by using the annotation name as a %note parameter.

For example, to just log annotation named foo, use %note{foo} instead of 
%note.

In many cases, folks that log multiple annotations, prepend the 
annotation name so that it is easier (especially for humans) to extract 
the right annotation from the access log record:

     ... foo=%note{foo} bar=%note{bar} ...


HTH,

Alex.


> Le 31/01/2024 ? 14:36, Andrey K a ?crit?:
>> Hello, David,
>>
>> > Anyway to remove these entries from the log ?
>> I think you should correct logformat directive in your squid 
>> configuration to disable annotations logging (%note): 
>> http://www.squid-cache.org/Doc/config/logformat/
>>
>> Kind regards,
>> ? ? ? Ankor.
>>
>>
>>
>>
>>
>> ??, 31 ???. 2024??. ? 15:51, David Touzeau <david at articatech.com>:
>>
>>     Anyway to remove these entries from the log ?
>>
>>     Le 31/01/2024 ? 10:01, Andrey K a ?crit?:
>>>     Hello, David,
>>>
>>>     group values in your logs are BASE64-encoded binary AD-groups SIDs.
>>>     You can try to decode them by a simple perl script sid-reader.pl
>>>     <http://sid-reader.pl> (see below):
>>>
>>>     echo AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==? | base64 -d | perl
>>>     sid-reader.pl <http://sid-reader.pl>
>>>
>>>     And finally convert SID to a group name:
>>>     wbinfo -s S-01-5-21-407062282-1694779757-312552118-71814
>>>
>>>     Kind?regards,
>>>     ? ? ? Ankor
>>>
>>>
>>>     *sid-reader.pl <http://sid-reader.pl>:*
>>>     #!/usr/bin/perl
>>>     #https://lists.samba.org/archive/linux/2005-September/014301.html
>>>
>>>     my $binary_sid;
>>>     my @parts;
>>>     while(<>){
>>>     ? push @parts, $_;
>>>     }
>>>     ? $binary_sid = join('', @parts);
>>>
>>>     ? my($sid_rev, $num_auths, $id1, $id2, @ids) =
>>>     ? ? ? ? ? ? ? ? unpack("H2 H2 n N V*", $binary_sid);
>>>     ? my $sid_string = join("-", "S", $sid_rev, ($id1<<32)+$id2, @ids);
>>>     ? print "$sid_string\n";
>>>
>>>
>>>     ??, 30 ???. 2024??. ? 18:49, David Touzeau <david at articatech.com>:
>>>
>>>
>>>         Hi when using Kerberos with Squid when in access log a long
>>>         Group tags:
>>>
>>>         I would like to know how to disable Squid to grab groups
>>>         suring authentication verification and in other way, how to
>>>         decode Group value
>>>
>>>         example of an access.log
>>>
>>>         |1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443
>>>         CONNECT eu-mobile.events.data.microsoft.com:443
>>>         <http://eu-mobile.events.data.microsoft.com:443> leblud
>>>         HIER_DIRECT/13.69.239.72:443 <http://13.69.239.72:443> -
>>>         mac="00:00:00:00:00:00"
>>>         user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A ua="-" exterr="-|-"|
>>>
>>>         -- 
>>>         David Touzeau - Artica Tech France
>>>         Development team, level 3 support
>>>         ----------------------------------
>>>         P: +33 6 58 44 69 46
>>>         www:https://wiki.articatech.com
>>>         www:http://articatech.net  
>>>
>>>         _______________________________________________
>>>         squid-users mailing list
>>>         squid-users at lists.squid-cache.org
>>>         https://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>>     -- 
>>     David Touzeau - Artica Tech France
>>     Development team, level 3 support
>>     ----------------------------------
>>     P: +33 6 58 44 69 46
>>     www:https://wiki.articatech.com
>>     www:http://articatech.net  
>>
> 
> -- 
> David Touzeau - Artica Tech France
> Development team, level 3 support
> ----------------------------------
> P: +33 6 58 44 69 46
> www:https://wiki.articatech.com
> www:http://articatech.net  
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Wed Jan 31 18:15:27 2024
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 Jan 2024 13:15:27 -0500
Subject: [squid-users] Squid - Queue overflow
In-Reply-To: <53662869-3e81-43b4-a61d-d0be33cbc33f@articatech.com>
References: <53662869-3e81-43b4-a61d-d0be33cbc33f@articatech.com>
Message-ID: <3275f0aa-69ce-400f-919d-917a2f10a5c9@measurement-factory.com>

On 2024-01-29 07:09, Andre Bolinhas wrote:

> I'm getting this error in cache.log
> 
> 2024/01/29 14:33:03 kid5| ERROR: Collapsed forwarding queue overflow for 
> kid1 at 1024 items
>  ??? current master transaction: master2163155

> This leads Squid stops filtering or check any of the ACL rules, allowing 
> users to navigate to all websites without any kind of filtering or control.

FWIW, I am surprised by such a side effect. Are you sure that it is this 
particular ERROR that leads to access controls bypass? Are there any 
other alarming messages in cache.log?


> Can you help to understand and correct this issue please.

My recommendation is to upgrade to Squid v6 and address "Your cache is 
running out of filedescriptors" WARNINGs that you have reported in 
another squid-users thread. Once your Squid descriptor resources match 
the traffic Squid receives, this ERROR may disappear. If the ERROR is 
still there after those two changes, it may be easier to triage it in a 
cleaner environment.


HTH,

Alex.



From david at articatech.com  Wed Jan 31 19:47:09 2024
From: david at articatech.com (David Touzeau)
Date: Wed, 31 Jan 2024 20:47:09 +0100
Subject: [squid-users] Long Group TAG in access.log when using kerberos
In-Reply-To: <ed5c55f2-0be1-4087-a6fd-175eea58a570@measurement-factory.com>
References: <ff6bda73-2228-42a5-b856-2b458028c787@articatech.com>
 <CADJd0Y0MBhVfxkasjS_+KxsKPvFVXOqUd2iJt=VC15EQ_pgFZg@mail.gmail.com>
 <1686b64e-6481-4e48-9cc0-d6f1839a3979@articatech.com>
 <CADJd0Y0b_07i6sXUU6AcF4b2LB_AjaF1ksMbt0=fJ2jXv9BCjQ@mail.gmail.com>
 <80cfb8d3-0149-4526-bbe6-29c46eb35bda@articatech.com>
 <ed5c55f2-0be1-4087-a6fd-175eea58a570@measurement-factory.com>
Message-ID: <3bcbccb9-1e73-443d-90a3-896603d8e33a@articatech.com>

Thank Alex

This will fix the issue!

Le 31/01/2024 ? 17:43, Alex Rousskov a ?crit?:
> On 2024-01-31 09:23, David Touzeau wrote:
>
>> Hi %note is used by our external_acls and for log other tokens
>> And we use also Group as token.
>> it can disabled by direcly removing source kerberos code before 
>> compiling but i would like to know if there is another way
>
> In most cases, one does not have to (and does not really want to) log 
> _all_ transaction annotations. It is possible to specify annotations 
> that should be logged by using the annotation name as a %note parameter.
>
> For example, to just log annotation named foo, use %note{foo} instead 
> of %note.
>
> In many cases, folks that log multiple annotations, prepend the 
> annotation name so that it is easier (especially for humans) to 
> extract the right annotation from the access log record:
>
> ??? ... foo=%note{foo} bar=%note{bar} ...
>
>
> HTH,
>
> Alex.
>
>
>> Le 31/01/2024 ? 14:36, Andrey K a ?crit?:
>>> Hello, David,
>>>
>>> > Anyway to remove these entries from the log ?
>>> I think you should correct logformat directive in your squid 
>>> configuration to disable annotations logging (%note): 
>>> http://www.squid-cache.org/Doc/config/logformat/
>>>
>>> Kind regards,
>>> ? ? ? Ankor.
>>>
>>>
>>>
>>>
>>>
>>> ??, 31 ???. 2024??. ? 15:51, David Touzeau <david at articatech.com>:
>>>
>>> ??? Anyway to remove these entries from the log ?
>>>
>>> ??? Le 31/01/2024 ? 10:01, Andrey K a ?crit?:
>>>> ??? Hello, David,
>>>>
>>>> ??? group values in your logs are BASE64-encoded binary AD-groups 
>>>> SIDs.
>>>> ??? You can try to decode them by a simple perl script sid-reader.pl
>>>> <http://sid-reader.pl> (see below):
>>>>
>>>> ??? echo AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==? | base64 -d | perl
>>>> ??? sid-reader.pl <http://sid-reader.pl>
>>>>
>>>> ??? And finally convert SID to a group name:
>>>> ??? wbinfo -s S-01-5-21-407062282-1694779757-312552118-71814
>>>>
>>>> ??? Kind?regards,
>>>> ??? ? ? ? Ankor
>>>>
>>>>
>>>> ??? *sid-reader.pl <http://sid-reader.pl>:*
>>>> ??? #!/usr/bin/perl
>>>> #https://lists.samba.org/archive/linux/2005-September/014301.html
>>>>
>>>> ??? my $binary_sid;
>>>> ??? my @parts;
>>>> ??? while(<>){
>>>> ??? ? push @parts, $_;
>>>> ??? }
>>>> ??? ? $binary_sid = join('', @parts);
>>>>
>>>> ??? ? my($sid_rev, $num_auths, $id1, $id2, @ids) =
>>>> ??? ? ? ? ? ? ? ? ? unpack("H2 H2 n N V*", $binary_sid);
>>>> ??? ? my $sid_string = join("-", "S", $sid_rev, ($id1<<32)+$id2, 
>>>> @ids);
>>>> ??? ? print "$sid_string\n";
>>>>
>>>>
>>>> ??? ??, 30 ???. 2024??. ? 18:49, David Touzeau <david at articatech.com>:
>>>>
>>>>
>>>> ??????? Hi when using Kerberos with Squid when in access log a long
>>>> ??????? Group tags:
>>>>
>>>> ??????? I would like to know how to disable Squid to grab groups
>>>> ??????? suring authentication verification and in other way, how to
>>>> ??????? decode Group value
>>>>
>>>> ??????? example of an access.log
>>>>
>>>> ??????? |1706629424.779 130984 10.1.12.120 TCP_TUNNEL/500 5443
>>>> ??????? CONNECT eu-mobile.events.data.microsoft.com:443
>>>> <http://eu-mobile.events.data.microsoft.com:443> leblud
>>>> ??????? HIER_DIRECT/13.69.239.72:443 <http://13.69.239.72:443> -
>>>> ??????? mac="00:00:00:00:00:00"
>>>> user:%20leblud%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBsMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESBaAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESj34AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQbcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlPQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNZUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/MMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESh5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuc4AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESl8QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGnsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESihgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESnsEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8QYBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNtcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESX+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8KMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShMcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0XgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMwIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESAQIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESufYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESNAkBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESccMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEStdYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFXkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESb6EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESFcAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESluoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESxY8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2cEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEST/MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESLaEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESlvQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESPLkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShxgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES98IAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShPgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaHsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmegAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiRgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES/tgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5IEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESN9cAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESbQEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjZwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESmsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESvtIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGAEBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESePYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESfp0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuj0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA8gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7p8AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ50AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJ8AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESdu0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjPYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgSUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESs9YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESCBQBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESjBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES4gIBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVaUAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES730AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESiBgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESGQgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESttYAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8P0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3g0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2sMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESaQ0BAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESuvsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESKNEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShscAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESDTsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6HsAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESZ3sAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTvMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES3HgAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESJdkAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES5YcAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES6AUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd/YAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESUsQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESz3gAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES2+0AAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShhgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESMLEAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESP+AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESk/QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESTfoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESixgBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqEShccAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESVwoAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQuwAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESA9AAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQcMAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES0QUBAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESQOAAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESu5wAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESYcIAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESE9MAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES7oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9YQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES9oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESd5EAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES84QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES8oQAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqES74QAAA==%0D%0Agroup:%20AQUAAAAAAAUVAAAACkdDGG1JBGW2KqESgHsAAA==%0D%0Agroup:%20AQEAAAAAABIBAAAA%0D%0Aaccessrule:%20final_allow%0D%0Afirst:%20ERROR%0D%0Awebfilter:%20pass%0D%0Aexterr:%20invalid_code_431%0D%0A 
>>>> ua="-" exterr="-|-"|
>>>>
>>>> ??????? -- ??????? David Touzeau - Artica Tech France
>>>> ??????? Development team, level 3 support
>>>> ??????? ----------------------------------
>>>> ??????? P: +33 6 58 44 69 46
>>>> ??????? www:https://wiki.articatech.com
>>>> ??????? www:http://articatech.net
>>>> ??????? _______________________________________________
>>>> ??????? squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> https://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>
>>> ??? -- ??? David Touzeau - Artica Tech France
>>> ??? Development team, level 3 support
>>> ??? ----------------------------------
>>> ??? P: +33 6 58 44 69 46
>>> ??? www:https://wiki.articatech.com
>>> ??? www:http://articatech.net
>>
>> -- 
>> David Touzeau - Artica Tech France
>> Development team, level 3 support
>> ----------------------------------
>> P: +33 6 58 44 69 46
>> www:https://wiki.articatech.com
>> www:http://articatech.net
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

-- 
David Touzeau - Artica Tech France
Development team, level 3 support
----------------------------------
P: +33 6 58 44 69 46
www:https://wiki.articatech.com
www:http://articatech.net  

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20240131/420f8591/attachment.htm>

From misho.98118 at gmail.com  Wed Jan 31 22:22:09 2024
From: misho.98118 at gmail.com (Miha Miha)
Date: Thu, 1 Feb 2024 00:22:09 +0200
Subject: [squid-users] Is Squid 6 production ready?
In-Reply-To: <d8faeb35-9b42-49ab-9862-c304d6a09a3b@treenet.co.nz>
References: <CABZyDWHB_Jj3wYF242WSOvHQQop2RmpAH-_1+ho+Cr5VvRBG0Q@mail.gmail.com>
 <d8faeb35-9b42-49ab-9862-c304d6a09a3b@treenet.co.nz>
Message-ID: <CABZyDWEtZzM5vLZX8u79VeUjeei_4n-N+rLBuCcsLi0yyNmFvQ@mail.gmail.com>

> On 10/01/24 12:18, Miha Miha wrote:
> > Release note of latest Squid 6.6 says: "...not deemed ready for
> > production use..."  For comparison Squid 5.1 was 'ready'. When v6 is
> > expected to be ready for prod systems?
>
> On Fri, Jan 12, 2024 at 3:37?PM Amos Jeffries <squid3 at treenet.co.nz> wrote:
> Sorry, that is an oversight in the release notes text. Removing it now.
>
> Squid 6 is production ready.

Hi Amos,
I still see the 6.6 release note unchanged. Could you please adjust.

Cheers,
Mihail


