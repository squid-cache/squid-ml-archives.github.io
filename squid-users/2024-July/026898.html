<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Socket handle leak?
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Socket%20handle%20leak%3F&In-Reply-To=%3C1026668535.2215882.1720781888145%40mail.yahoo.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="026940.html">
   <LINK REL="Next"  HREF="026901.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Socket handle leak?</H1>
    <B>paolo.prinx at gmail.com</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Socket%20handle%20leak%3F&In-Reply-To=%3C1026668535.2215882.1720781888145%40mail.yahoo.com%3E"
       TITLE="[squid-users] Socket handle leak?">paolo.prinx at gmail.com
       </A><BR>
    <I>Fri Jul 12 10:58:08 UTC 2024</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="026940.html">[squid-users] TCP_MISS_ABORTED/502
</A></li>
        <LI>Next message (by thread): <A HREF="026901.html">[squid-users] Socket handle leak?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26898">[ date ]</a>
              <a href="thread.html#26898">[ thread ]</a>
              <a href="subject.html#26898">[ subject ]</a>
              <a href="author.html#26898">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hello,&#160; &#160;apologies in advance for the silly question.
We are having some stability issues with our squid farms after a recent upgrade from Centos/Squid 3.5.x to Ubuntu/Squid 5.7/6.9. I wonder if anyone here has seen something similar, and might have some suggestion about what we are obviously missing?

In short, after running for a certain period the servers run out of file descriptors. We see a slowly growing number of TCP or TCPv6 socket handles, that eventually hits the configured maximum. The handles do not get released until after squid is restarted (-k restart)

It is somewhat similar to what reported under&#160;<A HREF="https://access.redhat.com/solutions/3362211&#160;.">https://access.redhat.com/solutions/3362211&#160;.</A> They state that&#160;&#160;   
   - If an application fails to&#160;close()&#160;it's socket descriptors and continues to allocate new sockets then it can use up all the system memory on TCP(v6) slab objects.
   - Note some of these sockets will not show up in&#160;/proc/net/sockstat(6). Sockets that still have a file descriptor but are in the&#160;TCP_CLOSE&#160;state will consume a slab object. But will not be accounted for in&#160;/proc/net/sockstat(6)&#160;or &quot;ss&quot; or &quot;netstat&quot;.
   - It can be determined whether this is an application sockets leak, by stopping the application processes that are consuming sockets. If the slab objects in&#160;/proc/slabinfo&#160;are freed then the application is responsible. As that means that destructor routines have found open file descriptors to sockets in the process.

&quot;This is most likely to be a case of the application not handling error conditions correctly and not calling&#160;close()&#160;to free the FD and socket.&quot;


For example, on a server with squid 5.7, unmodified package:

list of open files;
lsof |wc -l56963
 
of which 35K in TCPv6:
lsof |grep proxy |grep TCPv6 |wc -l
&#160;&#160;&#160;&#160;35301
under /proc I see less objects
&#160; &#160; cat&#160; /proc/net/tcp6 |wc -l
&#160;&#160;&#160;&#160;3095
but the number of objects in the slabs is high&#160;&#160;&#160;&#160;cat /proc/slabinfo |grep TCPv6 &#160;&#160;&#160;&#160;MPTCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160;2048&#160; &#160;16&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160;&#160;&#160;&#160;tw_sock_TCPv6&#160; &#160; &#160; &#160;1155&#160; &#160;1155&#160; &#160; 248&#160; &#160;33&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160;35&#160; &#160; &#160;35&#160; &#160; &#160; 0&#160;&#160;&#160;&#160;request_sock_TCPv6&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; 304&#160; &#160;26&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160;&#160;&#160;&#160;TCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; 38519&#160; 38519&#160; &#160;2432&#160; &#160;13&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160;2963&#160; &#160;2963&#160; &#160; &#160; 0
I have 35K of lines like this&#160;&#160;&#160;&#160;lsof |grep proxy |grep TCPv6 |more&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;13u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160; 5428173 protocol: TCPv6&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;14u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160;27941608 protocol: TCPv6&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;24u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160;45124047 protocol: TCPv6&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;25u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160;50689821 protocol: TCPv6...

We thought maybe this is a weird IPv6 thing, as we only route IPv4, so we compiled a more recent version of squid with no v6 support. The thing just moved to TCP4..
lsof |wc -l120313
cat /proc/slabinfo |grep TCPMPTCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160;2048&#160; &#160;16&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0tw_sock_TCPv6&#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; 248&#160; &#160;33&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0request_sock_TCPv6&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; 304&#160; &#160;26&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0TCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 208&#160; &#160; 208&#160; &#160;2432&#160; &#160;13&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160;16&#160; &#160; &#160;16&#160; &#160; &#160; 0MPTCP&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160;1856&#160; &#160;17&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0tw_sock_TCP&#160; &#160; &#160; &#160; &#160;5577&#160; &#160;5577&#160; &#160; 248&#160; &#160;33&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; 169&#160; &#160; 169&#160; &#160; &#160; 0request_sock_TCP&#160; &#160; 1898&#160; &#160;2002&#160; &#160; 304&#160; &#160;26&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160;77&#160; &#160; &#160;77&#160; &#160; &#160; 0TCP&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;102452 113274&#160; &#160;2240&#160; &#160;14&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160;8091&#160; &#160;8091&#160; &#160; &#160; 0

cat /proc/net/tcp |wc -l255
After restarting squid the slab objects are released and the open file descriptors drop to a reasonable value. This further suggests it is squid hanging on to these FDs.

lsof |grep proxy |wc -l1221

Any suggestion? I guess it's something blatantly obvious, but it's a couple of days we look at this and we're not going anywhere...
Thanks again

-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.squid-cache.org/pipermail/squid-users/attachments/20240712/185d00e9/attachment.htm">http://lists.squid-cache.org/pipermail/squid-users/attachments/20240712/185d00e9/attachment.htm</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="026940.html">[squid-users] TCP_MISS_ABORTED/502
</A></li>
	<LI>Next message (by thread): <A HREF="026901.html">[squid-users] Socket handle leak?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26898">[ date ]</a>
              <a href="thread.html#26898">[ thread ]</a>
              <a href="subject.html#26898">[ subject ]</a>
              <a href="author.html#26898">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
