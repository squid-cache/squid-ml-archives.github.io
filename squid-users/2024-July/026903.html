<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Socket handle leak?
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Socket%20handle%20leak%3F&In-Reply-To=%3C782252282.537183.1720789472761%40mail.yahoo.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="026901.html">
   <LINK REL="Next"  HREF="026904.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Socket handle leak?</H1>
    <B>Paolo Prinsecchi</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Socket%20handle%20leak%3F&In-Reply-To=%3C782252282.537183.1720789472761%40mail.yahoo.com%3E"
       TITLE="[squid-users] Socket handle leak?">paolo.prinx at gmail.com
       </A><BR>
    <I>Fri Jul 12 13:04:32 UTC 2024</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="026901.html">[squid-users] Socket handle leak?
</A></li>
        <LI>Next message (by thread): <A HREF="026904.html">[squid-users] Socket handle leak?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26903">[ date ]</a>
              <a href="thread.html#26903">[ thread ]</a>
              <a href="subject.html#26903">[ subject ]</a>
              <a href="author.html#26903">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Thanks. We have limits set&#160;at 100K, squid can easily reach that. The problem is that the number of FD in use&#160;keeps increasing. A workaround is to restart squid every time it goes over a certain value,&#160;but it&#8217;s not really a solution.&#160;In the same situation, with centos and squid 3.5, we seldom went over 20k FD in use.&#160;
Thanks for your reply.&#160;

Panem et circenses

On Friday, July 12, 2024, 7:05 PM, Yvain PAYEN &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">yvain.payen at tessi.fr</A>&gt; wrote:

#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 p.yiv4238416288MsoNormal, #yiv4238416288 li.yiv4238416288MsoNormal, #yiv4238416288 div.yiv4238416288MsoNormal {margin:0cm;font-size:11.0pt;font-family:sans-serif;}#yiv4238416288 a:link, #yiv4238416288 span.yiv4238416288MsoHyperlink {color:blue;text-decoration:underline;}#yiv4238416288 code {}#yiv4238416288 span.yiv4238416288EmailStyle19 {font-family:sans-serif;color:windowtext;}#yiv4238416288 .yiv4238416288MsoChpDefault {font-size:10.0pt;}#yiv4238416288 filtered {}#yiv4238416288 div.yiv4238416288WordSection1 {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 filtered {}#yiv4238416288 ol {margin-bottom:0cm;}#yiv4238416288 ul {margin-bottom:0cm;}
Hi,
 
 &#160;
 
I my setup (also ubuntu) I have made these changes :
 
 &#160;
 
<A HREF="https://lists.squid-cache.org/listinfo/squid-users">root at proxy</A>: # cat /etc/security/limits.d/squid.conf
 
squid&#160;&#160;&#160;&#160;&#160;&#160;&#160; soft&#160;&#160;&#160; nofile&#160; 64000
 
squid&#160;&#160;&#160;&#160;&#160;&#160;&#160; hard&#160;&#160;&#160; nofile&#160; 65500
 
 &#160;
 
<A HREF="https://lists.squid-cache.org/listinfo/squid-users">root at proxy</A>: # cat /etc/squid/squid.conf | grep max_file
 
max_filedesc 64000
 
 &#160;
 
This force the system limits for squid process and tell squid how much FD it can consume.
 
 &#160;
 
Regards,
 
 &#160;
 
Yvain PAYEN
 
 &#160;
 
De&#160;: squid-users &lt;<A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users-bounces at lists.squid-cache.org</A>&gt;De la part de <A HREF="https://lists.squid-cache.org/listinfo/squid-users">paolo.prinx at gmail.com</A>
Envoy&#233;&#160;: vendredi 12 juillet 2024 12:58
&#192;&#160;: <A HREF="https://lists.squid-cache.org/listinfo/squid-users">squid-users at lists.squid-cache.org</A>
Objet&#160;: [squid-users] Socket handle leak?
 
 &#160;
 
&#9888; FR : Ce message provient de l'ext&#233;rieur de l'organisation. N'ouvrez pas de liens ou de pi&#232;ces jointes &#224; moins que vous ne sachiez que le contenu est fiable. &#160;&#9888;

 &#160;
 
Hello,
 
&#160; &#160;apologies in advance for the silly question.
 
 &#160;
 
We are having some stability issues with our squid farms after a recent upgrade from Centos/Squid 3.5.x to Ubuntu/Squid 5.7/6.9. I wonder if anyone here has seen something similar, and might have some suggestion about what we are obviously missing?
 
 &#160;
 
 &#160;
 
In short, after running for a certain period the servers run out of file descriptors. We see a slowly growing number of TCP or TCPv6 socket handles, that eventually hits the configured maximum. The handles do not get released until after squid is restarted (-k restart)
 
 &#160;
 
 &#160;
 
It is somewhat similar to what reported under&#160;<A HREF="https://access.redhat.com/solutions/3362211&#160;.">https://access.redhat.com/solutions/3362211&#160;.</A> They state that&#160;&#160;
    
   - If an application fails to&#160;close()&#160;it's socket descriptors and continues to allocate new sockets then it can use up all the system memory on TCP(v6) slab objects.
   - Note some of these sockets will not show up in&#160;/proc/net/sockstat(6). Sockets that still have a file descriptor but are in the&#160;TCP_CLOSE&#160;state will consume a slab object. But will not be accounted for in&#160;/proc/net/sockstat(6)&#160;or &quot;ss&quot; or &quot;netstat&quot;.
   - It can be determined whether this is an application sockets leak, by stopping the application processes that are consuming sockets. If the slab objects in&#160;/proc/slabinfo&#160;are freed then the application is responsible. As that means that destructor routines have found open file descriptors to sockets in the process.
 
 &#160;
 
&quot;This is most likely to be a case of the application not handling error conditions correctly and not calling&#160;close()&#160;to free the FD and socket.&quot;
 
 &#160;
 
 &#160;
 
For example, on a server with squid 5.7, unmodified package:
 
 &#160;
 
list of open files;
 

lsof |wc -l
 
56963
 

 &#160;
 
of which 35K in TCPv6:
 

lsof |grep proxy |grep TCPv6 |wc -l
 

&#160;&#160;&#160;&#160;35301
 
 &#160;
 
under /proc I see less objects
&#160; &#160; cat&#160; /proc/net/tcp6 |wc -l
 
&#160;&#160;&#160;&#160;3095
 
 &#160;
 
but the number of objects in the slabs is high
 
&#160;&#160;&#160;&#160;cat /proc/slabinfo |grep TCPv6
 
&#160;&#160;&#160;&#160;MPTCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160;2048&#160; &#160;16&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0
 
&#160;&#160;&#160;&#160;tw_sock_TCPv6&#160; &#160; &#160; &#160;1155&#160; &#160;1155&#160; &#160; 248&#160; &#160;33&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160;35&#160; &#160; &#160;35&#160; &#160; &#160; 0
 
&#160;&#160;&#160;&#160;request_sock_TCPv6&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; 304&#160; &#160;26&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0
 
&#160;&#160;&#160;&#160;TCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160;38519&#160; 38519&#160; &#160;2432&#160; &#160;13&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160;2963&#160; &#160;2963&#160; &#160; &#160; 0
 
 &#160;
 
I have 35K of lines like this
 
&#160;&#160;&#160;&#160;lsof |grep proxy |grep TCPv6 |more
 
&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;13u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160; 5428173 protocol: TCPv6
 
&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;14u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160;27941608 protocol: TCPv6
 
&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;24u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160;45124047 protocol: TCPv6
 
&#160;&#160;&#160;&#160;squid&#160; &#160; &#160; &#160; 1049&#160; &#160; &#160; &#160; &#160; &#160; &#160; proxy&#160; &#160;25u&#160; &#160; &#160;sock&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0,8&#160; &#160; &#160; &#160; 0t0&#160; &#160;50689821 protocol: TCPv6
 
...
 
 &#160;
 
 &#160;
 
We thought maybe this is a weird IPv6 thing, as we only route IPv4, so we compiled a more recent version of squid with no v6 support. The thing just moved to TCP4..
 
 &#160;
 
lsof |wc -l
 
120313
 
 &#160;
 
cat /proc/slabinfo |grep TCP
 
MPTCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160;2048&#160; &#160;16&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0
 
tw_sock_TCPv6&#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; 248&#160; &#160;33&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0
 
request_sock_TCPv6&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; 304&#160; &#160;26&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0
 
TCPv6&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 208&#160; &#160; 208&#160; &#160;2432&#160; &#160;13&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160;16&#160; &#160; &#160;16&#160; &#160; &#160; 0
 
MPTCP&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160;1856&#160; &#160;17&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160; 0&#160; &#160; &#160; 0&#160; &#160; &#160; 0
 
tw_sock_TCP&#160; &#160; &#160; &#160; &#160;5577&#160; &#160;5577&#160; &#160; 248&#160; &#160;33&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; 169&#160; &#160; 169&#160; &#160; &#160; 0
 
request_sock_TCP&#160; &#160; 1898&#160; &#160;2002&#160; &#160; 304&#160; &#160;26&#160; &#160; 2 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160; &#160;77&#160; &#160; &#160;77&#160; &#160; &#160; 0
 
TCP&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;102452 113274&#160;&#160;2240&#160; &#160;14&#160; &#160; 8 : tunables&#160; &#160; 0&#160; &#160; 0&#160; &#160; 0 : slabdata&#160; &#160;8091&#160; &#160;8091&#160; &#160; &#160; 0
 
 &#160;
 
 &#160;
 
cat /proc/net/tcp |wc -l
 
255
 
 &#160;
 
After restarting squid the slab objects are released and the open file descriptors drop to a reasonable value. This further suggests it is squid hanging on to these FDs.
 
lsof |grep proxy |wc -l
 
1221
 
 &#160;
 
 &#160;
 
Any suggestion? I guess it's something blatantly obvious, but it's a couple of days we look at this and we're not going anywhere...
 
 &#160;
 
Thanks again
 
 &#160;
 
 &#160;
 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.squid-cache.org/pipermail/squid-users/attachments/20240712/e8fa01c6/attachment.htm">http://lists.squid-cache.org/pipermail/squid-users/attachments/20240712/e8fa01c6/attachment.htm</A>&gt;
</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="026901.html">[squid-users] Socket handle leak?
</A></li>
	<LI>Next message (by thread): <A HREF="026904.html">[squid-users] Socket handle leak?
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#26903">[ date ]</a>
              <a href="thread.html#26903">[ thread ]</a>
              <a href="subject.html#26903">[ subject ]</a>
              <a href="author.html#26903">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
