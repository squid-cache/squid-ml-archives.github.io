From squid at digi.ninja  Wed Jan  1 08:21:51 2025
From: squid at digi.ninja (Robin Wood)
Date: Wed, 1 Jan 2025 08:21:51 +0000
Subject: [squid-users] Thoughts on caching aspx jsp asp cgi-bin
In-Reply-To: <56BE53E0-84A1-47D7-A30D-0E9A339F76F3@gmail.com>
References: <B1A27C83-9CF6-48AF-940A-10B493FE4F7F@gmail.com>
 <9B993437-FC3B-417B-A7A7-89DB3AAB176C@gmail.com>
 <CALmccy6KaRu8y2kbtwBhngwFcKbi4L9q2Uag1p_45OUd5WO8vA@mail.gmail.com>
 <56BE53E0-84A1-47D7-A30D-0E9A339F76F3@gmail.com>
Message-ID: <CALmccy7cjUeZd91AtDH=S7-m0s5zrrf2pmUkt+sbkofLyrpMyw@mail.gmail.com>

I'm going to massively over simplify things here, but you can think of it
like this.

Files with html extensions are static web pages, you write them, put them
on the server, and they are served as they are, no changes.

Asp and the others are dynamic files, they are processed by an app on the
server before they are sent to the client. This app may do nothing, so the
page comes as it was, but usually it will add content. This content could
be to create a CMS page by pulling the page content from a database, it
could be your shopping orders pulled from your account, or it could be your
current bank statement.

Caching should never be done on anything that is specific to a single user,
so it's fine to cache public CMS content with an asp extension, but not
your bank statement.

There is more to it than that, but hopefully that gives you a general idea.

Robin

On Tue, 31 Dec 2024, 23:07 Jonathan Lee, <jonathanlee571 at gmail.com> wrote:

> Thanks I have to admit I am a student currently,
>
> I guess my last question is what do cgi-bin asp aspx asp files do inside
> of websites? The reason I ask this is the rule below I have seen the same
> rule on different websites so it must be the main ones that cause issues.
> Is it just for dynamic content?
>
> On Dec 31, 2024, at 14:47, Robin Wood <squid at digi.ninja> wrote:
>
> I would say that it depends on what the dynamic content is. If it is
> public content from a CMS and you are OK with it potentially being your
> cache age out of date, then caching it rather than reloading it from the
> database every page load is fine. If the pages are for anything sensitive,
> for example a user's account, then definitely do not cache it.
>
> Robin
>
> On Tue, 31 Dec 2024 at 17:55, Jonathan Lee <jonathanlee571 at gmail.com>
> wrote:
>
>> What are your thoughts? This is in relation to ssl intercept with
>> certificates installed and bump active.
>>
>> Keep in mind I am still a student and learning.
>> Is a rule like this recommended? Does anyone have a better version of
>> this?
>> Sent from my iPhone
>>
>> On Dec 30, 2024, at 14:10, Jonathan Lee <jonathanlee571 at gmail.com> wrote:
>>
>> ?Hello fellow Squid Users,
>>
>> Can you please help?
>>
>> What are your thoughts on this rule? Should cgi-bin aspx and jsp files be
>> excluded from the web-cache? They are dynamic correct? This could help
>> speed up systems right?
>>
>>
>> acl QUERY urlpath_regex cgi-bin \? asp aspx jsp
>>
>> ## Prevent caching jsp, cgi-bin etc
>> cache deny QUERY
>>
>> Ref:
>> Setting up Explicit Squid Proxy
>> <https://wiki.alpinelinux.org/wiki/Setting_up_Explicit_Squid_Proxy>
>> wiki.alpinelinux.org
>> <https://wiki.alpinelinux.org/wiki/Setting_up_Explicit_Squid_Proxy>
>> <favicon.png>
>> <https://wiki.alpinelinux.org/wiki/Setting_up_Explicit_Squid_Proxy>
>> <https://wiki.alpinelinux.org/wiki/Setting_up_Explicit_Squid_Proxy>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250101/48bdf5b8/attachment.htm>

From squid at digi.ninja  Wed Jan  1 08:27:36 2025
From: squid at digi.ninja (Robin Wood)
Date: Wed, 1 Jan 2025 08:27:36 +0000
Subject: [squid-users] StoreID Question
In-Reply-To: <A865F380-002F-47E9-BC77-78B4AF604B80@gmail.com>
References: <A865F380-002F-47E9-BC77-78B4AF604B80@gmail.com>
Message-ID: <CALmccy7OZL-dvjUD7cHfRPZxRhSA=EgrRR94BVCkcqMg0QNpvA@mail.gmail.com>

I've not got time to read your whole email, but you are asking about
regular expressions.

^http:\/\/[^\.]+\.dl\.sourceforge\.net\/(.*)
http://dl.sourceforge.net.squid.internal/$1

What this means is to match the first URL and "capture" the bit at the end,
the bit in brackets. This then gets rewritten to the second URL with the
captured bit added on to the end, that is $1. If you captured two things in
brackets the first would be $1, the second $2.

Do some reading on regex and regular expressions. The basics are relatively
easy to understand, beyond that, it can get very complicated very quickly.

Robin

On Tue, 31 Dec 2024, 23:05 Jonathan Lee, <jonathanlee571 at gmail.com> wrote:

> *Hello Fellow Squid Users,*
>
> *Can you please help? I have been researching this for a long time and
> cannot find any information on this "what is the $ mean? within StoreID?*
>
> *Below is my failed attempt to make StoreID work correctly. Sorry it's a
> mess. I have since disabled my customized StoreID patterns because it
> caused issues. My question is with regard to the $number part of the
> program. I disabled all the facebook and all my tests because my photos
> where showing up wrong and it would duplicate itself over everything, I
> would have to clear the cache and change items and try again below is my
> failed attempt to get it to work correctly. *
>
> *It did work sometimes however I would get issues the longer it went on
> for. I decided to stop the trial and testing of it because it was driving
> me crazy. It is a great puzzle to solve. Does anyone have any tips? I have
> some Squid text books like the Squid the definitive guide, and The Squid
> Proxy Server 3.1 guide still nothing really explains StoreID outside of the
> Squid website. Yes the website comes with a great database that does work,
> I tested some database items with Ubuntu updates inside of VMs and it
> worked and reserved them to other machines asking for the same update. So
> in my quest I thought can I also do this with Facebook? (I do not recommend
> you try it) or something else, Youtube. *
>
> *This is the Text file I have been testing and it was a failed test
> outside of Ubuntu updates however I do not use that OS anymore so it is
> removed I think I had the $ wrong I have no info on what it does some are 1
> some are 5 some are doubles $ and another of them:*
>
> *^https?:\/\/(fbcdn|scontent).*(akamaihd|fbcdn)\.net\/.*\/v\/.*\/(.*\.mp4)
> http://facebook.squid.internal/$3 <http://facebook.squid.internal/$3>*
> *^https?:\/\/fbcdn\-(static|profile)\-a\.akamaihd\.net\/static\-ak\/rsrc\.php\/((?!.*\.(?:js|css|swf)).*)
> http://facebook.squid.internal/static/$2
> <http://facebook.squid.internal/static/$2>*
> *^https?:\/\/(fbcdn|scontent).*(akamaihd|fbcdn)\.net\/(h|s)(profile|photos).*\/(.*\.(png|gif|jpg))(\?.+)?
> http://facebook.squid.internal/$5 <http://facebook.squid.internal/$5>*
> *^https?:\/\/fbstatic\-a\.akamaihd\.net\/rsrc\.php\/((?!.*\.(?:js|css|swf)).*)
> http://facebook.squid.internal/static/$1
> <http://facebook.squid.internal/static/$1>*
> *^http:\/\/.*[steampowered|steamcontent]\.com\/([^?]*)
> http://steamupdates.squid.internal/$1
> <http://steamupdates.squid.internal/$1>*
> *^https?\:\/\/download\.oracle\.com\/((otn\-pub|otn)\/[\d\w]+\/[\d\w]+\/[\w\d\-]+\/[\w\d\-]+\.(exe|dmg|rpm|msi|tar\.(gz|Z)))\?
> http://java.oracle.otn.ngtech.squid.internal/$1
> <http://java.oracle.otn.ngtech.squid.internal/$1>*
> *^https?\:\/\/([\d\w\-]+)\.oracle\.com\/(([\d\w]+)\/[\d\w]+\/[\d\w]+\/([\d\w\-]+)\/([\d\w]+\/)?[\d\w\-\.\_]+\.(dmg|msi|exe|tar\.gz|tar\.Z))\?
> http://java.oracle.download.ngtech.squid.internal/$2
> <http://java.oracle.download.ngtech.squid.internal/$2>*
> *^http:\/\/[^\.]+\.phobos\.apple\.com\/(.*)
> http://appupdates.apple.squid.internal/$1
> <http://appupdates.apple.squid.internal/$1>*
> *^http:\/\/[^\.]+\.c\.android\.clients\.google\.com\/(.*)
> http://androidupdates.google.squid.internal/$1
> <http://androidupdates.google.squid.internal/$1>*
>
> *My question here is:*
> *What does this $3 mean within the the store id program?*
>
> *This is the config and refresh patterns that I was learning with for
> Squid StoreID. Much of it is ?#?ed out but this is what I was using:*
>
> *#store_id_program /usr/local/libexec/squid/storeid_file_rewrite
> /var/squid/storeid/storeid_rewrite.txt*
> *#store_id_children 10 startup=5 idle=1 concurrency=0*
> *#always_direct allow all*
> *#store_id_access deny connect*
> *#store_id_access deny !getmethod*
> *#store_id_access allow rewritedoms*
> *#store_id_access deny all*
>
> *refresh_all_ims on*
> *reload_into_ims on*
> *max_stale 20 years*
> *minimum_expiry_time 0*
>
> *#refresh_pattern -i ^http.*squid\.internal.* 43200 100% 79900
> override-expire override-lastmod ignore-reload ignore-no-store
> ignore-must-revalidate ignore-private ignore-auth*
>
> *#FACEBOOK*
> *#refresh_pattern ^https.*.facebook.com/* <http://facebook.com/*> 10080
> 80% 43200*
>
> *#FACEBOOK IMAGES  *
> *#refresh_pattern -i pixel.facebook.com..(jpg|png|gif|ico|css|js|jpg?)
> 10080 80% 43200*
> *#refresh_pattern -i .akamaihd.net..(jpg|png|gif|ico|css|js|jpg?) 10080
> 80% 43200 *
> *#refresh_pattern -i facebook.com.(jpg|png|gif|jpg?) 10080 80% 43200
> store-stale*
> *#refresh_pattern static.(xx|ak).fbcdn.net.(jpg|gif|png|jpg?) 10080 80%
> 43200*
> *#refresh_pattern ^https.*profile.ak.fbcdn.net.*(jpg|gif|png|jpg?) 10080
> 80% 43200*
> *#refresh_pattern ^https.*fbcdn.net.*(jpg|gif|png|jpg?) 10080 80% 43200*
>
> *#FACEBOOK VIDEO*
> *#refresh_pattern -i .video.ak.fbcdn.net.*.(mp4|flv|mp3|amf) 10080 80%
> 43200*
> *#refresh_pattern (audio|video)/(webm|mp4) 10080 80% 43200*
>
> *#APPLE STUFF*
> *#refresh_pattern -i
> apple.com/..(cab|exe|msi|msu|msf|asf|wmv|wma|dat|zip|dist)$
> <http://apple.com/..(cab%7Cexe%7Cmsi%7Cmsu%7Cmsf%7Casf%7Cwmv%7Cwma%7Cdat%7Czip%7Cdist)$>
> 0 80% 43200  refresh-ims*
>
> *#apple update*
> *#refresh_pattern -i (download|adcdownload).apple.com/.*\.(pkg|dmg)
> <http://apple.com/.*%5C.(pkg%7Cdmg)> 4320 100% 43200*
> *#refresh_pattern -i appldnld\.apple\.com 129600 100% 129600*
> *#refresh_pattern -i phobos\.apple\.com 129600 100% 129600*
> *#refresh_pattern -i iosapps\.itunes\.apple\.com 129600 100% 129600*
>
> *refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf)
> <http://windowsupdate.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)>
> 43200 80% 129600 reload-into-ims*
> *refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf)
> <http://microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)>
> 43200 80% 129600 reload-into-ims*
> *refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf)
> <http://windows.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)>
> 43200 80% 129600 reload-into-ims*
> *refresh_pattern -i
> microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf)
> <http://microsoft.com.akadns.net/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)>
> 43200 80% 129600 reload-into-ims*
> *refresh_pattern -i
> deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf)
> <http://deploy.akamaitechnologies.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)>
> 43200 80% 129600 reload-into-ims*
>
> *# Updates: Windows*
> *#refresh_pattern -i
> microsoft.com/..(cab|exe|msi|msu|msf|asf|wma|dat|zip)$
> <http://microsoft.com/..(cab%7Cexe%7Cmsi%7Cmsu%7Cmsf%7Casf%7Cwma%7Cdat%7Czip)$>
> 4320 80% 43200  refresh-ims*
> *refresh_pattern -i
> windowsupdate.com/..(cab|exe|msi|msu|msf|asf|wma|wmv)|dat|zip)$
> <http://windowsupdate.com/..(cab%7Cexe%7Cmsi%7Cmsu%7Cmsf%7Casf%7Cwma%7Cwmv)%7Cdat%7Czip)$>
> 4320 80% 43200  refresh-ims*
> *#refresh_pattern -i
> windows.com/..(cab|exe|msi|msu|msf|asf|wmv|wma|dat|zip)$
> <http://windows.com/..(cab%7Cexe%7Cmsi%7Cmsu%7Cmsf%7Casf%7Cwmv%7Cwma%7Cdat%7Czip)$>
> 4320 80% 43200  refresh-ims*
> *#refresh_pattern -i
> microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 4320 80% 43200 *
> *#refresh_pattern -i
> windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://windowsupdate.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 4320 80% 43200 *
> *#refresh_pattern -i
> windows.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://windows.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 4320 80% 43200 *
> *#refresh_pattern -i .*windowsupdate.com/.*\.(cab|exe)
> <http://windowsupdate.com/.*%5C.(cab%7Cexe)> 259200 100% 259200   *
> *#refresh_pattern -i .*update.microsoft.com/.*\.(cab|exe|dll|msi|psf)
> <http://update.microsoft.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)> 259200
> 100% 259200   *
> *#refresh_pattern windowsupdate.com/.*\.(cab|exe|dll|msi|psf)
> <http://windowsupdate.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)> 10080 100%
> 43200 *
> *#refresh_pattern download.microsoft.com/.*\.(cab|exe|dll|msi|psf)
> <http://download.microsoft.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)> 10080
> 100% 43200 *
> *#refresh_pattern www.microsoft.com/.*\.(cab|exe|dll|msi|psf)
> <http://www.microsoft.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)> 10080 100%
> 43200 *
> *#refresh_pattern au.download.windowsupdate.com/.*\.(cab|exe|dll|msi|psf)
> <http://au.download.windowsupdate.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)>
> 4320 100% 43200 *
> *#refresh_pattern bg.v4.pr.dl.ws.microsoft.com/.*\.(cab|exe|dll|msi|psf)
> <http://bg.v4.pr.dl.ws.microsoft.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)>
> 4320 100% 43200*
> *#windows update NEW UPDATE 0.04*
> *#refresh_pattern update.microsoft.com/.*\.(cab|exe)
> <http://update.microsoft.com/.*%5C.(cab%7Cexe)> 43200 100% 129600    *
> *#refresh_pattern
> ([^.]+\.)?(download|(windows)?update)\.(microsoft\.)?com/.*\.(cab|exe|msi|msp|psf)
> 4320 100% 43200  *
> *#refresh_pattern update.microsoft.com/.*\.(cab|exe|dll|msi|psf)
> <http://update.microsoft.com/.*%5C.(cab%7Cexe%7Cdll%7Cmsi%7Cpsf)> 10080
> 100% 43200 *
> *#refresh_pattern -i
> \.update.microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://update.microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 525600 100% 525600       *
> *#refresh_pattern -i
> \.windowsupdate.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://windowsupdate.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 525600 100% 525600       *
> *#refresh_pattern -i
> \.download.microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://download.microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 525600 100% 525600       *
> *#refresh_pattern -i
> \.ws.microsoft.com/.*\.(cab|exe|ms[i|u|f]|[ap]sf|wm[v|a]|dat|zip)
> <http://ws.microsoft.com/.*%5C.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip)>
> 525600 100% 525600       *
>
> *#refresh_pattern
> ([^.]+\.)?(cs|content[1-9]|hsar|content-origin|client-download).[steampowered|steamcontent].com/.*\.*
> 43200 100% 43200     *
> *#refresh_pattern ([^.]+\.)?.akamai.steamstatic.com/.*\.*
> <http://akamai.steamstatic.com/.*%5C.*> 43200 100% 43200*
>
> *#refresh_pattern -i ([^.]+\.)?.adobe.com/.*\.(zip|exe)
> <http://adobe.com/.*%5C.(zip%7Cexe)> 43200 100% 43200*
> *#refresh_pattern -i ([^.]+\.)?.java.com/.*\.(zip|exe)
> <http://java.com/.*%5C.(zip%7Cexe)> 43200 100% 43200*
> *#refresh_pattern -i ([^.]+\.)?.sun.com/.*\.(zip|exe)
> <http://sun.com/.*%5C.(zip%7Cexe)> 43200 100% 43200*
> *#refresh_pattern -i ([^.]+\.)?.oracle.com/.*\.(zip|exe|tar.gz)
> <http://oracle.com/.*%5C.(zip%7Cexe%7Ctar.gz)> 43200 100% 43200*
>
> *#refresh_pattern -i appldnld\.apple\.com 43200 100% 43200*
> *#refresh_pattern -i ([^.]+\.)?apple.com/.*\.(ipa)
> <http://apple.com/.*%5C.(ipa)> 43200 100% 43200*
>
> *#refresh_pattern -i ([^.]+\.)?.google.com/.*\.(exe|crx)
> <http://google.com/.*%5C.(exe%7Ccrx)> 10080 80% 43200*
> *#refresh_pattern -i ([^.]+\.)?g.static.com/.*\.(exe|crx)
> <http://g.static.com/.*%5C.(exe%7Ccrx)> 10080 80% 43200*
>
> *acl https_login url_regex -i ^https.*(login|Login).**
> *cache deny https_login*
>
> *#range_offset_limit 512 MB windowsupdate*
> *range_offset_limit 0 !windowsupdate*
> *quick_abort_min -1 KB*
>
>
> *Store ID program:*
>
> *I am using the built in program attached here..*
> */usr/local/libexec/squid/storeid_file_rewrite*
> *#!/usr/local/bin/perl*
>
> *use strict;*
> *use warnings;*
> *use Pod::Usage;*
>
> *=pod*
>
> *=head1 NAME*
>
> * storeid_file_rewrite - File based Store-ID helper for Squid*
>
> *=head1 SYNOPSIS*
>
> * storeid_file_rewrite filepath*
>
> *=head1 DESCRIPTION*
>
> *This program acts as a store_id helper program, rewriting URLs passed*
> *by Squid into storage-ids that can be used to achieve better caching*
> *for websites that use different URLs for the same content.*
>
> *It takes a text file with two tab separated columns.*
> *Column 1: Regular expression to match against the URL*
> *Column 2: Rewrite rule to generate a Store-ID*
> *Eg:*
> *^http:\/\/[^\.]+\.dl\.sourceforge\.net\/(.*)
>  http://dl.sourceforge.net.squid.internal/$1
> <http://dl.sourceforge.net.squid.internal/$1>*
>
> *Rewrite rules are matched in the same order as they appear in the rules
> file.*
> *So for best performance, sort it in order of frequency of occurrence.*
>
> *This program will automatically detect the existence of a concurrency
> channel-ID and adjust appropriately.*
> *It may be used with any value 0 or above for the store_id_children
> concurrency= parameter.*
>
> *=head1 OPTIONS*
>
> *The only command line parameter this helper takes is the regex rules file
> name.*
>
> *=head1 AUTHOR*
>
> *This program and documentation was written by I<Alan Mizrahi
> <alan at mizrahi.com.ve <alan at mizrahi.com.ve>>>*
>
> *Based on prior work by I<Eliezer Croitoru <eliezer at ngtech.co.il
> <eliezer at ngtech.co.il>>>*
>
> *=head1 COPYRIGHT*
>
> * * Copyright (C) 1996-2023 The Squid Software Foundation and contributors*
> * **
> * * Squid software is distributed under GPLv2+ license and includes*
> * * contributions from numerous individuals and organizations.*
> * * Please see the COPYING and CONTRIBUTORS files for details.*
>
> * Copyright (C) 2013 Alan Mizrahi <alan at mizrahi.com.ve
> <alan at mizrahi.com.ve>>*
> * Based on code from Eliezer Croitoru <eliezer at ngtech.co.il
> <eliezer at ngtech.co.il>>*
>
> * This program is free software; you can redistribute it and/or modify*
> * it under the terms of the GNU General Public License as published by*
> * the Free Software Foundation; either version 2 of the License, or*
> * (at your option) any later version.*
>
> * This program is distributed in the hope that it will be useful,*
> * but WITHOUT ANY WARRANTY; without even the implied warranty of*
> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the*
> * GNU General Public License for more details.*
>
> * You should have received a copy of the GNU General Public License*
> * along with this program; if not, write to the Free Software*
> * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
> <https://www.google.com/maps/search/59+Temple+Place,+Suite+330,+Boston,+MA++02111-1307,+USA?entry=gmail&source=g> 02111-1307,
> USA
> <https://www.google.com/maps/search/59+Temple+Place,+Suite+330,+Boston,+MA++02111-1307,+USA?entry=gmail&source=g>.*
>
> *=head1 QUESTIONS*
>
> *Questions on the usage of this program can be sent to the I<Squid Users
> mailing list <squid-users at lists.squid-cache.org
> <squid-users at lists.squid-cache.org>>>*
>
> *=head1 REPORTING BUGS*
>
> *Bug reports need to be made in English.*
> *See http://wiki.squid-cache.org/SquidFaq/BugReporting
> <http://wiki.squid-cache.org/SquidFaq/BugReporting> for details of what you
> need to include with your bug report.*
>
> *Report bugs or bug fixes using http://bugs.squid-cache.org/
> <http://bugs.squid-cache.org/>*
>
> *Report serious security bugs to I<Squid Bugs
> <squid-bugs at lists.squid-cache.org <squid-bugs at lists.squid-cache.org>>>*
>
> *Report ideas for new improvements to the I<Squid Developers mailing list
> <squid-dev at lists.squid-cache.org <squid-dev at lists.squid-cache.org>>>*
>
> *=head1 SEE ALSO*
>
> *squid (8), GPL (7),*
>
> *The Squid wiki http://wiki.squid-cache.org/Features/StoreID
> <http://wiki.squid-cache.org/Features/StoreID>*
>
> *The Squid Configuration Manual http://www.squid-cache.org/Doc/config/
> <http://www.squid-cache.org/Doc/config/>*
>
> *=cut*
>
> *my @rules; # array of [regex, replacement string]*
>
> *die "Usage: $0 <rewrite-file>\n" unless $#ARGV == 0;*
>
> *# read config file*
> *open RULES, $ARGV[0] or die "Error opening $ARGV[0]: $!";*
> *while (<RULES>) {*
> *    chomp;*
> *    next if /^\s*#?$/;*
> *    if (/^\s*([^\t]+?)\s*\t+\s*([^\t]+?)\s*$/) {*
> *        push(@rules, [qr/$1/, $2]);*
> *    } else {*
> *        print STDERR "$0: Parse error in $ARGV[0] (line $.)\n";*
> *    }*
> *}*
> *close RULES;*
>
> *$|=1;*
> *# read urls from squid and do the replacement*
> *URL: while (<STDIN>) {*
> *    chomp;*
> *    last if $_ eq 'quit';*
>
> *    my $channel = "";*
> *    if (s/^(\d+\s+)//o) {*
> *        $channel = $1;*
> *    }*
>
> *    foreach my $rule (@rules) {*
> *        if (my @match = /$rule->[0]/) {*
> *            $_ = $rule->[1];*
>
> *            for (my $i=1; $i<=scalar(@match); $i++) {*
> *                s/\$$i/$match[$i-1]/g;*
> *            }*
> *            print $channel, "OK store-id=$_\n";*
> *            next URL;*
> *        }*
> *    }*
> *    print $channel, "ERR\n";*
> *}*
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250101/a0d20cb7/attachment.htm>

From enfal.gok2004 at gmail.com  Wed Jan  1 10:34:39 2025
From: enfal.gok2004 at gmail.com (Enfal Gok)
Date: Wed, 1 Jan 2025 10:34:39 +0000
Subject: [squid-users] Assistance Needed for Kerberos Authentication with AD
 Group-Based ACLs in Squid
Message-ID: <PAWPR03MB9010BC03BDA24EC5A47FCDF1F40B2@PAWPR03MB9010.eurprd03.prod.outlook.com>

Dear Squid Support Team,
I am currently configuring Squid to use Kerberos authentication with Active Directory (AD) group-based access control, but I am encountering an issue where the ACLs for AD groups are not being applied correctly. Below are the details of my setup and the challenges I am facing:
Setup Details:

  1.
Kerberos:
     *   Kerberos authentication is working successfully.
     *   The service principal and keytab are correctly configured, and the kinit command works as expected.
  2.
LDAP:
     *   LDAP connectivity is functional. I can successfully query the Active Directory using ldapsearch:

ldapsearch -x -H ldap://172.16.10.254 -D "CN=Administrator,CN=Users,DC=demo,DC=local" -w Passw0rd -b "DC=demo,DC=local" "(sAMAccountName=jon.jones)"


     *   The output includes the correct memberof attributes showing the user's group memberships.
  3.
Squid Configuration:
I have configured Squid for LDAP group-based access control as follows:

external_acl_type ldap_group %LOGIN /usr/lib/squid/ext_ldap_group_acl -R \
    -b "DC=demo,DC=local" \
    -D "CN=Administrator,CN=Users,DC=demo,DC=local" \
    -w Passw0rd \
    -f "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%a,CN=Users,DC=demo,DC=local))" \
    -h 172.16.10.254

acl FullAccess external ldap_group FullAccess
acl Restricted external ldap_group Restricted
acl Filtered external ldap_group Filtered
acl Blocked external ldap_group Blocked

http_access deny Blocked
http_access allow FullAccess
http_access allow Restricted allowed_sites
http_access deny Restricted
http_access deny Filtered bad_sites
http_access allow Filtered
http_access deny all


  4.
What Works:
     *   Kerberos authentication is functioning as expected.
     *   The ext_ldap_group_acl utility works correctly when tested manually:

echo "jon.jones FullAccess" | /usr/lib/squid/ext_ldap_group_acl -R \
    -b "DC=demo,DC=local" \
    -D "CN=Administrator,CN=Users,DC=demo,DC=local" \
    -w Passw0rd \
    -f "(&(objectclass=person)(sAMAccountName=%v)(memberof=CN=%a,CN=Users,DC=demo,DC=local))" \
    -h 172.16.10.254


The output returns OK, indicating that the LDAP group membership is correctly validated.
  5.
The Problem:
     *   When users authenticate via Kerberos, the Squid ACLs based on AD groups are not being matched.
     *   All users fall into the default http_access deny all rule, even if they belong to a permitted AD group.
  6.
Log Example:
In the cache.log file, I see the following entries:

WARNING: external_acl_type 'ldap_group' queue overload
...
Checklist.cc answer DENIED for match
...
setAuth: WARNING: Graceful closure on conn due to connection-auth erase from ConnStateData::SwanSong cleanup


Request for Assistance:

  *   How can I ensure that Squid properly applies AD group-based ACLs when users authenticate via Kerberos?
  *   Are there specific configurations or known limitations for combining Kerberos authentication with LDAP group validation in Squid?

I would greatly appreciate any guidance or suggestions to resolve this issue. If additional logs or details are needed, please let me know.
Thank you for your support!
Best regards,
Enfal gok

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250101/e6326390/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan  1 16:06:27 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Jan 2025 05:06:27 +1300
Subject: [squid-users] Thoughts on caching aspx jsp asp cgi-bin
In-Reply-To: <CALmccy7cjUeZd91AtDH=S7-m0s5zrrf2pmUkt+sbkofLyrpMyw@mail.gmail.com>
References: <B1A27C83-9CF6-48AF-940A-10B493FE4F7F@gmail.com>
 <9B993437-FC3B-417B-A7A7-89DB3AAB176C@gmail.com>
 <CALmccy6KaRu8y2kbtwBhngwFcKbi4L9q2Uag1p_45OUd5WO8vA@mail.gmail.com>
 <56BE53E0-84A1-47D7-A30D-0E9A339F76F3@gmail.com>
 <CALmccy7cjUeZd91AtDH=S7-m0s5zrrf2pmUkt+sbkofLyrpMyw@mail.gmail.com>
Message-ID: <6a5af98c-b4e7-4fba-adc5-6351aa7996bd@treenet.co.nz>

On 1/01/25 21:21, Robin Wood wrote:
> I'm going to massively over simplify things here, but you can think of 
> it like this.
> 
> Files with html extensions are static web pages, you write them, put 
> them on the server, and they are served as they are, no changes.
> 
> Asp and the others are dynamic files, they are processed by an app on 
> the server before they are sent to the client. This app may do nothing, 
> so the page comes as it was, but usually it will add content. This 
> content could be to create a CMS page by pulling the page content from a 
> database, it could be your shopping orders pulled from your account, or 
> it could be your current bank statement.
> 
> Caching should never be done on anything that is specific to a single 
> user, so it's fine to cache public CMS content with an asp extension, 
> but not your bank statement.
> 
> There is more to it than that, but hopefully that gives you a general idea.
> 


That is mostly correct for simple HTTP/1.0-like behaviour.

With HTTP/1.1 and later things are a little different. The biggest 
change is that URL no longer matters. The Content-Typereplaces "fiel 
extension" entirely, and Cache-Control headers take over the job of 
defining how and when something can be cached.

For Squid, the refresh_pattern directive is what provides compatibility 
with HTTP 1.0 behaviour. It provides values for any Cache-Control 
settings the server omitted (eg for servers acting like HTTP/1.0 still).

The default "refresh_pattern -i (/cgi-bin/|\?) 0 0% 0" configuration 
line tells Squid the values which will perform HTTP/1.0 caching 
behaviour for any of the dynamic content coming out of broken or old 
cgi-bin services or anythign with query-string ('?...') URL.


Jonathan: if you have not changed the refresh_pattern's you do not have 
to care specifically about dynamic-vs-static content caching. Whether it 
is plain-text HTTP(S) or SSL-Bump'ed HTTPS, it **should** all cache 
properly for its server-claimed needs.

Your "cache deny" policy in squid.conf is telling Squid **never** to 
cache any URL containing the ACL-matching strings. Even if they could be 
cached safely.


HTH
Amos



> Robin
> 
> On Tue, 31 Dec 2024, 23:07 Jonathan Lee wrote:
> 
>     Thanks I have to admit I am a student currently,
> 
>     I guess my last question is what do cgi-bin asp aspx asp files do
>     inside of websites? The reason I ask this is the rule below I have
>     seen the same rule on different websites so it must be the main ones
>     that cause issues. Is it just for dynamic content?
> 

...

>>>         acl QUERY urlpath_regex cgi-bin \? asp aspx jsp
>>>

This rule applies based on a string-match of the URL. It does not matter 
whether the content is "dynamic" or not. Nor does it matter what the 
server indicates about caching of the response for any matched URL.


>>>         ## Prevent caching jsp, cgi-bin etc
>>>         cache deny QUERY
>>>



>>>         Ref:
>>>         Setting up Explicit Squid Proxy <https://
>>>         wiki.alpinelinux.org/wiki/Setting_up_Explicit_Squid_Proxy>
>>>         wiki.alpinelinux.org>
>>>         	


Looking at that tutorial I see some major issues.

  1) the "basic configuration" is very different from the official 
squid.conf (see 
<https://wiki.squid-cache.org/Releases/Squid-5#squid-5-default-config>, 
same applies for v6)

  2) the SSL-Bump example configuration disables **all** security 
features of TLS and makes it extremely difficult to even detect 
hijacking of the proxy.
   Basically this is a tutorial of how to setup an open-proxy that 
allows malware to abuse your network as a base of operations.

While the official Squid wiki page on SSL-Bump peek and splice has not 
been updated in a while it is still **much** better to follow than this 
one. At least for that particular section of details.
  see <https://wiki.squid-cache.org/Features/SslPeekAndSplice>

(I hope someone in the Alpine community can fix the above issues ASAP.)


Amos



>>>         <favicon.png>
>>>          <https://wiki.alpinelinux.org/wiki/
>>>         Setting_up_Explicit_Squid_Proxy>
>>>
>>>         <https://wiki.alpinelinux.org/wiki/
>>>         Setting_up_Explicit_Squid_Proxy>
>>         _______________________________________________
>>         squid-users mailing list
>>         squid-users at lists.squid-cache.org <mailto:squid-
>>         users at lists.squid-cache.org>
>>         https://lists.squid-cache.org/listinfo/squid-users <https://
>>         lists.squid-cache.org/listinfo/squid-users>
>>
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Jan  1 16:18:41 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Jan 2025 05:18:41 +1300
Subject: [squid-users] StoreID Question
In-Reply-To: <CALmccy7OZL-dvjUD7cHfRPZxRhSA=EgrRR94BVCkcqMg0QNpvA@mail.gmail.com>
References: <A865F380-002F-47E9-BC77-78B4AF604B80@gmail.com>
 <CALmccy7OZL-dvjUD7cHfRPZxRhSA=EgrRR94BVCkcqMg0QNpvA@mail.gmail.com>
Message-ID: <149178a5-f591-4bfd-985c-aeefc6a368db@treenet.co.nz>

On 1/01/25 21:27, Robin Wood wrote:
> I've not got time to read your whole email, but you are asking about 
> regular expressions.
> 
> ^http:\/\/[^\.]+\.dl\.sourceforge\.net\/(.*) http:// 
> dl.sourceforge.net.squid.internal/$1
> 
> What this means is to match the first URL and "capture" the bit at the 
> end, the bit in brackets. This then gets rewritten to the second URL 
> with the captured bit added on to the end, that is $1. If you captured 
> two things in brackets the first would be $1, the second $2.
> 
> Do some reading on regex and regular expressions. The basics are 
> relatively easy to understand, beyond that, it can get very complicated 
> very quickly.
> 

Correct. Also, this is a configuration file for the particular helper 
performing StoreID changes. The pattern style and language may/will 
differ based on what the custom helper is doing.

The OP one (Squid provided storeid_file_rewrite) is written in Perl 
language, and passes the list from the file almost directly to the 
"Substitute Regular Expression - s///" function of Perl. As such, the 
documentation of that function is what you need to read for specific 
answers.
  (<https://www.tutorialspoint.com/perl/perl_regular_expressions.htm>)


Amos



From squid3 at treenet.co.nz  Wed Jan  1 16:55:19 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Jan 2025 05:55:19 +1300
Subject: [squid-users] Assistance Needed for Kerberos Authentication
 with AD Group-Based ACLs in Squid
In-Reply-To: <PAWPR03MB9010BC03BDA24EC5A47FCDF1F40B2@PAWPR03MB9010.eurprd03.prod.outlook.com>
References: <PAWPR03MB9010BC03BDA24EC5A47FCDF1F40B2@PAWPR03MB9010.eurprd03.prod.outlook.com>
Message-ID: <f3288d79-0d5b-4ed6-9e82-a9dad641478c@treenet.co.nz>

On 1/01/25 23:34, Enfal Gok wrote:
...
>     *The Problem:*
>       * When users authenticate via Kerberos, the Squid ACLs based on AD
>         groups are not being matched.
>       * All users fall into the default |http_access deny all|?rule,
>         even if they belong to a permitted AD group.
>  6.
>     *Log Example:*
>     In the |cache.log|?file, I see the following entries:
> 
>     |WARNING: external_acl_type 'ldap_group' queue overload ...


You have more lookups being performed than the helper processes can 
handle. Either they are too slow or too many queries per second are 
happening.


First thing to do (quick workaround) is to expand how many helper 
processes are running, and how many queries they can have queued.

These are done with the "children-max=" option on external_acl_type 
lines. Since you have 4 ACLs sharing the helpers, IMO you should set 
that to a 4x the default. Expand as needed if the problem remains.

Avoid "concurrency" as the helper you are using does not (yet) support 
that. If anything, set it to "0" explicitly.



>     Checklist.cc answer DENIED for match ... setAuth: WARNING: Graceful
>     closure on conn due to connection-auth erase from
>     ConnStateData::SwanSong cleanup |
> 
> *Request for Assistance:*
> 
>   * How can I ensure that Squid properly applies AD group-based ACLs
>     when users authenticate via Kerberos?

The ACL and http_access portions of your config look fine to me. At 
least for the LDAP_Group helper you are using.


If you can try to convert to the newer "note ACL" way of checking groups.

The latest of Kerberos negotiate_kerberos_auth helper should provide the 
"group=" annotations to Squid during the auth credentials check. Then 
you can replace the "external" type ACL with a "note" type, and drop the 
group lookup entirely.

Like so:
"
   acl FullAccess note group SSID-of-FullAccess
   acl Restricted note group SSID-of-Restricted
   acl Filtered note group SSID-of-Filtered
   acl Blocked note group SSID-of-Blocked
"

Where the SSID-of-XX are the values the auth helper produces for those 
groups.




>   * Are there specific configurations or known limitations for combining
>     Kerberos authentication with LDAP group validation in Squid?
> 

Big ones I know of are true for any use of helpers:
  * speed of the lookups,
  * resource overheads of using more processes,
  * HOL blocking for busy proxies.

Unfortunately LDAP group helper right now ticks all of those checkboxes 
for worst-case usage.



HTH
Amos


From enfal.gok2004 at gmail.com  Thu Jan  2 11:35:58 2025
From: enfal.gok2004 at gmail.com (Enfal Gok)
Date: Thu, 2 Jan 2025 11:35:58 +0000
Subject: [squid-users] Assistance with Kerberos Authentication and AD
 Group-Based ACLs in Squid
Message-ID: <PAWPR03MB90100328A3D366D242D6CFEEF4142@PAWPR03MB9010.eurprd03.prod.outlook.com>

Dear Squid Support Team,
I am currently configuring Squid with Kerberos authentication and would like to integrate Active Directory (AD) group-based access control. My Kerberos authentication is working, and I can access AD successfully from my Ubuntu server. Below is my current Squid configuration:

# Kerberos authentication
auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth
auth_param negotiate children 10
auth_param negotiate keep_alive on

# ACL's
acl kerberos-auth proxy_auth REQUIRED
http_access allow kerberos-auth

# General access
http_access allow localhost
http_access deny all

# Proxy settings
http_port 3128
cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid


What Works:

  1.  Kerberos authentication is successfully validating users, and authenticated requests are being allowed through the proxy.
  2.  My Ubuntu server is connected to AD, and I can query AD successfully using ldapsearch.

What I Need Assistance With:
I want to integrate AD group-based ACLs to control user access based on their group membership in Active Directory. Specifically:

  1.  Restrict access for users in certain groups (e.g., Blocked group).
  2.  Allow limited or filtered access for users in other groups (e.g., Restricted or Filtered groups).
  3.  Provide full internet access for users in a FullAccess group.

Questions:

  1.  What is the best way to combine Kerberos authentication with AD group-based access control in Squid?
  2.  Should I use the external_acl_type helper with LDAP queries, or is there a better way, such as leveraging note ACLs and group annotations from the Kerberos helper?
  3.  Are there specific configuration examples or optimizations you recommend to achieve this setup?

Additional Information:

  *   I am new to configuring Squid and AD integration and have very little experience with these systems. If possible, I would greatly appreciate clear and beginner-friendly guidance.
  *   I have tested ldapsearch and confirmed that I can retrieve user attributes, including memberof, from AD.
  *   Despite extensive searching online, I couldn?t find a complete configuration example for integrating Kerberos authentication and AD group-based ACLs. If such an example exists, could you share it or guide me in creating one?

Thank you in advance for your assistance. Please let me know if additional details or logs are needed.
Best regards,

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250102/8f5a4b50/attachment.htm>

From jonathanlee571 at gmail.com  Thu Jan  2 19:19:09 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Thu, 2 Jan 2025 11:19:09 -0800
Subject: [squid-users] StoreID Question
In-Reply-To: <149178a5-f591-4bfd-985c-aeefc6a368db@treenet.co.nz>
References: <149178a5-f591-4bfd-985c-aeefc6a368db@treenet.co.nz>
Message-ID: <436658F5-F7C5-4942-A0F1-474BBFF36FC7@gmail.com>

Thanks everyone I guess my next question I am mulling over is do I still need custom refresh patterns ontop of the storeid text file items?
Sent from my iPhone

> On Jan 1, 2025, at 08:18, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> ?On 1/01/25 21:27, Robin Wood wrote:
>> I've not got time to read your whole email, but you are asking about regular expressions.
>> ^http:\/\/[^\.]+\.dl\.sourceforge\.net\/(.*) http:// dl.sourceforge.net.squid.internal/$1
>> What this means is to match the first URL and "capture" the bit at the end, the bit in brackets. This then gets rewritten to the second URL with the captured bit added on to the end, that is $1. If you captured two things in brackets the first would be $1, the second $2.
>> Do some reading on regex and regular expressions. The basics are relatively easy to understand, beyond that, it can get very complicated very quickly.
> 
> Correct. Also, this is a configuration file for the particular helper performing StoreID changes. The pattern style and language may/will differ based on what the custom helper is doing.
> 
> The OP one (Squid provided storeid_file_rewrite) is written in Perl language, and passes the list from the file almost directly to the "Substitute Regular Expression - s///" function of Perl. As such, the documentation of that function is what you need to read for specific answers.
> (<https://www.tutorialspoint.com/perl/perl_regular_expressions.htm>)
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users


From jonathanlee571 at gmail.com  Fri Jan  3 08:23:11 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Fri, 3 Jan 2025 00:23:11 -0800
Subject: [squid-users] pipeline_prefetch directive
Message-ID: <AF807630-C672-464C-811F-79642F6167C2@gmail.com>

Hello fellow Squid Users,

I understand this directive is removed in Squid7 again I am still trying to understand more about what it did and does in the older versions of software. 

pipeline_prefetch historically was on or off for settings however today it is n+1 or a numerical value for the variable n.

My question is after many trial and errors what is a good range to use for a 4GB memory system, 

I have attempted many different n values some being 100, 200, 300, 5, 10 etc it appears to work well with 100 or maybe it was my other changes with 
using the directive read_ahead_gap 64 KB and testing 16 and 32 

what is a good solid go go juice number for pipeline_prefetch? I do notice massive increases in facebook loads when I have it at 100 however decreased performance with news websites. It is like a can?t win directive for me. Finally I thought best to ask. 

I do understand it is no longer recommended to be used this is simply for speed and the system is secure behind a firewall. 

Thank for all you do. 

From gkinkie at gmail.com  Fri Jan  3 09:52:40 2025
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Fri, 3 Jan 2025 09:52:40 +0000
Subject: [squid-users] pipeline_prefetch directive
In-Reply-To: <AF807630-C672-464C-811F-79642F6167C2@gmail.com>
References: <AF807630-C672-464C-811F-79642F6167C2@gmail.com>
Message-ID: <CA+Y8hcNm_FqcoBR3u=gHeUSs+LpQ7JEOErNGaQd0LvZYBo2oxA@mail.gmail.com>

On Fri, Jan 3, 2025 at 8:23?AM Jonathan Lee <jonathanlee571 at gmail.com>
wrote:

> Hello fellow Squid Users,
>
> I understand this directive is removed in Squid7 again I am still trying
> to understand more about what it did and does in the older versions of
> software.
>
> pipeline_prefetch historically was on or off for settings however today it
> is n+1 or a numerical value for the variable n.
>
> My question is after many trial and errors what is a good range to use for
> a 4GB memory system,
>
> I have attempted many different n values some being 100, 200, 300, 5, 10
> etc it appears to work well with 100 or maybe it was my other changes with
> using the directive read_ahead_gap 64 KB and testing 16 and 32
>
> what is a good solid go go juice number for pipeline_prefetch? I do notice
> massive increases in facebook loads when I have it at 100 however decreased
> performance with news websites. It is like a can?t win directive for me.
> Finally I thought best to ask.


HTTP is a request-response protocol, and HTTP/1.1 uses a single TCP
connection to issue several requests via the "keep-alive" feature.

The way it is supposed to work is:
- client opens TCP connection
- client sends request #1 on connection
- server sends response #1 on connection
- client sends request #2 on connection
- server sends response #2 on connection
- ...
- client or server closes TCP connection

There are several reasons why a connection might be closed. Among them,
axplicit decision by the client or server for instance due to wanting to
terminate the process to avoid leaking too much memory, or an inactivity
timeout, or because the server doesn't know in advance how long a response
is going to be, so the only way to terminate the response is by closing the
connection.

Sometimes however the client knows in advance several resources it wants to
fetch, and sometimes it will perform these requests optimistically, without
waiting for the previous responses. The flow in this case can look like:
- client opens TCP connection
- client sends request #1 on connection
- client sends request #2 on connection
- server sends response #1 on connection
- server sends response #2 on connection
- ...
- client or server closes TCP connection

pipeline_prefetch instructs squid to try and detect these events, and to
optimistically start processing requests before having completed the
previous responses. This can in theory help reduce page rendering latency,
but it comes with several drawbacks:
- the feature triggers bugs in Squid - see
https://joshua.hu/squid-security-audit-35-0days-45-exploits
- the feature can actually harm performance: if squid needs to close the
TCP connection at the end of response #1, but has already downloaded
uncacheable content for request #2, that content has to be discarded
- modern web clients adopt complex optimization strategies when picking
what resources to download, making this optimization less and less relevant
- that's the reason why it was dropped as a feature: its benefit is
doubtful, the drawbacks are known and measurable, the complexity it adds to
squid is significant. We developers would rather clean the slate and work
on supporting http/2 which makes this whole problem go away


> I do understand it is no longer recommended to be used this is simply for
> speed and the system is secure behind a firewall.


I'd just recommend to not use it :)

--
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250103/335957ac/attachment.htm>

From squid3 at treenet.co.nz  Fri Jan  3 10:05:14 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Jan 2025 23:05:14 +1300
Subject: [squid-users] pipeline_prefetch directive
In-Reply-To: <AF807630-C672-464C-811F-79642F6167C2@gmail.com>
References: <AF807630-C672-464C-811F-79642F6167C2@gmail.com>
Message-ID: <7d72e790-162b-455a-a915-f58f6dae7034@treenet.co.nz>

On 3/01/25 21:23, Jonathan Lee wrote:
> Hello fellow Squid Users,
> 
> I understand this directive is removed in Squid7 again I am still trying to understand more about what it did and does in the older versions of software.
> 

It was an attempt to implement the pipeline mechanism of HTTP/1.1
see <https://httpwg.org/specs/rfc9112.html#pipelining>


FYI; very few people have even considered turning it on. So do not be 
surprised if you do not get a response to your more detailed questions 
about its behaviour in current Squid.


> pipeline_prefetch historically was on or off for settings however today it is n+1 or a numerical value for the variable n.
> 
> My question is after many trial and errors what is a good range to use for a 4GB memory system,
> 

> I have attempted many different n values some being 100, 200, 300, 5, 10 etc it appears to work well with 100 or maybe it was my other changes with
> using the directive read_ahead_gap 64 KB and testing 16 and 32
> 
> what is a good solid go go juice number for pipeline_prefetch? I do notice massive increases in facebook loads when I have it at 100 however decreased performance with news websites. It is like a can?t win directive for me. Finally I thought best to ask.
> 

IMO the answer to that depends on your proxy traffic.

  * A server that is able to produce responses very fast, operating over 
a long-delay connection can benefit from requests being sent in a pipeline.

  * As can two proxies communicating over a multiplexed, fast, **and** 
reliable connection.


Most situations risk adding what is called Head-Of-Line (HOL) blocking 
delays for the pipelined requests. HTTP/2 was created to solve that 
problem and does far better.


> I do understand it is no longer recommended to be used this is simply for speed and the system is secure behind a firewall.
> 


Firewall existence and security are not relevant to pipeline_prefetch.

Pipeline as implemented by Squid is only of much benefit when you 
already have a high speed situation. As mentioned above.

The primary use-case of Squid pipeline_prefetch to my knowledge was for 
the cache_peer links on proxies acting as accelerators on either end of 
a geosync satellite communication link. Servicing international level 
traffic relay for military outposts or small(ish) national teleco's.
   The primary factor being very long-distance / high-lag TCP links.


Cheers
Amos



From enfal.gok2004 at gmail.com  Fri Jan  3 18:40:07 2025
From: enfal.gok2004 at gmail.com (Enfal Gok)
Date: Fri, 3 Jan 2025 18:40:07 +0000
Subject: [squid-users] Assistance Required: Issues with Squid Kerberos +
 LDAP Group Configuration
Message-ID: <PAWPR03MB9010DF5EEC64C9A281A03B24F4152@PAWPR03MB9010.eurprd03.prod.outlook.com>

Dear Squid Community/Support Team,
I am currently configuring Squid with Kerberos authentication and LDAP group-based access control. However, I am encountering persistent issues, and I would greatly appreciate your guidance. Below are the details of my configuration and the errors I am facing.
________________________________
Error Logs
The following errors repeatedly appear in the Squid logs:

2025/01/03 19:35:40 kid1| Starting new helpers
2025/01/03 19:35:40 kid1| helperOpenServers: Starting 1/5 'ext_kerberos_ldap_group_acl' processes
support_sasl.cc(276): pid=70855 :2025/01/03 19:35:40| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error
support_ldap.cc(1086): pid=70855 :2025/01/03 19:35:40| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error
support_ldap.cc(1172): pid=70855 :2025/01/03 19:35:40| kerberos_ldap_group: ERROR: Error while binding to ldap server with Username/Password: Encoding error
(ext_kerberos_ldap_group_acl): ../../../../libraries/liblber/io.c:108: ber_write: Assertion `buf != NULL' failed.
2025/01/03 19:35:41 kid1| WARNING: external_acl_type #Hlpr7 exited
2025/01/03 19:35:41 kid1| Too few external_acl_type processes are running (need 1/5)


________________________________
Current Configuration
Kerberos Authentication

auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth -s HTTP/ubuntuserver.demo.local
auth_param negotiate children 10
auth_param negotiate keep_alive on


External ACL for LDAP Groups

external_acl_type kerberos_ldap_group ttl=3600 negative_ttl=3600 %LOGIN /usr/lib/squid/ext_kerberos_ldap_group_acl \
    -P HTTP/ubuntuserver.demo.local at DEMO.LOCAL \
    -D demo.local \
    -b DC=demo,DC=local \
    -l ldap://dc.demo.local \
    -g FullAccess at DEMO.LOCAL:Restricted at DEMO.LOCAL:Filtered at DEMO.LOCAL:Blocked at DEMO.LOCAL


ACL Definitions

acl FullAccess external kerberos_ldap_group FullAccess at DEMO.LOCAL
acl Restricted external kerberos_ldap_group Restricted at DEMO.LOCAL
acl Filtered external kerberos_ldap_group Filtered at DEMO.LOCAL
acl Blocked external kerberos_ldap_group Blocked at DEMO.LOCAL

acl allowed_sites dstdomain .benedictuspoort.be .smartschool.be .microsoft.com
acl bad_sites dstdomain .adult.com .gambling.com


Access Rules

http_access allow FullAccess
http_access allow Restricted allowed_sites
http_access deny Restricted
http_access deny Blocked
http_access deny Filtered bad_sites
http_access allow Filtered
http_access deny all


Proxy Settings

http_port 3128
cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid


________________________________
What I Have Tried

  *   Verified that the Kerberos keytab is up-to-date and matches the Key Version Number (msDS-KeyVersionNumber) in Active Directory.
  *   Tested LDAP queries using ldapsearch with both simple and GSSAPI bindings, which work intermittently.
  *   Checked Squid logs and confirmed that Kerberos tickets are being issued successfully using kinit and klist.

Despite these efforts, the ext_kerberos_ldap_group_acl helper is unable to bind to the LDAP server, and the Squid service keeps restarting helpers.
________________________________
Request for Assistance
Could you please provide guidance on:

  1.  Debugging the ext_kerberos_ldap_group_acl helper?
  2.  Ensuring compatibility between Kerberos and LDAP for group-based access control?
  3.  Any potential misconfigurations or missing steps in my setup?

Thank you in advance for your assistance. I look forward to your recommendations.
Kind regards,
Enfal gok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250103/33f983d2/attachment.htm>

From kinkie at squid-cache.org  Sat Jan  4 14:38:16 2025
From: kinkie at squid-cache.org (Francesco Chemolli)
Date: Sat, 4 Jan 2025 14:38:16 +0000
Subject: [squid-users] Upcoming changes on the methods used to distribute
 Squid
Message-ID: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>

Hi Squid Users,
   there are some ongoing changes on how we distribute the squid sources;
some of them have already happened, some more will happen in the
upcoming weeks.

The end state we are aiming to settle on is to distribute Squid via Github
Releases (https://github.com/squid-cache/squid/releases) .

Each Squid release has been and will continue to be tagged in git with the
SQUID_MAJ_MIN tag, which will be the official release point. Signed release
tarballs will be made available as Github release assets. These are already
available at https://github.com/squid-cache/squid/releases for every squid
version from 1.0.0alpha to 6.12.
We will no longer provide patches, these can be obtained from git.

We have decommissioned the rsync and ftp distribution points on
www.squid-cache.org, and are no longer advertising Squid mirrors on the
website. We are very thankful to Squid mirror operators and volunteers for
their continued support through the years.

In the next few weeks we will rework the "Download" section of the squid
website (https://www.squid-cache.org/Versions/) to point to Github for
downloading instead of self-hosting tarballs, patches etc.

Our plans moving forward:
- we will restart announcing new releases to the squid-announce mailing list
  see https://www.squid-cache.org/Support/mailing-lists.html
- anyone wanting to track Squid releases can:
  - use git tags
  - use the 'gh' tool from github (https://cli.github.com/)
  - rely on the 'releases' github page:
https://github.com/squid-cache/squid/releases
  - to only track the latest supported release:
https://github.com/squid-cache/squid/releases/latest

Any feedback is welcome

-- 
    Francesco Chemolli
    Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250104/941cf390/attachment.htm>

From jonathanlee571 at gmail.com  Mon Jan  6 05:37:38 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Sun, 5 Jan 2025 21:37:38 -0800
Subject: [squid-users] File descriptor usage for squid statistics
Message-ID: <156191FE-0E13-4131-B9DA-2205644BCA1F@gmail.com>

Hello fellow Squid users,

Can you please help I have noticed for a long time under information page that Store Disk Files Open is a lot of the times showing 0

Is this of concern? I thought I should email and ask as I have not found any information that gives clarity on what this section of the status page relates to. I have seen it at 1 at times however it shows failed to parse headers or something also. Does anyone have any information on this?

File descriptor usage for squid:
	Maximum number of file descriptors:   97578
	Largest file desc currently in use:   1452
	Number of file desc currently in use: 1215
	Files queued for open:                   0
	Available number of file descriptors: 96363
	Reserved number of file descriptors:   100
	Store Disk files open:                   0
Internal Data Structures:
	 17774 StoreEntries
	  4091 StoreEntries with MemObjects
	  4032 Hot Object Cache Items
	 17715 on-disk objects
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250105/fd222799/attachment.htm>

From stu.lists at spacehopper.org  Mon Jan  6 12:22:22 2025
From: stu.lists at spacehopper.org (Stuart Henderson)
Date: Mon, 6 Jan 2025 12:22:22 -0000 (UTC)
Subject: [squid-users] Upcoming changes on the methods used to
 distribute Squid
References: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
Message-ID: <slrnvnninu.27s6.stu.lists@naiad.spacehopper.org>

On 2025-01-04, Francesco Chemolli <kinkie at squid-cache.org> wrote:
>                                                              Signed release
> tarballs will be made available as Github release assets.

Thanks for doing that, it's a lot more helpful than relying on the
autogenerated git-archive tar.gz.




From ngtech1ltd at gmail.com  Mon Jan  6 15:06:51 2025
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 6 Jan 2025 17:06:51 +0200
Subject: [squid-users] Upcoming changes on the methods used to
 distribute Squid
In-Reply-To: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
References: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
Message-ID: <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>

Hey Francesco,

Thank you for the big effort.
I had the next git working for the past 2 years now:
https://github.com/elico/squid-latest

I have been using it to release my binary builds.
I hope that the new releases github format will help to automate squid
builds in the long run.
Will it be ready for the 6.13 release?
Id it is, then I will update my builds and git to work with the releases
page.

Thanks,
Eliezer
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com


On Sat, Jan 4, 2025 at 4:52?PM Francesco Chemolli <kinkie at squid-cache.org>
wrote:

> Hi Squid Users,
>    there are some ongoing changes on how we distribute the squid sources;
> some of them have already happened, some more will happen in the
> upcoming weeks.
>
> The end state we are aiming to settle on is to distribute Squid via Github
> Releases (https://github.com/squid-cache/squid/releases) .
>
> Each Squid release has been and will continue to be tagged in git with the
> SQUID_MAJ_MIN tag, which will be the official release point. Signed release
> tarballs will be made available as Github release assets. These are already
> available at https://github.com/squid-cache/squid/releases for every
> squid version from 1.0.0alpha to 6.12.
> We will no longer provide patches, these can be obtained from git.
>
> We have decommissioned the rsync and ftp distribution points on
> www.squid-cache.org, and are no longer advertising Squid mirrors on the
> website. We are very thankful to Squid mirror operators and volunteers for
> their continued support through the years.
>
> In the next few weeks we will rework the "Download" section of the squid
> website (https://www.squid-cache.org/Versions/) to point to Github for
> downloading instead of self-hosting tarballs, patches etc.
>
> Our plans moving forward:
> - we will restart announcing new releases to the squid-announce mailing
> list
>   see https://www.squid-cache.org/Support/mailing-lists.html
> - anyone wanting to track Squid releases can:
>   - use git tags
>   - use the 'gh' tool from github (https://cli.github.com/)
>   - rely on the 'releases' github page:
> https://github.com/squid-cache/squid/releases
>   - to only track the latest supported release:
> https://github.com/squid-cache/squid/releases/latest
>
> Any feedback is welcome
>
> --
>     Francesco Chemolli
>     Squid Software Foundation
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250106/14fedfe2/attachment.htm>

From stu at spacehopper.org  Mon Jan  6 15:35:34 2025
From: stu at spacehopper.org (Stuart Henderson)
Date: Mon, 6 Jan 2025 15:35:34 -0000 (UTC)
Subject: [squid-users] Upcoming changes on the methods used to
 distribute Squid
References: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
 <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>
Message-ID: <slrnvnnu26.27s6.stu.lists@naiad.spacehopper.org>

On 2025-01-06, NgTech LTD <ngtech1ltd at gmail.com> wrote:
> I have been using it to release my binary builds.
> I hope that the new releases github format will help to automate squid
> builds in the long run.
> Will it be ready for the 6.13 release?
> Id it is, then I will update my builds and git to work with the releases
> page.

As mentioned in the post you replied to:

>> Each Squid release has been and will continue to be tagged in git with the
>> SQUID_MAJ_MIN tag, which will be the official release point. Signed release
>> tarballs will be made available as Github release assets. These are already
                                                             ^^^^^^^^^^^^^^^^^
>> available at https://github.com/squid-cache/squid/releases for every
   ^^^^^^^^^
>> squid version from 1.0.0alpha to 6.12.

If you want something you can machine-parse to check for updates:

https://api.github.com/repos/squid-cache/squid/releases/latest
https://api.github.com/repos/squid-cache/squid/releases




From kinkie at squid-cache.org  Mon Jan  6 16:22:54 2025
From: kinkie at squid-cache.org (Francesco Chemolli)
Date: Mon, 6 Jan 2025 16:22:54 +0000
Subject: [squid-users] Upcoming changes on the methods used to
 distribute Squid
In-Reply-To: <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>
References: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
 <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>
Message-ID: <CA+Y8hcM+Upq+KQ6+Zag31J+p1+0C09EUb4VyZTEcbUThX2d2Mw@mail.gmail.com>

It already is working, please test it.

URLs look like:
https://github.com/squid-cache/squid/releases/download/SQUID_6_12/squid-6.12.tar.bz2

SQUID_6_12 is the release git tag. They are by convention named
SQUID_MAJOR_MINOR

The next release file would be named
https://github.com/squid-cache/squid/releases/download/SQUID_6_13/squid-6.13.tar.bz2

There are several ways to track releases, the one I would find the most
convenient is via the
'gh' tool (https://cli.github.com/) . Its 'release' subcommand is very
powerful. See https://cli.github.com/manual/gh_release

On Mon, Jan 6, 2025 at 3:07?PM NgTech LTD <ngtech1ltd at gmail.com> wrote:

> Hey Francesco,
>
> Thank you for the big effort.
> I had the next git working for the past 2 years now:
> https://github.com/elico/squid-latest
>
> I have been using it to release my binary builds.
> I hope that the new releases github format will help to automate squid
> builds in the long run.
> Will it be ready for the 6.13 release?
> Id it is, then I will update my builds and git to work with the releases
> page.
>
> Thanks,
> Eliezer
> ----
> Eliezer Croitoru
> Tech Support
> Mobile: +972-5-28704261
> Email: ngtech1ltd at gmail.com
>
>
> On Sat, Jan 4, 2025 at 4:52?PM Francesco Chemolli <kinkie at squid-cache.org>
> wrote:
>
>> Hi Squid Users,
>>    there are some ongoing changes on how we distribute the squid sources;
>> some of them have already happened, some more will happen in the
>> upcoming weeks.
>>
>> The end state we are aiming to settle on is to distribute Squid via
>> Github Releases (https://github.com/squid-cache/squid/releases) .
>>
>> Each Squid release has been and will continue to be tagged in git with
>> the SQUID_MAJ_MIN tag, which will be the official release point. Signed
>> release tarballs will be made available as Github release assets. These are
>> already available at https://github.com/squid-cache/squid/releases for
>> every squid version from 1.0.0alpha to 6.12.
>> We will no longer provide patches, these can be obtained from git.
>>
>> We have decommissioned the rsync and ftp distribution points on
>> www.squid-cache.org, and are no longer advertising Squid mirrors on the
>> website. We are very thankful to Squid mirror operators and volunteers for
>> their continued support through the years.
>>
>> In the next few weeks we will rework the "Download" section of the squid
>> website (https://www.squid-cache.org/Versions/) to point to Github for
>> downloading instead of self-hosting tarballs, patches etc.
>>
>> Our plans moving forward:
>> - we will restart announcing new releases to the squid-announce mailing
>> list
>>   see https://www.squid-cache.org/Support/mailing-lists.html
>> - anyone wanting to track Squid releases can:
>>   - use git tags
>>   - use the 'gh' tool from github (https://cli.github.com/)
>>   - rely on the 'releases' github page:
>> https://github.com/squid-cache/squid/releases
>>   - to only track the latest supported release:
>> https://github.com/squid-cache/squid/releases/latest
>>
>> Any feedback is welcome
>>
>> --
>>     Francesco Chemolli
>>     Squid Software Foundation
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>

-- 
    Francesco Chemolli
    Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250106/43f6913e/attachment.htm>

From ngtech1ltd at gmail.com  Mon Jan  6 18:06:14 2025
From: ngtech1ltd at gmail.com (NgTech LTD)
Date: Mon, 6 Jan 2025 20:06:14 +0200
Subject: [squid-users] Upcoming changes on the methods used to
 distribute Squid
In-Reply-To: <CA+Y8hcM+Upq+KQ6+Zag31J+p1+0C09EUb4VyZTEcbUThX2d2Mw@mail.gmail.com>
References: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
 <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>
 <CA+Y8hcM+Upq+KQ6+Zag31J+p1+0C09EUb4VyZTEcbUThX2d2Mw@mail.gmail.com>
Message-ID: <CABA8h=QXJZk4qNsH=yVpbxOQYJC-acLsc0qbbsF-5kxJkRRk_A@mail.gmail.com>

OK, So I have just seen that the squid-cache page is not longer parsable
for some reason by my ruby script so I changed the source of the squid
version to be from github latest release.
If someone wants to write his own build scripting based on the latest
release of squid the next can script can be of some help:
squid-latest/get-latest-from-github-releases.sh at main ? elico/squid-latest
<https://github.com/elico/squid-latest/blob/main/get-latest-from-github-releases.sh>

it just works...

Eliezer
----
Eliezer Croitoru
Tech Support
Mobile: +972-5-28704261
Email: ngtech1ltd at gmail.com


On Mon, Jan 6, 2025 at 6:23?PM Francesco Chemolli <kinkie at squid-cache.org>
wrote:

> It already is working, please test it.
>
> URLs look like:
>
> https://github.com/squid-cache/squid/releases/download/SQUID_6_12/squid-6.12.tar.bz2
>
> SQUID_6_12 is the release git tag. They are by convention named
> SQUID_MAJOR_MINOR
>
> The next release file would be named
>
> https://github.com/squid-cache/squid/releases/download/SQUID_6_13/squid-6.13.tar.bz2
>
> There are several ways to track releases, the one I would find the most
> convenient is via the
> 'gh' tool (https://cli.github.com/) . Its 'release' subcommand is very
> powerful. See https://cli.github.com/manual/gh_release
>
> On Mon, Jan 6, 2025 at 3:07?PM NgTech LTD <ngtech1ltd at gmail.com> wrote:
>
>> Hey Francesco,
>>
>> Thank you for the big effort.
>> I had the next git working for the past 2 years now:
>> https://github.com/elico/squid-latest
>>
>> I have been using it to release my binary builds.
>> I hope that the new releases github format will help to automate squid
>> builds in the long run.
>> Will it be ready for the 6.13 release?
>> Id it is, then I will update my builds and git to work with the releases
>> page.
>>
>> Thanks,
>> Eliezer
>> ----
>> Eliezer Croitoru
>> Tech Support
>> Mobile: +972-5-28704261
>> Email: ngtech1ltd at gmail.com
>>
>>
>> On Sat, Jan 4, 2025 at 4:52?PM Francesco Chemolli <kinkie at squid-cache.org>
>> wrote:
>>
>>> Hi Squid Users,
>>>    there are some ongoing changes on how we distribute the squid
>>> sources; some of them have already happened, some more will happen in the
>>> upcoming weeks.
>>>
>>> The end state we are aiming to settle on is to distribute Squid via
>>> Github Releases (https://github.com/squid-cache/squid/releases) .
>>>
>>> Each Squid release has been and will continue to be tagged in git with
>>> the SQUID_MAJ_MIN tag, which will be the official release point. Signed
>>> release tarballs will be made available as Github release assets. These are
>>> already available at https://github.com/squid-cache/squid/releases for
>>> every squid version from 1.0.0alpha to 6.12.
>>> We will no longer provide patches, these can be obtained from git.
>>>
>>> We have decommissioned the rsync and ftp distribution points on
>>> www.squid-cache.org, and are no longer advertising Squid mirrors on the
>>> website. We are very thankful to Squid mirror operators and volunteers for
>>> their continued support through the years.
>>>
>>> In the next few weeks we will rework the "Download" section of the squid
>>> website (https://www.squid-cache.org/Versions/) to point to Github for
>>> downloading instead of self-hosting tarballs, patches etc.
>>>
>>> Our plans moving forward:
>>> - we will restart announcing new releases to the squid-announce mailing
>>> list
>>>   see https://www.squid-cache.org/Support/mailing-lists.html
>>> - anyone wanting to track Squid releases can:
>>>   - use git tags
>>>   - use the 'gh' tool from github (https://cli.github.com/)
>>>   - rely on the 'releases' github page:
>>> https://github.com/squid-cache/squid/releases
>>>   - to only track the latest supported release:
>>> https://github.com/squid-cache/squid/releases/latest
>>>
>>> Any feedback is welcome
>>>
>>> --
>>>     Francesco Chemolli
>>>     Squid Software Foundation
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> https://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>
> --
>     Francesco Chemolli
>     Squid Software Foundation
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250106/499031b3/attachment.htm>

From rousskov at measurement-factory.com  Mon Jan  6 21:32:55 2025
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 6 Jan 2025 16:32:55 -0500
Subject: [squid-users] File descriptor usage for squid statistics
In-Reply-To: <156191FE-0E13-4131-B9DA-2205644BCA1F@gmail.com>
References: <156191FE-0E13-4131-B9DA-2205644BCA1F@gmail.com>
Message-ID: <2b16f0c0-4cd0-49b2-a5fc-326e283c617a@measurement-factory.com>

On 2025-01-06 00:37, Jonathan Lee wrote:

> Can you please help I have noticed for a long time under information 
> page that Store Disk Files Open is a lot of the times showing 0
> 
> Is this of concern?

If your Squid is not configured to use a cache_dir, then seeing zero 
Store Disk files open is OK.

If your Squid is configured to use a rock cache_dir, then seeing zero or 
very few Store Disk files open is usually OK.

If your Squid is more-or-less idle at mgr:info query time, then seeing 
zero Store Disk files open is usually OK.


> I thought I should email and ask as I have not found 
> any information that gives clarity on what this section of the status 
> page relates to. 

This low-level statistics shows a very approximate counter of files open 
for the purpose related to managing responses in a cache_dir storage. 
The exact definitions of "file", "open", and "storage" vary from one 
cache_dir type to another.

N.B. All(?) other lines in "File descriptor usage for squid" section 
include network socket descriptors (and not just disk file descriptors).


If you can add this (or better) info to Squid wiki, please do.


> I have seen it at 1 at times however it shows failed to 
> parse headers or something also.

Those events are probably unrelated, but I cannot be sure without seeing 
the exact failure messages.


HTH,

Alex.


> File descriptor usage for squid:
> 	Maximum number of file descriptors:   97578
> 	Largest file desc currently in use:   1452
> 	Number of file desc currently in use: 1215
> 	Files queued for open:                   0
> 	Available number of file descriptors: 96363
> 	Reserved number of file descriptors:   100
> 	Store Disk files open:                   0
> Internal Data Structures:
> 	 17774 StoreEntries
> 	  4091 StoreEntries with MemObjects
> 	  4032 Hot Object Cache Items
> 	 17715 on-disk objects
> 




From squid3 at treenet.co.nz  Tue Jan  7 05:11:54 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Jan 2025 18:11:54 +1300
Subject: [squid-users] Upcoming changes on the methods used to
 distribute Squid
In-Reply-To: <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>
References: <CA+Y8hcN1nJdasaHMsr+UcFefKdEC8ZJMn5B_NPFJtLam8ag=eA@mail.gmail.com>
 <CABA8h=RC=3a33Mtj8U=YWX7EpKfsfdiFO0BHxfDMmvX4csZvTw@mail.gmail.com>
Message-ID: <e6d74555-f43b-4869-a47c-f77b7bcfb9a7@treenet.co.nz>

On 7/01/25 04:06, NgTech LTD wrote:
> Hey?Francesco,
> 
> Thank you for the big effort.
> I had the next git working for the past 2 years now:
> https://github.com/elico/squid-latest <https://github.com/elico/squid- 
> latest>
> 
> I have been using it to release?my binary builds.
> I hope that the new releases github format will help to automate squid 
> builds in the long run.
> Will it be ready for the 6.13 release?
> Id it is, then I will update my builds and git to work with the releases 
> page.
> 

Elico,
   The change is already done and working already for all the past Squid 
versions Francesco mentioned. So you should be able to do and test your 
process change today.

Cheers
Amos



From tony.albers at gmx.com  Tue Jan  7 09:49:37 2025
From: tony.albers at gmx.com (Tony Albers)
Date: Tue, 7 Jan 2025 10:49:37 +0100
Subject: [squid-users] Resource management, backend application
Message-ID: <6a0e642d-73e7-4b7e-84aa-fe329c36ef57@gmx.com>

Hi guys,

Is it possible in squid to ensure that a badly behaving backend
application doesn't eat up all squid resources?

E.g.: at work we have an Apache reverse proxy in front of a number of
backend hosts. If one of the backend applications misbehaves, this can
result in all of apache's worker processes being held up by this
application, resulting in apache hanging and all sites going offline.
In apache, AFAIK, there is no way to prevent this.

But can squid handle this scenario in a way that only the site with the
misbehaving application goes offline without pulling the other sites
down with it?

I understand that the way squid and apache works is different, but
that's not really important for me. I just want to use the best tool for
the job.

TIA,

/tony


From uhlar at fantomas.sk  Tue Jan  7 13:24:21 2025
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 7 Jan 2025 14:24:21 +0100
Subject: [squid-users] Resource management, backend application
In-Reply-To: <6a0e642d-73e7-4b7e-84aa-fe329c36ef57@gmx.com>
References: <6a0e642d-73e7-4b7e-84aa-fe329c36ef57@gmx.com>
Message-ID: <Z30rBbBTpzCu7mXQ@fantomas.sk>

On 07.01.25 10:49, Tony Albers wrote:
>Is it possible in squid to ensure that a badly behaving backend
>application doesn't eat up all squid resources?
>
>E.g.: at work we have an Apache reverse proxy in front of a number of
>backend hosts. If one of the backend applications misbehaves, this can
>result in all of apache's worker processes being held up by this
>application, resulting in apache hanging and all sites going offline.
>In apache, AFAIK, there is no way to prevent this.
>
>But can squid handle this scenario in a way that only the site with the
>misbehaving application goes offline without pulling the other sites
>down with it?
>
>I understand that the way squid and apache works is different, but
>that's not really important for me. I just want to use the best tool for
>the job.

squid handles multiple concurrent connections, so exactly described scenario 
is unlikely happen with squid.
You more likely get maximum munmer of open filedescriptors, but that one can 
be increased much more than number of apache processes.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
On the other hand, you have different fingers.


From rousskov at measurement-factory.com  Tue Jan  7 13:55:16 2025
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 7 Jan 2025 08:55:16 -0500
Subject: [squid-users] Resource management, backend application
In-Reply-To: <6a0e642d-73e7-4b7e-84aa-fe329c36ef57@gmx.com>
References: <6a0e642d-73e7-4b7e-84aa-fe329c36ef57@gmx.com>
Message-ID: <1d1b0d97-9a0c-45d4-aa2b-6f6d571d62b5@measurement-factory.com>

On 2025-01-07 04:49, Tony Albers wrote:
> Is it possible in squid to ensure that a badly behaving backend
> application doesn't eat up all squid resources?

Yes, especially if you know about that application behavior in advance. 
You can configure Squid to start denying requests for the problematic 
application once the number of concurrent requests for that application 
exceeds some threshold.

You will probably have to use external ACLs to track that concurrency 
level. I do not have a blueprint ready, but it should be doable in 
principle.


> E.g.: at work we have an Apache reverse proxy in front of a number of
> backend hosts. If one of the backend applications misbehaves, this can
> result in all of apache's worker processes being held up by this
> application, resulting in apache hanging and all sites going offline.
> In apache, AFAIK, there is no way to prevent this.

Squid worker processes are not dedicated to a single request or a single 
application so, as Matus UHLAR has already said, the above scenario is 
not going to happen with Squid (but an application can exhaust other 
resources such as socket descriptors or memory, so Squid can be slowed 
down in a similar scenario unless you configure it specially).


HTH,

Alex.


> But can squid handle this scenario in a way that only the site with the
> misbehaving application goes offline without pulling the other sites
> down with it?
> 
> I understand that the way squid and apache works is different, but
> that's not really important for me. I just want to use the best tool for
> the job.
> 
> TIA,
> 
> /tony
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users



From tony.albers at gmx.com  Tue Jan  7 14:03:53 2025
From: tony.albers at gmx.com (Tony Albers)
Date: Tue, 7 Jan 2025 15:03:53 +0100
Subject: [squid-users] Resource management, backend application
In-Reply-To: <1d1b0d97-9a0c-45d4-aa2b-6f6d571d62b5@measurement-factory.com>
References: <6a0e642d-73e7-4b7e-84aa-fe329c36ef57@gmx.com>
 <1d1b0d97-9a0c-45d4-aa2b-6f6d571d62b5@measurement-factory.com>
Message-ID: <440d2399-4211-4d52-9250-51ad071bad2f@gmx.com>



On 07/01/2025 14:55, Alex Rousskov wrote:
> On 2025-01-07 04:49, Tony Albers wrote:
>> Is it possible in squid to ensure that a badly behaving backend
>> application doesn't eat up all squid resources?
>
> Yes, especially if you know about that application behavior in advance.
> You can configure Squid to start denying requests for the problematic
> application once the number of concurrent requests for that application
> exceeds some threshold.
>
> You will probably have to use external ACLs to track that concurrency
> level. I do not have a blueprint ready, but it should be doable in
> principle.
>
>
>> E.g.: at work we have an Apache reverse proxy in front of a number of
>> backend hosts. If one of the backend applications misbehaves, this can
>> result in all of apache's worker processes being held up by this
>> application, resulting in apache hanging and all sites going offline.
>> In apache, AFAIK, there is no way to prevent this.
>
> Squid worker processes are not dedicated to a single request or a single
> application so, as Matus UHLAR has already said, the above scenario is
> not going to happen with Squid (but an application can exhaust other
> resources such as socket descriptors or memory, so Squid can be slowed
> down in a similar scenario unless you configure it specially).
>
>
> HTH,
>
> Alex.
>
>
>> But can squid handle this scenario in a way that only the site with the
>> misbehaving application goes offline without pulling the other sites
>> down with it?
>>
>> I understand that the way squid and apache works is different, but
>> that's not really important for me. I just want to use the best tool for
>> the job.
>>
>> TIA,
>>
>> /tony

Thanks Alex and Matus, much appreciated.

/tony


From orion at nwra.com  Wed Jan  8 23:33:43 2025
From: orion at nwra.com (Orion Poplawski)
Date: Wed, 8 Jan 2025 16:33:43 -0700
Subject: [squid-users] Using and trusting remote client IP address via
 upstream proxy
Message-ID: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>

We use e2guardian and squid in a combined method were requests can either go
to e2guardian first and get forwarded to squid, or go directly to squid.

I would like to be able to have squid allow connections for certain remote
client IPs without requiring authentication.  However, the connections that
come in through e2guardian appear to squid as coming from localhost.  Is there
a way that e2guardian could pass the IP address of the client on to squid?

Thanks

-- 
Orion Poplawski
he/him/his  - surely the least important thing about me
Manager of IT Systems                      720-772-5637
NWRA, Boulder Office                  FAX: 303-415-9702
3380 Mitchell Lane                       orion at nwra.com
Boulder, CO 80301                 https://www.nwra.com/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4087 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250108/e287eeef/attachment.bin>

From uhlar at fantomas.sk  Thu Jan  9 09:00:37 2025
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Jan 2025 10:00:37 +0100
Subject: [squid-users] Using and trusting remote client IP address via
 upstream proxy
In-Reply-To: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>
References: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>
Message-ID: <Z3-QNYNjdurVYw1S@fantomas.sk>

On 08.01.25 16:33, Orion Poplawski wrote:
>We use e2guardian and squid in a combined method were requests can either go
>to e2guardian first and get forwarded to squid, or go directly to squid.
>
>I would like to be able to have squid allow connections for certain remote
>client IPs without requiring authentication.  However, the connections that
>come in through e2guardian appear to squid as coming from localhost.  Is there
>a way that e2guardian could pass the IP address of the client on to squid?

if e2guardian provides x-forwarded-for header, squid can use it:

http://www.squid-cache.org/Doc/config/follow_x_forwarded_for/

note that you should this header should be only trusted when you trust the 
client, localhost should be fine 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Enter any 12-digit prime number to continue.

From squid at borrill.org.uk  Thu Jan  9 09:03:18 2025
From: squid at borrill.org.uk (Stephen Borrill)
Date: Thu, 9 Jan 2025 09:03:18 +0000
Subject: [squid-users] Using and trusting remote client IP address via
 upstream proxy
In-Reply-To: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>
References: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>
Message-ID: <92790744-2cce-4015-b965-3983d33e94a4@borrill.org.uk>

On 08/01/2025 23:33, Orion Poplawski wrote:
> We use e2guardian and squid in a combined method were requests can either go
> to e2guardian first and get forwarded to squid, or go directly to squid.
> 
> I would like to be able to have squid allow connections for certain remote
> client IPs without requiring authentication.  However, the connections that
> come in through e2guardian appear to squid as coming from localhost.  Is there
> a way that e2guardian could pass the IP address of the client on to squid?

You don't say how you select between e2guardian and direct to squid.
You could use e2guardian in ICAP mode, so that all clients go to squid 
first and then use acls to choose which requests go via e2guardian.

You could also try adding forwardedfor = yes in e2guardian.conf along 
with follow_x_forwarded_for in your squid configuration.

-- 
Stephen


From orion at nwra.com  Thu Jan  9 21:33:27 2025
From: orion at nwra.com (Orion Poplawski)
Date: Thu, 9 Jan 2025 14:33:27 -0700
Subject: [squid-users] Using and trusting remote client IP address via
 upstream proxy
In-Reply-To: <92790744-2cce-4015-b965-3983d33e94a4@borrill.org.uk>
References: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>
 <92790744-2cce-4015-b965-3983d33e94a4@borrill.org.uk>
Message-ID: <85f22872-5b3b-4e20-9ebf-da57346c4e6c@nwra.com>

On 1/9/25 02:03, Stephen Borrill wrote:
> On 08/01/2025 23:33, Orion Poplawski wrote:
>> We use e2guardian and squid in a combined method were requests can either go
>> to e2guardian first and get forwarded to squid, or go directly to squid.
>>
>> I would like to be able to have squid allow connections for certain remote
>> client IPs without requiring authentication.? However, the connections that
>> come in through e2guardian appear to squid as coming from localhost.? Is there
>> a way that e2guardian could pass the IP address of the client on to squid?
> 
> You don't say how you select between e2guardian and direct to squid.
> You could use e2guardian in ICAP mode, so that all clients go to squid first
> and then use acls to choose which requests go via e2guardian.

It ends up not really mattering I think for this application.

> You could also try adding forwardedfor = yes in e2guardian.conf along with
> follow_x_forwarded_for in your squid configuration.

I set that in e2guardian.conf and in squid.conf I ended up with:

# Trust X-Forwarded-For from local e2g connections
follow_x_forwarded_for allow localhost
follow_x_forwarded_for allow localnet
acl_uses_indirect_client on
log_uses_indirect_client off

# Do not pass X-Forwarded-For on
forwarded_for delete

And I added the forwarded-for to the log explicitly as I do still want to
distinguish between the direct and e2g proxied connections:

logformat squidlocal %{%Y-%m-%dT%H:%M:%S}tl.%03tu%{%z}tl %6tr %>a %Ss/%03>Hs
%<st %rm %ru %[un %Sh/%<a %mt %{X-Forwarded-For}>h

Thanks to you and Matus for the suggestions.

-- 
Orion Poplawski
he/him/his  - surely the least important thing about me
Manager of IT Systems                      720-772-5637
NWRA, Boulder Office                  FAX: 303-415-9702
3380 Mitchell Lane                       orion at nwra.com
Boulder, CO 80301                 https://www.nwra.com/
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4087 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250109/35096c41/attachment.bin>

From jonathanlee571 at gmail.com  Thu Jan  9 23:24:32 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Thu, 9 Jan 2025 15:24:32 -0800
Subject: [squid-users] Cache die
Message-ID: <6030E6DA-5C3C-4BF6-B2CE-6391E7C79911@gmail.com>

After trying every setting I am still asking the same question on a SG2100MAX 4GB ram 128GB disk and a NVMe 250 Optane m.2 drive over mpcie adapter. What is recommended disk cache ? Ufs aufs or diskd? What is the recommended memory cache? 

I currently have it set to ufs 16 level 1 folders 

For memory lru 

What should it be? Diskd?


I have tried every setting from 256 level one folders to 32 to 16 every different type of memory catch types all the memory types schemes were using and I just can?t seem to find one that is performance wise the best so I thought After many years of trying that I should just email you guys .
Sent from my iPhone

From jonathanlee571 at gmail.com  Fri Jan 10 07:04:00 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Thu, 9 Jan 2025 23:04:00 -0800
Subject: [squid-users] Cache dir
In-Reply-To: <6030E6DA-5C3C-4BF6-B2CE-6391E7C79911@gmail.com>
References: <6030E6DA-5C3C-4BF6-B2CE-6391E7C79911@gmail.com>
Message-ID: <0427E7AC-341B-45C0-8926-6125DEF37F16@gmail.com>

Sorry I had my iPhone do a voice to text, I do not understand how a Tesla can drive so well but voice to text looks wrong. 

After trying every setting inside of Squid I thought I should ask I have 4bg ram and 128GB M.2 SSD onboard disk I am using a NVMe secondary Intel Optane M.2 drive for my cache. 

What is a good recommendation for Hard Drive Cache System I use UFS but AUFS inside of the Squid definitive guide says it is way faster like formula one versus UFS the options I have are UFS AUFS DISKD, I have 16 Level 1 directories my memory replacement policy is LRU for memory cache it seems to run better with that, my options for memory replacement policy are HEAP GDSF I assume any HEAP will require more memory, HEAP LFUDF, HEAP LRU and just LRU. I also have a Cache Replacement policy with the same options I have it set to Heap LFUDA that is the default. Squid Memory cache Size I have set to default 64MB with max object size 256kb for the memory, for disk I have 256GB available I only have it set to 32000MB or 3.2GB for fear of overloading the RAM when it fills up. 

for level 1 directories I can have 4,8,16,32,64,128,256 each layer one containers 256 sub directories so this could hog memory if you did 
256*256=65,536 I imagine not ok with only 4GB I have onboard memory I can?t make it any bigger. I use this with SSL intercept it does cache and works well I just want to get rid of the lag on news websites. 

rewrite process children I have it set max 25  with process children startup at 12 and idle at 8 
SSL certificate deamon children I have it set to start 10 

it runs well I have tried many different things as you know from all the emails, I am sorry it is the most fascinating software to me. Code that runs as fast as the internet. Is there any thing I can do to make it go faster? Some website have a lag fox news yahoo only do on the SSL intercept devices the splice devices never have any issues, its lighting fast for them. I thought I should finally ask after 4-5 years of doing changes. I have got it to work as fast as I can on my own, time to ask the community. 

Thanks again sorry for the weird email before. 


> On Jan 9, 2025, at 15:24, Jonathan Lee <jonathanlee571 at gmail.com> wrote:
> 
> After trying every setting I am still asking the same question on a SG2100MAX 4GB ram 128GB disk and a NVMe 250 Optane m.2 drive over mpcie adapter. What is recommended disk cache ? Ufs aufs or diskd? What is the recommended memory cache? 
> 
> I currently have it set to ufs 16 level 1 folders 
> 
> For memory lru 
> 
> What should it be? Diskd?
> 
> 
> I have tried every setting from 256 level one folders to 32 to 16 every different type of memory catch types all the memory types schemes were using and I just can?t seem to find one that is performance wise the best so I thought After many years of trying that I should just email you guys .
> Sent from my iPhone


From gkinkie at gmail.com  Fri Jan 10 08:25:28 2025
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Fri, 10 Jan 2025 08:25:28 +0000
Subject: [squid-users] Cache dir
In-Reply-To: <0427E7AC-341B-45C0-8926-6125DEF37F16@gmail.com>
References: <6030E6DA-5C3C-4BF6-B2CE-6391E7C79911@gmail.com>
 <0427E7AC-341B-45C0-8926-6125DEF37F16@gmail.com>
Message-ID: <CA+Y8hcNxyytGZ8qXsXnbmr=VdaWVXkvBgnrwjV1HDCap91XDpw@mail.gmail.com>

On Fri, Jan 10, 2025 at 7:22?AM Jonathan Lee <jonathanlee571 at gmail.com>
wrote:

>
> After trying every setting inside of Squid I thought I should ask I have
> 4bg ram and 128GB M.2 SSD onboard disk I am using a NVMe secondary Intel
> Optane M.2 drive for my cache.


What OS are you using? How many CPU cores do you want to dedicate to Squid?
How much memory?


> What is a good recommendation for Hard Drive Cache System I use UFS but
> AUFS


UFS is the slowest option; AUFS or rock are considered the fastest


> inside of the Squid definitive guide says it is way faster like formula
> one versus UFS the options I have are UFS AUFS DISKD, I have 16 Level 1
> directories


What filesystem are you using? For modern filesystems (ext4, btrfs, apfs)
this parameter is much less meaningful than 10 years ago as they store
directories as trees instead of lists.


> my memory replacement policy is LRU for memory cache it seems to run
> better with that, my options for memory replacement policy are HEAP GDSF I
> assume any HEAP will require more memory, HEAP LFUDF, HEAP LRU and just LRU.


I think so but shouldn't be significantly more


> I also have a Cache Replacement policy with the same options I have it set
> to Heap LFUDA that is the default. Squid Memory cache Size I have set to
> default 64MB with max object size 256kb for the memory, for disk I have
> 256GB available I only have it set to 32000MB or 3.2GB for fear of
> overloading the RAM when it fills up.


4gb is plenty of memory; what other workloads do you want to run on that
machine?  You can also tune these parameters after checking behaviour in
practice, no need to fix them once and for all now


> for level 1 directories I can have 4,8,16,32,64,128,256 each layer one
> containers 256 sub directories so this could hog memory if you did
> 256*256=65,536 I imagine not ok with only 4GB I have onboard memory I
> can?t make it any bigger. I use this with SSL intercept it does cache and
> works well I just want to get rid of the lag on news websites.


number of directories has no impact on memory use. Just be aware that if
you change it, you need to wipe and rebuild your cache.


> rewrite process children I have it set max 25  with process children
> startup at 12 and idle at 8
> SSL certificate deamon children I have it set to start 10


Sure.

it runs well I have tried many different things as you know from all the
> emails, I am sorry it is the most fascinating software to me. Code that
> runs as fast as the internet. Is there any thing I can do to make it go
> faster? Some website have a lag fox news yahoo only do on the SSL intercept
> devices the splice devices never have any issues, its lighting fast for
> them. I thought I should finally ask after 4-5 years of doing changes. I
> have got it to work as fast as I can on my own, time to ask the community.
>

I think it boils down mainly to how much memory you're willing to dedicate
to squid. More memory more performance. Apart from that, they main advice
is to chang from UFS to just about anything else

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250110/bffa55ea/attachment.htm>

From jonathanlee571 at gmail.com  Fri Jan 10 17:39:32 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Fri, 10 Jan 2025 09:39:32 -0800
Subject: [squid-users] Cache dir
In-Reply-To: <CA+Y8hcNxyytGZ8qXsXnbmr=VdaWVXkvBgnrwjV1HDCap91XDpw@mail.gmail.com>
References: <6030E6DA-5C3C-4BF6-B2CE-6391E7C79911@gmail.com>
 <0427E7AC-341B-45C0-8926-6125DEF37F16@gmail.com>
 <CA+Y8hcNxyytGZ8qXsXnbmr=VdaWVXkvBgnrwjV1HDCap91XDpw@mail.gmail.com>
Message-ID: <94BFC20B-CD33-40CA-8C85-D4A018D5D91B@gmail.com>

Thanks for the reply 


> What OS are you using? How many CPU cores do you want to dedicate to Squid? How much memory?

I am using FreeBSD variant 4GB ram 2 CPUs pfSense plus

Starting CPU 1 (1)
FreeBSD/SMP: Multiprocessor System Detected: 2 CPUs
cpulist0: <Open Firmware CPU Group> on ofwbus0
cpu0: <Open Firmware CPU> on cpulist0
cpu1: <Open Firmware CPU> on cpulist0
e6000sw0: CPU port at 5
CPU 0: ARM Cortex-A53 r0p4 affinity: 0
CPU 1: ARM Cortex-A53 r0p4 affinity: 1
 
I do not know how to dedicate specific CPU cores to Squid I do not think I can with pfSense plus.


> What filesystem are you using? For modern filesystems (ext4, btrfs, apfs) this parameter is much less meaningful than 10 years ago as they store directories as trees instead of lists.

=>        1  250069679  ada0  MBR  (119G)
          1     532480     1  efi  (260M)
     532481     131072     2  fat32  (64M)
     663553  249406127     3  freebsd  [active]  (119G)

=>        0  249406127  ada0s3  BSD  (119G)
          0         16          - free -  (8.0K)
         16  235528175       1  freebsd-zfs  (112G)
  235528191   13877248       2  freebsd-swap  (6.6G)
  249405439        688          - free -  (344K)

=>       40  500118112  nda0  GPT  (238G)
         40       2008        - free -  (1.0M)
       2048   16777216     1  freebsd-swap  (8.0G)
   16779264  482344960     2  freebsd-ufs  (230G)
  499124224     993928        - free -  (485M)
	ada0 is for the host os 
	nda0 is my cache is uses freebsd-ufs I use the command mount_msdosfs /dev/nda0p2 /nvme/LOGS_Octane 
	The only way to mount the NVMe drive I have found is with mount_msdosfs maybe this causes a slow down I do not know but I can write and save to the drive this way. it is on nda0p2 230GB 

Geom name: nda0
modified: false
state: OK
fwheads: 255
fwsectors: 63
last: 500118151
first: 40
entries: 128
scheme: GPT
Providers:
1. Name: nda0p1
   Mediasize: 8589934592 (8.0G)
   Sectorsize: 512
   Stripesize: 0
   Stripeoffset: 1048576
   Mode: r1w1e2
   efimedia: HD(1,GPT,04d31fb2-c0fd-11ef-8536-90ec770dda25,0x800,0x1000000)
   rawuuid: 04d31fb2-c0fd-11ef-8536-90ec770dda25
   rawtype: 516e7cb5-6ecf-11d6-8ff8-00022d09712b
   label: swapUSB
   length: 8589934592
   offset: 1048576
   type: freebsd-swap
   index: 1
   end: 16779263
   start: 2048
2. Name: nda0p2
   Mediasize: 246960619520 (230G)
   Sectorsize: 512
   Stripesize: 0
   Stripeoffset: 8590983168
   Mode: r1w1e1
   efimedia: HD(2,GPT,d84dfc00-cb1c-11ef-afd9-90ec770dda25,0x1000800,0x1cc00000)
   rawuuid: d84dfc00-cb1c-11ef-afd9-90ec770dda25
   rawtype: 516e7cb6-6ecf-11d6-8ff8-00022d09712b
   label: LOG
   length: 246960619520
   offset: 8590983168
   type: freebsd-ufs
   index: 2
   end: 499124223
   start: 16779264
Consumers:
1. Name: nda0
   Mediasize: 256060514304 (238G)
   Sectorsize: 512
   Mode: r2w2e5

>> my memory replacement policy is LRU for memory cache it seems to run better with that, my options for memory replacement policy are HEAP GDSF I assume any HEAP will require more memory, HEAP LFUDF, HEAP LRU and just LRU.
> 
> I think so but shouldn't be significantly more


Should I change Memory Replacement Policy from LRU to HEAP LRU? I have tried every one again there is also the Cache Replacement Policy: Currently set to HEAP LFUDA Should memory replacement policy and cache replacement policy both be the same, and or does one cause any performance issues with the other?

Current Config 
# This file is automatically generated by pfSense
# Do not edit manually !

http_port 192.168.1.1:3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB cert=/usr/local/etc/squid/serverkey.pem cafile=/usr/local/share/certs/ca-root-nss.crt capath=/usr/local/share/certs/ cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS tls-dh=prime256v1:/etc/dh-parameters.2048 options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE

http_port 127.0.0.1:3128 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB cert=/usr/local/etc/squid/serverkey.pem cafile=/usr/local/share/certs/ca-root-nss.crt capath=/usr/local/share/certs/ cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS tls-dh=prime256v1:/etc/dh-parameters.2048 options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE

https_port 127.0.0.1:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB cert=/usr/local/etc/squid/serverkey.pem cafile=/usr/local/share/certs/ca-root-nss.crt capath=/usr/local/share/certs/ cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS tls-dh=prime256v1:/etc/dh-parameters.2048 options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE

icp_port 0
digest_generation off
dns_v4_first on
pid_filename /var/run/squid/squid.pid
cache_effective_user squid
cache_effective_group proxy
error_default_language en
icon_directory /usr/local/etc/squid/icons
visible_hostname Lee_Family.home.arpa
cache_mgr jonathanlee571 at gmail.com
access_log /nvme/LOGS_Optane/Squid_Logs/access.log
cache_log /nvme/LOGS_Optane/Squid_Logs/cache.log
cache_store_log none
netdb_filename /nvme/LOGS_Optane/Squid_Logs/netdb.state
pinger_enable on
pinger_program /usr/local/libexec/squid/pinger
sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s /var/squid/lib/ssl_db -M 4MB -b 2048
tls_outgoing_options cafile=/usr/local/share/certs/ca-root-nss.crt
tls_outgoing_options capath=/usr/local/share/certs/
tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
tls_outgoing_options cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_children 10

logfile_rotate 10
debug_options rotate=10
shutdown_lifetime 3 seconds
# Allow local network(s) on interface(s)
acl localnet src  192.168.1.0/27
forwarded_for delete
via off
httpd_suppress_version_string on
uri_whitespace strip

acl block_hours time 00:30-05:00
ssl_bump terminate all block_hours
http_access deny all block_hours
icp_port 0
htcp_port 0
snmp_port 0
icp_access deny all
htcp_access deny all
snmp_access deny all
acl getmethod method GET
acl to_ipv6 dst ipv6
acl from_ipv6 src ipv6

#tls_outgoing_options cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
tls_outgoing_options options=NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET,SINGLE_DH_USE,SINGLE_ECDH_USE
#tls_outgoing_options default-ca=on

acl HttpAccess dstdomain '/usr/local/pkg/http.access'
acl windowsupdate dstdomain '/usr/local/pkg/windowsupdate'
#acl rewritedoms dstdomain '/usr/local/pkg/desdom'

#store_id_program /usr/local/libexec/squid/storeid_file_rewrite /var/squid/storeid/storeid_rewrite.txt
#store_id_children 10 startup=5 idle=1 concurrency=0
#always_direct allow all
#store_id_access deny connect
#store_id_access deny !getmethod
#store_id_access allow rewritedoms
#store_id_access deny all

refresh_all_ims on
reload_into_ims on
max_stale 20 years
minimum_expiry_time 0

#refresh_pattern -i ^http.*squid.internal.* 43200 100% 79900 override-expire override-lastmod ignore-reload ignore-no-store ignore-must-revalidate ignore-private ignore-auth

refresh_pattern -i windowsupdate.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
refresh_pattern -i microsoft.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
refresh_pattern -i windows.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
refresh_pattern -i microsoft.com.akadns.net/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
refresh_pattern -i deploy.akamaitechnologies.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims


acl https_login url_regex -i ^https.*(login|Login).*
cache deny https_login

#range_offset_limit 512 MB windowsupdate
range_offset_limit 0 !windowsupdate
quick_abort_min -1 KB

cache_mem 64 MB
maximum_object_size_in_memory 256 KB
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
minimum_object_size 0 KB
maximum_object_size 512 MB
cache_dir aufs /nvme/LOGS_Optane/Squid_Cache 32000 16 256
offline_mode off
cache_swap_low 90
cache_swap_high 95
acl donotcache dstdomain '/var/squid/acl/donotcache.acl'
cache deny donotcache
cache allow all
# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:    1440  20%  10080
refresh_pattern ^gopher:  1440  0%  1440
refresh_pattern -i (/cgi-bin/|?) 0  0%  0
refresh_pattern .    0  20%  4320


#Remote proxies


# Setup some default acls
# ACLs all, manager, localhost, and to_localhost are predefined.
acl allsrc src all
acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 8080 3128 3129 1025-65535 
acl sslports port 443 563 8080 5223 2197

acl purge method PURGE
acl connect method CONNECT

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS

# SslBump Peek and Splice
# http://wiki.squid-cache.org/Features/SslPeekAndSplice
# http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
# Match against the current step during ssl_bump evaluation [fast]
# Never matches and should not be used outside the ssl_bump context.
#
# At each SslBump step, Squid evaluates ssl_bump directives to find
# the next bumping action (e.g., peek or splice). Valid SslBump step
# values and the corresponding ssl_bump evaluation moments are:
#   SslBump1: After getting TCP-level and HTTP CONNECT info.
#   SslBump2: After getting TLS Client Hello info.
#   SslBump3: After getting TLS Server Hello info.
# These ACLs exist even when 'SSL/MITM Mode' is set to 'Custom' so that
# they can be used there for custom configuration.
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl banned_hosts src '/var/squid/acl/banned_hosts.acl'
acl blacklist dstdom_regex -i '/var/squid/acl/blacklist.acl'
http_access allow manager localhost

http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny !safeports
http_access deny CONNECT !sslports

# Always allow localhost connections
http_access allow localhost

quick_abort_min 0 KB
quick_abort_max 0 KB
quick_abort_pct 95
request_body_max_size 0 KB
delay_pools 1
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_initial_bucket_level 100
delay_access 1 allow allsrc

# Reverse Proxy settings

deny_info TCP_RESET allsrc

# Package Integration
url_rewrite_program /usr/local/bin/squidGuard -c /usr/local/etc/squidGuard/squidGuard.conf
url_rewrite_bypass off
url_rewrite_children 25 startup=12 idle=8 concurrency=0

# Custom options before auth
#host_verify_strict on

# These hosts are banned
http_access deny banned_hosts
# Block access to blacklist domains
http_access deny blacklist
# List of domains allowed to logging in to Google services
request_header_access X-GoogApps-Allowed-Domains deny all
request_header_add X-GoogApps-Allowed-Domains consumer_accounts
# Set YouTube safesearch restriction
acl youtubedst dstdomain -n www.youtube.com m.youtube.com youtubei.googleapis.com youtube.googleapis.com www.youtube-nocookie.com
request_header_access YouTube-Restrict deny all
request_header_add YouTube-Restrict none youtubedst
# Custom SSL/MITM options before auth
acl wpad urlpath_regex ^/wpad.dat$
acl wpad urlpath_regex ^/proxy.pac$
acl wpad urlpath_regex ^/wpad.da$
deny_info TCP_RESET wpad
#deny_info 200:/etc/squid/wpad.dat wpad
reply_header_access Content-Type deny wpad
http_access deny wpad
http_access deny !safeports
http_access deny CONNECT !sslports
cachemgr_passwd disable offline_toggle reconfigure shutdown
cachemgr_passwd redacted all
eui_lookup on
acl no_miss url_regex -i gateway.facebook.com/ws/realtime?
acl no_miss url_regex -i web-chat-e2ee.facebook.com/ws/chat
acl CONNECT method CONNECT
acl wuCONNECT dstdomain www.update.microsoft.com
acl wuCONNECT dstdomain sls.microsoft.com
http_access allow CONNECT wuCONNECT localnet
http_access allow CONNECT wuCONNECT localhost
http_access allow CONNECT windowsupdate localnet
http_access allow CONNECT windowsupdate localhost
http_access allow CONNECT HttpAccess localnet
http_access allow CONNECT HttpAccess localhost
http_access deny to_ipv6
http_access deny from_ipv6

acl BrokenButTrustedServers dstdomain '/usr/local/pkg/dstdom.broken'
acl DomainMismatch ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
sslproxy_cert_error allow BrokenButTrustedServers DomainMismatch
sslproxy_cert_error deny all

acl splice_only_ip src 192.168.1.8 
acl splice_only_ip src 192.168.1.10 
acl splice_only_ip src 192.168.1.11 
acl splice_only_ip src 192.168.1.15 
acl splice_only_ip src 192.168.1.16 
:::: = redacted mac address
acl splice_only_mac arp :::::
acl splice_only_mac arp :::::
acl splice_only_mac arp :::::
acl splice_only_mac arp :::::
acl splice_only_mac arp :::::

acl NoSSLIntercept ssl::server_name_regex -i '/usr/local/pkg/reg.url.nobump'
acl NoBumpDNS dstdomain -n '/usr/local/pkg/dns.nobump'
acl SSL_Intercept_Terminate dstdomain -n '/usr/local/pkg/url.bump'

acl active_use annotate_client active=true

acl bump_only_ip src 192.168.1.3 
acl bump_only_ip src 192.168.1.4 
acl bump_only_ip src 192.168.1.5 
#acl bump_only_ip src 192.168.1.6 
acl bump_only_ip src 192.168.1.9 
acl bump_only_ip src 192.168.1.13 

acl bump_only_mac arp :::::
acl bump_only_mac arp :::::
acl bump_only_mac arp :::::
acl bump_only_mac arp :::::
acl bump_only_mac arp :::::
#acl bump_only_mac arp :::::

coredump_dir /nvme/LOGS_Optane/Squid_Dump

acl splice_group any-of https_login NoBumpDNS NoSSLIntercept
acl splice_only_local_group all-of splice_only_mac splice_only_ip
acl splice_main any-of splice_group splice_only_local_group
acl bump_main all-of bump_only_mac bump_only_ip

ssl_bump peek step1
ssl_bump terminate SSL_Intercept_Terminate
miss_access deny no_miss active_use
ssl_bump splice splice_main active_use
ssl_bump bump bump_main active_use
acl activated note active_use true
ssl_bump terminate !activated

# Setup allowed ACLs
# Allow local network(s) on interface(s)
http_access allow localnet
# Default block all to be sure
http_access deny allsrc
Does delay pool setting cause any issues? They seem to be default values one pool. 



> On Jan 10, 2025, at 00:25, Francesco Chemolli <gkinkie at gmail.com> wrote:
> 
> 
> 
> On Fri, Jan 10, 2025 at 7:22?AM Jonathan Lee <jonathanlee571 at gmail.com <mailto:jonathanlee571 at gmail.com>> wrote:
>> 
>> After trying every setting inside of Squid I thought I should ask I have 4bg ram and 128GB M.2 SSD onboard disk I am using a NVMe secondary Intel Optane M.2 drive for my cache.
> 
> What OS are you using? How many CPU cores do you want to dedicate to Squid? How much memory?
>  
>> What is a good recommendation for Hard Drive Cache System I use UFS but AUFS
> 
> UFS is the slowest option; AUFS or rock are considered the fastest
>  
>> inside of the Squid definitive guide says it is way faster like formula one versus UFS the options I have are UFS AUFS DISKD, I have 16 Level 1 directories
> 
> What filesystem are you using? For modern filesystems (ext4, btrfs, apfs) this parameter is much less meaningful than 10 years ago as they store directories as trees instead of lists.
>  
>> my memory replacement policy is LRU for memory cache it seems to run better with that, my options for memory replacement policy are HEAP GDSF I assume any HEAP will require more memory, HEAP LFUDF, HEAP LRU and just LRU.
> 
> I think so but shouldn't be significantly more
>  
>> I also have a Cache Replacement policy with the same options I have it set to Heap LFUDA that is the default. Squid Memory cache Size I have set to default 64MB with max object size 256kb for the memory, for disk I have 256GB available I only have it set to 32000MB or 3.2GB for fear of overloading the RAM when it fills up.
> 
> 4gb is plenty of memory; what other workloads do you want to run on that machine?  You can also tune these parameters after checking behaviour in practice, no need to fix them once and for all now
>  
>> for level 1 directories I can have 4,8,16,32,64,128,256 each layer one containers 256 sub directories so this could hog memory if you did 
>> 256*256=65,536 I imagine not ok with only 4GB I have onboard memory I can?t make it any bigger. I use this with SSL intercept it does cache and works well I just want to get rid of the lag on news websites.
> 
> number of directories has no impact on memory use. Just be aware that if you change it, you need to wipe and rebuild your cache.
>  
>> rewrite process children I have it set max 25  with process children startup at 12 and idle at 8 
>> SSL certificate deamon children I have it set to start 10
> 
> Sure. 
> 
>> it runs well I have tried many different things as you know from all the emails, I am sorry it is the most fascinating software to me. Code that runs as fast as the internet. Is there any thing I can do to make it go faster? Some website have a lag fox news yahoo only do on the SSL intercept devices the splice devices never have any issues, its lighting fast for them. I thought I should finally ask after 4-5 years of doing changes. I have got it to work as fast as I can on my own, time to ask the community.
> 
> I think it boils down mainly to how much memory you're willing to dedicate to squid. More memory more performance. Apart from that, they main advice is to chang from UFS to just about anything else
>  
> --
>     Francesco
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250110/e5c0959a/attachment-0001.htm>

From jonathanlee571 at gmail.com  Fri Jan 10 22:37:44 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Fri, 10 Jan 2025 14:37:44 -0800
Subject: [squid-users] Squid url redirector and DoH
Message-ID: <71F29A4E-1F4D-4627-A2B7-2C736AFAD5CE@gmail.com>

Hello fellow Squid users, can you please help? I was wondering about this for years, I have a massive block list with DoH servers. Do you really need to block DoH if you want Squid to use a specific dns? Let?s say you are using a dns over tls, to Google or cloudflare and your system sometimes wants the DoH one.one.one.one is blocking that url really needed? My list is so big it is like playing wackamole with DoH. If I block it I see all the url requests if not I see IP addresses sometimes in the get requests. I must have a ACL with thousands and thousands of DoH servers in it. 

What is recommended with sites that want DoH however clients must use Squid per firewall ACLs? 
Sent from my iPhone

From jonathanlee571 at gmail.com  Fri Jan 10 22:54:05 2025
From: jonathanlee571 at gmail.com (jonathanlee571 at gmail.com)
Date: Fri, 10 Jan 2025 14:54:05 -0800
Subject: [squid-users] Squid url redirector and DoH
In-Reply-To: <71F29A4E-1F4D-4627-A2B7-2C736AFAD5CE@gmail.com>
References: <71F29A4E-1F4D-4627-A2B7-2C736AFAD5CE@gmail.com>
Message-ID: <036201db63b2$8d240c40$a76c24c0$@gmail.com>

I have this hair brained idea to use the media type and get rid of the endless list. 

Could this work?

https://www.iana.org/assignments/media-types/media-types.xhtml

This lists mime types for doh with rfc 8484 and 8427 so technically could I just create a mime block for DoH and stop creating endless lists?

https://www.iana.org/assignments/media-types/application/dns-message
https://www.iana.org/assignments/media-types/application/dns+json

https://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes



-----Original Message-----
From: Jonathan Lee <jonathanlee571 at gmail.com> 
Sent: Friday, January 10, 2025 2:38 PM
To: squid-users <squid-users at lists.squid-cache.org>
Subject: Squid url redirector and DoH

Hello fellow Squid users, can you please help? I was wondering about this for years, I have a massive block list with DoH servers. Do you really need to block DoH if you want Squid to use a specific dns? Let?s say you are using a dns over tls, to Google or cloudflare and your system sometimes wants the DoH one.one.one.one is blocking that url really needed? My list is so big it is like playing wackamole with DoH. If I block it I see all the url requests if not I see IP addresses sometimes in the get requests. I must have a ACL with thousands and thousands of DoH servers in it. 

What is recommended with sites that want DoH however clients must use Squid per firewall ACLs? 
Sent from my iPhone


From jonathanlee571 at gmail.com  Fri Jan 10 23:04:29 2025
From: jonathanlee571 at gmail.com (jonathanlee571 at gmail.com)
Date: Fri, 10 Jan 2025 15:04:29 -0800
Subject: [squid-users] Squid url redirector and DoH
In-Reply-To: <036201db63b2$8d240c40$a76c24c0$@gmail.com>
References: <71F29A4E-1F4D-4627-A2B7-2C736AFAD5CE@gmail.com>
 <036201db63b2$8d240c40$a76c24c0$@gmail.com>
Message-ID: <0ef201db63b4$01194ce0$034be6a0$@gmail.com>

acl deny_rep_mime_doh rep_mime_type application/dns-message

for example would this work? I could get rid of a huge list and save memory if this solves my wackamole problem. I do not see anything on the Squid website but in theory that could resolve it right?

-----Original Message-----
From: jonathanlee571 at gmail.com <jonathanlee571 at gmail.com> 
Sent: Friday, January 10, 2025 2:54 PM
To: 'squid-users' <squid-users at lists.squid-cache.org>
Subject: RE: Squid url redirector and DoH

I have this hair brained idea to use the media type and get rid of the endless list. 

Could this work?

https://www.iana.org/assignments/media-types/media-types.xhtml

This lists mime types for doh with rfc 8484 and 8427 so technically could I just create a mime block for DoH and stop creating endless lists?

https://www.iana.org/assignments/media-types/application/dns-message
https://www.iana.org/assignments/media-types/application/dns+json

https://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes



-----Original Message-----
From: Jonathan Lee <jonathanlee571 at gmail.com> 
Sent: Friday, January 10, 2025 2:38 PM
To: squid-users <squid-users at lists.squid-cache.org>
Subject: Squid url redirector and DoH

Hello fellow Squid users, can you please help? I was wondering about this for years, I have a massive block list with DoH servers. Do you really need to block DoH if you want Squid to use a specific dns? Let?s say you are using a dns over tls, to Google or cloudflare and your system sometimes wants the DoH one.one.one.one is blocking that url really needed? My list is so big it is like playing wackamole with DoH. If I block it I see all the url requests if not I see IP addresses sometimes in the get requests. I must have a ACL with thousands and thousands of DoH servers in it. 

What is recommended with sites that want DoH however clients must use Squid per firewall ACLs? 
Sent from my iPhone



From jonathanlee571 at gmail.com  Fri Jan 10 23:19:29 2025
From: jonathanlee571 at gmail.com (jonathanlee571 at gmail.com)
Date: Fri, 10 Jan 2025 15:19:29 -0800
Subject: [squid-users] Squid url redirector and DoH
In-Reply-To: <036201db63b2$8d240c40$a76c24c0$@gmail.com>
References: <71F29A4E-1F4D-4627-A2B7-2C736AFAD5CE@gmail.com>
 <036201db63b2$8d240c40$a76c24c0$@gmail.com>
Message-ID: <0ef301db63b6$198c3ba0$4ca4b2e0$@gmail.com>

Last email on this

Can this be beneficial.... set all the dns over http to mime types and block them per rfc documents. 

acl deny_rep_mime_doh rep_mime_type application/dns-message
acl deny_rep_mime_doh rep_mime_type text/dns
acl deny_rep_mime_doh rep_mime_type application/dns+json
http_reply_access deny deny_rep_mime_doh

-----Original Message-----
From: jonathanlee571 at gmail.com <jonathanlee571 at gmail.com> 
Sent: Friday, January 10, 2025 2:54 PM
To: 'squid-users' <squid-users at lists.squid-cache.org>
Subject: RE: Squid url redirector and DoH

I have this hair brained idea to use the media type and get rid of the endless list. 

Could this work?

https://www.iana.org/assignments/media-types/media-types.xhtml

This lists mime types for doh with rfc 8484 and 8427 so technically could I just create a mime block for DoH and stop creating endless lists?

https://www.iana.org/assignments/media-types/application/dns-message
https://www.iana.org/assignments/media-types/application/dns+json

https://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes



-----Original Message-----
From: Jonathan Lee <jonathanlee571 at gmail.com> 
Sent: Friday, January 10, 2025 2:38 PM
To: squid-users <squid-users at lists.squid-cache.org>
Subject: Squid url redirector and DoH

Hello fellow Squid users, can you please help? I was wondering about this for years, I have a massive block list with DoH servers. Do you really need to block DoH if you want Squid to use a specific dns? Let?s say you are using a dns over tls, to Google or cloudflare and your system sometimes wants the DoH one.one.one.one is blocking that url really needed? My list is so big it is like playing wackamole with DoH. If I block it I see all the url requests if not I see IP addresses sometimes in the get requests. I must have a ACL with thousands and thousands of DoH servers in it. 

What is recommended with sites that want DoH however clients must use Squid per firewall ACLs? 
Sent from my iPhone



From squid3 at treenet.co.nz  Sat Jan 11 04:35:53 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Jan 2025 17:35:53 +1300
Subject: [squid-users] Using and trusting remote client IP address via
 upstream proxy
In-Reply-To: <85f22872-5b3b-4e20-9ebf-da57346c4e6c@nwra.com>
References: <c69d3967-99dc-415d-a13e-bc583e6c2a3e@nwra.com>
 <92790744-2cce-4015-b965-3983d33e94a4@borrill.org.uk>
 <85f22872-5b3b-4e20-9ebf-da57346c4e6c@nwra.com>
Message-ID: <66501a18-05b8-4146-a8fd-3a3fc3719455@treenet.co.nz>



On 10/01/25 10:33, Orion Poplawski wrote:
> On 1/9/25 02:03, Stephen Borrill wrote:
>> On 08/01/2025 23:33, Orion Poplawski wrote:
> 
>> You could also try adding forwardedfor = yes in e2guardian.conf along with
>> follow_x_forwarded_for in your squid configuration.
> 
> I set that in e2guardian.conf and in squid.conf I ended up with:
> 
> # Trust X-Forwarded-For from local e2g connections
> follow_x_forwarded_for allow localhost

This is fine, assuming that e2guardian is connecting to Squid *from* 
localhost IP.


> follow_x_forwarded_for allow localnet


This you should not do. It will allow any client on your LAN to make 
Squid log any fake IP they want.

If e2guardian is contacting Squid *from* a LAN IP address you should 
create an ACL containing only that IP for the XFF allow action.



Cheers
Amos


From gkinkie at gmail.com  Sat Jan 11 13:18:22 2025
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Sat, 11 Jan 2025 13:18:22 +0000
Subject: [squid-users] Squid url redirector and DoH
In-Reply-To: <0ef201db63b4$01194ce0$034be6a0$@gmail.com>
References: <71F29A4E-1F4D-4627-A2B7-2C736AFAD5CE@gmail.com>
 <036201db63b2$8d240c40$a76c24c0$@gmail.com>
 <0ef201db63b4$01194ce0$034be6a0$@gmail.com>
Message-ID: <CA+Y8hcPNG7H0rNh3p1cRpRgdYNqJBOPsQP=oSFNZWCofDsYC0A@mail.gmail.com>

Hi Jonathan,
  the problem is: can you even see the HTTP being exchanged?
This requires TLS interception.

If you can, then it's relatively easy: you can to filter on (untested)

acl doh_post_ct Content-Type -i application/dns-message
acl doh_path_rfc8484 urlpath_regex ^/dns-query
acl doh_query_rfc8484 urlpath_regex dns=
acl doh_path_json urlpath_regex ^/resolve

http_access deny doh_post_ct doh_path_json
http_access deny doh_path_rfc8484 doh_query_rfc8484

If, however, you cannot inspect the HTTP payload in TLS, your only option
is to blacklist all DOH providers by DNS name

On Sat, Jan 11, 2025 at 1:32?AM <jonathanlee571 at gmail.com> wrote:

> acl deny_rep_mime_doh rep_mime_type application/dns-message
>
> for example would this work? I could get rid of a huge list and save
> memory if this solves my wackamole problem. I do not see anything on the
> Squid website but in theory that could resolve it right?
>
> -----Original Message-----
> From: jonathanlee571 at gmail.com <jonathanlee571 at gmail.com>
> Sent: Friday, January 10, 2025 2:54 PM
> To: 'squid-users' <squid-users at lists.squid-cache.org>
> Subject: RE: Squid url redirector and DoH
>
> I have this hair brained idea to use the media type and get rid of the
> endless list.
>
> Could this work?
>
> https://www.iana.org/assignments/media-types/media-types.xhtml
>
> This lists mime types for doh with rfc 8484 and 8427 so technically could
> I just create a mime block for DoH and stop creating endless lists?
>
> https://www.iana.org/assignments/media-types/application/dns-message
> https://www.iana.org/assignments/media-types/application/dns+json
>
> https://wiki.squid-cache.org/ConfigExamples/BlockingMimeTypes
>
>
>
> -----Original Message-----
> From: Jonathan Lee <jonathanlee571 at gmail.com>
> Sent: Friday, January 10, 2025 2:38 PM
> To: squid-users <squid-users at lists.squid-cache.org>
> Subject: Squid url redirector and DoH
>
> Hello fellow Squid users, can you please help? I was wondering about this
> for years, I have a massive block list with DoH servers. Do you really need
> to block DoH if you want Squid to use a specific dns? Let?s say you are
> using a dns over tls, to Google or cloudflare and your system sometimes
> wants the DoH one.one.one.one is blocking that url really needed? My list
> is so big it is like playing wackamole with DoH. If I block it I see all
> the url requests if not I see IP addresses sometimes in the get requests. I
> must have a ACL with thousands and thousands of DoH servers in it.
>
> What is recommended with sites that want DoH however clients must use
> Squid per firewall ACLs?
> Sent from my iPhone
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250111/e1a67960/attachment.htm>

From gkinkie at gmail.com  Sat Jan 11 13:24:43 2025
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Sat, 11 Jan 2025 13:24:43 +0000
Subject: [squid-users] Cache dir
In-Reply-To: <94BFC20B-CD33-40CA-8C85-D4A018D5D91B@gmail.com>
References: <6030E6DA-5C3C-4BF6-B2CE-6391E7C79911@gmail.com>
 <0427E7AC-341B-45C0-8926-6125DEF37F16@gmail.com>
 <CA+Y8hcNxyytGZ8qXsXnbmr=VdaWVXkvBgnrwjV1HDCap91XDpw@mail.gmail.com>
 <94BFC20B-CD33-40CA-8C85-D4A018D5D91B@gmail.com>
Message-ID: <CA+Y8hcOYcrKcFdYFOf9YUuSFgDua7tGbwtWEHd0UG7mc9wOu7A@mail.gmail.com>

On Fri, Jan 10, 2025 at 5:39?PM Jonathan Lee <jonathanlee571 at gmail.com>
wrote:

> Thanks for the reply
>
>
> What OS are you using? How many CPU cores do you want to dedicate to
> Squid? How much memory?
>
>
> I am using FreeBSD variant 4GB ram 2 CPUs pfSense plus
>
> Starting CPU 1 (1)
> FreeBSD/SMP: Multiprocessor System Detected: 2 CPUs
> cpulist0: <Open Firmware CPU Group> on ofwbus0
> cpu0: <Open Firmware CPU> on cpulist0
> cpu1: <Open Firmware CPU> on cpulist0
> e6000sw0: CPU port at 5
> CPU 0: ARM Cortex-A53 r0p4 affinity: 0
> CPU 1: ARM Cortex-A53 r0p4 affinity: 1
> I do not know how to dedicate specific CPU cores to Squid I do not think I
> can with pfSense plus.
>

It's more about how many squid workers to start. Then the OS will do the
allocation


> What filesystem are you using? For modern filesystems (ext4, btrfs, apfs)
> this parameter is much less meaningful than 10 years ago as they store
> directories as trees instead of lists.
>
>
> =>        1  250069679  ada0  MBR  (119G)
>           1     532480     1  efi  (260M)
>      532481     131072     2  fat32  (64M)
>      663553  249406127     3  freebsd  [active]  (119G)
>
> =>        0  249406127  ada0s3  BSD  (119G)
>           0         16          - free -  (8.0K)
>          16  235528175       1  freebsd-zfs  (112G)
>   235528191   13877248       2  freebsd-swap  (6.6G)
>   249405439        688          - free -  (344K)
>
> =>       40  500118112  nda0  GPT  (238G)
>          40       2008        - free -  (1.0M)
>        2048   16777216     1  freebsd-swap  (8.0G)
>    16779264  482344960     2  freebsd-ufs  (230G)
>   499124224     993928        - free -  (485M)
>
> ada0 is for the host os
> nda0 is my cache is uses freebsd-ufs I use the command *mount_msdosfs
> /dev/nda0p2 /nvme/**LOGS_Octane*
>

huh? From the output above, it would seem that nda0 is using FreeBD UFS,
not msdos.
Might be related to the partition table type you're using? I'm not a
freebsd expert tho


> The only way to mount the NVMe drive I have found is with mount_msdosfs
> maybe this causes a slow down I do not know but I can write and save to the
> drive this way. it is on nda0p2 230GB
>
> Geom name: nda0
> modified: false
> state: OK
> fwheads: 255
> fwsectors: 63
> last: 500118151
> first: 40
> entries: 128
> scheme: GPT
> Providers:
> 1. Name: nda0p1
>    Mediasize: 8589934592 (8.0G)
>    Sectorsize: 512
>    Stripesize: 0
>    Stripeoffset: 1048576
>    Mode: r1w1e2
>    efimedia: HD(1,GPT,04d31fb2-c0fd-11ef-8536-90ec770dda25,0x800,0x1000000)
>    rawuuid: 04d31fb2-c0fd-11ef-8536-90ec770dda25
>    rawtype: 516e7cb5-6ecf-11d6-8ff8-00022d09712b
>    label: swapUSB
>    length: 8589934592
>    offset: 1048576
>    type: freebsd-swap
>    index: 1
>    end: 16779263
>    start: 2048
> 2. Name: nda0p2
>    Mediasize: 246960619520 (230G)
>    Sectorsize: 512
>    Stripesize: 0
>    Stripeoffset: 8590983168
>    Mode: r1w1e1
>    efimedia: HD(2,GPT,d84dfc00-cb1c-11ef-afd9-90ec770dda25,0x1000800,0x1cc00000)
>    rawuuid: d84dfc00-cb1c-11ef-afd9-90ec770dda25
>    rawtype: 516e7cb6-6ecf-11d6-8ff8-00022d09712b
>    label: LOG
>    length: 246960619520
>    offset: 8590983168
>    type: freebsd-ufs
>    index: 2
>    end: 499124223
>    start: 16779264
> Consumers:
> 1. Name: nda0
>    Mediasize: 256060514304 (238G)
>    Sectorsize: 512
>    Mode: r2w2e5
>
>
> my memory replacement policy is LRU for memory cache it seems to run
>> better with that, my options for memory replacement policy are HEAP GDSF I
>> assume any HEAP will require more memory, HEAP LFUDF, HEAP LRU and just LRU.
>
>
> I think so but shouldn't be significantly more
>
>
> Should I change Memory Replacement Policy from LRU to HEAP LRU? I have
> tried every one again there is also the Cache Replacement Policy: Currently
> set to HEAP LFUDA Should memory replacement policy and cache replacement
> policy both be the same, and or does one cause any performance issues
> with the other?
>

They should not be too different in terms of memory useage.


>
> Current Config
>
> # This file is automatically generated by pfSense
> # Do not edit manually !
>
> http_port 192.168.1.1:3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB cert=/usr/local/etc/squid/serverkey.pem cafile=/usr/local/share/certs/ca-root-nss.crt capath=/usr/local/share/certs/ cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS tls-dh=prime256v1:/etc/dh-parameters.2048 options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
>
> http_port 127.0.0.1:3128 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB cert=/usr/local/etc/squid/serverkey.pem cafile=/usr/local/share/certs/ca-root-nss.crt capath=/usr/local/share/certs/ cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS tls-dh=prime256v1:/etc/dh-parameters.2048 options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
>
> https_port 127.0.0.1:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=20MB cert=/usr/local/etc/squid/serverkey.pem cafile=/usr/local/share/certs/ca-root-nss.crt capath=/usr/local/share/certs/ cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS tls-dh=prime256v1:/etc/dh-parameters.2048 options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
>
> icp_port 0
> digest_generation off
> dns_v4_first on
> pid_filename /var/run/squid/squid.pid
> cache_effective_user squid
> cache_effective_group proxy
> error_default_language en
> icon_directory /usr/local/etc/squid/icons
> visible_hostname Lee_Family.home.arpa
> cache_mgr jonathanlee571 at gmail.com
> access_log /nvme/LOGS_Optane/Squid_Logs/access.log
> cache_log /nvme/LOGS_Optane/Squid_Logs/cache.log
> cache_store_log none
> netdb_filename /nvme/LOGS_Optane/Squid_Logs/netdb.state
> pinger_enable on
> pinger_program /usr/local/libexec/squid/pinger
> sslcrtd_program /usr/local/libexec/squid/security_file_certgen -s /var/squid/lib/ssl_db -M 4MB -b 2048
> tls_outgoing_options cafile=/usr/local/share/certs/ca-root-nss.crt
> tls_outgoing_options capath=/usr/local/share/certs/
> tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
> tls_outgoing_options cipher=EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslcrtd_children 10
>
> logfile_rotate 10
> debug_options rotate=10
> shutdown_lifetime 3 seconds
> # Allow local network(s) on interface(s)
> acl localnet src  192.168.1.0/27
> forwarded_for delete
> via off
> httpd_suppress_version_string on
> uri_whitespace strip
>
> acl block_hours time 00:30-05:00
> ssl_bump terminate all block_hours
> http_access deny all block_hours
> icp_port 0
> htcp_port 0
> snmp_port 0
> icp_access deny all
> htcp_access deny all
> snmp_access deny all
> acl getmethod method GET
> acl to_ipv6 dst ipv6
> acl from_ipv6 src ipv6
>
> #tls_outgoing_options cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> tls_outgoing_options options=NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET,SINGLE_DH_USE,SINGLE_ECDH_USE
> #tls_outgoing_options default-ca=on
>
> acl HttpAccess dstdomain '/usr/local/pkg/http.access'
> acl windowsupdate dstdomain '/usr/local/pkg/windowsupdate'
> #acl rewritedoms dstdomain '/usr/local/pkg/desdom'
>
> #store_id_program /usr/local/libexec/squid/storeid_file_rewrite /var/squid/storeid/storeid_rewrite.txt
> #store_id_children 10 startup=5 idle=1 concurrency=0
> #always_direct allow all
> #store_id_access deny connect
> #store_id_access deny !getmethod
> #store_id_access allow rewritedoms
> #store_id_access deny all
>
> refresh_all_ims on
> reload_into_ims on
> max_stale 20 years
> minimum_expiry_time 0
>
> #refresh_pattern -i ^http.*squid.internal.* 43200 100% 79900 override-expire override-lastmod ignore-reload ignore-no-store ignore-must-revalidate ignore-private ignore-auth
>
> refresh_pattern -i windowsupdate.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) <http://windowsupdate.com/.*.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)> 43200 80% 129600 reload-into-ims
> refresh_pattern -i microsoft.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) <http://microsoft.com/.*.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)> 43200 80% 129600 reload-into-ims
> refresh_pattern -i windows.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) <http://windows.com/.*.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)> 43200 80% 129600 reload-into-ims
> refresh_pattern -i microsoft.com.akadns.net/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) <http://microsoft.com.akadns.net/.*.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)> 43200 80% 129600 reload-into-ims
> refresh_pattern -i deploy.akamaitechnologies.com/.*.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) <http://deploy.akamaitechnologies.com/.*.(cab%7Cexe%7Cms%5Bi%7Cu%7Cf%7Cp%5D%7C%5Bap%5Dsf%7Cwm%5Bv%7Ca%5D%7Cdat%7Czip%7Cpsf)> 43200 80% 129600 reload-into-ims
>
>
> acl https_login url_regex -i ^https.*(login|Login).*
> cache deny https_login
>
> #range_offset_limit 512 MB windowsupdate
> range_offset_limit 0 !windowsupdate
> quick_abort_min -1 KB
> cache_mem 64 MB
> maximum_object_size_in_memory 256 KB
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> minimum_object_size 0 KB
> maximum_object_size 512 MBcache_dir aufs /nvme/LOGS_Optane/Squid_Cache 32000 16 256
> offline_mode off
> cache_swap_low 90
> cache_swap_high 95
> acl donotcache dstdomain '/var/squid/acl/donotcache.acl'
> cache deny donotcache
> cache allow all
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:    1440  20%  10080
> refresh_pattern ^gopher:  1440  0%  1440
> refresh_pattern -i (/cgi-bin/|?) 0  0%  0
> refresh_pattern .    0  20%  4320
>
>
> #Remote proxies
>
>
> # Setup some default acls
> # ACLs all, manager, localhost, and to_localhost are predefined.
> acl allsrc src all
> acl safeports port 21 70 80 210 280 443 488 563 591 631 777 901 8080 3128 3129 1025-65535
> acl sslports port 443 563 8080 5223 2197
>
> acl purge method PURGE
> acl connect method CONNECT
>
> # Define protocols used for redirects
> acl HTTP proto HTTP
> acl HTTPS proto HTTPS
>
> # SslBump Peek and Splice
> # http://wiki.squid-cache.org/Features/SslPeekAndSplice
> # http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> # Match against the current step during ssl_bump evaluation [fast]
> # Never matches and should not be used outside the ssl_bump context.
> #
> # At each SslBump step, Squid evaluates ssl_bump directives to find
> # the next bumping action (e.g., peek or splice). Valid SslBump step
> # values and the corresponding ssl_bump evaluation moments are:
> #   SslBump1: After getting TCP-level and HTTP CONNECT info.
> #   SslBump2: After getting TLS Client Hello info.
> #   SslBump3: After getting TLS Server Hello info.
> # These ACLs exist even when 'SSL/MITM Mode' is set to 'Custom' so that
> # they can be used there for custom configuration.
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> acl banned_hosts src '/var/squid/acl/banned_hosts.acl'
> acl blacklist dstdom_regex -i '/var/squid/acl/blacklist.acl'
> http_access allow manager localhost
>
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge
> http_access deny !safeports
> http_access deny CONNECT !sslports
>
> # Always allow localhost connections
> http_access allow localhost
>
> quick_abort_min 0 KB
> quick_abort_max 0 KB
> quick_abort_pct 95
> request_body_max_size 0 KBdelay_pools 1
> delay_class 1 2
> delay_parameters 1 -1/-1 -1/-1
> delay_initial_bucket_level 100
> delay_access 1 allow allsrc
>
> # Reverse Proxy settings
>
> deny_info TCP_RESET allsrc
>
> # Package Integration
> url_rewrite_program /usr/local/bin/squidGuard -c /usr/local/etc/squidGuard/squidGuard.conf
> url_rewrite_bypass off
> url_rewrite_children 25 startup=12 idle=8 concurrency=0
>
> # Custom options before auth
> #host_verify_strict on
>
> # These hosts are banned
> http_access deny banned_hosts
> # Block access to blacklist domains
> http_access deny blacklist
> # List of domains allowed to logging in to Google services
> request_header_access X-GoogApps-Allowed-Domains deny all
> request_header_add X-GoogApps-Allowed-Domains consumer_accounts
> # Set YouTube safesearch restriction
> acl youtubedst dstdomain -n www.youtube.com m.youtube.com youtubei.googleapis.com youtube.googleapis.com www.youtube-nocookie.com
> request_header_access YouTube-Restrict deny all
> request_header_add YouTube-Restrict none youtubedst
> # Custom SSL/MITM options before auth
> acl wpad urlpath_regex ^/wpad.dat$
> acl wpad urlpath_regex ^/proxy.pac$
> acl wpad urlpath_regex ^/wpad.da$
> deny_info TCP_RESET wpad
> #deny_info 200:/etc/squid/wpad.dat wpad
> reply_header_access Content-Type deny wpad
> http_access deny wpad
> http_access deny !safeports
> http_access deny CONNECT !sslports
> cachemgr_passwd disable offline_toggle reconfigure shutdown
> cachemgr_passwd redacted all
> eui_lookup on
> acl no_miss url_regex -i gateway.facebook.com/ws/realtime?
> acl no_miss url_regex -i web-chat-e2ee.facebook.com/ws/chat
> acl CONNECT method CONNECT
> acl wuCONNECT dstdomain www.update.microsoft.com
> acl wuCONNECT dstdomain sls.microsoft.com
> http_access allow CONNECT wuCONNECT localnet
> http_access allow CONNECT wuCONNECT localhost
> http_access allow CONNECT windowsupdate localnet
> http_access allow CONNECT windowsupdate localhost
> http_access allow CONNECT HttpAccess localnet
> http_access allow CONNECT HttpAccess localhost
> http_access deny to_ipv6
> http_access deny from_ipv6
>
> acl BrokenButTrustedServers dstdomain '/usr/local/pkg/dstdom.broken'
> acl DomainMismatch ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
> sslproxy_cert_error allow BrokenButTrustedServers DomainMismatch
> sslproxy_cert_error deny all
>
> acl splice_only_ip src 192.168.1.8
> acl splice_only_ip src 192.168.1.10
> acl splice_only_ip src 192.168.1.11
> acl splice_only_ip src 192.168.1.15
> acl splice_only_ip src 192.168.1.16
> :::: = redacted mac address
> acl splice_only_mac arp :::::
> acl splice_only_mac arp :::::
> acl splice_only_mac arp :::::
> acl splice_only_mac arp :::::
> acl splice_only_mac arp :::::
>
> acl NoSSLIntercept ssl::server_name_regex -i '/usr/local/pkg/reg.url.nobump'
> acl NoBumpDNS dstdomain -n '/usr/local/pkg/dns.nobump'
> acl SSL_Intercept_Terminate dstdomain -n '/usr/local/pkg/url.bump'
>
> acl active_use annotate_client active=true
>
> acl bump_only_ip src 192.168.1.3
> acl bump_only_ip src 192.168.1.4
> acl bump_only_ip src 192.168.1.5
> #acl bump_only_ip src 192.168.1.6
> acl bump_only_ip src 192.168.1.9
> acl bump_only_ip src 192.168.1.13
>
> acl bump_only_mac arp :::::
> acl bump_only_mac arp :::::
> acl bump_only_mac arp :::::
> acl bump_only_mac arp :::::
> acl bump_only_mac arp :::::
> #acl bump_only_mac arp :::::
>
> coredump_dir /nvme/LOGS_Optane/Squid_Dump
>
> acl splice_group any-of https_login NoBumpDNS NoSSLIntercept
> acl splice_only_local_group all-of splice_only_mac splice_only_ip
> acl splice_main any-of splice_group splice_only_local_group
> acl bump_main all-of bump_only_mac bump_only_ip
>
> ssl_bump peek step1
> ssl_bump terminate SSL_Intercept_Terminate
> miss_access deny no_miss active_use
> ssl_bump splice splice_main active_use
> ssl_bump bump bump_main active_use
> acl activated note active_use true
> ssl_bump terminate !activated
>
> # Setup allowed ACLs
> # Allow local network(s) on interface(s)
> http_access allow localnet
> # Default block all to be sure
> http_access deny allsrc
>
> Does delay pool setting cause any issues? They seem to be default values
> one pool.
>

If you need them, you need them.
There doesn't seem to be anything obviously wrong with the highlighted
parts of your config, except maybe that the memory cache is tiny.
How much RAM is the OS reporting that squid is using after a few hours or
days of use?


-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250111/e71ccd49/attachment-0001.htm>

From jonathanlee571 at gmail.com  Mon Jan 13 07:46:35 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Sun, 12 Jan 2025 23:46:35 -0800
Subject: [squid-users] Cache dir
In-Reply-To: <CA+Y8hcOYcrKcFdYFOf9YUuSFgDua7tGbwtWEHd0UG7mc9wOu7A@mail.gmail.com>
References: <CA+Y8hcOYcrKcFdYFOf9YUuSFgDua7tGbwtWEHd0UG7mc9wOu7A@mail.gmail.com>
Message-ID: <2A197C78-B1DD-4CDD-A384-36E5C57BA952@gmail.com>

It's more about how many squid workers to start. Then the OS will do the allocation

Thanks for the reply I only have one worker. 

I can?t do workers 3 on my system because I would have to disable the cache as it won?t do rock cache. This system does not support rock cache.

Worker directive does work with cache disabled but if I have it set to diskd and enable workers, I also have to increase buffering to do that. Can I add workers on diskd? It says assertion failed: Controller cc:930 failed if I try.  So I assume I am restricted to use disk cache or use workers right? 

net.local.dgram.recvspace: 262144
net.local.dgram.maxdgram: 16384




Sent from my iPhone

> On Jan 12, 2025, at 17:42, Francesco Chemolli <gkinkie at gmail.com> wrote:
> 
> It's more about how many squid workers to start. Then the OS will do the allocation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250112/0a7f7730/attachment.htm>

From jonathanlee571 at gmail.com  Mon Jan 13 08:07:56 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Mon, 13 Jan 2025 00:07:56 -0800
Subject: [squid-users] Squid url redirector and DoH
In-Reply-To: <CA+Y8hcPNG7H0rNh3p1cRpRgdYNqJBOPsQP=oSFNZWCofDsYC0A@mail.gmail.com>
References: <CA+Y8hcPNG7H0rNh3p1cRpRgdYNqJBOPsQP=oSFNZWCofDsYC0A@mail.gmail.com>
Message-ID: <4AD4E4FA-BA77-4946-9A94-D9B74C7469D7@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250113/510fdb29/attachment.htm>

From jonathanlee571 at gmail.com  Mon Jan 13 18:29:50 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Mon, 13 Jan 2025 10:29:50 -0800
Subject: [squid-users] Squid workers on non cache dir rock system
Message-ID: <59DB7C92-F570-483B-8F41-B73F91BBCD5A@gmail.com>

Hello fellow squid users, can you please help?

Is there anyway to use more workers on a non rock system, without disabling the cache? I can use them when cache is disabled. Without it I get assertion failed: controller:cc:930: EX&quot

I researched this and found you can only use workers with rock cache, is there a workaround ? My system says this proxy does not support rock and coss
Sent from my iPhone

From rousskov at measurement-factory.com  Mon Jan 13 19:17:01 2025
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 13 Jan 2025 14:17:01 -0500
Subject: [squid-users] Squid workers on non cache dir rock system
In-Reply-To: <59DB7C92-F570-483B-8F41-B73F91BBCD5A@gmail.com>
References: <59DB7C92-F570-483B-8F41-B73F91BBCD5A@gmail.com>
Message-ID: <40e3397c-0044-4ac1-a06c-824329414a5b@measurement-factory.com>

On 2025-01-13 13:29, Jonathan Lee wrote:

> Is there anyway to use more workers on a non rock system, without
> disabling the cache? I can use them when cache is disabled. Without
> it I get assertion failed: controller:cc:930: EX&quot


I will try to clarify in hope to reduce misunderstanding, especially by 
others reading the above question outside your environment context:

1. Using multiple workers without cache_dir is supported. This includes 
both memory-caching and non-caching Squid configurations.

2. Using multiple workers with rock cache_dir is also supported.

3. Using multiple workers with non-rock cache_dir is not supported. For 
some definitions of "work", in some deployment environments, one may be 
able to use such unsupported configurations (e.g., by using 
configuration macros to create worker-specific cache_dirs), but I do not 
recommend those unsupported tricks and am not going to detail them here. 
Sooner or later, those tricks may stop working without notice and with 
bad side effects.

4. Using a single worker with rock or ufs cache_dir is supported. Some 
believe that aufs and diskd cache_dirs are also supported. I am not 
going to argue, but do not recommend switching to those cache_dir types.


> My system says this proxy does not support rock and coss

Is that a Squid-generated error/message or an error/message generated by 
some other software? If it is the former, please quote the exact Squid 
message. If it is the latter, perhaps the authors of that other software 
can help you enable rock cache_dir?


N.B. Modern Squids do not support COSS cache_dirs.


HTH,

Alex.


From dwd at cern.ch  Mon Jan 13 22:35:49 2025
From: dwd at cern.ch (Dave Dykstra)
Date: Mon, 13 Jan 2025 16:35:49 -0600
Subject: [squid-users] Squid workers on non cache dir rock system
In-Reply-To: <40e3397c-0044-4ac1-a06c-824329414a5b@measurement-factory.com>
References: <59DB7C92-F570-483B-8F41-B73F91BBCD5A@gmail.com>
 <40e3397c-0044-4ac1-a06c-824329414a5b@measurement-factory.com>
Message-ID: <Z4WVRd3d0TPNBu8B@cern.ch>

On Mon, Jan 13, 2025 at 02:17:01PM -0500, Alex Rousskov wrote:
> On 2025-01-13 13:29, Jonathan Lee wrote:
...
> > Is there anyway to use more workers on a non rock system, without
> > disabling the cache? I can use them when cache is disabled. Without
> > it I get assertion failed: controller:cc:930: EX&quot
...
> 3. Using multiple workers with non-rock cache_dir is not supported. For 
> some definitions of "work", in some deployment environments, one may be 
> able to use such unsupported configurations (e.g., by using 
> configuration macros to create worker-specific cache_dirs), but I do not 
> recommend those unsupported tricks and am not going to detail them here. 
> Sooner or later, those tricks may stop working without notice and with 
> bad side effects.

But in case you want to try it anyway, see the ${process_number} macro.
    https://github.com/squid-cache/squid/blob/v6/src/cf.data.pre#L123

It has so far worked for me with ufs cache and enabled a far greater use
of full machines without the need for rock cache, which I can't use due
to https://bugs.squid-cache.org/show_bug.cgi?id=4890.

Dave

From squid3 at treenet.co.nz  Tue Jan 14 02:34:12 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Jan 2025 15:34:12 +1300
Subject: [squid-users] Cache dir
In-Reply-To: <2A197C78-B1DD-4CDD-A384-36E5C57BA952@gmail.com>
References: <CA+Y8hcOYcrKcFdYFOf9YUuSFgDua7tGbwtWEHd0UG7mc9wOu7A@mail.gmail.com>
 <2A197C78-B1DD-4CDD-A384-36E5C57BA952@gmail.com>
Message-ID: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>

On 13/01/25 20:46, Jonathan Lee wrote:
> It's more about how many squid workers to start. Then the OS will do the 
> allocation
> 
> Thanks for the reply I only have one worker.
> 
> I can?t do workers 3 on my system because I would have to disable the 
> cache as it won?t do rock cache. This system does not support rock cache.

That is odd. The system features used by "rock" cache controller are the 
same ones used by "workers 2".

I suggest looking into why "rock" is not usable. It is the best cache 
type for your needs.


> 
> Worker directive does work with cache disabled but if I have it set to 
> diskd and enable workers, I also have to increase buffering to do that.

"diskd" controller does not support SMP (simple multi-process) which is 
what the "workers" directive turns on when given values greater than 1.



> Can I add workers on diskd?

Both No and Yes.

No, in that you cannot just configure "workers 3" on a proxy with a 
diskd cache_dir.

Yes, in that you can restrict the cache_dir line with "if 
${process_number} = N"  conditions so only **one** worker will attempt 
to use that storage location.


> It says assertion failed: Controller cc:930 
> failed if I try. ?So I assume I am restricted to use disk cache or use 
> workers right?

See above.


Cheers
Amos

From jonathanlee571 at gmail.com  Tue Jan 14 06:07:42 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Mon, 13 Jan 2025 22:07:42 -0800
Subject: [squid-users] Squid workers on non cache dir rock system
In-Reply-To: <40e3397c-0044-4ac1-a06c-824329414a5b@measurement-factory.com>
References: <40e3397c-0044-4ac1-a06c-824329414a5b@measurement-factory.com>
Message-ID: <FE420DF8-2588-40D4-9FDC-52927D8B66DE@gmail.com>

Thank you all for your help in my computer science educational research trials and errors. 

This helped a lot. 
Sent from my iPhone

> On Jan 13, 2025, at 17:52, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> ?On 2025-01-13 13:29, Jonathan Lee wrote:
> 
>> Is there anyway to use more workers on a non rock system, without
>> disabling the cache? I can use them when cache is disabled. Without
>> it I get assertion failed: controller:cc:930: EX&quot
> 
> 
> I will try to clarify in hope to reduce misunderstanding, especially by others reading the above question outside your environment context:
> 
> 1. Using multiple workers without cache_dir is supported. This includes both memory-caching and non-caching Squid configurations.
> 
> 2. Using multiple workers with rock cache_dir is also supported.
> 
> 3. Using multiple workers with non-rock cache_dir is not supported. For some definitions of "work", in some deployment environments, one may be able to use such unsupported configurations (e.g., by using configuration macros to create worker-specific cache_dirs), but I do not recommend those unsupported tricks and am not going to detail them here. Sooner or later, those tricks may stop working without notice and with bad side effects.
> 
> 4. Using a single worker with rock or ufs cache_dir is supported. Some believe that aufs and diskd cache_dirs are also supported. I am not going to argue, but do not recommend switching to those cache_dir types.
> 
> 
>> My system says this proxy does not support rock and coss
> 
> Is that a Squid-generated error/message or an error/message generated by some other software? If it is the former, please quote the exact Squid message. If it is the latter, perhaps the authors of that other software can help you enable rock cache_dir?
> 
> 
> N.B. Modern Squids do not support COSS cache_dirs.
> 
> 
> HTH,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users

From jonathanlee571 at gmail.com  Tue Jan 14 07:14:28 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Mon, 13 Jan 2025 23:14:28 -0800
Subject: [squid-users] Cache dir
In-Reply-To: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>
References: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>
Message-ID: <D37FB9D5-BB38-4320-B3A6-E7E501F8BE8E@gmail.com>

Yes, in that you can restrict the cache_dir line with "if ${process_number} = N"  conditions so only **one** worker will attempt to use that storage location.

I did this and created a new location for a second cache however it would not allow me to create the folders it would not generate them I assumed it would auto generate them, it has user name and chmod done it?s on the same drive the other cache is located just a different partition. The workers started everything worked as planed but it would not create the sub folders. Is there a manual command to auto generate them outside of squid -z? 

Sent from my iPhone

> On Jan 13, 2025, at 22:12, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> Yes, in that you can restrict the cache_dir line with "if ${process_number} = N"  conditions so only **one** worker will attempt to use that storage location.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250113/a2fdab13/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan 14 22:23:16 2025
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Jan 2025 11:23:16 +1300
Subject: [squid-users] Cache dir
In-Reply-To: <D37FB9D5-BB38-4320-B3A6-E7E501F8BE8E@gmail.com>
References: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>
 <D37FB9D5-BB38-4320-B3A6-E7E501F8BE8E@gmail.com>
Message-ID: <c514f9ce-a165-4ecc-8d29-87bd3c100a30@treenet.co.nz>

On 14/01/25 20:14, Jonathan Lee wrote:
> Yes, in that you can restrict the cache_dir line with "if 
> ${process_number} = N" ?conditions so only **one** worker will attempt 
> to use that storage location.
> 
> I did this and created a new location for a second cache however it 
> would not allow me to create the folders it would not generate them I 
> assumed it would auto generate them, it has user name and chmod done 
> it?s on the same drive the other cache is located just a different 
> partition. The workers started everything worked as planed but it would 
> not create the sub folders. Is there a manual command to auto generate 
> them outside of squid -z?
> 

Oh. You can workaround that by making a foo.conf file that contains only 
the cache_dir line and running "squid -z -f /path/to/X.conf" to generate 
it. After that the worker can use it.

HTH
Amos



From jonathanlee571 at gmail.com  Wed Jan 15 03:23:37 2025
From: jonathanlee571 at gmail.com (jonathanlee571 at gmail.com)
Date: Tue, 14 Jan 2025 19:23:37 -0800
Subject: [squid-users] Cache dir
In-Reply-To: <c514f9ce-a165-4ecc-8d29-87bd3c100a30@treenet.co.nz>
References: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>
 <D37FB9D5-BB38-4320-B3A6-E7E501F8BE8E@gmail.com>
 <c514f9ce-a165-4ecc-8d29-87bd3c100a30@treenet.co.nz>
Message-ID: <000301db66fc$de9f86b0$9bde9410$@gmail.com>

Thanks that fixed my issue 

-----Original Message-----
From: Amos Jeffries <squid3 at treenet.co.nz> 
Sent: Tuesday, January 14, 2025 2:23 PM
To: Jonathan Lee <jonathanlee571 at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache dir

On 14/01/25 20:14, Jonathan Lee wrote:
> Yes, in that you can restrict the cache_dir line with "if 
> ${process_number} = N"  conditions so only **one** worker will attempt 
> to use that storage location.
> 
> I did this and created a new location for a second cache however it 
> would not allow me to create the folders it would not generate them I 
> assumed it would auto generate them, it has user name and chmod done 
> it?s on the same drive the other cache is located just a different 
> partition. The workers started everything worked as planed but it 
> would not create the sub folders. Is there a manual command to auto 
> generate them outside of squid -z?
> 

Oh. You can workaround that by making a foo.conf file that contains only the cache_dir line and running "squid -z -f /path/to/X.conf" to generate it. After that the worker can use it.

HTH
Amos




From ankor2023 at gmail.com  Thu Jan 16 04:09:28 2025
From: ankor2023 at gmail.com (Andrey K)
Date: Thu, 16 Jan 2025 07:09:28 +0300
Subject: [squid-users] Cache dir
In-Reply-To: <000301db66fc$de9f86b0$9bde9410$@gmail.com>
References: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>
 <D37FB9D5-BB38-4320-B3A6-E7E501F8BE8E@gmail.com>
 <c514f9ce-a165-4ecc-8d29-87bd3c100a30@treenet.co.nz>
 <000301db66fc$de9f86b0$9bde9410$@gmail.com>
Message-ID: <CADJd0Y3LeBdOXLmyAW847qBxeqyMYb5qFVXSD8Qn=X3rmhqi4Q@mail.gmail.com>

Hello, Jonathan,

> I can?t do workers 3 on my system because I would have to disable the
cache as it won?t do rock cache. This system does not support rock cache.

Why do you think that your system does not support the rock cache?
As far as I know, the rock cache is a feature of squid, not the operating
system.
https://wiki.squid-cache.org/Features/LargeRockStore
It is just a definition of the internal data structure in the regular file.

Kind regards,
    Ankor.






??, 15 ???. 2025??. ? 07:42, <jonathanlee571 at gmail.com>:

> Thanks that fixed my issue
>
> -----Original Message-----
> From: Amos Jeffries <squid3 at treenet.co.nz>
> Sent: Tuesday, January 14, 2025 2:23 PM
> To: Jonathan Lee <jonathanlee571 at gmail.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Cache dir
>
> On 14/01/25 20:14, Jonathan Lee wrote:
> > Yes, in that you can restrict the cache_dir line with "if
> > ${process_number} = N"  conditions so only **one** worker will attempt
> > to use that storage location.
> >
> > I did this and created a new location for a second cache however it
> > would not allow me to create the folders it would not generate them I
> > assumed it would auto generate them, it has user name and chmod done
> > it?s on the same drive the other cache is located just a different
> > partition. The workers started everything worked as planed but it
> > would not create the sub folders. Is there a manual command to auto
> > generate them outside of squid -z?
> >
>
> Oh. You can workaround that by making a foo.conf file that contains only
> the cache_dir line and running "squid -z -f /path/to/X.conf" to generate
> it. After that the worker can use it.
>
> HTH
> Amos
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250116/972b5f59/attachment.htm>

From jonathanlee571 at gmail.com  Thu Jan 16 15:23:43 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Thu, 16 Jan 2025 07:23:43 -0800
Subject: [squid-users] Cache dir
In-Reply-To: <CADJd0Y3LeBdOXLmyAW847qBxeqyMYb5qFVXSD8Qn=X3rmhqi4Q@mail.gmail.com>
References: <CADJd0Y3LeBdOXLmyAW847qBxeqyMYb5qFVXSD8Qn=X3rmhqi4Q@mail.gmail.com>
Message-ID: <916FD397-2C74-4986-A410-FCFE954AE230@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250116/5b5d3c67/attachment.htm>

From ankor2023 at gmail.com  Fri Jan 17 03:32:53 2025
From: ankor2023 at gmail.com (Andrey K)
Date: Fri, 17 Jan 2025 06:32:53 +0300
Subject: [squid-users] Cache dir
In-Reply-To: <916FD397-2C74-4986-A410-FCFE954AE230@gmail.com>
References: <CADJd0Y3LeBdOXLmyAW847qBxeqyMYb5qFVXSD8Qn=X3rmhqi4Q@mail.gmail.com>
 <916FD397-2C74-4986-A410-FCFE954AE230@gmail.com>
Message-ID: <CADJd0Y3fBUc3WfJk_9XTs+T=SG9=1F8rWMsUhgccWCw2CCkUMw@mail.gmail.com>

Hello, Jonathan,

> It says this proxy does not support rock when I manually enable it. Squid
is installed on pfSense plus with an arm processor.

OK, It looks like the pfSense squid was built without rock cache support.
Thank you.

Kind regards,
Ankor.



??, 16 ???. 2025??. ? 18:23, Jonathan Lee <jonathanlee571 at gmail.com>:

> It says this proxy does not support rock when I manually enable it. Squid
> is installed on pfSense plus with an arm processor.
> Sent from my iPhone
>
> On Jan 15, 2025, at 20:09, Andrey K <ankor2023 at gmail.com> wrote:
>
> ?
> Hello, Jonathan,
>
> > I can?t do workers 3 on my system because I would have to disable the
> cache as it won?t do rock cache. This system does not support rock cache.
>
> Why do you think that your system does not support the rock cache?
> As far as I know, the rock cache is a feature of squid, not the operating
> system.
> https://wiki.squid-cache.org/Features/LargeRockStore
> It is just a definition of the internal data structure in the regular file.
>
> Kind regards,
>     Ankor.
>
>
>
>
>
>
> ??, 15 ???. 2025??. ? 07:42, <jonathanlee571 at gmail.com>:
>
>> Thanks that fixed my issue
>>
>> -----Original Message-----
>> From: Amos Jeffries <squid3 at treenet.co.nz>
>> Sent: Tuesday, January 14, 2025 2:23 PM
>> To: Jonathan Lee <jonathanlee571 at gmail.com>
>> Cc: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Cache dir
>>
>> On 14/01/25 20:14, Jonathan Lee wrote:
>> > Yes, in that you can restrict the cache_dir line with "if
>> > ${process_number} = N"  conditions so only **one** worker will attempt
>> > to use that storage location.
>> >
>> > I did this and created a new location for a second cache however it
>> > would not allow me to create the folders it would not generate them I
>> > assumed it would auto generate them, it has user name and chmod done
>> > it?s on the same drive the other cache is located just a different
>> > partition. The workers started everything worked as planed but it
>> > would not create the sub folders. Is there a manual command to auto
>> > generate them outside of squid -z?
>> >
>>
>> Oh. You can workaround that by making a foo.conf file that contains only
>> the cache_dir line and running "squid -z -f /path/to/X.conf" to generate
>> it. After that the worker can use it.
>>
>> HTH
>> Amos
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> https://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250117/3cb54771/attachment.htm>

From uwe.hering at cgi.com  Sat Jan 18 20:22:06 2025
From: uwe.hering at cgi.com (Hering, Uwe)
Date: Sat, 18 Jan 2025 20:22:06 +0000
Subject: [squid-users] squid-6.10-150600.3.6.1.src.rpm and ident
In-Reply-To: <mailman.3.1734350401.725789.squid-users@lists.squid-cache.org>
References: <mailman.3.1734350401.725789.squid-users@lists.squid-cache.org>
Message-ID: <ab6f1f2838744d909ee10e9ac8d3e1c3@cgi.com>

Trusted 3rd Party

Hello all,

>This is a known bug. There was a pull request fixing it in master/v7[1], but it was decided that it is best to remove IDENT support from v7 instead of improving IDENT code. I think it was the right >decision.

>Squid v6 still has IDENT support. I cannot offer an official fix for Squid v6, but I know that the following _unofficial_ patch applies to the latest v6 sources. I have not tested whether that patch fixes the >bug in v6.

Since IDENT won't be available any longer with v7 - how are other users using an IDENT-setup today are changing their setups? I am still looking for a simple way. One way could be using external_acl_type and write some specific helper.

Regards, Uwe

From jonathanlee571 at gmail.com  Sun Jan 19 05:57:45 2025
From: jonathanlee571 at gmail.com (Jonathan Lee)
Date: Sun, 19 Jan 2025 05:57:45 +0000
Subject: [squid-users] SSL_Bump
Message-ID: <DS0PR19MB72705B1DDCED8357A1259AE3A7E42@DS0PR19MB7270.namprd19.prod.outlook.com>

Hello Fellow Squid Users can you please help?

Is there a better way to configure the access control lists?

ssl_bump peek step1
ssl_bump terminate SSL_Intercept_Terminate
miss_access deny no_miss active_use
ssl_bump splice splice_main active_use
ssl_bump bump bump_main active_use
acl activated note active_use true
ssl_bump terminate !activated
ssl_bump server-first all

Peek at step one get the http get request

Terminate the list I have configured (acl not seen)

Do not store any logins or etc (acl not seen)

Splice banks etc sites that ethically need to be spliced always that are used, and some devices that area always spliced iphones cell phones etc

Bump always devices and not the urls seens above they are spliced always.

Create my note as an acl as all ssl_bump items are noted with active_use

Now terminate everything that is not marked active (just a security precaution should the cache get an invasive container or something just a backup.

---> does server-first all need to be included ?

The ssl spice is super fast however bump is somewhat sluggish.

The goal here is to splice specific sites regardless of if it is a bump device or a splice always device,

Splice some devices all the time, and bump some devices all the time.

Thanks for your time.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250119/19bd5970/attachment.htm>

From huaraz at moeller.plus.com  Sun Jan 19 10:37:42 2025
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 19 Jan 2025 10:37:42 -0000
Subject: [squid-users] Assistance Required: Issues with Squid Kerberos +
 LDAP Group Configuration
In-Reply-To: <PAWPR03MB9010DF5EEC64C9A281A03B24F4152@PAWPR03MB9010.eurprd03.prod.outlook.com>
References: <PAWPR03MB9010DF5EEC64C9A281A03B24F4152@PAWPR03MB9010.eurprd03.prod.outlook.com>
Message-ID: <vmiklp$9dh$1@ciao.gmane.io>

Hi Enfal,

          Do you run also samba on the server ?   If so samba may change the AD host entry to which your keytab is associated. This means your keytab gets out of sync with AD.

Markus


"Enfal Gok" <enfal.gok2004 at gmail.com> wrote in message news:PAWPR03MB9010DF5EEC64C9A281A03B24F4152 at PAWPR03MB9010.eurprd03.prod.outlook.com...
Dear Squid Community/Support Team,
I am currently configuring Squid with Kerberos authentication and LDAP group-based access control. However, I am encountering persistent issues, and I would greatly appreciate your guidance. Below are the details of my configuration and the errors I am facing.

--------------------------------------------------------------------------------

Error Logs
The following errors repeatedly appear in the Squid logs:
2025/01/03 19:35:40 kid1| Starting new helpers
2025/01/03 19:35:40 kid1| helperOpenServers: Starting 1/5 'ext_kerberos_ldap_group_acl' processes
support_sasl.cc(276): pid=70855 :2025/01/03 19:35:40| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error
support_ldap.cc(1086): pid=70855 :2025/01/03 19:35:40| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error
support_ldap.cc(1172): pid=70855 :2025/01/03 19:35:40| kerberos_ldap_group: ERROR: Error while binding to ldap server with Username/Password: Encoding error
(ext_kerberos_ldap_group_acl): ../../../../libraries/liblber/io.c:108: ber_write: Assertion `buf != NULL' failed.
2025/01/03 19:35:41 kid1| WARNING: external_acl_type #Hlpr7 exited
2025/01/03 19:35:41 kid1| Too few external_acl_type processes are running (need 1/5)

--------------------------------------------------------------------------------

Current Configuration
Kerberos Authentication
 auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth -s HTTP/ubuntuserver.demo.local
auth_param negotiate children 10
auth_param negotiate keep_alive on
External ACL for LDAP Groups
 external_acl_type kerberos_ldap_group ttl=3600 negative_ttl=3600 %LOGIN /usr/lib/squid/ext_kerberos_ldap_group_acl \    -P HTTP/ubuntuserver.demo.local at DEMO.LOCAL \    -D demo.local \    -b DC=demo,DC=local \    -l ldap://dc.demo.local \    -g FullAccess at DEMO.LOCAL:Restricted at DEMO.LOCAL:Filtered at DEMO.LOCAL:Blocked at DEMO.LOCAL
ACL Definitions
 acl FullAccess external kerberos_ldap_group FullAccess at DEMO.LOCAL
acl Restricted external kerberos_ldap_group Restricted at DEMO.LOCAL
acl Filtered external kerberos_ldap_group Filtered at DEMO.LOCAL
acl Blocked external kerberos_ldap_group Blocked at DEMO.LOCAL

acl allowed_sites dstdomain .benedictuspoort.be .smartschool.be .microsoft.com
acl bad_sites dstdomain .adult.com .gambling.com
Access Rules
 http_access allow FullAccess
http_access allow Restricted allowed_sites
http_access deny Restricted
http_access deny Blocked
http_access deny Filtered bad_sites
http_access allow Filtered
http_access deny all
Proxy Settings
 http_port 3128
cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid

--------------------------------------------------------------------------------

What I Have Tried
  a.. Verified that the Kerberos keytab is up-to-date and matches the Key Version Number (msDS-KeyVersionNumber) in Active Directory. 
  b.. Tested LDAP queries using ldapsearch with both simple and GSSAPI bindings, which work intermittently. 
  c.. Checked Squid logs and confirmed that Kerberos tickets are being issued successfully using kinit and klist.
Despite these efforts, the ext_kerberos_ldap_group_acl helper is unable to bind to the LDAP server, and the Squid service keeps restarting helpers.

--------------------------------------------------------------------------------

Request for Assistance
Could you please provide guidance on:
  1.. Debugging the ext_kerberos_ldap_group_acl helper? 
  2.. Ensuring compatibility between Kerberos and LDAP for group-based access control? 
  3.. Any potential misconfigurations or missing steps in my setup?
Thank you in advance for your assistance. I look forward to your recommendations.
Kind regards,
Enfal gok 


--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250119/8eb45030/attachment-0001.htm>

From stu.lists at spacehopper.org  Tue Jan 21 00:57:29 2025
From: stu.lists at spacehopper.org (Stuart Henderson)
Date: Tue, 21 Jan 2025 00:57:29 -0000 (UTC)
Subject: [squid-users] Cache dir
References: <a36d42d8-8b79-42eb-9ca0-6a8ea42f0b92@treenet.co.nz>
 <D37FB9D5-BB38-4320-B3A6-E7E501F8BE8E@gmail.com>
 <c514f9ce-a165-4ecc-8d29-87bd3c100a30@treenet.co.nz>
 <000301db66fc$de9f86b0$9bde9410$@gmail.com>
 <CADJd0Y3LeBdOXLmyAW847qBxeqyMYb5qFVXSD8Qn=X3rmhqi4Q@mail.gmail.com>
Message-ID: <slrnvots7p.27s6.stu.lists@naiad.spacehopper.org>

On 2025-01-16, Andrey K <ankor2023 at gmail.com> wrote:
>
>> I can=E2=80=99t do workers 3 on my system because I would have to disable=
>  the
> cache as it won=E2=80=99t do rock cache. This system does not support rock =
> cache.
>
> Why do you think that your system does not support the rock cache?
> As far as I know, the rock cache is a feature of squid, not the operating
> system.
> https://wiki.squid-cache.org/Features/LargeRockStore
> It is just a definition of the internal data structure in the regular file.

With some OS you can run into problems with rock due to the default
buffer size used for unix sockets (it used to result in errors at least
on OpenBSD, though the default there has since been bumped and it now
works without sysctl changes).



From shelokov.ilya at yandex.ru  Tue Jan 21 09:48:46 2025
From: shelokov.ilya at yandex.ru (=?utf-8?B?0JjQu9GM0Y8g0KnQtdC70L7QutC+0LI=?=)
Date: Tue, 21 Jan 2025 12:48:46 +0300
Subject: [squid-users] squid_icap to icap to system
Message-ID: <1859141737452057@mail.yandex.ru>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250121/2693b025/attachment.htm>

From shelokov.ilya at yandex.ru  Tue Jan 21 10:41:11 2025
From: shelokov.ilya at yandex.ru (=?utf-8?B?0JjQu9GM0Y8g0KnQtdC70L7QutC+0LI=?=)
Date: Tue, 21 Jan 2025 13:41:11 +0300
Subject: [squid-users] squid_icap to icap to system_2
Message-ID: <322641737455939@mail.yandex.ru>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250121/e40352f7/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan 21 13:56:21 2025
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Jan 2025 08:56:21 -0500
Subject: [squid-users] squid_icap to icap to system_2
In-Reply-To: <322641737455939@mail.yandex.ru>
References: <322641737455939@mail.yandex.ru>
Message-ID: <542d6cc3-ef8f-47c1-9366-de9924010378@measurement-factory.com>

On 2025-01-21 05:41, ???? ??????? wrote:

> I have a squid proxy, it sends data to the system via icap. I need to 
> install another squid between the proxy and the system so that it 
> receives icap from the proxy

Squid is an ICAP client: it sends ICAP requests and received ICAP responses.

Squid is not an ICAP server: Squid does not receive ICAP requests and 
does not send ICAP responses.


> filters the icap by the ip of the external 
> sender (original IP) + packet size (no more than 200 bytes) and also 
> sends already filtered traffic to the system via icap. Please tell me if 
> this is possible, and if so, how to do it.

It is possible to write an ICAP proxy program that does what you want, 
but Squid is not such a program, and I doubt such a program exists today.


HTH,

Alex.


> Current version of squid is 6.6
> P.S.
> It is not possible to filter icap traffic directly on the main proxy due 
> to some restrictions.
> Best regards, Ilya
> 

From airween at gmail.com  Tue Jan 28 20:29:11 2025
From: airween at gmail.com (=?UTF-8?Q?Ervin_Heged=C3=BCs?=)
Date: Tue, 28 Jan 2025 21:29:11 +0100
Subject: [squid-users] Allowing URL with url_regex does not work
Message-ID: <CAJ2uXbeps_FK_AdWnHh=iaFSJo_RVenHaQxas=E2DO4QiYThRQ@mail.gmail.com>

Hi there,

I would like to allow a specific URL for a specific client. This is how I
try to do that:

acl wordpressgravity url_regex -i
^https?://s3\.amazonaws\.com\/gravityforms\/releases\/.*
acl vmapache1 src 172.30.40.5/32
http_access allow vmapache1 wordpressgravity

The URL is something like this:

https://s3.amazonaws.com/gravityforms/releases/gravityforms_2.9.2.zip?AWSAccessKeyId=AKblahblah4F&Expires=1712345678&Signature=0cblahblah%3D

but it does not work - I always get 403.

What do I do wrong?

Thanks,

a.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250128/92ccbffe/attachment.htm>

From gkinkie at gmail.com  Tue Jan 28 21:56:21 2025
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Tue, 28 Jan 2025 21:56:21 +0000
Subject: [squid-users] Allowing URL with url_regex does not work
In-Reply-To: <CAJ2uXbeps_FK_AdWnHh=iaFSJo_RVenHaQxas=E2DO4QiYThRQ@mail.gmail.com>
References: <CAJ2uXbeps_FK_AdWnHh=iaFSJo_RVenHaQxas=E2DO4QiYThRQ@mail.gmail.com>
Message-ID: <CA+Y8hcP9brRyO=Unq8spzod64F13eAQzui_DvVt8zLyo9CcdYQ@mail.gmail.com>

On Tue, Jan 28, 2025 at 9:47?PM Ervin Heged?s <airween at gmail.com> wrote:

> Hi there,
>
> I would like to allow a specific URL for a specific client. This is how I
> try to do that:
>
> acl wordpressgravity url_regex -i
> ^https?://s3\.amazonaws\.com\/gravityforms\/releases\/.*
> acl vmapache1 src 172.30.40.5/32
> http_access allow vmapache1 wordpressgravity
>

Unless you are using SSL man-in-the-middle, Squid never sees the request
URL for https.
What it sees is a CONNECT request to the domain s3.amazonaws.com; it never
sees the full path.
This is intentional, by design of https.


> The URL is something like this:
>
>
> https://s3.amazonaws.com/gravityforms/releases/gravityforms_2.9.2.zip?AWSAccessKeyId=AKblahblah4F&Expires=1712345678&Signature=0cblahblah%3D
>
> but it does not work - I always get 403.
>
> What do I do wrong?
>

You can filter to the domain, using a combination of 3 acls:
- a CONNECT acl
- the vmapache1 acl
- a dstdomain acl for s3.amazonaws.com

It's not possible to filter the path

-- 
    Francesco
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20250128/5c6189ff/attachment.htm>

From airween at gmail.com  Wed Jan 29 10:34:35 2025
From: airween at gmail.com (Ervin =?utf-8?Q?Heged=C3=BCs?=)
Date: Wed, 29 Jan 2025 11:34:35 +0100
Subject: [squid-users] Allowing URL with url_regex does not work
In-Reply-To: <CA+Y8hcP9brRyO=Unq8spzod64F13eAQzui_DvVt8zLyo9CcdYQ@mail.gmail.com>
References: <CAJ2uXbeps_FK_AdWnHh=iaFSJo_RVenHaQxas=E2DO4QiYThRQ@mail.gmail.com>
 <CA+Y8hcP9brRyO=Unq8spzod64F13eAQzui_DvVt8zLyo9CcdYQ@mail.gmail.com>
Message-ID: <20250129103435.u4z5e6lycyo6nmok@arxnet.hu>

Hi,

On Tue, Jan 28, 2025 at 09:56:21PM +0000, Francesco Chemolli wrote:
> On Tue, Jan 28, 2025 at 9:47?PM Ervin Heged?s <airween at gmail.com> wrote:
> >
> > acl wordpressgravity url_regex -i
> > ^https?://s3\.amazonaws\.com\/gravityforms\/releases\/.*
> > acl vmapache1 src 172.30.40.5/32
> > http_access allow vmapache1 wordpressgravity
> >
> 
> Unless you are using SSL man-in-the-middle, Squid never sees the request
> URL for https.
> What it sees is a CONNECT request to the domain s3.amazonaws.com; it never
> sees the full path.
> This is intentional, by design of https.

ah, thanks - I thought that's not possible, but I saw there are
many post/comments in forums where other admins used this form,
and mentioned that worked... But now it's clear.
 
> You can filter to the domain, using a combination of 3 acls:
> - a CONNECT acl
> - the vmapache1 acl
> - a dstdomain acl for s3.amazonaws.com
> 
> It's not possible to filter the path

yeah, that works for me.


Thanks again.


a.
 

